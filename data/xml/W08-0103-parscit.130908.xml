<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003325">
<title confidence="0.999554">
Learning N-Best Correction Models from Implicit User Feedback
in a Multi-Modal Local Search Application
</title>
<author confidence="0.998955">
Dan Bohus, Xiao Li, Patrick Nguyen, Geoffrey Zweig
</author>
<affiliation confidence="0.971923">
Microsoft Research
</affiliation>
<address confidence="0.942192">
One Microsoft Way
Redmond, WA, 98052
</address>
<email confidence="0.967076">
{dbohus, xiaol, panguyen, gzweig}@microsoft.com
</email>
<sectionHeader confidence="0.995194" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999926933333333">
We describe a novel n-best correction model
that can leverage implicit user feedback (in
the form of clicks) to improve performance in
a multi-modal speech-search application. The
proposed model works in two stages. First, the
n-best list generated by the speech recognizer
is expanded with additional candidates, based
on confusability information captured via user
click statistics. In the second stage, this ex-
panded list is rescored and pruned to produce
a more accurate and compact n-best list. Re-
sults indicate that the proposed n-best correc-
tion model leads to significant improvements
over the existing baseline, as well as other tra-
ditional n-best rescoring approaches.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999853466666667">
Supported by years of research in speech recogni-
tion and related technologies, as well as advances
in mobile devices, speech-enabled mobile applica-
tions are finally transitioning into day-to-day use.
One example is Live Search for Windows Mobile
(2008), a speech-enabled application that allows
users to get access to local information by speaking
a query into their device. Several other systems
operating in similar domains have recently become
available (TellMeByMobile, 2008; Nuance Mobile
Search, 2008; V-Lingo Mobile, 2008; VoiceSignal
Search, 2008.)
Traditionally, multi-modal systems leverage the
additional input channels such as text or buttons to
compensate for the current shortcomings of speech
</bodyText>
<page confidence="0.985885">
21
</page>
<bodyText confidence="0.999917029411765">
recognition technology. For instance, after the user
speaks a query, the Live Search for Windows Mo-
bile application displays a confirmation screen that
contains the n-best recognition results. The user
selects the correct hypothesis using the buttons on
the device, and only then the system displays the
corresponding search results (see Figure 1.)
We argue that ideally multi-modal systems
could use the additional, more accurate input chan-
nels not only for confirmation or immediate cor-
rection, but also to learn from the interaction and
improve their performance over time, without ex-
plicit human supervision. For example, in the inte-
raction paradigm described above, apart from
providing the means for selecting the correct rec-
ognition result from an n-best list, the user click on
a hypothesis can provide valuable information
about the errors made by system, which could be
exploited to further improve performance.
Consider for instance the following numbers
from an analysis of logged click data in the Live
Search for Windows Mobile system. Over a certain
period of time, the results Beer and Gear were dis-
played together in an n-best list 122 times. Out of
these cases, Beer was clicked 67% of the time, and
Gear was never clicked. In 25% of the cases when
Beer was selected, Gear was incorrectly presented
above (i.e. higher than) Beer in the n-best list.
More importantly, there are also 82 cases in which
Gear appears in an n-best list, but Beer does not. A
manual inspection reveals that, in 22% of these
cases, the actual spoken utterance was indeed Beer.
The clicks therefore indicate that the engine often
misrecognizes Gear instead of Beer.
</bodyText>
<note confidence="0.7106145">
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 21–28,
Columbus, June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999732347826087">
Ideally, the system should be able to take advan-
tage of this information and use the clicks to create
an automatic positive feedback loop. We can envi-
sion several ways in which this could be accom-
plished. A possible approach would be to use all
the clicked results to adapt the existing language or
acoustic models. Another, higher-level approach is
to treat the recognition process as a black-box, and
use the click feedback (perhaps also in conjunction
with other high-level information) to post-process
the results recognition results.
While both approaches have their merits, in this
work we concentrate on the latter paradigm. We
introduce a novel n-best correction model that le-
verages the click data to improve performance in a
speech-enabled multi-modal application. The pro-
posed model works in two stages. First, the n-best
list generated by the speech recognizer is expanded
with additional candidates, based on results confu-
sability information captured by the click statistics.
For instance, in the 82 cases we mentioned above
when Gear was present in the n-best list but Beer
was not, Beer (as well as potentially other results)
would also be added to form an expanded n-best
list. The expanded list is then rescored and pruned
to construct a corrected, more accurate n-best list.
The proposed approach, described in detail in
Section 3, draws inspiration from earlier work in
post-recognition error-correction models (Ringger
and Allen, 1996; Ringger and Allen, 1997) and n-
best rescoring (Chotimongkol and Rudnicky, 2001;
Birkenes et al., 2007). The novelty of our approach
lies in: (1) the use of user click data in a deployed
multi-modal system for creating a positive feed-
back loop, and (2) the development of an n-best
correction model based on implicit feedback that
outperforms traditional rescoring-only approaches.
Later on, in Section 5, we will discuss in more de-
tail the relationship of the proposed approach to
these and other works previously reported in the
literature.
Before moving on to describe the n-best correc-
tion model in more detail, we give a high-level
overview of Live Search for Windows Mobile, the
multi-modal, mobile local search application that
provided the test-bed for evaluating this work.
</bodyText>
<sectionHeader confidence="0.906095" genericHeader="method">
2 Live Search for Windows Mobile
</sectionHeader>
<bodyText confidence="0.99896888">
Live Search for Windows Mobile is an application
that enables local web-search on mobile devices. In
its current version, it allows users to find informa-
tion about local businesses and restaurants, to ob-
tain driving directions, explore maps, view current
traffic, get movie show-times, etc. A number of
screen-shots are illustrated in Figure 1.
Recently, Live Search for Windows Mobile has
been extended with a speech interface (notice the
Speak button assigned to the left soft-key in Figure
1.a.) The speech-based interaction with the system
proceeds as follows: the user clicks the Speak but-
ton and speaks the name of a local business, for
instance A-B-C Hauling, or a general category such
as Vietnamese Restaurants. The application end-
points the audio and forwards it over the data
channel to a server (Figure 1.b.) Recognition is
performed on the server side, and the resulting n-
best list is sent back to the client application, where
it is displayed to the user (Figure 1.c.) The user can
select the correct item from the n-best list, re-speak
the request, or abandon the interaction altogether
by pressing Cancel. Once the user selects an item in
the n-best list, the corresponding search results are
displayed (Figure 1.d.)
</bodyText>
<figure confidence="0.925303">
(a) (b) (c) (d)
</figure>
<figureCaption confidence="0.9860305">
Figure 1. Windows Live Search for Mobile. (a) initial screen; (b) user is speaking a request; (c) n-best list
is presented; (d) final search results are displayed
</figureCaption>
<page confidence="0.992034">
22
</page>
<bodyText confidence="0.999969058823529">
Apart from business names, the system also
handles speech input for addresses, as well as
compound requests, such as Shamiana Restaurant
in Kirkland, Washington. For the latter cases, a
two-tier recognition and confirmation process is
used. In the first stage a location n-best list is gen-
erated and sent to the client for confirmation. After
the user selects the location, a second recognition
stage uses a grammar tailored to that specific loca-
tion to re-recognize the utterance. The client then
displays the final n-best list from which the user
can select the correct result.
Several details about the system architecture and
the structure of the recognition process have been
omitted here due to space considerations. For the
interested reader, a more in-depth description of
this system is available in (Acero et al., 2008).
</bodyText>
<sectionHeader confidence="0.997392" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.998306">
We now turn our attention to the proposed n-best
correction model
</bodyText>
<subsectionHeader confidence="0.99393">
3.1 Overview
</subsectionHeader>
<bodyText confidence="0.999992368421053">
The model works in two stages, illustrated in Fig-
ure 2. In the first stage the n-best list produced by
the speech recognizer is expanded with several
alternative hypotheses. In the second stage, the
expanded n-best list is rescored to construct the
final, corrected n-best list.
The n-best expansion step relies on a result con-
fusion matrix, constructed from click information.
The matrix, which we will describe in more detail
in the following subsection, contains information
about which result was selected (clicked) by the
user when a certain result was displayed. For in-
stance, in the example from Figure 2, the matrix
indicates that when Burlington appeared in the n-
best list, Bar was clicked once, Bowling was
clicked 13 times, Burger King was clicked twice,
and Burlington was clicked 15 times (see hashed
row in matrix.) The last element in the row indi-
cates that there were 7 cases in which Burlington
was decoded, but nothing (∅) was clicked. Essen-
tially, the matrix captures information about the
confusability of different recognition results.
The expansion step adds to an n-best list gener-
ated by the recognizer all the results that were pre-
viously clicked in conjunction with any one of the
items in the given n-best list. For instance, in the
example from Figure 2, the n-best list contains
Sterling, Stirling, Burlington and Cooling. Based
on the confusion matrix, this list will be expanded
to also include Bar, Bowling, Burger King, Tow-
ing, and Turley. In this particular case, the correct
recognition result, Bowling, is added in the ex-
panded n-best list.
In the final step, the expanded list is rescored. In
the previous example, for simplicity of explana-
tion, a simple heuristic for re-scoring was used:
add all the counts on the columns corresponding to
each expanded result. As a consequence, the cor-
</bodyText>
<figure confidence="0.998117047619048">
Bar Bowling Burlington Sterling Stirling Towing ∅
Burger King Turley
Initial
N-Best
1 ... 13 2 15 ... 0 0 ... 0 0 7
0 ... 7 0 0 ... 0 0 ... 1 0 9
0 ... 4 0 0 ... 10 1 ... 2 2 5
0 ... 4 0 0 ... 4 1 ... 0 0 9
Result Confusion Matrix
Sterling
Stirling
Burlington
Cooling
Cooling
Burlington
+
Sterling
Stirling
Bar
Bowling
Burger King
Burlington
Sterling
Stirling
Towing
Turley
Expanded
N-Best
Bowling 28
Burlington 15
Sterling 14
Towing 3
Burger King 2
Stirling 2
Turley 2
Bar 1
Corrected
(expanded &amp;
rescored)
N-Best
4
4
</figure>
<figureCaption confidence="0.9991405">
Stage 1: Expansion Stage 2: Rescoring
Figure 2. A confusion-based n-best correction model
</figureCaption>
<page confidence="0.984641">
23
</page>
<bodyText confidence="0.9999049">
rect recognition result, Bowling, was pushed to the
top of the n-best list.
We begin by formally describing the construc-
tion of the results confusability matrix and the ex-
pansion process in the next two sub-sections. Then,
we describe three rescoring approaches. The first
one is based on an error-correction model con-
structed from the confusion matrix. The other two,
are more traditional rescoring approaches, based
on language model adaptation.
</bodyText>
<subsectionHeader confidence="0.999058">
3.2 The Result Confusion Matrix
</subsectionHeader>
<bodyText confidence="0.994738615384615">
The result confusion matrix is computed in a sim-
ple traversal of the click logs. The rows in the ma-
trix correspond to decoded results, i.e. results that
have appeared in an n-best list. The columns in the
matrix correspond to clicked (or intended) results,
i.e. results that the user has clicked on in the n-best
list. The entries at the intersection of row d and
column c correspond to the number of times result
c was clicked when result d was decoded:
md ,c = #(decoded = d, clicked = c).
In addition, the last column in the matrix, de-
noted 0 contains the number of times no result was
clicked when result d was displayed:
</bodyText>
<equation confidence="0.976122">
md,0 = #(decoded = d, clicked = 0).
</equation>
<bodyText confidence="0.996997333333333">
The rows in the matrix can therefore be used to
compute the maximum likelihood estimate for the
conditional probability distribution:
</bodyText>
<equation confidence="0.983164">
PML(c|d) = md,c
Ec md,c
</equation>
<bodyText confidence="0.999978777777778">
The full dimensions of the result confusion ma-
trix can grow very large since the matrix is con-
structed at the result level (the average number of
words per displayed result is 2.01). The number of
rows equals the number of previously decoded re-
sults, and the number of columns equals the num-
ber of previously clicked results. However, the
matrix is very sparse and can be stored efficiently
using a sparse matrix representation.
</bodyText>
<subsectionHeader confidence="0.980948">
3.3 N-Best Expansion
</subsectionHeader>
<bodyText confidence="0.999917">
The first step in the proposed n-best correction
model is to expand the initial n-best list with all
results that have been previously clicked in con-
junction with the items in the current n-best list.
Let’s denote by N = {d, },=1..n the initial n-best
list produced by the speech recognizer. Then, the
expanded n-best list EN will contain all d,, as well
as all previously clicked results c such that there
exists , with md,,c &gt; 0.
</bodyText>
<subsectionHeader confidence="0.968172">
3.4 Confusion Matrix Based Rescoring
</subsectionHeader>
<bodyText confidence="0.9998916">
Ideally, we would like to rank the hypotheses in
the expanded list EN according to P (i |a), where i
represents the intended result and a represents the
acoustics of the spoken utterance. This can be re-
written as follows:
</bodyText>
<equation confidence="0.644968">
P(il a) = Ed P(i|d) • P(d|a). [1]
</equation>
<bodyText confidence="0.999973">
The first component in this model is an error-
correction model P(i|d). This model describes the
conditional probability that the correct (or in-
tended) result is i given that result d has been de-
coded. While this conditional model cannot be
constructed directly, we can replace it by a proxy -
P (c |d), which models the probability that the re-
sult c will be clicked, given that result d was de-
coded. As mentioned earlier in subsection 3.2, this
conditional probability distribution can be com-
puted from the result confusion matrix. In replac-
ing P (i l d) with P (c |d), we are making the
assumption that the clicks correspond indeed to the
correct, intended results, and to nothing else1.
Notice that the result confusion matrix is gener-
ally very sparse. The maximum likelihood estima-
tor PML (c |d) will therefore often be inappropriate.
To address this data sparsity issue, we linearly in-
terpolate the maximum likelihood estimator with
an overall model Po (c|d):
</bodyText>
<equation confidence="0.962173">
P(cld)= APML(cld)+ (1 − A)Po(cld).
</equation>
<bodyText confidence="0.9823935">
The overall model is defined in terms of two
constants, a and fl, as follows:
</bodyText>
<equation confidence="0.893592">
Po(cld) = fa, if c = d
fl, ifc d
</equation>
<bodyText confidence="0.999609333333333">
where a is the overall probability in the whole
dataset of clicking on a given decoded result, and
fl is computed such that Po (c l d) normalizes to 1.
</bodyText>
<footnote confidence="0.993662833333333">
1 While this assumption generally holds, we have also ob-
served cases where it is violated: sometimes users (perhaps
accidentally) click on an incorrect result; other times the cor-
rect result is in the list but nothing is clicked (perhaps the user
was simply testing out the recognition capabilities of the sys-
tem, without having an actual information need)
</footnote>
<bodyText confidence="0.366156">
.
</bodyText>
<page confidence="0.993671">
24
</page>
<bodyText confidence="0.997825833333333">
Finally, the A interpolation parameter is determined
empirically on the development set.
The second component in the confusion based
rescoring model from equation [1] is P (d |a). This
is the recognition score for hypothesis d. The n-
best rescoring model from [1] becomes:
</bodyText>
<equation confidence="0.928663">
P(cl a) = Y [APmJcldr) + (1 − A)PO(cl dr)] . P(dr|a)
dr r:N
</equation>
<subsectionHeader confidence="0.561606">
3.5 Language Model Based Rescoring
</subsectionHeader>
<bodyText confidence="0.99991775">
A more traditional alternative for n-best rescoring
is to adapt the bigram language model used by the
system in light of the user click data, and re-rank
the decoded results by:
</bodyText>
<subsubsectionHeader confidence="0.459192">
P(il a) a P(drl a) a P(al dr)P(dr)
</subsubsectionHeader>
<bodyText confidence="0.999559153846154">
Here P(aldr) is the acoustic score assigned by
the recognizer to hypothesis dr, and P(dr) is the
adapted language model score for this hypothesis.
A simple approach for adapting the system’s
language model is to add the word sequences of
the user-clicked results to the original training sen-
tences and to re-estimate the language model P (d).
We will refer to this method as maximum likelih-
ood (ML) estimation. A second approach, referred
to as conditional maximum likelihood (CML) es-
timation, is to adapt the language model such as to
directly maximize the conditional likelihood of the
correct result given acoustics, i.e.,
</bodyText>
<equation confidence="0.963684">
P (a l i)P (i)
dr r:N P(al dr)P(dr)
</equation>
<bodyText confidence="0.999966666666667">
Note that this is the same objective function as
the one used in Section 3.4, except that here the
click data is used to estimate the language model
instead of the error correction model. Again, in
practice we assume that users click on correct re-
sults, i.e. i = c.
</bodyText>
<sectionHeader confidence="0.999738" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999919666666667">
We now discuss a number of experiments and the
results obtained using the proposed n-best correc-
tion approach.
</bodyText>
<subsectionHeader confidence="0.938992">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999929666666667">
For the purposes of the experiments described be-
low we extracted just over 800,000 queries from
the server logs in which the recognizer had gener-
ated a simple n-best list2. For each recognition
event, we collected from the system logs the n-best
list, and the result clicked by the user (if the user
clicked on any result).
In addition, for testing purposes, we also make
use of 11529 orthographically transcribed user re-
quests. The transcribed set was further divided into
a development set containing 5680 utterances and
a test set containing 5849 utterances.
</bodyText>
<subsectionHeader confidence="0.95214">
4.2 Initial N-Best Rescoring
</subsectionHeader>
<bodyText confidence="0.999967">
To tease apart the effects of expansion and rescor-
ing in the proposed n-best correction model, we
began by using the rescoring techniques on the
initial n-best lists, without first expanding them.
Since the actual recognition confidence scores
P (dr |a) were not available in the system logs, we
replaced them with an exponential probability den-
sity function based on the rank of the hypothesis:
</bodyText>
<equation confidence="0.851238">
P(drl a) = 2−r
</equation>
<bodyText confidence="0.999987434782609">
We then rescored the n-best lists from the test
set according to the three rescoring models de-
scribed earlier: confusion matrix, maximum like-
lihood (ML), and conditional maximum likelihood
(CML). We computed the sentence level accuracy
for the rescored n-best list, at different cutoffs. The
accuracy was measured by comparing the rescored
hypotheses against the available transcripts.
Note that the maximum depth of the n-best lists
generated by the recognizer is 10; this is the max-
imum number of hypotheses that can be displayed
on the mobile device. However, the system may
generate fewer than 10 hypotheses. The observed
average n-best list size in the test set was 4.2.
The rescoring results are illustrated in Figure 3
and reported in Table 1. The X axis in Figure 3
shows the cutoff at which the n-best accuracy was
computed. For instance in the baseline system, the
correct hypothesis was contained in the top result
in 46.2% of cases, in the top-2 results in 50.5% of
the cases and in the top-3 results in 51.5% of the
cases. The results indicate that all the rescoring
models improve performance relative to the base-
</bodyText>
<footnote confidence="0.6859132">
2 We did not consider cases where a false-recognition event
was fired (e.g. if no speech was detected in the audio signal) –
in these cases no n-best list is generated. We also did not con-
sider cases where a compound n-best was generated (e.g. for
compound requests like Shamiana in Kirkland, Washington)
</footnote>
<equation confidence="0.987508">
P(ila) =
</equation>
<page confidence="0.996728">
25
</page>
<figureCaption confidence="0.983411">
Figure 3. Initial n-best rescoring (test-set)
</figureCaption>
<table confidence="0.9977721">
Model 1- 2- 3- 10-
Best Best Best Best
0 Baseline 46.2 50.5 51.5 53.5
1 ML Rescoring 46.8 50.9 52.1 53.5
2 CML Rescoring 47.4 51.4 52.6 53.5
3 Confusion Matrix Resc. 47.3 51.5 52.5 53.5
4 Expansion + Rescoring 46.8 52.3 54.5 57.3
(size=7.09)
5 Expansion + Rescoring 46.8 52.3 54.4 56.5
(size=4.15)
</table>
<tableCaption confidence="0.746352333333333">
Table 1. Test-set sentence-level n-best accuracy;
(0) baseline; (1)-(3) initial n-best rescoring;
(4)-(5) expansion + rescoring
</tableCaption>
<bodyText confidence="0.999961083333333">
line. The improvement is smallest for the maxi-
mum likelihood (ML) language model rescoring
approach, but is still statistically significant
(p = 0.008 in a Wilcoxon sign-rank test.) The con-
fusion-matrix based rescoring and the CML rescor-
ing models perform similarly well, leading to a 1%
absolute improvement in 1-best and 2-best sen-
tence-level accuracy from the baseline (p &lt; 10−5).
No statistically significant difference can be de-
tected between these two models. At the same
time, they both outperform the maximum likelih-
ood rescoring model (p &lt; 0.03).
</bodyText>
<subsectionHeader confidence="0.995753">
4.3 N-Best Correction
</subsectionHeader>
<bodyText confidence="0.999972035714286">
Next, we evaluated the end-to-end n-best correc-
tion approach. The n-best lists were first expanded,
as described in section 3.3, and the expanded lists
were ranked using the confusion matrix based res-
coring model described in Section 3.4.
The expansion process enlarges the original n-
best lists. Immediately after expansion, the average
n-best size grows from 4.2 to 96.9. The oracle per-
formance for the expanded n-best lists increases to
59.8% (versus 53.5% in the initial n-best lists.)
After rescoring, we trimmed the expanded n-best
lists to a maximum of 10 hypotheses: we still want
to obey the mobile device display constraint. The
resulting average n-best size was 7.09 (this is low-
er than 10 since there are cases when the system
cannot generate enough expansion hypotheses.)
The sentence-level accuracy of the corrected n-
best lists is displayed in line 4 from Table 1. A di-
rect comparison with the rescoring-only models or
with the baseline is however unfair, due to the
larger average size of the corrected n-best lists. To
create a fair comparison and to better understand
the performance of the n-best correction process,
we pruned the corrected n-best lists by eliminating
all hypotheses with a score below a certain thre-
shold. By varying this rejection threshold, we can
therefore control the average depth of the resulting
corrected n-best lists. At a rejection threshold of
0.004, the average corrected n-best size is 4.15,
comparable to the baseline of 4.2 .
The performance for the corresponding cor-
rected (and pruned) n-best lists is shown in line 5
from Table 1 and illustrated in Figure 4. In contrast
to a rescoring-only approach, the expansion pro-
cess allows for improved performance at higher
depths in the n-best list. The maximum n-best per-
formance (while keeping the average n-best size at
4.15), is 56.5%, a 3% absolute improvement over
the baseline (p &lt; 10−5).
Figure 5 provides more insight into the relation-
ship between the sentence-level accuracy of the
corrected (and pruned) n-best lists and the average
n-best size (the plot was generated by varying the
rejection threshold.) The result we discussed above
can also be observed here: at the same average n-
best size, the n-best correction model significantly
outperforms the baseline. Furthermore, we can see
that we can attain the same level of accuracy as the
baseline system while cutting the average n-best
size by more than 50%, from 4.22 to 2. In the op-
posite direction, if we are less sensitive to the
number of items displayed in the n-best list (except
for the 10-maximum constraint we already obey),
we can further increase the overall performance by
another 0.8% absolute to 57.3%; this overall accu-
racy is attained at an average n-best size of 7.09.
</bodyText>
<page confidence="0.995247">
26
</page>
<figureCaption confidence="0.999309">
Figure 4. N-Best correction (test-set)
Figure 5. Overall n-best accuracy as a function of
the average n-best size
</figureCaption>
<bodyText confidence="0.999892714285714">
Finally, we also investigated rescoring the ex-
panded n-best lists using the CML approach. To
apply CML, an initial ranking of the expanded n-
best lists is however needed. If we use the ranking
produced by the confusion-matrix based model
discussed above, no further performance improve-
ments can be observed.
</bodyText>
<sectionHeader confidence="0.999968" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.999975785714286">
The n-best correction model we have described in
this paper draws inspiration from earlier works on
post-recognition error correction models, n-best
rescoring and implicitly supervised learning. In
this section we discuss some of the similarities and
differences between the proposed approach and
previous work.
The idea of correcting speech recognition errors
in a post-processing step has been proposed earlier
by (Ringger and Allen, 1996; Ringger and Allen,
1997). The authors showed that, in the presence of
transcribed data, a translation-based post-processor
can be trained to correct the results of a speech
recognizer, leading to a 15% relative WER im-
provement in a corpus of TRAINS-95 dialogues.
The n-best correction approach described here is
different in two important aspects. First, instead of
making use of transcripts, the proposed error-
correction model is trained using implicit user
feedback obtained in a multi-modal interface (in
this case user clicks in the n-best list.) This is a less
costly endeavor, as the system automatically ob-
tains the supervision signal directly from the inte-
raction; no transcripts are necessary. Second, the
approach operates on the entire n-best list, rather
than only on the top hypothesis; as such, it has ad-
ditional information that can be helpful in making
corrections. At Figure 2 illustrates, there is a poten-
tial for multiple incorrect hypotheses to point to-
wards and reinforce the same correction
hypothesis, leading to improved performance (in
this example, Burlington, Cooling, Sterling and
Stirling were all highly confusable with Bowling,
which was the correct hypothesis).
The n-best correction model we have described
includes a rescoring step. N-best rescoring ap-
proaches have been investigated extensively in the
speech recognition community. In the dialog
community, n-best rescoring techniques that use
higher-level, dialog features have also been pro-
posed and evaluated (Chotimongkol and Rudnicky,
2001). Apart from using the click feedback, the
novelty in our approach lies in the added expansion
step and in the use of an error-correction model for
rescoring. We have seen that the confusability-
based n-best expansion process leads to signifi-
cantly improved performance, even if we force the
model to keep the same average n-best size.
Finally, the work discussed in this paper has
commonalities with previous works on lightly su-
pervised learning in the speech community, e.g.
(Lamel and Gauvain, 2002) and leveraging implicit
feedback for learning from interaction, e.g. (Baner-
jee and Rudnicky, 2007; Bohus and Rudnicky,
2007). In all these cases, the goal is to minimize
the need for manually-labeled data, and learn di-
</bodyText>
<figure confidence="0.989265333333333">
57.3%
56.5%
53.5%
</figure>
<page confidence="0.992729">
27
</page>
<bodyText confidence="0.99994875">
rectly from the interaction. We believe that in the
long term this family of learning techniques will
play a key role towards building autonomous, self-
improving systems.
</bodyText>
<sectionHeader confidence="0.969797" genericHeader="conclusions">
6 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999989318181818">
We have proposed and evaluated a novel n-best
correction model that leverages implicit user feed-
back in a multi-modal interface to create a positive
feedback loop. While the experiments reported
here were conducted in the context of a local
search application, the approach is applicable in
any multi-modal interface that elicits selection in
an n-best list from the user.
The proposed n-best correction model works in
two stages. First, the n-best list generated by the
speech recognizer is expanded with additional hy-
potheses based on confusability information cap-
tured from previous user clicks. This expanded list
is then rescored and pruned to create a more accu-
rate and more compact n-best list. Our experiments
show that the proposed n-best correction approach
significantly outperforms both the baseline and
other traditional n-best rescoring approaches, with-
out increasing the average length of the n-best lists.
Several issues remain to be investigated. The
models discussed in this paper focus on post-
recognition processing. Other ways of using the
click data can also be envisioned. For instance, one
approach would be to add all the clicked results to
the existing language model training data and
create an updated recognition language model. In
the future, we plan to investigate the relationship
between these two approaches, and to whether they
can be used in conjunction. Earlier related work
(Ringger and Allen, 1997) suggests that this should
indeed be the case.
Second, the click-based error-correction model
we have described in section 3.4 operates at the
result level. The proposed model is essentially a
sentence level, memory-based translation model.
In the future, we also plan to investigate word-
level error-correction models, using machine trans-
lation techniques like the ones discussed in (Ring-
ger and Allen, 1997; Li et al., 2008).
Finally, we plan to investigate how this process
of learning from implicit feedback in a multi-
modal interface can be streamlined, such that the
system continuously learns online, with a minimal
amount of human intervention.
</bodyText>
<sectionHeader confidence="0.996558" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9995775">
This work would have not been possible without
the help of a number of other people. We would
like to especially thank Oliver Scholz, Julian
Odell, Christopher Dac, Tim Paek, Y.C. Ju, Paul
Bennett, Eric Horvitz and Alex Acero for their
help and for useful conversations and feedback.
</bodyText>
<sectionHeader confidence="0.999158" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999882785714286">
Acero, A., N. Bernstein, et al. (2008). &amp;quot;Live Search for
Mobile: Web Services by Voice on the Cellphone&amp;quot;.
ICASSP&apos;08. Las Vegas, NV.
Banerjee, S. and A. Rudnicky (2007). &amp;quot;Segmenting
Meetings into Agenda Items by Extracting Implicit
Supervision from Human Note-Taking&amp;quot;. IUI&apos;2007.
Honolulu, Hawaii.
Birkenes, O., T. Matsui, et al. (2007). &amp;quot;N-Best Rescor-
ing for Speech Recognition using Penalized Logis-
tic Regression Machines with Garbage Class&amp;quot;.
ICASSP&apos;2007, Honolulu, Hawaii.
Bohus, D. and A. Rudnicky (2007). &amp;quot;Implicitly-
supervised learning in spoken language interfaces:
an application to the confidence annotation prob-
lem&amp;quot;. SIGdial 2007, Antwerp, Belgium.
Chotimongkol, A. and A. Rudnicky (2001). &amp;quot;N-best
Speech Hypotheses Reordering Using Linear Re-
gression&amp;quot;. Eurospeech&apos;2001, Aalborg, Denmark.
Lamel, L. and J.-L. Gauvain (2002). &amp;quot;Lightly Super-
vised and Unsupervised Acoustic Model Training.&amp;quot;
Computer Speech and Language 16: 115-129.
Li, X., Y.-C. Ju, et al. (2008). &amp;quot;Language Modeling for
Voice Search: a Machine Translation Approach&amp;quot;.
ICASSP&apos;08, Las Vegas, NV.
Live Search for Windows Mobile (2008):
http://mobile.search.live.com
Nuance Mobile Search (2008):
http://www.nuance.com/mobilesearch.
Ringger, E. and J. Allen (1996). &amp;quot;Error Correction via
Post-Processor for Continuous Speech Recogni-
tion&amp;quot;. ICASSP&apos;96, Atlanta, GA.
Ringger, E. and J. Allen (1997). &amp;quot;Robust Error Correc-
tion of Continuous Speech Recognition&amp;quot;. ESCA-
NATO Workshop on Robust Speech Recognition
for Unknown Communication Channels, Pont-a-
Mousson, France.
TellMeByMobile (2008):
http://www.tellme.com/products/tellmebymobile.
V-Lingo Mobile. (2008):
http://www.vlingomobile.com/downloads.html.
VoiceSignal Search. (2008):
http://www.voicesignal.com/solutions/vsearch.php.
</reference>
<page confidence="0.99907">
28
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.738361">
<title confidence="0.9998525">Learning N-Best Correction Models from Implicit User in a Multi-Modal Local Search Application</title>
<author confidence="0.999667">Dan Bohus</author>
<author confidence="0.999667">Xiao Li</author>
<author confidence="0.999667">Patrick Nguyen</author>
<author confidence="0.999667">Geoffrey</author>
<affiliation confidence="0.958873">Microsoft</affiliation>
<address confidence="0.8855485">One Microsoft Redmond, WA,</address>
<email confidence="0.999323">dbohus@microsoft.com</email>
<email confidence="0.999323">xiaol@microsoft.com</email>
<email confidence="0.999323">panguyen@microsoft.com</email>
<email confidence="0.999323">gzweig@microsoft.com</email>
<abstract confidence="0.9988906875">We describe a novel n-best correction model that can leverage implicit user feedback (in the form of clicks) to improve performance in a multi-modal speech-search application. The proposed model works in two stages. First, the n-best list generated by the speech recognizer is expanded with additional candidates, based on confusability information captured via user click statistics. In the second stage, this expanded list is rescored and pruned to produce a more accurate and compact n-best list. Results indicate that the proposed n-best correction model leads to significant improvements over the existing baseline, as well as other traditional n-best rescoring approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Acero</author>
<author>N Bernstein</author>
</authors>
<title>Live Search for Mobile: Web Services by Voice on the Cellphone&amp;quot;. ICASSP&apos;08. Las Vegas,</title>
<date>2008</date>
<location>NV.</location>
<marker>Acero, Bernstein, 2008</marker>
<rawString>Acero, A., N. Bernstein, et al. (2008). &amp;quot;Live Search for Mobile: Web Services by Voice on the Cellphone&amp;quot;. ICASSP&apos;08. Las Vegas, NV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>A Rudnicky</author>
</authors>
<title>Segmenting Meetings into Agenda Items by Extracting Implicit Supervision from Human Note-Taking&amp;quot;.</title>
<date>2007</date>
<booktitle>IUI&apos;2007.</booktitle>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="25517" citStr="Banerjee and Rudnicky, 2007" startWordPosition="4208" endWordPosition="4212">imongkol and Rudnicky, 2001). Apart from using the click feedback, the novelty in our approach lies in the added expansion step and in the use of an error-correction model for rescoring. We have seen that the confusabilitybased n-best expansion process leads to significantly improved performance, even if we force the model to keep the same average n-best size. Finally, the work discussed in this paper has commonalities with previous works on lightly supervised learning in the speech community, e.g. (Lamel and Gauvain, 2002) and leveraging implicit feedback for learning from interaction, e.g. (Banerjee and Rudnicky, 2007; Bohus and Rudnicky, 2007). In all these cases, the goal is to minimize the need for manually-labeled data, and learn di57.3% 56.5% 53.5% 27 rectly from the interaction. We believe that in the long term this family of learning techniques will play a key role towards building autonomous, selfimproving systems. 6 Conclusion and future work We have proposed and evaluated a novel n-best correction model that leverages implicit user feedback in a multi-modal interface to create a positive feedback loop. While the experiments reported here were conducted in the context of a local search application</context>
</contexts>
<marker>Banerjee, Rudnicky, 2007</marker>
<rawString>Banerjee, S. and A. Rudnicky (2007). &amp;quot;Segmenting Meetings into Agenda Items by Extracting Implicit Supervision from Human Note-Taking&amp;quot;. IUI&apos;2007. Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Birkenes</author>
<author>T Matsui</author>
</authors>
<title>N-Best Rescoring for Speech Recognition using Penalized Logistic Regression Machines with Garbage Class&amp;quot;. ICASSP&apos;2007,</title>
<date>2007</date>
<location>Honolulu, Hawaii.</location>
<marker>Birkenes, Matsui, 2007</marker>
<rawString>Birkenes, O., T. Matsui, et al. (2007). &amp;quot;N-Best Rescoring for Speech Recognition using Penalized Logistic Regression Machines with Garbage Class&amp;quot;. ICASSP&apos;2007, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bohus</author>
<author>A Rudnicky</author>
</authors>
<title>Implicitlysupervised learning in spoken language interfaces: an application to the confidence annotation problem&amp;quot;. SIGdial</title>
<date>2007</date>
<location>Antwerp, Belgium.</location>
<contexts>
<context position="25544" citStr="Bohus and Rudnicky, 2007" startWordPosition="4213" endWordPosition="4216"> Apart from using the click feedback, the novelty in our approach lies in the added expansion step and in the use of an error-correction model for rescoring. We have seen that the confusabilitybased n-best expansion process leads to significantly improved performance, even if we force the model to keep the same average n-best size. Finally, the work discussed in this paper has commonalities with previous works on lightly supervised learning in the speech community, e.g. (Lamel and Gauvain, 2002) and leveraging implicit feedback for learning from interaction, e.g. (Banerjee and Rudnicky, 2007; Bohus and Rudnicky, 2007). In all these cases, the goal is to minimize the need for manually-labeled data, and learn di57.3% 56.5% 53.5% 27 rectly from the interaction. We believe that in the long term this family of learning techniques will play a key role towards building autonomous, selfimproving systems. 6 Conclusion and future work We have proposed and evaluated a novel n-best correction model that leverages implicit user feedback in a multi-modal interface to create a positive feedback loop. While the experiments reported here were conducted in the context of a local search application, the approach is applicabl</context>
</contexts>
<marker>Bohus, Rudnicky, 2007</marker>
<rawString>Bohus, D. and A. Rudnicky (2007). &amp;quot;Implicitlysupervised learning in spoken language interfaces: an application to the confidence annotation problem&amp;quot;. SIGdial 2007, Antwerp, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Chotimongkol</author>
<author>A Rudnicky</author>
</authors>
<title>N-best Speech Hypotheses Reordering Using Linear Regression&amp;quot;.</title>
<date>2001</date>
<location>Eurospeech&apos;2001, Aalborg, Denmark.</location>
<contexts>
<context position="5016" citStr="Chotimongkol and Rudnicky, 2001" startWordPosition="775" endWordPosition="778">ndidates, based on results confusability information captured by the click statistics. For instance, in the 82 cases we mentioned above when Gear was present in the n-best list but Beer was not, Beer (as well as potentially other results) would also be added to form an expanded n-best list. The expanded list is then rescored and pruned to construct a corrected, more accurate n-best list. The proposed approach, described in detail in Section 3, draws inspiration from earlier work in post-recognition error-correction models (Ringger and Allen, 1996; Ringger and Allen, 1997) and nbest rescoring (Chotimongkol and Rudnicky, 2001; Birkenes et al., 2007). The novelty of our approach lies in: (1) the use of user click data in a deployed multi-modal system for creating a positive feedback loop, and (2) the development of an n-best correction model based on implicit feedback that outperforms traditional rescoring-only approaches. Later on, in Section 5, we will discuss in more detail the relationship of the proposed approach to these and other works previously reported in the literature. Before moving on to describe the n-best correction model in more detail, we give a high-level overview of Live Search for Windows Mobile</context>
<context position="24918" citStr="Chotimongkol and Rudnicky, 2001" startWordPosition="4113" endWordPosition="4116">strates, there is a potential for multiple incorrect hypotheses to point towards and reinforce the same correction hypothesis, leading to improved performance (in this example, Burlington, Cooling, Sterling and Stirling were all highly confusable with Bowling, which was the correct hypothesis). The n-best correction model we have described includes a rescoring step. N-best rescoring approaches have been investigated extensively in the speech recognition community. In the dialog community, n-best rescoring techniques that use higher-level, dialog features have also been proposed and evaluated (Chotimongkol and Rudnicky, 2001). Apart from using the click feedback, the novelty in our approach lies in the added expansion step and in the use of an error-correction model for rescoring. We have seen that the confusabilitybased n-best expansion process leads to significantly improved performance, even if we force the model to keep the same average n-best size. Finally, the work discussed in this paper has commonalities with previous works on lightly supervised learning in the speech community, e.g. (Lamel and Gauvain, 2002) and leveraging implicit feedback for learning from interaction, e.g. (Banerjee and Rudnicky, 2007;</context>
</contexts>
<marker>Chotimongkol, Rudnicky, 2001</marker>
<rawString>Chotimongkol, A. and A. Rudnicky (2001). &amp;quot;N-best Speech Hypotheses Reordering Using Linear Regression&amp;quot;. Eurospeech&apos;2001, Aalborg, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lamel</author>
<author>J-L Gauvain</author>
</authors>
<title>Lightly Supervised and Unsupervised Acoustic Model Training.&amp;quot;</title>
<date>2002</date>
<journal>Computer Speech and Language</journal>
<volume>16</volume>
<pages>115--129</pages>
<contexts>
<context position="25419" citStr="Lamel and Gauvain, 2002" startWordPosition="4195" endWordPosition="4198">g techniques that use higher-level, dialog features have also been proposed and evaluated (Chotimongkol and Rudnicky, 2001). Apart from using the click feedback, the novelty in our approach lies in the added expansion step and in the use of an error-correction model for rescoring. We have seen that the confusabilitybased n-best expansion process leads to significantly improved performance, even if we force the model to keep the same average n-best size. Finally, the work discussed in this paper has commonalities with previous works on lightly supervised learning in the speech community, e.g. (Lamel and Gauvain, 2002) and leveraging implicit feedback for learning from interaction, e.g. (Banerjee and Rudnicky, 2007; Bohus and Rudnicky, 2007). In all these cases, the goal is to minimize the need for manually-labeled data, and learn di57.3% 56.5% 53.5% 27 rectly from the interaction. We believe that in the long term this family of learning techniques will play a key role towards building autonomous, selfimproving systems. 6 Conclusion and future work We have proposed and evaluated a novel n-best correction model that leverages implicit user feedback in a multi-modal interface to create a positive feedback loo</context>
</contexts>
<marker>Lamel, Gauvain, 2002</marker>
<rawString>Lamel, L. and J.-L. Gauvain (2002). &amp;quot;Lightly Supervised and Unsupervised Acoustic Model Training.&amp;quot; Computer Speech and Language 16: 115-129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>Y-C Ju</author>
</authors>
<title>Language Modeling for Voice Search: a Machine Translation Approach&amp;quot;. ICASSP&apos;08,</title>
<date>2008</date>
<location>Las Vegas, NV.</location>
<marker>Li, Ju, 2008</marker>
<rawString>Li, X., Y.-C. Ju, et al. (2008). &amp;quot;Language Modeling for Voice Search: a Machine Translation Approach&amp;quot;. ICASSP&apos;08, Las Vegas, NV.</rawString>
</citation>
<citation valid="true">
<title>Live Search for Windows Mobile (2008): http://mobile.search.live.com Nuance Mobile Search</title>
<date>2008</date>
<note>http://www.nuance.com/mobilesearch.</note>
<contexts>
<context position="1214" citStr="(2008)" startWordPosition="178" endWordPosition="178">ormation captured via user click statistics. In the second stage, this expanded list is rescored and pruned to produce a more accurate and compact n-best list. Results indicate that the proposed n-best correction model leads to significant improvements over the existing baseline, as well as other traditional n-best rescoring approaches. 1 Introduction Supported by years of research in speech recognition and related technologies, as well as advances in mobile devices, speech-enabled mobile applications are finally transitioning into day-to-day use. One example is Live Search for Windows Mobile (2008), a speech-enabled application that allows users to get access to local information by speaking a query into their device. Several other systems operating in similar domains have recently become available (TellMeByMobile, 2008; Nuance Mobile Search, 2008; V-Lingo Mobile, 2008; VoiceSignal Search, 2008.) Traditionally, multi-modal systems leverage the additional input channels such as text or buttons to compensate for the current shortcomings of speech 21 recognition technology. For instance, after the user speaks a query, the Live Search for Windows Mobile application displays a confirmation s</context>
</contexts>
<marker>2008</marker>
<rawString>Live Search for Windows Mobile (2008): http://mobile.search.live.com Nuance Mobile Search (2008): http://www.nuance.com/mobilesearch.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Ringger</author>
<author>J Allen</author>
</authors>
<title>Error Correction via Post-Processor for Continuous Speech Recognition&amp;quot;.</title>
<date>1996</date>
<booktitle>ICASSP&apos;96,</booktitle>
<location>Atlanta, GA.</location>
<contexts>
<context position="4937" citStr="Ringger and Allen, 1996" startWordPosition="763" endWordPosition="766"> list generated by the speech recognizer is expanded with additional candidates, based on results confusability information captured by the click statistics. For instance, in the 82 cases we mentioned above when Gear was present in the n-best list but Beer was not, Beer (as well as potentially other results) would also be added to form an expanded n-best list. The expanded list is then rescored and pruned to construct a corrected, more accurate n-best list. The proposed approach, described in detail in Section 3, draws inspiration from earlier work in post-recognition error-correction models (Ringger and Allen, 1996; Ringger and Allen, 1997) and nbest rescoring (Chotimongkol and Rudnicky, 2001; Birkenes et al., 2007). The novelty of our approach lies in: (1) the use of user click data in a deployed multi-modal system for creating a positive feedback loop, and (2) the development of an n-best correction model based on implicit feedback that outperforms traditional rescoring-only approaches. Later on, in Section 5, we will discuss in more detail the relationship of the proposed approach to these and other works previously reported in the literature. Before moving on to describe the n-best correction model </context>
<context position="23386" citStr="Ringger and Allen, 1996" startWordPosition="3880" endWordPosition="3883">nbest lists is however needed. If we use the ranking produced by the confusion-matrix based model discussed above, no further performance improvements can be observed. 5 Related work The n-best correction model we have described in this paper draws inspiration from earlier works on post-recognition error correction models, n-best rescoring and implicitly supervised learning. In this section we discuss some of the similarities and differences between the proposed approach and previous work. The idea of correcting speech recognition errors in a post-processing step has been proposed earlier by (Ringger and Allen, 1996; Ringger and Allen, 1997). The authors showed that, in the presence of transcribed data, a translation-based post-processor can be trained to correct the results of a speech recognizer, leading to a 15% relative WER improvement in a corpus of TRAINS-95 dialogues. The n-best correction approach described here is different in two important aspects. First, instead of making use of transcripts, the proposed errorcorrection model is trained using implicit user feedback obtained in a multi-modal interface (in this case user clicks in the n-best list.) This is a less costly endeavor, as the system a</context>
</contexts>
<marker>Ringger, Allen, 1996</marker>
<rawString>Ringger, E. and J. Allen (1996). &amp;quot;Error Correction via Post-Processor for Continuous Speech Recognition&amp;quot;. ICASSP&apos;96, Atlanta, GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Ringger</author>
<author>J Allen</author>
</authors>
<title>Robust Error Correction of Continuous Speech Recognition&amp;quot;. ESCANATO Workshop on Robust Speech Recognition for Unknown Communication Channels,</title>
<date>1997</date>
<location>Pont-aMousson, France.</location>
<contexts>
<context position="4963" citStr="Ringger and Allen, 1997" startWordPosition="767" endWordPosition="770">eech recognizer is expanded with additional candidates, based on results confusability information captured by the click statistics. For instance, in the 82 cases we mentioned above when Gear was present in the n-best list but Beer was not, Beer (as well as potentially other results) would also be added to form an expanded n-best list. The expanded list is then rescored and pruned to construct a corrected, more accurate n-best list. The proposed approach, described in detail in Section 3, draws inspiration from earlier work in post-recognition error-correction models (Ringger and Allen, 1996; Ringger and Allen, 1997) and nbest rescoring (Chotimongkol and Rudnicky, 2001; Birkenes et al., 2007). The novelty of our approach lies in: (1) the use of user click data in a deployed multi-modal system for creating a positive feedback loop, and (2) the development of an n-best correction model based on implicit feedback that outperforms traditional rescoring-only approaches. Later on, in Section 5, we will discuss in more detail the relationship of the proposed approach to these and other works previously reported in the literature. Before moving on to describe the n-best correction model in more detail, we give a </context>
<context position="23412" citStr="Ringger and Allen, 1997" startWordPosition="3884" endWordPosition="3887">eded. If we use the ranking produced by the confusion-matrix based model discussed above, no further performance improvements can be observed. 5 Related work The n-best correction model we have described in this paper draws inspiration from earlier works on post-recognition error correction models, n-best rescoring and implicitly supervised learning. In this section we discuss some of the similarities and differences between the proposed approach and previous work. The idea of correcting speech recognition errors in a post-processing step has been proposed earlier by (Ringger and Allen, 1996; Ringger and Allen, 1997). The authors showed that, in the presence of transcribed data, a translation-based post-processor can be trained to correct the results of a speech recognizer, leading to a 15% relative WER improvement in a corpus of TRAINS-95 dialogues. The n-best correction approach described here is different in two important aspects. First, instead of making use of transcripts, the proposed errorcorrection model is trained using implicit user feedback obtained in a multi-modal interface (in this case user clicks in the n-best list.) This is a less costly endeavor, as the system automatically obtains the s</context>
<context position="27294" citStr="Ringger and Allen, 1997" startWordPosition="4491" endWordPosition="4494">and other traditional n-best rescoring approaches, without increasing the average length of the n-best lists. Several issues remain to be investigated. The models discussed in this paper focus on postrecognition processing. Other ways of using the click data can also be envisioned. For instance, one approach would be to add all the clicked results to the existing language model training data and create an updated recognition language model. In the future, we plan to investigate the relationship between these two approaches, and to whether they can be used in conjunction. Earlier related work (Ringger and Allen, 1997) suggests that this should indeed be the case. Second, the click-based error-correction model we have described in section 3.4 operates at the result level. The proposed model is essentially a sentence level, memory-based translation model. In the future, we also plan to investigate wordlevel error-correction models, using machine translation techniques like the ones discussed in (Ringger and Allen, 1997; Li et al., 2008). Finally, we plan to investigate how this process of learning from implicit feedback in a multimodal interface can be streamlined, such that the system continuously learns on</context>
</contexts>
<marker>Ringger, Allen, 1997</marker>
<rawString>Ringger, E. and J. Allen (1997). &amp;quot;Robust Error Correction of Continuous Speech Recognition&amp;quot;. ESCANATO Workshop on Robust Speech Recognition for Unknown Communication Channels, Pont-aMousson, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>TellMeByMobile</author>
</authors>
<date>2008</date>
<note>http://www.tellme.com/products/tellmebymobile.</note>
<contexts>
<context position="1440" citStr="TellMeByMobile, 2008" startWordPosition="209" endWordPosition="210"> model leads to significant improvements over the existing baseline, as well as other traditional n-best rescoring approaches. 1 Introduction Supported by years of research in speech recognition and related technologies, as well as advances in mobile devices, speech-enabled mobile applications are finally transitioning into day-to-day use. One example is Live Search for Windows Mobile (2008), a speech-enabled application that allows users to get access to local information by speaking a query into their device. Several other systems operating in similar domains have recently become available (TellMeByMobile, 2008; Nuance Mobile Search, 2008; V-Lingo Mobile, 2008; VoiceSignal Search, 2008.) Traditionally, multi-modal systems leverage the additional input channels such as text or buttons to compensate for the current shortcomings of speech 21 recognition technology. For instance, after the user speaks a query, the Live Search for Windows Mobile application displays a confirmation screen that contains the n-best recognition results. The user selects the correct hypothesis using the buttons on the device, and only then the system displays the corresponding search results (see Figure 1.) We argue that idea</context>
</contexts>
<marker>TellMeByMobile, 2008</marker>
<rawString>TellMeByMobile (2008): http://www.tellme.com/products/tellmebymobile.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V-Lingo Mobile</author>
</authors>
<date>2008</date>
<note>http://www.vlingomobile.com/downloads.html.</note>
<contexts>
<context position="1214" citStr="Mobile (2008)" startWordPosition="177" endWordPosition="178">ity information captured via user click statistics. In the second stage, this expanded list is rescored and pruned to produce a more accurate and compact n-best list. Results indicate that the proposed n-best correction model leads to significant improvements over the existing baseline, as well as other traditional n-best rescoring approaches. 1 Introduction Supported by years of research in speech recognition and related technologies, as well as advances in mobile devices, speech-enabled mobile applications are finally transitioning into day-to-day use. One example is Live Search for Windows Mobile (2008), a speech-enabled application that allows users to get access to local information by speaking a query into their device. Several other systems operating in similar domains have recently become available (TellMeByMobile, 2008; Nuance Mobile Search, 2008; V-Lingo Mobile, 2008; VoiceSignal Search, 2008.) Traditionally, multi-modal systems leverage the additional input channels such as text or buttons to compensate for the current shortcomings of speech 21 recognition technology. For instance, after the user speaks a query, the Live Search for Windows Mobile application displays a confirmation s</context>
</contexts>
<marker>Mobile, 2008</marker>
<rawString>V-Lingo Mobile. (2008): http://www.vlingomobile.com/downloads.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>VoiceSignal Search</author>
</authors>
<date>2008</date>
<pages>http://www.voicesignal.com/solutions/vsearch.php.</pages>
<contexts>
<context position="1468" citStr="Search, 2008" startWordPosition="213" endWordPosition="214">ents over the existing baseline, as well as other traditional n-best rescoring approaches. 1 Introduction Supported by years of research in speech recognition and related technologies, as well as advances in mobile devices, speech-enabled mobile applications are finally transitioning into day-to-day use. One example is Live Search for Windows Mobile (2008), a speech-enabled application that allows users to get access to local information by speaking a query into their device. Several other systems operating in similar domains have recently become available (TellMeByMobile, 2008; Nuance Mobile Search, 2008; V-Lingo Mobile, 2008; VoiceSignal Search, 2008.) Traditionally, multi-modal systems leverage the additional input channels such as text or buttons to compensate for the current shortcomings of speech 21 recognition technology. For instance, after the user speaks a query, the Live Search for Windows Mobile application displays a confirmation screen that contains the n-best recognition results. The user selects the correct hypothesis using the buttons on the device, and only then the system displays the corresponding search results (see Figure 1.) We argue that ideally multi-modal systems coul</context>
</contexts>
<marker>Search, 2008</marker>
<rawString>VoiceSignal Search. (2008): http://www.voicesignal.com/solutions/vsearch.php.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>