<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.030858">
<title confidence="0.974325">
IIRG: A Naive Approach to Evaluating Phrasal Semantics
</title>
<author confidence="0.999644">
Lorna Byrne, Caroline Fenlon, John Dunnion
</author>
<affiliation confidence="0.811627">
School of Computer Science and Informatics
University College Dublin
Ireland
</affiliation>
<email confidence="0.997954">
{lorna.byrne@ucd.ie,caroline.fenlon@ucdconnect.ie,john.dunnion@ucd.ie}
</email>
<sectionHeader confidence="0.995624" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999953166666667">
This paper describes the IIRG 1 system en-
tered in SemEval-2013, the 7th International
Workshop on Semantic Evaluation. We partic-
ipated in Task 5 Evaluating Phrasal Semantics.
We have adopted a token-based approach to
solve this task using 1) Naive Bayes methods
and 2) Word Overlap methods, both of which
rely on the extraction of syntactic features. We
found that the word overlap method signifi-
cantly out-performs the Naive Bayes methods,
achieving our highest overall score with an ac-
curacy of approximately 78%.
</bodyText>
<sectionHeader confidence="0.998985" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99958605882353">
The Phrasal Semantics task consists of two related
subtasks. Task 5A requires systems to evaluate
the semantic similarity of words and compositional
phrases. Task 5B requires systems to evaluate the
compositionality of phrases in context. We partici-
pated in Task 5B and submitted three runs for eval-
uation, two runs using the Naive Bayes Machine
Learning Algorithm and a Word Overlap run using
a simple bag-of-words approach.
Identifying non-literal expressions poses a major
challenge in NLP because they occur frequently and
often exhibit irregular behavior by not adhering to
grammatical constraints. Previous research in the
area of identifying literal/non-literal use of expres-
sions includes generating a wide range of different
features for use with a machine learning prediction
algorithm. (Li and Sporleder, 2010) present a system
</bodyText>
<footnote confidence="0.509189">
1Intelligent Information Retrieval Group
</footnote>
<bodyText confidence="0.99998124">
involving identifying the global and local contexts of
a phrase. Global context was determined by look-
ing for occurrences of semantically related words
in a given passage, while local context focuses on
the words immediately preceding and following the
phrase. Windows of five words at each side of the
target were taken as features. More syntactic fea-
tures were also used, including details of nodes from
the dependency tree of each example. The system
produced approximately 90% accuracy when tested,
for both idiom-specific and generic models. It was
found that the statistical features (global and local
contexts) performed well, even on unseen phrases.
(Katz and Giesbrecht, 2006) found that similarities
between words in the expression and its context indi-
cate literal usage. This is comparable to (Sporleder
and Li, 2009), which used cohesion-based classi-
fiers based on lexical chains and graphs. Unsu-
pervised approaches to classifying idiomatic use in-
clude clustering (Fazly et al., 2009), which classified
data based on semantic analyzability (whether the
meaning of the expression is similar to the mean-
ings of its parts) and lexical and syntactic flexibility
(measurements of how much variation exists within
the expression).
</bodyText>
<sectionHeader confidence="0.981877" genericHeader="method">
2 Task 5B
</sectionHeader>
<bodyText confidence="0.928316571428571">
In Task 5B, participants were required to make a
binary decision as to whether a target phrase is used
figuratively or literally within a given context. The
phrase “drop the ball” can be used figuratively, for
example in the sentence
We get paid for completing work, so we’ve designed
a detailed workflow process to make sure we don’t
</bodyText>
<page confidence="0.983469">
103
</page>
<bodyText confidence="0.979546866666667">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 103–107, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
drop the ball.
and literally, for example in the sentence
In the end, the Referee drops the ball with the
attacking player nearby.
In order to train systems, participants were given
training data consisting of approximately 1400 text
snippets (one or more sentences) containing 10 tar-
get phrases, together with real usage examples sam-
pled from the WaCky (Baroni et al., 2009) corpora.
The number of examples and distribution of figura-
tive and literal instances varied for each phrase.
Participants were allowed to submit three runs for
evaluation purposes.
</bodyText>
<subsectionHeader confidence="0.963551">
2.1 Approach
</subsectionHeader>
<bodyText confidence="0.989513535714286">
The main assumption for our approach is that tokens
preceding and succeeding the target phrase might in-
dicate the usage of the target phrase, i.e. whether the
target phrase is being used in a literal or figurative
context. Firstly, each text snippet was processed us-
ing the Stanford Suite of Core NLP Tools2 to to-
kenise the snippet and produce part-of-speech tags
and lemmas for each token.
During the training phase, we identified and ex-
tracted a target phrase boundary for each of the tar-
get phrases. A target phrase boundary consists of a
window of tokens immediately before and after the
target phrase. The phrase boundaries identified for
the first two runs were restricted to windows of one,
i.e. the token immediately before and after the target
phrase were extracted, tokens were also restricted to
the canonical form.
For example, the target phrase boundary iden-
tified for the snippet: “The returning team will
drop the ball and give you a chance to recover .” is
as follows:
before:will
after:and
and the target phrase boundary identified for
the snippet: “Meanwhile , costs are going
through the roof .” is as follows:
before:go
after:.
</bodyText>
<footnote confidence="0.922352">
2http://nlp.stanford.edu/software
</footnote>
<table confidence="0.9362198">
IIRG Training Runs
RunID Accuracy (%)
Run0 85.29
Run1 81.84
Run2 95.92
</table>
<tableCaption confidence="0.999601">
Table 1: Results of IIRG Training Runs
</tableCaption>
<bodyText confidence="0.99979525">
We then trained multiple Naive Bayes classifiers
on these extracted phrase boundaries. The first clas-
sifier was trained on the set of target phrase bound-
aries extracted from the entire training set of tar-
get phrases and usage examples (Run0); the sec-
ond classifier was trained on the set of target phrase
boundaries extracted from the entire training set of
target phrases and usage examples including the
phrase itself as a predictor variable (Run1); and a
set of target-phrase classifiers, one per target phrase,
were trained on the set of target phrase boundaries
extracted from each individual target phrase (Run2).
The results of the initial training runs can be
seen in Table 1. Although Run0 yielded very high
accuracy scores on the training data, outperforming
Run1, in practice this approach performed poorly
on unseen data and was biased towards a figurative
classification. We thus opted not to implement this
run in the testing phase and instead concentrated on
Run1 and Run2.
For our third submitted run, we adopted a word
overlap method which implemented a simple bag-
of-words approach. For each target phrase we cre-
ated a bag-of-words by selecting the canonical form
of all of the noun tokens in each corresponding train-
ing usage example. The frequency of occurrence
of each token within a given context was recorded
and each token was labeled as figurative or literal
depending on its frequency of occurrence within a
given context. The frequency of occurrence of each
token was also recorded in order to adjust the thresh-
old of token occurrences for subsequent runs. For
this run, Run3, the token frequency threshold was
set to 2, so that a given token must occur two or more
times in a given context to be added to the bag-of-
words.
</bodyText>
<page confidence="0.998168">
104
</page>
<sectionHeader confidence="0.999914" genericHeader="evaluation">
3 Results
</sectionHeader>
<bodyText confidence="0.999978793103448">
System performance is measured in terms of accu-
racy. The results of the submitted runs can be seen
in Table 2.
Of the submitted runs, the Word Overlap method
(Run3) performed best overall. This approach was
also consistently good across all phrases, with scores
ranging from 70% to 80%, as seen in Table 3.
The classifiers trained on the canonical phrase
boundaries (Run1 and Run2) performed poorly on
unseen data. They were also biased towards a figura-
tive prediction. For several phrases they incorrectly
classified all literal expressions as figurative. They
were not effective at processing all of the phrases:
in Run1, some phrases had very high scores rela-
tive to the overall score (e.g. “break a leg”), while
others scored very poorly (e.g. “through the roof”).
In Run2, a similar effect was found. Interestingly,
even though separate classifiers were trained for
each phrase, the accuracy was lower than that of
Run1 in several cases (e.g. “through the roof”). This
may be a relic of the small, literally-skewed, train-
ing data for some of the phrases, or may suggest that
this approach is not suitable for those expressions.
The very high accuracy of the classifiers tested on a
subset of the training data may be attributed to over-
fitting. The approach used in Run1 and Run2 is un-
likely to yield very accurate results for the classifi-
cation of general data, due to the potential for many
unseen canonical forms of word boundaries.
</bodyText>
<subsectionHeader confidence="0.998473">
3.1 Additional Runs
</subsectionHeader>
<bodyText confidence="0.999108583333333">
After the submission deadline, we completed some
additional runs, the results of which can be seen in
Table 4.
These runs were similar to Run1 and Run2, where
we used Naive Bayes Classifiers to train on ex-
tracted target phrase boundaries. However, for Run4
and Run5 we restricted the phrase boundaries to the
canonical form of the nearest verb (Run4) or nearest
noun (Run5) that was present in a bag-of-words.
We used the same bag-of-words created for Run3
for the noun-based bag-of-words, and this same ap-
proach was used to create the (canonical form) verb-
based bag-of-words. If there were no such verbs or
nouns present then the label NULL was applied. If
a phrase occurred at the start or end of a text snip-
pet this information was also captured. The Naive
Bayes classifiers were then trained using labels from
the following set of input labels: FIGURATIVE,
LITERAL, START, END or NULL, which indicate
the target phrase boundaries of the target phrases.
For example, the target phrase boundaries identi-
fied for the snippet: “Meanwhile , costs are going
through the roof .” for Run4 and Run5, respectively,
are as follows:
</bodyText>
<equation confidence="0.53769">
before:FIGURATIVE
after:END
</equation>
<bodyText confidence="0.999595">
where the FIGURATIVE label is the classification
of the token ‘going’ as indicated in the verb-based
bag-of-words, and
</bodyText>
<equation confidence="0.688593">
before:FIGURATIVE
after:END
</equation>
<bodyText confidence="0.999989677419355">
where the FIGURATIVE label is the classification
of the token ‘costs’ as indicated in the noun-based
bag-of-words.
As in Run1 and Run2, an entire-set classifier and
individual target-phrase classifiers were trained for
both runs. These additional runs performed well,
yielding high accuracy results and significantly out-
performing Run1 and Run2.
The Run4 classifiers did not perform compara-
tively well across all phrases. In particular, the target
phrase “break a leg”, had very low accuracy scores,
possibly because the training data for the phrase was
small and contained mostly literal examples. The
ranges of phrase scores for the noun classification
runs (Run5) were similar to those of the Word Over-
lap runs. The results across each phrase were also
consistent, with no scores significantly lower than
the overall accuracy. Using target phrase boundaries
based on noun classifications may prove to yield rea-
sonable results when extended to more phrases, as
opposed to the erratic results found when using verb
classifications.
In both Run4 and Run5, very similar overall re-
sults were produced from both the entire-set and
target-phrase classifiers. In most cases, the run per-
formed poorly on the same phrases in both instances,
indicating that the approach may not be appropriate
for the particular phrase. For example, the verb clas-
sifications runs scored low accuracy for “drop the
ball”, while the noun classifications run was approx-
imately 80% accurate for the same phrase using both
</bodyText>
<page confidence="0.995073">
105
</page>
<table confidence="0.9995705">
IIRG Submitted Runs (%)
RunID Overall Precision Recall Precision Recall
Accuracy (Figurative) (Figurative) (Literal) (Literal)
Run1 53.03 52.03 89.97 60.25 15.65
Run2 50.17 50.81 41.81 54.06 58.84
Run3 77.95 79.65 75.92 76.62 80.27
</table>
<tableCaption confidence="0.989869">
Table 2: Results of Runs Submitted to Sem-Eval 2013
</tableCaption>
<table confidence="0.997190375">
IIRG Submitted Runs - Per Phrase Accuracy (%)
RunID At the Bread Break Drop In the In the Play Rub Through Under
end and a leg the bag fast ball it in the roof the
of the butter ball lane micro-
day scope
Run1 68.92 57.89 40.00 40.82 43.42 67.86 52.63 66.67 64.94 33.33
Run2 45.95 38.16 83.33 57.14 48.68 75.00 46.05 56.67 29.87 62.82
Run3 75.68 82.89 73.33 83.67 72.37 75.00 78.95 60.00 80.52 83.33
</table>
<tableCaption confidence="0.998044">
Table 3: Results of Runs Submitted to Sem-Eval 2013 (per phrase)
</tableCaption>
<table confidence="0.9960502">
IIRG Additional Runs - Accuracy (%)
RunID Entire-Set Target-Phrase
Classifier Classifier
Run4 64.81 65.99
Run5 75.25 76.60
</table>
<tableCaption confidence="0.999593">
Table 4: Accuracy of Additional Unsubmitted Runs
</tableCaption>
<bodyText confidence="0.938144">
an entire-set and target-phrase classifier.
</bodyText>
<sectionHeader confidence="0.997883" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999983941176471">
This is the first year we have taken part in the Se-
mantic Evaluation Exercises, participating in Task
5b, Evaluating Phrasal Semantics. Task 5B requires
systems to evaluate the compositionality of phrases
in context. We have adopted a token-based approach
to solve this task using 1) Naive Bayes methods
whereby target phrase boundaries were identified
and extracted in order to train multiple classifiers;
and 2) Word Overlap methods, whereby a simple
bag-of-words was created for each target phrase. We
submitted three runs for evaluation purposes, two
runs using Naive Bayes methods (Run1 and Run2)
and one run based on a Word Overlap approach
(Run3). The Word Overlap approach, which limited
each bag-of-words to using the canonical form of the
nouns in the text snippets, yielded the highest accu-
racy scores of all submitted runs, at approximately
78% accurate. An additional run (Run5), also us-
ing the canonical form of the nouns in the usage ex-
amples but implementing a Naive Bayes approach,
yielded similar results, almost 77% accuracy. The
approaches which were restricted to using the nouns
in the text snippets yielded the highest accuracy re-
sults, thus indicating that nouns provide important
contextual information for distinguishing literal and
figurative usage.
In future work, we will explore whether we can
improve the performance of the target phrase bound-
aries by experimenting with the local context win-
dow sizes. Another potential improvement might
be to examine whether implementing more sophisti-
cated strategies for selecting tokens for the bags-of-
words improves the effectiveness of the Word Over-
lap methods.
</bodyText>
<sectionHeader confidence="0.999284" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999776111111111">
M. Baroni, S. Bernardini, A. Ferraresi, and E. Zanchetta.
2009. The WaCky Wide Web: A Collection of
Very Large Linguistically Processed Web-Crawled
Corpora. Language Resources and Evaluation 43,
3(3):209–226.
Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised Type and Token Identification
of Idiomatic Expressions. Computational Linguistics,
35(1):61–103, March.
</reference>
<page confidence="0.934873">
106
</page>
<reference confidence="0.99954375">
Graham Katz and Eugenie Giesbrecht. 2006. Automatic
Identification of Non-Compositional Multi-Word Ex-
pressions using Latent Semantic Analysis. In Pro-
ceedings of the ACL/COLING-06 Workshop on Multi-
word Expressions: Identifying and Exploiting Under-
lying Properties, pages 12–19.
Linlin Li and Caroline Sporleder. 2010. Linguistic Cues
for Distinguishing Literal and Non-Literal Usages. In
Proceedings of the 23rd International Conference on
Computational Linguistics (COLING ’10), pages 683–
691.
Caroline Sporleder and Linlin Li. 2009. Unsupervised
Recognition of Literal and Non-Literal Use of Id-
iomatic expressions. In Proceedings of the 12th Con-
ference of the European Chapter of the ACL (EACL
2009).
</reference>
<page confidence="0.998683">
107
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.670690">
<title confidence="0.998041">IIRG: A Naive Approach to Evaluating Phrasal Semantics</title>
<author confidence="0.999861">Lorna Byrne</author>
<author confidence="0.999861">Caroline Fenlon</author>
<author confidence="0.999861">John</author>
<affiliation confidence="0.926372666666667">School of Computer Science and University College Ireland</affiliation>
<abstract confidence="0.982865076923077">paper describes the IIRG 1system entered in SemEval-2013, the 7th International Workshop on Semantic Evaluation. We participated in Task 5 Evaluating Phrasal Semantics. We have adopted a token-based approach to solve this task using 1) Naive Bayes methods and 2) Word Overlap methods, both of which rely on the extraction of syntactic features. We found that the word overlap method significantly out-performs the Naive Bayes methods, achieving our highest overall score with an accuracy of approximately 78%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>S Bernardini</author>
<author>A Ferraresi</author>
<author>E Zanchetta</author>
</authors>
<title>The WaCky Wide Web: A Collection of Very Large Linguistically Processed Web-Crawled Corpora.</title>
<date>2009</date>
<journal>Language Resources and Evaluation</journal>
<volume>43</volume>
<pages>3--3</pages>
<contexts>
<context position="3878" citStr="Baroni et al., 2009" startWordPosition="583" endWordPosition="586"> Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 103–107, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics drop the ball. and literally, for example in the sentence In the end, the Referee drops the ball with the attacking player nearby. In order to train systems, participants were given training data consisting of approximately 1400 text snippets (one or more sentences) containing 10 target phrases, together with real usage examples sampled from the WaCky (Baroni et al., 2009) corpora. The number of examples and distribution of figurative and literal instances varied for each phrase. Participants were allowed to submit three runs for evaluation purposes. 2.1 Approach The main assumption for our approach is that tokens preceding and succeeding the target phrase might indicate the usage of the target phrase, i.e. whether the target phrase is being used in a literal or figurative context. Firstly, each text snippet was processed using the Stanford Suite of Core NLP Tools2 to tokenise the snippet and produce part-of-speech tags and lemmas for each token. During the tra</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>M. Baroni, S. Bernardini, A. Ferraresi, and E. Zanchetta. 2009. The WaCky Wide Web: A Collection of Very Large Linguistically Processed Web-Crawled Corpora. Language Resources and Evaluation 43, 3(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Afsaneh Fazly</author>
<author>Paul Cook</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Unsupervised Type and Token Identification of Idiomatic Expressions.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>1</issue>
<contexts>
<context position="2662" citStr="Fazly et al., 2009" startWordPosition="392" endWordPosition="395">luding details of nodes from the dependency tree of each example. The system produced approximately 90% accuracy when tested, for both idiom-specific and generic models. It was found that the statistical features (global and local contexts) performed well, even on unseen phrases. (Katz and Giesbrecht, 2006) found that similarities between words in the expression and its context indicate literal usage. This is comparable to (Sporleder and Li, 2009), which used cohesion-based classifiers based on lexical chains and graphs. Unsupervised approaches to classifying idiomatic use include clustering (Fazly et al., 2009), which classified data based on semantic analyzability (whether the meaning of the expression is similar to the meanings of its parts) and lexical and syntactic flexibility (measurements of how much variation exists within the expression). 2 Task 5B In Task 5B, participants were required to make a binary decision as to whether a target phrase is used figuratively or literally within a given context. The phrase “drop the ball” can be used figuratively, for example in the sentence We get paid for completing work, so we’ve designed a detailed workflow process to make sure we don’t 103 Second Joi</context>
</contexts>
<marker>Fazly, Cook, Stevenson, 2009</marker>
<rawString>Afsaneh Fazly, Paul Cook, and Suzanne Stevenson. 2009. Unsupervised Type and Token Identification of Idiomatic Expressions. Computational Linguistics, 35(1):61–103, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Katz</author>
<author>Eugenie Giesbrecht</author>
</authors>
<title>Automatic Identification of Non-Compositional Multi-Word Expressions using Latent Semantic Analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL/COLING-06 Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties,</booktitle>
<pages>12--19</pages>
<contexts>
<context position="2351" citStr="Katz and Giesbrecht, 2006" startWordPosition="344" endWordPosition="347">rase. Global context was determined by looking for occurrences of semantically related words in a given passage, while local context focuses on the words immediately preceding and following the phrase. Windows of five words at each side of the target were taken as features. More syntactic features were also used, including details of nodes from the dependency tree of each example. The system produced approximately 90% accuracy when tested, for both idiom-specific and generic models. It was found that the statistical features (global and local contexts) performed well, even on unseen phrases. (Katz and Giesbrecht, 2006) found that similarities between words in the expression and its context indicate literal usage. This is comparable to (Sporleder and Li, 2009), which used cohesion-based classifiers based on lexical chains and graphs. Unsupervised approaches to classifying idiomatic use include clustering (Fazly et al., 2009), which classified data based on semantic analyzability (whether the meaning of the expression is similar to the meanings of its parts) and lexical and syntactic flexibility (measurements of how much variation exists within the expression). 2 Task 5B In Task 5B, participants were required</context>
</contexts>
<marker>Katz, Giesbrecht, 2006</marker>
<rawString>Graham Katz and Eugenie Giesbrecht. 2006. Automatic Identification of Non-Compositional Multi-Word Expressions using Latent Semantic Analysis. In Proceedings of the ACL/COLING-06 Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 12–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linlin Li</author>
<author>Caroline Sporleder</author>
</authors>
<title>Linguistic Cues for Distinguishing Literal and Non-Literal Usages.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING ’10),</booktitle>
<pages>683--691</pages>
<contexts>
<context position="1607" citStr="Li and Sporleder, 2010" startWordPosition="230" endWordPosition="233"> compositionality of phrases in context. We participated in Task 5B and submitted three runs for evaluation, two runs using the Naive Bayes Machine Learning Algorithm and a Word Overlap run using a simple bag-of-words approach. Identifying non-literal expressions poses a major challenge in NLP because they occur frequently and often exhibit irregular behavior by not adhering to grammatical constraints. Previous research in the area of identifying literal/non-literal use of expressions includes generating a wide range of different features for use with a machine learning prediction algorithm. (Li and Sporleder, 2010) present a system 1Intelligent Information Retrieval Group involving identifying the global and local contexts of a phrase. Global context was determined by looking for occurrences of semantically related words in a given passage, while local context focuses on the words immediately preceding and following the phrase. Windows of five words at each side of the target were taken as features. More syntactic features were also used, including details of nodes from the dependency tree of each example. The system produced approximately 90% accuracy when tested, for both idiom-specific and generic mo</context>
</contexts>
<marker>Li, Sporleder, 2010</marker>
<rawString>Linlin Li and Caroline Sporleder. 2010. Linguistic Cues for Distinguishing Literal and Non-Literal Usages. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING ’10), pages 683– 691.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Sporleder</author>
<author>Linlin Li</author>
</authors>
<title>Unsupervised Recognition of Literal and Non-Literal Use of Idiomatic expressions.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<contexts>
<context position="2494" citStr="Sporleder and Li, 2009" startWordPosition="367" endWordPosition="370">e words immediately preceding and following the phrase. Windows of five words at each side of the target were taken as features. More syntactic features were also used, including details of nodes from the dependency tree of each example. The system produced approximately 90% accuracy when tested, for both idiom-specific and generic models. It was found that the statistical features (global and local contexts) performed well, even on unseen phrases. (Katz and Giesbrecht, 2006) found that similarities between words in the expression and its context indicate literal usage. This is comparable to (Sporleder and Li, 2009), which used cohesion-based classifiers based on lexical chains and graphs. Unsupervised approaches to classifying idiomatic use include clustering (Fazly et al., 2009), which classified data based on semantic analyzability (whether the meaning of the expression is similar to the meanings of its parts) and lexical and syntactic flexibility (measurements of how much variation exists within the expression). 2 Task 5B In Task 5B, participants were required to make a binary decision as to whether a target phrase is used figuratively or literally within a given context. The phrase “drop the ball” c</context>
</contexts>
<marker>Sporleder, Li, 2009</marker>
<rawString>Caroline Sporleder and Linlin Li. 2009. Unsupervised Recognition of Literal and Non-Literal Use of Idiomatic expressions. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>