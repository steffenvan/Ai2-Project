<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000312">
<title confidence="0.994385">
Language-Aware Truth Assessment of Fact Candidates
</title>
<author confidence="0.981303">
Ndapandula Nakashole
</author>
<affiliation confidence="0.980532">
Carnegie Mellon University
</affiliation>
<address confidence="0.82954">
5000 Forbes Avenue
Pittsburgh, PA, 15213
</address>
<email confidence="0.999169">
ndapa@cs.cmu.edu
</email>
<sectionHeader confidence="0.993902" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999803166666667">
This paper introduces FactChecker,
language-aware approach to truth-finding.
FactChecker differs from prior approaches
in that it does not rely on iterative peer
voting, instead it leverages language to
infer believability of fact candidates. In
particular, FactChecker makes use of lin-
guistic features to detect if a given source
objectively states facts or is speculative
and opinionated. To ensure that fact
candidates mentioned in similar sources
have similar believability, FactChecker
augments objectivity with a co-mention
score to compute the overall believability
score of a fact candidate. Our experiments
on various datasets show that FactChecker
yields higher accuracy than existing
approaches.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999985333333334">
Truth-finding algorithms aim to separate true
statements (facts) from false information. More
specifically, given a set of statements whose truth-
fulness is unknown (fact candidates), the key goal
of truth-finding algorithms is to generate a ranking
such that true statements are ranked ahead of false
ones. Truth-finders have the potential to address a
major obstacle on the Web: the problem of sources
spreading inaccurate and conflicting information.
This problem continues to grow with the develop-
ment of tools for easy Web authorship. Blogs, fo-
rums and social networking websites are not sub-
ject to traditional journalistic standards. Conse-
quently, the accuracy of information reported by
these sources is often unclear. Even more estab-
lished newspapers and websites may sometimes
report false information as they race to break sto-
ries. Therefore, truth-finding is becoming an in-
</bodyText>
<note confidence="0.926696">
Tom M. Mitchell
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA, 15213
</note>
<email confidence="0.995564">
tom.mitchell@cs.cmu.edu
</email>
<bodyText confidence="0.999851175">
creasingly important problem. Information extrac-
tion projects aim to distill relational facts from nat-
ural language text (Auer et al., 2007; Bollacker et
al., 2008; Carlson et al., 2010; Fader et al., 2011;
Nakashole et al., 2011; Del Corro and Gemulla,
2013). These projects have produced knowledge
bases containing many millions of relational facts
between entities. However, despite these impres-
sive advances, there are still major limitations re-
garding precision. Within the context of informa-
tion extraction, fact extractors assign confidence
scores to extracted facts. However, such scores
are often tied to the extractor’s ability to read and
understand natural language text. This is differ-
ent from a score that indicates the degree to which
a given fact candidate is believable. Such a be-
lievability score is sometimes also referred to as
a credibility score or truthfulness score. The be-
lievability score reflects the likelihood that a given
statement is true. Truth-finding algorithms aim to
compute this score for each fact candidate.
Prior truth-finding methods are mostly based on
iterative voting, where votes are propagated from
sources to fact candidates and then back to sources
(Yin et al., 2007; Galland et al., 2010; Paster-
nack and Roth, 2010; Li et al., 2011; Yin and
Tan, 2011). At the core of iterative voting is the
assumption that candidates mentioned by many
sources are more likely to be true. However, ad-
ditional aspects of a source influence its trustwor-
thiness, besides external votes.
Our goal is to accurately assess truthfulness of
fact candidates by taking into account the lan-
guage of sources that mention them. A Mechan-
ical Turk study we carried out revealed that there
is a significant correlation between objectivity of
language and trustworthiness of sources. Objec-
tivity of language refers to the use of neutral,
impartial language, which is not personal, judg-
mental, or emotional. Trustworthiness refers to
</bodyText>
<page confidence="0.967506">
1009
</page>
<note confidence="0.8304375">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1009–1019,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999025948717949">
a source of information being reliable and truth-
ful. We use linguistics features to detect if a given
source objectively states facts or is speculative
and opinionated. Additionally, in order to ensure
that fact candidates mentioned in similar sources
have similar believability scores, our believability
computation model incorporates influence of co-
mentions. However, we must avoid falsely boost-
ing co-mentioned fact candidates. Our model ad-
dresses potential false boosts in two ways: first, by
ensuring that co-mention influence is only propa-
gated to related fact candidates; second, by ensur-
ing that the degree of co-mention influence is de-
termined by the trustworthiness of the sources in
which co-mentions occur.
The contribution of this paper is a language-
aware truth-finding approach. More precisely,
we make the following contributions: (1) Al-
ternative Fact Candidates: Truth-finders rank a
given fact candidate with respect to its alter-
natives. For example, alternative places where
Barack Obama could have been born. Virtually
all existing truth-finders assume that the alterna-
tives are provided. In contrast, we developed a
method for generating alternative fact candidates.
(2) Objectivity-Trustworthiness Correlation: We
hypothesize that objectivity of language and trust-
worthiness of sources are positively correlated. To
test this hypothesis, we designed a Mechanical
Turk study. The study showed that this correlation
does in fact hold. (3) Objectivity Classifier: Us-
ing labeled data from the Mechanical Turk study,
we developed and trained an objectivity classifier
which performed better than prior proposed lexi-
cons from literature. (4) Believability Computa-
tion: We developed FactChecker, a truth-finding
method that linearly combines objectivity and co-
mention influence. Our experiments showed that
FactChecker outperforms prior methods.
</bodyText>
<sectionHeader confidence="0.982872" genericHeader="method">
2 Fact Candidates
</sectionHeader>
<bodyText confidence="0.9998316">
In this section, we formally define what constitutes
a fact candidate and describe how we go about
understanding semantics of fact candidates. We
then present our approach for generating alterna-
tive fact candidates.
</bodyText>
<subsectionHeader confidence="0.924595">
2.1 Representation
</subsectionHeader>
<bodyText confidence="0.998233333333334">
The triple format is the most common representa-
tion of facts in knowledge bases. A formal specifi-
cation of the triple format is presented in the RDF
primer1. In RDF, data is represented as subject-
predicate-object (SPO) triples. In this work, we
restrict predicates to verbs (or verbal phrases such
as “plays for”, “graduated from”, etc.). Litera-
ture on automatic relation discovery (Fader et al.,
2011) has shown that verbal phrases uncover a
large fraction of binary predicates while reducing
the amount of noisy phrases that do not denote any
relations. Therefore, we define a fact candidate as
follows:
Definition 1(Fact Candidate) A fact candidate
fz is an (S) V (O) triple; where S is the subject,
V is a verbal phrase, and O is the object. We aim
to compute the truthfulness of fz, τ(fz) E IT, F},
where T and F stand for true and false, respec-
tively.
Note that in this paper we are interested in cases
where τ(fz) is either T or F. That is, we assess
truthfulness offactual statements and not opinions
whose truthfulness is often both T and F to some
degree. For example, the triples: (Obama) born in
(Kenya) and (Obama) graduated from (Harvard)
are valid fact candidates. However, the triple:
(Obama) deserves (Nobel Peace Prize) is not.
</bodyText>
<subsectionHeader confidence="0.998124">
2.2 Semantics
</subsectionHeader>
<bodyText confidence="0.999508714285714">
Based on the SVO triple, the meaning of a fact
candidate can be unclear and ambiguous. There-
fore, we first determine the semantics of a fact can-
didate before computing its truthfulness.
Entity Types. We first determine the expected
types of the subject and object in the SVO. For ex-
ample, for the SVO (Einstein) died in (Princeton),
the expected types are person x location. We de-
termine this by first computing the types of en-
tities that are valid for each verb (verbal phrase)
in a large SVO collection of 114m SVO triples
(Talukdar et al., 2012). Typing verbal phrases
is a once-off computation. Our phrase typing
method is similar to prior work on typing rela-
tional phrases (Nakashole et al., 2012). Exam-
ples of typed phrases are: (person) died in (year),
(person) died in (location), and (athlete) plays for
(team). Given a triple, we look up the types for the
subject and the object and then determine which
of the typed phrases are compatible with the cur-
rent triple. We look up entity types in a knowledge
</bodyText>
<footnote confidence="0.992334">
1http://www.w3.org/TR/rdf-primer/
</footnote>
<page confidence="0.994814">
1010
</page>
<bodyText confidence="0.994972216216216">
base containing entities and their types. In partic-
ular, we use the NELL entity typing API (Carlson
et al., 2010). NELL’s entity typing method has
high recall because when entities are not in the
knowledge base, it performs on-the-fly type infer-
ence using the Web. This is not the case for other
options such as (Auer et al., 2007; Bollacker et al.,
2008; Hoffart et al., 2011).
Relation Cardinality. Next, we learn cardinali-
ties of verbal phrases. Cardinality refers to how
arguments of a given relation relate to one another
numerically. We define the relation cardinality of a
verb Card(V ), as the average number of expected
arguments per given subject. For example, for the
relation “died in”, 1 location is expected for each
subject. For other relations, the expected number
of arguments can be greater than 1 but less than
n : n E R, n &gt; 1. We approximate n using
statistics from the 114m SVO corpus based on the
average number of arguments per given first argu-
ment. In a once-off computation, we generate car-
dinality approximations per typed verbal phrase V
and its inverse V −1. For example, we generate
the cardinality estimates for both: (person) died in
(location) and for (location) INVERSE-OF(died
in) (person).
Synonymous Relations. Natural language is di-
verse. Semantically similar phrases can be syntac-
tically different. Therefore, we learn other verbs
that can be used to substitute V in SVO. We
pre-compute synonymous phrases from the 114m
SVO corpus using distributional semantics in the
same spirit as (Lin and Pantel, 2001; Nakashole et
al., 2012).
Synonymous verbs, relation cardinalities, and
entity types enable us to generate alternative fact
candidates.
</bodyText>
<subsectionHeader confidence="0.99937">
2.3 Alternative Fact Candidates
</subsectionHeader>
<bodyText confidence="0.953766854166667">
Truth-finding methods rank fi relative to alter-
native candidates. While prior methods assume
the alternatives are known apriori, we developed
a method for generating alternative fact candi-
dates. For a given fi, we first identify the fixed
argument. The fixed argument is the argument of
the SVO which when fixed, requires finding the
fewest number of alternative candidates. For ex-
ample, for (Einstein) died in (Princeton), the so-
lution is to fix the subject. This is because the car-
dinality of (person) died in (location) is one (1).
On the other hand, the cardinality of “INVERSE-
OF(died in)” is many(n). In other words, the num-
ber of places where a person can be born (one)
is much fewer than the number of people that
can die in a place (many). In our example, al-
ternatives are possible places, other than Prince-
ton, where Einstein could have died. For example:
(Einstein) died in (Germany) or (Einstein) died in
(Switzerland). More generally, the fixed argument
of fact candidate fi, is defined as follows:
Definition 2 (Fixed Argument) Let Card(V) be
the cardinality of V and Card(V −1) be the car-
dinality of the inverse of V, if Card(V) &lt;
Card(V −1), then the fixed argument is the sub-
ject, Argfixed(fi) = S, else it is the object, O. If
Card(V) == Card(V −1), then both arguments
are fixed, one at a time.
We use the fixed argument to define a topic as the
fixed argument plus the verb. Therefore, for the
SVO (X) died in (Y), the topic “places where X
died”, (Argfixed = S), is not the same as the topic
“people who died in Y” (Argfixed = O).
To locate alternatives, we use the topic
(Argfixed + V ) as a query. We search three
sources to either locate relevant documents or rele-
vant triples: the Google Web search API, the 114m
SVO collection, and the NELL KB. The SVO col-
lection and the KB return triples, however, the
Web search API returns documents. Therefore,
we apply a triple extractor to the retrieved docu-
ments. For all potential alternative triples, we per-
form type checking to ensure that the arguments
of the triples are type-compatible with fi. Further-
more, we generate an additional query for every
synonymous verb sVi, replacing V with sVi. Ex-
ample queries are: “Einstein died in”, “Einstein
passed in”, etc.
</bodyText>
<sectionHeader confidence="0.983345" genericHeader="method">
3 Objectivity and Trustworthiness
</sectionHeader>
<bodyText confidence="0.999478888888889">
The principle of objective journalism, which is
a significant part of journalistic ethics, aims to
promote factual and fair reporting, undistorted by
emotion or personal bias (Schudson, 1978; Ka-
plan, 2002). Objectivity is also required in refer-
ence sources such as encyclopedias, scientific pub-
lications, and textbooks. For example, Wikipedia
enforces a neutral point-of-view policy (NPOV)2.
Articles violating the NPOV policy are marked
</bodyText>
<footnote confidence="0.954418">
2http://en.wikipedia.org/wiki/Wikipedia:Neutral point of view
</footnote>
<page confidence="0.992347">
1011
</page>
<bodyText confidence="0.890236805555556">
to indicate potential bias. While opinions, emo-
tions, and speculations can also be expressed us-
ing objective language, they are often stated using
subjective language (Turney et al., 2002; Riloff
and Wiebe, 2003; Yu and Hatzivassiloglou, 2003;
Wiebe et al., 2004; Liu et al., 2005; Recasens et
al., 2013). For example, consider the following
pieces of text:
(S) Well, I think Obama was born in Kenya
because his grandma who lives in Kenya said
he was born there.
(O) Theories allege that Obama’s published
birth certificate is a forgery, that his actual
birthplace is not Hawaii but Kenya.
Text S is a snippet from Yahoo Answers and
text O is a snippet from the Wikipedia page ti-
tled: “Barack Obama Citizenship Conspiracy The-
ories”. S is subjective, expressing the opinion of
the author. On the other hand, O is objective, stat-
ing only what has been alleged. Literature on sen-
timent analysis (Turney et al., 2002; Liu et al.,
2005), subjectivity detection (Riloff and Wiebe,
2003; Wiebe et al., 2004), and bias detection (Yu
and Hatzivassiloglou, 2003; Recasens et al., 2013)
has developed lexicons for identifying subjective
language. Due to the principle of objective jour-
nalism and the requirement of objectivity placed
on reference sources, we hypothesize a link be-
tween objectivity and trustworthiness as follows.
Hypothesis 1 Objective sources are more trust-
worthy than subjective sources. Therefore, we
can assume that fact candidates stated in objec-
tive sources are more likely to be true than those
stated in subjective sources.
To test the validity of the hypothesis, we carried
out a study where we solicited human input.
</bodyText>
<subsectionHeader confidence="0.998489">
3.1 Mechanical Turk Study
</subsectionHeader>
<bodyText confidence="0.99999175">
We deployed an annotation study on Amazon Me-
chanical Turk (MTurk)3, a crowd-sourcing plat-
form for tasks requiring human input. Tasks on
MTurk are small questionnaires consisting of a de-
scription and a set of questions. Our study con-
sisted of two independent tasks. The first task was
titled “Trustworthiness of News Articles”, where
annotators were given a link to a news article and
</bodyText>
<footnote confidence="0.973056">
3http://www.mturk.com
</footnote>
<figureCaption confidence="0.8208935">
Figure 1: Summary of the results of the annotation
study on objectivity and trustworthiness.
</figureCaption>
<bodyText confidence="0.975795774193548">
asked to judge if they thought it was trustworthy
or not. The second task was titled “Objectivity
of News Articles”. For this task, annotators were
asked to judge if a given article is objective or sub-
jective. For both tasks a third option of “not sure”
was provided. We randomly selected 500 news ar-
ticles from a corpus of about 300,000 news articles
obtained from Google News from the topics of
Top News, Business, Entertainment, and SciTech.
For each task, every article was judged by three
annotators. This produced a total of 3000 annota-
tions. When we analyzed the output, we accepted
a label as valid for a given article if the label was
selected by the majority of the judges. Based on
this criteria, we obtained a set of 420 articles that
were both labeled for trustworthiness and objec-
tivity.
A summary of the outcome of the study is
shown in Figure 1; 74% of the untrustworthy
articles were independently labeled as subjec-
tive. On the other hand, 64% of trustworthy
articles were independently labeled as objective.
These results indicate a non-trivial positive cor-
relation between objectivity and trustworthiness.
We leverage this correlation in our believability
computation model. To incorporate objectivity in
FactChecker, we require for a given source docu-
ment, an objectivity score ∈ [0, 1], where 0 means
the source is subjective and 1 means it is objec-
tive. Next, describe our method for automatically
determining objectivity of sources.
</bodyText>
<subsectionHeader confidence="0.997874">
3.2 Automatic Objectivity Detection
</subsectionHeader>
<bodyText confidence="0.999782">
We trained a logistic regression classifier to pre-
dict the objectivity of a document. For training
and testing data, we used the labeled data from
the Mechanical Turk study. We additionally used
labeled text from prior work on subjectivity de-
tection (Pang and Lee, 2004). This resulted in a
total of 4,600 documents, half subjective and the
other half objective. We used 4000 documents for
</bodyText>
<page confidence="0.912489">
1012
</page>
<table confidence="0.9998129">
# Feature
1 Subjectivity lexicon of strong and weak
subjective words (Riloff and Wiebe,
2003).
2 Sentiment lexicon of positive and negative
words (Liu et al., 2005).
3 Wikipedia-derived bias lexicon (Recasens
et al., 2013).
4 Part-of-speech (POS) tags
5 Frequent bi-grams
</table>
<tableCaption confidence="0.999517">
Table 1: Features used for the objectivity detector.
</tableCaption>
<bodyText confidence="0.998023518518519">
training, 2000 per label. The rest of the documents
were split into a development set (380) and a test
set (220).
A summary of the features we used is shown
in Table 1. Features 1-3 refer to lexicons devel-
oped by prior methods on subjectivity (Wiebe et
al., 2004), sentiment analysis (Liu et al., 2005) and
bias detection (Recasens et al., 2013). Feature 4
refers to part-of-speech tags of the terms found in
the document that are also in the lexicons. Feature
5 refers to bi-grams that frequently occur (men-
tion frequency of &gt; 10) in the 4,600 documents.
The most contributing features were the lexicons,
features (1-3) and the frequent bi-grams, feature
5. We discovered that using frequent bi-gram fea-
tures instead of uni-grams or bi-grams resulted in
higher precision. The classifier was able to de-
termine that for example bi-grams such as “think
that”, “so funny” and “you thought” are negative
features for objectivity. Evaluation results of our
objectivity detector vs. baselines are shown in Ta-
ble 2. FactChecker’s objectivity detector has pre-
cision of 0.7814 ± 0.0539, with a 0.9-confidence
Wilson score interval (Brown et al., 2001) and this
outperforms the baselines. Next, we describe how
we leverage objectivity into FactChecker’s truth-
fulness model.
</bodyText>
<sectionHeader confidence="0.998703" genericHeader="method">
4 Believability Computation Model
</sectionHeader>
<bodyText confidence="0.997625375">
FactChecker computes the believability score of a
fact candidate from its: i) objectivity score and
(ii) co-mention score. In this section we define
each of these scores.
The objectivity score reflects the trustworthi-
ness of sources where a fact candidate is men-
tioned. Given a fact candidate fi, mentioned in
a set of documents Di, where each document d E
</bodyText>
<table confidence="0.9997166">
Approach Accuracy
Sentiment Lexicon 0.65±0.06
Wikipedia bias Lexicon 0.69 ±0.06
Subjectivity Lexicon 0.70 ±0.06
FC-Objectivity Detector 0.78 ±0.05
</table>
<tableCaption confidence="0.99918">
Table 2: Accuracy of the objectivity detector.
</tableCaption>
<bodyText confidence="0.97659">
Di has objectivity O(d), fi’s objectivity score is
defined as follows:
</bodyText>
<equation confidence="0.872934333333333">
Definition 3 (Objectivity Score)
E O(dk)
dk∈Di (1)
</equation>
<bodyText confidence="0.999946681818182">
We do not use the sum of objectivity of sources
as the objectivity score because this enables fact
candidates mentioned in many low objectivity
sources to have high aggregate objectivity. Sim-
ilarly, we avoid using average objectivity of the
sources as it overestimates objectivity of candi-
dates stated in few sources. A candidate men-
tioned in 10 sources with 0.9 objectivity should
have higher objectivity than a candidate stated in
1 source of 0.9 objectivity. In Equation 1, log|Di|
addresses this issue.
The co-mention score aims to ensure that fact
candidates mentioned in similar sources have sim-
ilar believability scores. Suppose candidate fi is
mentioned in many highly objective sources, an-
other candidate fj is stated in only one highly ob-
jective source dk where fi is also mentioned. Then
the believability of fj should be boosted by it be-
ing co-mentioned with fi. If on the other hand fi
and fj were co-mentioned in a subjective source,
fj should receive less boost from fi. This leads us
to the co-mention score µ(fi) of a candidate.
</bodyText>
<equation confidence="0.887234">
Definition 4 (Co-Mention Score)
µ(fi) = ρ(fi) + � wijµ(fj) (2)
fj∈F
</equation>
<bodyText confidence="0.9985665">
Where ρ(fi) is the normalized mention fre-
quency of fi. The propagation weight wij controls
how much boost is obtained from a co-mentioned
candidate. We define propagation weight, wij, as
the average of the objectivity of the sources that
mention both candidates.
</bodyText>
<equation confidence="0.997065333333333">
wij = average O(dk) : dk E (Di n Dj) (3)
O(fi) = log|Di|.
|Di|
</equation>
<page confidence="0.847613">
1013
</page>
<bodyText confidence="0.98586428125">
where O(dk) is the objectivity of document dk,
Di and Dj are the sets of documents that mention
fi and fj, respectively. Notice that we could boost
co-mentioned but not related candidates, thereby
causing false boosts. To remedy this, we only al-
low wij to be greater than zero if the fact can-
didates fi and fj are on the same topic. Recall
that the topic is determined by the fixed argument
(Definition 2) and the verb. Allowing only fact
candidates on the same topic to influence each
other is important considering that many trivial
facts are often repeated in sources of diverse qual-
ity.
To leverage the inter-dependencies among re-
lated co-mentioned fact candidates, we model the
solution with a graph ranking method. Each fact
candidate is a node and there is an edge between
each pair of related fact candidate nodes fi and
fj, with wij as the edge weight. Thus, equation 2
can be reformulated as µ = Mµ, where µ is the
co-mention score vector and M is a Markov ma-
trix which is stochastic, irreducible and aperiodic.
Thus, a power method will converge to a solution
in a similar manner to PageRank. Implementation
consists of iteratively applying Equation 2 until
the change in the score is less than a threshold E.
The solution is the final co-mention scores of fact
candidates.
Finally, to compute the believability score of a
fact candidate, we linearly combine its objectivity
score with its co-mention as follows:
Definition 5 (Believability Score)
</bodyText>
<equation confidence="0.999341">
0(fi) = λO(fi) + (1 − λ)µ(fi) (4)
</equation>
<bodyText confidence="0.997786142857143">
Where λ is a weighting parameter ∈ [0, 1]
which controls the relative importance of the two
aspects of FactChecker. As we show in our exper-
iments, λ can be robustly chosen within the range
of 0.2 to 0.6. In our experiments we used λ = 0.6.
The entire procedure of FactChecker is summa-
rized in Algorithm 1.
</bodyText>
<sectionHeader confidence="0.998641" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.8360888">
We evaluated FactChecker for accuracy. We de-
fine accuracy as the probability of a true fact can-
didate having a higher believability score than a
false candidate. Let τ(fi) ∈ {T, F} be the truth-
fulness of a fact candidate fi, accuracy is defined
as:
Algorithm 1 FactChecker
Input: A set F of fact candidates
Input: KB K, SVO corpus C, Web W
Output: A set L of rankings ∀fi ∈ F
</bodyText>
<equation confidence="0.954812588235294">
L = ∅
while F =6 ∅ do
pick fi from F
A= getAlternatives(fi,K,C,W)
PriorityQueue Li = ∅
for all alternative fact candidates fj&apos; ∈ A do
0(f j) = getBelievabilityScore(fj)
Li.insert(fj, 0(f j))
end for
0(fi) = getBelievabilityScore(fi)
Li.insert(fi, 0(fi))
L ∪ Li
Remove fi from F
end while
return L
Acc = E(r(fi)=T:r(fj)=F)(0(fi) &gt; 0(fj))
|{∀(fi,fj) : τ(fi) = T ∧ τ(fj) = F}|
</equation>
<bodyText confidence="0.9836448">
Datasets. We evaluated FactChecker on three
datasets: i) KB Fact Candidates: The first dataset
consists of fact candidates taken from the fact ex-
traction pipeline of a state-of-the-art knowledge
base, NELL (Carlson et al., 2010). The fact candi-
dates span four different relation types: company
acquisitions, book authors, movie directors and
athlete teams. For each fact candidate, we applied
our alternative candidate generation method. We
only considered fact candidates with non-trivial
alternative candidate sets; where the alternative
candidate set is greater than zero. Since all of
the baselines we compared against assume alter-
natives are provided, we apply all methods to the
same set of alternative fact candidates discovered
by our method. Details of this dataset are shown
as rows starting with “KB-” in Table 3.
ii) Wikipedia Fact Candidates: For the sec-
ond dataset, we did not restrict the fact candidates
to specific topics from a knowledge base, instead
we aimed to evaluate all fact candidates about a
given entity. We selected entities from Wikipe-
dia. For this, we chose US politicians: all current
state senators, all current state governors, and all
44 presidents. First, we extracted fact candidates
</bodyText>
<page confidence="0.962323">
1014
</page>
<table confidence="0.99927">
#Candidates #Alternatives
KB-Acquisitions 50 241
KB-Authors 50 295
KB-Directors 50 228
KB-Teams 40 162
WKP Politicians 54 219
GK Quiz 18 72
</table>
<tableCaption confidence="0.998679">
Table 3: Fact candidate datasets.
</tableCaption>
<bodyText confidence="0.999846">
from the infoboxes of the Wikipedia pages of the
entities. Second, we applied our alternative can-
didate generation method to discover alternatives
from the Web, SVO corpus, and HELL. Details of
the resulting dataset are shown in the row “WKP
Politicians” in Table 3.
iii) General Knowledge Quiz: The third
dataset consists of questions from a general
knowledge quiz 4. We selected questions from
the inventions category. Questions are multiple
choice, with 4 options per question. Thus, from
each question, we created one fact candidate and
3 alternative candidates. Details of the resulting
dataset are shown in the row “KWP Quiz” in Ta-
ble 3.
Baselines. We compared FactChecker against five
baselines: i) Vote counts the number of sources
that mention the fact candidate. ii) TruthFinder is
an iterative voting approach where votes are prop-
agated from sources to fact candidates and then
back to sources. Implemented as described in (Yin
et al., 2007). iii) Investment is also based on tran-
sitive voting, however scores are updated differ-
ently. A source gets a vote of trust from each
candidate it “invests” in, but the vote is weighted
by the proportion of trust the source previously
“invested” in the candidate relative to other in-
vestors. Implemented as described in (Pasternack
and Roth, 2010). iv) PooledInvest is a variation
of investment, we report both because in their pa-
per, there was no clear winner among the two vari-
ations. v) 2-Estimates is a probabilistic model
which approximates error rates of sources and fact
candidates (Galland et al., 2010).
</bodyText>
<subsectionHeader confidence="0.992366">
5.1 Accuracy on KB Fact Candidates
</subsectionHeader>
<bodyText confidence="0.951591">
Figure 2 shows accuracy on KB fact candidates.
FactChecker achieves accuracy between 70% and
88% and is significantly more accurate than the
</bodyText>
<footnote confidence="0.959738">
4http://www.indiabix.com/general-knowledge/
</footnote>
<figureCaption confidence="0.9999975">
Figure 2: Accuracy of KB fact candidates.
Figure 3: FactChecker variations.
</figureCaption>
<bodyText confidence="0.999968642857143">
other approaches on all relations except com-
pany acquisitions. On book authors, movie di-
rectors, and athlete teams, FactChecker outper-
forms all other approaches by at least 10%, 9%,
and 8% respectively. On company acquisitions,
the different methods achieve similar accuracy,
with TruthFinder being the most accurate and
FactChecker is 4% behind. Company acquisitions
also yield the lowest difference between Vote and
the highest performing method, of 6%. For book
authors, movie directors, and athlete teams, the
difference between majority Vote and the highest
performing method (FactChecker in this case) is
13%, 12%, and 13% respectively.
</bodyText>
<subsectionHeader confidence="0.999953">
5.2 Accuracy of FactChecker Variations
</subsectionHeader>
<bodyText confidence="0.999884571428571">
To quantify how various aspects of our approach
affect overall performance, we studied two varia-
tions. The first variation is FC-Objectivity which
only uses objectivity to compute believability.
Thus, λ = 1 in Definition 5. The second varia-
tion is FC-CoMention which only uses co-mention
scores to compute believability, λ = 0. The
</bodyText>
<page confidence="0.965054">
1015
</page>
<table confidence="0.999595888888889">
Approach WKP Politicians GK Quiz
Vote 0.85±0.09 0.82±0.15
TruthFinder 0.85±0.09 0.82±0.15
2-Estimates 0.85±0.09 0.82±0.15
Investment 0.86±0.08 0.82±0.15
PooledInvest 0.85±0.09 0.82±0.15
FC-Objectivity 0.88±0.08 0.87±0.12
FC-CoMention 0.85±0.09 0.72±0.18
FactChecker 0.90±0.07 0.87±0.12
</table>
<tableCaption confidence="0.999863">
Table 4: Accuracy on politicians and quiz data sets
</tableCaption>
<bodyText confidence="0.99966175">
last variation is the full FactChecker method us-
ing both objectivity and co-mentions with A = 0.6
From Figure 3, it is clear that both the objectiv-
ity of sources and the influence of co-mentions
contribute to the overall accuracy of FactChecker.
Full-fledged FactChecker performs better than
both variations. In most cases, FC-Objectivity per-
forms better than FC-CoMention.
</bodyText>
<subsectionHeader confidence="0.999415">
5.3 Accuracy on Wikipedia Fact Candidates
</subsectionHeader>
<bodyText confidence="0.999669">
Table 4, column “WKP Politicians”, shows ac-
curacy on Wikipedia fact candidates, with a 0.9-
confidence Wilson score interval (Brown et al.,
2001). For this dataset we again see FactChecker
outperforming the other methods under compari-
son. On this dataset, FactChecker has a accuracy
of 0.9 ± 0.07 and a 5% accuracy advantage over
the other methods. The second best performance
comes from the FC-Objectivity variation, with ac-
curacy of 0.88 ± 0.08.
</bodyText>
<subsectionHeader confidence="0.99929">
5.4 Accuracy on General Knowledge Quiz
</subsectionHeader>
<bodyText confidence="0.998053">
Table 4, column “GK Quiz ”, shows accuracy on
the general knowledge quiz fact candidates. On
this dataset, FactChecker and its objectivity-only
variation (FC-objectivity) have the highest accu-
racy of 87%. Notice that this dataset was the only
one where we did not generate the alternative fact
candidates. Instead, we took the options of the
multiple choice questions as alternatives. Since
the quiz is meant to be taken by humans, the alter-
natives are often very close, plausible answers. Yet
even in this difficult setting, we see FactChecker
outperforming the baselines.
Sample fact candidates, with ranked alternatives
from all three datasets are shown in Table 5.
</bodyText>
<figureCaption confidence="0.997323">
Figure 4: Effect of A of FactChecker.
</figureCaption>
<subsectionHeader confidence="0.97889">
5.5 Parameter Sensitivity
</subsectionHeader>
<bodyText confidence="0.999884833333333">
We analyzed the effect of the selection of lambda
A (see Definition 5) on FactChecker’s perfor-
mance. The result of this analysis is shown in Fig-
ure 4. FactChecker is insensitive to this parame-
ter when A is varied from 0.2 to 0.6. Therefore,
lambda can be robustly chosen within this range.
</bodyText>
<subsectionHeader confidence="0.884487">
5.6 Discussion
</subsectionHeader>
<bodyText confidence="0.999986933333333">
Overall, from these results we make the follow-
ing observations: i) Majority vote is a competitive
baseline; ii) Iterative voting-based methods pro-
vide slight improvements on majority vote. This
is due to the fact that at the core of iterative vot-
ing is still the assumption that fact candidates
mentioned in many sources are more likely to be
true. Therefore, for both majority vote and it-
erative voting, when mention frequencies of var-
ious alternatives are the same, accuracy suffers.
Based on these observations, it is clear that truth-
finding solutions need to incorporate fine-grained
content-aware features outside of external votes.
FactChecker takes a step in this direction by incor-
porating the document-level feature of objectivity.
</bodyText>
<sectionHeader confidence="0.999902" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.984897833333333">
There is a fairly small body of work on truth-
finding (Yin et al., 2007; Galland et al., 2010;
Pasternack and Roth, 2010; Li et al., 2011; Yin and
Tan, 2011; Zhao et al., 2012; Pasternack and Roth,
2013). The method underlying most truth-finding
algorithms is iterative transitive voting (Yin et al.,
2007; Galland et al., 2010; Pasternack and Roth,
2010; Li et al., 2011). Fact candidates are ini-
tialized with a score. Trustworthiness of sources
is then computed from the believability of the fact
candidates they mention. In return, believability of
candidates is recomputed based on the trustworthi-
</bodyText>
<page confidence="0.964175">
1016
</page>
<table confidence="0.999641923076923">
Dataset Fact Candidate Alternatives &amp; Ranking
WKP (George W. Bush) lived in (Midland,TX) 1.Midland,TX
2.Compton,CA
3.Washington D.C.
4.Venezuela*
KB (Dirk Kuyt) plays for (Liverpool) 1. Liverpool
2.Cardiff City*
3.Netherlands
4.Hungary*
Quiz (Bifocals) invented by (Benjamin Franklin) 1. Benjamin Franklin
2. Rudolf Diesel*
3.Thomas Alva Edison*
4.Alfred B. Nobel*
</table>
<tableCaption confidence="0.8820745">
Table 5: Sample rankings by FactChecker, alternatives marked (*) are false. The ranking of the candidate
from the “KB” dataset is not completely accurate.
</tableCaption>
<bodyText confidence="0.999079638297872">
ness of their sources. This process is repeated over
several iterations until convergence. (Yin et al.,
2007) was the first to implement this idea, subse-
quent work improved upon iterative voting in sev-
eral directions. (Dong et al., 2009) incorporates
copying-detection; giving high trust to sources
that are independently authored. (Galland et al.,
2010) approximates error rates of sources and fact
candidates. (Pasternack and Roth, 2010) intro-
duces prior knowledge in the form of linear pro-
gramming constraints in order to ensure that the
truth discovered is consistent with what is already
known. (Yin and Tan, 2011) introduces supervi-
sion by using ground truth facts so that sources
that disagree with the ground truth are penalized.
(Li et al., 2011) uses search engine APIs to gather
additional evidence for believability of fact can-
didates. WikiTrust (Adler and Alfaro, 2007) is
a content-aware but domain-specific method. It
computes trustworthiness of wiki authors based
on the revision history of the articles they have
authored. Motivated by interpretability of prob-
abilistic scores, two recent papers addressed the
truth-finding problem as a probabilistic inference
problem over the sources and the fact candidates
(Zhao et al., 2012; Pasternack and Roth, 2013).
Truth-finders based on textual entailment such as
TruthTeller (Lotan et al., 2013) determine if a sen-
tence states something or not. The focus is on un-
derstanding natural language, including the use of
negation. This is similar to the goal of fact ex-
traction (Banko et al., 2007; Carlson et al., 2010;
Fader et al., 2011; Nakashole et al., 2011; Del
Corro and Gemulla, 2013).
In a departure from prior work, our method
leverages language of sources in its believability
computation model. Furthermore, we introduced
a co-mention score which is designed to avoid po-
tential false boots among fact candidates. Addi-
tionally, we developed a method for generating al-
ternative fact candidates. Prior methods assume
these are readily available. Only (Li et al., 2011)
uses the Web to identify alternatives, however, this
is only done after manually specifying the fixed ar-
gument. In contrast, we introduced a method for
identifying the fixed argument based on relation
cardinalities learned from SVO statistics.
</bodyText>
<sectionHeader confidence="0.998377" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.998764076923077">
In this paper, we presented FactChecker, a
language-aware approach to truth-finding. In con-
trast to prior approaches, which rely on external
votes, FactChecker includes objectivity of sources
in its believability computation model.
FactChecker can be seen as a first step to-
wards language-aware truth-finding. Future di-
rections include using more sentence-level fea-
tures such the use of hedges, assertive verbs, and
factive verbs. These types of words fall into a
class of words used to express certainties, spec-
ulations or doubts — these are important cues that
FactChecker can leverage.
</bodyText>
<sectionHeader confidence="0.998285" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99974625">
We thank members of the NELL team at CMU
for their helpful comments. This research was
supported by DARPA under contract number
FA8750-13-2-0005.
</bodyText>
<page confidence="0.995275">
1017
</page>
<sectionHeader confidence="0.990204" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99979279245283">
S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyga-
niak, Z.G. Ives: DBpedia: A Nucleus for a Web of
Open Data. In Proceedings of the 6th International
Semantic Web Conference (ISWC), pages 722–735,
Busan, Korea, 2007.
B. T. Adler, L. de Alfaro: A content-driven reputa-
tion system for the wikipedia. In Proceedings of the
16th International Conference on World Wide Web
(WWW), pages 261-270, 2007.
M. Banko, M. J. Cafarella, S. Soderland, M. Broad-
head, O. Etzioni: Open Information Extraction from
the Web. In Proceedings of the 20th International
Joint Conference on Artificial Intelligence (IJCAI),
pages 2670–2676, Hyderabad, India, 2007.
K. D. Bollacker, C. Evans, P. Paritosh, T. Sturge, J.
Taylor: Freebase: a Collaboratively Created Graph
Database for Structuring Human Knowledge. In
Proceedings of the ACM SIGMOD International
Conference on Management of Data (SIGMOD),
pages, 1247-1250, Vancouver, BC, Canada, 2008.
L. D. Brown, T.T. Cai, A. Dasgupta: Interval Estima-
tion for a Binomial Proportion. Statistical Science
16: pages 101–133, 2001.
E. Cabrio, S. Villata: Combining Textual Entailment
and Argumentation Theory for Supporting Online
Debates Interaction. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pp. 208-212, 2012.
A. Carlson, J. Betteridge, R.C. Wang, E.R. Hruschka,
T.M. Mitchell: Coupled Semi-supervised Learning
for Information Extraction. In Proceedings of the
Third International Conference on Web Search and
Web Data Mining (WSDM), pages 101–110, New
York, NY, USA, 2010.
L. Del Corro, R. Gemulla: ClausIE: clause-based
open information extraction. In Proceedings of the
22nd International Conference on World Wide Web
(WWW), pages 355-366. 2013.
X. Dong, L. Berti-Equille, D. Srivastava: Truth discov-
ery and copying detection in a dynamic world. In
Proceedings of the VLDB Endowment PVLDB, 2(1),
pp. 562-573, 2009.
A. Fader, S. Soderland, O. Etzioni: Identifying Rela-
tions for Open Information Extraction. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1535–1545, Edinburgh, UK, 2011.
A. Galland, S. Abiteboul, A. Marian, P. Senellart: Cor-
roborating information from disagreeing views. In
Proceedings of the 3rd International Conference on
Web Search and Web Data Mining (WSDM), pages
131-140, 2010.
C. Havasi, R. Speer, J. Alonso: ConceptNet 3: a Flex-
ible, Multilingual Semantic Network for Common
Sense Knowledge. In Proceedings of the Recent Ad-
vances in Natural Language Processing (RANLP),
Borovets, Bulgaria, 2007.
J. Hoffart, F. Suchanek, K. Berberich, E. Lewis-
Kelham, G. de Melo, G. Weikum: YAGO2: Ex-
ploring and Querying World Knowledge in Time,
Space, Context, and Many Languages. In Proceed-
ings of the 20th International Conference on World
Wide Web (WWW), pages 229–232, Hyderabad, In-
dia. 2011.
R. Kaplan: Politics and the American Press: The Rise
of Objectivity, pages 1865-1920, New York, Cam-
bridge University Press, 2002.
X. Li and W. Meng, C. T. Yu: T-verifier: Verifying
truthfulness of fact statements. In Proceedings of
the International Conference on Data Engineering
(ICDE), pp. 63-74, 2011.
D. Lin, P. Pantel: DIRT: discovery of inference rules
from text. KDD 2001
B. Liu, M. Hu, J. Cheng: Opinion Observer: analyzing
and comparing opinions on the Web. InProceedings
of the 14th International Conference on World Wide
Web (WWW), pages 342351, 2005.
A. Lotan, A. Stern, I. Dagan TruthTeller: Annotating
Predicate Truth. In Proceedings ofHuman Language
Technologies: Conference of the North American
Chapter of the Association of Computational Lin-
guistics (HLT-NAACL), pp. 752-757, 2013.
N. Nakashole, M. Theobald, G. Weikum: Scalable
Knowledge Harvesting with High Precision and
High Recall. In Proceedings of the 4th International
Conference on Web Search and Web Data Mining
(WSDM), pages 227–326, Hong Kong, China, 2011.
N. Nakashole, T. Tylenda, G. Weikum: Fine-grained
Semantic Typing of Emerging Entities. In Proceed-
ings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), pp. 1488-
1497, 2013.
N. Nakashole, G. Weikum, F. Suchanek: PATTY:
A Taxonomy of Relational Patterns with Seman-
tic Types. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 1135 -
1145, Jeju, South Korea, 2012.
V. Nastase, M. Strube, B. Boerschinger, C. Zirn, A.
Elghafari: WikiNet: A Very Large Scale Multi-
Lingual Concept Network. In Proceedings of the 7th
International Conference on Language Resources
and Evaluation(LREC), Malta, 2010.
B. Pang, L. Lee: A Sentimental Education: Sentiment
Analysis Using Subjectivity Summarization Based
</reference>
<page confidence="0.787287">
1018
</page>
<reference confidence="0.99980259375">
on Minimum Cuts. In Proceedings of the 42nd An-
nual Meeting of the Association for Computational
Linguistics (ACL), 271-278, 2004.
J. Pasternack, D. Roth: Knowing What to Believe. In
Proceedings the International Conference on Com-
putational Linguistics (COLING), pp. 877-885, Bei-
jing, China. 2010.
J. Pasternack, D. Roth: Latent credibility analysis. In
Proceedings of the 22nd International Conference
on World Wide Web (WWW), pp. 1009-1020, 2013.
E. Riloff, J. Wiebe: Learning Learning extraction pat-
terns for subjective expressions. In Proceedings of
the 2011 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 105112,
2013.
M. Recasens, C. Danescu-Niculescu-Mizil, D. Juraf-
sky: Linguistic Models for Analyzing and Detecting
Biased Language. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pp. 1650-1659, 2013.
F. Niu, C. Zhang, C. Re, J. W. Shavlik: DeepDive:
Web-scale Knowledge-base Construction using Sta-
tistical Learning and Inference. In the VLDS Work-
shop, pages 25-28, 2012.
M. Schudson: Discovering the News: A Social History
of American Newspapers. New York: Basic Books.
1978.
F. M. Suchanek, M. Sozio, G. Weikum: SOFIE: A
Self-organizing Framework for Information Extrac-
tion. InProceedings of the 18th International Con-
ference on World Wide Web (WWW), pages 631–640,
Madrid, Spain, 2009.
P. P. Talukdar, D. T. Wijaya, T.M. Mitchell: Acquir-
ing temporal constraints between relations. In Pro-
ceeding of the 21st ACM International Conference
on Information and Knowledge Management, pages
992-1001, CIKM 2012.
P. D. Turney: Thumbs up or thumbs down? Seman-
tic orientation applied to unsupervised classification
of reviews. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 417424. 2002.
J. Wiebe, T. Wilson, R. Bruce, M. Bell, M. Martin:
Learning subjective language. Computational Lin-
guistics, 30(3):277308. 2004.
X. Yin, J. Han, P. S. Yu: Truth Discovery with
Multiple Conflicting Information Providers on the
Web. In Proceedings of the International Confer-
ence on Knowledge Discovery in Databases (KDD)
, pages1048-1052. 2007.
X. Yin, W. Tan: Semi-supervised truth discover. In
Proceedings of the 19th International Conference on
World Wide Web (WWW), pp. 217-226, 2011.
H. Yu, V. Hatzivassiloglou: Towards Answering Opin-
ion Questions: Separating Facts from Opinions and
Identifying the Polarity of Opinion Sentences. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages. 129-
136, 2003.
B. Zhao, B. I. P. Rubinstein, J. Gemmell, J. Han: A
Bayesian approach to discovering truth from con-
flicting sources for data integration. In Proceedings
of the VLDB Endowment (PVLDB), 5(6):550-561,
2012.
</reference>
<page confidence="0.996049">
1019
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.847114">
<title confidence="0.999151">Language-Aware Truth Assessment of Fact Candidates</title>
<author confidence="0.96712">Ndapandula</author>
<affiliation confidence="0.993655">Carnegie Mellon</affiliation>
<address confidence="0.9543935">5000 Forbes Pittsburgh, PA,</address>
<email confidence="0.999494">ndapa@cs.cmu.edu</email>
<abstract confidence="0.998296368421053">This paper introduces FactChecker, language-aware approach to truth-finding. FactChecker differs from prior approaches in that it does not rely on iterative peer voting, instead it leverages language to infer believability of fact candidates. In particular, FactChecker makes use of linguistic features to detect if a given source objectively states facts or is speculative and opinionated. To ensure that fact candidates mentioned in similar sources have similar believability, FactChecker augments objectivity with a co-mention score to compute the overall believability score of a fact candidate. Our experiments on various datasets show that FactChecker yields higher accuracy than existing approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Auer</author>
<author>C Bizer</author>
<author>G Kobilarov</author>
<author>J Lehmann</author>
<author>R Cyganiak</author>
<author>Z G</author>
</authors>
<title>Ives: DBpedia: A Nucleus for a Web of Open Data.</title>
<date>2007</date>
<booktitle>In Proceedings of the 6th International Semantic Web Conference (ISWC),</booktitle>
<pages>722--735</pages>
<location>Busan,</location>
<contexts>
<context position="2018" citStr="Auer et al., 2007" startWordPosition="285" endWordPosition="288"> for easy Web authorship. Blogs, forums and social networking websites are not subject to traditional journalistic standards. Consequently, the accuracy of information reported by these sources is often unclear. Even more established newspapers and websites may sometimes report false information as they race to break stories. Therefore, truth-finding is becoming an inTom M. Mitchell Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA, 15213 tom.mitchell@cs.cmu.edu creasingly important problem. Information extraction projects aim to distill relational facts from natural language text (Auer et al., 2007; Bollacker et al., 2008; Carlson et al., 2010; Fader et al., 2011; Nakashole et al., 2011; Del Corro and Gemulla, 2013). These projects have produced knowledge bases containing many millions of relational facts between entities. However, despite these impressive advances, there are still major limitations regarding precision. Within the context of information extraction, fact extractors assign confidence scores to extracted facts. However, such scores are often tied to the extractor’s ability to read and understand natural language text. This is different from a score that indicates the degre</context>
<context position="8787" citStr="Auer et al., 2007" startWordPosition="1363" endWordPosition="1366">n) died in (location), and (athlete) plays for (team). Given a triple, we look up the types for the subject and the object and then determine which of the typed phrases are compatible with the current triple. We look up entity types in a knowledge 1http://www.w3.org/TR/rdf-primer/ 1010 base containing entities and their types. In particular, we use the NELL entity typing API (Carlson et al., 2010). NELL’s entity typing method has high recall because when entities are not in the knowledge base, it performs on-the-fly type inference using the Web. This is not the case for other options such as (Auer et al., 2007; Bollacker et al., 2008; Hoffart et al., 2011). Relation Cardinality. Next, we learn cardinalities of verbal phrases. Cardinality refers to how arguments of a given relation relate to one another numerically. We define the relation cardinality of a verb Card(V ), as the average number of expected arguments per given subject. For example, for the relation “died in”, 1 location is expected for each subject. For other relations, the expected number of arguments can be greater than 1 but less than n : n E R, n &gt; 1. We approximate n using statistics from the 114m SVO corpus based on the average nu</context>
</contexts>
<marker>Auer, Bizer, Kobilarov, Lehmann, Cyganiak, G, 2007</marker>
<rawString>S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, Z.G. Ives: DBpedia: A Nucleus for a Web of Open Data. In Proceedings of the 6th International Semantic Web Conference (ISWC), pages 722–735, Busan, Korea, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B T Adler</author>
<author>L</author>
</authors>
<title>de Alfaro: A content-driven reputation system for the wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th International Conference on World Wide Web (WWW),</booktitle>
<pages>261--270</pages>
<marker>Adler, L, 2007</marker>
<rawString>B. T. Adler, L. de Alfaro: A content-driven reputation system for the wikipedia. In Proceedings of the 16th International Conference on World Wide Web (WWW), pages 261-270, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>M J Cafarella</author>
<author>S Soderland</author>
<author>M Broadhead</author>
<author>O</author>
</authors>
<title>Etzioni: Open Information Extraction from the Web.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<pages>2670--2676</pages>
<location>Hyderabad, India,</location>
<contexts>
<context position="33282" citStr="Banko et al., 2007" startWordPosition="5337" endWordPosition="5340">omputes trustworthiness of wiki authors based on the revision history of the articles they have authored. Motivated by interpretability of probabilistic scores, two recent papers addressed the truth-finding problem as a probabilistic inference problem over the sources and the fact candidates (Zhao et al., 2012; Pasternack and Roth, 2013). Truth-finders based on textual entailment such as TruthTeller (Lotan et al., 2013) determine if a sentence states something or not. The focus is on understanding natural language, including the use of negation. This is similar to the goal of fact extraction (Banko et al., 2007; Carlson et al., 2010; Fader et al., 2011; Nakashole et al., 2011; Del Corro and Gemulla, 2013). In a departure from prior work, our method leverages language of sources in its believability computation model. Furthermore, we introduced a co-mention score which is designed to avoid potential false boots among fact candidates. Additionally, we developed a method for generating alternative fact candidates. Prior methods assume these are readily available. Only (Li et al., 2011) uses the Web to identify alternatives, however, this is only done after manually specifying the fixed argument. In con</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, O, 2007</marker>
<rawString>M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead, O. Etzioni: Open Information Extraction from the Web. In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI), pages 2670–2676, Hyderabad, India, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K D Bollacker</author>
<author>C Evans</author>
<author>P Paritosh</author>
<author>T Sturge</author>
<author>J Taylor</author>
</authors>
<title>Freebase: a Collaboratively Created Graph Database for Structuring Human Knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACM SIGMOD International Conference on Management of Data (SIGMOD),</booktitle>
<pages>1247--1250</pages>
<location>Vancouver, BC, Canada,</location>
<contexts>
<context position="2042" citStr="Bollacker et al., 2008" startWordPosition="289" endWordPosition="292">rship. Blogs, forums and social networking websites are not subject to traditional journalistic standards. Consequently, the accuracy of information reported by these sources is often unclear. Even more established newspapers and websites may sometimes report false information as they race to break stories. Therefore, truth-finding is becoming an inTom M. Mitchell Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA, 15213 tom.mitchell@cs.cmu.edu creasingly important problem. Information extraction projects aim to distill relational facts from natural language text (Auer et al., 2007; Bollacker et al., 2008; Carlson et al., 2010; Fader et al., 2011; Nakashole et al., 2011; Del Corro and Gemulla, 2013). These projects have produced knowledge bases containing many millions of relational facts between entities. However, despite these impressive advances, there are still major limitations regarding precision. Within the context of information extraction, fact extractors assign confidence scores to extracted facts. However, such scores are often tied to the extractor’s ability to read and understand natural language text. This is different from a score that indicates the degree to which a given fact </context>
<context position="8811" citStr="Bollacker et al., 2008" startWordPosition="1367" endWordPosition="1370">n), and (athlete) plays for (team). Given a triple, we look up the types for the subject and the object and then determine which of the typed phrases are compatible with the current triple. We look up entity types in a knowledge 1http://www.w3.org/TR/rdf-primer/ 1010 base containing entities and their types. In particular, we use the NELL entity typing API (Carlson et al., 2010). NELL’s entity typing method has high recall because when entities are not in the knowledge base, it performs on-the-fly type inference using the Web. This is not the case for other options such as (Auer et al., 2007; Bollacker et al., 2008; Hoffart et al., 2011). Relation Cardinality. Next, we learn cardinalities of verbal phrases. Cardinality refers to how arguments of a given relation relate to one another numerically. We define the relation cardinality of a verb Card(V ), as the average number of expected arguments per given subject. For example, for the relation “died in”, 1 location is expected for each subject. For other relations, the expected number of arguments can be greater than 1 but less than n : n E R, n &gt; 1. We approximate n using statistics from the 114m SVO corpus based on the average number of arguments per gi</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>K. D. Bollacker, C. Evans, P. Paritosh, T. Sturge, J. Taylor: Freebase: a Collaboratively Created Graph Database for Structuring Human Knowledge. In Proceedings of the ACM SIGMOD International Conference on Management of Data (SIGMOD), pages, 1247-1250, Vancouver, BC, Canada, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L D Brown</author>
<author>T T Cai</author>
<author>A Dasgupta</author>
</authors>
<title>Interval Estimation for a Binomial Proportion.</title>
<date>2001</date>
<journal>Statistical Science</journal>
<volume>16</volume>
<pages>101--133</pages>
<contexts>
<context position="18454" citStr="Brown et al., 2001" startWordPosition="2954" endWordPosition="2957">of &gt; 10) in the 4,600 documents. The most contributing features were the lexicons, features (1-3) and the frequent bi-grams, feature 5. We discovered that using frequent bi-gram features instead of uni-grams or bi-grams resulted in higher precision. The classifier was able to determine that for example bi-grams such as “think that”, “so funny” and “you thought” are negative features for objectivity. Evaluation results of our objectivity detector vs. baselines are shown in Table 2. FactChecker’s objectivity detector has precision of 0.7814 ± 0.0539, with a 0.9-confidence Wilson score interval (Brown et al., 2001) and this outperforms the baselines. Next, we describe how we leverage objectivity into FactChecker’s truthfulness model. 4 Believability Computation Model FactChecker computes the believability score of a fact candidate from its: i) objectivity score and (ii) co-mention score. In this section we define each of these scores. The objectivity score reflects the trustworthiness of sources where a fact candidate is mentioned. Given a fact candidate fi, mentioned in a set of documents Di, where each document d E Approach Accuracy Sentiment Lexicon 0.65±0.06 Wikipedia bias Lexicon 0.69 ±0.06 Subject</context>
<context position="28469" citStr="Brown et al., 2001" startWordPosition="4576" endWordPosition="4579">±0.12 Table 4: Accuracy on politicians and quiz data sets last variation is the full FactChecker method using both objectivity and co-mentions with A = 0.6 From Figure 3, it is clear that both the objectivity of sources and the influence of co-mentions contribute to the overall accuracy of FactChecker. Full-fledged FactChecker performs better than both variations. In most cases, FC-Objectivity performs better than FC-CoMention. 5.3 Accuracy on Wikipedia Fact Candidates Table 4, column “WKP Politicians”, shows accuracy on Wikipedia fact candidates, with a 0.9- confidence Wilson score interval (Brown et al., 2001). For this dataset we again see FactChecker outperforming the other methods under comparison. On this dataset, FactChecker has a accuracy of 0.9 ± 0.07 and a 5% accuracy advantage over the other methods. The second best performance comes from the FC-Objectivity variation, with accuracy of 0.88 ± 0.08. 5.4 Accuracy on General Knowledge Quiz Table 4, column “GK Quiz ”, shows accuracy on the general knowledge quiz fact candidates. On this dataset, FactChecker and its objectivity-only variation (FC-objectivity) have the highest accuracy of 87%. Notice that this dataset was the only one where we di</context>
</contexts>
<marker>Brown, Cai, Dasgupta, 2001</marker>
<rawString>L. D. Brown, T.T. Cai, A. Dasgupta: Interval Estimation for a Binomial Proportion. Statistical Science 16: pages 101–133, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Cabrio</author>
<author>S Villata</author>
</authors>
<title>Combining Textual Entailment and Argumentation Theory for Supporting Online Debates Interaction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>208--212</pages>
<marker>Cabrio, Villata, 2012</marker>
<rawString>E. Cabrio, S. Villata: Combining Textual Entailment and Argumentation Theory for Supporting Online Debates Interaction. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 208-212, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Carlson</author>
<author>J Betteridge</author>
<author>R C Wang</author>
<author>E R Hruschka</author>
<author>T M Mitchell</author>
</authors>
<title>Coupled Semi-supervised Learning for Information Extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the Third International Conference on Web Search and Web Data Mining (WSDM),</booktitle>
<pages>101--110</pages>
<location>New York, NY, USA,</location>
<contexts>
<context position="2064" citStr="Carlson et al., 2010" startWordPosition="293" endWordPosition="296"> social networking websites are not subject to traditional journalistic standards. Consequently, the accuracy of information reported by these sources is often unclear. Even more established newspapers and websites may sometimes report false information as they race to break stories. Therefore, truth-finding is becoming an inTom M. Mitchell Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA, 15213 tom.mitchell@cs.cmu.edu creasingly important problem. Information extraction projects aim to distill relational facts from natural language text (Auer et al., 2007; Bollacker et al., 2008; Carlson et al., 2010; Fader et al., 2011; Nakashole et al., 2011; Del Corro and Gemulla, 2013). These projects have produced knowledge bases containing many millions of relational facts between entities. However, despite these impressive advances, there are still major limitations regarding precision. Within the context of information extraction, fact extractors assign confidence scores to extracted facts. However, such scores are often tied to the extractor’s ability to read and understand natural language text. This is different from a score that indicates the degree to which a given fact candidate is believabl</context>
<context position="8570" citStr="Carlson et al., 2010" startWordPosition="1324" endWordPosition="1327">12). Typing verbal phrases is a once-off computation. Our phrase typing method is similar to prior work on typing relational phrases (Nakashole et al., 2012). Examples of typed phrases are: (person) died in (year), (person) died in (location), and (athlete) plays for (team). Given a triple, we look up the types for the subject and the object and then determine which of the typed phrases are compatible with the current triple. We look up entity types in a knowledge 1http://www.w3.org/TR/rdf-primer/ 1010 base containing entities and their types. In particular, we use the NELL entity typing API (Carlson et al., 2010). NELL’s entity typing method has high recall because when entities are not in the knowledge base, it performs on-the-fly type inference using the Web. This is not the case for other options such as (Auer et al., 2007; Bollacker et al., 2008; Hoffart et al., 2011). Relation Cardinality. Next, we learn cardinalities of verbal phrases. Cardinality refers to how arguments of a given relation relate to one another numerically. We define the relation cardinality of a verb Card(V ), as the average number of expected arguments per given subject. For example, for the relation “died in”, 1 location is </context>
<context position="23518" citStr="Carlson et al., 2010" startWordPosition="3817" endWordPosition="3820">f rankings ∀fi ∈ F L = ∅ while F =6 ∅ do pick fi from F A= getAlternatives(fi,K,C,W) PriorityQueue Li = ∅ for all alternative fact candidates fj&apos; ∈ A do 0(f j) = getBelievabilityScore(fj) Li.insert(fj, 0(f j)) end for 0(fi) = getBelievabilityScore(fi) Li.insert(fi, 0(fi)) L ∪ Li Remove fi from F end while return L Acc = E(r(fi)=T:r(fj)=F)(0(fi) &gt; 0(fj)) |{∀(fi,fj) : τ(fi) = T ∧ τ(fj) = F}| Datasets. We evaluated FactChecker on three datasets: i) KB Fact Candidates: The first dataset consists of fact candidates taken from the fact extraction pipeline of a state-of-the-art knowledge base, NELL (Carlson et al., 2010). The fact candidates span four different relation types: company acquisitions, book authors, movie directors and athlete teams. For each fact candidate, we applied our alternative candidate generation method. We only considered fact candidates with non-trivial alternative candidate sets; where the alternative candidate set is greater than zero. Since all of the baselines we compared against assume alternatives are provided, we apply all methods to the same set of alternative fact candidates discovered by our method. Details of this dataset are shown as rows starting with “KB-” in Table 3. ii)</context>
<context position="33304" citStr="Carlson et al., 2010" startWordPosition="5341" endWordPosition="5344">ess of wiki authors based on the revision history of the articles they have authored. Motivated by interpretability of probabilistic scores, two recent papers addressed the truth-finding problem as a probabilistic inference problem over the sources and the fact candidates (Zhao et al., 2012; Pasternack and Roth, 2013). Truth-finders based on textual entailment such as TruthTeller (Lotan et al., 2013) determine if a sentence states something or not. The focus is on understanding natural language, including the use of negation. This is similar to the goal of fact extraction (Banko et al., 2007; Carlson et al., 2010; Fader et al., 2011; Nakashole et al., 2011; Del Corro and Gemulla, 2013). In a departure from prior work, our method leverages language of sources in its believability computation model. Furthermore, we introduced a co-mention score which is designed to avoid potential false boots among fact candidates. Additionally, we developed a method for generating alternative fact candidates. Prior methods assume these are readily available. Only (Li et al., 2011) uses the Web to identify alternatives, however, this is only done after manually specifying the fixed argument. In contrast, we introduced a</context>
</contexts>
<marker>Carlson, Betteridge, Wang, Hruschka, Mitchell, 2010</marker>
<rawString>A. Carlson, J. Betteridge, R.C. Wang, E.R. Hruschka, T.M. Mitchell: Coupled Semi-supervised Learning for Information Extraction. In Proceedings of the Third International Conference on Web Search and Web Data Mining (WSDM), pages 101–110, New York, NY, USA, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Del Corro</author>
<author>R Gemulla</author>
</authors>
<title>ClausIE: clause-based open information extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd International Conference on World Wide Web (WWW),</booktitle>
<pages>355--366</pages>
<contexts>
<context position="2138" citStr="Corro and Gemulla, 2013" startWordPosition="306" endWordPosition="309">c standards. Consequently, the accuracy of information reported by these sources is often unclear. Even more established newspapers and websites may sometimes report false information as they race to break stories. Therefore, truth-finding is becoming an inTom M. Mitchell Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA, 15213 tom.mitchell@cs.cmu.edu creasingly important problem. Information extraction projects aim to distill relational facts from natural language text (Auer et al., 2007; Bollacker et al., 2008; Carlson et al., 2010; Fader et al., 2011; Nakashole et al., 2011; Del Corro and Gemulla, 2013). These projects have produced knowledge bases containing many millions of relational facts between entities. However, despite these impressive advances, there are still major limitations regarding precision. Within the context of information extraction, fact extractors assign confidence scores to extracted facts. However, such scores are often tied to the extractor’s ability to read and understand natural language text. This is different from a score that indicates the degree to which a given fact candidate is believable. Such a believability score is sometimes also referred to as a credibili</context>
<context position="33378" citStr="Corro and Gemulla, 2013" startWordPosition="5354" endWordPosition="5357"> have authored. Motivated by interpretability of probabilistic scores, two recent papers addressed the truth-finding problem as a probabilistic inference problem over the sources and the fact candidates (Zhao et al., 2012; Pasternack and Roth, 2013). Truth-finders based on textual entailment such as TruthTeller (Lotan et al., 2013) determine if a sentence states something or not. The focus is on understanding natural language, including the use of negation. This is similar to the goal of fact extraction (Banko et al., 2007; Carlson et al., 2010; Fader et al., 2011; Nakashole et al., 2011; Del Corro and Gemulla, 2013). In a departure from prior work, our method leverages language of sources in its believability computation model. Furthermore, we introduced a co-mention score which is designed to avoid potential false boots among fact candidates. Additionally, we developed a method for generating alternative fact candidates. Prior methods assume these are readily available. Only (Li et al., 2011) uses the Web to identify alternatives, however, this is only done after manually specifying the fixed argument. In contrast, we introduced a method for identifying the fixed argument based on relation cardinalities</context>
</contexts>
<marker>Corro, Gemulla, 2013</marker>
<rawString>L. Del Corro, R. Gemulla: ClausIE: clause-based open information extraction. In Proceedings of the 22nd International Conference on World Wide Web (WWW), pages 355-366. 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Dong</author>
<author>L Berti-Equille</author>
<author>D Srivastava</author>
</authors>
<title>Truth discovery and copying detection in a dynamic world.</title>
<date>2009</date>
<booktitle>In Proceedings of the VLDB Endowment PVLDB,</booktitle>
<volume>2</volume>
<issue>1</issue>
<pages>562--573</pages>
<contexts>
<context position="31967" citStr="Dong et al., 2009" startWordPosition="5132" endWordPosition="5135">zuela* KB (Dirk Kuyt) plays for (Liverpool) 1. Liverpool 2.Cardiff City* 3.Netherlands 4.Hungary* Quiz (Bifocals) invented by (Benjamin Franklin) 1. Benjamin Franklin 2. Rudolf Diesel* 3.Thomas Alva Edison* 4.Alfred B. Nobel* Table 5: Sample rankings by FactChecker, alternatives marked (*) are false. The ranking of the candidate from the “KB” dataset is not completely accurate. ness of their sources. This process is repeated over several iterations until convergence. (Yin et al., 2007) was the first to implement this idea, subsequent work improved upon iterative voting in several directions. (Dong et al., 2009) incorporates copying-detection; giving high trust to sources that are independently authored. (Galland et al., 2010) approximates error rates of sources and fact candidates. (Pasternack and Roth, 2010) introduces prior knowledge in the form of linear programming constraints in order to ensure that the truth discovered is consistent with what is already known. (Yin and Tan, 2011) introduces supervision by using ground truth facts so that sources that disagree with the ground truth are penalized. (Li et al., 2011) uses search engine APIs to gather additional evidence for believability of fact c</context>
</contexts>
<marker>Dong, Berti-Equille, Srivastava, 2009</marker>
<rawString>X. Dong, L. Berti-Equille, D. Srivastava: Truth discovery and copying detection in a dynamic world. In Proceedings of the VLDB Endowment PVLDB, 2(1), pp. 562-573, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fader</author>
<author>S Soderland</author>
<author>O</author>
</authors>
<title>Etzioni: Identifying Relations for Open Information Extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1535--1545</pages>
<location>Edinburgh, UK,</location>
<contexts>
<context position="2084" citStr="Fader et al., 2011" startWordPosition="297" endWordPosition="300">sites are not subject to traditional journalistic standards. Consequently, the accuracy of information reported by these sources is often unclear. Even more established newspapers and websites may sometimes report false information as they race to break stories. Therefore, truth-finding is becoming an inTom M. Mitchell Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA, 15213 tom.mitchell@cs.cmu.edu creasingly important problem. Information extraction projects aim to distill relational facts from natural language text (Auer et al., 2007; Bollacker et al., 2008; Carlson et al., 2010; Fader et al., 2011; Nakashole et al., 2011; Del Corro and Gemulla, 2013). These projects have produced knowledge bases containing many millions of relational facts between entities. However, despite these impressive advances, there are still major limitations regarding precision. Within the context of information extraction, fact extractors assign confidence scores to extracted facts. However, such scores are often tied to the extractor’s ability to read and understand natural language text. This is different from a score that indicates the degree to which a given fact candidate is believable. Such a believabil</context>
<context position="6546" citStr="Fader et al., 2011" startWordPosition="972" endWordPosition="975">e formally define what constitutes a fact candidate and describe how we go about understanding semantics of fact candidates. We then present our approach for generating alternative fact candidates. 2.1 Representation The triple format is the most common representation of facts in knowledge bases. A formal specification of the triple format is presented in the RDF primer1. In RDF, data is represented as subjectpredicate-object (SPO) triples. In this work, we restrict predicates to verbs (or verbal phrases such as “plays for”, “graduated from”, etc.). Literature on automatic relation discovery (Fader et al., 2011) has shown that verbal phrases uncover a large fraction of binary predicates while reducing the amount of noisy phrases that do not denote any relations. Therefore, we define a fact candidate as follows: Definition 1(Fact Candidate) A fact candidate fz is an (S) V (O) triple; where S is the subject, V is a verbal phrase, and O is the object. We aim to compute the truthfulness of fz, τ(fz) E IT, F}, where T and F stand for true and false, respectively. Note that in this paper we are interested in cases where τ(fz) is either T or F. That is, we assess truthfulness offactual statements and not op</context>
<context position="33324" citStr="Fader et al., 2011" startWordPosition="5345" endWordPosition="5348">sed on the revision history of the articles they have authored. Motivated by interpretability of probabilistic scores, two recent papers addressed the truth-finding problem as a probabilistic inference problem over the sources and the fact candidates (Zhao et al., 2012; Pasternack and Roth, 2013). Truth-finders based on textual entailment such as TruthTeller (Lotan et al., 2013) determine if a sentence states something or not. The focus is on understanding natural language, including the use of negation. This is similar to the goal of fact extraction (Banko et al., 2007; Carlson et al., 2010; Fader et al., 2011; Nakashole et al., 2011; Del Corro and Gemulla, 2013). In a departure from prior work, our method leverages language of sources in its believability computation model. Furthermore, we introduced a co-mention score which is designed to avoid potential false boots among fact candidates. Additionally, we developed a method for generating alternative fact candidates. Prior methods assume these are readily available. Only (Li et al., 2011) uses the Web to identify alternatives, however, this is only done after manually specifying the fixed argument. In contrast, we introduced a method for identify</context>
</contexts>
<marker>Fader, Soderland, O, 2011</marker>
<rawString>A. Fader, S. Soderland, O. Etzioni: Identifying Relations for Open Information Extraction. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1535–1545, Edinburgh, UK, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Galland</author>
<author>S Abiteboul</author>
<author>A Marian</author>
<author>P</author>
</authors>
<title>Senellart: Corroborating information from disagreeing views.</title>
<date>2010</date>
<booktitle>In Proceedings of the 3rd International Conference on Web Search and Web Data Mining (WSDM),</booktitle>
<pages>131--140</pages>
<contexts>
<context position="3115" citStr="Galland et al., 2010" startWordPosition="457" endWordPosition="460">e extractor’s ability to read and understand natural language text. This is different from a score that indicates the degree to which a given fact candidate is believable. Such a believability score is sometimes also referred to as a credibility score or truthfulness score. The believability score reflects the likelihood that a given statement is true. Truth-finding algorithms aim to compute this score for each fact candidate. Prior truth-finding methods are mostly based on iterative voting, where votes are propagated from sources to fact candidates and then back to sources (Yin et al., 2007; Galland et al., 2010; Pasternack and Roth, 2010; Li et al., 2011; Yin and Tan, 2011). At the core of iterative voting is the assumption that candidates mentioned by many sources are more likely to be true. However, additional aspects of a source influence its trustworthiness, besides external votes. Our goal is to accurately assess truthfulness of fact candidates by taking into account the language of sources that mention them. A Mechanical Turk study we carried out revealed that there is a significant correlation between objectivity of language and trustworthiness of sources. Objectivity of language refers to th</context>
<context position="26253" citStr="Galland et al., 2010" startWordPosition="4256" endWordPosition="4259">al., 2007). iii) Investment is also based on transitive voting, however scores are updated differently. A source gets a vote of trust from each candidate it “invests” in, but the vote is weighted by the proportion of trust the source previously “invested” in the candidate relative to other investors. Implemented as described in (Pasternack and Roth, 2010). iv) PooledInvest is a variation of investment, we report both because in their paper, there was no clear winner among the two variations. v) 2-Estimates is a probabilistic model which approximates error rates of sources and fact candidates (Galland et al., 2010). 5.1 Accuracy on KB Fact Candidates Figure 2 shows accuracy on KB fact candidates. FactChecker achieves accuracy between 70% and 88% and is significantly more accurate than the 4http://www.indiabix.com/general-knowledge/ Figure 2: Accuracy of KB fact candidates. Figure 3: FactChecker variations. other approaches on all relations except company acquisitions. On book authors, movie directors, and athlete teams, FactChecker outperforms all other approaches by at least 10%, 9%, and 8% respectively. On company acquisitions, the different methods achieve similar accuracy, with TruthFinder being the</context>
<context position="30697" citStr="Galland et al., 2010" startWordPosition="4940" endWordPosition="4943">e of iterative voting is still the assumption that fact candidates mentioned in many sources are more likely to be true. Therefore, for both majority vote and iterative voting, when mention frequencies of various alternatives are the same, accuracy suffers. Based on these observations, it is clear that truthfinding solutions need to incorporate fine-grained content-aware features outside of external votes. FactChecker takes a step in this direction by incorporating the document-level feature of objectivity. 6 Related Work There is a fairly small body of work on truthfinding (Yin et al., 2007; Galland et al., 2010; Pasternack and Roth, 2010; Li et al., 2011; Yin and Tan, 2011; Zhao et al., 2012; Pasternack and Roth, 2013). The method underlying most truth-finding algorithms is iterative transitive voting (Yin et al., 2007; Galland et al., 2010; Pasternack and Roth, 2010; Li et al., 2011). Fact candidates are initialized with a score. Trustworthiness of sources is then computed from the believability of the fact candidates they mention. In return, believability of candidates is recomputed based on the trustworthi1016 Dataset Fact Candidate Alternatives &amp; Ranking WKP (George W. Bush) lived in (Midland,TX</context>
<context position="32084" citStr="Galland et al., 2010" startWordPosition="5147" endWordPosition="5150">invented by (Benjamin Franklin) 1. Benjamin Franklin 2. Rudolf Diesel* 3.Thomas Alva Edison* 4.Alfred B. Nobel* Table 5: Sample rankings by FactChecker, alternatives marked (*) are false. The ranking of the candidate from the “KB” dataset is not completely accurate. ness of their sources. This process is repeated over several iterations until convergence. (Yin et al., 2007) was the first to implement this idea, subsequent work improved upon iterative voting in several directions. (Dong et al., 2009) incorporates copying-detection; giving high trust to sources that are independently authored. (Galland et al., 2010) approximates error rates of sources and fact candidates. (Pasternack and Roth, 2010) introduces prior knowledge in the form of linear programming constraints in order to ensure that the truth discovered is consistent with what is already known. (Yin and Tan, 2011) introduces supervision by using ground truth facts so that sources that disagree with the ground truth are penalized. (Li et al., 2011) uses search engine APIs to gather additional evidence for believability of fact candidates. WikiTrust (Adler and Alfaro, 2007) is a content-aware but domain-specific method. It computes trustworthin</context>
</contexts>
<marker>Galland, Abiteboul, Marian, P, 2010</marker>
<rawString>A. Galland, S. Abiteboul, A. Marian, P. Senellart: Corroborating information from disagreeing views. In Proceedings of the 3rd International Conference on Web Search and Web Data Mining (WSDM), pages 131-140, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Havasi</author>
<author>R Speer</author>
<author>J Alonso</author>
</authors>
<title>ConceptNet 3: a Flexible, Multilingual Semantic Network for Common Sense Knowledge.</title>
<date>2007</date>
<booktitle>In Proceedings of the Recent Advances in Natural Language Processing (RANLP), Borovets,</booktitle>
<location>Bulgaria,</location>
<marker>Havasi, Speer, Alonso, 2007</marker>
<rawString>C. Havasi, R. Speer, J. Alonso: ConceptNet 3: a Flexible, Multilingual Semantic Network for Common Sense Knowledge. In Proceedings of the Recent Advances in Natural Language Processing (RANLP), Borovets, Bulgaria, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hoffart</author>
<author>F Suchanek</author>
<author>K Berberich</author>
<author>E LewisKelham</author>
<author>G de Melo</author>
<author>G</author>
</authors>
<title>Weikum: YAGO2: Exploring and Querying World Knowledge in Time, Space, Context, and Many Languages.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th International Conference on World Wide Web (WWW),</booktitle>
<pages>229--232</pages>
<location>Hyderabad,</location>
<marker>Hoffart, Suchanek, Berberich, LewisKelham, de Melo, G, 2011</marker>
<rawString>J. Hoffart, F. Suchanek, K. Berberich, E. LewisKelham, G. de Melo, G. Weikum: YAGO2: Exploring and Querying World Knowledge in Time, Space, Context, and Many Languages. In Proceedings of the 20th International Conference on World Wide Web (WWW), pages 229–232, Hyderabad, India. 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kaplan</author>
</authors>
<title>Politics and the American Press: The Rise of Objectivity,</title>
<date>2002</date>
<pages>1865--1920</pages>
<publisher>University Press,</publisher>
<location>New York, Cambridge</location>
<contexts>
<context position="12642" citStr="Kaplan, 2002" startWordPosition="2015" endWordPosition="2017">ts. Therefore, we apply a triple extractor to the retrieved documents. For all potential alternative triples, we perform type checking to ensure that the arguments of the triples are type-compatible with fi. Furthermore, we generate an additional query for every synonymous verb sVi, replacing V with sVi. Example queries are: “Einstein died in”, “Einstein passed in”, etc. 3 Objectivity and Trustworthiness The principle of objective journalism, which is a significant part of journalistic ethics, aims to promote factual and fair reporting, undistorted by emotion or personal bias (Schudson, 1978; Kaplan, 2002). Objectivity is also required in reference sources such as encyclopedias, scientific publications, and textbooks. For example, Wikipedia enforces a neutral point-of-view policy (NPOV)2. Articles violating the NPOV policy are marked 2http://en.wikipedia.org/wiki/Wikipedia:Neutral point of view 1011 to indicate potential bias. While opinions, emotions, and speculations can also be expressed using objective language, they are often stated using subjective language (Turney et al., 2002; Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Wiebe et al., 2004; Liu et al., 2005; Recasens et al., 2</context>
</contexts>
<marker>Kaplan, 2002</marker>
<rawString>R. Kaplan: Politics and the American Press: The Rise of Objectivity, pages 1865-1920, New York, Cambridge University Press, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>W Meng</author>
<author>C T Yu</author>
</authors>
<title>T-verifier: Verifying truthfulness of fact statements.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Conference on Data Engineering (ICDE),</booktitle>
<pages>63--74</pages>
<contexts>
<context position="3159" citStr="Li et al., 2011" startWordPosition="466" endWordPosition="469">ral language text. This is different from a score that indicates the degree to which a given fact candidate is believable. Such a believability score is sometimes also referred to as a credibility score or truthfulness score. The believability score reflects the likelihood that a given statement is true. Truth-finding algorithms aim to compute this score for each fact candidate. Prior truth-finding methods are mostly based on iterative voting, where votes are propagated from sources to fact candidates and then back to sources (Yin et al., 2007; Galland et al., 2010; Pasternack and Roth, 2010; Li et al., 2011; Yin and Tan, 2011). At the core of iterative voting is the assumption that candidates mentioned by many sources are more likely to be true. However, additional aspects of a source influence its trustworthiness, besides external votes. Our goal is to accurately assess truthfulness of fact candidates by taking into account the language of sources that mention them. A Mechanical Turk study we carried out revealed that there is a significant correlation between objectivity of language and trustworthiness of sources. Objectivity of language refers to the use of neutral, impartial language, which </context>
<context position="30741" citStr="Li et al., 2011" startWordPosition="4948" endWordPosition="4951">t fact candidates mentioned in many sources are more likely to be true. Therefore, for both majority vote and iterative voting, when mention frequencies of various alternatives are the same, accuracy suffers. Based on these observations, it is clear that truthfinding solutions need to incorporate fine-grained content-aware features outside of external votes. FactChecker takes a step in this direction by incorporating the document-level feature of objectivity. 6 Related Work There is a fairly small body of work on truthfinding (Yin et al., 2007; Galland et al., 2010; Pasternack and Roth, 2010; Li et al., 2011; Yin and Tan, 2011; Zhao et al., 2012; Pasternack and Roth, 2013). The method underlying most truth-finding algorithms is iterative transitive voting (Yin et al., 2007; Galland et al., 2010; Pasternack and Roth, 2010; Li et al., 2011). Fact candidates are initialized with a score. Trustworthiness of sources is then computed from the believability of the fact candidates they mention. In return, believability of candidates is recomputed based on the trustworthi1016 Dataset Fact Candidate Alternatives &amp; Ranking WKP (George W. Bush) lived in (Midland,TX) 1.Midland,TX 2.Compton,CA 3.Washington D.C</context>
<context position="32485" citStr="Li et al., 2011" startWordPosition="5213" endWordPosition="5216">is idea, subsequent work improved upon iterative voting in several directions. (Dong et al., 2009) incorporates copying-detection; giving high trust to sources that are independently authored. (Galland et al., 2010) approximates error rates of sources and fact candidates. (Pasternack and Roth, 2010) introduces prior knowledge in the form of linear programming constraints in order to ensure that the truth discovered is consistent with what is already known. (Yin and Tan, 2011) introduces supervision by using ground truth facts so that sources that disagree with the ground truth are penalized. (Li et al., 2011) uses search engine APIs to gather additional evidence for believability of fact candidates. WikiTrust (Adler and Alfaro, 2007) is a content-aware but domain-specific method. It computes trustworthiness of wiki authors based on the revision history of the articles they have authored. Motivated by interpretability of probabilistic scores, two recent papers addressed the truth-finding problem as a probabilistic inference problem over the sources and the fact candidates (Zhao et al., 2012; Pasternack and Roth, 2013). Truth-finders based on textual entailment such as TruthTeller (Lotan et al., 201</context>
<context position="33763" citStr="Li et al., 2011" startWordPosition="5413" endWordPosition="5416">is on understanding natural language, including the use of negation. This is similar to the goal of fact extraction (Banko et al., 2007; Carlson et al., 2010; Fader et al., 2011; Nakashole et al., 2011; Del Corro and Gemulla, 2013). In a departure from prior work, our method leverages language of sources in its believability computation model. Furthermore, we introduced a co-mention score which is designed to avoid potential false boots among fact candidates. Additionally, we developed a method for generating alternative fact candidates. Prior methods assume these are readily available. Only (Li et al., 2011) uses the Web to identify alternatives, however, this is only done after manually specifying the fixed argument. In contrast, we introduced a method for identifying the fixed argument based on relation cardinalities learned from SVO statistics. 7 Conclusion In this paper, we presented FactChecker, a language-aware approach to truth-finding. In contrast to prior approaches, which rely on external votes, FactChecker includes objectivity of sources in its believability computation model. FactChecker can be seen as a first step towards language-aware truth-finding. Future directions include using </context>
</contexts>
<marker>Li, Meng, Yu, 2011</marker>
<rawString>X. Li and W. Meng, C. T. Yu: T-verifier: Verifying truthfulness of fact statements. In Proceedings of the International Conference on Data Engineering (ICDE), pp. 63-74, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>P</author>
</authors>
<title>Pantel: DIRT: discovery of inference rules from text.</title>
<date>2001</date>
<publisher>KDD</publisher>
<marker>Lin, P, 2001</marker>
<rawString>D. Lin, P. Pantel: DIRT: discovery of inference rules from text. KDD 2001</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Liu</author>
<author>M Hu</author>
<author>J Cheng</author>
</authors>
<title>Opinion Observer: analyzing and comparing opinions on the Web.</title>
<date>2005</date>
<booktitle>InProceedings of the 14th International Conference on World Wide Web (WWW),</booktitle>
<pages>342351</pages>
<contexts>
<context position="13222" citStr="Liu et al., 2005" startWordPosition="2096" endWordPosition="2099"> bias (Schudson, 1978; Kaplan, 2002). Objectivity is also required in reference sources such as encyclopedias, scientific publications, and textbooks. For example, Wikipedia enforces a neutral point-of-view policy (NPOV)2. Articles violating the NPOV policy are marked 2http://en.wikipedia.org/wiki/Wikipedia:Neutral point of view 1011 to indicate potential bias. While opinions, emotions, and speculations can also be expressed using objective language, they are often stated using subjective language (Turney et al., 2002; Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Wiebe et al., 2004; Liu et al., 2005; Recasens et al., 2013). For example, consider the following pieces of text: (S) Well, I think Obama was born in Kenya because his grandma who lives in Kenya said he was born there. (O) Theories allege that Obama’s published birth certificate is a forgery, that his actual birthplace is not Hawaii but Kenya. Text S is a snippet from Yahoo Answers and text O is a snippet from the Wikipedia page titled: “Barack Obama Citizenship Conspiracy Theories”. S is subjective, expressing the opinion of the author. On the other hand, O is objective, stating only what has been alleged. Literature on sentime</context>
<context position="17151" citStr="Liu et al., 2005" startWordPosition="2741" endWordPosition="2744">jectivity of sources. 3.2 Automatic Objectivity Detection We trained a logistic regression classifier to predict the objectivity of a document. For training and testing data, we used the labeled data from the Mechanical Turk study. We additionally used labeled text from prior work on subjectivity detection (Pang and Lee, 2004). This resulted in a total of 4,600 documents, half subjective and the other half objective. We used 4000 documents for 1012 # Feature 1 Subjectivity lexicon of strong and weak subjective words (Riloff and Wiebe, 2003). 2 Sentiment lexicon of positive and negative words (Liu et al., 2005). 3 Wikipedia-derived bias lexicon (Recasens et al., 2013). 4 Part-of-speech (POS) tags 5 Frequent bi-grams Table 1: Features used for the objectivity detector. training, 2000 per label. The rest of the documents were split into a development set (380) and a test set (220). A summary of the features we used is shown in Table 1. Features 1-3 refer to lexicons developed by prior methods on subjectivity (Wiebe et al., 2004), sentiment analysis (Liu et al., 2005) and bias detection (Recasens et al., 2013). Feature 4 refers to part-of-speech tags of the terms found in the document that are also in </context>
</contexts>
<marker>Liu, Hu, Cheng, 2005</marker>
<rawString>B. Liu, M. Hu, J. Cheng: Opinion Observer: analyzing and comparing opinions on the Web. InProceedings of the 14th International Conference on World Wide Web (WWW), pages 342351, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lotan</author>
<author>A Stern</author>
<author>I</author>
</authors>
<title>Dagan TruthTeller: Annotating Predicate Truth.</title>
<date>2013</date>
<booktitle>In Proceedings ofHuman Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL),</booktitle>
<pages>752--757</pages>
<contexts>
<context position="33087" citStr="Lotan et al., 2013" startWordPosition="5301" endWordPosition="5304">(Li et al., 2011) uses search engine APIs to gather additional evidence for believability of fact candidates. WikiTrust (Adler and Alfaro, 2007) is a content-aware but domain-specific method. It computes trustworthiness of wiki authors based on the revision history of the articles they have authored. Motivated by interpretability of probabilistic scores, two recent papers addressed the truth-finding problem as a probabilistic inference problem over the sources and the fact candidates (Zhao et al., 2012; Pasternack and Roth, 2013). Truth-finders based on textual entailment such as TruthTeller (Lotan et al., 2013) determine if a sentence states something or not. The focus is on understanding natural language, including the use of negation. This is similar to the goal of fact extraction (Banko et al., 2007; Carlson et al., 2010; Fader et al., 2011; Nakashole et al., 2011; Del Corro and Gemulla, 2013). In a departure from prior work, our method leverages language of sources in its believability computation model. Furthermore, we introduced a co-mention score which is designed to avoid potential false boots among fact candidates. Additionally, we developed a method for generating alternative fact candidat</context>
</contexts>
<marker>Lotan, Stern, I, 2013</marker>
<rawString>A. Lotan, A. Stern, I. Dagan TruthTeller: Annotating Predicate Truth. In Proceedings ofHuman Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL), pp. 752-757, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Nakashole</author>
<author>M Theobald</author>
<author>G Weikum</author>
</authors>
<title>Scalable Knowledge Harvesting with High Precision and High Recall.</title>
<date>2011</date>
<booktitle>In Proceedings of the 4th International Conference on Web Search and Web Data Mining (WSDM),</booktitle>
<pages>227--326</pages>
<location>Hong Kong, China,</location>
<contexts>
<context position="2108" citStr="Nakashole et al., 2011" startWordPosition="301" endWordPosition="304">t to traditional journalistic standards. Consequently, the accuracy of information reported by these sources is often unclear. Even more established newspapers and websites may sometimes report false information as they race to break stories. Therefore, truth-finding is becoming an inTom M. Mitchell Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA, 15213 tom.mitchell@cs.cmu.edu creasingly important problem. Information extraction projects aim to distill relational facts from natural language text (Auer et al., 2007; Bollacker et al., 2008; Carlson et al., 2010; Fader et al., 2011; Nakashole et al., 2011; Del Corro and Gemulla, 2013). These projects have produced knowledge bases containing many millions of relational facts between entities. However, despite these impressive advances, there are still major limitations regarding precision. Within the context of information extraction, fact extractors assign confidence scores to extracted facts. However, such scores are often tied to the extractor’s ability to read and understand natural language text. This is different from a score that indicates the degree to which a given fact candidate is believable. Such a believability score is sometimes a</context>
<context position="33348" citStr="Nakashole et al., 2011" startWordPosition="5349" endWordPosition="5352">history of the articles they have authored. Motivated by interpretability of probabilistic scores, two recent papers addressed the truth-finding problem as a probabilistic inference problem over the sources and the fact candidates (Zhao et al., 2012; Pasternack and Roth, 2013). Truth-finders based on textual entailment such as TruthTeller (Lotan et al., 2013) determine if a sentence states something or not. The focus is on understanding natural language, including the use of negation. This is similar to the goal of fact extraction (Banko et al., 2007; Carlson et al., 2010; Fader et al., 2011; Nakashole et al., 2011; Del Corro and Gemulla, 2013). In a departure from prior work, our method leverages language of sources in its believability computation model. Furthermore, we introduced a co-mention score which is designed to avoid potential false boots among fact candidates. Additionally, we developed a method for generating alternative fact candidates. Prior methods assume these are readily available. Only (Li et al., 2011) uses the Web to identify alternatives, however, this is only done after manually specifying the fixed argument. In contrast, we introduced a method for identifying the fixed argument b</context>
</contexts>
<marker>Nakashole, Theobald, Weikum, 2011</marker>
<rawString>N. Nakashole, M. Theobald, G. Weikum: Scalable Knowledge Harvesting with High Precision and High Recall. In Proceedings of the 4th International Conference on Web Search and Web Data Mining (WSDM), pages 227–326, Hong Kong, China, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Nakashole</author>
<author>T Tylenda</author>
<author>G Weikum</author>
</authors>
<title>Fine-grained Semantic Typing of Emerging Entities.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1488--1497</pages>
<marker>Nakashole, Tylenda, Weikum, 2013</marker>
<rawString>N. Nakashole, T. Tylenda, G. Weikum: Fine-grained Semantic Typing of Emerging Entities. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL), pp. 1488-1497, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Nakashole</author>
<author>G Weikum</author>
<author>F Suchanek</author>
</authors>
<title>PATTY: A Taxonomy of Relational Patterns with Semantic Types.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>1135--1145</pages>
<location>Jeju, South</location>
<contexts>
<context position="8106" citStr="Nakashole et al., 2012" startWordPosition="1246" endWordPosition="1249">iguous. Therefore, we first determine the semantics of a fact candidate before computing its truthfulness. Entity Types. We first determine the expected types of the subject and object in the SVO. For example, for the SVO (Einstein) died in (Princeton), the expected types are person x location. We determine this by first computing the types of entities that are valid for each verb (verbal phrase) in a large SVO collection of 114m SVO triples (Talukdar et al., 2012). Typing verbal phrases is a once-off computation. Our phrase typing method is similar to prior work on typing relational phrases (Nakashole et al., 2012). Examples of typed phrases are: (person) died in (year), (person) died in (location), and (athlete) plays for (team). Given a triple, we look up the types for the subject and the object and then determine which of the typed phrases are compatible with the current triple. We look up entity types in a knowledge 1http://www.w3.org/TR/rdf-primer/ 1010 base containing entities and their types. In particular, we use the NELL entity typing API (Carlson et al., 2010). NELL’s entity typing method has high recall because when entities are not in the knowledge base, it performs on-the-fly type inference</context>
<context position="10026" citStr="Nakashole et al., 2012" startWordPosition="1567" endWordPosition="1570">s per given first argument. In a once-off computation, we generate cardinality approximations per typed verbal phrase V and its inverse V −1. For example, we generate the cardinality estimates for both: (person) died in (location) and for (location) INVERSE-OF(died in) (person). Synonymous Relations. Natural language is diverse. Semantically similar phrases can be syntactically different. Therefore, we learn other verbs that can be used to substitute V in SVO. We pre-compute synonymous phrases from the 114m SVO corpus using distributional semantics in the same spirit as (Lin and Pantel, 2001; Nakashole et al., 2012). Synonymous verbs, relation cardinalities, and entity types enable us to generate alternative fact candidates. 2.3 Alternative Fact Candidates Truth-finding methods rank fi relative to alternative candidates. While prior methods assume the alternatives are known apriori, we developed a method for generating alternative fact candidates. For a given fi, we first identify the fixed argument. The fixed argument is the argument of the SVO which when fixed, requires finding the fewest number of alternative candidates. For example, for (Einstein) died in (Princeton), the solution is to fix the subje</context>
</contexts>
<marker>Nakashole, Weikum, Suchanek, 2012</marker>
<rawString>N. Nakashole, G. Weikum, F. Suchanek: PATTY: A Taxonomy of Relational Patterns with Semantic Types. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 1135 -1145, Jeju, South Korea, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Nastase</author>
<author>M Strube</author>
<author>B Boerschinger</author>
<author>C Zirn</author>
<author>A</author>
</authors>
<title>Elghafari: WikiNet: A Very Large Scale MultiLingual Concept Network.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th International Conference on Language Resources and Evaluation(LREC),</booktitle>
<marker>Nastase, Strube, Boerschinger, Zirn, A, 2010</marker>
<rawString>V. Nastase, M. Strube, B. Boerschinger, C. Zirn, A. Elghafari: WikiNet: A Very Large Scale MultiLingual Concept Network. In Proceedings of the 7th International Conference on Language Resources and Evaluation(LREC), Malta, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>271--278</pages>
<contexts>
<context position="16862" citStr="Pang and Lee, 2004" startWordPosition="2693" endWordPosition="2696">orrelation in our believability computation model. To incorporate objectivity in FactChecker, we require for a given source document, an objectivity score ∈ [0, 1], where 0 means the source is subjective and 1 means it is objective. Next, describe our method for automatically determining objectivity of sources. 3.2 Automatic Objectivity Detection We trained a logistic regression classifier to predict the objectivity of a document. For training and testing data, we used the labeled data from the Mechanical Turk study. We additionally used labeled text from prior work on subjectivity detection (Pang and Lee, 2004). This resulted in a total of 4,600 documents, half subjective and the other half objective. We used 4000 documents for 1012 # Feature 1 Subjectivity lexicon of strong and weak subjective words (Riloff and Wiebe, 2003). 2 Sentiment lexicon of positive and negative words (Liu et al., 2005). 3 Wikipedia-derived bias lexicon (Recasens et al., 2013). 4 Part-of-speech (POS) tags 5 Frequent bi-grams Table 1: Features used for the objectivity detector. training, 2000 per label. The rest of the documents were split into a development set (380) and a test set (220). A summary of the features we used is</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>B. Pang, L. Lee: A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL), 271-278, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pasternack</author>
<author>D Roth</author>
</authors>
<title>Knowing What to Believe.</title>
<date>2010</date>
<booktitle>In Proceedings the International Conference on Computational Linguistics (COLING),</booktitle>
<pages>877--885</pages>
<location>Beijing,</location>
<contexts>
<context position="3142" citStr="Pasternack and Roth, 2010" startWordPosition="461" endWordPosition="465">to read and understand natural language text. This is different from a score that indicates the degree to which a given fact candidate is believable. Such a believability score is sometimes also referred to as a credibility score or truthfulness score. The believability score reflects the likelihood that a given statement is true. Truth-finding algorithms aim to compute this score for each fact candidate. Prior truth-finding methods are mostly based on iterative voting, where votes are propagated from sources to fact candidates and then back to sources (Yin et al., 2007; Galland et al., 2010; Pasternack and Roth, 2010; Li et al., 2011; Yin and Tan, 2011). At the core of iterative voting is the assumption that candidates mentioned by many sources are more likely to be true. However, additional aspects of a source influence its trustworthiness, besides external votes. Our goal is to accurately assess truthfulness of fact candidates by taking into account the language of sources that mention them. A Mechanical Turk study we carried out revealed that there is a significant correlation between objectivity of language and trustworthiness of sources. Objectivity of language refers to the use of neutral, impartial</context>
<context position="25989" citStr="Pasternack and Roth, 2010" startWordPosition="4212" endWordPosition="4215">ecker against five baselines: i) Vote counts the number of sources that mention the fact candidate. ii) TruthFinder is an iterative voting approach where votes are propagated from sources to fact candidates and then back to sources. Implemented as described in (Yin et al., 2007). iii) Investment is also based on transitive voting, however scores are updated differently. A source gets a vote of trust from each candidate it “invests” in, but the vote is weighted by the proportion of trust the source previously “invested” in the candidate relative to other investors. Implemented as described in (Pasternack and Roth, 2010). iv) PooledInvest is a variation of investment, we report both because in their paper, there was no clear winner among the two variations. v) 2-Estimates is a probabilistic model which approximates error rates of sources and fact candidates (Galland et al., 2010). 5.1 Accuracy on KB Fact Candidates Figure 2 shows accuracy on KB fact candidates. FactChecker achieves accuracy between 70% and 88% and is significantly more accurate than the 4http://www.indiabix.com/general-knowledge/ Figure 2: Accuracy of KB fact candidates. Figure 3: FactChecker variations. other approaches on all relations exce</context>
<context position="30724" citStr="Pasternack and Roth, 2010" startWordPosition="4944" endWordPosition="4947">is still the assumption that fact candidates mentioned in many sources are more likely to be true. Therefore, for both majority vote and iterative voting, when mention frequencies of various alternatives are the same, accuracy suffers. Based on these observations, it is clear that truthfinding solutions need to incorporate fine-grained content-aware features outside of external votes. FactChecker takes a step in this direction by incorporating the document-level feature of objectivity. 6 Related Work There is a fairly small body of work on truthfinding (Yin et al., 2007; Galland et al., 2010; Pasternack and Roth, 2010; Li et al., 2011; Yin and Tan, 2011; Zhao et al., 2012; Pasternack and Roth, 2013). The method underlying most truth-finding algorithms is iterative transitive voting (Yin et al., 2007; Galland et al., 2010; Pasternack and Roth, 2010; Li et al., 2011). Fact candidates are initialized with a score. Trustworthiness of sources is then computed from the believability of the fact candidates they mention. In return, believability of candidates is recomputed based on the trustworthi1016 Dataset Fact Candidate Alternatives &amp; Ranking WKP (George W. Bush) lived in (Midland,TX) 1.Midland,TX 2.Compton,CA</context>
<context position="32169" citStr="Pasternack and Roth, 2010" startWordPosition="5159" endWordPosition="5162">Alva Edison* 4.Alfred B. Nobel* Table 5: Sample rankings by FactChecker, alternatives marked (*) are false. The ranking of the candidate from the “KB” dataset is not completely accurate. ness of their sources. This process is repeated over several iterations until convergence. (Yin et al., 2007) was the first to implement this idea, subsequent work improved upon iterative voting in several directions. (Dong et al., 2009) incorporates copying-detection; giving high trust to sources that are independently authored. (Galland et al., 2010) approximates error rates of sources and fact candidates. (Pasternack and Roth, 2010) introduces prior knowledge in the form of linear programming constraints in order to ensure that the truth discovered is consistent with what is already known. (Yin and Tan, 2011) introduces supervision by using ground truth facts so that sources that disagree with the ground truth are penalized. (Li et al., 2011) uses search engine APIs to gather additional evidence for believability of fact candidates. WikiTrust (Adler and Alfaro, 2007) is a content-aware but domain-specific method. It computes trustworthiness of wiki authors based on the revision history of the articles they have authored.</context>
</contexts>
<marker>Pasternack, Roth, 2010</marker>
<rawString>J. Pasternack, D. Roth: Knowing What to Believe. In Proceedings the International Conference on Computational Linguistics (COLING), pp. 877-885, Beijing, China. 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pasternack</author>
<author>D Roth</author>
</authors>
<title>Latent credibility analysis.</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd International Conference on World Wide Web (WWW),</booktitle>
<pages>1009--1020</pages>
<contexts>
<context position="30807" citStr="Pasternack and Roth, 2013" startWordPosition="4960" endWordPosition="4963">kely to be true. Therefore, for both majority vote and iterative voting, when mention frequencies of various alternatives are the same, accuracy suffers. Based on these observations, it is clear that truthfinding solutions need to incorporate fine-grained content-aware features outside of external votes. FactChecker takes a step in this direction by incorporating the document-level feature of objectivity. 6 Related Work There is a fairly small body of work on truthfinding (Yin et al., 2007; Galland et al., 2010; Pasternack and Roth, 2010; Li et al., 2011; Yin and Tan, 2011; Zhao et al., 2012; Pasternack and Roth, 2013). The method underlying most truth-finding algorithms is iterative transitive voting (Yin et al., 2007; Galland et al., 2010; Pasternack and Roth, 2010; Li et al., 2011). Fact candidates are initialized with a score. Trustworthiness of sources is then computed from the believability of the fact candidates they mention. In return, believability of candidates is recomputed based on the trustworthi1016 Dataset Fact Candidate Alternatives &amp; Ranking WKP (George W. Bush) lived in (Midland,TX) 1.Midland,TX 2.Compton,CA 3.Washington D.C. 4.Venezuela* KB (Dirk Kuyt) plays for (Liverpool) 1. Liverpool 2</context>
<context position="33003" citStr="Pasternack and Roth, 2013" startWordPosition="5289" endWordPosition="5292">sing ground truth facts so that sources that disagree with the ground truth are penalized. (Li et al., 2011) uses search engine APIs to gather additional evidence for believability of fact candidates. WikiTrust (Adler and Alfaro, 2007) is a content-aware but domain-specific method. It computes trustworthiness of wiki authors based on the revision history of the articles they have authored. Motivated by interpretability of probabilistic scores, two recent papers addressed the truth-finding problem as a probabilistic inference problem over the sources and the fact candidates (Zhao et al., 2012; Pasternack and Roth, 2013). Truth-finders based on textual entailment such as TruthTeller (Lotan et al., 2013) determine if a sentence states something or not. The focus is on understanding natural language, including the use of negation. This is similar to the goal of fact extraction (Banko et al., 2007; Carlson et al., 2010; Fader et al., 2011; Nakashole et al., 2011; Del Corro and Gemulla, 2013). In a departure from prior work, our method leverages language of sources in its believability computation model. Furthermore, we introduced a co-mention score which is designed to avoid potential false boots among fact cand</context>
</contexts>
<marker>Pasternack, Roth, 2013</marker>
<rawString>J. Pasternack, D. Roth: Latent credibility analysis. In Proceedings of the 22nd International Conference on World Wide Web (WWW), pp. 1009-1020, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>J Wiebe</author>
</authors>
<title>Learning Learning extraction patterns for subjective expressions.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>105112</pages>
<marker>Riloff, Wiebe, 2013</marker>
<rawString>E. Riloff, J. Wiebe: Learning Learning extraction patterns for subjective expressions. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 105112, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Recasens</author>
<author>C Danescu-Niculescu-Mizil</author>
<author>D Jurafsky</author>
</authors>
<title>Linguistic Models for Analyzing and Detecting Biased Language.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1650--1659</pages>
<contexts>
<context position="13246" citStr="Recasens et al., 2013" startWordPosition="2100" endWordPosition="2103">978; Kaplan, 2002). Objectivity is also required in reference sources such as encyclopedias, scientific publications, and textbooks. For example, Wikipedia enforces a neutral point-of-view policy (NPOV)2. Articles violating the NPOV policy are marked 2http://en.wikipedia.org/wiki/Wikipedia:Neutral point of view 1011 to indicate potential bias. While opinions, emotions, and speculations can also be expressed using objective language, they are often stated using subjective language (Turney et al., 2002; Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Wiebe et al., 2004; Liu et al., 2005; Recasens et al., 2013). For example, consider the following pieces of text: (S) Well, I think Obama was born in Kenya because his grandma who lives in Kenya said he was born there. (O) Theories allege that Obama’s published birth certificate is a forgery, that his actual birthplace is not Hawaii but Kenya. Text S is a snippet from Yahoo Answers and text O is a snippet from the Wikipedia page titled: “Barack Obama Citizenship Conspiracy Theories”. S is subjective, expressing the opinion of the author. On the other hand, O is objective, stating only what has been alleged. Literature on sentiment analysis (Turney et a</context>
<context position="17209" citStr="Recasens et al., 2013" startWordPosition="2749" endWordPosition="2752">tion We trained a logistic regression classifier to predict the objectivity of a document. For training and testing data, we used the labeled data from the Mechanical Turk study. We additionally used labeled text from prior work on subjectivity detection (Pang and Lee, 2004). This resulted in a total of 4,600 documents, half subjective and the other half objective. We used 4000 documents for 1012 # Feature 1 Subjectivity lexicon of strong and weak subjective words (Riloff and Wiebe, 2003). 2 Sentiment lexicon of positive and negative words (Liu et al., 2005). 3 Wikipedia-derived bias lexicon (Recasens et al., 2013). 4 Part-of-speech (POS) tags 5 Frequent bi-grams Table 1: Features used for the objectivity detector. training, 2000 per label. The rest of the documents were split into a development set (380) and a test set (220). A summary of the features we used is shown in Table 1. Features 1-3 refer to lexicons developed by prior methods on subjectivity (Wiebe et al., 2004), sentiment analysis (Liu et al., 2005) and bias detection (Recasens et al., 2013). Feature 4 refers to part-of-speech tags of the terms found in the document that are also in the lexicons. Feature 5 refers to bi-grams that frequently</context>
</contexts>
<marker>Recasens, Danescu-Niculescu-Mizil, Jurafsky, 2013</marker>
<rawString>M. Recasens, C. Danescu-Niculescu-Mizil, D. Jurafsky: Linguistic Models for Analyzing and Detecting Biased Language. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL), pp. 1650-1659, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Niu</author>
<author>C Zhang</author>
<author>C Re</author>
<author>J W</author>
</authors>
<title>Shavlik: DeepDive: Web-scale Knowledge-base Construction using Statistical Learning and Inference.</title>
<date>2012</date>
<booktitle>In the VLDS Workshop,</booktitle>
<pages>25--28</pages>
<marker>Niu, Zhang, Re, W, 2012</marker>
<rawString>F. Niu, C. Zhang, C. Re, J. W. Shavlik: DeepDive: Web-scale Knowledge-base Construction using Statistical Learning and Inference. In the VLDS Workshop, pages 25-28, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Schudson</author>
</authors>
<title>Discovering the News: A Social History of American Newspapers.</title>
<date>1978</date>
<location>New York: Basic Books.</location>
<contexts>
<context position="12627" citStr="Schudson, 1978" startWordPosition="2013" endWordPosition="2014"> returns documents. Therefore, we apply a triple extractor to the retrieved documents. For all potential alternative triples, we perform type checking to ensure that the arguments of the triples are type-compatible with fi. Furthermore, we generate an additional query for every synonymous verb sVi, replacing V with sVi. Example queries are: “Einstein died in”, “Einstein passed in”, etc. 3 Objectivity and Trustworthiness The principle of objective journalism, which is a significant part of journalistic ethics, aims to promote factual and fair reporting, undistorted by emotion or personal bias (Schudson, 1978; Kaplan, 2002). Objectivity is also required in reference sources such as encyclopedias, scientific publications, and textbooks. For example, Wikipedia enforces a neutral point-of-view policy (NPOV)2. Articles violating the NPOV policy are marked 2http://en.wikipedia.org/wiki/Wikipedia:Neutral point of view 1011 to indicate potential bias. While opinions, emotions, and speculations can also be expressed using objective language, they are often stated using subjective language (Turney et al., 2002; Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Wiebe et al., 2004; Liu et al., 2005; Rec</context>
</contexts>
<marker>Schudson, 1978</marker>
<rawString>M. Schudson: Discovering the News: A Social History of American Newspapers. New York: Basic Books. 1978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M Suchanek</author>
<author>M Sozio</author>
<author>G Weikum</author>
</authors>
<title>SOFIE: A Self-organizing Framework for Information Extraction.</title>
<date>2009</date>
<booktitle>InProceedings of the 18th International Conference on World Wide Web (WWW),</booktitle>
<pages>631--640</pages>
<location>Madrid,</location>
<marker>Suchanek, Sozio, Weikum, 2009</marker>
<rawString>F. M. Suchanek, M. Sozio, G. Weikum: SOFIE: A Self-organizing Framework for Information Extraction. InProceedings of the 18th International Conference on World Wide Web (WWW), pages 631–640, Madrid, Spain, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P P Talukdar</author>
<author>D T Wijaya</author>
<author>T M Mitchell</author>
</authors>
<title>Acquiring temporal constraints between relations.</title>
<date>2012</date>
<booktitle>In Proceeding of the 21st ACM International Conference on Information and Knowledge Management,</booktitle>
<pages>992--1001</pages>
<contexts>
<context position="7952" citStr="Talukdar et al., 2012" startWordPosition="1221" endWordPosition="1224">r, the triple: (Obama) deserves (Nobel Peace Prize) is not. 2.2 Semantics Based on the SVO triple, the meaning of a fact candidate can be unclear and ambiguous. Therefore, we first determine the semantics of a fact candidate before computing its truthfulness. Entity Types. We first determine the expected types of the subject and object in the SVO. For example, for the SVO (Einstein) died in (Princeton), the expected types are person x location. We determine this by first computing the types of entities that are valid for each verb (verbal phrase) in a large SVO collection of 114m SVO triples (Talukdar et al., 2012). Typing verbal phrases is a once-off computation. Our phrase typing method is similar to prior work on typing relational phrases (Nakashole et al., 2012). Examples of typed phrases are: (person) died in (year), (person) died in (location), and (athlete) plays for (team). Given a triple, we look up the types for the subject and the object and then determine which of the typed phrases are compatible with the current triple. We look up entity types in a knowledge 1http://www.w3.org/TR/rdf-primer/ 1010 base containing entities and their types. In particular, we use the NELL entity typing API (Car</context>
</contexts>
<marker>Talukdar, Wijaya, Mitchell, 2012</marker>
<rawString>P. P. Talukdar, D. T. Wijaya, T.M. Mitchell: Acquiring temporal constraints between relations. In Proceeding of the 21st ACM International Conference on Information and Knowledge Management, pages 992-1001, CIKM 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>417424</pages>
<marker>Turney, 2002</marker>
<rawString>P. D. Turney: Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 417424. 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>T Wilson</author>
<author>R Bruce</author>
<author>M Bell</author>
<author>M Martin</author>
</authors>
<title>Learning subjective language.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="13204" citStr="Wiebe et al., 2004" startWordPosition="2092" endWordPosition="2095"> emotion or personal bias (Schudson, 1978; Kaplan, 2002). Objectivity is also required in reference sources such as encyclopedias, scientific publications, and textbooks. For example, Wikipedia enforces a neutral point-of-view policy (NPOV)2. Articles violating the NPOV policy are marked 2http://en.wikipedia.org/wiki/Wikipedia:Neutral point of view 1011 to indicate potential bias. While opinions, emotions, and speculations can also be expressed using objective language, they are often stated using subjective language (Turney et al., 2002; Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Wiebe et al., 2004; Liu et al., 2005; Recasens et al., 2013). For example, consider the following pieces of text: (S) Well, I think Obama was born in Kenya because his grandma who lives in Kenya said he was born there. (O) Theories allege that Obama’s published birth certificate is a forgery, that his actual birthplace is not Hawaii but Kenya. Text S is a snippet from Yahoo Answers and text O is a snippet from the Wikipedia page titled: “Barack Obama Citizenship Conspiracy Theories”. S is subjective, expressing the opinion of the author. On the other hand, O is objective, stating only what has been alleged. Lit</context>
<context position="17575" citStr="Wiebe et al., 2004" startWordPosition="2813" endWordPosition="2816">e used 4000 documents for 1012 # Feature 1 Subjectivity lexicon of strong and weak subjective words (Riloff and Wiebe, 2003). 2 Sentiment lexicon of positive and negative words (Liu et al., 2005). 3 Wikipedia-derived bias lexicon (Recasens et al., 2013). 4 Part-of-speech (POS) tags 5 Frequent bi-grams Table 1: Features used for the objectivity detector. training, 2000 per label. The rest of the documents were split into a development set (380) and a test set (220). A summary of the features we used is shown in Table 1. Features 1-3 refer to lexicons developed by prior methods on subjectivity (Wiebe et al., 2004), sentiment analysis (Liu et al., 2005) and bias detection (Recasens et al., 2013). Feature 4 refers to part-of-speech tags of the terms found in the document that are also in the lexicons. Feature 5 refers to bi-grams that frequently occur (mention frequency of &gt; 10) in the 4,600 documents. The most contributing features were the lexicons, features (1-3) and the frequent bi-grams, feature 5. We discovered that using frequent bi-gram features instead of uni-grams or bi-grams resulted in higher precision. The classifier was able to determine that for example bi-grams such as “think that”, “so f</context>
</contexts>
<marker>Wiebe, Wilson, Bruce, Bell, Martin, 2004</marker>
<rawString>J. Wiebe, T. Wilson, R. Bruce, M. Bell, M. Martin: Learning subjective language. Computational Linguistics, 30(3):277308. 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yin</author>
<author>J Han</author>
<author>P S Yu</author>
</authors>
<title>Truth Discovery with Multiple Conflicting Information Providers on the Web.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Knowledge Discovery in Databases (KDD) ,</booktitle>
<pages>1048--1052</pages>
<contexts>
<context position="3093" citStr="Yin et al., 2007" startWordPosition="453" endWordPosition="456">e often tied to the extractor’s ability to read and understand natural language text. This is different from a score that indicates the degree to which a given fact candidate is believable. Such a believability score is sometimes also referred to as a credibility score or truthfulness score. The believability score reflects the likelihood that a given statement is true. Truth-finding algorithms aim to compute this score for each fact candidate. Prior truth-finding methods are mostly based on iterative voting, where votes are propagated from sources to fact candidates and then back to sources (Yin et al., 2007; Galland et al., 2010; Pasternack and Roth, 2010; Li et al., 2011; Yin and Tan, 2011). At the core of iterative voting is the assumption that candidates mentioned by many sources are more likely to be true. However, additional aspects of a source influence its trustworthiness, besides external votes. Our goal is to accurately assess truthfulness of fact candidates by taking into account the language of sources that mention them. A Mechanical Turk study we carried out revealed that there is a significant correlation between objectivity of language and trustworthiness of sources. Objectivity of</context>
<context position="25642" citStr="Yin et al., 2007" startWordPosition="4154" endWordPosition="4157"> from a general knowledge quiz 4. We selected questions from the inventions category. Questions are multiple choice, with 4 options per question. Thus, from each question, we created one fact candidate and 3 alternative candidates. Details of the resulting dataset are shown in the row “KWP Quiz” in Table 3. Baselines. We compared FactChecker against five baselines: i) Vote counts the number of sources that mention the fact candidate. ii) TruthFinder is an iterative voting approach where votes are propagated from sources to fact candidates and then back to sources. Implemented as described in (Yin et al., 2007). iii) Investment is also based on transitive voting, however scores are updated differently. A source gets a vote of trust from each candidate it “invests” in, but the vote is weighted by the proportion of trust the source previously “invested” in the candidate relative to other investors. Implemented as described in (Pasternack and Roth, 2010). iv) PooledInvest is a variation of investment, we report both because in their paper, there was no clear winner among the two variations. v) 2-Estimates is a probabilistic model which approximates error rates of sources and fact candidates (Galland et</context>
<context position="30675" citStr="Yin et al., 2007" startWordPosition="4936" endWordPosition="4939">ct that at the core of iterative voting is still the assumption that fact candidates mentioned in many sources are more likely to be true. Therefore, for both majority vote and iterative voting, when mention frequencies of various alternatives are the same, accuracy suffers. Based on these observations, it is clear that truthfinding solutions need to incorporate fine-grained content-aware features outside of external votes. FactChecker takes a step in this direction by incorporating the document-level feature of objectivity. 6 Related Work There is a fairly small body of work on truthfinding (Yin et al., 2007; Galland et al., 2010; Pasternack and Roth, 2010; Li et al., 2011; Yin and Tan, 2011; Zhao et al., 2012; Pasternack and Roth, 2013). The method underlying most truth-finding algorithms is iterative transitive voting (Yin et al., 2007; Galland et al., 2010; Pasternack and Roth, 2010; Li et al., 2011). Fact candidates are initialized with a score. Trustworthiness of sources is then computed from the believability of the fact candidates they mention. In return, believability of candidates is recomputed based on the trustworthi1016 Dataset Fact Candidate Alternatives &amp; Ranking WKP (George W. Bush</context>
</contexts>
<marker>Yin, Han, Yu, 2007</marker>
<rawString>X. Yin, J. Han, P. S. Yu: Truth Discovery with Multiple Conflicting Information Providers on the Web. In Proceedings of the International Conference on Knowledge Discovery in Databases (KDD) , pages1048-1052. 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yin</author>
<author>W Tan</author>
</authors>
<title>Semi-supervised truth discover.</title>
<date>2011</date>
<booktitle>In Proceedings of the 19th International Conference on World Wide Web (WWW),</booktitle>
<pages>217--226</pages>
<contexts>
<context position="3179" citStr="Yin and Tan, 2011" startWordPosition="470" endWordPosition="473">. This is different from a score that indicates the degree to which a given fact candidate is believable. Such a believability score is sometimes also referred to as a credibility score or truthfulness score. The believability score reflects the likelihood that a given statement is true. Truth-finding algorithms aim to compute this score for each fact candidate. Prior truth-finding methods are mostly based on iterative voting, where votes are propagated from sources to fact candidates and then back to sources (Yin et al., 2007; Galland et al., 2010; Pasternack and Roth, 2010; Li et al., 2011; Yin and Tan, 2011). At the core of iterative voting is the assumption that candidates mentioned by many sources are more likely to be true. However, additional aspects of a source influence its trustworthiness, besides external votes. Our goal is to accurately assess truthfulness of fact candidates by taking into account the language of sources that mention them. A Mechanical Turk study we carried out revealed that there is a significant correlation between objectivity of language and trustworthiness of sources. Objectivity of language refers to the use of neutral, impartial language, which is not personal, jud</context>
<context position="30760" citStr="Yin and Tan, 2011" startWordPosition="4952" endWordPosition="4955"> mentioned in many sources are more likely to be true. Therefore, for both majority vote and iterative voting, when mention frequencies of various alternatives are the same, accuracy suffers. Based on these observations, it is clear that truthfinding solutions need to incorporate fine-grained content-aware features outside of external votes. FactChecker takes a step in this direction by incorporating the document-level feature of objectivity. 6 Related Work There is a fairly small body of work on truthfinding (Yin et al., 2007; Galland et al., 2010; Pasternack and Roth, 2010; Li et al., 2011; Yin and Tan, 2011; Zhao et al., 2012; Pasternack and Roth, 2013). The method underlying most truth-finding algorithms is iterative transitive voting (Yin et al., 2007; Galland et al., 2010; Pasternack and Roth, 2010; Li et al., 2011). Fact candidates are initialized with a score. Trustworthiness of sources is then computed from the believability of the fact candidates they mention. In return, believability of candidates is recomputed based on the trustworthi1016 Dataset Fact Candidate Alternatives &amp; Ranking WKP (George W. Bush) lived in (Midland,TX) 1.Midland,TX 2.Compton,CA 3.Washington D.C. 4.Venezuela* KB (</context>
<context position="32349" citStr="Yin and Tan, 2011" startWordPosition="5190" endWordPosition="5193">ess of their sources. This process is repeated over several iterations until convergence. (Yin et al., 2007) was the first to implement this idea, subsequent work improved upon iterative voting in several directions. (Dong et al., 2009) incorporates copying-detection; giving high trust to sources that are independently authored. (Galland et al., 2010) approximates error rates of sources and fact candidates. (Pasternack and Roth, 2010) introduces prior knowledge in the form of linear programming constraints in order to ensure that the truth discovered is consistent with what is already known. (Yin and Tan, 2011) introduces supervision by using ground truth facts so that sources that disagree with the ground truth are penalized. (Li et al., 2011) uses search engine APIs to gather additional evidence for believability of fact candidates. WikiTrust (Adler and Alfaro, 2007) is a content-aware but domain-specific method. It computes trustworthiness of wiki authors based on the revision history of the articles they have authored. Motivated by interpretability of probabilistic scores, two recent papers addressed the truth-finding problem as a probabilistic inference problem over the sources and the fact can</context>
</contexts>
<marker>Yin, Tan, 2011</marker>
<rawString>X. Yin, W. Tan: Semi-supervised truth discover. In Proceedings of the 19th International Conference on World Wide Web (WWW), pp. 217-226, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yu</author>
<author>V</author>
</authors>
<title>Hatzivassiloglou: Towards Answering Opinion Questions: Separating Facts from Opinions and Identifying the Polarity of Opinion Sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>129--136</pages>
<marker>Yu, V, 2003</marker>
<rawString>H. Yu, V. Hatzivassiloglou: Towards Answering Opinion Questions: Separating Facts from Opinions and Identifying the Polarity of Opinion Sentences. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages. 129-136, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Zhao</author>
<author>B I P Rubinstein</author>
<author>J Gemmell</author>
<author>J Han</author>
</authors>
<title>A Bayesian approach to discovering truth from conflicting sources for data integration.</title>
<date>2012</date>
<booktitle>In Proceedings of the VLDB Endowment (PVLDB),</booktitle>
<pages>5--6</pages>
<contexts>
<context position="30779" citStr="Zhao et al., 2012" startWordPosition="4956" endWordPosition="4959">sources are more likely to be true. Therefore, for both majority vote and iterative voting, when mention frequencies of various alternatives are the same, accuracy suffers. Based on these observations, it is clear that truthfinding solutions need to incorporate fine-grained content-aware features outside of external votes. FactChecker takes a step in this direction by incorporating the document-level feature of objectivity. 6 Related Work There is a fairly small body of work on truthfinding (Yin et al., 2007; Galland et al., 2010; Pasternack and Roth, 2010; Li et al., 2011; Yin and Tan, 2011; Zhao et al., 2012; Pasternack and Roth, 2013). The method underlying most truth-finding algorithms is iterative transitive voting (Yin et al., 2007; Galland et al., 2010; Pasternack and Roth, 2010; Li et al., 2011). Fact candidates are initialized with a score. Trustworthiness of sources is then computed from the believability of the fact candidates they mention. In return, believability of candidates is recomputed based on the trustworthi1016 Dataset Fact Candidate Alternatives &amp; Ranking WKP (George W. Bush) lived in (Midland,TX) 1.Midland,TX 2.Compton,CA 3.Washington D.C. 4.Venezuela* KB (Dirk Kuyt) plays fo</context>
<context position="32975" citStr="Zhao et al., 2012" startWordPosition="5285" endWordPosition="5288">es supervision by using ground truth facts so that sources that disagree with the ground truth are penalized. (Li et al., 2011) uses search engine APIs to gather additional evidence for believability of fact candidates. WikiTrust (Adler and Alfaro, 2007) is a content-aware but domain-specific method. It computes trustworthiness of wiki authors based on the revision history of the articles they have authored. Motivated by interpretability of probabilistic scores, two recent papers addressed the truth-finding problem as a probabilistic inference problem over the sources and the fact candidates (Zhao et al., 2012; Pasternack and Roth, 2013). Truth-finders based on textual entailment such as TruthTeller (Lotan et al., 2013) determine if a sentence states something or not. The focus is on understanding natural language, including the use of negation. This is similar to the goal of fact extraction (Banko et al., 2007; Carlson et al., 2010; Fader et al., 2011; Nakashole et al., 2011; Del Corro and Gemulla, 2013). In a departure from prior work, our method leverages language of sources in its believability computation model. Furthermore, we introduced a co-mention score which is designed to avoid potential</context>
</contexts>
<marker>Zhao, Rubinstein, Gemmell, Han, 2012</marker>
<rawString>B. Zhao, B. I. P. Rubinstein, J. Gemmell, J. Han: A Bayesian approach to discovering truth from conflicting sources for data integration. In Proceedings of the VLDB Endowment (PVLDB), 5(6):550-561, 2012.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>