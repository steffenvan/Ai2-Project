<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001063">
<title confidence="0.983731">
Optimal Multi-Paragraph Text Segmentation by Dynamic Programming
</title>
<author confidence="0.995351">
Oskari Heinonen
</author>
<affiliation confidence="0.997507">
University of Helsinki, Department of Computer Science
</affiliation>
<address confidence="0.74849">
P.O. Box 26 (Teollisuuskatu 23), FIN-00014 University of Helsinki, Finland
</address>
<email confidence="0.959617">
Oskari.Heinonen@cs.Helsinki.F1
</email>
<sectionHeader confidence="0.996835" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999930038461539">
There exist several methods of calculating a similar-
ity curve, or a sequence of similarity values, repre-
senting the lexical cohesion of successive text con-
stituents, e.g., paragraphs. Methods for deciding
the locations of fragment boundaries are, however,
scarce. We propose a fragmentation method based
on dynamic programming. The method is theoret-
ically sound and guaranteed to provide an optimal
splitting on the basis of a similarity curve, a pre-
ferred fragment length, and a cost function defined.
The method is especially useful when control on
fragment size is of importance.
remaining words, i.e., terms. Then we take a pre-
defined number, e.g., 50, of the most frequent terms
to represent the paragraph, and count the similar-
ity using the cosine coefficient (see, e.g., (Salton,
1989)). Furthermore, we have applied a sliding win-
dow method: instead of just one paragraph, sev-
eral paragraphs on both sides of each paragraph
boundary are considered. The paragraph vectors are
weighted based on their distance from the boundary
in question with immediate paragraphs having the
highest weight. The benefit of using a larger win-
dow is that we can smooth the effect of short para-
graphs and such, perhaps example-type, paragraphs
that interrupt a chain of coherent paragraphs.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998814">
Electronic full-text documents and digital libraries
make the utilization of texts much more effective
than before; yet, they pose new problems and re-
quirements. For example, document retrieval based
on string searches typically returns either the whole
document or just the occurrences of the searched
words. What the user often is after, however, is mi-
crodocument: a part of the document that contains
the occurrences and is reasonably self-contained.
Microdocuments can be created by utilizing lex-
ical cohesion (term repetition and semantic rela-
tions) present in the text. There exist several meth-
ods of calculating a similarity curve, or a sequence
of similarity values, representing the lexical cohe-
sion of successive constituents (such as paragraphs)
of text (see, e.g., (Hearst, 1994; Hearst, 1997; Koz-
ima, 1993; Morris and Hirst, 1991; Yaari, 1997;
Youmans, 1991)). Methods for deciding the loca-
tions of fragment boundaries are, however, not that
common, and those that exist are often rather heuris-
tic in nature.
To evaluate our fragmentation method, to be ex-
plained in Section 2, we calculate the paragraph
similarities as follows. We employ stemming, re-
move stopwords, and count the frequencies of the
</bodyText>
<page confidence="0.971717">
1484
</page>
<sectionHeader confidence="0.8326755" genericHeader="method">
2 Fragmentation by Dynamic
Programming
</sectionHeader>
<bodyText confidence="0.999900347826087">
Fragmentation is a problem of choosing the para-
graph boundaries that make the best fragment
boundaries. The local minima of the similarity
curve are the points of low lexical cohesion and thus
the natural candidates. To get reasonably-sized mi-
crodocuments, the similarity information alone is
not enough; also the lengths of the created frag-
ments have to be considered. In this section, we de-
scribe an approach that performs the fragmentation
by using both the similarities and the length infor-
mation in a robust manner. The method is based on
a programming paradigm called dynamic program-
ming (see, e.g., (Cormen et al., 1990)). Dynamic
programming as a method guarantees the optimal-
ity of the result with respect to the input and the
parameters.
The idea of the fragmentation algorithm is as fol-
lows (see also Fig. 1). We start from the first bound-
ary and calculate a cost for it as if the first paragraph
was a single fragment. Then we take the second
boundary and attach to it the minimum of the two
available possibilities: the cost of the first two para-
graphs as if they were a single fragment and the cost
</bodyText>
<figure confidence="0.995552">
0.6
0.5
0.4
0.3
0.2
0.1
0
0.6
0.5
0.4
0.3
0.2
0.1
0
</figure>
<equation confidence="0.779607">
fragmentation(n, p, h, len[1..n], sim[1..n — 1])
/* n no. of pars, p preferred frag length, h scaling */
I* len[1..n] par lengths, sim[1..n — 1] similarities */
sim[0] := 0.0; cost [0] := 0.0; B := 0;
for par := 1 ton {
lens. := 0; /* cumulative fragment length */
cmin := MAXREAL;
for i := par to 1 (
iensum := lens. + len[i];
c := cien(lensum, P, h);
if C&gt; cmin /* optimization */
exit the innermost for loop;
C := c + cos* — 1] + sim[i — 1];
if c &lt; cmin {
Cmin := C; :=j — 1;
cost [par] := cmin; linkprey [par] := loc_cmm;
</equation>
<table confidence="0.973577642857143">
Mars. Chapter II. Section I.
. • ° •
&amp;quot;W6ClinH0.25L&amp;quot;
&apos;W6ClinH0.5L&apos;
&apos;W6ClinH0.75L&apos;
&apos;W6ClinH1.0e
&apos;W6C1inH1.25L&apos;
I &apos;W6ClinH1.51:
. A .
A I I
i ,
■
. \.,. c.f....----, /1,, j
1 liii It II I I JIM I It I I I I -III :Ill
</table>
<figure confidence="0.979829238095238">
1000 2000 3000 4000 5000 6000 7000
hurdcount
(a)
Mars. Chapter II. Section I.
&amp;quot;W6CparH0.25L
&apos;W6Cpa,H0.5L&apos;
&amp;quot;:(N6a&apos;aparmoi.gt:
&apos;46=&amp;quot;61.,2.21c.:
,.
. .
,„ „ , nnt I [ i I i
1000 2000 3000 4000 5000 6000 7000
wordcount
(b)
while linkprev[i] &gt; 0{
:=
prey j, prey j;
link r
B := B U link
return(B); /* set of chosen fragment boundaries */
1
</figure>
<figureCaption confidence="0.8536025">
Figure 1: The dynamic programming algorithm for
fragment boundary detection.
</figureCaption>
<bodyText confidence="0.9829237">
of the second paragraph as a separate fragment. In
the following steps, the evaluation moves on by one
paragraph at each time, and all the possible loca-
tions of the previous breakpoint are considered. We
continue this procedure till the end of the text, and
finally we can generate a list of breakpoints that in-
dicate the fragmentation.
The cost at each boundary is a combination of
three components: the cost of fragment length cien,
and the cost cost[.] and similarity sim [.] of some
previous boundary. The cost function gen gives the
lowest cost for the preferred fragment length given
by the user, say, e.g., 500 words. A fragment which
is either shorter or longer gets a higher cost, i.e., is
punished for its length. We have experimented with
two families of cost functions, a family of second
degree functions (parabolas),
cien(x,p, h) = h(2-x2 — + 1),
and V-shape linear functions,
gen (x, h) = 1h(1; — 1)1,
</bodyText>
<figureCaption confidence="0.7399265">
Figure 2: Similarity curve and detected fragment
boundaries with different cost functions. (a) Lin-
ear. (b) Parabola. p is 600 words in both (a) &amp; (b).
&amp;quot;H0.25&amp;quot;, etc., indicates the value of h. Vertical bars
</figureCaption>
<bodyText confidence="0.9937045">
indicate fragment boundaries while short bars below
horizontal axis indicate paragraph boundaries.
where x is the actual fragment length, p is the pre-
ferred fragment length given by the user, and h is a
scaling parameter that allows us to adjust the weight
given to fragment length. The smaller the value of
h, the less weight is given to the preferred fragment
length in comparison with the similarity measure.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.998392533333333">
As test data we used Mars by Percival Lowell, 1895.
As an illustrative example, we present the analysis
of Section I. Evidence of it of Chapter II. Atmo-
sphere. The length of the section is approximately
6600 words and it contains 55 paragraphs. The frag-
ments found with different parameter settings can
be seen in Figure 2. One of the most interesting is
the one with parabola cost function and h = .5. In
this case the fragment length adjusts nicely accord-
ing to the similarity curve. Looking at the text, most
fragments have an easily identifiable topic, like at-
mospheric chemistry in fragment 7. Fragments 2
and 3 seem to have roughly the same topic: measur-
ing the diameter of the planet Mars. The fact that
they do not form a single fragment can be explained
</bodyText>
<page confidence="0.984425">
1485
</page>
<table confidence="0.433488">
h 1
-avg lmin imax davg
</table>
<tableCaption confidence="0.81837">
Table 1: Variation of fragment length. Columns:
/avg, /min, /max average, minimum, and maximum
fragment length; and davg average deviation.
</tableCaption>
<bodyText confidence="0.969654842105263">
by the preferred fragment length requirement.
Table 1 summarizes the effect of the scaling fac-
tor h in relation to the fragment length variation
with the two cost functions over those 8 sections
of Mars that have a length of at least 20 para-
graphs. The average deviation davg with respect
to the preferred fragment length p is defined as
davg = (Eim-i 1.73 - /il)/m where l is the length of
fragment i, and m is the number of fragments. The
parametric cost function chosen affects the result a
lot. As expected, the second degree cost function
allows more variation than the linear one but roles
change with a small h. Although the experiment is
insufficient, we can see that in this example a factor
h &gt; 1.0 is unsuitable with the linear cost function
(and h = 1.5 with the parabola) since in these cases
so much weight is given to the fragment length that
fragment boundaries can appear very close to quite
strong local maxima of the similarity curve.
</bodyText>
<sectionHeader confidence="0.997381" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.99997480952381">
In this article, we presented a method for detect-
ing fragment boundaries in text. The fragmentation
method is based on dynamic programming and is
guaranteed to give an optimal solution with respect
to a similarity curve, a preferred fragment length,
and a parametric fragment-length cost function de-
fined. The method is independent of the similarity
calculation. This means that any method, not nec-
essarily based on lexical cohesion, producing a suit-
able sequence of similarities can be used prior to
our fragmentation method. For example, the lexical
cohesion profile (Kozima, 1993) should be perfectly
usable with our fragmentation method.
The method is especially useful when control
over fragment size is required. This is the case
in passage retrieval since windows of 1000 bytes
(Wilkinson and Zobel, 1995) or some hundred
words (Callan, 1994) have been proposed as best
passage sizes. Furthermore, we believe that frag-
ments of reasonably similar size are beneficial in
our intended purpose of document assembly.
</bodyText>
<sectionHeader confidence="0.996402" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9995385">
This work has been supported by the Finnish
Technology Development Centre (TEKES) together
with industrial partners, and by a grant from the
350th Anniversary Foundation of the University
of Helsinki. The author thanks Helena Ahonen,
Barbara Heikkinen, Mika Klemettinen, and Juha
Karlckainen for their contributions to the work de-
scribed.
</bodyText>
<sectionHeader confidence="0.999376" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999642838709678">
J. P. Callan. 1994. Passage-level evidence in doc-
ument retrieval. In Proc. SIGIR&apos;94, Dublin, Ire-
land.
T. H. Cormen, C. E. Leiserson, and R. L. Rivest.
1990. Introduction to Algorithms. MIT Press,
Cambridge, MA, USA.
M. A. Hearst. 1994. Multi-paragraph segmentation
of expository text. In Proc. ACL-94, Las Cruces,
NM, USA.
M. A. Hearst. 1997. TextTiling: Segmenting text
into multi-paragraph subtopic passages. Compu-
tational Linguistics, 23(1):33-64, March.
H. Kozima. 1993. Text segmentation based on sim-
ilarity between words. In Proc. ACL-93, Colum-
bus, OH, USA.
J. Morris and G. Hirst. 1991. Lexical cohesion
computed by thesaural relation as an indicator of
the structure of text. Computational Linguistics,
17(1):21-48.
G. Salton. 1989. Automatic Text Processing: The
Transformation, Analysis, and Retrieval of Infor-
mation by Computer. Addison-Wesley, Reading,
MA, USA.
R. Wilkinson and J. Zobel. 1995. Comparison of
fragmentation schemes for document retrieval. In
Overview of TREC-3, Gaithersburg, MD, USA.
Y. Yaari. 1997. Segmentation of expository texts by
hierarchical agglomerative clustering. In Proc.
RANLP &apos;97, Tzigov Chark, Bulgaria.
G. Youmans. 1991. A new tool for discourse anal-
ysis. Language, 67(4):763-789.
</reference>
<figure confidence="0.9992006">
cost function
linear
parabola
.25 1096.1 501 3101 476.5
.50 706.4 501 1328 110.5
.75 635.7 515 835 60.1
1.00 635.7 515 835 59.5
1.25 635.7 515 835 59.5
1.50 635.7 515 835 57.6
.25 908.2 501 1236 269.4
.50 691.0 319 1020 126.0
.75 676.3 371 922 105.8
1.00 662.2 371 866 94.2
1.25 648.7 466 835 82.4
1.50 635.7 473 835 69.9
</figure>
<page confidence="0.945467">
1486
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.436943">
<title confidence="0.999921">Optimal Multi-Paragraph Text Segmentation by Dynamic Programming</title>
<author confidence="0.999935">Oskari Heinonen</author>
<affiliation confidence="0.999738">University of Helsinki, Department of Computer Science</affiliation>
<address confidence="0.993646">P.O. Box 26 (Teollisuuskatu 23), FIN-00014 University of Helsinki, Finland</address>
<note confidence="0.448636">Oskari.Heinonen@cs.Helsinki.F1</note>
<abstract confidence="0.999082703703704">There exist several methods of calculating a similarity curve, or a sequence of similarity values, representing the lexical cohesion of successive text constituents, e.g., paragraphs. Methods for deciding the locations of fragment boundaries are, however, scarce. We propose a fragmentation method based on dynamic programming. The method is theoretically sound and guaranteed to provide an optimal splitting on the basis of a similarity curve, a preferred fragment length, and a cost function defined. The method is especially useful when control on fragment size is of importance. remaining words, i.e., terms. Then we take a predefined number, e.g., 50, of the most frequent terms to represent the paragraph, and count the similarity using the cosine coefficient (see, e.g., (Salton, 1989)). Furthermore, we have applied a sliding window method: instead of just one paragraph, several paragraphs on both sides of each paragraph boundary are considered. The paragraph vectors are weighted based on their distance from the boundary in question with immediate paragraphs having the highest weight. The benefit of using a larger window is that we can smooth the effect of short paragraphs and such, perhaps example-type, paragraphs that interrupt a chain of coherent paragraphs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J P Callan</author>
</authors>
<title>Passage-level evidence in document retrieval.</title>
<date>1994</date>
<booktitle>In Proc. SIGIR&apos;94,</booktitle>
<location>Dublin, Ireland.</location>
<marker>Callan, 1994</marker>
<rawString>J. P. Callan. 1994. Passage-level evidence in document retrieval. In Proc. SIGIR&apos;94, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T H Cormen</author>
<author>C E Leiserson</author>
<author>R L Rivest</author>
</authors>
<title>Introduction to Algorithms.</title>
<date>1990</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="3428" citStr="Cormen et al., 1990" startWordPosition="529" endWordPosition="532">tation is a problem of choosing the paragraph boundaries that make the best fragment boundaries. The local minima of the similarity curve are the points of low lexical cohesion and thus the natural candidates. To get reasonably-sized microdocuments, the similarity information alone is not enough; also the lengths of the created fragments have to be considered. In this section, we describe an approach that performs the fragmentation by using both the similarities and the length information in a robust manner. The method is based on a programming paradigm called dynamic programming (see, e.g., (Cormen et al., 1990)). Dynamic programming as a method guarantees the optimality of the result with respect to the input and the parameters. The idea of the fragmentation algorithm is as follows (see also Fig. 1). We start from the first boundary and calculate a cost for it as if the first paragraph was a single fragment. Then we take the second boundary and attach to it the minimum of the two available possibilities: the cost of the first two paragraphs as if they were a single fragment and the cost 0.6 0.5 0.4 0.3 0.2 0.1 0 0.6 0.5 0.4 0.3 0.2 0.1 0 fragmentation(n, p, h, len[1..n], sim[1..n — 1]) /* n no. of p</context>
</contexts>
<marker>Cormen, Leiserson, Rivest, 1990</marker>
<rawString>T. H. Cormen, C. E. Leiserson, and R. L. Rivest. 1990. Introduction to Algorithms. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>Multi-paragraph segmentation of expository text.</title>
<date>1994</date>
<booktitle>In Proc. ACL-94, Las</booktitle>
<location>Cruces, NM, USA.</location>
<contexts>
<context position="2334" citStr="Hearst, 1994" startWordPosition="353" endWordPosition="354">ument retrieval based on string searches typically returns either the whole document or just the occurrences of the searched words. What the user often is after, however, is microdocument: a part of the document that contains the occurrences and is reasonably self-contained. Microdocuments can be created by utilizing lexical cohesion (term repetition and semantic relations) present in the text. There exist several methods of calculating a similarity curve, or a sequence of similarity values, representing the lexical cohesion of successive constituents (such as paragraphs) of text (see, e.g., (Hearst, 1994; Hearst, 1997; Kozima, 1993; Morris and Hirst, 1991; Yaari, 1997; Youmans, 1991)). Methods for deciding the locations of fragment boundaries are, however, not that common, and those that exist are often rather heuristic in nature. To evaluate our fragmentation method, to be explained in Section 2, we calculate the paragraph similarities as follows. We employ stemming, remove stopwords, and count the frequencies of the 1484 2 Fragmentation by Dynamic Programming Fragmentation is a problem of choosing the paragraph boundaries that make the best fragment boundaries. The local minima of the simil</context>
</contexts>
<marker>Hearst, 1994</marker>
<rawString>M. A. Hearst. 1994. Multi-paragraph segmentation of expository text. In Proc. ACL-94, Las Cruces, NM, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>TextTiling: Segmenting text into multi-paragraph subtopic passages.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--1</pages>
<contexts>
<context position="2348" citStr="Hearst, 1997" startWordPosition="355" endWordPosition="356">l based on string searches typically returns either the whole document or just the occurrences of the searched words. What the user often is after, however, is microdocument: a part of the document that contains the occurrences and is reasonably self-contained. Microdocuments can be created by utilizing lexical cohesion (term repetition and semantic relations) present in the text. There exist several methods of calculating a similarity curve, or a sequence of similarity values, representing the lexical cohesion of successive constituents (such as paragraphs) of text (see, e.g., (Hearst, 1994; Hearst, 1997; Kozima, 1993; Morris and Hirst, 1991; Yaari, 1997; Youmans, 1991)). Methods for deciding the locations of fragment boundaries are, however, not that common, and those that exist are often rather heuristic in nature. To evaluate our fragmentation method, to be explained in Section 2, we calculate the paragraph similarities as follows. We employ stemming, remove stopwords, and count the frequencies of the 1484 2 Fragmentation by Dynamic Programming Fragmentation is a problem of choosing the paragraph boundaries that make the best fragment boundaries. The local minima of the similarity curve ar</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>M. A. Hearst. 1997. TextTiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics, 23(1):33-64, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kozima</author>
</authors>
<title>Text segmentation based on similarity between words.</title>
<date>1993</date>
<booktitle>In Proc. ACL-93,</booktitle>
<location>Columbus, OH, USA.</location>
<contexts>
<context position="2362" citStr="Kozima, 1993" startWordPosition="357" endWordPosition="359">ing searches typically returns either the whole document or just the occurrences of the searched words. What the user often is after, however, is microdocument: a part of the document that contains the occurrences and is reasonably self-contained. Microdocuments can be created by utilizing lexical cohesion (term repetition and semantic relations) present in the text. There exist several methods of calculating a similarity curve, or a sequence of similarity values, representing the lexical cohesion of successive constituents (such as paragraphs) of text (see, e.g., (Hearst, 1994; Hearst, 1997; Kozima, 1993; Morris and Hirst, 1991; Yaari, 1997; Youmans, 1991)). Methods for deciding the locations of fragment boundaries are, however, not that common, and those that exist are often rather heuristic in nature. To evaluate our fragmentation method, to be explained in Section 2, we calculate the paragraph similarities as follows. We employ stemming, remove stopwords, and count the frequencies of the 1484 2 Fragmentation by Dynamic Programming Fragmentation is a problem of choosing the paragraph boundaries that make the best fragment boundaries. The local minima of the similarity curve are the points o</context>
<context position="9150" citStr="Kozima, 1993" startWordPosition="1563" endWordPosition="1564">milarity curve. 4 Conclusions In this article, we presented a method for detecting fragment boundaries in text. The fragmentation method is based on dynamic programming and is guaranteed to give an optimal solution with respect to a similarity curve, a preferred fragment length, and a parametric fragment-length cost function defined. The method is independent of the similarity calculation. This means that any method, not necessarily based on lexical cohesion, producing a suitable sequence of similarities can be used prior to our fragmentation method. For example, the lexical cohesion profile (Kozima, 1993) should be perfectly usable with our fragmentation method. The method is especially useful when control over fragment size is required. This is the case in passage retrieval since windows of 1000 bytes (Wilkinson and Zobel, 1995) or some hundred words (Callan, 1994) have been proposed as best passage sizes. Furthermore, we believe that fragments of reasonably similar size are beneficial in our intended purpose of document assembly. Acknowledgements This work has been supported by the Finnish Technology Development Centre (TEKES) together with industrial partners, and by a grant from the 350th </context>
</contexts>
<marker>Kozima, 1993</marker>
<rawString>H. Kozima. 1993. Text segmentation based on similarity between words. In Proc. ACL-93, Columbus, OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Morris</author>
<author>G Hirst</author>
</authors>
<title>Lexical cohesion computed by thesaural relation as an indicator of the structure of text.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<pages>17--1</pages>
<contexts>
<context position="2386" citStr="Morris and Hirst, 1991" startWordPosition="360" endWordPosition="363">ypically returns either the whole document or just the occurrences of the searched words. What the user often is after, however, is microdocument: a part of the document that contains the occurrences and is reasonably self-contained. Microdocuments can be created by utilizing lexical cohesion (term repetition and semantic relations) present in the text. There exist several methods of calculating a similarity curve, or a sequence of similarity values, representing the lexical cohesion of successive constituents (such as paragraphs) of text (see, e.g., (Hearst, 1994; Hearst, 1997; Kozima, 1993; Morris and Hirst, 1991; Yaari, 1997; Youmans, 1991)). Methods for deciding the locations of fragment boundaries are, however, not that common, and those that exist are often rather heuristic in nature. To evaluate our fragmentation method, to be explained in Section 2, we calculate the paragraph similarities as follows. We employ stemming, remove stopwords, and count the frequencies of the 1484 2 Fragmentation by Dynamic Programming Fragmentation is a problem of choosing the paragraph boundaries that make the best fragment boundaries. The local minima of the similarity curve are the points of low lexical cohesion a</context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>J. Morris and G. Hirst. 1991. Lexical cohesion computed by thesaural relation as an indicator of the structure of text. Computational Linguistics, 17(1):21-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
</authors>
<title>Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer.</title>
<date>1989</date>
<publisher>Addison-Wesley,</publisher>
<location>Reading, MA, USA.</location>
<contexts>
<context position="1043" citStr="Salton, 1989" startWordPosition="152" endWordPosition="153">phs. Methods for deciding the locations of fragment boundaries are, however, scarce. We propose a fragmentation method based on dynamic programming. The method is theoretically sound and guaranteed to provide an optimal splitting on the basis of a similarity curve, a preferred fragment length, and a cost function defined. The method is especially useful when control on fragment size is of importance. remaining words, i.e., terms. Then we take a predefined number, e.g., 50, of the most frequent terms to represent the paragraph, and count the similarity using the cosine coefficient (see, e.g., (Salton, 1989)). Furthermore, we have applied a sliding window method: instead of just one paragraph, several paragraphs on both sides of each paragraph boundary are considered. The paragraph vectors are weighted based on their distance from the boundary in question with immediate paragraphs having the highest weight. The benefit of using a larger window is that we can smooth the effect of short paragraphs and such, perhaps example-type, paragraphs that interrupt a chain of coherent paragraphs. 1 Introduction Electronic full-text documents and digital libraries make the utilization of texts much more effect</context>
</contexts>
<marker>Salton, 1989</marker>
<rawString>G. Salton. 1989. Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer. Addison-Wesley, Reading, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Wilkinson</author>
<author>J Zobel</author>
</authors>
<title>Comparison of fragmentation schemes for document retrieval.</title>
<date>1995</date>
<booktitle>In Overview of TREC-3,</booktitle>
<location>Gaithersburg, MD, USA.</location>
<marker>Wilkinson, Zobel, 1995</marker>
<rawString>R. Wilkinson and J. Zobel. 1995. Comparison of fragmentation schemes for document retrieval. In Overview of TREC-3, Gaithersburg, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yaari</author>
</authors>
<title>Segmentation of expository texts by hierarchical agglomerative clustering.</title>
<date>1997</date>
<booktitle>In Proc. RANLP &apos;97,</booktitle>
<location>Tzigov Chark, Bulgaria.</location>
<contexts>
<context position="2399" citStr="Yaari, 1997" startWordPosition="364" endWordPosition="365">the whole document or just the occurrences of the searched words. What the user often is after, however, is microdocument: a part of the document that contains the occurrences and is reasonably self-contained. Microdocuments can be created by utilizing lexical cohesion (term repetition and semantic relations) present in the text. There exist several methods of calculating a similarity curve, or a sequence of similarity values, representing the lexical cohesion of successive constituents (such as paragraphs) of text (see, e.g., (Hearst, 1994; Hearst, 1997; Kozima, 1993; Morris and Hirst, 1991; Yaari, 1997; Youmans, 1991)). Methods for deciding the locations of fragment boundaries are, however, not that common, and those that exist are often rather heuristic in nature. To evaluate our fragmentation method, to be explained in Section 2, we calculate the paragraph similarities as follows. We employ stemming, remove stopwords, and count the frequencies of the 1484 2 Fragmentation by Dynamic Programming Fragmentation is a problem of choosing the paragraph boundaries that make the best fragment boundaries. The local minima of the similarity curve are the points of low lexical cohesion and thus the n</context>
</contexts>
<marker>Yaari, 1997</marker>
<rawString>Y. Yaari. 1997. Segmentation of expository texts by hierarchical agglomerative clustering. In Proc. RANLP &apos;97, Tzigov Chark, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Youmans</author>
</authors>
<title>A new tool for discourse analysis.</title>
<date>1991</date>
<journal>Language,</journal>
<pages>67--4</pages>
<contexts>
<context position="2415" citStr="Youmans, 1991" startWordPosition="366" endWordPosition="367">ument or just the occurrences of the searched words. What the user often is after, however, is microdocument: a part of the document that contains the occurrences and is reasonably self-contained. Microdocuments can be created by utilizing lexical cohesion (term repetition and semantic relations) present in the text. There exist several methods of calculating a similarity curve, or a sequence of similarity values, representing the lexical cohesion of successive constituents (such as paragraphs) of text (see, e.g., (Hearst, 1994; Hearst, 1997; Kozima, 1993; Morris and Hirst, 1991; Yaari, 1997; Youmans, 1991)). Methods for deciding the locations of fragment boundaries are, however, not that common, and those that exist are often rather heuristic in nature. To evaluate our fragmentation method, to be explained in Section 2, we calculate the paragraph similarities as follows. We employ stemming, remove stopwords, and count the frequencies of the 1484 2 Fragmentation by Dynamic Programming Fragmentation is a problem of choosing the paragraph boundaries that make the best fragment boundaries. The local minima of the similarity curve are the points of low lexical cohesion and thus the natural candidate</context>
</contexts>
<marker>Youmans, 1991</marker>
<rawString>G. Youmans. 1991. A new tool for discourse analysis. Language, 67(4):763-789.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>