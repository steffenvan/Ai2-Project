<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000111">
<title confidence="0.973804">
The LIG machine translation system for WMT 2010
</title>
<author confidence="0.980426">
Marion Potet, Laurent Besacier and Herv´e Blanchon
</author>
<affiliation confidence="0.9186985">
LIG Laboratory, GETALP Team
University Joseph Fourier, Grenoble, France.
</affiliation>
<email confidence="0.839469">
Marion.Potet@imag.fr
Laurent.Besacier@imag.fr
Herve.Blanchon@imag.fr
</email>
<sectionHeader confidence="0.993449" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999782117647059">
This paper describes the system submit-
ted by the Laboratory of Informatics of
Grenoble (LIG) for the fifth Workshop
on Statistical Machine Translation. We
participated to the news shared transla-
tion task for the French-English language
pair. We investigated differents techniques
to simply deal with Out-Of-Vocabulary
words in a statistical phrase-based ma-
chine translation system and analyze their
impact on translation quality. The final
submission is a combination between a
standard phrase-based system using the
Moses decoder, with appropriate setups
and pre-processing, and a lemmatized sys-
tem to deal with Out-Of-Vocabulary con-
jugated verbs.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999889388888889">
We participated, for the first time, to the shared
news translation task of the fifth Workshop on Ma-
chine Translation (WMT 2010) for the French-
English language pair. The submission was
performed using a standard phrase-based trans-
lation system with appropriate setups and pre-
processings in order to deal with system’s un-
known words. Indeed, as shown in (Carpuat,
2009), (Habash, 2008) and (Niessen, 2004), han-
dling Ou-of-Vocabulary words with techniques
like lemmatization, phrase table extension or mor-
phological pre-processing is a way to improve
translation quality. After a short presentation of
our baseline system setups we discuss the effect
of Out-Of-Vocabulary words in the system and in-
troduce some ideas we chose to implement. In the
last part, we evaluate their impact on translation
quality using automatic and human evaluations.
</bodyText>
<sectionHeader confidence="0.975957" genericHeader="method">
2 Baseline System Setup
</sectionHeader>
<subsectionHeader confidence="0.944251">
2.1 Used Resources
</subsectionHeader>
<bodyText confidence="0.999984846153846">
We used the provided Europarl and News par-
allel corpora (total 1,638,440 sentences) to train
the translation model and the News monolin-
gual corpora (48,653,884 sentences) to train the
language model. The 2008 News test corpora
(news-test2008; 2,028 sentences) was used to tune
the produced system and last year’s test corpora
(news-test2009; 3,027 sentences) was used for
evaluation purposes. These corpora will be ref-
ered to as Dev and Test later in the paper. As pre-
processing steps, we applied the PERL scripts pro-
vided with the corpora to lowercase and tokenise
the data.
</bodyText>
<subsectionHeader confidence="0.998267">
2.2 Language modeling
</subsectionHeader>
<bodyText confidence="0.999945">
The target language model is a standard n-gram
language model trained using the SRI language
modeling toolkit (Stocke, 2002) on the news
monolingual corpus. The smoothing technique we
applied is the modified Kneser-Ney discounting
with interpolation.
</bodyText>
<subsectionHeader confidence="0.99912">
2.3 Translation modeling
</subsectionHeader>
<bodyText confidence="0.9999766">
The translation model was trained using the par-
allel corpus described earlier (Europarl+News).
First, the corpus was word aligned and then, the
pairs of source and corresponding target phrases
were extracted from the word-aligned bilingual
training corpus using the scripts provided with
the Moses decoder (Koehn et al., 2007). The re-
sult is a phrase-table containing all the aligned
phrases. This phrase-table, produced by the trans-
lation modeling, is used to extract several transla-
tions models. In our experiment we used thirteen
standard translation models: six distortion models,
a lexicon word-based and a phrase-based transla-
tion model for both direction, and a phrase, word
and distortion penalty.
</bodyText>
<page confidence="0.976792">
161
</page>
<note confidence="0.602144">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 161–166,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.994169">
2.4 Tuning and decoding
</subsectionHeader>
<bodyText confidence="0.99979875">
For the decoding (i.e. translation of the test
set), the system uses a log-linear combination of
the previous target language model and the thir-
teen translation models extracted from the phrase-
table. As the system can be beforehand tuned by
adjusting log-linear combination weights on a de-
velopement corpus, we used the Minimum Error
Rate Training (MERT) method, by (Och, 2003).
</bodyText>
<sectionHeader confidence="0.938845" genericHeader="method">
3 Ways of Improvements
</sectionHeader>
<subsectionHeader confidence="0.993895">
3.1 Discussion about Out-Of-Vocabulary
words in PBMT systems
</subsectionHeader>
<bodyText confidence="0.999706166666667">
Phrase-based statistical machine translation
(PBMT) use phrases as units in the translation
process. A phrase is a sequence of n consecutive
words known by the system. During the training,
these phrases are automaticaly learned and each
source phrase is mapped with its corresponding
target phrase. Throughout test set decoding, a
word not being part of this vocabulary list is
labeled as “Out-Of-Vocabulary” (OOV) and, as it
doesn’t appear in the translation table, the system
is unable to translate it. During the decoding,
Out-Of-Vocabulary words lead to “broken”
phrases and degrade translation quality. For these
reasons, we present some techniques to handle
Out-Of-Vocabulary words in a PBMT system and
combine these techniques before evaluating them.
In a preliminary study, we automatically ex-
tracted and manually analyzed OOVs of a 1000
sentences sample extracted from the test cor-
pus (news-test2009). There were altogether 487
OOVs tokens wich include 64.34% proper nouns
and words in foreign languages, 17.62% common
nouns, 15.16% conjugated verbs, 1.84% errors in
source corpus and 1.02% numbers. Note that, as
our system is configured to copy systematically
the OOVs in the produced translated sentence, the
rewriting of proper nouns and words in foreign
language is straightforward in that case. However,
we still have to deal with common nouns and con-
jugated verbs.
</bodyText>
<figure confidence="0.2955705">
Initial sentence:
“Cela ne marchera pas” souligna-t-il par la suite.
Normalised sentence:
“Cela ne marchera pas” il souligna par la suite
</figure>
<figureCaption confidence="0.999139">
Figure 1: Normalisation of the euphonious “t”
</figureCaption>
<subsectionHeader confidence="0.631277">
3.2 Term expansion with dictionary
</subsectionHeader>
<bodyText confidence="0.986011511111111">
The first idea is to expand the vocabulary size,
more specifically minimizing Out-Of-Vocabulary
common nouns adding a French-English dictio-
nary during the training process. In our experi-
ment, we used a free dictionnary made available
by the Wiktionary1 collaborative project (wich
aims to produce free-content multilingual dictio-
naries). The provided dictionnary, containing
15,200 entries, is added to the bilingual training
corpus before phrase-table extraction.
3.3 Lemmatization of the French source
verbs
To avoid Out-Of-Vocabulary conjugated verbs one
idea is to lemmatize verbs in the source train-
ing and test corpus to train a so-called lemma-
tized system. We used the freely available French
lemmatiser LIA TAGG (B´echet, 2001). But, ap-
plying lemmatization leads to a loss of informa-
tion (tense, person, number) which may affect
deeply the translation quality. Thus, we decided
to use the lematized system only when OOV verbs
are present in the source sentence to be trans-
lated. Consequently, we differentiate two kinds
of sentences: -sentences containing at least one
OOV conjugated verb, and - sentences which do
not have any conjugated verb (these latter sen-
tences obviously don’t need any lemmatization!).
Thereby, we decided to build a combined trans-
lation system which call the lemmatized system
only when the source sentence contains at least
one Out-Of-Vocabulary conjugated verb (other-
wise, the sentence will be translated by the stan-
dard system). To detect sentences with Out-Of-
Vocabulary conjugated verb we translate each sen-
tence with both systems (lemmatized and stan-
dard), count OOV and use the lemmatized transla-
tion only if it contains less OOV than the standard
translation. For example, a translation containing
k Out-Of-Vocabulary conjugated verbs and n oth-
ers Out-Of-Vocabulary words (in total k+n OOV)
with the standard system, contains, most probably,
only n Out-Of-Vocabulary words with the lemma-
tised system because the conjugated verbs will be
lemmatized, recognized and translated by the sys-
tem.
</bodyText>
<footnote confidence="0.990109">
1http://wiki.webz.cz/dict/
</footnote>
<page confidence="0.991105">
162
</page>
<subsectionHeader confidence="0.992748">
3.4 Normalization of a special French form
</subsectionHeader>
<bodyText confidence="0.999984631578947">
We observed, in the French source corpra, a spe-
cial French form which generates almost always
Out-Of-Vocabulary words in the English transla-
tion. The special French form, named euphonious
“t”, consists of adding the letter “t” between a verb
(ended by “a”, “e” or “c”) and a personal pronoun
and, then, inverse them in order to facilitate the
prononciation. The sequence is represented by:
verb-t-pronoun like annonca-t-elle, arrive-t-il, a-
t-on, etc. This form concerns 1.75% of the French
sentences in the test corpus whereas these account
for 0.66% and 0.78% respetively in the training
and the developement corpora. The normalized
proposed form, illustrated below in figure 1, con-
tains the subject pronoun (in first posistion) and
the verb (in the second position). This change has
no influence on the French source sentence and ac-
cordingly on the correctness and fluency of the En-
glish translation.
</bodyText>
<subsectionHeader confidence="0.995922">
3.5 Adaptation of the language model
</subsectionHeader>
<bodyText confidence="0.999994066666667">
Finally, for each system, we decided to apply dif-
ferent language models and to look at those who
perfom well. In addition to the 5-gram language
model, we trained and tested 3-gram and 4-gram
language models with two different kinds of vo-
cabularies : - the first one (conventional, refered to
as n-gram in table 3) contains an open-vocabulary
extracted from the monolingual English training
data, and -the second one (refered to as n-gram-
vocab in table 3) contains a closed-vocabulary ex-
tracted from the English part of the bilingual train-
ing data. In both cases, language model probabil-
ities are trained from the monolingual LM train-
ing data but, in the second case, the lexicon is re-
stricted to the one of the phrase-table.
</bodyText>
<sectionHeader confidence="0.99814" genericHeader="method">
4 Experimental results
</sectionHeader>
<bodyText confidence="0.999844272727272">
In the automatic evaluation, the reported evalu-
ation metric is the BLEU score (Papineni et al.,
2002) computed by MTEval version 13a. The re-
sults are reported in table 1. Note that in our ex-
periments, according to the resampling method of
(Koehn, 2004), there are significative variations
(improvement or deterioration), with 95% cer-
tainty, only if the difference between two BLEU
scores represent, at least, 0.33 points. To complete
this automatic evaluation, we performed a human
analysis of the systems outputs.
</bodyText>
<subsectionHeader confidence="0.975594">
4.1 Standard systems
</subsectionHeader>
<subsubsectionHeader confidence="0.764018">
4.1.1 Term expansion with dictionary
</subsubsectionHeader>
<bodyText confidence="0.99932824">
Regarding the results of automatic evaluation (ta-
ble 1, system (2)), adding the dictionary do not
leads to a significant improvement. The OOV
rate and system perplexity are reduced but, ignor-
ing the tuned system which presents lower per-
formance, the BLEU score decreases significatly
on the test set. The BLEU score of the system
augmented with the dictionary is 24.50 whereas
the baseline one is 24.94. So we can conclude
that there is not a meaningfull positive contribu-
tion, probably because the size of the dictionary
is very small regarding the bilingual training cor-
pus. We found out very few Out-Of-Vocabulary
words of the standard system recognized by the
system with the dictionary, see figure 2 for exam-
ple (among them: coupon, cafard, blonde, retar-
dataire, m´edicaments, pamplemousse, etc.). But,
as the dictionnary is very small, most OOV com-
mon words like hˆotesse and clignotant are still un-
known. Regarding the output sentences, we note
that there are very few differences and the quality
is equivalent. The dictionary used is to small to
extend the system’s vocabulary and most of words
still Out-Of-Vocabulary are conjugated verbs and
unrecognized forms.
</bodyText>
<figure confidence="0.92043175">
Baseline system:
A cafard fled before the danger, but if he felt fear?
System with dictionary:
A blues fled before the danger, but if he felt fear?
</figure>
<figureCaption confidence="0.991809">
Figure 2: Example of sentence with an OOV com-
mon noun
</figureCaption>
<subsectionHeader confidence="0.807406">
4.1.2 Normalisation of special French form
</subsectionHeader>
<bodyText confidence="0.999968375">
Considering the BLEU score, the normalization of
French euphonious “t” have, apparently, very few
repercussion on the translation result (table 1, sys-
tem (3)) but the human analysis indicates that, in
our context, the normalisation of euphonious “t”
brings a clear improvement as seen in example 3.
Consequently, this preprocessing is kept in the fi-
nal system.
</bodyText>
<subsectionHeader confidence="0.850913">
4.1.3 Tuning
</subsectionHeader>
<bodyText confidence="0.996779">
We can see in table 1 that the usual tuning with
Minimum Error Rate Training algorithm deterio-
rates systematically performance scores on the test
set, for all systems. This can be explained by the
</bodyText>
<page confidence="0.997148">
163
</page>
<table confidence="0.8925068">
System OOVs ppl Dev score Test score
(1) Baseline 2.32% 207 29.72 (19.93) 23.77 (24.94)
(2) + dictionary 2.30% 204 30.01 (23.92) 24.32 (24.50)
(3) + normalization 2.31% 204 30.07 (19.90) 23.99 (24.98)
(4) + normalization + Dev data 2.30% 204 / (/) / (25,05)
</table>
<tableCaption confidence="0.993977">
Table 1: Standard systems BLEU scores with tuning (without tuning)/ LM 5-gram
</tableCaption>
<figure confidence="0.47026925">
Baseline system:
“It will not work” souligna-t-il afterwards.
System with normalisation:
“It will not work” he stressed afterwards.
</figure>
<figureCaption confidence="0.9779885">
Figure 3: Example of sentence with a “verb-t-
pronoun” form
</figureCaption>
<bodyText confidence="0.999505916666667">
gap between the developement and test corpora (ie
the Dev set may be not representative of the Test
set). So, even if it is recommanded in the standard
process, we do not tune our system (we use the de-
fault weights proposed by the Moses decoder) and
add the developement corpus to train it. In this
case, the training set contains 1,640,468 sentences
(the initial 1,638,440 sentences and the 2,028 sen-
tences of the developement set). This slightly im-
proves the system (from 24.98, the BLEU score
raise to 25,05 after adding the developpement set
to the training).
</bodyText>
<subsectionHeader confidence="0.992669">
4.2 Lemmatised systems
</subsectionHeader>
<bodyText confidence="0.999734787878788">
Results of lemmatised systems are reported on ta-
ble 2. First, we can notice that, in this particular
case, the tuning (with MERT method) is manda-
tory to adapt the weights of the log linear model.
Our analysis of the tuned weight of the lemma-
tised system shows that, in particular, the word
penalty model has a very low weight (this favours
short sentences) and the lexical word-based trans-
lation models have a very low weight (no use of
the lexical translation probability). We also no-
tice that the lemmatization leads to a real drop-off
of OOV rate (fall from 2.32% for the baseline, to
2.23% for the lemmatized system) and perplexity
(fall from 207 for the baseline, to 178 for the lem-
matized system). We can observe a clear decrease
of the performance with the lemmatized system
(BLEU score of 20.50) compared with a non-
lemmatized one (BLEU score of 24.94). This can
be significatively improved applying euphonious
“t” normalization to the source data (BLEU score
of 22.14). Almost all French OOV conjugated
verbs with the standard system were recognized
by the lemmatized one (trierait, joues, testaient,
immerg´ee, ´economiseraient, baisserait, pr´epares,
etc.) but the small decrease of the translation qual-
ity can be explained, among other things, by sev-
eral tense errors. See illustration in figure 4. So,
we conclude that the systematic normalization of
French verbs, as a pre-process, reduce the Out-Of-
Vocabulary conjugated verbs but decrease slighly
the final translation quality. The use of such a sys-
tem is helpfull especially when the sentence con-
tains conjugated verbs (see example 5).
</bodyText>
<subsectionHeader confidence="0.999114">
4.3 Adaptation of the language model
</subsectionHeader>
<bodyText confidence="0.999963647058824">
We applied five differents language models (3-
gram and 4-gram language models with selected
vocabulary or not and a 5-gram language model)
to the four standard systems and the two lemma-
tised one. The results, reported in table 3, show
that BLEU score can be significantly different de-
pending on the language model used. For exam-
ple, the fifth system (5) obtained a BLEU score of
21.48 with a 3-gram language model and a BLEU
score of 22.84 with a 4-gram language model. We
can also notice that five out of our six systems out-
perform using a language model with selected vo-
cabulary (n-gram-vocab). One possible explana-
tion is that with LM using selected vocabulary (n-
gram-vocab), there is no loss of probability mass
for english words not present in the translation ta-
ble.
</bodyText>
<subsectionHeader confidence="0.9996">
4.4 Final combined system
</subsectionHeader>
<bodyText confidence="0.999997">
Considering the previous observations, we believe
that the best choice is to apply the lemmatized
system only if necessary i.e. only if the sentence
contains OOV conjugated verbs, otherwise, a stan-
dard system should be used. We consider system
(4), with 4-gram-vocab language model (selected
vocabulary) without tuning, as the best standard
system and system (6), with 3-gram-vocab lan-
guage model (selected vocabulary) not tuned ei-
ther, as the best lemmatized system. The final
</bodyText>
<page confidence="0.994434">
164
</page>
<table confidence="0.908709666666667">
System OOVs ppl Dev score Test score
(5) lemmatization 2.23% 178 20.97 (8.57) 20.50 (8.56)
(6) lemmatization + normalization 2.18% 175 27.81 (9.20) 22.14 (10.82)
</table>
<tableCaption confidence="0.989219">
Table 2: Lemmatised systems BLEU scores with tuning (without tuning)/ LM 5-gram
</tableCaption>
<note confidence="0.439069">
Baseline system: You will be limited by the absence of exit for headphones.
Lemmatised system: You are limited by the lack of exit for ordinary headphones.
reference: You will be limited by the absence of output on ordinary headphones.
</note>
<figureCaption confidence="0.99956">
Figure 4: Example of sentences without OOV verbs
</figureCaption>
<bodyText confidence="0.987342666666667">
system translations are those of the lemmatized
system (6) when we translate sentences with one
or more Out-Of-Vocabulary conjugated verbs and
those of the un-lemmatized system (4) otherwise.
Around 6% of test set sentences were translated
by the lemmatized system. Considering the results
reported in table 4, the combined system’s BLEU
score is comparable to the standard one (25.11
against 25.17).
</bodyText>
<table confidence="0.59793925">
System Test score sentences
(4) Standard sys. 25.17 94 %
(6) Lemmatised sys. 22.89 6%
(7) Combined 25.11 100 %
</table>
<tableCaption confidence="0.9733475">
Table 4: Combined system’s results and % trans-
lated sentences by each system
</tableCaption>
<sectionHeader confidence="0.847401" genericHeader="method">
5 Human evaluation
</sectionHeader>
<bodyText confidence="0.999923740740741">
We compared two data set. The first set (selected
sent.) contains 301 sentences selected from test
data by the combined system (7) to be translated
by the lemmatized system (6) whereas the second
set (random sent.) contains 301 sentences ran-
domly picked up. The latter is our control data set.
We compared for both groups the translation hy-
pothesis given by the lemmatized system and the
standard one.
We performed a subjective evaluation with the
NIST five points scales to measure fluency and ad-
equacy of each sentences through SECtra w inter-
face (Huynh et al., 2009).We involved a total of 6
volunteers judges (3 for each set). We evaluated
the inter-annotator agreement using a generalized
version of Kappa. The results show a slight to fair
agreement according (Landis, 1977).
The evaluation results, detailled in table 5 and 6,
showed that both fluency and adequacy were im-
proved using our combined system. Indeed, for a
random input (random sent.), the lemmatized sys-
tem lowers the translations quality (fluency and
adequacy are degraded for, respectively, 35.8%
and 37.5% of the sentences), while it improves
the quality for sentences selected by the combined
system (for ”selected sent.”, fluency and adequacy
are improved or stable for 81% of the sentences).
</bodyText>
<figure confidence="0.737581666666667">
Adequacy selected sent. random sent.
(6) &gt; (4) 81% 62.4%
(6) &lt; (4) 18.9% 37.5%
</figure>
<tableCaption confidence="0.860151333333333">
Table 5: Subjective evaluation of sentences ade-
quacy ((6) lemmatized system - (4) standard sys-
tem)
</tableCaption>
<table confidence="0.977037">
Fluency selected sent. random sent.
(6) &gt; (4) 81% 64.1%
(6)&lt;(4) 18.9% 35.8%
</table>
<tableCaption confidence="0.913513333333333">
Table 6: Subjective evaluation of sentences flu-
ency ((6) lemmatized system - (4) standard sys-
tem)
</tableCaption>
<sectionHeader confidence="0.993265" genericHeader="conclusions">
6 Conclusion and Discussion
</sectionHeader>
<bodyText confidence="0.999954916666667">
We have described the system used for our sub-
mission to the WMT’10 shared translation task for
the French-English language pair.
We propose dsome very simple techniques to
improve rapidely a statistical machine translation.
Those techniques particularly aim at handling
Out-Of-Vocabulary words in statistical phrase-
based machine translation and lead an improved
fluency in translation results. The submited sys-
tem (see section 4.4) is a combination between a
standard system and a lemmatized system with ap-
propriate setup.
</bodyText>
<page confidence="0.99606">
165
</page>
<table confidence="0.84782175">
Baseline system: At the end of trade, the stock market in the negative bascula.
Lemmatised system: At the end of trade, the stock market exchange stumbled into the negative.
Baseline system: You can choose conseillera.
Lemmatised system: We would advise you, how to choose.
</table>
<figureCaption confidence="0.652261">
Figure 5: Example of sentences with OOV conjugated verbs
</figureCaption>
<table confidence="0.999862428571429">
System 3-gram 3-gram-vocab 4-gram 4-gram-vocab 5-gram
24.60 24.95 24.94 25.11 24.94
25.14 25.17 24.50 23.49 24.50
24.88 25.00 24.98 25.15 24.98
24.92 24.99 25.05 25.17 25.05
21.48 19.48 22.84 20.18 20.50
22.60 22.89 22.14 22.24 22.14
</table>
<tableCaption confidence="0.999691">
Table 3: Systems’s results on test set with differents language models
</tableCaption>
<bodyText confidence="0.998916">
This system evaluation showed a positive influ-
ence on translation quality, indeed, while the im-
provements on automatic metrics are small, man-
ual inspection suggests a significant improvements
of translation fluency and adequacy.
In future work, we plan to investigate and de-
velop more sophisticated methods to deal with
Out-Of-Vocabulary words, still relying on the an-
alyze of our system output. We believe, for ex-
ample, that an appropriate way to use the dictio-
nary, a sensible pre-processings of French source
texts (in particular normalization of some specific
French forms) and a factorial lemmatization with
the tense information can highly reduce OOV rate
and improve translation quality.
</bodyText>
<sectionHeader confidence="0.999254" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999895528301887">
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
Annual Meeting of the Association for Computa-
tional Linguistics (ACL). Prague, Czech Republic.
Papineni K., Roukos S., Ward T., and Zhu W.J. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. ACL-2002: 40th Annual meeting
of the Association for Computational Linguistics,
pp. 311–318. Philadelphia, Pennsylvania, USA.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. International Conference
on Spoken Language Processing, Vol. 2, pp 901–
904. Denver, Colorado, USA.
Frederic B´echet. 2001. LIA TAGG. http://old.lia.univ
-avignon.fr/chercheurs/bechet/download fred.html.
Franz Josef Och. 2003. Minimum error rate train-
ing for statistical machine translation. Annual Meet-
ing of the Association for Computational Linguistics
(ACL). Sapporo, July.
Philipp Koehn. 2004. Statistical significance tests
for machine translation evaluation. conference on
Empirical Methods in Natural Language Processing
(EMNLP), pp 388–395. Barcelona, Spain.
Marine Carpuat. 2009. Toward Using Morphology in
French-English Phrase-based SMT. Workshop on
Machine Translation in European Association for
Computational Linguistics (EACL-WMT), pp 150–
154. Athens, Greece.
Sonja Niessen and Hermann Ney. 2004. Statistical
Machine Translation with Scarce Resources Using
Morpho-syntactic Information. Computational Lin-
guistics, vol. 30, pp 181–204.
Nizar Habash. 2008. Four techniques for Online
Handling of Out-Of-Vocabulary Words in Arabic-
English Statistical Machine Translation. Human
Language Technology Workshop in Association for
Computational Linguistics, (ACL-HTL), pp 57–60.
Columbus, Ohio, USA.
Landis J. R. and Koch G. G.. 1977. The Measurement
of Observer Agreement for Categorical Data. Bio-
metrics, vol. 33, pp. 159–174.
Herv´e Blanchon, Christian Boitet and Cong-Phap
Huynh. 2009. A Web Service Enabling Grad-
able Post-edition of Pre-translations Produced by
Existing Translation Tools: Practical Use to Provide
High-quality Translation of an Online Encyclopedia.
MT SummitXII, Beyond Translation Memories: New
Tools for Translators Workshop, pp 20–27. Ottawa,
Canada.
</reference>
<page confidence="0.998762">
166
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.300422">
<title confidence="0.640425">The LIG machine translation system for WMT 2010</title>
<author confidence="0.97213">Marion Potet</author>
<author confidence="0.97213">Laurent Besacier</author>
<author confidence="0.97213">Herv´e</author>
<affiliation confidence="0.7881535">LIG Laboratory, GETALP University Joseph Fourier, Grenoble,</affiliation>
<email confidence="0.931438">Herve.Blanchon@imag.fr</email>
<abstract confidence="0.992442777777778">This paper describes the system submitted by the Laboratory of Informatics of Grenoble (LIG) for the fifth Workshop on Statistical Machine Translation. We participated to the news shared translation task for the French-English language pair. We investigated differents techniques to simply deal with Out-Of-Vocabulary words in a statistical phrase-based machine translation system and analyze their impact on translation quality. The final submission is a combination between a standard phrase-based system using the Moses decoder, with appropriate setups and pre-processing, and a lemmatized system to deal with Out-Of-Vocabulary conjugated verbs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation. Annual Meeting of the Association for Computational Linguistics (ACL).</title>
<date>2007</date>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, Evan Herbst.</location>
<contexts>
<context position="3000" citStr="Koehn et al., 2007" startWordPosition="443" endWordPosition="446">anguage modeling The target language model is a standard n-gram language model trained using the SRI language modeling toolkit (Stocke, 2002) on the news monolingual corpus. The smoothing technique we applied is the modified Kneser-Ney discounting with interpolation. 2.3 Translation modeling The translation model was trained using the parallel corpus described earlier (Europarl+News). First, the corpus was word aligned and then, the pairs of source and corresponding target phrases were extracted from the word-aligned bilingual training corpus using the scripts provided with the Moses decoder (Koehn et al., 2007). The result is a phrase-table containing all the aligned phrases. This phrase-table, produced by the translation modeling, is used to extract several translations models. In our experiment we used thirteen standard translation models: six distortion models, a lexicon word-based and a phrase-based translation model for both direction, and a phrase, word and distortion penalty. 161 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 161–166, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics 2.4 Tuning and decoding For </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. Annual Meeting of the Association for Computational Linguistics (ACL). Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>ACL-2002: 40th Annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania, USA.</location>
<contexts>
<context position="9553" citStr="Papineni et al., 2002" startWordPosition="1465" endWordPosition="1468">inds of vocabularies : - the first one (conventional, refered to as n-gram in table 3) contains an open-vocabulary extracted from the monolingual English training data, and -the second one (refered to as n-gramvocab in table 3) contains a closed-vocabulary extracted from the English part of the bilingual training data. In both cases, language model probabilities are trained from the monolingual LM training data but, in the second case, the lexicon is restricted to the one of the phrase-table. 4 Experimental results In the automatic evaluation, the reported evaluation metric is the BLEU score (Papineni et al., 2002) computed by MTEval version 13a. The results are reported in table 1. Note that in our experiments, according to the resampling method of (Koehn, 2004), there are significative variations (improvement or deterioration), with 95% certainty, only if the difference between two BLEU scores represent, at least, 0.33 points. To complete this automatic evaluation, we performed a human analysis of the systems outputs. 4.1 Standard systems 4.1.1 Term expansion with dictionary Regarding the results of automatic evaluation (table 1, system (2)), adding the dictionary do not leads to a significant improve</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni K., Roukos S., Ward T., and Zhu W.J. 2002. BLEU: a method for automatic evaluation of machine translation. ACL-2002: 40th Annual meeting of the Association for Computational Linguistics, pp. 311–318. Philadelphia, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - An Extensible Language Modeling</title>
<date>2002</date>
<booktitle>Toolkit. International Conference on Spoken Language Processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<location>Denver, Colorado, USA.</location>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - An Extensible Language Modeling Toolkit. International Conference on Spoken Language Processing, Vol. 2, pp 901– 904. Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederic B´echet</author>
</authors>
<date>2001</date>
<note>LIA TAGG. http://old.lia.univ -avignon.fr/chercheurs/bechet/download fred.html.</note>
<marker>B´echet, 2001</marker>
<rawString>Frederic B´echet. 2001. LIA TAGG. http://old.lia.univ -avignon.fr/chercheurs/bechet/download fred.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<location>Sapporo,</location>
<contexts>
<context position="3973" citStr="Och, 2003" startWordPosition="593" endWordPosition="594">tortion penalty. 161 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 161–166, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics 2.4 Tuning and decoding For the decoding (i.e. translation of the test set), the system uses a log-linear combination of the previous target language model and the thirteen translation models extracted from the phrasetable. As the system can be beforehand tuned by adjusting log-linear combination weights on a developement corpus, we used the Minimum Error Rate Training (MERT) method, by (Och, 2003). 3 Ways of Improvements 3.1 Discussion about Out-Of-Vocabulary words in PBMT systems Phrase-based statistical machine translation (PBMT) use phrases as units in the translation process. A phrase is a sequence of n consecutive words known by the system. During the training, these phrases are automaticaly learned and each source phrase is mapped with its corresponding target phrase. Throughout test set decoding, a word not being part of this vocabulary list is labeled as “Out-Of-Vocabulary” (OOV) and, as it doesn’t appear in the translation table, the system is unable to translate it. During th</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training for statistical machine translation. Annual Meeting of the Association for Computational Linguistics (ACL). Sapporo, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation. conference on</title>
<date>2004</date>
<booktitle>Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>388--395</pages>
<location>Barcelona,</location>
<contexts>
<context position="9704" citStr="Koehn, 2004" startWordPosition="1494" endWordPosition="1495">g data, and -the second one (refered to as n-gramvocab in table 3) contains a closed-vocabulary extracted from the English part of the bilingual training data. In both cases, language model probabilities are trained from the monolingual LM training data but, in the second case, the lexicon is restricted to the one of the phrase-table. 4 Experimental results In the automatic evaluation, the reported evaluation metric is the BLEU score (Papineni et al., 2002) computed by MTEval version 13a. The results are reported in table 1. Note that in our experiments, according to the resampling method of (Koehn, 2004), there are significative variations (improvement or deterioration), with 95% certainty, only if the difference between two BLEU scores represent, at least, 0.33 points. To complete this automatic evaluation, we performed a human analysis of the systems outputs. 4.1 Standard systems 4.1.1 Term expansion with dictionary Regarding the results of automatic evaluation (table 1, system (2)), adding the dictionary do not leads to a significant improvement. The OOV rate and system perplexity are reduced but, ignoring the tuned system which presents lower performance, the BLEU score decreases signific</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. conference on Empirical Methods in Natural Language Processing (EMNLP), pp 388–395. Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
</authors>
<title>Toward Using Morphology in French-English Phrase-based SMT.</title>
<date>2009</date>
<booktitle>Workshop on Machine Translation in European Association for Computational Linguistics (EACL-WMT),</booktitle>
<pages>150--154</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="1282" citStr="Carpuat, 2009" startWordPosition="182" endWordPosition="183">on quality. The final submission is a combination between a standard phrase-based system using the Moses decoder, with appropriate setups and pre-processing, and a lemmatized system to deal with Out-Of-Vocabulary conjugated verbs. 1 Introduction We participated, for the first time, to the shared news translation task of the fifth Workshop on Machine Translation (WMT 2010) for the FrenchEnglish language pair. The submission was performed using a standard phrase-based translation system with appropriate setups and preprocessings in order to deal with system’s unknown words. Indeed, as shown in (Carpuat, 2009), (Habash, 2008) and (Niessen, 2004), handling Ou-of-Vocabulary words with techniques like lemmatization, phrase table extension or morphological pre-processing is a way to improve translation quality. After a short presentation of our baseline system setups we discuss the effect of Out-Of-Vocabulary words in the system and introduce some ideas we chose to implement. In the last part, we evaluate their impact on translation quality using automatic and human evaluations. 2 Baseline System Setup 2.1 Used Resources We used the provided Europarl and News parallel corpora (total 1,638,440 sentences</context>
</contexts>
<marker>Carpuat, 2009</marker>
<rawString>Marine Carpuat. 2009. Toward Using Morphology in French-English Phrase-based SMT. Workshop on Machine Translation in European Association for Computational Linguistics (EACL-WMT), pp 150– 154. Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Niessen</author>
<author>Hermann Ney</author>
</authors>
<title>Statistical Machine Translation with Scarce Resources Using Morpho-syntactic Information.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<pages>181--204</pages>
<marker>Niessen, Ney, 2004</marker>
<rawString>Sonja Niessen and Hermann Ney. 2004. Statistical Machine Translation with Scarce Resources Using Morpho-syntactic Information. Computational Linguistics, vol. 30, pp 181–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
</authors>
<title>Four techniques for Online Handling of Out-Of-Vocabulary Words</title>
<date>2008</date>
<booktitle>in ArabicEnglish Statistical Machine Translation. Human Language Technology Workshop in Association for Computational Linguistics, (ACL-HTL),</booktitle>
<pages>57--60</pages>
<location>Columbus, Ohio, USA.</location>
<contexts>
<context position="1298" citStr="Habash, 2008" startWordPosition="184" endWordPosition="185">inal submission is a combination between a standard phrase-based system using the Moses decoder, with appropriate setups and pre-processing, and a lemmatized system to deal with Out-Of-Vocabulary conjugated verbs. 1 Introduction We participated, for the first time, to the shared news translation task of the fifth Workshop on Machine Translation (WMT 2010) for the FrenchEnglish language pair. The submission was performed using a standard phrase-based translation system with appropriate setups and preprocessings in order to deal with system’s unknown words. Indeed, as shown in (Carpuat, 2009), (Habash, 2008) and (Niessen, 2004), handling Ou-of-Vocabulary words with techniques like lemmatization, phrase table extension or morphological pre-processing is a way to improve translation quality. After a short presentation of our baseline system setups we discuss the effect of Out-Of-Vocabulary words in the system and introduce some ideas we chose to implement. In the last part, we evaluate their impact on translation quality using automatic and human evaluations. 2 Baseline System Setup 2.1 Used Resources We used the provided Europarl and News parallel corpora (total 1,638,440 sentences) to train the t</context>
</contexts>
<marker>Habash, 2008</marker>
<rawString>Nizar Habash. 2008. Four techniques for Online Handling of Out-Of-Vocabulary Words in ArabicEnglish Statistical Machine Translation. Human Language Technology Workshop in Association for Computational Linguistics, (ACL-HTL), pp 57–60. Columbus, Ohio, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Landis</author>
<author>G G Koch</author>
</authors>
<title>The Measurement of Observer Agreement for Categorical Data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<volume>33</volume>
<pages>159--174</pages>
<marker>Landis, Koch, 1977</marker>
<rawString>Landis J. R. and Koch G. G.. 1977. The Measurement of Observer Agreement for Categorical Data. Biometrics, vol. 33, pp. 159–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herv´e Blanchon</author>
<author>Christian Boitet</author>
<author>Cong-Phap Huynh</author>
</authors>
<title>A Web Service Enabling Gradable Post-edition of Pre-translations Produced by Existing Translation Tools: Practical Use to Provide High-quality Translation of an Online Encyclopedia. MT SummitXII, Beyond Translation Memories: New Tools for Translators Workshop, pp 20–27.</title>
<date>2009</date>
<location>Ottawa, Canada.</location>
<marker>Blanchon, Boitet, Huynh, 2009</marker>
<rawString>Herv´e Blanchon, Christian Boitet and Cong-Phap Huynh. 2009. A Web Service Enabling Gradable Post-edition of Pre-translations Produced by Existing Translation Tools: Practical Use to Provide High-quality Translation of an Online Encyclopedia. MT SummitXII, Beyond Translation Memories: New Tools for Translators Workshop, pp 20–27. Ottawa, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>