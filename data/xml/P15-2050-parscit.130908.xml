<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000882">
<title confidence="0.958563">
Open IE as an Intermediate Structure for Semantic Tasks
</title>
<author confidence="0.997919">
Gabriel Stanovskyt Ido Dagant Mausam§
</author>
<affiliation confidence="0.999759">
t,tDepartment of Computer Science, Bar-Ilan University
§Department of Computer Science &amp; Engg, Indian Institute of Technology, Delhi
</affiliation>
<email confidence="0.926926">
tgabriel.satanovsky@gmail.com
tdagan@cs.biu.ac.il
§mausam@cse.iitd.ac.in
</email>
<sectionHeader confidence="0.993507" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999921666666667">
Semantic applications typically extract in-
formation from intermediate structures de-
rived from sentences, such as dependency
parse or semantic role labeling. In this pa-
per, we study Open Information Extrac-
tion’s (Open IE) output as an additional in-
termediate structure and find that for tasks
such as text comprehension, word similar-
ity and word analogy it can be very effec-
tive. Specifically, for word analogy, Open
IE-based embeddings surpass the state of
the art. We suggest that semantic applica-
tions will likely benefit from adding Open
IE format to their set of potential sentence-
level structures.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999590893617021">
Semantic applications, such as QA or summa-
rization, typically extract sentence features from
a derived intermediate structure. Common in-
termediate structures include: (1) Lexical repre-
sentations, in which features are extracted from
the original word sequence or the bag of words,
(2) Stanford dependency parse trees (De Marneffe
and Manning, 2008), which draw syntactic rela-
tions between words, and (3) Semantic role label-
ing (SRL), which extracts frames linking predi-
cates with their semantic arguments (Carreras and
M`arquez, 2005). For instance, a QA application
can evaluate a question and a candidate answer
by examining their lexical overlap (P´erez-Couti˜no
et al., 2006), by using short dependency paths as
features to compare their syntactic relationships
(Liang et al., 2013), or by using SRL to compare
their predicate-argument structures (Shen and La-
pata, 2007).
In a seemingly independent research direction,
Open Information Extraction (Open IE) extracts
coherent propositions from a sentence, each com-
prising a relation phrase and two or more argument
phrases (Etzioni et al., 2008; Fader et al., 2011;
Mausam et al., 2012). We observe that while Open
IE is primarily used as an end goal in itself (e.g.,
(Fader et al., 2014)), it also makes certain struc-
tural design choices which differ from those made
by dependency or SRL. For example, Open IE
chooses different predicate and argument bound-
aries and assigns different relations between them.
Given the differences between Open IE and
other intermediate structures (see Section 2), a re-
search question arises: Can certain downstream
applications gain additional benefits from utiliz-
ing Open IE structures? To answer this question
we quantitatively evaluate the use of Open IE out-
put against other dominant structures (Sections 3
and 4). For each of text comprehension, word
similarity and word analogy tasks, we choose a
state-of-the-art algorithm in which we can easily
swap the intermediate structure while preserving
the algorithmic computations over the features ex-
tracted from it. We find that in several tasks Open
IE substantially outperforms other structures, sug-
gesting that it can provide an additional set of use-
ful sentence-level features.
</bodyText>
<sectionHeader confidence="0.99601" genericHeader="method">
2 Intermediate Structures
</sectionHeader>
<bodyText confidence="0.999896933333333">
In this section we review how intermediate struc-
tures differ from each other, in terms of their im-
posed structure, predicate and argument bound-
aries, and the type of relations that they introduce.
We include Open IE in this analysis, along with
lexical, dependency and SRL representations, and
highlight its unique properties. As we show in
Section 4, these differences have an impact on the
overall performance of certain downstream appli-
cations.
Lexical representations introduce little or no
structure over the input text. Features for follow-
ing computations are extracted directly from the
original word sequence, e.g., word count statistics
or lexical overlap (see Figure 1a).
</bodyText>
<page confidence="0.975718">
303
</page>
<bodyText confidence="0.972929021739131">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 303–308,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
Syntactic dependencies impose a tree structure
(see Figure 1b), and use words as atomic elements.
This structure implies that predicates are generally
composed of a single word and that arguments are
computed either as single words or as entire spans
of subtrees subordinate to the predicate word.
In SRL (see Figure 1c), several non-connected
frames are extracted from the sentence. The
atomic elements of each frame consist of a single-
word predicate (e.g., the different frames for visit
and refused), and a list of its semantic arguments,
without marking their internal structure. Each ar-
gument is listed along with its semantic relation
(e.g., agent, instrument, etc.) and usually spans
several words.
Open IE (see Figure 1d) also extracts non-
connected propositions, consisting of a predicate
and its arguments. In contrast to SRL, argument
relations are not analyzed, and predicates (as well
as arguments) may consist of several consecu-
tive words. Since Open IE focuses on human-
readability, infinitive constructions (e.g., refused
to visit), and multi-word predicates (e.g., took ad-
vantage) are grouped in a single predicate slot.
Additionally, arguments are truncated in cases
such as prepositional phrases and reduced rela-
tive clauses. The resulting structure can be under-
stood as an extension of shallow syntactic chunk-
ing (Abney, 1992), where chunks are labeled as
either predicates or arguments, and are then inter-
linked to form a complete proposition.
It is not clear apriory whether the differences
manifested in Open IE’s structure could be ben-
eficial as intermediate structures for downstream
applications. Although a few end tasks have made
use of Open IE’s output (Christensen et al., 2013;
Balasubramanian et al., 2013), there has been no
systematic comparison against other structures. In
the following sections, we quantitatively study and
analyze the value of Open IE structures against
the more common intermediate structures – lexi-
cal, dependency and SRL, for three downstream
NLP tasks.
</bodyText>
<sectionHeader confidence="0.983158" genericHeader="method">
3 Tasks and Algorithms
</sectionHeader>
<bodyText confidence="0.919634">
Comparing the effectiveness of intermediate struc-
tures in semantic applications is hard for several
reasons: (1) extracting the underlying structure de-
pends on the accuracy of the specific system used,
(2) the overall performance in the task depends
heavily on the computations carried on top of these
S: John refused to visit a Vegas casino
CA: John visited a Vegas casino
</bodyText>
<listItem confidence="0.883854714285714">
(a) Lexical matching of a 5 words window (marked with a box).
Current window yields a score of 4 - words contributing to the
score are marked in bold.
(b) Dependency matching yields a score of 3. Contributing
triplets are marked in bold.
S: refused0.1: A0: John A1: to visit a Vegas casino
visit0.1: A0: John A1: a Vegas casino
CA: visit0.1: A0: John A1: a Vegas casino
(c) SRL frames matching yields a score of 4, frame elements
contributing to the score marked in bold.
S: (John, refused to visit, a Vegas casino)
CA: (John, visited, a Vegas casino)
(d) Open IE matching yields a score of 2, contributing entries
marked in bold.
</listItem>
<figureCaption confidence="0.678388714285714">
Figure 1: Different intermediate structures used to
compute the modified text comprehension match-
ing score (Section 3), when answering a question
”Where did John visit?”, given an input sentence
S: ”John refused to visit a Vegas casino”, and a
wrong candidate answer CA: ”John visited a Ve-
gas casino”.
</figureCaption>
<bodyText confidence="0.999956666666667">
structures, and (3) different structures may be suit-
able for different tasks. To mitigate these com-
plications, and comparatively evaluate the effec-
tiveness of different types of structures, we choose
three semantic tasks along with state-of-the-art al-
gorithms that make a clear separation between fea-
ture extraction and subsequent computation. We
then compare performance by using features from
four intermediate structures – lexical, dependency,
SRL and Open IE. Each of these is extracted using
state-of-the-art systems. Thus, while our compar-
isons are valid only for the tested tasks and sys-
tems, they do provide valuable evidence for the
general question of effective intermediate struc-
tures.
</bodyText>
<subsectionHeader confidence="0.997019">
3.1 Text Comprehension Task
</subsectionHeader>
<bodyText confidence="0.9992035">
Text comprehension tasks extrinsically test natural
language understanding through question answer-
</bodyText>
<page confidence="0.995333">
304
</page>
<figure confidence="0.5451344">
Target Lexical Dependency SRL Open IE
John nsubj John A0 John 0 John
to xcomp visit A1 to 1 to
refused visit A1 visit 1 visit
Vegas A1 Vegas 2 Vegas
</figure>
<tableCaption confidence="0.909316">
Table 1: Some of the different contexts for the tar-
</tableCaption>
<bodyText confidence="0.995404384615385">
get word “refused” in the sentence ”John refused
to visit Vegas”. SRL and Open IE contexts are pre-
ceded by their element (predicate or argument) in-
dex. See figure 1 for the different representations
of this sentence.
ing. We use the MCTest corpus (Richardson et
al., 2013), which is composed of short stories fol-
lowed by multiple choice questions. The MCTest
task does not require extensive world knowledge,
which makes it ideal for testing underlying sen-
tence representations, as performance will mostly
depend on accuracy and informativeness of the ex-
tracted structures.
We adapt the unsupervised lexical matching
algorithm from the original MCTest paper. It
counts lexical matches between an assertion ob-
tained from a candidate answer (CA) and a sliding
window over the story. The selected answer is the
one for which the maximum number of matches
are found. Our adaptation changes the algorithm
to compute a modified matching score by counting
matches between structure units. The correspond-
ing units are either dependency edges, SRL frame
elements or Open IE tuple elements. Figure 1 il-
lustrates computations for a sentence - candidate
answer pair.
</bodyText>
<subsectionHeader confidence="0.998218">
3.2 Similarity and Analogy Tasks
</subsectionHeader>
<bodyText confidence="0.999986411764706">
Word similarity tasks deal with assessing the de-
gree of ”similarity” between two input words. Tur-
ney (2012) classifies two types of similarity: (1)
domain similarity, e.g., carpenter is similar to
wood, hammer, and nail, (2) functional similarity,
in which carpenter will be similar to other profes-
sions, e.g., shoemaker, brewer, miner etc. Several
evaluation test sets exist for this task, each target-
ing a slightly different aspect of similarity. While
Bruni (2012), Luong (2013), Radinsky (2011),
and ws353 (Finkelstein et al., 2001) can be largely
categorized as targeting domain similarity, sim-
lex999 (Hill et al., 2014) specifically targets func-
tional aspects of similarity (e.g., coast will be sim-
ilar to shore, while closet will not be similar to
clothes). A related task is word analogy, in which
systems take three input words (A:A*, B:?) and
output a word B*, such that the relation between
B and B* is closest to the relation between A and
A*. For instance, queen is the desired answer for
the triple (man:king, woman:?).
Some recent state-of-the-art approaches to these
two tasks derive a similarity score via arithmetic
computations on word embeddings (Mikolov et
al., 2013b). While original training of word em-
beddings used lexical contexts (n-grams), recently
Levy and Goldberg (2014) generalized this to ar-
bitrary contexts, such as dependency paths. We
use their software1 and recompute the word em-
beddings using contexts from our four structures:
lexical context, dependency paths, SRL’s seman-
tic relations, and Open IE’s surrounding tuple ele-
ments. Table 1 shows the different contexts for a
sample word.
</bodyText>
<sectionHeader confidence="0.999436" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9999486">
In our experiments we use MaltParser (Nivre et
al., 2007) for dependency parsing, and ClearNLP
(Choi and Palmer, 2011) for SRL.
To obtain Open-IE structures, we use the re-
cent Open IE-4 system2 which produces n-ary ex-
tractions of both verb-based relation phrases using
SRLIE (an improvement over (Christensen et al.,
2011)) and nominal relations using regular expres-
sions. SRLIE first processes sentences using SRL
and then uses hand-coded rules to convert SRL
frames and associated dependency parses to open
extractions.
We choose these tools as they are on par with
state-of-the-art in their respective fields, and there-
fore represent the current available off-the-shelf
intermediate structures for semantic applications.
Furthermore, Open IE-4 is based on ClearNLP’s
SRL, allowing for a direct comparison. For SRL
systems, we take argument boundaries as their
complete parse subtrees.3
</bodyText>
<sectionHeader confidence="0.594108" genericHeader="evaluation">
Results on Text Comprehension Task We re-
</sectionHeader>
<bodyText confidence="0.9995505">
port results (in percentage of correct answers) on
the whole of MC500 dataset (ignoring train-dev-
test split) since all our methods are unsupervised.
Figure 2 shows the accuracies obtained on the
multiple-choice questions, categorized by single
(the question can be answered based on a sin-
</bodyText>
<footnote confidence="0.99865375">
1https://bitbucket.org/yoavgo/word2vecf
2http://knowitall.github.io/openie/
3We tried an alternative approach which takes only the
heads as arguments, but that performed much worse.
</footnote>
<page confidence="0.98262">
305
</page>
<table confidence="0.999909375">
Open IE Lexical Deps SRL
bruni .757 .735 .618 .491
luong .288 .229 .197 .171
radinsky .681 .674 .592 .433
simlex .39 .365 .447 .306
ws353-rel .647 .64 .492 .551
ws353-sym .77 .763 .759 .439
ws353-full .711 .703 .629 .693
</table>
<tableCaption confidence="0.953238">
Table 2: Performance in word similarity tasks
(Spearman’s ρ)
</tableCaption>
<table confidence="0.9996995">
Google MSR
Add Mul Add Mul
Open IE .714 .719 .529 .55
Lexical .651 .656 .438 .455
Deps .34 .367 .4 .434
SRL .352 .362 .389 .406
</table>
<tableCaption confidence="0.930121">
Table 3: Performance in word analogy tasks (per-
centage of correct answers)
</tableCaption>
<bodyText confidence="0.999476333333334">
gle story sentence) , multiple (multiple sentences
needed) and all (single + multiple).4
In this task, we find that Open IE and depen-
dency edges substantially outperform lexical and
SRL. We conjecture that SRL’s weak performance
is due to its treatment of infinitives and multi-word
predicates as different propositions (see Section
2). This adds noise by wrongly counting partial
matching between predications, as exemplified in
Figure 1c. The gain over the lexical approach
can be explained by the ability to capture longer
range relations than the fixed size window.5 In
our results Open IE slightly improves over depen-
dency. This can be traced back to the different
structural choices depicted in Section 2 – Open
IE counts matches at the proposition level while
the dependency variant may count path matches
over unrelated sentence parts. The differences be-
tween the performance of Open IE and all other
systems were found to be statistically significant
(p &lt; 0.01).
</bodyText>
<subsectionHeader confidence="0.774086">
Results on Similarity and Analogy Tasks For
</subsectionHeader>
<bodyText confidence="0.58304">
these tasks, we train the various word embeddings
</bodyText>
<footnote confidence="0.998879">
4As expected, all sentence-level intermediate structures
perform best on the single partition, yet results show that
some of the questions from the multiple partition may also be
answered correctly using information from a single sentence.
5We experimented with various window sizes and found
that window size of the length of the current candidate-
answer performed best.
</footnote>
<bodyText confidence="0.999806588235294">
on a Wikipedia dump (August 2013 dump), con-
taining 77.5M sentences and 1.5B tokens. We
used the default hyperparameters from Levy and
Goldberg (2014): 300 dimensions, skip gram with
negative sampling of size 5. Lexical embeddings
were trained with 5-gram contexts. Performance
is measured using Spearman’s ρ, in order to assess
the correlation of the predictions to the gold anno-
tations, rather than comparing their values directly.
Table 2 compares the results on the word similar-
ity task using cosine similarity between embed-
dings as the similarity predictor. For the ws353
test set we report results on the whole corpus (full)
as well as on the partition suggested by (Agirre
et al., 2009) into relatedness (mainly meronym-
holonym) and similarity (synonyms, antonyms, or
hyponym-hypernym).
We find that Open IE-based embeddings consis-
tently do well; performing best across all test sets,
except for simlex999. Analysis reveals that Open
IE’s ability to represent multi-word predicates and
arguments allows it to naturally incorporate both
notions of similarity. Context words originating
from the same Open IE slot (either predicate or ar-
gument) are lexically close and indicate domain-
similarity, whereas context words from other ele-
ments in the tuple express semantic relationships,
and target functional similarity.
Thus, Open IE performs better on word-pairs
which exhibit both topical and functional similar-
ity, such as (latinist, classicist), or (provincialism,
narrow-mindedness), which were taken from the
Luong test set. Table 4 further illustrates this dual
capturing of both types of similarity in Open IE
space.
Our results also reiterate previous findings –
lexical contexts do well on domain-similarity test
sets (Mikolov et al., 2013b). The results on the
simlex999 test set can be explained by its focus
on functional similarity, previously identified as
better captured by dependency contexts (Levy and
Goldberg, 2014).
For the Word analogy task we use the Google
(Mikolov et al., 2013a) and the Microsoft cor-
pora (Mikolov et al., 2013b), which are composed
of ∼ 195K and 8K instances respectively. We
obtain the analogy vectors using both the addi-
tive and multiplicative measures (Mikolov et al.,
2013b; Levy and Goldberg, 2014). Table 3 shows
the results – Open IE obtains the best accuracies
by vast margins (p &lt; 0.01), for reasons simi-
</bodyText>
<page confidence="0.998283">
306
</page>
<figureCaption confidence="0.9964375">
Figure 2: Performance in MCTest (percentage of
correct answers).
</figureCaption>
<bodyText confidence="0.999939076923077">
lar to the word similarity tasks. To our knowl-
edge, Open IE results on both analogy datasets
surpass the state of the art. An example (from
the Microsoft test set) which supports the observa-
tion regarding Open IE embeddings space is (gen-
tlest:gentler, loudest:?), for which only Open IE
answers correctly as louder, while lexical respond
with higher-pitched (domain similar to loudest),
and dependency with thinnest (functionally sim-
ilar to loudest). Our Open-IE embeddings are
freely available6 and we note that these can serve
as plug-in features for other NLP applications, as
demonstrated in (Turian et al., 2010).
</bodyText>
<sectionHeader confidence="0.996565" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999950444444444">
We studied Open IE’s output compared with other
dominant structures, highlighting their main dif-
ferences. We then conduct experiments and anal-
ysis suggesting that these structural differences
prove beneficial for certain downstream semantic
applications. A key strength is Open IE’s ability to
balance lexical proximity with long range depen-
dencies in a single representation. Specifically, for
the word analogy task, Open IE-based embeddings
</bodyText>
<footnote confidence="0.683171">
6http://www.cs.bgu.ac.il/˜gabriels
</footnote>
<subsectionHeader confidence="0.806439">
Target Word Lexical Dependency Open IE
</subsectionHeader>
<bodyText confidence="0.797558692307692">
dog feline dog
incisor bovine carnassial
canine dentition equine feline
parvovirus porcine fang-like
dysplasia murine bovine
Table 4: Closest words to canine in various word
embeddings. Illustrating domain similarity (Lex-
ical), functional similarity (Dependency), and a
mixture of both (Open IE).
surpass all prior results. We conclude that an NLP
practitioner will likely benefit from adding Open
IE to their toolkit of potential sentence representa-
tions.
</bodyText>
<sectionHeader confidence="0.977436" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9875115">
This work was partially supported by the Israel
Science Foundation grant 880/12, the German
Research Foundation through the German-Israeli
Project Cooperation (DIP, grant DA 1600/1-1),
a Google Research Award to Ido Dagan, and
Google’s Language Understanding and Knowl-
edge Discovery Focused Research Award to
Mausam.
</bodyText>
<sectionHeader confidence="0.991115" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.990723405405405">
Steven P Abney. 1992. Parsing by chunks. Principle-
based parsing, pages 257–278.
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 19–27. Association for Computational Lin-
guistics.
Niranjan Balasubramanian, Stephen Soderland,
Mausam, and Oren Etzioni. 2013. Generating
coherent event schemas at scale. In Conference
on Empirical Methods in Natural Language
Processing, pages 1721–1731.
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012. Distributional semantics in tech-
nicolor. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 136–145. Asso-
ciation for Computational Linguistics.
Xavier Carreras and Llu´ıs M`arquez. 2005. Introduc-
tion to the conll-2005 shared task: Semantic role
labeling. In Proceedings of The SIGNLL Confer-
ence on Computational Natural Language Learning
(CoNLL), pages 152–164.
Jinho D Choi and Martha Palmer. 2011. Transition-
based semantic role labeling using predicate argu-
ment clustering. In Proceedings of the ACL 2011
Workshop on Relational Models of Semantics, pages
37–45. Association for Computational Linguistics.
Janara Christensen, Mausam, Stephen Soderland, and
Oren Etzioni. 2011. An analysis of open informa-
tion extraction based on semantic role labeling. In
Proceedings of the 6th International Conference on
Knowledge Capture (K-CAP ’11).
</reference>
<page confidence="0.992578">
307
</page>
<reference confidence="0.998785433962264">
Janara Christensen, Mausam, Stephen Soderland, and
Oren Etzioni. 2013. Towards coherent multi-
document summarization. In Conference of the
North American Chapter of the Association for
Computational Linguistics Human Language Tech-
nologies, pages 1163–1173.
Marie-Catherine De Marneffe and Christopher D Man-
ning. 2008. The stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation, pages 1–8.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S Weld. 2008. Open information extrac-
tion from the Web. Communications of the ACM,
51(12):68–74.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 1535–1545. Association for Computational
Linguistics.
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2014. Open question answering over curated and
extracted knowledge bases. In The 20th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ’14, New York,
NY, USA -August 24 - 27, 2014, pages 1156–1165.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: The
concept revisited. In Proceedings of the 10th inter-
national conference on World Wide Web, pages 406–
414. ACM.
Felix Hill, Roi Reichart, and Anna Korhonen. 2014.
Simlex-999: Evaluating semantic models with
(genuine) similarity estimation. arXiv preprint
arXiv:1408.3456.
Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics, volume 2.
Percy Liang, Michael I. Jordan, and Dan Klein. 2013.
Learning dependency-based compositional seman-
tics. Computational Linguistics, 39(2):389–446.
Minh-Thang Luong, Richard Socher, and Christo-
pher D. Manning. 2013. Better word representa-
tions with recursive neural networks for morphol-
ogy. The SIGNLL Conference on Computational
Natural Language Learning (CoNLL), 104.
Mausam, Michael Schmitz, Stephen Soderland, Robert
Bart, and Oren Etzioni. 2012. Open language learn-
ing for information extraction. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 523–534, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word rep-
resentations in vector space. In Workshop at The
International Conference on Learning Representa-
tions (ICLR).
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Conference of the North
American Chapter of the Association for Computa-
tional Linguistics Human Language Technologies,
pages 746–751.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(02):95–135.
Manuel P´erez-Couti˜no, Manuel Montes-y G´omez, Au-
relio L´opez-L´opez, and Luis Villase˜nor-Pineda.
2006. The role of lexical features in Question An-
swering for Spanish. Springer.
Kira Radinsky, Eugene Agichtein, Evgeniy
Gabrilovich, and Shaul Markovitch. 2011. A
word at a time: computing word relatedness using
temporal semantic analysis. In Proceedings of the
20th international conference on World wide web,
pages 337–346. ACM.
Matthew Richardson, Christopher JC Burges, and Erin
Renshaw. 2013. Mctest: A challenge dataset for
the open-domain machine comprehension of text. In
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 193–203.
Dan Shen and Mirella Lapata. 2007. Using semantic
roles to improve question answering. In EMNLP-
CoNLL 2007, Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, June 28-30, 2007, Prague, Czech Repub-
lic, pages 12–21.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th annual meeting of the association for compu-
tational linguistics, pages 384–394. Association for
Computational Linguistics.
Peter D Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533–
585.
</reference>
<page confidence="0.998541">
308
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.886580">
<title confidence="0.994606">Open IE as an Intermediate Structure for Semantic Tasks</title>
<author confidence="0.95449">of Computer Science</author>
<author confidence="0.95449">Bar-Ilan</author>
<affiliation confidence="0.98097">of Computer Science &amp; Engg, Indian Institute of Technology,</affiliation>
<abstract confidence="0.9961315625">Semantic applications typically extract information from intermediate structures derived from sentences, such as dependency parse or semantic role labeling. In this paper, we study Open Information Extraction’s (Open IE) output as an additional intermediate structure and find that for tasks such as text comprehension, word similarity and word analogy it can be very effective. Specifically, for word analogy, Open IE-based embeddings surpass the state of the art. We suggest that semantic applications will likely benefit from adding Open IE format to their set of potential sentencelevel structures.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven P Abney</author>
</authors>
<title>Parsing by chunks. Principlebased parsing,</title>
<date>1992</date>
<pages>257--278</pages>
<contexts>
<context position="5480" citStr="Abney, 1992" startWordPosition="828" endWordPosition="829">so extracts nonconnected propositions, consisting of a predicate and its arguments. In contrast to SRL, argument relations are not analyzed, and predicates (as well as arguments) may consist of several consecutive words. Since Open IE focuses on humanreadability, infinitive constructions (e.g., refused to visit), and multi-word predicates (e.g., took advantage) are grouped in a single predicate slot. Additionally, arguments are truncated in cases such as prepositional phrases and reduced relative clauses. The resulting structure can be understood as an extension of shallow syntactic chunking (Abney, 1992), where chunks are labeled as either predicates or arguments, and are then interlinked to form a complete proposition. It is not clear apriory whether the differences manifested in Open IE’s structure could be beneficial as intermediate structures for downstream applications. Although a few end tasks have made use of Open IE’s output (Christensen et al., 2013; Balasubramanian et al., 2013), there has been no systematic comparison against other structures. In the following sections, we quantitatively study and analyze the value of Open IE structures against the more common intermediate structur</context>
</contexts>
<marker>Abney, 1992</marker>
<rawString>Steven P Abney. 1992. Parsing by chunks. Principlebased parsing, pages 257–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Enrique Alfonseca</author>
<author>Keith Hall</author>
<author>Jana Kravalova</author>
<author>Marius Pas¸ca</author>
<author>Aitor Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and wordnet-based approaches.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>pages</pages>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pas¸ca, Soroa, 2009</marker>
<rawString>Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and wordnet-based approaches. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 19–27. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niranjan Balasubramanian</author>
<author>Stephen Soderland</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Generating coherent event schemas at scale.</title>
<date>2013</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1721--1731</pages>
<contexts>
<context position="5872" citStr="Balasubramanian et al., 2013" startWordPosition="888" endWordPosition="891"> in a single predicate slot. Additionally, arguments are truncated in cases such as prepositional phrases and reduced relative clauses. The resulting structure can be understood as an extension of shallow syntactic chunking (Abney, 1992), where chunks are labeled as either predicates or arguments, and are then interlinked to form a complete proposition. It is not clear apriory whether the differences manifested in Open IE’s structure could be beneficial as intermediate structures for downstream applications. Although a few end tasks have made use of Open IE’s output (Christensen et al., 2013; Balasubramanian et al., 2013), there has been no systematic comparison against other structures. In the following sections, we quantitatively study and analyze the value of Open IE structures against the more common intermediate structures – lexical, dependency and SRL, for three downstream NLP tasks. 3 Tasks and Algorithms Comparing the effectiveness of intermediate structures in semantic applications is hard for several reasons: (1) extracting the underlying structure depends on the accuracy of the specific system used, (2) the overall performance in the task depends heavily on the computations carried on top of these S</context>
</contexts>
<marker>Balasubramanian, Soderland, Mausam, Etzioni, 2013</marker>
<rawString>Niranjan Balasubramanian, Stephen Soderland, Mausam, and Oren Etzioni. 2013. Generating coherent event schemas at scale. In Conference on Empirical Methods in Natural Language Processing, pages 1721–1731.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Gemma Boleda</author>
<author>Marco Baroni</author>
<author>NamKhanh Tran</author>
</authors>
<title>Distributional semantics in technicolor.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>136--145</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Bruni, Boleda, Baroni, Tran, 2012</marker>
<rawString>Elia Bruni, Gemma Boleda, Marco Baroni, and NamKhanh Tran. 2012. Distributional semantics in technicolor. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 136–145. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Introduction to the conll-2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of The SIGNLL Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>152--164</pages>
<marker>Carreras, M`arquez, 2005</marker>
<rawString>Xavier Carreras and Llu´ıs M`arquez. 2005. Introduction to the conll-2005 shared task: Semantic role labeling. In Proceedings of The SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 152–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinho D Choi</author>
<author>Martha Palmer</author>
</authors>
<title>Transitionbased semantic role labeling using predicate argument clustering.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL 2011 Workshop on Relational Models of Semantics,</booktitle>
<pages>37--45</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11445" citStr="Choi and Palmer, 2011" startWordPosition="1788" endWordPosition="1791"> computations on word embeddings (Mikolov et al., 2013b). While original training of word embeddings used lexical contexts (n-grams), recently Levy and Goldberg (2014) generalized this to arbitrary contexts, such as dependency paths. We use their software1 and recompute the word embeddings using contexts from our four structures: lexical context, dependency paths, SRL’s semantic relations, and Open IE’s surrounding tuple elements. Table 1 shows the different contexts for a sample word. 4 Evaluation In our experiments we use MaltParser (Nivre et al., 2007) for dependency parsing, and ClearNLP (Choi and Palmer, 2011) for SRL. To obtain Open-IE structures, we use the recent Open IE-4 system2 which produces n-ary extractions of both verb-based relation phrases using SRLIE (an improvement over (Christensen et al., 2011)) and nominal relations using regular expressions. SRLIE first processes sentences using SRL and then uses hand-coded rules to convert SRL frames and associated dependency parses to open extractions. We choose these tools as they are on par with state-of-the-art in their respective fields, and therefore represent the current available off-the-shelf intermediate structures for semantic applicat</context>
</contexts>
<marker>Choi, Palmer, 2011</marker>
<rawString>Jinho D Choi and Martha Palmer. 2011. Transitionbased semantic role labeling using predicate argument clustering. In Proceedings of the ACL 2011 Workshop on Relational Models of Semantics, pages 37–45. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janara Christensen</author>
<author>Stephen Soderland Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>An analysis of open information extraction based on semantic role labeling.</title>
<date>2011</date>
<booktitle>In Proceedings of the 6th International Conference on Knowledge Capture (K-CAP ’11).</booktitle>
<contexts>
<context position="11649" citStr="Christensen et al., 2011" startWordPosition="1821" endWordPosition="1824">texts, such as dependency paths. We use their software1 and recompute the word embeddings using contexts from our four structures: lexical context, dependency paths, SRL’s semantic relations, and Open IE’s surrounding tuple elements. Table 1 shows the different contexts for a sample word. 4 Evaluation In our experiments we use MaltParser (Nivre et al., 2007) for dependency parsing, and ClearNLP (Choi and Palmer, 2011) for SRL. To obtain Open-IE structures, we use the recent Open IE-4 system2 which produces n-ary extractions of both verb-based relation phrases using SRLIE (an improvement over (Christensen et al., 2011)) and nominal relations using regular expressions. SRLIE first processes sentences using SRL and then uses hand-coded rules to convert SRL frames and associated dependency parses to open extractions. We choose these tools as they are on par with state-of-the-art in their respective fields, and therefore represent the current available off-the-shelf intermediate structures for semantic applications. Furthermore, Open IE-4 is based on ClearNLP’s SRL, allowing for a direct comparison. For SRL systems, we take argument boundaries as their complete parse subtrees.3 Results on Text Comprehension Tas</context>
</contexts>
<marker>Christensen, Mausam, Etzioni, 2011</marker>
<rawString>Janara Christensen, Mausam, Stephen Soderland, and Oren Etzioni. 2011. An analysis of open information extraction based on semantic role labeling. In Proceedings of the 6th International Conference on Knowledge Capture (K-CAP ’11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janara Christensen</author>
<author>Stephen Soderland Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Towards coherent multidocument summarization.</title>
<date>2013</date>
<booktitle>In Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies,</booktitle>
<pages>1163--1173</pages>
<contexts>
<context position="5841" citStr="Christensen et al., 2013" startWordPosition="884" endWordPosition="887">ook advantage) are grouped in a single predicate slot. Additionally, arguments are truncated in cases such as prepositional phrases and reduced relative clauses. The resulting structure can be understood as an extension of shallow syntactic chunking (Abney, 1992), where chunks are labeled as either predicates or arguments, and are then interlinked to form a complete proposition. It is not clear apriory whether the differences manifested in Open IE’s structure could be beneficial as intermediate structures for downstream applications. Although a few end tasks have made use of Open IE’s output (Christensen et al., 2013; Balasubramanian et al., 2013), there has been no systematic comparison against other structures. In the following sections, we quantitatively study and analyze the value of Open IE structures against the more common intermediate structures – lexical, dependency and SRL, for three downstream NLP tasks. 3 Tasks and Algorithms Comparing the effectiveness of intermediate structures in semantic applications is hard for several reasons: (1) extracting the underlying structure depends on the accuracy of the specific system used, (2) the overall performance in the task depends heavily on the computa</context>
</contexts>
<marker>Christensen, Mausam, Etzioni, 2013</marker>
<rawString>Janara Christensen, Mausam, Stephen Soderland, and Oren Etzioni. 2013. Towards coherent multidocument summarization. In Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies, pages 1163–1173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The stanford typed dependencies representation.</title>
<date>2008</date>
<booktitle>In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation,</booktitle>
<pages>1--8</pages>
<marker>De Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine De Marneffe and Christopher D Manning. 2008. The stanford typed dependencies representation. In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michele Banko</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
</authors>
<title>Open information extraction from the Web.</title>
<date>2008</date>
<journal>Communications of the ACM,</journal>
<volume>51</volume>
<issue>12</issue>
<contexts>
<context position="2024" citStr="Etzioni et al., 2008" startWordPosition="293" endWordPosition="296">th their semantic arguments (Carreras and M`arquez, 2005). For instance, a QA application can evaluate a question and a candidate answer by examining their lexical overlap (P´erez-Couti˜no et al., 2006), by using short dependency paths as features to compare their syntactic relationships (Liang et al., 2013), or by using SRL to compare their predicate-argument structures (Shen and Lapata, 2007). In a seemingly independent research direction, Open Information Extraction (Open IE) extracts coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases (Etzioni et al., 2008; Fader et al., 2011; Mausam et al., 2012). We observe that while Open IE is primarily used as an end goal in itself (e.g., (Fader et al., 2014)), it also makes certain structural design choices which differ from those made by dependency or SRL. For example, Open IE chooses different predicate and argument boundaries and assigns different relations between them. Given the differences between Open IE and other intermediate structures (see Section 2), a research question arises: Can certain downstream applications gain additional benefits from utilizing Open IE structures? To answer this questio</context>
</contexts>
<marker>Etzioni, Banko, Soderland, Weld, 2008</marker>
<rawString>Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S Weld. 2008. Open information extraction from the Web. Communications of the ACM, 51(12):68–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Identifying relations for open information extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1535--1545</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2044" citStr="Fader et al., 2011" startWordPosition="297" endWordPosition="300">ments (Carreras and M`arquez, 2005). For instance, a QA application can evaluate a question and a candidate answer by examining their lexical overlap (P´erez-Couti˜no et al., 2006), by using short dependency paths as features to compare their syntactic relationships (Liang et al., 2013), or by using SRL to compare their predicate-argument structures (Shen and Lapata, 2007). In a seemingly independent research direction, Open Information Extraction (Open IE) extracts coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases (Etzioni et al., 2008; Fader et al., 2011; Mausam et al., 2012). We observe that while Open IE is primarily used as an end goal in itself (e.g., (Fader et al., 2014)), it also makes certain structural design choices which differ from those made by dependency or SRL. For example, Open IE chooses different predicate and argument boundaries and assigns different relations between them. Given the differences between Open IE and other intermediate structures (see Section 2), a research question arises: Can certain downstream applications gain additional benefits from utilizing Open IE structures? To answer this question we quantitatively </context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1535–1545. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Luke Zettlemoyer</author>
<author>Oren Etzioni</author>
</authors>
<title>Open question answering over curated and extracted knowledge bases.</title>
<date>2014</date>
<booktitle>In The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’14,</booktitle>
<volume>24</volume>
<pages>1156--1165</pages>
<location>New York, NY, USA</location>
<contexts>
<context position="2168" citStr="Fader et al., 2014" startWordPosition="321" endWordPosition="324">ning their lexical overlap (P´erez-Couti˜no et al., 2006), by using short dependency paths as features to compare their syntactic relationships (Liang et al., 2013), or by using SRL to compare their predicate-argument structures (Shen and Lapata, 2007). In a seemingly independent research direction, Open Information Extraction (Open IE) extracts coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases (Etzioni et al., 2008; Fader et al., 2011; Mausam et al., 2012). We observe that while Open IE is primarily used as an end goal in itself (e.g., (Fader et al., 2014)), it also makes certain structural design choices which differ from those made by dependency or SRL. For example, Open IE chooses different predicate and argument boundaries and assigns different relations between them. Given the differences between Open IE and other intermediate structures (see Section 2), a research question arises: Can certain downstream applications gain additional benefits from utilizing Open IE structures? To answer this question we quantitatively evaluate the use of Open IE output against other dominant structures (Sections 3 and 4). For each of text comprehension, wor</context>
</contexts>
<marker>Fader, Zettlemoyer, Etzioni, 2014</marker>
<rawString>Anthony Fader, Luke Zettlemoyer, and Oren Etzioni. 2014. Open question answering over curated and extracted knowledge bases. In The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’14, New York, NY, USA -August 24 - 27, 2014, pages 1156–1165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2001</date>
<booktitle>In Proceedings of the 10th international conference on World Wide Web,</booktitle>
<pages>406--414</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="10226" citStr="Finkelstein et al., 2001" startWordPosition="1592" endWordPosition="1595">es computations for a sentence - candidate answer pair. 3.2 Similarity and Analogy Tasks Word similarity tasks deal with assessing the degree of ”similarity” between two input words. Turney (2012) classifies two types of similarity: (1) domain similarity, e.g., carpenter is similar to wood, hammer, and nail, (2) functional similarity, in which carpenter will be similar to other professions, e.g., shoemaker, brewer, miner etc. Several evaluation test sets exist for this task, each targeting a slightly different aspect of similarity. While Bruni (2012), Luong (2013), Radinsky (2011), and ws353 (Finkelstein et al., 2001) can be largely categorized as targeting domain similarity, simlex999 (Hill et al., 2014) specifically targets functional aspects of similarity (e.g., coast will be similar to shore, while closet will not be similar to clothes). A related task is word analogy, in which systems take three input words (A:A*, B:?) and output a word B*, such that the relation between B and B* is closest to the relation between A and A*. For instance, queen is the desired answer for the triple (man:king, woman:?). Some recent state-of-the-art approaches to these two tasks derive a similarity score via arithmetic co</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2001</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2001. Placing search in context: The concept revisited. In Proceedings of the 10th international conference on World Wide Web, pages 406– 414. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Hill</author>
<author>Roi Reichart</author>
<author>Anna Korhonen</author>
</authors>
<title>Simlex-999: Evaluating semantic models with (genuine) similarity estimation. arXiv preprint arXiv:1408.3456.</title>
<date>2014</date>
<contexts>
<context position="10315" citStr="Hill et al., 2014" startWordPosition="1606" endWordPosition="1609">milarity tasks deal with assessing the degree of ”similarity” between two input words. Turney (2012) classifies two types of similarity: (1) domain similarity, e.g., carpenter is similar to wood, hammer, and nail, (2) functional similarity, in which carpenter will be similar to other professions, e.g., shoemaker, brewer, miner etc. Several evaluation test sets exist for this task, each targeting a slightly different aspect of similarity. While Bruni (2012), Luong (2013), Radinsky (2011), and ws353 (Finkelstein et al., 2001) can be largely categorized as targeting domain similarity, simlex999 (Hill et al., 2014) specifically targets functional aspects of similarity (e.g., coast will be similar to shore, while closet will not be similar to clothes). A related task is word analogy, in which systems take three input words (A:A*, B:?) and output a word B*, such that the relation between B and B* is closest to the relation between A and A*. For instance, queen is the desired answer for the triple (man:king, woman:?). Some recent state-of-the-art approaches to these two tasks derive a similarity score via arithmetic computations on word embeddings (Mikolov et al., 2013b). While original training of word em</context>
</contexts>
<marker>Hill, Reichart, Korhonen, 2014</marker>
<rawString>Felix Hill, Roi Reichart, and Anna Korhonen. 2014. Simlex-999: Evaluating semantic models with (genuine) similarity estimation. arXiv preprint arXiv:1408.3456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Dependencybased word embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>2</volume>
<contexts>
<context position="10990" citStr="Levy and Goldberg (2014)" startWordPosition="1716" endWordPosition="1719">ity (e.g., coast will be similar to shore, while closet will not be similar to clothes). A related task is word analogy, in which systems take three input words (A:A*, B:?) and output a word B*, such that the relation between B and B* is closest to the relation between A and A*. For instance, queen is the desired answer for the triple (man:king, woman:?). Some recent state-of-the-art approaches to these two tasks derive a similarity score via arithmetic computations on word embeddings (Mikolov et al., 2013b). While original training of word embeddings used lexical contexts (n-grams), recently Levy and Goldberg (2014) generalized this to arbitrary contexts, such as dependency paths. We use their software1 and recompute the word embeddings using contexts from our four structures: lexical context, dependency paths, SRL’s semantic relations, and Open IE’s surrounding tuple elements. Table 1 shows the different contexts for a sample word. 4 Evaluation In our experiments we use MaltParser (Nivre et al., 2007) for dependency parsing, and ClearNLP (Choi and Palmer, 2011) for SRL. To obtain Open-IE structures, we use the recent Open IE-4 system2 which produces n-ary extractions of both verb-based relation phrases </context>
<context position="14801" citStr="Levy and Goldberg (2014)" startWordPosition="2314" endWordPosition="2317"> &lt; 0.01). Results on Similarity and Analogy Tasks For these tasks, we train the various word embeddings 4As expected, all sentence-level intermediate structures perform best on the single partition, yet results show that some of the questions from the multiple partition may also be answered correctly using information from a single sentence. 5We experimented with various window sizes and found that window size of the length of the current candidateanswer performed best. on a Wikipedia dump (August 2013 dump), containing 77.5M sentences and 1.5B tokens. We used the default hyperparameters from Levy and Goldberg (2014): 300 dimensions, skip gram with negative sampling of size 5. Lexical embeddings were trained with 5-gram contexts. Performance is measured using Spearman’s ρ, in order to assess the correlation of the predictions to the gold annotations, rather than comparing their values directly. Table 2 compares the results on the word similarity task using cosine similarity between embeddings as the similarity predictor. For the ws353 test set we report results on the whole corpus (full) as well as on the partition suggested by (Agirre et al., 2009) into relatedness (mainly meronymholonym) and similarity </context>
<context position="16588" citStr="Levy and Goldberg, 2014" startWordPosition="2585" endWordPosition="2588">larity. Thus, Open IE performs better on word-pairs which exhibit both topical and functional similarity, such as (latinist, classicist), or (provincialism, narrow-mindedness), which were taken from the Luong test set. Table 4 further illustrates this dual capturing of both types of similarity in Open IE space. Our results also reiterate previous findings – lexical contexts do well on domain-similarity test sets (Mikolov et al., 2013b). The results on the simlex999 test set can be explained by its focus on functional similarity, previously identified as better captured by dependency contexts (Levy and Goldberg, 2014). For the Word analogy task we use the Google (Mikolov et al., 2013a) and the Microsoft corpora (Mikolov et al., 2013b), which are composed of ∼ 195K and 8K instances respectively. We obtain the analogy vectors using both the additive and multiplicative measures (Mikolov et al., 2013b; Levy and Goldberg, 2014). Table 3 shows the results – Open IE obtains the best accuracies by vast margins (p &lt; 0.01), for reasons simi306 Figure 2: Performance in MCTest (percentage of correct answers). lar to the word similarity tasks. To our knowledge, Open IE results on both analogy datasets surpass the state</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014. Dependencybased word embeddings. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>2</issue>
<contexts>
<context position="1713" citStr="Liang et al., 2013" startWordPosition="246" endWordPosition="249">(1) Lexical representations, in which features are extracted from the original word sequence or the bag of words, (2) Stanford dependency parse trees (De Marneffe and Manning, 2008), which draw syntactic relations between words, and (3) Semantic role labeling (SRL), which extracts frames linking predicates with their semantic arguments (Carreras and M`arquez, 2005). For instance, a QA application can evaluate a question and a candidate answer by examining their lexical overlap (P´erez-Couti˜no et al., 2006), by using short dependency paths as features to compare their syntactic relationships (Liang et al., 2013), or by using SRL to compare their predicate-argument structures (Shen and Lapata, 2007). In a seemingly independent research direction, Open Information Extraction (Open IE) extracts coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases (Etzioni et al., 2008; Fader et al., 2011; Mausam et al., 2012). We observe that while Open IE is primarily used as an end goal in itself (e.g., (Fader et al., 2014)), it also makes certain structural design choices which differ from those made by dependency or SRL. For example, Open IE chooses different pred</context>
</contexts>
<marker>Liang, Jordan, Klein, 2013</marker>
<rawString>Percy Liang, Michael I. Jordan, and Dan Klein. 2013. Learning dependency-based compositional semantics. Computational Linguistics, 39(2):389–446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minh-Thang Luong</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Better word representations with recursive neural networks for morphology.</title>
<date>2013</date>
<booktitle>The SIGNLL Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>104</pages>
<marker>Luong, Socher, Manning, 2013</marker>
<rawString>Minh-Thang Luong, Richard Socher, and Christopher D. Manning. 2013. Better word representations with recursive neural networks for morphology. The SIGNLL Conference on Computational Natural Language Learning (CoNLL), 104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Schmitz Mausam</author>
<author>Stephen Soderland</author>
<author>Robert Bart</author>
<author>Oren Etzioni</author>
</authors>
<title>Open language learning for information extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>523--534</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="2066" citStr="Mausam et al., 2012" startWordPosition="301" endWordPosition="304">M`arquez, 2005). For instance, a QA application can evaluate a question and a candidate answer by examining their lexical overlap (P´erez-Couti˜no et al., 2006), by using short dependency paths as features to compare their syntactic relationships (Liang et al., 2013), or by using SRL to compare their predicate-argument structures (Shen and Lapata, 2007). In a seemingly independent research direction, Open Information Extraction (Open IE) extracts coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases (Etzioni et al., 2008; Fader et al., 2011; Mausam et al., 2012). We observe that while Open IE is primarily used as an end goal in itself (e.g., (Fader et al., 2014)), it also makes certain structural design choices which differ from those made by dependency or SRL. For example, Open IE chooses different predicate and argument boundaries and assigns different relations between them. Given the differences between Open IE and other intermediate structures (see Section 2), a research question arises: Can certain downstream applications gain additional benefits from utilizing Open IE structures? To answer this question we quantitatively evaluate the use of Op</context>
</contexts>
<marker>Mausam, Soderland, Bart, Etzioni, 2012</marker>
<rawString>Mausam, Michael Schmitz, Stephen Soderland, Robert Bart, and Oren Etzioni. 2012. Open language learning for information extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 523–534, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Workshop at The International Conference on Learning Representations (ICLR).</booktitle>
<contexts>
<context position="10877" citStr="Mikolov et al., 2013" startWordPosition="1700" endWordPosition="1703">targeting domain similarity, simlex999 (Hill et al., 2014) specifically targets functional aspects of similarity (e.g., coast will be similar to shore, while closet will not be similar to clothes). A related task is word analogy, in which systems take three input words (A:A*, B:?) and output a word B*, such that the relation between B and B* is closest to the relation between A and A*. For instance, queen is the desired answer for the triple (man:king, woman:?). Some recent state-of-the-art approaches to these two tasks derive a similarity score via arithmetic computations on word embeddings (Mikolov et al., 2013b). While original training of word embeddings used lexical contexts (n-grams), recently Levy and Goldberg (2014) generalized this to arbitrary contexts, such as dependency paths. We use their software1 and recompute the word embeddings using contexts from our four structures: lexical context, dependency paths, SRL’s semantic relations, and Open IE’s surrounding tuple elements. Table 1 shows the different contexts for a sample word. 4 Evaluation In our experiments we use MaltParser (Nivre et al., 2007) for dependency parsing, and ClearNLP (Choi and Palmer, 2011) for SRL. To obtain Open-IE stru</context>
<context position="16401" citStr="Mikolov et al., 2013" startWordPosition="2557" endWordPosition="2560">icate or argument) are lexically close and indicate domainsimilarity, whereas context words from other elements in the tuple express semantic relationships, and target functional similarity. Thus, Open IE performs better on word-pairs which exhibit both topical and functional similarity, such as (latinist, classicist), or (provincialism, narrow-mindedness), which were taken from the Luong test set. Table 4 further illustrates this dual capturing of both types of similarity in Open IE space. Our results also reiterate previous findings – lexical contexts do well on domain-similarity test sets (Mikolov et al., 2013b). The results on the simlex999 test set can be explained by its focus on functional similarity, previously identified as better captured by dependency contexts (Levy and Goldberg, 2014). For the Word analogy task we use the Google (Mikolov et al., 2013a) and the Microsoft corpora (Mikolov et al., 2013b), which are composed of ∼ 195K and 8K instances respectively. We obtain the analogy vectors using both the additive and multiplicative measures (Mikolov et al., 2013b; Levy and Goldberg, 2014). Table 3 shows the results – Open IE obtains the best accuracies by vast margins (p &lt; 0.01), for reas</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. In Workshop at The International Conference on Learning Representations (ICLR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies,</booktitle>
<pages>746--751</pages>
<contexts>
<context position="10877" citStr="Mikolov et al., 2013" startWordPosition="1700" endWordPosition="1703">targeting domain similarity, simlex999 (Hill et al., 2014) specifically targets functional aspects of similarity (e.g., coast will be similar to shore, while closet will not be similar to clothes). A related task is word analogy, in which systems take three input words (A:A*, B:?) and output a word B*, such that the relation between B and B* is closest to the relation between A and A*. For instance, queen is the desired answer for the triple (man:king, woman:?). Some recent state-of-the-art approaches to these two tasks derive a similarity score via arithmetic computations on word embeddings (Mikolov et al., 2013b). While original training of word embeddings used lexical contexts (n-grams), recently Levy and Goldberg (2014) generalized this to arbitrary contexts, such as dependency paths. We use their software1 and recompute the word embeddings using contexts from our four structures: lexical context, dependency paths, SRL’s semantic relations, and Open IE’s surrounding tuple elements. Table 1 shows the different contexts for a sample word. 4 Evaluation In our experiments we use MaltParser (Nivre et al., 2007) for dependency parsing, and ClearNLP (Choi and Palmer, 2011) for SRL. To obtain Open-IE stru</context>
<context position="16401" citStr="Mikolov et al., 2013" startWordPosition="2557" endWordPosition="2560">icate or argument) are lexically close and indicate domainsimilarity, whereas context words from other elements in the tuple express semantic relationships, and target functional similarity. Thus, Open IE performs better on word-pairs which exhibit both topical and functional similarity, such as (latinist, classicist), or (provincialism, narrow-mindedness), which were taken from the Luong test set. Table 4 further illustrates this dual capturing of both types of similarity in Open IE space. Our results also reiterate previous findings – lexical contexts do well on domain-similarity test sets (Mikolov et al., 2013b). The results on the simlex999 test set can be explained by its focus on functional similarity, previously identified as better captured by dependency contexts (Levy and Goldberg, 2014). For the Word analogy task we use the Google (Mikolov et al., 2013a) and the Microsoft corpora (Mikolov et al., 2013b), which are composed of ∼ 195K and 8K instances respectively. We obtain the analogy vectors using both the additive and multiplicative measures (Mikolov et al., 2013b; Levy and Goldberg, 2014). Table 3 shows the results – Open IE obtains the best accuracies by vast margins (p &lt; 0.01), for reas</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013b. Linguistic regularities in continuous space word representations. In Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies, pages 746–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>02</issue>
<contexts>
<context position="11384" citStr="Nivre et al., 2007" startWordPosition="1779" endWordPosition="1782">o these two tasks derive a similarity score via arithmetic computations on word embeddings (Mikolov et al., 2013b). While original training of word embeddings used lexical contexts (n-grams), recently Levy and Goldberg (2014) generalized this to arbitrary contexts, such as dependency paths. We use their software1 and recompute the word embeddings using contexts from our four structures: lexical context, dependency paths, SRL’s semantic relations, and Open IE’s surrounding tuple elements. Table 1 shows the different contexts for a sample word. 4 Evaluation In our experiments we use MaltParser (Nivre et al., 2007) for dependency parsing, and ClearNLP (Choi and Palmer, 2011) for SRL. To obtain Open-IE structures, we use the recent Open IE-4 system2 which produces n-ary extractions of both verb-based relation phrases using SRLIE (an improvement over (Christensen et al., 2011)) and nominal relations using regular expressions. SRLIE first processes sentences using SRL and then uses hand-coded rules to convert SRL frames and associated dependency parses to open extractions. We choose these tools as they are on par with state-of-the-art in their respective fields, and therefore represent the current availabl</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi. 2007. Maltparser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13(02):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manuel P´erez-Couti˜no</author>
<author>Manuel Montes-y G´omez</author>
<author>Aurelio L´opez-L´opez</author>
<author>Luis Villase˜nor-Pineda</author>
</authors>
<title>The role of lexical features in Question Answering for Spanish.</title>
<date>2006</date>
<publisher>Springer.</publisher>
<marker>P´erez-Couti˜no, G´omez, L´opez-L´opez, Villase˜nor-Pineda, 2006</marker>
<rawString>Manuel P´erez-Couti˜no, Manuel Montes-y G´omez, Aurelio L´opez-L´opez, and Luis Villase˜nor-Pineda. 2006. The role of lexical features in Question Answering for Spanish. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kira Radinsky</author>
<author>Eugene Agichtein</author>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>A word at a time: computing word relatedness using temporal semantic analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th international conference on World wide web,</booktitle>
<pages>337--346</pages>
<publisher>ACM.</publisher>
<marker>Radinsky, Agichtein, Gabrilovich, Markovitch, 2011</marker>
<rawString>Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich, and Shaul Markovitch. 2011. A word at a time: computing word relatedness using temporal semantic analysis. In Proceedings of the 20th international conference on World wide web, pages 337–346. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Richardson</author>
<author>Christopher JC Burges</author>
<author>Erin Renshaw</author>
</authors>
<title>Mctest: A challenge dataset for the open-domain machine comprehension of text.</title>
<date>2013</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>193--203</pages>
<contexts>
<context position="8775" citStr="Richardson et al., 2013" startWordPosition="1366" endWordPosition="1369">ermediate structures. 3.1 Text Comprehension Task Text comprehension tasks extrinsically test natural language understanding through question answer304 Target Lexical Dependency SRL Open IE John nsubj John A0 John 0 John to xcomp visit A1 to 1 to refused visit A1 visit 1 visit Vegas A1 Vegas 2 Vegas Table 1: Some of the different contexts for the target word “refused” in the sentence ”John refused to visit Vegas”. SRL and Open IE contexts are preceded by their element (predicate or argument) index. See figure 1 for the different representations of this sentence. ing. We use the MCTest corpus (Richardson et al., 2013), which is composed of short stories followed by multiple choice questions. The MCTest task does not require extensive world knowledge, which makes it ideal for testing underlying sentence representations, as performance will mostly depend on accuracy and informativeness of the extracted structures. We adapt the unsupervised lexical matching algorithm from the original MCTest paper. It counts lexical matches between an assertion obtained from a candidate answer (CA) and a sliding window over the story. The selected answer is the one for which the maximum number of matches are found. Our adapta</context>
</contexts>
<marker>Richardson, Burges, Renshaw, 2013</marker>
<rawString>Matthew Richardson, Christopher JC Burges, and Erin Renshaw. 2013. Mctest: A challenge dataset for the open-domain machine comprehension of text. In Conference on Empirical Methods in Natural Language Processing, pages 193–203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Shen</author>
<author>Mirella Lapata</author>
</authors>
<title>Using semantic roles to improve question answering.</title>
<date>2007</date>
<booktitle>In EMNLPCoNLL 2007, Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>12--21</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1801" citStr="Shen and Lapata, 2007" startWordPosition="259" endWordPosition="263">equence or the bag of words, (2) Stanford dependency parse trees (De Marneffe and Manning, 2008), which draw syntactic relations between words, and (3) Semantic role labeling (SRL), which extracts frames linking predicates with their semantic arguments (Carreras and M`arquez, 2005). For instance, a QA application can evaluate a question and a candidate answer by examining their lexical overlap (P´erez-Couti˜no et al., 2006), by using short dependency paths as features to compare their syntactic relationships (Liang et al., 2013), or by using SRL to compare their predicate-argument structures (Shen and Lapata, 2007). In a seemingly independent research direction, Open Information Extraction (Open IE) extracts coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases (Etzioni et al., 2008; Fader et al., 2011; Mausam et al., 2012). We observe that while Open IE is primarily used as an end goal in itself (e.g., (Fader et al., 2014)), it also makes certain structural design choices which differ from those made by dependency or SRL. For example, Open IE chooses different predicate and argument boundaries and assigns different relations between them. Given the di</context>
</contexts>
<marker>Shen, Lapata, 2007</marker>
<rawString>Dan Shen and Mirella Lapata. 2007. Using semantic roles to improve question answering. In EMNLPCoNLL 2007, Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, June 28-30, 2007, Prague, Czech Republic, pages 12–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th annual meeting of the association for computational linguistics,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="17695" citStr="Turian et al., 2010" startWordPosition="2767" endWordPosition="2770">). lar to the word similarity tasks. To our knowledge, Open IE results on both analogy datasets surpass the state of the art. An example (from the Microsoft test set) which supports the observation regarding Open IE embeddings space is (gentlest:gentler, loudest:?), for which only Open IE answers correctly as louder, while lexical respond with higher-pitched (domain similar to loudest), and dependency with thinnest (functionally similar to loudest). Our Open-IE embeddings are freely available6 and we note that these can serve as plug-in features for other NLP applications, as demonstrated in (Turian et al., 2010). 5 Conclusions We studied Open IE’s output compared with other dominant structures, highlighting their main differences. We then conduct experiments and analysis suggesting that these structural differences prove beneficial for certain downstream semantic applications. A key strength is Open IE’s ability to balance lexical proximity with long range dependencies in a single representation. Specifically, for the word analogy task, Open IE-based embeddings 6http://www.cs.bgu.ac.il/˜gabriels Target Word Lexical Dependency Open IE dog feline dog incisor bovine carnassial canine dentition equine fe</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384–394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Domain and function: A dualspace model of semantic relations and compositions.</title>
<date>2012</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>44</volume>
<pages>585</pages>
<contexts>
<context position="9797" citStr="Turney (2012)" startWordPosition="1528" endWordPosition="1530">es between an assertion obtained from a candidate answer (CA) and a sliding window over the story. The selected answer is the one for which the maximum number of matches are found. Our adaptation changes the algorithm to compute a modified matching score by counting matches between structure units. The corresponding units are either dependency edges, SRL frame elements or Open IE tuple elements. Figure 1 illustrates computations for a sentence - candidate answer pair. 3.2 Similarity and Analogy Tasks Word similarity tasks deal with assessing the degree of ”similarity” between two input words. Turney (2012) classifies two types of similarity: (1) domain similarity, e.g., carpenter is similar to wood, hammer, and nail, (2) functional similarity, in which carpenter will be similar to other professions, e.g., shoemaker, brewer, miner etc. Several evaluation test sets exist for this task, each targeting a slightly different aspect of similarity. While Bruni (2012), Luong (2013), Radinsky (2011), and ws353 (Finkelstein et al., 2001) can be largely categorized as targeting domain similarity, simlex999 (Hill et al., 2014) specifically targets functional aspects of similarity (e.g., coast will be simila</context>
</contexts>
<marker>Turney, 2012</marker>
<rawString>Peter D Turney. 2012. Domain and function: A dualspace model of semantic relations and compositions. Journal of Artificial Intelligence Research, 44:533– 585.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>