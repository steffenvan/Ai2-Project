<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.775342">
PARSING THE LOB CORPUS
Carl G. de Marcken
MIT Al Laboratory Room 838
545 Technology Square
</note>
<keyword confidence="0.258037">
Cambridge, MA 02142
Internet: cgdemarc(gai.mit.edu
</keyword>
<sectionHeader confidence="0.976917" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.9999155">
This paper l presents a rapid and robust pars-
ing system currently used to learn from large
bodies of unedited text. The system contains a
multivalued part-of-speech disambiguator and
a novel parser employing bottom-up recogni-
tion to find the constituent phrases of larger
structures that might be too difficult to ana-
lyze. The results of applying the disambiguator
and parser to large sections of the Lancaster/
Oslo-Bergen corpus are presented.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
INTRODUCTION
</sectionHeader>
<bodyText confidence="0.996048242424242">
We have implemented and tested a pars-
ing system which is rapid and robust enough
to apply to large bodies of unedited text. We
have used our system to gather data from the
Lancaster/Oslo-Bergen (LOB) corpus, generat-
ing parses which conform to a version of current
Government-Binding theory, and aim to use the
system to parse 25 million words of text
The system consists of an interface to the
LOB corpus, a part of speech disambiguator,
and a novel parser. The disambiguator uses
multivaluedness to perform, in conjunction with
the parser, substantially more accurately than
current algorithms. The parser employs bottom-
up recognition to create rules which fire top-
down, enabling it to rapidly parse the constituent
phrases of a larger structure that might itself be
difficult to analyze. The complexity of some of
the free text in the LOB demands this, and we
have not sought to parse sentences completely,
but rather to ensure that our parses are accu-
rate. The parser output can be modified to con-
form to any of a number of linguistic theories.
This paper is divided into sections discussing
the LOB corpus, statistical disambiguation, the
parser, and our results.
This paper reports work done at the MIT
Artificial Intelligence Laboratory. Support for
this research was provided in part by grants
from the National Science Foundation (under a
Presidential Young Investigator award to Prof.
Robert C. Berwick); the Kapor Family Foun-
dation; and the Siemens Corporation.
</bodyText>
<sectionHeader confidence="0.946307" genericHeader="method">
THE LOB CORPUS
</sectionHeader>
<bodyText confidence="0.999971774193548">
The Lancaster/Oslo-Bergen Corpus is an on-
line collection of more than 1,000,000 words of
English text taken from a variety of sources,
broken up into sentences which are often 50 or
more words long. Approximately 40,000 differ-
ent words and 50,000 sentences appear in the
corpus.
We have used the LOB corpus in a standard
way to build several statistical tables of part of
speech usage. Foremost is a dictionary keying
every word found in the corpus to the number
of times it is used as a certain part of speech,
which allows us to compute the probability that
a word takes on a given part of speech. In ad-
dition, we recorded the number of times each
part of speech occurred in the corpus, and built
a cligram array, listing the number of times
one part of speech was followed by another.
These numbers can be used to compute the
probability of one category preceding another.
Some disambiguation schemes require knowing
the number of trigrant occurrences (three spe-
cific categories in a row). Unfortunately, with
a 132 category system and only one million
words of tagged text, the statistical accuracy of
LOB trigrams would be minimal. Indeed, even
in the &amp;gram table we have built, fewer than
3100 of the 17,500 digrams occur more than 10
times. When using the digram table in statisti-
cal schemes, we treat each of the 10,500 digrams
which never occur as if they occur once.
</bodyText>
<sectionHeader confidence="0.978204" genericHeader="method">
STATISTICAL DISAMBIGUATION
</sectionHeader>
<bodyText confidence="0.999949538461538">
Many different schemes have been proposed
to disambiguate word categories before or dur-
ing parsing. One common style of disambigua-
tors, detailed in this paper, rely on statistical
cooccurance information such as that discussed
in the section above. Specific statistical disam-
biguators are described in both DeRose 1988
and Church 1988. They can be thought of as
algorithms which maximize a function over the
possible selections of categories. For instance,
for each word Az in a sentence, the DeRose al-
gorithm takes a set of categories {of, (4, ...} as
input. It outputs a particular category a such
</bodyText>
<page confidence="0.997416">
243
</page>
<bodyText confidence="0.990785958333333">
that the product of the probability that A.= is
the category a;z and the probability that the
category af: occurs before the category of,++1, is
maximized. Although such an algorithm might
seem to be exponential in sentence length since
there are an exponential number of&apos; combina-
tions of categories, its limited leftward and right-
ward dependencies permit linear time dynamic
programming method. Applying his algorithm
to the Brown Corpus, DeRose claims the ac-
curacy rate of 96%. Throughout this paper we
will present accuracy figures in terms of how of-
ten words are incorrectly disambiguated. Thus,
we write 96% correctness as an accuracy of 25
(words per error).
We have applied the DeRose scheme and
several variations to the LOB corpus in order
to find an optimal disambiguation method, and
display our findings below in Figure 1. First,
we describe the four functions we maximize:
Method A: Method A is also described in
the DeRose paper. It maximizes the product
of the probabilities of each category occurring
before the next, or
</bodyText>
<equation confidence="0.616179">
n-1
H P (afzis-flwd-by e+1
z.i
</equation>
<bodyText confidence="0.9880654">
Method B: Method B is the other half of
the DeRose scheme, maximizing the product of
the probabilities of each category occurring for
its word. Method B simply selects each word&apos;s
most probable category, regardless of context.
</bodyText>
<equation confidence="0.695136571428571">
JJ P (Azis-cat cif)
z=i
Method C: The DeRose scheme, or the
maximum of
- 1
P (Azis-cat al.) P (ajz..is-flwd-by (0+1 )
Method D: No statistical disambiguator
</equation>
<bodyText confidence="0.997027253968254">
can perform perfectly if it only returns one part
of speech per word, because there are words and
sequences of words which can be truly ambigu-
ous in certain contexts. Method D addresses
this problem by on occasion returning more
than one category per word.
The DeRose algorithm moves from left to
right assigning to each category cr; an optimal
path of categories leading from the start of the
sentence to ctE, and a corresponding probability.
The Brown Corpus is a large, tagged text
database quite similar to the LOB.
It then extends each path with the categories of
the word A&apos;+1 and computes new probabilities
for the new paths. Call the greatest new prob-
ability P. Method D assigns to the word A&apos;
those categories {eq.} which occur in those new
paths which have a probability within a factor
F of P. It remains a linear time algorithm.
Naturally, Method D will return several cat-
egories for some words, and only one for others,
depending on the particular sentence and the
factor F. If F = 1, Method D will return only
one category per word, but they are not nec-
essarily the same categories as DeRose would
return. A more obvious variation of DeRose,
in which alternate categories are substituted
into the DeRose disambiguation and accepted
if they do not reduce the overall disambigua-
tion probability significantly, would approach
DeRose as F went to 1, but turns out not to
perform as well as Method D.3
Disambiguator Results: Each method
was applied to the same 64,000 words of the
LOB corpus. The results were compared to the
LOB part of speech pre-tags, and are listed in
Figure 1.4 If a word was pre-tagged as being
a proper noun, the proper noun category was
included in the dictionary, but no special infor-
mation such as capitalization was used to dis-
tinguish that category from others during dis-
ambiguation. For that reason, when judging
accuracy, we provide two metrics: one simply
comparing disambiguator output with the pre-
tags, and another that gives the disambiguator
the benefit of the doubt on proper nouns, under
the assumption that an &amp;quot;oracle&amp;quot; pre-processor
could distinguish proper nouns from contextual
or capitalization information. Since Method D
can return several categories for each word, we
provide the average number of categories per
word returned, and we also note the setting of
the parameter F, which determines how many
categories, on average, are returned.
The numbers in Figure 1 show that sim-
ple statistical schemes can accurately disam-
biguate parts of speech in normal text, con-
firming DeRose and others. The extraordinary
To be more precise, for a given average
number of parts of speech returned V, the &amp;quot;sub-
stitution&amp;quot; method is about 10% less accurate
when 1 &lt; V &lt; 1.1 and is almost 50% less ac-
curate for 1.1 &lt; V &lt; 1.2.
</bodyText>
<footnote confidence="0.5059105">
4 In all figures quoted, punctuation marks
have been counted as words, and are
treated as parts of speech by the statistical
di s ambi gua t ors .
</footnote>
<page confidence="0.996226">
244
</page>
<table confidence="0.999446625">
Method: A B C D(1) D(.3)
Accuracy: 7.9 17 23 25 41
with oracle: 8.8 18 30 31 54
of Cats: 1 1 1 1 1.04
Method: D(.1) D(.03) D(.01) D(.003)
Accuracy: 70 126 265 1340
with oracle: 105 230 575 1840
No. of Cats: 1.09 1.14 1.20 1.27
</table>
<figureCaption confidence="0.7648925">
Figure 1: Accuracy of various disambiguation
strategies, in number of words per error. On
average, the dictionary had 2.2 parts of speech
listed per word.
</figureCaption>
<bodyText confidence="0.996865779661017">
accuracy one can achieve by accepting an ad-
ditional category every several words indicates
that disambiguators can predict when their an-
swers are unreliable.
Readers may worry about correlation result-
ing from using the same corpus to both learn
from and disambiguate. We have run tests by
first learning from half of the LOB (600,000
words) and then disambiguating 80,000 words
of random text from the other half, The ac-
curacy figures varied by less than 5% from the
ones we present, which, given the size of the
LOB, is to be expected. We have also applied
each disambiguation method to several smaller
(13,000 word) sets of sentences which were se-
lected at complete random from throughout the
LOB. Accuracy varied both up and down from
the figures we present, by up to 20% in terms of
words per error, but relative accuracy between
methods remained constant.
The fact the Method D with F -= 1 (with
F = 1 Method D returns only one category per
word) performs as well or even better on the
LOB than DeRose&apos;s algorithm indicates that,
with exceptions, disambiguation has very lim-
ited rightward dependence: Method D employs
a one category lookaheacl, whereas DeRose&apos;s
looks to the end of the sentence. This sug-
gests that Church&apos;s strategy of using trigrams
instead of cligrams may be wasteful. Church
manages to achieve results similar or slightly
better than DeRose&apos;s by defining the probabil-
ity that a category A appears in a sequence
ABC to be the number of times the sequence
ABC appears divided by the number of times
the sequence BC appears. In a 100 category
system, this scheme requires an enormous ta-
ble of data, which must be culled from tagged
text_ If the rightward dependence of disam-
biguation is small, as the data suggests, then
the extra effort may be for naught. Based on
our results, it is more efficient to use digrams
in general and only mark special cases for tri-
grams, which would reduce space and learning
requirements substantially.
Integrating Disambiguator and Parser:
As the LOB corpus is pretagged, we could ig-
nore disambiguation problems altogether, but
to guarantee that our system can be applied to
arbitrary texts, we have integrated a variation
of disambiguation Method D with our parser.
When a sentence is parsed, the parser is ini-
tially passed all categories returned by Method
D with F = .01. The disambiguator substan-
tially reduces the time and space the parser
needs for a given parse, and increases the parser&apos;s
accuracy. The parser introduces syntactic con-
straints that perform the remaining disambigua-
tion well.
</bodyText>
<subsectionHeader confidence="0.715611">
THE PARSER
</subsectionHeader>
<bodyText confidence="0.998004294117647">
Introduction: The LOB corpus contains
unedited English, some of which is quite com-
plex and some of which is ungrammatical. No
known parser could produce full parses of all
the material, and even one powerful enough to
do so would undoubtably take an impractical
length of time. To facilitate the analysis of
the LOB, we have implemented a simple parser
which is capable of rapidly parsing simple con-
structs and of &amp;quot;failing gracefully&amp;quot; in more com-
plicated situations. By trading completeness
for accuracy, and by utilizing the statistical dis-
ambiguator, the parser can perform rapidly and
correctly enough to usefully parse the entire
LOB in a few hours. Figure 2 presents a sample
parse from the LOB.
The parser employs three methods to build
phrases. CFG-like rules are used to recognize
lengthy, less structured constructions such as
NPs, names, dates, and verb systems. Neigh-
boring phrases can connect to build the higher
level binary-branching structure found in En-
glish, and single phrases can he projected into
new ones. The ability of neighboring phrase
pairs to initiate the CFG-like rules permits context-
sensitive parsing. And, to increase the effi-
ciency of the parser, an innovative system of
deterministically discarding certain phrases is
used, called &amp;quot;lowering&amp;quot;.
Some Parser Details: Each word in an
input sentence is tagged as starting and end-
ing at a specific numerical location, hi the
sentence &amp;quot;I saw Mary.&amp;quot; the parser would in-
sert the locations 0-4, 0 I 1 SAW 2 MARY 3 .
</bodyText>
<page confidence="0.991955">
245
</page>
<note confidence="0.831958666666667">
MR MICHAEL FOOT HAS PUT DOWN I RESOLUTION ON THE
SUBJECT AND HE IS TO BE BACXED BY MR WILL
GRIFFITHS , MP FOR MANCHESTER EXCHANGE .
</note>
<equation confidence="0.900363117647059">
&gt; (IP
(NP (PROP (N MR) (NAME MICHAEL) (NAME FOOT)))
(I-BAR (I (HAVE HAS) (RP DOWN))
(VP (V roT) (MP (DET A) (N RESOLUTION)))))
&gt; (PP (P ON) (NP (DET THE) (N SUBJECT)))
&gt; (CC AND)
&gt; (IP (NP HE)
(I-BAR (I)
(VP (IS IS)
(I-BAR (1 (PP (P BY) (NP (PROP (N MR)
(um WILL) (NAME GRIFFITHS)))))
(TO TO) (IS BE)) (VP (V BACKED))))))
&gt; (*CMA &amp;quot;,&amp;quot;)
&gt; (NP (K NP))
&gt; (PP (P FOR) (NP (PROP (NAME MANCHESTER)
(NAME EXCHANGE))))
&gt; (*PER &amp;quot;.&amp;quot;)
</equation>
<figureCaption confidence="0.680061">
Figure 2: The parse of a sentence taken ver-
batim from the LOB corpus, printed without
features. Notice that the grammar does not at-
tach PP adjuncts.
</figureCaption>
<bodyText confidence="0.997644344444445">
4. A phrase consists of a category, starting
and ending locations, and a collection of fea-
ture and tree information. A verb phrase ex-
tending from 1 to 3 would print as [VP 1 3].
Rules consist of a state name and a location.
If a verb phrase recognition rule was firing in
location 1, it would get printed as MO at
1) where VP0 is the name of the rule state.
Phrases and rules which have yet to be pro-
cessed are placed on a queue. At parse initial-
ization, phrases are created from each word and
its category(ies), and placed on the queue along
with an end-of-sentence marker. The parse pro-
ceeds by popping the top rule or phrase off the
queue and performing actions on it. Figure 3
contains a detailed specification of the parser
algorithm, along with parts of a grammar. It
should be comprehensible after the following
overview and parse example.
When a phrase is popped off the queue, rules
are checked to see if they fire on it, a table
is examined to see if the phrase automatically
projects to another phrase or creates a rule,
and neighboring phrases are examined in case
they can pair with the popped phrase to ei-
ther connect into a new phrase or create a rule.
Thus the grammar consists of three tables, the
&amp;quot;rule-action-table&amp;quot; which specifies what action
a rule in a certain state should take if it en-
counters a phrase with a given category and
features; a &amp;quot;single- phrase-action-table&amp;quot; which
specifies whether a phrase with a given category
and features should project or start a rule; and
a &amp;quot;paired-phrase-action-table&amp;quot; which specifies
possible actions to take if two certain phrases
abut each other.
For a rule to fire on a phrase, the rule must
be at the starting position of the phrase. Pos-
sible actions that can be taken by the rule are:
accepting the phrase (shift the dot in the rule);
closing, or creating a phrase from all phrases
accepted so far; or both, creating a phrase and
continuing the rule to recognize a larger phrase
should it exist. Interestingly, when an enqueued
phrase is accepted, it is &amp;quot;lowered&amp;quot; to the bot-
tom of the queue, and when a rule closes to
create a phrase, all other phrases it may have
already created are lowered also.
As phrases are created, a call is made to
a set of transducer functions which generate
more principled interpretations of the phrases,
with appropriate features and tree relations.
The representations they build are only for out-
put, and do not affect the parse. An exception
is made to allow the functions to project and
modify features, which eases handling of sub-
categorization and agreement. The transduc-
ers can be used to generate a constant output
syntax as the internal grammar varies, and vice
versa.
New phrases and rules are placed on the
queue only after all actions resulting from a
given pop of the queue have been taken. The
ordering of their placement has a dramatic ef-
fect on how the parse proceeds. By varying
the queuing placement and the definition of
when a parse is finished, the efficiency and ac-
curacy of the parser can be radically altered.
The parser orders these new rules and phrases
by placing rules first, and then pushes all of
them onto the stack. This means that new
rules will always have precedence over newly
created phrases, and hence will fire in a succes-
sive &amp;quot;rule chain&amp;quot;. If all items were eventually
popped off the stack, the ordering would be ir-
relevant. However, since the parse is stopped at
the end-of-sentence marker, all phrases which
have been &amp;quot;lowered&amp;quot; past the marker are never
examined. The part of speech disambiguator
can pass in several categories for any one word,
which are ordered on the stack by likelihood,
most probable first. When any lexical phrase
is lowered to the back of the queue (presum-
ably because it was accepted by some rule) all
other lexical phrases associated with the same
word are also lowered. We have found that this
both speeds up parsing and increases accuracy.
That this speeds up parsing should be obvi-
ous. That it increases accuracy is much less so.
Remember that disambiguation Method D is
</bodyText>
<page confidence="0.993286">
246
</page>
<table confidence="0.84106053125">
The Parser Algorithm
To parse a sentences S of length n:
Perform multivalued disambiguation of S.
Create empty queue Q. Place Endsof-Se.ntence marker on Q.
Create new phrases from disambiguator output categories.
and place them on Q.
Until Q is empty. or top( Q) = End-of-Sentence marker.
Let ./-= pop(Q). Let new-items = nil
If lie phrase [cat i
Let rules = all rules at location i.
Let lefts = all phrases ending at location
Let rights = all-plirases starting a.t location j.
Perform rule-actions (rules, OD
Perform paired-phrase-actions(1efts1{1})
Perform paired-phrase-actions({i}, rights)
Perform single-phrase-actions(/).
If /is rule (state at i)
Let phrases = all phrases starting at location i.
Perform rule-actions ({1} ,phrases).
Place each item in new-items on Q. rules first.
Let i = 0. Until i =
Output longest phrase [cat ijj Let. i = j.
To perform rule-actions( rules ,phroses):
For all rules R = (state at i) in rules,
And all phrases P = Icat-Ffeatures i A in phrases,
If there is an action A in the rule-action-table with key
(state, cat-I-features),
If A = (accept new-state) or (accept-and-close new-
state new-cat).
Create new rule (new-state at j).
If A (close new-cat) or (accept-and-close new-
state new-cat).
</table>
<bodyText confidence="0.886759888888889">
Let daughters= the set of all phrases which have been
accepted in the rule chain which led to R. including
the phrase P.
Let 1 = the leftmost starting location of any phrase
in daughters. Create new phrase. [new-cat 1 A ‘villi
rinugliters daughters.
For all phrases p in daughters. perform lower (p).
For all phrases p created (via accept-and-close) by
the rule chain which led to .R. perform lower (p).
</bodyText>
<figure confidence="0.626755296296296">
To perform paired-phrase-actions (lefts, rights):
For all phrases P1 = [left-cat-ffeatures ii] in lefts,
And all phrases Pr = Cright-cati-features i Ft] in rights,
If there is an action A in the paired-phrase-action-
table with key ( left-cat+ features, right-cat+ features)
If A = (connect new-cat),
Create new phrase [new-cat/r] with daughters Pt and
Pr.
If A = (project new-cat).
Create new plira.se [new-cat i r] with daughter Pr.
If A = (start-new-rule state).
Create new rule (state at O.
Perform lower (P1) and lower (Pr).
To perform single-phrase-actions ( [cat-Ffeatures I):
If there is an action A in the single-phrase-action-table
with key cat-i-features.
If A = (project new-cat).
Create new phrase [new-cat z
If A = (start-rule new-state).
Create new rude (state at I).
To perform lower(/):
If /is in Q. remove it from Q and reinsert it at end of Q.
lf .1 is a lexical level phrase [cat i 1+1] created from tin- dis-
ambiguator output categories.
For all other lexical level phrases p ci acting at 2. perform
lower (p).
When creating a new rule R:
Add R to list of new-items.
When creating a new phrase P = [cat+features with
daughters D:
Add P to list of new-items.
If there is a hook function F in the hook-function-table
with key cat+features. perform F(P,D). Hook functions can
add features to P.
A section of a rule-action-table.
A section of a paired-phrase-action-table.
J;ey(Cat.. (.4t Action
COMP. S (connect (P)
NP +poss. NP (connect NP)
NP. S {project CP)
NP. VP ext-up +tense expect-nil (connect S
NP. CM:1* (start-ride C.NIAnt
VP expect-pp. PP (connect VP)
A section of a single-phrase-action-table.
Key(Cat) Act ion Key Act ion
DET +pro (start-rule DETO) PRO (project NP)
N (project NP) V (start-rule VP3)
NAME (start-rule DET I) IS (start-rule VPI I
(start-rule NMI ) (start-rule ISQ1)
A section of a hook-function-table.
Key((Iat) Hook Function
VP Cet-Snlicat egori sat ion-Info
Check-Agreement
CP Cliec.k-Comp-St ruct ure
</figure>
<figureCaption confidence="0.998956">
Figure 3: A pseudo-code representation of the parser algo-
rithm, omitting implementation details. Included in table
form are representative sections from a grammar.
</figureCaption>
<figure confidence="0.997496846153846">
Key (State. Cat) Action
DETo. DET
DET1. JJ
DETI. N +pi
DETI. N
JJo. JJ
17P1, ADV
(accept DETI
accept DETI)
(close NP)
(accept-and-close DET2 NP)
(accept-and-close J.Iu API
(accept VP-I)
</figure>
<page confidence="0.991415">
247
</page>
<bodyText confidence="0.986104525423729">
substantially more accurate the DeRose&apos;s algo-
rithm only because it can return more than one
category per word. One might guess that if the
parser were to lower all extra categories on the
queue that nothing would have been gained.
But the top-down nature of the parser is suf-
ficient in most cases to &amp;quot;pick out&amp;quot; the correct
category from the several available (see Milne
1988 for a detailed exposition of this).
A Parse in Detail: Figure 4 shows a
parse of the sentence &amp;quot;The pastry chef placed
the pie in the oven.&amp;quot; In the figure, items to
the left of the vertical line are the phrases and
rules popped off the stack. To the right of each
item is a list of all new items created as a result
of it being popped. At the start of the parse,
phrases were created from each word and their
corresponding categories, which were correctly
(and uniquely) determined by the disambigua-
t or.
The first item is popped off the queue, this
being the [DET 0 1] phrase corresponding to
the word &amp;quot;the&amp;quot;. The single-phrase action ta-
ble indicates that a DETO rule should be started
at location 0 and immediately fires on &amp;quot;the&amp;quot;,
which is accepted and the rule (DET1 at 1) is
accordingly created and placed on the queue.
This rule is then popped off the queue, and ac-
cepts the [N 1 2] corresponding to &amp;quot;pastry&amp;quot;,
also closing and creating the phrase ENP 0 2].
When this phrase is created, all queued phrases
which contributed to it are lowered in priority,
i.e., &amp;quot;pastry&amp;quot;. The rule (DET2 at 2) is cre-
ated to recognize a possibly longer NP, and is
popped off the queue in line 4. Here much the
same thing happens as in line 3, except that
the [NP 0 2] previously created is lowered as
the phrase DIP 0 3] is created. In line 5, the
rule chain keeps firing, but there are no phrases
starting at location 3 which can be used by the
rule state DET2.
The next item on the queue is the newly
created [NP 0 3], but it neither fires a rule
(which would have to be in location 0), finds
any action in the single-phrase table, or pairs
with any neighboring phrase to fire an action
in the paired-phrase table, so no new phrases
or rules are created. Hence, the verb &amp;quot;placed&amp;quot;
is popped and the single-phrase table indicates
that it should create a rule which then immedi-
ately accepts &amp;quot;placed&amp;quot;, creating a VP and plac-
ing the rule (VP4 at 4) in location 4. The VP
is popped off the stack, but not attached to [NP
0 3] to form a sentence, because the paired-
phrase table specifies that for those two phrases
to connect to become an S, the verb phrase
must have the feature (expect . nil), indi-
0 The 1 pastry 2 chef 3 placed 4 the 5 pie 6 in
7 the 8 oven 9 . 10
</bodyText>
<listItem confidence="0.91610325">
1. Phrase [DET 0 1] (DETO at 0)
2. Rule (DETO at 0) (DET1 at 1)
3. Rule (DET1 at 1) [NP 0 2] (DET1 at 2)
Lowering: [N 1 2]
4. Rule (DET2 at 2) [NP 0 3] (DET2 at 3)
Lowering: [NP 0 2]
Lowering: [N 2 3]
5. Rule (DET2 at 3)
6. Phrase [NP 0 31
7. Phrase [V 3 4]
8. Rule (VP3 at 3)
9. Rule (VP4 at 4)
</listItem>
<figure confidence="0.987692304347826">
10. Phrase [VP 3 4]
11. Phrase [DET 4 51
12. Phrase (DETO at 4)
13. Rule (DET1 at 5)
14. Rule (DET2 at 6)
15. Phrase [NP 4 6]
16. Phrase [VP 3 6]
17. Phrase ES 0 6]
18. Phrase [P 6 7]
19. Phrase [DET 7 81
20. Rule (DETO at 7)
21. Rule (DET1 at 8)
22. Rule (DET2 at 9)
23. Phrase [NP 7 9]
24. Phrase [PP 6 9]
25. Phrase E*PER 9 101
&gt; (IP (NP (DET &amp;quot;The&amp;quot;) (N &amp;quot;pastry&amp;quot;) (N &amp;quot;chef&amp;quot;))
(I-BAR (I) (VP (V &amp;quot;placed&amp;quot;)
(NP (DET &amp;quot;the&amp;quot;) (N &amp;quot;pie&amp;quot;)))))
&gt; (PP (P &amp;quot;in&amp;quot;) (NP (DET &amp;quot;the&amp;quot;) (N &amp;quot;oven&amp;quot;)))
&gt; (*PER &amp;quot;.&amp;quot;)
Phrases left on Queue: [N 1 2] [N 2 3] [NP 0 21
[N 5 6] [N 8 9]
</figure>
<figureCaption confidence="0.999938">
Figure 3: A detailed parse of the sentence
</figureCaption>
<bodyText confidence="0.993165588235294">
&amp;quot;The pastry chef placed the pie in the oven&amp;quot;.
Dictionary look-up and disambiguation were
performed prior to the parse.
eating that all of its argument positions have
been filled. However when the VP was cre-
ated, the VP transducer call gave it the feature
(expect . NP), indicating that it is lacking an
NP argument.
In line 15, such an argument is popped from
the stack and pairs with the VP as specified in
the paired-phrase table, creating a new phrase,
[VP 3 6]. This new vp then pairs with the
subject, forming Is 0 6]. In line 18, the prepo-
sition &amp;quot;in&amp;quot; is popped, but it does not create any
rules or phrases. Only when the NP &amp;quot;the oven&amp;quot;
is popped does it pair to create [PP 6 9]. Al-
though it should be attached as an argument
</bodyText>
<equation confidence="0.809033076923077">
(VP3 at 3)
[VP 3 4] (VP4 at 4)
(DETO at 4)
(DET1 at 5)
[UP 4 6] (DET2 at 6)
Lowering: [N 5 6]
[VP 3 6]
IS 0 6]
(DETO at 7)
(DET1 at 8)
[NF 7 9] (DET2 at 9)
Lowering: [N 8 9]
[PP 6 9]
</equation>
<page confidence="0.99358">
248
</page>
<bodyText confidence="0.9982258">
to the verb, the subcategorization frames (con-
tained in the expect feature of the VP) do not
allow for a prepositional phrase argument. Af-
ter the period is popped in line 25, the end-of-
sentence marker is popped and the parse stops.
At this time, 5 phrases have been lowered and
remain on the queue. To choose which phrases
to output, the parser picks the longest phrase
starting at location 0, and then the longest
phrase starting where the first ended, etc.
The Reasoning behind the Details: The
parser has a number of salient features to it, in-
cluding the combination of top-down and bottom-
up methods, the use of transducer functions to
create tree structure, and the system of lower-
ing phrases off the queue. Each was necessary
to achieve sufficient flexibility and efficiency to
parse the LOB corpus.
As we have mentioned, it would be naive of
us to believe that we could completely parse the
more difficult sentences in the corpus. The next
best thing is to recognize smaller phrases in
these sentences. This requires some bottom-up
capacity, which the parser achieves through the
single-phrase and paired-phrase action tables.
In order to avoid overgeneration of phrases, the
rules (in conjunction with the &amp;quot;lowering&amp;quot; sys-
tem and method of selecting output phrases)
provide a top-down capability which can pre-
vent some valid smaller phrases from being built.
Although this can stifle some correct parses&apos; we
have not found it to do so often.
Readers may notice that the use of special
mechanisms to project single phrases and to
connect neighboring phrases is unnecessary, since
rules could perform the same task. However,
since projection and binary attachment are so
common, the parser&apos;s efficiency is greatly im-
proved by the additional methods.
The choice of transducer functions to create
tree structure has roots in our previous expe-
riences with principle-based structures. Mod-
ern linguistic theories have shown themselves
to be valuable constraint systems when applied
to sentence tree-structure, but do not necessar-
ily provide efficient means of initially generat-
ing the structure. By using transducers to map
For instance, the parser always generates
the longest possible phrase it can from a se-
quence of words, a heuristic which can in some
cases fail. We have found that the only situ-
ation in which this heuristic fails regularly is
in verb argument attachment; with a more re-
strictive sub categorization system, it would not
be much of a problem.
between surface structure and more principled
trees, we have eliminated much of the compu-
tational cost involved in principled representa-
tions.
The mechanism of lowering phrases off the
stack is also intended to reduce computational
cost, by introducing determinism into the parser.
The effectiveness of the method can be seen
in the tables of Figure 5, which compare the
parser&apos;s speed with and without lowering.
</bodyText>
<sectionHeader confidence="0.99573" genericHeader="method">
RESULTS
</sectionHeader>
<bodyText confidence="0.999963933333334">
We have used the parser, both with and
without the lexical disambiguator, to analyze
large portions of the LOB corpus. Our gram-
mar is small; the three primary tables have a
total of 134 actions, and the transducer func-
tions are restricted to (outside of building tree
structure) projecting categories from daughter
phrases upward, checking agreement and case,
and dealing with verb sub categorization fea-
tures. Verb sub categorization information is
obtained from the Oxford Advanced Learner&apos;s
Dictionary of Contemporary English (Hornby
et al 1973), which often includes unusual verb
aspects, and consequently the parser tends to
accept too many verb arguments.
The parser identifies phrase boundaries sur-
prisingly well, and usually builds structures up
to the point of major sentence breaks such as
commas or conjunctions. Disambiguation fail-
ure is almost nonexistent. At the end of this pa-
per is a sequence of parses of sentences from the
corpus. The parses illustrate the need for a bet-
ter sub categorization system and some method
for dealing with conjunctions and parentheti-
cals, which tend to break up sentences.
Figure 5 presents some plots of parser speed
on a random 624 sentence subset of the LOB,
and compares parser performance with and with-
out lowering, and with arid without disambigua-
tion. Graphs 1 and 2 (2 is a zoom of 1) illustrate
the speed of the parser, and Graph 3 plots the
number of phrases the parser returns for a sen-
tence of a given length, which is a measure of
how much coverage the grammar has and how
much the parser accomplishes. Graph 4 plots
the number of phrases the parser builds during
an entire parse, a good measure of the work
it performs. Not surprisingly, there is a very
smooth curve relating the number of phrases
built and parse time. Graphs 5 and 6 are in-
cluded to show the necessity of disambiguation
and lowering, and indicate a substantial reduc-
tion in speed if either is absent. There is also a
substantial reduction inaccuracy. In the no dis-
ambiguation case, the parser is passed all cate-
</bodyText>
<page confidence="0.992732">
249
</page>
<figure confidence="0.998005533333333">
t (seconds)
20 ° a C a C
18 C 0 C °
16 l&apos;o813° .13a
14 ea r.0° .8 a
12 0 I al °
10
8
6
4
e
40 50
Ca
70
# of phrases built
- 200
- 180
- 160
- 140
-120
- 100
- 80
-60 a a
-40 g
-20
20 30 40 50
60 70
0
°
a
S 0° a o
an a 04:Inm
00 °a:• 0% in
&amp;quot; en°
112 Boo.° JIB CI
Graph 2: # of words in sentence
# of phrases returned
S
Graph 4: # of words in sentence
t (seconds)
-60 a
a
-SO
-40
30 Ca
a
20 00 a a a
S
° la .0 ° al3 C
10 a &amp;quot;eDMEnairuu
ao.igagaBiiaea0 °
elidadlil 30 40 50 60
Graph 5[No Dis.]: # of words in sentence
t (seconds)
60
Graph 1: # of words in sentence
t (seconds)
4
aa
R
00g a
° 13 . 0
Da:
ff0..01
B nn-Ba a
0
nUAO.:a go °.
n
a 0 Bn °oil°
n°mnge 1412E adEBr
saga cc 013
R°411111.
.ih:illi110114B°°B °
INN ° 20 25 30 35 40 45 50
3.5
3
2.5
2
1.5
1
• B0 a a
an
C
U
a a
a a:
30
25
20 0 a
an
15 ▪ CD 0 OM 10 an
aa ▪ 0 0 0
an 0:12 .a a
a a ea= a an a arne
CM000 00 n a 0
M MECOa COMMID 0 La 03
10 so mem cm n n
C C120:=331307COCO a 13
COM CM= CIM C117123
CC CM3 co CO CCCCC 13
cam 1:03030:momm C
=MOM= CCC
CM3 C0100311= CC 0 Jo 40 50
0000000com
a a
a
SO
a a
40
a 30
a
60 70 80 0 . °B . B E B.&apos;s °4.9
20
10 . °D0 a uaa a B °
neannilil 11111.1111NPIBM gElSd &apos; ° 40 50
E a ,i?, aml,&apos;,°1 a a. 0 Em ., a m a
a 0 a m M Eic c m0 Can a
u cla ° alla g . . a a
60
a
</figure>
<figureCaption confidence="0.802689333333333">
Graph 3: # of words in sentence
Figure 4: Performance graphs of parser on
subset of LOB. See text for explanations.
</figureCaption>
<bodyText confidence="0.879246153846154">
gories every word can take, in random order.
Parser accuracy is a difficult statistic to mea-
sure. We have carefully analyzed the parses
.assigned to many hundreds of LOB sentences,
and are quite pleased with the results. Al-
Graph 6[No Lowering]: # of words in sentence
Figure 5: Performance graphs of parser on
subset of LOB. See text for explanations.
though there are many sentences where the parser
is unable to build substantial structure, it rarely
builds incorrect phrases. A pointed exception
is the propensity for verbs to take too many
arguments. To get a feel for the parser&apos;s ac-
</bodyText>
<page confidence="0.973262">
250
</page>
<bodyText confidence="0.994297">
curacy, examine the Appendix, which contains
unedited parses from the LOB.
</bodyText>
<sectionHeader confidence="0.982494" genericHeader="method">
BIBLIOGRAPHY
</sectionHeader>
<reference confidence="0.995485642857143">
Church, K. W. 1988 A Stochastic Parts Pro-
gram and Noun Phrase Parser for Unrestricted
Text. Proceedings of the Second Conference on
Applied Natural Language Processing, 136-143
DeRose, S. J. 1988 Grammatical Category
Disambiguation by Statistical Optimization. Com-
putational Linguistics 14: 31-39
Oxford Advanced Learner&apos;s Dictionary of Con-
temporary English, eds. Hornby, A.S., and Covie,
A. P. (Oxford University Press, 1973)
Milne, R. Lexical Ambiguity Resolution in a
Deterministic Parser, in Lexical Ambiguity Res-
olution, ed. by S. Small et a/ (Morgan Kauf-
mann, 1988)
</reference>
<sectionHeader confidence="0.886254" genericHeader="method">
APPENDIX: Sample Parses
</sectionHeader>
<bodyText confidence="0.997002">
The following are several sentences from the
beginning of the LOB, parsed with our system.
Because of space considerations, indenting does
not necessarily reflect tree structure.
</bodyText>
<figure confidence="0.7347345">
A MOVE TO STOP MR GAITSKELL FROM NOMINATING ANT
MORE LABOUR LIFE PEERS IS TO BE MADE AT A
MEETING OF LABOUR MPS TOMORROW .
&gt; (NP (DET A) (N MOVE))
5 (I-BAR (I (TO TO)) (VP (V STOP)
(NP (mop (N MR) (NAME GAITSKELL)))
(P FROM)))
&gt; (I-BAR (I) (VP (v NOMINATING)
(NP (DET ANY) (Ap mORE) (N LABOUR)
(N LIFE) (N PEERS))))
&gt; (I-BAR (1) (VP (IS Is)
(I—BAR (1 (NP (N TOMORROW))
(TO TO) (IS BE))
(V MADE) (P AT)
(NP (NP (DET (N MEETING))
(PP (P OF)
(NP (N LABOUR) (N MPS))))))))
• (*PER .)
</figure>
<sectionHeader confidence="0.927006" genericHeader="method">
THOUGH THEY MAY GATHER SOME LEFT-WING SUPPORT ,
A LARGE NAJORITY OF LABOUR MPS ARE LIKELY TO
TURN DOWN THE FOOT-GRIFFITHS RESOLUTION .
</sectionHeader>
<reference confidence="0.681745545454545">
&gt; (CP (c-BAR (COMP THOUGH))
(IP (NP THEY)
(I-BAR (I (MD MAY))
(VP (V GATHER)
(HP (DET SOME) (JJ LEFT-WING)
(N SUPPORT))))))
&gt; (*CMA ,)
&gt; (IP (NP (NP (DET A) (JJ LARGE) (N MAJORITY))
(PP (P OF) (NP (N LABOUR) (N MPS))))
(I-BAR (I) (VP (IS ARE) (AP (JJ LIKELY)))))
&gt; (I-BAR (I (TO TO) (RP DOWN))
</reference>
<figure confidence="0.962907666666666">
(VP (V TURN)
(NP (DET TEE)
(PROP (NAME FOOT-GRIFFITHS))
wasourrIoN))))
&gt; (*PER .)
MR FOOT&apos;S LINE WILL BE THAT AS LABOUR MPS OPPOSED
THE GOVERNMENT BILL WHICH BROUGHT LIFE PEERS INTO
EXISTENCE , THEY SHOULD NOT NOW PUT FORWARD
NOMINEES .
• (Ip (NP (NP (PROP (N MR) (DARE FOOT)))
(NP (N LINE)))
(I-BAR (I (MD WILL)) (VP (Is BE) (NP THAT))))
&gt; (CP (c-BAR (COMP As))
(IP (NP (N LABOUR) (N MPS))
(I-BAR (I) (VP (V OPPOSED)
(NP (NP (DET THE) (N GOVERNMENT) (N BILL))
(CP (C-BAR (COMP WHICH))
(IP (NP)
(I-BAR (I) (VP (V BROUGHT)
(NP (N LIFE) (N PEERS)))))))
(P INTO) (NP (N EXISTENCE))))))
• (*CMA
&gt; (IP (NP THEY)
(I-BAR (1 (ADV FORWARD) (MD SHOULD) (HOT NOT)
(ADV NOW))
(VP (V PUT) (NP (N NOMINEES)))))
&gt; (*PER .)
</figure>
<sectionHeader confidence="0.543906" genericHeader="method">
THE TWO RIVAL AFRICAN NATIONALIST PARTIES OF
NORTHERN RHODESIA HAVE AGREED TO GET TOGETHER
TO FACE THE CHALLENGE FROM SIR ROY WELENsKY ,
THE FEDERAL PREMIER .
</sectionHeader>
<listItem confidence="0.7932695">
• (Ip (NP (HP (DET THE) (NUM (CD TWO)) (JJ RIVAL)
(JJ AFRICAN) (JJ NATIONALIST)
</listItem>
<equation confidence="0.960557294117647">
(N PARTIES))
(PP (P OF) (NP (PROP (NAME NORTHERN)
(NAME RHODESIA)))))
(I-BAR (I (HAVE HAVE)) (VP (V AGREED)
(I-BAR (1 (ADV TOGETHER) (TO TO))
(VP (V GET)
(I-BAR (I (TO TO))
(VP (V FACE)
(NP (DET THE) (N CHALLENGE))
(P FROM)
(NP (NP (PROP (N SIR) (NAME ROY)
(NAME wELENsKY)))
(*CMA ,)
(NP (DET THE) (JJ FEDERAL)
(N+++ PREMIER))))))))))
&gt;
(*PER .)
</equation>
<page confidence="0.991736">
251
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.433298">
<title confidence="0.999656">PARSING THE LOB CORPUS</title>
<author confidence="0.998798">Carl G de_Marcken</author>
<address confidence="0.824974">MIT Al Laboratory Room 838 545 Technology Square Cambridge, MA 02142</address>
<email confidence="0.996583">Internet:cgdemarc(gai.mit.edu</email>
<abstract confidence="0.990836545454545">This paper l presents a rapid and robust parsing system currently used to learn from large bodies of unedited text. The system contains a multivalued part-of-speech disambiguator and a novel parser employing bottom-up recognition to find the constituent phrases of larger structures that might be too difficult to analyze. The results of applying the disambiguator and parser to large sections of the Lancaster/ Oslo-Bergen corpus are presented.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>K W Church</author>
</authors>
<title>A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text.</title>
<date>1988</date>
<booktitle>Proceedings of the Second Conference on Applied Natural Language Processing,</booktitle>
<pages>136--143</pages>
<editor>DeRose, S. J.</editor>
<publisher>Oxford University Press,</publisher>
<contexts>
<context position="3835" citStr="Church 1988" startWordPosition="630" endWordPosition="631"> minimal. Indeed, even in the &amp;gram table we have built, fewer than 3100 of the 17,500 digrams occur more than 10 times. When using the digram table in statistical schemes, we treat each of the 10,500 digrams which never occur as if they occur once. STATISTICAL DISAMBIGUATION Many different schemes have been proposed to disambiguate word categories before or during parsing. One common style of disambiguators, detailed in this paper, rely on statistical cooccurance information such as that discussed in the section above. Specific statistical disambiguators are described in both DeRose 1988 and Church 1988. They can be thought of as algorithms which maximize a function over the possible selections of categories. For instance, for each word Az in a sentence, the DeRose algorithm takes a set of categories {of, (4, ...} as input. It outputs a particular category a such 243 that the product of the probability that A.= is the category a;z and the probability that the category af: occurs before the category of,++1, is maximized. Although such an algorithm might seem to be exponential in sentence length since there are an exponential number of&apos; combinations of categories, its limited leftward and righ</context>
</contexts>
<marker>Church, 1988</marker>
<rawString> Church, K. W. 1988 A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text. Proceedings of the Second Conference on Applied Natural Language Processing, 136-143 DeRose, S. J. 1988 Grammatical Category Disambiguation by Statistical Optimization. Computational Linguistics 14: 31-39 Oxford Advanced Learner&apos;s Dictionary of Contemporary English, eds. Hornby, A.S., and Covie, A. P. (Oxford University Press, 1973) Milne, R. Lexical Ambiguity Resolution in a Deterministic Parser, in Lexical Ambiguity Resolution, ed. by S. Small et a/ (Morgan Kaufmann, 1988) &gt; (CP (c-BAR (COMP THOUGH))</rawString>
</citation>
<citation valid="false">
<publisher></publisher>
<marker>(I-BAR (I (MD MAY)</marker>
<rawString>)</rawString>
</citation>
<citation valid="false">
<institution>(JJ LEFT-WING)</institution>
<marker>(HP (DET SOME)</marker>
<rawString>(JJ LEFT-WING)</rawString>
</citation>
<citation valid="false">
<journal>(*CMA ,) &gt; (IP (NP (NP (DET A) (JJ LARGE) (N MAJORITY</journal>
<marker>(N SUPPORT)</marker>
<rawString>))))) &gt; (*CMA ,) &gt; (IP (NP (NP (DET A) (JJ LARGE) (N MAJORITY))</rawString>
</citation>
<citation valid="false">
<journal>NP (N LABOUR) (N MPS</journal>
<marker>(PP (P OF)</marker>
<rawString>(NP (N LABOUR) (N MPS))))</rawString>
</citation>
<citation valid="false">
<journal>VP (IS ARE) (AP (JJ LIKELY))))) &gt; (I-BAR (I (TO TO) (RP DOWN</journal>
<marker>(I-BAR (I)</marker>
<rawString>(VP (IS ARE) (AP (JJ LIKELY))))) &gt; (I-BAR (I (TO TO) (RP DOWN))</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>