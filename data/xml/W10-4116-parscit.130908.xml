<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005442">
<title confidence="0.98195">
Exploring English Lexicon Knowledge for Chinese Sentiment Analysis
</title>
<author confidence="0.992081">
Yulan He Harith Alani
</author>
<affiliation confidence="0.9706765">
Knowledge Media Institute
The Open University
</affiliation>
<address confidence="0.888604">
Milton Keynes MK6 6AA, UK
</address>
<email confidence="0.997316">
{y.he, h.alani}@open.ac.uk
</email>
<sectionHeader confidence="0.993824" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999845416666667">
This paper presents a weakly-supervised
method for Chinese sentiment analysis
by incorporating lexical prior knowledge
obtained from English sentiment lexi-
cons through machine translation. A
mechanism is introduced to incorpo-
rate the prior information about polarity-
bearing words obtained from existing
sentiment lexicons into latent Dirichlet
allocation (LDA) where sentiment labels
are considered as topics. Experiments
on Chinese product reviews on mobile
phones, digital cameras, MP3 players,
and monitors demonstrate the feasibil-
ity and effectiveness of the proposed ap-
proach and show that the weakly su-
pervised LDA model performs as well
as supervised classifiers such as Naive
Bayes and Support vector Machines with
an average of 83% accuracy achieved
over a total of 5484 review documents.
Moreover, the LDA model is able to
extract highly domain-salient polarity
words from text.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.983092620253165">
Sentiment analysis aims to understand subjec-
tive information such as opinions, attitudes, and
feelings expressed in text. It has become a hot
topic in recent years because of the explosion in
availability of people’s attitudes and opinions ex-
pressed in social media including blogs, discus-
sion forums, tweets, etc. Research in sentiment
analysis has mainly focused on the English lan-
guage. There have been few studies in sentiment
analysis in other languages due to the lack of re-
sources, such as subjectivity lexicons consisting
of a list of words marked with their respective
polarity (positive, negative or neutral) and manu-
ally labeled subjectivity corpora with documents
labeled with their polarity.
Pilot studies on cross-lingual sentiment anal-
ysis utilize machine translation to perform senti-
ment analysis on the English translation of for-
eign language text (Banea et al., 2008; Bautin
et al., 2008; Wan, 2009). The major problem
is that they cannot be generalized well when
there is a domain mismatch between the source
and target languages. There have also been in-
creasing interests in exploiting bootstrapping-
style approaches for weakly-supervised senti-
ment classification in languages other than En-
glish (Zagibalov and Carroll, 2008b; Zagibalov
and Carroll, 2008a; Qiu et al., 2009). Other
approaches use ensemble techniques by either
combining lexicon-based and corpus-based algo-
rithms (Tan et al., 2008) or combining sentiment
classification outputs from different experimen-
tal settings (Wan, 2008). Nevertheless, all these
approaches are either complex or require careful
tuning of domain and data specific parameters.
This paper proposes a weakly-supervised ap-
proach for Chinese sentiment classification by
incorporating language-specific lexical knowl-
edge obtained from available English senti-
ment lexicons through machine translation. Un-
like other cross-lingual sentiment classification
methods which often require labeled corpora for
training and therefore hinder their applicability
for cross-domain sentiment analysis, the pro-
posed approach does not require labeled docu-
ments. Moreover, as opposed to existing weakly-
supervised sentiment classification approaches
which are rather complex, slow, and require care-
ful parameter tuning, the proposed approach is
simple and computationally efficient; rendering
more suitable for online and real-time sentiment
Deyu Zhou
School of Computer Science and Engineering
Southeast University
Nanjing, China
d.zhou@seu.edu.cn
classification from the Web.
Our experimental results on the Chinese re-
views of four different product types show that
the LDA model performs as well as the super-
vised classifiers such as Naive Bayes and Sup-
port Vector Machines trained from labeled cor-
pora. Although this paper primarily studies sen-
timent analysis in Chinese, the proposed ap-
proach is applicable to any other language so
long as a machine translation engine is available
between the selected language and English.
The remainder of the paper is organized as
follows. Related work on cross-lingual senti-
ment classification and weakly-supervised sen-
timent classification in languages other than En-
glish are discussed in Section 2. The proposed
mechanism of incorporating prior word polarity
knowledge into the LDA model is introduced in
Section 3. The experimental setup and results of
sentiment classification on the Chinese reviews
of four different products are presented in Sec-
tion 4 and 5 respectively. Finally, Section 6 con-
cludes the paper.
</bodyText>
<sectionHeader confidence="0.999693" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999851986842106">
Pilot studies on cross-lingual sentiment analysis
rely on English corpora for subjectivity classifi-
cation in other languages. For example, Mihal-
cea et al. (2007) make use of a bilingual lexicon
and a manually translated parallel text to gener-
ate the resources to build subjectivity classifiers
based on Support Vector Machines (SVMs) and
Naive Bayes (NB) in a new language; Banea et
al. (2008) use machine translation to produce a
corpus in a new language and train SVMs and
NB for subjectivity classification in the new lan-
guage. Bautin et al. (2008) also utilize machine
translation to perform sentiment analysis on the
English translation of a foreign language text.
More recently, Wan (2009) proposed a co-
training approach to tackle the problem of cross-
lingual sentiment classification by leveraging an
available English corpus for Chinese sentiment
classification. Similar to the approach proposed
in (Banea et al., 2008), Wan’s method also uses
machine translation to produced a labeled Chi-
nese review corpus from the available labeled
English review data. However, in order to allevi-
ate the language gap problem that the underlying
distributions between the source and target lan-
guage are different, Wan builds two SVM classi-
fiers, one based on English features and the other
based on Chinese features, and uses a bootstrap-
ping method based on co-training to iteratively
improve classifiers until convergence.
The major problem of the aforementioned
cross-lingual sentiment analysis algorithms is
that they all utilize supervised learning to train
sentiment classifiers from annotated English cor-
pora (or the translated target language corpora
generated by machine translation). As such, they
cannot be generalized well when there is a do-
main mismatch between the source and target
language. For example, For example, the word
‘compact’ might express positive polarity when
used to describe a digital camera, but it could
have negative orientation if it is used to describe
a hotel room. Thus, classifiers trained on one
domain often fail to produce satisfactory results
when shifting to another domain.
Recent efforts have also been made for
weakly-supervised sentiment classification in
Chinese. Zagibalov and Carroll (2008b) starts
with a one-word sentiment seed vocabulary and
use iterative retraining to gradually enlarge the
seed vocabulary by adding more sentiment-
bearing lexical items based on their relative fre-
quency in both the positive and negative parts
of the current training data. Sentiment direction
of a document is then determined by the sum
of sentiment scores of all the sentiment-bearing
lexical items found in the document. The prob-
lem with this approach is that there is no princi-
pal way to set the optimal number of iterations.
They then suggested an iteration control method
in (Zagibalov and Carroll, 2008a) where itera-
tive training stops when there is no change to the
classification of any document over the previous
two iterations. However, this does not necessar-
ily correlate to the best classification accuracy.
Similar to (Zagibalov and Carroll, 2008b),
Qiu et al. (2009) also uses a lexicon-based iter-
ative process as the first phase to iteratively en-
large an initial sentiment dictionary. But instead
documents is denoted by C = {d1, d2,..., dD};
each document in the corpus is a sequence of Nd
words denoted by d = (w1, w2, ..., wNd), and
each word in the document is an item from a vo-
cabulary index with V distinct terms denoted by
11, 2,..., V }. The generative process is as fol-
lows:
</bodyText>
<listItem confidence="0.92397525">
• Choose distributions cp — Dir(Q).
• For each document d E [1, D], choose dis-
tributions 7rd — Dir(&apos;Y).
• For each of the Nd word posi-
</listItem>
<bodyText confidence="0.962237833333333">
tion wt, choose a sentiment label
lt — Multinomial(7rd), and then choose a
word wt — Multinomial(cplt).
The joint probability of words and sentiment
label assignment in LDA can be factored into
two terms:
</bodyText>
<equation confidence="0.990521">
P(w, l) = P(wJl)P(lJd). (1)
</equation>
<bodyText confidence="0.999889823529412">
Letting the superscript —t denote a quantity that
excludes data from the tth position, the condi-
tional posterior for lt by marginalizing out the
random variables cp and 7r is
where Nwt,k is the number of times word wt has
associated with sentiment label k; Nk is the the
number of times words in the corpus assigned to
sentiment label k; Nk,d is the number of times
sentiment label k has been assigned to some
word tokens in document d; Nd is the total num-
ber of words in the document collection.
Each words in documents can either bear pos-
itive polarity (lt = 1), or negative polarity (lt =
2), or is neutral (lt = 0). We now show how
to incorporate polarized words in sentiment lex-
icons as prior information in the Gibbs sampling
process. Let
</bodyText>
<equation confidence="0.988245909090909">
Qt,k = N−t N−t
wt,k + Q k,d + &apos;Yk (3)
Nkt+ V Q x N−t
d +Ek&apos;Yk
P(lt = kJw,l−t, Q,γ) a
N−t
wt,k + Q
Nkt+ V Q x
N−t
k,d + &apos;Yk ,(2)
N−t
</equation>
<bodyText confidence="0.986749">
d +Ek&apos;Yk
of using a one-word seed dictionary as in (Za-
gibalov and Carroll, 2008b), they started with a
much larger HowNet Chinese sentiment dictio-
nary1 as the initial lexicon. Documents classified
by the first phase are taken as the training set to
train the SVMs which are subsequently used to
revise the results produced by the first phase.
Other researchers investigated ensemble tech-
niques for weakly-supervised sentiment classifi-
cation. Tan et al. (2008) proposed a combination
of lexicon-based and corpus-based approaches
that first labels some examples from a give do-
main using a sentiment lexicon and then trains
a supervised classifier based on the labeled ones
from the first stage. Wan (2008) combined sen-
timent scores calculated from Chinese product
reviews using the Chinese HowNet sentiment
dictionary and from the English translation of
Chinese reviews using the English MPQA sub-
jectivity lexicon2. Various weighting strategies
were explored to combine sentiment classifica-
tion outputs from different experimental settings
in order to improve classification accuracy.
Nevertheless, all these weakly-supervised
sentiment classification approaches are rather
complex and require either iterative training or
careful tuning of domain and data specific pa-
rameters, and hence unsuitable for online and
real-time sentiment analysis in practical applica-
tions.
</bodyText>
<sectionHeader confidence="0.983625" genericHeader="method">
3 Incorporating Prior Word Polarity
Knowledge into LDA
</sectionHeader>
<bodyText confidence="0.998576181818182">
Unlike existing approaches, we view sentiment
classification as a generative problem that when
an author writes a review document, he/she first
decides on the overall sentiment or polarity (pos-
itive, negative, or neutral) of a document, then
for each sentiment, decides on the words to be
used. We use LDA to model a mixture of only
three topics or sentiment labels, i.e. positive,
negative and neutral.
Assuming that we have a total number of S
sentiment labels; a corpus with a collection of D
</bodyText>
<footnote confidence="0.999379">
1http://www.keenage.com/download/
sentiment.rar
2http://www.cs.pitt.edu/mpqa/
</footnote>
<bodyText confidence="0.557173">
We can then modify the Gibbs sampling equa-
tion as follows:
Table 2: Data statistics of the four Chinese prod-
uct reviews corpora.
</bodyText>
<equation confidence="0.997771">
P(lt = k|w,l−t, 0,&apos;Y) a
�1I(k = S(wt)) x Qt,k if S(wt) is defined
Qt,k otherwise
(4)
</equation>
<bodyText confidence="0.999949769230769">
where the function S(wt) returns the prior senti-
ment label of wt in a sentiment lexicon and it is
defined if word wt is found in the sentiment lex-
icon. 1I(k = S(wt)) is an indicator function that
takes on value 1 if k = S(wt) and 0 otherwise.
Equation 4 in fact applies a hard constraint
that when a word is found in a sentiment lexi-
con, its sampled sentiment label is restricted to
be the same as its prior sentiment label defined
in the lexicon. This constraint can be relaxed by
introducing a parameter to control the strength of
the constraint such that when word wt is found in
the sentiment lexicon, Equation 4 becomes
</bodyText>
<equation confidence="0.980439333333333">
P(lt = k|w,l−t, 0,&apos;Y) a
(1 − A) x Qt,k + A x 1I(k = S(wt)) x Qt,k
(5)
</equation>
<bodyText confidence="0.99870735">
where 0 &lt; A &lt; 1. When A = 1, the hard con-
straint will be applied; when A = 0, Equation 5
is reduced to the original unconstrained Gibbs
sampling as defined in Equation 2.
While sentiment prior information is incor-
porated by modifying conditional probabilities
used in Gibbs sampling here, it is also possible to
explore other mechanisms to define expectation
or posterior constraints, for example, using the
generalized expectation criteria (McCallum et
al., 2007) to express preferences on expectations
of sentiment labels of those lexicon words. We
leave the exploitation of other mechanisms of in-
corporating prior knowledge into model training
as future work.
The document sentiment is classified based on
P(l|d), the probability of sentiment label given
document, which can be directly obtained from
the document-sentiment distribution. We de-
fine that a document d is classified as positive
</bodyText>
<table confidence="0.924018857142857">
if P(lpos|d) &gt; P(lneg|d), and vice versa.
No. of Reviews Vocab
Corpus positive Negative Size
Mobile 1159 1158 8945
DigiCam 853 852 5716
MP3 390 389 4324
Monitor 341 342 4712
</table>
<sectionHeader confidence="0.996083" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999736827586207">
We conducted experiments on the four corpora3
which were derived from product reviews har-
vested from the website IT1684 with each cor-
responding to different types of product reviews
including mobile phones, digital cameras, MP3
players, and monitors. All the reviews were
tagged by their authors as either positive or neg-
ative overall. The statistics of the four corpora
are shown in Table 2.
We explored three widely used English sen-
timent lexicons in our experiments, namely the
MPQA subjectivity lexicon, the appraisal lexi-
con5, and the SentiWordNet6 (Esuli and Sebas-
tiani, 2006). For all these lexicons, we only ex-
tracted words bearing positive or negative polar-
ities and discarded words bearing neutral polar-
ity. For SentiWordNet, as it consists of words
marked with positive and negative orientation
scores ranging from 0 to 1, we extracted a subset
of 8,780 opinionated words, by selecting those
whose orientation strength is above a threshold
of 0.6.
We used Google translator toolkit7 to translate
these three English lexicons into Chinese. After
translation, duplicate entries, words that failed to
translate, and words with contradictory polarities
were removed. For comparison, we also tested a
Chinese sentiment lexicon, NTU Sentiment Dic-
tionary (NTUSD)8 (Ku and Chen, 2007) which
</bodyText>
<footnote confidence="0.999754375">
3http://www.informatics.sussex.ac.uk/
users/tz21/dataZH.tar.gz
4http://product.it168.com
5http://lingcog.iit.edu/arc/
appraisal_lexicon_2007b.tar.gz
6http://sentiwordnet.isti.cnr.it/
7http://translate.google.com
8http://nlg18.csie.ntu.edu.tw:
</footnote>
<tableCaption confidence="0.997861">
Table 1: Matched polarity words statistics (positive/negative).
</tableCaption>
<table confidence="0.988270111111111">
Lexicon Chinese English
Mobile DigiCam MP3 Monitors Mobile DigiCam MP3 Monitors
(a)MPQA 261/253 183/174 162/135 169/147 293/331 220/241 201/153 210/174
(b)Appraisal 279/165 206/127 180/104 198/105 392/271 330/206 304/153 324/157
(c)SentiWN 304/365 222/276 202/213 222/236 394/497 306/397 276/310 313/331
(d)NTUSD 338/319 263/242 239/167 277/241 –
(a)+(c) 425/465 307/337 274/268 296/289 516/607 400/468 356/345 396/381
(a)+(b)+(c) 495/481 364/353 312/280 344/302 624/634 496/482 447/356 494/389
(a)+(c)+(d) 586/608 429/452 382/336 421/410 –
</table>
<bodyText confidence="0.999673666666667">
was automatically generated by enlarging an ini-
tial manually created seed vocabulary by con-
sulting two thesauri, tong2yi4ci2ci2lin2 and the
Academia Sinica Bilingual Ontological Word-
Net 3.
Chinese word segmentation was performed on
the four corpora using the conditional random
fields based Chinese Word Segmenter9. The to-
tal numbers of matched polarity words in each
corpus using different lexicon are shown in Ta-
ble 1 with the left half showing the statistics
against the Chinese lexicons (the original En-
glish lexicons have been translated into Chinese)
and the right half listing the statistics against the
English lexicons. We did not translate the Chi-
nese lexicon NTUSD into English since we fo-
cused on Chinese sentiment classification here.
It can be easily seen from the table that in gen-
eral the matched positive words outnumbered the
matched negative words using any single lexi-
con except SentiWordNet. But the combination
of the lexicons results in more matched polarity
words and thus gives more balanced number of
positive and negative words. We also observed
the increasing number of the matched polarity
words on the translated English corpora com-
pared to their original Chinese corpora. How-
ever, as will be discussed in Section 5.2 that the
increasing number of the matched polarity words
does not necessarily lead to the improvement of
the sentiment classification accuracy.
We modified GibbsLDA++ package10 for the
model implementation and only used hard con-
</bodyText>
<footnote confidence="0.9895596">
8080/opinion/pub1.html
9http://nlp.stanford.edu/software/
stanford-chinese-segmenter-2008-05-21.
tar.gz
10http://gibbslda.sourceforge.net/
</footnote>
<bodyText confidence="0.999953615384615">
straints as defined in Equation 4 in our experi-
ments. The word prior polarity information was
also utilized during the initialization stage that
if a word can be found in a sentiment lexicon,
the word token is assigned with its correspond-
ing sentiment label. Otherwise, a sentiment label
is randomly sampled for the word. Symmetric
Dirichlet prior β was used for sentiment-word
distribution and was set to 0.01, while asym-
metric Dirichlet prior γ was used for document-
sentiment distribution and was set to 0.01 for
positive and neutral sentiment labels, and 0.05
for negative sentiment label.
</bodyText>
<sectionHeader confidence="0.991242" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.999987166666667">
This section presents the experimental results
obtained under two different settings: LDA
model with translated English lexicons tested on
the original Chinese product review corpora; and
LDA model with original English lexicons tested
on the translated product review corpora.
</bodyText>
<subsectionHeader confidence="0.93209">
5.1 Results with Different Sentiment
Lexicons
</subsectionHeader>
<bodyText confidence="0.999491714285714">
Table 3 gives the classification accuracy results
using the LDA model with prior sentiment la-
bel information provided by different sentiment
lexicons. Since we did not use any labeled in-
formation, the accuracies were averaged over 5
runs and on the whole corpora. For comparison
purposes, we have also implemented a baseline
model which simply assigns a score +1 and -1
to any matched positive and negative word re-
spectively based on a sentiment lexicon. A re-
view document is then classified as either posi-
tive or negative according to the aggregated sen-
timent scores. The baseline results were shown
in brackets in Table 3 .
</bodyText>
<tableCaption confidence="0.996552">
Table 3: Sentiment classification accuracy (%) by LDA, numbers in brackets are baseline results.
</tableCaption>
<table confidence="0.9983205">
Lexicon Mobile DigiCam MP3 Monitors Average
(a)MPQA 82.00 (63.53) 80.93 (67.59) 78.31 (68.42) 81.41 (64.86) 80.66 (66.10)
(b)Appraisal 71.95 (56.28) 80.46 (60.54) 77.28 (61.36) 80.67 (57.98) 77.59 (59.04)
(c)SentiWN 81.10 (62.45) 78.52 (57.13) 79.08 (64.57) 75.55 (55.34) 78.56 (59.87)
(d)NTUSD 82.61 (71.21) 78.70 (68.23) 78.69 (75.87) 84.63 (74.96) 81.16 (72.57)
(a)+(c) 81.18 (65.95) 78.70 (65.18) 83.83 (67.52) 80.53 (62.08) 81.06 (65.18)
(a)+(b)+(c) 81.48 (62.84) 80.22 (65.88) 80.23 (65.60) 78.62 (61.35) 80.14 (63.92)
(a)+(c)+(d) 82.48 (69.96) 84.33 (69.58) 83.70 (71.12) 82.72 (65.59) 83.31 (69.06)
Naive Bayes 86.52 82.27 82.64 86.21 84.41
SVMs 84.49 82.04 79.43 83.87 82.46
</table>
<bodyText confidence="0.999744816326531">
It can be observed from Table 3 that the
LDA model performs significantly better than
the baseline model. The improvement ranges be-
tween 9% and 19% and this roughly corresponds
to how much the model learned from the data.
We can thus speculate that LDA is indeed able to
learn the sentiment-word distributions from data.
Translated English sentiment lexicons per-
form comparably with the Chinese sentiment
lexicon NTUSD. As for the individual lexicon,
using MPQA subjectivity lexicon gives the best
result among all the English lexicons on all the
corpora except the MP3 corpus where MPQA
performs slightly worse than SentiWordNet. The
combination of MPQA and SentiWordNet per-
forms significantly better than other lexicons on
the MP3 corpus, with almost 5% improvement
compared to the second best result. We also
notice that the combination of all the three En-
glish lexicons does not lead to the improvement
of classification accuracy which implies that the
quality of a sentiment lexicon is indeed impor-
tant to sentiment classification. The above re-
sults suggest that in the absence of any Chinese
sentiment lexicon, MPQA subjectivity lexicon
appears to be the best candidate to be used to
provide sentiment prior information to the LDA
model for Chinese sentiment classification.
We also conducted experiments by includ-
ing the Chinese sentiment lexicon NTUSD and
found that the combination of MPQA, Senti-
WordNet, and NTUSD gives the best overall
classification accuracy with 83.31% achieved.
For comparison purposes, we list the 10-fold
cross validation results obtained using the super-
vised classifiers, Naive Bayes and SVMs, trained
on the labeled corpora as previously reported in
(Zagibalov and Carroll, 2008a). It can be ob-
served that using only English lexicons (the com-
bination of MPQA and SentiWordNet), we ob-
tain better results than both NB and SVMs on
the MP3 corpus. With an additional inclusion
of NTUSD, LDA outperforms NB and SVMs
on both DigiCam and MP3. Furthermore, LDA
gives a better overall accuracy when compared
to SVMs. Thus, we may conclude that the un-
supervised LDA model performs as well as the
supervised classifiers such as NB and SVMs on
the Chinese product review corpora.
</bodyText>
<subsectionHeader confidence="0.950343">
5.2 Results with Translated Corpora
</subsectionHeader>
<bodyText confidence="0.999526058823529">
We ran a second set of experiments on the trans-
lated Chinese product review corpora using the
original English sentiment lexicons. Both the
translated corpora and the sentiment lexicons
have gone through stopword removal and stem-
ming in order to reduce the vocabulary size and
thereby alleviate data sparseness problem. It can
be observed from Figure 1 that in general senti-
ment classification on the original Chinese cor-
pora using the translated English sentiment lex-
icons gives better results than classifying on the
translated review corpora using the original En-
glish lexicons on both the Mobile and Digicam
corpora. However, reversed results are observed
on the Monitor corpus that classifying on the
translated review corpus using the English sen-
timent lexicons outperforms classifying on the
</bodyText>
<figureCaption confidence="0.950696">
Figure 1: Comparison of the performance on the Chinese corpora and their translated corpora in
English.
</figureCaption>
<bodyText confidence="0.999842647058824">
original Chinese review corpus using the trans-
lated sentiment lexicons. In particular, the com-
bination of the MPQA subjectivity lexicon and
SentiWordNet gives the best result of 84% on
the Monitor corpus. As for the MP3 corpus,
classifying on the original Chinese reviews or on
the translated reviews does not differ much ex-
cept that a better result is obtained on the Chi-
nese corpus when using the combination of the
MPQA subjectivity lexicon and SentiWordNet.
The above results can be partially explained by
the ambiguities and changes of meanings intro-
duced in the translation. The Mobile and Digi-
Cam corpora are relatively larger than the MP3
and Monitors corpora and we therefore expect
more ambiguities being introduced which might
result in the change of document polarities.
</bodyText>
<subsectionHeader confidence="0.989375">
5.3 Extracted Polarity-Bearing Words
</subsectionHeader>
<bodyText confidence="0.999973">
LDA is able to extract polarity-bearing words.
Table 4 lists some of the polarity words identi-
fied by the LDA model which are not found in
the original sentiment lexicons. We can see that
LDA is indeed able to recognize domain-specific
positive or negative words, for example, �❨
(bluetooth) for mobile phones, ✎I1� (compact)
for digital cameras, 4,�-❫ (metallic) for MP3, ➥
� (flat screen) and x❜ (deformation) for mon-
itors.
The iterative approach proposed in (Zagibalov
and Carroll, 2008a) can also automatically ac-
quire polarity words from data. However, it ap-
pears that only positive words were identified
by their approach. Our proposed LDA model
can extract both positive and negative words and
most of them are highly domain-salient as can be
seen from Table 4.
</bodyText>
<sectionHeader confidence="0.999625" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999989631578948">
This paper has proposed a mechanism to incor-
porate prior information about polarity words
from English sentiment lexicons into LDA
model learning for weakly-supervised Chinese
sentiment classification. Experimental results of
sentiment classification on Chinese product re-
views show that in the absence of a language-
specific sentiment lexicon, the translated En-
glish lexicons can still produce satisfactory re-
sults with the sentiment classification accuracy
of 81% achieved averaging over four different
types of product reviews. With the incorpora-
tion of the Chinese sentiment lexicon NTUSD,
the classification accuracy is further improved to
83%. Compared to the existing approaches to
cross-lingual sentiment classification which ei-
ther rely on labeled corpora for classifier learn-
ing or iterative training for performance gains,
the proposed approach is simple and readily to
</bodyText>
<tableCaption confidence="0.994077">
Table 4: Extracted example polarity words by LDA.
</tableCaption>
<figure confidence="0.5708946">
Corpus Positive Negative
Mobile Tt 4 (advantage), ✬ (large), ⑥ ✭ (easy to
use), (fast), ✒dK (comfortable), 9❨ (blue-
tooth), ➦ (new), ➵✓ (easy)
DigiCam Tt 4 (advantage), ✎ ç (compact), ✿
</figure>
<equation confidence="0.942482454545454">
(strong;strength), ⑧✫ (telephoto), 4i✁ (dy-
namic), ❤ (comprehensive), 1L (profes-
sional), ±❑ (get started)
MP3 ✎ç (compact), (fast), ✿ (strong;strength),
ô (even), ✭✤ (textual), ❤ (comprehensive),
�❫ (metallic),❆✝ (very)
Monitors ➵✓ (easy), ➦ (new), ➥� (flat screen), ✒
dK (comfortable), ❃ (looks bright), ✏ ✮
(sharp), (bright), �]4i (automatic)
ff (bad), î (poor),,[* (slow), → (no;not), ➽
(difficult;hard), ✑ (less), �✴ (but), TIS` (repair)
✍➈ (regret), ff (bad), î (poor), ,[* (slow), ➋
(dark), ✺ (expensive), ➽ (difficult;hard), ✗✺
(consume much electricity), ◗➍ (plastic), TIS`
(repair)
T- (no;not), î (poor), ff (bad), ✠4 (rather),
✾✱ (simply), ✦ (substandard), ④JJL (crash),
→ (no), ❋✴ (but)
xff (deformation), ❖� (color cast bad), ff
(bad), î (poor), → (no;not), ✎■ (leakage of
light), M❖ (black screen), ifl, (refund;return),
➋ (dark), ➊4i (jitter)
</equation>
<bodyText confidence="0.99958525">
be used for online and real-time sentiment clas-
sification from the Web.
One issue relating to the proposed approach
is that it still depends on the quality of ma-
chine translation and the performance of senti-
ment classification is thus affected by the lan-
guage gap between the source and target lan-
guage. A possible way to alleviate this problem
is to construct a language-specific sentiment lex-
icon automatically from data and use it as the
prior information source to be incorporated into
the LDA model learning.
</bodyText>
<sectionHeader confidence="0.999257" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999855541666666">
Banea, C., R. Mihalcea, J. Wiebe, and S. Hassan.
2008. Multilingual subjectivity analysis using ma-
chine translation. In Proceedings of the EMNLP,
pages 127–135.
Bautin, M., L. Vijayarenu, and S. Skiena. 2008. In-
ternational sentiment analysis for news and blogs.
In Proceedings of the International Conference on
Weblogs and Social Media (ICWSM).
Esuli, A. and F. Sebastiani. 2006. SentiWordNet:
A publicly available lexical resource for opinion
mining. In Proceedings of LREC, volume 6.
Ku, L.W. and H.H. Chen. 2007. Mining opinions
from the Web: Beyond relevance retrieval. Jour-
nal of the American Society for Information Sci-
ence and Technology, 58(12):1838–1850.
McCallum, A., G. Mann, and G. Druck. 2007. Gen-
eralized expectation criteria. Technical Report
2007-60, University of Massachusetts Amherst.
Mihalcea, R., C. Banea, and J. Wiebe. 2007. Learn-
ing multilingual subjective language via cross-
lingual projections. In Proceedings of the ACL,
pages 976–983.
Qiu, L., W. Zhang, C. Hu, and K. Zhao. 2009. Selc: a
self-supervised model for sentiment classification.
In Proceeding of the CIKM, pages 929–936.
Tan, S., Y. Wang, and X. Cheng. 2008. Combining
learn-based and lexicon-based techniques for sen-
timent detection without using labeled examples.
In Proceedings of the SIGIR, pages 743–744.
Tseng, H., P. Chang, G. Andrew, D. Jurafsky, and
C. Manning. 2005. A conditional random field
word segmenter. In Fourth SIGHAN Workshop on
Chinese Language Processing, volume 37.
Wan, X. 2008. Using bilingual knowledge and en-
semble techniques for unsupervised Chinese sen-
timent analysis. In Proceedings of the EMNLP,
pages 553–561.
Wan, X. 2009. Co-training for cross-lingual senti-
ment classification. In Proceedings of the ACL,
pages 235–243.
Zagibalov, T. and J. Carroll. 2008a. Automatic seed
word selection for unsupervised sentiment classifi-
cation of Chinese text. In Proceedings of the COL-
ING, pages 1073–1080.
Zagibalov, T. and J. Carroll. 2008b. Unsupervised
classification of sentiment and objectivity in chi-
nese text. In Proceedings of the IJCNLP, pages
304–311.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.565670">
<title confidence="0.999933">Exploring English Lexicon Knowledge for Chinese Sentiment Analysis</title>
<author confidence="0.782266">Yulan He Harith</author>
<affiliation confidence="0.773971">Knowledge Media</affiliation>
<title confidence="0.83336">The Open</title>
<author confidence="0.897641">Milton Keynes MK AA</author>
<abstract confidence="0.99785164">This paper presents a weakly-supervised method for Chinese sentiment analysis by incorporating lexical prior knowledge obtained from English sentiment lexicons through machine translation. A mechanism is introduced to incorporate the prior information about polaritybearing words obtained from existing sentiment lexicons into latent Dirichlet allocation (LDA) where sentiment labels are considered as topics. Experiments on Chinese product reviews on mobile phones, digital cameras, MP3 players, and monitors demonstrate the feasibility and effectiveness of the proposed approach and show that the weakly supervised LDA model performs as well as supervised classifiers such as Naive Bayes and Support vector Machines with an average of 83% accuracy achieved over a total of 5484 review documents. Moreover, the LDA model is able to extract highly domain-salient polarity words from text.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Banea</author>
<author>R Mihalcea</author>
<author>J Wiebe</author>
<author>S Hassan</author>
</authors>
<title>Multilingual subjectivity analysis using machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the EMNLP,</booktitle>
<pages>127--135</pages>
<contexts>
<context position="1985" citStr="Banea et al., 2008" startWordPosition="293" endWordPosition="296">including blogs, discussion forums, tweets, etc. Research in sentiment analysis has mainly focused on the English language. There have been few studies in sentiment analysis in other languages due to the lack of resources, such as subjectivity lexicons consisting of a list of words marked with their respective polarity (positive, negative or neutral) and manually labeled subjectivity corpora with documents labeled with their polarity. Pilot studies on cross-lingual sentiment analysis utilize machine translation to perform sentiment analysis on the English translation of foreign language text (Banea et al., 2008; Bautin et al., 2008; Wan, 2009). The major problem is that they cannot be generalized well when there is a domain mismatch between the source and target languages. There have also been increasing interests in exploiting bootstrappingstyle approaches for weakly-supervised sentiment classification in languages other than English (Zagibalov and Carroll, 2008b; Zagibalov and Carroll, 2008a; Qiu et al., 2009). Other approaches use ensemble techniques by either combining lexicon-based and corpus-based algorithms (Tan et al., 2008) or combining sentiment classification outputs from different experi</context>
<context position="5003" citStr="Banea et al. (2008)" startWordPosition="739" endWordPosition="742">Section 3. The experimental setup and results of sentiment classification on the Chinese reviews of four different products are presented in Section 4 and 5 respectively. Finally, Section 6 concludes the paper. 2 Related Work Pilot studies on cross-lingual sentiment analysis rely on English corpora for subjectivity classification in other languages. For example, Mihalcea et al. (2007) make use of a bilingual lexicon and a manually translated parallel text to generate the resources to build subjectivity classifiers based on Support Vector Machines (SVMs) and Naive Bayes (NB) in a new language; Banea et al. (2008) use machine translation to produce a corpus in a new language and train SVMs and NB for subjectivity classification in the new language. Bautin et al. (2008) also utilize machine translation to perform sentiment analysis on the English translation of a foreign language text. More recently, Wan (2009) proposed a cotraining approach to tackle the problem of crosslingual sentiment classification by leveraging an available English corpus for Chinese sentiment classification. Similar to the approach proposed in (Banea et al., 2008), Wan’s method also uses machine translation to produced a labeled </context>
</contexts>
<marker>Banea, Mihalcea, Wiebe, Hassan, 2008</marker>
<rawString>Banea, C., R. Mihalcea, J. Wiebe, and S. Hassan. 2008. Multilingual subjectivity analysis using machine translation. In Proceedings of the EMNLP, pages 127–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bautin</author>
<author>L Vijayarenu</author>
<author>S Skiena</author>
</authors>
<title>International sentiment analysis for news and blogs.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Weblogs and Social Media (ICWSM).</booktitle>
<contexts>
<context position="2006" citStr="Bautin et al., 2008" startWordPosition="297" endWordPosition="300">cussion forums, tweets, etc. Research in sentiment analysis has mainly focused on the English language. There have been few studies in sentiment analysis in other languages due to the lack of resources, such as subjectivity lexicons consisting of a list of words marked with their respective polarity (positive, negative or neutral) and manually labeled subjectivity corpora with documents labeled with their polarity. Pilot studies on cross-lingual sentiment analysis utilize machine translation to perform sentiment analysis on the English translation of foreign language text (Banea et al., 2008; Bautin et al., 2008; Wan, 2009). The major problem is that they cannot be generalized well when there is a domain mismatch between the source and target languages. There have also been increasing interests in exploiting bootstrappingstyle approaches for weakly-supervised sentiment classification in languages other than English (Zagibalov and Carroll, 2008b; Zagibalov and Carroll, 2008a; Qiu et al., 2009). Other approaches use ensemble techniques by either combining lexicon-based and corpus-based algorithms (Tan et al., 2008) or combining sentiment classification outputs from different experimental settings (Wan,</context>
<context position="5161" citStr="Bautin et al. (2008)" startWordPosition="767" endWordPosition="770">respectively. Finally, Section 6 concludes the paper. 2 Related Work Pilot studies on cross-lingual sentiment analysis rely on English corpora for subjectivity classification in other languages. For example, Mihalcea et al. (2007) make use of a bilingual lexicon and a manually translated parallel text to generate the resources to build subjectivity classifiers based on Support Vector Machines (SVMs) and Naive Bayes (NB) in a new language; Banea et al. (2008) use machine translation to produce a corpus in a new language and train SVMs and NB for subjectivity classification in the new language. Bautin et al. (2008) also utilize machine translation to perform sentiment analysis on the English translation of a foreign language text. More recently, Wan (2009) proposed a cotraining approach to tackle the problem of crosslingual sentiment classification by leveraging an available English corpus for Chinese sentiment classification. Similar to the approach proposed in (Banea et al., 2008), Wan’s method also uses machine translation to produced a labeled Chinese review corpus from the available labeled English review data. However, in order to alleviate the language gap problem that the underlying distribution</context>
</contexts>
<marker>Bautin, Vijayarenu, Skiena, 2008</marker>
<rawString>Bautin, M., L. Vijayarenu, and S. Skiena. 2008. International sentiment analysis for news and blogs. In Proceedings of the International Conference on Weblogs and Social Media (ICWSM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Esuli</author>
<author>F Sebastiani</author>
</authors>
<title>SentiWordNet: A publicly available lexical resource for opinion mining.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<volume>6</volume>
<contexts>
<context position="13962" citStr="Esuli and Sebastiani, 2006" startWordPosition="2226" endWordPosition="2230">24 Monitor 341 342 4712 4 Experimental Setup We conducted experiments on the four corpora3 which were derived from product reviews harvested from the website IT1684 with each corresponding to different types of product reviews including mobile phones, digital cameras, MP3 players, and monitors. All the reviews were tagged by their authors as either positive or negative overall. The statistics of the four corpora are shown in Table 2. We explored three widely used English sentiment lexicons in our experiments, namely the MPQA subjectivity lexicon, the appraisal lexicon5, and the SentiWordNet6 (Esuli and Sebastiani, 2006). For all these lexicons, we only extracted words bearing positive or negative polarities and discarded words bearing neutral polarity. For SentiWordNet, as it consists of words marked with positive and negative orientation scores ranging from 0 to 1, we extracted a subset of 8,780 opinionated words, by selecting those whose orientation strength is above a threshold of 0.6. We used Google translator toolkit7 to translate these three English lexicons into Chinese. After translation, duplicate entries, words that failed to translate, and words with contradictory polarities were removed. For comp</context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Esuli, A. and F. Sebastiani. 2006. SentiWordNet: A publicly available lexical resource for opinion mining. In Proceedings of LREC, volume 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L W Ku</author>
<author>H H Chen</author>
</authors>
<title>Mining opinions from the Web: Beyond relevance retrieval.</title>
<date>2007</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>58</volume>
<issue>12</issue>
<contexts>
<context position="14667" citStr="Ku and Chen, 2007" startWordPosition="2335" endWordPosition="2338">es and discarded words bearing neutral polarity. For SentiWordNet, as it consists of words marked with positive and negative orientation scores ranging from 0 to 1, we extracted a subset of 8,780 opinionated words, by selecting those whose orientation strength is above a threshold of 0.6. We used Google translator toolkit7 to translate these three English lexicons into Chinese. After translation, duplicate entries, words that failed to translate, and words with contradictory polarities were removed. For comparison, we also tested a Chinese sentiment lexicon, NTU Sentiment Dictionary (NTUSD)8 (Ku and Chen, 2007) which 3http://www.informatics.sussex.ac.uk/ users/tz21/dataZH.tar.gz 4http://product.it168.com 5http://lingcog.iit.edu/arc/ appraisal_lexicon_2007b.tar.gz 6http://sentiwordnet.isti.cnr.it/ 7http://translate.google.com 8http://nlg18.csie.ntu.edu.tw: Table 1: Matched polarity words statistics (positive/negative). Lexicon Chinese English Mobile DigiCam MP3 Monitors Mobile DigiCam MP3 Monitors (a)MPQA 261/253 183/174 162/135 169/147 293/331 220/241 201/153 210/174 (b)Appraisal 279/165 206/127 180/104 198/105 392/271 330/206 304/153 324/157 (c)SentiWN 304/365 222/276 202/213 222/236 394/497 306/39</context>
</contexts>
<marker>Ku, Chen, 2007</marker>
<rawString>Ku, L.W. and H.H. Chen. 2007. Mining opinions from the Web: Beyond relevance retrieval. Journal of the American Society for Information Science and Technology, 58(12):1838–1850.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>G Mann</author>
<author>G Druck</author>
</authors>
<title>Generalized expectation criteria.</title>
<date>2007</date>
<tech>Technical Report 2007-60,</tech>
<institution>University of Massachusetts Amherst.</institution>
<contexts>
<context position="12754" citStr="McCallum et al., 2007" startWordPosition="2035" endWordPosition="2038">constraint such that when word wt is found in the sentiment lexicon, Equation 4 becomes P(lt = k|w,l−t, 0,&apos;Y) a (1 − A) x Qt,k + A x 1I(k = S(wt)) x Qt,k (5) where 0 &lt; A &lt; 1. When A = 1, the hard constraint will be applied; when A = 0, Equation 5 is reduced to the original unconstrained Gibbs sampling as defined in Equation 2. While sentiment prior information is incorporated by modifying conditional probabilities used in Gibbs sampling here, it is also possible to explore other mechanisms to define expectation or posterior constraints, for example, using the generalized expectation criteria (McCallum et al., 2007) to express preferences on expectations of sentiment labels of those lexicon words. We leave the exploitation of other mechanisms of incorporating prior knowledge into model training as future work. The document sentiment is classified based on P(l|d), the probability of sentiment label given document, which can be directly obtained from the document-sentiment distribution. We define that a document d is classified as positive if P(lpos|d) &gt; P(lneg|d), and vice versa. No. of Reviews Vocab Corpus positive Negative Size Mobile 1159 1158 8945 DigiCam 853 852 5716 MP3 390 389 4324 Monitor 341 342 </context>
</contexts>
<marker>McCallum, Mann, Druck, 2007</marker>
<rawString>McCallum, A., G. Mann, and G. Druck. 2007. Generalized expectation criteria. Technical Report 2007-60, University of Massachusetts Amherst.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>C Banea</author>
<author>J Wiebe</author>
</authors>
<title>Learning multilingual subjective language via crosslingual projections.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>976--983</pages>
<contexts>
<context position="4771" citStr="Mihalcea et al. (2007)" startWordPosition="699" endWordPosition="703">timent classification and weakly-supervised sentiment classification in languages other than English are discussed in Section 2. The proposed mechanism of incorporating prior word polarity knowledge into the LDA model is introduced in Section 3. The experimental setup and results of sentiment classification on the Chinese reviews of four different products are presented in Section 4 and 5 respectively. Finally, Section 6 concludes the paper. 2 Related Work Pilot studies on cross-lingual sentiment analysis rely on English corpora for subjectivity classification in other languages. For example, Mihalcea et al. (2007) make use of a bilingual lexicon and a manually translated parallel text to generate the resources to build subjectivity classifiers based on Support Vector Machines (SVMs) and Naive Bayes (NB) in a new language; Banea et al. (2008) use machine translation to produce a corpus in a new language and train SVMs and NB for subjectivity classification in the new language. Bautin et al. (2008) also utilize machine translation to perform sentiment analysis on the English translation of a foreign language text. More recently, Wan (2009) proposed a cotraining approach to tackle the problem of crossling</context>
</contexts>
<marker>Mihalcea, Banea, Wiebe, 2007</marker>
<rawString>Mihalcea, R., C. Banea, and J. Wiebe. 2007. Learning multilingual subjective language via crosslingual projections. In Proceedings of the ACL, pages 976–983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Qiu</author>
<author>W Zhang</author>
<author>C Hu</author>
<author>K Zhao</author>
</authors>
<title>Selc: a self-supervised model for sentiment classification.</title>
<date>2009</date>
<booktitle>In Proceeding of the CIKM,</booktitle>
<pages>929--936</pages>
<contexts>
<context position="2394" citStr="Qiu et al., 2009" startWordPosition="356" endWordPosition="359">labeled with their polarity. Pilot studies on cross-lingual sentiment analysis utilize machine translation to perform sentiment analysis on the English translation of foreign language text (Banea et al., 2008; Bautin et al., 2008; Wan, 2009). The major problem is that they cannot be generalized well when there is a domain mismatch between the source and target languages. There have also been increasing interests in exploiting bootstrappingstyle approaches for weakly-supervised sentiment classification in languages other than English (Zagibalov and Carroll, 2008b; Zagibalov and Carroll, 2008a; Qiu et al., 2009). Other approaches use ensemble techniques by either combining lexicon-based and corpus-based algorithms (Tan et al., 2008) or combining sentiment classification outputs from different experimental settings (Wan, 2008). Nevertheless, all these approaches are either complex or require careful tuning of domain and data specific parameters. This paper proposes a weakly-supervised approach for Chinese sentiment classification by incorporating language-specific lexical knowledge obtained from available English sentiment lexicons through machine translation. Unlike other cross-lingual sentiment clas</context>
<context position="7721" citStr="Qiu et al. (2009)" startWordPosition="1163" endWordPosition="1166">ing data. Sentiment direction of a document is then determined by the sum of sentiment scores of all the sentiment-bearing lexical items found in the document. The problem with this approach is that there is no principal way to set the optimal number of iterations. They then suggested an iteration control method in (Zagibalov and Carroll, 2008a) where iterative training stops when there is no change to the classification of any document over the previous two iterations. However, this does not necessarily correlate to the best classification accuracy. Similar to (Zagibalov and Carroll, 2008b), Qiu et al. (2009) also uses a lexicon-based iterative process as the first phase to iteratively enlarge an initial sentiment dictionary. But instead documents is denoted by C = {d1, d2,..., dD}; each document in the corpus is a sequence of Nd words denoted by d = (w1, w2, ..., wNd), and each word in the document is an item from a vocabulary index with V distinct terms denoted by 11, 2,..., V }. The generative process is as follows: • Choose distributions cp — Dir(Q). • For each document d E [1, D], choose distributions 7rd — Dir(&apos;Y). • For each of the Nd word position wt, choose a sentiment label lt — Multinom</context>
</contexts>
<marker>Qiu, Zhang, Hu, Zhao, 2009</marker>
<rawString>Qiu, L., W. Zhang, C. Hu, and K. Zhao. 2009. Selc: a self-supervised model for sentiment classification. In Proceeding of the CIKM, pages 929–936.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tan</author>
<author>Y Wang</author>
<author>X Cheng</author>
</authors>
<title>Combining learn-based and lexicon-based techniques for sentiment detection without using labeled examples.</title>
<date>2008</date>
<booktitle>In Proceedings of the SIGIR,</booktitle>
<pages>743--744</pages>
<contexts>
<context position="2517" citStr="Tan et al., 2008" startWordPosition="373" endWordPosition="376">ment analysis on the English translation of foreign language text (Banea et al., 2008; Bautin et al., 2008; Wan, 2009). The major problem is that they cannot be generalized well when there is a domain mismatch between the source and target languages. There have also been increasing interests in exploiting bootstrappingstyle approaches for weakly-supervised sentiment classification in languages other than English (Zagibalov and Carroll, 2008b; Zagibalov and Carroll, 2008a; Qiu et al., 2009). Other approaches use ensemble techniques by either combining lexicon-based and corpus-based algorithms (Tan et al., 2008) or combining sentiment classification outputs from different experimental settings (Wan, 2008). Nevertheless, all these approaches are either complex or require careful tuning of domain and data specific parameters. This paper proposes a weakly-supervised approach for Chinese sentiment classification by incorporating language-specific lexical knowledge obtained from available English sentiment lexicons through machine translation. Unlike other cross-lingual sentiment classification methods which often require labeled corpora for training and therefore hinder their applicability for cross-doma</context>
<context position="9843" citStr="Tan et al. (2008)" startWordPosition="1559" endWordPosition="1562">bbs sampling process. Let Qt,k = N−t N−t wt,k + Q k,d + &apos;Yk (3) Nkt+ V Q x N−t d +Ek&apos;Yk P(lt = kJw,l−t, Q,γ) a N−t wt,k + Q Nkt+ V Q x N−t k,d + &apos;Yk ,(2) N−t d +Ek&apos;Yk of using a one-word seed dictionary as in (Zagibalov and Carroll, 2008b), they started with a much larger HowNet Chinese sentiment dictionary1 as the initial lexicon. Documents classified by the first phase are taken as the training set to train the SVMs which are subsequently used to revise the results produced by the first phase. Other researchers investigated ensemble techniques for weakly-supervised sentiment classification. Tan et al. (2008) proposed a combination of lexicon-based and corpus-based approaches that first labels some examples from a give domain using a sentiment lexicon and then trains a supervised classifier based on the labeled ones from the first stage. Wan (2008) combined sentiment scores calculated from Chinese product reviews using the Chinese HowNet sentiment dictionary and from the English translation of Chinese reviews using the English MPQA subjectivity lexicon2. Various weighting strategies were explored to combine sentiment classification outputs from different experimental settings in order to improve c</context>
</contexts>
<marker>Tan, Wang, Cheng, 2008</marker>
<rawString>Tan, S., Y. Wang, and X. Cheng. 2008. Combining learn-based and lexicon-based techniques for sentiment detection without using labeled examples. In Proceedings of the SIGIR, pages 743–744.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Tseng</author>
<author>P Chang</author>
<author>G Andrew</author>
<author>D Jurafsky</author>
<author>C Manning</author>
</authors>
<title>A conditional random field word segmenter.</title>
<date>2005</date>
<booktitle>In Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<volume>37</volume>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>Tseng, H., P. Chang, G. Andrew, D. Jurafsky, and C. Manning. 2005. A conditional random field word segmenter. In Fourth SIGHAN Workshop on Chinese Language Processing, volume 37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wan</author>
</authors>
<title>Using bilingual knowledge and ensemble techniques for unsupervised Chinese sentiment analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the EMNLP,</booktitle>
<pages>553--561</pages>
<contexts>
<context position="2612" citStr="Wan, 2008" startWordPosition="387" endWordPosition="388">2008; Wan, 2009). The major problem is that they cannot be generalized well when there is a domain mismatch between the source and target languages. There have also been increasing interests in exploiting bootstrappingstyle approaches for weakly-supervised sentiment classification in languages other than English (Zagibalov and Carroll, 2008b; Zagibalov and Carroll, 2008a; Qiu et al., 2009). Other approaches use ensemble techniques by either combining lexicon-based and corpus-based algorithms (Tan et al., 2008) or combining sentiment classification outputs from different experimental settings (Wan, 2008). Nevertheless, all these approaches are either complex or require careful tuning of domain and data specific parameters. This paper proposes a weakly-supervised approach for Chinese sentiment classification by incorporating language-specific lexical knowledge obtained from available English sentiment lexicons through machine translation. Unlike other cross-lingual sentiment classification methods which often require labeled corpora for training and therefore hinder their applicability for cross-domain sentiment analysis, the proposed approach does not require labeled documents. Moreover, as o</context>
<context position="10087" citStr="Wan (2008)" startWordPosition="1600" endWordPosition="1601">ed with a much larger HowNet Chinese sentiment dictionary1 as the initial lexicon. Documents classified by the first phase are taken as the training set to train the SVMs which are subsequently used to revise the results produced by the first phase. Other researchers investigated ensemble techniques for weakly-supervised sentiment classification. Tan et al. (2008) proposed a combination of lexicon-based and corpus-based approaches that first labels some examples from a give domain using a sentiment lexicon and then trains a supervised classifier based on the labeled ones from the first stage. Wan (2008) combined sentiment scores calculated from Chinese product reviews using the Chinese HowNet sentiment dictionary and from the English translation of Chinese reviews using the English MPQA subjectivity lexicon2. Various weighting strategies were explored to combine sentiment classification outputs from different experimental settings in order to improve classification accuracy. Nevertheless, all these weakly-supervised sentiment classification approaches are rather complex and require either iterative training or careful tuning of domain and data specific parameters, and hence unsuitable for on</context>
</contexts>
<marker>Wan, 2008</marker>
<rawString>Wan, X. 2008. Using bilingual knowledge and ensemble techniques for unsupervised Chinese sentiment analysis. In Proceedings of the EMNLP, pages 553–561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wan</author>
</authors>
<title>Co-training for cross-lingual sentiment classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>235--243</pages>
<contexts>
<context position="2018" citStr="Wan, 2009" startWordPosition="301" endWordPosition="302">s, etc. Research in sentiment analysis has mainly focused on the English language. There have been few studies in sentiment analysis in other languages due to the lack of resources, such as subjectivity lexicons consisting of a list of words marked with their respective polarity (positive, negative or neutral) and manually labeled subjectivity corpora with documents labeled with their polarity. Pilot studies on cross-lingual sentiment analysis utilize machine translation to perform sentiment analysis on the English translation of foreign language text (Banea et al., 2008; Bautin et al., 2008; Wan, 2009). The major problem is that they cannot be generalized well when there is a domain mismatch between the source and target languages. There have also been increasing interests in exploiting bootstrappingstyle approaches for weakly-supervised sentiment classification in languages other than English (Zagibalov and Carroll, 2008b; Zagibalov and Carroll, 2008a; Qiu et al., 2009). Other approaches use ensemble techniques by either combining lexicon-based and corpus-based algorithms (Tan et al., 2008) or combining sentiment classification outputs from different experimental settings (Wan, 2008). Neve</context>
<context position="5305" citStr="Wan (2009)" startWordPosition="790" endWordPosition="791">ivity classification in other languages. For example, Mihalcea et al. (2007) make use of a bilingual lexicon and a manually translated parallel text to generate the resources to build subjectivity classifiers based on Support Vector Machines (SVMs) and Naive Bayes (NB) in a new language; Banea et al. (2008) use machine translation to produce a corpus in a new language and train SVMs and NB for subjectivity classification in the new language. Bautin et al. (2008) also utilize machine translation to perform sentiment analysis on the English translation of a foreign language text. More recently, Wan (2009) proposed a cotraining approach to tackle the problem of crosslingual sentiment classification by leveraging an available English corpus for Chinese sentiment classification. Similar to the approach proposed in (Banea et al., 2008), Wan’s method also uses machine translation to produced a labeled Chinese review corpus from the available labeled English review data. However, in order to alleviate the language gap problem that the underlying distributions between the source and target language are different, Wan builds two SVM classifiers, one based on English features and the other based on Chi</context>
</contexts>
<marker>Wan, 2009</marker>
<rawString>Wan, X. 2009. Co-training for cross-lingual sentiment classification. In Proceedings of the ACL, pages 235–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Zagibalov</author>
<author>J Carroll</author>
</authors>
<title>Automatic seed word selection for unsupervised sentiment classification of Chinese text.</title>
<date>2008</date>
<booktitle>In Proceedings of the COLING,</booktitle>
<pages>1073--1080</pages>
<contexts>
<context position="2344" citStr="Zagibalov and Carroll, 2008" startWordPosition="348" endWordPosition="351">l) and manually labeled subjectivity corpora with documents labeled with their polarity. Pilot studies on cross-lingual sentiment analysis utilize machine translation to perform sentiment analysis on the English translation of foreign language text (Banea et al., 2008; Bautin et al., 2008; Wan, 2009). The major problem is that they cannot be generalized well when there is a domain mismatch between the source and target languages. There have also been increasing interests in exploiting bootstrappingstyle approaches for weakly-supervised sentiment classification in languages other than English (Zagibalov and Carroll, 2008b; Zagibalov and Carroll, 2008a; Qiu et al., 2009). Other approaches use ensemble techniques by either combining lexicon-based and corpus-based algorithms (Tan et al., 2008) or combining sentiment classification outputs from different experimental settings (Wan, 2008). Nevertheless, all these approaches are either complex or require careful tuning of domain and data specific parameters. This paper proposes a weakly-supervised approach for Chinese sentiment classification by incorporating language-specific lexical knowledge obtained from available English sentiment lexicons through machine tran</context>
<context position="6842" citStr="Zagibalov and Carroll (2008" startWordPosition="1021" endWordPosition="1024">ra (or the translated target language corpora generated by machine translation). As such, they cannot be generalized well when there is a domain mismatch between the source and target language. For example, For example, the word ‘compact’ might express positive polarity when used to describe a digital camera, but it could have negative orientation if it is used to describe a hotel room. Thus, classifiers trained on one domain often fail to produce satisfactory results when shifting to another domain. Recent efforts have also been made for weakly-supervised sentiment classification in Chinese. Zagibalov and Carroll (2008b) starts with a one-word sentiment seed vocabulary and use iterative retraining to gradually enlarge the seed vocabulary by adding more sentimentbearing lexical items based on their relative frequency in both the positive and negative parts of the current training data. Sentiment direction of a document is then determined by the sum of sentiment scores of all the sentiment-bearing lexical items found in the document. The problem with this approach is that there is no principal way to set the optimal number of iterations. They then suggested an iteration control method in (Zagibalov and Carrol</context>
<context position="9463" citStr="Zagibalov and Carroll, 2008" startWordPosition="1499" endWordPosition="1503">label k; Nk,d is the number of times sentiment label k has been assigned to some word tokens in document d; Nd is the total number of words in the document collection. Each words in documents can either bear positive polarity (lt = 1), or negative polarity (lt = 2), or is neutral (lt = 0). We now show how to incorporate polarized words in sentiment lexicons as prior information in the Gibbs sampling process. Let Qt,k = N−t N−t wt,k + Q k,d + &apos;Yk (3) Nkt+ V Q x N−t d +Ek&apos;Yk P(lt = kJw,l−t, Q,γ) a N−t wt,k + Q Nkt+ V Q x N−t k,d + &apos;Yk ,(2) N−t d +Ek&apos;Yk of using a one-word seed dictionary as in (Zagibalov and Carroll, 2008b), they started with a much larger HowNet Chinese sentiment dictionary1 as the initial lexicon. Documents classified by the first phase are taken as the training set to train the SVMs which are subsequently used to revise the results produced by the first phase. Other researchers investigated ensemble techniques for weakly-supervised sentiment classification. Tan et al. (2008) proposed a combination of lexicon-based and corpus-based approaches that first labels some examples from a give domain using a sentiment lexicon and then trains a supervised classifier based on the labeled ones from the</context>
<context position="21197" citStr="Zagibalov and Carroll, 2008" startWordPosition="3289" endWordPosition="3292">any Chinese sentiment lexicon, MPQA subjectivity lexicon appears to be the best candidate to be used to provide sentiment prior information to the LDA model for Chinese sentiment classification. We also conducted experiments by including the Chinese sentiment lexicon NTUSD and found that the combination of MPQA, SentiWordNet, and NTUSD gives the best overall classification accuracy with 83.31% achieved. For comparison purposes, we list the 10-fold cross validation results obtained using the supervised classifiers, Naive Bayes and SVMs, trained on the labeled corpora as previously reported in (Zagibalov and Carroll, 2008a). It can be observed that using only English lexicons (the combination of MPQA and SentiWordNet), we obtain better results than both NB and SVMs on the MP3 corpus. With an additional inclusion of NTUSD, LDA outperforms NB and SVMs on both DigiCam and MP3. Furthermore, LDA gives a better overall accuracy when compared to SVMs. Thus, we may conclude that the unsupervised LDA model performs as well as the supervised classifiers such as NB and SVMs on the Chinese product review corpora. 5.2 Results with Translated Corpora We ran a second set of experiments on the translated Chinese product revie</context>
<context position="23935" citStr="Zagibalov and Carroll, 2008" startWordPosition="3732" endWordPosition="3735">re expect more ambiguities being introduced which might result in the change of document polarities. 5.3 Extracted Polarity-Bearing Words LDA is able to extract polarity-bearing words. Table 4 lists some of the polarity words identified by the LDA model which are not found in the original sentiment lexicons. We can see that LDA is indeed able to recognize domain-specific positive or negative words, for example, �❨ (bluetooth) for mobile phones, ✎I1� (compact) for digital cameras, 4,�-❫ (metallic) for MP3, ➥ � (flat screen) and x❜ (deformation) for monitors. The iterative approach proposed in (Zagibalov and Carroll, 2008a) can also automatically acquire polarity words from data. However, it appears that only positive words were identified by their approach. Our proposed LDA model can extract both positive and negative words and most of them are highly domain-salient as can be seen from Table 4. 6 Conclusions This paper has proposed a mechanism to incorporate prior information about polarity words from English sentiment lexicons into LDA model learning for weakly-supervised Chinese sentiment classification. Experimental results of sentiment classification on Chinese product reviews show that in the absence of </context>
</contexts>
<marker>Zagibalov, Carroll, 2008</marker>
<rawString>Zagibalov, T. and J. Carroll. 2008a. Automatic seed word selection for unsupervised sentiment classification of Chinese text. In Proceedings of the COLING, pages 1073–1080.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Zagibalov</author>
<author>J Carroll</author>
</authors>
<title>Unsupervised classification of sentiment and objectivity in chinese text.</title>
<date>2008</date>
<booktitle>In Proceedings of the IJCNLP,</booktitle>
<pages>304--311</pages>
<contexts>
<context position="2344" citStr="Zagibalov and Carroll, 2008" startWordPosition="348" endWordPosition="351">l) and manually labeled subjectivity corpora with documents labeled with their polarity. Pilot studies on cross-lingual sentiment analysis utilize machine translation to perform sentiment analysis on the English translation of foreign language text (Banea et al., 2008; Bautin et al., 2008; Wan, 2009). The major problem is that they cannot be generalized well when there is a domain mismatch between the source and target languages. There have also been increasing interests in exploiting bootstrappingstyle approaches for weakly-supervised sentiment classification in languages other than English (Zagibalov and Carroll, 2008b; Zagibalov and Carroll, 2008a; Qiu et al., 2009). Other approaches use ensemble techniques by either combining lexicon-based and corpus-based algorithms (Tan et al., 2008) or combining sentiment classification outputs from different experimental settings (Wan, 2008). Nevertheless, all these approaches are either complex or require careful tuning of domain and data specific parameters. This paper proposes a weakly-supervised approach for Chinese sentiment classification by incorporating language-specific lexical knowledge obtained from available English sentiment lexicons through machine tran</context>
<context position="6842" citStr="Zagibalov and Carroll (2008" startWordPosition="1021" endWordPosition="1024">ra (or the translated target language corpora generated by machine translation). As such, they cannot be generalized well when there is a domain mismatch between the source and target language. For example, For example, the word ‘compact’ might express positive polarity when used to describe a digital camera, but it could have negative orientation if it is used to describe a hotel room. Thus, classifiers trained on one domain often fail to produce satisfactory results when shifting to another domain. Recent efforts have also been made for weakly-supervised sentiment classification in Chinese. Zagibalov and Carroll (2008b) starts with a one-word sentiment seed vocabulary and use iterative retraining to gradually enlarge the seed vocabulary by adding more sentimentbearing lexical items based on their relative frequency in both the positive and negative parts of the current training data. Sentiment direction of a document is then determined by the sum of sentiment scores of all the sentiment-bearing lexical items found in the document. The problem with this approach is that there is no principal way to set the optimal number of iterations. They then suggested an iteration control method in (Zagibalov and Carrol</context>
<context position="9463" citStr="Zagibalov and Carroll, 2008" startWordPosition="1499" endWordPosition="1503">label k; Nk,d is the number of times sentiment label k has been assigned to some word tokens in document d; Nd is the total number of words in the document collection. Each words in documents can either bear positive polarity (lt = 1), or negative polarity (lt = 2), or is neutral (lt = 0). We now show how to incorporate polarized words in sentiment lexicons as prior information in the Gibbs sampling process. Let Qt,k = N−t N−t wt,k + Q k,d + &apos;Yk (3) Nkt+ V Q x N−t d +Ek&apos;Yk P(lt = kJw,l−t, Q,γ) a N−t wt,k + Q Nkt+ V Q x N−t k,d + &apos;Yk ,(2) N−t d +Ek&apos;Yk of using a one-word seed dictionary as in (Zagibalov and Carroll, 2008b), they started with a much larger HowNet Chinese sentiment dictionary1 as the initial lexicon. Documents classified by the first phase are taken as the training set to train the SVMs which are subsequently used to revise the results produced by the first phase. Other researchers investigated ensemble techniques for weakly-supervised sentiment classification. Tan et al. (2008) proposed a combination of lexicon-based and corpus-based approaches that first labels some examples from a give domain using a sentiment lexicon and then trains a supervised classifier based on the labeled ones from the</context>
<context position="21197" citStr="Zagibalov and Carroll, 2008" startWordPosition="3289" endWordPosition="3292">any Chinese sentiment lexicon, MPQA subjectivity lexicon appears to be the best candidate to be used to provide sentiment prior information to the LDA model for Chinese sentiment classification. We also conducted experiments by including the Chinese sentiment lexicon NTUSD and found that the combination of MPQA, SentiWordNet, and NTUSD gives the best overall classification accuracy with 83.31% achieved. For comparison purposes, we list the 10-fold cross validation results obtained using the supervised classifiers, Naive Bayes and SVMs, trained on the labeled corpora as previously reported in (Zagibalov and Carroll, 2008a). It can be observed that using only English lexicons (the combination of MPQA and SentiWordNet), we obtain better results than both NB and SVMs on the MP3 corpus. With an additional inclusion of NTUSD, LDA outperforms NB and SVMs on both DigiCam and MP3. Furthermore, LDA gives a better overall accuracy when compared to SVMs. Thus, we may conclude that the unsupervised LDA model performs as well as the supervised classifiers such as NB and SVMs on the Chinese product review corpora. 5.2 Results with Translated Corpora We ran a second set of experiments on the translated Chinese product revie</context>
<context position="23935" citStr="Zagibalov and Carroll, 2008" startWordPosition="3732" endWordPosition="3735">re expect more ambiguities being introduced which might result in the change of document polarities. 5.3 Extracted Polarity-Bearing Words LDA is able to extract polarity-bearing words. Table 4 lists some of the polarity words identified by the LDA model which are not found in the original sentiment lexicons. We can see that LDA is indeed able to recognize domain-specific positive or negative words, for example, �❨ (bluetooth) for mobile phones, ✎I1� (compact) for digital cameras, 4,�-❫ (metallic) for MP3, ➥ � (flat screen) and x❜ (deformation) for monitors. The iterative approach proposed in (Zagibalov and Carroll, 2008a) can also automatically acquire polarity words from data. However, it appears that only positive words were identified by their approach. Our proposed LDA model can extract both positive and negative words and most of them are highly domain-salient as can be seen from Table 4. 6 Conclusions This paper has proposed a mechanism to incorporate prior information about polarity words from English sentiment lexicons into LDA model learning for weakly-supervised Chinese sentiment classification. Experimental results of sentiment classification on Chinese product reviews show that in the absence of </context>
</contexts>
<marker>Zagibalov, Carroll, 2008</marker>
<rawString>Zagibalov, T. and J. Carroll. 2008b. Unsupervised classification of sentiment and objectivity in chinese text. In Proceedings of the IJCNLP, pages 304–311.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>