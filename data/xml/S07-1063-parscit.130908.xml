<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.053154">
<title confidence="0.995341">
SHEF: Semantic Tagging and Summarization Techniques Applied to
Cross-document Coreference
</title>
<author confidence="0.996618">
Horacio Saggion
</author>
<affiliation confidence="0.997663">
Department of Computer Science
University of Sheffield
</affiliation>
<address confidence="0.895342">
211 Portobello Street - Sheffield, England, UK, S1 4DP
Tel: +44-114-222-1947
Fax: +44-114-222-1810
</address>
<email confidence="0.9982">
saggion@dcs.shef.ac.uk
</email>
<sectionHeader confidence="0.995627" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999988181818182">
We describe experiments for the cross-
document coreference task in SemEval
2007. Our cross-document coreference sys-
tem uses an in-house agglomerative clus-
tering implementation to group documents
referring to the same entity. Clustering
uses vector representations created by sum-
marization and semantic tagging analysis
components. We present evaluation results
for four system configurations demonstrat-
ing the potential of the applied techniques.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999713548387097">
Cross-document coreference resolution is the task of
identifying if two mentions of the same (or similar)
name in different sources refer to the same individ-
ual. Deciding if two documents refer to the same
individual is a difficult problem because names are
highly ambiguous. Automatic techniques for solv-
ing this problem are required not only for better ac-
cess to information but also in natural language pro-
cessing applications such as multidocument summa-
rization and information extraction. Here, we con-
centrate on the following SemEval 2007 Web Peo-
ple Search Task (Artiles et al., 2007): a search en-
gine user types in a person name as a query. Instead
of ranking web pages, an ideal system should orga-
nize search results in as many clusters as there are
different people sharing the same name in the doc-
uments returned by the search engine. The input is,
therefore, the results given by a web search engine
using a person name as query. The output is a num-
ber of sets, each containing documents referring to
the same individual.
As past and recent research (Bagga and Baldwin,
1998; Phan et al., 2006), we have addressed the
problem as a document clustering problem. For our
first participation in SemEval 2007, we use two ap-
proaches: a lexical or bag-of-words approach and a
semantic based approach. We have implemented our
own clustering algorithms but rely on available ex-
traction and summarization technology developed in
our laboratory to produce document representations
used as input for the clustering procedure.
</bodyText>
<sectionHeader confidence="0.933901" genericHeader="method">
2 Clustering Algorithm
</sectionHeader>
<bodyText confidence="0.9995914">
We have implemented an agglomerative clustering
algorithm. The input to the algorithm is a set of
document representations implemented as vectors of
terms and weights. Initially, there are as many clus-
ters as input documents; as the algorithm proceeds
clusters are merged until a certain termination condi-
tion is reached. The algorithm computes the similar-
ity between vector representations in order to decide
whether or not to merge two clusters. The similar-
ity metric we use is the cosine of the angle between
two vectors. This metric gives value one for identi-
cal vectors and zero for vectors which are orthogo-
nal (non related). Various options have been imple-
mented in order to measure how close two clusters
are, but for the experiments reported here we have
used the following approach: the similarity between
two clusters (sim ) is equivalent to the “document”
similarity (sim ) between the two more similar doc-
uments in the two clusters; the following formula is
used:
</bodyText>
<page confidence="0.9547">
292
</page>
<bodyText confidence="0.839805">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 292–295,
Prague, June 2007. c�2007 Association for Computational Linguistics
</bodyText>
<equation confidence="0.526062">
sim (C ,C )
max sim (d ,d✒ )
Where are clusters, are document represen-
</equation>
<bodyText confidence="0.996878">
tations (e.g., vectors), and sim is the cosine metric.
If this similarity is greater than a threshold – ex-
perimentally obtained – the two clusters are merged
together. At each iteration the most similar pair of
clusters is merged. If this similarity is less than a
certain threshold the algorithm stops.
</bodyText>
<sectionHeader confidence="0.960113" genericHeader="method">
3 Extraction and Summarization
</sectionHeader>
<bodyText confidence="0.99800075">
The input for analysis is a set of documents and
a person name (first name and last name). The
documents are analysed by the default GATE1
ANNIE system (Cunningham et al., 2002) and
single document summarization modules (Saggion
and Gaizauskas, 2004b) from our summarization
toolkit2. No attempt is made to analyse or use con-
textual information given with the input document.
The processing elements include:
Document tokenisation
Sentence splitting
Parts-of-speech tagging
Named Entity Recognition using a gazetteer
lookup module and regular expressions
Named entity coreference using an ortho-
graphic name matcher
Named entities of type person, organization, ad-
dress, date, and location are considered relevant
document terms and stored in a special named en-
tity called Mention.
Coreference chains are created and analysed and
if they contain an entity matching the target person’s
surname, all elements of the chain are marked. Ex-
tractive summaries are created for each document,
a sentence belongs to the summary if it contains a
mention which is coreferent with the target entity.
Using language resources creation modules from
the summarization tool, two frequency tables are
</bodyText>
<footnote confidence="0.996832">
1http://gate.ac.uk
2http://www.dcs.shef.ac.uk/˜saggion
</footnote>
<bodyText confidence="0.999758571428571">
created for each document set (or person): (i) an in-
verted document frequency table for words (no nor-
malisation is applied); and (ii) an inverted frequency
table for Mentions (the full entity string is used, no
normalisation is applied).
Statistics (term frequencies and tj*idj) are com-
puted over tokens and Mentions using the appropri-
ate tables (these tools are part of the summarization
toolkit) and vector representations created for each
document (same as in (Bagga and Baldwin, 1998)).
Two types of representations were considered for
these experiments: (i) full document or summary
(terms in the summary are considered for vector cre-
ation); and (ii) words or Mentions.
</bodyText>
<sectionHeader confidence="0.998355" genericHeader="method">
4 System Configurations
</sectionHeader>
<bodyText confidence="0.998983304347826">
Four system configurations were prepared for Se-
mEval:
System I: vector representations were created
for full documents. Words were used as terms
and local inverted document frequencies used
(word frequencies) for weighting.
System II: vector representations were created
for full documents. Mentions were used as
terms and local inverted document frequencies
used (Mentions frequencies) for weighting.
System III: vector representations were created
for person summaries. Words were used as
terms and local inverted document frequencies
used (word frequencies) for weighting.
System IV: vector representations were created
for person summaries. Mentions were used as
terms and local inverted document frequencies
used (Mentions frequencies) for weighting.
Because only one system configuration was al-
lowed per participant team, we decided to select
System II for official evaluation interested in eval-
uating the effect of semantic information in the clus-
tering process.
</bodyText>
<sectionHeader confidence="0.957508" genericHeader="method">
5 Parameter Setting and Results
</sectionHeader>
<bodyText confidence="0.999500666666667">
Evaluation of the task was carried out using standard
clustering evaluation measures of ”purity” and ”in-
verse purity” (Hotho et al., 2003), and the harmonic
</bodyText>
<page confidence="0.989187">
293
</page>
<table confidence="0.999967">
Configuration Purity Inv.Purity F-Score
System I 0.68 0.85 0.74
System II 0.62 0.85 0.68
System III 0.84 0.70 0.74
System IV 0.65 0.75 0.64
</table>
<tableCaption confidence="0.874305666666667">
Table 1: Results for our configurations omitting one
set. System II was the system we evaluated in Se-
mEval 2007.
</tableCaption>
<bodyText confidence="0.999852125">
mean of purity and inverse purity: F-score. We esti-
mated the threshold for the clustering algorithm us-
ing the ECDL subset of the training data provided
by SemEval. We applied the clustering algorithm to
each document set and computed purity, inverse pu-
rity, and F-score at each iteration of the algorithm,
recording the similarity value of each newly created
cluster. The similarity values for the best clustering
results (best F-score) were recorded, and the max-
imum and minimum values discarded. The rest of
the values were averaged to obtain an estimate of
the optimal threshold. Two different thresholds were
obtained: 0.10 for word vectors and 0.12 for named
entity vectors.
Results for the test set in SemEval are presented
in Table 1 (One set – “Jerry Hobbs” – was ignored
when computing these numbers: due to a failure
during document analysis this set could not be clus-
tered. The error was identified too close to the sub-
mission’s date to allow us to re-process the cluster).
Our official submission System II (SHEF in the offi-
cial results) obtained an F-score of 0.66 positioning
itself in 5th place (out of 16 systems). Our best con-
figuration obtained 0.74 F-score, so a fourth place
would be in theory possible.
Our system obtained an F-score greater than the
average of 0.60 of all participant systems. Our
optimal configurations (System I and System II)
both perform similarly with respect to F-score.
While System I favours “inverse purity”, System III
favours “purity”. Results for every individual set are
reported in the Appendix.
</bodyText>
<sectionHeader confidence="0.997561" genericHeader="method">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999976941176471">
We have presented a system used to participate in
the SemEval 2007 Web People Search task. The
system uses an in-house clustering algorithm and
available extraction and summarization techniques
to produce representations needed by the clustering
algorithm. Although the configuration we submit-
ted was suboptimal, we have obtained good results;
in fact all our system configurations produce results
well above the average of all participants. Our future
work will explore how the use of contextual infor-
mation available on the web can lead to better per-
formance. We will explore if a similar approach to
our method for creating profiles or answering def-
inition questions (Saggion and Gaizauskas, 2004a)
which uses co-occurence information to identify
pieces of information related to a given entity can
be applied here.
</bodyText>
<sectionHeader confidence="0.984459" genericHeader="method">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999352333333333">
This work was partially supported by the EU-funded
MUSING project (IST-2004-027097) and the EU-
funded LIRICS project (eContent project 22236).
</bodyText>
<sectionHeader confidence="0.987891" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.994509357142857">
J. Artiles, J. Gonzalo, and S. Sekine. 2007. The
SemEval-2007 WePS Evaluation: Establishing a
benchmark for Web People Search Task. In Proceed-
ings of Semeval 2007, Association for Computational
Linguistics.
A. Bagga and B. Baldwin. 1998. Entity-Based Cross-
Document Coreferencing Using the Vector Space
Model. In Proceedings of the 36th Annual Meeting
of the Association for Computational Linguistics and
the 17th International Conference on Computational
Linguistics (COLING-ACL’98), pages 79–85.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A Framework and Graphical
Development Environment for Robust NLP Tools and
Applications. In Proceedings of the 40th Anniversary
Meeting ofthe Association for Computational Linguis-
tics (ACL’02).
A. Hotho, S. Staab, and G. Stumme. 2003. WordNet im-
proves text document clustering. In Proc. of the SIGIR
2003 Semantic Web Workshop.
X.-H. Phan, L.-M. Nguyen, and S. Horiguchi. 2006.
Personal name resolution crossover documents by a
semantics-based approach. IEICE Trans. Inf. &amp; Syst.,
Feb 2006.
H. Saggion and R. Gaizauskas. 2004a. Mining on-line
sources for definition knowledge. In Proceedings of
the 17th FLAIRS 2004, Miami Bearch, Florida, USA,
May 17-19. AAAI.
</reference>
<page confidence="0.995547">
294
</page>
<bodyText confidence="0.53167225">
H. Saggion and R. Gaizauskas. 2004b. Multi-document
summarization by cluster/profile relevance and redun-
dancy removal. In Proceedings of the Document Un-
derstanding Conference 2004. NIST.
</bodyText>
<sectionHeader confidence="0.991598" genericHeader="method">
Appendix I: Detailed Results
</sectionHeader>
<bodyText confidence="0.999197">
The following tables present purity, inverse purity,
and F-score results for all sets and systems. These
results were computed after re-processing the “Jerry
Hobbs” missing set.
</bodyText>
<table confidence="0.999984424242425">
Person System I System II
Pur. I-Pur. F Pur. I-Pur. F
Alvin Cooper 0.72 0.87 0.79 0.86 0.70 0.77
Arthur Morgan 0.90 0.83 0.86 0.75 0.92 0.83
Chris Brockett 0.87 0.85 0.86 0.94 0.67 0.78
Dekang Lin 1.00 0.63 0.77 1.00 0.66 0.79
Frank Keller 0.68 0.81 0.74 0.65 0.66 0.66
George Foster 0.61 0.83 0.71 0.45 0.88 0.60
Harry Hughes 0.82 0.80 0.81 0.71 0.93 0.80
James Curran 0.76 0.74 0.75 0.53 0.84 0.65
James Davidson 0.74 0.91 0.82 0.59 0.90 0.71
James Hamilton 0.52 0.90 0.66 0.25 0.97 0.39
James Morehead 0.38 0.91 0.54 0.39 0.92 0.55
Jerry Hobbs 0.67 0.86 0.75 0.61 0.85 0.71
John Nelson 0.64 0.93 0.76 0.56 0.90 0.69
Jonathan Brooks 0.70 0.89 0.78 0.54 0.89 0.67
Jude Brown 0.75 0.80 0.78 0.74 0.77 0.75
Karen Peterson 0.60 0.92 0.72 0.19 1.00 0.32
Leon Barrett 0.75 0.84 0.80 0.43 0.96 0.59
Marcy Jackson 0.60 0.91 0.72 0.87 0.85 0.86
Mark Johnson 0.57 0.86 0.68 0.33 0.94 0.49
Martha Edwards 0.49 0.96 0.65 0.43 0.91 0.58
Neil Clark 0.74 0.83 0.78 0.60 0.76 0.67
Patrick Killen 0.83 0.77 0.80 0.82 0.77 0.79
Robert Moore 0.64 0.78 0.71 0.44 0.91 0.60
Sharon Goldwater 1.00 0.80 0.89 1.00 0.80 0.89
Stephan Johnson 0.84 0.87 0.85 0.97 0.69 0.81
Stephen Clark 0.63 0.87 0.73 0.57 0.83 0.67
Thomas Fraser 0.51 0.94 0.66 0.44 0.94 0.60
Thomas Kirk 0.66 0.94 0.78 0.87 0.92 0.90
Violet Howard 0.34 0.96 0.51 0.71 0.90 0.80
William Dickson 0.55 0.94 0.70 0.38 0.95 0.54
AVERAGES 0.68 0.86 0.74 0.62 0.85 0.68
Person System III System VI
Pur. I-Pur. F Pur. I-Pur. F
Alvin Cooper 0.98 0.58 0.73 0.93 0.52 0.67
Arthur Morgan 0.98 0.64 0.78 0.71 0.79 0.75
Chris Brockett 1.00 0.32 0.49 0.95 0.31 0.47
Dekang Lin 1.00 0.40 0.58 1.00 0.34 0.51
Frank Keller 0.85 0.65 0.74 0.50 0.71 0.59
George Foster 0.80 0.80 0.80 0.48 0.86 0.61
Harry Hughes 0.91 0.65 0.76 0.76 0.77 0.77
James Curran 0.92 0.69 0.79 0.64 0.77 0.70
James Davidson 0.82 0.85 0.83 0.48 0.93 0.63
James Hamilton 0.65 0.87 0.74 0.26 0.96 0.41
James Morehead 0.66 0.73 0.70 0.57 0.70 0.63
Jerry Hobbs 0.67 0.82 0.74 0.63 0.86 0.73
John Nelson 0.80 0.78 0.79 0.52 0.92 0.66
Jonathan Brooks 0.84 0.85 0.85 0.55 0.86 0.67
Jude Brown 0.75 0.72 0.74 0.80 0.69 0.74
Karen Peterson 0.80 0.86 0.83 0.26 0.94 0.41
Leon Barrett 0.91 0.52 0.66 0.79 0.62 0.69
Marcy Jackson 0.95 0.58 0.72 0.98 0.57 0.72
Mark Johnson 0.76 0.84 0.80 0.44 0.90 0.60
Martha Edwards 0.78 0.85 0.81 0.57 0.87 0.69
Neil Clark 0.85 0.53 0.65 0.60 0.75 0.67
Patrick Killen 0.99 0.57 0.73 0.90 0.61 0.73
Robert Moore 0.74 0.67 0.71 0.49 0.85 0.62
Sharon Goldwater 1.00 0.15 0.26 1.00 0.23 0.37
Stephan Johnson 0.94 0.71 0.81 0.95 0.71 0.81
Stephen Clark 0.87 0.80 0.83 0.55 0.82 0.66
Thomas Fraser 0.62 0.89 0.73 0.47 0.92 0.62
Thomas Kirk 0.81 0.87 0.84 0.84 0.86 0.85
Violet Howard 0.89 0.78 0.83 0.87 0.75 0.81
William Dickson 0.68 0.88 0.77 0.52 0.88 0.66
AVERAGES 0.84 0.70 0.73 0.67 0.74 0.65
</table>
<page confidence="0.99313">
295
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.963139">
<title confidence="0.9972215">SHEF: Semantic Tagging and Summarization Techniques Applied to Cross-document Coreference</title>
<author confidence="0.998833">Horacio Saggion</author>
<affiliation confidence="0.9999615">Department of Computer Science University of Sheffield</affiliation>
<address confidence="0.980812">211 Portobello Street - Sheffield, England, UK, S1 4DP</address>
<phone confidence="0.999237">Tel: +44-114-222-1947 Fax: +44-114-222-1810</phone>
<email confidence="0.999061">saggion@dcs.shef.ac.uk</email>
<abstract confidence="0.999182333333333">We describe experiments for the crossdocument coreference task in SemEval 2007. Our cross-document coreference system uses an in-house agglomerative clustering implementation to group documents referring to the same entity. Clustering uses vector representations created by summarization and semantic tagging analysis components. We present evaluation results for four system configurations demonstrating the potential of the applied techniques.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Artiles</author>
<author>J Gonzalo</author>
<author>S Sekine</author>
</authors>
<title>The SemEval-2007 WePS Evaluation: Establishing a benchmark for Web People Search Task.</title>
<date>2007</date>
<booktitle>In Proceedings of Semeval 2007, Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1343" citStr="Artiles et al., 2007" startWordPosition="188" endWordPosition="191">ied techniques. 1 Introduction Cross-document coreference resolution is the task of identifying if two mentions of the same (or similar) name in different sources refer to the same individual. Deciding if two documents refer to the same individual is a difficult problem because names are highly ambiguous. Automatic techniques for solving this problem are required not only for better access to information but also in natural language processing applications such as multidocument summarization and information extraction. Here, we concentrate on the following SemEval 2007 Web People Search Task (Artiles et al., 2007): a search engine user types in a person name as a query. Instead of ranking web pages, an ideal system should organize search results in as many clusters as there are different people sharing the same name in the documents returned by the search engine. The input is, therefore, the results given by a web search engine using a person name as query. The output is a number of sets, each containing documents referring to the same individual. As past and recent research (Bagga and Baldwin, 1998; Phan et al., 2006), we have addressed the problem as a document clustering problem. For our first parti</context>
</contexts>
<marker>Artiles, Gonzalo, Sekine, 2007</marker>
<rawString>J. Artiles, J. Gonzalo, and S. Sekine. 2007. The SemEval-2007 WePS Evaluation: Establishing a benchmark for Web People Search Task. In Proceedings of Semeval 2007, Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bagga</author>
<author>B Baldwin</author>
</authors>
<title>Entity-Based CrossDocument Coreferencing Using the Vector Space Model.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics (COLING-ACL’98),</booktitle>
<pages>79--85</pages>
<contexts>
<context position="1838" citStr="Bagga and Baldwin, 1998" startWordPosition="279" endWordPosition="282">ion and information extraction. Here, we concentrate on the following SemEval 2007 Web People Search Task (Artiles et al., 2007): a search engine user types in a person name as a query. Instead of ranking web pages, an ideal system should organize search results in as many clusters as there are different people sharing the same name in the documents returned by the search engine. The input is, therefore, the results given by a web search engine using a person name as query. The output is a number of sets, each containing documents referring to the same individual. As past and recent research (Bagga and Baldwin, 1998; Phan et al., 2006), we have addressed the problem as a document clustering problem. For our first participation in SemEval 2007, we use two approaches: a lexical or bag-of-words approach and a semantic based approach. We have implemented our own clustering algorithms but rely on available extraction and summarization technology developed in our laboratory to produce document representations used as input for the clustering procedure. 2 Clustering Algorithm We have implemented an agglomerative clustering algorithm. The input to the algorithm is a set of document representations implemented as</context>
<context position="5580" citStr="Bagga and Baldwin, 1998" startWordPosition="862" endWordPosition="865">resources creation modules from the summarization tool, two frequency tables are 1http://gate.ac.uk 2http://www.dcs.shef.ac.uk/˜saggion created for each document set (or person): (i) an inverted document frequency table for words (no normalisation is applied); and (ii) an inverted frequency table for Mentions (the full entity string is used, no normalisation is applied). Statistics (term frequencies and tj*idj) are computed over tokens and Mentions using the appropriate tables (these tools are part of the summarization toolkit) and vector representations created for each document (same as in (Bagga and Baldwin, 1998)). Two types of representations were considered for these experiments: (i) full document or summary (terms in the summary are considered for vector creation); and (ii) words or Mentions. 4 System Configurations Four system configurations were prepared for SemEval: System I: vector representations were created for full documents. Words were used as terms and local inverted document frequencies used (word frequencies) for weighting. System II: vector representations were created for full documents. Mentions were used as terms and local inverted document frequencies used (Mentions frequencies) fo</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>A. Bagga and B. Baldwin. 1998. Entity-Based CrossDocument Coreferencing Using the Vector Space Model. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics (COLING-ACL’98), pages 79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cunningham</author>
<author>D Maynard</author>
<author>K Bontcheva</author>
<author>V Tablan</author>
</authors>
<title>GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Anniversary Meeting ofthe Association for Computational Linguistics (ACL’02).</booktitle>
<contexts>
<context position="4033" citStr="Cunningham et al., 2002" startWordPosition="634" endWordPosition="637">Association for Computational Linguistics sim (C ,C ) max sim (d ,d✒ ) Where are clusters, are document representations (e.g., vectors), and sim is the cosine metric. If this similarity is greater than a threshold – experimentally obtained – the two clusters are merged together. At each iteration the most similar pair of clusters is merged. If this similarity is less than a certain threshold the algorithm stops. 3 Extraction and Summarization The input for analysis is a set of documents and a person name (first name and last name). The documents are analysed by the default GATE1 ANNIE system (Cunningham et al., 2002) and single document summarization modules (Saggion and Gaizauskas, 2004b) from our summarization toolkit2. No attempt is made to analyse or use contextual information given with the input document. The processing elements include: Document tokenisation Sentence splitting Parts-of-speech tagging Named Entity Recognition using a gazetteer lookup module and regular expressions Named entity coreference using an orthographic name matcher Named entities of type person, organization, address, date, and location are considered relevant document terms and stored in a special named entity called Mentio</context>
</contexts>
<marker>Cunningham, Maynard, Bontcheva, Tablan, 2002</marker>
<rawString>H. Cunningham, D. Maynard, K. Bontcheva, and V. Tablan. 2002. GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications. In Proceedings of the 40th Anniversary Meeting ofthe Association for Computational Linguistics (ACL’02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hotho</author>
<author>S Staab</author>
<author>G Stumme</author>
</authors>
<title>WordNet improves text document clustering.</title>
<date>2003</date>
<booktitle>In Proc. of the SIGIR 2003 Semantic Web Workshop.</booktitle>
<contexts>
<context position="6931" citStr="Hotho et al., 2003" startWordPosition="1059" endWordPosition="1062">ent frequencies used (word frequencies) for weighting. System IV: vector representations were created for person summaries. Mentions were used as terms and local inverted document frequencies used (Mentions frequencies) for weighting. Because only one system configuration was allowed per participant team, we decided to select System II for official evaluation interested in evaluating the effect of semantic information in the clustering process. 5 Parameter Setting and Results Evaluation of the task was carried out using standard clustering evaluation measures of ”purity” and ”inverse purity” (Hotho et al., 2003), and the harmonic 293 Configuration Purity Inv.Purity F-Score System I 0.68 0.85 0.74 System II 0.62 0.85 0.68 System III 0.84 0.70 0.74 System IV 0.65 0.75 0.64 Table 1: Results for our configurations omitting one set. System II was the system we evaluated in SemEval 2007. mean of purity and inverse purity: F-score. We estimated the threshold for the clustering algorithm using the ECDL subset of the training data provided by SemEval. We applied the clustering algorithm to each document set and computed purity, inverse purity, and F-score at each iteration of the algorithm, recording the simi</context>
</contexts>
<marker>Hotho, Staab, Stumme, 2003</marker>
<rawString>A. Hotho, S. Staab, and G. Stumme. 2003. WordNet improves text document clustering. In Proc. of the SIGIR 2003 Semantic Web Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X-H Phan</author>
<author>L-M Nguyen</author>
<author>S Horiguchi</author>
</authors>
<title>Personal name resolution crossover documents by a semantics-based approach.</title>
<date>2006</date>
<journal>IEICE Trans. Inf. &amp; Syst.,</journal>
<contexts>
<context position="1858" citStr="Phan et al., 2006" startWordPosition="283" endWordPosition="286">ction. Here, we concentrate on the following SemEval 2007 Web People Search Task (Artiles et al., 2007): a search engine user types in a person name as a query. Instead of ranking web pages, an ideal system should organize search results in as many clusters as there are different people sharing the same name in the documents returned by the search engine. The input is, therefore, the results given by a web search engine using a person name as query. The output is a number of sets, each containing documents referring to the same individual. As past and recent research (Bagga and Baldwin, 1998; Phan et al., 2006), we have addressed the problem as a document clustering problem. For our first participation in SemEval 2007, we use two approaches: a lexical or bag-of-words approach and a semantic based approach. We have implemented our own clustering algorithms but rely on available extraction and summarization technology developed in our laboratory to produce document representations used as input for the clustering procedure. 2 Clustering Algorithm We have implemented an agglomerative clustering algorithm. The input to the algorithm is a set of document representations implemented as vectors of terms an</context>
</contexts>
<marker>Phan, Nguyen, Horiguchi, 2006</marker>
<rawString>X.-H. Phan, L.-M. Nguyen, and S. Horiguchi. 2006. Personal name resolution crossover documents by a semantics-based approach. IEICE Trans. Inf. &amp; Syst., Feb 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Saggion</author>
<author>R Gaizauskas</author>
</authors>
<title>Mining on-line sources for definition knowledge.</title>
<date>2004</date>
<booktitle>In Proceedings of the 17th FLAIRS 2004,</booktitle>
<publisher>AAAI.</publisher>
<location>Miami Bearch, Florida, USA,</location>
<contexts>
<context position="4105" citStr="Saggion and Gaizauskas, 2004" startWordPosition="643" endWordPosition="646"> ) Where are clusters, are document representations (e.g., vectors), and sim is the cosine metric. If this similarity is greater than a threshold – experimentally obtained – the two clusters are merged together. At each iteration the most similar pair of clusters is merged. If this similarity is less than a certain threshold the algorithm stops. 3 Extraction and Summarization The input for analysis is a set of documents and a person name (first name and last name). The documents are analysed by the default GATE1 ANNIE system (Cunningham et al., 2002) and single document summarization modules (Saggion and Gaizauskas, 2004b) from our summarization toolkit2. No attempt is made to analyse or use contextual information given with the input document. The processing elements include: Document tokenisation Sentence splitting Parts-of-speech tagging Named Entity Recognition using a gazetteer lookup module and regular expressions Named entity coreference using an orthographic name matcher Named entities of type person, organization, address, date, and location are considered relevant document terms and stored in a special named entity called Mention. Coreference chains are created and analysed and if they contain an en</context>
</contexts>
<marker>Saggion, Gaizauskas, 2004</marker>
<rawString>H. Saggion and R. Gaizauskas. 2004a. Mining on-line sources for definition knowledge. In Proceedings of the 17th FLAIRS 2004, Miami Bearch, Florida, USA, May 17-19. AAAI.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>