<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005680">
<affiliation confidence="0.317035">
The University of Alicante Word Sense Disambiguation System*
</affiliation>
<note confidence="0.7490966">
Andres Montoyo and Armando Suarez
Departamento de Lenguajes y Sistemas Informaticos
Universidad de Alicante
Alicante, Spain
{montoyo I armando}@dls i ua . es
</note>
<sectionHeader confidence="0.92576" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997519">
The WSD system presented at SENSEVAL-2
uses a knowledge-based method for noun dis-
ambiguation and a corpus-based method for
verbs and adjectives. The methods are, respec-
tively, Specification Marks and Maximum En-
tropy probability models. So, we can say that
this is a hybrid system which joins an unsuper-
vised method with a supervised method. The
whole system has been used in lexical sample
english task and lexical sample spanish task.
</bodyText>
<sectionHeader confidence="0.995505" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.98287671875">
In this paper a Word Sense Disambiguation sys-
tem based on Specification Marks (SM) and
Maximum Entropy probability models (ME) is
presented. SM is an unsupervised knowledge-
based method and has been applied to noun
disambiguation. ME belongs to the statistical
approach to WSD in NLP and uses a tagged cor-
pus in order to learn a probability model that
can be used to predict the correct sense of a
word. SM does not need a previously tagged
corpus, it uses the semantic information stored
in WordNet.
The weakness of supervised corpus-based ap-
proaches rely on availability of corpora and their
dependency of the data which were used in the
training phase. Knowledge-based approaches
use previously acquire linguistic knowledge.
This knowledge is extracted from human lex-
icographers experience and can be in form of
electronic dictionary or lexicon. While their
success seems poorest than statistical methods,
they don&apos;t need neither an existing corpus nor
a training phase and they can be more domain
independent.
* This paper has been partially supported by the Span-
ish Government (CICYT) project number TIC2000-
0664-0O2-02.
So, the University of Alicante system per-
forms the WSD task combining unsupervised
with supervised methods. The whole system
has been used in lexical sample English task and
lexical sample Spanish task.
</bodyText>
<sectionHeader confidence="0.930664" genericHeader="method">
2 Specification Marks Framework
</sectionHeader>
<bodyText confidence="0.990771636363636">
The method we present here consists basically
of the automatic sense-disambiguating of nouns
that appear within the context of a sentence
and whose different possible senses are quite re-
lated. Its context is the group of words that
co-occur with it in the sentence and their rela-
tionship to the noun to be disambiguated. The
disambiguation is resolved with the use of the
WordNet lexical knowledge base.
The intuition underlying this approach is that
the more similar two words are, the more infor-
mative the most specific concept that subsumes
them both will be. In other words, their low-
est upper bound in the taxonomy. (A &amp;quot;con-
cept&amp;quot; here, corresponds to a Specification Mark
(SM)). In other words, the more information
two concepts share in common, the more similar
they obviously are, and the information com-
monly shared by two concepts is indicated by
the concept that subsumes them in the taxon-
omy.
The input for the WSD module will be the
group of words W = W2, ..., 144l. Each
word wi is sought in WordNet, each one has
an associated set Si = Si2, Sin} of pos-
sible senses. Furthermore, each sense has a
set of concepts in the IS-A taxonomy (hyper-
nymy/Hyponymy relations). First, the concept
that is common to all the senses of all the words
that form the context is sought. We call this
concept the Initial Specification Mark (ISM),
and if it does not immediately resolve the ambi-
guity of the word, we descend from one level
</bodyText>
<page confidence="0.997426">
131
</page>
<bodyText confidence="0.999910272727273">
to another through WordNet &apos;s hierarchy, as-
signing new Specification Marks. The number
of concepts that contain the subhierarchy will
then be counted for each Specification Mark.
The sense that corresponds to the Specification
Mark with highest number of words will then be
chosen as the sense disambiguation of the noun
in question, within its given context.
At this point, we should like to point out that
after having evaluated the method, we subse-
quently discovered that it could be improved
with a set of heuristics, providing even better
results in disambiguation. The set of heuristics
are Heuristic of Hypernym, Heuristic of Defini-
tion, Heuristic of Common Specification Mark,
Heuristic of Gloss Hypernym, Heuristic of Hy-
ponym and Heuristic of Gloss Hyponym. De-
tailed explanation and evaluation of the method
and heuristics can be found in (Montoyo and
Palomar, 2000; Montoyo and Palomar, 2001),
while its application to NLP tasks are addressed
in (Montoyo et al., 2001).
</bodyText>
<sectionHeader confidence="0.982982" genericHeader="method">
3 Maximum Entropy Framework
</sectionHeader>
<bodyText confidence="0.999963476190476">
Maximum Entropy(ME) modeling is a frame-
work for integrating information from many
heterogeneous information sources for classifica-
tion. ME probability models were successfully
applied to some NLP tasks such as POS tagging
or sentence boundary detection (Ratnaparkhi,
1998).
The WSD system presented in this paper
is based on conditional ME probability mod-
els (Saiz-Noeda et al., 2001). It implements
a supervised learning method consisting of the
building of word sense classifiers through train-
ing on a semantically tagged corpus. A classifier
obtained by means of a ME technique consists of
a set of parameters or coefficients estimated by
means of an optimization procedure. Each co-
efficient is associated to one feature observed in
training data. A feature is a function that gives
a measure for some characteristic in a context
associated to a class. The main purpose is to
obtain the probability distribution that maxi-
mizes the entropy, that is, maximum ignorance
is assumed and nothing apart of training data
is considered. As advantages of ME framework,
knowledge-poor features applying and accuracy
can be mentioned; ME framework allows a vir-
tually unrestricted ability to represent problem-
specific knowledge in the form of features (Rat-
naparkhi, 1998).
Let us assume a set of contexts X and a
set of classes C. The function c/ : X -4 C
that performs the classification in a condi-
tional probability model p chooses the class with
the highest conditional probability: c/(x) =
arg max,p(clx), where x is a context and c a
class. The features have the form of (1), where
cp(x) is some observable characteristic&apos;. The
conditional probability p(clx) is defined as (2)
where ai are the parameters or weights of each
feature, and Z(x) is a constant to ensure that
the sum of probabilities for each possible class
in this context is equal to 1.
</bodyText>
<equation confidence="0.971189625">
fe (x,
{ 1 if
= = c and cp(x) = true
0 otherwise
1
P(Clx) -Fr a.fi(x,c)
i=11
Z(x)
</equation>
<sectionHeader confidence="0.79539" genericHeader="method">
4 The system at Sense-val.2
</sectionHeader>
<bodyText confidence="0.999946642857143">
The Spanish and English lexical sample tasks at
the SENSEVAL-2 workshop had been performed
by our system in three phases. The first one is
a naive multi-word detection; the second one,
the disambiguation of nouns by means of the
SM method, and the third one, the disambigua-
tion of verbs and adjectives by means of the ME
method.
In a previous step, training and test data had
been tagged with Tree-Tagger(Schmid, 1994)
for English files and Conexor&apos;s FDG Parser
(Tapanainen and Jarvinen, ) for Spanish files
in order to get the part-of-speech information
and identify nouns, verbs and adjectives.
</bodyText>
<sectionHeader confidence="0.663474" genericHeader="method">
Multi-words detection
</sectionHeader>
<bodyText confidence="0.998681428571429">
The multi-word detection has been performed
by combining the words around the target word
in each sample and consulting WordNet for En-
glish (examining the training data, we conclude
that this is not necessary for Spanish data). If
a multi-word is found in WordNet a multi-word
instance is assigned and no further single word
</bodyText>
<footnote confidence="0.5055755">
&apos;The ME approach is not limited to binary fun-
tions, but the optimization procedure( Generalized Iter-
ative Scaling) used for the estimation of the parameters
needs this kind of features.
</footnote>
<page confidence="0.992103">
132
</page>
<bodyText confidence="0.9998205">
disambiguation will be done. This kind of in-
stances has been disambiguated with the first
sense of WordNet (even if it is a polysemous
one).
</bodyText>
<subsectionHeader confidence="0.983422">
Nouns with Specification Marks
</subsectionHeader>
<bodyText confidence="0.968143111111111">
The second phase consist of noun classification,
and has been performed by the SM method de-
scribed previously.
Verbs and adjectives with Maximum
Entropy
The third and final phase, the verbs and ad-
jectives disambiguation, has been performed by
the ME method. The SENSEVAL-2 training data
has been used in order to obtain the classifica-
tion functions to be applied on the test data.
The set of features defined for ME training is
described below and it is based on features se-
lection made in (Ng and Lee, 1996) and (Escud-
ero et al., 2000).
The set of features corresponds to words
around the word to classify and POS la-
bels at positions related to the target word
in each sentence: wo, w-1, w-2,
</bodyText>
<equation confidence="0.953972">
W+2, W+3, (111-25 W-1), (10-11 WA,
(W+11 W+2), (W-3,W-21W-1), (V-1-21 W-1,W+1),
(W-1,W+11W+2), (W+1, w+2,w+3), P-3, P-2, P-1,
</equation>
<bodyText confidence="0.999906">
p+i, p+2, p+3. Each wi is the lemma of the word
at position i in the context (in collocations, at
least one of the words must be a content word).
Each pi is the POS label at position i.
Other set of features consists of a surround-
ing nouns selection. This selection is doing by
means of frequency information of nouns co-
occurring with a sense. Nouns co-occurring
with a class in a K% of examples of that class
in the corpus or more are selected to build a
feature for each possible class2.
</bodyText>
<sectionHeader confidence="0.753725" genericHeader="method">
5 Senseval-2 results analysis
</sectionHeader>
<bodyText confidence="0.951601071428571">
Analyzing the first evaluation results of
the English lexical sample task (fine-grained
scoring) reported by SENSEVAL-2 committee
(precision = 0.421 and recall = 0.411) , some
conclusions can be extracted from them.
The nouns disambiguation obtains the worst
results (see table 1). We can mostly assure
2For example, in a set of 100 examples of sense four
of the noun &amp;quot;interest&amp;quot;, if &amp;quot;bank&amp;quot; is observed 10 times or
more (K = 10%) then a feature for each possible sense
of &amp;quot;interest&amp;quot; is defined with &amp;quot;bank&amp;quot;.
that the reason is the kind of method used:
knowledge-based for nouns and corpus-based for
verbs and adjectives.
</bodyText>
<table confidence="0.9983075">
POS precision recall
Nouns 0.299 0.292
Verbs 0.486 - 0.480
Adjectives 0.709 0.635
</table>
<tableCaption confidence="0.992487">
Table 1: Results of the English Lexical Sample
Task (Fine-grained)
</tableCaption>
<bodyText confidence="0.9964395">
The results of the Spanish lexical sample task
(fine-grained scoring) reported by SENSEVAL-2
committee are precision = 0.514 and recall
0.503. Nevertheless, the nouns results rise to
56% of precision (table 2). It seems that the
set of nouns selected for this task is easier to
Specification Marks than English ones, maybe
related to lexical resources used and the lan-
guage itself. However, the recall of nouns is
too low because a implementation error causes
that the accented words had not been recognize
(coraz6n, operacion and organo).
</bodyText>
<table confidence="0.9987525">
POS precision recall
Nouns 0.566 0.435
Verbs 0.511 0.511
Adjectives 0.687 0.687
</table>
<tableCaption confidence="0.9912435">
Table 2: Results of the Spanish Lexical Sample
Task (Fine-grained)
</tableCaption>
<bodyText confidence="0.9997562">
The preprocessing of the train and test data
are relevant. Some errors of the POS-tagger had
been detected and they affect some answer in-
stances. Multi-words are a not resolved prob-
lem. The detection and disambiguation method
is too simple and causes too much errors. More
preprocessing is necessary, as well: the con-
text information can be enriched and accuracy
increased with entity recognition, full-parsing,
and so on.
</bodyText>
<sectionHeader confidence="0.976323" genericHeader="method">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.7816065">
The University of Alicante system presented at
SENSEVAL-2 workshop joins the two general ap-
proaches to the WSD task: knowledge-based
and corpus-based methods. The Specification
Marks method belongs to the first one and Max-
imum Entropy-based method to the second one.
</bodyText>
<page confidence="0.998142">
133
</page>
<bodyText confidence="0.999977269230769">
Specification Marks for nouns, and Maximum
Entropy for verbs and adjectives had been used
in order to process the test data of the En-
glish and the Spanish lexical sample tasks. The
training and the test data had been used with a
minimum preprocessing, just cleaning of XML-
tags in order to run the Tree-Tagger. Besides,
the two WSD modules had been used in the
same manner as for other corpora with minor
modifications: no specific changes to the algo-
rithms used in both methods had been made for
SENSEVAL-2, apart from the necessary modules
to make data files available to the computer pro-
grams.
Due to the distinct approaches used in each
POS, the whole system has been classified as
supervised system. In the English task, the sys-
tem obtains a poor score when it is compared
with other supervised systems, and a great re-
sult against the unsupervised systems (we have
no such information of systems for Spanish).
But the truth is that our system is unsuper-
vised for nouns but supervised for verbs and ad-
jectives. Therefore, comparing our results with
the other systems must be done separating the
results of nouns, verbs and adjectives.
</bodyText>
<sectionHeader confidence="0.851025" genericHeader="discussions">
7 Future and in progress work
</sectionHeader>
<bodyText confidence="0.99724225">
At this moment, the two methods presented
here are being improved with new knowledge
sources like full parsing information and domain
categories that in order to decrease the Word-
Net granularity. The WSD system will be com-
pleted with other NLP software like Name En-
tity recognition and multi-words detection mod-
ules.
Recent work in our research group indicates
that it is possible to combine the two methods in
a hybrid method that assign a sense to a context
combining the answers of both methods with a
relevant improvement of accuracy (Suarez and
Montoyo, 2001). Our intention is to extent this
combination with the help of other well known
WSD methods and to establish a voting method
or some other manner of cooperation.
Our main objective is to develop a complete
WSD system in order to help other NLP activ-
ities in our research group. The work presented
here is our first attempt to participate at Sen-
seval and we hope to get the proper conclusions
in order to improve our system and compete in
the next Senseval.
</bodyText>
<sectionHeader confidence="0.986989" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999687354166667">
Gerard Escudero, Lluis Marquez, and Ger-
man Rigau. 2000. Boosting applied to
word sense disambiguation. In Proceedings
of the 12th Conference on Machine Learning
ECML2000, Barcelona, Spain.
A. Montoyo and M. Palomar. 2000. Word Sense
Disambiguation with Specification Marks in
Unrestricted Texts. pages 103-107.
A. Montoyo and M. Palomar. 2001. Specifi-
cation Marks for Word Sense Disambigua-
tion: New Development. In Proceedings of
2nd International conference on Intelligent
Text Processing and Computational Linguis-
tics (CICLing-2001), pages 182-191.
A. Montoyo, M. Palomar, and G. Rigau. 2001.
WordNet Enrichment with Classification Sys-
tems. In ACL, editor, Proceedings of NAACL
Workshop WordNet and Other Lexical Re-
sources: Applications, Extensions and Cus-
tomizations, Pittsburgh, PA, USA.
Hwee Tou Ng and Hian Beng Lee. 1996. In-
tegrating multiple knowledge sources to dis-
ambiguate word senses: An exemplar-based
approach. In Proceedings 3Ath Annual Meet-
ing of the ACL-1996., San Francisco, USA.
Adwait Ratnaparkhi. 1998. Maximum Entropy
Models for Natural Language Ambiguity Res-
olution. Ph.D. thesis, University of Pennsyl-
vania.
Maximiliano Saiz-Noeda, Armando Suarez, and
Manuel Palomar. 2001. Semantic pattern
learning through maximum entropy-based
wsd technique. In Proceedings of CoNLL-
2001, pages 23-29. Toulouse, France.
Helmut Schmid. 1994. Probabilistic part-of-
speech tagging using decision trees. In Pro-
ceedings International Conference on New
Methods in Language Processing., pages 44-
49, Manchester, UK.
Armando Suarez and Andres Montoyo. 2001.
Estudio de cooperacion entre metodos de
desambiguacion lexica: Marcas de especifi-
cidad vs. maxima entropia. Procesamiento
Lenguaje Natural, 27(1):207-214, september.
Pasi Tapanainen and Timo Jarvinen. A non-
projective dependency parser. In Proceedings
of the Fifth Conference on Applied Natural
Language Processing, pages 64-71.
</reference>
<page confidence="0.998622">
134
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.396089">
<title confidence="0.949904">The University of Alicante Word Sense Disambiguation System*</title>
<author confidence="0.696463">Montoyo</author>
<affiliation confidence="0.956007">Departamento de Lenguajes y Sistemas Universidad de</affiliation>
<address confidence="0.614546">Alicante,</address>
<email confidence="0.913794">{montoyo I armando}@dls i ua . es</email>
<abstract confidence="0.999128545454546">The WSD system presented at SENSEVAL-2 uses a knowledge-based method for noun disambiguation and a corpus-based method for verbs and adjectives. The methods are, respectively, Specification Marks and Maximum Entropy probability models. So, we can say that this is a hybrid system which joins an unsupervised method with a supervised method. The whole system has been used in lexical sample english task and lexical sample spanish task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gerard Escudero</author>
<author>Lluis Marquez</author>
<author>German Rigau</author>
</authors>
<title>Boosting applied to word sense disambiguation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 12th Conference on Machine Learning ECML2000,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="8281" citStr="Escudero et al., 2000" startWordPosition="1370" endWordPosition="1374">rst sense of WordNet (even if it is a polysemous one). Nouns with Specification Marks The second phase consist of noun classification, and has been performed by the SM method described previously. Verbs and adjectives with Maximum Entropy The third and final phase, the verbs and adjectives disambiguation, has been performed by the ME method. The SENSEVAL-2 training data has been used in order to obtain the classification functions to be applied on the test data. The set of features defined for ME training is described below and it is based on features selection made in (Ng and Lee, 1996) and (Escudero et al., 2000). The set of features corresponds to words around the word to classify and POS labels at positions related to the target word in each sentence: wo, w-1, w-2, W+2, W+3, (111-25 W-1), (10-11 WA, (W+11 W+2), (W-3,W-21W-1), (V-1-21 W-1,W+1), (W-1,W+11W+2), (W+1, w+2,w+3), P-3, P-2, P-1, p+i, p+2, p+3. Each wi is the lemma of the word at position i in the context (in collocations, at least one of the words must be a content word). Each pi is the POS label at position i. Other set of features consists of a surrounding nouns selection. This selection is doing by means of frequency information of noun</context>
</contexts>
<marker>Escudero, Marquez, Rigau, 2000</marker>
<rawString>Gerard Escudero, Lluis Marquez, and German Rigau. 2000. Boosting applied to word sense disambiguation. In Proceedings of the 12th Conference on Machine Learning ECML2000, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Montoyo</author>
<author>M Palomar</author>
</authors>
<title>Word Sense Disambiguation with Specification Marks in Unrestricted Texts.</title>
<date>2000</date>
<pages>103--107</pages>
<contexts>
<context position="4349" citStr="Montoyo and Palomar, 2000" startWordPosition="712" endWordPosition="715">er of words will then be chosen as the sense disambiguation of the noun in question, within its given context. At this point, we should like to point out that after having evaluated the method, we subsequently discovered that it could be improved with a set of heuristics, providing even better results in disambiguation. The set of heuristics are Heuristic of Hypernym, Heuristic of Definition, Heuristic of Common Specification Mark, Heuristic of Gloss Hypernym, Heuristic of Hyponym and Heuristic of Gloss Hyponym. Detailed explanation and evaluation of the method and heuristics can be found in (Montoyo and Palomar, 2000; Montoyo and Palomar, 2001), while its application to NLP tasks are addressed in (Montoyo et al., 2001). 3 Maximum Entropy Framework Maximum Entropy(ME) modeling is a framework for integrating information from many heterogeneous information sources for classification. ME probability models were successfully applied to some NLP tasks such as POS tagging or sentence boundary detection (Ratnaparkhi, 1998). The WSD system presented in this paper is based on conditional ME probability models (Saiz-Noeda et al., 2001). It implements a supervised learning method consisting of the building of word se</context>
</contexts>
<marker>Montoyo, Palomar, 2000</marker>
<rawString>A. Montoyo and M. Palomar. 2000. Word Sense Disambiguation with Specification Marks in Unrestricted Texts. pages 103-107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Montoyo</author>
<author>M Palomar</author>
</authors>
<title>Specification Marks for Word Sense Disambiguation: New Development.</title>
<date>2001</date>
<booktitle>In Proceedings of 2nd International conference on Intelligent Text Processing and Computational Linguistics (CICLing-2001),</booktitle>
<pages>182--191</pages>
<contexts>
<context position="4377" citStr="Montoyo and Palomar, 2001" startWordPosition="716" endWordPosition="719">osen as the sense disambiguation of the noun in question, within its given context. At this point, we should like to point out that after having evaluated the method, we subsequently discovered that it could be improved with a set of heuristics, providing even better results in disambiguation. The set of heuristics are Heuristic of Hypernym, Heuristic of Definition, Heuristic of Common Specification Mark, Heuristic of Gloss Hypernym, Heuristic of Hyponym and Heuristic of Gloss Hyponym. Detailed explanation and evaluation of the method and heuristics can be found in (Montoyo and Palomar, 2000; Montoyo and Palomar, 2001), while its application to NLP tasks are addressed in (Montoyo et al., 2001). 3 Maximum Entropy Framework Maximum Entropy(ME) modeling is a framework for integrating information from many heterogeneous information sources for classification. ME probability models were successfully applied to some NLP tasks such as POS tagging or sentence boundary detection (Ratnaparkhi, 1998). The WSD system presented in this paper is based on conditional ME probability models (Saiz-Noeda et al., 2001). It implements a supervised learning method consisting of the building of word sense classifiers through trai</context>
</contexts>
<marker>Montoyo, Palomar, 2001</marker>
<rawString>A. Montoyo and M. Palomar. 2001. Specification Marks for Word Sense Disambiguation: New Development. In Proceedings of 2nd International conference on Intelligent Text Processing and Computational Linguistics (CICLing-2001), pages 182-191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Montoyo</author>
<author>M Palomar</author>
<author>G Rigau</author>
</authors>
<title>WordNet Enrichment with Classification Systems.</title>
<date>2001</date>
<booktitle>Proceedings of NAACL Workshop WordNet and Other Lexical Resources: Applications, Extensions and Customizations,</booktitle>
<editor>In ACL, editor,</editor>
<location>Pittsburgh, PA, USA.</location>
<contexts>
<context position="4453" citStr="Montoyo et al., 2001" startWordPosition="729" endWordPosition="732">t. At this point, we should like to point out that after having evaluated the method, we subsequently discovered that it could be improved with a set of heuristics, providing even better results in disambiguation. The set of heuristics are Heuristic of Hypernym, Heuristic of Definition, Heuristic of Common Specification Mark, Heuristic of Gloss Hypernym, Heuristic of Hyponym and Heuristic of Gloss Hyponym. Detailed explanation and evaluation of the method and heuristics can be found in (Montoyo and Palomar, 2000; Montoyo and Palomar, 2001), while its application to NLP tasks are addressed in (Montoyo et al., 2001). 3 Maximum Entropy Framework Maximum Entropy(ME) modeling is a framework for integrating information from many heterogeneous information sources for classification. ME probability models were successfully applied to some NLP tasks such as POS tagging or sentence boundary detection (Ratnaparkhi, 1998). The WSD system presented in this paper is based on conditional ME probability models (Saiz-Noeda et al., 2001). It implements a supervised learning method consisting of the building of word sense classifiers through training on a semantically tagged corpus. A classifier obtained by means of a ME</context>
</contexts>
<marker>Montoyo, Palomar, Rigau, 2001</marker>
<rawString>A. Montoyo, M. Palomar, and G. Rigau. 2001. WordNet Enrichment with Classification Systems. In ACL, editor, Proceedings of NAACL Workshop WordNet and Other Lexical Resources: Applications, Extensions and Customizations, Pittsburgh, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Hian Beng Lee</author>
</authors>
<title>Integrating multiple knowledge sources to disambiguate word senses: An exemplar-based approach.</title>
<date>1996</date>
<booktitle>In Proceedings 3Ath Annual Meeting of the ACL-1996.,</booktitle>
<location>San Francisco, USA.</location>
<contexts>
<context position="8253" citStr="Ng and Lee, 1996" startWordPosition="1365" endWordPosition="1368">sambiguated with the first sense of WordNet (even if it is a polysemous one). Nouns with Specification Marks The second phase consist of noun classification, and has been performed by the SM method described previously. Verbs and adjectives with Maximum Entropy The third and final phase, the verbs and adjectives disambiguation, has been performed by the ME method. The SENSEVAL-2 training data has been used in order to obtain the classification functions to be applied on the test data. The set of features defined for ME training is described below and it is based on features selection made in (Ng and Lee, 1996) and (Escudero et al., 2000). The set of features corresponds to words around the word to classify and POS labels at positions related to the target word in each sentence: wo, w-1, w-2, W+2, W+3, (111-25 W-1), (10-11 WA, (W+11 W+2), (W-3,W-21W-1), (V-1-21 W-1,W+1), (W-1,W+11W+2), (W+1, w+2,w+3), P-3, P-2, P-1, p+i, p+2, p+3. Each wi is the lemma of the word at position i in the context (in collocations, at least one of the words must be a content word). Each pi is the POS label at position i. Other set of features consists of a surrounding nouns selection. This selection is doing by means of f</context>
</contexts>
<marker>Ng, Lee, 1996</marker>
<rawString>Hwee Tou Ng and Hian Beng Lee. 1996. Integrating multiple knowledge sources to disambiguate word senses: An exemplar-based approach. In Proceedings 3Ath Annual Meeting of the ACL-1996., San Francisco, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Maximum Entropy Models for Natural Language Ambiguity Resolution.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="4755" citStr="Ratnaparkhi, 1998" startWordPosition="773" endWordPosition="774">on Specification Mark, Heuristic of Gloss Hypernym, Heuristic of Hyponym and Heuristic of Gloss Hyponym. Detailed explanation and evaluation of the method and heuristics can be found in (Montoyo and Palomar, 2000; Montoyo and Palomar, 2001), while its application to NLP tasks are addressed in (Montoyo et al., 2001). 3 Maximum Entropy Framework Maximum Entropy(ME) modeling is a framework for integrating information from many heterogeneous information sources for classification. ME probability models were successfully applied to some NLP tasks such as POS tagging or sentence boundary detection (Ratnaparkhi, 1998). The WSD system presented in this paper is based on conditional ME probability models (Saiz-Noeda et al., 2001). It implements a supervised learning method consisting of the building of word sense classifiers through training on a semantically tagged corpus. A classifier obtained by means of a ME technique consists of a set of parameters or coefficients estimated by means of an optimization procedure. Each coefficient is associated to one feature observed in training data. A feature is a function that gives a measure for some characteristic in a context associated to a class. The main purpose</context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>Adwait Ratnaparkhi. 1998. Maximum Entropy Models for Natural Language Ambiguity Resolution. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maximiliano Saiz-Noeda</author>
<author>Armando Suarez</author>
<author>Manuel Palomar</author>
</authors>
<title>Semantic pattern learning through maximum entropy-based wsd technique.</title>
<date>2001</date>
<booktitle>In Proceedings of CoNLL2001,</booktitle>
<pages>23--29</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="4867" citStr="Saiz-Noeda et al., 2001" startWordPosition="790" endWordPosition="793">etailed explanation and evaluation of the method and heuristics can be found in (Montoyo and Palomar, 2000; Montoyo and Palomar, 2001), while its application to NLP tasks are addressed in (Montoyo et al., 2001). 3 Maximum Entropy Framework Maximum Entropy(ME) modeling is a framework for integrating information from many heterogeneous information sources for classification. ME probability models were successfully applied to some NLP tasks such as POS tagging or sentence boundary detection (Ratnaparkhi, 1998). The WSD system presented in this paper is based on conditional ME probability models (Saiz-Noeda et al., 2001). It implements a supervised learning method consisting of the building of word sense classifiers through training on a semantically tagged corpus. A classifier obtained by means of a ME technique consists of a set of parameters or coefficients estimated by means of an optimization procedure. Each coefficient is associated to one feature observed in training data. A feature is a function that gives a measure for some characteristic in a context associated to a class. The main purpose is to obtain the probability distribution that maximizes the entropy, that is, maximum ignorance is assumed and</context>
</contexts>
<marker>Saiz-Noeda, Suarez, Palomar, 2001</marker>
<rawString>Maximiliano Saiz-Noeda, Armando Suarez, and Manuel Palomar. 2001. Semantic pattern learning through maximum entropy-based wsd technique. In Proceedings of CoNLL2001, pages 23-29. Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-ofspeech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings International Conference on New Methods in Language Processing.,</booktitle>
<pages>44--49</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="6857" citStr="Schmid, 1994" startWordPosition="1135" endWordPosition="1136">that the sum of probabilities for each possible class in this context is equal to 1. fe (x, { 1 if = = c and cp(x) = true 0 otherwise 1 P(Clx) -Fr a.fi(x,c) i=11 Z(x) 4 The system at Sense-val.2 The Spanish and English lexical sample tasks at the SENSEVAL-2 workshop had been performed by our system in three phases. The first one is a naive multi-word detection; the second one, the disambiguation of nouns by means of the SM method, and the third one, the disambiguation of verbs and adjectives by means of the ME method. In a previous step, training and test data had been tagged with Tree-Tagger(Schmid, 1994) for English files and Conexor&apos;s FDG Parser (Tapanainen and Jarvinen, ) for Spanish files in order to get the part-of-speech information and identify nouns, verbs and adjectives. Multi-words detection The multi-word detection has been performed by combining the words around the target word in each sample and consulting WordNet for English (examining the training data, we conclude that this is not necessary for Spanish data). If a multi-word is found in WordNet a multi-word instance is assigned and no further single word &apos;The ME approach is not limited to binary funtions, but the optimization p</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-ofspeech tagging using decision trees. In Proceedings International Conference on New Methods in Language Processing., pages 44-49, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Armando Suarez</author>
<author>Andres Montoyo</author>
</authors>
<title>Estudio de cooperacion entre metodos de desambiguacion lexica: Marcas de especificidad vs. maxima entropia. Procesamiento Lenguaje Natural,</title>
<date>2001</date>
<pages>27--1</pages>
<marker>Suarez, Montoyo, 2001</marker>
<rawString>Armando Suarez and Andres Montoyo. 2001. Estudio de cooperacion entre metodos de desambiguacion lexica: Marcas de especificidad vs. maxima entropia. Procesamiento Lenguaje Natural, 27(1):207-214, september.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Pasi Tapanainen</author>
<author>Timo Jarvinen</author>
</authors>
<title>A nonprojective dependency parser.</title>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>64--71</pages>
<marker>Tapanainen, Jarvinen, </marker>
<rawString>Pasi Tapanainen and Timo Jarvinen. A nonprojective dependency parser. In Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 64-71.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>