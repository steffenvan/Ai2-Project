<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015389">
<title confidence="0.987659">
Data-driven computational linguistics at FaMAF-UNC, Argentina
</title>
<author confidence="0.971654">
Laura Alonso i Alemany and Gabriel Infante-Lopez
</author>
<affiliation confidence="0.967933">
Grupo de Procesamiento de Lenguaje Natural
Secci´on de Ciencias de la Computaci´on
</affiliation>
<address confidence="0.721807333333333">
Facultad de Matem´atica, Astronom´ıa y F´ısica
Universidad Nacional de C´ordoba
C´ordoba, Argentina
</address>
<email confidence="0.999623">
{gabriel|alemany}@famaf.unc.edu.ar
</email>
<sectionHeader confidence="0.998605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996925833333333">
This paper provides a survey of some on-
going research projects in computational lin-
guistics within the group of Natural Language
Processing at the University of C´ordoba, Ar-
gentina. We outline our future plans and spot-
light some opportunities for collaboration.
</bodyText>
<sectionHeader confidence="0.9994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99972475">
In this paper we present our group, describe its
members, research agenda, interests and possible
collaboration opportunities. The research agenda
of the NLP group contains diverse lines of work.
As a group, we have a special interest in produc-
ing language technologies for our languages, at a
level comparable in performance with the state-of-
the-art technology for English. We are developing
such technology by deeply understanding its under-
ling models and either adapting them to our lan-
guages or by creating new ones.
In this paper we present only those related to Nat-
ural Language Parsing and data-driven characterisa-
tion of linguistic phenomena. For both lines we pro-
vide a small survey of our results so far, we describe
our current research questions and we spotlight pos-
sible opportunities of collaboration.
The paper is organized as follows. The follow-
ing Section describes the group, its composition,
projects and goals. Section 3 briefly introduces the
research agenda related to natural language pars-
ing and structure finding. Section 4 sketches the
work on data-driven characterisation of linguistic
phenomena in three main parts: semi-structured text
mining, characterisation of verbal behaviour and
mining of relations in biomedical text. Finally, Sec-
tion 5 presents outlines our overall vision for collab-
oration with other researchers in the Americas.
</bodyText>
<sectionHeader confidence="0.906561" genericHeader="method">
2 Description of the group
</sectionHeader>
<bodyText confidence="0.999918">
The NLP Group1 is part of the Computer Science
section at the Facultad de Matem´atica, Astronom´ıa
y F´ısica, at the Universidad Nacional de C´ordoba.
The group was started in 2005, with two full time re-
searchers who had just got their doctorate degree in
Amsterdam and Barcelona. Then, in 2009 and 2010
three more full-time researchers joined the group,
coming from the Universities of Geneva and Nancy.
As of 2010, the group has 5 faculty researchers,
4 PhD students and several undergraduate students.
The computer science section has around 20 mem-
bers – including the NLP group, faculty members
and PhD students.
The faculty researchers are, by alphabetical order:
</bodyText>
<listItem confidence="0.999214272727273">
• Laura Alonso Alemany, working in text mining
and data-driven systematization of language.
• Carlos Areces, investigating different reason-
ing tasks and their applications in natural lan-
guage processing.
• Luciana Benotti, investigates the addition of
pragmatic abilities into dialogue systems.
• Paula Estrella, working in Machine Transla-
tion.
• Gabriel Infante-Lopez, working on Natural
Language Parsing and Structure Finding.
</listItem>
<footnote confidence="0.980655">
1http://www.cs.famaf.unc.edu.ar/˜pln/
</footnote>
<page confidence="0.948158">
8
</page>
<note confidence="0.947463">
Proceedings of the NAACL HLT 2010 Young Investigators Workshop on Computational Approaches to Languages of the Americas,
pages 8–14, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999631155555556">
One of the main aims of the group has been ed-
ucation, both at undergraduate and graduate lev-
els. Computer Science is an under-developed area
in Argentina, and Natural Language Processing even
more so. When the group was created, there were
very few NLP researchers in the country, and they
worked in isolation, with little connection to other
researchers from neighbouring countries. One of
the strategic goals of our University and of the NLP
group itself were to create a critical mass of re-
searchers in NLP. To that aim, we worked on in-
corporating researchers to our group and establish-
ing relations with other groups. Researchers were
incorporated via special programmes from both the
Faculty and the Argentinean Government to increase
the number of doctors in Computer Science in the
scientific system in Argentina.
Most of our efforts in the first years went to raise
awareness about the area and provide foundational
and advanced courses. This policy lead to a signifi-
cant number of graduation theses2 and to the incor-
poration of various PhD students to our group.
We taught several undergraduate and graduate
courses on various NLP topics at our own Univer-
sity, at the University of Rio Cuarto, at the Univer-
sity of Buenos Aires and at the Universidad de la
Rep´ublica (Uruguay), as well as crash courses at the
Society for Operative Investigations (SADIO) and
at the Conferencia Latinoamericana de Inform´atica
(CLEI 2008). We also gave several talks at vari-
ous universities in the country, and participated in
local events, like JALIMI’05 (Jornadas Argentinas
de Ling¨u´ıstica Inform´atica: Modelizaci´on e Inge-
nier´ıa) or the Argentinean Symposium on Artificial
Intelligence.
Since the beginning of its activities, the group
has received funding for two major basic research
projects, funded by the Argentinean Agency for the
Development of Science and Technology. A third
such project is pending approval.
We have a special interest in establishing work-
ing relations and strengthening the synergies with
the research community in NLP, both within South
America and the rest of the world. We have had sci-
entific and teaching exchanges with the NLP group
</bodyText>
<footnote confidence="0.831616">
2http://cs.famaf.unc.edu.ar/˜pln/
Investigacion/tesis_grado/tesis_grado.html
</footnote>
<bodyText confidence="0.997968871794872">
in Montevideo, Uruguay. From that collaboration,
the Microbio project emerged3, bringing together
researchers on NLP from Chile, Brazil, Uruguay,
France and Argentina. This project was funded
by each country’s scientific institutions (MinCyT,
in the case of Argentina) within STIC-AmSud4,
a scientific-technological cooperation programme
aimed to promote and strengthen South America re-
gional capacities and their cooperation with France
in the area of Information Technologies and Com-
munication. Within this project, we hosted the kick-
off workshop on February 2008, with attendants rep-
resenting all groups in the project.
We have also had billateral international cooper-
ation in some smaller projects. Together with the
CNR-INRIA in Rennes, France, we have worked in
a project concerning the smallest grammar problem.
We tackle the same problem, finding small gram-
mars in two different domains: ADN sequences
and Natural Language sentences. In collaboration
with several universities in Spain (UB, UOC, UPC,
EHU/UPV), we have taken part in the major basic
research programme KNOW5, aiming to aggregate
meaning, knowledge and reasoning to current infor-
mation technologies. This project has now received
funding to carry on a continuating project6.
Moreover, we are putting forward some propos-
als for further international collaboration. Follow-
ing the path opened by the Microbio project, we
are working on a proposal to the Ecos Sud pro-
gramme for joint collaboration with research teams
in France7.
We are also working in strengthening relations
within Argentinean NLP groups. To that aim, we are
collaborating with the NLP group at the University
of Buenos Aires in the organization of the School
on Computational Linguistics ELiC8, with several
grants for students sponsored by NAACL. We are
also putting forward a proposal for a workshop on
</bodyText>
<footnote confidence="0.9994035">
3http://www.microbioamsud.net/
4http://www.sticamsud.org/
5KNOW project: http://ixa.si.ehu.es/know.
6Representation of Semantic Knowledge, TIN2009-14715-
C04-03 (Plan Nacional de I+D+i 2008-2011).
7ECOS-SUD programme: http://www.mincyt.gov.
ar/coopinter_archivos/bilateral/francia.
htm.
8ELiC school on Computational Linguistics: http://
www.glyc.dc.uba.ar/elic2010/.
</footnote>
<page confidence="0.985604">
9
</page>
<note confidence="0.545125666666667">
NLP to be co-located with the IBERAMIA confer-
ence on Artificial Intelligence, to be held at Bahia
Blanca on November 2010.
</note>
<sectionHeader confidence="0.746007" genericHeader="method">
3 Natural Language Parsing and
Structure Finding
</sectionHeader>
<subsectionHeader confidence="0.999712">
3.1 Unsupervised Parsing
</subsectionHeader>
<bodyText confidence="0.994396609195403">
Unsupervised parsing of Natural Language Syntax
is a key technology for the development of lan-
guage technology. It is specially important for lan-
guages that have either small treebanks or none at
all. Clearly, there is a big difference between pro-
ducing or using a treebank for evaluation and pro-
ducing or using them for training. In the former
case, the size of the treebank can be significantly
smaller. In our group, we have investigated differ-
ent approaches to unsupervised learning of natural
language. and we are currently following two dif-
ferent lines, one that aims at characterizing the po-
tential of a grammar formalism to learn a given tree-
bank structure and a second that uses only regular
automata to learn syntax.
Characterization of Structures In (Luque and
Infante-Lopez, 2009) we present a rather unusual
result for language learning. We show an upper
bound for the performance of a class of languages
when a grammar from that class is used to parse
the sentences in any given treebank. The class of
languages we studied is the defined by Unambigu-
ous Non-Terminally Separated (UNTS) grammars
(Clark, 2006). UNTS grammars are interesting be-
cause, first, they have nice learnability properties
like PAC learnability (Clark, 2006), and, second,
they are used as the background formalism that won
the Omphalos competition (Clark, 2007). Our strat-
egy consists on characterizing all possible ways of
parsing all the sentences in a treebank using UNTS
grammars, then, we find the one that is closest to the
treebank. We show that, in contrast to the results ob-
tained for learning formal languages, UNTS are not
capable of producing structures that score as state-
of-the-art models on the treebanks we experimented
with.
Our results are for a particular, very specific type
of grammar. We are currently exploring how to
widen our technique to provide upper bounds to a
more general class of languages. Our technique does
not state how to actually produce a grammar that
performs as well as the upper bound, but it can be
useful for determining how to transform the training
material to make upper bounds go up. In particu-
lar we have defined a generalization of UNTS gram-
mars, called k-l-UNTS grammars, that transform a
word w in the training material in a 3-uple (α, w, Q)
where α contains the k previous symbols to w and
Q contains the l symbols following w. Intuitively, k-
l-UNTS augments each word with a variable length
context. It turns out that the resulting class of lan-
guages is more general than UNTS grammars: they
are PAC learnable, they can be learned with the same
learning algorithm as UNTS and, moreover, their
upper bound for performance is much higher than
for UNTS. Still, it might be the case that the exist-
ing algorithm for finding UNTS is not the right one
for learning the structure of a treebank, it might be
the case that strings in the PTB have not been pro-
duced by a k-l-UNTS grammar. We are currently
investigating how to produce an algorithm that fits
better the structure given in a treebank.
Learning Structure Using Probabilistic Au-
tomata DMV+CCM (Klein and Manning, 2004;
Klein and Manning, 2002) is a probabilistic model
for unsupervised parsing, that can be successfully
trained with the EM algorithm to achieve state of
the art performance. It is the combination of the
Constituent-Context Model, that models unlabeled
constituent parsing, and the Dependency Model with
Valence, that models projective dependency parsing.
On the other hand, CCM encodes the probability that
a given string of POS tags is a constituent. DMV is
more of our interest in this work, because it encodes
a top-down generative process where the heads gen-
erate their dependents to both directions until there
is a decision to stop, in a way that resembles suc-
cessful supervised dependency models such as in
(Collins, 1999). The generation of dependents of
a head on a specific direction can be seen as an im-
plicit probabilistic regular language generated by a
probabilistic deterministic finite automaton.
Under this perspective, the DMV model is in fact
an algorithm for learning several automata at the
same time. All automata have in common that they
have the same number of states and the same num-
ber of arcs between states, which is given by the def-
</bodyText>
<page confidence="0.992877">
10
</page>
<bodyText confidence="0.99997975">
inition of the DMV model. Automata differ in that
they have different probabilities assigned to the tran-
sitions. The simple observation that DMV actually
suppose a fixed structure for the automata it induces
might explain its poor performance with freer order
languages like Spanish. Using our own implementa-
tion (see (Luque, 2009)) we have empirically tested
that DMV+CMV works well in languages with strict
word order, like English, but for other languages
with freer word order, like Spanish, DMV+CMV
performance decreases dramatically. In order to
improve DMV+CCM performance for this type of
languages, the structure of the automaton might be
modified, but since the DMV model has an ad hoc
learning algorithm, a new parametric learning algo-
rithm has to be defined. We are currently investigat-
ing different automaton structures for different lan-
guages and we are also investigating not only the
induction of the parameters for fixed structure, but
also inducing the structure of the automata itself.
</bodyText>
<subsectionHeader confidence="0.999184">
3.2 Smallest Grammar and Compression for
Natural Language
</subsectionHeader>
<bodyText confidence="0.999957564102564">
The smallest grammar problem has been widely
studied in the literature. The aim of the problem is
to find the smallest (smallest in the sense of number
of symbols that occur in the grammar) context free
grammar that produces only one given string. The
smallest grammar can be thought as a relaxation of
the definition of Kolmogorov Complexity where the
complexity is given by a context free grammar in-
stead of a Turing machine. It is believed that the
smallest grammar can be used both for computing
optimal compression codes and for finding meaning-
ful patterns in strings.
Moreover, since the procedure for finding the
smallest grammar is in fact a procedure that assigns
a tree structure to a string, the smallest grammar
problem is, in fact, a particular case of unsupervised
parsing that has a very particular objective function
to be optimized.
Since the search space is exponentially big, all
existing algorithms are in fact heuristics that look
for a small grammar. In (Carrascosa et al., 2010)
we presented two algorithms that outperform all ex-
isting heuristics. We have produce and algorithm
that produces 10% smaller grammars for natural lan-
guage strings and 1.5% smaller grammars for DNA
sequences.
Even more, we show evidence that it is possi-
ble to find grammars that share approximately the
same small score but that have very little structure
in common. Moreover, the structure that is found
by the smallest grammar algorithm for the sentences
in PTB have little in common with the structure that
the PTB defines for those sentences.
Currently, we are trying to find answers to two dif-
ferent questions. First, is there a small piece of struc-
ture that is common to all grammars having compa-
rable sizes? and second, can the grammars that are
found by our algorithms be used for improving com-
pression algorithms?
</bodyText>
<sectionHeader confidence="0.9894325" genericHeader="method">
4 Data-driven characterisation of
linguistic phenomena
</sectionHeader>
<subsectionHeader confidence="0.998548">
4.1 Semi-structured text mining
</subsectionHeader>
<bodyText confidence="0.999974074074074">
One of our lines of research is to apply standard text
mining techniques to unstructured text, mostly user
generated content like that found in blogs, social net-
works, short messaging services or advertisements.
Our main corpus of study is constituted by classi-
fied advertisements from a local newspaper, but one
of our lines of work within this project is to assess
the portability of methods and techniques to differ-
ent genres.
The goals we pursue are:
creating corpora and related resources, and mak-
ing them publicly available. A corpus of news-
paper advertisements and a corpus of short text
messages are underway.
normalization of text bringing ortographic vari-
ants of a word (mostly abbreviations) to a
canonical form. To do that, we apply machine
learning techniques to learn the parameters for
edit distances, as in (G´omez-Ballester et al.,
1997; Ristad and Yanilos, 1998; Bilenko and
Mooney, 2003; McCallum et al., 2005; Oncina
and Sebban, 2006). We build upon previous
work on normalization by (Choudhury et al.,
2007; Okazaki et al., 2008; Cook and Steven-
son, 2009; Stevenson et al., 2009). Prelimi-
nary results show a significant improvement of
learned distances over standard distances.
</bodyText>
<page confidence="0.998255">
11
</page>
<bodyText confidence="0.999929444444444">
syntactic analysis applying a robust shallow pars-
ing approach aimed to identify entities and their
modifiers.
ontology induction from very restricted domains,
to aid generalization in the step of information
extraction. We will be following the approach
presented in (Michelson and Knoblock, 2009).
information extraction inducing templates from
corpus using unsupervised and semi-
supervised techniques, and using induced
templates to extract information to populate
a relational database, as in (Michelson and
Knoblock, 2006).
data mining applying traditional knowledge dis-
covery techniques on a relational database pop-
ulated by the information extraction techniques
used in the previous item.
This line of research has been funded for three
years (2009-2012) by the Argentinean Ministry for
Science and Technology, within the PAE project, as
a PICT project (PAE-PICT-2007-02290).
This project opens many opportunities for collab-
oration. The resulting corpora will be of use for lin-
guistic studies. The results of learning edit distances
to find abbreviations can also be used by linguists as
an input to study the regularities found in this kind
of genres, as proposed in (Alonso Alemany, 2010).
We think that some joint work on learning string
edit distances would be very well integrated within
this project. We are also very interested in collabo-
rations with researchers who have some experience
in NLP in similar genres, like short text messages or
abbreviations in medical papers.
Finally, interactions with data mining communi-
ties, both academic and industrial, would surely be
very enriching for this project.
</bodyText>
<subsectionHeader confidence="0.99916">
4.2 Characterisation of verbal behaviour
</subsectionHeader>
<bodyText confidence="0.999383">
One of our research interests is the empirical charac-
terization of the subcategorization of lexical items,
with a special interest on verbs. This line of work
has been pursued mainly within the KNOW project,
in collaboration with the UB-GRIAL group9.
Besides the theoretical interest of describing the
behaviour of verbs based on corpus evidence, this
</bodyText>
<footnote confidence="0.780285">
9http://grial.uab.es/
</footnote>
<bodyText confidence="0.999882955555556">
line has an applied aim, namely, enriching syntac-
tic analyzers with subcategorization information, to
help resolving structural ambiguities by using lexi-
cal information. We have focused on the behaviour
of Spanish verbs, and implemented some of our find-
ings as a lexicalized enhancement of the dependency
grammars used by Freeling10. An evaluation of the
impact of this information on parsing accuracy is un-
derway.
We have applied clustering techniques to obtain
a corpus-based characterization of the subcatego-
rization behaviour of verbs (Alonso Alemany et al.,
2007; Castell´on et al., 2007). We explored the be-
haviour of the 250 most frequent verbs of Spanish
on the SenSem corpus (Castell´on et al., 2006), man-
ually annotated with the analysis of verbs at various
linguistic levels (sense, aspect, voice, type of con-
struction, arguments, role, function, etc.). Apply-
ing clustering techniques to the instances of verbs in
these corpus, we obtained coarse-grained classes of
verbs with the same subcategorization. A classifier
was learned from considering clustered instances as
classes. With this classifier, verbs in unseen sen-
tences were assigned a subcategorization behaviour.
Also with the aim of associating subcategoriza-
tion information to verbs using evidence found
in corpora, we developed IRASubcat (Altamirano,
2009). IRASubcat11. is a highly flexible system de-
signed to gather information about the behaviour of
verbs from corpora annotated at any level, and in
any language. It identifies patterns of linguistic con-
stituents that co-occur with verbs, detects optional
constituents and performs hypothesis testing of the
co-occurrence of verbs and patterns.
We have also been working on connecting pred-
icates in FrameNet and SenSem, using WordNet
synsets as an interlingua (Alonso Alemany et al.,
SEPLN). We have found many dissimilarities be-
tween FrameNet and SenSem, but have been able
to connect some of their predicates and enrich these
resources with information from each other.
We are currently investigating the impact of dif-
ferent kinds of information on the resolution of pp-
attachment ambiguities in Spanish, using the AN-
CORA corpus (Taul´e et al., 2006). We are exploring
</bodyText>
<footnote confidence="0.9993895">
10http://www.lsi.upc.edu/˜nlp/freeling/
11http://www.irasubcat.com.ar/
</footnote>
<page confidence="0.998308">
12
</page>
<bodyText confidence="0.99928847826087">
the utility of various WordNet-related information,
like features extracted from the Top Concept Ontol-
ogy, in combination with corpus-based information,
like frequencies of occurrence and co-occurrence of
words in corpus.
The line of research of characterisation of verbal
behaviour presents many points for collaboration.
In collaboration with linguists, the tools and meth-
ods that we have explained here provide valuable in-
formation for the description and systematization of
subcategorization of verbs and other lexical pieces.
It would be very interesting to see whether these
techniques, that have been successfully applied to
Spanish, apply to other languages or with different
resources. We are also interested in bringing to-
gether information from different resources or from
different sources (corpora, dictionaries, task-specific
lexica, etc.), in order to achieve richer resources.
We also have an interest for the study of hypothe-
sis testing as applied to corpus-based computational
linguistics, to get some insight on the information
that these techniques may provide to guide research
and validate results.
</bodyText>
<subsectionHeader confidence="0.99981">
4.3 Discovering relations between entitites
</subsectionHeader>
<bodyText confidence="0.997574583333333">
As a result of the Microbio project, we have devel-
oped a module to detect relations between entities
in biomedical text (Bruno, 2009). This module has
been trained with the GENIA corpus (Kim et al.,
2008), obtaining good results (Alonso Alemany and
Bruno, 2009). We have also explored different ways
to overcome the data sparseness problem caused by
the small amount of manually annotated examples
that are available in the GENIA corpus. We have
used the corpus as the initial seed of a bootstrapping
procedure, generalized classes of relations via the
GENIA ontology and generalized classes via clus-
tering. Of these three procedures, only generaliza-
tion via an ontology produced good results. How-
ever, we have hopes that a more insightful charac-
terization of the examples and smarter learning tech-
niques (semi-supervised, active learning) will im-
prove the results for these other lines.
Since this area of NLP has ambitious goals, op-
portunities for collaboration are very diverse. In
general, we would like to join efforts with other re-
searchers to solve part of these complex problems,
with a special focus in relations between entities and
semi-supervised techniques.
</bodyText>
<sectionHeader confidence="0.999569" genericHeader="conclusions">
5 Opportunities for Collaboration
</sectionHeader>
<bodyText confidence="0.999995115384615">
We are looking for opportunities of collaboration
with other groups in the Americas, producing a syn-
ergy between groups. We believe that we can artic-
ulate collaboration by identifying common interests
and writing joint proposals. In Argentina there are
some agreements for billateral or multi-lateral col-
laboration with other countries or specific institu-
tions of research, which may provide a framework
for starting collaborations.
We are looking for collaborations that promote
the exchange of members of the group, specially
graduate students. Our aim is to gain a level of col-
laboration strong enough that would consider, for
example, co-supervision of PhD students. Ideally,
co-supervised students would spend half of their
time in each group, tackle a problem that is common
for both groups and work together with two super-
visors. The standard PhD scholarship in Argentina,
provided by Conicet, allows such modality of doc-
torate studies, as long as financial support for travels
and stays abroad is provided by the co-supervising
programme. We believe that this kind of collabora-
tion is one that builds very stable relations between
groups, helps students learn different research id-
iosyncrasies and devotes specific resources to main-
tain the collaboration.
</bodyText>
<sectionHeader confidence="0.998908" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.986243176470588">
Laura Alonso Alemany and Santiago E. Bruno. 2009.
Learning to learn biological relations from a small
training set. In CiCLing, pages 418–429.
Laura Alonso Alemany, Irene Castell´on, and Nevena
Tinkova Tincheva. 2007. Obtaining coarse-grained
classes of subcategorization patterns for spanish. In
RANLP’07.
Laura Alonso Alemany, Irene Castell´on, Egoitz Laparra,
and German Rigau. SEPLN. Evaluaci´on de m´etodos
semi-autom´aticos para la conexi´on entre FrameNet y
SenSem. In 2009.
Laura Alonso Alemany. 2010. Learning parameters
for an edit distance can learn us tendencies in user-
generated content. Invited talk at NLP in the So-
cial Sciences, Instituto de Altos Estudios en Psicolo-
gia y Ciencias Sociales, Buenos Aires, Argentina, May
2010.
</reference>
<page confidence="0.995861">
13
</page>
<reference confidence="0.987751037383177">
I. Romina Altamirano. 2009. Irasubcat: Un sistema
para adquisici´on autom´atica de marcos de subcatego-
rizaci´on de piezas l´exicas a partir de corpus. Master’s
thesis, Facultad de Matem´atica, Astronomfa y Ffsica,
Universidad Nacional de C´ordoba, Argentina.
Mikhail Bilenko and Raymond J. Mooney. 2003. Adap-
tive duplicate detection using learnable string simi-
larity measures. In Proceedings of the ninth ACM
SIGKDD.
Santiago E. Bruno. 2009. Detecci´on de relaciones entre
entidades en textos de biomedicina. Master’s thesis,
Facultad de Matem´atica, Astronomfa y Ffsica, Univer-
sidad Nacional de C´ordoba, Argentina.
Rafael Carrascosa, Franc¸ois Coste, Matthias Gall´e, and
Gabriel Infante-Lopez. 2010. Choosing Word Occur-
rences for the Smallest Grammar Problem. In Pro-
ceedings of LATA 2010. Springer.
Irene Castell´on, Ana Fern´andez-Montraveta, Gl`oria
V´azquez, Laura Alonso, and Joanan Capilla. 2006.
The SENSEM corpus: a corpus annotated at the syntac-
tic and semantic level. In 5th International Conference
on Language Resources and Evaluation (LREC 2006).
Irene Castell´on, Laura Alonso Alemany, and Nevena Tin-
kova Tincheva. 2007. A procedure to automatically
enrich verbal lexica with subcategorization frames. In
Proceedings of the Argentine Simposium on Artificial
Intelligence, ASAI’07.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure
of texting language. Int. J. Doc. Anal. Recognit.,
10(3):157–174.
Alexander Clark. 2006. Pac-learning unambiguous nts
languages. In International Colloquium on Grammat-
ical Inference, pages 59–71.
Alexander Clark. 2007. Learning deterministic context
free grammars: the omphalos competition. Machine
Learning, 66(1):93–110.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania, PA.
Paul Cook and Suzanne Stevenson. 2009. An unsuper-
vised model for text message normalization. In Work-
shop on Computational Approaches to Linguistic Cre-
ativity. NAACL HLT 2009.
E. G´omez-Ballester, M. L. Mic´o-Andr´es, J. Oncina,
and M. L. Forcada-Zubizarreta. 1997. An empir-
ical method to improve edit-distance parameters for
a nearest-neighbor-based classification task. In VII
Spanish Symposium on Pattern Recognition and Image
Analysis, Barcelona, Spain.
Jin D. Kim, Tomoko Ohta, and Jun’ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
literature. BMCBioinformatics, 9(1).
Dan Klein and Christopher D. Manning. 2002. A gener-
ative constituent-context model for improved grammar
induction. In ACL, pages 128–135.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proc. of ACL 42.
Franco Luque and Gabriel Infante-Lopez. 2009. Upper
bounds for unsupervised parsing with unambiguous
non-terminally. In International Workshop Compu-
tational Linguistic Aspects of Grammatical Inference.
EACL, Greece.
Franco M. Luque. 2009. Implementation of the
DMV+CCM parser. http://www.cs.famaf.
unc.edu.ar/˜francolq/en/proyectos/
dmvccm.
Andrew McCallum, Kedar Bellare, and Fernando Pereira.
2005. A conditional random field for discriminatively-
trained finite-state string edit distance. In Proceedings
of the Proceedings of the Twenty-First Conference An-
nual Conference on Uncertainty in Artificial Intelli-
gence (UAI-05), pages 388–395, Arlington, Virginia.
AUAI Press.
Matthew Michelson and Craig A. Knoblock. 2006.
Phoebus: a system for extracting and integrating data
from unstructured and ungrammatical sources. In
AAAI’06: proceedings of the 21st national conference
on Artificial intelligence, pages 1947–1948. AAAI
Press.
Matthew Michelson and Craig A. Knoblock. 2009. Ex-
ploiting background knowledge to build reference sets
for information extraction. In Proceedings of the 21st
International Joint Conference on Artific ial Intelli-
gence (IJCAI-2009), Pasadena, CA.
Naoaki Okazaki, Sophia Ananiadou, and Jun’ichi Tsujii.
2008. A discriminative alignment model for abbrevia-
tion recognition. In COLING ’08: Proceedings of the
22nd International Conference on Computational Lin-
guistics, pages 657–664, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Jos´e Oncina and Marc Sebban. 2006. Learning stochas-
tic edit distance: Application in handwritten character
recognition. Pattern Recognition, 39(9):1575–1587.
E. S. Ristad and P. N. Yanilos. 1998. Learning string edit
distance. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 20:522–532.
Mark Stevenson, Yikun Guo, Abdulaziz Al Amri, and
Robert Gaizauskas. 2009. Disambiguation of biomed-
ical abbreviations. In BioNLP ’09: Proceedings of the
Workshop on BioNLP, pages 71–79, Morristown, NJ,
USA. Association for Computational Linguistics.
M. Taul´e, M.A. Martf, and M. Recasens. 2006. Ancora:
Multilevel annotated corpora for catalan and spanish.
In LREC’06.
</reference>
<page confidence="0.999266">
14
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.347796">
<title confidence="0.965043">Data-driven computational linguistics at FaMAF-UNC, Argentina</title>
<author confidence="0.983158">Alonso i Alemany</author>
<affiliation confidence="0.83371375">Grupo de Procesamiento de Lenguaje Secci´on de Ciencias de la Facultad de Matem´atica, Astronom´ıa y Universidad Nacional de</affiliation>
<address confidence="0.65403">C´ordoba,</address>
<abstract confidence="0.977507571428571">This paper provides a survey of some ongoing research projects in computational linguistics within the group of Natural Language Processing at the University of C´ordoba, Argentina. We outline our future plans and spotlight some opportunities for collaboration.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Laura Alonso Alemany</author>
<author>Santiago E Bruno</author>
</authors>
<title>Learning to learn biological relations from a small training set.</title>
<date>2009</date>
<booktitle>In CiCLing,</booktitle>
<pages>418--429</pages>
<contexts>
<context position="22197" citStr="Alemany and Bruno, 2009" startWordPosition="3443" endWordPosition="3446">(corpora, dictionaries, task-specific lexica, etc.), in order to achieve richer resources. We also have an interest for the study of hypothesis testing as applied to corpus-based computational linguistics, to get some insight on the information that these techniques may provide to guide research and validate results. 4.3 Discovering relations between entitites As a result of the Microbio project, we have developed a module to detect relations between entities in biomedical text (Bruno, 2009). This module has been trained with the GENIA corpus (Kim et al., 2008), obtaining good results (Alonso Alemany and Bruno, 2009). We have also explored different ways to overcome the data sparseness problem caused by the small amount of manually annotated examples that are available in the GENIA corpus. We have used the corpus as the initial seed of a bootstrapping procedure, generalized classes of relations via the GENIA ontology and generalized classes via clustering. Of these three procedures, only generalization via an ontology produced good results. However, we have hopes that a more insightful characterization of the examples and smarter learning techniques (semi-supervised, active learning) will improve the resu</context>
</contexts>
<marker>Alemany, Bruno, 2009</marker>
<rawString>Laura Alonso Alemany and Santiago E. Bruno. 2009. Learning to learn biological relations from a small training set. In CiCLing, pages 418–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Alonso Alemany</author>
</authors>
<title>Irene Castell´on, and Nevena Tinkova Tincheva.</title>
<date>2007</date>
<booktitle>In RANLP’07.</booktitle>
<marker>Alemany, 2007</marker>
<rawString>Laura Alonso Alemany, Irene Castell´on, and Nevena Tinkova Tincheva. 2007. Obtaining coarse-grained classes of subcategorization patterns for spanish. In RANLP’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SEPLN</author>
</authors>
<title>Evaluaci´on de m´etodos semi-autom´aticos para la conexi´on entre FrameNet y SenSem. In</title>
<date>2009</date>
<marker>SEPLN, 2009</marker>
<rawString>Laura Alonso Alemany, Irene Castell´on, Egoitz Laparra, and German Rigau. SEPLN. Evaluaci´on de m´etodos semi-autom´aticos para la conexi´on entre FrameNet y SenSem. In 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Alonso Alemany</author>
</authors>
<title>Learning parameters for an edit distance can learn us tendencies in usergenerated content. Invited talk at NLP in the Social Sciences, Instituto de Altos Estudios en Psicologia y Ciencias Sociales, Buenos Aires,</title>
<date>2010</date>
<location>Argentina,</location>
<contexts>
<context position="17662" citStr="Alemany, 2010" startWordPosition="2766" endWordPosition="2767">echniques on a relational database populated by the information extraction techniques used in the previous item. This line of research has been funded for three years (2009-2012) by the Argentinean Ministry for Science and Technology, within the PAE project, as a PICT project (PAE-PICT-2007-02290). This project opens many opportunities for collaboration. The resulting corpora will be of use for linguistic studies. The results of learning edit distances to find abbreviations can also be used by linguists as an input to study the regularities found in this kind of genres, as proposed in (Alonso Alemany, 2010). We think that some joint work on learning string edit distances would be very well integrated within this project. We are also very interested in collaborations with researchers who have some experience in NLP in similar genres, like short text messages or abbreviations in medical papers. Finally, interactions with data mining communities, both academic and industrial, would surely be very enriching for this project. 4.2 Characterisation of verbal behaviour One of our research interests is the empirical characterization of the subcategorization of lexical items, with a special interest on ve</context>
</contexts>
<marker>Alemany, 2010</marker>
<rawString>Laura Alonso Alemany. 2010. Learning parameters for an edit distance can learn us tendencies in usergenerated content. Invited talk at NLP in the Social Sciences, Instituto de Altos Estudios en Psicologia y Ciencias Sociales, Buenos Aires, Argentina, May 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Romina Altamirano</author>
</authors>
<title>Irasubcat: Un sistema para adquisici´on autom´atica de marcos de subcategorizaci´on de piezas l´exicas a partir de corpus. Master’s thesis, Facultad de Matem´atica, Astronomfa y Ffsica, Universidad Nacional de</title>
<date>2009</date>
<location>C´ordoba, Argentina.</location>
<contexts>
<context position="19829" citStr="Altamirano, 2009" startWordPosition="3093" endWordPosition="3094">anually annotated with the analysis of verbs at various linguistic levels (sense, aspect, voice, type of construction, arguments, role, function, etc.). Applying clustering techniques to the instances of verbs in these corpus, we obtained coarse-grained classes of verbs with the same subcategorization. A classifier was learned from considering clustered instances as classes. With this classifier, verbs in unseen sentences were assigned a subcategorization behaviour. Also with the aim of associating subcategorization information to verbs using evidence found in corpora, we developed IRASubcat (Altamirano, 2009). IRASubcat11. is a highly flexible system designed to gather information about the behaviour of verbs from corpora annotated at any level, and in any language. It identifies patterns of linguistic constituents that co-occur with verbs, detects optional constituents and performs hypothesis testing of the co-occurrence of verbs and patterns. We have also been working on connecting predicates in FrameNet and SenSem, using WordNet synsets as an interlingua (Alonso Alemany et al., SEPLN). We have found many dissimilarities between FrameNet and SenSem, but have been able to connect some of their pr</context>
</contexts>
<marker>Altamirano, 2009</marker>
<rawString>I. Romina Altamirano. 2009. Irasubcat: Un sistema para adquisici´on autom´atica de marcos de subcategorizaci´on de piezas l´exicas a partir de corpus. Master’s thesis, Facultad de Matem´atica, Astronomfa y Ffsica, Universidad Nacional de C´ordoba, Argentina.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikhail Bilenko</author>
<author>Raymond J Mooney</author>
</authors>
<title>Adaptive duplicate detection using learnable string similarity measures.</title>
<date>2003</date>
<booktitle>In Proceedings of the ninth ACM SIGKDD.</booktitle>
<contexts>
<context position="16170" citStr="Bilenko and Mooney, 2003" startWordPosition="2539" endWordPosition="2542">from a local newspaper, but one of our lines of work within this project is to assess the portability of methods and techniques to different genres. The goals we pursue are: creating corpora and related resources, and making them publicly available. A corpus of newspaper advertisements and a corpus of short text messages are underway. normalization of text bringing ortographic variants of a word (mostly abbreviations) to a canonical form. To do that, we apply machine learning techniques to learn the parameters for edit distances, as in (G´omez-Ballester et al., 1997; Ristad and Yanilos, 1998; Bilenko and Mooney, 2003; McCallum et al., 2005; Oncina and Sebban, 2006). We build upon previous work on normalization by (Choudhury et al., 2007; Okazaki et al., 2008; Cook and Stevenson, 2009; Stevenson et al., 2009). Preliminary results show a significant improvement of learned distances over standard distances. 11 syntactic analysis applying a robust shallow parsing approach aimed to identify entities and their modifiers. ontology induction from very restricted domains, to aid generalization in the step of information extraction. We will be following the approach presented in (Michelson and Knoblock, 2009). info</context>
</contexts>
<marker>Bilenko, Mooney, 2003</marker>
<rawString>Mikhail Bilenko and Raymond J. Mooney. 2003. Adaptive duplicate detection using learnable string similarity measures. In Proceedings of the ninth ACM SIGKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Santiago E Bruno</author>
</authors>
<title>Detecci´on de relaciones entre entidades en textos de biomedicina.</title>
<date>2009</date>
<booktitle>Master’s thesis, Facultad de Matem´atica, Astronomfa y Ffsica, Universidad Nacional de</booktitle>
<location>C´ordoba, Argentina.</location>
<contexts>
<context position="22069" citStr="Bruno, 2009" startWordPosition="3424" endWordPosition="3425">sources. We are also interested in bringing together information from different resources or from different sources (corpora, dictionaries, task-specific lexica, etc.), in order to achieve richer resources. We also have an interest for the study of hypothesis testing as applied to corpus-based computational linguistics, to get some insight on the information that these techniques may provide to guide research and validate results. 4.3 Discovering relations between entitites As a result of the Microbio project, we have developed a module to detect relations between entities in biomedical text (Bruno, 2009). This module has been trained with the GENIA corpus (Kim et al., 2008), obtaining good results (Alonso Alemany and Bruno, 2009). We have also explored different ways to overcome the data sparseness problem caused by the small amount of manually annotated examples that are available in the GENIA corpus. We have used the corpus as the initial seed of a bootstrapping procedure, generalized classes of relations via the GENIA ontology and generalized classes via clustering. Of these three procedures, only generalization via an ontology produced good results. However, we have hopes that a more insi</context>
</contexts>
<marker>Bruno, 2009</marker>
<rawString>Santiago E. Bruno. 2009. Detecci´on de relaciones entre entidades en textos de biomedicina. Master’s thesis, Facultad de Matem´atica, Astronomfa y Ffsica, Universidad Nacional de C´ordoba, Argentina.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rafael Carrascosa</author>
<author>Franc¸ois Coste</author>
<author>Matthias Gall´e</author>
<author>Gabriel Infante-Lopez</author>
</authors>
<title>Choosing Word Occurrences for the Smallest Grammar Problem.</title>
<date>2010</date>
<booktitle>In Proceedings of LATA</booktitle>
<publisher>Springer.</publisher>
<marker>Carrascosa, Coste, Gall´e, Infante-Lopez, 2010</marker>
<rawString>Rafael Carrascosa, Franc¸ois Coste, Matthias Gall´e, and Gabriel Infante-Lopez. 2010. Choosing Word Occurrences for the Smallest Grammar Problem. In Proceedings of LATA 2010. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Castell´on</author>
<author>Ana Fern´andez-Montraveta</author>
<author>Gl`oria V´azquez</author>
<author>Laura Alonso</author>
<author>Joanan Capilla</author>
</authors>
<title>The SENSEM corpus: a corpus annotated at the syntactic and semantic level.</title>
<date>2006</date>
<booktitle>In 5th International Conference on Language Resources and Evaluation (LREC</booktitle>
<marker>Castell´on, Fern´andez-Montraveta, V´azquez, Alonso, Capilla, 2006</marker>
<rawString>Irene Castell´on, Ana Fern´andez-Montraveta, Gl`oria V´azquez, Laura Alonso, and Joanan Capilla. 2006. The SENSEM corpus: a corpus annotated at the syntactic and semantic level. In 5th International Conference on Language Resources and Evaluation (LREC 2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Castell´on</author>
</authors>
<title>Laura Alonso Alemany, and Nevena Tinkova Tincheva.</title>
<date>2007</date>
<booktitle>In Proceedings of the Argentine Simposium on Artificial Intelligence, ASAI’07.</booktitle>
<marker>Castell´on, 2007</marker>
<rawString>Irene Castell´on, Laura Alonso Alemany, and Nevena Tinkova Tincheva. 2007. A procedure to automatically enrich verbal lexica with subcategorization frames. In Proceedings of the Argentine Simposium on Artificial Intelligence, ASAI’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Monojit Choudhury</author>
<author>Rahul Saraf</author>
<author>Vijit Jain</author>
<author>Animesh Mukherjee</author>
<author>Sudeshna Sarkar</author>
<author>Anupam Basu</author>
</authors>
<title>Investigation and modeling of the structure of texting language.</title>
<date>2007</date>
<journal>Int. J. Doc. Anal. Recognit.,</journal>
<volume>10</volume>
<issue>3</issue>
<contexts>
<context position="16292" citStr="Choudhury et al., 2007" startWordPosition="2559" endWordPosition="2562">ues to different genres. The goals we pursue are: creating corpora and related resources, and making them publicly available. A corpus of newspaper advertisements and a corpus of short text messages are underway. normalization of text bringing ortographic variants of a word (mostly abbreviations) to a canonical form. To do that, we apply machine learning techniques to learn the parameters for edit distances, as in (G´omez-Ballester et al., 1997; Ristad and Yanilos, 1998; Bilenko and Mooney, 2003; McCallum et al., 2005; Oncina and Sebban, 2006). We build upon previous work on normalization by (Choudhury et al., 2007; Okazaki et al., 2008; Cook and Stevenson, 2009; Stevenson et al., 2009). Preliminary results show a significant improvement of learned distances over standard distances. 11 syntactic analysis applying a robust shallow parsing approach aimed to identify entities and their modifiers. ontology induction from very restricted domains, to aid generalization in the step of information extraction. We will be following the approach presented in (Michelson and Knoblock, 2009). information extraction inducing templates from corpus using unsupervised and semisupervised techniques, and using induced temp</context>
</contexts>
<marker>Choudhury, Saraf, Jain, Mukherjee, Sarkar, Basu, 2007</marker>
<rawString>Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh Mukherjee, Sudeshna Sarkar, and Anupam Basu. 2007. Investigation and modeling of the structure of texting language. Int. J. Doc. Anal. Recognit., 10(3):157–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Pac-learning unambiguous nts languages.</title>
<date>2006</date>
<booktitle>In International Colloquium on Grammatical Inference,</booktitle>
<pages>59--71</pages>
<contexts>
<context position="9140" citStr="Clark, 2006" startWordPosition="1376" endWordPosition="1377">nd we are currently following two different lines, one that aims at characterizing the potential of a grammar formalism to learn a given treebank structure and a second that uses only regular automata to learn syntax. Characterization of Structures In (Luque and Infante-Lopez, 2009) we present a rather unusual result for language learning. We show an upper bound for the performance of a class of languages when a grammar from that class is used to parse the sentences in any given treebank. The class of languages we studied is the defined by Unambiguous Non-Terminally Separated (UNTS) grammars (Clark, 2006). UNTS grammars are interesting because, first, they have nice learnability properties like PAC learnability (Clark, 2006), and, second, they are used as the background formalism that won the Omphalos competition (Clark, 2007). Our strategy consists on characterizing all possible ways of parsing all the sentences in a treebank using UNTS grammars, then, we find the one that is closest to the treebank. We show that, in contrast to the results obtained for learning formal languages, UNTS are not capable of producing structures that score as stateof-the-art models on the treebanks we experimented</context>
</contexts>
<marker>Clark, 2006</marker>
<rawString>Alexander Clark. 2006. Pac-learning unambiguous nts languages. In International Colloquium on Grammatical Inference, pages 59–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Learning deterministic context free grammars: the omphalos competition.</title>
<date>2007</date>
<booktitle>Machine Learning,</booktitle>
<volume>66</volume>
<issue>1</issue>
<contexts>
<context position="9366" citStr="Clark, 2007" startWordPosition="1409" endWordPosition="1410">rization of Structures In (Luque and Infante-Lopez, 2009) we present a rather unusual result for language learning. We show an upper bound for the performance of a class of languages when a grammar from that class is used to parse the sentences in any given treebank. The class of languages we studied is the defined by Unambiguous Non-Terminally Separated (UNTS) grammars (Clark, 2006). UNTS grammars are interesting because, first, they have nice learnability properties like PAC learnability (Clark, 2006), and, second, they are used as the background formalism that won the Omphalos competition (Clark, 2007). Our strategy consists on characterizing all possible ways of parsing all the sentences in a treebank using UNTS grammars, then, we find the one that is closest to the treebank. We show that, in contrast to the results obtained for learning formal languages, UNTS are not capable of producing structures that score as stateof-the-art models on the treebanks we experimented with. Our results are for a particular, very specific type of grammar. We are currently exploring how to widen our technique to provide upper bounds to a more general class of languages. Our technique does not state how to ac</context>
</contexts>
<marker>Clark, 2007</marker>
<rawString>Alexander Clark. 2007. Learning deterministic context free grammars: the omphalos competition. Machine Learning, 66(1):93–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania, PA.</institution>
<contexts>
<context position="11856" citStr="Collins, 1999" startWordPosition="1835" endWordPosition="1836"> the EM algorithm to achieve state of the art performance. It is the combination of the Constituent-Context Model, that models unlabeled constituent parsing, and the Dependency Model with Valence, that models projective dependency parsing. On the other hand, CCM encodes the probability that a given string of POS tags is a constituent. DMV is more of our interest in this work, because it encodes a top-down generative process where the heads generate their dependents to both directions until there is a decision to stop, in a way that resembles successful supervised dependency models such as in (Collins, 1999). The generation of dependents of a head on a specific direction can be seen as an implicit probabilistic regular language generated by a probabilistic deterministic finite automaton. Under this perspective, the DMV model is in fact an algorithm for learning several automata at the same time. All automata have in common that they have the same number of states and the same number of arcs between states, which is given by the def10 inition of the DMV model. Automata differ in that they have different probabilities assigned to the transitions. The simple observation that DMV actually suppose a f</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Cook</author>
<author>Suzanne Stevenson</author>
</authors>
<title>An unsupervised model for text message normalization.</title>
<date>2009</date>
<booktitle>In Workshop on Computational Approaches to Linguistic Creativity. NAACL HLT</booktitle>
<contexts>
<context position="16340" citStr="Cook and Stevenson, 2009" startWordPosition="2567" endWordPosition="2571">re: creating corpora and related resources, and making them publicly available. A corpus of newspaper advertisements and a corpus of short text messages are underway. normalization of text bringing ortographic variants of a word (mostly abbreviations) to a canonical form. To do that, we apply machine learning techniques to learn the parameters for edit distances, as in (G´omez-Ballester et al., 1997; Ristad and Yanilos, 1998; Bilenko and Mooney, 2003; McCallum et al., 2005; Oncina and Sebban, 2006). We build upon previous work on normalization by (Choudhury et al., 2007; Okazaki et al., 2008; Cook and Stevenson, 2009; Stevenson et al., 2009). Preliminary results show a significant improvement of learned distances over standard distances. 11 syntactic analysis applying a robust shallow parsing approach aimed to identify entities and their modifiers. ontology induction from very restricted domains, to aid generalization in the step of information extraction. We will be following the approach presented in (Michelson and Knoblock, 2009). information extraction inducing templates from corpus using unsupervised and semisupervised techniques, and using induced templates to extract information to populate a relat</context>
</contexts>
<marker>Cook, Stevenson, 2009</marker>
<rawString>Paul Cook and Suzanne Stevenson. 2009. An unsupervised model for text message normalization. In Workshop on Computational Approaches to Linguistic Creativity. NAACL HLT 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E G´omez-Ballester</author>
<author>M L Mic´o-Andr´es</author>
<author>J Oncina</author>
<author>M L Forcada-Zubizarreta</author>
</authors>
<title>An empirical method to improve edit-distance parameters for a nearest-neighbor-based classification task.</title>
<date>1997</date>
<booktitle>In VII Spanish Symposium on Pattern Recognition and Image Analysis,</booktitle>
<location>Barcelona,</location>
<marker>G´omez-Ballester, Mic´o-Andr´es, Oncina, Forcada-Zubizarreta, 1997</marker>
<rawString>E. G´omez-Ballester, M. L. Mic´o-Andr´es, J. Oncina, and M. L. Forcada-Zubizarreta. 1997. An empirical method to improve edit-distance parameters for a nearest-neighbor-based classification task. In VII Spanish Symposium on Pattern Recognition and Image Analysis, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin D Kim</author>
<author>Tomoko Ohta</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Corpus annotation for mining biomedical events from literature.</title>
<date>2008</date>
<journal>BMCBioinformatics,</journal>
<volume>9</volume>
<issue>1</issue>
<contexts>
<context position="22140" citStr="Kim et al., 2008" startWordPosition="3435" endWordPosition="3438">rom different resources or from different sources (corpora, dictionaries, task-specific lexica, etc.), in order to achieve richer resources. We also have an interest for the study of hypothesis testing as applied to corpus-based computational linguistics, to get some insight on the information that these techniques may provide to guide research and validate results. 4.3 Discovering relations between entitites As a result of the Microbio project, we have developed a module to detect relations between entities in biomedical text (Bruno, 2009). This module has been trained with the GENIA corpus (Kim et al., 2008), obtaining good results (Alonso Alemany and Bruno, 2009). We have also explored different ways to overcome the data sparseness problem caused by the small amount of manually annotated examples that are available in the GENIA corpus. We have used the corpus as the initial seed of a bootstrapping procedure, generalized classes of relations via the GENIA ontology and generalized classes via clustering. Of these three procedures, only generalization via an ontology produced good results. However, we have hopes that a more insightful characterization of the examples and smarter learning techniques</context>
</contexts>
<marker>Kim, Ohta, Tsujii, 2008</marker>
<rawString>Jin D. Kim, Tomoko Ohta, and Jun’ichi Tsujii. 2008. Corpus annotation for mining biomedical events from literature. BMCBioinformatics, 9(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>A generative constituent-context model for improved grammar induction.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>128--135</pages>
<contexts>
<context position="11153" citStr="Klein and Manning, 2002" startWordPosition="1720" endWordPosition="1723"> UNTS grammars: they are PAC learnable, they can be learned with the same learning algorithm as UNTS and, moreover, their upper bound for performance is much higher than for UNTS. Still, it might be the case that the existing algorithm for finding UNTS is not the right one for learning the structure of a treebank, it might be the case that strings in the PTB have not been produced by a k-l-UNTS grammar. We are currently investigating how to produce an algorithm that fits better the structure given in a treebank. Learning Structure Using Probabilistic Automata DMV+CCM (Klein and Manning, 2004; Klein and Manning, 2002) is a probabilistic model for unsupervised parsing, that can be successfully trained with the EM algorithm to achieve state of the art performance. It is the combination of the Constituent-Context Model, that models unlabeled constituent parsing, and the Dependency Model with Valence, that models projective dependency parsing. On the other hand, CCM encodes the probability that a given string of POS tags is a constituent. DMV is more of our interest in this work, because it encodes a top-down generative process where the heads generate their dependents to both directions until there is a decis</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Dan Klein and Christopher D. Manning. 2002. A generative constituent-context model for improved grammar induction. In ACL, pages 128–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Corpusbased induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>42</pages>
<contexts>
<context position="11127" citStr="Klein and Manning, 2004" startWordPosition="1716" endWordPosition="1719">ages is more general than UNTS grammars: they are PAC learnable, they can be learned with the same learning algorithm as UNTS and, moreover, their upper bound for performance is much higher than for UNTS. Still, it might be the case that the existing algorithm for finding UNTS is not the right one for learning the structure of a treebank, it might be the case that strings in the PTB have not been produced by a k-l-UNTS grammar. We are currently investigating how to produce an algorithm that fits better the structure given in a treebank. Learning Structure Using Probabilistic Automata DMV+CCM (Klein and Manning, 2004; Klein and Manning, 2002) is a probabilistic model for unsupervised parsing, that can be successfully trained with the EM algorithm to achieve state of the art performance. It is the combination of the Constituent-Context Model, that models unlabeled constituent parsing, and the Dependency Model with Valence, that models projective dependency parsing. On the other hand, CCM encodes the probability that a given string of POS tags is a constituent. DMV is more of our interest in this work, because it encodes a top-down generative process where the heads generate their dependents to both directi</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Christopher D. Manning. 2004. Corpusbased induction of syntactic structure: Models of dependency and constituency. In Proc. of ACL 42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franco Luque</author>
<author>Gabriel Infante-Lopez</author>
</authors>
<title>Upper bounds for unsupervised parsing with unambiguous non-terminally.</title>
<date>2009</date>
<booktitle>In International Workshop Computational Linguistic Aspects of Grammatical Inference. EACL,</booktitle>
<contexts>
<context position="8811" citStr="Luque and Infante-Lopez, 2009" startWordPosition="1318" endWordPosition="1321">either small treebanks or none at all. Clearly, there is a big difference between producing or using a treebank for evaluation and producing or using them for training. In the former case, the size of the treebank can be significantly smaller. In our group, we have investigated different approaches to unsupervised learning of natural language. and we are currently following two different lines, one that aims at characterizing the potential of a grammar formalism to learn a given treebank structure and a second that uses only regular automata to learn syntax. Characterization of Structures In (Luque and Infante-Lopez, 2009) we present a rather unusual result for language learning. We show an upper bound for the performance of a class of languages when a grammar from that class is used to parse the sentences in any given treebank. The class of languages we studied is the defined by Unambiguous Non-Terminally Separated (UNTS) grammars (Clark, 2006). UNTS grammars are interesting because, first, they have nice learnability properties like PAC learnability (Clark, 2006), and, second, they are used as the background formalism that won the Omphalos competition (Clark, 2007). Our strategy consists on characterizing all</context>
</contexts>
<marker>Luque, Infante-Lopez, 2009</marker>
<rawString>Franco Luque and Gabriel Infante-Lopez. 2009. Upper bounds for unsupervised parsing with unambiguous non-terminally. In International Workshop Computational Linguistic Aspects of Grammatical Inference. EACL, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franco M Luque</author>
</authors>
<date>2009</date>
<journal>Implementation of the DMV+CCM</journal>
<note>parser. http://www.cs.famaf. unc.edu.ar/˜francolq/en/proyectos/ dmvccm.</note>
<contexts>
<context position="12622" citStr="Luque, 2009" startWordPosition="1963" endWordPosition="1964">terministic finite automaton. Under this perspective, the DMV model is in fact an algorithm for learning several automata at the same time. All automata have in common that they have the same number of states and the same number of arcs between states, which is given by the def10 inition of the DMV model. Automata differ in that they have different probabilities assigned to the transitions. The simple observation that DMV actually suppose a fixed structure for the automata it induces might explain its poor performance with freer order languages like Spanish. Using our own implementation (see (Luque, 2009)) we have empirically tested that DMV+CMV works well in languages with strict word order, like English, but for other languages with freer word order, like Spanish, DMV+CMV performance decreases dramatically. In order to improve DMV+CCM performance for this type of languages, the structure of the automaton might be modified, but since the DMV model has an ad hoc learning algorithm, a new parametric learning algorithm has to be defined. We are currently investigating different automaton structures for different languages and we are also investigating not only the induction of the parameters for</context>
</contexts>
<marker>Luque, 2009</marker>
<rawString>Franco M. Luque. 2009. Implementation of the DMV+CCM parser. http://www.cs.famaf. unc.edu.ar/˜francolq/en/proyectos/ dmvccm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Kedar Bellare</author>
<author>Fernando Pereira</author>
</authors>
<title>A conditional random field for discriminativelytrained finite-state string edit distance.</title>
<date>2005</date>
<booktitle>In Proceedings of the Proceedings of the Twenty-First Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-05),</booktitle>
<pages>388--395</pages>
<publisher>AUAI Press.</publisher>
<location>Arlington, Virginia.</location>
<contexts>
<context position="16193" citStr="McCallum et al., 2005" startWordPosition="2543" endWordPosition="2546">t one of our lines of work within this project is to assess the portability of methods and techniques to different genres. The goals we pursue are: creating corpora and related resources, and making them publicly available. A corpus of newspaper advertisements and a corpus of short text messages are underway. normalization of text bringing ortographic variants of a word (mostly abbreviations) to a canonical form. To do that, we apply machine learning techniques to learn the parameters for edit distances, as in (G´omez-Ballester et al., 1997; Ristad and Yanilos, 1998; Bilenko and Mooney, 2003; McCallum et al., 2005; Oncina and Sebban, 2006). We build upon previous work on normalization by (Choudhury et al., 2007; Okazaki et al., 2008; Cook and Stevenson, 2009; Stevenson et al., 2009). Preliminary results show a significant improvement of learned distances over standard distances. 11 syntactic analysis applying a robust shallow parsing approach aimed to identify entities and their modifiers. ontology induction from very restricted domains, to aid generalization in the step of information extraction. We will be following the approach presented in (Michelson and Knoblock, 2009). information extraction indu</context>
</contexts>
<marker>McCallum, Bellare, Pereira, 2005</marker>
<rawString>Andrew McCallum, Kedar Bellare, and Fernando Pereira. 2005. A conditional random field for discriminativelytrained finite-state string edit distance. In Proceedings of the Proceedings of the Twenty-First Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-05), pages 388–395, Arlington, Virginia. AUAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Michelson</author>
<author>Craig A Knoblock</author>
</authors>
<title>Phoebus: a system for extracting and integrating data from unstructured and ungrammatical sources.</title>
<date>2006</date>
<booktitle>In AAAI’06: proceedings of the 21st national conference on Artificial intelligence,</booktitle>
<pages>1947--1948</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="16992" citStr="Michelson and Knoblock, 2006" startWordPosition="2659" endWordPosition="2662">9). Preliminary results show a significant improvement of learned distances over standard distances. 11 syntactic analysis applying a robust shallow parsing approach aimed to identify entities and their modifiers. ontology induction from very restricted domains, to aid generalization in the step of information extraction. We will be following the approach presented in (Michelson and Knoblock, 2009). information extraction inducing templates from corpus using unsupervised and semisupervised techniques, and using induced templates to extract information to populate a relational database, as in (Michelson and Knoblock, 2006). data mining applying traditional knowledge discovery techniques on a relational database populated by the information extraction techniques used in the previous item. This line of research has been funded for three years (2009-2012) by the Argentinean Ministry for Science and Technology, within the PAE project, as a PICT project (PAE-PICT-2007-02290). This project opens many opportunities for collaboration. The resulting corpora will be of use for linguistic studies. The results of learning edit distances to find abbreviations can also be used by linguists as an input to study the regulariti</context>
</contexts>
<marker>Michelson, Knoblock, 2006</marker>
<rawString>Matthew Michelson and Craig A. Knoblock. 2006. Phoebus: a system for extracting and integrating data from unstructured and ungrammatical sources. In AAAI’06: proceedings of the 21st national conference on Artificial intelligence, pages 1947–1948. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Michelson</author>
<author>Craig A Knoblock</author>
</authors>
<title>Exploiting background knowledge to build reference sets for information extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 21st International Joint Conference on Artific ial Intelligence (IJCAI-2009),</booktitle>
<location>Pasadena, CA.</location>
<contexts>
<context position="16764" citStr="Michelson and Knoblock, 2009" startWordPosition="2629" endWordPosition="2632">los, 1998; Bilenko and Mooney, 2003; McCallum et al., 2005; Oncina and Sebban, 2006). We build upon previous work on normalization by (Choudhury et al., 2007; Okazaki et al., 2008; Cook and Stevenson, 2009; Stevenson et al., 2009). Preliminary results show a significant improvement of learned distances over standard distances. 11 syntactic analysis applying a robust shallow parsing approach aimed to identify entities and their modifiers. ontology induction from very restricted domains, to aid generalization in the step of information extraction. We will be following the approach presented in (Michelson and Knoblock, 2009). information extraction inducing templates from corpus using unsupervised and semisupervised techniques, and using induced templates to extract information to populate a relational database, as in (Michelson and Knoblock, 2006). data mining applying traditional knowledge discovery techniques on a relational database populated by the information extraction techniques used in the previous item. This line of research has been funded for three years (2009-2012) by the Argentinean Ministry for Science and Technology, within the PAE project, as a PICT project (PAE-PICT-2007-02290). This project ope</context>
</contexts>
<marker>Michelson, Knoblock, 2009</marker>
<rawString>Matthew Michelson and Craig A. Knoblock. 2009. Exploiting background knowledge to build reference sets for information extraction. In Proceedings of the 21st International Joint Conference on Artific ial Intelligence (IJCAI-2009), Pasadena, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naoaki Okazaki</author>
<author>Sophia Ananiadou</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A discriminative alignment model for abbreviation recognition.</title>
<date>2008</date>
<booktitle>In COLING ’08: Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>657--664</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="16314" citStr="Okazaki et al., 2008" startWordPosition="2563" endWordPosition="2566"> The goals we pursue are: creating corpora and related resources, and making them publicly available. A corpus of newspaper advertisements and a corpus of short text messages are underway. normalization of text bringing ortographic variants of a word (mostly abbreviations) to a canonical form. To do that, we apply machine learning techniques to learn the parameters for edit distances, as in (G´omez-Ballester et al., 1997; Ristad and Yanilos, 1998; Bilenko and Mooney, 2003; McCallum et al., 2005; Oncina and Sebban, 2006). We build upon previous work on normalization by (Choudhury et al., 2007; Okazaki et al., 2008; Cook and Stevenson, 2009; Stevenson et al., 2009). Preliminary results show a significant improvement of learned distances over standard distances. 11 syntactic analysis applying a robust shallow parsing approach aimed to identify entities and their modifiers. ontology induction from very restricted domains, to aid generalization in the step of information extraction. We will be following the approach presented in (Michelson and Knoblock, 2009). information extraction inducing templates from corpus using unsupervised and semisupervised techniques, and using induced templates to extract infor</context>
</contexts>
<marker>Okazaki, Ananiadou, Tsujii, 2008</marker>
<rawString>Naoaki Okazaki, Sophia Ananiadou, and Jun’ichi Tsujii. 2008. A discriminative alignment model for abbreviation recognition. In COLING ’08: Proceedings of the 22nd International Conference on Computational Linguistics, pages 657–664, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e Oncina</author>
<author>Marc Sebban</author>
</authors>
<title>Learning stochastic edit distance: Application in handwritten character recognition.</title>
<date>2006</date>
<journal>Pattern Recognition,</journal>
<volume>39</volume>
<issue>9</issue>
<contexts>
<context position="16219" citStr="Oncina and Sebban, 2006" startWordPosition="2547" endWordPosition="2550">ork within this project is to assess the portability of methods and techniques to different genres. The goals we pursue are: creating corpora and related resources, and making them publicly available. A corpus of newspaper advertisements and a corpus of short text messages are underway. normalization of text bringing ortographic variants of a word (mostly abbreviations) to a canonical form. To do that, we apply machine learning techniques to learn the parameters for edit distances, as in (G´omez-Ballester et al., 1997; Ristad and Yanilos, 1998; Bilenko and Mooney, 2003; McCallum et al., 2005; Oncina and Sebban, 2006). We build upon previous work on normalization by (Choudhury et al., 2007; Okazaki et al., 2008; Cook and Stevenson, 2009; Stevenson et al., 2009). Preliminary results show a significant improvement of learned distances over standard distances. 11 syntactic analysis applying a robust shallow parsing approach aimed to identify entities and their modifiers. ontology induction from very restricted domains, to aid generalization in the step of information extraction. We will be following the approach presented in (Michelson and Knoblock, 2009). information extraction inducing templates from corpus</context>
</contexts>
<marker>Oncina, Sebban, 2006</marker>
<rawString>Jos´e Oncina and Marc Sebban. 2006. Learning stochastic edit distance: Application in handwritten character recognition. Pattern Recognition, 39(9):1575–1587.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E S Ristad</author>
<author>P N Yanilos</author>
</authors>
<title>Learning string edit distance.</title>
<date>1998</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>20--522</pages>
<contexts>
<context position="16144" citStr="Ristad and Yanilos, 1998" startWordPosition="2535" endWordPosition="2538">classified advertisements from a local newspaper, but one of our lines of work within this project is to assess the portability of methods and techniques to different genres. The goals we pursue are: creating corpora and related resources, and making them publicly available. A corpus of newspaper advertisements and a corpus of short text messages are underway. normalization of text bringing ortographic variants of a word (mostly abbreviations) to a canonical form. To do that, we apply machine learning techniques to learn the parameters for edit distances, as in (G´omez-Ballester et al., 1997; Ristad and Yanilos, 1998; Bilenko and Mooney, 2003; McCallum et al., 2005; Oncina and Sebban, 2006). We build upon previous work on normalization by (Choudhury et al., 2007; Okazaki et al., 2008; Cook and Stevenson, 2009; Stevenson et al., 2009). Preliminary results show a significant improvement of learned distances over standard distances. 11 syntactic analysis applying a robust shallow parsing approach aimed to identify entities and their modifiers. ontology induction from very restricted domains, to aid generalization in the step of information extraction. We will be following the approach presented in (Michelson</context>
</contexts>
<marker>Ristad, Yanilos, 1998</marker>
<rawString>E. S. Ristad and P. N. Yanilos. 1998. Learning string edit distance. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20:522–532.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Stevenson</author>
<author>Yikun Guo</author>
<author>Abdulaziz Al Amri</author>
<author>Robert Gaizauskas</author>
</authors>
<title>Disambiguation of biomedical abbreviations.</title>
<date>2009</date>
<booktitle>In BioNLP ’09: Proceedings of the Workshop on BioNLP,</booktitle>
<pages>71--79</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="16365" citStr="Stevenson et al., 2009" startWordPosition="2572" endWordPosition="2575">elated resources, and making them publicly available. A corpus of newspaper advertisements and a corpus of short text messages are underway. normalization of text bringing ortographic variants of a word (mostly abbreviations) to a canonical form. To do that, we apply machine learning techniques to learn the parameters for edit distances, as in (G´omez-Ballester et al., 1997; Ristad and Yanilos, 1998; Bilenko and Mooney, 2003; McCallum et al., 2005; Oncina and Sebban, 2006). We build upon previous work on normalization by (Choudhury et al., 2007; Okazaki et al., 2008; Cook and Stevenson, 2009; Stevenson et al., 2009). Preliminary results show a significant improvement of learned distances over standard distances. 11 syntactic analysis applying a robust shallow parsing approach aimed to identify entities and their modifiers. ontology induction from very restricted domains, to aid generalization in the step of information extraction. We will be following the approach presented in (Michelson and Knoblock, 2009). information extraction inducing templates from corpus using unsupervised and semisupervised techniques, and using induced templates to extract information to populate a relational database, as in (Mi</context>
</contexts>
<marker>Stevenson, Guo, Amri, Gaizauskas, 2009</marker>
<rawString>Mark Stevenson, Yikun Guo, Abdulaziz Al Amri, and Robert Gaizauskas. 2009. Disambiguation of biomedical abbreviations. In BioNLP ’09: Proceedings of the Workshop on BioNLP, pages 71–79, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Taul´e</author>
<author>M A Martf</author>
<author>M Recasens</author>
</authors>
<title>Ancora: Multilevel annotated corpora for catalan and spanish.</title>
<date>2006</date>
<booktitle>In LREC’06.</booktitle>
<marker>Taul´e, Martf, Recasens, 2006</marker>
<rawString>M. Taul´e, M.A. Martf, and M. Recasens. 2006. Ancora: Multilevel annotated corpora for catalan and spanish. In LREC’06.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>