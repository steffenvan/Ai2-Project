<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.110736">
<title confidence="0.998086">
Comparing Spoken Language Route Instructions
for Robots across Environment Representations
</title>
<author confidence="0.998263">
Matthew Marge
</author>
<affiliation confidence="0.985797">
School of Computer Science
Carnegie Mellon University
</affiliation>
<address confidence="0.612538">
Pittsburgh, PA 15213
</address>
<email confidence="0.998551">
mrmarge@cs.cmu.edu
</email>
<author confidence="0.987684">
Alexander I. Rudnicky
</author>
<affiliation confidence="0.9822345">
School of Computer Science
Carnegie Mellon University
</affiliation>
<address confidence="0.773733">
Pittsburgh, PA 15213
</address>
<email confidence="0.999564">
air@cs.cmu.edu
</email>
<sectionHeader confidence="0.99565" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999450533333333">
Spoken language interaction between humans
and robots in natural environments will neces-
sarily involve communication about space and
distance. The current study examines people’s
close-range route instructions for robots and
how the presentation format (schematic, vir-
tual or natural) and the complexity of the route
affect the content of instructions. We find that
people have a general preference for providing
metric-based instructions. At the same time,
presentation format appears to have less im-
pact on the formulation of these instructions.
We conclude that understanding of spatial lan-
guage requires handling both landmark-based
and metric-based expressions.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999251">
Spoken language interaction between humans
and robots in natural environments will necessar-
ily involve communication about space and dis-
tance. It is consequently useful to understand the
nature of the language that humans would use for
this purpose. In the present study we examine
this question in the context of formulating route
instructions given to robots. For practical pur-
poses, we are also interested in understanding
how presentation format affects such language.
Instructions given in a physical space might dif-
fer from those given in a virtual world, which in
turn may differ from those given when only a
schematic representation (e.g., a map or drawing)
is available.
There is general agreement that landmarks
play an important role in spatial language (Dan-
iel and Denis, 2004; Klippel and Winter, 2005;
Lovelace et al., 1999; MacMahon, 2007; Michon
and Denis, 2001; Nothegger et al., 2004; Raubal
and Winter, 2002; Weissensteiner and Winter,
2004). However, landmarks might not necessar-
ily be used uniformly in instructions across pres-
entation formats. For example, people may use
objects in the environment as landmarks more
often when they do not have a good sense of dis-
tance in the environment. Behaviors related to
spatial language may change based on the com-
plexity of the route that a robot must take. This
could be due to a combination of factors, includ-
ing ease of use and personal assessment of a ro-
bot’s ability to interpret specific distances over
landmarks.
Several studies have investigated written or
typed spatial language (e.g., MacMahon et al.,
2006; Koulori and Lauria, 2009; Kollar et al.,
2010). In addition, Ross (2008) studied models
of spoken language interpretation in schematic
views of areas. In the current study we focus on
close-range spoken language route instructions.
</bodyText>
<sectionHeader confidence="0.999745" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999784882352941">
Interpreting spatial language is an important ca-
pability for systems (e.g., mobile robots) that
share space with people. Human-human commu-
nication of spatial language has been extensively
studied. Talmy (1983) proposed that the nature
of language places constraints on how people
communicate about space with others (i.e.,
schematization). Spatial descriptions are primar-
ily influenced by how reference objects fit along
fundamental axes that exhibit clear relationships
with the target, and secondly by the salience of
references (Carlson and Hill, 2008). People also
tend to keep their spatial descriptions consistent
after making an initial choice of strategy based
on any existing relationships between the target
to be described and other references (Vorwerg,
2009).
</bodyText>
<subsubsectionHeader confidence="0.676004">
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 157–164,
</subsubsectionHeader>
<affiliation confidence="0.889694">
The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics
</affiliation>
<page confidence="0.988573">
157
</page>
<figure confidence="0.99889025">
Mok Aki
Mok Aki
Mok Aki
(a) (b) (c)
</figure>
<figureCaption confidence="0.955007666666667">
Figure 1. Stimuli from the (a) schematic, (b) virtual, and (c) real-world scene experiments. Each
scenario has 2 robots, Mok (left) and Aki (right). Mok is the actor in all scenarios. Outlined are
possible destinations for Mok.
</figureCaption>
<bodyText confidence="0.999986416666667">
Studies involving spatial language with robots
have thus far focused on scenarios where one
robot is moved around an area using spatial
prepositions (Stopp et al., 1994; Moratz et al.,
2003) and further with landmarks (Skubic et al.,
2002; Perzanowski et al., 2003). A number of
these approaches, however, were crafted by the
designers of the robots themselves and not nec-
essarily based on an understanding of what
comes naturally to people. Indeed, Shi and Ten-
brink (2009) found that a person’s internal lin-
guistic representations may differ significantly
from what a robot is capable of interpreting.
Bugmann et al. (2004) motivated the concept of
corpus-based robotics, where spontaneous spo-
ken commands are collected and in turn used for
designing the functionality of robots. They col-
lected natural language instructions from people
commanding robots in a miniature of a real-
world environment. Our approach follows this
same reasoning; we explore naturally occurring
spatial language through route instructions to
robots in three distinct formats (schematic, vir-
tual, and natural environments).
</bodyText>
<sectionHeader confidence="0.979242" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.999968974358975">
We designed and conducted three experiments
using a navigation task that required the partici-
pant to “tell” a robot how to move to a target lo-
cation. We varied the presentation formats of the
stimuli (two-dimensional schematics, three-
dimensional virtual scenes, real-world areas in-
person). In each variant, the participant observed
a static scene depicting two robots (“Mok” and
“Aki”) and a destination marker. The partici-
pant’s task was to move Mok to the target desti-
nation using spoken instructions. Participants
were told to act as if they were an observer of the
scene but that were themselves not present in the
scene; put otherwise, the robots could hear par-
ticipants but not see them (and thus the partici-
pant could not figure in the instructions).
The experiment instructions directed partici-
pants to assume that Mok would understand
natural language and were told to use natural ex-
pressions to specify instructions (that is, there
was no “special language” necessary). Partici-
pants were told that they could take the orienta-
tions of the robots into account when they formu-
lated their instructions. They were moreover
asked to include all necessary steps in a single
utterance (i.e., a turn composed of one or more
spatial language commands). The robots did not
move in the experiments.
Since our aim was to learn about spoken lan-
guage route instructions, all participants recorded
their requests using a simple recorder interface
that could be activated while viewing the scene.
A standard headset microphone was used. To
avoid self-correction while speaking, the instruc-
tions directed participants to think about their
instructions before recording. Participants could
playback their instructions, and re-record them if
they deemed them unsatisfactory. All interface
activity was time-stamped and logged.
</bodyText>
<subsectionHeader confidence="0.997753">
3.1 General variations
</subsectionHeader>
<bodyText confidence="0.9999413">
In their work, Hayward and Tarr (1995) found
that people used spatial language with reference
to landmarks most often and found it most suit-
able when the objects in a scene were horizon-
tally or vertically aligned. We systematically var-
ied three elements of the stimuli in this study: the
orientations of the two robots, Mok and Aki, and
the location of the destination marker. Each ro-
bot’s orientation was varied four ways: directly
pointing forward, right, left, or backward. The
</bodyText>
<page confidence="0.994752">
158
</page>
<figureCaption confidence="0.988793">
Figure 2. Specified are four potential goal desti-
</figureCaption>
<bodyText confidence="0.991442181818182">
nations for Mok, the actor in all scenarios. Only
one of the destinations is shown on a particular
trial.
destination marker was also varied four ways:
directly in front of, behind, right of, or left of
Aki. These three dimensions were varied using a
factorial design, yielding 64 different configura-
tions that were presented in randomized order.
Thus each participant produced 64 sets of in-
structions. Participants received a break at the
halfway point of the session.
</bodyText>
<subsectionHeader confidence="0.999562">
3.2 Schematic (2-D) Scene Experiment
</subsectionHeader>
<bodyText confidence="0.999984266666667">
Participants observed two-dimensional configu-
rations of schematics that contained two robots
(Mok and Aki) and a destination marker in this
experiment. Each participant viewed a single
monitor displaying a recording interface overlaid
by static slides that contained the stimuli. After
each participant was shown the speech recording
interface and had tried it out, they proceeded
through a randomly ordered slide set. In this ex-
periment, participants viewed an overhead per-
spective of the scene, with the robots represented
as arrows and the destination marked by purple
circles (see Figures 1a and 2). The robots were
represented by arrows that were meant to indi-
cate their orientations in the scene.
</bodyText>
<subsectionHeader confidence="0.9989165">
3.3 Virtual (3-D) Scene and Distance
Awareness Variation Experiment
</subsectionHeader>
<bodyText confidence="0.999993594594595">
In this experiment, we crafted stimuli with a
three-dimensional map builder and USARSim, a
virtual simulation platform designed for conduct-
ing experiments with robots (Carpin et al., 2007).
The map was designed such that trials were
“rooms” in a multi-room environment. Partici-
pants did not walk through the environment; they
only viewed static configurations. Included in the
map were instances of two Pioneer P2AT robots.
All visual stimuli were presented at an eye-level
view, with eyes at a height of 5’10” (see Figure
1b). The room was designed such that walls
would be too far away to serve as landmarks.
Visual stimuli for this experiment required full-
screen access to the game engine, so the record-
ing interface was moved to an adjoining monitor.
We included an additional condition: inform-
ing participants (or not) of the distance between
the two robots. We recruited fourteen partici-
pants for this study, seven in each of two condi-
tions. In one condition (no-dist), participants
were not given any information related to the
scale of the robots and area in the stimuli. This is
equivalent to what participants experienced in
the schematic scene experiment. In the second
condition (dist), the instructions indicated that
the two robots, Mok and Aki, were seven feet
apart. However, no scale information (e.g., a
ruler) was provided in the scene itself. This
would provide the option to cast instructions in
terms of absolute distances. The option to use
Aki as a landmark reference point remained the
same as in the first experiment. We hypothesize
that participants that are not given a sense of
scale will use landmarks much more often than
those participants that are provided distance in-
formation.
</bodyText>
<subsectionHeader confidence="0.982188">
3.4 Real-World Scene Experiment
</subsectionHeader>
<bodyText confidence="0.999985782608696">
In natural environments, it can be assumed that
people generally have a good sense of scale. In
this experiment, participants viewed similar
stimuli to the virtual scenarios (eye-level view),
but in-person (see Figure 1c). Bins were used to
represent the two robots, with two eyes placed on
top of each bin to indicate orientation. As in the
previous experiments, participants were told to
give instructions to one robot (Mok) so that it
would arrive at the destination. We recorded par-
ticipant instructions for 8 different configurations
of the two robots (destination varied four ways,
Mok’s orientation varied two ways, right and
left; Aki’s orientation did not change). We sim-
plified the number of orientations because we
found that orientations of Mok and Aki did not
influence landmark use in the previous experi-
ments. After each instruction, participants were
asked to close their eyes as the experimenter
changed the orientations. Since they were not at
a computer screen for this experiment, only ver-
bal instructions were recorded, with no task
times.
</bodyText>
<subsectionHeader confidence="0.706172">
3.5 Participation
</subsectionHeader>
<bodyText confidence="0.9998405">
A total of 35 participants were recruited for this
study, 10 in the schematic scene experiment, 14
</bodyText>
<page confidence="0.987456">
159
</page>
<table confidence="0.999701125">
Environment Type Spoken language route instruction (transcribed with fillers removed)
2-D Mixed Mok turn left / and stop at the right hand side of Aki.
2-D Mixed Turn right about sixty degrees / then go forward until you&apos;re in front of Aki.
3-D no-dist Mixed Mok turn to your left / move towards Aki when you are pretty close to Aki stop there /
turn to your right / continue moving in a straight line path you will find a blue dot to your
left at some point stop there / turn to your left / and reach the blue dot which is your
destination.
3-D no-dist Relative Go forward half the distance between you and Aki.
3-D dist Absolute Rotate to your right / move forward about five feet / rotate again to your left / and move
forward about seven feet.
3-D dist Absolute Turn to your right / move forward one foot / turn to your left / move forward ten feet /
turn to your left again / move forward one foot.
Real-world Absolute Okay Mok I want you to go straight ahead for about five feet / then turn to your right
forty five degrees / and go ahead and you&apos;re gonna hit the spot in about four feet from
there.
Real-world Mixed Mok move to Aki / turn left / and move forward three feet.
</table>
<tableCaption confidence="0.959203">
Table 1. Spoken language route instructions for Mok, the moving robot, were transcribed and di-
</tableCaption>
<bodyText confidence="0.992224333333333">
vided into absolute and relative steps (absolute step / relative step). Absolute steps are explicit in-
structions that contain metric or metric-like distances, while relative steps include Aki (the static
robot) as a reference.
in the virtual scene experiment, and 11 in the
real-world scene experiment. Participants ranged
in age from 19 to 61 (M = 28.4 years, SD = 9.9).
Of all participants, 22 were male and 14 were
female. All participants were self-reported fluent
English speakers.
</bodyText>
<sectionHeader confidence="0.998693" genericHeader="method">
4 Data
</sectionHeader>
<bodyText confidence="0.9999924">
The first study (schematic stimuli) yielded a total
of 640 route instructions (64 from each of 10
participants). All of these instructions were tran-
scribed in-house using the CMU Communicator
guidelines (Bennett and Rudnicky, 2002). In ad-
dition to the recorded instructions, we also
logged participants’ interactions with the speech
recording interface. Since the experiment instruc-
tions ask participants to think about what they
plan to say before recording their speech, we as-
sessed their “thinking time” from this logging
information.
In the second study (virtual stimuli), more par-
ticipants were recruited, but they were divided
into two conditions (presence/absence of an ex-
plicitly stated metric distance between the two
robots in the stimuli). A total of 896 route in-
structions were collected in the second study (64
from each of 14 participants). Of the 14 partici-
pants recruited for this study, 12 were transcribed
using Amazon’s Mechanical Turk (Marge et al.,
2010) with the same guidelines as the first study.
In the real-world study, 8 route instructions were
recorded from 11 participants and transcribed,
yielding a total of 88 utterances.
</bodyText>
<sectionHeader confidence="0.998563" genericHeader="method">
5 Measurements
</sectionHeader>
<bodyText confidence="0.999947882352941">
Several outcomes were analyzed in this study,
including the time needed to formulate directions
to the robot and the number of discrete steps that
participants included in their instructions. We
analyzed two measures, “thinking time” and
word count. Thinking time represents the time
between starting viewing a stimulus and pressing
the “Record” button. We measured utterance
length by counting the number of words spoken
by participants for each instruction. Utterance-
level restarts and mispronunciations were ex-
cluded from this count.
We also coded the instructions in terms of the
number of discrete “steps” (see Table 1). We
defined a “step” as any action where motion by
Mok (the moving robot) was required to com-
plete a sub-goal. For example, “turn left and
</bodyText>
<page confidence="0.996564">
160
</page>
<figureCaption confidence="0.992111666666667">
Figure 3. Mean proportion of relative steps to
absolute steps across distance-naïve 2-D (sche-
matic), distance-naïve 3-D (virtual), distance-
aware 3-D (virtual), and real-world scenarios
(with a 1% margin of error).
Figure 4. Proportions of instruction types across
distance-naïve 2-D (schematic), distance-naïve
3-D (virtual), distance-aware 3-D (virtual), and
real-world scenarios.
</figureCaption>
<bodyText confidence="0.999318125">
move forward five feet” consists of two steps: (1)
a ninety degree turn to the left and (2) a move-
ment forward of five feet to get to a new loca-
tion. We divided steps into two categories, abso-
lute steps and relative steps (similar to Levin-
son’s (1996) absolute and intrinsic reference sys-
tems). An absolute step is one with explicit in-
structions that contain metric or metric-like dis-
tances (e.g., “move forward two feet”, “turn right
ninety degrees”, “move forward three steps”).
We assume that simple turns (e.g., “turn right”)
are turns of 90 degrees, and thus are absolute
steps. We define a relative step as one that in-
cludes Aki, the static robot, in the reference (e.g.,
“move forward until you reach Aki”, “turn right
until you face Aki”).
</bodyText>
<sectionHeader confidence="0.999878" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999982375">
We conducted analyses based on measures of
thinking time, word count, and the number of
discrete “steps” in participants’ spoken language
route instructions. Among the folds of the data
we examined were observations from schematics
without distance information (i.e., “2-D no-
dist”), virtual scenes without giving participants
distance information (i.e., “3-D no-dist”), virtual
scenes with giving participants initial distance
information (i.e., “3-D dist”), and real-world
scenes (i.e., “realworld”). Since we collected an
equal number of route instructions in the two
virtual scene conditions (i.e., with and without
being told about the distance in the environ-
ment), we directly compared properties of these
instructions.
In Sections 6.2 and 6.3, absolute steps, rela-
tive steps, word count (log-10 transformed), and
thinking timing (log-10 transformed) were the
dependent measures in mixed-effects models of
analysis of variance (for significance testing).
ParticipantID was modeled as a random effect.
We are interested in the population from which
participants were drawn.
</bodyText>
<subsectionHeader confidence="0.999846">
6.1 Adjusting Spatial Information
</subsectionHeader>
<bodyText confidence="0.999919">
Landmark use was affected by participants’
awareness of scale. The fewer scale cues avail-
able, the greater the number of references to
landmarks. Thus, landmarks were most prevalent
in instructions generated for schematic scenarios
and least prevalent in the condition that explicitly
specified a scale. See Figure 3 for the actual pro-
portions. We did not inform participants of scale
in the real-world condition. Interestingly, their
absolute/relative mix was closer to the no-scale
conditions even though they were observing an
actual scene and could presumably make infer-
ences about distances. Figure 4 shows that pres-
entation format also affected participants’ use of
instructions that were entirely absolute in nature.
There were fewer mixed instructions (i.e., in-
structions where absolute instructions were sup-
ported by landmarks) in conditions where par-
ticipants had a sense of scale.
Though distances may be self-evident in real-
world scenarios, they often are not in virtual en-
</bodyText>
<figure confidence="0.99826405">
26.9%
93.5%
58.9% 68.5%
73.1%
2d 3d nodist 3d dist realworld
100%
90%
80%
70%
60%
50%
40%
30%
20%
10%
0%
Absolute Proportion Relative Proportion
41.1% 31.5%
6.5%
40%
30%
20%
10%
100%
58.0% 56.2%
21.5% 29.1%
0.9%
90%
80%
70%
60%
50%
14.4% 7.9%
27.7% 36.0%
77.6%
54.7%
0%
16.3%
2d 3d nodist 3d dist realworld
Absolute Relative Mixed
</figure>
<page confidence="0.99392">
161
</page>
<bodyText confidence="0.9995708">
vironments. Participants behaved differently
from real-world scenarios when we presented a
non-trivial indication of scale. Participants’ in-
structions were dominated by absolute instruc-
tions when they had a sense of scale in a virtual
environment. This suggests that despite similari-
ties in scale awareness, people formulate spatial
language instructions differently when they can-
not for themselves determine a sense of distance
in an environment.
</bodyText>
<subsectionHeader confidence="0.999974">
6.2 Sense of Distance in Virtual Stimuli
</subsectionHeader>
<bodyText confidence="0.999997018181818">
We directly compared participants’ spoken lan-
guage route instructions with respect to the pres-
ence (i.e., “dist”) or absence (i.e., “no-dist”) of
distance information in the virtual environment.
Though participants already had an initial prefer-
ence toward using metric-based instructions,
these became dominant when participants were
aware of the distance in the virtual environment.
Participants that were not given a sense of dis-
tance referred to Aki as a landmark much more
than when participants were given a sense of dis-
tance, confirming our initial hypothesis. We ob-
served that the mean number of relative steps in
the no-dist condition was nearly four times
greater (1.0 relative steps per instruction) than
the dist condition (0.2 relative steps per instruc-
tion) (F[1, 12] = 4.6, p = 0.05). As expected, par-
ticipants used absolute references more in the
dist condition, given the lack of landmark use.
The mean number of absolute steps was greater
in the dist condition (3.3 per instruction) com-
pared to the no-dist condition (mean 2.4 absolute
steps per instruction) (F[1, 12] = 5.5, p &lt; 0.05).
As shown in Figure 3, the proportions of abso-
lute to relative steps in participants’ instructions
show clear differences in strategy. When partici-
pants received distance information, an over-
whelming majority of steps were absolute in na-
ture (i.e., steps containing metric or metric-like
distances). Aki was mentioned in steps only
6.5% of the time in the dist condition (i.e., rela-
tive steps). The proportions were more balanced
in the no-dist condition, with 68% of steps being
absolute. The remaining 32% of steps referred to
Aki. The difference between proportions from
the no-dist and dist conditions was statistically
significant (F[1,12] = 7.5, p &lt; 0.05). From these
analyses we can see that distance greatly influ-
enced participants’ language instructions in vir-
tual environments.
We further classified participants’ instructions
as entirely absolute, relative, or mixed in nature.
When participants used landmarks, they tended
to mix them with absolute steps in their instruc-
tions. Participants in the dist condition comprised
most instructions with only absolute steps. How-
ever, even though 6.5% of steps were absolute in
nature, they were distributed among one-fifth of
instructions. In the no-dist condition, though
relative steps comprised only 31.5% of total
steps, they were distributed among a majority of
the instructions. These results suggest that se-
quences of absolute steps may be sufficient on
their own, but relative steps, when used, depend
on the presence of some absolute terms.
</bodyText>
<subsectionHeader confidence="0.999469">
6.3 Goal Location and Orientation Results
</subsectionHeader>
<bodyText confidence="0.999996472222222">
Our analysis showed that the goal location in
scenarios impacted participants’ instructions. For
word count, participants used significantly dif-
ferent numbers of words based on the goal loca-
tion (F[3, 1580] = 252.2, p &lt; 0.0001). Upon fur-
ther analysis, across all experiments, when the
goal was closest to the Mok, the moving robot,
people spoke fewer words (14 fewer words on
average) compared to other locations (analysis
conducted with a Tukey pairwise comparisons
test). Participants also had significantly different
thinking times based on the goal location (F[3,
1502] = 6.21, p &lt; 0.05). Thinking time for the
destination closest to Mok was lowest overall (on
average at least 1.3s lower) and significantly dif-
ferent from two of the three remaining goal loca-
tions (via a Tukey pairwise comparisons test).
There were no significant differences in word
count and thinking time when varying Mok’s
orientation or Aki’s orientation.
We also observed patterns in the steps people
gave in their instructions. A landmark’s place-
ment, when directly interfering with a goal, in-
creased its reference in spatial language instruc-
tions. When the goal location was blocked by
Aki, we observed a high proportion of relative
steps. For schematic stimuli, participants often
required Mok to move past Aki in order to get to
the destination. After observing the proportions
of absolute steps and relative steps out of the to-
tal number of steps across destination, we found
that stimuli with this destination yielded an aver-
age of 45% relative steps to 55% absolute steps.
This is a greater proportion than any of the other
destinations (their relative step proportions
ranged from 33% to 38%).
</bodyText>
<sectionHeader confidence="0.94678" genericHeader="conclusions">
7 Summary and Conclusions
</sectionHeader>
<bodyText confidence="0.99786">
We presented a study that examines people’s
close-range spoken language route instructions
</bodyText>
<page confidence="0.993089">
162
</page>
<bodyText confidence="0.999992711538462">
for robots and how the presentation format and
the complexity of the route influenced the con-
tent of instructions. Across all presentation for-
mats, people preferred providing instructions that
were absolute in nature (i.e., metric-based). De-
spite this preference, landmarks were used on
occasion. When they were, participants’ use of
them was influenced by the presentation format
(schematic, virtual or natural). When participants
had a general sense of distance in scenes, they
were much more acclimated to using specific
distances to give route instructions to a robot.
Our results indicate that the goal location can
influence participant effort (i.e., time to formu-
late) and the pattern (absolute/relative) in spoken
language route instructions to robots. Several of
these were predictable (e.g., least effort when
goal location was closest to moving robot).
When participants viewed these configurations in
virtual environments, there were clear differ-
ences in their instructions based on whether or
not they were given a sense of scale.
We compared the natural language instruc-
tions from the real-world condition to those from
virtual stimuli. Figure 3 shows that in general,
real-world participants’ instructions contained
similar proportions of landmarks to the 3d no-
dist (virtual) condition. However, there was a
greater preference to use absolute steps in the
real-world than in the virtual world; participants
apparently access their own sense of scale when
formulating these instructions. With respect to
spatial language instructions, participants tended
to treat virtual environments much like real-
world environments.
This study provides useful information about
methodology in the study of spatial language and
also suggests principles for the design of spatial
language understanding capabilities for robots in
human environments. Specifically, virtual world
representations, under suitable conditions, elicit
language similar to that found under real-world
situations, although the more information people
have about the metric properties of the environ-
ment the more likely they are to use them. But
even in the absence of unambiguous metrics
people seem to want to use such language in the
instructions that they produce. These observa-
tions can be used to inform the design of spatial
language understanding for robot systems as well
as guide the development of requirements for a
spatial reasoning component.
</bodyText>
<sectionHeader confidence="0.995204" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999957">
This work was supported by the Boeing Com-
pany and a National Science Foundation Gradu-
ate Research Fellowship. The authors would like
to thank Carolyn Rosé, Satanjeev Banerjee,
Aasish Pappu, and the anonymous reviewers for
their helpful comments on this work. The views
and conclusions expressed in this document only
represent those of the authors.
</bodyText>
<sectionHeader confidence="0.999194" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999853">
C. Bennett and A. I. Rudnicky. 2002. The Carnegie
Mellon Communicator Corpus, ICSLP, 2002.
G. Bugmann, E. Klein, S. Lauria, and T. Kyriacou.
2004. Corpus-based robotics: A route instruction
example, Intelligent Autonomous System, pp. 96-
103.
L. A. Carlson and P. L. Hill. 2008. Processing the
presence, placement and properties of a distractor
during spatial language tasks, Memory and
Cognition, 36, pp. 240-255.
S. Carpin, M. Lewis, J. Wang, S. Balakirsky, and C.
Scrapper. 2007. USARSim: A Robot Simulator for
Research and Education, International Conference
on Robotics and Automation, 2007, pp. 1400-1405.
M. P. Daniel and M. Denis. 2004. The production of
route directions: Investigating conditions that
favour conciseness in spatial discourse, Applied
Cognitive Psychology, 18, pp. 57-75.
W. G. Hayward and M. J. Tarr. 1995. Spatial
language and spatial representation, Cognition, 55
(1), pp. 39-84.
A. Klippel and S. Winter. 2005. Structural Salience of
Landmarks for Route Directions, COSIT 2005, pp.
347-362.
T. Kollar, S. Tellex, D. Roy, and N. Roy. 2010.
Toward Understanding Natural Language
Directions, Human Robot Interaction Conference
(HRI-2010), pp. 259-266.
T. Koulouri and S. Lauria. 2009. Exploring
Miscommunication and Collaborative Behaviour in
Human-Robot Interaction, SIGdial 2009, pp. 111-
119.
S. C. Levinson. 1996. Frames of reference and
Molyneux’s question: cross-linguistic evidence, in
P. Bloom, M. Peterson, L. Nadel, and M. Garrett
(Eds.), Language and space, pp. 109-169.
K. Lovelace, M. Hegarty, and D. R. Montello. 1999.
Elements of good route directions in familiar and
unfamiliar environments, in C. Freksa and D. M.
Mark (Eds.), Spatial information theory: Cognitive
and computational foundations of geographic
information science. Berlin: Springer.
M. MacMahon. 2007. Following Natural Language
Route Instructions, Ph.D. Thesis, University of
Texas at Austin.
M. MacMahon, B. Stankiewicz, and B. Kuipers.
2006. Walk the Talk: Connecting Language,
Knowledge, and Action in Route Instructions, 21st
</reference>
<page confidence="0.999119">
163
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.503610">
<title confidence="0.9997595">Comparing Spoken Language Route for Robots across Environment Representations</title>
<author confidence="0.998642">Matthew</author>
<affiliation confidence="0.9997955">School of Computer Carnegie Mellon</affiliation>
<address confidence="0.509573">Pittsburgh, PA</address>
<email confidence="0.999019">mrmarge@cs.cmu.edu</email>
<author confidence="0.999944">Alexander I Rudnicky</author>
<affiliation confidence="0.99995">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.99982">Pittsburgh, PA 15213</address>
<email confidence="0.997017">air@cs.cmu.edu</email>
<abstract confidence="0.999646625">Spoken language interaction between humans and robots in natural environments will necessarily involve communication about space and distance. The current study examines people’s close-range route instructions for robots and how the presentation format (schematic, virtual or natural) and the complexity of the route affect the content of instructions. We find that people have a general preference for providing metric-based instructions. At the same time, presentation format appears to have less impact on the formulation of these instructions. We conclude that understanding of spatial language requires handling both landmark-based and metric-based expressions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Bennett</author>
<author>A I Rudnicky</author>
</authors>
<title>The Carnegie Mellon Communicator Corpus,</title>
<date>2002</date>
<location>ICSLP,</location>
<contexts>
<context position="13781" citStr="Bennett and Rudnicky, 2002" startWordPosition="2200" endWordPosition="2203">t instructions that contain metric or metric-like distances, while relative steps include Aki (the static robot) as a reference. in the virtual scene experiment, and 11 in the real-world scene experiment. Participants ranged in age from 19 to 61 (M = 28.4 years, SD = 9.9). Of all participants, 22 were male and 14 were female. All participants were self-reported fluent English speakers. 4 Data The first study (schematic stimuli) yielded a total of 640 route instructions (64 from each of 10 participants). All of these instructions were transcribed in-house using the CMU Communicator guidelines (Bennett and Rudnicky, 2002). In addition to the recorded instructions, we also logged participants’ interactions with the speech recording interface. Since the experiment instructions ask participants to think about what they plan to say before recording their speech, we assessed their “thinking time” from this logging information. In the second study (virtual stimuli), more participants were recruited, but they were divided into two conditions (presence/absence of an explicitly stated metric distance between the two robots in the stimuli). A total of 896 route instructions were collected in the second study (64 from ea</context>
</contexts>
<marker>Bennett, Rudnicky, 2002</marker>
<rawString>C. Bennett and A. I. Rudnicky. 2002. The Carnegie Mellon Communicator Corpus, ICSLP, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Bugmann</author>
<author>E Klein</author>
<author>S Lauria</author>
<author>T Kyriacou</author>
</authors>
<title>Corpus-based robotics: A route instruction example, Intelligent Autonomous System,</title>
<date>2004</date>
<pages>96--103</pages>
<contexts>
<context position="4717" citStr="Bugmann et al. (2004)" startWordPosition="716" endWordPosition="719">dies involving spatial language with robots have thus far focused on scenarios where one robot is moved around an area using spatial prepositions (Stopp et al., 1994; Moratz et al., 2003) and further with landmarks (Skubic et al., 2002; Perzanowski et al., 2003). A number of these approaches, however, were crafted by the designers of the robots themselves and not necessarily based on an understanding of what comes naturally to people. Indeed, Shi and Tenbrink (2009) found that a person’s internal linguistic representations may differ significantly from what a robot is capable of interpreting. Bugmann et al. (2004) motivated the concept of corpus-based robotics, where spontaneous spoken commands are collected and in turn used for designing the functionality of robots. They collected natural language instructions from people commanding robots in a miniature of a realworld environment. Our approach follows this same reasoning; we explore naturally occurring spatial language through route instructions to robots in three distinct formats (schematic, virtual, and natural environments). 3 Method We designed and conducted three experiments using a navigation task that required the participant to “tell” a robot</context>
</contexts>
<marker>Bugmann, Klein, Lauria, Kyriacou, 2004</marker>
<rawString>G. Bugmann, E. Klein, S. Lauria, and T. Kyriacou. 2004. Corpus-based robotics: A route instruction example, Intelligent Autonomous System, pp. 96-103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L A Carlson</author>
<author>P L Hill</author>
</authors>
<title>Processing the presence, placement and properties of a distractor during spatial language tasks,</title>
<date>2008</date>
<journal>Memory and Cognition,</journal>
<volume>36</volume>
<pages>240--255</pages>
<contexts>
<context position="3386" citStr="Carlson and Hill, 2008" startWordPosition="505" endWordPosition="508">s on close-range spoken language route instructions. 2 Related Work Interpreting spatial language is an important capability for systems (e.g., mobile robots) that share space with people. Human-human communication of spatial language has been extensively studied. Talmy (1983) proposed that the nature of language places constraints on how people communicate about space with others (i.e., schematization). Spatial descriptions are primarily influenced by how reference objects fit along fundamental axes that exhibit clear relationships with the target, and secondly by the salience of references (Carlson and Hill, 2008). People also tend to keep their spatial descriptions consistent after making an initial choice of strategy based on any existing relationships between the target to be described and other references (Vorwerg, 2009). Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 157–164, The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics 157 Mok Aki Mok Aki Mok Aki (a) (b) (c) Figure 1. Stimuli from the (a) schematic, (b) virtual, and (c) real-world scene experiments. Each scenario has 2 robots, </context>
</contexts>
<marker>Carlson, Hill, 2008</marker>
<rawString>L. A. Carlson and P. L. Hill. 2008. Processing the presence, placement and properties of a distractor during spatial language tasks, Memory and Cognition, 36, pp. 240-255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Carpin</author>
<author>M Lewis</author>
<author>J Wang</author>
<author>S Balakirsky</author>
<author>C Scrapper</author>
</authors>
<title>USARSim: A Robot Simulator for Research and</title>
<date>2007</date>
<booktitle>Education, International Conference on Robotics and Automation,</booktitle>
<pages>1400--1405</pages>
<contexts>
<context position="9042" citStr="Carpin et al., 2007" startWordPosition="1393" endWordPosition="1396">face and had tried it out, they proceeded through a randomly ordered slide set. In this experiment, participants viewed an overhead perspective of the scene, with the robots represented as arrows and the destination marked by purple circles (see Figures 1a and 2). The robots were represented by arrows that were meant to indicate their orientations in the scene. 3.3 Virtual (3-D) Scene and Distance Awareness Variation Experiment In this experiment, we crafted stimuli with a three-dimensional map builder and USARSim, a virtual simulation platform designed for conducting experiments with robots (Carpin et al., 2007). The map was designed such that trials were “rooms” in a multi-room environment. Participants did not walk through the environment; they only viewed static configurations. Included in the map were instances of two Pioneer P2AT robots. All visual stimuli were presented at an eye-level view, with eyes at a height of 5’10” (see Figure 1b). The room was designed such that walls would be too far away to serve as landmarks. Visual stimuli for this experiment required fullscreen access to the game engine, so the recording interface was moved to an adjoining monitor. We included an additional conditi</context>
</contexts>
<marker>Carpin, Lewis, Wang, Balakirsky, Scrapper, 2007</marker>
<rawString>S. Carpin, M. Lewis, J. Wang, S. Balakirsky, and C. Scrapper. 2007. USARSim: A Robot Simulator for Research and Education, International Conference on Robotics and Automation, 2007, pp. 1400-1405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Daniel</author>
<author>M Denis</author>
</authors>
<title>The production of route directions: Investigating conditions that favour conciseness in spatial discourse,</title>
<date>2004</date>
<journal>Applied Cognitive Psychology,</journal>
<volume>18</volume>
<pages>57--75</pages>
<contexts>
<context position="1790" citStr="Daniel and Denis, 2004" startWordPosition="256" endWordPosition="260">ul to understand the nature of the language that humans would use for this purpose. In the present study we examine this question in the context of formulating route instructions given to robots. For practical purposes, we are also interested in understanding how presentation format affects such language. Instructions given in a physical space might differ from those given in a virtual world, which in turn may differ from those given when only a schematic representation (e.g., a map or drawing) is available. There is general agreement that landmarks play an important role in spatial language (Daniel and Denis, 2004; Klippel and Winter, 2005; Lovelace et al., 1999; MacMahon, 2007; Michon and Denis, 2001; Nothegger et al., 2004; Raubal and Winter, 2002; Weissensteiner and Winter, 2004). However, landmarks might not necessarily be used uniformly in instructions across presentation formats. For example, people may use objects in the environment as landmarks more often when they do not have a good sense of distance in the environment. Behaviors related to spatial language may change based on the complexity of the route that a robot must take. This could be due to a combination of factors, including ease of u</context>
</contexts>
<marker>Daniel, Denis, 2004</marker>
<rawString>M. P. Daniel and M. Denis. 2004. The production of route directions: Investigating conditions that favour conciseness in spatial discourse, Applied Cognitive Psychology, 18, pp. 57-75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W G Hayward</author>
<author>M J Tarr</author>
</authors>
<title>Spatial language and spatial representation,</title>
<date>1995</date>
<journal>Cognition,</journal>
<volume>55</volume>
<issue>1</issue>
<pages>39--84</pages>
<contexts>
<context position="7080" citStr="Hayward and Tarr (1995)" startWordPosition="1082" endWordPosition="1085">e robots did not move in the experiments. Since our aim was to learn about spoken language route instructions, all participants recorded their requests using a simple recorder interface that could be activated while viewing the scene. A standard headset microphone was used. To avoid self-correction while speaking, the instructions directed participants to think about their instructions before recording. Participants could playback their instructions, and re-record them if they deemed them unsatisfactory. All interface activity was time-stamped and logged. 3.1 General variations In their work, Hayward and Tarr (1995) found that people used spatial language with reference to landmarks most often and found it most suitable when the objects in a scene were horizontally or vertically aligned. We systematically varied three elements of the stimuli in this study: the orientations of the two robots, Mok and Aki, and the location of the destination marker. Each robot’s orientation was varied four ways: directly pointing forward, right, left, or backward. The 158 Figure 2. Specified are four potential goal destinations for Mok, the actor in all scenarios. Only one of the destinations is shown on a particular trial</context>
</contexts>
<marker>Hayward, Tarr, 1995</marker>
<rawString>W. G. Hayward and M. J. Tarr. 1995. Spatial language and spatial representation, Cognition, 55 (1), pp. 39-84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Klippel</author>
<author>S Winter</author>
</authors>
<title>Structural Salience of Landmarks for Route Directions,</title>
<date>2005</date>
<pages>347--362</pages>
<location>COSIT</location>
<contexts>
<context position="1816" citStr="Klippel and Winter, 2005" startWordPosition="261" endWordPosition="264">ure of the language that humans would use for this purpose. In the present study we examine this question in the context of formulating route instructions given to robots. For practical purposes, we are also interested in understanding how presentation format affects such language. Instructions given in a physical space might differ from those given in a virtual world, which in turn may differ from those given when only a schematic representation (e.g., a map or drawing) is available. There is general agreement that landmarks play an important role in spatial language (Daniel and Denis, 2004; Klippel and Winter, 2005; Lovelace et al., 1999; MacMahon, 2007; Michon and Denis, 2001; Nothegger et al., 2004; Raubal and Winter, 2002; Weissensteiner and Winter, 2004). However, landmarks might not necessarily be used uniformly in instructions across presentation formats. For example, people may use objects in the environment as landmarks more often when they do not have a good sense of distance in the environment. Behaviors related to spatial language may change based on the complexity of the route that a robot must take. This could be due to a combination of factors, including ease of use and personal assessment</context>
</contexts>
<marker>Klippel, Winter, 2005</marker>
<rawString>A. Klippel and S. Winter. 2005. Structural Salience of Landmarks for Route Directions, COSIT 2005, pp. 347-362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kollar</author>
<author>S Tellex</author>
<author>D Roy</author>
<author>N Roy</author>
</authors>
<title>Toward Understanding Natural Language Directions,</title>
<date>2010</date>
<booktitle>Human Robot Interaction Conference (HRI-2010),</booktitle>
<pages>259--266</pages>
<contexts>
<context position="2630" citStr="Kollar et al., 2010" startWordPosition="395" endWordPosition="398">ed uniformly in instructions across presentation formats. For example, people may use objects in the environment as landmarks more often when they do not have a good sense of distance in the environment. Behaviors related to spatial language may change based on the complexity of the route that a robot must take. This could be due to a combination of factors, including ease of use and personal assessment of a robot’s ability to interpret specific distances over landmarks. Several studies have investigated written or typed spatial language (e.g., MacMahon et al., 2006; Koulori and Lauria, 2009; Kollar et al., 2010). In addition, Ross (2008) studied models of spoken language interpretation in schematic views of areas. In the current study we focus on close-range spoken language route instructions. 2 Related Work Interpreting spatial language is an important capability for systems (e.g., mobile robots) that share space with people. Human-human communication of spatial language has been extensively studied. Talmy (1983) proposed that the nature of language places constraints on how people communicate about space with others (i.e., schematization). Spatial descriptions are primarily influenced by how refere</context>
</contexts>
<marker>Kollar, Tellex, Roy, Roy, 2010</marker>
<rawString>T. Kollar, S. Tellex, D. Roy, and N. Roy. 2010. Toward Understanding Natural Language Directions, Human Robot Interaction Conference (HRI-2010), pp. 259-266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koulouri</author>
<author>S Lauria</author>
</authors>
<date>2009</date>
<booktitle>Exploring Miscommunication and Collaborative Behaviour in Human-Robot Interaction, SIGdial</booktitle>
<pages>111--119</pages>
<marker>Koulouri, Lauria, 2009</marker>
<rawString>T. Koulouri and S. Lauria. 2009. Exploring Miscommunication and Collaborative Behaviour in Human-Robot Interaction, SIGdial 2009, pp. 111-119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S C Levinson</author>
</authors>
<title>Frames of reference and Molyneux’s question: cross-linguistic evidence, in</title>
<date>1996</date>
<pages>109--169</pages>
<marker>Levinson, 1996</marker>
<rawString>S. C. Levinson. 1996. Frames of reference and Molyneux’s question: cross-linguistic evidence, in P. Bloom, M. Peterson, L. Nadel, and M. Garrett (Eds.), Language and space, pp. 109-169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lovelace</author>
<author>M Hegarty</author>
<author>D R Montello</author>
</authors>
<title>Elements of good route directions in familiar and unfamiliar environments, in C. Freksa</title>
<date>1999</date>
<publisher>Springer.</publisher>
<location>Berlin:</location>
<contexts>
<context position="1839" citStr="Lovelace et al., 1999" startWordPosition="265" endWordPosition="268">umans would use for this purpose. In the present study we examine this question in the context of formulating route instructions given to robots. For practical purposes, we are also interested in understanding how presentation format affects such language. Instructions given in a physical space might differ from those given in a virtual world, which in turn may differ from those given when only a schematic representation (e.g., a map or drawing) is available. There is general agreement that landmarks play an important role in spatial language (Daniel and Denis, 2004; Klippel and Winter, 2005; Lovelace et al., 1999; MacMahon, 2007; Michon and Denis, 2001; Nothegger et al., 2004; Raubal and Winter, 2002; Weissensteiner and Winter, 2004). However, landmarks might not necessarily be used uniformly in instructions across presentation formats. For example, people may use objects in the environment as landmarks more often when they do not have a good sense of distance in the environment. Behaviors related to spatial language may change based on the complexity of the route that a robot must take. This could be due to a combination of factors, including ease of use and personal assessment of a robot’s ability t</context>
</contexts>
<marker>Lovelace, Hegarty, Montello, 1999</marker>
<rawString>K. Lovelace, M. Hegarty, and D. R. Montello. 1999. Elements of good route directions in familiar and unfamiliar environments, in C. Freksa and D. M. Mark (Eds.), Spatial information theory: Cognitive and computational foundations of geographic information science. Berlin: Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M MacMahon</author>
</authors>
<title>Following Natural Language Route Instructions,</title>
<date>2007</date>
<tech>Ph.D. Thesis,</tech>
<institution>University of Texas at Austin.</institution>
<contexts>
<context position="1855" citStr="MacMahon, 2007" startWordPosition="269" endWordPosition="270">s purpose. In the present study we examine this question in the context of formulating route instructions given to robots. For practical purposes, we are also interested in understanding how presentation format affects such language. Instructions given in a physical space might differ from those given in a virtual world, which in turn may differ from those given when only a schematic representation (e.g., a map or drawing) is available. There is general agreement that landmarks play an important role in spatial language (Daniel and Denis, 2004; Klippel and Winter, 2005; Lovelace et al., 1999; MacMahon, 2007; Michon and Denis, 2001; Nothegger et al., 2004; Raubal and Winter, 2002; Weissensteiner and Winter, 2004). However, landmarks might not necessarily be used uniformly in instructions across presentation formats. For example, people may use objects in the environment as landmarks more often when they do not have a good sense of distance in the environment. Behaviors related to spatial language may change based on the complexity of the route that a robot must take. This could be due to a combination of factors, including ease of use and personal assessment of a robot’s ability to interpret spec</context>
</contexts>
<marker>MacMahon, 2007</marker>
<rawString>M. MacMahon. 2007. Following Natural Language Route Instructions, Ph.D. Thesis, University of Texas at Austin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M MacMahon</author>
<author>B Stankiewicz</author>
<author>B Kuipers</author>
</authors>
<title>Walk the Talk: Connecting Language, Knowledge, and Action in Route Instructions,</title>
<date>2006</date>
<contexts>
<context position="2582" citStr="MacMahon et al., 2006" startWordPosition="387" endWordPosition="390">). However, landmarks might not necessarily be used uniformly in instructions across presentation formats. For example, people may use objects in the environment as landmarks more often when they do not have a good sense of distance in the environment. Behaviors related to spatial language may change based on the complexity of the route that a robot must take. This could be due to a combination of factors, including ease of use and personal assessment of a robot’s ability to interpret specific distances over landmarks. Several studies have investigated written or typed spatial language (e.g., MacMahon et al., 2006; Koulori and Lauria, 2009; Kollar et al., 2010). In addition, Ross (2008) studied models of spoken language interpretation in schematic views of areas. In the current study we focus on close-range spoken language route instructions. 2 Related Work Interpreting spatial language is an important capability for systems (e.g., mobile robots) that share space with people. Human-human communication of spatial language has been extensively studied. Talmy (1983) proposed that the nature of language places constraints on how people communicate about space with others (i.e., schematization). Spatial des</context>
</contexts>
<marker>MacMahon, Stankiewicz, Kuipers, 2006</marker>
<rawString>M. MacMahon, B. Stankiewicz, and B. Kuipers. 2006. Walk the Talk: Connecting Language, Knowledge, and Action in Route Instructions, 21st</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>