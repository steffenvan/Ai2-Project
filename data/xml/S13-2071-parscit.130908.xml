<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.027554">
<title confidence="0.963394">
NTNU: Domain Semi-Independent Short Message Sentiment Classification
</title>
<author confidence="0.98539">
Øyvind Selmer Mikael Brevik Bj¨orn Gamb¨ack Lars Bungum
</author>
<affiliation confidence="0.99941">
Department of Computer and Information Science
Norwegian University of Science and Technology (NTNU)
</affiliation>
<address confidence="0.871965">
Sem Sælands vei 7–9, NO–7491 Trondheim, Norway
</address>
<email confidence="0.998874">
{oyvinsel,mikaelbr}@stud.ntnu.no {gamback,larsbun}@idi.ntnu.no
</email>
<sectionHeader confidence="0.998599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999936">
The paper describes experiments using grid
searches over various combinations of ma-
chine learning algorithms, features and pre-
processing strategies in order to produce the
optimal systems for sentiment classification of
microblog messages. The approach is fairly
domain independent, as demonstrated by the
systems achieving quite competitive results
when applied to short text message data, i.e.,
input they were not originally trained on.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954789473684">
The informal texts in microblogs such as Twitter
and on other social media represent challenges for
traditional language processing systems. The posts
(“tweets”) are limited to 140 characters and often
contain misspellings, slang and abbreviations. On
the other hand, the posts are often opinionated in
nature as a very result of their informal character,
which has led Twitter to being a gold mine for sen-
timent analysis (SA). SA for longer texts, such as
movie reviews, has been explored since the 1990s;1
however, the limited amount of attributes in tweets
makes the feature vectors shorter than in documents
and the task of analysing them closely related to
phrase- and sentence-level SA (Wilson et al., 2005;
Yu and Hatzivassiloglou, 2003). Hence there are
no guarantees that algorithms that perform well on
document-level SA will do as well on tweets. On
the other hand, it is possible to exploit some of the
special features of the web language, e.g., emoticons
</bodyText>
<footnote confidence="0.506118">
1See Pang and Lee (2008); Feldman (2013) for overviews.
</footnote>
<bodyText confidence="0.999771823529412">
and emotionally loaded abbreviations. Thus the data
will normally go through some preprocessing before
any classification is attempted, e.g., by filtering out
Twitter specific symbols and functions, in particular
retweets (reposting another user’s tweet), mentions
(’@’, tags used to mention another user), hashtags
(’#’, used to tag a tweet to a certain topic), emoti-
cons, and URLs (linking to an external resource,
e.g., a news article or a photo). The first system to re-
ally use Twitter as a corpus was created as a student
course project at Stanford (Go et al., 2009). Pak and
Paroubek (2010) experimented with sentiment clas-
sification of tweets using Support Vector Machines
and Conditional Random Fields, benchmarked with
a Naive Bayes Classifier baseline, but were unable
to beat the baseline. Later, and as Twitter has grown
in popularity, many other systems for Twitter Senti-
ment Analysis (TSA) have been developed (see, e.g.,
Maynard and Funk, 2011; Mukherjee et al., 2012;
Saif et al., 2012; Chamlertwat et al., 2012).
Clearly, it is possible to classify the sentiment of
tweets in a single step; however, the approach to
TSA most used so far is a two-step strategy where
the first step is subjectivity classification and the
second step is polarity classification. The goal of
subjectivity classification is to separate subjective
and objective statements. Pak and Paroubek (2010)
counted word frequencies in a subjective vs an ob-
jective set of tweets; the results showed that in-
terjections and personal pronouns are the strongest
indicators of subjectivity. In general, these word
classes, adverbs and (in particular) adjectives (Hatzi-
vassiloglou and Wiebe, 2000) have shown to be
good subjectivity indicators, which has made part-
</bodyText>
<page confidence="0.966833">
430
</page>
<bodyText confidence="0.989004525">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 430–437, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
of-speech (POS) tagging a reasonable technique for
filtering out objective tweets. Early research on
TSA showed that the challenging vocabulary made
it harder to accurately tag tweets; however, Gimpel
et al. (2011) report on using a POS tagger for mark-
ing tweets, performing with almost 90% accuracy.
Polarity classification is the task of separating the
subjective statements into positives and negatives.
Kouloumpis et al. (2011) tried different solutions for
tweet polarity classification, and found that the best
performance came from using n-grams together with
lexicon and microblog features. Interestingly, per-
formance dropped when a POS tagger was included.
They speculate that this can be due to the accuracy
of the POS tagger itself, or that POS tagging just is
less effective for analysing tweet polarity.
In this paper we will explore the application of
a set of machine learning algorithms to the task of
Twitter sentiment classification, comparing one-step
and two-step approaches, and investigate a range of
different preprocessing methods. What we explic-
itly will not do, is to utilise a sentiment lexicon, even
though many methods in TSA rely on lexica with a
sentiment score for each word. Nielsen (2011) man-
ually built a sentiment lexicon specialized for Twit-
ter, while others have tried to induce such lexica
automatically with good results (Velikovich et al.,
2010; Mohammad et al., 2013). However, sentiment
lexica — and in particular specialized Twitter senti-
ment lexica — make the classification more domain
dependent. Here we will instead aim to exploit do-
main independent approaches as far as possible, and
thus abstain from using sentiment lexica. The rest of
the paper is laid out as follows: Section 2 introduces
the twitter data sets used in the study. Then Section 3
describes the system built for carrying out the twitter
sentiment classification experiments, which in turn
are reported and discussed in Sections 4 and 5.
</bodyText>
<sectionHeader confidence="0.986078" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.984448428571429">
Manually collecting information from Twitter would
be a tedious task, but Twitter offers a well doc-
umented Representational State Transfer Applica-
tion Programming Interface (REST API) which al-
lows users to collect a corpus from the micro-
blogosphere. Most of the data used in TSA re-
search is collected through the Twitter API, either by
</bodyText>
<table confidence="0.999825333333333">
Training Dev 1 Dev 2 NTNU
Class Num % Num % Num % Num %
Negative 1288 15 176 21 340 26 86 19
Neutral 4151 48 144 45 739 21 232 50
Positive 3270 37 368 35 575 54 142 31
Total 8709 688 1654 461
</table>
<tableCaption confidence="0.999944">
Table 1: The data sets used in the experiments
</tableCaption>
<bodyText confidence="0.999927055555555">
searching for a certain topic/keyword or by stream-
ing realtime data. Four different data sets were used
in the experiments described below. three were sup-
plied by the organisers of the SemEval’13 shared
task on Twitter sentiment analysis (Wilson et al.,
2013), in the form of a training set, a smaller initial
development set, and a larger development set. All
sets consist of manually annotated tweets on a range
of topics, including different products and events.
Tweet-level classification (Task 2B) was split into
two subtasks in SemEval’13, one allowing training
only on the data sets supplied by the organisers (con-
strained) and one allowing training also on external
data (unconstrained). To this end, a web applica-
tion2 for manual annotation of tweets was built and
used to annotate a small fourth data set (‘NTNU’).
Each of the 461 tweets in the ‘NTNU’ data set was
annotated by one person only.
The distribution of target classes in the data sets is
shown in Table 1. The data was neither preprocessed
nor filtered, and thus contain hashtags, URLs, emoti-
cons, etc. However, all the data sets provided by
SemEval’13 had more than three target classes (e.g.,
‘objective’, ‘objective-OR-neutral’), so tweets that
were not annotated as ‘positive’ or ‘negative’ were
merged into the ‘neutral’ target class.
Due to Twitter’s privacy policy, the given data sets
do not contain the tweet text, but only the tweet ID
which in turn can be used to download the text. The
Twitter API has a limit on the number of downloads
per hour, so SemEval’13 provided a Python script
to scrape texts from https://twitter.com. This
script was slow and did not download the texts for all
tweet IDs in the data sets, so a faster and more pre-
cise download script3 for node.js was implemented
and submitted to the shared task organisers.
</bodyText>
<footnote confidence="0.9999465">
2http://tinyurl.com/tweetannotator
3http://tinyurl.com/twitscraper
</footnote>
<page confidence="0.998483">
431
</page>
<sectionHeader confidence="0.998809" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.9999858">
In order to run sentiment classification experiments,
a general system was built. It has a Sentiment Anal-
ysis API Layer which works as a thin extension of
the Twitter API, sending all tweets received in par-
allel to a Sentiment Analysis Classifier server. After
classification, the SA API returns the same JSON
structure as the Twitter API sends out, only with an
additional attribute denoting the tweet’s sentiment.
The Sentiment Analysis Classifier system consists
of preprocessing and classification, described below.
</bodyText>
<subsectionHeader confidence="0.99915">
3.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999847588235294">
As mentioned in the introduction, most approaches
to Twitter sentiment analysis start with a pre-
processing step, filtering out some Twitter specific
symbols and functions. Go et al. (2009) used ‘:)’
and ‘:(’ as labels for the polarity, so did not remove
these emoticons, but replaced URLs and user names
with placeholders. Kouloumpis et al. (2011) used
both an emoticon set and a hashtagged set. The lat-
ter is a subset of the Edinburgh Twitter corpus which
consists of 97 million tweets (Petrovi´c et al., 2010).
Some approaches have also experimented with nor-
malizing the tweets, and removing redundant letters,
e.g., “loooove” and “crazyyy”, that are used to ex-
press a stronger sentiment in tweets. Redundant let-
ters are therefore often not deleted, but words rather
trimmed down to one additional redundant letter, so
that the stronger sentiment can be taken into consid-
eration by a score/weight adjustment for that feature.
To find the best features to use, a set of eight dif-
ferent combinations of preprocessing methods was
designed, as detailed in Table 2. These include no
preprocessing (P0, not shown in the table), where
all characters are included as features; full remove
(P4), where all special Twitter features like user
names, URLs, hashtags, retweet (RT) tags, and
emoticons are stripped; and replacing Twitter fea-
tures with placeholder texts to reduce vocabulary.
The “hashtag as word” method transforms a hashtag
to a regular word and uses the hashtag as a feature.
“Reduce letter duplicate” removes redundant char-
acters more than three (“happyyyyyyyy! !!!!!” —*
“happyyy!!!”). Some methods, like P1, P2, P4, P5
and P7 remove user names from the text, as they
most likely are just noise for the sentiment. Still,
</bodyText>
<table confidence="0.998709333333333">
Method P1 P2 P3 P4 P5 P6 P7
Remove Usernames X X X X X
Username placeholder X
Remove URLs X X X X
URL placeholder X
Remove hashtags X X
Hashtag as word X
Hashtag placeholder X
Remove RT-tags X X X
Remove emoticons X X
Reduce letter duplicate X X X X
Negation attachment X X X
</table>
<tableCaption confidence="0.99944">
Table 2: Overview of the preprocessing methods
</tableCaption>
<bodyText confidence="0.999949214285714">
the fact that there are references to URLs and user
names might be relevant for the sentiment. To make
these features more informative for the machine
learning algorithms, a preprocessing method (P3)
was implemented for replacing them with place-
holders. In addition, a very rudimentary treatment
of negation was added, in which the negation is at-
tached to the preceding and following words, so that
they will also reflect the change in sentence polarity.
Even though this preprocessing obviously is
Twitter-specific, the results after it will still be do-
main semi-independent, in as far as the strings pro-
duced after the removal of URLs, user names, etc.,
will be general, and can be used for system training.
</bodyText>
<subsectionHeader confidence="0.992354">
3.2 Classification
</subsectionHeader>
<bodyText confidence="0.999835461538462">
The classification step currently supports three
machine learning algorithms from the Python
scikit-learn4 package: Naive Bayes (NB),
Maximum Entropy (MaxEnt), and Support Vector
Machines (SVM). These are all among the super-
vised learners that previously have been shown to
perform well on TSA, e.g., by Bermingham and
Smeaton (2010) who compared SVM and NB for
microblogs. Interestingly, while the SVM technique
normally beats NB and MaxEnt on longer texts, that
comparison indicated that it has some trouble with
outperforming NB when feature vectors are shorter.
Three different models were implemented:
</bodyText>
<listItem confidence="0.999506">
1. One-step model: a single algorithm classifies
tweets as negative, neutral or positive.
2. Two-step model: the tweets are first classified
as either subjective or neutral. Those that are
</listItem>
<footnote confidence="0.993985">
4http://scikit-learn.org
</footnote>
<page confidence="0.990481">
432
</page>
<figureCaption confidence="0.999905">
Figure 1: Performance across all models (red=precision, blue=recall, green=F1-score, brown=accuracy)
</figureCaption>
<bodyText confidence="0.996536333333333">
classified as subjective are then sent to polarity
classification (i.e., negative or positive).
3. Boosting (Freund and Schapire, 1997): a way
to combine classifiers by generating a set of
sub-models, each of which predicts a sentiment
on its own and then sends it to a voting process
that selects the sentiment with highest score.
In all cases, the final classification is returned to the
API Layer sentiment provider.
</bodyText>
<sectionHeader confidence="0.998313" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.9999626">
Experiments were carried out using the platform in-
troduced in the previous section, with models built
on the training set of Table 1. The testing system
generates and trains different models based on a set
of parameters, such as classification algorithm, pre-
processing methods, whether or not to use inverse
document frequency (IDF) or stop words. A grid
search option can be activated, so that a model is
generated with the best possible parameter set for
the given algorithm, using 10-fold cross validation.
</bodyText>
<subsectionHeader confidence="0.999957">
4.1 Selection of Learners and Features
</subsectionHeader>
<bodyText confidence="0.99986165">
An extensive grid search was conducted. This search
cycled through different algorithms, parameters and
preprocessing techniques. The following param-
eters were included in the search. Three binary
(Yes/No) parameters: Use IDF, Use Smooth
IDF, and Use Sublinear IDF, together with
ngram (unigram/bigram/trigram). SVM
and MaxEnt models in addition included C and
NB models alpha parameters, all with the value
ranges [0.1/0.3/0.5/0.7/0.8/1.0]. SVM
and MaxEnt models also had penalty (L1/L2).
Figure 1 displays the precision, recall, F1-score,
and accuracy for each of the thirteen classifiers with
the Dev 2 data set (see Table 1) used for evaluation.
Note that most classifiers involving the NB algo-
rithm perform badly, both in terms of accuracy and
F-score. This was observed for the other data sets as
well. Further, we can see that one-step classifiers did
better than two-step models, with MaxEnt obtaining
the best accuracy, but SVM a slightly better F-score.
</bodyText>
<page confidence="0.998752">
433
</page>
<table confidence="0.999712666666667">
Data set Dev 2 Dev 1
Learner SVM MaxEnt SVM MaxEnt
Precision 0.627 0.647 0.700 0.561
Recall 0.592 0.578 0.726 0.589
F1-score 0.598 0.583 0.707 0.556
Accuracy 0.638 0.645 0.728 0.581
</table>
<tableCaption confidence="0.959174">
Table 3: Best classifier performance (bold=best score;
all classifiers were trained on the training set of Table 1)
</tableCaption>
<bodyText confidence="0.999928864864865">
A second grid search with the two best classifiers
from the first search was performed instead using the
smaller Dev 1 data set for evaluation. The results
for both the SVM and MaxEnt classifiers are shown
in Table 3. With the Dev 1 data set, SVM performs
much better than MaxEnt. The larger Dev 2 develop-
ment set contains more neutral tweets than the Dev 1
set, which gives us reasons to believe that evaluating
on the Dev 2 set favours the MaxEnt classifier.
A detailed error analysis was conducted by in-
specting the confusion matrices of all classifiers. In
general, classifiers involving SVM tend to give bet-
ter confusion matrices than the others. Using SVM
only in a one-step model works well for positive and
neutral tweets, but a bit poorer for negative. Two-
step models with SVM-based subjectivity classifica-
tion exhibit the same basic behaviour. The one-step
MaxEnt model classifies more tweets as neutral than
the other classifiers. Using MaxEnt for subjectivity
classification and either MaxEnt or SVM for polarity
classification performs well, but is too heavy on the
positive class. Boosting does not improve and be-
haves in a fashion similar to two-step MaxEnt mod-
els. All combinations involving NB tend to heavily
favour positive predictions; only the two-step mod-
els involving another algorithm for polarity classifi-
cation gave some improvement for negative tweets.
The confusion matrices of the two best learners
are shown in Figures 2a-2d, where a learner is shown
to perform better if it has redish colours on the main
diagonal and blueish in the other fields, as is the case
for SVM on the Dev 1 data set (Figure 2c).
As a part of the grid search, all preprocessing
methods were tested for each classifier. Figure 3
shows that P2 (removing user names, URLs, hash-
tags prefixes, retweet tokens, and redundant letters)
is the preprocessing method which performs best
</bodyText>
<figure confidence="0.684732">
(c) SVM Dev 1 (d) MaxEnt Dev 1
</figure>
<figureCaption confidence="0.999696666666667">
Figure 2: SVM and MaxEnt confusion matrices (out-
put is shown from left-to-right: negative-neutral-positive;
the correct classes are in the same order, top-to-bottom.
“Hotter” colours (red) indicate that more instances were
assigned; “colder” colours (blue) mean fewer instances.)
Figure 3: Statistics of preprocessing usage
</figureCaption>
<bodyText confidence="0.999244857142857">
(gives the best accuracy) and thus used most of-
ten (10 times). Figure 3 also indicates that URLs
are noisy and do not contain much sentiment, while
hashtags and emoticons tend to be more valuable
features (P2 and P7 — removing URLs — perform
best, while P4 and P5 — removing hashtags and
emoticons in addition to URLs — perform badly).
</bodyText>
<figure confidence="0.997764727272727">
(a) SVM Dev 2 (b) MaxEnt Dev 2
10
4
3 3
1
10
8
6
4
2
0
</figure>
<page confidence="0.868398">
434
</page>
<table confidence="0.7464199">
SMS
Data set
Twitter
5 Discussion and Conclusion
System
NTNUC NTNUU NTNUC NTNUU
Precision 0.652 0.633 0.659 0.623
Recall 0.579 0.564 0.646 0.623
F1-score 0.590 0.572 0.652 0.623
F1 + /− 0.532 0.507 0.580 0.546
</table>
<tableCaption confidence="0.999758">
Table 4: The NTNU systems in SemEval’13
</tableCaption>
<subsectionHeader confidence="0.995849">
4.2 SemEval’13 NTNU Systems and Results
</subsectionHeader>
<bodyText confidence="0.999985480519481">
Based on the information from the grid search, two
systems were built for SemEval’13. Since one-step
SVM-based classification showed the best perfor-
mance on the training data, it was chosen for the
system participating in the constrained subtask, NT-
NUC. The preprocessing also was the one with the
best performance on the provided data, P2 which
involves lower-casing all letters; reducing letter du-
plicates; using hashtags as words (removing #); and
removing all URLs, user names and RT-tags.
Given the small size of the in-house (‘NTNU’)
data set, no major improvement was expected from
adding it in the unconstrained task. Instead, a rad-
ically different set-up was chosen to create a new
system, and train it on both the in-house and pro-
vided data. NTNUU utilizes a two-step approach,
with SVM for subjectivity and MaxEnt for polarity
classification, a combination intended to capture the
strengths of both algorithms. No preprocessing was
used for the subjectivity step, but user names were
removed before attempting polarity classification.
As further described by Wilson et al. (2013), the
SemEval’13 shared task involved testing on a set of
3813 tweets (1572 positive, 601 negative, and 1640
neutral). In order to evaluate classification perfor-
mance on data of roughly the same length and type,
but from a different domain, the evaluation data also
included 2094 Short Message Service texts (SMS;
492 positive, 394 negative, and 1208 neutral).
Table 4 shows the results obtained by the NTNU
systems on the SemEval’13 evaluation data, in terms
of average precision, recall and F-score for all three
classes, as well as average F-score for positive and
negative tweets only (F1+/−; i.e., the measure used
to rank the systems participating in the shared task).
As can be seen in Table 4, the extra data available
to train the NTNUU system did not really help it:
it gets outperformed by NTNUC on all measures.
Notably, both systems perform well on the out-
of-domain data represented by the SMS messages,
which is encouraging and indicates that the approach
taken really is domain semi-independent. This was
also reflected in the rankings of the two systems in
the shared task: both were on the lower half among
the participating systems on Twitter data (24th/36
resp. 10th/15), but near the top on SMS data, with
NTNUC being ranked 5th of 28 constrained systems
and NTNUU 6th of 15 unconstrained systems.
Comparing the results to those shown in Table 3
and Figure 1, NTNUC’s (SVM) performance is in
line with that on Dev 2, but substantially worse
than on Dev 1; NTNUU (SVM—*MaxEnt) performs
slightly worse too. Looking at system output with
and without the ‘NTNU’ data, both one-step SVM
and MaxEnt models and SVM—*MaxEnt classified
more tweets as negative when trained on the ex-
tra data; however, while NTNUC benefited slightly
from this, NTNUU even performed better without it.
An obvious extension to the present work would
be to try other classification algorithms (e.g., Condi-
tional Random Fields or more elaborate ensembles)
or other features (e.g., character n-grams). Rather
than the very simple treatment of negation used
here, an approach to automatic induction of scope
through a negation detector (Councill et al., 2010)
could be used. It would also be possible to relax
the domain-independence further, in particular to
utilize sentiment lexica (including twitter specific),
e.g., by automatic phrase-polarity lexicon extraction
(Velikovich et al., 2010). Since many users tweet
from their smartphones, and a large number of them
use iPhones, several tweets contain iPhone-specific
smilies (“Emoji”). Emoji are implemented as their
own character set (rather than consisting of charac-
ters such as ‘:)’ and ‘:(’, etc.), so a potentially major
improvement could be to convert them to character-
based smilies or to emotion-specific placeholders.
</bodyText>
<sectionHeader confidence="0.997779" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9847855">
Thanks to Amitava Das for initial discussions and to
the human annotators of the ‘NTNU’ data set.
</bodyText>
<page confidence="0.999026">
435
</page>
<sectionHeader confidence="0.995747" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.920819188888889">
Bermingham, A. and Smeaton, A. F. (2010). Clas-
sifying sentiment in microblogs: Is brevity an
advantage? In Proceedings of the 19th Inter-
national Conference on Information and Knowl-
edge Management, pages 1833–1836, Toronto,
Canada. ACM.
Chamlertwat, W., Bhattarakosol, P., Rungkasiri, T.,
and Haruechaiyasak, C. (2012). Discovering con-
sumer insight from Twitter via sentiment anal-
ysis. Journal of Universal Computer Science,
18(8):973–992.
Councill, I. G., McDonald, R., and Velikovich, L.
(2010). What’s great and what’s not: learning
to classify the scope of negation for improved
sentiment analysis. In Proceedings of the 48th
Annual Meeting of the Association for Compu-
tational Linguistics, pages 51–59, Uppsala, Swe-
den. ACL. Workshop on Negation and Specula-
tion in Natural Language Processing.
Feldman, R. (2013). Techniques and applications for
sentiment analysis. Communications of the ACM,
56(4):82–89.
Freund, Y. and Schapire, R. E. (1997). A decision-
theoretic generalization of on-line learning and
application to boosting. Journal of Computer and
System Sciences, 55(1):119–139.
Gimpel, K., Schneider, N., O’Connor, B., Das, D.,
Mills, D., Eisenstein, J., Heilman, M., Yogatama,
D., Flanigan, J., and Smith, N. A. (2011). Part-
of-speech tagging for Twitter: Annotation, fea-
tures, and experiments. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technolo-
gies, volume 2: short papers, pages 42–47, Port-
land, Oregon. ACL.
Go, A., Huang, L., and Bhayani, R. (2009). Twit-
ter sentiment analysis. Technical Report CS224N
Project Report, Department of Computer Science,
Stanford University, Stanford, California.
Hatzivassiloglou, V. and Wiebe, J. M. (2000). Ef-
fects of adjective orientation and gradability on
sentence subjectivity. In Proceedings of the 18th
International Conference on Computational Lin-
guistics, pages 299–305, Saarbr¨ucken, Germany.
ACL.
HLT10 (2010). Proceedings of the 2010 Human
Language Technology Conference of the North
American Chapter of the Association for Com-
putational Linguistics, Los Angeles, California.
ACL.
Kouloumpis, E., Wilson, T., and Moore, J. (2011).
Twitter sentiment analysis: The good the bad and
the OMG! In Proceedings of the 5th International
Conference on Weblogs and Social Media, pages
538–541, Barcelona, Spain. AAAI.
Maynard, D. and Funk, A. (2011). Automatic detec-
tion of political opinions in tweets. In #MSM2011
(2011), pages 81–92.
Mohammad, S., Kiritchenko, S., and Zhu, X.
(2013). NRC-Canada: Building the state-of-the-
art in sentiment analysis of tweets. In SemEval’13
(2013).
#MSM2011 (2011). Proceedings of the 1st
Workshop on Making Sense of Microposts
(#MSM2011), Heraklion, Greece.
Mukherjee, S., Malu, A., Balamurali, A., and Bhat-
tacharyya, P. (2012). TwiSent: A multistage sys-
tem for analyzing sentiment in Twitter. In Pro-
ceedings of the 21st International Conference on
Information and Knowledge Management, pages
2531–2534, Maui, Hawaii. ACM.
Nielsen, F. ˚A. (2011). A new ANEW: Evaluation of
a word list for sentiment analysis in microblogs.
In #MSM2011 (2011), pages 93–98.
Pak, A. and Paroubek, P. (2010). Twitter as a cor-
pus for sentiment analysis and opinion mining. In
Proceedings of the 7th International Conference
on Language Resources and Evaluation, Valetta,
Malta. ELRA.
Pang, B. and Lee, L. (2008). Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2):1–135.
Petrovi´c, S., Osborne, M., and Lavrenko, V. (2010).
The Edinburgh Twitter corpus. In HLT10 (2010),
pages 25–26. Workshop on Computational Lin-
guistics in a World of Social Media.
Saif, H., He, Y., and Alani, H. (2012). Semantic
sentiment analysis of Twitter. In Proceedings of
the 11th International Semantic Web Conference,
pages 508–524, Boston, Massachusetts. Springer.
</reference>
<page confidence="0.990151">
436
</page>
<reference confidence="0.99958608">
SemEval’13 (2013). Proceedings of the Interna-
tional Workshop on Semantic Evaluation, Sem-
Eval ’13, Atlanta, Georgia. ACL.
Velikovich, L., Blair-Goldensohn, S., Hannan, K.,
and McDonald, R. (2010). The viability of web-
derived polarity lexicons. In HLT10 (2010), pages
777–785.
Wilson, T., Kozareva, Z., Nakov, P., Ritter, A.,
Rosenthal, S., and Stoyanov, V. (2013). SemEval-
2013 Task 2: Sentiment analysis in Twitter. In
SemEval’13 (2013).
Wilson, T., Wiebe, J., and Hoffmann, P. (2005). Rec-
ognizing contextual polarity in phrase-level senti-
ment analysis. In Proceedings of the 2005 Hu-
man Language Technology Conference and Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 347–354, Vancouver,
British Columbia, Canada. ACL.
Yu, H. and Hatzivassiloglou, V. (2003). Towards an-
swering opinion questions: Separating facts from
opinions and identifying the polarity of opinion
sentences. In Collins, M. and Steedman, M., edi-
tors, Proceedings of the 2003 Conference on Em-
pirical Methods in Natural Language Processing,
pages 129–136, Sapporo, Japan. ACL.
</reference>
<page confidence="0.998634">
437
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.836308">
<title confidence="0.999759">NTNU: Domain Semi-Independent Short Message Sentiment Classification</title>
<author confidence="0.994881">Øyvind Selmer Mikael Brevik Bj¨orn Gamb¨ack Lars Bungum</author>
<affiliation confidence="0.997896">Department of Computer and Information Norwegian University of Science and Technology</affiliation>
<address confidence="0.859308">Sem Sælands vei 7–9, NO–7491 Trondheim,</address>
<abstract confidence="0.998085363636364">The paper describes experiments using grid searches over various combinations of machine learning algorithms, features and preprocessing strategies in order to produce the optimal systems for sentiment classification of microblog messages. The approach is fairly domain independent, as demonstrated by the systems achieving quite competitive results when applied to short text message data, i.e., input they were not originally trained on.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Bermingham</author>
<author>A F Smeaton</author>
</authors>
<title>Classifying sentiment in microblogs: Is brevity an advantage?</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th International Conference on Information and Knowledge Management,</booktitle>
<pages>1833--1836</pages>
<publisher>ACM.</publisher>
<location>Toronto, Canada.</location>
<contexts>
<context position="11923" citStr="Bermingham and Smeaton (2010)" startWordPosition="1905" endWordPosition="1908">in sentence polarity. Even though this preprocessing obviously is Twitter-specific, the results after it will still be domain semi-independent, in as far as the strings produced after the removal of URLs, user names, etc., will be general, and can be used for system training. 3.2 Classification The classification step currently supports three machine learning algorithms from the Python scikit-learn4 package: Naive Bayes (NB), Maximum Entropy (MaxEnt), and Support Vector Machines (SVM). These are all among the supervised learners that previously have been shown to perform well on TSA, e.g., by Bermingham and Smeaton (2010) who compared SVM and NB for microblogs. Interestingly, while the SVM technique normally beats NB and MaxEnt on longer texts, that comparison indicated that it has some trouble with outperforming NB when feature vectors are shorter. Three different models were implemented: 1. One-step model: a single algorithm classifies tweets as negative, neutral or positive. 2. Two-step model: the tweets are first classified as either subjective or neutral. Those that are 4http://scikit-learn.org 432 Figure 1: Performance across all models (red=precision, blue=recall, green=F1-score, brown=accuracy) classif</context>
</contexts>
<marker>Bermingham, Smeaton, 2010</marker>
<rawString>Bermingham, A. and Smeaton, A. F. (2010). Classifying sentiment in microblogs: Is brevity an advantage? In Proceedings of the 19th International Conference on Information and Knowledge Management, pages 1833–1836, Toronto, Canada. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Chamlertwat</author>
<author>P Bhattarakosol</author>
<author>T Rungkasiri</author>
<author>C Haruechaiyasak</author>
</authors>
<title>Discovering consumer insight from Twitter via sentiment analysis.</title>
<date>2012</date>
<journal>Journal of Universal Computer Science,</journal>
<volume>18</volume>
<issue>8</issue>
<contexts>
<context position="2853" citStr="Chamlertwat et al., 2012" startWordPosition="430" endWordPosition="433">nal resource, e.g., a news article or a photo). The first system to really use Twitter as a corpus was created as a student course project at Stanford (Go et al., 2009). Pak and Paroubek (2010) experimented with sentiment classification of tweets using Support Vector Machines and Conditional Random Fields, benchmarked with a Naive Bayes Classifier baseline, but were unable to beat the baseline. Later, and as Twitter has grown in popularity, many other systems for Twitter Sentiment Analysis (TSA) have been developed (see, e.g., Maynard and Funk, 2011; Mukherjee et al., 2012; Saif et al., 2012; Chamlertwat et al., 2012). Clearly, it is possible to classify the sentiment of tweets in a single step; however, the approach to TSA most used so far is a two-step strategy where the first step is subjectivity classification and the second step is polarity classification. The goal of subjectivity classification is to separate subjective and objective statements. Pak and Paroubek (2010) counted word frequencies in a subjective vs an objective set of tweets; the results showed that interjections and personal pronouns are the strongest indicators of subjectivity. In general, these word classes, adverbs and (in particula</context>
</contexts>
<marker>Chamlertwat, Bhattarakosol, Rungkasiri, Haruechaiyasak, 2012</marker>
<rawString>Chamlertwat, W., Bhattarakosol, P., Rungkasiri, T., and Haruechaiyasak, C. (2012). Discovering consumer insight from Twitter via sentiment analysis. Journal of Universal Computer Science, 18(8):973–992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I G Councill</author>
<author>R McDonald</author>
<author>L Velikovich</author>
</authors>
<title>What’s great and what’s not: learning to classify the scope of negation for improved sentiment analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>51--59</pages>
<location>Uppsala,</location>
<contexts>
<context position="20932" citStr="Councill et al., 2010" startWordPosition="3369" endWordPosition="3372"> Looking at system output with and without the ‘NTNU’ data, both one-step SVM and MaxEnt models and SVM—*MaxEnt classified more tweets as negative when trained on the extra data; however, while NTNUC benefited slightly from this, NTNUU even performed better without it. An obvious extension to the present work would be to try other classification algorithms (e.g., Conditional Random Fields or more elaborate ensembles) or other features (e.g., character n-grams). Rather than the very simple treatment of negation used here, an approach to automatic induction of scope through a negation detector (Councill et al., 2010) could be used. It would also be possible to relax the domain-independence further, in particular to utilize sentiment lexica (including twitter specific), e.g., by automatic phrase-polarity lexicon extraction (Velikovich et al., 2010). Since many users tweet from their smartphones, and a large number of them use iPhones, several tweets contain iPhone-specific smilies (“Emoji”). Emoji are implemented as their own character set (rather than consisting of characters such as ‘:)’ and ‘:(’, etc.), so a potentially major improvement could be to convert them to characterbased smilies or to emotion-s</context>
</contexts>
<marker>Councill, McDonald, Velikovich, 2010</marker>
<rawString>Councill, I. G., McDonald, R., and Velikovich, L. (2010). What’s great and what’s not: learning to classify the scope of negation for improved sentiment analysis. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 51–59, Uppsala, Sweden. ACL. Workshop on Negation and Speculation in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Feldman</author>
</authors>
<title>Techniques and applications for sentiment analysis.</title>
<date>2013</date>
<journal>Communications of the ACM,</journal>
<volume>56</volume>
<issue>4</issue>
<contexts>
<context position="1809" citStr="Feldman (2013)" startWordPosition="266" endWordPosition="267">ine for sentiment analysis (SA). SA for longer texts, such as movie reviews, has been explored since the 1990s;1 however, the limited amount of attributes in tweets makes the feature vectors shorter than in documents and the task of analysing them closely related to phrase- and sentence-level SA (Wilson et al., 2005; Yu and Hatzivassiloglou, 2003). Hence there are no guarantees that algorithms that perform well on document-level SA will do as well on tweets. On the other hand, it is possible to exploit some of the special features of the web language, e.g., emoticons 1See Pang and Lee (2008); Feldman (2013) for overviews. and emotionally loaded abbreviations. Thus the data will normally go through some preprocessing before any classification is attempted, e.g., by filtering out Twitter specific symbols and functions, in particular retweets (reposting another user’s tweet), mentions (’@’, tags used to mention another user), hashtags (’#’, used to tag a tweet to a certain topic), emoticons, and URLs (linking to an external resource, e.g., a news article or a photo). The first system to really use Twitter as a corpus was created as a student course project at Stanford (Go et al., 2009). Pak and Par</context>
</contexts>
<marker>Feldman, 2013</marker>
<rawString>Feldman, R. (2013). Techniques and applications for sentiment analysis. Communications of the ACM, 56(4):82–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>A decisiontheoretic generalization of on-line learning and application to boosting.</title>
<date>1997</date>
<journal>Journal of Computer and System Sciences,</journal>
<volume>55</volume>
<issue>1</issue>
<contexts>
<context position="12651" citStr="Freund and Schapire, 1997" startWordPosition="2006" endWordPosition="2009">MaxEnt on longer texts, that comparison indicated that it has some trouble with outperforming NB when feature vectors are shorter. Three different models were implemented: 1. One-step model: a single algorithm classifies tweets as negative, neutral or positive. 2. Two-step model: the tweets are first classified as either subjective or neutral. Those that are 4http://scikit-learn.org 432 Figure 1: Performance across all models (red=precision, blue=recall, green=F1-score, brown=accuracy) classified as subjective are then sent to polarity classification (i.e., negative or positive). 3. Boosting (Freund and Schapire, 1997): a way to combine classifiers by generating a set of sub-models, each of which predicts a sentiment on its own and then sends it to a voting process that selects the sentiment with highest score. In all cases, the final classification is returned to the API Layer sentiment provider. 4 Experimental Results Experiments were carried out using the platform introduced in the previous section, with models built on the training set of Table 1. The testing system generates and trains different models based on a set of parameters, such as classification algorithm, preprocessing methods, whether or not</context>
</contexts>
<marker>Freund, Schapire, 1997</marker>
<rawString>Freund, Y. and Schapire, R. E. (1997). A decisiontheoretic generalization of on-line learning and application to boosting. Journal of Computer and System Sciences, 55(1):119–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Gimpel</author>
<author>N Schneider</author>
<author>B O’Connor</author>
<author>D Das</author>
<author>D Mills</author>
<author>J Eisenstein</author>
<author>M Heilman</author>
<author>D Yogatama</author>
<author>J Flanigan</author>
<author>N A Smith</author>
</authors>
<title>Partof-speech tagging for Twitter: Annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<volume>volume</volume>
<pages>42--47</pages>
<publisher>ACL.</publisher>
<location>Portland, Oregon.</location>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Gimpel, K., Schneider, N., O’Connor, B., Das, D., Mills, D., Eisenstein, J., Heilman, M., Yogatama, D., Flanigan, J., and Smith, N. A. (2011). Partof-speech tagging for Twitter: Annotation, features, and experiments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, volume 2: short papers, pages 42–47, Portland, Oregon. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Go</author>
<author>L Huang</author>
<author>R Bhayani</author>
</authors>
<title>Twitter sentiment analysis.</title>
<date>2009</date>
<tech>Technical Report CS224N Project Report,</tech>
<institution>Department of Computer Science, Stanford University,</institution>
<location>Stanford, California.</location>
<contexts>
<context position="2396" citStr="Go et al., 2009" startWordPosition="359" endWordPosition="362">nd Lee (2008); Feldman (2013) for overviews. and emotionally loaded abbreviations. Thus the data will normally go through some preprocessing before any classification is attempted, e.g., by filtering out Twitter specific symbols and functions, in particular retweets (reposting another user’s tweet), mentions (’@’, tags used to mention another user), hashtags (’#’, used to tag a tweet to a certain topic), emoticons, and URLs (linking to an external resource, e.g., a news article or a photo). The first system to really use Twitter as a corpus was created as a student course project at Stanford (Go et al., 2009). Pak and Paroubek (2010) experimented with sentiment classification of tweets using Support Vector Machines and Conditional Random Fields, benchmarked with a Naive Bayes Classifier baseline, but were unable to beat the baseline. Later, and as Twitter has grown in popularity, many other systems for Twitter Sentiment Analysis (TSA) have been developed (see, e.g., Maynard and Funk, 2011; Mukherjee et al., 2012; Saif et al., 2012; Chamlertwat et al., 2012). Clearly, it is possible to classify the sentiment of tweets in a single step; however, the approach to TSA most used so far is a two-step str</context>
<context position="8996" citStr="Go et al. (2009)" startWordPosition="1418" endWordPosition="1421">API Layer which works as a thin extension of the Twitter API, sending all tweets received in parallel to a Sentiment Analysis Classifier server. After classification, the SA API returns the same JSON structure as the Twitter API sends out, only with an additional attribute denoting the tweet’s sentiment. The Sentiment Analysis Classifier system consists of preprocessing and classification, described below. 3.1 Preprocessing As mentioned in the introduction, most approaches to Twitter sentiment analysis start with a preprocessing step, filtering out some Twitter specific symbols and functions. Go et al. (2009) used ‘:)’ and ‘:(’ as labels for the polarity, so did not remove these emoticons, but replaced URLs and user names with placeholders. Kouloumpis et al. (2011) used both an emoticon set and a hashtagged set. The latter is a subset of the Edinburgh Twitter corpus which consists of 97 million tweets (Petrovi´c et al., 2010). Some approaches have also experimented with normalizing the tweets, and removing redundant letters, e.g., “loooove” and “crazyyy”, that are used to express a stronger sentiment in tweets. Redundant letters are therefore often not deleted, but words rather trimmed down to one</context>
</contexts>
<marker>Go, Huang, Bhayani, 2009</marker>
<rawString>Go, A., Huang, L., and Bhayani, R. (2009). Twitter sentiment analysis. Technical Report CS224N Project Report, Department of Computer Science, Stanford University, Stanford, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Hatzivassiloglou</author>
<author>J M Wiebe</author>
</authors>
<title>Effects of adjective orientation and gradability on sentence subjectivity.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>299--305</pages>
<publisher>ACL.</publisher>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="3501" citStr="Hatzivassiloglou and Wiebe, 2000" startWordPosition="529" endWordPosition="533">s possible to classify the sentiment of tweets in a single step; however, the approach to TSA most used so far is a two-step strategy where the first step is subjectivity classification and the second step is polarity classification. The goal of subjectivity classification is to separate subjective and objective statements. Pak and Paroubek (2010) counted word frequencies in a subjective vs an objective set of tweets; the results showed that interjections and personal pronouns are the strongest indicators of subjectivity. In general, these word classes, adverbs and (in particular) adjectives (Hatzivassiloglou and Wiebe, 2000) have shown to be good subjectivity indicators, which has made part430 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 430–437, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics of-speech (POS) tagging a reasonable technique for filtering out objective tweets. Early research on TSA showed that the challenging vocabulary made it harder to accurately tag tweets; however, Gimpel et al. (2011) report on using a POS tagger for marking tweets, performing wit</context>
</contexts>
<marker>Hatzivassiloglou, Wiebe, 2000</marker>
<rawString>Hatzivassiloglou, V. and Wiebe, J. M. (2000). Effects of adjective orientation and gradability on sentence subjectivity. In Proceedings of the 18th International Conference on Computational Linguistics, pages 299–305, Saarbr¨ucken, Germany. ACL.</rawString>
</citation>
<citation valid="true">
<date>2010</date>
<booktitle>Proceedings of the 2010 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<publisher>ACL.</publisher>
<location>Los Angeles, California.</location>
<contexts>
<context position="2421" citStr="(2010)" startWordPosition="366" endWordPosition="366">verviews. and emotionally loaded abbreviations. Thus the data will normally go through some preprocessing before any classification is attempted, e.g., by filtering out Twitter specific symbols and functions, in particular retweets (reposting another user’s tweet), mentions (’@’, tags used to mention another user), hashtags (’#’, used to tag a tweet to a certain topic), emoticons, and URLs (linking to an external resource, e.g., a news article or a photo). The first system to really use Twitter as a corpus was created as a student course project at Stanford (Go et al., 2009). Pak and Paroubek (2010) experimented with sentiment classification of tweets using Support Vector Machines and Conditional Random Fields, benchmarked with a Naive Bayes Classifier baseline, but were unable to beat the baseline. Later, and as Twitter has grown in popularity, many other systems for Twitter Sentiment Analysis (TSA) have been developed (see, e.g., Maynard and Funk, 2011; Mukherjee et al., 2012; Saif et al., 2012; Chamlertwat et al., 2012). Clearly, it is possible to classify the sentiment of tweets in a single step; however, the approach to TSA most used so far is a two-step strategy where the first ste</context>
<context position="11923" citStr="(2010)" startWordPosition="1908" endWordPosition="1908">ven though this preprocessing obviously is Twitter-specific, the results after it will still be domain semi-independent, in as far as the strings produced after the removal of URLs, user names, etc., will be general, and can be used for system training. 3.2 Classification The classification step currently supports three machine learning algorithms from the Python scikit-learn4 package: Naive Bayes (NB), Maximum Entropy (MaxEnt), and Support Vector Machines (SVM). These are all among the supervised learners that previously have been shown to perform well on TSA, e.g., by Bermingham and Smeaton (2010) who compared SVM and NB for microblogs. Interestingly, while the SVM technique normally beats NB and MaxEnt on longer texts, that comparison indicated that it has some trouble with outperforming NB when feature vectors are shorter. Three different models were implemented: 1. One-step model: a single algorithm classifies tweets as negative, neutral or positive. 2. Two-step model: the tweets are first classified as either subjective or neutral. Those that are 4http://scikit-learn.org 432 Figure 1: Performance across all models (red=precision, blue=recall, green=F1-score, brown=accuracy) classif</context>
</contexts>
<marker>2010</marker>
<rawString>HLT10 (2010). Proceedings of the 2010 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, Los Angeles, California. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Kouloumpis</author>
<author>T Wilson</author>
<author>J Moore</author>
</authors>
<title>Twitter sentiment analysis: The good the bad and the OMG!</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Conference on Weblogs and Social Media,</booktitle>
<pages>538--541</pages>
<publisher>AAAI.</publisher>
<location>Barcelona,</location>
<contexts>
<context position="4254" citStr="Kouloumpis et al. (2011)" startWordPosition="638" endWordPosition="641">tics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 430–437, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics of-speech (POS) tagging a reasonable technique for filtering out objective tweets. Early research on TSA showed that the challenging vocabulary made it harder to accurately tag tweets; however, Gimpel et al. (2011) report on using a POS tagger for marking tweets, performing with almost 90% accuracy. Polarity classification is the task of separating the subjective statements into positives and negatives. Kouloumpis et al. (2011) tried different solutions for tweet polarity classification, and found that the best performance came from using n-grams together with lexicon and microblog features. Interestingly, performance dropped when a POS tagger was included. They speculate that this can be due to the accuracy of the POS tagger itself, or that POS tagging just is less effective for analysing tweet polarity. In this paper we will explore the application of a set of machine learning algorithms to the task of Twitter sentiment classification, comparing one-step and two-step approaches, and investigate a range of differen</context>
<context position="9155" citStr="Kouloumpis et al. (2011)" startWordPosition="1445" endWordPosition="1448">classification, the SA API returns the same JSON structure as the Twitter API sends out, only with an additional attribute denoting the tweet’s sentiment. The Sentiment Analysis Classifier system consists of preprocessing and classification, described below. 3.1 Preprocessing As mentioned in the introduction, most approaches to Twitter sentiment analysis start with a preprocessing step, filtering out some Twitter specific symbols and functions. Go et al. (2009) used ‘:)’ and ‘:(’ as labels for the polarity, so did not remove these emoticons, but replaced URLs and user names with placeholders. Kouloumpis et al. (2011) used both an emoticon set and a hashtagged set. The latter is a subset of the Edinburgh Twitter corpus which consists of 97 million tweets (Petrovi´c et al., 2010). Some approaches have also experimented with normalizing the tweets, and removing redundant letters, e.g., “loooove” and “crazyyy”, that are used to express a stronger sentiment in tweets. Redundant letters are therefore often not deleted, but words rather trimmed down to one additional redundant letter, so that the stronger sentiment can be taken into consideration by a score/weight adjustment for that feature. To find the best fe</context>
</contexts>
<marker>Kouloumpis, Wilson, Moore, 2011</marker>
<rawString>Kouloumpis, E., Wilson, T., and Moore, J. (2011). Twitter sentiment analysis: The good the bad and the OMG! In Proceedings of the 5th International Conference on Weblogs and Social Media, pages 538–541, Barcelona, Spain. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Maynard</author>
<author>A Funk</author>
</authors>
<title>Automatic detection of political opinions in tweets.</title>
<date>2011</date>
<booktitle>In #MSM2011</booktitle>
<pages>81--92</pages>
<contexts>
<context position="2783" citStr="Maynard and Funk, 2011" startWordPosition="418" endWordPosition="421">tweet to a certain topic), emoticons, and URLs (linking to an external resource, e.g., a news article or a photo). The first system to really use Twitter as a corpus was created as a student course project at Stanford (Go et al., 2009). Pak and Paroubek (2010) experimented with sentiment classification of tweets using Support Vector Machines and Conditional Random Fields, benchmarked with a Naive Bayes Classifier baseline, but were unable to beat the baseline. Later, and as Twitter has grown in popularity, many other systems for Twitter Sentiment Analysis (TSA) have been developed (see, e.g., Maynard and Funk, 2011; Mukherjee et al., 2012; Saif et al., 2012; Chamlertwat et al., 2012). Clearly, it is possible to classify the sentiment of tweets in a single step; however, the approach to TSA most used so far is a two-step strategy where the first step is subjectivity classification and the second step is polarity classification. The goal of subjectivity classification is to separate subjective and objective statements. Pak and Paroubek (2010) counted word frequencies in a subjective vs an objective set of tweets; the results showed that interjections and personal pronouns are the strongest indicators of s</context>
</contexts>
<marker>Maynard, Funk, 2011</marker>
<rawString>Maynard, D. and Funk, A. (2011). Automatic detection of political opinions in tweets. In #MSM2011 (2011), pages 81–92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Mohammad</author>
<author>S Kiritchenko</author>
<author>X Zhu</author>
</authors>
<title>NRC-Canada: Building the state-of-theart in sentiment analysis of tweets.</title>
<date>2013</date>
<booktitle>In SemEval’13</booktitle>
<contexts>
<context position="5232" citStr="Mohammad et al., 2013" startWordPosition="794" endWordPosition="797">for analysing tweet polarity. In this paper we will explore the application of a set of machine learning algorithms to the task of Twitter sentiment classification, comparing one-step and two-step approaches, and investigate a range of different preprocessing methods. What we explicitly will not do, is to utilise a sentiment lexicon, even though many methods in TSA rely on lexica with a sentiment score for each word. Nielsen (2011) manually built a sentiment lexicon specialized for Twitter, while others have tried to induce such lexica automatically with good results (Velikovich et al., 2010; Mohammad et al., 2013). However, sentiment lexica — and in particular specialized Twitter sentiment lexica — make the classification more domain dependent. Here we will instead aim to exploit domain independent approaches as far as possible, and thus abstain from using sentiment lexica. The rest of the paper is laid out as follows: Section 2 introduces the twitter data sets used in the study. Then Section 3 describes the system built for carrying out the twitter sentiment classification experiments, which in turn are reported and discussed in Sections 4 and 5. 2 Data Manually collecting information from Twitter wou</context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Mohammad, S., Kiritchenko, S., and Zhu, X. (2013). NRC-Canada: Building the state-of-theart in sentiment analysis of tweets. In SemEval’13 (2013).</rawString>
</citation>
<citation valid="true">
<date>2011</date>
<booktitle>Proceedings of the 1st Workshop on Making Sense of Microposts (#MSM2011),</booktitle>
<location>Heraklion, Greece.</location>
<contexts>
<context position="4037" citStr="(2011)" startWordPosition="608" endWordPosition="608">erbs and (in particular) adjectives (Hatzivassiloglou and Wiebe, 2000) have shown to be good subjectivity indicators, which has made part430 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 430–437, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics of-speech (POS) tagging a reasonable technique for filtering out objective tweets. Early research on TSA showed that the challenging vocabulary made it harder to accurately tag tweets; however, Gimpel et al. (2011) report on using a POS tagger for marking tweets, performing with almost 90% accuracy. Polarity classification is the task of separating the subjective statements into positives and negatives. Kouloumpis et al. (2011) tried different solutions for tweet polarity classification, and found that the best performance came from using n-grams together with lexicon and microblog features. Interestingly, performance dropped when a POS tagger was included. They speculate that this can be due to the accuracy of the POS tagger itself, or that POS tagging just is less effective for analysing tweet polarit</context>
<context position="9155" citStr="(2011)" startWordPosition="1448" endWordPosition="1448">e SA API returns the same JSON structure as the Twitter API sends out, only with an additional attribute denoting the tweet’s sentiment. The Sentiment Analysis Classifier system consists of preprocessing and classification, described below. 3.1 Preprocessing As mentioned in the introduction, most approaches to Twitter sentiment analysis start with a preprocessing step, filtering out some Twitter specific symbols and functions. Go et al. (2009) used ‘:)’ and ‘:(’ as labels for the polarity, so did not remove these emoticons, but replaced URLs and user names with placeholders. Kouloumpis et al. (2011) used both an emoticon set and a hashtagged set. The latter is a subset of the Edinburgh Twitter corpus which consists of 97 million tweets (Petrovi´c et al., 2010). Some approaches have also experimented with normalizing the tweets, and removing redundant letters, e.g., “loooove” and “crazyyy”, that are used to express a stronger sentiment in tweets. Redundant letters are therefore often not deleted, but words rather trimmed down to one additional redundant letter, so that the stronger sentiment can be taken into consideration by a score/weight adjustment for that feature. To find the best fe</context>
</contexts>
<marker>2011</marker>
<rawString>#MSM2011 (2011). Proceedings of the 1st Workshop on Making Sense of Microposts (#MSM2011), Heraklion, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Mukherjee</author>
<author>A Malu</author>
<author>A Balamurali</author>
<author>P Bhattacharyya</author>
</authors>
<title>TwiSent: A multistage system for analyzing sentiment in Twitter.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st International Conference on Information and Knowledge Management,</booktitle>
<pages>2531--2534</pages>
<publisher>ACM.</publisher>
<location>Maui, Hawaii.</location>
<contexts>
<context position="2807" citStr="Mukherjee et al., 2012" startWordPosition="422" endWordPosition="425">), emoticons, and URLs (linking to an external resource, e.g., a news article or a photo). The first system to really use Twitter as a corpus was created as a student course project at Stanford (Go et al., 2009). Pak and Paroubek (2010) experimented with sentiment classification of tweets using Support Vector Machines and Conditional Random Fields, benchmarked with a Naive Bayes Classifier baseline, but were unable to beat the baseline. Later, and as Twitter has grown in popularity, many other systems for Twitter Sentiment Analysis (TSA) have been developed (see, e.g., Maynard and Funk, 2011; Mukherjee et al., 2012; Saif et al., 2012; Chamlertwat et al., 2012). Clearly, it is possible to classify the sentiment of tweets in a single step; however, the approach to TSA most used so far is a two-step strategy where the first step is subjectivity classification and the second step is polarity classification. The goal of subjectivity classification is to separate subjective and objective statements. Pak and Paroubek (2010) counted word frequencies in a subjective vs an objective set of tweets; the results showed that interjections and personal pronouns are the strongest indicators of subjectivity. In general,</context>
</contexts>
<marker>Mukherjee, Malu, Balamurali, Bhattacharyya, 2012</marker>
<rawString>Mukherjee, S., Malu, A., Balamurali, A., and Bhattacharyya, P. (2012). TwiSent: A multistage system for analyzing sentiment in Twitter. In Proceedings of the 21st International Conference on Information and Knowledge Management, pages 2531–2534, Maui, Hawaii. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F ˚A Nielsen</author>
</authors>
<title>A new ANEW: Evaluation of a word list for sentiment analysis in microblogs.</title>
<date>2011</date>
<booktitle>In #MSM2011</booktitle>
<pages>93--98</pages>
<contexts>
<context position="5045" citStr="Nielsen (2011)" startWordPosition="766" endWordPosition="767">ngly, performance dropped when a POS tagger was included. They speculate that this can be due to the accuracy of the POS tagger itself, or that POS tagging just is less effective for analysing tweet polarity. In this paper we will explore the application of a set of machine learning algorithms to the task of Twitter sentiment classification, comparing one-step and two-step approaches, and investigate a range of different preprocessing methods. What we explicitly will not do, is to utilise a sentiment lexicon, even though many methods in TSA rely on lexica with a sentiment score for each word. Nielsen (2011) manually built a sentiment lexicon specialized for Twitter, while others have tried to induce such lexica automatically with good results (Velikovich et al., 2010; Mohammad et al., 2013). However, sentiment lexica — and in particular specialized Twitter sentiment lexica — make the classification more domain dependent. Here we will instead aim to exploit domain independent approaches as far as possible, and thus abstain from using sentiment lexica. The rest of the paper is laid out as follows: Section 2 introduces the twitter data sets used in the study. Then Section 3 describes the system bui</context>
</contexts>
<marker>Nielsen, 2011</marker>
<rawString>Nielsen, F. ˚A. (2011). A new ANEW: Evaluation of a word list for sentiment analysis in microblogs. In #MSM2011 (2011), pages 93–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pak</author>
<author>P Paroubek</author>
</authors>
<title>Twitter as a corpus for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th International Conference on Language Resources and Evaluation,</booktitle>
<location>Valetta, Malta. ELRA.</location>
<contexts>
<context position="2421" citStr="Pak and Paroubek (2010)" startWordPosition="363" endWordPosition="366">dman (2013) for overviews. and emotionally loaded abbreviations. Thus the data will normally go through some preprocessing before any classification is attempted, e.g., by filtering out Twitter specific symbols and functions, in particular retweets (reposting another user’s tweet), mentions (’@’, tags used to mention another user), hashtags (’#’, used to tag a tweet to a certain topic), emoticons, and URLs (linking to an external resource, e.g., a news article or a photo). The first system to really use Twitter as a corpus was created as a student course project at Stanford (Go et al., 2009). Pak and Paroubek (2010) experimented with sentiment classification of tweets using Support Vector Machines and Conditional Random Fields, benchmarked with a Naive Bayes Classifier baseline, but were unable to beat the baseline. Later, and as Twitter has grown in popularity, many other systems for Twitter Sentiment Analysis (TSA) have been developed (see, e.g., Maynard and Funk, 2011; Mukherjee et al., 2012; Saif et al., 2012; Chamlertwat et al., 2012). Clearly, it is possible to classify the sentiment of tweets in a single step; however, the approach to TSA most used so far is a two-step strategy where the first ste</context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>Pak, A. and Paroubek, P. (2010). Twitter as a corpus for sentiment analysis and opinion mining. In Proceedings of the 7th International Conference on Language Resources and Evaluation, Valetta, Malta. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="1793" citStr="Pang and Lee (2008)" startWordPosition="262" endWordPosition="265">ter to being a gold mine for sentiment analysis (SA). SA for longer texts, such as movie reviews, has been explored since the 1990s;1 however, the limited amount of attributes in tweets makes the feature vectors shorter than in documents and the task of analysing them closely related to phrase- and sentence-level SA (Wilson et al., 2005; Yu and Hatzivassiloglou, 2003). Hence there are no guarantees that algorithms that perform well on document-level SA will do as well on tweets. On the other hand, it is possible to exploit some of the special features of the web language, e.g., emoticons 1See Pang and Lee (2008); Feldman (2013) for overviews. and emotionally loaded abbreviations. Thus the data will normally go through some preprocessing before any classification is attempted, e.g., by filtering out Twitter specific symbols and functions, in particular retweets (reposting another user’s tweet), mentions (’@’, tags used to mention another user), hashtags (’#’, used to tag a tweet to a certain topic), emoticons, and URLs (linking to an external resource, e.g., a news article or a photo). The first system to really use Twitter as a corpus was created as a student course project at Stanford (Go et al., 20</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Pang, B. and Lee, L. (2008). Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrovi´c</author>
<author>M Osborne</author>
<author>V Lavrenko</author>
</authors>
<title>The Edinburgh Twitter corpus.</title>
<date>2010</date>
<booktitle>In HLT10 (2010),</booktitle>
<pages>25--26</pages>
<marker>Petrovi´c, Osborne, Lavrenko, 2010</marker>
<rawString>Petrovi´c, S., Osborne, M., and Lavrenko, V. (2010). The Edinburgh Twitter corpus. In HLT10 (2010), pages 25–26. Workshop on Computational Linguistics in a World of Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Saif</author>
<author>Y He</author>
<author>H Alani</author>
</authors>
<title>Semantic sentiment analysis of Twitter.</title>
<date>2012</date>
<booktitle>In Proceedings of the 11th International Semantic Web Conference,</booktitle>
<pages>508--524</pages>
<publisher>Springer.</publisher>
<location>Boston, Massachusetts.</location>
<contexts>
<context position="2826" citStr="Saif et al., 2012" startWordPosition="426" endWordPosition="429">linking to an external resource, e.g., a news article or a photo). The first system to really use Twitter as a corpus was created as a student course project at Stanford (Go et al., 2009). Pak and Paroubek (2010) experimented with sentiment classification of tweets using Support Vector Machines and Conditional Random Fields, benchmarked with a Naive Bayes Classifier baseline, but were unable to beat the baseline. Later, and as Twitter has grown in popularity, many other systems for Twitter Sentiment Analysis (TSA) have been developed (see, e.g., Maynard and Funk, 2011; Mukherjee et al., 2012; Saif et al., 2012; Chamlertwat et al., 2012). Clearly, it is possible to classify the sentiment of tweets in a single step; however, the approach to TSA most used so far is a two-step strategy where the first step is subjectivity classification and the second step is polarity classification. The goal of subjectivity classification is to separate subjective and objective statements. Pak and Paroubek (2010) counted word frequencies in a subjective vs an objective set of tweets; the results showed that interjections and personal pronouns are the strongest indicators of subjectivity. In general, these word classes</context>
</contexts>
<marker>Saif, He, Alani, 2012</marker>
<rawString>Saif, H., He, Y., and Alani, H. (2012). Semantic sentiment analysis of Twitter. In Proceedings of the 11th International Semantic Web Conference, pages 508–524, Boston, Massachusetts. Springer.</rawString>
</citation>
<citation valid="true">
<date>2013</date>
<booktitle>Proceedings of the International Workshop on Semantic Evaluation, SemEval ’13,</booktitle>
<publisher>ACL.</publisher>
<location>Atlanta,</location>
<contexts>
<context position="1809" citStr="(2013)" startWordPosition="267" endWordPosition="267">sentiment analysis (SA). SA for longer texts, such as movie reviews, has been explored since the 1990s;1 however, the limited amount of attributes in tweets makes the feature vectors shorter than in documents and the task of analysing them closely related to phrase- and sentence-level SA (Wilson et al., 2005; Yu and Hatzivassiloglou, 2003). Hence there are no guarantees that algorithms that perform well on document-level SA will do as well on tweets. On the other hand, it is possible to exploit some of the special features of the web language, e.g., emoticons 1See Pang and Lee (2008); Feldman (2013) for overviews. and emotionally loaded abbreviations. Thus the data will normally go through some preprocessing before any classification is attempted, e.g., by filtering out Twitter specific symbols and functions, in particular retweets (reposting another user’s tweet), mentions (’@’, tags used to mention another user), hashtags (’#’, used to tag a tweet to a certain topic), emoticons, and URLs (linking to an external resource, e.g., a news article or a photo). The first system to really use Twitter as a corpus was created as a student course project at Stanford (Go et al., 2009). Pak and Par</context>
<context position="18786" citStr="(2013)" startWordPosition="3018" endWordPosition="3018"> RT-tags. Given the small size of the in-house (‘NTNU’) data set, no major improvement was expected from adding it in the unconstrained task. Instead, a radically different set-up was chosen to create a new system, and train it on both the in-house and provided data. NTNUU utilizes a two-step approach, with SVM for subjectivity and MaxEnt for polarity classification, a combination intended to capture the strengths of both algorithms. No preprocessing was used for the subjectivity step, but user names were removed before attempting polarity classification. As further described by Wilson et al. (2013), the SemEval’13 shared task involved testing on a set of 3813 tweets (1572 positive, 601 negative, and 1640 neutral). In order to evaluate classification performance on data of roughly the same length and type, but from a different domain, the evaluation data also included 2094 Short Message Service texts (SMS; 492 positive, 394 negative, and 1208 neutral). Table 4 shows the results obtained by the NTNU systems on the SemEval’13 evaluation data, in terms of average precision, recall and F-score for all three classes, as well as average F-score for positive and negative tweets only (F1+/−; i.e</context>
</contexts>
<marker>2013</marker>
<rawString>SemEval’13 (2013). Proceedings of the International Workshop on Semantic Evaluation, SemEval ’13, Atlanta, Georgia. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Velikovich</author>
<author>S Blair-Goldensohn</author>
<author>K Hannan</author>
<author>R McDonald</author>
</authors>
<title>The viability of webderived polarity lexicons.</title>
<date>2010</date>
<booktitle>In HLT10</booktitle>
<pages>777--785</pages>
<contexts>
<context position="5208" citStr="Velikovich et al., 2010" startWordPosition="790" endWordPosition="793">g just is less effective for analysing tweet polarity. In this paper we will explore the application of a set of machine learning algorithms to the task of Twitter sentiment classification, comparing one-step and two-step approaches, and investigate a range of different preprocessing methods. What we explicitly will not do, is to utilise a sentiment lexicon, even though many methods in TSA rely on lexica with a sentiment score for each word. Nielsen (2011) manually built a sentiment lexicon specialized for Twitter, while others have tried to induce such lexica automatically with good results (Velikovich et al., 2010; Mohammad et al., 2013). However, sentiment lexica — and in particular specialized Twitter sentiment lexica — make the classification more domain dependent. Here we will instead aim to exploit domain independent approaches as far as possible, and thus abstain from using sentiment lexica. The rest of the paper is laid out as follows: Section 2 introduces the twitter data sets used in the study. Then Section 3 describes the system built for carrying out the twitter sentiment classification experiments, which in turn are reported and discussed in Sections 4 and 5. 2 Data Manually collecting info</context>
</contexts>
<marker>Velikovich, Blair-Goldensohn, Hannan, McDonald, 2010</marker>
<rawString>Velikovich, L., Blair-Goldensohn, S., Hannan, K., and McDonald, R. (2010). The viability of webderived polarity lexicons. In HLT10 (2010), pages 777–785.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>Z Kozareva</author>
<author>P Nakov</author>
<author>A Ritter</author>
<author>S Rosenthal</author>
<author>V Stoyanov</author>
</authors>
<date>2013</date>
<booktitle>SemEval2013 Task 2: Sentiment analysis in Twitter. In SemEval’13</booktitle>
<contexts>
<context position="6618" citStr="Wilson et al., 2013" startWordPosition="1038" endWordPosition="1041">a corpus from the microblogosphere. Most of the data used in TSA research is collected through the Twitter API, either by Training Dev 1 Dev 2 NTNU Class Num % Num % Num % Num % Negative 1288 15 176 21 340 26 86 19 Neutral 4151 48 144 45 739 21 232 50 Positive 3270 37 368 35 575 54 142 31 Total 8709 688 1654 461 Table 1: The data sets used in the experiments searching for a certain topic/keyword or by streaming realtime data. Four different data sets were used in the experiments described below. three were supplied by the organisers of the SemEval’13 shared task on Twitter sentiment analysis (Wilson et al., 2013), in the form of a training set, a smaller initial development set, and a larger development set. All sets consist of manually annotated tweets on a range of topics, including different products and events. Tweet-level classification (Task 2B) was split into two subtasks in SemEval’13, one allowing training only on the data sets supplied by the organisers (constrained) and one allowing training also on external data (unconstrained). To this end, a web application2 for manual annotation of tweets was built and used to annotate a small fourth data set (‘NTNU’). Each of the 461 tweets in the ‘NTN</context>
<context position="18786" citStr="Wilson et al. (2013)" startWordPosition="3015" endWordPosition="3018">user names and RT-tags. Given the small size of the in-house (‘NTNU’) data set, no major improvement was expected from adding it in the unconstrained task. Instead, a radically different set-up was chosen to create a new system, and train it on both the in-house and provided data. NTNUU utilizes a two-step approach, with SVM for subjectivity and MaxEnt for polarity classification, a combination intended to capture the strengths of both algorithms. No preprocessing was used for the subjectivity step, but user names were removed before attempting polarity classification. As further described by Wilson et al. (2013), the SemEval’13 shared task involved testing on a set of 3813 tweets (1572 positive, 601 negative, and 1640 neutral). In order to evaluate classification performance on data of roughly the same length and type, but from a different domain, the evaluation data also included 2094 Short Message Service texts (SMS; 492 positive, 394 negative, and 1208 neutral). Table 4 shows the results obtained by the NTNU systems on the SemEval’13 evaluation data, in terms of average precision, recall and F-score for all three classes, as well as average F-score for positive and negative tweets only (F1+/−; i.e</context>
</contexts>
<marker>Wilson, Kozareva, Nakov, Ritter, Rosenthal, Stoyanov, 2013</marker>
<rawString>Wilson, T., Kozareva, Z., Nakov, P., Ritter, A., Rosenthal, S., and Stoyanov, V. (2013). SemEval2013 Task 2: Sentiment analysis in Twitter. In SemEval’13 (2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>P Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<publisher>ACL.</publisher>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="1512" citStr="Wilson et al., 2005" startWordPosition="214" endWordPosition="217">es for traditional language processing systems. The posts (“tweets”) are limited to 140 characters and often contain misspellings, slang and abbreviations. On the other hand, the posts are often opinionated in nature as a very result of their informal character, which has led Twitter to being a gold mine for sentiment analysis (SA). SA for longer texts, such as movie reviews, has been explored since the 1990s;1 however, the limited amount of attributes in tweets makes the feature vectors shorter than in documents and the task of analysing them closely related to phrase- and sentence-level SA (Wilson et al., 2005; Yu and Hatzivassiloglou, 2003). Hence there are no guarantees that algorithms that perform well on document-level SA will do as well on tweets. On the other hand, it is possible to exploit some of the special features of the web language, e.g., emoticons 1See Pang and Lee (2008); Feldman (2013) for overviews. and emotionally loaded abbreviations. Thus the data will normally go through some preprocessing before any classification is attempted, e.g., by filtering out Twitter specific symbols and functions, in particular retweets (reposting another user’s tweet), mentions (’@’, tags used to men</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Wilson, T., Wiebe, J., and Hoffmann, P. (2005). Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 347–354, Vancouver, British Columbia, Canada. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yu</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>129--136</pages>
<editor>In Collins, M. and Steedman, M., editors,</editor>
<publisher>ACL.</publisher>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1544" citStr="Yu and Hatzivassiloglou, 2003" startWordPosition="218" endWordPosition="221">nguage processing systems. The posts (“tweets”) are limited to 140 characters and often contain misspellings, slang and abbreviations. On the other hand, the posts are often opinionated in nature as a very result of their informal character, which has led Twitter to being a gold mine for sentiment analysis (SA). SA for longer texts, such as movie reviews, has been explored since the 1990s;1 however, the limited amount of attributes in tweets makes the feature vectors shorter than in documents and the task of analysing them closely related to phrase- and sentence-level SA (Wilson et al., 2005; Yu and Hatzivassiloglou, 2003). Hence there are no guarantees that algorithms that perform well on document-level SA will do as well on tweets. On the other hand, it is possible to exploit some of the special features of the web language, e.g., emoticons 1See Pang and Lee (2008); Feldman (2013) for overviews. and emotionally loaded abbreviations. Thus the data will normally go through some preprocessing before any classification is attempted, e.g., by filtering out Twitter specific symbols and functions, in particular retweets (reposting another user’s tweet), mentions (’@’, tags used to mention another user), hashtags (’#</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>Yu, H. and Hatzivassiloglou, V. (2003). Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In Collins, M. and Steedman, M., editors, Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 129–136, Sapporo, Japan. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>