<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008208">
<title confidence="0.931749333333333">
Lemmatization and Lexicalized Statistical Parsing of Morphologically Rich
Languages: the Case of French
Djamé Seddah Grzegorz Chrupała
</title>
<author confidence="0.882169">
Alpage Inria &amp; Univ. Paris-Sorbonne Spoken Language System, Saarland Univ.
</author>
<affiliation confidence="0.798112">
Paris, France Saarbrücken, Germany
</affiliation>
<author confidence="0.947439">
Özlem Çetino˘glu and Josef van Genabith Marie Candito
</author>
<affiliation confidence="0.9288255">
NCLT &amp; CNGL, Dublin City Univ. Alpage Inria &amp; Univ. Paris 7
Dublin, Ireland Paris, France
</affiliation>
<sectionHeader confidence="0.987682" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999782571428572">
This paper shows that training a lexicalized
parser on a lemmatized morphologically-rich
treebank such as the French Treebank slightly
improves parsing results. We also show that
lemmatizing a similar in size subset of the En-
glish Penn Treebank has almost no effect on
parsing performance with gold lemmas and
leads to a small drop of performance when au-
tomatically assigned lemmas and POS tags are
used. This highlights two facts: (i) lemmati-
zation helps to reduce lexicon data-sparseness
issues for French, (ii) it also makes the pars-
ing process sensitive to correct assignment of
POS tags to unknown words.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998922">
Large parse-annotated corpora have led to an explo-
sion of interest in statistical parsing methods, includ-
ing the development of highly successful models for
parsing English using the Wall Street Journal Penn
Treebank (PTB, (Marcus et al., 1994)). Over the
last 10 years, parsing performance on the PTB has
hit a performance plateau of 90-92% f-score using
the PARSEVAL evaluation metric. When adapted to
other language/treebank pairs (such as German, He-
brew, Arabic, Italian or French), to date these mod-
els have performed much worse.
A number of arguments have been advanced
to explain this performance gap, including limited
amounts of training data, differences in treebank an-
notation schemes, inadequacies of evaluation met-
rics, linguistic factors such as the degree of word or-
der variation, the amount of morphological informa-
tion available to the parser as well as the effects of
syncretism prevalent in many morphologically rich
languages.
</bodyText>
<page confidence="0.996366">
85
</page>
<bodyText confidence="0.999958135135135">
Even though none of these arguments in isola-
tion can account for the systematic performance gap,
a pattern is beginning to emerge: morphologically
rich languages tend to be susceptible to parsing per-
formance degradation.
Except for a residual clitic case system, French
does not have explicit case marking, yet its mor-
phology is considerably richer than that of English,
and French is therefore a candidate to serve as an
instance of a morphologically rich language (MRL)
that requires specific treatment to achieve reasonable
parsing performance.
Interestingly, French also exhibits a limited
amount of word order variation occurring at dif-
ferent syntactic levels including (i) the word level
(e.g. pre or post nominal adjective, pre or post ver-
bal adverbs); (ii) phrase level (e.g. possible alterna-
tions between post verbal NPs and PPs). In order
to avoid discontinuous constituents as well as traces
and coindexations, treebanks for this language, such
as the French Treebank (FTB, (Abeillé et al., 2003))
or the Modified French Treebank (MFT, (Schluter
and van Genabith, 2007)), propose a flat annota-
tion scheme with a non-configurational distinction
between adjunct and arguments.
Finally, the extraction of treebank grammars from
the French treebanks, which contain less than a third
of the annotated data as compared to PTB, is subject
to many data sparseness issues that contribute to a
performance ceiling, preventing the statistical pars-
ing of French to reach the same level of performance
as for PTB-trained parsers (Candito et al., 2009).
This data sparseness bottleneck can be summa-
rized as a problem of optimizing a parsing model
along two axes: the grammar and the lexicon. In
both cases, the goal is either to get a more compact
grammar at the rule level or to obtain a consider-
</bodyText>
<note confidence="0.984533">
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 85–93,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999919">
ably less sparse lexicon. So far, both approaches
have been tested for French using different means
and with different degrees of success.
To obtain better grammars, Schluter and van Gen-
abith (2007) extracted a subset of an early release
of the FTB and carried out extensive restructuring,
extensions and corrections (referred to as the Modi-
fied French Treebank MFT) to support grammar ac-
quisition for PCFG-based LFG Parsing (Cahill et
al., 2004) while Crabbé and Candito (2008) slightly
modified the original FTB POS tagset to optimize
the grammar with latent annotations extracted by the
Berkeley parser (BKY, (Petrov et al., 2006)).
Moreover, research oriented towards adapting
more complex parsing models to French showed
that lexicalized models such as Collins’ model 2
(Collins, 1999) can be tuned to cope effectively with
the flatness of the annotation scheme in the FTB,
with the Charniak model (Charniak, 2000) perform-
ing particularly well, but outperformed by the BKY
parser on French data (Seddah et al., 2009).
Focusing on the lexicon, experiments have been
carried out to study the impact of different forms of
word clustering on the BKY parser trained on the
FTB. Candito et al. (2009) showed that using gold
lemmatization provides a significant increase in per-
formance. Obviously, less sparse lexical data which
retains critical pieces of information can only help a
model to perform better. This was shown in (Candito
and Crabbé, 2009) where distributional word clus-
ters were acquired from a 125 million words corpus
and combined with inflectional suffixes extracted
from the training data. Training the BKY parser
with 1000 clusters boosts its performance to the cur-
rent state-of-the-art with a PARSEVAL Fi score of
88.28% (baseline was 86.29 %).
We performed the same experiment using the
CHARNIAK parser and recorded only a small im-
provement (from 84.96% to 85.51%). Given the
fact that lexical information is crucial for lexicalized
parsers in the form of bilexical dependencies, this
result raises the question whether this kind of clus-
tering is in fact too drastic for lexicalized parsers as
it may give rise to head-to-head dependencies which
are too coarse. To answer this question, in this paper
we explore the impact of lemmatization, as a (rather
limited) constrained form of clustering, on a state-
of-the-art lexicalized parser (CHARNIAK). In order
to evaluate the influence of lemmatization on this
parser (which is known to be highly tuned for En-
glish) we carry out experiments on both the FTB and
on a lemmatized version of the PTB. We used gold
lemmatization when available and an automatic sta-
tistical morphological analyzer (Chrupała, 2010) to
provide more realistic parsing results.
The idea is to verify whether lemmatization will help
to reduce data sparseness issues due to the French
rich morphology and to see if this process, when
applied to English will harm the performance of a
parser optimized for the limited morphology of En-
glish.
Our results show that the key issue is the way un-
seen tokens (lemmas or words) are handled by the
CHARNIAK parser. Indeed, using pure lemma is
equally suboptimal for both languages. On the other
hand, feeding the parser with both lemma and part-
of-speech slightly enhances parsing performance for
French.
We first describe our data sets in Section 2, intro-
duce our data driven morphology process in Section
3, then present experiments in Section 4. We dis-
cuss our results in Section 5 and compare them with
related research in Section 6 before concluding and
outlining further research.
</bodyText>
<sectionHeader confidence="0.989291" genericHeader="introduction">
2 Corpus
</sectionHeader>
<bodyText confidence="0.999745684210526">
THE FRENCH TREEBANK is the first annotated and
manually corrected treebank for French. The data is
annotated with labeled constituent trees augmented
with morphological annotations and functional an-
notations of verbal dependents. Its key properties,
compared with the PTB, are the following:
Size: The FTB consists of 350,931 tokens and
12,351 sentences, that is less than a third of the size
of PTB. The average length of a sentence is 28.41
tokens. By contrast, the average sentence length in
the Wall Street Journal section of the PTB is 25.4
tokens.
A Flat Annotation Scheme: Both the FTB and the
PTB are annotated with constituent trees. However,
the annotation scheme is flatter in the FTB. For in-
stance, there are no VPs for finite verbs and only one
sentential level for clauses or sentences whether or
not they are introduced by a complementizer. Only
the verbal nucleus (VN) is annotated and comprises
</bodyText>
<page confidence="0.994169">
86
</page>
<bodyText confidence="0.999799083333333">
the verb, its clitics, auxiliaries, adverbs and nega-
tion.
Inflection: French morphology is richer than En-
glish and leads to increased data sparseness for sta-
tistical parsing. There are 24,098 lexical types in
the FTB, with an average of 16 tokens occurring for
each type.
Compounds: Compounds are explicitly annotated
and very frequent in the treebank: 14.52% of to-
kens are part of a compound. Following Candito
and Crabbé (2009), we use a variation of the tree-
bank where compounds with regular syntactic pat-
terns have been expanded. We refer to this instance
as FTB-UC.
Lemmatization: Lemmas are included in the tree-
bank’s morphological annotations and denote an ab-
straction over a group of inflected forms. As there
is no distinction between semantically ambiguous
lexemes at the word form level, polysemic homo-
graphs with common inflections are associated with
the same lemma (Abeillé et al., 2003). Thus, except
for some very rare cases, a pair consisting of a word
form and its part-of-speech unambiguously maps to
the same lemma.
</bodyText>
<subsectionHeader confidence="0.999145">
2.1 Lemmatizing the Penn Treebank
</subsectionHeader>
<bodyText confidence="0.99997794117647">
Unlike the FTB, the PTB does not have gold lem-
mas provided within the treebank. We use the finite
state morphological analyzer which comes within
the English ParGram Grammar (Butt et al., 1999) for
lemmatization. For open class words (nouns, verbs,
adjectives, adverbs) the word form is sent to the mor-
phological analyzer. The English ParGram morpho-
logical analyzer outputs all possible analyses of the
word form. The associated gold POS from the PTB
is used to disambiguate the result. The same process
is applied to closed class words where the word form
is different from the lemma (e.g. ’ll for will). For the
remaining parts of speech the word form is assigned
to the lemma.
Since gold lemmas are not available for the PTB,
a large-scale automatic evaluation of the lemmatizer
is not possible. Instead, we conducted two manual
evaluations. First, we randomly extracted 5 sam-
ples of 200 &lt;POS,word&gt; pairs from Section 23 of
the PTB. Each data set is fed into the lemmatiza-
tion script, and the output is manually checked. For
the 5x200 &lt;POS,word&gt; sets the number of incorrect
lemmas is 1, 3, 2, 0, and 2. The variance is small
indicating that the results are fairly stable. For the
second evaluation, we extracted each unseen word
from Section 23 and manually checked the accuracy
of the lemmatization. Of the total of 1802 unseen
words, 394 words are associated with an incorrect
lemma (331 unique) and only 8 with an incorrect
&lt;POS,lemma&gt; pair (5 unique). For an overall un-
seen word percentage of 3.22%, the lemma accu-
racy is 77.70%. If we assume that all seen words
are correctly lemmatized, overall accuracy would be
99.28%.
</bodyText>
<subsectionHeader confidence="0.997679">
2.2 Treebank properties
</subsectionHeader>
<bodyText confidence="0.999602">
In order to evaluate the influence of lemmatization
on comparable corpora, we extracted a random sub-
set of the PTB with properties comparable to the
FTB-UC (mainly with respect to CFG size and num-
ber of tokens). We call this PTB subset S.PTB. Ta-
ble 1 presents a summary of some relevant features
of those treebanks.
</bodyText>
<table confidence="0.997279666666667">
FTBUC S.PTB PTB
#oftokens 350,931 350,992 1,152,305
# of sentences 12,351 13,811 45,293
average length 28,41 25.41 25.44
CFG size 607,162 638,955 2,097,757
# unique CFG rules 43,413 46,783 91,027
# unique word forms 27,130 26,536 47,678
# unique lemmas 17,570 20,226 36,316
ratio words/lemma 1.544 1.311 1.312
</table>
<tableCaption confidence="0.999801">
Table 1: French and Penn Treebanks properties
</tableCaption>
<bodyText confidence="0.9786148">
Table 1 shows that the average number of word
forms associated with a lemma (i.e. the lemma ratio)
is higher in the FTB-UC (1.54 words/lemma) than in
the PTB (1.31). Even though the PTB ratio is lower,
it is still large enough to suggest that even the limited
English morphology should be taken into account
when aiming at reducing lexicon sparseness.
Trying to learn French and English morphology
in a data driven fashion in order to predict lemma
from word forms is the subject of the next section.
</bodyText>
<sectionHeader confidence="0.990247" genericHeader="method">
3 Morphology learning
</sectionHeader>
<bodyText confidence="0.99986125">
In order to assign morphological tags and lemmas
to words we use the MORFETTE model (Chrupała,
2010), which is a variation of the approach described
in (Chrupała et al., 2008).
</bodyText>
<page confidence="0.993562">
87
</page>
<bodyText confidence="0.996501">
MORFETTE is a sequence labeling model which
combines the predictions of two classification mod-
els (one for morphological tagging and one for
lemmatization) at decoding time, using beam search.
</bodyText>
<subsectionHeader confidence="0.998298">
3.1 Overview of the Morfette model
</subsectionHeader>
<bodyText confidence="0.999890888888889">
The morphological classes correspond simply to the
(fine-grained) POS tags. Lemma classes are edit
scripts computed from training data: they specify
which string manipulations (such as character dele-
tions and insertions) need to be performed in order
to transform the input string (word form) into the
corresponding output string (lemma).
The best sequence of lemmas and morphological
tags for input sentence x is defined as:
</bodyText>
<equation confidence="0.983419">
(�l, m) = arg max
(l,M)
The joint probability is decomposed as follows:
P(l0...li,m0...mi|x) =PL(li|mi,x)PM(mi|x)
× P(m0...mi−1,l0...li−1|x)
</equation>
<bodyText confidence="0.99997604">
where PL(li|mi, x) is the probability of lemma class
l at position i according to the lemma classifier,
PM(mi|x) is the probability of the tag m at posi-
tion i according to the morphological tag classifier,
and x is the sequence of words to label.
While Chrupała et al. (2008) use Maximum En-
tropy training to learn PM and PL, here we learn
them using Averaged Perceptron algorithm due to
Freund and Schapire (1999). It is a much simpler
algorithm which in many scenarios (including ours)
performs as well as or better than MaxEnt.
We also use the general Edit Tree instantiation of
the edit script as developed in (Chrupała, 2008). We
find the longest common substring (LCS) between
the form w and the lemma w′. The portions of the
string in the word form before (prefix) and after (suf-
fix) the LCS need to be modified in some way, while
the LCS (stem) stays the same. If there is no LCS,
then we simply record that we need to replace w
with w′ . As for the modifications to the prefix and
the suffix, we apply the same procedure recursively:
we try to find the LCS between the prefix of w and
the prefix of w′. If we find one, we recurse; if we do
not, we record the replacement; we do the same for
the suffix.
</bodyText>
<subsectionHeader confidence="0.999714">
3.2 Data Set
</subsectionHeader>
<bodyText confidence="0.999958086956522">
We trained MORFETTE on the standard splits of the
FTB with the first 10% as test set, the next 10% for
the development set and the remaining for training
(i.e. 1235/1235/9881 sentences). Lemmas and part-
of-speech tags are given by the treebank annotation
scheme.
As pointed out in section 2.1, PTB’s lemmas have
been automatically generated by a deterministic pro-
cess, and only a random subset of them have been
manually checked. For the remainder of this paper,
we treat them as gold, regardless of the errors in-
duced by our PTB lemmatizer.
The S.PTB follows the same split as the FTB-UC,
first 10% for test, next 10% for dev and the last 80%
for training (i.e. 1380/1381/11050 sentences).
MORFETTE can optionally use a morphological
lexicon to extract features. For French, we used the
extended version of Leff (Sagot et al., 2006) and for
English, the lexicon used in the Penn XTAG project
(Doran et al., 1994). We reduced the granularity of
the XTAG tag set, keeping only the bare categories.
Both lexicons contain around 225 thousands word
form entries.
</bodyText>
<subsectionHeader confidence="0.99771">
3.3 Performance on French and English
</subsectionHeader>
<bodyText confidence="0.999985238095238">
Table 2 presents results of MORFETTE applied to the
development and test sets of our treebanks. Part-of-
speech tagging performance for French is state-of-
the-art on the FTB-UC, with an accuracy of 97.68%,
on the FTB-UC test set, only 0.02 points (absolute)
below the MaxEnt POS tagger of Denis and Sagot
(2009). Comparing MORFETTE’s tagging perfor-
mance for English is a bit more challenging as we
only trained on one third of the full PTB and evalu-
ated on approximately one section, whereas results
reported in the literature are usually based on train-
ing on sections 02-18 and evaluating on either sec-
tions 19-21 or 22-24. For this setting, state-of-the-
art POS accuracy for PTB tagging is around 97.33%.
On our PTB sample, MORFETTE achieves 96.36%
for all words and 89.64 for unseen words.
Comparing the lemmatization performance for both
languages on the same kind of data is even more dif-
ficult as we are not aware of any data driven lem-
matizer on the same data. However, with an overall
accuracy above 98% for the FTB-UC (91.5% for un-
</bodyText>
<equation confidence="0.9279">
P(l, m|x)
</equation>
<page confidence="0.984996">
88
</page>
<bodyText confidence="0.998265333333333">
seen words) and above 99% for the S.PTB (95% for
unseen words), lemmatization performs well enough
to properly evaluate parsing on lemmatized data.
</bodyText>
<table confidence="0.996805888888889">
FTBUC S.PTB
DEV All Unk. (4.8) All Unk. (4.67)
POS acc 97.38 91.95 96.36 88.90
Lemma acc 98.20 92.52 99.11 95.51
Joint acc 96.35 87.16 96.26 87.05
TEST All Unk. (4.62) All Unk. (5.04)
POS acc 97.68 90.52 96.53 89.64
Lemma acc 98.36 91.54 99.13 95.72
Joint acc 96.74 85.28 96.45 88.49
</table>
<tableCaption confidence="0.962229">
Table 2: POS tagging and lemmatization performance on
the FTB and on the S.PTB
</tableCaption>
<sectionHeader confidence="0.991751" genericHeader="method">
4 Parsing Experiments
</sectionHeader>
<bodyText confidence="0.999966">
In this section, we present the results of two sets
of experiments to evaluate the impact of lemmatiza-
tion on the lexicalized statistical parsing of two lan-
guages, one morphologically rich (French), but with
none of its morphological features exploited by the
CHARNIAK parser, the other (English) being quite
the opposite, with the parser developed mainly for
this language and PTB annotated data. We show that
lemmatization results in increased performance for
French, while doing the same for English penalizes
parser performance.
</bodyText>
<subsectionHeader confidence="0.922309">
4.1 Experimental Protocol
</subsectionHeader>
<bodyText confidence="0.9995818125">
Data The data sets described in section 3.2 are used
throughout. The version of the CHARNIAK parser
(Charniak, 2000) was released in August 2005 and
recently adapted to French (Seddah et al., 2009).
Metrics We report results on sentences of length
less than 40 words, with three evaluation met-
rics: the classical PARSEVAL Labeled brackets Fi
score, POS tagging accuracy (excluding punctua-
tion tags) and the Leaf Ancestor metric (Sampson
and Babarczy, 2003) which is believed to be some-
what more neutral with respect to the treebank an-
notation scheme than PARSEVAL (Rehbein and van
Genabith, 2007).
Treebank tag sets Our experiments involve the in-
clusion of POS tags directly in tokens. We briefly
describe our treebank tag sets below.
</bodyText>
<listItem confidence="0.9938275">
• FTB-UC TAG SET: “CC” This is the tag set de-
veloped by (Crabbé and Candito, 2008) (Table
</listItem>
<bodyText confidence="0.989870428571428">
4), known to provide the best parsing perfor-
mance for French (Seddah et al., 2009). Like in
the FTB, preterminals are the main categories,
but they are also augmented with a WH flag
for A, ADV, PRO and with the mood for verbs
(there are 6 moods). No information is propa-
gated to non-terminal symbols.
</bodyText>
<table confidence="0.985315">
ADJ ADJWH ADV ADVWH CC CLO CLR CLS CS DET
DETWH ET I NC NPP P P+D P+PRO PONCT PREF PRO
PROREL PROWH V VIMP VINF VPP VPR VS
</table>
<tableCaption confidence="0.998192">
Table 4: CC tag set
</tableCaption>
<listItem confidence="0.867616090909091">
• THE PTB TAG SET This tag set is described
at length in (Marcus et al., 1994) and contains
supplementary morphological information (e.g.
number) over and above what is represented in
the CC tag set for French. Note that some infor-
mation is marked at the morphological level in
English (superlative, “the greatest (JJS)”) and
not in French (“ le plus (ADV) grand (ADJ)”).
CC CD DT EX FW IN JJ JJR JJS LS MD NN NNP NNPS
NNS PDT POS PRP PRP$ RB RBR RBS RP SYM TO UH
VB VBD VBG VBN VBP VBZ WDT WP WP$ WRB
</listItem>
<tableCaption confidence="0.992871">
Table 5: PTB tag set
</tableCaption>
<subsectionHeader confidence="0.991988">
4.2 Cross token variation and parsing impact
</subsectionHeader>
<bodyText confidence="0.999962578947369">
From the source treebanks, we produce 5 versions
of tokens: tokens are generated as either simple
POS tag, gold lemma, gold lemma+gold POS, word
form, and word form+gold POS. The token versions
successively add more morphological information.
Parsing results are presented in Table 3.
Varying the token form The results show that
having no lexical information at all (POS-only) re-
sults in a small drop of PARSEVAL performance for
French compared to parsing lemmas, while the cor-
responding Leaf Ancestor score is actually higher.
For English having no lexical information at all
leads to a drop of 2 points in PARSEVAL. The so-
called impoverished morphology of English appears
to bring enough morphological information to raise
tagging performance to 95.92% (from POS-only to
word-only).
For French the corresponding gain is only 2 points
of POS tagging accuracy. Moreover, between these
</bodyText>
<page confidence="0.997973">
89
</page>
<table confidence="0.885214307692307">
French Treebank UC Sampled Penn Treebank
F1 score Pos acc. leaf-Anc. F1 score Pos acc. leaf-Anc.
84.48 100 93.97 85.62 100 94.02
84.77 94.23 93.76 87.69 89.22 94.92
84.96 96.26 94.08 88.64 95.92 95.10
86.83(1) 98.79 94.65 89.59(3) 99.97 95.41
86.13(2) 98.4 94.46 89.53(4) 99.96 95.38
Tokens
POS-only
lemma-only
word-only
lemma-POS
word-POS
</table>
<tableCaption confidence="0.833236">
Table 3: Parsing performance on the FTB-UC and the S.PTB with tokens variations using gold lemmas and gold POS.
(p-value (1) &amp; (2) = 0.007; p-value (3) &amp; (4) = 0.146. All other configurations are statistically significant.)
</tableCaption>
<bodyText confidence="0.999759982758621">
two tokens variations, POS-only and word-only,
parsing results gain only half a point in PARSEVAL
and almost nothing in leaf Ancestor.
Thus, it seems that encoding more morphology
(i.e. including word forms) in the tokens does not
lead to much improvement for parsing French as op-
posed to English. The reduction in data sparseness
due to the use of lemmas alone is thus not sufficient
to counterbalance the lack of morphological infor-
mation.
However, the large gap between POS tagging
accuracy seen between lemma-only and word-only
for English indicates that the parser makes use of
this information to provide at least reasonable POS
guesses.
For French, only 0.2 points are gained for PAR-
SEVAL results between lemma-only to word-only,
while POS accuracy benefits a bit more from includ-
ing richer morphological information.
This raises the question whether the FTB-UC pro-
vides enough data to make its richer morphology in-
formative enough for a parsing model.
Suffixing tokens with POS tags It is only when
gold POS are added to the lemmas that one can see
the advantage of a reduced lexicon for French. In-
deed, performance peaks for this setting (lemma-
POS). The situation is not as clear for English, where
performance is almost identical when gold POS are
added to lemmas or words. POS Tagging is nearly
perfect, thus a performance ceiling is reached. The
very small differences between those two configura-
tions (most noticeable with the Leaf Ancestor score
of 95.41 vs. 95.38) indicates that the reduced lemma
lexicon is actually of some limited use but its impact
is negligible compared to perfect tagging.
While the lemma+POS setting clearly boosts per-
formance for parsing the FTB, the situation is less
clear for English. Indeed, the lemma+POS and the
word+POS gold variations give almost the same re-
sults. The fact that the POS tagging accuracy is close
to 100% in this mode shows that the key parameter
for optimum parsing performance in this experiment
is the ability to guess POS for unknown words well.
In fact, the CHARNIAK parser uses a two letter
suffix context for its tagging model, and when gold
POS are suffixed to any type of token (being lemma
or word form), the PTB POS tagset is used as a sub-
stitute for lack of morphology.
It should also be noted that the FTB-UC tag set
does include some discriminative features (such as
PART, INF and so on) but those are expressed by
more than two letters, and therefore a two letter
suffix tag cannot really be useful to discriminate
a richer morphology. For example, in the PTB,
the suffix BZ, as in VBZ, always refers to a verb,
whereas the FTB pos tag suffix PP, as in NPP
(Proper Noun) is also found in POS labels such as
VPP (past participle verb).
</bodyText>
<subsectionHeader confidence="0.9191145">
4.3 Realistic Setup: Using Morfette to help
parsing
</subsectionHeader>
<bodyText confidence="0.982652076923077">
Having shown that parsing French benefits from a
reduced lexicon is not enough as results imply that a
key factor is POS tag guessing. We therefore test our
hypothesis in a more realistic set up. We use MOR-
FETTE to lemmatize and tag raw words (instead of
the “gold” lemma-based approach described above),
and the resulting corpus is then parsed using the cor-
responding training set.
In order to be consistent with PARSEVAL POS eval-
uation, which does not take punctuation POS into
account, we provide a summary of MORFETTE’s
performance for such a configuration in (Table 6).
Results shown in Table 7 confirm our initial hy-
</bodyText>
<page confidence="0.975299">
90
</page>
<table confidence="0.974482666666666">
POS acc Lemma acc Joint acc
FTB-UC 97.34 98.12 96.26
S.PTB 96.15 99.04 96.07
</table>
<tableCaption confidence="0.989505">
Table 6: PARSEVAL Pos tagging accuracy of treebanks
test set
</tableCaption>
<bodyText confidence="0.999508625">
pothesis for French. Indeed, parsing performance
peaks with a setup involving automatically gener-
ated lemma and POS pairs, even though the differ-
ence with raw words+auto POS is not statistically
significant for the PARSEVAL F1 metric1. Note that
parser POS accuracy does not follow this pattern. It
is unclear exactly why this is the case. We specu-
late that the parser is helped by the reduced lexicon
but that performance suffers when a &lt;lemma,POS&gt;
pair has been incorrectly assigned by MORFETTE,
leading to an increase in unseen tokens. This is con-
firmed by parsing the same lemma but with gold
POS. In that case, parsing performance does not suf-
fer too much from CHARNIAK’s POS guessing on
unseen data.
For the S.PTB, results clearly show that both the
automatic &lt;lemma,POS&gt; and &lt;word,POS&gt; config-
urations lead to very similar results (yet statistically
significant with a F1 p-value = 0.027); having the
same POS accuracy indicates that most of the work
is done at the level of POS guessing for unseen
tokens, and in this respect the CHARNIAK parser
clearly takes advantage of the information included
in the PTB tag set.
</bodyText>
<construct confidence="0.942326083333333">
S.PTB
auto lemma only
auto lemma+auto pos (a)
word +auto pos (b)
Fl p-value: (a) and (b)
auto lemma+gold pos
FTB-UC
auto lemma only
auto lemma+auto pos (c)
word +auto pos (d)
Fl p-value: (c) and (d)
auto lemma+gold pos
</construct>
<tableCaption confidence="0.994464">
Table 7: Realistic evaluation of parsing performance
</tableCaption>
<footnote confidence="0.981361666666667">
1Statistical significance is computed using Dan Bikel’s
stratified shuffling implementation: www.cis.upenn.edu/
~dbikel/software.html.
</footnote>
<sectionHeader confidence="0.995633" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.99969325">
When we started this work, we wanted to explore
the benefit of lemmatization as a means to reduce
data sparseness issues underlying statistical lexical-
ized parsing of small treebanks for morphologically
rich languages, such as the FTB. We showed that
the expected benefit of lemmatization, a less sparse
lexicon, was in fact hidden by the absence of inflec-
tional information, as required by e.g. the CHAR-
NIAK parser to provide good POS guesses for un-
seen words. Even the inclusion of POS tags gen-
erated by a state-of-the-art tagger (MORFETTE) did
not lead to much improvement compared to a parser
run in a regular bare word set up.
An unexpected effect is that the POS accuracy
of the parser trained on the French data does not
reach the same level of performance as our tag-
ger (96.47% for &lt;word, auto POS&gt; vs. 97.34% for
MORFETTE). Of course, extending the CHARNIAK
tagging model to cope with lemmatized input should
be enough, because its POS guessing model builds
on features such as capitalization, hyphenation and
a two-letter suffix (Charniak, 2000). Those features
are not present in our current lemmatized input and
thus cannot be properly estimated.
CHARNIAK also uses the probability that a given
POS is realized by a previously unobserved word.
If any part of a &lt;lemma,POS&gt; pair is incorrect, the
number of unseen words in the test set would be
higher than the one estimated from the training set,
which only contained correct lemmas and POS tags
in our setting. This would lead to unsatisfying POS
accuracy. This inadequate behavior of the unknown
word tagging model may be responsible for the POS
accuracy result for &lt;auto lemma&gt; (cf. Table 7, lines
&lt;auto lemma only&gt; for both treebanks).
We believe that this performance degradation (or
in this case the somewhat less than expected im-
provement in parsing results) calls for the inclusion
of all available lexical information in the parsing
model. For example, nothing prevents a parsing
model to condition the generation of a head upon
a lemma, while the probability to generate a POS
would depend on both morphological features and
(potentially) the supplied POS.
</bodyText>
<table confidence="0.999743272727273">
Fl score Pos acc. leaf-Anc.
87.11 89.82 94.71
88.15 96.21 94.85
88.28 96.21 94.88
0.027
89.51 99.96 95,36
83.92 92.98 93.53
85.06 96.04 94.14
84.99 96.47 94.09
0.247
86.39 97.35 94.68
</table>
<page confidence="0.996239">
91
</page>
<sectionHeader confidence="0.999966" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999948487804878">
A fair amount of recent research in parsing morpho-
logically rich languages has focused on coping with
unknowns words and more generally with the small
and limited lexicons acquired from treebanks. For
instance, Goldberg et al. (2009) augment the lex-
icon for a generative parsing model by including
lexical probabilities coming from an external lexi-
con. These are estimated using an HMM tagger with
Baum-Welch training. This method leads to a sig-
nificant increase of parsing performance over pre-
viously reported results for Modern Hebrew. Our
method is more stratified: external lexical resources
are included as features for MORFETTE and there-
fore are not directly seen by the parser besides gen-
erated lemma and POS.
For parsing German, Versley and Rehbein (2009)
cluster words according to linear context features.
The clusters are then integrated as features to boost a
discriminative parsing model to cope with unknown
words. Interestingly, they also include all possible
information: valence information, extracted from a
lexicon, is added to verbs and preterminal nodes are
annotated with case/number. This leads their dis-
criminative model to state-of-the-art results for pars-
ing German.
Concerning French, Candito and Crabbé (2009)
present the results of different clustering methods
applied to the parsing of FTB with the BKY parser.
They applied an unsupervised clustering algorithm
on the 125 millions words “Est Republicain” corpus
to get a reduced lexicon of 1000 clusters which they
then augmented with various features such as capi-
talization and suffixes. Their method is the best cur-
rent approach for the probabilistic parsing of French
with a F1 score (&lt;=40) of 88.29% on the standard
test set. We run the CHARNIAK parser on their clus-
terized corpus. Table 8 summarizes the current state-
of-the-art for lexicalized parsing on the FTB-UC.2
Clearly, the approach consisting in extending clus-
ters with features and suffixes seems to improve
CHARNIAK’s performance more than our method.
</bodyText>
<footnote confidence="0.892714">
2For this comparison, we also trained the CHARNIAK parser
on a disinflected variation of the FTB-UC. Disinflection is a de-
terministic, lexicon based process, standing between stemming
and lemmatization, which preserves POS assignment ambigui-
ties (Candito and Crabbé, 2009).
</footnote>
<bodyText confidence="0.998792857142857">
In that case, the lexicon is drastically reduced, as
well as the amount of out of vocabulary words
(OOVs). Nevertheless, the relatively low POS ac-
curacy, with only 36 OOVs, for this configuration
confirms that POS guessing is the current bottleneck
if a process of reducing the lexicon increases POS
assignment ambiguities.
</bodyText>
<table confidence="0.9987104">
Fl Pos acc % of OOVs
84.96 96.26 4.89
85.06 96.04 6.47
85.45 96.51 3.59
85.51 96.89 0.10
</table>
<tableCaption confidence="0.96317525">
Table 8: CHARNIAK parser performance summary on the
FTB-UC test set (36340 tokens). Compared to (a), all Fl re-
sults, but (b), are statistically significant (P-values &lt; 0.05), dif-
ferences between (c) &amp; (d), (b) &amp; (c) and (b) &amp; (d) are not
(P-values are resp. 0.12, 0.41 and 0.11). Note that the (b) &amp;
(d) P-value for all sentences is of 0.034, correlating thus the
observed gap in parsing performance between these two con-
figuration.
</tableCaption>
<sectionHeader confidence="0.997857" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999793125">
We showed that while lemmatization can be of
some benefit to reduce lexicon size and remedy data
sparseness for a MRL such as French, the key factor
that drives parsing performance for the CHARNIAK
parser is the amount of unseen words resulting from
the generation of &lt;lemma,POS&gt; pairs for the FTB-
UC. For a sample of the English PTB, morphologi-
cal analysis did not produce any significant improve-
ment.
Finally, even if this architecture has the potential to
help out-of-domain parsing, adding morphological
analysis on top of an existing highly tuned statisti-
cal parsing system can result in suboptimal perfor-
mance. Thus, in future we will investigate tighter
integration of the morphological features with the
parsing model.
</bodyText>
<sectionHeader confidence="0.999104" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.559430166666667">
D. Seddah and M. Candito were supported by the ANR
Sequoia (ANR-08-EMER-013); Ö. Çetino˘glu and J.
van Genabith by the Science Foundation Ireland (Grant
07/CE/I1142) as part of the Centre for Next Generation
Localisation at Dublin City University; G. Chrupała by
BMBF project NL-Search (contract 01IS08020B).
</bodyText>
<figure confidence="0.9010526">
tokens
raw word (a)
auto &lt;lemma,pos&gt; (b)
disinflected (c)
cluster+caps+suffixes (d)
</figure>
<page confidence="0.931387">
92
</page>
<sectionHeader confidence="0.991967" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999923">
Anne Abeill6, Lionel Cl6ment, and François Toussenel,
2003. Building a Treebank for French. Kluwer, Dor-
drecht.
Miriam Butt, María-Eugenia Niño, and Fr6d6rique
Segond. 1999. A Grammar Writer’s Cookbook. CSLI
Publications, Stanford, CA.
Aoife Cahill, Michael Burke, Ruth O’Donovan, Josef
van Genabith, and Andy Way. 2004. Long-Distance
Dependency Resolution in Automatically Acquired
Wide-Coverage PCFG-Based LFG Approximations.
In Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics, pages 320–
327, Barcelona, Spain.
Marie Candito and Benoît Crabb6. 2009. Im-
proving generative statistical parsing with semi-
supervised word clustering. In Proceedings of the
11th International Conference on Parsing Technolo-
gies (IWPT’09), pages 138–141, Paris, France, Octo-
ber. Association for Computational Linguistics.
Marie Candito, Benoit Crabb6, and Djam6 Seddah. 2009.
On statistical parsing of french with supervised and
semi-supervised strategies. In EACL 2009 Workshop
Grammatical inference for Computational Linguistics,
Athens, Greece.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of the First Annual Meeting
of the North American Chapter of the Association for
Computational Linguistics (NAACL 2000), pages 132–
139, Seattle, WA.
Grzegorz Chrupała, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with morfette. In
In Proceedings of LREC 2008, Marrakech, Morocco.
ELDA/ELRA.
Grzegorz Chrupała. 2008. Towards a machine-learning
architecture for lexical functional grammar parsing.
Ph.D. thesis, Dublin City University.
Grzegorz Chrupała. 2010. Morfette: A tool for su-
pervised learning of morphology. http://sites.
google.com/site/morfetteweb/. Version
0.3.1.
Michael Collins. 1999. Head Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia.
Benoit Crabb6 and Marie Candito. 2008. Exp6riences
d’analyse syntaxique statistique du français. In Actes
de la 15ème Conférence sur le Traitement Automatique
des Langues Naturelles (TALN’08), pages 45–54, Avi-
gnon, France.
Pascal Denis and Benoît Sagot. 2009. Coupling an anno-
tated corpus and a morphosyntactic lexicon for state-
of-the-art pos tagging with less human effort. In Proc.
ofPACLIC, Hong Kong, China.
Christy Doran, Dania Egedi, Beth Ann Hockey, B. Srini-
vas, and Martin Zaidel. 1994. Xtag system: A wide
coverage grammar for english. In Proceedings of the
15th conference on Computational linguistics, pages
922–928, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Yoav Freund and Robert E. Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. Ma-
chine learning, 37(3):277–296.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and EM-HMM-based lexical probabilities.
In Proc. of EACL-09, pages 327–335, Athens, Greece.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of English: The Penn TreeBank. Computational
Linguistics, 19(2):313–330.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Ines Rehbein and Josef van Genabith. 2007. Treebank
annotation schemes and parser evaluation for german.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), Prague.
Benoit Sagot, Lionel Cl6ment, Eric V. de La Clergerie,
and Pierre Boullier. 2006. The lefff 2 syntactic lexi-
con for french: Architecture, acquisition, use. Proc. of
LREC 06, Genoa, Italy.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365–380.
Natalie Schluter and Josef van Genabith. 2007. Prepar-
ing, restructuring, and augmenting a French Treebank:
Lexicalised parsers or coherent treebanks? In Proc. of
PACLING 07, Melbourne, Australia.
Djam6 Seddah, Marie Candito, and Benoit Crabb6. 2009.
Cross parser evaluation and tagset variation: A French
Treebank study. In Proceedings of the 11th Interna-
tion Conference on Parsing Technologies (IWPT’09),
pages 150–161, Paris, France, October. Association
for Computational Linguistics.
Yannick Versley and Ines Rehbein. 2009. Scalable dis-
criminative parsing for german. In Proceedings of the
11th International Conference on Parsing Technolo-
gies (IWPT’09), pages 134–137, Paris, France, Octo-
ber. Association for Computational Linguistics.
</reference>
<page confidence="0.999173">
93
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.158026">
<title confidence="0.9726745">Lemmatization and Lexicalized Statistical Parsing of Morphologically Languages: the Case of French</title>
<author confidence="0.78118">Djamé Seddah Grzegorz Chrupała</author>
<affiliation confidence="0.941726">Alpage Inria &amp; Univ. Paris-Sorbonne Spoken Language System, Saarland Univ.</affiliation>
<address confidence="0.843814">Paris, France Saarbrücken, Germany</address>
<author confidence="0.516285">Çetino˘glu van_Genabith Marie Candito</author>
<affiliation confidence="0.278638">NCLT &amp; CNGL, Dublin City Univ. Alpage Inria &amp; Univ. Paris 7</affiliation>
<address confidence="0.352015">Dublin, Ireland Paris, France</address>
<abstract confidence="0.999247266666667">This paper shows that training a lexicalized parser on a lemmatized morphologically-rich treebank such as the French Treebank slightly improves parsing results. We also show that lemmatizing a similar in size subset of the English Penn Treebank has almost no effect on parsing performance with gold lemmas and leads to a small drop of performance when automatically assigned lemmas and POS tags are used. This highlights two facts: (i) lemmatization helps to reduce lexicon data-sparseness issues for French, (ii) it also makes the parsing process sensitive to correct assignment of POS tags to unknown words.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anne Abeill6</author>
</authors>
<title>Lionel Cl6ment, and François Toussenel,</title>
<date>2003</date>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<marker>Abeill6, 2003</marker>
<rawString>Anne Abeill6, Lionel Cl6ment, and François Toussenel, 2003. Building a Treebank for French. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miriam Butt</author>
</authors>
<title>María-Eugenia Niño, and Fr6d6rique Segond.</title>
<date>1999</date>
<publisher>CSLI Publications,</publisher>
<location>Stanford, CA.</location>
<marker>Butt, 1999</marker>
<rawString>Miriam Butt, María-Eugenia Niño, and Fr6d6rique Segond. 1999. A Grammar Writer’s Cookbook. CSLI Publications, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Michael Burke</author>
<author>Ruth O’Donovan</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Long-Distance Dependency Resolution in Automatically Acquired Wide-Coverage PCFG-Based LFG Approximations.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>320--327</pages>
<location>Barcelona,</location>
<marker>Cahill, Burke, O’Donovan, van Genabith, Way, 2004</marker>
<rawString>Aoife Cahill, Michael Burke, Ruth O’Donovan, Josef van Genabith, and Andy Way. 2004. Long-Distance Dependency Resolution in Automatically Acquired Wide-Coverage PCFG-Based LFG Approximations. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 320– 327, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie Candito</author>
<author>Benoît Crabb6</author>
</authors>
<title>Improving generative statistical parsing with semisupervised word clustering.</title>
<date>2009</date>
<booktitle>In Proceedings of the 11th International Conference on Parsing Technologies (IWPT’09),</booktitle>
<pages>138--141</pages>
<location>Paris, France,</location>
<marker>Candito, Crabb6, 2009</marker>
<rawString>Marie Candito and Benoît Crabb6. 2009. Improving generative statistical parsing with semisupervised word clustering. In Proceedings of the 11th International Conference on Parsing Technologies (IWPT’09), pages 138–141, Paris, France, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie Candito</author>
<author>Benoit Crabb6</author>
<author>Djam6 Seddah</author>
</authors>
<title>On statistical parsing of french with supervised and semi-supervised strategies.</title>
<date>2009</date>
<booktitle>In EACL 2009 Workshop Grammatical inference for Computational Linguistics,</booktitle>
<location>Athens, Greece.</location>
<marker>Candito, Crabb6, Seddah, 2009</marker>
<rawString>Marie Candito, Benoit Crabb6, and Djam6 Seddah. 2009. On statistical parsing of french with supervised and semi-supervised strategies. In EACL 2009 Workshop Grammatical inference for Computational Linguistics, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum entropy inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the First Annual Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL</booktitle>
<pages>132--139</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="4891" citStr="Charniak, 2000" startWordPosition="763" endWordPosition="764"> corrections (referred to as the Modified French Treebank MFT) to support grammar acquisition for PCFG-based LFG Parsing (Cahill et al., 2004) while Crabbé and Candito (2008) slightly modified the original FTB POS tagset to optimize the grammar with latent annotations extracted by the Berkeley parser (BKY, (Petrov et al., 2006)). Moreover, research oriented towards adapting more complex parsing models to French showed that lexicalized models such as Collins’ model 2 (Collins, 1999) can be tuned to cope effectively with the flatness of the annotation scheme in the FTB, with the Charniak model (Charniak, 2000) performing particularly well, but outperformed by the BKY parser on French data (Seddah et al., 2009). Focusing on the lexicon, experiments have been carried out to study the impact of different forms of word clustering on the BKY parser trained on the FTB. Candito et al. (2009) showed that using gold lemmatization provides a significant increase in performance. Obviously, less sparse lexical data which retains critical pieces of information can only help a model to perform better. This was shown in (Candito and Crabbé, 2009) where distributional word clusters were acquired from a 125 million</context>
<context position="17931" citStr="Charniak, 2000" startWordPosition="2953" endWordPosition="2954">iments to evaluate the impact of lemmatization on the lexicalized statistical parsing of two languages, one morphologically rich (French), but with none of its morphological features exploited by the CHARNIAK parser, the other (English) being quite the opposite, with the parser developed mainly for this language and PTB annotated data. We show that lemmatization results in increased performance for French, while doing the same for English penalizes parser performance. 4.1 Experimental Protocol Data The data sets described in section 3.2 are used throughout. The version of the CHARNIAK parser (Charniak, 2000) was released in August 2005 and recently adapted to French (Seddah et al., 2009). Metrics We report results on sentences of length less than 40 words, with three evaluation metrics: the classical PARSEVAL Labeled brackets Fi score, POS tagging accuracy (excluding punctuation tags) and the Leaf Ancestor metric (Sampson and Babarczy, 2003) which is believed to be somewhat more neutral with respect to the treebank annotation scheme than PARSEVAL (Rehbein and van Genabith, 2007). Treebank tag sets Our experiments involve the inclusion of POS tags directly in tokens. We briefly describe our treeba</context>
<context position="27234" citStr="Charniak, 2000" startWordPosition="4538" endWordPosition="4539">ses for unseen words. Even the inclusion of POS tags generated by a state-of-the-art tagger (MORFETTE) did not lead to much improvement compared to a parser run in a regular bare word set up. An unexpected effect is that the POS accuracy of the parser trained on the French data does not reach the same level of performance as our tagger (96.47% for &lt;word, auto POS&gt; vs. 97.34% for MORFETTE). Of course, extending the CHARNIAK tagging model to cope with lemmatized input should be enough, because its POS guessing model builds on features such as capitalization, hyphenation and a two-letter suffix (Charniak, 2000). Those features are not present in our current lemmatized input and thus cannot be properly estimated. CHARNIAK also uses the probability that a given POS is realized by a previously unobserved word. If any part of a &lt;lemma,POS&gt; pair is incorrect, the number of unseen words in the test set would be higher than the one estimated from the training set, which only contained correct lemmas and POS tags in our setting. This would lead to unsatisfying POS accuracy. This inadequate behavior of the unknown word tagging model may be responsible for the POS accuracy result for &lt;auto lemma&gt; (cf. Table 7</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum entropy inspired parser. In Proceedings of the First Annual Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL 2000), pages 132– 139, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Chrupała</author>
<author>Georgiana Dinu</author>
<author>Josef van Genabith</author>
</authors>
<title>Learning morphology with morfette. In</title>
<date>2008</date>
<booktitle>In Proceedings of LREC 2008,</booktitle>
<location>Marrakech, Morocco. ELDA/ELRA.</location>
<marker>Chrupała, Dinu, van Genabith, 2008</marker>
<rawString>Grzegorz Chrupała, Georgiana Dinu, and Josef van Genabith. 2008. Learning morphology with morfette. In In Proceedings of LREC 2008, Marrakech, Morocco. ELDA/ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Chrupała</author>
</authors>
<title>Towards a machine-learning architecture for lexical functional grammar parsing.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>Dublin City University.</institution>
<contexts>
<context position="13983" citStr="Chrupała, 2008" startWordPosition="2268" endWordPosition="2269">, x) is the probability of lemma class l at position i according to the lemma classifier, PM(mi|x) is the probability of the tag m at position i according to the morphological tag classifier, and x is the sequence of words to label. While Chrupała et al. (2008) use Maximum Entropy training to learn PM and PL, here we learn them using Averaged Perceptron algorithm due to Freund and Schapire (1999). It is a much simpler algorithm which in many scenarios (including ours) performs as well as or better than MaxEnt. We also use the general Edit Tree instantiation of the edit script as developed in (Chrupała, 2008). We find the longest common substring (LCS) between the form w and the lemma w′. The portions of the string in the word form before (prefix) and after (suffix) the LCS need to be modified in some way, while the LCS (stem) stays the same. If there is no LCS, then we simply record that we need to replace w with w′ . As for the modifications to the prefix and the suffix, we apply the same procedure recursively: we try to find the LCS between the prefix of w and the prefix of w′. If we find one, we recurse; if we do not, we record the replacement; we do the same for the suffix. 3.2 Data Set We tr</context>
</contexts>
<marker>Chrupała, 2008</marker>
<rawString>Grzegorz Chrupała. 2008. Towards a machine-learning architecture for lexical functional grammar parsing. Ph.D. thesis, Dublin City University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Chrupała</author>
</authors>
<title>Morfette: A tool for supervised learning of morphology.</title>
<date>2010</date>
<journal>http://sites. google.com/site/morfetteweb/. Version</journal>
<volume>0</volume>
<contexts>
<context position="6645" citStr="Chrupała, 2010" startWordPosition="1047" endWordPosition="1048">ering is in fact too drastic for lexicalized parsers as it may give rise to head-to-head dependencies which are too coarse. To answer this question, in this paper we explore the impact of lemmatization, as a (rather limited) constrained form of clustering, on a stateof-the-art lexicalized parser (CHARNIAK). In order to evaluate the influence of lemmatization on this parser (which is known to be highly tuned for English) we carry out experiments on both the FTB and on a lemmatized version of the PTB. We used gold lemmatization when available and an automatic statistical morphological analyzer (Chrupała, 2010) to provide more realistic parsing results. The idea is to verify whether lemmatization will help to reduce data sparseness issues due to the French rich morphology and to see if this process, when applied to English will harm the performance of a parser optimized for the limited morphology of English. Our results show that the key issue is the way unseen tokens (lemmas or words) are handled by the CHARNIAK parser. Indeed, using pure lemma is equally suboptimal for both languages. On the other hand, feeding the parser with both lemma and partof-speech slightly enhances parsing performance for </context>
<context position="12479" citStr="Chrupała, 2010" startWordPosition="2026" endWordPosition="2027">shows that the average number of word forms associated with a lemma (i.e. the lemma ratio) is higher in the FTB-UC (1.54 words/lemma) than in the PTB (1.31). Even though the PTB ratio is lower, it is still large enough to suggest that even the limited English morphology should be taken into account when aiming at reducing lexicon sparseness. Trying to learn French and English morphology in a data driven fashion in order to predict lemma from word forms is the subject of the next section. 3 Morphology learning In order to assign morphological tags and lemmas to words we use the MORFETTE model (Chrupała, 2010), which is a variation of the approach described in (Chrupała et al., 2008). 87 MORFETTE is a sequence labeling model which combines the predictions of two classification models (one for morphological tagging and one for lemmatization) at decoding time, using beam search. 3.1 Overview of the Morfette model The morphological classes correspond simply to the (fine-grained) POS tags. Lemma classes are edit scripts computed from training data: they specify which string manipulations (such as character deletions and insertions) need to be performed in order to transform the input string (word form)</context>
</contexts>
<marker>Chrupała, 2010</marker>
<rawString>Grzegorz Chrupała. 2010. Morfette: A tool for supervised learning of morphology. http://sites. google.com/site/morfetteweb/. Version 0.3.1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia.</location>
<contexts>
<context position="4762" citStr="Collins, 1999" startWordPosition="741" endWordPosition="742">nd van Genabith (2007) extracted a subset of an early release of the FTB and carried out extensive restructuring, extensions and corrections (referred to as the Modified French Treebank MFT) to support grammar acquisition for PCFG-based LFG Parsing (Cahill et al., 2004) while Crabbé and Candito (2008) slightly modified the original FTB POS tagset to optimize the grammar with latent annotations extracted by the Berkeley parser (BKY, (Petrov et al., 2006)). Moreover, research oriented towards adapting more complex parsing models to French showed that lexicalized models such as Collins’ model 2 (Collins, 1999) can be tuned to cope effectively with the flatness of the annotation scheme in the FTB, with the Charniak model (Charniak, 2000) performing particularly well, but outperformed by the BKY parser on French data (Seddah et al., 2009). Focusing on the lexicon, experiments have been carried out to study the impact of different forms of word clustering on the BKY parser trained on the FTB. Candito et al. (2009) showed that using gold lemmatization provides a significant increase in performance. Obviously, less sparse lexical data which retains critical pieces of information can only help a model to</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Crabb6</author>
<author>Marie Candito</author>
</authors>
<title>Exp6riences d’analyse syntaxique statistique du français.</title>
<date>2008</date>
<booktitle>In Actes de la 15ème Conférence sur le Traitement Automatique des Langues Naturelles (TALN’08),</booktitle>
<pages>45--54</pages>
<location>Avignon, France.</location>
<marker>Crabb6, Candito, 2008</marker>
<rawString>Benoit Crabb6 and Marie Candito. 2008. Exp6riences d’analyse syntaxique statistique du français. In Actes de la 15ème Conférence sur le Traitement Automatique des Langues Naturelles (TALN’08), pages 45–54, Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Benoît Sagot</author>
</authors>
<title>Coupling an annotated corpus and a morphosyntactic lexicon for stateof-the-art pos tagging with less human effort.</title>
<date>2009</date>
<booktitle>In Proc. ofPACLIC,</booktitle>
<location>Hong Kong, China.</location>
<contexts>
<context position="15982" citStr="Denis and Sagot (2009)" startWordPosition="2624" endWordPosition="2627"> the extended version of Leff (Sagot et al., 2006) and for English, the lexicon used in the Penn XTAG project (Doran et al., 1994). We reduced the granularity of the XTAG tag set, keeping only the bare categories. Both lexicons contain around 225 thousands word form entries. 3.3 Performance on French and English Table 2 presents results of MORFETTE applied to the development and test sets of our treebanks. Part-ofspeech tagging performance for French is state-ofthe-art on the FTB-UC, with an accuracy of 97.68%, on the FTB-UC test set, only 0.02 points (absolute) below the MaxEnt POS tagger of Denis and Sagot (2009). Comparing MORFETTE’s tagging performance for English is a bit more challenging as we only trained on one third of the full PTB and evaluated on approximately one section, whereas results reported in the literature are usually based on training on sections 02-18 and evaluating on either sections 19-21 or 22-24. For this setting, state-of-theart POS accuracy for PTB tagging is around 97.33%. On our PTB sample, MORFETTE achieves 96.36% for all words and 89.64 for unseen words. Comparing the lemmatization performance for both languages on the same kind of data is even more difficult as we are no</context>
</contexts>
<marker>Denis, Sagot, 2009</marker>
<rawString>Pascal Denis and Benoît Sagot. 2009. Coupling an annotated corpus and a morphosyntactic lexicon for stateof-the-art pos tagging with less human effort. In Proc. ofPACLIC, Hong Kong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christy Doran</author>
<author>Dania Egedi</author>
<author>Beth Ann Hockey</author>
<author>B Srinivas</author>
<author>Martin Zaidel</author>
</authors>
<title>Xtag system: A wide coverage grammar for english.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th conference on Computational linguistics,</booktitle>
<pages>922--928</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="15490" citStr="Doran et al., 1994" startWordPosition="2543" endWordPosition="2546">PTB’s lemmas have been automatically generated by a deterministic process, and only a random subset of them have been manually checked. For the remainder of this paper, we treat them as gold, regardless of the errors induced by our PTB lemmatizer. The S.PTB follows the same split as the FTB-UC, first 10% for test, next 10% for dev and the last 80% for training (i.e. 1380/1381/11050 sentences). MORFETTE can optionally use a morphological lexicon to extract features. For French, we used the extended version of Leff (Sagot et al., 2006) and for English, the lexicon used in the Penn XTAG project (Doran et al., 1994). We reduced the granularity of the XTAG tag set, keeping only the bare categories. Both lexicons contain around 225 thousands word form entries. 3.3 Performance on French and English Table 2 presents results of MORFETTE applied to the development and test sets of our treebanks. Part-ofspeech tagging performance for French is state-ofthe-art on the FTB-UC, with an accuracy of 97.68%, on the FTB-UC test set, only 0.02 points (absolute) below the MaxEnt POS tagger of Denis and Sagot (2009). Comparing MORFETTE’s tagging performance for English is a bit more challenging as we only trained on one t</context>
</contexts>
<marker>Doran, Egedi, Hockey, Srinivas, Zaidel, 1994</marker>
<rawString>Christy Doran, Dania Egedi, Beth Ann Hockey, B. Srinivas, and Martin Zaidel. 1994. Xtag system: A wide coverage grammar for english. In Proceedings of the 15th conference on Computational linguistics, pages 922–928, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine learning,</booktitle>
<pages>37--3</pages>
<contexts>
<context position="13767" citStr="Freund and Schapire (1999)" startWordPosition="2229" endWordPosition="2232">ce of lemmas and morphological tags for input sentence x is defined as: (�l, m) = arg max (l,M) The joint probability is decomposed as follows: P(l0...li,m0...mi|x) =PL(li|mi,x)PM(mi|x) × P(m0...mi−1,l0...li−1|x) where PL(li|mi, x) is the probability of lemma class l at position i according to the lemma classifier, PM(mi|x) is the probability of the tag m at position i according to the morphological tag classifier, and x is the sequence of words to label. While Chrupała et al. (2008) use Maximum Entropy training to learn PM and PL, here we learn them using Averaged Perceptron algorithm due to Freund and Schapire (1999). It is a much simpler algorithm which in many scenarios (including ours) performs as well as or better than MaxEnt. We also use the general Edit Tree instantiation of the edit script as developed in (Chrupała, 2008). We find the longest common substring (LCS) between the form w and the lemma w′. The portions of the string in the word form before (prefix) and after (suffix) the LCS need to be modified in some way, while the LCS (stem) stays the same. If there is no LCS, then we simply record that we need to replace w with w′ . As for the modifications to the prefix and the suffix, we apply the</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine learning, 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Reut Tsarfaty</author>
<author>Meni Adler</author>
<author>Michael Elhadad</author>
</authors>
<title>Enhancing unlexicalized parsing performance using a wide coverage lexicon, fuzzy tag-set mapping, and EM-HMM-based lexical probabilities.</title>
<date>2009</date>
<booktitle>In Proc. of EACL-09,</booktitle>
<pages>327--335</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="28743" citStr="Goldberg et al. (2009)" startWordPosition="4785" endWordPosition="4788"> parsing model to condition the generation of a head upon a lemma, while the probability to generate a POS would depend on both morphological features and (potentially) the supplied POS. Fl score Pos acc. leaf-Anc. 87.11 89.82 94.71 88.15 96.21 94.85 88.28 96.21 94.88 0.027 89.51 99.96 95,36 83.92 92.98 93.53 85.06 96.04 94.14 84.99 96.47 94.09 0.247 86.39 97.35 94.68 91 6 Related Work A fair amount of recent research in parsing morphologically rich languages has focused on coping with unknowns words and more generally with the small and limited lexicons acquired from treebanks. For instance, Goldberg et al. (2009) augment the lexicon for a generative parsing model by including lexical probabilities coming from an external lexicon. These are estimated using an HMM tagger with Baum-Welch training. This method leads to a significant increase of parsing performance over previously reported results for Modern Hebrew. Our method is more stratified: external lexical resources are included as features for MORFETTE and therefore are not directly seen by the parser besides generated lemma and POS. For parsing German, Versley and Rehbein (2009) cluster words according to linear context features. The clusters are </context>
</contexts>
<marker>Goldberg, Tsarfaty, Adler, Elhadad, 2009</marker>
<rawString>Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael Elhadad. 2009. Enhancing unlexicalized parsing performance using a wide coverage lexicon, fuzzy tag-set mapping, and EM-HMM-based lexical probabilities. In Proc. of EACL-09, pages 327–335, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn TreeBank. Computational Linguistics,</title>
<date>1994</date>
<contexts>
<context position="1267" citStr="Marcus et al., 1994" startWordPosition="191" endWordPosition="194">almost no effect on parsing performance with gold lemmas and leads to a small drop of performance when automatically assigned lemmas and POS tags are used. This highlights two facts: (i) lemmatization helps to reduce lexicon data-sparseness issues for French, (ii) it also makes the parsing process sensitive to correct assignment of POS tags to unknown words. 1 Introduction Large parse-annotated corpora have led to an explosion of interest in statistical parsing methods, including the development of highly successful models for parsing English using the Wall Street Journal Penn Treebank (PTB, (Marcus et al., 1994)). Over the last 10 years, parsing performance on the PTB has hit a performance plateau of 90-92% f-score using the PARSEVAL evaluation metric. When adapted to other language/treebank pairs (such as German, Hebrew, Arabic, Italian or French), to date these models have performed much worse. A number of arguments have been advanced to explain this performance gap, including limited amounts of training data, differences in treebank annotation schemes, inadequacies of evaluation metrics, linguistic factors such as the degree of word order variation, the amount of morphological information availabl</context>
<context position="19162" citStr="Marcus et al., 1994" startWordPosition="3176" endWordPosition="3179"> below. • FTB-UC TAG SET: “CC” This is the tag set developed by (Crabbé and Candito, 2008) (Table 4), known to provide the best parsing performance for French (Seddah et al., 2009). Like in the FTB, preterminals are the main categories, but they are also augmented with a WH flag for A, ADV, PRO and with the mood for verbs (there are 6 moods). No information is propagated to non-terminal symbols. ADJ ADJWH ADV ADVWH CC CLO CLR CLS CS DET DETWH ET I NC NPP P P+D P+PRO PONCT PREF PRO PROREL PROWH V VIMP VINF VPP VPR VS Table 4: CC tag set • THE PTB TAG SET This tag set is described at length in (Marcus et al., 1994) and contains supplementary morphological information (e.g. number) over and above what is represented in the CC tag set for French. Note that some information is marked at the morphological level in English (superlative, “the greatest (JJS)”) and not in French (“ le plus (ADV) grand (ADJ)”). CC CD DT EX FW IN JJ JJR JJS LS MD NN NNP NNPS NNS PDT POS PRP PRP$ RB RBR RBS RP SYM TO UH VB VBD VBG VBN VBP VBZ WDT WP WP$ WRB Table 5: PTB tag set 4.2 Cross token variation and parsing impact From the source treebanks, we produce 5 versions of tokens: tokens are generated as either simple POS tag, gol</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1994. Building a large annotated corpus of English: The Penn TreeBank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="4605" citStr="Petrov et al., 2006" startWordPosition="717" endWordPosition="720">rse lexicon. So far, both approaches have been tested for French using different means and with different degrees of success. To obtain better grammars, Schluter and van Genabith (2007) extracted a subset of an early release of the FTB and carried out extensive restructuring, extensions and corrections (referred to as the Modified French Treebank MFT) to support grammar acquisition for PCFG-based LFG Parsing (Cahill et al., 2004) while Crabbé and Candito (2008) slightly modified the original FTB POS tagset to optimize the grammar with latent annotations extracted by the Berkeley parser (BKY, (Petrov et al., 2006)). Moreover, research oriented towards adapting more complex parsing models to French showed that lexicalized models such as Collins’ model 2 (Collins, 1999) can be tuned to cope effectively with the flatness of the annotation scheme in the FTB, with the Charniak model (Charniak, 2000) performing particularly well, but outperformed by the BKY parser on French data (Seddah et al., 2009). Focusing on the lexicon, experiments have been carried out to study the impact of different forms of word clustering on the BKY parser trained on the FTB. Candito et al. (2009) showed that using gold lemmatizat</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ines Rehbein</author>
<author>Josef van Genabith</author>
</authors>
<title>Treebank annotation schemes and parser evaluation for german.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<location>Prague.</location>
<marker>Rehbein, van Genabith, 2007</marker>
<rawString>Ines Rehbein and Josef van Genabith. 2007. Treebank annotation schemes and parser evaluation for german. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Sagot</author>
<author>Lionel Cl6ment</author>
<author>Eric V de La Clergerie</author>
<author>Pierre Boullier</author>
</authors>
<title>The lefff 2 syntactic lexicon for french: Architecture, acquisition, use.</title>
<date>2006</date>
<booktitle>Proc. of LREC 06,</booktitle>
<location>Genoa, Italy.</location>
<marker>Sagot, Cl6ment, Clergerie, Boullier, 2006</marker>
<rawString>Benoit Sagot, Lionel Cl6ment, Eric V. de La Clergerie, and Pierre Boullier. 2006. The lefff 2 syntactic lexicon for french: Architecture, acquisition, use. Proc. of LREC 06, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Sampson</author>
<author>Anna Babarczy</author>
</authors>
<title>A test of the leaf-ancestor metric for parse accuracy.</title>
<date>2003</date>
<journal>Natural Language Engineering,</journal>
<volume>9</volume>
<issue>04</issue>
<contexts>
<context position="18271" citStr="Sampson and Babarczy, 2003" startWordPosition="3005" endWordPosition="3008">ted data. We show that lemmatization results in increased performance for French, while doing the same for English penalizes parser performance. 4.1 Experimental Protocol Data The data sets described in section 3.2 are used throughout. The version of the CHARNIAK parser (Charniak, 2000) was released in August 2005 and recently adapted to French (Seddah et al., 2009). Metrics We report results on sentences of length less than 40 words, with three evaluation metrics: the classical PARSEVAL Labeled brackets Fi score, POS tagging accuracy (excluding punctuation tags) and the Leaf Ancestor metric (Sampson and Babarczy, 2003) which is believed to be somewhat more neutral with respect to the treebank annotation scheme than PARSEVAL (Rehbein and van Genabith, 2007). Treebank tag sets Our experiments involve the inclusion of POS tags directly in tokens. We briefly describe our treebank tag sets below. • FTB-UC TAG SET: “CC” This is the tag set developed by (Crabbé and Candito, 2008) (Table 4), known to provide the best parsing performance for French (Seddah et al., 2009). Like in the FTB, preterminals are the main categories, but they are also augmented with a WH flag for A, ADV, PRO and with the mood for verbs (ther</context>
</contexts>
<marker>Sampson, Babarczy, 2003</marker>
<rawString>Geoffrey Sampson and Anna Babarczy. 2003. A test of the leaf-ancestor metric for parse accuracy. Natural Language Engineering, 9(04):365–380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Natalie Schluter</author>
<author>Josef van Genabith</author>
</authors>
<title>Preparing, restructuring, and augmenting a French Treebank: Lexicalised parsers or coherent treebanks?</title>
<date>2007</date>
<booktitle>In Proc. of PACLING 07,</booktitle>
<location>Melbourne, Australia.</location>
<marker>Schluter, van Genabith, 2007</marker>
<rawString>Natalie Schluter and Josef van Genabith. 2007. Preparing, restructuring, and augmenting a French Treebank: Lexicalised parsers or coherent treebanks? In Proc. of PACLING 07, Melbourne, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Djam6 Seddah</author>
<author>Marie Candito</author>
<author>Benoit Crabb6</author>
</authors>
<title>Cross parser evaluation and tagset variation: A French Treebank study.</title>
<date>2009</date>
<booktitle>In Proceedings of the 11th Internation Conference on Parsing Technologies (IWPT’09),</booktitle>
<pages>150--161</pages>
<location>Paris, France,</location>
<marker>Seddah, Candito, Crabb6, 2009</marker>
<rawString>Djam6 Seddah, Marie Candito, and Benoit Crabb6. 2009. Cross parser evaluation and tagset variation: A French Treebank study. In Proceedings of the 11th Internation Conference on Parsing Technologies (IWPT’09), pages 150–161, Paris, France, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yannick Versley</author>
<author>Ines Rehbein</author>
</authors>
<title>Scalable discriminative parsing for german.</title>
<date>2009</date>
<booktitle>In Proceedings of the 11th International Conference on Parsing Technologies (IWPT’09),</booktitle>
<pages>134--137</pages>
<location>Paris, France,</location>
<contexts>
<context position="29273" citStr="Versley and Rehbein (2009)" startWordPosition="4870" endWordPosition="4873">ith the small and limited lexicons acquired from treebanks. For instance, Goldberg et al. (2009) augment the lexicon for a generative parsing model by including lexical probabilities coming from an external lexicon. These are estimated using an HMM tagger with Baum-Welch training. This method leads to a significant increase of parsing performance over previously reported results for Modern Hebrew. Our method is more stratified: external lexical resources are included as features for MORFETTE and therefore are not directly seen by the parser besides generated lemma and POS. For parsing German, Versley and Rehbein (2009) cluster words according to linear context features. The clusters are then integrated as features to boost a discriminative parsing model to cope with unknown words. Interestingly, they also include all possible information: valence information, extracted from a lexicon, is added to verbs and preterminal nodes are annotated with case/number. This leads their discriminative model to state-of-the-art results for parsing German. Concerning French, Candito and Crabbé (2009) present the results of different clustering methods applied to the parsing of FTB with the BKY parser. They applied an unsupe</context>
</contexts>
<marker>Versley, Rehbein, 2009</marker>
<rawString>Yannick Versley and Ines Rehbein. 2009. Scalable discriminative parsing for german. In Proceedings of the 11th International Conference on Parsing Technologies (IWPT’09), pages 134–137, Paris, France, October. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>