<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005735">
<title confidence="0.997758">
Extending a Single-Document Summarizer to Multi-Document: a
Hierarchical Approach
</title>
<author confidence="0.974155">
Luis Marujo1,2,3, Ricardo Ribeiro1,4, David Martins de Matos1,2,
Jo˜ao P. Neto1,2, Anatole Gershman3, and Jaime Carbonell3
</author>
<affiliation confidence="0.9650945">
1INESC-ID Lisboa, 2IST/ULisboa, 4ISCTE-IUL, Lisboa, Portugal
3School of Computer Science, CMU, Pittsburgh, USA
</affiliation>
<email confidence="0.889189">
{lmarujo,anatoleg,jgc}@cs.cmu.edu
{ricardo.ribeiro,david.matos,joao.neto}@inesc-id.pt
</email>
<sectionHeader confidence="0.996622" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998515333333333">
The increasing amount of online content mo-
tivated the development of multi-document
summarization methods. In this work, we
explore straightforward approaches to extend
single-document summarization methods to
multi-document summarization. The pro-
posed methods are based on the hierarchical
combination of single-document summaries,
and achieves state of the art results.
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999964961538461">
The use of the Internet to fulfill generic informa-
tion needs motivated pioneer multi-document sum-
marization efforts as NewsInEssence (Radev et al.,
2005) or Newsblaster (McKeown et al., 2002), on-
line since 2001. In general, multi-document sum-
marization approaches have to address two differ-
ent problems: passage selection and information or-
dering. Current multi-document systems adopt, for
passage selection, approaches similar to the ones
used in single-document summarization, and use the
chronological order of the documents for informa-
tion ordering (Christensen et al., 2013). The prob-
lem is that most approaches fail to generate sum-
maries that cover generic topics which comprehend
different, equally important, subtopics.
We propose to extend a state-of-the-art
single-document summarization method, KP-
CENTRALITY (Ribeiro et al., 2013), capable of
focusing on diverse important topics while ignoring
unimportant ones, to perform multi-document sum-
marization. We explore two hierarchical strategies
to perform this extension.
This document is organized as follows: Sect. 2 ad-
dresses the related work; Sect. 3 presents our multi-
document summarization appproach; experimental
results close the paper.
</bodyText>
<sectionHeader confidence="0.999944" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999949321428572">
Most of the current work in automatic summariza-
tion focuses on extractive summarization. The most
popular baselines for multi-document summariza-
tion fall into one of the following general mod-
els: Centrality-based (Radev et al., 2004; Erkan
and Radev, 2004; Wang et al., 2008; Ribeiro and
de Matos, 2011), Maximal Marginal Relevance
(MMR) (Carbonell and Goldstein, 1998; Guo and
Sanner, 2010; Sanner et al., 2011; Lim et al., 2012),
and Coverage-base methods (Lin and Hovy, 2000;
Sipos et al., 2012). Additionally, methods such as
KP-CENTRALITY (Ribeiro et al., 2013), which is
centrality and coverage-based, follow more than one
paradigm. In general, Centrality-based models are
used to produce generic summaries, while the MMR
family generates query-oriented ones. Coverage-
base models produce summaries driven by words,
topics or events.
Centrality-as-relevance methods base the detec-
tion of the most salient passages on the identification
of the central passages of the input source(s). One of
the main representatives of this family is Passage-
to-Centroid Similarity-based Centrality. Centroid-
based methods build on the idea of a pseudo-passage
that represents the central topic of the input source—
the centroid—selecting as passages to be included in
the summary the ones that are close to the centroid.
Another approach to centrality estimation is to com-
</bodyText>
<page confidence="0.988825">
176
</page>
<note confidence="0.4402835">
Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 176–181,
Denver, Colorado, June 4–5, 2015.
</note>
<bodyText confidence="0.945777571428571">
pare each candidate passage to every other passage
and select the ones with higher scores (the ones that
are closer to every other passage): the Pair-wise Pas-
sage Similarity-based Centrality.
MMR (Carbonell and Goldstein, 1998) is a query
driven relevance model based on the following
mathematical model:
</bodyText>
<equation confidence="0.481143666666667">
arg max [λ(Sim1 (Si, Q))−(1−λ) (max Sim2(Si, Sj))J
Sj
Si
</equation>
<bodyText confidence="0.999897214285714">
where Sim1 and Sim2 are similarity metrics that
do not have to be different; Si are the yet unselected
passages and Sj are the previously selected ones; Q
is the required query to apply the model; and, λ is
a parameter that allows to configure the result to be
from a standard relevance-ranked list (λ = 1) to a
maximal diversity ranking (λ = 0).
Coverage-based summarization defines a set of
concepts that need to occur in the sentences selected
for the summaries. The concepts are events (Filatova
and Hatzivassiloglou, 2004), topics (Lin and Hovy,
2000), salient words (Lin and Bilmes, 2010; Sipos
et al., 2012), and word n-grams (Gillick et al., 2008;
Almeida and Martins, 2013).
</bodyText>
<sectionHeader confidence="0.996462" genericHeader="method">
3 Multi-Document Summarization
</sectionHeader>
<bodyText confidence="0.999871136363636">
Our multi-document approach is built upon a cen-
trality and coverage-based single-document summa-
rization method, KP-CENTRALITY (Ribeiro et al.,
2013). This method, through the use of key phrases,
is easily adaptable and has been shown to be robust
in the presence of noisy input. This is an important
aspect considering that using as input several docu-
ments frequently increases the amount of unimpor-
tant content).
When adapting a single-document summarization
method to perform multi-document summarization,
a possible strategy is to combine the summaries of
each document. To iteratively combine the sum-
maries, we explore two different approaches: single-
layer hierarchical and waterfall. Given that the sum-
marization method also uses as input a set of key
phrases, we extract from each input document the
required set of key phrases, join the extracted sets,
and rank the key phrases using their frequency. To
generate each summary, we use the top key phrases,
excluding the ones that do not occur in the input doc-
ument.
</bodyText>
<subsectionHeader confidence="0.999796">
3.1 Single-Document Summarization Method
</subsectionHeader>
<bodyText confidence="0.999979586206897">
To retrieve the most important sentences of an in-
formation source, we used the KP-CENTRALITY
method (Ribeiro et al., 2013). We chose this model
for its adaptability to different types of information
sources (e.g., text, audio and video), while support-
ing privacy (Marujo et al., 2014), and offering state-
of-art performance. It is based on the notion of com-
bining key phrases with support sets. A support set
is a group of the most semantically related passages.
These semantic passages are chosen using heuristics
based on the passage order method (Ribeiro and de
Matos, 2011). This type of heuristics uses the struc-
ture of the input document (source) to partition the
candidate passages to be included in the support set
in two subsets: the ones closer to the passage asso-
ciated with the support set under construction and
the ones further apart. These heuristics use a per-
mutation, di1, di2, , diN−1, of the distances of the
passages sk to the passage pi, related to the support
set under construction, with dik = dist(sk, pi), 1 &lt;
k &lt; N −1, where N is the number of passages, cor-
responding to the order of occurrence of passages sk
in the input source. The metric that is normally used
is the cosine distance.
The KP-Centrality method consists of two steps.
First, it extracts key phrases using a supervised ap-
proach (Marujo et al., 2012) and combines them
with a bag-of-words model in a compact matrix rep-
resentation, given by:
</bodyText>
<equation confidence="0.956577166666667">
�w(t1,p1) ... w(t1,pN) w(t1, k1) ... w(t1, km)
� ..
� ..
. .
w(tT, p1) ... w(tT, pN) w(tT, k1) ... w(tT, km)
(1)
</equation>
<bodyText confidence="0.9970509">
where w is a function of the number of occur-
rences of term ti in passage pj or key phrase kt,
T is the number of terms and M is the number of
key phrases. Then, using a segmented information
source I °= p1, p2, ... , pN, a support set Si is com-
puted for each passage pi using:
Si ° {s E I U K : sim(s,qi) &gt; εi ∧ s =�qil, (2)
for i = 1, ... , N + M. Passages are ranked exclud-
ing the key phrases K (artificial passages) accord-
ing to:
</bodyText>
<equation confidence="0.6769665">
I{Si : s E Sill. (3)
1 ,
arg max
sE(Un i=1Si)−K
</equation>
<page confidence="0.97019">
177
</page>
<subsectionHeader confidence="0.995265">
3.2 Single-Layer Hierarchical
</subsectionHeader>
<bodyText confidence="0.999689125">
In this model, we use KP-CENTRALITY to generate,
for each news document, an intermediate summary
with the same size of the output summary for the in-
put documents. An aggregated summary is obtained
by concatenating the chronologically ordered inter-
mediate summaries. The output summary is again
generated by applying KP-CENTRALITY to the ag-
gregated summary as Figure 1 shows.
</bodyText>
<figureCaption confidence="0.998686">
Figure 1: Single-layer architecture.
</figureCaption>
<subsectionHeader confidence="0.994966">
3.3 Waterfall
</subsectionHeader>
<bodyText confidence="0.999373">
This model differs from the previous one in the
merging process. The underlying merging of the
documents follows a cascaded process: it starts by
merging the intermediate summaries, with the same
size of the output summary, of the first two docu-
ments, according to their chronological order. This
document is then summarized and merged with the
summary of following document. We iterate this
process through all the documents until the most re-
cent one as Figure 2 illustrates.
</bodyText>
<figureCaption confidence="0.986282">
Figure 2: Waterfall architecture.
</figureCaption>
<sectionHeader confidence="0.996958" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999981636363637">
We compare the performance of our methods against
other representative models, namely MEAD, MMR,
Expected n-call@k (Lim et al., 2012), and the Port-
folio Theory (Wang and Zhu, 2009). MEAD is a
centroid-based method and one of the most popu-
lar centrality-based methods. MMR is one of the
most used query-based methods. Expected n-call@k
adapts and extends MMR as a probabilistic model
(Probabilistic Latent MMR). The Portfolio Theory
also extends MMR based on the idea of ranking un-
der uncertainty. As baseline, we used the straight-
forward idea of combining all input documents into
a single one, and then submit the document to the
single-document summarization method. Consider-
ing that most coverage-based systems explore event
information, we opted for not including them in this
comparative analysis.
To assess the informativeness of the summaries
generated by our methods, we used ROUGE-1 and
ROUGE-2 (Lin, 2004) on DUC 2007 and TAC 2009
datasets. The main summarization task in DUC
20071 is the generation of 250-word summaries of
45 clusters of 25 newswire documents (from the
AQUAINT corpus) and 4 human reference sum-
maries. The TAC 2009 Summarization task2 has 44
topic clusters. Each topic has 2 sets of 10 news docu-
ments obtained from the AQUAINT 2 corpus.There
are 4 human 100-word reference summaries for each
set, where the reference summaries for the first set
are query-oriented, and for the second set are update
summaries. In this work, we used the first set of ref-
erence summaries. We evaluate the different models
by generating summaries with 250 words. We only
present the best results.
The used features include the bag-of-words model
representation of the sentences (TF-IDF), the key
phrases and the query (obtained from the topics de-
scriptions). Including the query is a new exten-
sion to the KP-CENTRALITY method, which, in
general, improved the results. We experimented
with different numbers of key phrases, obtaining
the best results with 40 key phrases. To compare
and rank the sentences, we use several distance met-
rics, namely: Frac133 (generic Minkowski distance,
</bodyText>
<footnote confidence="0.9999415">
1http://www-nlpir.nist.gov/projects/duc/duc2007/tasks.html
2http://www.nist.gov/tac/2009/Summarization/
</footnote>
<page confidence="0.96668">
178
</page>
<table confidence="0.9999762">
Distance Model DUC 2007 TAC 2009
R1 R2 R1 R2
frac133 baseline 0.3565 0.0744 0.4706 0.1268
cosine 0.3406 0.0670 0.4746 0.1391
frac133 waterfall 0.3569 0.0765 0.4943 0.1441
frac133 single-layer 0.3775 0.0882 0.4983 0.1526
cosine waterfall 0.3701 0.0904 0.5137 0.1693
cosine single-layer 0.3707 0.0822 0.4993 0.1590
frac133 single-layer (shuffle) 0.3689 0.0807 0.5060 0.1483
cosine waterfall (shuffle) 0.3626 0.0844 0.5107 0.1630
MEAD 0.3282 0.0765 0.4153 0.0845
MMR 0.3269 0.0780 0.3917 0.0801
E.n-call@k 0.3209 0.0701 0.3873 0.0699
Portfolio 0.3595 0.0792 0.4292 0.0758
LexRank 0.2881 0.0534 0.3845 0.0623
</table>
<tableCaption confidence="0.999847">
Table 1: ROUGE-1 (R1) and ROUGE-2 (R2) scores.
</tableCaption>
<bodyText confidence="0.9999368">
with N = 1.(3)), Euclidean, Chebyshev, Manhat-
tan, Minkowski, the Jensen-Shannon Divergence,
and the cosine similarity. Table 1 shows that the
best results were obtained by the proposed hierar-
chical models, in both datasets. Overal, the best
performing distance metric for our centrality-based
method was the cosine similarity and the best strat-
egy for combining the information was the water-
fall approach, namely, in terms of ROUGE-2. In
DUC 2007, frac133 using the single-layer method
achieved the best ROUGE-1 score, although the dif-
ference for cosine is hardly noticeable. Single-layer
with frac133 shows a performance improvement
of 0.0180 ROUGE-1 points (relative performance
improvement of 5.0%) over the best of the other
systems, Portfolio, in DUC 2007, and of 0.0845
ROUGE-1 points (19.7% relative performance im-
provement) in TAC 2009. In terms of ROUGE-
2, the waterfall method using cosine achieved an
improvement of 0.0112 (relative performance im-
provement of 14.1%) over Portfolio, in DUC 2007,
and of 0.0848 (relative performance improvement
of 100.4%) over MEAD, the best performing of the
reference systems using this metric, in TAC 2009.
Note that our baseline obtained results similar to the
best reference system in DUC 2007 and better re-
sults than all reference systems in TAC 2009 (0.0454
ROUGE-1 points corresponding to a 10.6% rela-
tive performance improvement; 0.0546 ROUGE-2
points corresponding to a 64.6% relative perfor-
mance improvement). The better results obtained on
the TAC 2009 dataset are due to the small size of
the reference summaries and to the fact that the doc-
uments sets to be summarized contain topics with
higher diversity of subtopics.
The shuffle results included in Table 1 are aver-
ages of 10 trials. They are lower than the other ob-
tained using the documents organized in chronolog-
ical order. This suggests that the order of the input
documents is important to the summarization meth-
ods.
Figure 3 shows an example of summary produced
by our multi-document method. The figure also in-
cludes the respective reference summary for com-
parison.
</bodyText>
<sectionHeader confidence="0.997409" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999914090909091">
In this work, we explore two different approaches to
extend a single-document summarization method to
multi-document summarization: single-layer hierar-
chical and waterfall.
Experimental results show that the proposed ap-
proaches perform better than previous state-of-the-
art methods on standard datasets used to evaluate
this task. In general, the best performing approach is
the waterfall approach using the cosine similarity. In
fact, this configuration achieves the best results on
the TAC 2009 dataset, considering both ROUGE-1
</bodyText>
<page confidence="0.996149">
179
</page>
<subsectionHeader confidence="0.555952">
Generated Summary:
</subsectionHeader>
<bodyText confidence="0.991388">
President Bill Clinton said Friday he will appeal a fed-
eral judge’s ruling that struck down a law giving the pres-
ident the power to veto specific items in bills passed by
Congress. The law, passed by Congress last year, allowed
the president for the first time to veto particular items in
spending bills and certain limited tax provisions passed
by Congress. Clinton said the funding that Congress
has added to the bill is excessive and threatened to veto
some items by using the line-item veto power. The White
House said that the president used his authority to can-
cel projects that were not requested in the budget and
would not substantially improve the quality of life of mil-
itary service members. Judge Thomas Hogan ruled that
the law – which gives the president the power to strike
items from tax and spending measures without vetoing
the entire bill – violates the traditional balance of pow-
ers between the various branches of government ”The
Line-Item Veto Act is unconstitutional because it imper-
missibly disrupts the balance of powers among the three
branches of government,” said Thomas Hogan.” In its ap-
peal, the Justice Department argues that the new chal-
lengers also do not have standing to challenge the law,
and that in any case the law is in line with the historic
relationship between Congress and the president.
</bodyText>
<subsectionHeader confidence="0.555024">
Reference summary:
</subsectionHeader>
<bodyText confidence="0.990556961538462">
Congress passed a law authorizing the line item veto
(LIV) in 1996 accepting arguments that the measure
would help preserve the integrity of federal spending by
allowing the president to strike unnecessary spending and
tax items from legislation thus encouraging the govern-
ment to live within its means. It was considered in line
with the historic relationship between Congress and the
president and would provide a tool for eliminating waste-
ful pork barrel spending while enlivening debate over the
best use of funds. It was argued that the LIV would rep-
resent presidential exercise of spending authority dele-
gated by Congress. President Clinton exercised the LIV
on 82 items in 1997 saving $1.9 billion in spending pro-
jected over five years. The affected items were projects
for specific localities, many in the area of military con-
struction, which had been added to the president’s budget
by Congress. The first court ruling on the LIV act was in
U.S. District Court when in February 1998 it was ruled
unconstitutional on the grounds that it violated the sep-
aration of powers. The Department of Justice appealed
that decision and in June 1998 the Supreme Court ruled
the LIV act unconstitutional but on the grounds that it vi-
olated Article I, 7, Clause 2 (The ”presentment clause”)
of the Constitution that establishes the process by which
a bill becomes law. President Clinton expressed his deep
disappointment.
</bodyText>
<figureCaption confidence="0.968706">
Figure 3: Example of summary produced by our summa-
rizer and the reference summary Topic D0730G of DUC
</figureCaption>
<page confidence="0.704957">
2007
</page>
<bodyText confidence="0.999797875">
and ROUGE-2 metrics, and, although not achieving
the best results in the DUC 2007 dataset, in terms of
ROUGE-1, it also achieves a performance improve-
ment over Portfolio of 0.0106 ROUGE-1 points (rel-
ative performance improvement of 3%).
In future work, we aim to adapt the proposed
multi-document summarization method to perform
abstractive summarization.
</bodyText>
<sectionHeader confidence="0.998091" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99995325">
This work has been partially supported by national
funds through Fundac¸˜ao para a Ciˆencia e a Tecnolo-
gia (FCT) with reference UID/CEC/50021/2013, the
grant numbers SFRH/BD/33769/2009 and CMUP-
EPB/TIC/0026/2013. The authors would also like
to thank Eduard Hovy, Isabel Trancoso, Ricardo
Baeza-Yates, and the anonymous reviewers for fruit-
ful comments.
</bodyText>
<sectionHeader confidence="0.998726" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9849874">
Miguel Almeida and Andre Martins. 2013. Fast and ro-
bust compressive summarization with dual decompo-
sition and multi-task learning. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, pages 196–206, Sofia, Bulgaria,
August. ACL.
Jaime Carbonell and Jade Goldstein. 1998. The use
of mmr, diversity-based reranking for reordering doc-
uments and producing summaries. In Proceedings
of the 21st Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, pages 335–336. ACM.
Janara Christensen, Mausam, Stephen Soderland, and
Oren Etzioni. 2013. Towards coherent multi-
document summarization. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics. ACL.
G¨unes¸ Erkan and Dragomir R. Radev. 2004. LexRank:
Graph-based Centrality as Salience in Text Summa-
rization. Journal of Artificial Intelligence Research,
22:457–479.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
Event-based extractive summarization. In Proc. of
ACL Workshop on Summarization, pages 104–111.
Dan Gillick, Benoit Favre, and Dilek Hakkani-Tur. 2008.
The icsi summarization system at tac 2008. In Pro-
ceedings of the Text Understanding Conference.
Shengbo Guo and Scott Sanner. 2010. Probabilistic la-
tent maximal marginal relevance. In Proc. of the 33rd
International ACM SIGIR Conference on Research
</reference>
<page confidence="0.98908">
180
</page>
<reference confidence="0.999224447368422">
and Development in Information Retrieval, pages 833–
834. ACM.
Kar Wai Lim, Scott Sanner, and Shengbo Guo. 2012. On
the Math. Relationship Between Expected N-call@K
and the Relevance vs. Diversity Trade-off. In Proc.
of the 35th International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 1117–1118. ACM.
Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submodu-
lar functions. In Proceedings of the North American
Chapter of the Association for Computational Linguis-
tics, pages 912–920. ACL.
Chin-Yew Lin and Eduard Hovy. 2000. The automated
acquisition of topic signatures for text summarization.
In Proceedings of the 18th Conference on Computa-
tional Linguistics - Volume 1, pages 495–501. ACL.
Chin-Yew Lin. 2004. ROUGE: A Package for Automatic
Evaluation of Summaries. In Text Summ. Branches
Out: Proc. of the ACL-04 Workshop.
Luis Marujo, Anatole Gershman, Jaime Carbonell,
Robert Frederking, and Jo˜a P. Neto. 2012. Super-
vised topical key phrase extraction of news stories
using crowdsourcing, light filtering and co-reference
normalization. In Proceedings of the Eight Interna-
tional Conference on Language Resources and Evalu-
ation (LREC’12). ELRA.
Luis Marujo, Jos´e Portˆelo, David Martins de Matos,
Jo˜ao P Neto, Anatole Gershman, Jaime Carbonell, Is-
abel Trancoso, and Bhiksha Raj. 2014. Privacy-
preserving important passage retrieval. In ACM SIGIR
PIR workshop.
Kathleen R. McKeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and Summarizing News on
a Daily Basis with Columbia’s Newsblaster. In HLT.
Dragomir R. Radev, Hongyan Jing, Małgorzata Sty´s, and
Daniel Tam. 2004. Centroid-based summarization
of multiple documents. Information Processing and
Management, 40.
Dragomir R. Radev, Jahna Otterbacher, Adam Winkel,
and Sasha Blair-Goldensohn. 2005. NewsInEssence:
Summarizing Online News Topics. Communications
of the ACM, 48(10):95–98.
Ricardo Ribeiro and David Martins de Matos. 2011.
Revisiting Centrality-as-Relevance: Support Sets and
Similarity as Geometric Proximity. JAIR, 42:275–308.
Ricardo Ribeiro, Luis Marujo, David Martins de Matos,
Jo˜ao P. Neto, Anatole Gershman, and Jaime Carbonell.
2013. Self reinforcement for important passage re-
trieval. In Proceedings of the 36th International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 845–848. ACM.
Scott Sanner, Shengbo Guo, Thore Graepel, Sadegh
Kharazmi, and Sarvnaz Karimi. 2011. Diverse re-
trieval via greedy optimization of expected 1-call@k
in a latent subtopic relevance model. In Proc. of
the 20th ACM International Conference on Informa-
tion and Knowledge Management, pages 1977–1980.
ACM.
Ruben Sipos, Adith Swaminathan, Pannaga Shivaswamy,
and Thorsten Joachims. 2012. Temporal corpus sum-
marization using submodular word coverage. In Proc.
of the 21st ACM International Conference on Informa-
tion and Knowledge Management. ACM.
Jun Wang and Jianhan Zhu. 2009. Portfolio theory of in-
formation retrieval. In Proceedings of the 32Nd Inter-
national ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 115–122.
Dingding Wang, Tao Li, Shenghuo Zhu, and Chris Ding.
2008. Multi-document summarization via sentence-
level semantic analysis and symmetric matrix factor-
ization. In Proc. of the 31st Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 307–314. ACM.
</reference>
<page confidence="0.998379">
181
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.266223">
<title confidence="0.9996605">Extending a Single-Document Summarizer to Multi-Document: Hierarchical Approach</title>
<author confidence="0.973986">Ricardo David Martins de_P Anatole</author>
<author confidence="0.973986">Jaime</author>
<address confidence="0.4664525">Lisboa, Lisboa, of Computer Science, CMU, Pittsburgh,</address>
<abstract confidence="0.9981229">The increasing amount of online content motivated the development of multi-document summarization methods. In this work, we explore straightforward approaches to extend single-document summarization methods to multi-document summarization. The proposed methods are based on the hierarchical combination of single-document summaries, and achieves state of the art results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Miguel Almeida</author>
<author>Andre Martins</author>
</authors>
<title>Fast and robust compressive summarization with dual decomposition and multi-task learning.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>196--206</pages>
<publisher>ACL.</publisher>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="4568" citStr="Almeida and Martins, 2013" startWordPosition="664" endWordPosition="667">ave to be different; Si are the yet unselected passages and Sj are the previously selected ones; Q is the required query to apply the model; and, λ is a parameter that allows to configure the result to be from a standard relevance-ranked list (λ = 1) to a maximal diversity ranking (λ = 0). Coverage-based summarization defines a set of concepts that need to occur in the sentences selected for the summaries. The concepts are events (Filatova and Hatzivassiloglou, 2004), topics (Lin and Hovy, 2000), salient words (Lin and Bilmes, 2010; Sipos et al., 2012), and word n-grams (Gillick et al., 2008; Almeida and Martins, 2013). 3 Multi-Document Summarization Our multi-document approach is built upon a centrality and coverage-based single-document summarization method, KP-CENTRALITY (Ribeiro et al., 2013). This method, through the use of key phrases, is easily adaptable and has been shown to be robust in the presence of noisy input. This is an important aspect considering that using as input several documents frequently increases the amount of unimportant content). When adapting a single-document summarization method to perform multi-document summarization, a possible strategy is to combine the summaries of each doc</context>
</contexts>
<marker>Almeida, Martins, 2013</marker>
<rawString>Miguel Almeida and Andre Martins. 2013. Fast and robust compressive summarization with dual decomposition and multi-task learning. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 196–206, Sofia, Bulgaria, August. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime Carbonell</author>
<author>Jade Goldstein</author>
</authors>
<title>The use of mmr, diversity-based reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>335--336</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2385" citStr="Carbonell and Goldstein, 1998" startWordPosition="319" endWordPosition="322">ation. We explore two hierarchical strategies to perform this extension. This document is organized as follows: Sect. 2 addresses the related work; Sect. 3 presents our multidocument summarization appproach; experimental results close the paper. 2 Related Work Most of the current work in automatic summarization focuses on extractive summarization. The most popular baselines for multi-document summarization fall into one of the following general models: Centrality-based (Radev et al., 2004; Erkan and Radev, 2004; Wang et al., 2008; Ribeiro and de Matos, 2011), Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998; Guo and Sanner, 2010; Sanner et al., 2011; Lim et al., 2012), and Coverage-base methods (Lin and Hovy, 2000; Sipos et al., 2012). Additionally, methods such as KP-CENTRALITY (Ribeiro et al., 2013), which is centrality and coverage-based, follow more than one paradigm. In general, Centrality-based models are used to produce generic summaries, while the MMR family generates query-oriented ones. Coveragebase models produce summaries driven by words, topics or events. Centrality-as-relevance methods base the detection of the most salient passages on the identification of the central passages of </context>
<context position="3751" citStr="Carbonell and Goldstein, 1998" startWordPosition="525" endWordPosition="528">hods build on the idea of a pseudo-passage that represents the central topic of the input source— the centroid—selecting as passages to be included in the summary the ones that are close to the centroid. Another approach to centrality estimation is to com176 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 176–181, Denver, Colorado, June 4–5, 2015. pare each candidate passage to every other passage and select the ones with higher scores (the ones that are closer to every other passage): the Pair-wise Passage Similarity-based Centrality. MMR (Carbonell and Goldstein, 1998) is a query driven relevance model based on the following mathematical model: arg max [λ(Sim1 (Si, Q))−(1−λ) (max Sim2(Si, Sj))J Sj Si where Sim1 and Sim2 are similarity metrics that do not have to be different; Si are the yet unselected passages and Sj are the previously selected ones; Q is the required query to apply the model; and, λ is a parameter that allows to configure the result to be from a standard relevance-ranked list (λ = 1) to a maximal diversity ranking (λ = 0). Coverage-based summarization defines a set of concepts that need to occur in the sentences selected for the summaries.</context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>Jaime Carbonell and Jade Goldstein. 1998. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 335–336. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janara Christensen</author>
<author>Stephen Soderland Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Towards coherent multidocument summarization.</title>
<date>2013</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics. ACL.</booktitle>
<contexts>
<context position="1377" citStr="Christensen et al., 2013" startWordPosition="172" endWordPosition="175">eves state of the art results. 1 Introduction The use of the Internet to fulfill generic information needs motivated pioneer multi-document summarization efforts as NewsInEssence (Radev et al., 2005) or Newsblaster (McKeown et al., 2002), online since 2001. In general, multi-document summarization approaches have to address two different problems: passage selection and information ordering. Current multi-document systems adopt, for passage selection, approaches similar to the ones used in single-document summarization, and use the chronological order of the documents for information ordering (Christensen et al., 2013). The problem is that most approaches fail to generate summaries that cover generic topics which comprehend different, equally important, subtopics. We propose to extend a state-of-the-art single-document summarization method, KPCENTRALITY (Ribeiro et al., 2013), capable of focusing on diverse important topics while ignoring unimportant ones, to perform multi-document summarization. We explore two hierarchical strategies to perform this extension. This document is organized as follows: Sect. 2 addresses the related work; Sect. 3 presents our multidocument summarization appproach; experimental </context>
</contexts>
<marker>Christensen, Mausam, Etzioni, 2013</marker>
<rawString>Janara Christensen, Mausam, Stephen Soderland, and Oren Etzioni. 2013. Towards coherent multidocument summarization. In Proceedings of the North American Chapter of the Association for Computational Linguistics. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes¸ Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>LexRank: Graph-based Centrality as Salience in Text Summarization.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>22--457</pages>
<contexts>
<context position="2272" citStr="Erkan and Radev, 2004" startWordPosition="302" endWordPosition="305"> focusing on diverse important topics while ignoring unimportant ones, to perform multi-document summarization. We explore two hierarchical strategies to perform this extension. This document is organized as follows: Sect. 2 addresses the related work; Sect. 3 presents our multidocument summarization appproach; experimental results close the paper. 2 Related Work Most of the current work in automatic summarization focuses on extractive summarization. The most popular baselines for multi-document summarization fall into one of the following general models: Centrality-based (Radev et al., 2004; Erkan and Radev, 2004; Wang et al., 2008; Ribeiro and de Matos, 2011), Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998; Guo and Sanner, 2010; Sanner et al., 2011; Lim et al., 2012), and Coverage-base methods (Lin and Hovy, 2000; Sipos et al., 2012). Additionally, methods such as KP-CENTRALITY (Ribeiro et al., 2013), which is centrality and coverage-based, follow more than one paradigm. In general, Centrality-based models are used to produce generic summaries, while the MMR family generates query-oriented ones. Coveragebase models produce summaries driven by words, topics or events. Centrality-as-re</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G¨unes¸ Erkan and Dragomir R. Radev. 2004. LexRank: Graph-based Centrality as Salience in Text Summarization. Journal of Artificial Intelligence Research, 22:457–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Filatova</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Event-based extractive summarization.</title>
<date>2004</date>
<booktitle>In Proc. of ACL Workshop on Summarization,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="4413" citStr="Filatova and Hatzivassiloglou, 2004" startWordPosition="638" endWordPosition="641">ce model based on the following mathematical model: arg max [λ(Sim1 (Si, Q))−(1−λ) (max Sim2(Si, Sj))J Sj Si where Sim1 and Sim2 are similarity metrics that do not have to be different; Si are the yet unselected passages and Sj are the previously selected ones; Q is the required query to apply the model; and, λ is a parameter that allows to configure the result to be from a standard relevance-ranked list (λ = 1) to a maximal diversity ranking (λ = 0). Coverage-based summarization defines a set of concepts that need to occur in the sentences selected for the summaries. The concepts are events (Filatova and Hatzivassiloglou, 2004), topics (Lin and Hovy, 2000), salient words (Lin and Bilmes, 2010; Sipos et al., 2012), and word n-grams (Gillick et al., 2008; Almeida and Martins, 2013). 3 Multi-Document Summarization Our multi-document approach is built upon a centrality and coverage-based single-document summarization method, KP-CENTRALITY (Ribeiro et al., 2013). This method, through the use of key phrases, is easily adaptable and has been shown to be robust in the presence of noisy input. This is an important aspect considering that using as input several documents frequently increases the amount of unimportant content)</context>
</contexts>
<marker>Filatova, Hatzivassiloglou, 2004</marker>
<rawString>Elena Filatova and Vasileios Hatzivassiloglou. 2004. Event-based extractive summarization. In Proc. of ACL Workshop on Summarization, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
<author>Benoit Favre</author>
<author>Dilek Hakkani-Tur</author>
</authors>
<title>The icsi summarization system at tac</title>
<date>2008</date>
<booktitle>In Proceedings of the Text Understanding Conference.</booktitle>
<contexts>
<context position="4540" citStr="Gillick et al., 2008" startWordPosition="660" endWordPosition="663"> metrics that do not have to be different; Si are the yet unselected passages and Sj are the previously selected ones; Q is the required query to apply the model; and, λ is a parameter that allows to configure the result to be from a standard relevance-ranked list (λ = 1) to a maximal diversity ranking (λ = 0). Coverage-based summarization defines a set of concepts that need to occur in the sentences selected for the summaries. The concepts are events (Filatova and Hatzivassiloglou, 2004), topics (Lin and Hovy, 2000), salient words (Lin and Bilmes, 2010; Sipos et al., 2012), and word n-grams (Gillick et al., 2008; Almeida and Martins, 2013). 3 Multi-Document Summarization Our multi-document approach is built upon a centrality and coverage-based single-document summarization method, KP-CENTRALITY (Ribeiro et al., 2013). This method, through the use of key phrases, is easily adaptable and has been shown to be robust in the presence of noisy input. This is an important aspect considering that using as input several documents frequently increases the amount of unimportant content). When adapting a single-document summarization method to perform multi-document summarization, a possible strategy is to combi</context>
</contexts>
<marker>Gillick, Favre, Hakkani-Tur, 2008</marker>
<rawString>Dan Gillick, Benoit Favre, and Dilek Hakkani-Tur. 2008. The icsi summarization system at tac 2008. In Proceedings of the Text Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shengbo Guo</author>
<author>Scott Sanner</author>
</authors>
<title>Probabilistic latent maximal marginal relevance.</title>
<date>2010</date>
<booktitle>In Proc. of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>833--834</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2407" citStr="Guo and Sanner, 2010" startWordPosition="323" endWordPosition="326">cal strategies to perform this extension. This document is organized as follows: Sect. 2 addresses the related work; Sect. 3 presents our multidocument summarization appproach; experimental results close the paper. 2 Related Work Most of the current work in automatic summarization focuses on extractive summarization. The most popular baselines for multi-document summarization fall into one of the following general models: Centrality-based (Radev et al., 2004; Erkan and Radev, 2004; Wang et al., 2008; Ribeiro and de Matos, 2011), Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998; Guo and Sanner, 2010; Sanner et al., 2011; Lim et al., 2012), and Coverage-base methods (Lin and Hovy, 2000; Sipos et al., 2012). Additionally, methods such as KP-CENTRALITY (Ribeiro et al., 2013), which is centrality and coverage-based, follow more than one paradigm. In general, Centrality-based models are used to produce generic summaries, while the MMR family generates query-oriented ones. Coveragebase models produce summaries driven by words, topics or events. Centrality-as-relevance methods base the detection of the most salient passages on the identification of the central passages of the input source(s). O</context>
</contexts>
<marker>Guo, Sanner, 2010</marker>
<rawString>Shengbo Guo and Scott Sanner. 2010. Probabilistic latent maximal marginal relevance. In Proc. of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 833– 834. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kar Wai Lim</author>
<author>Scott Sanner</author>
<author>Shengbo Guo</author>
</authors>
<title>On the Math. Relationship Between Expected N-call@K and the Relevance vs. Diversity Trade-off.</title>
<date>2012</date>
<booktitle>In Proc. of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>1117--1118</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2447" citStr="Lim et al., 2012" startWordPosition="331" endWordPosition="334">his document is organized as follows: Sect. 2 addresses the related work; Sect. 3 presents our multidocument summarization appproach; experimental results close the paper. 2 Related Work Most of the current work in automatic summarization focuses on extractive summarization. The most popular baselines for multi-document summarization fall into one of the following general models: Centrality-based (Radev et al., 2004; Erkan and Radev, 2004; Wang et al., 2008; Ribeiro and de Matos, 2011), Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998; Guo and Sanner, 2010; Sanner et al., 2011; Lim et al., 2012), and Coverage-base methods (Lin and Hovy, 2000; Sipos et al., 2012). Additionally, methods such as KP-CENTRALITY (Ribeiro et al., 2013), which is centrality and coverage-based, follow more than one paradigm. In general, Centrality-based models are used to produce generic summaries, while the MMR family generates query-oriented ones. Coveragebase models produce summaries driven by words, topics or events. Centrality-as-relevance methods base the detection of the most salient passages on the identification of the central passages of the input source(s). One of the main representatives of this f</context>
<context position="8817" citStr="Lim et al., 2012" startWordPosition="1391" endWordPosition="1394">rging process. The underlying merging of the documents follows a cascaded process: it starts by merging the intermediate summaries, with the same size of the output summary, of the first two documents, according to their chronological order. This document is then summarized and merged with the summary of following document. We iterate this process through all the documents until the most recent one as Figure 2 illustrates. Figure 2: Waterfall architecture. 4 Experimental Results We compare the performance of our methods against other representative models, namely MEAD, MMR, Expected n-call@k (Lim et al., 2012), and the Portfolio Theory (Wang and Zhu, 2009). MEAD is a centroid-based method and one of the most popular centrality-based methods. MMR is one of the most used query-based methods. Expected n-call@k adapts and extends MMR as a probabilistic model (Probabilistic Latent MMR). The Portfolio Theory also extends MMR based on the idea of ranking under uncertainty. As baseline, we used the straightforward idea of combining all input documents into a single one, and then submit the document to the single-document summarization method. Considering that most coverage-based systems explore event infor</context>
</contexts>
<marker>Lim, Sanner, Guo, 2012</marker>
<rawString>Kar Wai Lim, Scott Sanner, and Shengbo Guo. 2012. On the Math. Relationship Between Expected N-call@K and the Relevance vs. Diversity Trade-off. In Proc. of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1117–1118. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Lin</author>
<author>Jeff Bilmes</author>
</authors>
<title>Multi-document summarization via budgeted maximization of submodular functions.</title>
<date>2010</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>912--920</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="4479" citStr="Lin and Bilmes, 2010" startWordPosition="649" endWordPosition="652"> (max Sim2(Si, Sj))J Sj Si where Sim1 and Sim2 are similarity metrics that do not have to be different; Si are the yet unselected passages and Sj are the previously selected ones; Q is the required query to apply the model; and, λ is a parameter that allows to configure the result to be from a standard relevance-ranked list (λ = 1) to a maximal diversity ranking (λ = 0). Coverage-based summarization defines a set of concepts that need to occur in the sentences selected for the summaries. The concepts are events (Filatova and Hatzivassiloglou, 2004), topics (Lin and Hovy, 2000), salient words (Lin and Bilmes, 2010; Sipos et al., 2012), and word n-grams (Gillick et al., 2008; Almeida and Martins, 2013). 3 Multi-Document Summarization Our multi-document approach is built upon a centrality and coverage-based single-document summarization method, KP-CENTRALITY (Ribeiro et al., 2013). This method, through the use of key phrases, is easily adaptable and has been shown to be robust in the presence of noisy input. This is an important aspect considering that using as input several documents frequently increases the amount of unimportant content). When adapting a single-document summarization method to perform </context>
</contexts>
<marker>Lin, Bilmes, 2010</marker>
<rawString>Hui Lin and Jeff Bilmes. 2010. Multi-document summarization via budgeted maximization of submodular functions. In Proceedings of the North American Chapter of the Association for Computational Linguistics, pages 912–920. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>The automated acquisition of topic signatures for text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th Conference on Computational Linguistics -</booktitle>
<volume>1</volume>
<pages>495--501</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="2494" citStr="Lin and Hovy, 2000" startWordPosition="338" endWordPosition="341">addresses the related work; Sect. 3 presents our multidocument summarization appproach; experimental results close the paper. 2 Related Work Most of the current work in automatic summarization focuses on extractive summarization. The most popular baselines for multi-document summarization fall into one of the following general models: Centrality-based (Radev et al., 2004; Erkan and Radev, 2004; Wang et al., 2008; Ribeiro and de Matos, 2011), Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998; Guo and Sanner, 2010; Sanner et al., 2011; Lim et al., 2012), and Coverage-base methods (Lin and Hovy, 2000; Sipos et al., 2012). Additionally, methods such as KP-CENTRALITY (Ribeiro et al., 2013), which is centrality and coverage-based, follow more than one paradigm. In general, Centrality-based models are used to produce generic summaries, while the MMR family generates query-oriented ones. Coveragebase models produce summaries driven by words, topics or events. Centrality-as-relevance methods base the detection of the most salient passages on the identification of the central passages of the input source(s). One of the main representatives of this family is Passageto-Centroid Similarity-based Ce</context>
<context position="4442" citStr="Lin and Hovy, 2000" startWordPosition="643" endWordPosition="646">odel: arg max [λ(Sim1 (Si, Q))−(1−λ) (max Sim2(Si, Sj))J Sj Si where Sim1 and Sim2 are similarity metrics that do not have to be different; Si are the yet unselected passages and Sj are the previously selected ones; Q is the required query to apply the model; and, λ is a parameter that allows to configure the result to be from a standard relevance-ranked list (λ = 1) to a maximal diversity ranking (λ = 0). Coverage-based summarization defines a set of concepts that need to occur in the sentences selected for the summaries. The concepts are events (Filatova and Hatzivassiloglou, 2004), topics (Lin and Hovy, 2000), salient words (Lin and Bilmes, 2010; Sipos et al., 2012), and word n-grams (Gillick et al., 2008; Almeida and Martins, 2013). 3 Multi-Document Summarization Our multi-document approach is built upon a centrality and coverage-based single-document summarization method, KP-CENTRALITY (Ribeiro et al., 2013). This method, through the use of key phrases, is easily adaptable and has been shown to be robust in the presence of noisy input. This is an important aspect considering that using as input several documents frequently increases the amount of unimportant content). When adapting a single-docu</context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2000. The automated acquisition of topic signatures for text summarization. In Proceedings of the 18th Conference on Computational Linguistics - Volume 1, pages 495–501. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>ROUGE: A Package for Automatic Evaluation of Summaries.</title>
<date>2004</date>
<booktitle>In Text Summ. Branches Out: Proc. of the ACL-04 Workshop.</booktitle>
<contexts>
<context position="9599" citStr="Lin, 2004" startWordPosition="1517" endWordPosition="1518">ed methods. Expected n-call@k adapts and extends MMR as a probabilistic model (Probabilistic Latent MMR). The Portfolio Theory also extends MMR based on the idea of ranking under uncertainty. As baseline, we used the straightforward idea of combining all input documents into a single one, and then submit the document to the single-document summarization method. Considering that most coverage-based systems explore event information, we opted for not including them in this comparative analysis. To assess the informativeness of the summaries generated by our methods, we used ROUGE-1 and ROUGE-2 (Lin, 2004) on DUC 2007 and TAC 2009 datasets. The main summarization task in DUC 20071 is the generation of 250-word summaries of 45 clusters of 25 newswire documents (from the AQUAINT corpus) and 4 human reference summaries. The TAC 2009 Summarization task2 has 44 topic clusters. Each topic has 2 sets of 10 news documents obtained from the AQUAINT 2 corpus.There are 4 human 100-word reference summaries for each set, where the reference summaries for the first set are query-oriented, and for the second set are update summaries. In this work, we used the first set of reference summaries. We evaluate the </context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summ. Branches Out: Proc. of the ACL-04 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis Marujo</author>
<author>Anatole Gershman</author>
<author>Jaime Carbonell</author>
<author>Robert Frederking</author>
<author>Jo˜a P Neto</author>
</authors>
<title>Supervised topical key phrase extraction of news stories using crowdsourcing, light filtering and co-reference normalization.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12). ELRA.</booktitle>
<contexts>
<context position="7005" citStr="Marujo et al., 2012" startWordPosition="1067" endWordPosition="1070"> in two subsets: the ones closer to the passage associated with the support set under construction and the ones further apart. These heuristics use a permutation, di1, di2, , diN−1, of the distances of the passages sk to the passage pi, related to the support set under construction, with dik = dist(sk, pi), 1 &lt; k &lt; N −1, where N is the number of passages, corresponding to the order of occurrence of passages sk in the input source. The metric that is normally used is the cosine distance. The KP-Centrality method consists of two steps. First, it extracts key phrases using a supervised approach (Marujo et al., 2012) and combines them with a bag-of-words model in a compact matrix representation, given by: �w(t1,p1) ... w(t1,pN) w(t1, k1) ... w(t1, km) � .. � .. . . w(tT, p1) ... w(tT, pN) w(tT, k1) ... w(tT, km) (1) where w is a function of the number of occurrences of term ti in passage pj or key phrase kt, T is the number of terms and M is the number of key phrases. Then, using a segmented information source I °= p1, p2, ... , pN, a support set Si is computed for each passage pi using: Si ° {s E I U K : sim(s,qi) &gt; εi ∧ s =�qil, (2) for i = 1, ... , N + M. Passages are ranked excluding the key phrases K</context>
</contexts>
<marker>Marujo, Gershman, Carbonell, Frederking, Neto, 2012</marker>
<rawString>Luis Marujo, Anatole Gershman, Jaime Carbonell, Robert Frederking, and Jo˜a P. Neto. 2012. Supervised topical key phrase extraction of news stories using crowdsourcing, light filtering and co-reference normalization. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12). ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis Marujo</author>
<author>Jos´e Portˆelo</author>
<author>David Martins de Matos</author>
<author>Jo˜ao P Neto</author>
<author>Anatole Gershman</author>
<author>Jaime Carbonell</author>
<author>Isabel Trancoso</author>
<author>Bhiksha Raj</author>
</authors>
<title>Privacypreserving important passage retrieval.</title>
<date>2014</date>
<booktitle>In ACM SIGIR PIR workshop.</booktitle>
<marker>Marujo, Portˆelo, de Matos, Neto, Gershman, Carbonell, Trancoso, Raj, 2014</marker>
<rawString>Luis Marujo, Jos´e Portˆelo, David Martins de Matos, Jo˜ao P Neto, Anatole Gershman, Jaime Carbonell, Isabel Trancoso, and Bhiksha Raj. 2014. Privacypreserving important passage retrieval. In ACM SIGIR PIR workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
<author>Regina Barzilay</author>
<author>David Evans</author>
<author>Vasileios Hatzivassiloglou</author>
<author>Judith L Klavans</author>
<author>Ani Nenkova</author>
<author>Carl Sable</author>
<author>Barry Schiffman</author>
<author>Sergey Sigelman</author>
</authors>
<title>Tracking and Summarizing News on a Daily Basis with Columbia’s Newsblaster.</title>
<date>2002</date>
<booktitle>In HLT.</booktitle>
<contexts>
<context position="989" citStr="McKeown et al., 2002" startWordPosition="117" endWordPosition="120">avid.matos,joao.neto}@inesc-id.pt Abstract The increasing amount of online content motivated the development of multi-document summarization methods. In this work, we explore straightforward approaches to extend single-document summarization methods to multi-document summarization. The proposed methods are based on the hierarchical combination of single-document summaries, and achieves state of the art results. 1 Introduction The use of the Internet to fulfill generic information needs motivated pioneer multi-document summarization efforts as NewsInEssence (Radev et al., 2005) or Newsblaster (McKeown et al., 2002), online since 2001. In general, multi-document summarization approaches have to address two different problems: passage selection and information ordering. Current multi-document systems adopt, for passage selection, approaches similar to the ones used in single-document summarization, and use the chronological order of the documents for information ordering (Christensen et al., 2013). The problem is that most approaches fail to generate summaries that cover generic topics which comprehend different, equally important, subtopics. We propose to extend a state-of-the-art single-document summari</context>
</contexts>
<marker>McKeown, Barzilay, Evans, Hatzivassiloglou, Klavans, Nenkova, Sable, Schiffman, Sigelman, 2002</marker>
<rawString>Kathleen R. McKeown, Regina Barzilay, David Evans, Vasileios Hatzivassiloglou, Judith L. Klavans, Ani Nenkova, Carl Sable, Barry Schiffman, and Sergey Sigelman. 2002. Tracking and Summarizing News on a Daily Basis with Columbia’s Newsblaster. In HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Hongyan Jing</author>
<author>Małgorzata Sty´s</author>
<author>Daniel Tam</author>
</authors>
<title>Centroid-based summarization of multiple documents.</title>
<date>2004</date>
<journal>Information Processing and Management,</journal>
<volume>40</volume>
<marker>Radev, Jing, Sty´s, Tam, 2004</marker>
<rawString>Dragomir R. Radev, Hongyan Jing, Małgorzata Sty´s, and Daniel Tam. 2004. Centroid-based summarization of multiple documents. Information Processing and Management, 40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Jahna Otterbacher</author>
<author>Adam Winkel</author>
<author>Sasha Blair-Goldensohn</author>
</authors>
<title>NewsInEssence: Summarizing Online News Topics.</title>
<date>2005</date>
<journal>Communications of the ACM,</journal>
<volume>48</volume>
<issue>10</issue>
<contexts>
<context position="951" citStr="Radev et al., 2005" startWordPosition="111" endWordPosition="114">g,jgc}@cs.cmu.edu {ricardo.ribeiro,david.matos,joao.neto}@inesc-id.pt Abstract The increasing amount of online content motivated the development of multi-document summarization methods. In this work, we explore straightforward approaches to extend single-document summarization methods to multi-document summarization. The proposed methods are based on the hierarchical combination of single-document summaries, and achieves state of the art results. 1 Introduction The use of the Internet to fulfill generic information needs motivated pioneer multi-document summarization efforts as NewsInEssence (Radev et al., 2005) or Newsblaster (McKeown et al., 2002), online since 2001. In general, multi-document summarization approaches have to address two different problems: passage selection and information ordering. Current multi-document systems adopt, for passage selection, approaches similar to the ones used in single-document summarization, and use the chronological order of the documents for information ordering (Christensen et al., 2013). The problem is that most approaches fail to generate summaries that cover generic topics which comprehend different, equally important, subtopics. We propose to extend a st</context>
</contexts>
<marker>Radev, Otterbacher, Winkel, Blair-Goldensohn, 2005</marker>
<rawString>Dragomir R. Radev, Jahna Otterbacher, Adam Winkel, and Sasha Blair-Goldensohn. 2005. NewsInEssence: Summarizing Online News Topics. Communications of the ACM, 48(10):95–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ricardo Ribeiro</author>
<author>David Martins de Matos</author>
</authors>
<title>Revisiting Centrality-as-Relevance: Support Sets and Similarity as Geometric Proximity.</title>
<date>2011</date>
<pages>42--275</pages>
<publisher>JAIR,</publisher>
<marker>Ribeiro, de Matos, 2011</marker>
<rawString>Ricardo Ribeiro and David Martins de Matos. 2011. Revisiting Centrality-as-Relevance: Support Sets and Similarity as Geometric Proximity. JAIR, 42:275–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ricardo Ribeiro</author>
<author>Luis Marujo</author>
<author>David Martins de Matos</author>
<author>Jo˜ao P Neto</author>
<author>Anatole Gershman</author>
<author>Jaime Carbonell</author>
</authors>
<title>Self reinforcement for important passage retrieval.</title>
<date>2013</date>
<booktitle>In Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>845--848</pages>
<publisher>ACM.</publisher>
<marker>Ribeiro, Marujo, de Matos, Neto, Gershman, Carbonell, 2013</marker>
<rawString>Ricardo Ribeiro, Luis Marujo, David Martins de Matos, Jo˜ao P. Neto, Anatole Gershman, and Jaime Carbonell. 2013. Self reinforcement for important passage retrieval. In Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 845–848. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Sanner</author>
<author>Shengbo Guo</author>
<author>Thore Graepel</author>
<author>Sadegh Kharazmi</author>
<author>Sarvnaz Karimi</author>
</authors>
<title>Diverse retrieval via greedy optimization of expected 1-call@k in a latent subtopic relevance model.</title>
<date>2011</date>
<booktitle>In Proc. of the 20th ACM International Conference on Information and Knowledge Management,</booktitle>
<pages>1977--1980</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2428" citStr="Sanner et al., 2011" startWordPosition="327" endWordPosition="330">orm this extension. This document is organized as follows: Sect. 2 addresses the related work; Sect. 3 presents our multidocument summarization appproach; experimental results close the paper. 2 Related Work Most of the current work in automatic summarization focuses on extractive summarization. The most popular baselines for multi-document summarization fall into one of the following general models: Centrality-based (Radev et al., 2004; Erkan and Radev, 2004; Wang et al., 2008; Ribeiro and de Matos, 2011), Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998; Guo and Sanner, 2010; Sanner et al., 2011; Lim et al., 2012), and Coverage-base methods (Lin and Hovy, 2000; Sipos et al., 2012). Additionally, methods such as KP-CENTRALITY (Ribeiro et al., 2013), which is centrality and coverage-based, follow more than one paradigm. In general, Centrality-based models are used to produce generic summaries, while the MMR family generates query-oriented ones. Coveragebase models produce summaries driven by words, topics or events. Centrality-as-relevance methods base the detection of the most salient passages on the identification of the central passages of the input source(s). One of the main repres</context>
</contexts>
<marker>Sanner, Guo, Graepel, Kharazmi, Karimi, 2011</marker>
<rawString>Scott Sanner, Shengbo Guo, Thore Graepel, Sadegh Kharazmi, and Sarvnaz Karimi. 2011. Diverse retrieval via greedy optimization of expected 1-call@k in a latent subtopic relevance model. In Proc. of the 20th ACM International Conference on Information and Knowledge Management, pages 1977–1980. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruben Sipos</author>
<author>Adith Swaminathan</author>
<author>Pannaga Shivaswamy</author>
<author>Thorsten Joachims</author>
</authors>
<title>Temporal corpus summarization using submodular word coverage.</title>
<date>2012</date>
<booktitle>In Proc. of the 21st ACM International Conference on Information and Knowledge Management.</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="2515" citStr="Sipos et al., 2012" startWordPosition="342" endWordPosition="345">d work; Sect. 3 presents our multidocument summarization appproach; experimental results close the paper. 2 Related Work Most of the current work in automatic summarization focuses on extractive summarization. The most popular baselines for multi-document summarization fall into one of the following general models: Centrality-based (Radev et al., 2004; Erkan and Radev, 2004; Wang et al., 2008; Ribeiro and de Matos, 2011), Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998; Guo and Sanner, 2010; Sanner et al., 2011; Lim et al., 2012), and Coverage-base methods (Lin and Hovy, 2000; Sipos et al., 2012). Additionally, methods such as KP-CENTRALITY (Ribeiro et al., 2013), which is centrality and coverage-based, follow more than one paradigm. In general, Centrality-based models are used to produce generic summaries, while the MMR family generates query-oriented ones. Coveragebase models produce summaries driven by words, topics or events. Centrality-as-relevance methods base the detection of the most salient passages on the identification of the central passages of the input source(s). One of the main representatives of this family is Passageto-Centroid Similarity-based Centrality. Centroidbas</context>
<context position="4500" citStr="Sipos et al., 2012" startWordPosition="653" endWordPosition="656">j Si where Sim1 and Sim2 are similarity metrics that do not have to be different; Si are the yet unselected passages and Sj are the previously selected ones; Q is the required query to apply the model; and, λ is a parameter that allows to configure the result to be from a standard relevance-ranked list (λ = 1) to a maximal diversity ranking (λ = 0). Coverage-based summarization defines a set of concepts that need to occur in the sentences selected for the summaries. The concepts are events (Filatova and Hatzivassiloglou, 2004), topics (Lin and Hovy, 2000), salient words (Lin and Bilmes, 2010; Sipos et al., 2012), and word n-grams (Gillick et al., 2008; Almeida and Martins, 2013). 3 Multi-Document Summarization Our multi-document approach is built upon a centrality and coverage-based single-document summarization method, KP-CENTRALITY (Ribeiro et al., 2013). This method, through the use of key phrases, is easily adaptable and has been shown to be robust in the presence of noisy input. This is an important aspect considering that using as input several documents frequently increases the amount of unimportant content). When adapting a single-document summarization method to perform multi-document summar</context>
</contexts>
<marker>Sipos, Swaminathan, Shivaswamy, Joachims, 2012</marker>
<rawString>Ruben Sipos, Adith Swaminathan, Pannaga Shivaswamy, and Thorsten Joachims. 2012. Temporal corpus summarization using submodular word coverage. In Proc. of the 21st ACM International Conference on Information and Knowledge Management. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Wang</author>
<author>Jianhan Zhu</author>
</authors>
<title>Portfolio theory of information retrieval.</title>
<date>2009</date>
<booktitle>In Proceedings of the 32Nd International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>115--122</pages>
<contexts>
<context position="8864" citStr="Wang and Zhu, 2009" startWordPosition="1400" endWordPosition="1403">documents follows a cascaded process: it starts by merging the intermediate summaries, with the same size of the output summary, of the first two documents, according to their chronological order. This document is then summarized and merged with the summary of following document. We iterate this process through all the documents until the most recent one as Figure 2 illustrates. Figure 2: Waterfall architecture. 4 Experimental Results We compare the performance of our methods against other representative models, namely MEAD, MMR, Expected n-call@k (Lim et al., 2012), and the Portfolio Theory (Wang and Zhu, 2009). MEAD is a centroid-based method and one of the most popular centrality-based methods. MMR is one of the most used query-based methods. Expected n-call@k adapts and extends MMR as a probabilistic model (Probabilistic Latent MMR). The Portfolio Theory also extends MMR based on the idea of ranking under uncertainty. As baseline, we used the straightforward idea of combining all input documents into a single one, and then submit the document to the single-document summarization method. Considering that most coverage-based systems explore event information, we opted for not including them in this</context>
</contexts>
<marker>Wang, Zhu, 2009</marker>
<rawString>Jun Wang and Jianhan Zhu. 2009. Portfolio theory of information retrieval. In Proceedings of the 32Nd International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 115–122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dingding Wang</author>
<author>Tao Li</author>
<author>Shenghuo Zhu</author>
<author>Chris Ding</author>
</authors>
<title>Multi-document summarization via sentencelevel semantic analysis and symmetric matrix factorization.</title>
<date>2008</date>
<booktitle>In Proc. of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>307--314</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2291" citStr="Wang et al., 2008" startWordPosition="306" endWordPosition="309">portant topics while ignoring unimportant ones, to perform multi-document summarization. We explore two hierarchical strategies to perform this extension. This document is organized as follows: Sect. 2 addresses the related work; Sect. 3 presents our multidocument summarization appproach; experimental results close the paper. 2 Related Work Most of the current work in automatic summarization focuses on extractive summarization. The most popular baselines for multi-document summarization fall into one of the following general models: Centrality-based (Radev et al., 2004; Erkan and Radev, 2004; Wang et al., 2008; Ribeiro and de Matos, 2011), Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998; Guo and Sanner, 2010; Sanner et al., 2011; Lim et al., 2012), and Coverage-base methods (Lin and Hovy, 2000; Sipos et al., 2012). Additionally, methods such as KP-CENTRALITY (Ribeiro et al., 2013), which is centrality and coverage-based, follow more than one paradigm. In general, Centrality-based models are used to produce generic summaries, while the MMR family generates query-oriented ones. Coveragebase models produce summaries driven by words, topics or events. Centrality-as-relevance methods bas</context>
</contexts>
<marker>Wang, Li, Zhu, Ding, 2008</marker>
<rawString>Dingding Wang, Tao Li, Shenghuo Zhu, and Chris Ding. 2008. Multi-document summarization via sentencelevel semantic analysis and symmetric matrix factorization. In Proc. of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 307–314. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>