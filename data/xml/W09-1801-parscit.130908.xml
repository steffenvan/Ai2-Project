<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9987915">
Summarization with a Joint Model
for Sentence Extraction and Compression
</title>
<author confidence="0.98701">
Andr´e F. T. Martins*† and Noah A. Smith*
</author>
<affiliation confidence="0.943202">
*School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
†Instituto de Telecomunicac¸˜oes, Instituto Superior T´ecnico, Lisboa, Portugal
</affiliation>
<email confidence="0.995187">
{afm,nasmith}@cs.cmu.edu
</email>
<sectionHeader confidence="0.997335" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997270625">
Text summarization is one of the oldest prob-
lems in natural language processing. Popu-
lar approaches rely on extracting relevant sen-
tences from the original documents. As a side
effect, sentences that are too long but partly
relevant are doomed to either not appear in the
final summary, or prevent inclusion of other
relevant sentences. Sentence compression is a
recent framework that aims to select the short-
est subsequence of words that yields an infor-
mative and grammatical sentence. This work
proposes a one-step approach for document
summarization that jointly performs sentence
extraction and compression by solving an in-
teger linear program. We report favorable ex-
perimental results on newswire data.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999938782608696">
Automatic text summarization dates back to the
1950s and 1960s (Luhn, 1958; Baxendale, 1958; Ed-
mundson, 1969). Today, the proliferation of digital
information makes research on summarization tech-
nologies more important than ever before. In the last
two decades, machine learning techniques have been
employed in extractive summarization of single
documents (Kupiec et al., 1995; Aone et al., 1999;
Osborne, 2002) and multiple documents (Radev and
McKeown, 1998; Carbonell and Goldstein, 1998;
Radev et al., 2000). Most of this work aims only
to extract relevant sentences from the original doc-
uments and present them as the summary; this sim-
plification of the problem yields scalable solutions.
Some attention has been devoted by the NLP
community to the related problem of sentence com-
pression (Knight and Marcu, 2000): given a long
sentence, how to maximally compress it into a gram-
matical sentence that still preserves all the rele-
vant information? While sentence compression is
a promising framework with applications, for exam-
ple, in headline generation (Dorr et al., 2003; Jin,
2003), little work has been done to include it as a
module in document summarization systems. Most
existing approaches (with some exceptions, like the
vine-growth model of Daum´e, 2006) use a two-stage
architecture, either by first extracting a certain num-
ber of salient sentences and then feeding them into
a sentence compressor, or by first compressing all
sentences and extracting later. However, regardless
of which operation is performed first—compression
or extraction—two-step “pipeline” approaches may
fail to find overall-optimal solutions; often the sum-
maries are not better that the ones produced by ex-
tractive summarization. On the other hand, a pilot
study carried out by Lin (2003) suggests that sum-
marization systems that perform sentence compres-
sion have the potential to beat pure extractive sys-
tems if they model cross-sentence effects.
In this work, we address this issue by merging the
tasks of sentence extraction and sentence compres-
sion into a global optimization problem. A careful
design of the objective function encourages “sparse
solutions,” i.e., solutions that involve only a small
number of sentences whose compressions are to be
included in the summary. Our contributions are:
</bodyText>
<listItem confidence="0.99799725">
• We cast joint sentence extraction and compression
as an integer linear program (ILP);
• We provide a new formulation of sentence com-
pression using dependency parsing information
that only requires a linear number of variables,
and combine it with a bigram model;
• We show how the full model can be trained in a
max-margin framework. Since a dataset of sum-
maries comprised of extracted, compressed sen-
tences is unavailable, we present a procedure that
trains the compression and extraction models sep-
arately and tunes a parameter to interpolate the
</listItem>
<page confidence="0.772288">
1
</page>
<note confidence="0.991755">
Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 1–9,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.98632">
two models.
The compression model and the full system are
compared with state-of-the-art baselines in standard
newswire datasets. This paper is organized as fol-
lows: §2–3 provide an overview of our two building
blocks, sentence extraction and sentence compres-
sion. §4 describes our method to perform one-step
sentence compression and extraction. §5 shows ex-
periments in newswire data. Finally, §6 concludes
the paper and suggests future work.
</bodyText>
<sectionHeader confidence="0.993757" genericHeader="method">
2 Extractive summarization
</sectionHeader>
<bodyText confidence="0.988477095238095">
Extractive summarization builds a summary by ex-
tracting a few informative sentences from the docu-
ments. Let D °_ {t1, ... , tM} be a set of sentences,
contained in a single or in multiple related docu-
ments.1 The goal is to extract the best sequence of
sentences hti1, ... , ti,i that summarizes D whose
total length does not exceed a fixed budget of J
words. We describe some well-known approaches
that wil serve as our experimental baselines.
Extract the leading sentences (Lead). For
single-document summarization, the simplest
method consists of greedily extracting the leading
sentences while they fit into the summary. A sen-
tence is skipped if its inclusion exceeds the budget,
and the next is examined. This performs extremely
well in newswire articles, due to the journalistic
convention of summarizing the article first.
Rank by relevance (Rel). This method ranks sen-
tences by a relevance score, and then extracts the top
ones that can fit into the summary. The score is typ-
ically a linear function of feature values:
</bodyText>
<equation confidence="0.986862">
scorerel(ti) O&gt;f(ti) = EDd=1 θdfd(ti), (1)
</equation>
<bodyText confidence="0.983968428571429">
Here, each fd(ti) is a feature extracted from sen-
tence ti, and θd is the corresponding weight. In our
experiments, relevance features include (i) the recip-
rocal position in the document, (ii) a binary feature
indicating whether the sentence is the first one, and
(iii) the 1-gram and 2-gram cosine similarity with
the headline and with the full document.
</bodyText>
<footnote confidence="0.9727885">
1For simplicity, we describe a unified framework for single
and multi-document summarization, although they may require
specialized strategies. Here we experiment only with single-
document summarization and assume ti,..., tm are ordered.
</footnote>
<bodyText confidence="0.991491">
Maximal Marginal Relevance (MMR). For long
documents or large collections, it becomes impor-
tant to penalize the redundancy among the extracted
sentences. Carbonell and Goldstein (1998) proposed
greedily adding sentences to the summary S to max-
imize, at each step, a score of the form
</bodyText>
<equation confidence="0.926646">
λ · scorerel(ti) − (1 − λ) · scorered(ti, S), (2)
</equation>
<bodyText confidence="0.997655272727273">
where scorerel(ti) is as in Eq. 1 and scorered(ti, S)
accounts for the redundancy between ti and the cur-
rent summary S. In our experiments, redundancy is
the 1-gram cosine similarity between the sentence
ti and the current summary S. The trade-off be-
tween relevance and redundancy is controlled by
λ ∈ [0, 1], which is tuned on development data.
McDonald (2007) proposed a non-greedy variant
of MMR that takes into account the redundancy be-
tween each pair of candidate sentences. This is cast
as a global optimization problem:
</bodyText>
<equation confidence="0.999836">
λ · Etz∈S scorerel(ti) −
(1 − λ) · Etz,tj∈S scorered(ti, tj), (3)
</equation>
<bodyText confidence="0.933314111111111">
where scorerel(ti)= Orel
&gt;frel(ti), scorered(ti, tj) °_
O&gt;redfred(ti, tj), and frel(ti) and fred(ti, tj) are feature
vectors with corresponding learned weight vectors
Orel and Ored. He has shown how the relevance-based
method and the MMR framework (in the non-greedy
form of Eq. 3) can be cast as an ILP. By introducing
indicator variables hµiii=1,...,M and hµijii,j=1,...,M
with the meanings
</bodyText>
<equation confidence="0.9042511">
µi =
� 1 if ti and tj are both to be extracted
µij = 0 otherwise
(4)
one can reformulate Eq. 3 as an ILP with O(M2)
variables and constraints:
λ · EMi=1 µiscorerel(ti) − (5)
(1 − λ) · EM EM j=1 µijscorered(ti, tj),
i=1
subject to binary constraints µi, µij ∈ {0, 1}, the
</equation>
<bodyText confidence="0.917053666666667">
length constraint EMi=1 µiNi ≤ J (where Ni is the
number of words of the ith sentence), and the fol-
lowing “agreement constraints” for i, j = 1, ... , M
</bodyText>
<equation confidence="0.839055111111111">
Sˆ = arg max
S
�1 if ti is to be extracted
0 otherwise
max
hµzi,hµzji
2
(that impose the logical relation µij = µi n µj):
µij &lt; µi, µij &lt; µj, µij ≥ µi + µj − 1 (6)
</equation>
<bodyText confidence="0.999754">
Let us provide a compact representation of the pro-
gram in Eq. 5 that will be used later. Define our vec-
tor of parameters as θ , [λθrel, −(1−λ)θred]. Pack-
ing all the feature vectors (one for each sentence, and
one for each pair of sentences) into a matrix F,
</bodyText>
<equation confidence="0.52478425">
�
�Frel 0
F , (7)
0 Fred ,
</equation>
<bodyText confidence="0.92460175">
with Frel ,[frel(ti)]1&lt;i&lt;M and Fred ,
[fred(ti,tj)]1&lt;i&lt;j&lt;M, and packing all the variables
µi and µij into a vector µ, the program in Eq. 5 can
be compactly written as
</bodyText>
<equation confidence="0.684006">
max θTFµ, (8)
µ
</equation>
<bodyText confidence="0.99944575">
subject to binary and linear constraints on µ. This
formulation requires O(M2) variables and con-
straints. If we do not penalize sentence redundancy,
the redundancy term may be dropped; in this simpler
case, F = Frel, the vector µ only contains the vari-
ables (µi), and the program in Eq. 8 only requires
O(M) variables and constraints. Our method (to be
presented in §4) will build on this latter formulation.
</bodyText>
<sectionHeader confidence="0.988157" genericHeader="method">
3 Sentence Compression
</sectionHeader>
<bodyText confidence="0.9998376">
Despite its simplicity, extractive summarization has
a few shortcomings: for example, if the original sen-
tences are too long or embed several clauses, there
is no way of preventing lengthy sentences from ap-
pearing in the final summary. The sentence com-
pression framework (Knight and Marcu, 2000) aims
to select the best subsequence of words that still
yields a short, informative and grammatical sen-
tence. Such a sentence compressor is given a sen-
tence t , (w1, ... , wN) as input and outputs a sub-
sequence of length L, c , (wjl, ... , wjL), with
1 &lt; j1 &lt; ... &lt; jL &lt; N. We may represent
this output as a binary vector s of length N, where
sj = 1 iff word wj is included in the compression.
Note that there are O(2N) possible subsequences.
</bodyText>
<subsectionHeader confidence="0.516809">
3.1 Related Work
</subsectionHeader>
<bodyText confidence="0.998518933333333">
Past approaches to sentence compression include
a noisy channel formulation (Knight and Marcu,
2000; Daum´e and Marcu, 2002), heuristic methods
that parse the sentence and then trim constituents ac-
cording to linguistic criteria (Dorr et al., 2003; Zajic
et al., 2006), a pure discriminative model (McDon-
ald, 2006), and an ILP formulation (Clarke and La-
pata, 2008). We next give an overview of the two
latter approaches.
McDonald (2006) uses the outputs of two parsers
(a phrase-based and a dependency parser) as fea-
tures in a discriminative model that decomposes
over pairs of consecutive words. Formally, given a
sentence t = (w1, ... , wN), the score of a compres-
sion c = (wjl, ... , wjL) decomposes as:
</bodyText>
<equation confidence="0.946974">
score(c; t) = ELl=2 θTf(t, jl−1, jl) (9)
</equation>
<bodyText confidence="0.999619">
where f(t, jl−1, jl) are feature vectors that depend
on the original sentence t and consecutive positions
jl−1 and jl, and θ is a learned weight vector. The
factorization in Eq. 9 allows exact decoding with dy-
namic programming.
Clarke and Lapata (2008) cast the problem as an
ILP. In their formulation, Eq. 9 may be expressed as:
</bodyText>
<equation confidence="0.996789625">
N
score(c;t) = αiθTf(t, 0, i) +
i=1
N
βiθTf(t, i,n + 1) +
i=1
N−1� N γijθTf(t, i, j), (10)
i=1 j=i+1
</equation>
<bodyText confidence="0.9998655">
where αi, βi, and γij are additional binary variables
with the following meanings:
</bodyText>
<listItem confidence="0.9514184">
• αi = 1 iff word wi starts the compression;
• βi = 1 iff word wi ends the compression;
• γij = 1 iff words wi and wj appear consecutively
in the compression;
and subject to the following agreement constraints:
</listItem>
<equation confidence="0.958610833333333">
ENi=1 αi = 1
EN 1
i=1 βi =
sj = αj + Ej−1 i=1 γij
N
si = βi + Ej=i+1 γij. (11)
</equation>
<page confidence="0.948849">
3
</page>
<bodyText confidence="0.9990826">
This framework also allows the inclusion of con-
straints to enforce grammaticality.
To compress a sentence, one needs to maximize
the score in Eq. 10 subject to the constraints in
Eq. 11. Representing the variables through
</bodyText>
<equation confidence="0.98226">
ν , hα1,...,αN,β1, ... ,βN,γ11, ... ,γNNi
(12)
</equation>
<bodyText confidence="0.9993235">
and packing the feature vectors into a matrix F, we
obtain the ILP
</bodyText>
<equation confidence="0.545068">
max θTFν (13)
s,ν
</equation>
<bodyText confidence="0.645176333333333">
subject to linear and integer constraints on the vari-
ables s and ν. This particular formulation requires
O(N2) variables and constraints.
</bodyText>
<subsectionHeader confidence="0.997922">
3.2 Proposed Method
</subsectionHeader>
<bodyText confidence="0.999936230769231">
We propose an alternative model for sentence com-
pression that may be formulated as an ILP, as in
Eq. 13, but with only O(N) variables and con-
straints. This formulation is based on the output of a
dependency parser.
Directed arcs in a dependency tree link pairs of
words, namely a head to its modifier. A dependency
parse tree is characterized by a set of labeled arcs
of the form (head, modifier, label); see Fig.1 for an
example. Given a sentence t = hw1, ... , wNi, we
write i = π(j) to denote that the ith word is the
head (the “parent”) of the jth word; if j is the root,
we write π(j) = 0. Let s be the binary vector de-
</bodyText>
<figureCaption confidence="0.9982365">
Figure 1: A dependency parse for an English sentence;
example from McDonald and Satta (2007).
</figureCaption>
<bodyText confidence="0.99928525">
scribing a possible compression c for the sentence
t. For each word j, we consider four possible cases,
accounting for the inclusion or not of j and π(j) in
the compression. We introduce (mutually exclusive)
</bodyText>
<figureCaption confidence="0.880613">
Figure 2: Non-projective dependency graph.
</figureCaption>
<bodyText confidence="0.953219">
binary variables νj11, νj10, νj01, and νj00 to indicate
eac of ese cases, i.e., for a, b ∈ {0, 1},
those that assume each dependency decis
ndeny gaphs must be trees.
</bodyText>
<equation confidence="0.707645">
νjab , sj = a ∧ sπ(j) = b. od-
(14)
</equation>
<bodyText confidence="0.9999592">
Consider feature vectors f11(t, j), f10(t, j), f01(t, j),
and f00(t, j), that look at the surface sentence and at
the status of the word j and its head π(j); these fea-
tures have corresponding weight vectors θ11, θ10,
θ01, and θ00. The score of c is written as:
</bodyText>
<equation confidence="0.98185075">
score(c; t) = �N �a,b∈{0,1} νjabθT abfab(t,j)
j=1
T
= Ea,b∈{0,1} θabFabνab
</equation>
<bodyText confidence="0.9609058">
= θTFν, (15)
where Fab , [fab(t,1), ... , fab(t, N)], νab ,
(νjab)j=1,...,N, θ , (θ11, θ10, θ01, θ00), and F ,
Diag(F11, F10, F01, F00) (a block-diagonal matrix).
We have reached in Eq. 15 an ILP isomorphic to
the one in Eq. 13, but only with O(N) variables.
There are some agreement constraints between the
variables ν and s that reflect the logical relations in
Eq. 14; these may be written as linear inequalities
(cf. Eq. 6), yielding O(N) constraints.
Given this proposal and §3.1, it is also straight-
forward to extend this model to include bigram fea-
tures as in Eq. 10; the combination of dependency
relation features and bigram features yields a model
that is more powerful than both models in Eq. 15 and
Eq. 10. Such a model is expressible as an ILP with
O(N2) variables and constraints, making use of the
variables s, ν, α, β and γ. In §5, we compare the
performance of this model (called “Bigram”) and the
model in Eq. 15 (called “NoBigram”).2
</bodyText>
<sectionHeader confidence="0.977932" genericHeader="method">
4 Joint Compression and Extraction
</sectionHeader>
<bodyText confidence="0.957228323529412">
We ext describe our joint mdl or sentenc com-
arning via the EM algorithm – none of which have
reviously been known t hve exac non-prjve
pression and extraction. Let D , {t1, . .. , tM} be
mplementations.
a set of sentences as in §2, each expressed as a se-
qunc of words, ti , hwi1, ... , wiNii. Following
We then switch fcu to mod at accont for
on-oca information, in particular ariy and neigh
§3, we represent a compression of ti as a binary vec-
ouring pase dn. For systems at model ar
tor si = hsi1, ... , siNii, where sij = 1 iff word wij
graph prolem suggesting that the parsing prob-
2It should be noted that more efficient decoders are possible
that do not require solving an ILP. In particular, inference in the
m is intractable in this case For neighbouring
NoBigram variant can performed in polynomial time with dy-
arse deciions, we extend the work of McDonald
namic programming algorithms that propagate messages along
nd Preira (2006) and shw that modeling vertical
the dependency parse tree; for the Bigram variant, dynamic pro-
eighbourhood makes arsing intractable in add-
gramming can still be employed with some additional storage.
on to modeling hrizontal neghburhoos A con-
Our ILP formulation, however, is more suited to the final goal
equence ofthese results is that t is unlikely that
of performing document summarization (of which our sentence
compression model will be a component); furthermore, it also
ac nnpojectivedpendny psing i table
allows the straightforward inclusion of global linguistic con-
or any model asumptions weaker han those made
straints, which, as shown by Clarke and Lapata (2008), can
y the edge-factord models.
greatly improve the grammaticality of the compressions.
</bodyText>
<page confidence="0.988525">
4
</page>
<bodyText confidence="0.998809777777778">
is included in the compression. Now, define a sum-
mary of D as a set of sentences obtained by extract-
ing and compressing sentences from D. More pre-
cisely, let µ1, ... , µM be binary variables, one for
each sentence ti in D; define µi = 1 iff a compres-
sion of sentence ti is used in the summary. A sum-
mary of D is then represented by the binary vari-
ables (µ1,... , µM, s1, ... , sM). Notice that these
variables are redundant:
</bodyText>
<equation confidence="0.982689">
µi = 0 #? bj E {1, ... , Ni} sij = 0, (16)
</equation>
<bodyText confidence="0.999623090909091">
i.e., an empty compression means that the sentence
is not to be extracted. In the sequel, it will become
clear why this redundancy is convenient.
Most approaches up to now are concerned with ei-
ther extraction or compression, not both at the same
time. We will combine the extraction scores in Eq. 8
and the compression scores in Eq. 15 to obtain a sin-
gle, global optimization problem;3 we rename the
extraction features and parameters to Fe and θe and
the compression features and parameters to Fc and
θc:
</bodyText>
<equation confidence="0.9973555">
T M T ( )
θe Feµ + Ei=1 θc Fciνi, 17
</equation>
<bodyText confidence="0.846676166666667">
subject to agreement constraints on the variables νi
and si (see Eqs. 11 and 14), and new agreement con-
straints on the variables µ and s1, ... , sM to enforce
the relation in Eq. 16:
sij &lt; µi, bi = 1, ... , M, bj = 1, ... , Ni
µiNi
</bodyText>
<equation confidence="0.997092">
Ej=1 sij, bi = 1, ... , M
(18)
</equation>
<bodyText confidence="0.9262905">
The constraint that the length of the summary cannot
exceed J words is encoded as:
</bodyText>
<equation confidence="0.995677">
M=1 Ej Ni
Ei=1 sij &lt; J. (19)
</equation>
<bodyText confidence="0.999798142857143">
All variables are further restricted to be binary. We
also want to avoid picking just a few words from
many sentences, which typically leads to ungram-
matical summaries. Hence it is desirable to obtain
“sparse” solutions with only a few sentences ex-
tracted and compressed (and most components of µ
are zero) To do so, we add the constraint
</bodyText>
<equation confidence="0.531861">
ENi j=1 sij ? µiρNi, i = 1, ... , M, (20)
</equation>
<footnote confidence="0.937269">
3In what follows, we use the formulation in Eq. 8 with-
out the redundancy terms; however these can be included in
a straightforward way, naturally increasing the number of vari-
ables/constraints.
</footnote>
<bodyText confidence="0.999968071428571">
which states, for each sentence ti, that ti should be
ignored or have at least ρNi words extracted. We fix
ρ = 0.8, enforcing compression rates below 80%.4
To learn the model parameters θ = (θe, θc), we
can use a max-margin discriminative learning al-
gorithm like MIRA (Crammer and Singer, 2003),
which is quite effective and scalable. However, there
is not (to our knowledge) a single dataset of ex-
tracted and compressed sentences. Instead, as will
be described in Sec. 5.1, there are separate datasets
of extracted sentences, and datasets of compressed
sentences. Therefore, instead of globally learning
the model parameters, θ = (θe, θc), we propose the
following strategy to learn them separately:
</bodyText>
<listItem confidence="0.8894406">
• Learn θ&apos;e using a corpus of extracted sentences,
• Learn θ&apos; c using a corpus of compressed sentences,
• Tune η so that θ = (θ&apos;e, ηθ&apos;c) has good perfor-
mance on development data. (This is necessary
since each set of weights is learned up to scaling.)
</listItem>
<sectionHeader confidence="0.999425" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.993949">
5.1 Datasets, Evaluation and Environment
</subsectionHeader>
<bodyText confidence="0.990598066666667">
For our experiments, two datasets were used:
The DUC 2002 dataset. This is a collection of
newswire articles, comprised of 59 document clus-
ters. Each document within the collections (out of
a total of 567 documents) has one or two manually
created abstracts with approximately 100 words.5
Clarke’s dataset for sentence compression. This
is the dataset used by Clarke and Lapata (2008). It
contains manually created compressions of 82 news-
paper articles (1,433 sentences) from the British Na-
tional Corpus and the American News Text corpus.6
To evaluate the sentence compressor alone, we
measured the compression rate and the precision,
recall, and F1-measure (both macro and micro-
averaged) with respect to the “gold” compressed
</bodyText>
<footnote confidence="0.745867833333333">
4There are alternative ways to achieve “sparseness,” either
in a soft way, by adding a term −λ F_i pi to the objective, or
using a different hard constraint, like F_i pi ≤ K, to limit the
number of sentences from which to pick words.
5http://duc.nist.gov
6http://homepages.inf.ed.ac.uk/s0460084/data
</footnote>
<figure confidence="0.901635733333333">
max
µ,ν,S
5
Compression Micro-Av. Macro-Av.
Ratio P R F1 P R F1
HedgeTrimmer 57.64% 0.7099 0.5925 0.6459 0.7195 0.6547 0.6367
McDonald (2006) 71.40% 0.7444 0.7697 0.7568 0.7711 0.7852 0.7696
NoBigram
Bigram
71.20%
71.35%
0.7399 0.7626 0.7510
0.7472 0.7720 0.7594
0.7645 0.7730 0.7604
0.7737 0.7848 0.7710
</figure>
<tableCaption confidence="0.5709288">
Table 1: Results for sentence compression in the Clarke’s test dataset (441 sentences) for our implementation of the
baseline systems (HedgeTrimmer and the system described in McDonald, 2006), and the two variants of our model,
NoBigram and Bigram. The compression ratio associated with the reference compressed sentences in this dataset is
69.06%. In the rightmost column, the statistically indistinguishable best results are emboldened, based on a paired
t-test applied to the sequence of Fl measures (p &lt; 0.01).
</tableCaption>
<bodyText confidence="0.994986636363636">
sentences, calculated on unigrams.7
To evaluate the full system, we used Rouge-N
(Lin and Hovy, 2002), a popular n-gram recall-
based automatic evaluation measure. This score
compares the summary produced by a system with
one or more valid reference summaries.
All our experiments were conducted on a PC with
a Intel dual-core processor with 2.66 GHz and 2 Gb
RAM memory. We used ILOG CPLEX, a commer-
cial integer programming solver. The interface with
CPLEX was coded in Java.
</bodyText>
<subsectionHeader confidence="0.998439">
5.2 Sentence Compression
</subsectionHeader>
<bodyText confidence="0.999514754716981">
We split Clarke’s dataset into two partitions, one
used for training (1,188 sentences) and the other for
testing (441 sentences). This dataset includes one
manual compression for each sentence, that we use
as reference for evaluation purposes. Compression
ratio, i.e., the fraction of words included in the com-
pressed sentences, is 69.32% (micro-averaged over
the training partition).
For comparison, two baselines were imple-
mented: a simple compressor based on Hedge Trim-
mer, the headline generation system of Dorr et al.
(2003) and Zajic et al. (2006),8 and the discrimina-
7Notice that this evaluation score is not able to properly cap-
ture the grammaticality of the compression; this is a known is-
sue that typically is addressed by requiring human judgments.
8Hedge Trimmer applies a deterministic compression proce-
dure whose first step is to identify the lowest leftmost S node in
the parse tree that contains a NP and a VP; this node is taken as
the root of the compressed sentence (i.e., all words that are not
spanned by this node are discarded). Further steps described
by Dorr et al. (2003) include removal of low content units, and
an “iterative shortening” loop that keeps removing constituents
until a desired compression ratio is achieved. The best results
were obtained without iterative shortening, which is explained
by the fact that the selection of the lowest leftmost S node (first
tive model described by McDonald (2006), which
captures “soft syntactic evidence” (we reproduced
the same set of features). Both systems require
a phrase-structure parser; we used Collins’ parser
(Collins, 1999);9 the latter system also derives fea-
tures from a dependency parser; we used the MST-
Parser (McDonald et al., 2005).10
We implemented the two variants of our compres-
sor described in §3.2.
NoBigram. This variant factors the compression
score as a sum over individual scores, each depend-
ing on the inclusion or not of each word and its head
in the compression (see Eq. 15). An upper bound of
70% was placed on the compression ratio. As stated
in §3.2, inference amounts to solving an ILP with
O(N) variables and constraints, N being the sen-
tence length. We also used MSTParser to obtain the
dependency parse trees.
Bigram. This variant includes an extra term stand-
ing for a bigram score, which factors as a sum over
pairs of consecutive words. As in McDonald (2006),
we include features that depend on the “in-between”
words in the original sentence that are to be omitted
in the compression.11 As stated in §3.2, inference
through this model can be done by solving an ILP
with O(N2) variables and constraints.
step of the algorithm) already provides significant compression,
as illustrated in Table 1.
</bodyText>
<footnote confidence="0.992604777777778">
9http://people.csail.mit.edu/mcollins/code.
html
10http://sourceforge.net/projects/mstparser
11The major difference between this variant and model of
McDonald (2006) is that the latter employs “soft syntactic ev-
idence” as input features, while we make the dependency rela-
tions part of the output features. All the non-syntactic features
are the same. Apart from this, notice that our variant does not
employ a phrase-structure parser.
</footnote>
<page confidence="0.998676">
6
</page>
<bodyText confidence="0.999961230769231">
For both variants, we used MSTParser to obtain
the dependency parse trees. The model parameters
are learned in a pure discriminative way through a
max-margin approach. We used the 1-best MIRA
algorithm (Crammer and Singer, 2003; McDonald
et al., 2005) for training; this is a fast online algo-
rithm that requires solving the inference problem at
each step. Although inference amounts to solving
an ILP, which in the worst case scales exponentially
with the size of the sentence, training the model is
in practice very fast for the NoBigram model (a few
minutes in the environment described in §5.1) and
fast enough for the Bigram model (a couple of hours
using the same equipment). This is explained by the
fact that sentences don’t usually exceed a few tens
of words, and because of the structure of the ILPs,
whose constraint matrices are very sparse.
Table 1 depicts the micro- and macro-averaged
precision, recall and Fl-measure. We can see that
both variants outperform the Hedge Trimmer base-
line by a great margin, and are in line with the sys-
tem of McDonald (2006); however, none of our vari-
ants employ a phrase-structure parser. We also ob-
serve that our simpler NoBigram variant, which uses
a linear-sized ILP, achieves results similar to these
two systems.
</bodyText>
<subsectionHeader confidence="0.999399">
5.3 Joint Compression and Extraction
</subsectionHeader>
<bodyText confidence="0.9999203125">
For the summarization task, we split the DUC 2002
dataset into a training partition (427 documents) and
a testing partition (140 documents). The training
partition was further split into a training and a de-
velopment set. We evaluated the performance of
Lead, Rel, and MMR as baselines (all are described
in §2). Weights for Rel were learned via the SVM-
Rank algorithm;12 to create a gold-standard ranking,
we sorted the sentences by Rouge-2 score13 (with re-
spect to the human created summaries). We include
a Pipeline baseline as well, which ranks all sentences
by relevance, then includes their compressions (us-
ing the Bigram variant) while they fit into the sum-
mary.
We tested two variants of our joint model, com-
bining the Rel extraction model with (i) the NoBi-
</bodyText>
<footnote confidence="0.98807675">
12SVMRank is implemented in the SVMdight toolkit
(Joachims, 1999), http://svmlight.joachims.org.
13A similar system was implemented that optimizes the
Rouge-1 score instead, but it led to inferior performance.
</footnote>
<table confidence="0.998845285714286">
Rouge-1 Rouge-2
Lead 0.384 f 0.080 0.177 f 0.083
Rel 0.389 f 0.074 0.178 f 0.080
MMR A = 0.25 0.392 f 0.071 0.178 f 0.077
Pipeline 0.380 f 0.073 0.173 f 0.073
Rel + NoBigr rl = 1.5 0.403 f 0.080 0.180 f 0.082
Rel + Bigr rl = 4.0 0.403 f 0.076 0.180 f 0.076
</table>
<tableCaption confidence="0.982661">
Table 2: Results for sentence extraction in the DUC2002
</tableCaption>
<bodyText confidence="0.995776459459459">
dataset (140 documents). Bold indicates the best results
with statistical significance, according to a paired t-test
(p &lt; 0.01); Rouge-2 scores of all systems except Pipeline
are indistinguishable according to the same test, with p &gt;
0.05.
gram compression model (§3.2) and (ii) the Bigram
variant. Each variant was trained with the proce-
dure described in §4. To keep tractability, the in-
ference ILP problem was relaxed (the binary con-
straints were relaxed to unit interval constraints) and
non-integer solution values were rounded to produce
a valid summary, both for training and testing.14
Whenever this procedure yielded a summary longer
than 100 words, we truncated it to fit the word limit.
Table 2 depicts the results of each of the above
systems in terms of Rouge-1 and Rouge-2 scores.
We can see that both variants of our system are able
to achieve the best results in terms of Rouge-1 and
Rouge-2 scores. The suboptimality of extracting and
compressing in separate stages is clear from the ta-
ble, as Pipeline performs worse than the pure ex-
tractive systems. We also note that the configuration
Rel + Bigram is not able to outperform Rel + No-
Bigram, despite being computationally more expen-
sive (about 25 minutes to process the whole test set,
against the 7 minutes taken by the Rel + NoBigram
variant). Fig. 2 exemplifies the summaries produced
by our system. We see that both variants were able
to include new pieces of information in the summary
without sacrificing grammaticality.
These results suggest that our system, being capa-
ble of performing joint sentence extraction and com-
pression to summarize a document, offers a power-
ful alternative to pure extractive systems. Finally, we
note that no labeled datasets currently exist on which
our full model could have been trained with super-
vision; therefore, although inference is performed
</bodyText>
<footnote confidence="0.7599405">
14See Martins et al. (2009) for a study concerning the impact
of LP relaxations in the learning problem.
</footnote>
<page confidence="0.99729">
7
</page>
<note confidence="0.45309">
MMR baseline:
</note>
<bodyText confidence="0.967727666666667">
Australian novelist Peter Carey was awarded the coveted Booker
Prize for fiction Tuesday night for his love story, “Oscar and Lu-
cinda”.
A panel of five judges unanimously announced the award of the
$26,250 prize after an 80-minute deliberation during a banquet at
London’s ancient Guildhall.
Carey, who lives in Sydney with his wife and son, said in a brief
speech that like the other five finalists he had been asked to attend
with a short speech in his pocket in case he won.
</bodyText>
<subsectionHeader confidence="0.637008">
Rel + NoBigram:
</subsectionHeader>
<bodyText confidence="0.992161181818182">
Australian novelist Peter Carey was awarded the coveted Booker
Prize for fiction Tuesday night for his love story, “Oscar and Lu-
cinda”.
A panel of five judges unanimously announced the award of the
$26,250 prize after an 80-minute deliberation during a banquet at
London’s ancient Guildhall.
The judges made their selection from 102 books published in Britain
in the past 12 months and which they read in their homes.
Carey, who lives in Sydney with his wife and son, said in a brief
speech that like the other five finalists he had been asked to attend
with a short speech in his pocket in case he won.
</bodyText>
<subsectionHeader confidence="0.490743">
Rel + Bigram:
</subsectionHeader>
<bodyText confidence="0.95148125">
Australian novelist Peter Carey was awarded the coveted Booker
Prize for fiction Tuesday night for his love story, “Oscar and Lu-
cinda”.
A panel of five judges unanimously announced the award of the
$26,250 prize after an 80-minute deliberation during a banquet at
London’s ancient Guildhall.
He was unsuccessful in the prize competition in 1985 when his
novel, “Illywhacker,” was among the final six.
Carey called the award a “great honor” and he thanked the prize
sponsors for “provoking so much passionate discussion about liter-
ature perhaps there will be more tomorrow”.
Carey was the only non-Briton in the final six.
</bodyText>
<figureCaption confidence="0.999906">
Figure 2: Summaries produced by the strongest base-
line (MMR) and the two variants of our system. Deleted
words are marked as such.
</figureCaption>
<bodyText confidence="0.999821">
jointly, our training procedure had to learn sepa-
rately the extraction and the compression models,
and to tune a scalar parameter to trade off the two
models. We conjecture that a better model could
have been learned if a labeled dataset with extracted
compressed sentences existed.
</bodyText>
<sectionHeader confidence="0.998674" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999992275">
We have presented a summarization system that per-
forms sentence extraction and compression in a sin-
gle step, by casting the problem as an ILP. The sum-
mary optimizes an objective function that includes
both extraction and compression scores. Our model
encourages “sparse” summaries that involve only a
few sentences. Experiments in newswire data sug-
gest that our system is a valid alternative to exist-
ing extraction-based systems. However, it is worth
noting that further evaluation (e.g., human judg-
ments) needs to be carried out to assert the quality
of our summaries, e.g., their grammaticality, some-
thing that the Rouge scores cannot fully capture.
Future work will address the possibility of in-
cluding linguistic features and constraints to further
improve the grammaticality of the produced sum-
maries.
Another straightforward extension is the inclusion
of a redundancy term and a query relevance term
in the objective function. For redundancy, a simi-
lar idea of that of McDonald (2007) can be applied,
yielding a ILP with O(M2 + N) variables and con-
straints (M being the number of sentences and N the
total number of words). However, such model will
take into account the redundancy among the origi-
nal sentences and not their compressions; to model
the redundancy accross compressions, a possibil-
ity is to consider a linear redundancy score (similar
to cosine similarity, but without the normalization),
which would result in an ILP with O(N + Ei Pi 2 )
variables and constraints, where Pi ≤ M is the num-
ber of sentences in which word wi occurs; this is no
worse than O(M2N).
We also intend to model discourse, which, as
shown by Daum´e and Marcu (2002), plays an im-
portant role in document summarization. Another
future direction is to extend our ILP formulations
to more sophisticated models that go beyond word
deletion, like the ones proposed by Cohn and Lapata
(2008).
</bodyText>
<sectionHeader confidence="0.998979" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.984484">
The authors thank the anonymous reviewers for helpful
comments, Yiming Yang for interesting discussions, and
Dipanjan Das and Sourish Chaudhuri for providing their
code. This research was supported by a grant from FCT
through the CMU-Portugal Program and the Informa-
tion and Communications Technologies Institute (ICTI)
at CMU, and also by Priberam Inform´atica.
</bodyText>
<page confidence="0.997023">
8
</page>
<sectionHeader confidence="0.998345" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999859857142858">
C. Aone, M. E. Okurowski, J. Gorlinsky, and B. Larsen.
1999. A trainable summarizer with knowledge ac-
quired from robust nlp techniques. In Advances in Au-
tomatic Text Summarization. MIT Press.
P. B. Baxendale. 1958. Machine-made index for tech-
nical literature—an experiment. IBM Journal of Re-
search Development, 2(4):354–361.
J. Carbonell and J. Goldstein. 1998. The use of MMR,
diversity-based reranking for reordering documents
and producing summaries. In Proc. of SIGIR.
J. Clarke and M. Lapata. 2008. Global inference for
sentence compression an integer linear programming
approach. JAIR, 31:399–429.
T. Cohn and M. Lapata. 2008. Sentence compression
beyond word deletion. In Proc. COLING.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
K. Crammer and Yoram Singer. 2003. Ultraconservative
online algorithms for multiclass problems. J. Mach.
Learn. Res., 3:951–991.
H. Daum´e and D. Marcu. 2002. A noisy-channel model
for document compression. In Proc. ofACL.
H. Daum´e. 2006. Practical Structured Learning Tech-
niquesfor Natural Language Processing. Ph.D. thesis,
University of Southern California.
B. Dorr, D. Zajic, and R. Schwartz. 2003. Hedge trim-
mer: A parse-and-trim approach to headline gener-
ation. In Proc. of HLT-NAACL Text Summarization
Workshop and DUC.
H. P. Edmundson. 1969. New methods in automatic ex-
tracting. Journal of the ACM, 16(2):264–285.
R. Jin. 2003. Statistical Approaches Toward Title Gener-
ation. Ph.D. thesis, Carnegie Mellon University.
T. Joachims. 1999. Making large-scale SVM learning
practical. In Advances in Kernel Methods - Support
Vector Learning. MIT Press.
K. Knight and D. Marcu. 2000. Statistics-based
summarization—step one: Sentence compression. In
Proc. ofAAAI/IAAI.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A trainable
document summarizer. In Proc. of SIGIR.
C.-Y. Lin and E. Hovy. 2002. Manual and automatic
evaluation of summaries. In Proc. of the ACL Work-
shop on Automatic Summarization.
C.-Y. Lin. 2003. Improving summarization performance
by sentence compression-a pilot study. In Proc. of the
Int. Workshop on Inf. Ret. with Asian Languages.
H. P. Luhn. 1958. The automatic creation of litera-
ture abstracts. IBMJournal ofResearch Development,
2(2):159–165.
A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009.
Polyhedral outer approximations with application to
natural language parsing. In Proc. ofICML.
R. T. McDonald, F. Pereira, K. Ribarov, and J. Hajiˇc.
2005. Non-projective dependency parsing using span-
ning tree algorithms. In Proc. ofHLT-EMNLP.
R. McDonald. 2006. Discriminative sentence compres-
sion with soft syntactic constraints. In Proc. of EACL.
R. McDonald. 2007. A study of global inference algo-
rithms in multi-document summarization. In Proc. of
ECIR.
M. Osborne. 2002. Using maximum entropy for sen-
tence extraction. In Proc. of the ACL Workshop on
Automatic Summarization.
D. R. Radev and K. McKeown. 1998. Generating natural
language summaries from multiple on-line sources.
Computational Linguistics, 24(3):469–500.
D. R. Radev, H. Jing, and M. Budzikowska. 2000.
Centroid-based summarization of multiple documents:
sentence extraction, utility-based evaluation, and user
studies. In Proc. of the NAACL-ANLP Workshop on
Automatic Summarization.
D. Zajic, B. Dorr, J. Lin, and R. Schwartz. 2006.
Sentence compression as a component of a multi-
document summarization system. In Proc. of the ACL
DUC Workshop.
</reference>
<page confidence="0.997057">
9
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.604687">
<title confidence="0.9867265">Summarization with a Joint for Sentence Extraction and Compression</title>
<author confidence="0.938175">F T A</author>
<affiliation confidence="0.7829565">of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, de Instituto Superior T´ecnico, Lisboa,</affiliation>
<abstract confidence="0.998904">Text summarization is one of the oldest problems in natural language processing. Popular approaches rely on extracting relevant sentences from the original documents. As a side effect, sentences that are too long but partly relevant are doomed to either not appear in the final summary, or prevent inclusion of other relevant sentences. Sentence compression is a recent framework that aims to select the shortest subsequence of words that yields an informative and grammatical sentence. This work proposes a one-step approach for document summarization that jointly performs sentence extraction and compression by solving an integer linear program. We report favorable experimental results on newswire data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Aone</author>
<author>M E Okurowski</author>
<author>J Gorlinsky</author>
<author>B Larsen</author>
</authors>
<title>A trainable summarizer with knowledge acquired from robust nlp techniques.</title>
<date>1999</date>
<booktitle>In Advances in Automatic Text Summarization.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1431" citStr="Aone et al., 1999" startWordPosition="207" endWordPosition="210">ses a one-step approach for document summarization that jointly performs sentence extraction and compression by solving an integer linear program. We report favorable experimental results on newswire data. 1 Introduction Automatic text summarization dates back to the 1950s and 1960s (Luhn, 1958; Baxendale, 1958; Edmundson, 1969). Today, the proliferation of digital information makes research on summarization technologies more important than ever before. In the last two decades, machine learning techniques have been employed in extractive summarization of single documents (Kupiec et al., 1995; Aone et al., 1999; Osborne, 2002) and multiple documents (Radev and McKeown, 1998; Carbonell and Goldstein, 1998; Radev et al., 2000). Most of this work aims only to extract relevant sentences from the original documents and present them as the summary; this simplification of the problem yields scalable solutions. Some attention has been devoted by the NLP community to the related problem of sentence compression (Knight and Marcu, 2000): given a long sentence, how to maximally compress it into a grammatical sentence that still preserves all the relevant information? While sentence compression is a promising fr</context>
</contexts>
<marker>Aone, Okurowski, Gorlinsky, Larsen, 1999</marker>
<rawString>C. Aone, M. E. Okurowski, J. Gorlinsky, and B. Larsen. 1999. A trainable summarizer with knowledge acquired from robust nlp techniques. In Advances in Automatic Text Summarization. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P B Baxendale</author>
</authors>
<title>Machine-made index for technical literature—an experiment.</title>
<date>1958</date>
<journal>IBM Journal of Research Development,</journal>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="1126" citStr="Baxendale, 1958" startWordPosition="164" endWordPosition="165"> are too long but partly relevant are doomed to either not appear in the final summary, or prevent inclusion of other relevant sentences. Sentence compression is a recent framework that aims to select the shortest subsequence of words that yields an informative and grammatical sentence. This work proposes a one-step approach for document summarization that jointly performs sentence extraction and compression by solving an integer linear program. We report favorable experimental results on newswire data. 1 Introduction Automatic text summarization dates back to the 1950s and 1960s (Luhn, 1958; Baxendale, 1958; Edmundson, 1969). Today, the proliferation of digital information makes research on summarization technologies more important than ever before. In the last two decades, machine learning techniques have been employed in extractive summarization of single documents (Kupiec et al., 1995; Aone et al., 1999; Osborne, 2002) and multiple documents (Radev and McKeown, 1998; Carbonell and Goldstein, 1998; Radev et al., 2000). Most of this work aims only to extract relevant sentences from the original documents and present them as the summary; this simplification of the problem yields scalable solutio</context>
</contexts>
<marker>Baxendale, 1958</marker>
<rawString>P. B. Baxendale. 1958. Machine-made index for technical literature—an experiment. IBM Journal of Research Development, 2(4):354–361.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carbonell</author>
<author>J Goldstein</author>
</authors>
<title>The use of MMR, diversity-based reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In Proc. of SIGIR.</booktitle>
<contexts>
<context position="1526" citStr="Carbonell and Goldstein, 1998" startWordPosition="220" endWordPosition="223">xtraction and compression by solving an integer linear program. We report favorable experimental results on newswire data. 1 Introduction Automatic text summarization dates back to the 1950s and 1960s (Luhn, 1958; Baxendale, 1958; Edmundson, 1969). Today, the proliferation of digital information makes research on summarization technologies more important than ever before. In the last two decades, machine learning techniques have been employed in extractive summarization of single documents (Kupiec et al., 1995; Aone et al., 1999; Osborne, 2002) and multiple documents (Radev and McKeown, 1998; Carbonell and Goldstein, 1998; Radev et al., 2000). Most of this work aims only to extract relevant sentences from the original documents and present them as the summary; this simplification of the problem yields scalable solutions. Some attention has been devoted by the NLP community to the related problem of sentence compression (Knight and Marcu, 2000): given a long sentence, how to maximally compress it into a grammatical sentence that still preserves all the relevant information? While sentence compression is a promising framework with applications, for example, in headline generation (Dorr et al., 2003; Jin, 2003), </context>
<context position="6387" citStr="Carbonell and Goldstein (1998)" startWordPosition="984" endWordPosition="987">the reciprocal position in the document, (ii) a binary feature indicating whether the sentence is the first one, and (iii) the 1-gram and 2-gram cosine similarity with the headline and with the full document. 1For simplicity, we describe a unified framework for single and multi-document summarization, although they may require specialized strategies. Here we experiment only with singledocument summarization and assume ti,..., tm are ordered. Maximal Marginal Relevance (MMR). For long documents or large collections, it becomes important to penalize the redundancy among the extracted sentences. Carbonell and Goldstein (1998) proposed greedily adding sentences to the summary S to maximize, at each step, a score of the form λ · scorerel(ti) − (1 − λ) · scorered(ti, S), (2) where scorerel(ti) is as in Eq. 1 and scorered(ti, S) accounts for the redundancy between ti and the current summary S. In our experiments, redundancy is the 1-gram cosine similarity between the sentence ti and the current summary S. The trade-off between relevance and redundancy is controlled by λ ∈ [0, 1], which is tuned on development data. McDonald (2007) proposed a non-greedy variant of MMR that takes into account the redundancy between each</context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>J. Carbonell and J. Goldstein. 1998. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proc. of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Clarke</author>
<author>M Lapata</author>
</authors>
<title>Global inference for sentence compression an integer linear programming approach.</title>
<date>2008</date>
<journal>JAIR,</journal>
<pages>31--399</pages>
<contexts>
<context position="10129" citStr="Clarke and Lapata, 2008" startWordPosition="1666" endWordPosition="1670">subsequence of length L, c , (wjl, ... , wjL), with 1 &lt; j1 &lt; ... &lt; jL &lt; N. We may represent this output as a binary vector s of length N, where sj = 1 iff word wj is included in the compression. Note that there are O(2N) possible subsequences. 3.1 Related Work Past approaches to sentence compression include a noisy channel formulation (Knight and Marcu, 2000; Daum´e and Marcu, 2002), heuristic methods that parse the sentence and then trim constituents according to linguistic criteria (Dorr et al., 2003; Zajic et al., 2006), a pure discriminative model (McDonald, 2006), and an ILP formulation (Clarke and Lapata, 2008). We next give an overview of the two latter approaches. McDonald (2006) uses the outputs of two parsers (a phrase-based and a dependency parser) as features in a discriminative model that decomposes over pairs of consecutive words. Formally, given a sentence t = (w1, ... , wN), the score of a compression c = (wjl, ... , wjL) decomposes as: score(c; t) = ELl=2 θTf(t, jl−1, jl) (9) where f(t, jl−1, jl) are feature vectors that depend on the original sentence t and consecutive positions jl−1 and jl, and θ is a learned weight vector. The factorization in Eq. 9 allows exact decoding with dynamic p</context>
<context position="15935" citStr="Clarke and Lapata (2008)" startWordPosition="2720" endWordPosition="2723">arse tree; for the Bigram variant, dynamic proeighbourhood makes arsing intractable in addgramming can still be employed with some additional storage. on to modeling hrizontal neghburhoos A conOur ILP formulation, however, is more suited to the final goal equence ofthese results is that t is unlikely that of performing document summarization (of which our sentence compression model will be a component); furthermore, it also ac nnpojectivedpendny psing i table allows the straightforward inclusion of global linguistic conor any model asumptions weaker han those made straints, which, as shown by Clarke and Lapata (2008), can y the edge-factord models. greatly improve the grammaticality of the compressions. 4 is included in the compression. Now, define a summary of D as a set of sentences obtained by extracting and compressing sentences from D. More precisely, let µ1, ... , µM be binary variables, one for each sentence ti in D; define µi = 1 iff a compression of sentence ti is used in the summary. A summary of D is then represented by the binary variables (µ1,... , µM, s1, ... , sM). Notice that these variables are redundant: µi = 0 #? bj E {1, ... , Ni} sij = 0, (16) i.e., an empty compression means that the</context>
<context position="19378" citStr="Clarke and Lapata (2008)" startWordPosition="3352" endWordPosition="3355">ing a corpus of compressed sentences, • Tune η so that θ = (θ&apos;e, ηθ&apos;c) has good performance on development data. (This is necessary since each set of weights is learned up to scaling.) 5 Experiments 5.1 Datasets, Evaluation and Environment For our experiments, two datasets were used: The DUC 2002 dataset. This is a collection of newswire articles, comprised of 59 document clusters. Each document within the collections (out of a total of 567 documents) has one or two manually created abstracts with approximately 100 words.5 Clarke’s dataset for sentence compression. This is the dataset used by Clarke and Lapata (2008). It contains manually created compressions of 82 newspaper articles (1,433 sentences) from the British National Corpus and the American News Text corpus.6 To evaluate the sentence compressor alone, we measured the compression rate and the precision, recall, and F1-measure (both macro and microaveraged) with respect to the “gold” compressed 4There are alternative ways to achieve “sparseness,” either in a soft way, by adding a term −λ F_i pi to the objective, or using a different hard constraint, like F_i pi ≤ K, to limit the number of sentences from which to pick words. 5http://duc.nist.gov 6h</context>
</contexts>
<marker>Clarke, Lapata, 2008</marker>
<rawString>J. Clarke and M. Lapata. 2008. Global inference for sentence compression an integer linear programming approach. JAIR, 31:399–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cohn</author>
<author>M Lapata</author>
</authors>
<title>Sentence compression beyond word deletion.</title>
<date>2008</date>
<booktitle>In Proc. COLING.</booktitle>
<marker>Cohn, Lapata, 2008</marker>
<rawString>T. Cohn and M. Lapata. 2008. Sentence compression beyond word deletion. In Proc. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="22951" citStr="Collins, 1999" startWordPosition="3915" endWordPosition="3916">words that are not spanned by this node are discarded). Further steps described by Dorr et al. (2003) include removal of low content units, and an “iterative shortening” loop that keeps removing constituents until a desired compression ratio is achieved. The best results were obtained without iterative shortening, which is explained by the fact that the selection of the lowest leftmost S node (first tive model described by McDonald (2006), which captures “soft syntactic evidence” (we reproduced the same set of features). Both systems require a phrase-structure parser; we used Collins’ parser (Collins, 1999);9 the latter system also derives features from a dependency parser; we used the MSTParser (McDonald et al., 2005).10 We implemented the two variants of our compressor described in §3.2. NoBigram. This variant factors the compression score as a sum over individual scores, each depending on the inclusion or not of each word and its head in the compression (see Eq. 15). An upper bound of 70% was placed on the compression ratio. As stated in §3.2, inference amounts to solving an ILP with O(N) variables and constraints, N being the sentence length. We also used MSTParser to obtain the dependency p</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--951</pages>
<contexts>
<context position="18281" citStr="Crammer and Singer, 2003" startWordPosition="3172" endWordPosition="3175">s extracted and compressed (and most components of µ are zero) To do so, we add the constraint ENi j=1 sij ? µiρNi, i = 1, ... , M, (20) 3In what follows, we use the formulation in Eq. 8 without the redundancy terms; however these can be included in a straightforward way, naturally increasing the number of variables/constraints. which states, for each sentence ti, that ti should be ignored or have at least ρNi words extracted. We fix ρ = 0.8, enforcing compression rates below 80%.4 To learn the model parameters θ = (θe, θc), we can use a max-margin discriminative learning algorithm like MIRA (Crammer and Singer, 2003), which is quite effective and scalable. However, there is not (to our knowledge) a single dataset of extracted and compressed sentences. Instead, as will be described in Sec. 5.1, there are separate datasets of extracted sentences, and datasets of compressed sentences. Therefore, instead of globally learning the model parameters, θ = (θe, θc), we propose the following strategy to learn them separately: • Learn θ&apos;e using a corpus of extracted sentences, • Learn θ&apos; c using a corpus of compressed sentences, • Tune η so that θ = (θ&apos;e, ηθ&apos;c) has good performance on development data. (This is neces</context>
<context position="24715" citStr="Crammer and Singer, 2003" startWordPosition="4196" endWordPosition="4199">de. html 10http://sourceforge.net/projects/mstparser 11The major difference between this variant and model of McDonald (2006) is that the latter employs “soft syntactic evidence” as input features, while we make the dependency relations part of the output features. All the non-syntactic features are the same. Apart from this, notice that our variant does not employ a phrase-structure parser. 6 For both variants, we used MSTParser to obtain the dependency parse trees. The model parameters are learned in a pure discriminative way through a max-margin approach. We used the 1-best MIRA algorithm (Crammer and Singer, 2003; McDonald et al., 2005) for training; this is a fast online algorithm that requires solving the inference problem at each step. Although inference amounts to solving an ILP, which in the worst case scales exponentially with the size of the sentence, training the model is in practice very fast for the NoBigram model (a few minutes in the environment described in §5.1) and fast enough for the Bigram model (a couple of hours using the same equipment). This is explained by the fact that sentences don’t usually exceed a few tens of words, and because of the structure of the ILPs, whose constraint </context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>K. Crammer and Yoram Singer. 2003. Ultraconservative online algorithms for multiclass problems. J. Mach. Learn. Res., 3:951–991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daum´e</author>
<author>D Marcu</author>
</authors>
<title>A noisy-channel model for document compression.</title>
<date>2002</date>
<booktitle>In Proc. ofACL.</booktitle>
<marker>Daum´e, Marcu, 2002</marker>
<rawString>H. Daum´e and D. Marcu. 2002. A noisy-channel model for document compression. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daum´e</author>
</authors>
<title>Practical Structured Learning Techniquesfor Natural Language Processing.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Southern California.</institution>
<marker>Daum´e, 2006</marker>
<rawString>H. Daum´e. 2006. Practical Structured Learning Techniquesfor Natural Language Processing. Ph.D. thesis, University of Southern California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dorr</author>
<author>D Zajic</author>
<author>R Schwartz</author>
</authors>
<title>Hedge trimmer: A parse-and-trim approach to headline generation.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL Text Summarization Workshop and DUC.</booktitle>
<contexts>
<context position="2112" citStr="Dorr et al., 2003" startWordPosition="316" endWordPosition="319">; Carbonell and Goldstein, 1998; Radev et al., 2000). Most of this work aims only to extract relevant sentences from the original documents and present them as the summary; this simplification of the problem yields scalable solutions. Some attention has been devoted by the NLP community to the related problem of sentence compression (Knight and Marcu, 2000): given a long sentence, how to maximally compress it into a grammatical sentence that still preserves all the relevant information? While sentence compression is a promising framework with applications, for example, in headline generation (Dorr et al., 2003; Jin, 2003), little work has been done to include it as a module in document summarization systems. Most existing approaches (with some exceptions, like the vine-growth model of Daum´e, 2006) use a two-stage architecture, either by first extracting a certain number of salient sentences and then feeding them into a sentence compressor, or by first compressing all sentences and extracting later. However, regardless of which operation is performed first—compression or extraction—two-step “pipeline” approaches may fail to find overall-optimal solutions; often the summaries are not better that the</context>
<context position="10012" citStr="Dorr et al., 2003" startWordPosition="1647" endWordPosition="1650">rammatical sentence. Such a sentence compressor is given a sentence t , (w1, ... , wN) as input and outputs a subsequence of length L, c , (wjl, ... , wjL), with 1 &lt; j1 &lt; ... &lt; jL &lt; N. We may represent this output as a binary vector s of length N, where sj = 1 iff word wj is included in the compression. Note that there are O(2N) possible subsequences. 3.1 Related Work Past approaches to sentence compression include a noisy channel formulation (Knight and Marcu, 2000; Daum´e and Marcu, 2002), heuristic methods that parse the sentence and then trim constituents according to linguistic criteria (Dorr et al., 2003; Zajic et al., 2006), a pure discriminative model (McDonald, 2006), and an ILP formulation (Clarke and Lapata, 2008). We next give an overview of the two latter approaches. McDonald (2006) uses the outputs of two parsers (a phrase-based and a dependency parser) as features in a discriminative model that decomposes over pairs of consecutive words. Formally, given a sentence t = (w1, ... , wN), the score of a compression c = (wjl, ... , wjL) decomposes as: score(c; t) = ELl=2 θTf(t, jl−1, jl) (9) where f(t, jl−1, jl) are feature vectors that depend on the original sentence t and consecutive pos</context>
<context position="21870" citStr="Dorr et al. (2003)" startWordPosition="3737" endWordPosition="3740">programming solver. The interface with CPLEX was coded in Java. 5.2 Sentence Compression We split Clarke’s dataset into two partitions, one used for training (1,188 sentences) and the other for testing (441 sentences). This dataset includes one manual compression for each sentence, that we use as reference for evaluation purposes. Compression ratio, i.e., the fraction of words included in the compressed sentences, is 69.32% (micro-averaged over the training partition). For comparison, two baselines were implemented: a simple compressor based on Hedge Trimmer, the headline generation system of Dorr et al. (2003) and Zajic et al. (2006),8 and the discrimina7Notice that this evaluation score is not able to properly capture the grammaticality of the compression; this is a known issue that typically is addressed by requiring human judgments. 8Hedge Trimmer applies a deterministic compression procedure whose first step is to identify the lowest leftmost S node in the parse tree that contains a NP and a VP; this node is taken as the root of the compressed sentence (i.e., all words that are not spanned by this node are discarded). Further steps described by Dorr et al. (2003) include removal of low content </context>
</contexts>
<marker>Dorr, Zajic, Schwartz, 2003</marker>
<rawString>B. Dorr, D. Zajic, and R. Schwartz. 2003. Hedge trimmer: A parse-and-trim approach to headline generation. In Proc. of HLT-NAACL Text Summarization Workshop and DUC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Edmundson</author>
</authors>
<title>New methods in automatic extracting.</title>
<date>1969</date>
<journal>Journal of the ACM,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="1144" citStr="Edmundson, 1969" startWordPosition="166" endWordPosition="168"> partly relevant are doomed to either not appear in the final summary, or prevent inclusion of other relevant sentences. Sentence compression is a recent framework that aims to select the shortest subsequence of words that yields an informative and grammatical sentence. This work proposes a one-step approach for document summarization that jointly performs sentence extraction and compression by solving an integer linear program. We report favorable experimental results on newswire data. 1 Introduction Automatic text summarization dates back to the 1950s and 1960s (Luhn, 1958; Baxendale, 1958; Edmundson, 1969). Today, the proliferation of digital information makes research on summarization technologies more important than ever before. In the last two decades, machine learning techniques have been employed in extractive summarization of single documents (Kupiec et al., 1995; Aone et al., 1999; Osborne, 2002) and multiple documents (Radev and McKeown, 1998; Carbonell and Goldstein, 1998; Radev et al., 2000). Most of this work aims only to extract relevant sentences from the original documents and present them as the summary; this simplification of the problem yields scalable solutions. Some attention</context>
</contexts>
<marker>Edmundson, 1969</marker>
<rawString>H. P. Edmundson. 1969. New methods in automatic extracting. Journal of the ACM, 16(2):264–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jin</author>
</authors>
<title>Statistical Approaches Toward Title Generation.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="2124" citStr="Jin, 2003" startWordPosition="320" endWordPosition="321">dstein, 1998; Radev et al., 2000). Most of this work aims only to extract relevant sentences from the original documents and present them as the summary; this simplification of the problem yields scalable solutions. Some attention has been devoted by the NLP community to the related problem of sentence compression (Knight and Marcu, 2000): given a long sentence, how to maximally compress it into a grammatical sentence that still preserves all the relevant information? While sentence compression is a promising framework with applications, for example, in headline generation (Dorr et al., 2003; Jin, 2003), little work has been done to include it as a module in document summarization systems. Most existing approaches (with some exceptions, like the vine-growth model of Daum´e, 2006) use a two-stage architecture, either by first extracting a certain number of salient sentences and then feeding them into a sentence compressor, or by first compressing all sentences and extracting later. However, regardless of which operation is performed first—compression or extraction—two-step “pipeline” approaches may fail to find overall-optimal solutions; often the summaries are not better that the ones produc</context>
</contexts>
<marker>Jin, 2003</marker>
<rawString>R. Jin. 2003. Statistical Approaches Toward Title Generation. Ph.D. thesis, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>In Advances in Kernel Methods - Support Vector Learning.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="26618" citStr="Joachims, 1999" startWordPosition="4518" endWordPosition="4519">nt set. We evaluated the performance of Lead, Rel, and MMR as baselines (all are described in §2). Weights for Rel were learned via the SVMRank algorithm;12 to create a gold-standard ranking, we sorted the sentences by Rouge-2 score13 (with respect to the human created summaries). We include a Pipeline baseline as well, which ranks all sentences by relevance, then includes their compressions (using the Bigram variant) while they fit into the summary. We tested two variants of our joint model, combining the Rel extraction model with (i) the NoBi12SVMRank is implemented in the SVMdight toolkit (Joachims, 1999), http://svmlight.joachims.org. 13A similar system was implemented that optimizes the Rouge-1 score instead, but it led to inferior performance. Rouge-1 Rouge-2 Lead 0.384 f 0.080 0.177 f 0.083 Rel 0.389 f 0.074 0.178 f 0.080 MMR A = 0.25 0.392 f 0.071 0.178 f 0.077 Pipeline 0.380 f 0.073 0.173 f 0.073 Rel + NoBigr rl = 1.5 0.403 f 0.080 0.180 f 0.082 Rel + Bigr rl = 4.0 0.403 f 0.076 0.180 f 0.076 Table 2: Results for sentence extraction in the DUC2002 dataset (140 documents). Bold indicates the best results with statistical significance, according to a paired t-test (p &lt; 0.01); Rouge-2 score</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Making large-scale SVM learning practical. In Advances in Kernel Methods - Support Vector Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>Statistics-based summarization—step one: Sentence compression.</title>
<date>2000</date>
<booktitle>In Proc. ofAAAI/IAAI.</booktitle>
<contexts>
<context position="1854" citStr="Knight and Marcu, 2000" startWordPosition="275" endWordPosition="278">ologies more important than ever before. In the last two decades, machine learning techniques have been employed in extractive summarization of single documents (Kupiec et al., 1995; Aone et al., 1999; Osborne, 2002) and multiple documents (Radev and McKeown, 1998; Carbonell and Goldstein, 1998; Radev et al., 2000). Most of this work aims only to extract relevant sentences from the original documents and present them as the summary; this simplification of the problem yields scalable solutions. Some attention has been devoted by the NLP community to the related problem of sentence compression (Knight and Marcu, 2000): given a long sentence, how to maximally compress it into a grammatical sentence that still preserves all the relevant information? While sentence compression is a promising framework with applications, for example, in headline generation (Dorr et al., 2003; Jin, 2003), little work has been done to include it as a module in document summarization systems. Most existing approaches (with some exceptions, like the vine-growth model of Daum´e, 2006) use a two-stage architecture, either by first extracting a certain number of salient sentences and then feeding them into a sentence compressor, or b</context>
<context position="9305" citStr="Knight and Marcu, 2000" startWordPosition="1516" endWordPosition="1519">. If we do not penalize sentence redundancy, the redundancy term may be dropped; in this simpler case, F = Frel, the vector µ only contains the variables (µi), and the program in Eq. 8 only requires O(M) variables and constraints. Our method (to be presented in §4) will build on this latter formulation. 3 Sentence Compression Despite its simplicity, extractive summarization has a few shortcomings: for example, if the original sentences are too long or embed several clauses, there is no way of preventing lengthy sentences from appearing in the final summary. The sentence compression framework (Knight and Marcu, 2000) aims to select the best subsequence of words that still yields a short, informative and grammatical sentence. Such a sentence compressor is given a sentence t , (w1, ... , wN) as input and outputs a subsequence of length L, c , (wjl, ... , wjL), with 1 &lt; j1 &lt; ... &lt; jL &lt; N. We may represent this output as a binary vector s of length N, where sj = 1 iff word wj is included in the compression. Note that there are O(2N) possible subsequences. 3.1 Related Work Past approaches to sentence compression include a noisy channel formulation (Knight and Marcu, 2000; Daum´e and Marcu, 2002), heuristic met</context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>K. Knight and D. Marcu. 2000. Statistics-based summarization—step one: Sentence compression. In Proc. ofAAAI/IAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
<author>J Pedersen</author>
<author>F Chen</author>
</authors>
<title>A trainable document summarizer.</title>
<date>1995</date>
<booktitle>In Proc. of SIGIR.</booktitle>
<contexts>
<context position="1412" citStr="Kupiec et al., 1995" startWordPosition="203" endWordPosition="206">ence. This work proposes a one-step approach for document summarization that jointly performs sentence extraction and compression by solving an integer linear program. We report favorable experimental results on newswire data. 1 Introduction Automatic text summarization dates back to the 1950s and 1960s (Luhn, 1958; Baxendale, 1958; Edmundson, 1969). Today, the proliferation of digital information makes research on summarization technologies more important than ever before. In the last two decades, machine learning techniques have been employed in extractive summarization of single documents (Kupiec et al., 1995; Aone et al., 1999; Osborne, 2002) and multiple documents (Radev and McKeown, 1998; Carbonell and Goldstein, 1998; Radev et al., 2000). Most of this work aims only to extract relevant sentences from the original documents and present them as the summary; this simplification of the problem yields scalable solutions. Some attention has been devoted by the NLP community to the related problem of sentence compression (Knight and Marcu, 2000): given a long sentence, how to maximally compress it into a grammatical sentence that still preserves all the relevant information? While sentence compressio</context>
</contexts>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>J. Kupiec, J. Pedersen, and F. Chen. 1995. A trainable document summarizer. In Proc. of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
<author>E Hovy</author>
</authors>
<title>Manual and automatic evaluation of summaries.</title>
<date>2002</date>
<booktitle>In Proc. of the ACL Workshop on Automatic Summarization.</booktitle>
<contexts>
<context position="20942" citStr="Lin and Hovy, 2002" startWordPosition="3590" endWordPosition="3593">10 Table 1: Results for sentence compression in the Clarke’s test dataset (441 sentences) for our implementation of the baseline systems (HedgeTrimmer and the system described in McDonald, 2006), and the two variants of our model, NoBigram and Bigram. The compression ratio associated with the reference compressed sentences in this dataset is 69.06%. In the rightmost column, the statistically indistinguishable best results are emboldened, based on a paired t-test applied to the sequence of Fl measures (p &lt; 0.01). sentences, calculated on unigrams.7 To evaluate the full system, we used Rouge-N (Lin and Hovy, 2002), a popular n-gram recallbased automatic evaluation measure. This score compares the summary produced by a system with one or more valid reference summaries. All our experiments were conducted on a PC with a Intel dual-core processor with 2.66 GHz and 2 Gb RAM memory. We used ILOG CPLEX, a commercial integer programming solver. The interface with CPLEX was coded in Java. 5.2 Sentence Compression We split Clarke’s dataset into two partitions, one used for training (1,188 sentences) and the other for testing (441 sentences). This dataset includes one manual compression for each sentence, that we</context>
</contexts>
<marker>Lin, Hovy, 2002</marker>
<rawString>C.-Y. Lin and E. Hovy. 2002. Manual and automatic evaluation of summaries. In Proc. of the ACL Workshop on Automatic Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
</authors>
<title>Improving summarization performance by sentence compression-a pilot study.</title>
<date>2003</date>
<booktitle>In Proc. of the Int. Workshop on Inf. Ret. with Asian Languages.</booktitle>
<contexts>
<context position="2814" citStr="Lin (2003)" startWordPosition="425" endWordPosition="426">systems. Most existing approaches (with some exceptions, like the vine-growth model of Daum´e, 2006) use a two-stage architecture, either by first extracting a certain number of salient sentences and then feeding them into a sentence compressor, or by first compressing all sentences and extracting later. However, regardless of which operation is performed first—compression or extraction—two-step “pipeline” approaches may fail to find overall-optimal solutions; often the summaries are not better that the ones produced by extractive summarization. On the other hand, a pilot study carried out by Lin (2003) suggests that summarization systems that perform sentence compression have the potential to beat pure extractive systems if they model cross-sentence effects. In this work, we address this issue by merging the tasks of sentence extraction and sentence compression into a global optimization problem. A careful design of the objective function encourages “sparse solutions,” i.e., solutions that involve only a small number of sentences whose compressions are to be included in the summary. Our contributions are: • We cast joint sentence extraction and compression as an integer linear program (ILP)</context>
</contexts>
<marker>Lin, 2003</marker>
<rawString>C.-Y. Lin. 2003. Improving summarization performance by sentence compression-a pilot study. In Proc. of the Int. Workshop on Inf. Ret. with Asian Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>The automatic creation of literature abstracts. IBMJournal ofResearch Development,</title>
<date>1958</date>
<contexts>
<context position="1109" citStr="Luhn, 1958" startWordPosition="162" endWordPosition="163">ntences that are too long but partly relevant are doomed to either not appear in the final summary, or prevent inclusion of other relevant sentences. Sentence compression is a recent framework that aims to select the shortest subsequence of words that yields an informative and grammatical sentence. This work proposes a one-step approach for document summarization that jointly performs sentence extraction and compression by solving an integer linear program. We report favorable experimental results on newswire data. 1 Introduction Automatic text summarization dates back to the 1950s and 1960s (Luhn, 1958; Baxendale, 1958; Edmundson, 1969). Today, the proliferation of digital information makes research on summarization technologies more important than ever before. In the last two decades, machine learning techniques have been employed in extractive summarization of single documents (Kupiec et al., 1995; Aone et al., 1999; Osborne, 2002) and multiple documents (Radev and McKeown, 1998; Carbonell and Goldstein, 1998; Radev et al., 2000). Most of this work aims only to extract relevant sentences from the original documents and present them as the summary; this simplification of the problem yields</context>
</contexts>
<marker>Luhn, 1958</marker>
<rawString>H. P. Luhn. 1958. The automatic creation of literature abstracts. IBMJournal ofResearch Development, 2(2):159–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>N A Smith</author>
<author>E P Xing</author>
</authors>
<title>Polyhedral outer approximations with application to natural language parsing.</title>
<date>2009</date>
<booktitle>In Proc. ofICML.</booktitle>
<contexts>
<context position="28956" citStr="Martins et al. (2009)" startWordPosition="4910" endWordPosition="4913">t the 7 minutes taken by the Rel + NoBigram variant). Fig. 2 exemplifies the summaries produced by our system. We see that both variants were able to include new pieces of information in the summary without sacrificing grammaticality. These results suggest that our system, being capable of performing joint sentence extraction and compression to summarize a document, offers a powerful alternative to pure extractive systems. Finally, we note that no labeled datasets currently exist on which our full model could have been trained with supervision; therefore, although inference is performed 14See Martins et al. (2009) for a study concerning the impact of LP relaxations in the learning problem. 7 MMR baseline: Australian novelist Peter Carey was awarded the coveted Booker Prize for fiction Tuesday night for his love story, “Oscar and Lucinda”. A panel of five judges unanimously announced the award of the $26,250 prize after an 80-minute deliberation during a banquet at London’s ancient Guildhall. Carey, who lives in Sydney with his wife and son, said in a brief speech that like the other five finalists he had been asked to attend with a short speech in his pocket in case he won. Rel + NoBigram: Australian n</context>
</contexts>
<marker>Martins, Smith, Xing, 2009</marker>
<rawString>A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009. Polyhedral outer approximations with application to natural language parsing. In Proc. ofICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R T McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proc. ofHLT-EMNLP.</booktitle>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>R. T. McDonald, F. Pereira, K. Ribarov, and J. Hajiˇc. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proc. ofHLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
</authors>
<title>Discriminative sentence compression with soft syntactic constraints.</title>
<date>2006</date>
<booktitle>In Proc. of EACL.</booktitle>
<contexts>
<context position="10079" citStr="McDonald, 2006" startWordPosition="1659" endWordPosition="1661"> , (w1, ... , wN) as input and outputs a subsequence of length L, c , (wjl, ... , wjL), with 1 &lt; j1 &lt; ... &lt; jL &lt; N. We may represent this output as a binary vector s of length N, where sj = 1 iff word wj is included in the compression. Note that there are O(2N) possible subsequences. 3.1 Related Work Past approaches to sentence compression include a noisy channel formulation (Knight and Marcu, 2000; Daum´e and Marcu, 2002), heuristic methods that parse the sentence and then trim constituents according to linguistic criteria (Dorr et al., 2003; Zajic et al., 2006), a pure discriminative model (McDonald, 2006), and an ILP formulation (Clarke and Lapata, 2008). We next give an overview of the two latter approaches. McDonald (2006) uses the outputs of two parsers (a phrase-based and a dependency parser) as features in a discriminative model that decomposes over pairs of consecutive words. Formally, given a sentence t = (w1, ... , wN), the score of a compression c = (wjl, ... , wjL) decomposes as: score(c; t) = ELl=2 θTf(t, jl−1, jl) (9) where f(t, jl−1, jl) are feature vectors that depend on the original sentence t and consecutive positions jl−1 and jl, and θ is a learned weight vector. The factoriza</context>
<context position="20162" citStr="McDonald (2006)" startWordPosition="3475" endWordPosition="3476">he sentence compressor alone, we measured the compression rate and the precision, recall, and F1-measure (both macro and microaveraged) with respect to the “gold” compressed 4There are alternative ways to achieve “sparseness,” either in a soft way, by adding a term −λ F_i pi to the objective, or using a different hard constraint, like F_i pi ≤ K, to limit the number of sentences from which to pick words. 5http://duc.nist.gov 6http://homepages.inf.ed.ac.uk/s0460084/data max µ,ν,S 5 Compression Micro-Av. Macro-Av. Ratio P R F1 P R F1 HedgeTrimmer 57.64% 0.7099 0.5925 0.6459 0.7195 0.6547 0.6367 McDonald (2006) 71.40% 0.7444 0.7697 0.7568 0.7711 0.7852 0.7696 NoBigram Bigram 71.20% 71.35% 0.7399 0.7626 0.7510 0.7472 0.7720 0.7594 0.7645 0.7730 0.7604 0.7737 0.7848 0.7710 Table 1: Results for sentence compression in the Clarke’s test dataset (441 sentences) for our implementation of the baseline systems (HedgeTrimmer and the system described in McDonald, 2006), and the two variants of our model, NoBigram and Bigram. The compression ratio associated with the reference compressed sentences in this dataset is 69.06%. In the rightmost column, the statistically indistinguishable best results are emboldene</context>
<context position="22779" citStr="McDonald (2006)" startWordPosition="3891" endWordPosition="3892">ose first step is to identify the lowest leftmost S node in the parse tree that contains a NP and a VP; this node is taken as the root of the compressed sentence (i.e., all words that are not spanned by this node are discarded). Further steps described by Dorr et al. (2003) include removal of low content units, and an “iterative shortening” loop that keeps removing constituents until a desired compression ratio is achieved. The best results were obtained without iterative shortening, which is explained by the fact that the selection of the lowest leftmost S node (first tive model described by McDonald (2006), which captures “soft syntactic evidence” (we reproduced the same set of features). Both systems require a phrase-structure parser; we used Collins’ parser (Collins, 1999);9 the latter system also derives features from a dependency parser; we used the MSTParser (McDonald et al., 2005).10 We implemented the two variants of our compressor described in §3.2. NoBigram. This variant factors the compression score as a sum over individual scores, each depending on the inclusion or not of each word and its head in the compression (see Eq. 15). An upper bound of 70% was placed on the compression ratio</context>
<context position="24216" citStr="McDonald (2006)" startWordPosition="4117" endWordPosition="4118"> term standing for a bigram score, which factors as a sum over pairs of consecutive words. As in McDonald (2006), we include features that depend on the “in-between” words in the original sentence that are to be omitted in the compression.11 As stated in §3.2, inference through this model can be done by solving an ILP with O(N2) variables and constraints. step of the algorithm) already provides significant compression, as illustrated in Table 1. 9http://people.csail.mit.edu/mcollins/code. html 10http://sourceforge.net/projects/mstparser 11The major difference between this variant and model of McDonald (2006) is that the latter employs “soft syntactic evidence” as input features, while we make the dependency relations part of the output features. All the non-syntactic features are the same. Apart from this, notice that our variant does not employ a phrase-structure parser. 6 For both variants, we used MSTParser to obtain the dependency parse trees. The model parameters are learned in a pure discriminative way through a max-margin approach. We used the 1-best MIRA algorithm (Crammer and Singer, 2003; McDonald et al., 2005) for training; this is a fast online algorithm that requires solving the infe</context>
<context position="25558" citStr="McDonald (2006)" startWordPosition="4344" endWordPosition="4345"> size of the sentence, training the model is in practice very fast for the NoBigram model (a few minutes in the environment described in §5.1) and fast enough for the Bigram model (a couple of hours using the same equipment). This is explained by the fact that sentences don’t usually exceed a few tens of words, and because of the structure of the ILPs, whose constraint matrices are very sparse. Table 1 depicts the micro- and macro-averaged precision, recall and Fl-measure. We can see that both variants outperform the Hedge Trimmer baseline by a great margin, and are in line with the system of McDonald (2006); however, none of our variants employ a phrase-structure parser. We also observe that our simpler NoBigram variant, which uses a linear-sized ILP, achieves results similar to these two systems. 5.3 Joint Compression and Extraction For the summarization task, we split the DUC 2002 dataset into a training partition (427 documents) and a testing partition (140 documents). The training partition was further split into a training and a development set. We evaluated the performance of Lead, Rel, and MMR as baselines (all are described in §2). Weights for Rel were learned via the SVMRank algorithm;1</context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>R. McDonald. 2006. Discriminative sentence compression with soft syntactic constraints. In Proc. of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
</authors>
<title>A study of global inference algorithms in multi-document summarization.</title>
<date>2007</date>
<booktitle>In Proc. of ECIR.</booktitle>
<contexts>
<context position="6898" citStr="McDonald (2007)" startWordPosition="1078" endWordPosition="1079">comes important to penalize the redundancy among the extracted sentences. Carbonell and Goldstein (1998) proposed greedily adding sentences to the summary S to maximize, at each step, a score of the form λ · scorerel(ti) − (1 − λ) · scorered(ti, S), (2) where scorerel(ti) is as in Eq. 1 and scorered(ti, S) accounts for the redundancy between ti and the current summary S. In our experiments, redundancy is the 1-gram cosine similarity between the sentence ti and the current summary S. The trade-off between relevance and redundancy is controlled by λ ∈ [0, 1], which is tuned on development data. McDonald (2007) proposed a non-greedy variant of MMR that takes into account the redundancy between each pair of candidate sentences. This is cast as a global optimization problem: λ · Etz∈S scorerel(ti) − (1 − λ) · Etz,tj∈S scorered(ti, tj), (3) where scorerel(ti)= Orel &gt;frel(ti), scorered(ti, tj) °_ O&gt;redfred(ti, tj), and frel(ti) and fred(ti, tj) are feature vectors with corresponding learned weight vectors Orel and Ored. He has shown how the relevance-based method and the MMR framework (in the non-greedy form of Eq. 3) can be cast as an ILP. By introducing indicator variables hµiii=1,...,M and hµijii,j=1</context>
<context position="32217" citStr="McDonald (2007)" startWordPosition="5457" endWordPosition="5458">alid alternative to existing extraction-based systems. However, it is worth noting that further evaluation (e.g., human judgments) needs to be carried out to assert the quality of our summaries, e.g., their grammaticality, something that the Rouge scores cannot fully capture. Future work will address the possibility of including linguistic features and constraints to further improve the grammaticality of the produced summaries. Another straightforward extension is the inclusion of a redundancy term and a query relevance term in the objective function. For redundancy, a similar idea of that of McDonald (2007) can be applied, yielding a ILP with O(M2 + N) variables and constraints (M being the number of sentences and N the total number of words). However, such model will take into account the redundancy among the original sentences and not their compressions; to model the redundancy accross compressions, a possibility is to consider a linear redundancy score (similar to cosine similarity, but without the normalization), which would result in an ILP with O(N + Ei Pi 2 ) variables and constraints, where Pi ≤ M is the number of sentences in which word wi occurs; this is no worse than O(M2N). We also i</context>
</contexts>
<marker>McDonald, 2007</marker>
<rawString>R. McDonald. 2007. A study of global inference algorithms in multi-document summarization. In Proc. of ECIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Osborne</author>
</authors>
<title>Using maximum entropy for sentence extraction.</title>
<date>2002</date>
<booktitle>In Proc. of the ACL Workshop on Automatic Summarization.</booktitle>
<contexts>
<context position="1447" citStr="Osborne, 2002" startWordPosition="211" endWordPosition="212">oach for document summarization that jointly performs sentence extraction and compression by solving an integer linear program. We report favorable experimental results on newswire data. 1 Introduction Automatic text summarization dates back to the 1950s and 1960s (Luhn, 1958; Baxendale, 1958; Edmundson, 1969). Today, the proliferation of digital information makes research on summarization technologies more important than ever before. In the last two decades, machine learning techniques have been employed in extractive summarization of single documents (Kupiec et al., 1995; Aone et al., 1999; Osborne, 2002) and multiple documents (Radev and McKeown, 1998; Carbonell and Goldstein, 1998; Radev et al., 2000). Most of this work aims only to extract relevant sentences from the original documents and present them as the summary; this simplification of the problem yields scalable solutions. Some attention has been devoted by the NLP community to the related problem of sentence compression (Knight and Marcu, 2000): given a long sentence, how to maximally compress it into a grammatical sentence that still preserves all the relevant information? While sentence compression is a promising framework with app</context>
</contexts>
<marker>Osborne, 2002</marker>
<rawString>M. Osborne. 2002. Using maximum entropy for sentence extraction. In Proc. of the ACL Workshop on Automatic Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Radev</author>
<author>K McKeown</author>
</authors>
<title>Generating natural language summaries from multiple on-line sources.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>3</issue>
<contexts>
<context position="1495" citStr="Radev and McKeown, 1998" startWordPosition="216" endWordPosition="219">intly performs sentence extraction and compression by solving an integer linear program. We report favorable experimental results on newswire data. 1 Introduction Automatic text summarization dates back to the 1950s and 1960s (Luhn, 1958; Baxendale, 1958; Edmundson, 1969). Today, the proliferation of digital information makes research on summarization technologies more important than ever before. In the last two decades, machine learning techniques have been employed in extractive summarization of single documents (Kupiec et al., 1995; Aone et al., 1999; Osborne, 2002) and multiple documents (Radev and McKeown, 1998; Carbonell and Goldstein, 1998; Radev et al., 2000). Most of this work aims only to extract relevant sentences from the original documents and present them as the summary; this simplification of the problem yields scalable solutions. Some attention has been devoted by the NLP community to the related problem of sentence compression (Knight and Marcu, 2000): given a long sentence, how to maximally compress it into a grammatical sentence that still preserves all the relevant information? While sentence compression is a promising framework with applications, for example, in headline generation (</context>
</contexts>
<marker>Radev, McKeown, 1998</marker>
<rawString>D. R. Radev and K. McKeown. 1998. Generating natural language summaries from multiple on-line sources. Computational Linguistics, 24(3):469–500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Radev</author>
<author>H Jing</author>
<author>M Budzikowska</author>
</authors>
<title>Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies.</title>
<date>2000</date>
<booktitle>In Proc. of the NAACL-ANLP Workshop on Automatic Summarization.</booktitle>
<contexts>
<context position="1547" citStr="Radev et al., 2000" startWordPosition="224" endWordPosition="227">lving an integer linear program. We report favorable experimental results on newswire data. 1 Introduction Automatic text summarization dates back to the 1950s and 1960s (Luhn, 1958; Baxendale, 1958; Edmundson, 1969). Today, the proliferation of digital information makes research on summarization technologies more important than ever before. In the last two decades, machine learning techniques have been employed in extractive summarization of single documents (Kupiec et al., 1995; Aone et al., 1999; Osborne, 2002) and multiple documents (Radev and McKeown, 1998; Carbonell and Goldstein, 1998; Radev et al., 2000). Most of this work aims only to extract relevant sentences from the original documents and present them as the summary; this simplification of the problem yields scalable solutions. Some attention has been devoted by the NLP community to the related problem of sentence compression (Knight and Marcu, 2000): given a long sentence, how to maximally compress it into a grammatical sentence that still preserves all the relevant information? While sentence compression is a promising framework with applications, for example, in headline generation (Dorr et al., 2003; Jin, 2003), little work has been </context>
</contexts>
<marker>Radev, Jing, Budzikowska, 2000</marker>
<rawString>D. R. Radev, H. Jing, and M. Budzikowska. 2000. Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies. In Proc. of the NAACL-ANLP Workshop on Automatic Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zajic</author>
<author>B Dorr</author>
<author>J Lin</author>
<author>R Schwartz</author>
</authors>
<title>Sentence compression as a component of a multidocument summarization system.</title>
<date>2006</date>
<booktitle>In Proc. of the ACL DUC Workshop.</booktitle>
<contexts>
<context position="10033" citStr="Zajic et al., 2006" startWordPosition="1651" endWordPosition="1654">. Such a sentence compressor is given a sentence t , (w1, ... , wN) as input and outputs a subsequence of length L, c , (wjl, ... , wjL), with 1 &lt; j1 &lt; ... &lt; jL &lt; N. We may represent this output as a binary vector s of length N, where sj = 1 iff word wj is included in the compression. Note that there are O(2N) possible subsequences. 3.1 Related Work Past approaches to sentence compression include a noisy channel formulation (Knight and Marcu, 2000; Daum´e and Marcu, 2002), heuristic methods that parse the sentence and then trim constituents according to linguistic criteria (Dorr et al., 2003; Zajic et al., 2006), a pure discriminative model (McDonald, 2006), and an ILP formulation (Clarke and Lapata, 2008). We next give an overview of the two latter approaches. McDonald (2006) uses the outputs of two parsers (a phrase-based and a dependency parser) as features in a discriminative model that decomposes over pairs of consecutive words. Formally, given a sentence t = (w1, ... , wN), the score of a compression c = (wjl, ... , wjL) decomposes as: score(c; t) = ELl=2 θTf(t, jl−1, jl) (9) where f(t, jl−1, jl) are feature vectors that depend on the original sentence t and consecutive positions jl−1 and jl, a</context>
<context position="21894" citStr="Zajic et al. (2006)" startWordPosition="3742" endWordPosition="3745"> interface with CPLEX was coded in Java. 5.2 Sentence Compression We split Clarke’s dataset into two partitions, one used for training (1,188 sentences) and the other for testing (441 sentences). This dataset includes one manual compression for each sentence, that we use as reference for evaluation purposes. Compression ratio, i.e., the fraction of words included in the compressed sentences, is 69.32% (micro-averaged over the training partition). For comparison, two baselines were implemented: a simple compressor based on Hedge Trimmer, the headline generation system of Dorr et al. (2003) and Zajic et al. (2006),8 and the discrimina7Notice that this evaluation score is not able to properly capture the grammaticality of the compression; this is a known issue that typically is addressed by requiring human judgments. 8Hedge Trimmer applies a deterministic compression procedure whose first step is to identify the lowest leftmost S node in the parse tree that contains a NP and a VP; this node is taken as the root of the compressed sentence (i.e., all words that are not spanned by this node are discarded). Further steps described by Dorr et al. (2003) include removal of low content units, and an “iterative</context>
</contexts>
<marker>Zajic, Dorr, Lin, Schwartz, 2006</marker>
<rawString>D. Zajic, B. Dorr, J. Lin, and R. Schwartz. 2006. Sentence compression as a component of a multidocument summarization system. In Proc. of the ACL DUC Workshop.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>