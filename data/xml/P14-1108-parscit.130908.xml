<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999263">
A Generalized Language Model as the Combination of Skipped n-grams
and Modified Kneser-Ney Smoothing
</title>
<author confidence="0.999595">
Rene Pickhardt, Thomas Gottron, Martin K¨orner, Steffen Staab
</author>
<affiliation confidence="0.9976615">
Institute for Web Science and Technologies,
University of Koblenz-Landau, Germany
</affiliation>
<note confidence="0.343625">
{rpickhardt,gottron,mkoerner,staab}@uni-koblenz.de
</note>
<title confidence="0.388715">
Paul Georg Wagner and Till Speicher
</title>
<author confidence="0.600189">
Typology GbR
</author>
<email confidence="0.986866">
mail@typology.de
</email>
<sectionHeader confidence="0.997258" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999880692307692">
We introduce a novel approach for build-
ing language models based on a system-
atic, recursive exploration of skip n-gram
models which are interpolated using modi-
fied Kneser-Ney smoothing. Our approach
generalizes language models as it contains
the classical interpolation with lower or-
der models as a special case. In this pa-
per we motivate, formalize and present
our approach. In an extensive empirical
experiment over English text corpora we
demonstrate that our generalized language
models lead to a substantial reduction of
perplexity between 3.1% and 12.7% in
comparison to traditional language mod-
els using modified Kneser-Ney smoothing.
Furthermore, we investigate the behaviour
over three other languages and a domain
specific corpus where we observed consis-
tent improvements. Finally, we also show
that the strength of our approach lies in
its ability to cope in particular with sparse
training data. Using a very small train-
ing data set of only 736 KB text we yield
improvements of even 25.7% reduction of
perplexity.
</bodyText>
<sectionHeader confidence="0.997738" genericHeader="categories and subject descriptors">
1 Introduction motivation
</sectionHeader>
<bodyText confidence="0.999605916666667">
Language Models are a probabilistic approach for
predicting the occurrence of a sequence of words.
They are used in many applications, e.g. word
prediction (Bickel et al., 2005), speech recogni-
tion (Rabiner and Juang, 1993), machine trans-
lation (Brown et al., 1990), or spelling correc-
tion (Mays et al., 1991). The task language models
attempt to solve is the estimation of a probability
of a given sequence of words wl1 = w1, ... , wl.
The probability P(wl1) of this sequence can be
broken down into a product of conditional prob-
abilities:
</bodyText>
<equation confidence="0.999957">
P(wl1) =P(w1) · P(w2|w1) · ...· P(wl|w1 ··· wl_1)
P(wi|w1 ··· wi_1) (1)
</equation>
<bodyText confidence="0.968414756097561">
Because of combinatorial explosion and data
sparsity, it is very difficult to reliably estimate the
probabilities that are conditioned on a longer sub-
sequence. Therefore, by making a Markov as-
sumption the true probability of a word sequence
is only approximated by restricting conditional
probabilities to depend only on a local context
wi−1
i−n+1 of n − 1 preceding words rather than the
full sequence wi−1
1 . The challenge in the construc-
tion of language models is to provide reliable esti-
mators for the conditional probabilities. While the
estimators can be learnt—using, e.g., a maximum
likelihood estimator over n-grams obtained from
training data—the obtained values are not very re-
liable for events which may have been observed
only a few times or not at all in the training data.
Smoothing is a standard technique to over-
come this data sparsity problem. Various smooth-
ing approaches have been developed and ap-
plied in the context of language models. Chen
and Goodman (Chen and Goodman, 1999) in-
troduced modified Kneser-Ney Smoothing, which
up to now has been considered the state-of-the-
art method for language modelling over the last
15 years. Modified Kneser-Ney Smoothing is
an interpolating method which combines the es-
timated conditional probabilities P(wi|wi−1
i−n+1)
recursively with lower order models involving a
shorter local context wi−1
i−n+2 and their estimate for
P(wi|wi−1
i−n+2). The motivation for using lower
order models is that shorter contexts may be ob-
served more often and, thus, suffer less from data
sparsity. However, a single rare word towards the
end of the local context will always cause the con-
text to be observed rarely in the training data and
hence will lead to an unreliable estimation.
</bodyText>
<equation confidence="0.8928105">
=
l
ri
i=1
</equation>
<page confidence="0.931543">
1145
</page>
<note confidence="0.8263695">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1145–1154,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99667184375">
Because of Zipfian word distributions, most
words occur very rarely and hence their true prob-
ability of occurrence may be estimated only very
poorly. One word that appears at the end of a local
context w&amp;quot;−&apos;l
&amp;quot;−��&apos;l and for which only a poor approx-
imation exists may adversely affect the conditional
probabilities in language models of all lengths —
leading to severe errors even for smoothed lan-
guage models. Thus, the idea motivating our ap-
proach is to involve several lower order models
which systematically leave out one position in the
context (one may think of replacing the affected
word in the context with a wildcard) instead of
shortening the sequence only by one word at the
beginning.
This concept of introducing gaps in n-grams
is referred to as skip n-grams (Ney et al., 1994;
Huang et al., 1993). Among other techniques, skip
n-grams have also been considered as an approach
to overcome problems of data sparsity (Goodman,
2001). However, to best of our knowledge, lan-
guage models making use of skip n-grams mod-
els have never been investigated to their full ex-
tent and over different levels of lower order mod-
els. Our approach differs as we consider all pos-
sible combinations of gaps in a local context and
interpolate the higher order model with all possi-
ble lower order models derived from adding gaps
in all different ways.
In this paper we make the following contribu-
tions:
</bodyText>
<listItem confidence="0.9631386">
1. We provide a framework for using modified
Kneser-Ney smoothing in combination with a
systematic exploration of lower order models
based on skip n-grams.
2. We show how our novel approach can indeed
easily be interpreted as a generalized version
of the current state-of-the-art language mod-
els.
3. We present a large scale empirical analysis
of our generalized language models on eight
data sets spanning four different languages,
namely, a wikipedia-based text corpus and
the JRC-Acquis corpus of legislative texts.
4. We empirically observe that introducing skip
n-gram models may reduce perplexity by
12.7% compared to the current state-of-the-
art using modified Kneser-Ney models on
large data sets. Using small training data sets
we observe even higher reductions of per-
plexity of up to 25.6%.
</listItem>
<bodyText confidence="0.999985833333333">
The rest of the paper is organized as follows.
We start with reviewing related work in Section 2.
We will then introduce our generalized language
models in Section 3. After explaining the evalua-
tion methodology and introducing the data sets in
Section 4 we will present the results of our evalu-
ation in Section 5. In Section 6 we discuss why a
generalized language model performs better than
a standard language model. Finally, in Section 7
we summarize our findings and conclude with an
overview of further interesting research challenges
in the field of generalized language models.
</bodyText>
<sectionHeader confidence="0.999918" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999816">
Work related to our generalized language model
approach can be divided in two categories: var-
ious smoothing techniques for language models
and approaches making use of skip n-grams.
Smoothing techniques for language models
have a long history. Their aim is to overcome data
sparsity and provide more reliable estimators—in
particular for rare events. The Good Turing es-
timator (Good, 1953), deleted interpolation (Je-
linek and Mercer, 1980), Katz backoff (Katz,
1987) and Kneser-Ney smoothing (Kneser and
Ney, 1995) are just some of the approaches to
be mentioned. Common strategies of these ap-
proaches are to either backoff to lower order mod-
els when a higher order model lacks sufficient
training data for good estimation, to interpolate
between higher and lower order models or to inter-
polate with a prior distribution. Furthermore, the
estimation of the amount of unseen events from
rare events aims to find the right weights for in-
terpolation as well as for discounting probability
mass from unreliable estimators and to retain it for
unseen events.
The state of the art is a modified version of
Kneser-Ney smoothing introduced in (Chen and
Goodman, 1999). The modified version imple-
ments a recursive interpolation with lower order
models, making use of different discount values
for more or less frequently observed events. This
variation has been compared to other smooth-
ing techniques on various corpora and has shown
to outperform competing approaches. We will
review modified Kneser-Ney smoothing in Sec-
tion 2.1 in more detail as we reuse some ideas to
define our generalized language model.
</bodyText>
<page confidence="0.9879">
1146
</page>
<bodyText confidence="0.9999836875">
Smoothing techniques which do not rely on us-
ing lower order models involve clustering (Brown
et al., 1992; Ney et al., 1994), i.e. grouping to-
gether similar words to form classes of words, as
well as skip n-grams (Ney et al., 1994; Huang et
al., 1993). Yet other approaches make use of per-
mutations of the word order in n-grams (Schukat-
Talamazzini et al., 1995; Goodman, 2001).
Skip n-grams are typically used to incorporate
long distance relations between words. Introduc-
ing the possibility of gaps between the words in
an n-gram allows for capturing word relations be-
yond the level of n consecutive words without an
exponential increase in the parameter space. How-
ever, with their restriction on a subsequence of
words, skip n-grams are also used as a technique
to overcome data sparsity (Goodman, 2001). In re-
lated work different terminology and different def-
initions have been used to describe skip n-grams.
Variations modify the number of words which can
be skipped between elements in an n-gram as well
as the manner in which the skipped words are de-
termined (e.g. fixed patterns (Goodman, 2001) or
functional words (Gao and Suzuki, 2005)).
The impact of various extensions and smooth-
ing techniques for language models is investigated
in (Goodman, 2001; Goodman, 2000). In partic-
ular, the authors compared Kneser-Ney smooth-
ing, Katz backoff smoothing, caching, clustering,
inclusion of higher order n-grams, sentence mix-
ture and skip n-grams. They also evaluated com-
binations of techniques, for instance, using skip
n-gram models in combination with Kneser-Ney
smoothing. The experiments in this case followed
two paths: (1) interpolating a 5-gram model with
lower order distribution introducing a single gap
and (2) interpolating higher order models with
skip n-grams which retained only combinations of
two words. Goodman reported on small data sets
and in the best case a moderate improvement of
cross entropy in the range of 0.02 to 0.04.
In (Guthrie et al., 2006), the authors investi-
gated the increase of observed word combinations
when including skips in n-grams. The conclusion
was that using skip n-grams is often more effective
for increasing the number of observations than in-
creasing the corpus size. This observation aligns
well with our experiments.
</bodyText>
<subsectionHeader confidence="0.994962">
2.1 Review of Modified Kneser-Ney
Smoothing
</subsectionHeader>
<bodyText confidence="0.998825714285714">
We briefly recall modified Kneser-Ney Smoothing
as presented in (Chen and Goodman, 1999). Mod-
ified Kneser-Ney implements smoothing by inter-
polating between higher and lower order n-gram
language models. The highest order distribution
is interpolated with lower order distribution as fol-
lows:
</bodyText>
<equation confidence="0.9993745">
PMKN(wi|wi−1
i−n+1) �
max{c(wii−n+1) − D(c(wii−n+1)), 01
c(i−1
wi−n+1)
+ γhigh(wi−1
i−n+1)�PMKN(wi|wi−1
i−n+2) (2)
</equation>
<bodyText confidence="0.9999931">
where c(wii−n+1) provides the frequency count
that sequence wii−n+1 occurs in training data, D is
a discount value (which depends on the frequency
of the sequence) and γhigh depends on D and is the
interpolation factor to mix in the lower order dis-
tribution&apos;. Essentially, interpolation with a lower
order model corresponds to leaving out the first
word in the considered sequence. The lower order
models are computed differently using the notion
of continuation counts rather than absolute counts:
</bodyText>
<equation confidence="0.998897375">
�PMKN(wi|(wi−1
i−n+1)) �
max{N1+(•wii−n+1) − D(c(wii−n+1)), 01
N1+(•wi−1
i−n+1•)
i−1 l�-_�
+ γmid(wi−n+1) A-N(wi|wi−1
i−n+2)) (3)
</equation>
<bodyText confidence="0.999931">
where the continuation counts are defined as
N1+(•wii−n+1) = |lwi−n : c(wii−n) &gt; 01|, i.e.
the number of different words which precede the
sequence wii−n+1. The term γmid is again an inter-
polation factor which depends on the discounted
probability mass D in the first term of the for-
mula.
</bodyText>
<sectionHeader confidence="0.999385" genericHeader="method">
3 Generalized Language Models
</sectionHeader>
<subsectionHeader confidence="0.992785">
3.1 Notation for Skip n-gram with k Skips
</subsectionHeader>
<bodyText confidence="0.999895333333333">
We express skip n-grams using an operator no-
tation. The operator ∂i applied to an n-gram
removes the word at the i-th position. For in-
stance: ∂3w1w2w3w4 = w1w2 w4, where is
used as wildcard placeholder to indicate a re-
moved word. The wildcard operator allows for
</bodyText>
<footnote confidence="0.971986666666667">
1The factors γ and D are quite technical and lengthy. As
they do not play a significant role for understanding our novel
approach we refer to Appendix A for details.
</footnote>
<page confidence="0.990949">
1147
</page>
<bodyText confidence="0.896030583333333">
larger number of matches. For instance, when
c(w1w2w3aw4) = x and c(w1w2w3bw4) = y then
c(w1w2 w4) &gt; x + y since at least the two se-
quences w1w2w3aw4 and w1w2w3bw4 match the
sequence w1w2 w4. In order to align with stan-
dard language models the skip operator applied to
the first word of a sequence will remove the word
instead of introducing a wildcard. In particular the
equation a1wii−n+1 = wii−n+2 holds where the
right hand side is the subsequence of wii−n+1 omit-
ting the first word. We can thus formulate the in-
terpolation step of modified Kneser-Ney smooth-
</bodyText>
<equation confidence="0.964542">
ˆPMKN(wi|wi−1
i−n+2) =
ˆPMKN(wi|a1wi−1
i−n+1).
</equation>
<bodyText confidence="0.999603333333333">
Thus, our skip n-grams correspond to n-grams
of which we only use k words, after having applied
the skip operators ai1 ... ain−k
</bodyText>
<subsectionHeader confidence="0.969473">
3.2 Generalized Language Model
</subsectionHeader>
<bodyText confidence="0.999125083333333">
Interpolation with lower order models is motivated
by the problem of data sparsity in higher order
models. However, lower order models omit only
the first word in the local context, which might not
necessarily be the cause for the overall n-gram to
be rare. This is the motivation for our general-
ized language models to not only interpolate with
one lower order model, where the first word in a
sequence is omitted, but also with all other skip n-
gram models, where one word is left out. Combin-
ing this idea with modified Kneser-Ney smoothing
leads to a formula similar to (2).
</bodyText>
<equation confidence="0.996812428571429">
i−1
PGLM(wi|wi−n+1) =
max{c(wii−n+1) − D(c(wii−n+1)), 0}
c(i−1
wi−n+1)
PGLM(wi|ajwi−1 i−n+1)
(4)
</equation>
<bodyText confidence="0.993469">
The difference between formula (2) and formula
(4) is the way in which lower order models are
interpolated.
Note, the sum over all possible positions in
the context wi−1
i−n+1 for which we can skip a
word and the according lower order models
PGLM(wi|aj(wi−1
i−n+1)). We give all lower order
models the same weight1
n−1.
The same principle is recursively applied in the
lower order models in which some words of the
full n-gram are already skipped. As in modi-
fied Kneser-Ney smoothing we use continuation
counts for the lower order models, incorporating
the skip operator also for these counts. Incor-
porating this directly into modified Kneser-Ney
smoothing leads in the second highest model to:
</bodyText>
<equation confidence="0.924458">
�PGLM(wi|aj(wi−1
i−n+1)) = (5)
max{N1+(aj(wii−n)) − D(c(aj(wii−n+1))), 0}
N1+(aj(wi−1
i−n+1)•)
�PGLM(wi|ajak(wi−1
i−n+1))
</equation>
<bodyText confidence="0.9995995">
Given that we skip words at different positions,
we have to extend the notion of the count function
and the continuation counts. The count function
applied to a skip n-gram is given by c(aj(wii−n))=
Ew. c(wii ), i.e. we aggregate the count informa-
tion �over all words which fill the gap in the n-
gram. Regarding the continuation counts we de-
fine:
</bodyText>
<equation confidence="0.999942333333333">
N1+(aj(wi i−n)) = |{wi−n+j−1:c(wii−n)&gt;0} |(6)
N1+(aj(wi−1
i−n)•) = |{(wi−n+j−1, wi):c(wi i−n)&gt;0} |(7)
</equation>
<bodyText confidence="0.999929714285714">
As lowest order model we use—just as done for
traditional modified Kneser-Ney (Chen and Good-
man, 1999)—a unigram model interpolated with a
uniform distribution for unseen words.
The overall process is depicted in Figure 1, il-
lustrating how the higher level models are recur-
sively smoothed with several lower order ones.
</bodyText>
<sectionHeader confidence="0.972153" genericHeader="method">
4 Experimental Setup and Data Sets
</sectionHeader>
<bodyText confidence="0.9999835">
To evaluate the quality of our generalized lan-
guage models we empirically compare their abil-
ity to explain sequences of words. To this end we
use text corpora, split them into test and training
data, build language models as well as generalized
language models over the training data and apply
them on the test data. We employ established met-
rics, such as cross entropy and perplexity. In the
following we explain the details of our experimen-
tal setup.
</bodyText>
<subsectionHeader confidence="0.990523">
4.1 Data Sets
</subsectionHeader>
<bodyText confidence="0.999878">
For evaluation purposes we employed eight differ-
ent data sets. The data sets cover different domains
and languages. As languages we considered En-
glish (en), German (de), French (fr), and Italian
(it). As general domain data set we used the full
collection of articles from Wikipedia (wiki) in the
corresponding languages. The download dates of
the dumps are displayed in Table 1.
</bodyText>
<figure confidence="0.7586474">
ing using our notation as
i−1
+ &apos;&apos;high (wi−n+1)
1
n−1E
j=1
n−1
+&apos;&apos;mid(aj(wi−1
i−n+1))
1
n−1�
k=1
ki4j
n−2
1148
</figure>
<figureCaption confidence="0.728942333333333">
Figure 1: Interpolation of models of different or-
der and using skip patterns. The value of n in-
dicates the length of the raw n-grams necessary
</figureCaption>
<bodyText confidence="0.9894558">
for computing the model, the value of k indicates
the number of words actually used in the model.
The wild card symbol marks skipped words in
an n-gram. The arrows indicate how a higher or-
der model is interpolated with lower order mod-
els which skips one word. The bold arrows cor-
respond to interpolation of models in traditional
modified Kneser-Ney smoothing. The lighter ar-
rows illustrate the additional interpolations intro-
duced by our generalized language models.
</bodyText>
<table confidence="0.984424">
de en fr it
Nov 22nd Nov 04th Nov 20th Nov 25th
</table>
<tableCaption confidence="0.8674055">
Table 1: Download dates of Wikipedia snapshots
in November 2013.
</tableCaption>
<bodyText confidence="0.993963666666667">
Special purpose domain data are provided by
the multi-lingual JRC-Acquis corpus of legislative
texts (JRC) (Steinberger et al., 2006). Table 2
gives an overview of the data sets and provides
some simple statistics of the covered languages
and the size of the collections.
</bodyText>
<table confidence="0.865503363636364">
Statistics
total words unique words
in Mio. in Mio.
579 9.82
30.9 0.66
1689 11.7
39.2 0.46
339 4.06
35.8 0.46
193 3.09
34.4 0.47
</table>
<tableCaption confidence="0.9410255">
Table 2: Word statistics and size of of evaluation
corpora
</tableCaption>
<bodyText confidence="0.999545785714286">
The data sets come in the form of structured text
corpora which we cleaned from markup and tok-
enized to generate word sequences. We filtered the
word tokens by removing all character sequences
which did not contain any letter, digit or common
punctuation marks. Eventually, the word token se-
quences were split into word sequences of length
n which provided the basis for the training and
test sets for all algorithms. Note that we did not
perform case-folding nor did we apply stemming
algorithms to normalize the word forms. Also,
we did our evaluation using case sensitive training
and test data. Additionally, we kept all tokens for
named entities such as names of persons or places.
</bodyText>
<subsectionHeader confidence="0.935713">
4.2 Evaluation Methodology
</subsectionHeader>
<bodyText confidence="0.999980967741936">
All data sets have been randomly split into a train-
ing and a test set on a sentence level. The train-
ing sets consist of 80% of the sentences, which
have been used to derive n-grams, skip n-grams
and corresponding continuation counts for values
of n between 1 and 5. Note that we have trained
a prediction model for each data set individually.
From the remaining 20% of the sequences we have
randomly sampled a separate set of 100, 000 se-
quences of 5 words each. These test sequences
have also been shortened to sequences of length 3,
and 4 and provide a basis to conduct our final ex-
periments to evaluate the performance of the dif-
ferent algorithms.
We learnt the generalized language models on
the same split of the training corpus as the stan-
dard language model using modified Kneser-Ney
smoothing and we also used the same set of test se-
quences for a direct comparison. To ensure rigour
and openness of research the data set for training
as well as the test sequences and the entire source
code is open source. 2 3 4 We compared the
probabilities of our language model implementa-
tion (which is a subset of the generalized language
model) using KN as well as MKN smoothing with
the Kyoto Language Model Toolkit 5. Since we
got the same results for small n and small data sets
we believe that our implementation is correct.
In a second experiment we have investigated
the impact of the size of the training data set.
The wikipedia corpus consists of 1.7 bn. words.
</bodyText>
<footnote confidence="0.995896">
2http://west.uni-koblenz.de/Research
3https://github.com/renepickhardt/generalized-language-
modeling-toolkit
4http://glm.rene-pickhardt.de
5http://www.phontron.com/kylm/
</footnote>
<figure confidence="0.990853222222222">
Corpus
wiki-de
JRC-de
wiki-en
JRC-en
wiki-fr
JRC-fr
wiki-it
JRC-it
</figure>
<page confidence="0.994671">
1149
</page>
<bodyText confidence="0.999969866666667">
Thus, the 80% split for training consists of 1.3 bn.
words. We have iteratively created smaller train-
ing sets by decreasing the split factor by an order
of magnitude. So we created 8% / 92% and 0.8%
/ 99.2% split, and so on. We have stopped at the
0.008%/99.992% split as the training data set in
this case consisted of less words than our 100k
test sequences which we still randomly sampled
from the test data of each split. Then we trained
a generalized language model as well as a stan-
dard language model with modified Kneser-Ney
smoothing on each of these samples of the train-
ing data. Again we have evaluated these language
models on the same random sample of 100, 000
sequences as mentioned above.
</bodyText>
<subsectionHeader confidence="0.994782">
4.3 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.9991854">
As evaluation metric we use perplexity: a standard
measure in the field of language models (Manning
and Sch¨utze, 1999). First we calculate the cross
entropy of a trained language model given a test
set using
</bodyText>
<equation confidence="0.989681">
H(Palg) = − � PMLE(s) · lo92 Palg(s) (8)
SET
</equation>
<bodyText confidence="0.9996235">
Where Palg will be replaced by the probability
estimates provided by our generalized language
models and the estimates of a language model us-
ing modified Kneser-Ney smoothing. PMLE, in-
stead, is a maximum likelihood estimator of the
test sequence to occur in the test corpus. Finally,
T is the set of test sequences. The perplexity is
defined as:
</bodyText>
<equation confidence="0.999827">
Perplexity(Palg) = 2H(P-lg) (9)
</equation>
<bodyText confidence="0.964419">
Lower perplexity values indicate better results.
</bodyText>
<sectionHeader confidence="0.817335" genericHeader="method">
5 Results
5.1 Baseline
</sectionHeader>
<bodyText confidence="0.999254166666667">
As a baseline for our generalized language model
(GLM) we have trained standard language models
using modified Kneser-Ney Smoothing (MKN).
These models have been trained for model lengths
3 to 5. For unigram and bigram models MKN and
GLM are identical.
</bodyText>
<subsectionHeader confidence="0.995308">
5.2 Evaluation Experiments
</subsectionHeader>
<bodyText confidence="0.9153982">
The perplexity values for all data sets and various
model orders can be seen in Table 3. In this table
we also present the relative reduction of perplexity
in comparison to the baseline.
model length
</bodyText>
<table confidence="0.98351276">
n = 3 n = 4 n = 5
1074.1 778.5 597.1
1031.1 709.4 521.5
4.0% 8.9% 12.7%
235.4 138.4 94.7
229.4 131.8 86.0
2.5% 4.8% 9.2%
586.9 404 307.3
571.6 378.1 275
2.6% 6.1% 10.5%
147.2 82.9 54.6
145.3 80.6 52.5
1.3% 2.8% 3.9%
538.6 385.9 298.9
526.7 363.8 272.9
2.2% 5.7% 8.7%
155.2 92.5 63.9
153.5 90.1 61.7
1.1% 2.5% 3.5%
738.4 532.9 416.7
718.2 500.7 382.2
2.7% 6.0% 8.3%
177.5 104.4 71.8
175.1 101.8 69.6
1.3% 2.6% 3.1%
</table>
<tableCaption confidence="0.997316">
Table 3: Absolute perplexity values and relative
</tableCaption>
<bodyText confidence="0.976542055555556">
reduction of perplexity from MKN to GLM on all
data sets for models of order 3 to 5
As we can see, the GLM clearly outperforms
the baseline for all model lengths and data sets.
In general we see a larger improvement in perfor-
mance for models of higher orders (n = 5). The
gain for 3-gram models, instead, is negligible. For
German texts the increase in performance is the
highest (12.7%) for a model of order 5. We also
note that GLMs seem to work better on broad do-
main text rather than special purpose text as the
reduction on the wiki corpora is constantly higher
than the reduction of perplexity on the JRC cor-
pora.
We made consistent observations in our second
experiment where we iteratively shrank the size
of the training data set. We calculated the rela-
tive reduction in perplexity from MKN to GLM
</bodyText>
<figure confidence="0.99908568">
Experiments
wiki-de MKN
wiki-de GLM
rel. change
JRC-de MKN
JRC-de GLM
rel. change
wiki-en MKN
wiki-en GLM
rel. change
JRC-en MKN
JRC-en GLM
rel. change
wiki-fr MKN
wiki-fr GLM
rel. change
JRC-fr MKN
JRC-fr GLM
rel. change
wiki-it MKN
wiki-it GLM
rel. change
JRC-it MKN
JRC-it GLM
rel. change
</figure>
<page confidence="0.976548">
1150
</page>
<bodyText confidence="0.999847272727273">
for various model lengths and the different sizes
of the training data. The results for the English
Wikipedia data set are illustrated in Figure 2.
We see that the GLM performs particularly well
on small training data. As the size of the training
data set becomes smaller (even smaller than the
evaluation data), the GLM achieves a reduction of
perplexity of up to 25.7% compared to language
models with modified Kneser-Ney smoothing on
the same data set. The absolute perplexity values
for this experiment are presented in Table 4.
</bodyText>
<figure confidence="0.896607466666667">
model length
Experiments n = 3 n = 4 n = 5
MKNcomplete
GLMcomplete
rel. change 2.6% 6.5% 10.5%
14696.8 2199.8 846.1
13058.7 1902.4 714.4
11.2% 13.5% 15.6%
MKNobserved
GLMobserved
586.9 404 307.3
571.6 378.1 275
MKNunseen
GLMunseen
rel. change
</figure>
<table confidence="0.903974333333333">
220.2 88.0 43.4
220.6 88.3 43.5
rel. change
−0.16% −0.28% −0.15%
model length
Experiments n = 3 n = 4 n = 5
80% MKN 586.9 404 307.3
80% GLM 571.6 378.1 275
rel. change 2.6% 6.5% 10.5%
8% MKN 712.6 539.8 436.5
8% GLM 683.7 492.8 382.5
rel. change 4.1% 8.7% 12.4%
0.8% MKN 894.0 730.0 614.1
0.8% GLM 838.7 650.1 528.7
rel. change 6.2% 10.9% 13.9%
0.08% MKN 1099.5 963.8 845.2
0.08% GLM 996.6 820.7 693.4
rel. change 9.4% 14.9% 18.0%
0.008% MKN 1212.1 1120.5 1009.6
0.008% GLM 1025.6 875.5 750.3
rel. change 15.4% 21.9% 25.7%
</table>
<tableCaption confidence="0.98592">
Table 4: Absolute perplexity values and relative
</tableCaption>
<bodyText confidence="0.994124256410256">
reduction of perplexity from MKN to GLM on
shrunk training data sets for the English Wikipedia
for models of order 3 to 5
Our theory as well as the results so far suggest
that the GLM performs particularly well on sparse
training data. This conjecture has been investi-
gated in a last experiment. For each model length
we have split the test data of the largest English
Wikipedia corpus into two disjoint evaluation data
sets. The data set unseen consists of all test se-
quences which have never been observed in the
training data. The set observed consists only of
test sequences which have been observed at least
once in the training data. Again we have calcu-
lated the perplexity of each set. For reference, also
the values of the complete test data set are shown
in Table 5.
Table 5: Absolute perplexity values and relative
reduction of perplexity from MKN to GLM for the
complete and split test file into observed and un-
seen sequences for models of order 3 to 5. The
data set is the largest English Wikipedia corpus.
As expected we see the overall perplexity values
rise for the unseen test case and decline for the ob-
served test case. More interestingly we see that the
relative reduction of perplexity of the GLM over
MKN increases from 10.5% to 15.6% on the un-
seen test case. This indicates that the superior per-
formance of the GLM on small training corpora
and for higher order models indeed comes from its
good performance properties with regard to sparse
training data. It also confirms that our motivation
to produce lower order n-grams by omitting not
only the first word of the local context but system-
atically all words has been fruitful. However, we
also see that for the observed sequences the GLM
performs slightly worse than MKN. For the ob-
served cases we find the relative change to be neg-
ligible.
</bodyText>
<sectionHeader confidence="0.9987" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999929692307692">
In our experiments we have observed an im-
provement of our generalized language models
over classical language models using Kneser-Ney
smoothing. The improvements have been ob-
served for different languages, different domains
as well as different sizes of the training data. In
the experiments we have also seen that the GLM
performs well in particular for small training data
sets and sparse data, encouraging our initial mo-
tivation. This feature of the GLM is of partic-
ular value, as data sparsity becomes a more and
more immanent problem for higher values of n.
This known fact is underlined also by the statis-
</bodyText>
<page confidence="0.987689">
1151
</page>
<figure confidence="0.815671">
Relative change of perplexity for GLM over MKN
0.1 1 10 100 1000
data set size [mio words]
</figure>
<figureCaption confidence="0.991686">
Figure 2: Variation of the size of the training data on 100k test sequences on the English Wikipedia data
set with different model lengths for GLM.
</figureCaption>
<figure confidence="0.973019583333333">
25%
20%
30%
15%
10%
5%
0%
MKN (baseline) for n=3,4, and 5
n=5
n=4
n=3
relative change in perplexity
</figure>
<bodyText confidence="0.995704285714286">
tics shown in Table 6. The fraction of total n-
grams which appear only once in our Wikipedia
corpus increases for higher values of n. However,
for the same value of n the skip n-grams are less
rare. Our generalized language models leverage
this additional information to obtain more reliable
estimates for the probability of word sequences.
</bodyText>
<equation confidence="0.995783055555556">
w�
1
w1
w1w2
w1 w3
w1 w4
w1 w5
w1w2w3
w1 w3w4
w1w2 w4
w1 w4w5
w1 w3 w5
w1w2 w5
w1w2w3w4
w1 w3w4w5
w1w2 w4w5
w1w2w3 w5
w1w2w3w4w5
</equation>
<bodyText confidence="0.9962376">
Table 6: Percentage of generalized n-grams which
occur only once in the English Wikipedia cor-
pus. Total means a percentage relative to the total
amount of sequences. Unique means a percentage
relative to the amount of unique sequences of this
pattern in the data set.
Beyond the general improvements there is an
additional path for benefitting from generalized
language models. As it is possible to better lever-
age the information in smaller and sparse data sets,
we can build smaller models of competitive per-
formance. For instance, when looking at Table 4
we observe the 3-gram MKN approach on the full
training data set to achieve a perplexity of 586.9.
This model has been trained on 7 GB of text and
the resulting model has a size of 15 GB and 742
Mio. entries for the count and continuation count
values. Looking for a GLM with comparable but
better performance we see that the 5-gram model
trained on 1% of the training data has a perplexity
of 528.7. This GLM model has a size of 9.5 GB
and contains only 427 Mio. entries. So, using a far
smaller set of training data we can build a smaller
model which still demonstrates a competitive per-
formance.
</bodyText>
<sectionHeader confidence="0.99651" genericHeader="method">
7 Conclusion and Future Work
</sectionHeader>
<subsectionHeader confidence="0.920513">
7.1 Conclusion
</subsectionHeader>
<bodyText confidence="0.9999878125">
We have introduced a novel generalized language
model as the systematic combination of skip n-
grams and modified Kneser-Ney smoothing. The
main strength of our approach is the combination
of a simple and elegant idea with an an empiri-
cally convincing result. Mathematically one can
see that the GLM includes the standard language
model with modified Kneser-Ney smoothing as a
sub model and is consequently a real generaliza-
tion.
In an empirical evaluation, we have demon-
strated that for higher orders the GLM outper-
forms MKN for all test cases. The relative im-
provement in perplexity is up to 12.7% for large
data sets. GLMs also performs particularly well
on small and sparse sets of training data. On a very
</bodyText>
<figure confidence="0.999510764705882">
total unique
0.5% 64.0%
5.1% 68.2%
8.0% 79.9%
9.6% 72.1%
10.1% 72.7%
21.1% 77.5%
28.2% 80.4%
28.2% 80.7%
31.7% 81.9%
35.3% 83.0%
31.5% 82.2%
44.7% 85.4%
52.7% 87.6%
52.6% 88.0%
52.3% 87.7%
64.4% 90.7%
</figure>
<page confidence="0.993085">
1152
</page>
<bodyText confidence="0.9999468">
small training data set we observed a reduction of
perplexity by 25.7%. Our experiments underline
that the generalized language models overcome in
particular the weaknesses of modified Kneser-Ney
smoothing on sparse training data.
</bodyText>
<subsectionHeader confidence="0.695135">
7.2 Future work
</subsectionHeader>
<bodyText confidence="0.999992514285714">
A desirable extension of our current definition of
GLMs will be the combination of different lower
lower order models in our generalized language
model using different weights for each model.
Such weights can be used to model the statistical
reliability of the different lower order models. The
value of the weights would have to be chosen ac-
cording to the probability or counts of the respec-
tive skip n-grams.
Another important step that has not been con-
sidered yet is compressing and indexing of gen-
eralized language models to improve the perfor-
mance of the computation and be able to store
them in main memory. Regarding the scalability
of the approach to very large data sets we intend to
apply the Map Reduce techniques from (Heafield
et al., 2013) to our generalized language models in
order to have a more scalable calculation.
This will open the path also to another interest-
ing experiment. Goodman (Goodman, 2001) ob-
served that increasing the length of n-grams in
combination with modified Kneser-Ney smooth-
ing did not lead to improvements for values of
n beyond 7. We believe that our generalized
language models could still benefit from such an
increase. They suffer less from the sparsity of
long n-grams and can overcome this sparsity when
interpolating with the lower order skip n-grams
while benefiting from the larger context.
Finally, it would be interesting to see how ap-
plications of language models—like next word
prediction, machine translation, speech recogni-
tion, text classification, spelling correction, e.g.—
benefit from the better performance of generalized
language models.
</bodyText>
<sectionHeader confidence="0.99775" genericHeader="method">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.997083142857143">
We would like to thank Heinrich Hartmann for
a fruitful discussion regarding notation of the
skip operator for n-grams. The research lead-
ing to these results has received funding from the
European Community’s Seventh Framework Pro-
gramme (FP7/2007-2013), REVEAL (Grant agree
number 610928).
</bodyText>
<sectionHeader confidence="0.995637" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.999779653846154">
Steffen Bickel, Peter Haider, and Tobias Scheffer.
2005. Predicting sentences using n-gram language
models. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, HLT ’05, pages
193–200, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Peter F Brown, John Cocke, Stephen A Della Pietra,
Vincent J Della Pietra, Fredrick Jelinek, John D Laf-
ferty, Robert L Mercer, and Paul S Roossin. 1990.
A statistical approach to machine translation. Com-
putational linguistics, 16(2):79–85.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Comput. Linguist., 18(4):467–479, Decem-
ber.
Stanley Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical report, TR-10-98, Harvard
University, August.
Stanley Chen and Joshua Goodman. 1999. An
empirical study of smoothing techniques for lan-
guage modeling. Computer Speech &amp; Language,
13(4):359–393.
Jianfeng Gao and Hisami Suzuki. 2005. Long dis-
tance dependency in language modeling: An em-
pirical study. In Keh-Yih Su, Junichi Tsujii, Jong-
Hyeok Lee, and OiYee Kwong, editors, Natural
Language Processing IJCNLP 2004, volume 3248
of Lecture Notes in Computer Science, pages 396–
405. Springer Berlin Heidelberg.
Irwin J. Good. 1953. The population frequencies of
species and the estimation of population parameters.
Biometrika, 40(3-4):237–264.
Joshua T. Goodman. 2000. Putting it all together:
language model combination. In Acoustics, Speech,
and Signal Processing, 2000. ICASSP ’00. Proceed-
ings. 2000 IEEE International Conference on, vol-
ume 3, pages 1647–1650 vol.3.
Joshua T. Goodman. 2001. A bit of progress in lan-
guage modeling – extended version. Technical Re-
port MSR-TR-2001-72, Microsoft Research.
David Guthrie, Ben Allison, Wei Liu, Louise Guthrie,
and York Wilks. 2006. A closer look at skip-
gram modelling. In Proceedings LREC’2006, pages
1222–1225.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modified
kneser-ney language model estimation. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics.
</reference>
<page confidence="0.653412">
1153
</page>
<reference confidence="0.99970105">
Xuedong Huang, Fileno Alleva, Hsiao-Wuen Hon,
Mei-Yuh Hwang, Kai-Fu Lee, and Ronald Rosen-
feld. 1993. The sphinx-ii speech recognition sys-
tem: an overview. Computer Speech &amp; Language,
7(2):137 – 148.
F. Jelinek and R.L. Mercer. 1980. Interpolated estima-
tion of markov source parameters from sparse data.
In Proceedings of the Workshop on Pattern Recogni-
tion in Practice, pages 381–397.
S. Katz. 1987. Estimation of probabilities from sparse
data for the language model component of a speech
recognizer. Acoustics, Speech and Signal Process-
ing, IEEE Transactions on, 35(3):400–401.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Acoustics, Speech, and Signal Processing, 1995.
ICASSP-95., 1995 International Conference on, vol-
ume 1, pages 181–184. IEEE.
Christopher D. Manning and Hinrich Sch¨utze. 1999.
Foundations of statistical natural language process-
ing. MIT Press, Cambridge, MA, USA.
Eric Mays, Fred J Damerau, and Robert L Mercer.
1991. Context based spelling correction. Informa-
tion Processing &amp; Management, 27(5):517–522.
Hermann Ney, Ute Essen, and Reinhard Kneser. 1994.
On structuring probabilistic dependences in stochas-
tic language modelling. Computer Speech &amp; Lan-
guage, 8(1):1 – 38.
Lawrence Rabiner and Biing-Hwang Juang. 1993.
Fundamentals of Speech Recognition. Prentice Hall.
Ernst-G¨unter Schukat-Talamazzini, R Hendrych, Ralf
Kompe, and Heinrich Niemann. 1995. Permugram
language models. In Fourth European Conference
on Speech Communication and Technology.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, Dan Tufis, and
Daniel Varga. 2006. The jrc-acquis: A multi-
lingual aligned parallel corpus with 20+ languages.
In LREC’06: Proceedings of the 5th International
Conference on Language Resources and Evaluation.
</reference>
<sectionHeader confidence="0.572503" genericHeader="method">
A Discount Values and Weights in
Modified Kneser Ney
</sectionHeader>
<bodyText confidence="0.998718">
The discount value D(c) used in formula (2) is de-
fined as (Chen and Goodman, 1999):
</bodyText>
<equation confidence="0.99983025">
0 if c = 0
D1 ifc= 1
D2 if c = 2
D3+ if c &gt; 2
</equation>
<bodyText confidence="0.9999535">
The discounting values D1, D2, and D3+ are de-
fined as (Chen and Goodman, 1998)
</bodyText>
<equation confidence="0.989837714285714">
D1 = 1 − 2Y n2 (11a)
n1
D2 = 2 − 3Y n3 (11b)
n2
D3+ = 3 − 4Y n4 (11c)
n3
with Y = n1
</equation>
<bodyText confidence="0.998101333333333">
n1+n2 and ni is the total number of n-
grams which appear exactly i times in the training
data. The weight �high(wi−1
</bodyText>
<equation confidence="0.953937642857143">
i−n+1) is defined as:
1&apos;hi,h(wi−1
i−n+1) = (12)
D1N1(wi−1
i−n+1•)+D2N2(wi−1
i−n+1•)+D3+N3+(wi−1
i−n+1•)
c(i−1
wi−n+1)
And the weight ��id(wi−1
i−n+1) is defined as:
�mid(wi−1
i−n+1) = (13)
D1N1(wi−1
i−n+1•)+D2N2(wi−1
i−n+1•)+D3+N3+(wi−1
i−n+1•)
N1+(•wi−1
i−n+1•)
where N1(wi−1
i−n+1•), N2(wi−1
i−n+1•), and
N1+(N3+(wi−1
i−n+1•) are analogously defined to
wi−1
i−n+1•).
D(c) = I
(10)
</equation>
<page confidence="0.979749">
1154
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.408565">
<title confidence="0.996768">Generalized Language Model as the Combination of Skipped and Modified Kneser-Ney Smoothing</title>
<author confidence="0.999707">Rene Pickhardt</author>
<author confidence="0.999707">Thomas Gottron</author>
<author confidence="0.999707">Martin K¨orner</author>
<author confidence="0.999707">Steffen</author>
<affiliation confidence="0.9988995">Institute for Web Science and University of Koblenz-Landau,</affiliation>
<author confidence="0.598693">Paul Georg Wagner</author>
<author confidence="0.598693">Till Typology GbR</author>
<email confidence="0.987164">mail@typology.de</email>
<abstract confidence="0.997909925925926">We introduce a novel approach for building language models based on a systemrecursive exploration of skip models which are interpolated using modified Kneser-Ney smoothing. Our approach generalizes language models as it contains the classical interpolation with lower order models as a special case. In this paper we motivate, formalize and present our approach. In an extensive empirical experiment over English text corpora we demonstrate that our generalized language models lead to a substantial reduction of between comparison to traditional language models using modified Kneser-Ney smoothing. Furthermore, we investigate the behaviour over three other languages and a domain specific corpus where we observed consistent improvements. Finally, we also show that the strength of our approach lies in its ability to cope in particular with sparse training data. Using a very small traindata set of only text we yield of even of perplexity.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steffen Bickel</author>
<author>Peter Haider</author>
<author>Tobias Scheffer</author>
</authors>
<title>Predicting sentences using n-gram language models.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>193--200</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1600" citStr="Bickel et al., 2005" startWordPosition="232" endWordPosition="235">g modified Kneser-Ney smoothing. Furthermore, we investigate the behaviour over three other languages and a domain specific corpus where we observed consistent improvements. Finally, we also show that the strength of our approach lies in its ability to cope in particular with sparse training data. Using a very small training data set of only 736 KB text we yield improvements of even 25.7% reduction of perplexity. 1 Introduction motivation Language Models are a probabilistic approach for predicting the occurrence of a sequence of words. They are used in many applications, e.g. word prediction (Bickel et al., 2005), speech recognition (Rabiner and Juang, 1993), machine translation (Brown et al., 1990), or spelling correction (Mays et al., 1991). The task language models attempt to solve is the estimation of a probability of a given sequence of words wl1 = w1, ... , wl. The probability P(wl1) of this sequence can be broken down into a product of conditional probabilities: P(wl1) =P(w1) · P(w2|w1) · ...· P(wl|w1 ··· wl_1) P(wi|w1 ··· wi_1) (1) Because of combinatorial explosion and data sparsity, it is very difficult to reliably estimate the probabilities that are conditioned on a longer subsequence. Ther</context>
</contexts>
<marker>Bickel, Haider, Scheffer, 2005</marker>
<rawString>Steffen Bickel, Peter Haider, and Tobias Scheffer. 2005. Predicting sentences using n-gram language models. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 193–200, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>John Cocke</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Fredrick Jelinek</author>
<author>John D Lafferty</author>
<author>Robert L Mercer</author>
<author>Paul S Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational linguistics,</journal>
<pages>16--2</pages>
<contexts>
<context position="1688" citStr="Brown et al., 1990" startWordPosition="246" endWordPosition="249">er languages and a domain specific corpus where we observed consistent improvements. Finally, we also show that the strength of our approach lies in its ability to cope in particular with sparse training data. Using a very small training data set of only 736 KB text we yield improvements of even 25.7% reduction of perplexity. 1 Introduction motivation Language Models are a probabilistic approach for predicting the occurrence of a sequence of words. They are used in many applications, e.g. word prediction (Bickel et al., 2005), speech recognition (Rabiner and Juang, 1993), machine translation (Brown et al., 1990), or spelling correction (Mays et al., 1991). The task language models attempt to solve is the estimation of a probability of a given sequence of words wl1 = w1, ... , wl. The probability P(wl1) of this sequence can be broken down into a product of conditional probabilities: P(wl1) =P(w1) · P(w2|w1) · ...· P(wl|w1 ··· wl_1) P(wi|w1 ··· wi_1) (1) Because of combinatorial explosion and data sparsity, it is very difficult to reliably estimate the probabilities that are conditioned on a longer subsequence. Therefore, by making a Markov assumption the true probability of a word sequence is only app</context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>Peter F Brown, John Cocke, Stephen A Della Pietra, Vincent J Della Pietra, Fredrick Jelinek, John D Lafferty, Robert L Mercer, and Paul S Roossin. 1990. A statistical approach to machine translation. Computational linguistics, 16(2):79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Comput. Linguist.,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="8483" citStr="Brown et al., 1992" startWordPosition="1358" endWordPosition="1361">ion of Kneser-Ney smoothing introduced in (Chen and Goodman, 1999). The modified version implements a recursive interpolation with lower order models, making use of different discount values for more or less frequently observed events. This variation has been compared to other smoothing techniques on various corpora and has shown to outperform competing approaches. We will review modified Kneser-Ney smoothing in Section 2.1 in more detail as we reuse some ideas to define our generalized language model. 1146 Smoothing techniques which do not rely on using lower order models involve clustering (Brown et al., 1992; Ney et al., 1994), i.e. grouping together similar words to form classes of words, as well as skip n-grams (Ney et al., 1994; Huang et al., 1993). Yet other approaches make use of permutations of the word order in n-grams (SchukatTalamazzini et al., 1995; Goodman, 2001). Skip n-grams are typically used to incorporate long distance relations between words. Introducing the possibility of gaps between the words in an n-gram allows for capturing word relations beyond the level of n consecutive words without an exponential increase in the parameter space. However, with their restriction on a subse</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based n-gram models of natural language. Comput. Linguist., 18(4):467–479, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical report, TR-10-98,</tech>
<institution>Harvard University,</institution>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical report, TR-10-98, Harvard University, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1999</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>13</volume>
<issue>4</issue>
<contexts>
<context position="3036" citStr="Chen and Goodman, 1999" startWordPosition="474" endWordPosition="477">er than the full sequence wi−1 1 . The challenge in the construction of language models is to provide reliable estimators for the conditional probabilities. While the estimators can be learnt—using, e.g., a maximum likelihood estimator over n-grams obtained from training data—the obtained values are not very reliable for events which may have been observed only a few times or not at all in the training data. Smoothing is a standard technique to overcome this data sparsity problem. Various smoothing approaches have been developed and applied in the context of language models. Chen and Goodman (Chen and Goodman, 1999) introduced modified Kneser-Ney Smoothing, which up to now has been considered the state-of-theart method for language modelling over the last 15 years. Modified Kneser-Ney Smoothing is an interpolating method which combines the estimated conditional probabilities P(wi|wi−1 i−n+1) recursively with lower order models involving a shorter local context wi−1 i−n+2 and their estimate for P(wi|wi−1 i−n+2). The motivation for using lower order models is that shorter contexts may be observed more often and, thus, suffer less from data sparsity. However, a single rare word towards the end of the local </context>
<context position="7931" citStr="Chen and Goodman, 1999" startWordPosition="1270" endWordPosition="1273">approaches to be mentioned. Common strategies of these approaches are to either backoff to lower order models when a higher order model lacks sufficient training data for good estimation, to interpolate between higher and lower order models or to interpolate with a prior distribution. Furthermore, the estimation of the amount of unseen events from rare events aims to find the right weights for interpolation as well as for discounting probability mass from unreliable estimators and to retain it for unseen events. The state of the art is a modified version of Kneser-Ney smoothing introduced in (Chen and Goodman, 1999). The modified version implements a recursive interpolation with lower order models, making use of different discount values for more or less frequently observed events. This variation has been compared to other smoothing techniques on various corpora and has shown to outperform competing approaches. We will review modified Kneser-Ney smoothing in Section 2.1 in more detail as we reuse some ideas to define our generalized language model. 1146 Smoothing techniques which do not rely on using lower order models involve clustering (Brown et al., 1992; Ney et al., 1994), i.e. grouping together simi</context>
<context position="10780" citStr="Chen and Goodman, 1999" startWordPosition="1725" endWordPosition="1728">h retained only combinations of two words. Goodman reported on small data sets and in the best case a moderate improvement of cross entropy in the range of 0.02 to 0.04. In (Guthrie et al., 2006), the authors investigated the increase of observed word combinations when including skips in n-grams. The conclusion was that using skip n-grams is often more effective for increasing the number of observations than increasing the corpus size. This observation aligns well with our experiments. 2.1 Review of Modified Kneser-Ney Smoothing We briefly recall modified Kneser-Ney Smoothing as presented in (Chen and Goodman, 1999). Modified Kneser-Ney implements smoothing by interpolating between higher and lower order n-gram language models. The highest order distribution is interpolated with lower order distribution as follows: PMKN(wi|wi−1 i−n+1) � max{c(wii−n+1) − D(c(wii−n+1)), 01 c(i−1 wi−n+1) + γhigh(wi−1 i−n+1)�PMKN(wi|wi−1 i−n+2) (2) where c(wii−n+1) provides the frequency count that sequence wii−n+1 occurs in training data, D is a discount value (which depends on the frequency of the sequence) and γhigh depends on D and is the interpolation factor to mix in the lower order distribution&apos;. Essentially, interpol</context>
<context position="15333" citStr="Chen and Goodman, 1999" startWordPosition="2466" endWordPosition="2470"> − D(c(aj(wii−n+1))), 0} N1+(aj(wi−1 i−n+1)•) �PGLM(wi|ajak(wi−1 i−n+1)) Given that we skip words at different positions, we have to extend the notion of the count function and the continuation counts. The count function applied to a skip n-gram is given by c(aj(wii−n))= Ew. c(wii ), i.e. we aggregate the count information �over all words which fill the gap in the ngram. Regarding the continuation counts we define: N1+(aj(wi i−n)) = |{wi−n+j−1:c(wii−n)&gt;0} |(6) N1+(aj(wi−1 i−n)•) = |{(wi−n+j−1, wi):c(wi i−n)&gt;0} |(7) As lowest order model we use—just as done for traditional modified Kneser-Ney (Chen and Goodman, 1999)—a unigram model interpolated with a uniform distribution for unseen words. The overall process is depicted in Figure 1, illustrating how the higher level models are recursively smoothed with several lower order ones. 4 Experimental Setup and Data Sets To evaluate the quality of our generalized language models we empirically compare their ability to explain sequences of words. To this end we use text corpora, split them into test and training data, build language models as well as generalized language models over the training data and apply them on the test data. We employ established metrics,</context>
</contexts>
<marker>Chen, Goodman, 1999</marker>
<rawString>Stanley Chen and Joshua Goodman. 1999. An empirical study of smoothing techniques for language modeling. Computer Speech &amp; Language, 13(4):359–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Hisami Suzuki</author>
</authors>
<title>Long distance dependency in language modeling: An empirical study.</title>
<date>2005</date>
<booktitle>In Keh-Yih Su, Junichi Tsujii, JongHyeok Lee, and OiYee Kwong, editors, Natural Language Processing IJCNLP 2004,</booktitle>
<volume>3248</volume>
<pages>396--405</pages>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="9522" citStr="Gao and Suzuki, 2005" startWordPosition="1533" endWordPosition="1536"> n-gram allows for capturing word relations beyond the level of n consecutive words without an exponential increase in the parameter space. However, with their restriction on a subsequence of words, skip n-grams are also used as a technique to overcome data sparsity (Goodman, 2001). In related work different terminology and different definitions have been used to describe skip n-grams. Variations modify the number of words which can be skipped between elements in an n-gram as well as the manner in which the skipped words are determined (e.g. fixed patterns (Goodman, 2001) or functional words (Gao and Suzuki, 2005)). The impact of various extensions and smoothing techniques for language models is investigated in (Goodman, 2001; Goodman, 2000). In particular, the authors compared Kneser-Ney smoothing, Katz backoff smoothing, caching, clustering, inclusion of higher order n-grams, sentence mixture and skip n-grams. They also evaluated combinations of techniques, for instance, using skip n-gram models in combination with Kneser-Ney smoothing. The experiments in this case followed two paths: (1) interpolating a 5-gram model with lower order distribution introducing a single gap and (2) interpolating higher </context>
</contexts>
<marker>Gao, Suzuki, 2005</marker>
<rawString>Jianfeng Gao and Hisami Suzuki. 2005. Long distance dependency in language modeling: An empirical study. In Keh-Yih Su, Junichi Tsujii, JongHyeok Lee, and OiYee Kwong, editors, Natural Language Processing IJCNLP 2004, volume 3248 of Lecture Notes in Computer Science, pages 396– 405. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irwin J Good</author>
</authors>
<title>The population frequencies of species and the estimation of population parameters.</title>
<date>1953</date>
<journal>Biometrika,</journal>
<pages>40--3</pages>
<contexts>
<context position="7161" citStr="Good, 1953" startWordPosition="1146" endWordPosition="1147">s better than a standard language model. Finally, in Section 7 we summarize our findings and conclude with an overview of further interesting research challenges in the field of generalized language models. 2 Related Work Work related to our generalized language model approach can be divided in two categories: various smoothing techniques for language models and approaches making use of skip n-grams. Smoothing techniques for language models have a long history. Their aim is to overcome data sparsity and provide more reliable estimators—in particular for rare events. The Good Turing estimator (Good, 1953), deleted interpolation (Jelinek and Mercer, 1980), Katz backoff (Katz, 1987) and Kneser-Ney smoothing (Kneser and Ney, 1995) are just some of the approaches to be mentioned. Common strategies of these approaches are to either backoff to lower order models when a higher order model lacks sufficient training data for good estimation, to interpolate between higher and lower order models or to interpolate with a prior distribution. Furthermore, the estimation of the amount of unseen events from rare events aims to find the right weights for interpolation as well as for discounting probability mas</context>
</contexts>
<marker>Good, 1953</marker>
<rawString>Irwin J. Good. 1953. The population frequencies of species and the estimation of population parameters. Biometrika, 40(3-4):237–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua T Goodman</author>
</authors>
<title>Putting it all together: language model combination.</title>
<date>2000</date>
<booktitle>In Acoustics, Speech, and Signal Processing,</booktitle>
<volume>3</volume>
<pages>1647--1650</pages>
<contexts>
<context position="9652" citStr="Goodman, 2000" startWordPosition="1554" endWordPosition="1555">e. However, with their restriction on a subsequence of words, skip n-grams are also used as a technique to overcome data sparsity (Goodman, 2001). In related work different terminology and different definitions have been used to describe skip n-grams. Variations modify the number of words which can be skipped between elements in an n-gram as well as the manner in which the skipped words are determined (e.g. fixed patterns (Goodman, 2001) or functional words (Gao and Suzuki, 2005)). The impact of various extensions and smoothing techniques for language models is investigated in (Goodman, 2001; Goodman, 2000). In particular, the authors compared Kneser-Ney smoothing, Katz backoff smoothing, caching, clustering, inclusion of higher order n-grams, sentence mixture and skip n-grams. They also evaluated combinations of techniques, for instance, using skip n-gram models in combination with Kneser-Ney smoothing. The experiments in this case followed two paths: (1) interpolating a 5-gram model with lower order distribution introducing a single gap and (2) interpolating higher order models with skip n-grams which retained only combinations of two words. Goodman reported on small data sets and in the best </context>
</contexts>
<marker>Goodman, 2000</marker>
<rawString>Joshua T. Goodman. 2000. Putting it all together: language model combination. In Acoustics, Speech, and Signal Processing, 2000. ICASSP ’00. Proceedings. 2000 IEEE International Conference on, volume 3, pages 1647–1650 vol.3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua T Goodman</author>
</authors>
<title>A bit of progress in language modeling – extended version.</title>
<date>2001</date>
<tech>Technical Report MSR-TR-2001-72, Microsoft Research.</tech>
<contexts>
<context position="4922" citStr="Goodman, 2001" startWordPosition="781" endWordPosition="782">els of all lengths — leading to severe errors even for smoothed language models. Thus, the idea motivating our approach is to involve several lower order models which systematically leave out one position in the context (one may think of replacing the affected word in the context with a wildcard) instead of shortening the sequence only by one word at the beginning. This concept of introducing gaps in n-grams is referred to as skip n-grams (Ney et al., 1994; Huang et al., 1993). Among other techniques, skip n-grams have also been considered as an approach to overcome problems of data sparsity (Goodman, 2001). However, to best of our knowledge, language models making use of skip n-grams models have never been investigated to their full extent and over different levels of lower order models. Our approach differs as we consider all possible combinations of gaps in a local context and interpolate the higher order model with all possible lower order models derived from adding gaps in all different ways. In this paper we make the following contributions: 1. We provide a framework for using modified Kneser-Ney smoothing in combination with a systematic exploration of lower order models based on skip n-g</context>
<context position="8754" citStr="Goodman, 2001" startWordPosition="1409" endWordPosition="1410">er smoothing techniques on various corpora and has shown to outperform competing approaches. We will review modified Kneser-Ney smoothing in Section 2.1 in more detail as we reuse some ideas to define our generalized language model. 1146 Smoothing techniques which do not rely on using lower order models involve clustering (Brown et al., 1992; Ney et al., 1994), i.e. grouping together similar words to form classes of words, as well as skip n-grams (Ney et al., 1994; Huang et al., 1993). Yet other approaches make use of permutations of the word order in n-grams (SchukatTalamazzini et al., 1995; Goodman, 2001). Skip n-grams are typically used to incorporate long distance relations between words. Introducing the possibility of gaps between the words in an n-gram allows for capturing word relations beyond the level of n consecutive words without an exponential increase in the parameter space. However, with their restriction on a subsequence of words, skip n-grams are also used as a technique to overcome data sparsity (Goodman, 2001). In related work different terminology and different definitions have been used to describe skip n-grams. Variations modify the number of words which can be skipped betwe</context>
<context position="31511" citStr="Goodman, 2001" startWordPosition="5258" endWordPosition="5259">weights would have to be chosen according to the probability or counts of the respective skip n-grams. Another important step that has not been considered yet is compressing and indexing of generalized language models to improve the performance of the computation and be able to store them in main memory. Regarding the scalability of the approach to very large data sets we intend to apply the Map Reduce techniques from (Heafield et al., 2013) to our generalized language models in order to have a more scalable calculation. This will open the path also to another interesting experiment. Goodman (Goodman, 2001) observed that increasing the length of n-grams in combination with modified Kneser-Ney smoothing did not lead to improvements for values of n beyond 7. We believe that our generalized language models could still benefit from such an increase. They suffer less from the sparsity of long n-grams and can overcome this sparsity when interpolating with the lower order skip n-grams while benefiting from the larger context. Finally, it would be interesting to see how applications of language models—like next word prediction, machine translation, speech recognition, text classification, spelling corre</context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>Joshua T. Goodman. 2001. A bit of progress in language modeling – extended version. Technical Report MSR-TR-2001-72, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Guthrie</author>
<author>Ben Allison</author>
<author>Wei Liu</author>
<author>Louise Guthrie</author>
<author>York Wilks</author>
</authors>
<title>A closer look at skipgram modelling.</title>
<date>2006</date>
<booktitle>In Proceedings LREC’2006,</booktitle>
<pages>1222--1225</pages>
<contexts>
<context position="10352" citStr="Guthrie et al., 2006" startWordPosition="1661" endWordPosition="1664">ing, caching, clustering, inclusion of higher order n-grams, sentence mixture and skip n-grams. They also evaluated combinations of techniques, for instance, using skip n-gram models in combination with Kneser-Ney smoothing. The experiments in this case followed two paths: (1) interpolating a 5-gram model with lower order distribution introducing a single gap and (2) interpolating higher order models with skip n-grams which retained only combinations of two words. Goodman reported on small data sets and in the best case a moderate improvement of cross entropy in the range of 0.02 to 0.04. In (Guthrie et al., 2006), the authors investigated the increase of observed word combinations when including skips in n-grams. The conclusion was that using skip n-grams is often more effective for increasing the number of observations than increasing the corpus size. This observation aligns well with our experiments. 2.1 Review of Modified Kneser-Ney Smoothing We briefly recall modified Kneser-Ney Smoothing as presented in (Chen and Goodman, 1999). Modified Kneser-Ney implements smoothing by interpolating between higher and lower order n-gram language models. The highest order distribution is interpolated with lower</context>
</contexts>
<marker>Guthrie, Allison, Liu, Guthrie, Wilks, 2006</marker>
<rawString>David Guthrie, Ben Allison, Wei Liu, Louise Guthrie, and York Wilks. 2006. A closer look at skipgram modelling. In Proceedings LREC’2006, pages 1222–1225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Ivan Pouzyrevsky</author>
<author>Jonathan H Clark</author>
<author>Philipp Koehn</author>
</authors>
<title>Scalable modified kneser-ney language model estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="31342" citStr="Heafield et al., 2013" startWordPosition="5229" endWordPosition="5232">lized language model using different weights for each model. Such weights can be used to model the statistical reliability of the different lower order models. The value of the weights would have to be chosen according to the probability or counts of the respective skip n-grams. Another important step that has not been considered yet is compressing and indexing of generalized language models to improve the performance of the computation and be able to store them in main memory. Regarding the scalability of the approach to very large data sets we intend to apply the Map Reduce techniques from (Heafield et al., 2013) to our generalized language models in order to have a more scalable calculation. This will open the path also to another interesting experiment. Goodman (Goodman, 2001) observed that increasing the length of n-grams in combination with modified Kneser-Ney smoothing did not lead to improvements for values of n beyond 7. We believe that our generalized language models could still benefit from such an increase. They suffer less from the sparsity of long n-grams and can overcome this sparsity when interpolating with the lower order skip n-grams while benefiting from the larger context. Finally, i</context>
</contexts>
<marker>Heafield, Pouzyrevsky, Clark, Koehn, 2013</marker>
<rawString>Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark, and Philipp Koehn. 2013. Scalable modified kneser-ney language model estimation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuedong Huang</author>
<author>Fileno Alleva</author>
<author>Hsiao-Wuen Hon</author>
<author>Mei-Yuh Hwang</author>
<author>Kai-Fu Lee</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>The sphinx-ii speech recognition system: an overview.</title>
<date>1993</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>7</volume>
<issue>2</issue>
<pages>148</pages>
<contexts>
<context position="4789" citStr="Huang et al., 1993" startWordPosition="759" endWordPosition="762">cal context w&amp;quot;−&apos;l &amp;quot;−��&apos;l and for which only a poor approximation exists may adversely affect the conditional probabilities in language models of all lengths — leading to severe errors even for smoothed language models. Thus, the idea motivating our approach is to involve several lower order models which systematically leave out one position in the context (one may think of replacing the affected word in the context with a wildcard) instead of shortening the sequence only by one word at the beginning. This concept of introducing gaps in n-grams is referred to as skip n-grams (Ney et al., 1994; Huang et al., 1993). Among other techniques, skip n-grams have also been considered as an approach to overcome problems of data sparsity (Goodman, 2001). However, to best of our knowledge, language models making use of skip n-grams models have never been investigated to their full extent and over different levels of lower order models. Our approach differs as we consider all possible combinations of gaps in a local context and interpolate the higher order model with all possible lower order models derived from adding gaps in all different ways. In this paper we make the following contributions: 1. We provide a f</context>
<context position="8629" citStr="Huang et al., 1993" startWordPosition="1386" endWordPosition="1389">dels, making use of different discount values for more or less frequently observed events. This variation has been compared to other smoothing techniques on various corpora and has shown to outperform competing approaches. We will review modified Kneser-Ney smoothing in Section 2.1 in more detail as we reuse some ideas to define our generalized language model. 1146 Smoothing techniques which do not rely on using lower order models involve clustering (Brown et al., 1992; Ney et al., 1994), i.e. grouping together similar words to form classes of words, as well as skip n-grams (Ney et al., 1994; Huang et al., 1993). Yet other approaches make use of permutations of the word order in n-grams (SchukatTalamazzini et al., 1995; Goodman, 2001). Skip n-grams are typically used to incorporate long distance relations between words. Introducing the possibility of gaps between the words in an n-gram allows for capturing word relations beyond the level of n consecutive words without an exponential increase in the parameter space. However, with their restriction on a subsequence of words, skip n-grams are also used as a technique to overcome data sparsity (Goodman, 2001). In related work different terminology and di</context>
</contexts>
<marker>Huang, Alleva, Hon, Hwang, Lee, Rosenfeld, 1993</marker>
<rawString>Xuedong Huang, Fileno Alleva, Hsiao-Wuen Hon, Mei-Yuh Hwang, Kai-Fu Lee, and Ronald Rosenfeld. 1993. The sphinx-ii speech recognition system: an overview. Computer Speech &amp; Language, 7(2):137 – 148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>R L Mercer</author>
</authors>
<title>Interpolated estimation of markov source parameters from sparse data.</title>
<date>1980</date>
<booktitle>In Proceedings of the Workshop on Pattern Recognition in Practice,</booktitle>
<pages>381--397</pages>
<contexts>
<context position="7211" citStr="Jelinek and Mercer, 1980" startWordPosition="1150" endWordPosition="1154">del. Finally, in Section 7 we summarize our findings and conclude with an overview of further interesting research challenges in the field of generalized language models. 2 Related Work Work related to our generalized language model approach can be divided in two categories: various smoothing techniques for language models and approaches making use of skip n-grams. Smoothing techniques for language models have a long history. Their aim is to overcome data sparsity and provide more reliable estimators—in particular for rare events. The Good Turing estimator (Good, 1953), deleted interpolation (Jelinek and Mercer, 1980), Katz backoff (Katz, 1987) and Kneser-Ney smoothing (Kneser and Ney, 1995) are just some of the approaches to be mentioned. Common strategies of these approaches are to either backoff to lower order models when a higher order model lacks sufficient training data for good estimation, to interpolate between higher and lower order models or to interpolate with a prior distribution. Furthermore, the estimation of the amount of unseen events from rare events aims to find the right weights for interpolation as well as for discounting probability mass from unreliable estimators and to retain it for </context>
</contexts>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>F. Jelinek and R.L. Mercer. 1980. Interpolated estimation of markov source parameters from sparse data. In Proceedings of the Workshop on Pattern Recognition in Practice, pages 381–397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer. Acoustics, Speech and Signal Processing,</title>
<date>1987</date>
<journal>IEEE Transactions on,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="7238" citStr="Katz, 1987" startWordPosition="1157" endWordPosition="1158">ur findings and conclude with an overview of further interesting research challenges in the field of generalized language models. 2 Related Work Work related to our generalized language model approach can be divided in two categories: various smoothing techniques for language models and approaches making use of skip n-grams. Smoothing techniques for language models have a long history. Their aim is to overcome data sparsity and provide more reliable estimators—in particular for rare events. The Good Turing estimator (Good, 1953), deleted interpolation (Jelinek and Mercer, 1980), Katz backoff (Katz, 1987) and Kneser-Ney smoothing (Kneser and Ney, 1995) are just some of the approaches to be mentioned. Common strategies of these approaches are to either backoff to lower order models when a higher order model lacks sufficient training data for good estimation, to interpolate between higher and lower order models or to interpolate with a prior distribution. Furthermore, the estimation of the amount of unseen events from rare events aims to find the right weights for interpolation as well as for discounting probability mass from unreliable estimators and to retain it for unseen events. The state of</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>S. Katz. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. Acoustics, Speech and Signal Processing, IEEE Transactions on, 35(3):400–401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Acoustics, Speech, and Signal Processing,</booktitle>
<volume>1</volume>
<pages>181--184</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="7286" citStr="Kneser and Ney, 1995" startWordPosition="1162" endWordPosition="1165">iew of further interesting research challenges in the field of generalized language models. 2 Related Work Work related to our generalized language model approach can be divided in two categories: various smoothing techniques for language models and approaches making use of skip n-grams. Smoothing techniques for language models have a long history. Their aim is to overcome data sparsity and provide more reliable estimators—in particular for rare events. The Good Turing estimator (Good, 1953), deleted interpolation (Jelinek and Mercer, 1980), Katz backoff (Katz, 1987) and Kneser-Ney smoothing (Kneser and Ney, 1995) are just some of the approaches to be mentioned. Common strategies of these approaches are to either backoff to lower order models when a higher order model lacks sufficient training data for good estimation, to interpolate between higher and lower order models or to interpolate with a prior distribution. Furthermore, the estimation of the amount of unseen events from rare events aims to find the right weights for interpolation as well as for discounting probability mass from unreliable estimators and to retain it for unseen events. The state of the art is a modified version of Kneser-Ney smo</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Acoustics, Speech, and Signal Processing, 1995. ICASSP-95., 1995 International Conference on, volume 1, pages 181–184. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Foundations of statistical natural language processing.</title>
<date>1999</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Sch¨utze. 1999. Foundations of statistical natural language processing. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Mays</author>
<author>Fred J Damerau</author>
<author>Robert L Mercer</author>
</authors>
<title>Context based spelling correction.</title>
<date>1991</date>
<journal>Information Processing &amp; Management,</journal>
<volume>27</volume>
<issue>5</issue>
<contexts>
<context position="1732" citStr="Mays et al., 1991" startWordPosition="254" endWordPosition="257">re we observed consistent improvements. Finally, we also show that the strength of our approach lies in its ability to cope in particular with sparse training data. Using a very small training data set of only 736 KB text we yield improvements of even 25.7% reduction of perplexity. 1 Introduction motivation Language Models are a probabilistic approach for predicting the occurrence of a sequence of words. They are used in many applications, e.g. word prediction (Bickel et al., 2005), speech recognition (Rabiner and Juang, 1993), machine translation (Brown et al., 1990), or spelling correction (Mays et al., 1991). The task language models attempt to solve is the estimation of a probability of a given sequence of words wl1 = w1, ... , wl. The probability P(wl1) of this sequence can be broken down into a product of conditional probabilities: P(wl1) =P(w1) · P(w2|w1) · ...· P(wl|w1 ··· wl_1) P(wi|w1 ··· wi_1) (1) Because of combinatorial explosion and data sparsity, it is very difficult to reliably estimate the probabilities that are conditioned on a longer subsequence. Therefore, by making a Markov assumption the true probability of a word sequence is only approximated by restricting conditional probabi</context>
</contexts>
<marker>Mays, Damerau, Mercer, 1991</marker>
<rawString>Eric Mays, Fred J Damerau, and Robert L Mercer. 1991. Context based spelling correction. Information Processing &amp; Management, 27(5):517–522.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hermann Ney</author>
<author>Ute Essen</author>
<author>Reinhard Kneser</author>
</authors>
<title>On structuring probabilistic dependences in stochastic language modelling.</title>
<date>1994</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="4768" citStr="Ney et al., 1994" startWordPosition="755" endWordPosition="758">at the end of a local context w&amp;quot;−&apos;l &amp;quot;−��&apos;l and for which only a poor approximation exists may adversely affect the conditional probabilities in language models of all lengths — leading to severe errors even for smoothed language models. Thus, the idea motivating our approach is to involve several lower order models which systematically leave out one position in the context (one may think of replacing the affected word in the context with a wildcard) instead of shortening the sequence only by one word at the beginning. This concept of introducing gaps in n-grams is referred to as skip n-grams (Ney et al., 1994; Huang et al., 1993). Among other techniques, skip n-grams have also been considered as an approach to overcome problems of data sparsity (Goodman, 2001). However, to best of our knowledge, language models making use of skip n-grams models have never been investigated to their full extent and over different levels of lower order models. Our approach differs as we consider all possible combinations of gaps in a local context and interpolate the higher order model with all possible lower order models derived from adding gaps in all different ways. In this paper we make the following contributio</context>
<context position="8502" citStr="Ney et al., 1994" startWordPosition="1362" endWordPosition="1365">oothing introduced in (Chen and Goodman, 1999). The modified version implements a recursive interpolation with lower order models, making use of different discount values for more or less frequently observed events. This variation has been compared to other smoothing techniques on various corpora and has shown to outperform competing approaches. We will review modified Kneser-Ney smoothing in Section 2.1 in more detail as we reuse some ideas to define our generalized language model. 1146 Smoothing techniques which do not rely on using lower order models involve clustering (Brown et al., 1992; Ney et al., 1994), i.e. grouping together similar words to form classes of words, as well as skip n-grams (Ney et al., 1994; Huang et al., 1993). Yet other approaches make use of permutations of the word order in n-grams (SchukatTalamazzini et al., 1995; Goodman, 2001). Skip n-grams are typically used to incorporate long distance relations between words. Introducing the possibility of gaps between the words in an n-gram allows for capturing word relations beyond the level of n consecutive words without an exponential increase in the parameter space. However, with their restriction on a subsequence of words, sk</context>
</contexts>
<marker>Ney, Essen, Kneser, 1994</marker>
<rawString>Hermann Ney, Ute Essen, and Reinhard Kneser. 1994. On structuring probabilistic dependences in stochastic language modelling. Computer Speech &amp; Language, 8(1):1 – 38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Rabiner</author>
<author>Biing-Hwang Juang</author>
</authors>
<title>Fundamentals of Speech Recognition.</title>
<date>1993</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="1646" citStr="Rabiner and Juang, 1993" startWordPosition="239" endWordPosition="242">re, we investigate the behaviour over three other languages and a domain specific corpus where we observed consistent improvements. Finally, we also show that the strength of our approach lies in its ability to cope in particular with sparse training data. Using a very small training data set of only 736 KB text we yield improvements of even 25.7% reduction of perplexity. 1 Introduction motivation Language Models are a probabilistic approach for predicting the occurrence of a sequence of words. They are used in many applications, e.g. word prediction (Bickel et al., 2005), speech recognition (Rabiner and Juang, 1993), machine translation (Brown et al., 1990), or spelling correction (Mays et al., 1991). The task language models attempt to solve is the estimation of a probability of a given sequence of words wl1 = w1, ... , wl. The probability P(wl1) of this sequence can be broken down into a product of conditional probabilities: P(wl1) =P(w1) · P(w2|w1) · ...· P(wl|w1 ··· wl_1) P(wi|w1 ··· wi_1) (1) Because of combinatorial explosion and data sparsity, it is very difficult to reliably estimate the probabilities that are conditioned on a longer subsequence. Therefore, by making a Markov assumption the true </context>
</contexts>
<marker>Rabiner, Juang, 1993</marker>
<rawString>Lawrence Rabiner and Biing-Hwang Juang. 1993. Fundamentals of Speech Recognition. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ernst-G¨unter Schukat-Talamazzini</author>
<author>R Hendrych</author>
<author>Ralf Kompe</author>
<author>Heinrich Niemann</author>
</authors>
<title>Permugram language models.</title>
<date>1995</date>
<booktitle>In Fourth European Conference on Speech Communication and Technology.</booktitle>
<marker>Schukat-Talamazzini, Hendrych, Kompe, Niemann, 1995</marker>
<rawString>Ernst-G¨unter Schukat-Talamazzini, R Hendrych, Ralf Kompe, and Heinrich Niemann. 1995. Permugram language models. In Fourth European Conference on Speech Communication and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Steinberger</author>
<author>Bruno Pouliquen</author>
<author>Anna Widiger</author>
<author>Camelia Ignat</author>
<author>Tomaz Erjavec</author>
<author>Dan Tufis</author>
<author>Daniel Varga</author>
</authors>
<title>The jrc-acquis: A multilingual aligned parallel corpus with 20+ languages.</title>
<date>2006</date>
<booktitle>In LREC’06: Proceedings of the 5th International Conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="17400" citStr="Steinberger et al., 2006" startWordPosition="2815" endWordPosition="2818">he model. The wild card symbol marks skipped words in an n-gram. The arrows indicate how a higher order model is interpolated with lower order models which skips one word. The bold arrows correspond to interpolation of models in traditional modified Kneser-Ney smoothing. The lighter arrows illustrate the additional interpolations introduced by our generalized language models. de en fr it Nov 22nd Nov 04th Nov 20th Nov 25th Table 1: Download dates of Wikipedia snapshots in November 2013. Special purpose domain data are provided by the multi-lingual JRC-Acquis corpus of legislative texts (JRC) (Steinberger et al., 2006). Table 2 gives an overview of the data sets and provides some simple statistics of the covered languages and the size of the collections. Statistics total words unique words in Mio. in Mio. 579 9.82 30.9 0.66 1689 11.7 39.2 0.46 339 4.06 35.8 0.46 193 3.09 34.4 0.47 Table 2: Word statistics and size of of evaluation corpora The data sets come in the form of structured text corpora which we cleaned from markup and tokenized to generate word sequences. We filtered the word tokens by removing all character sequences which did not contain any letter, digit or common punctuation marks. Eventually,</context>
</contexts>
<marker>Steinberger, Pouliquen, Widiger, Ignat, Erjavec, Tufis, Varga, 2006</marker>
<rawString>Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaz Erjavec, Dan Tufis, and Daniel Varga. 2006. The jrc-acquis: A multilingual aligned parallel corpus with 20+ languages. In LREC’06: Proceedings of the 5th International Conference on Language Resources and Evaluation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>