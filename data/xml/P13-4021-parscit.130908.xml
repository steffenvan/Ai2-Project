<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003722">
<title confidence="0.88514">
DKPro Similarity: An Open Source Framework for Text Similarity
</title>
<author confidence="0.89976">
Daniel B¨ar†, Torsten Zesch†$, and Iryna Gurevych†$
</author>
<affiliation confidence="0.840446">
†Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science, Technische Universit¨at Darmstadt
$Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
</affiliation>
<email confidence="0.883071">
www.ukp.tu-darmstadt.de
</email>
<sectionHeader confidence="0.991428" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999901777777778">
We present DKPro Similarity, an open
source framework for text similarity. Our
goal is to provide a comprehensive repos-
itory of text similarity measures which
are implemented using standardized inter-
faces. DKPro Similarity comprises a wide
variety of measures ranging from ones
based on simple n-grams and common
subsequences to high-dimensional vector
comparisons and structural, stylistic, and
phonetic measures. In order to promote
the reproducibility of experimental results
and to provide reliable, permanent ex-
perimental conditions for future studies,
DKPro Similarity additionally comes with
a set of full-featured experimental setups
which can be run out-of-the-box and be
used for future systems to built upon.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999951125">
Computing text similarity is key to several natu-
ral language processing applications such as au-
tomatic essay grading, paraphrase recognition, or
plagiarism detection. However, only a few text
similarity measures proposed in the literature are
released publicly, and those then typically do not
comply with any standardization. We are currently
not aware of any designated text similarity frame-
work which goes beyond simple lexical similarity
or contains more than a small number of measures,
even though related frameworks exist, which we
discuss in Section 6. This fact was also realized
by the organizers of the pilot Semantic Textual
Similarity Task at SemEval-2012 (see Section 5),
as they argue for the creation of an open source
framework for text similarity (Agirre et al., 2012).
In order to fill this gap, we present DKPro Sim-
ilarity, an open source framework for text simi-
larity. DKPro Similarity is designed to comple-
ment DKPro Core1, a collection of software com-
ponents for natural language processing based on
the Apache UIMA framework (Ferrucci and Lally,
2004). Our goal is to provide a comprehensive
repository of text similarity measures which are
implemented in a common framework using stan-
dardized interfaces. Besides the already available
measures, DKPro Similarity is easily extensible
and intended to allow for custom implementations,
for which it offers various templates and exam-
ples. The Java implementation is publicly avail-
able at Google Code2 under the Apache Software
License v2 and partly under GNU GPL v3.
</bodyText>
<sectionHeader confidence="0.987837" genericHeader="introduction">
2 Architecture
</sectionHeader>
<bodyText confidence="0.999875521739131">
DKPro Similarity is designed to operate in ei-
ther of two modes: The stand-alone mode al-
lows to use text similarity measures as indepen-
dent components in any experimental setup, but
does not offer means for further language process-
ing, e.g. lemmatization. The UIMA-coupled mode
tightly integrates similarity computation with full-
fledged Apache UIMA-based language processing
pipelines. That way, it allows to perform any num-
ber of languge processing steps, e.g. coreference
or named-entitiy resolution, along with the text
similarity computation.
Stand-alone Mode In this mode, text similarity
measures can be used independently of any lan-
guage processing pipeline just by passing them a
pair of texts as (i) two strings, or (ii) two lists of
strings (e.g. already lemmatized texts). We there-
fore provide an API module, which contains Java
interfaces and abstract base classes for the mea-
sures. That way, DKPro Similarity allows for a
maximum flexibility in experimental design, as the
text similarity measures can easily be integrated
with any existing experimental setup:
</bodyText>
<footnote confidence="0.8234925">
1code.google.com/p/dkpro-core-asl
2code.google.com/p/dkpro-similarity-asl
</footnote>
<page confidence="0.917012">
121
</page>
<note confidence="0.35859">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 121–126,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<equation confidence="0.9907365">
1 TextSimilarityMeasure m =
new GreedyStringTiling();
2 double similarity =
m.getSimilarity(text1, text2);
</equation>
<bodyText confidence="0.999889790697675">
The above code snippet instantiates the Greedy
String Tiling measure (Wise, 1996) and then com-
putes the text similarity between the given pair of
texts. The resulting similarity score is normal-
ized into [0, 1] where 0 means not similar at all,
and 1 corresponds to perfectly similar.3 By us-
ing the common TextSimilarityMeasure
interface, it is easy to replace Greedy String Tiling
with any measure of choice, such as Latent Se-
mantic Analysis (Landauer et al., 1998) or Explicit
Semantic Analysis (Gabrilovich and Markovitch,
2007). We give an overview of measures available
in DKPro Similarity in Section 3.
UIMA-coupled Mode In this mode, DKPro
Similarity allows text similarity computation to
be directly integrated with any UIMA-based lan-
guage processing pipeline. That way, it is easy to
use text similarity components in addition to other
UIMA-based components in the same pipeline.
For example, an experimental setup may require to
first compute text similarity scores and then to run
a classification algorithm on the resulting scores.
In Figure 1, we show a graphical overview of
the integration of text similarity measures (right)
with a UIMA-based pipeline (left). The pipeline
starts by reading a given dataset, then performs
any number of pre-processing steps such as to-
kenization, sentence splitting, lemmatization, or
stopword filtering, then runs the text similar-
ity computation, before executing any subsequent
post-processing steps and finally returning the pro-
cessed texts in a suitable format for evaluation or
manual inspection. As all text similarity measures
in DKPro Similarity conform to standardized in-
terfaces, they can be easily exchanged in the text
similarity computation step.
With DKPro Similarity, we offer various sub-
classes of the generic UIMA components which
are specifically tailored towards text similarity ex-
periments, e.g. corpus readers for standard eval-
uation datasets as well as evaluation components
for running typical evaluation metrics. By lever-
aging UIMA’s architecture, we also define an
</bodyText>
<footnote confidence="0.827059">
3Some string distance measures such as the Levenshtein
distance (Levenshtein, 1966) return a raw distance score
where less distance corresponds to higher similarity. How-
ever, the score can easily be normalized, e.g. by text length.
</footnote>
<figure confidence="0.895913333333333">
UIMA-based Pipeline Text Similarity Measures
Corpus Reader
Evaluation
</figure>
<figureCaption confidence="0.999315">
Figure 1: DKPro Similarity allows to integrate any
</figureCaption>
<bodyText confidence="0.84278825">
text similarity measure (right) which conforms to
standardized interfaces into a UIMA-based lan-
guage processing pipeline (left) by means of a
dedicated Similarity Scorer component (middle).
additional interface to text similarity measures:
The JCasTextSimilarityMeasure inherits
from TextSimilarityMeasure, and adds a
method for two JCas text representations:4
</bodyText>
<subsectionHeader confidence="0.426409">
double getSimilarity
</subsectionHeader>
<bodyText confidence="0.988068333333333">
(JCas text1, JCas text2);
The additional interface allows to implement mea-
sures which have full access to UIMA’s document
structure. That way, it is possible to create text
similarity measures which can use any piece of in-
formation that has been annotated in the processed
documents, such as dependency trees or morpho-
logical information. We detail the new set of com-
ponents offered by DKPro Similarity in Section 4.
</bodyText>
<sectionHeader confidence="0.964219" genericHeader="method">
3 Text Similarity Measures
</sectionHeader>
<bodyText confidence="0.9999807">
In this section, we give an overview of the text
similarity measures which are already available in
DKPro Similarity. While we provide new imple-
mentations for a multitude of measures, we rely on
specialized libraries such as the S-Space Package
(see Section 6) if available. Due to space limi-
tations and due to the fact that the framework is
actively under development, we do not provide an
exhaustive list here, but rather mention the most
interesting and most popular measures.
</bodyText>
<subsectionHeader confidence="0.999948">
3.1 Simple String-based Measures
</subsectionHeader>
<bodyText confidence="0.999835666666667">
DKPro Similarity includes text similarity mea-
sures which operate on string sequences and
determine, for example, the longest common
</bodyText>
<footnote confidence="0.972012333333333">
4The JCas is an object-oriented Java interface to the
Common Analysis Structure (Ferrucci and Lally, 2004),
Apache UIMA’s internal document representation format.
</footnote>
<figure confidence="0.994194111111111">
Pre-processing
Text Similarity
Computation
Post-processing
Greedy String Tiling
Double Metaphone
Explicit Sem. Analysis
. . .
Similarity Scorer
</figure>
<page confidence="0.980071">
122
</page>
<bodyText confidence="0.9989339">
(non-)contiguous sequence of characters. It also
contains Greedy String Tiling (Wise, 1996), a mea-
sure which allows to compare strings if parts have
been reordered. The framework also offers mea-
sures which compute sets of character and word
n-grams and compare them using different overlap
coefficients, e.g. the Jaccard index. It further in-
cludes popular string distance metrics such as the
Jaro-Winkler (Winkler, 1990), Monge and Elkan
(1997) and Levenshtein (1966) distance measures.
</bodyText>
<subsectionHeader confidence="0.999909">
3.2 Semantic Similarity Measures
</subsectionHeader>
<bodyText confidence="0.99928372972973">
DKPro Similarity also contains several measures
which go beyond simple character sequences and
compute text similarity on a semantic level.
Pairwise Word Similarity These measures are
based on pairwise word similarity computations
which are then aggregated for the complete texts.
The measures typically operate on a graph-based
representation of words and the semantic relations
among them within a lexical-semantic resource.
DKPro Similarity therefore contains adapters for
WordNet, Wiktionary5, and Wikipedia, while the
framework can easily be extended to other data
sources that conform to a common interface
(Garoufi et al., 2008). Pairwise similarity mea-
sures in DKPro Similarity include Jiang and Con-
rath (1997) or Resnik (1995). The aggregation for
the complete texts can for example be done using
the strategy by Mihalcea et al. (2006).
Vector Space Models These text similarity
measures project texts onto high-dimensional vec-
tors which are then compared. Cosine similar-
ity, a basic measure often used in information re-
trieval, weights words according to their term fre-
quencies or tf-idf scores, and computes the co-
sine between two text vectors. Latent Seman-
tic Analysis (Landauer et al., 1998) alleviates the
inherent sparseness of a high-dimensional term-
document matrix by reducing it to one of reduced
rank. Explicit Semantic Analysis (Gabrilovich and
Markovitch, 2007) constructs the vector space on
corpora where the documents are assumed to de-
scribe natural concepts such as cat or dog. Orig-
inally, Wikipedia was proposed as the document
collection of choice.
DKPro Similarity goes beyond a single im-
plementation of these measures and comes with
highly customizable code which allows to set var-
</bodyText>
<footnote confidence="0.948147">
5http://www.wiktionary.org
</footnote>
<bodyText confidence="0.99958925">
ious parameters for the construction of the vector
space and the comparison of the document vectors,
and further allows to construct the vector space for
arbitrary collections, e.g. domain-specific corpora.
</bodyText>
<subsectionHeader confidence="0.986092">
3.3 Further Measures
</subsectionHeader>
<bodyText confidence="0.99978">
Previous research (B¨ar et al., 2012b) has shown
promising results for the inclusion of measures
which go beyond textual content and compute
similarity along other text characteristics. Thus,
DKPro Similarity also includes measures for
structural, stylistic, and phonetic similarity.
Structural Similarity Structural similarity be-
tween texts can be computed, for example, by
comparing sets of stopword n-grams (Stamatatos,
2011). The idea here is that similar texts may pre-
serve syntactic similarity while exchanging only
content words. Other measures in DKPro Simi-
larity allow to compare texts by part-of-speech n-
grams, and order and distance features for pairs of
words (Hatzivassiloglou et al., 1999).
Stylistic Similarity DKPro Similarity includes,
for example, a measure which compares function
word frequencies (Dinu and Popescu, 2009) be-
tween two texts. The framework also includes a
set of measures which capture statistical properties
of texts such as the type-token ratio (TTR) and the
sequential TTR (McCarthy and Jarvis, 2010).
Phonetic Similarity DKPro Similarity also al-
lows to compute text similarity based on pair-
wise phonetic comparisons of words. It therefore
contains implementations of well-known phonetic
algorithms such as Double Metaphone (Philips,
2000) and Soundex (Knuth, 1973), which also con-
form to the common text similarity interface.
</bodyText>
<sectionHeader confidence="0.99873" genericHeader="method">
4 UIMA Components
</sectionHeader>
<bodyText confidence="0.999933">
In addition to a rich set of text similarity mea-
sures as partly described above, DKPro Similar-
ity includes components which allow to integrate
text similarity measures with any UIMA-based
pipeline, as outlined in Figure 1. In the following,
we introduce these components along with their
resources.
Readers &amp; Datasets DKPro Similarity includes
corpus readers specifically tailored towards com-
bining the input texts in a number of ways, e.g.
all possible combinations, or each text paired with
n others by random. Standard datasets for which
</bodyText>
<page confidence="0.99528">
123
</page>
<bodyText confidence="0.9993110625">
readers come pre-packaged include, among oth-
ers, the SemEval-2012 STS data (Agirre et al.,
2012), the METER corpus (Clough et al., 2002),
or the RTE 1–5 data (Dagan et al., 2006). As far
as license terms allow redistribution, the datasets
themselves are integrated into the framework.
Similarity Scorer The Similarity Scorer allows
to integrate any text similarity measure (which is
decoupled from UIMA by default) into a UIMA-
based pipeline. It builds upon the standardized text
similarity interfaces and thus allows to easily ex-
change the text similarity measure as well as to
specify the data types the measure should operate
on, e.g. tokens or lemmas.
Machine Learning Previous research (Agirre et
al., 2012) has shown that different text similarity
measures can be combined using machine learning
classifiers. Such a combination shows improve-
ments over single measures due to the fact that dif-
ferent measures capture different text characteris-
tics. DKPro Similarity thus provides adapters for
the Weka framework (Hall et al., 2009) and allows
to first pre-compute sets of text similarity scores
which can then be used as features for various ma-
chine learning classifiers.
Evaluation Metrics In the final step of a UIMA
pipeline, the processed data is read by a dedicated
evaluation component. DKPro Similarity ships
with a set of components which for example com-
pute Pearson or Spearman correlation with human
judgments, or apply task-specific metrics such as
average precision as used in the RTE challenges.
</bodyText>
<sectionHeader confidence="0.997536" genericHeader="method">
5 Experimental Setups
</sectionHeader>
<bodyText confidence="0.999913262295083">
DKPro Similarity further encourages the creation
and publication of complete experimental setups.
That way, we promote the reproducibility of ex-
perimental results, and provide reliable, perma-
nent experimental conditions which can benefit fu-
ture studies and help to stimulate the reuse of par-
ticular experimental steps and software modules.
The experimental setups are instantiations of
the generic UIMA-based language processing
pipeline depicted in Figure 1 and are designed to
precisely match the particular task at hand. They
thus come pre-configured with corpus readers for
the relevant input data, with a set of pre- and post-
processing as well as evaluation components, and
with a set of text similarity measures which are
well-suited for the particular task. The experimen-
tal setups are self-contained systems and can be
run out-of-the-box without further configuration.6
DKPro Similarity contains two major types of
experimental setups: (i) those for an intrinsic eval-
uation allow to evaluate the system performance in
an isolated setting by comparing the system results
with a human gold standard, and (ii) those for an
extrinsic evaluation allow to evaluate the system
with respect to a particular task at hand, where text
similarity is a means for solving a concrete prob-
lem, e.g. recognizing textual entailment.
Intrinsic Evaluation DKPro Similarity con-
tains the setup (B¨ar et al., 2012a) which partic-
ipated in the Semantic Textual Similarity (STS)
Task at SemEval-2012 (Agirre et al., 2012) and
which has become one of the recommended base-
line systems for the second task of this series.7
The system combines a multitude of text similar-
ity measures of varying complexity using a simple
log-linear regression model. The provided setup
allows to evaluate how well the system output re-
sembles human similarity judgments on short texts
which are taken from five different sources, e.g.
paraphrases of news texts or video descriptions.
Extrinsic Evaluation Our framework includes
two setups for an extrinsic evaluation: detecting
text reuse, and recognizing textual entailment.
For detecting text reuse (Clough et al., 2002),
the setup we provide (B¨ar et al., 2012b) combines
a multitude of text similarity measures along dif-
ferent text characteristics. Thereby, it not only
combines simple string-based and semantic sim-
ilarity measures (see Sections 3.1 and 3.2), but
makes extensive use of measures along structural
and stylistic text characteristics (see Section 3.3).
Across three standard evaluation datasets, the sys-
tem consistently outperforms all previous work.
For recognizing textual entailment, we provide
a setup which is similar in configuration to the one
described above, but contains corpus readers and
evaluation components precisely tailored towards
the RTE challenge series (Dagan et al., 2006). We
believe that our setup can be used for filtering
those text pairs which need further analysis by a
dedicated textual entailment system.
</bodyText>
<footnote confidence="0.9990134">
6A one-time setup of local lexical-semantic resources
such as WordNet may be necessary, though.
7In 2013, the STS Task is a shared task of the Second
Joint Conference on Lexical and Computational Semantics,
http://ixa2.si.ehu.es/sts
</footnote>
<page confidence="0.997027">
124
</page>
<sectionHeader confidence="0.999043" genericHeader="method">
6 Related Frameworks
</sectionHeader>
<bodyText confidence="0.999947444444444">
To the best of our knowledge, only a few general-
ized similarity frameworks exist at all. In the fol-
lowing, we discuss them and give insights where
DKPro Similarity uses implementations of these
existing libraries. That way, DKPro Similarity
brings together the scattered efforts by offering ac-
cess to all measures through common interfaces. It
goes far beyond the functionality of the original li-
braries as it generalizes the resources used, allows
a tight integration with any UIMA-based pipeline,
and comes with full-featured experimental setups
which are pre-configured stand-alone text similar-
ity systems that can be run out-of-the-box.
S-Space Package Even though no designated
text similarity library, the S-Space Package (Jur-
gens and Stevens, 2010)8 contains some text sim-
ilarity measures such as Latent Semantic Analysis
(LSA) and Explicit Semantic Analysis (see Sec-
tion 3.2). However, it is primarily focused on
word space models which operate on word distri-
butions in text. Besides such algorithms, it offers
a variety of interfaces, data structures, evaluation
datasets and metrics, and global operation utili-
ties e.g. for dimension reduction using Singular
Value Decomposition or randomized projections,
which are particularly useful with such distribu-
tional word space models. DKPro Similarity inte-
grates LSA based on the S-Space Package.
Semantic Vectors The Semantic Vectors pack-
age is a package for distributional semantics (Wid-
dows and Cohen, 2010)9 that contains measures
such as LSA and allows for comparing documents
within a given vector space. The main focus lies
on word space models with a number of dimension
reduction techniques, and applications on word
spaces such as automatic thesaurus generation.
WordNet::Similarity The open source package
by Pedersen et al. (2004)10 is a popular Perl li-
brary for the similarity computation on WordNet.
It comprises six word similarity measures that op-
erate on WordNet, e.g. Jiang and Conrath (1997)
or Resnik (1995). Unfortunately, no strategies
have been added to the package yet which aggre-
gate the word similarity scores for complete texts
in a similar manner as described in Section 3.2.
</bodyText>
<footnote confidence="0.933041333333333">
8code.google.com/p/airhead-research
9code.google.com/p/semanticvectors
10sourceforge.net/projects/wn-similarity
</footnote>
<bodyText confidence="0.999723318181818">
In DKPro Similarity, we offer native Java imple-
mentations of all measures contained in Word-
Net::Similarity, and allow to go beyond WordNet
and use the measures with any lexical-semantic re-
source of choice, e.g. Wiktionary or Wikipedia.
SimMetrics Library The Java library by Chap-
man et al. (2005)11 exclusively comprises text sim-
ilarity measures which compute lexical similar-
ity on string sequences and compare texts with-
out any semantic processing. It contains mea-
sures such as the Levenshtein (1966) or Monge and
Elkan (1997) distance metrics. In DKPro Similar-
ity, some string-based measures (see Section 3.1)
are based on implementations from this library.
SecondString Toolkit The freely available li-
brary by Cohen et al. (2003)12 is similar to Sim-
Metrics, and also implemented in Java. It also con-
tains several well-known text similarity measures
on string sequences, and includes many of the
measures which are also part of the SimMetrics
Library. Some string-based measures in DKPro
Similarity are based on the SecondString Toolkit.
</bodyText>
<sectionHeader confidence="0.999209" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.929161416666667">
We presented DKPro Similarity, an open source
framework designed to streamline the develop-
ment of text similarity measures. All measures
conform to standardized interfaces and can either
be used as stand-alone components in any ex-
perimental setup (e.g. an already existing system
which is not based on Apache UIMA), or can be
tightly coupled with a full-featured UIMA-based
language processing pipeline in order to allow for
advanced processing capabilities.
We would like to encourage other researchers
to participate in our efforts and invite them to ex-
plore our existing experimental setups as outlined
in Section 5, run modified versions of our setups,
and contribute own text similarity measures to
the framework. For that, DKPro Similarity also
comes with an example module for getting started,
which guides first-time users through both the
stand-alone and the UIMA-coupled modes.
Acknowledgements This work has been supported by the
Volkswagen Foundation as part of the Lichtenberg Profes-
sorship Program under grant No. I/82806, and by the Klaus
Tschira Foundation under project No. 00.133.2008. We thank
Richard Eckart de Castilho and all other contributors.
</bodyText>
<footnote confidence="0.8916855">
11sourceforge.net/projects/simmetrics
12sourceforge.net/projects/secondstring
</footnote>
<page confidence="0.997172">
125
</page>
<sectionHeader confidence="0.986712" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999411981481481">
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A
Pilot on Semantic Textual Similarity. In Proc. of the
6th Int’l Works. on Semantic Eval., pages 385–393.
Daniel B¨ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012a. UKP: Computing Semantic
Textual Similarity by Combining Multiple Content
Similarity Measures. In Proc. of the 6th Int’l Work-
shop on Semantic Evaluation, pages 435–440.
Daniel B¨ar, Torsten Zesch, and Iryna Gurevych. 2012b.
Text Reuse Detection Using a Composition of Text
Similarity Measures. In Proc. of the 24th Int’l Conf.
on Computational Linguistics, pages 167–184.
Sam Chapman, Barry Norton, and Fabio Ciravegna.
2005. Armadillo: Integrating Knowledge for the Se-
mantic Web. In Proceedings of the Dagstuhl Semi-
nar in Machine Learning for the Semantic Web.
Paul Clough, Robert Gaizauskas, Scott S.L. Piao, and
Yorick Wilks. 2002. METER: MEasuring TExt
Reuse. In Proceedings of ACL, pages 152–159.
William W. Cohen, Pradeep Ravikumar, and Stephen
Fienberg. 2003. A Comparison of String Metrics
for Matching Names and Records. In Proc. of KDD
Works. on Data Cleaning and Object Consolidation.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL Recognising Textual Entail-
ment Challenge. In Machine Learning Challenges,
Lecture Notes in Computer Science, pages 177–190.
Liviu P. Dinu and Marius Popescu. 2009. Ordinal mea-
sures in authorship identification. In Proceedings of
the 3rd PAN Workshop. Uncovering Plagiarism, Au-
thorship and Social Software Misuse, pages 62–66.
David Ferrucci and Adam Lally. 2004. UIMA: An
Architectural Approach to Unstructured Information
Processing in the Corporate Research Environment.
Natural Language Engineering, 10(3-4):327–348.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing Semantic Relatedness using Wikipedia-
based Explicit Semantic Analysis. In Proceedings
of IJCAI, pages 1606–1611, Hyderabad, India.
Konstantina Garoufi, Torsten Zesch, and Iryna
Gurevych. 2008. Representational Interoperability
of Linguistic and Collaborative Knowledge Bases.
In Proceedings of the KONVENS Workshop on
Lexical-Semantic and Ontological Resources.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Up-
date. SIGKDD Explorations, 11(1):10–18.
Vasileios Hatzivassiloglou, Judith L. Klavans, and
Eleazar Eskin. 1999. Detecting text similarity over
short passages: Exploring linguistic feature com-
binations via machine learning. In Proceedings of
EMNLP/VLC, pages 203–212.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical tax-
onomy. In Proceedings of ROCLING, pages 19–33.
David Jurgens and Keith Stevens. 2010. The S-Space
Package: An Open Source Package for Word Space
Models. In Proceedings of the ACL 2010 System
Demonstrations, pages 30–35, Uppsala, Sweden.
Donald E. Knuth. 1973. The Art of Computer
Programming: Volume 3, Sorting and Searching.
Addison-Wesley.
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. An Introduction to Latent Semantic
Analysis. Discourse Processes, 25(2):259–284.
Vladimir I. Levenshtein. 1966. Binary codes capa-
ble of correcting deletions, insertions, and reversals.
Soviet Physics Doklady, 10(8):707–710.
Philip M. McCarthy and Scott Jarvis. 2010. MTLD,
vocd-D, and HD-D: A validation study of sophis-
ticated approaches to lexical diversity assessment.
Behavior research methods, 42(2):381–392.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and Knowledge-based
Measures of Text Semantic Similarity. In Proceed-
ings ofAAAI-06, pages 775–780, Boston, MA, USA.
Alvaro Monge and Charles Elkan. 1997. An ef-
ficient domain-independent algorithm for detecting
approximately duplicate database records. In Pro-
ceedings of the SIGMOD Workshop on Data Mining
and Knowledge Discovery, pages 23–29.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - Measuring the
Relatedness of Concepts. In Proceedings of the
HLT-NAACL: Demonstration Papers, pages 38–41.
Lawrence Philips. 2000. The double metaphone
search algorithm. C/C++ Users Jour., 18(6):38–43.
Philip Resnik. 1995. Using Information Content to
Evaluate Semantic Similarity in a Taxonomy. In
Proceedings of the IJCAI, pages 448–453.
Efstathios Stamatatos. 2011. Plagiarism detection
using stopword n-grams. Journal of the Ameri-
can Society for Information Science and Technology,
62(12):2512–2527.
Dominic Widdows and Trevor Cohen. 2010. The Se-
mantic Vectors Package: New Algorithms and Pub-
lic Tools for Distributional Semantics. In Proceed-
ings of IEEE-ICSC, pages 9–15.
William E. Winkler. 1990. String Comparator Metrics
and Enhanced Decision Rules in the Fellegi-Sunter
Model of Record Linkage. In Proceedings of the
Survey Research Methods Section, pages 354–359.
Michael J. Wise. 1996. YAP3: Improved detection of
similarities in computer program and other texts. In
Proc. of the 27th SIGCSE Technical Symposium on
Computer Science Education, pages 130–134.
</reference>
<page confidence="0.998503">
126
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.698986">
<title confidence="0.942898">DKPro Similarity: An Open Source Framework for Text Similarity Torsten and Iryna</title>
<author confidence="0.744566">Knowledge Processing Lab</author>
<affiliation confidence="0.996976">Department of Computer Science, Technische Universit¨at Knowledge Processing Lab German Institute for Educational Research and Educational</affiliation>
<email confidence="0.98286">www.ukp.tu-darmstadt.de</email>
<abstract confidence="0.999421105263158">present an open source framework for text similarity. Our goal is to provide a comprehensive repository of text similarity measures which are implemented using standardized interfaces. DKPro Similarity comprises a wide variety of measures ranging from ones on simple and common subsequences to high-dimensional vector comparisons and structural, stylistic, and phonetic measures. In order to promote the reproducibility of experimental results and to provide reliable, permanent experimental conditions for future studies, DKPro Similarity additionally comes with a set of full-featured experimental setups which can be run out-of-the-box and be used for future systems to built upon.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<date>2012</date>
<booktitle>SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity. In Proc. of the 6th Int’l Works. on Semantic Eval.,</booktitle>
<pages>385--393</pages>
<contexts>
<context position="1901" citStr="Agirre et al., 2012" startWordPosition="268" endWordPosition="271"> However, only a few text similarity measures proposed in the literature are released publicly, and those then typically do not comply with any standardization. We are currently not aware of any designated text similarity framework which goes beyond simple lexical similarity or contains more than a small number of measures, even though related frameworks exist, which we discuss in Section 6. This fact was also realized by the organizers of the pilot Semantic Textual Similarity Task at SemEval-2012 (see Section 5), as they argue for the creation of an open source framework for text similarity (Agirre et al., 2012). In order to fill this gap, we present DKPro Similarity, an open source framework for text similarity. DKPro Similarity is designed to complement DKPro Core1, a collection of software components for natural language processing based on the Apache UIMA framework (Ferrucci and Lally, 2004). Our goal is to provide a comprehensive repository of text similarity measures which are implemented in a common framework using standardized interfaces. Besides the already available measures, DKPro Similarity is easily extensible and intended to allow for custom implementations, for which it offers various </context>
<context position="12794" citStr="Agirre et al., 2012" startWordPosition="1900" endWordPosition="1903"> set of text similarity measures as partly described above, DKPro Similarity includes components which allow to integrate text similarity measures with any UIMA-based pipeline, as outlined in Figure 1. In the following, we introduce these components along with their resources. Readers &amp; Datasets DKPro Similarity includes corpus readers specifically tailored towards combining the input texts in a number of ways, e.g. all possible combinations, or each text paired with n others by random. Standard datasets for which 123 readers come pre-packaged include, among others, the SemEval-2012 STS data (Agirre et al., 2012), the METER corpus (Clough et al., 2002), or the RTE 1–5 data (Dagan et al., 2006). As far as license terms allow redistribution, the datasets themselves are integrated into the framework. Similarity Scorer The Similarity Scorer allows to integrate any text similarity measure (which is decoupled from UIMA by default) into a UIMAbased pipeline. It builds upon the standardized text similarity interfaces and thus allows to easily exchange the text similarity measure as well as to specify the data types the measure should operate on, e.g. tokens or lemmas. Machine Learning Previous research (Agirr</context>
<context position="15736" citStr="Agirre et al., 2012" startWordPosition="2359" endWordPosition="2362"> Similarity contains two major types of experimental setups: (i) those for an intrinsic evaluation allow to evaluate the system performance in an isolated setting by comparing the system results with a human gold standard, and (ii) those for an extrinsic evaluation allow to evaluate the system with respect to a particular task at hand, where text similarity is a means for solving a concrete problem, e.g. recognizing textual entailment. Intrinsic Evaluation DKPro Similarity contains the setup (B¨ar et al., 2012a) which participated in the Semantic Textual Similarity (STS) Task at SemEval-2012 (Agirre et al., 2012) and which has become one of the recommended baseline systems for the second task of this series.7 The system combines a multitude of text similarity measures of varying complexity using a simple log-linear regression model. The provided setup allows to evaluate how well the system output resembles human similarity judgments on short texts which are taken from five different sources, e.g. paraphrases of news texts or video descriptions. Extrinsic Evaluation Our framework includes two setups for an extrinsic evaluation: detecting text reuse, and recognizing textual entailment. For detecting tex</context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity. In Proc. of the 6th Int’l Works. on Semantic Eval., pages 385–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel B¨ar</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>UKP: Computing Semantic Textual Similarity by Combining Multiple Content Similarity Measures.</title>
<date>2012</date>
<booktitle>In Proc. of the 6th Int’l Workshop on Semantic Evaluation,</booktitle>
<pages>435--440</pages>
<marker>B¨ar, Biemann, Gurevych, Zesch, 2012</marker>
<rawString>Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012a. UKP: Computing Semantic Textual Similarity by Combining Multiple Content Similarity Measures. In Proc. of the 6th Int’l Workshop on Semantic Evaluation, pages 435–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel B¨ar</author>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>Text Reuse Detection Using a Composition of Text Similarity Measures.</title>
<date>2012</date>
<booktitle>In Proc. of the 24th Int’l Conf. on Computational Linguistics,</booktitle>
<pages>167--184</pages>
<marker>B¨ar, Zesch, Gurevych, 2012</marker>
<rawString>Daniel B¨ar, Torsten Zesch, and Iryna Gurevych. 2012b. Text Reuse Detection Using a Composition of Text Similarity Measures. In Proc. of the 24th Int’l Conf. on Computational Linguistics, pages 167–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sam Chapman</author>
<author>Barry Norton</author>
<author>Fabio Ciravegna</author>
</authors>
<title>Armadillo: Integrating Knowledge for the Semantic Web.</title>
<date>2005</date>
<booktitle>In Proceedings of the Dagstuhl Seminar in Machine Learning for the Semantic Web.</booktitle>
<contexts>
<context position="20021" citStr="Chapman et al. (2005)" startWordPosition="3001" endWordPosition="3005"> and Conrath (1997) or Resnik (1995). Unfortunately, no strategies have been added to the package yet which aggregate the word similarity scores for complete texts in a similar manner as described in Section 3.2. 8code.google.com/p/airhead-research 9code.google.com/p/semanticvectors 10sourceforge.net/projects/wn-similarity In DKPro Similarity, we offer native Java implementations of all measures contained in WordNet::Similarity, and allow to go beyond WordNet and use the measures with any lexical-semantic resource of choice, e.g. Wiktionary or Wikipedia. SimMetrics Library The Java library by Chapman et al. (2005)11 exclusively comprises text similarity measures which compute lexical similarity on string sequences and compare texts without any semantic processing. It contains measures such as the Levenshtein (1966) or Monge and Elkan (1997) distance metrics. In DKPro Similarity, some string-based measures (see Section 3.1) are based on implementations from this library. SecondString Toolkit The freely available library by Cohen et al. (2003)12 is similar to SimMetrics, and also implemented in Java. It also contains several well-known text similarity measures on string sequences, and includes many of th</context>
</contexts>
<marker>Chapman, Norton, Ciravegna, 2005</marker>
<rawString>Sam Chapman, Barry Norton, and Fabio Ciravegna. 2005. Armadillo: Integrating Knowledge for the Semantic Web. In Proceedings of the Dagstuhl Seminar in Machine Learning for the Semantic Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Clough</author>
<author>Robert Gaizauskas</author>
<author>Scott S L Piao</author>
<author>Yorick Wilks</author>
</authors>
<title>METER: MEasuring TExt Reuse.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>152--159</pages>
<contexts>
<context position="12834" citStr="Clough et al., 2002" startWordPosition="1907" endWordPosition="1910">ly described above, DKPro Similarity includes components which allow to integrate text similarity measures with any UIMA-based pipeline, as outlined in Figure 1. In the following, we introduce these components along with their resources. Readers &amp; Datasets DKPro Similarity includes corpus readers specifically tailored towards combining the input texts in a number of ways, e.g. all possible combinations, or each text paired with n others by random. Standard datasets for which 123 readers come pre-packaged include, among others, the SemEval-2012 STS data (Agirre et al., 2012), the METER corpus (Clough et al., 2002), or the RTE 1–5 data (Dagan et al., 2006). As far as license terms allow redistribution, the datasets themselves are integrated into the framework. Similarity Scorer The Similarity Scorer allows to integrate any text similarity measure (which is decoupled from UIMA by default) into a UIMAbased pipeline. It builds upon the standardized text similarity interfaces and thus allows to easily exchange the text similarity measure as well as to specify the data types the measure should operate on, e.g. tokens or lemmas. Machine Learning Previous research (Agirre et al., 2012) has shown that different</context>
<context position="16365" citStr="Clough et al., 2002" startWordPosition="2456" endWordPosition="2459">h has become one of the recommended baseline systems for the second task of this series.7 The system combines a multitude of text similarity measures of varying complexity using a simple log-linear regression model. The provided setup allows to evaluate how well the system output resembles human similarity judgments on short texts which are taken from five different sources, e.g. paraphrases of news texts or video descriptions. Extrinsic Evaluation Our framework includes two setups for an extrinsic evaluation: detecting text reuse, and recognizing textual entailment. For detecting text reuse (Clough et al., 2002), the setup we provide (B¨ar et al., 2012b) combines a multitude of text similarity measures along different text characteristics. Thereby, it not only combines simple string-based and semantic similarity measures (see Sections 3.1 and 3.2), but makes extensive use of measures along structural and stylistic text characteristics (see Section 3.3). Across three standard evaluation datasets, the system consistently outperforms all previous work. For recognizing textual entailment, we provide a setup which is similar in configuration to the one described above, but contains corpus readers and eval</context>
</contexts>
<marker>Clough, Gaizauskas, Piao, Wilks, 2002</marker>
<rawString>Paul Clough, Robert Gaizauskas, Scott S.L. Piao, and Yorick Wilks. 2002. METER: MEasuring TExt Reuse. In Proceedings of ACL, pages 152–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>Pradeep Ravikumar</author>
<author>Stephen Fienberg</author>
</authors>
<title>A Comparison of String Metrics for Matching Names and Records.</title>
<date>2003</date>
<booktitle>In Proc. of KDD Works. on Data Cleaning and Object Consolidation.</booktitle>
<contexts>
<context position="20457" citStr="Cohen et al. (2003)" startWordPosition="3069" endWordPosition="3072">llow to go beyond WordNet and use the measures with any lexical-semantic resource of choice, e.g. Wiktionary or Wikipedia. SimMetrics Library The Java library by Chapman et al. (2005)11 exclusively comprises text similarity measures which compute lexical similarity on string sequences and compare texts without any semantic processing. It contains measures such as the Levenshtein (1966) or Monge and Elkan (1997) distance metrics. In DKPro Similarity, some string-based measures (see Section 3.1) are based on implementations from this library. SecondString Toolkit The freely available library by Cohen et al. (2003)12 is similar to SimMetrics, and also implemented in Java. It also contains several well-known text similarity measures on string sequences, and includes many of the measures which are also part of the SimMetrics Library. Some string-based measures in DKPro Similarity are based on the SecondString Toolkit. 7 Conclusions We presented DKPro Similarity, an open source framework designed to streamline the development of text similarity measures. All measures conform to standardized interfaces and can either be used as stand-alone components in any experimental setup (e.g. an already existing syste</context>
</contexts>
<marker>Cohen, Ravikumar, Fienberg, 2003</marker>
<rawString>William W. Cohen, Pradeep Ravikumar, and Stephen Fienberg. 2003. A Comparison of String Metrics for Matching Names and Records. In Proc. of KDD Works. on Data Cleaning and Object Consolidation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL Recognising Textual Entailment Challenge.</title>
<date>2006</date>
<booktitle>In Machine Learning Challenges, Lecture Notes in Computer Science,</booktitle>
<pages>177--190</pages>
<contexts>
<context position="12876" citStr="Dagan et al., 2006" startWordPosition="1916" endWordPosition="1919">es components which allow to integrate text similarity measures with any UIMA-based pipeline, as outlined in Figure 1. In the following, we introduce these components along with their resources. Readers &amp; Datasets DKPro Similarity includes corpus readers specifically tailored towards combining the input texts in a number of ways, e.g. all possible combinations, or each text paired with n others by random. Standard datasets for which 123 readers come pre-packaged include, among others, the SemEval-2012 STS data (Agirre et al., 2012), the METER corpus (Clough et al., 2002), or the RTE 1–5 data (Dagan et al., 2006). As far as license terms allow redistribution, the datasets themselves are integrated into the framework. Similarity Scorer The Similarity Scorer allows to integrate any text similarity measure (which is decoupled from UIMA by default) into a UIMAbased pipeline. It builds upon the standardized text similarity interfaces and thus allows to easily exchange the text similarity measure as well as to specify the data types the measure should operate on, e.g. tokens or lemmas. Machine Learning Previous research (Agirre et al., 2012) has shown that different text similarity measures can be combined </context>
<context position="17055" citStr="Dagan et al., 2006" startWordPosition="2557" endWordPosition="2560">t similarity measures along different text characteristics. Thereby, it not only combines simple string-based and semantic similarity measures (see Sections 3.1 and 3.2), but makes extensive use of measures along structural and stylistic text characteristics (see Section 3.3). Across three standard evaluation datasets, the system consistently outperforms all previous work. For recognizing textual entailment, we provide a setup which is similar in configuration to the one described above, but contains corpus readers and evaluation components precisely tailored towards the RTE challenge series (Dagan et al., 2006). We believe that our setup can be used for filtering those text pairs which need further analysis by a dedicated textual entailment system. 6A one-time setup of local lexical-semantic resources such as WordNet may be necessary, though. 7In 2013, the STS Task is a shared task of the Second Joint Conference on Lexical and Computational Semantics, http://ixa2.si.ehu.es/sts 124 6 Related Frameworks To the best of our knowledge, only a few generalized similarity frameworks exist at all. In the following, we discuss them and give insights where DKPro Similarity uses implementations of these existin</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL Recognising Textual Entailment Challenge. In Machine Learning Challenges, Lecture Notes in Computer Science, pages 177–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liviu P Dinu</author>
<author>Marius Popescu</author>
</authors>
<title>Ordinal measures in authorship identification.</title>
<date>2009</date>
<booktitle>In Proceedings of the 3rd PAN Workshop. Uncovering Plagiarism, Authorship and Social Software Misuse,</booktitle>
<pages>62--66</pages>
<contexts>
<context position="11611" citStr="Dinu and Popescu, 2009" startWordPosition="1717" endWordPosition="1720">udes measures for structural, stylistic, and phonetic similarity. Structural Similarity Structural similarity between texts can be computed, for example, by comparing sets of stopword n-grams (Stamatatos, 2011). The idea here is that similar texts may preserve syntactic similarity while exchanging only content words. Other measures in DKPro Similarity allow to compare texts by part-of-speech ngrams, and order and distance features for pairs of words (Hatzivassiloglou et al., 1999). Stylistic Similarity DKPro Similarity includes, for example, a measure which compares function word frequencies (Dinu and Popescu, 2009) between two texts. The framework also includes a set of measures which capture statistical properties of texts such as the type-token ratio (TTR) and the sequential TTR (McCarthy and Jarvis, 2010). Phonetic Similarity DKPro Similarity also allows to compute text similarity based on pairwise phonetic comparisons of words. It therefore contains implementations of well-known phonetic algorithms such as Double Metaphone (Philips, 2000) and Soundex (Knuth, 1973), which also conform to the common text similarity interface. 4 UIMA Components In addition to a rich set of text similarity measures as p</context>
</contexts>
<marker>Dinu, Popescu, 2009</marker>
<rawString>Liviu P. Dinu and Marius Popescu. 2009. Ordinal measures in authorship identification. In Proceedings of the 3rd PAN Workshop. Uncovering Plagiarism, Authorship and Social Software Misuse, pages 62–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Ferrucci</author>
<author>Adam Lally</author>
</authors>
<title>UIMA: An Architectural Approach to Unstructured Information Processing in the Corporate Research Environment.</title>
<date>2004</date>
<journal>Natural Language Engineering,</journal>
<pages>10--3</pages>
<contexts>
<context position="2190" citStr="Ferrucci and Lally, 2004" startWordPosition="316" endWordPosition="319">ins more than a small number of measures, even though related frameworks exist, which we discuss in Section 6. This fact was also realized by the organizers of the pilot Semantic Textual Similarity Task at SemEval-2012 (see Section 5), as they argue for the creation of an open source framework for text similarity (Agirre et al., 2012). In order to fill this gap, we present DKPro Similarity, an open source framework for text similarity. DKPro Similarity is designed to complement DKPro Core1, a collection of software components for natural language processing based on the Apache UIMA framework (Ferrucci and Lally, 2004). Our goal is to provide a comprehensive repository of text similarity measures which are implemented in a common framework using standardized interfaces. Besides the already available measures, DKPro Similarity is easily extensible and intended to allow for custom implementations, for which it offers various templates and examples. The Java implementation is publicly available at Google Code2 under the Apache Software License v2 and partly under GNU GPL v3. 2 Architecture DKPro Similarity is designed to operate in either of two modes: The stand-alone mode allows to use text similarity measure</context>
<context position="8080" citStr="Ferrucci and Lally, 2004" startWordPosition="1198" endWordPosition="1201"> provide new implementations for a multitude of measures, we rely on specialized libraries such as the S-Space Package (see Section 6) if available. Due to space limitations and due to the fact that the framework is actively under development, we do not provide an exhaustive list here, but rather mention the most interesting and most popular measures. 3.1 Simple String-based Measures DKPro Similarity includes text similarity measures which operate on string sequences and determine, for example, the longest common 4The JCas is an object-oriented Java interface to the Common Analysis Structure (Ferrucci and Lally, 2004), Apache UIMA’s internal document representation format. Pre-processing Text Similarity Computation Post-processing Greedy String Tiling Double Metaphone Explicit Sem. Analysis . . . Similarity Scorer 122 (non-)contiguous sequence of characters. It also contains Greedy String Tiling (Wise, 1996), a measure which allows to compare strings if parts have been reordered. The framework also offers measures which compute sets of character and word n-grams and compare them using different overlap coefficients, e.g. the Jaccard index. It further includes popular string distance metrics such as the Jar</context>
</contexts>
<marker>Ferrucci, Lally, 2004</marker>
<rawString>David Ferrucci and Adam Lally. 2004. UIMA: An Architectural Approach to Unstructured Information Processing in the Corporate Research Environment. Natural Language Engineering, 10(3-4):327–348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing Semantic Relatedness using Wikipediabased Explicit Semantic Analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<pages>1606--1611</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="4643" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="680" endWordPosition="683">xtSimilarityMeasure m = new GreedyStringTiling(); 2 double similarity = m.getSimilarity(text1, text2); The above code snippet instantiates the Greedy String Tiling measure (Wise, 1996) and then computes the text similarity between the given pair of texts. The resulting similarity score is normalized into [0, 1] where 0 means not similar at all, and 1 corresponds to perfectly similar.3 By using the common TextSimilarityMeasure interface, it is easy to replace Greedy String Tiling with any measure of choice, such as Latent Semantic Analysis (Landauer et al., 1998) or Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007). We give an overview of measures available in DKPro Similarity in Section 3. UIMA-coupled Mode In this mode, DKPro Similarity allows text similarity computation to be directly integrated with any UIMA-based language processing pipeline. That way, it is easy to use text similarity components in addition to other UIMA-based components in the same pipeline. For example, an experimental setup may require to first compute text similarity scores and then to run a classification algorithm on the resulting scores. In Figure 1, we show a graphical overview of the integration of text similarity measure</context>
<context position="10187" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="1508" endWordPosition="1511">ion for the complete texts can for example be done using the strategy by Mihalcea et al. (2006). Vector Space Models These text similarity measures project texts onto high-dimensional vectors which are then compared. Cosine similarity, a basic measure often used in information retrieval, weights words according to their term frequencies or tf-idf scores, and computes the cosine between two text vectors. Latent Semantic Analysis (Landauer et al., 1998) alleviates the inherent sparseness of a high-dimensional termdocument matrix by reducing it to one of reduced rank. Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) constructs the vector space on corpora where the documents are assumed to describe natural concepts such as cat or dog. Originally, Wikipedia was proposed as the document collection of choice. DKPro Similarity goes beyond a single implementation of these measures and comes with highly customizable code which allows to set var5http://www.wiktionary.org ious parameters for the construction of the vector space and the comparison of the document vectors, and further allows to construct the vector space for arbitrary collections, e.g. domain-specific corpora. 3.3 Further Measures Previous research</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing Semantic Relatedness using Wikipediabased Explicit Semantic Analysis. In Proceedings of IJCAI, pages 1606–1611, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Konstantina Garoufi</author>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>Representational Interoperability of Linguistic and Collaborative Knowledge Bases.</title>
<date>2008</date>
<booktitle>In Proceedings of the KONVENS Workshop on Lexical-Semantic and Ontological Resources.</booktitle>
<contexts>
<context position="9440" citStr="Garoufi et al., 2008" startWordPosition="1391" endWordPosition="1394">ty also contains several measures which go beyond simple character sequences and compute text similarity on a semantic level. Pairwise Word Similarity These measures are based on pairwise word similarity computations which are then aggregated for the complete texts. The measures typically operate on a graph-based representation of words and the semantic relations among them within a lexical-semantic resource. DKPro Similarity therefore contains adapters for WordNet, Wiktionary5, and Wikipedia, while the framework can easily be extended to other data sources that conform to a common interface (Garoufi et al., 2008). Pairwise similarity measures in DKPro Similarity include Jiang and Conrath (1997) or Resnik (1995). The aggregation for the complete texts can for example be done using the strategy by Mihalcea et al. (2006). Vector Space Models These text similarity measures project texts onto high-dimensional vectors which are then compared. Cosine similarity, a basic measure often used in information retrieval, weights words according to their term frequencies or tf-idf scores, and computes the cosine between two text vectors. Latent Semantic Analysis (Landauer et al., 1998) alleviates the inherent sparse</context>
</contexts>
<marker>Garoufi, Zesch, Gurevych, 2008</marker>
<rawString>Konstantina Garoufi, Torsten Zesch, and Iryna Gurevych. 2008. Representational Interoperability of Linguistic and Collaborative Knowledge Bases. In Proceedings of the KONVENS Workshop on Lexical-Semantic and Ontological Resources.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA Data Mining Software: An Update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="13733" citStr="Hall et al., 2009" startWordPosition="2048" endWordPosition="2051">to a UIMAbased pipeline. It builds upon the standardized text similarity interfaces and thus allows to easily exchange the text similarity measure as well as to specify the data types the measure should operate on, e.g. tokens or lemmas. Machine Learning Previous research (Agirre et al., 2012) has shown that different text similarity measures can be combined using machine learning classifiers. Such a combination shows improvements over single measures due to the fact that different measures capture different text characteristics. DKPro Similarity thus provides adapters for the Weka framework (Hall et al., 2009) and allows to first pre-compute sets of text similarity scores which can then be used as features for various machine learning classifiers. Evaluation Metrics In the final step of a UIMA pipeline, the processed data is read by a dedicated evaluation component. DKPro Similarity ships with a set of components which for example compute Pearson or Spearman correlation with human judgments, or apply task-specific metrics such as average precision as used in the RTE challenges. 5 Experimental Setups DKPro Similarity further encourages the creation and publication of complete experimental setups. Th</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA Data Mining Software: An Update. SIGKDD Explorations, 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Judith L Klavans</author>
<author>Eleazar Eskin</author>
</authors>
<title>Detecting text similarity over short passages: Exploring linguistic feature combinations via machine learning.</title>
<date>1999</date>
<booktitle>In Proceedings of EMNLP/VLC,</booktitle>
<pages>203--212</pages>
<contexts>
<context position="11473" citStr="Hatzivassiloglou et al., 1999" startWordPosition="1699" endWordPosition="1702">e inclusion of measures which go beyond textual content and compute similarity along other text characteristics. Thus, DKPro Similarity also includes measures for structural, stylistic, and phonetic similarity. Structural Similarity Structural similarity between texts can be computed, for example, by comparing sets of stopword n-grams (Stamatatos, 2011). The idea here is that similar texts may preserve syntactic similarity while exchanging only content words. Other measures in DKPro Similarity allow to compare texts by part-of-speech ngrams, and order and distance features for pairs of words (Hatzivassiloglou et al., 1999). Stylistic Similarity DKPro Similarity includes, for example, a measure which compares function word frequencies (Dinu and Popescu, 2009) between two texts. The framework also includes a set of measures which capture statistical properties of texts such as the type-token ratio (TTR) and the sequential TTR (McCarthy and Jarvis, 2010). Phonetic Similarity DKPro Similarity also allows to compute text similarity based on pairwise phonetic comparisons of words. It therefore contains implementations of well-known phonetic algorithms such as Double Metaphone (Philips, 2000) and Soundex (Knuth, 1973)</context>
</contexts>
<marker>Hatzivassiloglou, Klavans, Eskin, 1999</marker>
<rawString>Vasileios Hatzivassiloglou, Judith L. Klavans, and Eleazar Eskin. 1999. Detecting text similarity over short passages: Exploring linguistic feature combinations via machine learning. In Proceedings of EMNLP/VLC, pages 203–212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings of ROCLING,</booktitle>
<pages>19--33</pages>
<contexts>
<context position="9523" citStr="Jiang and Conrath (1997)" startWordPosition="1403" endWordPosition="1407"> compute text similarity on a semantic level. Pairwise Word Similarity These measures are based on pairwise word similarity computations which are then aggregated for the complete texts. The measures typically operate on a graph-based representation of words and the semantic relations among them within a lexical-semantic resource. DKPro Similarity therefore contains adapters for WordNet, Wiktionary5, and Wikipedia, while the framework can easily be extended to other data sources that conform to a common interface (Garoufi et al., 2008). Pairwise similarity measures in DKPro Similarity include Jiang and Conrath (1997) or Resnik (1995). The aggregation for the complete texts can for example be done using the strategy by Mihalcea et al. (2006). Vector Space Models These text similarity measures project texts onto high-dimensional vectors which are then compared. Cosine similarity, a basic measure often used in information retrieval, weights words according to their term frequencies or tf-idf scores, and computes the cosine between two text vectors. Latent Semantic Analysis (Landauer et al., 1998) alleviates the inherent sparseness of a high-dimensional termdocument matrix by reducing it to one of reduced ran</context>
<context position="19419" citStr="Jiang and Conrath (1997)" startWordPosition="2919" endWordPosition="2922">e Package. Semantic Vectors The Semantic Vectors package is a package for distributional semantics (Widdows and Cohen, 2010)9 that contains measures such as LSA and allows for comparing documents within a given vector space. The main focus lies on word space models with a number of dimension reduction techniques, and applications on word spaces such as automatic thesaurus generation. WordNet::Similarity The open source package by Pedersen et al. (2004)10 is a popular Perl library for the similarity computation on WordNet. It comprises six word similarity measures that operate on WordNet, e.g. Jiang and Conrath (1997) or Resnik (1995). Unfortunately, no strategies have been added to the package yet which aggregate the word similarity scores for complete texts in a similar manner as described in Section 3.2. 8code.google.com/p/airhead-research 9code.google.com/p/semanticvectors 10sourceforge.net/projects/wn-similarity In DKPro Similarity, we offer native Java implementations of all measures contained in WordNet::Similarity, and allow to go beyond WordNet and use the measures with any lexical-semantic resource of choice, e.g. Wiktionary or Wikipedia. SimMetrics Library The Java library by Chapman et al. (200</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J. Jiang and David W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of ROCLING, pages 19–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Keith Stevens</author>
</authors>
<title>The S-Space Package: An Open Source Package for Word Space Models.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 System Demonstrations,</booktitle>
<pages>30--35</pages>
<location>Uppsala,</location>
<contexts>
<context position="18209" citStr="Jurgens and Stevens, 2010" startWordPosition="2732" endWordPosition="2736">ve insights where DKPro Similarity uses implementations of these existing libraries. That way, DKPro Similarity brings together the scattered efforts by offering access to all measures through common interfaces. It goes far beyond the functionality of the original libraries as it generalizes the resources used, allows a tight integration with any UIMA-based pipeline, and comes with full-featured experimental setups which are pre-configured stand-alone text similarity systems that can be run out-of-the-box. S-Space Package Even though no designated text similarity library, the S-Space Package (Jurgens and Stevens, 2010)8 contains some text similarity measures such as Latent Semantic Analysis (LSA) and Explicit Semantic Analysis (see Section 3.2). However, it is primarily focused on word space models which operate on word distributions in text. Besides such algorithms, it offers a variety of interfaces, data structures, evaluation datasets and metrics, and global operation utilities e.g. for dimension reduction using Singular Value Decomposition or randomized projections, which are particularly useful with such distributional word space models. DKPro Similarity integrates LSA based on the S-Space Package. Sem</context>
</contexts>
<marker>Jurgens, Stevens, 2010</marker>
<rawString>David Jurgens and Keith Stevens. 2010. The S-Space Package: An Open Source Package for Word Space Models. In Proceedings of the ACL 2010 System Demonstrations, pages 30–35, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald E Knuth</author>
</authors>
<title>The Art of Computer Programming: Volume 3, Sorting and Searching.</title>
<date>1973</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="12073" citStr="Knuth, 1973" startWordPosition="1788" endWordPosition="1789">t al., 1999). Stylistic Similarity DKPro Similarity includes, for example, a measure which compares function word frequencies (Dinu and Popescu, 2009) between two texts. The framework also includes a set of measures which capture statistical properties of texts such as the type-token ratio (TTR) and the sequential TTR (McCarthy and Jarvis, 2010). Phonetic Similarity DKPro Similarity also allows to compute text similarity based on pairwise phonetic comparisons of words. It therefore contains implementations of well-known phonetic algorithms such as Double Metaphone (Philips, 2000) and Soundex (Knuth, 1973), which also conform to the common text similarity interface. 4 UIMA Components In addition to a rich set of text similarity measures as partly described above, DKPro Similarity includes components which allow to integrate text similarity measures with any UIMA-based pipeline, as outlined in Figure 1. In the following, we introduce these components along with their resources. Readers &amp; Datasets DKPro Similarity includes corpus readers specifically tailored towards combining the input texts in a number of ways, e.g. all possible combinations, or each text paired with n others by random. Standar</context>
</contexts>
<marker>Knuth, 1973</marker>
<rawString>Donald E. Knuth. 1973. The Art of Computer Programming: Volume 3, Sorting and Searching. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Peter W Foltz</author>
<author>Darrell Laham</author>
</authors>
<title>An Introduction to Latent Semantic Analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--2</pages>
<contexts>
<context position="4578" citStr="Landauer et al., 1998" startWordPosition="672" endWordPosition="675"> c�2013 Association for Computational Linguistics 1 TextSimilarityMeasure m = new GreedyStringTiling(); 2 double similarity = m.getSimilarity(text1, text2); The above code snippet instantiates the Greedy String Tiling measure (Wise, 1996) and then computes the text similarity between the given pair of texts. The resulting similarity score is normalized into [0, 1] where 0 means not similar at all, and 1 corresponds to perfectly similar.3 By using the common TextSimilarityMeasure interface, it is easy to replace Greedy String Tiling with any measure of choice, such as Latent Semantic Analysis (Landauer et al., 1998) or Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007). We give an overview of measures available in DKPro Similarity in Section 3. UIMA-coupled Mode In this mode, DKPro Similarity allows text similarity computation to be directly integrated with any UIMA-based language processing pipeline. That way, it is easy to use text similarity components in addition to other UIMA-based components in the same pipeline. For example, an experimental setup may require to first compute text similarity scores and then to run a classification algorithm on the resulting scores. In Figure 1, we show a</context>
<context position="10009" citStr="Landauer et al., 1998" startWordPosition="1483" endWordPosition="1486"> conform to a common interface (Garoufi et al., 2008). Pairwise similarity measures in DKPro Similarity include Jiang and Conrath (1997) or Resnik (1995). The aggregation for the complete texts can for example be done using the strategy by Mihalcea et al. (2006). Vector Space Models These text similarity measures project texts onto high-dimensional vectors which are then compared. Cosine similarity, a basic measure often used in information retrieval, weights words according to their term frequencies or tf-idf scores, and computes the cosine between two text vectors. Latent Semantic Analysis (Landauer et al., 1998) alleviates the inherent sparseness of a high-dimensional termdocument matrix by reducing it to one of reduced rank. Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) constructs the vector space on corpora where the documents are assumed to describe natural concepts such as cat or dog. Originally, Wikipedia was proposed as the document collection of choice. DKPro Similarity goes beyond a single implementation of these measures and comes with highly customizable code which allows to set var5http://www.wiktionary.org ious parameters for the construction of the vector space and the co</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Thomas K. Landauer, Peter W. Foltz, and Darrell Laham. 1998. An Introduction to Latent Semantic Analysis. Discourse Processes, 25(2):259–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions, and reversals.</title>
<date>1966</date>
<journal>Soviet Physics Doklady,</journal>
<volume>10</volume>
<issue>8</issue>
<contexts>
<context position="6233" citStr="Levenshtein, 1966" startWordPosition="920" endWordPosition="921">le format for evaluation or manual inspection. As all text similarity measures in DKPro Similarity conform to standardized interfaces, they can be easily exchanged in the text similarity computation step. With DKPro Similarity, we offer various subclasses of the generic UIMA components which are specifically tailored towards text similarity experiments, e.g. corpus readers for standard evaluation datasets as well as evaluation components for running typical evaluation metrics. By leveraging UIMA’s architecture, we also define an 3Some string distance measures such as the Levenshtein distance (Levenshtein, 1966) return a raw distance score where less distance corresponds to higher similarity. However, the score can easily be normalized, e.g. by text length. UIMA-based Pipeline Text Similarity Measures Corpus Reader Evaluation Figure 1: DKPro Similarity allows to integrate any text similarity measure (right) which conforms to standardized interfaces into a UIMA-based language processing pipeline (left) by means of a dedicated Similarity Scorer component (middle). additional interface to text similarity measures: The JCasTextSimilarityMeasure inherits from TextSimilarityMeasure, and adds a method for t</context>
<context position="8752" citStr="Levenshtein (1966)" startWordPosition="1296" endWordPosition="1297">t. Pre-processing Text Similarity Computation Post-processing Greedy String Tiling Double Metaphone Explicit Sem. Analysis . . . Similarity Scorer 122 (non-)contiguous sequence of characters. It also contains Greedy String Tiling (Wise, 1996), a measure which allows to compare strings if parts have been reordered. The framework also offers measures which compute sets of character and word n-grams and compare them using different overlap coefficients, e.g. the Jaccard index. It further includes popular string distance metrics such as the Jaro-Winkler (Winkler, 1990), Monge and Elkan (1997) and Levenshtein (1966) distance measures. 3.2 Semantic Similarity Measures DKPro Similarity also contains several measures which go beyond simple character sequences and compute text similarity on a semantic level. Pairwise Word Similarity These measures are based on pairwise word similarity computations which are then aggregated for the complete texts. The measures typically operate on a graph-based representation of words and the semantic relations among them within a lexical-semantic resource. DKPro Similarity therefore contains adapters for WordNet, Wiktionary5, and Wikipedia, while the framework can easily be </context>
<context position="20226" citStr="Levenshtein (1966)" startWordPosition="3035" endWordPosition="3036">3.2. 8code.google.com/p/airhead-research 9code.google.com/p/semanticvectors 10sourceforge.net/projects/wn-similarity In DKPro Similarity, we offer native Java implementations of all measures contained in WordNet::Similarity, and allow to go beyond WordNet and use the measures with any lexical-semantic resource of choice, e.g. Wiktionary or Wikipedia. SimMetrics Library The Java library by Chapman et al. (2005)11 exclusively comprises text similarity measures which compute lexical similarity on string sequences and compare texts without any semantic processing. It contains measures such as the Levenshtein (1966) or Monge and Elkan (1997) distance metrics. In DKPro Similarity, some string-based measures (see Section 3.1) are based on implementations from this library. SecondString Toolkit The freely available library by Cohen et al. (2003)12 is similar to SimMetrics, and also implemented in Java. It also contains several well-known text similarity measures on string sequences, and includes many of the measures which are also part of the SimMetrics Library. Some string-based measures in DKPro Similarity are based on the SecondString Toolkit. 7 Conclusions We presented DKPro Similarity, an open source f</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Vladimir I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics Doklady, 10(8):707–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip M McCarthy</author>
<author>Scott Jarvis</author>
</authors>
<title>MTLD, vocd-D, and HD-D: A validation study of sophisticated approaches to lexical diversity assessment. Behavior research methods,</title>
<date>2010</date>
<pages>42--2</pages>
<contexts>
<context position="11808" citStr="McCarthy and Jarvis, 2010" startWordPosition="1749" endWordPosition="1752">tamatatos, 2011). The idea here is that similar texts may preserve syntactic similarity while exchanging only content words. Other measures in DKPro Similarity allow to compare texts by part-of-speech ngrams, and order and distance features for pairs of words (Hatzivassiloglou et al., 1999). Stylistic Similarity DKPro Similarity includes, for example, a measure which compares function word frequencies (Dinu and Popescu, 2009) between two texts. The framework also includes a set of measures which capture statistical properties of texts such as the type-token ratio (TTR) and the sequential TTR (McCarthy and Jarvis, 2010). Phonetic Similarity DKPro Similarity also allows to compute text similarity based on pairwise phonetic comparisons of words. It therefore contains implementations of well-known phonetic algorithms such as Double Metaphone (Philips, 2000) and Soundex (Knuth, 1973), which also conform to the common text similarity interface. 4 UIMA Components In addition to a rich set of text similarity measures as partly described above, DKPro Similarity includes components which allow to integrate text similarity measures with any UIMA-based pipeline, as outlined in Figure 1. In the following, we introduce t</context>
</contexts>
<marker>McCarthy, Jarvis, 2010</marker>
<rawString>Philip M. McCarthy and Scott Jarvis. 2010. MTLD, vocd-D, and HD-D: A validation study of sophisticated approaches to lexical diversity assessment. Behavior research methods, 42(2):381–392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and Knowledge-based Measures of Text Semantic Similarity.</title>
<date>2006</date>
<booktitle>In Proceedings ofAAAI-06,</booktitle>
<pages>775--780</pages>
<location>Boston, MA, USA.</location>
<contexts>
<context position="9649" citStr="Mihalcea et al. (2006)" startWordPosition="1426" endWordPosition="1429">utations which are then aggregated for the complete texts. The measures typically operate on a graph-based representation of words and the semantic relations among them within a lexical-semantic resource. DKPro Similarity therefore contains adapters for WordNet, Wiktionary5, and Wikipedia, while the framework can easily be extended to other data sources that conform to a common interface (Garoufi et al., 2008). Pairwise similarity measures in DKPro Similarity include Jiang and Conrath (1997) or Resnik (1995). The aggregation for the complete texts can for example be done using the strategy by Mihalcea et al. (2006). Vector Space Models These text similarity measures project texts onto high-dimensional vectors which are then compared. Cosine similarity, a basic measure often used in information retrieval, weights words according to their term frequencies or tf-idf scores, and computes the cosine between two text vectors. Latent Semantic Analysis (Landauer et al., 1998) alleviates the inherent sparseness of a high-dimensional termdocument matrix by reducing it to one of reduced rank. Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) constructs the vector space on corpora where the documents ar</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and Knowledge-based Measures of Text Semantic Similarity. In Proceedings ofAAAI-06, pages 775–780, Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alvaro Monge</author>
<author>Charles Elkan</author>
</authors>
<title>An efficient domain-independent algorithm for detecting approximately duplicate database records.</title>
<date>1997</date>
<booktitle>In Proceedings of the SIGMOD Workshop on Data Mining and Knowledge Discovery,</booktitle>
<pages>23--29</pages>
<contexts>
<context position="8729" citStr="Monge and Elkan (1997)" startWordPosition="1291" endWordPosition="1294">cument representation format. Pre-processing Text Similarity Computation Post-processing Greedy String Tiling Double Metaphone Explicit Sem. Analysis . . . Similarity Scorer 122 (non-)contiguous sequence of characters. It also contains Greedy String Tiling (Wise, 1996), a measure which allows to compare strings if parts have been reordered. The framework also offers measures which compute sets of character and word n-grams and compare them using different overlap coefficients, e.g. the Jaccard index. It further includes popular string distance metrics such as the Jaro-Winkler (Winkler, 1990), Monge and Elkan (1997) and Levenshtein (1966) distance measures. 3.2 Semantic Similarity Measures DKPro Similarity also contains several measures which go beyond simple character sequences and compute text similarity on a semantic level. Pairwise Word Similarity These measures are based on pairwise word similarity computations which are then aggregated for the complete texts. The measures typically operate on a graph-based representation of words and the semantic relations among them within a lexical-semantic resource. DKPro Similarity therefore contains adapters for WordNet, Wiktionary5, and Wikipedia, while the f</context>
<context position="20252" citStr="Monge and Elkan (1997)" startWordPosition="3038" endWordPosition="3041">p/airhead-research 9code.google.com/p/semanticvectors 10sourceforge.net/projects/wn-similarity In DKPro Similarity, we offer native Java implementations of all measures contained in WordNet::Similarity, and allow to go beyond WordNet and use the measures with any lexical-semantic resource of choice, e.g. Wiktionary or Wikipedia. SimMetrics Library The Java library by Chapman et al. (2005)11 exclusively comprises text similarity measures which compute lexical similarity on string sequences and compare texts without any semantic processing. It contains measures such as the Levenshtein (1966) or Monge and Elkan (1997) distance metrics. In DKPro Similarity, some string-based measures (see Section 3.1) are based on implementations from this library. SecondString Toolkit The freely available library by Cohen et al. (2003)12 is similar to SimMetrics, and also implemented in Java. It also contains several well-known text similarity measures on string sequences, and includes many of the measures which are also part of the SimMetrics Library. Some string-based measures in DKPro Similarity are based on the SecondString Toolkit. 7 Conclusions We presented DKPro Similarity, an open source framework designed to strea</context>
</contexts>
<marker>Monge, Elkan, 1997</marker>
<rawString>Alvaro Monge and Charles Elkan. 1997. An efficient domain-independent algorithm for detecting approximately duplicate database records. In Proceedings of the SIGMOD Workshop on Data Mining and Knowledge Discovery, pages 23–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>WordNet::Similarity - Measuring the Relatedness of Concepts.</title>
<date>2004</date>
<booktitle>In Proceedings of the HLT-NAACL: Demonstration Papers,</booktitle>
<pages>38--41</pages>
<contexts>
<context position="19251" citStr="Pedersen et al. (2004)" startWordPosition="2891" endWordPosition="2894">Decomposition or randomized projections, which are particularly useful with such distributional word space models. DKPro Similarity integrates LSA based on the S-Space Package. Semantic Vectors The Semantic Vectors package is a package for distributional semantics (Widdows and Cohen, 2010)9 that contains measures such as LSA and allows for comparing documents within a given vector space. The main focus lies on word space models with a number of dimension reduction techniques, and applications on word spaces such as automatic thesaurus generation. WordNet::Similarity The open source package by Pedersen et al. (2004)10 is a popular Perl library for the similarity computation on WordNet. It comprises six word similarity measures that operate on WordNet, e.g. Jiang and Conrath (1997) or Resnik (1995). Unfortunately, no strategies have been added to the package yet which aggregate the word similarity scores for complete texts in a similar manner as described in Section 3.2. 8code.google.com/p/airhead-research 9code.google.com/p/semanticvectors 10sourceforge.net/projects/wn-similarity In DKPro Similarity, we offer native Java implementations of all measures contained in WordNet::Similarity, and allow to go be</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. WordNet::Similarity - Measuring the Relatedness of Concepts. In Proceedings of the HLT-NAACL: Demonstration Papers, pages 38–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Philips</author>
</authors>
<title>The double metaphone search algorithm.</title>
<date>2000</date>
<journal>C/C++ Users Jour.,</journal>
<volume>18</volume>
<issue>6</issue>
<contexts>
<context position="12047" citStr="Philips, 2000" startWordPosition="1784" endWordPosition="1785">of words (Hatzivassiloglou et al., 1999). Stylistic Similarity DKPro Similarity includes, for example, a measure which compares function word frequencies (Dinu and Popescu, 2009) between two texts. The framework also includes a set of measures which capture statistical properties of texts such as the type-token ratio (TTR) and the sequential TTR (McCarthy and Jarvis, 2010). Phonetic Similarity DKPro Similarity also allows to compute text similarity based on pairwise phonetic comparisons of words. It therefore contains implementations of well-known phonetic algorithms such as Double Metaphone (Philips, 2000) and Soundex (Knuth, 1973), which also conform to the common text similarity interface. 4 UIMA Components In addition to a rich set of text similarity measures as partly described above, DKPro Similarity includes components which allow to integrate text similarity measures with any UIMA-based pipeline, as outlined in Figure 1. In the following, we introduce these components along with their resources. Readers &amp; Datasets DKPro Similarity includes corpus readers specifically tailored towards combining the input texts in a number of ways, e.g. all possible combinations, or each text paired with n</context>
</contexts>
<marker>Philips, 2000</marker>
<rawString>Lawrence Philips. 2000. The double metaphone search algorithm. C/C++ Users Jour., 18(6):38–43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using Information Content to Evaluate Semantic Similarity in a Taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of the IJCAI,</booktitle>
<pages>448--453</pages>
<contexts>
<context position="9540" citStr="Resnik (1995)" startWordPosition="1409" endWordPosition="1410">a semantic level. Pairwise Word Similarity These measures are based on pairwise word similarity computations which are then aggregated for the complete texts. The measures typically operate on a graph-based representation of words and the semantic relations among them within a lexical-semantic resource. DKPro Similarity therefore contains adapters for WordNet, Wiktionary5, and Wikipedia, while the framework can easily be extended to other data sources that conform to a common interface (Garoufi et al., 2008). Pairwise similarity measures in DKPro Similarity include Jiang and Conrath (1997) or Resnik (1995). The aggregation for the complete texts can for example be done using the strategy by Mihalcea et al. (2006). Vector Space Models These text similarity measures project texts onto high-dimensional vectors which are then compared. Cosine similarity, a basic measure often used in information retrieval, weights words according to their term frequencies or tf-idf scores, and computes the cosine between two text vectors. Latent Semantic Analysis (Landauer et al., 1998) alleviates the inherent sparseness of a high-dimensional termdocument matrix by reducing it to one of reduced rank. Explicit Seman</context>
<context position="19436" citStr="Resnik (1995)" startWordPosition="2924" endWordPosition="2925">The Semantic Vectors package is a package for distributional semantics (Widdows and Cohen, 2010)9 that contains measures such as LSA and allows for comparing documents within a given vector space. The main focus lies on word space models with a number of dimension reduction techniques, and applications on word spaces such as automatic thesaurus generation. WordNet::Similarity The open source package by Pedersen et al. (2004)10 is a popular Perl library for the similarity computation on WordNet. It comprises six word similarity measures that operate on WordNet, e.g. Jiang and Conrath (1997) or Resnik (1995). Unfortunately, no strategies have been added to the package yet which aggregate the word similarity scores for complete texts in a similar manner as described in Section 3.2. 8code.google.com/p/airhead-research 9code.google.com/p/semanticvectors 10sourceforge.net/projects/wn-similarity In DKPro Similarity, we offer native Java implementations of all measures contained in WordNet::Similarity, and allow to go beyond WordNet and use the measures with any lexical-semantic resource of choice, e.g. Wiktionary or Wikipedia. SimMetrics Library The Java library by Chapman et al. (2005)11 exclusively </context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using Information Content to Evaluate Semantic Similarity in a Taxonomy. In Proceedings of the IJCAI, pages 448–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efstathios Stamatatos</author>
</authors>
<title>Plagiarism detection using stopword n-grams.</title>
<date>2011</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>62</volume>
<issue>12</issue>
<contexts>
<context position="11198" citStr="Stamatatos, 2011" startWordPosition="1657" endWordPosition="1658"> the vector space and the comparison of the document vectors, and further allows to construct the vector space for arbitrary collections, e.g. domain-specific corpora. 3.3 Further Measures Previous research (B¨ar et al., 2012b) has shown promising results for the inclusion of measures which go beyond textual content and compute similarity along other text characteristics. Thus, DKPro Similarity also includes measures for structural, stylistic, and phonetic similarity. Structural Similarity Structural similarity between texts can be computed, for example, by comparing sets of stopword n-grams (Stamatatos, 2011). The idea here is that similar texts may preserve syntactic similarity while exchanging only content words. Other measures in DKPro Similarity allow to compare texts by part-of-speech ngrams, and order and distance features for pairs of words (Hatzivassiloglou et al., 1999). Stylistic Similarity DKPro Similarity includes, for example, a measure which compares function word frequencies (Dinu and Popescu, 2009) between two texts. The framework also includes a set of measures which capture statistical properties of texts such as the type-token ratio (TTR) and the sequential TTR (McCarthy and Jar</context>
</contexts>
<marker>Stamatatos, 2011</marker>
<rawString>Efstathios Stamatatos. 2011. Plagiarism detection using stopword n-grams. Journal of the American Society for Information Science and Technology, 62(12):2512–2527.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
<author>Trevor Cohen</author>
</authors>
<title>The Semantic Vectors Package: New Algorithms and Public Tools for Distributional Semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of IEEE-ICSC,</booktitle>
<pages>9--15</pages>
<contexts>
<context position="18919" citStr="Widdows and Cohen, 2010" startWordPosition="2839" endWordPosition="2843">plicit Semantic Analysis (see Section 3.2). However, it is primarily focused on word space models which operate on word distributions in text. Besides such algorithms, it offers a variety of interfaces, data structures, evaluation datasets and metrics, and global operation utilities e.g. for dimension reduction using Singular Value Decomposition or randomized projections, which are particularly useful with such distributional word space models. DKPro Similarity integrates LSA based on the S-Space Package. Semantic Vectors The Semantic Vectors package is a package for distributional semantics (Widdows and Cohen, 2010)9 that contains measures such as LSA and allows for comparing documents within a given vector space. The main focus lies on word space models with a number of dimension reduction techniques, and applications on word spaces such as automatic thesaurus generation. WordNet::Similarity The open source package by Pedersen et al. (2004)10 is a popular Perl library for the similarity computation on WordNet. It comprises six word similarity measures that operate on WordNet, e.g. Jiang and Conrath (1997) or Resnik (1995). Unfortunately, no strategies have been added to the package yet which aggregate t</context>
</contexts>
<marker>Widdows, Cohen, 2010</marker>
<rawString>Dominic Widdows and Trevor Cohen. 2010. The Semantic Vectors Package: New Algorithms and Public Tools for Distributional Semantics. In Proceedings of IEEE-ICSC, pages 9–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William E Winkler</author>
</authors>
<title>String Comparator Metrics and Enhanced Decision Rules in the Fellegi-Sunter Model of Record Linkage.</title>
<date>1990</date>
<booktitle>In Proceedings of the Survey Research Methods Section,</booktitle>
<pages>354--359</pages>
<contexts>
<context position="8705" citStr="Winkler, 1990" startWordPosition="1289" endWordPosition="1290">MA’s internal document representation format. Pre-processing Text Similarity Computation Post-processing Greedy String Tiling Double Metaphone Explicit Sem. Analysis . . . Similarity Scorer 122 (non-)contiguous sequence of characters. It also contains Greedy String Tiling (Wise, 1996), a measure which allows to compare strings if parts have been reordered. The framework also offers measures which compute sets of character and word n-grams and compare them using different overlap coefficients, e.g. the Jaccard index. It further includes popular string distance metrics such as the Jaro-Winkler (Winkler, 1990), Monge and Elkan (1997) and Levenshtein (1966) distance measures. 3.2 Semantic Similarity Measures DKPro Similarity also contains several measures which go beyond simple character sequences and compute text similarity on a semantic level. Pairwise Word Similarity These measures are based on pairwise word similarity computations which are then aggregated for the complete texts. The measures typically operate on a graph-based representation of words and the semantic relations among them within a lexical-semantic resource. DKPro Similarity therefore contains adapters for WordNet, Wiktionary5, an</context>
</contexts>
<marker>Winkler, 1990</marker>
<rawString>William E. Winkler. 1990. String Comparator Metrics and Enhanced Decision Rules in the Fellegi-Sunter Model of Record Linkage. In Proceedings of the Survey Research Methods Section, pages 354–359.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Wise</author>
</authors>
<title>YAP3: Improved detection of similarities in computer program and other texts.</title>
<date>1996</date>
<booktitle>In Proc. of the 27th SIGCSE Technical Symposium on Computer Science Education,</booktitle>
<pages>130--134</pages>
<contexts>
<context position="4194" citStr="Wise, 1996" startWordPosition="608" endWordPosition="609">arity allows for a maximum flexibility in experimental design, as the text similarity measures can easily be integrated with any existing experimental setup: 1code.google.com/p/dkpro-core-asl 2code.google.com/p/dkpro-similarity-asl 121 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 121–126, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 1 TextSimilarityMeasure m = new GreedyStringTiling(); 2 double similarity = m.getSimilarity(text1, text2); The above code snippet instantiates the Greedy String Tiling measure (Wise, 1996) and then computes the text similarity between the given pair of texts. The resulting similarity score is normalized into [0, 1] where 0 means not similar at all, and 1 corresponds to perfectly similar.3 By using the common TextSimilarityMeasure interface, it is easy to replace Greedy String Tiling with any measure of choice, such as Latent Semantic Analysis (Landauer et al., 1998) or Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007). We give an overview of measures available in DKPro Similarity in Section 3. UIMA-coupled Mode In this mode, DKPro Similarity allows text similarity c</context>
<context position="8376" citStr="Wise, 1996" startWordPosition="1237" endWordPosition="1238">most interesting and most popular measures. 3.1 Simple String-based Measures DKPro Similarity includes text similarity measures which operate on string sequences and determine, for example, the longest common 4The JCas is an object-oriented Java interface to the Common Analysis Structure (Ferrucci and Lally, 2004), Apache UIMA’s internal document representation format. Pre-processing Text Similarity Computation Post-processing Greedy String Tiling Double Metaphone Explicit Sem. Analysis . . . Similarity Scorer 122 (non-)contiguous sequence of characters. It also contains Greedy String Tiling (Wise, 1996), a measure which allows to compare strings if parts have been reordered. The framework also offers measures which compute sets of character and word n-grams and compare them using different overlap coefficients, e.g. the Jaccard index. It further includes popular string distance metrics such as the Jaro-Winkler (Winkler, 1990), Monge and Elkan (1997) and Levenshtein (1966) distance measures. 3.2 Semantic Similarity Measures DKPro Similarity also contains several measures which go beyond simple character sequences and compute text similarity on a semantic level. Pairwise Word Similarity These </context>
</contexts>
<marker>Wise, 1996</marker>
<rawString>Michael J. Wise. 1996. YAP3: Improved detection of similarities in computer program and other texts. In Proc. of the 27th SIGCSE Technical Symposium on Computer Science Education, pages 130–134.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>