<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002068">
<title confidence="0.9979465">
UTD-HLT-CG: Semantic Architecture for Metonymy Resolution and
Classification of Nominal Relations
</title>
<author confidence="0.991482">
Cristina Nicolae, Gabriel Nicolae and Sanda Harabagiu
</author>
<affiliation confidence="0.9812725">
Human Language Technology Research Institute
The University of Texas at Dallas
</affiliation>
<address confidence="0.599216">
Richardson, Texas
</address>
<email confidence="0.978474">
{cristina, gabriel, sanda}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.995778" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996726588235294">
In this paper we present a semantic archi-
tecture that was employed for processing
two different SemEval 2007 tasks: Task
4 (Classification of Semantic Relations be-
tween Nominals) and Task 8 (Metonymy
Resolution). The architecture uses multi-
ple forms of syntactic, lexical, and semantic
information to inform a classification-based
approach that generates a different model for
each machine learning algorithm that imple-
ments the classification. We used decision
trees, decision rules, logistic regression and
lazy classifiers. A voting module selects the
best performing module for each task evalu-
ated in SemEval 2007. The paper details the
results obtained when using the semantic ar-
chitecture.
</bodyText>
<sectionHeader confidence="0.999123" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999796533333334">
Automatic semantic interpretations of natural lan-
guage text rely on (1) semantic theories that cap-
ture the subtleties employed by human communi-
cations; (2) lexico-semantic resources that encode
various forms of semantic knowledge; and (3) com-
putational methods that model the selection of the
optimal interpretation derived from the textual data.
Two of the SemEval 2007 tasks, namely Task 4
(Classification of Semantic Relations between Nom-
inals)and Task 8 (Metonymy Resolution) employed
distinct theories for the interpretation of their cor-
responding semantic phenomena, but, nevertheless,
they also shared several lexico-semantic resources,
and, furthermore, both these tasks could have been
cast as classification problems, in vein with most of
the recent work in computational semantic process-
ing. Based on this observation, we have designed
and implemented a semantic architecture that was
used in both tasks. In Section 2 of this paper we
give a brief description of the semantic theories cor-
responding to each of the two tasks, while in Section
3 we detail the semantic architecture. Section 4 de-
scribes the experimental results and evaluation.
We have used three lexico-semantic resources: (i)
the WordNet lexico-semantic database; (ii) VerbNet;
and (iii) the Lexical Conceptual Structure (LCS)
database. Used only by Task 4, WordNet is a lexico-
semantic database created at Princeton University1
(Fellbaum, 1998), which encodes a vast majority
of the English nouns, verbs, adjectives and adverbs,
and groups synonym words into synsets. VerbNet2
is a broad-coverage, comprehensive verb lexicon
created at University of Pennsylvania, compatible
with WordNet, but with explicitly stated syntactic
and semantic information, using Levin verb classes
(Levin, 1993) to systematically construct lexical en-
tities. Classes are hierarchically organized and each
class in the hierarchy has its corresponding syntac-
tic frames, semantic predicates and a list of typical
verb arguments. The Lexical Conceptual Structure
(Traum and Habash, 2000) is a compositional ab-
straction with language-independent properties. An
LCS is a directed graph with a root. Each node is as-
sociated with certain information, including a type, a
primitive and a field. An LCS captures the semantics
</bodyText>
<footnote confidence="0.885371">
lhttp://wordnet.princeton.edu
2http://verbs.colorado.edu/verb-index/verbnet-2.1.tar.gz
</footnote>
<page confidence="0.985516">
454
</page>
<bodyText confidence="0.66855">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 454–459,
Prague, June 2007. c�2007 Association for Computational Linguistics
</bodyText>
<listItem confidence="0.943698875">
Relation Positive example
1. CAUSE-EFFECT Earplugs relieve the discomfort from traveling with a cold allergy or sinus condition.
2. INSTRUMENT-AGENCY The judge hesitates, gavel poised, shooting them a warning look.
3. PRODUCT-PRODUCER The boy who made the threat was arrested, charged, and had items confiscated from his home.
4. ORIGIN-ENTITY Cinnamon oil is distilled from bark chips and used to alleviate stomach upsets.
5. THEME-TOOL The port scanner is a utility to scan a system to get the status of the TCP.
6. PART-WHOLE The granite benches are former windowsills from the Hearst Memorial Mining Building.
7. CONTENT-CONTAINER The kitchen holds patient drinks and snacks.
</listItem>
<tableCaption confidence="0.998085">
Table 1: Examples of semantic relations.
</tableCaption>
<bodyText confidence="0.993689">
of a lexical item through a combination of semantic
structure and semantic content.
</bodyText>
<sectionHeader confidence="0.963913" genericHeader="introduction">
2 Semantic Tasks
</sectionHeader>
<bodyText confidence="0.952952205882353">
The two semantic tasks addressed in this paper
are: Classification of Semantic Relations between
Nominals (Task 4), defined in (Girju et al., 2007)
and Metonymy Resolution (Task 8), defined in
(Markert and Nissim, 2007). Please refer to these
task description papers for more details. Both are
cast as classification tasks: given an unlabeled in-
stance, a system must label it according to one class
of a set specific to each task.
The training and testing datasets for the
metonymy resolution task are annotated in an
XML format. There are 1090 training and 842
testing instances for companies, and 941 training
and 908 testing instances for locations. Each
training instance corresponds to a context in
which a single name is annotated with its read-
ing (metonymic/literal/mixed) and, in case of
metonymy, its type (metotype). The testing dataset
for this task is annotated in a similar manner, only
the reading of the name is left unknown and must
be decided by the system.
For the classification of semantic relations be-
tween nominals, there exist seven training sets of
140 instances each for the seven semantic relations,
and seven corresponding testing sets of around 70
instances each. A training instance is annotated with
information about the boundaries of the two nom-
inals whose relation must be determined, the truth
value of their relation, the WordNet sense of each
nominal, and the query that was employed by the an-
notators to retrieve this example from the Web. The
testing instances are similar, with the only difference
being that the truth value of the relations is unknown
and must be determined.
</bodyText>
<sectionHeader confidence="0.96202" genericHeader="method">
3 Semantic Architecture
</sectionHeader>
<bodyText confidence="0.999951823529412">
The semantic architecture that we have designed
is illustrated in Figure 1, which contains the basic
modules and resources used in the various phases of
processing the input data towards the final submis-
sion format. The grayed-out modules are all used
only for the semantic relations classification task,
while the part of the figure represented by dotted
lines appears only in the metonymy resolution al-
gorithm. The input to the system, for both tasks,
comprises the annotated instances, either from the
training or the testing dataset. Before any feature is
extracted, the data passes through a pipeline of pre-
processing modules. The text is first split into tokens
in a heuristic manner. The resulting tokenized text is
given as input to Brill’s part of speech tagger3, which
associates each word with its part of speech (e.g.,
NN, PRP). The data further goes through Collins’
syntactic parser4, which builds the syntactic trees for
all the sentences in the text.
Additionally, for semantic relations classification,
the system creates the dependency structures for
all the sentences, using the dependency parser built
at Stanford5 and described in (de Marneffe et al.,
2006). The dependency parser extracts some of 48
grammatical relations for each pair of words in a
sentence. A second module that is specific only to
this task is (Surdeanu and Turmo, 2005)’s seman-
tic role labeler, which extracts the shallow seman-
tic structure for each sentence, that is, the predicates
and their arguments.
In order to extract the features for the machine
learning algorithm, the modules described above
are used, and, in addition, information from Word-
Net, VerbNet and the LCS Database is incorporated,
</bodyText>
<footnote confidence="0.998994333333333">
3http://www.cs.jhu.edu/—brill/
4http://people.csail.mit.edu/mcollins/code.html
5http://nlp.stanford.edu/downloads/lex-parser.shtml
</footnote>
<page confidence="0.99821">
455
</page>
<sectionHeader confidence="0.510982" genericHeader="method">
INSTANCES
</sectionHeader>
<figureCaption confidence="0.999387">
Figure 1: Semantic architecture.
</figureCaption>
<bodyText confidence="0.997364214285714">
Category Feature name Feature description
syntactic prevpos part of speech of previous word in the sentence
nextpos part of speech of next word in the sentence
determiner if the word has a determiner
prepgoverning if the word is governed by a prepositional phrase (PP), we extract the preposition
insidequotes if the word is inside quotes
lemmapost if the word is postmodifier for a noun, take the lemma of the noun
lemmapre if the word is premodifier for a noun, take the lemma of the noun
possession if the word is a possessor, and what it possesses
semantic role the role(s) of the name in the sentence: subject, object, under PP
rolelemma the combination between the role and the lemma of the verb whose argument the word is
rolevn same as above, but using the VerbNet class instead of the verb’s lemma
rolelevin same as above, but using the Levin class instead of the verb’s lemma
rolelcs same as above, but using LCS primitives from the LCS database instead of the verb’s lemma
</bodyText>
<tableCaption confidence="0.694881">
Table 2: Features for metonymy resolution.
</tableCaption>
<figure confidence="0.996760962962963">
Syntactic
Parser
POS Tagger
Tokenizer
Dependency
Parser
Feature
Extraction
Feature
Selection
PropBank
Parser
WordNet
VerbNet
LCS Database
Handcrafted
model1
model2
Voting
...
modelk
ANNOTATED
INSTANCES
Submission
Generation
Models
Generation
</figure>
<bodyText confidence="0.999908045454545">
along with other features, based on the manual an-
notations for both the training and testing datasets
by the task organizers. These other features use the
grammatical annotations for the possibly metonymic
name, in the case of metonymy resolution, and the
query that was used to retrieve that particular in-
stance and the disambiguated WordNet sense for the
two nominals, in the case of semantic relations clas-
sification.
The features implemented for the two tasks are
described in Tables 2 and 3. Their types are: syn-
tactic, semantic, lexical and other. The syntactic
features express the relationships between the tar-
get words and words from the rest of the sentence
(e.g., the part of speech of the previous word in the
sentence, or the dependency relations between two
words). The semantic features make use of the in-
formation given by the resources used by the system
(e.g., the VerbNet class of the verb whose argument
the word is, or the lexicographic category of a word
in WordNet). The lexical feature is the lemma of the
word. The other feature is the query provided by
Task 4.
Using these sets of features, a number of models
were generated by different machine learning tech-
niques included with the Weka data mining software
(Witten and Frank, 2005). The machine learning
classifiers comprise decision trees, decision rules,
logistic regression, and ”lazy” classifiers like k-
nearest-neighbor. Because of too many features gen-
erated for a relatively small training dataset, feature
selection is performed by Weka before creating the
models. Metonymy resolution uses in addition the
entire set of features, since the dataset has seven
times more instances than the other task. For the
classification of semantic relations, the initial total
and the number of features that remain after the se-
lection are printed in Table 4.
For metonymy resolution, there are six sub-
tasks to be resolved, which result from all
combinations between organization/location and
coarse/medium/fine granularity of the label. For the
classification of nominal relations, there are 28 sub-
tasks, resulting from the processing of the seven se-
</bodyText>
<page confidence="0.971201">
456
</page>
<bodyText confidence="0.993563125">
Category Feature name Feature description
syntactic dependency the dependency relations between the two words
modifier if one word is a modifier of the other
prepositions the prepositions immediately before and after both words
determiners the determiners of the two words
pattern the simplified pattern that exists in the sentence between the two words
lexical lemmas the lemmas of the words
semantic predicates the predicates whose arguments the two words are
predtypes the predicate types of the predicates above
samepred if the two words are arguments of the same predicate, which one that is
lexname the lexicographic category of each word in WordNet
hyponym if one word is a hyponym of the other in WordNet
partof if one word is a part of the other in WordNet
shareholonym if the two words share a holonym in WordNet
shareparent if the two words share a parent in WordNet
other query the query that was used by the annotators to retrieve the training example from the Web
</bodyText>
<tableCaption confidence="0.863246">
Table 3: Features for classification of semantic relations between nominals.
</tableCaption>
<table confidence="0.997542333333333">
R1 R2 R3 R4 RS R6 R7
before 682 1200 913 898 861 849 677
after 13 19 10 15 15 8 16
</table>
<tableCaption confidence="0.984827">
Table 4: The number of features before and af-
</tableCaption>
<bodyText confidence="0.986376333333333">
ter Weka selection, for each semantic relation
dataset: R1 CAUSE-EFFECT, R2 INSTRUMENT-
AGENCY, R3 PRODUCT-PRODUCER, R4 ORIGIN-
ENTITY, R5 THEME-TOOL, R6 PART-WHOLE, and
R7 CONTENT-CONTAINER.
mantic relations, in which four experiments are con-
ducted, each with an increasing number of train-
ing instances. We treated each subtask as a sepa-
rate classification problem. Its training set and fea-
tures are fed into Weka to create several models.
Each classification algorithm mentioned before is
employed to obtain one model. For each subtask,
the voting module selects the best performing model
on 10-fold crossvalidation, which is used to classify
the test instances. These annotated instances make
up the submission dataset for that particular subtask.
To note is that the coarse metonymic level and the
semantic relations classification are binary classifi-
cations, while the rest of the metonymic subtasks
are multi-class classifications, performed in a single
stage.
</bodyText>
<sectionHeader confidence="0.981263" genericHeader="method">
4 Experimental Results and Evaluation
</sectionHeader>
<bodyText confidence="0.997005666666667">
Both the metonymy resolution system and the sys-
tem for classification of semantic relations per-
formed well in the SemEval 2007 competition. The
</bodyText>
<table confidence="0.990038333333333">
Base type Coarse Medium Fine BA
Locations 84.1 84.0 82.2 79.4
Organizations 73.9 71.1 71.1 61.8
</table>
<tableCaption confidence="0.932527">
Table 5: Accuracy for the metonymy resolution sys-
tem at three granularity levels.
</tableCaption>
<table confidence="0.9170684">
Base type Reading P R F BA
Locations literal 88.2 92.4 90.2 79.4
non-literal 64.1 52.4 57.6 20.6
Organizations literal 75.8 84.8 80.0 61.8
non-literal 69.6 56.2 62.2 38.2
</table>
<tableCaption confidence="0.76794">
Table 6: Performance for the metonymy resolution
system for the coarse level.
</tableCaption>
<bodyText confidence="0.99925225">
experiments presented in this paper were done on
the training and testing datasets for each subtask. To
note is that no other training data was collected or
used than the one provided by the organizers.
</bodyText>
<sectionHeader confidence="0.707804" genericHeader="method">
4.1 Results for Metonymy Resolution
</sectionHeader>
<bodyText confidence="0.998274636363636">
This system was scored by measuring its accuracy at
three granularity levels (coarse, medium, and fine)
and the precision, recall and F score for all com-
binations of locations/organizations and literal/non-
literal. These results are tabulated in Tables 5, 6, 7
and 8.
All results are compared with the baseline accu-
racy values (BA). In Table 5, the baselines are com-
puted by taking all readings to be literal; for the rest,
the baseline is the percentage in the gold test data
of each reading. As can be observed, the readings
</bodyText>
<page confidence="0.99499">
457
</page>
<table confidence="0.998691">
Base type Reading P R F BA
Locations literal 87.8 93.5 90.5 79.4
mixed 0.0 0.0 0.0 2.2
metonymic 63.6 52.3 58.0 18.4
Organizations literal 74.3 90.0 81.4 61.8
mixed 28.6 13.1 18.0 7.2
metonymic 66.8 47.1 55.3 31.0
</table>
<tableCaption confidence="0.9961205">
Table 7: Performance for the metonymy resolution
system for the medium level.
</tableCaption>
<table confidence="0.999735684210526">
Base type Reading P R F BA
Loc literal 85.7 94.6 89.9 79.4
mixed 0.0 0.0 0.0 2.2
othermet 0.0 0.0 0.0 1.2
obj-for-name 0.0 0.0 0.0 0.0
obj-for-repr 0.0 0.0 0.0 0.0
place-for-people 57.1 45.4 50.6 15.5
place-for-event 0.0 0.0 0.0 1.1
place-for-prod 0.0 0.0 0.0 0.1
Org literal 74.4 90.4 81.6 61.8
mixed 50.0 3.33 6.25 7.1
othermet 0.0 0.0 0.0 1.0
obj-for-name 80.0 66.7 72.7 0.7
obj-for-repr 0.0 0.0 0.0 0.0
org-for-members 61.3 64.0 62.6 19.1
org-for-event 0.0 0.0 0.0 0.1
org-for-prod 60.6 29.9 40.0 8.0
org-for-fac 0.0 0.0 0.0 1.9
org-for-index 0.0 0.0 0.0 0.4
</table>
<tableCaption confidence="0.8732425">
Table 8: Performance for the metonymy resolution
system for the fine level.
</tableCaption>
<bodyText confidence="0.999601142857143">
for locations were more reliably identified than the
ones for companies. An explanation for this differ-
ence in performance lies in the fact that locations, in
their literal readings, are inactive entities, whereas in
their non-literal readings they are very often active,
especially in the annotated instances of the training
dataset. This cannot be said for organizations– they
can be active in their literal readings. The active vs.
inactive criterion, therefore, functions better for lo-
cations. Furthermore, since the training set contains
a ratio literals/non-literals of 1.7 for organizations
and 3.9 for locations, the models were skewed, iden-
tifying literal readings more easily than non-literal
ones, as shown in Table 6.
</bodyText>
<sectionHeader confidence="0.761112" genericHeader="method">
4.2 Results for Classification of Semantic
Relations between Nominals
</sectionHeader>
<bodyText confidence="0.9972175">
This task’s performance was measured by accuracy,
precision, recall and F-measure, the latter constitut-
</bodyText>
<table confidence="0.9990229">
Semantic relation P R F Acc Inst
Cause-Effect 65.5 87.8 75.0 70.0 80
Instrument-Agency 68.3 73.7 70.9 70.5 78
Product-Producer 66.7 96.8 78.9 65.6 93
Origin-Entity 62.9 61.1 62.0 66.7 81
Theme-Tool 70.0 24.1 35.9 64.8 71
Part-Whole 55.6 76.9 64.5 69.4 72
Content-Container 82.4 36.8 50.9 63.5 74
Average 67.3 65.3 62.6 67.2 78.4
Avg baseline 81.3 42.9 56.2 57.0 78.4
</table>
<tableCaption confidence="0.9381045">
Table 9: Performance of the semantic relations clas-
sification system for each semantic relation.
</tableCaption>
<bodyText confidence="0.999956142857143">
ing the score for ranking the systems in the com-
petition. Table 9 presents these scores by seman-
tic relation. The column entitled “Inst” contains the
number of instances in the testing sets correspond-
ing to each relation. The average baseline values
were computed by guessing the label to be the ma-
jority in the dataset for each relation. From this table
it can be observed that the PRODUCT-PRODUCER,
INSTRUMENT-AGENCY, and CAUSE-EFFECT rela-
tions were detected with a relatively very high per-
formance score, whereas the THEME-TOOL relation
classification yielded a relatively small score. This
can be explained as the effect of their specifications;
the three best-ranked relations are well-defined by
human standards, while the THEME-TOOL relation
is more ambiguous.
Table 10 contains the scores of the 10-fold cross-
validation experiments that were performed on the
training dataset in order to select the best classifi-
cation algorithm. The classifiers used in these ex-
periments were, in the order of appearance in the
table: JRip, Random Forest, ADTree, Logistic Re-
gression, IBk, and Random Tree. The Logistic Re-
gression classifier was chosen in the vast majority of
cases, because it achieved the highest score for six
out of the seven relations. For R6, PART-WHOLE,
Random Forest was preferred. This ranking between
the scores of classifying relations, done consider-
ing training accuracy only, does not however antic-
ipate the final F score ranking in Table 9. In par-
ticular, the crossvalidation accuracy of R5, THEME-
TOOL, is better than the accuracy for R3, PRODUCT-
PRODUCER, which came first in the final results,
whereas R5 came last and at a large distance from
the others. These lower-than-expected results in the
</bodyText>
<page confidence="0.994973">
458
</page>
<table confidence="0.999846285714286">
Alg R1 R2 R3 R4 R5 R6 R7
JRip 72.1 76.4 68.6 66.4 68.6 66.4 73.6
RandF 78.6 85.0 72.1 77.1 74.3 70.7 73.6
ADTree 72.9 79.3 70.0 70.7 70.7 68.6 69.3
LogReg 79.3 85.7 72.1 80.0 76.4 70.0 75.7
IBk 78.6 83.6 70.7 75.7 74.3 70.0 72.1
RandT 79.3 85.7 71.4 77.1 75.0 70.0 72.1
</table>
<tableCaption confidence="0.8785515">
Table 10: Results on 10-fold crossvalidation for
each relation and each classifier.
</tableCaption>
<bodyText confidence="0.996068">
evaluation were caused in part by the drastic feature
selection module that was applied before generating
the models. In experiments performed on the de-
velopment data, the accuracy on 10-fold crossvali-
dation was increased with an average of 7% by fea-
ture selection, but the same feature set on the test-
ing data obtained a final score 4.7% less than the
one obtained by using all the features (F=67.3%).
The results submitted in the evaluation were based
on feature selection because of this misleading per-
formance shift observed on the development set.
The task of classification of semantic relations be-
tween nominals required data to be separated into
four training sets: the first 35 instances (D1), the
first 70 instances (D2), the first 105 instances (D3),
and the entire set, 140 instances (D4). The letter “D”
stands for systems that use both the WordNet and the
query information provided by the organizers. The
results on the four sets are illustrated in Figure 2.
The results generally increase with the size of train-
ing data, and tend to be the same on D3 and D4,
which means that the D4 set does not bring signifi-
cant new information compared to D3.
</bodyText>
<figure confidence="0.998977727272727">
90 R1
80 R2
70 R3
60 R4
50 R5
40 R6
30 R7
20
10
0
D1 D2 D3 D4
</figure>
<figureCaption confidence="0.9774705">
Figure 2: Results of training on different portions of
the training dataset.
</figureCaption>
<sectionHeader confidence="0.998495" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999757111111111">
This paper has presented a semantic architecture
that participated in the SemEval 2007 competition
to evaluate two tasks, one for metonymy resolution,
and the other for the classification of semantic re-
lations between nominals. Although the tasks were
very different, the architecture produced competitive
results. The experimental results are reported in this
paper in a detailed manner, and some interesting ob-
servations can be drawn from them.
</bodyText>
<sectionHeader confidence="0.999163" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999250058823529">
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In 5th
International Conference on Language Resources and
Evaluation (LREC 2006).
C. Fellbaum, editor. 1998. WordNet: An Electronic Lexi-
cal Database and Some ofits Applications. MIT Press.
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel
Antohe. 2005. On the semantics of noun compounds.
In Computer Speech and Language, volume 19, pages
479–496.
Roxana Girju, Marti Hearst, Preslav Nakov, Vivi Nas-
tase, Stan Szpakowicz, Peter Turney, and Deniz Yuret.
2007. Task 04: Classification of semantic relations be-
tween nominal at semeval 2007. In SemEval 2007.
Beth Levin. 1993. English Verb Classes and Alterna-
tions. The University of Chicago Press, Chicago and
London.
Katja Markert and Malvina Nissim. 2002. Metonymy
resolution as a classification task. In the 2002 Con-
ference on Empirical Methods in Natural LAnguage
Processing (EMNLP2002).
Katja Markert and Malvina Nissim. 2007. Task 08:
Metonymy resolution at semeval 2007. In SemEval
2007.
Mihai Surdeanu and Jordi Turmo. 2005. Semantic role
labeling using complete syntactic analysis. In CoNLL
2005, Shared Task.
David Traum and Nizar Habash. 2000. Generation from
lexical conceptual structure. In Workshop on Applied
Interlinguas, ANLP-2000.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques. Morgan
Kaufmann, 2nd edition.
</reference>
<page confidence="0.999125">
459
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.676982">
<title confidence="0.9989285">UTD-HLT-CG: Semantic Architecture for Metonymy Resolution and Classification of Nominal Relations</title>
<author confidence="0.996872">Cristina Nicolae</author>
<author confidence="0.996872">Gabriel Nicolae</author>
<author confidence="0.996872">Sanda Harabagiu</author>
<affiliation confidence="0.99833">Human Language Technology Research Institute The University of Texas at Dallas</affiliation>
<address confidence="0.936747">Richardson, Texas</address>
<email confidence="0.935175">gabriel,</email>
<abstract confidence="0.987365166666667">In this paper we present a semantic architecture that was employed for processing two different SemEval 2007 tasks: Task 4 (Classification of Semantic Relations between Nominals) and Task 8 (Metonymy Resolution). The architecture uses multiple forms of syntactic, lexical, and semantic information to inform a classification-based approach that generates a different model for each machine learning algorithm that implements the classification. We used decision trees, decision rules, logistic regression and lazy classifiers. A voting module selects the best performing module for each task evaluated in SemEval 2007. The paper details the results obtained when using the semantic architecture.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In 5th International Conference on Language Resources and Evaluation (LREC</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In 5th International Conference on Language Resources and Evaluation (LREC 2006).</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database and Some ofits Applications.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database and Some ofits Applications. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Dan Moldovan</author>
<author>Marta Tatu</author>
<author>Daniel Antohe</author>
</authors>
<title>On the semantics of noun compounds.</title>
<date>2005</date>
<journal>In Computer Speech and Language,</journal>
<volume>19</volume>
<pages>479--496</pages>
<marker>Girju, Moldovan, Tatu, Antohe, 2005</marker>
<rawString>Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel Antohe. 2005. On the semantics of noun compounds. In Computer Speech and Language, volume 19, pages 479–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Marti Hearst</author>
<author>Preslav Nakov</author>
<author>Vivi Nastase</author>
<author>Stan Szpakowicz</author>
<author>Peter Turney</author>
<author>Deniz Yuret</author>
</authors>
<title>Task 04: Classification of semantic relations between nominal at semeval 2007. In SemEval</title>
<date>2007</date>
<contexts>
<context position="4513" citStr="Girju et al., 2007" startWordPosition="648" endWordPosition="651">on oil is distilled from bark chips and used to alleviate stomach upsets. 5. THEME-TOOL The port scanner is a utility to scan a system to get the status of the TCP. 6. PART-WHOLE The granite benches are former windowsills from the Hearst Memorial Mining Building. 7. CONTENT-CONTAINER The kitchen holds patient drinks and snacks. Table 1: Examples of semantic relations. of a lexical item through a combination of semantic structure and semantic content. 2 Semantic Tasks The two semantic tasks addressed in this paper are: Classification of Semantic Relations between Nominals (Task 4), defined in (Girju et al., 2007) and Metonymy Resolution (Task 8), defined in (Markert and Nissim, 2007). Please refer to these task description papers for more details. Both are cast as classification tasks: given an unlabeled instance, a system must label it according to one class of a set specific to each task. The training and testing datasets for the metonymy resolution task are annotated in an XML format. There are 1090 training and 842 testing instances for companies, and 941 training and 908 testing instances for locations. Each training instance corresponds to a context in which a single name is annotated with its r</context>
</contexts>
<marker>Girju, Hearst, Nakov, Nastase, Szpakowicz, Turney, Yuret, 2007</marker>
<rawString>Roxana Girju, Marti Hearst, Preslav Nakov, Vivi Nastase, Stan Szpakowicz, Peter Turney, and Deniz Yuret. 2007. Task 04: Classification of semantic relations between nominal at semeval 2007. In SemEval 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations.</title>
<date>1993</date>
<publisher>The University of Chicago Press,</publisher>
<location>Chicago and London.</location>
<contexts>
<context position="2780" citStr="Levin, 1993" startWordPosition="400" endWordPosition="401"> have used three lexico-semantic resources: (i) the WordNet lexico-semantic database; (ii) VerbNet; and (iii) the Lexical Conceptual Structure (LCS) database. Used only by Task 4, WordNet is a lexicosemantic database created at Princeton University1 (Fellbaum, 1998), which encodes a vast majority of the English nouns, verbs, adjectives and adverbs, and groups synonym words into synsets. VerbNet2 is a broad-coverage, comprehensive verb lexicon created at University of Pennsylvania, compatible with WordNet, but with explicitly stated syntactic and semantic information, using Levin verb classes (Levin, 1993) to systematically construct lexical entities. Classes are hierarchically organized and each class in the hierarchy has its corresponding syntactic frames, semantic predicates and a list of typical verb arguments. The Lexical Conceptual Structure (Traum and Habash, 2000) is a compositional abstraction with language-independent properties. An LCS is a directed graph with a root. Each node is associated with certain information, including a type, a primitive and a field. An LCS captures the semantics lhttp://wordnet.princeton.edu 2http://verbs.colorado.edu/verb-index/verbnet-2.1.tar.gz 454 Proce</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes and Alternations. The University of Chicago Press, Chicago and London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Markert</author>
<author>Malvina Nissim</author>
</authors>
<title>Metonymy resolution as a classification task.</title>
<date>2002</date>
<booktitle>In the 2002 Conference on Empirical Methods in Natural LAnguage Processing (EMNLP2002).</booktitle>
<marker>Markert, Nissim, 2002</marker>
<rawString>Katja Markert and Malvina Nissim. 2002. Metonymy resolution as a classification task. In the 2002 Conference on Empirical Methods in Natural LAnguage Processing (EMNLP2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Markert</author>
<author>Malvina Nissim</author>
</authors>
<title>Task 08: Metonymy resolution at semeval</title>
<date>2007</date>
<booktitle>In SemEval</booktitle>
<contexts>
<context position="4585" citStr="Markert and Nissim, 2007" startWordPosition="659" endWordPosition="662">upsets. 5. THEME-TOOL The port scanner is a utility to scan a system to get the status of the TCP. 6. PART-WHOLE The granite benches are former windowsills from the Hearst Memorial Mining Building. 7. CONTENT-CONTAINER The kitchen holds patient drinks and snacks. Table 1: Examples of semantic relations. of a lexical item through a combination of semantic structure and semantic content. 2 Semantic Tasks The two semantic tasks addressed in this paper are: Classification of Semantic Relations between Nominals (Task 4), defined in (Girju et al., 2007) and Metonymy Resolution (Task 8), defined in (Markert and Nissim, 2007). Please refer to these task description papers for more details. Both are cast as classification tasks: given an unlabeled instance, a system must label it according to one class of a set specific to each task. The training and testing datasets for the metonymy resolution task are annotated in an XML format. There are 1090 training and 842 testing instances for companies, and 941 training and 908 testing instances for locations. Each training instance corresponds to a context in which a single name is annotated with its reading (metonymic/literal/mixed) and, in case of metonymy, its type (met</context>
</contexts>
<marker>Markert, Nissim, 2007</marker>
<rawString>Katja Markert and Malvina Nissim. 2007. Task 08: Metonymy resolution at semeval 2007. In SemEval 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Jordi Turmo</author>
</authors>
<title>Semantic role labeling using complete syntactic analysis.</title>
<date>2005</date>
<booktitle>In CoNLL 2005, Shared Task.</booktitle>
<contexts>
<context position="7368" citStr="Surdeanu and Turmo, 2005" startWordPosition="1114" endWordPosition="1117"> to Brill’s part of speech tagger3, which associates each word with its part of speech (e.g., NN, PRP). The data further goes through Collins’ syntactic parser4, which builds the syntactic trees for all the sentences in the text. Additionally, for semantic relations classification, the system creates the dependency structures for all the sentences, using the dependency parser built at Stanford5 and described in (de Marneffe et al., 2006). The dependency parser extracts some of 48 grammatical relations for each pair of words in a sentence. A second module that is specific only to this task is (Surdeanu and Turmo, 2005)’s semantic role labeler, which extracts the shallow semantic structure for each sentence, that is, the predicates and their arguments. In order to extract the features for the machine learning algorithm, the modules described above are used, and, in addition, information from WordNet, VerbNet and the LCS Database is incorporated, 3http://www.cs.jhu.edu/—brill/ 4http://people.csail.mit.edu/mcollins/code.html 5http://nlp.stanford.edu/downloads/lex-parser.shtml 455 INSTANCES Figure 1: Semantic architecture. Category Feature name Feature description syntactic prevpos part of speech of previous wo</context>
</contexts>
<marker>Surdeanu, Turmo, 2005</marker>
<rawString>Mihai Surdeanu and Jordi Turmo. 2005. Semantic role labeling using complete syntactic analysis. In CoNLL 2005, Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Traum</author>
<author>Nizar Habash</author>
</authors>
<title>Generation from lexical conceptual structure.</title>
<date>2000</date>
<booktitle>In Workshop on Applied Interlinguas, ANLP-2000.</booktitle>
<contexts>
<context position="3051" citStr="Traum and Habash, 2000" startWordPosition="437" endWordPosition="440"> 1998), which encodes a vast majority of the English nouns, verbs, adjectives and adverbs, and groups synonym words into synsets. VerbNet2 is a broad-coverage, comprehensive verb lexicon created at University of Pennsylvania, compatible with WordNet, but with explicitly stated syntactic and semantic information, using Levin verb classes (Levin, 1993) to systematically construct lexical entities. Classes are hierarchically organized and each class in the hierarchy has its corresponding syntactic frames, semantic predicates and a list of typical verb arguments. The Lexical Conceptual Structure (Traum and Habash, 2000) is a compositional abstraction with language-independent properties. An LCS is a directed graph with a root. Each node is associated with certain information, including a type, a primitive and a field. An LCS captures the semantics lhttp://wordnet.princeton.edu 2http://verbs.colorado.edu/verb-index/verbnet-2.1.tar.gz 454 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 454–459, Prague, June 2007. c�2007 Association for Computational Linguistics Relation Positive example 1. CAUSE-EFFECT Earplugs relieve the discomfort from traveling with a cold allerg</context>
</contexts>
<marker>Traum, Habash, 2000</marker>
<rawString>David Traum and Nizar Habash. 2000. Generation from lexical conceptual structure. In Workshop on Applied Interlinguas, ANLP-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<title>Data Mining: Practical machine learning tools and techniques.</title>
<date>2005</date>
<publisher>Morgan Kaufmann,</publisher>
<note>2nd edition.</note>
<contexts>
<context position="10408" citStr="Witten and Frank, 2005" startWordPosition="1596" endWordPosition="1599"> from the rest of the sentence (e.g., the part of speech of the previous word in the sentence, or the dependency relations between two words). The semantic features make use of the information given by the resources used by the system (e.g., the VerbNet class of the verb whose argument the word is, or the lexicographic category of a word in WordNet). The lexical feature is the lemma of the word. The other feature is the query provided by Task 4. Using these sets of features, a number of models were generated by different machine learning techniques included with the Weka data mining software (Witten and Frank, 2005). The machine learning classifiers comprise decision trees, decision rules, logistic regression, and ”lazy” classifiers like knearest-neighbor. Because of too many features generated for a relatively small training dataset, feature selection is performed by Weka before creating the models. Metonymy resolution uses in addition the entire set of features, since the dataset has seven times more instances than the other task. For the classification of semantic relations, the initial total and the number of features that remain after the selection are printed in Table 4. For metonymy resolution, th</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical machine learning tools and techniques. Morgan Kaufmann, 2nd edition.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>