<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9905895">
Estimating Upper and Lower Bounds
on the Performance of Word-Sense Disambiguation Programs
</title>
<author confidence="0.738719666666667">
William Gale
Kenneth Ward Church
David Yarowsky
</author>
<affiliation confidence="0.509441">
AT&amp;T Bell Laboratories
</affiliation>
<address confidence="0.905954">
600 Mountain Ave.
Murray Hill, NJ 07974
</address>
<email confidence="0.994375">
kwc@research.att.com
</email>
<sectionHeader confidence="0.969453" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997963222222222">
We have recently reported on two new word-sense
disambiguation systems, one trained on bilingual
material (the Canadian Hansards) and the other trained
on monolingual material (Roget&apos;s Thesaurus and
Grolier&apos;s Encyclopedia). After using both the
monolingual and bilingual classifiers for a few months,
we have convinced ourselves that the performance is
remarkably good. Nevertheless, we would really like to
be able to make a stronger statement, and therefore, we
decided to try to develop some more objective
evaluation measures. Although there has been a fair
amount of literature on sense-disambiguation, the
literature does not offer much guidance in how we might
establish the success or failure of a proposed solution
such as the two systems mentioned in the previous
paragraph. Many papers avoid quantitative evaluations
altogether, because it is so difficult to come up with
credible estimates of performance.
This paper will attempt to establish upper and lower
bounds on the level of performance that can be expected
in an evaluation. An estimate of the lower bound of
75% (averaged over ambiguous types) is obtained by
measuring the performance produced by a baseline
system that ignores context and simply assigns the most
likely sense in all cases. An estimate of the upper bound
is obtained by assuming that our ability to measure
performance is largely limited by our ability obtain
reliable judgments from human informants. Not
surprisingly, the upper bound is very dependent on the
instructions given to the judges. Jorgensen, for example,
suspected that lexicographers tend to depend too much
on judgments by a single informant and found
considerable variation over judgments (only 68%
agreement), as she had suspected. In our own
experiments, we have set out to find word-sense
disambiguation tasks where the judges can agree often
</bodyText>
<footnote confidence="0.8659285">
enough so that we could show that they were
outperforming the baseline system. Under quite
different conditions, we have found 96.8% agreement
over judges.
</footnote>
<sectionHeader confidence="0.555775" genericHeader="method">
1. Introduction: Using Massive Lexicographic Resources
</sectionHeader>
<bodyText confidence="0.999171794871795">
Word-sense disambiguation is a long-standing problem
in computational linguistics (e.g., Kaplan (1950), Yngve
(1955), Bar-Hillel (1960), Masterson (1967)), with
important implications for a number of practical
applications including text-to-speech (TTS), machine
translation (MT), information retrieval (IR), and many
others. The recent interest in computational
lexicography has fueled a large body of recent work on
this 40-year-old problem, e.g., Black (1988), Brown et
al. (1991), Choueka and Lusignan (1985), Clear (1989),
Dagan et al. (1991), Gale et al. (to appear), Hearst
(1991), Lesk (1986), Smadja and McKeown (1990),
Walker (1987), Veronis and Ide (1990), Yarowsky
(1992), Zernik (1990, 1991). Much of this work offers
the prospect that a disambiguation system might be able
to input unrestricted text and tag each word with the
most likely sense with fairly reasonable accuracy and
efficiency, just as part of speech taggers (e.g., Church
(1988)) can now input unrestricted text and assign each
word with the most likely part of speech with fairly
reasonable accuracy and efficiency.
The availability of massive lexicographic databases
offers a promising route to overcoming the knowledge
acquisition bottleneck. More than thirty years ago, Bar-
Hillel (1960) predicted that it would be &amp;quot;futile&amp;quot; to write
expert-system-like rules by-hand (as they had been doing
at Georgetown at the time) because there would be no
way to scale up such rules to cope with unrestricted
input. Indeed, it is now well-known that expert-system-
like rules can be notoriously difficult to scale up, as
Small and Reiger (1982) and many others have
observed:
&amp;quot;The expert for THROW is currently six pages long.., but
it should be 10 times that size.&amp;quot;
Bar-Hillel was very early in realizing the scope of the
problem; he observed that people have a large set of
facts at their disposal, and it is not obvious how a
computer could ever hope to gain access to this wealth
of knowledge.
</bodyText>
<page confidence="0.997244">
249
</page>
<bodyText confidence="0.998518214285715">
&amp;quot; &apos;But why not envisage a system which will put this
knowledge at the disposal of the translation machine?&apos;
Understandable as this reaction is, it is very easy to show
its futility. What such a suggestion amounts to, if taken
seriously, is the requirement that a translation machine
should not only be supplied with a dictionary but also with
a universal encyclopedia. This is surely utterly chimerical
and hardly deserves any further discussion. Since,
however, the idea of a machine with encyclopedic
knowledge has popped up also on other occasions, let me
add a few words on this topic. The number of facts we
human beings know is, in a certain very pregnant sense,
infinite.&amp;quot; (Bar-Hillel, 1960)
Ironically, much of the research cited above is taking
exactly the approach that Bar-Hillel ridiculed as utterly
chimerical and hardly deserving of any further
discussion. Back in 1960, it may have been hard to
imagine how it would be possible to supply a machine
with both a dictionary and an encyclopedia. But much
of the recent work cited above goes much further; not
only does it supply a machine with a dictionary and an
encyclopedia, but many other extensive references works
as well, including Roget&apos;s Thesaurus and numerous
large corpora. Of course, we are using these reference
works in a very superficial way; we are certainly not
suggesting that the machine should attempt to solve the
&amp;quot;Al Complete&amp;quot; problem of &amp;quot;understanding&amp;quot; these
reference works.
</bodyText>
<sectionHeader confidence="0.681933" genericHeader="method">
2. A Brief Summary of Our Previous Work
</sectionHeader>
<bodyText confidence="0.999993365384615">
Our own work has made use of many of these lexical
resources. In particular, (Gale et al., to appear) achieved
considerable progress by using well-understood
statistical methods and very large datasets of tens of
millions of words of parallel English and French text
(e.g., the Canadian Hansards). By aligning the text as
we have, we were able to collect a large set of examples
of polysemous words (e.g., sentence) in each sense (e.g.,
judicial sentence vs. syntactic sentence), by extracting
instances from the corpus that were translated one way
or the other (e.g, peine or phrase). These data sets were
then analyzed using well-understood Bayesian
discrimination methods, which have been used very
successfully in many other applications, especially
author identification (Mosteller and Wallace, 1964,
section 3.1) and information retrieval (1R) (van
Rijsbergen, 1979, chapter 6; Salton, 1989, section 10.3),
though their application to word-sense disambiguation is
novel.
In author identification and information retrieval, it is
customary to split the discrimination process up into a
testing phase and a training phase. During the training
phase, we are given two (or more) sets of documents and
are asked to construct a discriminator which can
distinguish between the two (or more) classes of
documents. These discriminators are then applied to
new documents during the testing phase. In the author
identification task, for example, the training set consists
of several documents written by each of the two (or
more) authors. The resulting discriminator is then tested
on documents whose authorship is disputed. In the
information retrieval application, the training set consists
of a set of one or more relevant documents and a set of
zero or more irrelevant documents. The resulting
discriminator is then applied to all documents in the
library in order to separate the more relevant ones from
the less relevant ones.
There is an embarrassing wealth of information in the
collection of documents that could be used as the basis
for discrimination. It is common practice to treat
documents as &amp;quot;merely&amp;quot; a bag of words, and to ignore
much of the linguistic structure, especially dependencies
on word order and correlations between pairs of words.
In other words, one assumes that there are two (or more)
sources of word probabilities, rel and irrel, in the IR
application, and author1 and author 2 in the author
identification application. During the training phase, we
attempt to estimate Pr(wI source) for all words w in the
vocabulary and all sources. Then during the testing
phase, we score all documents as follows and select high
scoring documents as being relatively likely to have
been generated by the source of interest.
</bodyText>
<equation confidence="0.989707">
r7 Pr(wirel)
wlidoc Pr(wiirrel)
Pr(w1 author 1)
w in doe Pr(w1 author 2)
</equation>
<bodyText confidence="0.999689666666667">
In the sense disambiguation application, the 100-word
context surrounding instances of a polysemous word
(e.g., sentence) are treated very much like a document.&apos;
</bodyText>
<equation confidence="0.936886">
Pr(wl sense 1)
w in context
</equation>
<bodyText confidence="0.9944636">
That is, during the testing phase, we are given a new
instance of a polysemous word, e.g., sentence, and asked
to assign it to one or more senses. We score the words
in the 100-word context using the formula given above,
and assign the instance to sense1 if the score is large.
</bodyText>
<footnote confidence="0.87061">
I. It is common to use very small contexts (e.g., 5-words) based on
the observation that people seem to be able to disambiguate word-
senses based on very little context. We have taken a different
approach. Since we have been able to find useful information out
to 100 words (and measurable information out to 10,000 words),
we feel we might as well make use of the the larger contexts. This
task is very difficult for the machine; it needs all the help it can get.
</footnote>
<figure confidence="0.83132075">
Information Retreival (IR)
Author Identification
Sense Disambiguation
Pr(w I sense 2 )
</figure>
<page confidence="0.99353">
250
</page>
<bodyText confidence="0.999985695652174">
The conditional probabilities, Pr(w1 sense), are
determined during the training phase by counting the
number of times that each word in the vocabulary was
found near each sense of the polysemous word (and then
smoothing these estimates in order to deal with the
sparse-data problems). See Gale et al. (to appear) for
further details.
At first, we thought that the method was completely
dependent on the availability of parallel corpora for
training. This has been a problem since parallel text
remains somewhat difficult to obtain in large quantity,
and what little is available is often fairly unbalanced and
unrepresentative of general language. Moreover, the
assumption that differences in translation correspond to
differences in word-sense has always been somewhat
suspect. Recently, Yarowsky (1992) has found a way to
extend our use of the Bayesian techniques by training on
the Roget&apos;s Thesaurus (Chapman, 1977)2 and Grolier&apos;s
Encyclopedia (1991) instead of the Canadian Hansards,
thus circumventing many of the objections to our use of
the Hansards. Yarowsky (1992) inputs a 100-word
context surrounding a polysemous word and scores each
of the 1042 Roget Categories by:
</bodyText>
<equation confidence="0.7670415">
fi Pr(w1 Roget Category)
w in context
</equation>
<bodyText confidence="0.995594714285714">
The program can also be run in a mode where it takes
unrestricted text as input and tags each word with its
most likely Roget Category. Some results for the word
crane are presented below, showing that the program can
be used to sort a concordance by sense.
Input Output
Treadmills attached to cranes were used to lift heavy TOOLS
for supplying power for cranes , hoists , and lifts TOOLS
Above this height , a tower crane is often used .SB This TOOLS
elaborate courtship rituals cranes build a nest of vegetation ANIMAL
are more closely related to cranes and rails .SB They range ANIMAL
low trees .PP At least five crane species are in danger of ANIMAL
After using both the monolingual and bilingual
classifiers for a few months, we have convinced
ourselves that the performance is remarkably good.
Nevertheless, we would really like to be able to make a
stronger statement, and therefore, we decided to try to
develop some more objective evaluation measures.
2. Note that this edition of the Roget&apos;s Thesaurus is much more
extensive than the 1911 version, though somewhat more difficult to
obtain in electronic form.
</bodyText>
<sectionHeader confidence="0.381406" genericHeader="method">
3. The Literature on Evaluation
</sectionHeader>
<bodyText confidence="0.999888368421053">
Although there has been a fair amount of literature on
sense-disambiguation, the literature does not offer much
guidance in how we might establish the success or
failure of a proposed solution such as the two described
above. Most papers tend to avoid quantitative
evaluations. Lesk (1986), an extremely innovative and
commonly cited reference on the subject, provides a
short discussion of evaluation, but fails to offer any very
satisfying solutions that we might adopt to quantify the
performance of our two disambiguation algorithms.3
Perhaps the most common evaluation technique is to
select a small sample of words and compare the results
of the machine with those of a human judge. This
method has been used very effectively by Kelly and
Stone (1975), Black (1988), Hearst (1991), and many
others. Nevertheless, this technique is not without its
problems, perhaps the worst of which is that the sample
may not be very representative of the general
vocabulary. Zernik (1990, p. 27), for example, reports
70% performance for the word interest, and then
acknowledges that this level of performance may not
generalize very well to other words.4
Although we agree with Zernik&apos;s prediction that interest
is not very representative of other words, we suspect that
interest is actually more difficult than most other words,
not less difficult. Table 1 shows the performance of
Yarowsky (1992) on twelve words which have been
previously discussed in the literature. Note that interest
is at the bottom of the list.
The reader should exercise some caution in interpreting
the numbers in Table 1. It is natural to try to use these
numbers to predict performance on new words, but the
study was not designed for that purpose. The test words
were selected from the literature in order to make
comparisons over systems. If the study had been
intended to support predictions on new words, then the
study should have used a random sample of such words,
rather than a sample of words from the literature.
</bodyText>
<listItem confidence="0.941174">
3. &amp;quot;What is the current performance of this program? Some very
brief experimentation with my program has yielded accuracies of
50-70% on short samples of Pride and Prejudice and an Associated
Press news story. Considerably more work is needed both to
improve the program and to do more thorough evaluation... There
is too much subjectivity in these measurements.&amp;quot; (Lesk, 1986, p. 6)
4. &amp;quot;For all 4 senses of INTEREST, both recall and precision are over
70%... However, not for all words are the obtained results that
positive... The fact is that almost any English word possesses
multiple senses. (Zenilk, 1990, p. 27)
</listItem>
<page confidence="0.991276">
251
</page>
<bodyText confidence="0.999967">
In addition to the sampling questions, one feels
uncomfortable about comparing results across
experiments, since there are many potentially important
differences including different corpora, different words,
different judges, differences in treatment of precision
and recall, and differences in the use of tools such as
parsers and part of speech taggers, etc. In short, there
seem to be a number of serious questions regarding the
commonly used technique of reporting percent correct
on a few words chosen by hand. Apparently, the
literature on evaluation of word-sense disambiguation
algorithms fails to offer a clear role model that we might
follow in order to quantify the performance of our
disambiguation algorithms.
</bodyText>
<listItem confidence="0.529314">
4. What is the State-of-the-Art, and How Good Does It
</listItem>
<subsectionHeader confidence="0.425613">
Need To Be?
</subsectionHeader>
<bodyText confidence="0.998207727272727">
Moreover, there doesn&apos;t seem to be a very clear sense of
what is possible. Is interest a relatively easy word or is
it a relatively hard word? Zemik says it is relatively
easy; we say it is relatively hard.5 Should we expect the
next word to be easier than interest or harder than
interest?
One might ask if 70% is good or bad. In fact, both
Black (1988) and Yarowsky (1992) report 72%
performance on this very same word. Although it is
dangerous to compare such results since there are many
potentially important differences (e.g., corpora, judges,
</bodyText>
<listItem confidence="0.4810536">
5. As evidence that interest is relatively difficult, we note that both the
Oxford Advanced Learner&apos;s Dictionary (OALD) (Crowie et al.,
1989, P. 654) and COBUILD (Sinclair et al., 1987), for example,
devote more than a full column to this word, indicating that it is an
extremely complex word, at least by their standards.
</listItem>
<bodyText confidence="0.994979">
etc.), it appears that Zernik&apos;s 70% figure is fairly
representative of the state of the art.6
Should we be happy with 70% performance? In fact,
70% really isn&apos;t very good. Recall that Bar-Hillel (1960,
p. 159) abandoned the machine translation field when he
couldn&apos;t see how a machine could possibly do a decent
job in translating text if it couldn&apos;t do better than this in
disambiguating word senses. Bar-Hillel&apos;s real objection
was an empirical one. Using his numbers,7 it appears
that programs, at the time, could disambiguate only
about 75% of the words in a sentence (e.g., 15 out of
20). If interest is a relatively easy word, as Zemik
(1990) suggests, then it would seem that Bar-Hillel&apos;s
argument remains as true today as it was in 1960, and we
ought to follow his lead and find something more
productive to do with our time. On the other hand, if we
are correct and interest is a relatively difficult word, then
it is possible that we have made some progress over the
past thirty years...
</bodyText>
<sectionHeader confidence="0.675792" genericHeader="method">
5. Upper and Lower Bounds
</sectionHeader>
<subsectionHeader confidence="0.905264">
5.1 Lower Bounds
</subsectionHeader>
<bodyText confidence="0.99994625">
We could be in a better position to address the question
of the relative difficulty of interest if we could establish
a rough estimate of the upper and lower bounds on the
level of performance that can be expected. We will
estimate the lower bound by evaluating the performance
of a straw man system, which ignores context and
simply assigns the most likely sense in all cases. One
might hope that reasonable systems should generally
</bodyText>
<listItem confidence="0.945806352941176">
6. In fact, Zernik&apos;s 70% figure is probably significantly inferior to the
72% reported by Black and Yarowsky, because Zemik reports
precision and recall separately, whereas the others report a single
figure of merit which combines both Type I (false rejection) and
Type II (false acceptance) errors by reporting precision at 100%
recall. Gale et al. show that error rates for 70% recall were half of
those for 100% recall, on their test sample.
7. &amp;quot;Let me state rather dogmatically that there exists at this moment
no method of reducing the polysemy of the, say, twenty words of
an average Russian sentence in a scientific article below a
remainder of, I would estimate, at least five or six words with
multiple English renderings, which would not seriously endanger
the quality of the machine output. Many tend to believe that by
reducing the number of initially possible renderings of a twenty
word Russian sentence from a few tens of thousands (which is the
approximate number resulting from the assumption that each of the
twenty Russian words has two renderings on the average, while
</listItem>
<bodyText confidence="0.908336142857143">
seven or eight of them have only one rendering) to some eighty
(which would be the number of renderings on the assumption that
sixteen words are uniquely rendered and four have three renderings
apiece, forgetting now about all the other aspects such as change of
word order, etc.) the main bulk of this kind of work has been
achieved, the remainder requiring only some slight additional
effort.&amp;quot; (Bar-Hillel, 1960, p. 163)
</bodyText>
<tableCaption confidence="0.992142">
Table 1: Comparison over Systems
</tableCaption>
<table confidence="0.998797466666667">
Word Yarowsky (1992) Previous Systems
bow 91% &lt;67% (Clear, 1989)
bass 99% 100% (Hearst, 1991)
galley 99% 50-70% (Lesk, 1986)
mole 99% N/A (Hirst, 1987)
sentence 98% 90% (Gale et al.)
slug 97% N/A (Hirst, 1987)
star 96% N/A (Hirst, 1987)
duty 96% 96% (Gale et al.)
issue 94% &lt;70% (Zemik, 1990)
taste 93% &lt;65% (Clear, 1989)
cone 77% 50-70% (Lesk, 1986)
interest 72% 72% (Black, 1988);
70% (Zernik, 1990)
AVERAGE 92% N/A
</table>
<page confidence="0.99039">
252
</page>
<bodyText confidence="0.954638460526315">
outperform this baseline system, though not all such
systems actually do. In fact, Yarowsky (1992) falls
below the baseline for one of the twelve words (issue),
although perhaps, we needn&apos;t be too concerned about
this one deviation.8
There are, of course, a number of problems with this
estimate of the baseline. First, the baseline system is not
operational, at least as we have defined it. Ideally, the
baseline system ought to try to estimate the most likely
sense for each word in the vocabulary and then assign
that sense to each instance of the word in the test set.
Unfortunately, since it isn&apos;t clear just how this
estimation should be accomplished, we decided to
&amp;quot;cheat&amp;quot; and let the baseline system peek at the test set
and &amp;quot;estimate&amp;quot; the most likely sense for each word as
the more frequent sense in the test set. Consequently,
the performance of the baseline cannot fall below chance
(100/k% for a particular word with k senses).9
In addition, the baseline system assumes that Type I
(false rejection) errors are just as bad as Type II (false
acceptance) errors. If one desires extremely high recall
and is willing to sacrifice precision in order to obtain this
level of recall, then it might be sensible to tune a system
to produce behavior which might appear to fall below
the baseline. We have run into such situations when we
have attempted to help lexicographers find extremely
unusual events. In such a case, a lexicographer might be
quite happy receiving a long list of potential candidates,
only a small fraction of which are actually the case of
interest. One can come up with quite a number of other
scenarios where the baseline performance could be
somewhat misleading, especially when there is an
unusual trade-off between the cost of a Type I error and
the cost of a Type II error.
Nevertheless, the proposed baseline does seem to
provide a usable rough estimate of the lower bound on
performance. Table 2 shows the baseline performance
for each of the twelve words in Table 1. Note that
performance is generally above the baseline as we would
8. Many of the systems mentioned in Table 2 including Yarowsky
(1992) do not currently take advantage of the prior probabilities of
the senses, so they would be at a disadvantage relative to the
baseline if one of the senses had a very high prior, as is the case for
the test word issue.
9. In addition, the baseline doesn&apos;t deal as well as it could with
skewed distributions. One could almost certainly improve the
model of the baseline by making use of a notion like entropy that
could deal more effectively with skewed distributions.
Nevertheless, we will stick with our simpler notion of the baseline
for expository convenience.
hope.
As mentioned previously, the test words in Tables 1 and
2 were selected from the literature on polysemy, and
therefore, tend to focus on the more difficult cases. In
another experiment, we selected a random sample of 97
words; 67 of them were unambiguous and therefore had
a baseline performance of 100%.10 The remaining thirty
words are listed along with the number of senses and
baseline performance: virus (2, 98%), device (3, 97%),
direction (2, 96%), reader (2, 96%), core (3, 94%), hull
(2, 94%), right (5, 94%), proposition (2, 89%), deposit
(2, 88%), hour (4, 87%), path (2, 86%), view (3, 86%),
pyramid (3, 82%), antenna (2, 81%), trough (3, 77%),
tyranny (2, 75%), figure (6, 73%), institution (4, 71%),
crown (4, 64%), drum (2, 63%), pipe (4, 60%),
processing (2, 59%), coverage (2, 58%), execution (2,
57%), min (2, 57%), interior (4, 56%), campaign (2,
51%), output (2, 51%), gin (3, 50%), drive (3, 49%). In
studying these 97 words, we found that the average
baseline performance is much higher than we might have
guessed (93% averaged over tokens, 92% averaged over
types). In particular, note that this baseline is well above
the 75% figure that we associated with Bar-Hillel above.
Of course, the large number of unambiguous words
contributes greatly to the baseline. If we exclude the
unambiguous words, then the average baseline
</bodyText>
<construct confidence="0.878496428571429">
10. The 67 unambiguous words were: acid, annexation, benzene, berry,
capacity, cereal, clock, coke, colon, commander, consort, contract,
cruise, cultivation, delegate, designation, dialogue, disaster,
equation, esophagus, fact, fear;, fertility, flesh, fox, gold, interface,
interruption, intrigue, journey, knife, label, landscape, laurel, lb,
liberty, lily, locomotion, lynx, marine, memorial, menstruation,
miracle, monasticism mountain, nitrate, orthodoxy, pest, planning,
</construct>
<page confidence="0.630458666666667">
possibility, pottery, projector, regiment, relaxation, reunification,
shore, sodium, specialty, stretch, surnmer, testing, tungsten,
universe, variant, vigor, wire, worship.
</page>
<tableCaption confidence="0.949317">
Table 2: The Baseline
</tableCaption>
<table confidence="0.999790785714286">
Word Baseline Yarowsky (1992)
issue 96% 94%
duty 87% 96%
galley 83% 99%
star 83% 96%
taste 74% 93%
bass 70% 99%
slug 62% 97%
sentence 62% 98%
interest 60% 72%
mole 59% 99%
cone 51% 77%
bow 48% 91%
AVERAGE 70% 92%
</table>
<page confidence="0.998668">
253
</page>
<bodyText confidence="0.974928">
performance falls to 81% averaged over tokens and 75%
averaged over types.
</bodyText>
<subsectionHeader confidence="0.999187">
5.2 Upper Bounds
</subsectionHeader>
<bodyText confidence="0.999986375">
We will attempt to estimate an upper bound on
performance by estimating the ability for human judges
to agree with one another (or themselves). We will find,
not surprisingly, that the estimate varies widely
depending on a number of factors, especially the
definition of the task. Jorgensen (1990) has collected
some interesting data that may be relevant for estimating
the agreement among judges. As part of her dissertation
under George Miller at Princeton, she was interested in
assessing &amp;quot;the extent of psychologically real polysemy
in the mental lexicon for nouns.&amp;quot; Her experiment was
designed to study one of the more commonly employed
methods in lexicography for writing dictionary
definitions, namely the use of citation indexes. She was
concerned that lexicographers and computational
linguists have tended to depend too much on the
intuitions of a single informant. Not surprisingly, she
found considerable variation across judgements, just as
she had suspected. This finding could have serious
implications for evaluation. How do we measure
performance if we can&apos;t depend on the judges?
Jorgensen selected twelve high frequency nouns at
random from the Brown Corpus, six were highly
polysemous (head, life, world, way, side, hand) and six
were less so (fact, group, night, development, something,
war). Sentences containing each of these words were
drawn from the Brown Corpus and typed on filing cards.
Nine subjects where then asked to cluster a packet of
these filing cards by sense. A week or two later, the
same nine subjects were asked to repeat the experiment,
but this time they were given access to the dictionary
definitions.
Jorgensen reported performance in terms of the
&amp;quot;Agreement-Disagreement&amp;quot; (A-D) ratio (Shipstone,
1960) for each subject and each of the twelve test words.
We have found it convenient to transform the A-D ratio
into a quantity which we call the percent agreement, the
number of observed agreements over the total number of
possible agreements. The grand mean percent
agreement over all subjects and words is only 68%. In
other words, at least under these conditions, there is
considerable variation across judgements, perhaps so
much so that it would be hard to show that a proposed
system was outperforming the baseline system (75%,
averaged over ambiguous types). Moreover, if we
accept Bar-Hillel&apos;s argument that 75% is not-good-
enough, then it would be hard to show that a system was
doing well-enough.
</bodyText>
<sectionHeader confidence="0.940372" genericHeader="method">
6. A Discrimination Experiment
</sectionHeader>
<bodyText confidence="0.99973046">
For evaluation purposes, it is important to find a task that
is somewhat easier for the judges. If the task is too hard
(as Jorgensen&apos;s classification task may be), then there
will be almost no room between the limits of the
measurement and the baseline. In other words, there
won&apos;t be enough dynamic range to measure differences
between better systems and worse systems. In contrast,
if we focus on easier tasks, then we might have enough
dynamic range to show some interesting differences.
Therefore, unlike Jorgensen who was interested in
highlighting differences among judgments, we are much
more interested in highlighting agreements. Fortunately,
we have found in (Gale et al., 1992) that the agreement
rate can be very high (96.8%), which is well above the
baseline, under very different experimental conditions.
Of course, it is a fairly major step to redefine the
problem from a classification task to a discrimination
one, as we are proposing. One might have preferred not
to do so, but we simply don&apos;t know how one could
establish enough dynamic range in that case to show any
interesting differences. It has been our experience that it
is very hard to design an experiment of any kind which
will produce the desired agreement among judges. We
are very happy with the 96.8% agreement that we were
able to show, even if it is limited to a much easier task
than the one that Jorgensen was interested in.
We originally designed the experiment in Gale et al.
(1992) to test the hypothesis that multiple uses of a
polysemous word tend to have the same sense within a
common discourse. A simple (but non-blind) pilot
experiment provided some suggestive evidence
confirming the hypothesis. A random sample of 108
nouns (which included the 97 words previously
mentioned) was extracted for further study. A panel of
three judges (the three authors of this paper) were given
100 sets of concordance lines containing one of the test
words selected from a single article in Grolier&apos;s. The
judges were asked to indicate if the set of concordance
lines used the same sense or not. Only 6 of 300 article-
judgements were judged to contain multiple senses of
one of the test words. All three judges were convinced
after grading 100 articles that there was considerable
validity to the hypothesis.
With this promising preliminary verification, the
following blind test was devised. Five subjects (the
three authors and two of their colleagues) were given a
questionnaire starting with a set of definitions selected
from OALD (Crowie et al., 1989) and followed by a
number of pairs of concordance lines, randomly selected
from Grolier&apos;s Encyclopedia (1991). The subjects were
</bodyText>
<page confidence="0.995426">
254
</page>
<bodyText confidence="0.861399333333333">
asked to decide for each pair, whether the two
concordance lines corresponded to the same sense or not.
antenna
</bodyText>
<listItem confidence="0.974188">
1. jointed organ found in pairs on the heads of
insects and crustaceans, used for feeling, etc. ---&gt; the
illus at insect.
2. radio or TV aerial.
</listItem>
<bodyText confidence="0.9950515">
lack eyes , legs, wings , antennae, and distinct mouthparts and
The Brachycera have short antennae and include the more evolved
silk moths passes over the antennae .SB Only males that detect
relatively simple form of antenna is the dipole , or doublet
The questionnaire contained a total of 82 pairs of
concordance lines for 9 polysemous words: antenna,
campaign, deposit, drum, hull, interior, knife, landscape,
and marine. The results of the experiment are shown
below in Table 3. With the exception of judge 2, all of
the judges agreed with the majority opinion in all but
one or two of the 82 cases. The agreement rate was
96.8%, averaged over all judges, or 99.1%, averaged
over the four best judges. In either case, the agreement
rate is well above the previously described ceiling.
</bodyText>
<tableCaption confidence="0.79666">
Table 3
</tableCaption>
<table confidence="0.6933515">
Judge
1 82 100.0%
2 72 87.8%
3 81 98.7%
4 82 100.0%
5 80 97.6%
Average 96.8%
Average (without Judge 2) 99.1%
</table>
<bodyText confidence="0.99992025">
Incidentally, the experiment did, in fact, confirm the
hypothesis that multiple uses of a polysemous word will
generally take on the same sense within a discourse. Of
the 82 judgments, 54 were selected from the same
discourse and were judged to have the same sense by the
majority in 96.9% of the cases. (The remaining 28 of
the 82 judgments were used as a control to force the
judges to say that some pairs were different.)
Note that the tendency for multiple uses of a polysemous
word to have the same sense is extremely strong; 96.9%
is much greater than the baseline, and indeed, it is
considerably above the level of performance that might
be expected from state-of-the-art word-sense
disambiguation systems. Since it is so reliable and so
easy to compute, it might be used as a quick-and-dirty
measure for testing such systems. Unfortunately, we
also need a complementary measure that would penalize
a system like the baseline system that simply assigned
all instances of a polysemous word to the same sense.
At present, we have yet to identify a quick-and-dirty
measure that accomplishes this control, and
consequently, we are forced to continue to depend on the
relatively expensive panel of judges. But, at least, we
have been able to establish that it is possible to design a
discrimination experiment such that the panel of judges
can agree with themselves often enough to be useful. In
addition, we have established that the discourse
constraint on polysemy is extremely strong, much
stronger than our ability to tag word-senses
automatically. Consequently, it ought to be possible to
use this constraint in our next word-sense tagging
algorithm to produce even better performance.
</bodyText>
<sectionHeader confidence="0.962034" genericHeader="conclusions">
7. Conclusions
</sectionHeader>
<bodyText confidence="0.999834894736842">
We began this discussion with a review of our recent
work on word-sense disambiguation, which extends the
approach of using massive lexicographic resources (e.g.,
parallel corpora, dictionaries, thesauruses and
encyclopedia) in order to attack the knowledge-
acquisition bottleneck that Bar-Hillel identified over
thirty years ago. After using both the monolingual and
bilingual classifiers for a few months, we have
convinced ourselves that the performance is remarkably
good. Nevertheless, we would really like to be able to
make a stronger statement, and therefore, we decided to
try to develop some more objective evaluation measures.
A survey of the literature on evaluation failed to identify
an attractive role model. In addition, we found it
particularly difficult to obtain a clear estimate of the
state-of-the-art.
In order to address this state of affairs, we decided to try
to establish upper and lower bounds on the level of
performance that we could expect to obtain. We
estimated the lower bound by positing a simple baseline
system which ignored context and simply assigned the
most likely sense in all cases. Hopefully, most
reasonable systems would outperform this system. The
upper bound was approximated by trying to estimate the
limit of our ability to measure performance. We
assumed that this limit was largely dominated by the
ability for the human judges to agree with one another.
The estimate depends very much, not surprisingly, on
the particular experimental design. Jorgensen, who was
interested in highlighting differences among informants,
found a very low estimate (68%), well below the
baseline (75%), and also well below the level that Bar-
Hillel asserted as not-good-enough. In our own work,
we have attempted to highlight agreements, so that there
would more dynamic range between the baseline and the
limit of our ability to measure performance. In so doing,
we were able to obtain a much more usable estimate of
(96.8%) by redefining the task from a classification task
</bodyText>
<page confidence="0.992469">
255
</page>
<bodyText confidence="0.999983454545455">
to a discrimination task. In addition, we also made use
of the constraint that multiple instances of a polysemous
word in the same discourse have a very strong tendency
to take on the same sense. This constraint will probably
prove useful for improving the performance of future
word-sense disambiguation algorithms.
Similar attempts to establish upper and lower bounds on
performance have been made in other areas of
computational linguistics, specifically part of speech
tagging. For that application, it is generally accepted
that the baseline part-of-speech tagging performance is
about 90% (as estimated by a similar baseline system
that ignores context and simply assigns the most likely
part of speech to all instances of a word) and that the
upper bound (imposed by the limit for judges to agree
with one another) is about 95%. Incidentally, most part
of speech algorithms are currently performing at or near
the limit of our ability to measure performance,
indicating that there may be room for refining the
experimental conditions along similar lines to what we
have done here, in order to improve the dynamic range
of the evaluation.
</bodyText>
<sectionHeader confidence="0.994881" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999765340909091">
Bar-Hillel (1960), &amp;quot;Automatic Translation of Languages,&amp;quot; in
Advances in Computers, Donald Booth and R. E. Meagher, eds.,
Academic, NY.
Black, Ewa (1988), &amp;quot;An Experiment in Computational Discrimination
of English Word Senses,&amp;quot; IBM Journal of Research and Development,
v 32, pp 185-194.
Brown, Peter, Stephen Della Pietra, Vincent Della Pietra, and Robert
Mercer (1991), &amp;quot;Word Sense Disambiguation using Statistical
Methods,&amp;quot; ACL, pp. 264-270.
Chapman, Robert (1977). Roget&apos;s International Thesaurus (Fourth
Edition), Harper and Row, NY.
Chouelca, Yaacov, and Serge Lusignan (1985), &amp;quot;Disambiguation by
Short Contexts,&amp;quot; Computers and the Humanities, v 19. pp. 147-158.
Church, Kenneth (1988), &amp;quot;A Stochastic Parts Program an Noun Phrase
Parser for Unrestricted Text,&amp;quot; Applied ACL Conference, Austin, Texas.
Clear, Jeremy (1989). &amp;quot;An Experiment in Automatic Word Sense
Identification,&amp;quot; Internal Document, Oxford University Press, Oxford.
Crowie, Anthony et al. (eds.) (1989), &amp;quot;Oxford Advanced Learner&apos;s
Dictionary,&amp;quot; Fourth Edition, Oxford University Press.
Dagan, Ido, Alon Rai, and Ulrike Schwall (1991), &amp;quot;Two Languages are
more Informative than One,&amp;quot; ACL, pp. 130-137.
Gale, William, Kenneth Church, and David Yarowsky (to appear) &amp;quot;A
Method for Disambiguating Word Senses in a Large Corpus,&amp;quot;
Computers and Humanities.
Gale, William, Kenneth Church, and David Yarowsky (1992) &amp;quot;One
Sense Per Discourse,&amp;quot; Darpa Speech and Natural Language Workshop.
Gove, Philip et at. (eds.) (1975) &amp;quot;Webster&apos;s Seventh New Collegiate
Dictionary,&amp;quot; G. &amp; C. Merriam Company, Springfield, MA.
Grolier&apos;s Inc. (1991) New Grolier&apos;s Electronic Encyclopedia.
Hanks, Patrick (ed.) (1979), Collins English Dictionary, Collins,
London and Glasgow.
Hearst, Marti (1991), &amp;quot;Noun Homograph Disambiguation Using Local
Context in Large Text Corpora,&amp;quot; Using Corpora, University of
Waterloo, Waterloo, Ontario.
Hirst, Graeme. (1987), Semantic Interpretation and the Resolution of
Ambiguity, Cambridge University Press, Cambridge.
Jorgensen, Julia (1990) &amp;quot;The Psychological Reality of Word Senses,&amp;quot;
Journal of Psycholinguistic Research, v. 19, pp 167-190.
Kaplan, Abraham (1950), &amp;quot;An Experimental Study of Ambiguity in
Context,&amp;quot; cited in Mechanical Translation, v. 1, nos. 1-3.
Kelly, Edward, and Phillip Stone (1975), Computer Recognition of
English Word Senses, North-Holland, Amsterdam.
Lesk, Michael (1986), &amp;quot;Automatic Sense Disambiguation: How to tell
a Pine Cone from an Ice Cream Cone,&amp;quot; Proceeding of the 1986
SIGDOC Conference, ACM, NY.
Masterson, Margaret (1967), &amp;quot;Mechanical Pidgin Translation,&amp;quot; in
Machine Translation, Donald Booth, ed., Wiley, 1967.
Mosteller, Fredrick, and David Wallace (1964) Inference and Disputed
Authorship: The Federalist, Addison-Wesley, Reading, Massachusetts.
Procter, P., R. &apos;bon, J. Ayto, et al. (1978), Longman Dictionary of
Contemporary English, Longman, Harlow and London.
Salton, G. (1989) Automatic Text Processing, Addison-Wesley.
Shipstone, E. (1960) &amp;quot;Some Variables Affecting Pattern Conception,&amp;quot;
Psychological Monographs, General and Applied, v. 74, pp. 1-41.
Sinclair, J., Hanks, P., Fox, G., Moon, R., Stock, P. et al. (eds.) (1987)
Collins Cobuild English Language Dictionary, Collins, London and
Glasgow.
Smadja, F. and K. McKeown (1990), &amp;quot;Automatically Extracting and
Representing Collocations for Language Generation,&amp;quot; ACL, pp. 252-
259.
Small, S. and C. Rieger (1982), &amp;quot;Parsing and Comprehending with
Word Experts (A Theory and its Realization),&amp;quot; in Strategies for
Natural Language Processing, W. Lehnert and M. Ringle, eds.,
Lawrence Erlbaum Associates, Hillsdale, NJ.
van Rijsbergen, C. (1979) Information Retrieval, Second Editional,
Butterworths, London.
Veronis, Jean and Nancy Ide (1990), &amp;quot;Word Sense Disambiguation
with Very Large Neural Networks Extracted from Machine Readable
Dictionaries,&amp;quot; in Proceedings COLING-90, pp 389-394.
Walker, Donald (1987), &amp;quot;Knowledge Resource Tools for Accessing
Large Text Files,&amp;quot; in Machine Translation: Theoretical and
Methodological Issues, Sergei Nirenberg, ed., Cambridge University
Press, Cambridge, England.
Weiss, Stephen (1973), &amp;quot;Learning to Disambiguate,&amp;quot; Information
Storage and Retrieval, v.9, pp 33-41.
Yarowsky, David (1992), &amp;quot;Word-Sense Disambiguation Using
Statistical Models of Roget&apos; s Categories Trained on Large. Corpora,&amp;quot;
Proceedings COLING-92.
Yngve, Victor (1955), &amp;quot;Syntax and the Problem of Multiple
Meaning,&amp;quot; in Machine Translation of Languages, William Locke and
Donald Booth, eds., Wiley, NY.
Zemik, Uri (1990) &amp;quot;Tagging Word Senses in Corpus: The Needle in
the Haystack Revisited,&amp;quot; in Text-Based Intelligent Systems: Current
Research in Text Analysis, Information Extraction, and Retrieval, P.S.
Jacobs, ed., GE Research &amp; Development Center, Schenectady, NY.
Zemilc, Uri (1991) &amp;quot;Trainl vs. Train2: Tagging Word Senses in
Corpus,&amp;quot; in Zemik (ed.) Lexical Acquisition: Exploiting On-Line
Resources to Build a Lexicon, Lawrence Erlbaum, Hillsdale, NJ.
</reference>
<page confidence="0.998421">
256
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.975551">
<title confidence="0.9983385">Estimating Upper and Lower Bounds on the Performance of Word-Sense Disambiguation Programs</title>
<author confidence="0.997564333333333">William Gale Kenneth Ward Church David Yarowsky</author>
<affiliation confidence="0.99988">AT&amp;T Bell Laboratories</affiliation>
<address confidence="0.999806">600 Mountain Ave. Murray Hill, NJ 07974</address>
<email confidence="0.999865">kwc@research.att.com</email>
<abstract confidence="0.999659146341463">We have recently reported on two new word-sense disambiguation systems, one trained on bilingual material (the Canadian Hansards) and the other trained on monolingual material (Roget&apos;s Thesaurus and Grolier&apos;s Encyclopedia). After using both the monolingual and bilingual classifiers for a few months, we have convinced ourselves that the performance is remarkably good. Nevertheless, we would really like to be able to make a stronger statement, and therefore, we decided to try to develop some more objective evaluation measures. Although there has been a fair amount of literature on sense-disambiguation, the literature does not offer much guidance in how we might establish the success or failure of a proposed solution such as the two systems mentioned in the previous paragraph. Many papers avoid quantitative evaluations altogether, because it is so difficult to come up with credible estimates of performance. This paper will attempt to establish upper and lower bounds on the level of performance that can be expected in an evaluation. An estimate of the lower bound of 75% (averaged over ambiguous types) is obtained by measuring the performance produced by a baseline system that ignores context and simply assigns the most likely sense in all cases. An estimate of the upper bound is obtained by assuming that our ability to measure performance is largely limited by our ability obtain reliable judgments from human informants. Not surprisingly, the upper bound is very dependent on the instructions given to the judges. Jorgensen, for example, suspected that lexicographers tend to depend too much on judgments by a single informant and found considerable variation over judgments (only 68% agreement), as she had suspected. In our own experiments, we have set out to find word-sense disambiguation tasks where the judges can agree often that we could show that they were outperforming the baseline system. Under quite different conditions, we have found 96.8% agreement over judges.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bar-Hillel</author>
</authors>
<title>Automatic Translation of Languages,&amp;quot;</title>
<date>1960</date>
<booktitle>in Advances in Computers, Donald Booth</booktitle>
<editor>and R. E. Meagher, eds.,</editor>
<publisher>Academic, NY.</publisher>
<contexts>
<context position="2429" citStr="Bar-Hillel (1960)" startWordPosition="363" endWordPosition="364">aphers tend to depend too much on judgments by a single informant and found considerable variation over judgments (only 68% agreement), as she had suspected. In our own experiments, we have set out to find word-sense disambiguation tasks where the judges can agree often enough so that we could show that they were outperforming the baseline system. Under quite different conditions, we have found 96.8% agreement over judges. 1. Introduction: Using Massive Lexicographic Resources Word-sense disambiguation is a long-standing problem in computational linguistics (e.g., Kaplan (1950), Yngve (1955), Bar-Hillel (1960), Masterson (1967)), with important implications for a number of practical applications including text-to-speech (TTS), machine translation (MT), information retrieval (IR), and many others. The recent interest in computational lexicography has fueled a large body of recent work on this 40-year-old problem, e.g., Black (1988), Brown et al. (1991), Choueka and Lusignan (1985), Clear (1989), Dagan et al. (1991), Gale et al. (to appear), Hearst (1991), Lesk (1986), Smadja and McKeown (1990), Walker (1987), Veronis and Ide (1990), Yarowsky (1992), Zernik (1990, 1991). Much of this work offers the </context>
<context position="4962" citStr="Bar-Hillel, 1960" startWordPosition="774" endWordPosition="775"> the translation machine?&apos; Understandable as this reaction is, it is very easy to show its futility. What such a suggestion amounts to, if taken seriously, is the requirement that a translation machine should not only be supplied with a dictionary but also with a universal encyclopedia. This is surely utterly chimerical and hardly deserves any further discussion. Since, however, the idea of a machine with encyclopedic knowledge has popped up also on other occasions, let me add a few words on this topic. The number of facts we human beings know is, in a certain very pregnant sense, infinite.&amp;quot; (Bar-Hillel, 1960) Ironically, much of the research cited above is taking exactly the approach that Bar-Hillel ridiculed as utterly chimerical and hardly deserving of any further discussion. Back in 1960, it may have been hard to imagine how it would be possible to supply a machine with both a dictionary and an encyclopedia. But much of the recent work cited above goes much further; not only does it supply a machine with a dictionary and an encyclopedia, but many other extensive references works as well, including Roget&apos;s Thesaurus and numerous large corpora. Of course, we are using these reference works in a v</context>
<context position="16445" citStr="Bar-Hillel (1960" startWordPosition="2656" endWordPosition="2657"> such results since there are many potentially important differences (e.g., corpora, judges, 5. As evidence that interest is relatively difficult, we note that both the Oxford Advanced Learner&apos;s Dictionary (OALD) (Crowie et al., 1989, P. 654) and COBUILD (Sinclair et al., 1987), for example, devote more than a full column to this word, indicating that it is an extremely complex word, at least by their standards. etc.), it appears that Zernik&apos;s 70% figure is fairly representative of the state of the art.6 Should we be happy with 70% performance? In fact, 70% really isn&apos;t very good. Recall that Bar-Hillel (1960, p. 159) abandoned the machine translation field when he couldn&apos;t see how a machine could possibly do a decent job in translating text if it couldn&apos;t do better than this in disambiguating word senses. Bar-Hillel&apos;s real objection was an empirical one. Using his numbers,7 it appears that programs, at the time, could disambiguate only about 75% of the words in a sentence (e.g., 15 out of 20). If interest is a relatively easy word, as Zemik (1990) suggests, then it would seem that Bar-Hillel&apos;s argument remains as true today as it was in 1960, and we ought to follow his lead and find something mor</context>
<context position="19222" citStr="Bar-Hillel, 1960" startWordPosition="3131" endWordPosition="3132"> a twenty word Russian sentence from a few tens of thousands (which is the approximate number resulting from the assumption that each of the twenty Russian words has two renderings on the average, while seven or eight of them have only one rendering) to some eighty (which would be the number of renderings on the assumption that sixteen words are uniquely rendered and four have three renderings apiece, forgetting now about all the other aspects such as change of word order, etc.) the main bulk of this kind of work has been achieved, the remainder requiring only some slight additional effort.&amp;quot; (Bar-Hillel, 1960, p. 163) Table 1: Comparison over Systems Word Yarowsky (1992) Previous Systems bow 91% &lt;67% (Clear, 1989) bass 99% 100% (Hearst, 1991) galley 99% 50-70% (Lesk, 1986) mole 99% N/A (Hirst, 1987) sentence 98% 90% (Gale et al.) slug 97% N/A (Hirst, 1987) star 96% N/A (Hirst, 1987) duty 96% 96% (Gale et al.) issue 94% &lt;70% (Zemik, 1990) taste 93% &lt;65% (Clear, 1989) cone 77% 50-70% (Lesk, 1986) interest 72% 72% (Black, 1988); 70% (Zernik, 1990) AVERAGE 92% N/A 252 outperform this baseline system, though not all such systems actually do. In fact, Yarowsky (1992) falls below the baseline for one of </context>
</contexts>
<marker>Bar-Hillel, 1960</marker>
<rawString>Bar-Hillel (1960), &amp;quot;Automatic Translation of Languages,&amp;quot; in Advances in Computers, Donald Booth and R. E. Meagher, eds., Academic, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ewa Black</author>
</authors>
<title>An Experiment in Computational Discrimination of English Word Senses,&amp;quot;</title>
<date>1988</date>
<journal>IBM Journal of Research and Development,</journal>
<volume>32</volume>
<pages>185--194</pages>
<contexts>
<context position="2756" citStr="Black (1988)" startWordPosition="407" endWordPosition="408"> baseline system. Under quite different conditions, we have found 96.8% agreement over judges. 1. Introduction: Using Massive Lexicographic Resources Word-sense disambiguation is a long-standing problem in computational linguistics (e.g., Kaplan (1950), Yngve (1955), Bar-Hillel (1960), Masterson (1967)), with important implications for a number of practical applications including text-to-speech (TTS), machine translation (MT), information retrieval (IR), and many others. The recent interest in computational lexicography has fueled a large body of recent work on this 40-year-old problem, e.g., Black (1988), Brown et al. (1991), Choueka and Lusignan (1985), Clear (1989), Dagan et al. (1991), Gale et al. (to appear), Hearst (1991), Lesk (1986), Smadja and McKeown (1990), Walker (1987), Veronis and Ide (1990), Yarowsky (1992), Zernik (1990, 1991). Much of this work offers the prospect that a disambiguation system might be able to input unrestricted text and tag each word with the most likely sense with fairly reasonable accuracy and efficiency, just as part of speech taggers (e.g., Church (1988)) can now input unrestricted text and assign each word with the most likely part of speech with fairly r</context>
<context position="12737" citStr="Black (1988)" startWordPosition="2043" endWordPosition="2044">or failure of a proposed solution such as the two described above. Most papers tend to avoid quantitative evaluations. Lesk (1986), an extremely innovative and commonly cited reference on the subject, provides a short discussion of evaluation, but fails to offer any very satisfying solutions that we might adopt to quantify the performance of our two disambiguation algorithms.3 Perhaps the most common evaluation technique is to select a small sample of words and compare the results of the machine with those of a human judge. This method has been used very effectively by Kelly and Stone (1975), Black (1988), Hearst (1991), and many others. Nevertheless, this technique is not without its problems, perhaps the worst of which is that the sample may not be very representative of the general vocabulary. Zernik (1990, p. 27), for example, reports 70% performance for the word interest, and then acknowledges that this level of performance may not generalize very well to other words.4 Although we agree with Zernik&apos;s prediction that interest is not very representative of other words, we suspect that interest is actually more difficult than most other words, not less difficult. Table 1 shows the performanc</context>
<context position="15726" citStr="Black (1988)" startWordPosition="2539" endWordPosition="2540"> literature on evaluation of word-sense disambiguation algorithms fails to offer a clear role model that we might follow in order to quantify the performance of our disambiguation algorithms. 4. What is the State-of-the-Art, and How Good Does It Need To Be? Moreover, there doesn&apos;t seem to be a very clear sense of what is possible. Is interest a relatively easy word or is it a relatively hard word? Zemik says it is relatively easy; we say it is relatively hard.5 Should we expect the next word to be easier than interest or harder than interest? One might ask if 70% is good or bad. In fact, both Black (1988) and Yarowsky (1992) report 72% performance on this very same word. Although it is dangerous to compare such results since there are many potentially important differences (e.g., corpora, judges, 5. As evidence that interest is relatively difficult, we note that both the Oxford Advanced Learner&apos;s Dictionary (OALD) (Crowie et al., 1989, P. 654) and COBUILD (Sinclair et al., 1987), for example, devote more than a full column to this word, indicating that it is an extremely complex word, at least by their standards. etc.), it appears that Zernik&apos;s 70% figure is fairly representative of the state </context>
<context position="19646" citStr="Black, 1988" startWordPosition="3205" endWordPosition="3206">l the other aspects such as change of word order, etc.) the main bulk of this kind of work has been achieved, the remainder requiring only some slight additional effort.&amp;quot; (Bar-Hillel, 1960, p. 163) Table 1: Comparison over Systems Word Yarowsky (1992) Previous Systems bow 91% &lt;67% (Clear, 1989) bass 99% 100% (Hearst, 1991) galley 99% 50-70% (Lesk, 1986) mole 99% N/A (Hirst, 1987) sentence 98% 90% (Gale et al.) slug 97% N/A (Hirst, 1987) star 96% N/A (Hirst, 1987) duty 96% 96% (Gale et al.) issue 94% &lt;70% (Zemik, 1990) taste 93% &lt;65% (Clear, 1989) cone 77% 50-70% (Lesk, 1986) interest 72% 72% (Black, 1988); 70% (Zernik, 1990) AVERAGE 92% N/A 252 outperform this baseline system, though not all such systems actually do. In fact, Yarowsky (1992) falls below the baseline for one of the twelve words (issue), although perhaps, we needn&apos;t be too concerned about this one deviation.8 There are, of course, a number of problems with this estimate of the baseline. First, the baseline system is not operational, at least as we have defined it. Ideally, the baseline system ought to try to estimate the most likely sense for each word in the vocabulary and then assign that sense to each instance of the word in </context>
</contexts>
<marker>Black, 1988</marker>
<rawString>Black, Ewa (1988), &amp;quot;An Experiment in Computational Discrimination of English Word Senses,&amp;quot; IBM Journal of Research and Development, v 32, pp 185-194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Brown</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>Robert Mercer</author>
</authors>
<title>Word Sense Disambiguation using Statistical Methods,&amp;quot; ACL,</title>
<date>1991</date>
<pages>264--270</pages>
<contexts>
<context position="2777" citStr="Brown et al. (1991)" startWordPosition="409" endWordPosition="412">em. Under quite different conditions, we have found 96.8% agreement over judges. 1. Introduction: Using Massive Lexicographic Resources Word-sense disambiguation is a long-standing problem in computational linguistics (e.g., Kaplan (1950), Yngve (1955), Bar-Hillel (1960), Masterson (1967)), with important implications for a number of practical applications including text-to-speech (TTS), machine translation (MT), information retrieval (IR), and many others. The recent interest in computational lexicography has fueled a large body of recent work on this 40-year-old problem, e.g., Black (1988), Brown et al. (1991), Choueka and Lusignan (1985), Clear (1989), Dagan et al. (1991), Gale et al. (to appear), Hearst (1991), Lesk (1986), Smadja and McKeown (1990), Walker (1987), Veronis and Ide (1990), Yarowsky (1992), Zernik (1990, 1991). Much of this work offers the prospect that a disambiguation system might be able to input unrestricted text and tag each word with the most likely sense with fairly reasonable accuracy and efficiency, just as part of speech taggers (e.g., Church (1988)) can now input unrestricted text and assign each word with the most likely part of speech with fairly reasonable accuracy an</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1991</marker>
<rawString>Brown, Peter, Stephen Della Pietra, Vincent Della Pietra, and Robert Mercer (1991), &amp;quot;Word Sense Disambiguation using Statistical Methods,&amp;quot; ACL, pp. 264-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Chapman</author>
</authors>
<title>Roget&apos;s International Thesaurus (Fourth Edition),</title>
<date>1977</date>
<location>Harper and Row, NY.</location>
<contexts>
<context position="10516" citStr="Chapman, 1977" startWordPosition="1674" endWordPosition="1675"> appear) for further details. At first, we thought that the method was completely dependent on the availability of parallel corpora for training. This has been a problem since parallel text remains somewhat difficult to obtain in large quantity, and what little is available is often fairly unbalanced and unrepresentative of general language. Moreover, the assumption that differences in translation correspond to differences in word-sense has always been somewhat suspect. Recently, Yarowsky (1992) has found a way to extend our use of the Bayesian techniques by training on the Roget&apos;s Thesaurus (Chapman, 1977)2 and Grolier&apos;s Encyclopedia (1991) instead of the Canadian Hansards, thus circumventing many of the objections to our use of the Hansards. Yarowsky (1992) inputs a 100-word context surrounding a polysemous word and scores each of the 1042 Roget Categories by: fi Pr(w1 Roget Category) w in context The program can also be run in a mode where it takes unrestricted text as input and tags each word with its most likely Roget Category. Some results for the word crane are presented below, showing that the program can be used to sort a concordance by sense. Input Output Treadmills attached to cranes </context>
</contexts>
<marker>Chapman, 1977</marker>
<rawString>Chapman, Robert (1977). Roget&apos;s International Thesaurus (Fourth Edition), Harper and Row, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaacov Chouelca</author>
<author>Serge Lusignan</author>
</authors>
<title>Disambiguation by Short Contexts,&amp;quot;</title>
<date>1985</date>
<journal>Computers and the Humanities, v</journal>
<volume>19</volume>
<pages>147--158</pages>
<marker>Chouelca, Lusignan, 1985</marker>
<rawString>Chouelca, Yaacov, and Serge Lusignan (1985), &amp;quot;Disambiguation by Short Contexts,&amp;quot; Computers and the Humanities, v 19. pp. 147-158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
</authors>
<title>A Stochastic Parts Program an Noun Phrase Parser for Unrestricted Text,&amp;quot;</title>
<date>1988</date>
<booktitle>Applied ACL Conference,</booktitle>
<location>Austin, Texas.</location>
<contexts>
<context position="3252" citStr="Church (1988)" startWordPosition="487" endWordPosition="488"> in computational lexicography has fueled a large body of recent work on this 40-year-old problem, e.g., Black (1988), Brown et al. (1991), Choueka and Lusignan (1985), Clear (1989), Dagan et al. (1991), Gale et al. (to appear), Hearst (1991), Lesk (1986), Smadja and McKeown (1990), Walker (1987), Veronis and Ide (1990), Yarowsky (1992), Zernik (1990, 1991). Much of this work offers the prospect that a disambiguation system might be able to input unrestricted text and tag each word with the most likely sense with fairly reasonable accuracy and efficiency, just as part of speech taggers (e.g., Church (1988)) can now input unrestricted text and assign each word with the most likely part of speech with fairly reasonable accuracy and efficiency. The availability of massive lexicographic databases offers a promising route to overcoming the knowledge acquisition bottleneck. More than thirty years ago, BarHillel (1960) predicted that it would be &amp;quot;futile&amp;quot; to write expert-system-like rules by-hand (as they had been doing at Georgetown at the time) because there would be no way to scale up such rules to cope with unrestricted input. Indeed, it is now well-known that expert-systemlike rules can be notorio</context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Church, Kenneth (1988), &amp;quot;A Stochastic Parts Program an Noun Phrase Parser for Unrestricted Text,&amp;quot; Applied ACL Conference, Austin, Texas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeremy Clear</author>
</authors>
<title>An Experiment in Automatic Word Sense Identification,&amp;quot; Internal Document,</title>
<date>1989</date>
<publisher>University Press,</publisher>
<location>Oxford</location>
<contexts>
<context position="2820" citStr="Clear (1989)" startWordPosition="417" endWordPosition="418">d 96.8% agreement over judges. 1. Introduction: Using Massive Lexicographic Resources Word-sense disambiguation is a long-standing problem in computational linguistics (e.g., Kaplan (1950), Yngve (1955), Bar-Hillel (1960), Masterson (1967)), with important implications for a number of practical applications including text-to-speech (TTS), machine translation (MT), information retrieval (IR), and many others. The recent interest in computational lexicography has fueled a large body of recent work on this 40-year-old problem, e.g., Black (1988), Brown et al. (1991), Choueka and Lusignan (1985), Clear (1989), Dagan et al. (1991), Gale et al. (to appear), Hearst (1991), Lesk (1986), Smadja and McKeown (1990), Walker (1987), Veronis and Ide (1990), Yarowsky (1992), Zernik (1990, 1991). Much of this work offers the prospect that a disambiguation system might be able to input unrestricted text and tag each word with the most likely sense with fairly reasonable accuracy and efficiency, just as part of speech taggers (e.g., Church (1988)) can now input unrestricted text and assign each word with the most likely part of speech with fairly reasonable accuracy and efficiency. The availability of massive l</context>
<context position="19329" citStr="Clear, 1989" startWordPosition="3148" endWordPosition="3149">e assumption that each of the twenty Russian words has two renderings on the average, while seven or eight of them have only one rendering) to some eighty (which would be the number of renderings on the assumption that sixteen words are uniquely rendered and four have three renderings apiece, forgetting now about all the other aspects such as change of word order, etc.) the main bulk of this kind of work has been achieved, the remainder requiring only some slight additional effort.&amp;quot; (Bar-Hillel, 1960, p. 163) Table 1: Comparison over Systems Word Yarowsky (1992) Previous Systems bow 91% &lt;67% (Clear, 1989) bass 99% 100% (Hearst, 1991) galley 99% 50-70% (Lesk, 1986) mole 99% N/A (Hirst, 1987) sentence 98% 90% (Gale et al.) slug 97% N/A (Hirst, 1987) star 96% N/A (Hirst, 1987) duty 96% 96% (Gale et al.) issue 94% &lt;70% (Zemik, 1990) taste 93% &lt;65% (Clear, 1989) cone 77% 50-70% (Lesk, 1986) interest 72% 72% (Black, 1988); 70% (Zernik, 1990) AVERAGE 92% N/A 252 outperform this baseline system, though not all such systems actually do. In fact, Yarowsky (1992) falls below the baseline for one of the twelve words (issue), although perhaps, we needn&apos;t be too concerned about this one deviation.8 There ar</context>
</contexts>
<marker>Clear, 1989</marker>
<rawString>Clear, Jeremy (1989). &amp;quot;An Experiment in Automatic Word Sense Identification,&amp;quot; Internal Document, Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="true">
<title>Oxford Advanced Learner&apos;s Dictionary,&amp;quot; Fourth Edition,</title>
<date>1989</date>
<editor>Crowie, Anthony et al. (eds.)</editor>
<publisher>University Press.</publisher>
<location>Oxford</location>
<contexts>
<context position="2820" citStr="(1989)" startWordPosition="418" endWordPosition="418">% agreement over judges. 1. Introduction: Using Massive Lexicographic Resources Word-sense disambiguation is a long-standing problem in computational linguistics (e.g., Kaplan (1950), Yngve (1955), Bar-Hillel (1960), Masterson (1967)), with important implications for a number of practical applications including text-to-speech (TTS), machine translation (MT), information retrieval (IR), and many others. The recent interest in computational lexicography has fueled a large body of recent work on this 40-year-old problem, e.g., Black (1988), Brown et al. (1991), Choueka and Lusignan (1985), Clear (1989), Dagan et al. (1991), Gale et al. (to appear), Hearst (1991), Lesk (1986), Smadja and McKeown (1990), Walker (1987), Veronis and Ide (1990), Yarowsky (1992), Zernik (1990, 1991). Much of this work offers the prospect that a disambiguation system might be able to input unrestricted text and tag each word with the most likely sense with fairly reasonable accuracy and efficiency, just as part of speech taggers (e.g., Church (1988)) can now input unrestricted text and assign each word with the most likely part of speech with fairly reasonable accuracy and efficiency. The availability of massive l</context>
</contexts>
<marker>1989</marker>
<rawString>Crowie, Anthony et al. (eds.) (1989), &amp;quot;Oxford Advanced Learner&apos;s Dictionary,&amp;quot; Fourth Edition, Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Alon Rai</author>
<author>Ulrike Schwall</author>
</authors>
<title>Two Languages are more Informative than One,&amp;quot; ACL,</title>
<date>1991</date>
<pages>130--137</pages>
<contexts>
<context position="2841" citStr="Dagan et al. (1991)" startWordPosition="419" endWordPosition="422">ent over judges. 1. Introduction: Using Massive Lexicographic Resources Word-sense disambiguation is a long-standing problem in computational linguistics (e.g., Kaplan (1950), Yngve (1955), Bar-Hillel (1960), Masterson (1967)), with important implications for a number of practical applications including text-to-speech (TTS), machine translation (MT), information retrieval (IR), and many others. The recent interest in computational lexicography has fueled a large body of recent work on this 40-year-old problem, e.g., Black (1988), Brown et al. (1991), Choueka and Lusignan (1985), Clear (1989), Dagan et al. (1991), Gale et al. (to appear), Hearst (1991), Lesk (1986), Smadja and McKeown (1990), Walker (1987), Veronis and Ide (1990), Yarowsky (1992), Zernik (1990, 1991). Much of this work offers the prospect that a disambiguation system might be able to input unrestricted text and tag each word with the most likely sense with fairly reasonable accuracy and efficiency, just as part of speech taggers (e.g., Church (1988)) can now input unrestricted text and assign each word with the most likely part of speech with fairly reasonable accuracy and efficiency. The availability of massive lexicographic database</context>
</contexts>
<marker>Dagan, Rai, Schwall, 1991</marker>
<rawString>Dagan, Ido, Alon Rai, and Ulrike Schwall (1991), &amp;quot;Two Languages are more Informative than One,&amp;quot; ACL, pp. 130-137.</rawString>
</citation>
<citation valid="false">
<authors>
<author>William Gale</author>
<author>Kenneth Church</author>
<author>David</author>
</authors>
<title>Yarowsky (to appear) &amp;quot;A Method for Disambiguating Word Senses in a Large Corpus,&amp;quot; Computers and Humanities.</title>
<marker>Gale, Church, David, </marker>
<rawString>Gale, William, Kenneth Church, and David Yarowsky (to appear) &amp;quot;A Method for Disambiguating Word Senses in a Large Corpus,&amp;quot; Computers and Humanities.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Gale</author>
<author>Kenneth Church</author>
<author>David Yarowsky</author>
</authors>
<title>One Sense Per Discourse,&amp;quot; Darpa Speech and Natural Language Workshop.</title>
<date>1992</date>
<contexts>
<context position="27895" citStr="Gale et al., 1992" startWordPosition="4541" endWordPosition="4544">t easier for the judges. If the task is too hard (as Jorgensen&apos;s classification task may be), then there will be almost no room between the limits of the measurement and the baseline. In other words, there won&apos;t be enough dynamic range to measure differences between better systems and worse systems. In contrast, if we focus on easier tasks, then we might have enough dynamic range to show some interesting differences. Therefore, unlike Jorgensen who was interested in highlighting differences among judgments, we are much more interested in highlighting agreements. Fortunately, we have found in (Gale et al., 1992) that the agreement rate can be very high (96.8%), which is well above the baseline, under very different experimental conditions. Of course, it is a fairly major step to redefine the problem from a classification task to a discrimination one, as we are proposing. One might have preferred not to do so, but we simply don&apos;t know how one could establish enough dynamic range in that case to show any interesting differences. It has been our experience that it is very hard to design an experiment of any kind which will produce the desired agreement among judges. We are very happy with the 96.8% agre</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>Gale, William, Kenneth Church, and David Yarowsky (1992) &amp;quot;One Sense Per Discourse,&amp;quot; Darpa Speech and Natural Language Workshop.</rawString>
</citation>
<citation valid="true">
<title>Webster&apos;s Seventh New Collegiate Dictionary,&amp;quot;</title>
<date>1975</date>
<editor>Gove, Philip et at. (eds.)</editor>
<contexts>
<context position="12723" citStr="(1975)" startWordPosition="2042" endWordPosition="2042">success or failure of a proposed solution such as the two described above. Most papers tend to avoid quantitative evaluations. Lesk (1986), an extremely innovative and commonly cited reference on the subject, provides a short discussion of evaluation, but fails to offer any very satisfying solutions that we might adopt to quantify the performance of our two disambiguation algorithms.3 Perhaps the most common evaluation technique is to select a small sample of words and compare the results of the machine with those of a human judge. This method has been used very effectively by Kelly and Stone (1975), Black (1988), Hearst (1991), and many others. Nevertheless, this technique is not without its problems, perhaps the worst of which is that the sample may not be very representative of the general vocabulary. Zernik (1990, p. 27), for example, reports 70% performance for the word interest, and then acknowledges that this level of performance may not generalize very well to other words.4 Although we agree with Zernik&apos;s prediction that interest is not very representative of other words, we suspect that interest is actually more difficult than most other words, not less difficult. Table 1 shows </context>
</contexts>
<marker>1975</marker>
<rawString>Gove, Philip et at. (eds.) (1975) &amp;quot;Webster&apos;s Seventh New Collegiate Dictionary,&amp;quot; G. &amp; C. Merriam Company, Springfield, MA. Grolier&apos;s Inc. (1991) New Grolier&apos;s Electronic Encyclopedia.</rawString>
</citation>
<citation valid="true">
<date>1979</date>
<booktitle>Collins English Dictionary,</booktitle>
<editor>Hanks, Patrick (ed.)</editor>
<location>Collins, London and Glasgow.</location>
<marker>1979</marker>
<rawString>Hanks, Patrick (ed.) (1979), Collins English Dictionary, Collins, London and Glasgow.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>Noun Homograph Disambiguation Using Local Context in Large Text Corpora,&amp;quot; Using Corpora,</title>
<date>1991</date>
<institution>University of Waterloo,</institution>
<location>Waterloo, Ontario.</location>
<contexts>
<context position="2881" citStr="Hearst (1991)" startWordPosition="428" endWordPosition="429">e Lexicographic Resources Word-sense disambiguation is a long-standing problem in computational linguistics (e.g., Kaplan (1950), Yngve (1955), Bar-Hillel (1960), Masterson (1967)), with important implications for a number of practical applications including text-to-speech (TTS), machine translation (MT), information retrieval (IR), and many others. The recent interest in computational lexicography has fueled a large body of recent work on this 40-year-old problem, e.g., Black (1988), Brown et al. (1991), Choueka and Lusignan (1985), Clear (1989), Dagan et al. (1991), Gale et al. (to appear), Hearst (1991), Lesk (1986), Smadja and McKeown (1990), Walker (1987), Veronis and Ide (1990), Yarowsky (1992), Zernik (1990, 1991). Much of this work offers the prospect that a disambiguation system might be able to input unrestricted text and tag each word with the most likely sense with fairly reasonable accuracy and efficiency, just as part of speech taggers (e.g., Church (1988)) can now input unrestricted text and assign each word with the most likely part of speech with fairly reasonable accuracy and efficiency. The availability of massive lexicographic databases offers a promising route to overcoming</context>
<context position="12752" citStr="Hearst (1991)" startWordPosition="2045" endWordPosition="2046">a proposed solution such as the two described above. Most papers tend to avoid quantitative evaluations. Lesk (1986), an extremely innovative and commonly cited reference on the subject, provides a short discussion of evaluation, but fails to offer any very satisfying solutions that we might adopt to quantify the performance of our two disambiguation algorithms.3 Perhaps the most common evaluation technique is to select a small sample of words and compare the results of the machine with those of a human judge. This method has been used very effectively by Kelly and Stone (1975), Black (1988), Hearst (1991), and many others. Nevertheless, this technique is not without its problems, perhaps the worst of which is that the sample may not be very representative of the general vocabulary. Zernik (1990, p. 27), for example, reports 70% performance for the word interest, and then acknowledges that this level of performance may not generalize very well to other words.4 Although we agree with Zernik&apos;s prediction that interest is not very representative of other words, we suspect that interest is actually more difficult than most other words, not less difficult. Table 1 shows the performance of Yarowsky (</context>
<context position="19358" citStr="Hearst, 1991" startWordPosition="3153" endWordPosition="3154">e twenty Russian words has two renderings on the average, while seven or eight of them have only one rendering) to some eighty (which would be the number of renderings on the assumption that sixteen words are uniquely rendered and four have three renderings apiece, forgetting now about all the other aspects such as change of word order, etc.) the main bulk of this kind of work has been achieved, the remainder requiring only some slight additional effort.&amp;quot; (Bar-Hillel, 1960, p. 163) Table 1: Comparison over Systems Word Yarowsky (1992) Previous Systems bow 91% &lt;67% (Clear, 1989) bass 99% 100% (Hearst, 1991) galley 99% 50-70% (Lesk, 1986) mole 99% N/A (Hirst, 1987) sentence 98% 90% (Gale et al.) slug 97% N/A (Hirst, 1987) star 96% N/A (Hirst, 1987) duty 96% 96% (Gale et al.) issue 94% &lt;70% (Zemik, 1990) taste 93% &lt;65% (Clear, 1989) cone 77% 50-70% (Lesk, 1986) interest 72% 72% (Black, 1988); 70% (Zernik, 1990) AVERAGE 92% N/A 252 outperform this baseline system, though not all such systems actually do. In fact, Yarowsky (1992) falls below the baseline for one of the twelve words (issue), although perhaps, we needn&apos;t be too concerned about this one deviation.8 There are, of course, a number of pro</context>
</contexts>
<marker>Hearst, 1991</marker>
<rawString>Hearst, Marti (1991), &amp;quot;Noun Homograph Disambiguation Using Local Context in Large Text Corpora,&amp;quot; Using Corpora, University of Waterloo, Waterloo, Ontario.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
</authors>
<title>Semantic Interpretation and the Resolution of Ambiguity,</title>
<date>1987</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="19416" citStr="Hirst, 1987" startWordPosition="3163" endWordPosition="3164">hile seven or eight of them have only one rendering) to some eighty (which would be the number of renderings on the assumption that sixteen words are uniquely rendered and four have three renderings apiece, forgetting now about all the other aspects such as change of word order, etc.) the main bulk of this kind of work has been achieved, the remainder requiring only some slight additional effort.&amp;quot; (Bar-Hillel, 1960, p. 163) Table 1: Comparison over Systems Word Yarowsky (1992) Previous Systems bow 91% &lt;67% (Clear, 1989) bass 99% 100% (Hearst, 1991) galley 99% 50-70% (Lesk, 1986) mole 99% N/A (Hirst, 1987) sentence 98% 90% (Gale et al.) slug 97% N/A (Hirst, 1987) star 96% N/A (Hirst, 1987) duty 96% 96% (Gale et al.) issue 94% &lt;70% (Zemik, 1990) taste 93% &lt;65% (Clear, 1989) cone 77% 50-70% (Lesk, 1986) interest 72% 72% (Black, 1988); 70% (Zernik, 1990) AVERAGE 92% N/A 252 outperform this baseline system, though not all such systems actually do. In fact, Yarowsky (1992) falls below the baseline for one of the twelve words (issue), although perhaps, we needn&apos;t be too concerned about this one deviation.8 There are, of course, a number of problems with this estimate of the baseline. First, the basel</context>
</contexts>
<marker>Hirst, 1987</marker>
<rawString>Hirst, Graeme. (1987), Semantic Interpretation and the Resolution of Ambiguity, Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Jorgensen</author>
</authors>
<title>The Psychological Reality of Word Senses,&amp;quot;</title>
<date>1990</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>19</volume>
<pages>167--190</pages>
<contexts>
<context position="25021" citStr="Jorgensen (1990)" startWordPosition="4087" endWordPosition="4088"> Baseline Word Baseline Yarowsky (1992) issue 96% 94% duty 87% 96% galley 83% 99% star 83% 96% taste 74% 93% bass 70% 99% slug 62% 97% sentence 62% 98% interest 60% 72% mole 59% 99% cone 51% 77% bow 48% 91% AVERAGE 70% 92% 253 performance falls to 81% averaged over tokens and 75% averaged over types. 5.2 Upper Bounds We will attempt to estimate an upper bound on performance by estimating the ability for human judges to agree with one another (or themselves). We will find, not surprisingly, that the estimate varies widely depending on a number of factors, especially the definition of the task. Jorgensen (1990) has collected some interesting data that may be relevant for estimating the agreement among judges. As part of her dissertation under George Miller at Princeton, she was interested in assessing &amp;quot;the extent of psychologically real polysemy in the mental lexicon for nouns.&amp;quot; Her experiment was designed to study one of the more commonly employed methods in lexicography for writing dictionary definitions, namely the use of citation indexes. She was concerned that lexicographers and computational linguists have tended to depend too much on the intuitions of a single informant. Not surprisingly, she</context>
</contexts>
<marker>Jorgensen, 1990</marker>
<rawString>Jorgensen, Julia (1990) &amp;quot;The Psychological Reality of Word Senses,&amp;quot; Journal of Psycholinguistic Research, v. 19, pp 167-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Kaplan</author>
</authors>
<title>An Experimental Study of Ambiguity in Context,&amp;quot; cited</title>
<date>1950</date>
<booktitle>in Mechanical Translation,</booktitle>
<volume>1</volume>
<pages>1--3</pages>
<contexts>
<context position="2396" citStr="Kaplan (1950)" startWordPosition="359" endWordPosition="360">mple, suspected that lexicographers tend to depend too much on judgments by a single informant and found considerable variation over judgments (only 68% agreement), as she had suspected. In our own experiments, we have set out to find word-sense disambiguation tasks where the judges can agree often enough so that we could show that they were outperforming the baseline system. Under quite different conditions, we have found 96.8% agreement over judges. 1. Introduction: Using Massive Lexicographic Resources Word-sense disambiguation is a long-standing problem in computational linguistics (e.g., Kaplan (1950), Yngve (1955), Bar-Hillel (1960), Masterson (1967)), with important implications for a number of practical applications including text-to-speech (TTS), machine translation (MT), information retrieval (IR), and many others. The recent interest in computational lexicography has fueled a large body of recent work on this 40-year-old problem, e.g., Black (1988), Brown et al. (1991), Choueka and Lusignan (1985), Clear (1989), Dagan et al. (1991), Gale et al. (to appear), Hearst (1991), Lesk (1986), Smadja and McKeown (1990), Walker (1987), Veronis and Ide (1990), Yarowsky (1992), Zernik (1990, 199</context>
</contexts>
<marker>Kaplan, 1950</marker>
<rawString>Kaplan, Abraham (1950), &amp;quot;An Experimental Study of Ambiguity in Context,&amp;quot; cited in Mechanical Translation, v. 1, nos. 1-3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Kelly</author>
<author>Phillip Stone</author>
</authors>
<date>1975</date>
<journal>Computer Recognition of English Word Senses,</journal>
<location>North-Holland, Amsterdam.</location>
<contexts>
<context position="12723" citStr="Kelly and Stone (1975)" startWordPosition="2039" endWordPosition="2042">t establish the success or failure of a proposed solution such as the two described above. Most papers tend to avoid quantitative evaluations. Lesk (1986), an extremely innovative and commonly cited reference on the subject, provides a short discussion of evaluation, but fails to offer any very satisfying solutions that we might adopt to quantify the performance of our two disambiguation algorithms.3 Perhaps the most common evaluation technique is to select a small sample of words and compare the results of the machine with those of a human judge. This method has been used very effectively by Kelly and Stone (1975), Black (1988), Hearst (1991), and many others. Nevertheless, this technique is not without its problems, perhaps the worst of which is that the sample may not be very representative of the general vocabulary. Zernik (1990, p. 27), for example, reports 70% performance for the word interest, and then acknowledges that this level of performance may not generalize very well to other words.4 Although we agree with Zernik&apos;s prediction that interest is not very representative of other words, we suspect that interest is actually more difficult than most other words, not less difficult. Table 1 shows </context>
</contexts>
<marker>Kelly, Stone, 1975</marker>
<rawString>Kelly, Edward, and Phillip Stone (1975), Computer Recognition of English Word Senses, North-Holland, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lesk</author>
</authors>
<title>Automatic Sense Disambiguation: How</title>
<date>1986</date>
<note>to tell</note>
<contexts>
<context position="2894" citStr="Lesk (1986)" startWordPosition="430" endWordPosition="431"> Resources Word-sense disambiguation is a long-standing problem in computational linguistics (e.g., Kaplan (1950), Yngve (1955), Bar-Hillel (1960), Masterson (1967)), with important implications for a number of practical applications including text-to-speech (TTS), machine translation (MT), information retrieval (IR), and many others. The recent interest in computational lexicography has fueled a large body of recent work on this 40-year-old problem, e.g., Black (1988), Brown et al. (1991), Choueka and Lusignan (1985), Clear (1989), Dagan et al. (1991), Gale et al. (to appear), Hearst (1991), Lesk (1986), Smadja and McKeown (1990), Walker (1987), Veronis and Ide (1990), Yarowsky (1992), Zernik (1990, 1991). Much of this work offers the prospect that a disambiguation system might be able to input unrestricted text and tag each word with the most likely sense with fairly reasonable accuracy and efficiency, just as part of speech taggers (e.g., Church (1988)) can now input unrestricted text and assign each word with the most likely part of speech with fairly reasonable accuracy and efficiency. The availability of massive lexicographic databases offers a promising route to overcoming the knowledg</context>
<context position="12255" citStr="Lesk (1986)" startWordPosition="1965" endWordPosition="1966"> like to be able to make a stronger statement, and therefore, we decided to try to develop some more objective evaluation measures. 2. Note that this edition of the Roget&apos;s Thesaurus is much more extensive than the 1911 version, though somewhat more difficult to obtain in electronic form. 3. The Literature on Evaluation Although there has been a fair amount of literature on sense-disambiguation, the literature does not offer much guidance in how we might establish the success or failure of a proposed solution such as the two described above. Most papers tend to avoid quantitative evaluations. Lesk (1986), an extremely innovative and commonly cited reference on the subject, provides a short discussion of evaluation, but fails to offer any very satisfying solutions that we might adopt to quantify the performance of our two disambiguation algorithms.3 Perhaps the most common evaluation technique is to select a small sample of words and compare the results of the machine with those of a human judge. This method has been used very effectively by Kelly and Stone (1975), Black (1988), Hearst (1991), and many others. Nevertheless, this technique is not without its problems, perhaps the worst of which</context>
<context position="14336" citStr="Lesk, 1986" startWordPosition="2307" endWordPosition="2308">iterature in order to make comparisons over systems. If the study had been intended to support predictions on new words, then the study should have used a random sample of such words, rather than a sample of words from the literature. 3. &amp;quot;What is the current performance of this program? Some very brief experimentation with my program has yielded accuracies of 50-70% on short samples of Pride and Prejudice and an Associated Press news story. Considerably more work is needed both to improve the program and to do more thorough evaluation... There is too much subjectivity in these measurements.&amp;quot; (Lesk, 1986, p. 6) 4. &amp;quot;For all 4 senses of INTEREST, both recall and precision are over 70%... However, not for all words are the obtained results that positive... The fact is that almost any English word possesses multiple senses. (Zenilk, 1990, p. 27) 251 In addition to the sampling questions, one feels uncomfortable about comparing results across experiments, since there are many potentially important differences including different corpora, different words, different judges, differences in treatment of precision and recall, and differences in the use of tools such as parsers and part of speech tagger</context>
<context position="19389" citStr="Lesk, 1986" startWordPosition="3158" endWordPosition="3159">nderings on the average, while seven or eight of them have only one rendering) to some eighty (which would be the number of renderings on the assumption that sixteen words are uniquely rendered and four have three renderings apiece, forgetting now about all the other aspects such as change of word order, etc.) the main bulk of this kind of work has been achieved, the remainder requiring only some slight additional effort.&amp;quot; (Bar-Hillel, 1960, p. 163) Table 1: Comparison over Systems Word Yarowsky (1992) Previous Systems bow 91% &lt;67% (Clear, 1989) bass 99% 100% (Hearst, 1991) galley 99% 50-70% (Lesk, 1986) mole 99% N/A (Hirst, 1987) sentence 98% 90% (Gale et al.) slug 97% N/A (Hirst, 1987) star 96% N/A (Hirst, 1987) duty 96% 96% (Gale et al.) issue 94% &lt;70% (Zemik, 1990) taste 93% &lt;65% (Clear, 1989) cone 77% 50-70% (Lesk, 1986) interest 72% 72% (Black, 1988); 70% (Zernik, 1990) AVERAGE 92% N/A 252 outperform this baseline system, though not all such systems actually do. In fact, Yarowsky (1992) falls below the baseline for one of the twelve words (issue), although perhaps, we needn&apos;t be too concerned about this one deviation.8 There are, of course, a number of problems with this estimate of the</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Lesk, Michael (1986), &amp;quot;Automatic Sense Disambiguation: How to tell</rawString>
</citation>
<citation valid="false">
<authors>
<author>a Pine</author>
</authors>
<title>Cone from an Ice Cream Cone,&amp;quot;</title>
<booktitle>Proceeding of the 1986 SIGDOC Conference, ACM,</booktitle>
<location>NY.</location>
<marker>Pine, </marker>
<rawString>a Pine Cone from an Ice Cream Cone,&amp;quot; Proceeding of the 1986 SIGDOC Conference, ACM, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Masterson</author>
</authors>
<title>Mechanical Pidgin Translation,&amp;quot;</title>
<date>1967</date>
<editor>in Machine Translation, Donald Booth, ed.,</editor>
<publisher>Wiley,</publisher>
<contexts>
<context position="2447" citStr="Masterson (1967)" startWordPosition="365" endWordPosition="366">nd too much on judgments by a single informant and found considerable variation over judgments (only 68% agreement), as she had suspected. In our own experiments, we have set out to find word-sense disambiguation tasks where the judges can agree often enough so that we could show that they were outperforming the baseline system. Under quite different conditions, we have found 96.8% agreement over judges. 1. Introduction: Using Massive Lexicographic Resources Word-sense disambiguation is a long-standing problem in computational linguistics (e.g., Kaplan (1950), Yngve (1955), Bar-Hillel (1960), Masterson (1967)), with important implications for a number of practical applications including text-to-speech (TTS), machine translation (MT), information retrieval (IR), and many others. The recent interest in computational lexicography has fueled a large body of recent work on this 40-year-old problem, e.g., Black (1988), Brown et al. (1991), Choueka and Lusignan (1985), Clear (1989), Dagan et al. (1991), Gale et al. (to appear), Hearst (1991), Lesk (1986), Smadja and McKeown (1990), Walker (1987), Veronis and Ide (1990), Yarowsky (1992), Zernik (1990, 1991). Much of this work offers the prospect that a di</context>
</contexts>
<marker>Masterson, 1967</marker>
<rawString>Masterson, Margaret (1967), &amp;quot;Mechanical Pidgin Translation,&amp;quot; in Machine Translation, Donald Booth, ed., Wiley, 1967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fredrick Mosteller</author>
<author>David Wallace</author>
</authors>
<title>Inference and Disputed Authorship: The Federalist,</title>
<date>1964</date>
<publisher>Addison-Wesley,</publisher>
<location>Reading, Massachusetts.</location>
<contexts>
<context position="6568" citStr="Mosteller and Wallace, 1964" startWordPosition="1028" endWordPosition="1031"> and very large datasets of tens of millions of words of parallel English and French text (e.g., the Canadian Hansards). By aligning the text as we have, we were able to collect a large set of examples of polysemous words (e.g., sentence) in each sense (e.g., judicial sentence vs. syntactic sentence), by extracting instances from the corpus that were translated one way or the other (e.g, peine or phrase). These data sets were then analyzed using well-understood Bayesian discrimination methods, which have been used very successfully in many other applications, especially author identification (Mosteller and Wallace, 1964, section 3.1) and information retrieval (1R) (van Rijsbergen, 1979, chapter 6; Salton, 1989, section 10.3), though their application to word-sense disambiguation is novel. In author identification and information retrieval, it is customary to split the discrimination process up into a testing phase and a training phase. During the training phase, we are given two (or more) sets of documents and are asked to construct a discriminator which can distinguish between the two (or more) classes of documents. These discriminators are then applied to new documents during the testing phase. In the auth</context>
</contexts>
<marker>Mosteller, Wallace, 1964</marker>
<rawString>Mosteller, Fredrick, and David Wallace (1964) Inference and Disputed Authorship: The Federalist, Addison-Wesley, Reading, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Procter</author>
<author>R &apos;bon</author>
<author>J Ayto</author>
</authors>
<date>1978</date>
<institution>Longman Dictionary of Contemporary English,</institution>
<location>Longman, Harlow and London.</location>
<marker>Procter, &apos;bon, Ayto, 1978</marker>
<rawString>Procter, P., R. &apos;bon, J. Ayto, et al. (1978), Longman Dictionary of Contemporary English, Longman, Harlow and London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
</authors>
<title>Automatic Text Processing,</title>
<date>1989</date>
<journal>Psychological Monographs, General and Applied,</journal>
<volume>74</volume>
<pages>1--41</pages>
<publisher>Addison-Wesley. Shipstone, E.</publisher>
<contexts>
<context position="6660" citStr="Salton, 1989" startWordPosition="1043" endWordPosition="1044">an Hansards). By aligning the text as we have, we were able to collect a large set of examples of polysemous words (e.g., sentence) in each sense (e.g., judicial sentence vs. syntactic sentence), by extracting instances from the corpus that were translated one way or the other (e.g, peine or phrase). These data sets were then analyzed using well-understood Bayesian discrimination methods, which have been used very successfully in many other applications, especially author identification (Mosteller and Wallace, 1964, section 3.1) and information retrieval (1R) (van Rijsbergen, 1979, chapter 6; Salton, 1989, section 10.3), though their application to word-sense disambiguation is novel. In author identification and information retrieval, it is customary to split the discrimination process up into a testing phase and a training phase. During the training phase, we are given two (or more) sets of documents and are asked to construct a discriminator which can distinguish between the two (or more) classes of documents. These discriminators are then applied to new documents during the testing phase. In the author identification task, for example, the training set consists of several documents written </context>
</contexts>
<marker>Salton, 1989</marker>
<rawString>Salton, G. (1989) Automatic Text Processing, Addison-Wesley. Shipstone, E. (1960) &amp;quot;Some Variables Affecting Pattern Conception,&amp;quot; Psychological Monographs, General and Applied, v. 74, pp. 1-41.</rawString>
</citation>
<citation valid="true">
<title>Collins Cobuild English Language Dictionary, Collins, London and Glasgow.</title>
<date>1987</date>
<editor>Sinclair, J., Hanks, P., Fox, G., Moon, R., Stock, P. et al. (eds.)</editor>
<contexts>
<context position="2936" citStr="(1987)" startWordPosition="437" endWordPosition="437">standing problem in computational linguistics (e.g., Kaplan (1950), Yngve (1955), Bar-Hillel (1960), Masterson (1967)), with important implications for a number of practical applications including text-to-speech (TTS), machine translation (MT), information retrieval (IR), and many others. The recent interest in computational lexicography has fueled a large body of recent work on this 40-year-old problem, e.g., Black (1988), Brown et al. (1991), Choueka and Lusignan (1985), Clear (1989), Dagan et al. (1991), Gale et al. (to appear), Hearst (1991), Lesk (1986), Smadja and McKeown (1990), Walker (1987), Veronis and Ide (1990), Yarowsky (1992), Zernik (1990, 1991). Much of this work offers the prospect that a disambiguation system might be able to input unrestricted text and tag each word with the most likely sense with fairly reasonable accuracy and efficiency, just as part of speech taggers (e.g., Church (1988)) can now input unrestricted text and assign each word with the most likely part of speech with fairly reasonable accuracy and efficiency. The availability of massive lexicographic databases offers a promising route to overcoming the knowledge acquisition bottleneck. More than thirty</context>
</contexts>
<marker>1987</marker>
<rawString>Sinclair, J., Hanks, P., Fox, G., Moon, R., Stock, P. et al. (eds.) (1987) Collins Cobuild English Language Dictionary, Collins, London and Glasgow.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Smadja</author>
<author>K McKeown</author>
</authors>
<title>Automatically Extracting and Representing Collocations for Language Generation,&amp;quot; ACL,</title>
<date>1990</date>
<pages>252--259</pages>
<contexts>
<context position="2921" citStr="Smadja and McKeown (1990)" startWordPosition="432" endWordPosition="435">rd-sense disambiguation is a long-standing problem in computational linguistics (e.g., Kaplan (1950), Yngve (1955), Bar-Hillel (1960), Masterson (1967)), with important implications for a number of practical applications including text-to-speech (TTS), machine translation (MT), information retrieval (IR), and many others. The recent interest in computational lexicography has fueled a large body of recent work on this 40-year-old problem, e.g., Black (1988), Brown et al. (1991), Choueka and Lusignan (1985), Clear (1989), Dagan et al. (1991), Gale et al. (to appear), Hearst (1991), Lesk (1986), Smadja and McKeown (1990), Walker (1987), Veronis and Ide (1990), Yarowsky (1992), Zernik (1990, 1991). Much of this work offers the prospect that a disambiguation system might be able to input unrestricted text and tag each word with the most likely sense with fairly reasonable accuracy and efficiency, just as part of speech taggers (e.g., Church (1988)) can now input unrestricted text and assign each word with the most likely part of speech with fairly reasonable accuracy and efficiency. The availability of massive lexicographic databases offers a promising route to overcoming the knowledge acquisition bottleneck. M</context>
</contexts>
<marker>Smadja, McKeown, 1990</marker>
<rawString>Smadja, F. and K. McKeown (1990), &amp;quot;Automatically Extracting and Representing Collocations for Language Generation,&amp;quot; ACL, pp. 252-259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Small</author>
<author>C Rieger</author>
</authors>
<title>Parsing and Comprehending with Word Experts (A Theory and its Realization),&amp;quot;</title>
<date>1982</date>
<booktitle>in Strategies for Natural Language Processing,</booktitle>
<editor>W. Lehnert and M. Ringle, eds., Lawrence Erlbaum Associates,</editor>
<location>Hillsdale, NJ.</location>
<marker>Small, Rieger, 1982</marker>
<rawString>Small, S. and C. Rieger (1982), &amp;quot;Parsing and Comprehending with Word Experts (A Theory and its Realization),&amp;quot; in Strategies for Natural Language Processing, W. Lehnert and M. Ringle, eds., Lawrence Erlbaum Associates, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C van Rijsbergen</author>
</authors>
<title>Information Retrieval,</title>
<date>1979</date>
<location>Second Editional, Butterworths, London.</location>
<marker>van Rijsbergen, 1979</marker>
<rawString>van Rijsbergen, C. (1979) Information Retrieval, Second Editional, Butterworths, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Veronis</author>
<author>Nancy Ide</author>
</authors>
<title>Word Sense Disambiguation with Very Large Neural Networks Extracted from Machine Readable Dictionaries,&amp;quot;</title>
<date>1990</date>
<booktitle>in Proceedings COLING-90,</booktitle>
<pages>389--394</pages>
<contexts>
<context position="2960" citStr="Veronis and Ide (1990)" startWordPosition="438" endWordPosition="441"> problem in computational linguistics (e.g., Kaplan (1950), Yngve (1955), Bar-Hillel (1960), Masterson (1967)), with important implications for a number of practical applications including text-to-speech (TTS), machine translation (MT), information retrieval (IR), and many others. The recent interest in computational lexicography has fueled a large body of recent work on this 40-year-old problem, e.g., Black (1988), Brown et al. (1991), Choueka and Lusignan (1985), Clear (1989), Dagan et al. (1991), Gale et al. (to appear), Hearst (1991), Lesk (1986), Smadja and McKeown (1990), Walker (1987), Veronis and Ide (1990), Yarowsky (1992), Zernik (1990, 1991). Much of this work offers the prospect that a disambiguation system might be able to input unrestricted text and tag each word with the most likely sense with fairly reasonable accuracy and efficiency, just as part of speech taggers (e.g., Church (1988)) can now input unrestricted text and assign each word with the most likely part of speech with fairly reasonable accuracy and efficiency. The availability of massive lexicographic databases offers a promising route to overcoming the knowledge acquisition bottleneck. More than thirty years ago, BarHillel (1</context>
</contexts>
<marker>Veronis, Ide, 1990</marker>
<rawString>Veronis, Jean and Nancy Ide (1990), &amp;quot;Word Sense Disambiguation with Very Large Neural Networks Extracted from Machine Readable Dictionaries,&amp;quot; in Proceedings COLING-90, pp 389-394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Walker</author>
</authors>
<title>Knowledge Resource Tools for Accessing Large Text Files,&amp;quot;</title>
<date>1987</date>
<booktitle>in Machine Translation: Theoretical and Methodological Issues,</booktitle>
<editor>Sergei Nirenberg, ed.,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England.</location>
<contexts>
<context position="2936" citStr="Walker (1987)" startWordPosition="436" endWordPosition="437">a long-standing problem in computational linguistics (e.g., Kaplan (1950), Yngve (1955), Bar-Hillel (1960), Masterson (1967)), with important implications for a number of practical applications including text-to-speech (TTS), machine translation (MT), information retrieval (IR), and many others. The recent interest in computational lexicography has fueled a large body of recent work on this 40-year-old problem, e.g., Black (1988), Brown et al. (1991), Choueka and Lusignan (1985), Clear (1989), Dagan et al. (1991), Gale et al. (to appear), Hearst (1991), Lesk (1986), Smadja and McKeown (1990), Walker (1987), Veronis and Ide (1990), Yarowsky (1992), Zernik (1990, 1991). Much of this work offers the prospect that a disambiguation system might be able to input unrestricted text and tag each word with the most likely sense with fairly reasonable accuracy and efficiency, just as part of speech taggers (e.g., Church (1988)) can now input unrestricted text and assign each word with the most likely part of speech with fairly reasonable accuracy and efficiency. The availability of massive lexicographic databases offers a promising route to overcoming the knowledge acquisition bottleneck. More than thirty</context>
</contexts>
<marker>Walker, 1987</marker>
<rawString>Walker, Donald (1987), &amp;quot;Knowledge Resource Tools for Accessing Large Text Files,&amp;quot; in Machine Translation: Theoretical and Methodological Issues, Sergei Nirenberg, ed., Cambridge University Press, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Weiss</author>
</authors>
<title>Learning to Disambiguate,&amp;quot;</title>
<date>1973</date>
<journal>Information Storage and Retrieval,</journal>
<volume>9</volume>
<pages>33--41</pages>
<marker>Weiss, 1973</marker>
<rawString>Weiss, Stephen (1973), &amp;quot;Learning to Disambiguate,&amp;quot; Information Storage and Retrieval, v.9, pp 33-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Word-Sense Disambiguation Using Statistical Models of Roget&apos; s Categories Trained on Large. Corpora,&amp;quot;</title>
<date>1992</date>
<booktitle>Proceedings COLING-92.</booktitle>
<contexts>
<context position="2977" citStr="Yarowsky (1992)" startWordPosition="442" endWordPosition="443">l linguistics (e.g., Kaplan (1950), Yngve (1955), Bar-Hillel (1960), Masterson (1967)), with important implications for a number of practical applications including text-to-speech (TTS), machine translation (MT), information retrieval (IR), and many others. The recent interest in computational lexicography has fueled a large body of recent work on this 40-year-old problem, e.g., Black (1988), Brown et al. (1991), Choueka and Lusignan (1985), Clear (1989), Dagan et al. (1991), Gale et al. (to appear), Hearst (1991), Lesk (1986), Smadja and McKeown (1990), Walker (1987), Veronis and Ide (1990), Yarowsky (1992), Zernik (1990, 1991). Much of this work offers the prospect that a disambiguation system might be able to input unrestricted text and tag each word with the most likely sense with fairly reasonable accuracy and efficiency, just as part of speech taggers (e.g., Church (1988)) can now input unrestricted text and assign each word with the most likely part of speech with fairly reasonable accuracy and efficiency. The availability of massive lexicographic databases offers a promising route to overcoming the knowledge acquisition bottleneck. More than thirty years ago, BarHillel (1960) predicted th</context>
<context position="10402" citStr="Yarowsky (1992)" startWordPosition="1654" endWordPosition="1655">emous word (and then smoothing these estimates in order to deal with the sparse-data problems). See Gale et al. (to appear) for further details. At first, we thought that the method was completely dependent on the availability of parallel corpora for training. This has been a problem since parallel text remains somewhat difficult to obtain in large quantity, and what little is available is often fairly unbalanced and unrepresentative of general language. Moreover, the assumption that differences in translation correspond to differences in word-sense has always been somewhat suspect. Recently, Yarowsky (1992) has found a way to extend our use of the Bayesian techniques by training on the Roget&apos;s Thesaurus (Chapman, 1977)2 and Grolier&apos;s Encyclopedia (1991) instead of the Canadian Hansards, thus circumventing many of the objections to our use of the Hansards. Yarowsky (1992) inputs a 100-word context surrounding a polysemous word and scores each of the 1042 Roget Categories by: fi Pr(w1 Roget Category) w in context The program can also be run in a mode where it takes unrestricted text as input and tags each word with its most likely Roget Category. Some results for the word crane are presented below</context>
<context position="13357" citStr="Yarowsky (1992)" startWordPosition="2141" endWordPosition="2142">rst (1991), and many others. Nevertheless, this technique is not without its problems, perhaps the worst of which is that the sample may not be very representative of the general vocabulary. Zernik (1990, p. 27), for example, reports 70% performance for the word interest, and then acknowledges that this level of performance may not generalize very well to other words.4 Although we agree with Zernik&apos;s prediction that interest is not very representative of other words, we suspect that interest is actually more difficult than most other words, not less difficult. Table 1 shows the performance of Yarowsky (1992) on twelve words which have been previously discussed in the literature. Note that interest is at the bottom of the list. The reader should exercise some caution in interpreting the numbers in Table 1. It is natural to try to use these numbers to predict performance on new words, but the study was not designed for that purpose. The test words were selected from the literature in order to make comparisons over systems. If the study had been intended to support predictions on new words, then the study should have used a random sample of such words, rather than a sample of words from the literatu</context>
<context position="15746" citStr="Yarowsky (1992)" startWordPosition="2542" endWordPosition="2543">aluation of word-sense disambiguation algorithms fails to offer a clear role model that we might follow in order to quantify the performance of our disambiguation algorithms. 4. What is the State-of-the-Art, and How Good Does It Need To Be? Moreover, there doesn&apos;t seem to be a very clear sense of what is possible. Is interest a relatively easy word or is it a relatively hard word? Zemik says it is relatively easy; we say it is relatively hard.5 Should we expect the next word to be easier than interest or harder than interest? One might ask if 70% is good or bad. In fact, both Black (1988) and Yarowsky (1992) report 72% performance on this very same word. Although it is dangerous to compare such results since there are many potentially important differences (e.g., corpora, judges, 5. As evidence that interest is relatively difficult, we note that both the Oxford Advanced Learner&apos;s Dictionary (OALD) (Crowie et al., 1989, P. 654) and COBUILD (Sinclair et al., 1987), for example, devote more than a full column to this word, indicating that it is an extremely complex word, at least by their standards. etc.), it appears that Zernik&apos;s 70% figure is fairly representative of the state of the art.6 Should </context>
<context position="19285" citStr="Yarowsky (1992)" startWordPosition="3141" endWordPosition="3142">ich is the approximate number resulting from the assumption that each of the twenty Russian words has two renderings on the average, while seven or eight of them have only one rendering) to some eighty (which would be the number of renderings on the assumption that sixteen words are uniquely rendered and four have three renderings apiece, forgetting now about all the other aspects such as change of word order, etc.) the main bulk of this kind of work has been achieved, the remainder requiring only some slight additional effort.&amp;quot; (Bar-Hillel, 1960, p. 163) Table 1: Comparison over Systems Word Yarowsky (1992) Previous Systems bow 91% &lt;67% (Clear, 1989) bass 99% 100% (Hearst, 1991) galley 99% 50-70% (Lesk, 1986) mole 99% N/A (Hirst, 1987) sentence 98% 90% (Gale et al.) slug 97% N/A (Hirst, 1987) star 96% N/A (Hirst, 1987) duty 96% 96% (Gale et al.) issue 94% &lt;70% (Zemik, 1990) taste 93% &lt;65% (Clear, 1989) cone 77% 50-70% (Lesk, 1986) interest 72% 72% (Black, 1988); 70% (Zernik, 1990) AVERAGE 92% N/A 252 outperform this baseline system, though not all such systems actually do. In fact, Yarowsky (1992) falls below the baseline for one of the twelve words (issue), although perhaps, we needn&apos;t be too c</context>
<context position="21824" citStr="Yarowsky (1992)" startWordPosition="3579" endWordPosition="3580"> fraction of which are actually the case of interest. One can come up with quite a number of other scenarios where the baseline performance could be somewhat misleading, especially when there is an unusual trade-off between the cost of a Type I error and the cost of a Type II error. Nevertheless, the proposed baseline does seem to provide a usable rough estimate of the lower bound on performance. Table 2 shows the baseline performance for each of the twelve words in Table 1. Note that performance is generally above the baseline as we would 8. Many of the systems mentioned in Table 2 including Yarowsky (1992) do not currently take advantage of the prior probabilities of the senses, so they would be at a disadvantage relative to the baseline if one of the senses had a very high prior, as is the case for the test word issue. 9. In addition, the baseline doesn&apos;t deal as well as it could with skewed distributions. One could almost certainly improve the model of the baseline by making use of a notion like entropy that could deal more effectively with skewed distributions. Nevertheless, we will stick with our simpler notion of the baseline for expository convenience. hope. As mentioned previously, the t</context>
<context position="24444" citStr="Yarowsky (1992)" startWordPosition="3984" endWordPosition="3985">clock, coke, colon, commander, consort, contract, cruise, cultivation, delegate, designation, dialogue, disaster, equation, esophagus, fact, fear;, fertility, flesh, fox, gold, interface, interruption, intrigue, journey, knife, label, landscape, laurel, lb, liberty, lily, locomotion, lynx, marine, memorial, menstruation, miracle, monasticism mountain, nitrate, orthodoxy, pest, planning, possibility, pottery, projector, regiment, relaxation, reunification, shore, sodium, specialty, stretch, surnmer, testing, tungsten, universe, variant, vigor, wire, worship. Table 2: The Baseline Word Baseline Yarowsky (1992) issue 96% 94% duty 87% 96% galley 83% 99% star 83% 96% taste 74% 93% bass 70% 99% slug 62% 97% sentence 62% 98% interest 60% 72% mole 59% 99% cone 51% 77% bow 48% 91% AVERAGE 70% 92% 253 performance falls to 81% averaged over tokens and 75% averaged over types. 5.2 Upper Bounds We will attempt to estimate an upper bound on performance by estimating the ability for human judges to agree with one another (or themselves). We will find, not surprisingly, that the estimate varies widely depending on a number of factors, especially the definition of the task. Jorgensen (1990) has collected some int</context>
</contexts>
<marker>Yarowsky, 1992</marker>
<rawString>Yarowsky, David (1992), &amp;quot;Word-Sense Disambiguation Using Statistical Models of Roget&apos; s Categories Trained on Large. Corpora,&amp;quot; Proceedings COLING-92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Yngve</author>
</authors>
<title>Syntax and the Problem of Multiple Meaning,&amp;quot;</title>
<date>1955</date>
<booktitle>in Machine Translation of Languages, William Locke and</booktitle>
<editor>Donald Booth, eds.,</editor>
<publisher>Wiley, NY.</publisher>
<contexts>
<context position="2410" citStr="Yngve (1955)" startWordPosition="361" endWordPosition="362"> that lexicographers tend to depend too much on judgments by a single informant and found considerable variation over judgments (only 68% agreement), as she had suspected. In our own experiments, we have set out to find word-sense disambiguation tasks where the judges can agree often enough so that we could show that they were outperforming the baseline system. Under quite different conditions, we have found 96.8% agreement over judges. 1. Introduction: Using Massive Lexicographic Resources Word-sense disambiguation is a long-standing problem in computational linguistics (e.g., Kaplan (1950), Yngve (1955), Bar-Hillel (1960), Masterson (1967)), with important implications for a number of practical applications including text-to-speech (TTS), machine translation (MT), information retrieval (IR), and many others. The recent interest in computational lexicography has fueled a large body of recent work on this 40-year-old problem, e.g., Black (1988), Brown et al. (1991), Choueka and Lusignan (1985), Clear (1989), Dagan et al. (1991), Gale et al. (to appear), Hearst (1991), Lesk (1986), Smadja and McKeown (1990), Walker (1987), Veronis and Ide (1990), Yarowsky (1992), Zernik (1990, 1991). Much of th</context>
</contexts>
<marker>Yngve, 1955</marker>
<rawString>Yngve, Victor (1955), &amp;quot;Syntax and the Problem of Multiple Meaning,&amp;quot; in Machine Translation of Languages, William Locke and Donald Booth, eds., Wiley, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uri Zemik</author>
</authors>
<title>Tagging Word Senses in Corpus: The Needle in the Haystack Revisited,&amp;quot;</title>
<date>1990</date>
<booktitle>in Text-Based Intelligent Systems: Current Research in Text Analysis, Information Extraction,</booktitle>
<editor>and Retrieval, P.S. Jacobs, ed., GE Research &amp; Development Center,</editor>
<location>Schenectady, NY.</location>
<contexts>
<context position="16893" citStr="Zemik (1990)" startWordPosition="2733" endWordPosition="2734">figure is fairly representative of the state of the art.6 Should we be happy with 70% performance? In fact, 70% really isn&apos;t very good. Recall that Bar-Hillel (1960, p. 159) abandoned the machine translation field when he couldn&apos;t see how a machine could possibly do a decent job in translating text if it couldn&apos;t do better than this in disambiguating word senses. Bar-Hillel&apos;s real objection was an empirical one. Using his numbers,7 it appears that programs, at the time, could disambiguate only about 75% of the words in a sentence (e.g., 15 out of 20). If interest is a relatively easy word, as Zemik (1990) suggests, then it would seem that Bar-Hillel&apos;s argument remains as true today as it was in 1960, and we ought to follow his lead and find something more productive to do with our time. On the other hand, if we are correct and interest is a relatively difficult word, then it is possible that we have made some progress over the past thirty years... 5. Upper and Lower Bounds 5.1 Lower Bounds We could be in a better position to address the question of the relative difficulty of interest if we could establish a rough estimate of the upper and lower bounds on the level of performance that can be ex</context>
<context position="19557" citStr="Zemik, 1990" startWordPosition="3190" endWordPosition="3191">ords are uniquely rendered and four have three renderings apiece, forgetting now about all the other aspects such as change of word order, etc.) the main bulk of this kind of work has been achieved, the remainder requiring only some slight additional effort.&amp;quot; (Bar-Hillel, 1960, p. 163) Table 1: Comparison over Systems Word Yarowsky (1992) Previous Systems bow 91% &lt;67% (Clear, 1989) bass 99% 100% (Hearst, 1991) galley 99% 50-70% (Lesk, 1986) mole 99% N/A (Hirst, 1987) sentence 98% 90% (Gale et al.) slug 97% N/A (Hirst, 1987) star 96% N/A (Hirst, 1987) duty 96% 96% (Gale et al.) issue 94% &lt;70% (Zemik, 1990) taste 93% &lt;65% (Clear, 1989) cone 77% 50-70% (Lesk, 1986) interest 72% 72% (Black, 1988); 70% (Zernik, 1990) AVERAGE 92% N/A 252 outperform this baseline system, though not all such systems actually do. In fact, Yarowsky (1992) falls below the baseline for one of the twelve words (issue), although perhaps, we needn&apos;t be too concerned about this one deviation.8 There are, of course, a number of problems with this estimate of the baseline. First, the baseline system is not operational, at least as we have defined it. Ideally, the baseline system ought to try to estimate the most likely sense fo</context>
</contexts>
<marker>Zemik, 1990</marker>
<rawString>Zemik, Uri (1990) &amp;quot;Tagging Word Senses in Corpus: The Needle in the Haystack Revisited,&amp;quot; in Text-Based Intelligent Systems: Current Research in Text Analysis, Information Extraction, and Retrieval, P.S. Jacobs, ed., GE Research &amp; Development Center, Schenectady, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uri Zemilc</author>
</authors>
<title>Trainl vs. Train2: Tagging Word Senses in Corpus,&amp;quot; in Zemik (ed.) Lexical Acquisition: Exploiting On-Line Resources to Build a Lexicon, Lawrence Erlbaum,</title>
<date>1991</date>
<location>Hillsdale, NJ.</location>
<marker>Zemilc, 1991</marker>
<rawString>Zemilc, Uri (1991) &amp;quot;Trainl vs. Train2: Tagging Word Senses in Corpus,&amp;quot; in Zemik (ed.) Lexical Acquisition: Exploiting On-Line Resources to Build a Lexicon, Lawrence Erlbaum, Hillsdale, NJ.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>