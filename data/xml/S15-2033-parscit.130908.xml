<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003014">
<title confidence="0.844519">
ASAP-II: From the Alignment of Phrases to Text Similarity
</title>
<author confidence="0.8085605">
Ana O. Alves&apos; ,2
David Sim˜oes&apos;
</author>
<affiliation confidence="0.8504905">
&apos;Polytechnic Institute of Coimbra
Portugal
</affiliation>
<email confidence="0.9630795">
aalves@isec.pt
a21210644@alunos.isec.pt
</email>
<title confidence="0.401051">
Hugo Gonc¸alo Oliveira2
</title>
<author confidence="0.82189">
Adriana Ferrugento2
</author>
<affiliation confidence="0.8420795">
2CISUC, University of Coimbra
Portugal
</affiliation>
<email confidence="0.9821525">
hroliv@dei.uc.pt
aferr@student.dei.uc.pt
</email>
<sectionHeader confidence="0.993556" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998812666666667">
ThisThis work is licensed under a Creative
Commons Attribution 4.0 International Li-
cence. Page numbers and proceedings footer
are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
paper describes the second version of the
ASAP system1 and its participation in the
SemEval-2015, task 2a on Semantic Textual
Similarity (STS). Our approach is based on
computing the WordNet semantic relatedness
and similarity of phrases from distinct sen-
tences. We also apply topic modeling to get
topic distributions over a set of sentences as
well as some linguistic heuristics. In a special
addition for this task, we retrieve named
entities and compound nouns from DBPedia.
All these features are used to feed a regression
algorithm that learns the STS function.
</bodyText>
<sectionHeader confidence="0.998981" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999006461538462">
Semantic Textual Similarity (STS), which is the task
of computing the similarity between two sentences,
has received an increasing amount of attention in re-
cent years (Agirre et al., 2012; Agirre et al., 2013;
Marelli et al., 2014a; Agirre et al., 2014; Agirre et
al., 2015). Our contribution to this challenge is to
learn the STS function for English texts. ASAP-II
is an evolution of the ASAP system (Alves et al.,
2014), which participated in SemEval 2014 - Task 1:
Evaluation of compositional distributional semantic
models on full sentences through semantic related-
ness and textual entailment. Although with a differ-
ent goal from STS, which goes beyond relatedness
</bodyText>
<footnote confidence="0.925021">
1This work was supported by the InfoCrowds project - FCT-
PTDC/ECM-TRA/1898/2012
</footnote>
<bodyText confidence="0.999873875">
and entailment, and different datasets, which include
pairs of short texts instead of controlled sentences,
we believe that, rather than specifying rules, con-
straints and lexicons manually, it is possible to adapt
a system from one to the other task, by automat-
ically acquiring linguistic knowledge through ma-
chine learning (ML) methods. For this purpose, we
apply some pre-processing techniques to the train-
ing set in order to extract different types of features.
On the semantic aspect, we compute the similar-
ity/relatedness between phrases using known mea-
sures over WordNet (Miller, 1995).
Considering the problem of modeling a text cor-
pus to find short descriptions of documents, we aim
at an efficient processing of large collections, while
preserving the essential statistical relationships that
are useful for similarity judgment. Therefore, we
also apply topic modeling, in order to get topic dis-
tribution over each sentence set. These features are
then used to feed an ensemble ML algorithm for
learning the STS function. Our system is entirely
developed as a Java independent software package,
publicly available2 for training and testing on given
and new datasets containing pairs of texts.
The remainder of this paper comprises 4 sections.
In section 2, fundamental concepts are introduced
in order to understand the proposed approach delin-
eated in section 3. Section 4 presents some results
of our approach, using not only the SemEval-2015’s
dataset, but also datasets from previous tasks. Fi-
nally, section 5 presents some conclusions and com-
plementary work to be done in a near future.
</bodyText>
<footnote confidence="0.99902">
2See https://github.com/examinus-/ASAP
</footnote>
<page confidence="0.934069">
184
</page>
<note confidence="0.619454">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 184–189,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.988621" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.999068">
2.1 Knowledge Bases
</subsectionHeader>
<bodyText confidence="0.9991849375">
WordNet (Miller, 1995) is a lexical knowledge base
structured in synsets – groups of synonymous words
that may be seen as possible lexicalizations of a con-
cept – and relations between them, including hyper-
nymy or part-of. DBpedia (Auer et al., 2007) is
an effort for extracting structured information from
Wikipedia, a well-known collaborative encyclope-
dia. DBPedia is a central part of the Linked Data
initiative and consequently, it is linked to many
other resources, including a RDF version of Word-
Net. In fact, some DBPedia entities are connected
to their abstract category in WordNet, through the
wordnet type property. For instance, CNN is con-
nected to the synset {channel, transmission chan-
nel} and Berlusconi to {chancellor, premier, prime
minister}.
</bodyText>
<subsectionHeader confidence="0.999621">
2.2 Semantic Similarity
</subsectionHeader>
<bodyText confidence="0.999717916666667">
There are two main approaches to semantic sim-
ilarity: (i) semantic relatedness is based on co-
occurrence statistics, typically over a large corpus;
(ii) classic semantic similarity exploits semantic re-
lations in a lexical knowledge base, such as Word-
Net. Semantic similarity differs from semantic re-
latedness because it computes proximity between
concepts in a given concept hierarchy (see (Resnik,
1995) and (Jiang and Conrath, 1997)), while the for-
mer computes the usage of common concepts to-
gether (see (Lesk, 1986), in this case on dictionary
definitions/glosses).
</bodyText>
<subsectionHeader confidence="0.998134">
2.3 Topic Modeling
</subsectionHeader>
<bodyText confidence="0.999962545454545">
Topic modeling relies on the assumption that doc-
uments are mixtures of topics, which, in turn, are
probability distributions over words. Latent Dirich-
let Allocation (LDA) is a generative probabilistic
topic model (Blei et al., 2003) where documents are
represented as random mixtures over latent topics,
characterized by a distribution over words. Assump-
tions are not made on the word order, only their fre-
quency is relevant. In LDA, main variables are the
topic-word distribution Φ and topic distributions θ
for each document.
</bodyText>
<sectionHeader confidence="0.965744" genericHeader="method">
3 Proposed Approach
</sectionHeader>
<bodyText confidence="0.999839285714286">
Our approach to STS is based on a regression func-
tion, learned automatically to compute the similarity
between sentences, using their components as fea-
tures. Sentence features are obtained after a pre-
processing stage, where sentences are lexically, syn-
tactically and semantically decomposed to obtain
different partial similarities. Clustering is applied
by LDA in order to obtain the difference of topic
distribution between pairs of sentences, which can
be considered a composed partial similarity on each
topic distribution. Partial similarities are used as fea-
tures in the supervised learning process. In the fol-
lowing section, complementary stages of our system
are explained in detail.
</bodyText>
<subsectionHeader confidence="0.998312">
3.1 Natural Language Preprocessing
</subsectionHeader>
<bodyText confidence="0.999981727272727">
Sentences are decomposed after applying well-
known Natural Language Processing subtasks,
namely tokenization, part-of-speech tagging and
chunking. For this purpose, we use OpenNLP3,
a tool for processing natural language text out-of-
the-box, based on a maximum entropy (ME) ap-
proach (Berger et al., 1996). Although OpenNLP
offers an English stemmer, this is not sufficient for
our approach. Instead, we rely on the lemmatization
performed by the WS4J library4, with some addi-
tional heuristics (see section 3.2.3).
</bodyText>
<subsectionHeader confidence="0.99952">
3.2 Feature Engineering
</subsectionHeader>
<bodyText confidence="0.9997785">
Features encode information from raw data that al-
lows machine learning algorithms to estimate an un-
known value. We focus on, what we call, light fea-
tures since they are computed automatically, not re-
quiring a specific labeled dataset and we are using
already trained models. Each feature is computed
as a partial similarity metric, which will later feed
the posterior regression analysis. This process is
fully automatized, as all features are extracted us-
ing OpenNLP and other tools that will be introduced
later. For convenience, we set an id for each feature,
which has the form f#n, n E {1..}.
</bodyText>
<footnote confidence="0.9912845">
3Seehttp://opennlp.sourceforge.net
4A thread-safe, self-contained, Java implementation of
some of useful functions over WordNet.See https://code.
google.com/p/ws4j/
</footnote>
<page confidence="0.995626">
185
</page>
<subsectionHeader confidence="0.899303">
3.2.1 Lexical Features
</subsectionHeader>
<bodyText confidence="0.9999893">
Some basic similarity metrics are used as fea-
tures related exclusively with word forms. In this
set, we include for each text: the number of stop
words, from the Snowball list (Porter, 2001) (f1 and
f2 respectively) and the absolute difference of those
counts (f3 = |f1− f2|); the number of those words
expressing negation (f4 and f5 respectively) and the
absolute difference of those counts (f6 = |f4−f5|).
In addition, we used the absolute difference of over-
lapping words for each text pair (f7..10)5.
</bodyText>
<subsectionHeader confidence="0.82171">
3.2.2 Syntactic Features
</subsectionHeader>
<bodyText confidence="0.99996575">
The Max Entropy models of OpenNLP were used
for tokenization, part-of-speech tagging and text
chunking, applied in a pipeline for identifying Noun
Phrases (NPs), Verbal Phrases (VPs) and Preposi-
tional Phrases (PPs) of each sentence. Heuristically,
these NPs are further identified as subjects if they are
in the beginning of sentences. This kind of shallow
parser is useful for identifying the syntactic structure
of texts. Considering only this property, different
features were computed as the absolute value of the
difference of the number of NPs (f11), VPs (f12)
and PPs(f13) for each text pair.
</bodyText>
<subsectionHeader confidence="0.838232">
3.2.3 Semantic Features
</subsectionHeader>
<bodyText confidence="0.999966125">
When possible, suitable WordNet synsets are re-
trieved for NPs, VPs and PPs of each sentence.
These will enable the computation of similarity mea-
sures to be used as semantic features. These phrases
might be simple words or compounds, language
words or named entities, and they might be inflected
(e.g. nouns as electrics or economic electric cars are
in the plural form). In order to increase the cover-
age of named entities, when a word is not in Word-
Net, we look it up in DBPedia to identify WordNet
synset corresponding to its category. Inflected words
can also be problematic because WordNet synsets
are retrieved by the lemma of their words. Al-
though some WordNet APIs already perform some
kind of lemmatization, many situations are not cov-
ered. Therefore, to increase the number of words
</bodyText>
<footnote confidence="0.7662605">
5We thank the SemEval 2014 - Task 1 organizers for
providing a Python script for computing baselines available
at http://alt.qcri.org/semeval2014/task1/
data/uploads/sick_baseline.zip, which we used
as a different setting for stop word removal (from 0 to 3, 4
different combinations)
</footnote>
<bodyText confidence="0.999847666666667">
with a suitable synset, the leftmost word of a com-
pound phrase, generally a modifier, is removed until
the phrase is empty or a synset is retrieved. If still
unsuccessful and the last word ends with an ‘s’, the
last character is removed and the word is looked up
again.
After retrieving a WordNet sense for each phrase,
semantic similarity is computed for each pair, using
Resnik (1995) (f14), Jiang &amp; Conrath (1997) (f15)
and the Adapted Lesk metrics (Banerjee and Peder-
sen, 2003) (f16) using WS4j tool, where algorithms
in the WordNet::Similarity (Pedersen et al., 2004)
Perl package are implemented. For part-of-speech
tagged words with multiple senses, the one maxi-
mizing partial similarity is selected.
</bodyText>
<subsectionHeader confidence="0.992305">
3.3 Distributional Features
</subsectionHeader>
<bodyText confidence="0.999981041666667">
The distribution of topics over documents (in our
case, short texts) may contribute to model Seman-
tic Similarity since there is no notion of mutual ex-
clusivity that restricts words to be part of one topic
only. This allows topic models to capture polysemy.
We may thus see topics as natural word sense con-
texts, as words occur in different topics with distinct
“senses”.
Gensim (ˇReh˚uˇrek and Sojka, 2010) is a ma-
chine learning framework for topic modeling. It
includes several pre-processing techniques, such as
stop-word removal and TF-IDF, a standard statisti-
cal method that combines the frequency of a term in
a particular document with its inverse document fre-
quency in general use (Salton and Buckley, 1988).
This score is high for rare terms that occur fre-
quently in a document and are therefore more likely
to be significant.
Gensim computes a distribution of 25 topics over
texts with or without using TF-IDF (f17...41). Each
feature is the absolute difference of topici (i.e.
topic[i] = |topic[i]s1 − topic[i]s2|). The euclidean
distance over the difference of topic distribution be-
tween text pairs was used as another feature (f42).
</bodyText>
<subsectionHeader confidence="0.987099">
3.4 Supervised Learning
</subsectionHeader>
<bodyText confidence="0.99988575">
WEKA (Hall et al., 2009) is a large collection of
machine learning algorithms, written in Java, used
for learning our STS function from aforementioned
features.
</bodyText>
<page confidence="0.997714">
186
</page>
<bodyText confidence="0.999925621621621">
One of four approaches is commonly adopted for
building classifier ensembles, each focused on a dif-
ferent level of action. Approach A concerns the dif-
ferent ways of combining the results from the clas-
sifiers. Approach B uses different models.At feature
level (Approach C), different feature subsets can be
used for the classifiers, either if they use the same
classification model or not. Finally, datasets can be
modified so that each classifier in the ensemble is
trained on its own dataset (Approach D) (Kuncheva
and Whitaker, 2003).
Different methods where applied such as Voting
(Franke and Mandler, 1992) (Approach A), Stacking
(Seewald, 2002) (Approach B), and variation of the
feature subset used (Approach C). Voting is perhaps
a simpler approach, as it selects the class with the
largest number of votes. Stacking is used to com-
bine different types of classifiers and demands the
use of another learning algorithm to predict which of
the models would be the most reliable for each case.
This is done with a meta-learner, another learning
scheme that combines the output of the base learn-
ers. The predictions of base learners are used as in-
put to the meta-learner.
We used WEKA’s “Stacking” (Wolpert, 1992)
meta-classifier in our first run, combining the fol-
lowing base models: three K-Nearest Neighbour
(KNN) classifiers (K = 1, K = 3, K = 5) (Aha et
al., 1991); a Linear Regression model without an at-
tribute selection method (−S1) and default ridge pa-
rameter (1.0−8); three M5P classifiers which imple-
ment base routines for generating M5 Model trees
and rules with a different minimum number of in-
stances (M = 4, M = 10, M = 20) (Quinlan,
1992; Wang and Witten, 1997). The meta-classifier
was a M5P classifier with M = 4. Other ensembles
were added for the second and third runs:
</bodyText>
<listItem confidence="0.9934205">
1. Stacking combining three base models: KNN
classifier (K = 1); Linear Regression model
without an attribute selection method (−S1)
and default ridge parameter (1.0−8); M5P, with
M = 4, being the meta-classifier6.
2. Stacking combining four base models: KNN
classifier (K = 1); Linear Regression model
without an attribute selection method (−S1)
</listItem>
<footnote confidence="0.38673">
6A Regression Tree using the M5 algorithm (Quinlan, 1992)
</footnote>
<bodyText confidence="0.9987272">
and default ridge parameter (1.0−8); ZeroR, a
simple rule-based classifier which determines
the median similarity score; and Isotonic Re-
gression model. M5P, with M = 4, as the
meta-classifier.
</bodyText>
<listItem confidence="0.757248">
3. Voting model of the seven classifiers of the first
run.
</listItem>
<bodyText confidence="0.999775333333333">
Specifically, the second and third run consisted in
the average similarity score produced by the three
models presented above, plus the model considered
in the first run. The only difference between the two
runs was that distributional features were not con-
sidered in the third run (Approach C).
</bodyText>
<sectionHeader confidence="0.755404" genericHeader="method">
4 Some Results and Discussion
</sectionHeader>
<bodyText confidence="0.999961086956522">
Although, STS might look similar to SemEval 2014
- Task 1, available datasets showed that they are very
different from each other. Therefore, we made indi-
vidual sets of data for training models and for ex-
tracting distributional features to evaluate with each
target dataset. In SemEval 2014 - Task 1, there was
only one homogeneous dataset, SICK (Marelli et al.,
2014b), with a relatively big amount of entries (5000
for training, 5000 for evaluation) which generally re-
sults in better ML outcome. Since answers-forums,
answers-students and belief were from new sources,
we opted to target these with the same systems, built
with most of the available data from previous STS
tasks.Table 1 shows that ASAP-II performed better
in the SICK dataset, followed by the two datasets
that are recurring (images and headlines). Unexpect-
edly though, the configuration targeting answers-
students performed well with only a little difference
to the best performance on the headlines, especially
if compared to the very low correlation achieved on
both answers-forums and belief. Finally, weighted
average pearson coefficient was computed consider-
ing the size of each evaluation dataset.
</bodyText>
<sectionHeader confidence="0.992847" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999526833333333">
We used complementary features for learning the
STS function, which is also part of the challenge
of building Compositional Distributional Semantic
Models. For this purpose, for each sentence, we ex-
tracted lexical, syntactic, semantic and distributional
features. On the semantic aspect, we computed the
</bodyText>
<page confidence="0.992125">
187
</page>
<table confidence="0.999845375">
First-run Second-run Third-run
answers-forums 0.2304 0.2374 0.2302
answers-students 0.6503 0.7095 0.6719
belief 0.3928 0.3986 0.4342
headlines 0.6614 0.7039 0.7156
images 0.6548 0.7294 0.7250
SICK 0.7200 0.7013 0.7735
Weighted Average 0.57 ± 0.07 0.62 ± 0.08 0.61 ± 0.07
</table>
<tableCaption confidence="0.997617333333333">
Table 1: Pearson’s correlation coefficient for ASAP-II in
SemEval2015-STS, by dataset, and a simulation of Se-
mEval2014 - Task 1, with the same configuration.
</tableCaption>
<bodyText confidence="0.999926666666667">
semantic similarity and relatedness between phrases
using known measures on WordNet, whose “cover-
age” was increased with the help of DBPedia. We
also applied topic modeling to get topic distribu-
tions over sets of sentences. All these features were
used to feed an ensemble algorithm for learning the
STS function. This resulted in a Pearson’s r of
0.62 ± 0.08 in our best performance over different
datasets.
We are motivated by this participation in STS and
intend to participate in further editions, while im-
proving ASAP. To this end, we should: make a
deeper analysis of the ensemble, to identify where
it can be improved; try to complement the feature
set with additional relevant features; explore differ-
ent topic distributions while varying the number of
topics and hopefully maximizing the log likelihood;
and assess the impact of each feature.
</bodyText>
<sectionHeader confidence="0.997593" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995187197183099">
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot
on semantic textual similarity. In Proceedings of the
First Joint Conference on Lexical and Computational
Semantics - Volume 1: Proceedings of the Main Con-
ference and the Shared Task, and Volume 2: Proceed-
ings of the Sixth International Workshop on Seman-
tic Evaluation, SemEval’12, pages 385–393, Strouds-
burg, PA, USA.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
agirre, and Weiwei Guo. 2013. sem 2013 shared task:
Semantic textual similarity, including a pilot on typed-
similarity. In In *SEM 2013: The Second Joint Con-
ference on Lexical and Computational Semantics.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo,
Rada Mihalcea, German Rigau, and Janyce Wiebe.
2014. Semeval-2014 task 10: Multilingual semantic
textual similarity. SemEval-2014.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo,
Iigo Lopez-Gazpio, Montse Maritxalar, Rada Mihal-
cea, German Rigau, Larraitz Uria, and Janyce Wiebe.
2015. SemEval-2015 Task 2: Semantic Textual Simi-
larity, English, Spanish and Pilot on Interpretability. In
Proceedings of the 9th International Workshop on Se-
mantic Evaluation (SemEval 2015), Denver, CO, June.
David W. Aha, Dennis Kibler, and Marc K. Albert. 1991.
Instance-based learning algorithms. Mach. Learn.,
6(1):37–66.
Ana Alves, Adriana Ferrugento, Mariana Loureno, and
Filipe Rodrigues. 2014. Asap: Automatic semantic
alignment for phrases. In SemEval Workshop, COL-
ING 2014, Ireland, n/a.
S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives. 2007.
Dbpedia: A nucleus for a web of open data. In
Proceedings of the 6th International The Semantic
Web and 2Nd Asian Conference on Asian Seman-
tic Web Conference, ISWC’07/ASWC’07, pages 722–
735, Berlin, Heidelberg.
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relatedness.
In Proceedings of the 18th International Joint Confer-
ence on Artificial Intelligence (IJCAI’03), pages 805–
810, CA, USA.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Comput. Linguist.,
22(1):39–71.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Machine
Learning Research, 3:993–1022.
J¨urgen Franke and Eberhard Mandler. 1992. A com-
parison of two approaches for combining the votes of
cooperating classifiers. In Pattern Recognition, 1992.
Vol.II. Conference B: Pattern Recognition Methodol-
ogy and Systems, Proceedings., 11th IAPR Interna-
tional Conference on, pages 611–614, Aug.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
SIGKDDExplor. Newsl., 11(1):10–18.
Jay J. Jiang and David W. Conrath. 1997. Semantic sim-
ilarity based on corpus statistics and lexical taxonomy.
In Proc. of the Int’l. Conf. on Research in Computa-
tional Linguistics, pages 19–33.
Ludmila I. Kuncheva and Christopher J. Whitaker. 2003.
Measures of diversity in classifier ensembles and
their relationship with the ensemble accuracy. Mach.
Learn., 51(2):181–207, May.
</reference>
<page confidence="0.98234">
188
</page>
<reference confidence="0.999499788461538">
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the
5th Annual International Conference on Systems Doc-
umentation (SIGDOC ’86), pages 24–26, NY, USA.
Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zampar-
elli. 2014a. Semeval-2014 task 1: Evaluation of com-
positional distributional semantic models on full sen-
tences through semantic relatedness and textual entail-
ment. SemEval-2014.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Robertomode
Zamparelli. 2014b. A sick cure for the evaluation of
compositional distributional semantic models. In Pro-
ceedings of LREC 2014.
George A. Miller. 1995. Wordnet: A lexical database for
english. Commun. ACM, 38(11):39–41, November.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity: Measuring the
relatedness of concepts. In Demonstration Papers
at HLT-NAACL 2004, HLT-NAACL–Demonstrations
’04, pages 38–41, PA, USA.
Martin F. Porter. 2001. Snowball: A language for stem-
ming algorithms. Published online.
Ross J. Quinlan. 1992. Learning with continuous
classes. In 5th Australian Joint Conference on Arti-
ficial Intelligence, pages 343–348, Singapore.
Radim ˇReh˚uˇrek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora. In
Proceedings of the Workshop on New Challenges for
NLP Frameworks (LREC 2010), pages 45–50, Valletta,
Malta.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proceedings
of the 14th International Joint Conference on Artificial
Intelligence - Volume 1, IJCAI’95, pages 448–453, San
Francisco, CA, USA.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval. Inf.
Process. Manage., 24(5):513–523.
Alexander K. Seewald. 2002. How to make stack-
ing better and faster while also taking care of an un-
known weakness. In C. Sammut and A. Hoffmann,
editors, Nineteenth International Conference on Ma-
chine Learning, pages 554–561.
Yong Wang and Ian H. Witten. 1997. Induction of model
trees for predicting continuous classes. In Poster
papers of the 9th European Conference on Machine
Learning.
David H. Wolpert. 1992. Stacked generalization. Neural
Networks, 5:241–259.
</reference>
<page confidence="0.998925">
189
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.064004">
<title confidence="0.95881">ASAP-II: From the Alignment of Phrases to Text Similarity</title>
<author confidence="0.271052">O</author>
<affiliation confidence="0.558866">Institute of a21210644@alunos.isec.pt University of</affiliation>
<email confidence="0.727949">aferr@student.dei.uc.pt</email>
<abstract confidence="0.946708789473684">ThisThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ paper describes the second version of the its participation in the SemEval-2015, task 2a on Semantic Textual Similarity (STS). Our approach is based on computing the WordNet semantic relatedness and similarity of phrases from distinct sentences. We also apply topic modeling to get topic distributions over a set of sentences as well as some linguistic heuristics. In a special addition for this task, we retrieve named entities and compound nouns from DBPedia. All these features are used to feed a regression algorithm that learns the STS function.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Mona Diab</author>
<author>Daniel Cer</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>Semeval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the Main Conference and the Shared Task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, SemEval’12,</booktitle>
<pages>385--393</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1292" citStr="Agirre et al., 2012" startWordPosition="180" endWordPosition="183">roach is based on computing the WordNet semantic relatedness and similarity of phrases from distinct sentences. We also apply topic modeling to get topic distributions over a set of sentences as well as some linguistic heuristics. In a special addition for this task, we retrieve named entities and compound nouns from DBPedia. All these features are used to feed a regression algorithm that learns the STS function. 1 Introduction Semantic Textual Similarity (STS), which is the task of computing the similarity between two sentences, has received an increasing amount of attention in recent years (Agirre et al., 2012; Agirre et al., 2013; Marelli et al., 2014a; Agirre et al., 2014; Agirre et al., 2015). Our contribution to this challenge is to learn the STS function for English texts. ASAP-II is an evolution of the ASAP system (Alves et al., 2014), which participated in SemEval 2014 - Task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. Although with a different goal from STS, which goes beyond relatedness 1This work was supported by the InfoCrowds project - FCTPTDC/ECM-TRA/1898/2012 and entailment, and different datasets</context>
</contexts>
<marker>Agirre, Diab, Cer, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Mona Diab, Daniel Cer, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot on semantic textual similarity. In Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the Main Conference and the Shared Task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, SemEval’12, pages 385–393, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalezagirre</author>
<author>Weiwei Guo</author>
</authors>
<title>sem 2013 shared task: Semantic textual similarity, including a pilot on typedsimilarity.</title>
<date>2013</date>
<booktitle>In In *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics.</booktitle>
<contexts>
<context position="1313" citStr="Agirre et al., 2013" startWordPosition="184" endWordPosition="187">puting the WordNet semantic relatedness and similarity of phrases from distinct sentences. We also apply topic modeling to get topic distributions over a set of sentences as well as some linguistic heuristics. In a special addition for this task, we retrieve named entities and compound nouns from DBPedia. All these features are used to feed a regression algorithm that learns the STS function. 1 Introduction Semantic Textual Similarity (STS), which is the task of computing the similarity between two sentences, has received an increasing amount of attention in recent years (Agirre et al., 2012; Agirre et al., 2013; Marelli et al., 2014a; Agirre et al., 2014; Agirre et al., 2015). Our contribution to this challenge is to learn the STS function for English texts. ASAP-II is an evolution of the ASAP system (Alves et al., 2014), which participated in SemEval 2014 - Task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. Although with a different goal from STS, which goes beyond relatedness 1This work was supported by the InfoCrowds project - FCTPTDC/ECM-TRA/1898/2012 and entailment, and different datasets, which include pairs</context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalezagirre, Guo, 2013</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalezagirre, and Weiwei Guo. 2013. sem 2013 shared task: Semantic textual similarity, including a pilot on typedsimilarity. In In *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Banea</author>
<author>Claire Cardie</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
<author>Weiwei Guo</author>
<author>Rada Mihalcea</author>
<author>German Rigau</author>
<author>Janyce Wiebe</author>
</authors>
<date>2014</date>
<booktitle>Semeval-2014 task 10: Multilingual semantic textual similarity. SemEval-2014.</booktitle>
<contexts>
<context position="1357" citStr="Agirre et al., 2014" startWordPosition="192" endWordPosition="195">similarity of phrases from distinct sentences. We also apply topic modeling to get topic distributions over a set of sentences as well as some linguistic heuristics. In a special addition for this task, we retrieve named entities and compound nouns from DBPedia. All these features are used to feed a regression algorithm that learns the STS function. 1 Introduction Semantic Textual Similarity (STS), which is the task of computing the similarity between two sentences, has received an increasing amount of attention in recent years (Agirre et al., 2012; Agirre et al., 2013; Marelli et al., 2014a; Agirre et al., 2014; Agirre et al., 2015). Our contribution to this challenge is to learn the STS function for English texts. ASAP-II is an evolution of the ASAP system (Alves et al., 2014), which participated in SemEval 2014 - Task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. Although with a different goal from STS, which goes beyond relatedness 1This work was supported by the InfoCrowds project - FCTPTDC/ECM-TRA/1898/2012 and entailment, and different datasets, which include pairs of short texts instead of controlled senten</context>
</contexts>
<marker>Agirre, Banea, Cardie, Cer, Diab, Gonzalez-Agirre, Guo, Mihalcea, Rigau, Wiebe, 2014</marker>
<rawString>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2014. Semeval-2014 task 10: Multilingual semantic textual similarity. SemEval-2014.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Banea</author>
<author>Claire Cardie</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
</authors>
<title>Aitor Gonzalez-Agirre, Weiwei Guo, Iigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe.</title>
<date>2015</date>
<booktitle>SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015),</booktitle>
<location>Denver, CO,</location>
<contexts>
<context position="1379" citStr="Agirre et al., 2015" startWordPosition="196" endWordPosition="199"> from distinct sentences. We also apply topic modeling to get topic distributions over a set of sentences as well as some linguistic heuristics. In a special addition for this task, we retrieve named entities and compound nouns from DBPedia. All these features are used to feed a regression algorithm that learns the STS function. 1 Introduction Semantic Textual Similarity (STS), which is the task of computing the similarity between two sentences, has received an increasing amount of attention in recent years (Agirre et al., 2012; Agirre et al., 2013; Marelli et al., 2014a; Agirre et al., 2014; Agirre et al., 2015). Our contribution to this challenge is to learn the STS function for English texts. ASAP-II is an evolution of the ASAP system (Alves et al., 2014), which participated in SemEval 2014 - Task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. Although with a different goal from STS, which goes beyond relatedness 1This work was supported by the InfoCrowds project - FCTPTDC/ECM-TRA/1898/2012 and entailment, and different datasets, which include pairs of short texts instead of controlled sentences, we believe that, </context>
</contexts>
<marker>Agirre, Banea, Cardie, Cer, Diab, 2015</marker>
<rawString>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Iigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe. 2015. SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), Denver, CO, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David W Aha</author>
<author>Dennis Kibler</author>
<author>Marc K Albert</author>
</authors>
<title>Instance-based learning algorithms.</title>
<date>1991</date>
<journal>Mach. Learn.,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="13354" citStr="Aha et al., 1991" startWordPosition="2082" endWordPosition="2085">h, as it selects the class with the largest number of votes. Stacking is used to combine different types of classifiers and demands the use of another learning algorithm to predict which of the models would be the most reliable for each case. This is done with a meta-learner, another learning scheme that combines the output of the base learners. The predictions of base learners are used as input to the meta-learner. We used WEKA’s “Stacking” (Wolpert, 1992) meta-classifier in our first run, combining the following base models: three K-Nearest Neighbour (KNN) classifiers (K = 1, K = 3, K = 5) (Aha et al., 1991); a Linear Regression model without an attribute selection method (−S1) and default ridge parameter (1.0−8); three M5P classifiers which implement base routines for generating M5 Model trees and rules with a different minimum number of instances (M = 4, M = 10, M = 20) (Quinlan, 1992; Wang and Witten, 1997). The meta-classifier was a M5P classifier with M = 4. Other ensembles were added for the second and third runs: 1. Stacking combining three base models: KNN classifier (K = 1); Linear Regression model without an attribute selection method (−S1) and default ridge parameter (1.0−8); M5P, with</context>
</contexts>
<marker>Aha, Kibler, Albert, 1991</marker>
<rawString>David W. Aha, Dennis Kibler, and Marc K. Albert. 1991. Instance-based learning algorithms. Mach. Learn., 6(1):37–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana Alves</author>
<author>Adriana Ferrugento</author>
<author>Mariana Loureno</author>
<author>Filipe Rodrigues</author>
</authors>
<title>Asap: Automatic semantic alignment for phrases.</title>
<date>2014</date>
<booktitle>In SemEval Workshop, COLING 2014,</booktitle>
<location>Ireland, n/a.</location>
<contexts>
<context position="1527" citStr="Alves et al., 2014" startWordPosition="222" endWordPosition="225"> special addition for this task, we retrieve named entities and compound nouns from DBPedia. All these features are used to feed a regression algorithm that learns the STS function. 1 Introduction Semantic Textual Similarity (STS), which is the task of computing the similarity between two sentences, has received an increasing amount of attention in recent years (Agirre et al., 2012; Agirre et al., 2013; Marelli et al., 2014a; Agirre et al., 2014; Agirre et al., 2015). Our contribution to this challenge is to learn the STS function for English texts. ASAP-II is an evolution of the ASAP system (Alves et al., 2014), which participated in SemEval 2014 - Task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. Although with a different goal from STS, which goes beyond relatedness 1This work was supported by the InfoCrowds project - FCTPTDC/ECM-TRA/1898/2012 and entailment, and different datasets, which include pairs of short texts instead of controlled sentences, we believe that, rather than specifying rules, constraints and lexicons manually, it is possible to adapt a system from one to the other task, by automatically acqui</context>
</contexts>
<marker>Alves, Ferrugento, Loureno, Rodrigues, 2014</marker>
<rawString>Ana Alves, Adriana Ferrugento, Mariana Loureno, and Filipe Rodrigues. 2014. Asap: Automatic semantic alignment for phrases. In SemEval Workshop, COLING 2014, Ireland, n/a.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S¨oren Auer</author>
<author>Christian Bizer</author>
<author>Georgi Kobilarov</author>
<author>Jens Lehmann</author>
<author>Richard Cyganiak</author>
<author>Zachary Ives</author>
</authors>
<title>Dbpedia: A nucleus for a web of open data.</title>
<date>2007</date>
<booktitle>In Proceedings of the 6th International The Semantic Web and 2Nd Asian Conference on Asian Semantic Web Conference, ISWC’07/ASWC’07,</booktitle>
<pages>722--735</pages>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="3962" citStr="Auer et al., 2007" startWordPosition="594" endWordPosition="597">vious tasks. Finally, section 5 presents some conclusions and complementary work to be done in a near future. 2See https://github.com/examinus-/ASAP 184 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 184–189, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics 2 Background 2.1 Knowledge Bases WordNet (Miller, 1995) is a lexical knowledge base structured in synsets – groups of synonymous words that may be seen as possible lexicalizations of a concept – and relations between them, including hypernymy or part-of. DBpedia (Auer et al., 2007) is an effort for extracting structured information from Wikipedia, a well-known collaborative encyclopedia. DBPedia is a central part of the Linked Data initiative and consequently, it is linked to many other resources, including a RDF version of WordNet. In fact, some DBPedia entities are connected to their abstract category in WordNet, through the wordnet type property. For instance, CNN is connected to the synset {channel, transmission channel} and Berlusconi to {chancellor, premier, prime minister}. 2.2 Semantic Similarity There are two main approaches to semantic similarity: (i) semantic</context>
</contexts>
<marker>Auer, Bizer, Kobilarov, Lehmann, Cyganiak, Ives, 2007</marker>
<rawString>S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007. Dbpedia: A nucleus for a web of open data. In Proceedings of the 6th International The Semantic Web and 2Nd Asian Conference on Asian Semantic Web Conference, ISWC’07/ASWC’07, pages 722– 735, Berlin, Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Extended gloss overlaps as a measure of semantic relatedness.</title>
<date>2003</date>
<booktitle>In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI’03),</booktitle>
<pages>805</pages>
<location>810, CA, USA.</location>
<contexts>
<context position="10394" citStr="Banerjee and Pedersen, 2003" startWordPosition="1600" endWordPosition="1604">eval2014/task1/ data/uploads/sick_baseline.zip, which we used as a different setting for stop word removal (from 0 to 3, 4 different combinations) with a suitable synset, the leftmost word of a compound phrase, generally a modifier, is removed until the phrase is empty or a synset is retrieved. If still unsuccessful and the last word ends with an ‘s’, the last character is removed and the word is looked up again. After retrieving a WordNet sense for each phrase, semantic similarity is computed for each pair, using Resnik (1995) (f14), Jiang &amp; Conrath (1997) (f15) and the Adapted Lesk metrics (Banerjee and Pedersen, 2003) (f16) using WS4j tool, where algorithms in the WordNet::Similarity (Pedersen et al., 2004) Perl package are implemented. For part-of-speech tagged words with multiple senses, the one maximizing partial similarity is selected. 3.3 Distributional Features The distribution of topics over documents (in our case, short texts) may contribute to model Semantic Similarity since there is no notion of mutual exclusivity that restricts words to be part of one topic only. This allows topic models to capture polysemy. We may thus see topics as natural word sense contexts, as words occur in different topic</context>
</contexts>
<marker>Banerjee, Pedersen, 2003</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2003. Extended gloss overlaps as a measure of semantic relatedness. In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI’03), pages 805– 810, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Comput. Linguist.,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="6660" citStr="Berger et al., 1996" startWordPosition="1003" endWordPosition="1006">ribution between pairs of sentences, which can be considered a composed partial similarity on each topic distribution. Partial similarities are used as features in the supervised learning process. In the following section, complementary stages of our system are explained in detail. 3.1 Natural Language Preprocessing Sentences are decomposed after applying wellknown Natural Language Processing subtasks, namely tokenization, part-of-speech tagging and chunking. For this purpose, we use OpenNLP3, a tool for processing natural language text out-ofthe-box, based on a maximum entropy (ME) approach (Berger et al., 1996). Although OpenNLP offers an English stemmer, this is not sufficient for our approach. Instead, we rely on the lemmatization performed by the WS4J library4, with some additional heuristics (see section 3.2.3). 3.2 Feature Engineering Features encode information from raw data that allows machine learning algorithms to estimate an unknown value. We focus on, what we call, light features since they are computed automatically, not requiring a specific labeled dataset and we are using already trained models. Each feature is computed as a partial similarity metric, which will later feed the posterio</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Vincent J. Della Pietra, and Stephen A. Della Pietra. 1996. A maximum entropy approach to natural language processing. Comput. Linguist., 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="5315" citStr="Blei et al., 2003" startWordPosition="801" endWordPosition="804">ations in a lexical knowledge base, such as WordNet. Semantic similarity differs from semantic relatedness because it computes proximity between concepts in a given concept hierarchy (see (Resnik, 1995) and (Jiang and Conrath, 1997)), while the former computes the usage of common concepts together (see (Lesk, 1986), in this case on dictionary definitions/glosses). 2.3 Topic Modeling Topic modeling relies on the assumption that documents are mixtures of topics, which, in turn, are probability distributions over words. Latent Dirichlet Allocation (LDA) is a generative probabilistic topic model (Blei et al., 2003) where documents are represented as random mixtures over latent topics, characterized by a distribution over words. Assumptions are not made on the word order, only their frequency is relevant. In LDA, main variables are the topic-word distribution Φ and topic distributions θ for each document. 3 Proposed Approach Our approach to STS is based on a regression function, learned automatically to compute the similarity between sentences, using their components as features. Sentence features are obtained after a preprocessing stage, where sentences are lexically, syntactically and semantically deco</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨urgen Franke</author>
<author>Eberhard Mandler</author>
</authors>
<title>A comparison of two approaches for combining the votes of cooperating classifiers.</title>
<date>1992</date>
<booktitle>In Pattern Recognition,</booktitle>
<pages>611--614</pages>
<contexts>
<context position="12593" citStr="Franke and Mandler, 1992" startWordPosition="1951" endWordPosition="1954"> features. 186 One of four approaches is commonly adopted for building classifier ensembles, each focused on a different level of action. Approach A concerns the different ways of combining the results from the classifiers. Approach B uses different models.At feature level (Approach C), different feature subsets can be used for the classifiers, either if they use the same classification model or not. Finally, datasets can be modified so that each classifier in the ensemble is trained on its own dataset (Approach D) (Kuncheva and Whitaker, 2003). Different methods where applied such as Voting (Franke and Mandler, 1992) (Approach A), Stacking (Seewald, 2002) (Approach B), and variation of the feature subset used (Approach C). Voting is perhaps a simpler approach, as it selects the class with the largest number of votes. Stacking is used to combine different types of classifiers and demands the use of another learning algorithm to predict which of the models would be the most reliable for each case. This is done with a meta-learner, another learning scheme that combines the output of the base learners. The predictions of base learners are used as input to the meta-learner. We used WEKA’s “Stacking” (Wolpert, </context>
</contexts>
<marker>Franke, Mandler, 1992</marker>
<rawString>J¨urgen Franke and Eberhard Mandler. 1992. A comparison of two approaches for combining the votes of cooperating classifiers. In Pattern Recognition, 1992. Vol.II. Conference B: Pattern Recognition Methodology and Systems, Proceedings., 11th IAPR International Conference on, pages 611–614, Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: An update.</title>
<date>2009</date>
<journal>SIGKDDExplor. Newsl.,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="11842" citStr="Hall et al., 2009" startWordPosition="1832" endWordPosition="1835">hat combines the frequency of a term in a particular document with its inverse document frequency in general use (Salton and Buckley, 1988). This score is high for rare terms that occur frequently in a document and are therefore more likely to be significant. Gensim computes a distribution of 25 topics over texts with or without using TF-IDF (f17...41). Each feature is the absolute difference of topici (i.e. topic[i] = |topic[i]s1 − topic[i]s2|). The euclidean distance over the difference of topic distribution between text pairs was used as another feature (f42). 3.4 Supervised Learning WEKA (Hall et al., 2009) is a large collection of machine learning algorithms, written in Java, used for learning our STS function from aforementioned features. 186 One of four approaches is commonly adopted for building classifier ensembles, each focused on a different level of action. Approach A concerns the different ways of combining the results from the classifiers. Approach B uses different models.At feature level (Approach C), different feature subsets can be used for the classifiers, either if they use the same classification model or not. Finally, datasets can be modified so that each classifier in the ensem</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The weka data mining software: An update. SIGKDDExplor. Newsl., 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proc. of the Int’l. Conf. on Research in Computational Linguistics,</booktitle>
<pages>pages</pages>
<contexts>
<context position="4929" citStr="Jiang and Conrath, 1997" startWordPosition="741" endWordPosition="744">rough the wordnet type property. For instance, CNN is connected to the synset {channel, transmission channel} and Berlusconi to {chancellor, premier, prime minister}. 2.2 Semantic Similarity There are two main approaches to semantic similarity: (i) semantic relatedness is based on cooccurrence statistics, typically over a large corpus; (ii) classic semantic similarity exploits semantic relations in a lexical knowledge base, such as WordNet. Semantic similarity differs from semantic relatedness because it computes proximity between concepts in a given concept hierarchy (see (Resnik, 1995) and (Jiang and Conrath, 1997)), while the former computes the usage of common concepts together (see (Lesk, 1986), in this case on dictionary definitions/glosses). 2.3 Topic Modeling Topic modeling relies on the assumption that documents are mixtures of topics, which, in turn, are probability distributions over words. Latent Dirichlet Allocation (LDA) is a generative probabilistic topic model (Blei et al., 2003) where documents are represented as random mixtures over latent topics, characterized by a distribution over words. Assumptions are not made on the word order, only their frequency is relevant. In LDA, main variabl</context>
<context position="10329" citStr="Jiang &amp; Conrath (1997)" startWordPosition="1590" endWordPosition="1593">or computing baselines available at http://alt.qcri.org/semeval2014/task1/ data/uploads/sick_baseline.zip, which we used as a different setting for stop word removal (from 0 to 3, 4 different combinations) with a suitable synset, the leftmost word of a compound phrase, generally a modifier, is removed until the phrase is empty or a synset is retrieved. If still unsuccessful and the last word ends with an ‘s’, the last character is removed and the word is looked up again. After retrieving a WordNet sense for each phrase, semantic similarity is computed for each pair, using Resnik (1995) (f14), Jiang &amp; Conrath (1997) (f15) and the Adapted Lesk metrics (Banerjee and Pedersen, 2003) (f16) using WS4j tool, where algorithms in the WordNet::Similarity (Pedersen et al., 2004) Perl package are implemented. For part-of-speech tagged words with multiple senses, the one maximizing partial similarity is selected. 3.3 Distributional Features The distribution of topics over documents (in our case, short texts) may contribute to model Semantic Similarity since there is no notion of mutual exclusivity that restricts words to be part of one topic only. This allows topic models to capture polysemy. We may thus see topics </context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J. Jiang and David W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proc. of the Int’l. Conf. on Research in Computational Linguistics, pages 19–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ludmila I Kuncheva</author>
<author>Christopher J Whitaker</author>
</authors>
<title>Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy.</title>
<date>2003</date>
<journal>Mach. Learn.,</journal>
<volume>51</volume>
<issue>2</issue>
<contexts>
<context position="12518" citStr="Kuncheva and Whitaker, 2003" startWordPosition="1940" endWordPosition="1943">ithms, written in Java, used for learning our STS function from aforementioned features. 186 One of four approaches is commonly adopted for building classifier ensembles, each focused on a different level of action. Approach A concerns the different ways of combining the results from the classifiers. Approach B uses different models.At feature level (Approach C), different feature subsets can be used for the classifiers, either if they use the same classification model or not. Finally, datasets can be modified so that each classifier in the ensemble is trained on its own dataset (Approach D) (Kuncheva and Whitaker, 2003). Different methods where applied such as Voting (Franke and Mandler, 1992) (Approach A), Stacking (Seewald, 2002) (Approach B), and variation of the feature subset used (Approach C). Voting is perhaps a simpler approach, as it selects the class with the largest number of votes. Stacking is used to combine different types of classifiers and demands the use of another learning algorithm to predict which of the models would be the most reliable for each case. This is done with a meta-learner, another learning scheme that combines the output of the base learners. The predictions of base learners </context>
</contexts>
<marker>Kuncheva, Whitaker, 2003</marker>
<rawString>Ludmila I. Kuncheva and Christopher J. Whitaker. 2003. Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy. Mach. Learn., 51(2):181–207, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the 5th Annual International Conference on Systems Documentation (SIGDOC ’86),</booktitle>
<pages>24--26</pages>
<location>NY, USA.</location>
<contexts>
<context position="5013" citStr="Lesk, 1986" startWordPosition="758" endWordPosition="759">ion channel} and Berlusconi to {chancellor, premier, prime minister}. 2.2 Semantic Similarity There are two main approaches to semantic similarity: (i) semantic relatedness is based on cooccurrence statistics, typically over a large corpus; (ii) classic semantic similarity exploits semantic relations in a lexical knowledge base, such as WordNet. Semantic similarity differs from semantic relatedness because it computes proximity between concepts in a given concept hierarchy (see (Resnik, 1995) and (Jiang and Conrath, 1997)), while the former computes the usage of common concepts together (see (Lesk, 1986), in this case on dictionary definitions/glosses). 2.3 Topic Modeling Topic modeling relies on the assumption that documents are mixtures of topics, which, in turn, are probability distributions over words. Latent Dirichlet Allocation (LDA) is a generative probabilistic topic model (Blei et al., 2003) where documents are represented as random mixtures over latent topics, characterized by a distribution over words. Assumptions are not made on the word order, only their frequency is relevant. In LDA, main variables are the topic-word distribution Φ and topic distributions θ for each document. 3 </context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Michael Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone. In Proceedings of the 5th Annual International Conference on Systems Documentation (SIGDOC ’86), pages 24–26, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Marelli</author>
<author>Luisa Bentivogli</author>
<author>Marco Baroni</author>
<author>Raffaella Bernardi</author>
<author>Stefano Menini</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment.</title>
<date>2014</date>
<tech>SemEval-2014.</tech>
<contexts>
<context position="1335" citStr="Marelli et al., 2014" startWordPosition="188" endWordPosition="191">mantic relatedness and similarity of phrases from distinct sentences. We also apply topic modeling to get topic distributions over a set of sentences as well as some linguistic heuristics. In a special addition for this task, we retrieve named entities and compound nouns from DBPedia. All these features are used to feed a regression algorithm that learns the STS function. 1 Introduction Semantic Textual Similarity (STS), which is the task of computing the similarity between two sentences, has received an increasing amount of attention in recent years (Agirre et al., 2012; Agirre et al., 2013; Marelli et al., 2014a; Agirre et al., 2014; Agirre et al., 2015). Our contribution to this challenge is to learn the STS function for English texts. ASAP-II is an evolution of the ASAP system (Alves et al., 2014), which participated in SemEval 2014 - Task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. Although with a different goal from STS, which goes beyond relatedness 1This work was supported by the InfoCrowds project - FCTPTDC/ECM-TRA/1898/2012 and entailment, and different datasets, which include pairs of short texts instea</context>
<context position="15125" citStr="Marelli et al., 2014" startWordPosition="2374" endWordPosition="2377">y score produced by the three models presented above, plus the model considered in the first run. The only difference between the two runs was that distributional features were not considered in the third run (Approach C). 4 Some Results and Discussion Although, STS might look similar to SemEval 2014 - Task 1, available datasets showed that they are very different from each other. Therefore, we made individual sets of data for training models and for extracting distributional features to evaluate with each target dataset. In SemEval 2014 - Task 1, there was only one homogeneous dataset, SICK (Marelli et al., 2014b), with a relatively big amount of entries (5000 for training, 5000 for evaluation) which generally results in better ML outcome. Since answers-forums, answers-students and belief were from new sources, we opted to target these with the same systems, built with most of the available data from previous STS tasks.Table 1 shows that ASAP-II performed better in the SICK dataset, followed by the two datasets that are recurring (images and headlines). Unexpectedly though, the configuration targeting answersstudents performed well with only a little difference to the best performance on the headline</context>
</contexts>
<marker>Marelli, Bentivogli, Baroni, Bernardi, Menini, Zamparelli, 2014</marker>
<rawString>Marco Marelli, Luisa Bentivogli, Marco Baroni, Raffaella Bernardi, Stefano Menini, and Roberto Zamparelli. 2014a. Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. SemEval-2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Marelli</author>
<author>Stefano Menini</author>
<author>Marco Baroni</author>
<author>Luisa Bentivogli</author>
<author>Raffaella Bernardi</author>
<author>Robertomode Zamparelli</author>
</authors>
<title>A sick cure for the evaluation of compositional distributional semantic models.</title>
<date>2014</date>
<booktitle>In Proceedings of LREC</booktitle>
<contexts>
<context position="1335" citStr="Marelli et al., 2014" startWordPosition="188" endWordPosition="191">mantic relatedness and similarity of phrases from distinct sentences. We also apply topic modeling to get topic distributions over a set of sentences as well as some linguistic heuristics. In a special addition for this task, we retrieve named entities and compound nouns from DBPedia. All these features are used to feed a regression algorithm that learns the STS function. 1 Introduction Semantic Textual Similarity (STS), which is the task of computing the similarity between two sentences, has received an increasing amount of attention in recent years (Agirre et al., 2012; Agirre et al., 2013; Marelli et al., 2014a; Agirre et al., 2014; Agirre et al., 2015). Our contribution to this challenge is to learn the STS function for English texts. ASAP-II is an evolution of the ASAP system (Alves et al., 2014), which participated in SemEval 2014 - Task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. Although with a different goal from STS, which goes beyond relatedness 1This work was supported by the InfoCrowds project - FCTPTDC/ECM-TRA/1898/2012 and entailment, and different datasets, which include pairs of short texts instea</context>
<context position="15125" citStr="Marelli et al., 2014" startWordPosition="2374" endWordPosition="2377">y score produced by the three models presented above, plus the model considered in the first run. The only difference between the two runs was that distributional features were not considered in the third run (Approach C). 4 Some Results and Discussion Although, STS might look similar to SemEval 2014 - Task 1, available datasets showed that they are very different from each other. Therefore, we made individual sets of data for training models and for extracting distributional features to evaluate with each target dataset. In SemEval 2014 - Task 1, there was only one homogeneous dataset, SICK (Marelli et al., 2014b), with a relatively big amount of entries (5000 for training, 5000 for evaluation) which generally results in better ML outcome. Since answers-forums, answers-students and belief were from new sources, we opted to target these with the same systems, built with most of the available data from previous STS tasks.Table 1 shows that ASAP-II performed better in the SICK dataset, followed by the two datasets that are recurring (images and headlines). Unexpectedly though, the configuration targeting answersstudents performed well with only a little difference to the best performance on the headline</context>
</contexts>
<marker>Marelli, Menini, Baroni, Bentivogli, Bernardi, Zamparelli, 2014</marker>
<rawString>Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Robertomode Zamparelli. 2014b. A sick cure for the evaluation of compositional distributional semantic models. In Proceedings of LREC 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: A lexical database for english.</title>
<date>1995</date>
<journal>Commun. ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="2445" citStr="Miller, 1995" startWordPosition="363" endWordPosition="364">TDC/ECM-TRA/1898/2012 and entailment, and different datasets, which include pairs of short texts instead of controlled sentences, we believe that, rather than specifying rules, constraints and lexicons manually, it is possible to adapt a system from one to the other task, by automatically acquiring linguistic knowledge through machine learning (ML) methods. For this purpose, we apply some pre-processing techniques to the training set in order to extract different types of features. On the semantic aspect, we compute the similarity/relatedness between phrases using known measures over WordNet (Miller, 1995). Considering the problem of modeling a text corpus to find short descriptions of documents, we aim at an efficient processing of large collections, while preserving the essential statistical relationships that are useful for similarity judgment. Therefore, we also apply topic modeling, in order to get topic distribution over each sentence set. These features are then used to feed an ensemble ML algorithm for learning the STS function. Our system is entirely developed as a Java independent software package, publicly available2 for training and testing on given and new datasets containing pairs</context>
<context position="3735" citStr="Miller, 1995" startWordPosition="557" endWordPosition="558">ndamental concepts are introduced in order to understand the proposed approach delineated in section 3. Section 4 presents some results of our approach, using not only the SemEval-2015’s dataset, but also datasets from previous tasks. Finally, section 5 presents some conclusions and complementary work to be done in a near future. 2See https://github.com/examinus-/ASAP 184 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 184–189, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics 2 Background 2.1 Knowledge Bases WordNet (Miller, 1995) is a lexical knowledge base structured in synsets – groups of synonymous words that may be seen as possible lexicalizations of a concept – and relations between them, including hypernymy or part-of. DBpedia (Auer et al., 2007) is an effort for extracting structured information from Wikipedia, a well-known collaborative encyclopedia. DBPedia is a central part of the Linked Data initiative and consequently, it is linked to many other resources, including a RDF version of WordNet. In fact, some DBPedia entities are connected to their abstract category in WordNet, through the wordnet type propert</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. Wordnet: A lexical database for english. Commun. ACM, 38(11):39–41, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>Wordnet::similarity: Measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Demonstration Papers at HLT-NAACL 2004, HLT-NAACL–Demonstrations ’04,</booktitle>
<pages>38--41</pages>
<location>PA, USA.</location>
<contexts>
<context position="10485" citStr="Pedersen et al., 2004" startWordPosition="1614" endWordPosition="1617">d removal (from 0 to 3, 4 different combinations) with a suitable synset, the leftmost word of a compound phrase, generally a modifier, is removed until the phrase is empty or a synset is retrieved. If still unsuccessful and the last word ends with an ‘s’, the last character is removed and the word is looked up again. After retrieving a WordNet sense for each phrase, semantic similarity is computed for each pair, using Resnik (1995) (f14), Jiang &amp; Conrath (1997) (f15) and the Adapted Lesk metrics (Banerjee and Pedersen, 2003) (f16) using WS4j tool, where algorithms in the WordNet::Similarity (Pedersen et al., 2004) Perl package are implemented. For part-of-speech tagged words with multiple senses, the one maximizing partial similarity is selected. 3.3 Distributional Features The distribution of topics over documents (in our case, short texts) may contribute to model Semantic Similarity since there is no notion of mutual exclusivity that restricts words to be part of one topic only. This allows topic models to capture polysemy. We may thus see topics as natural word sense contexts, as words occur in different topics with distinct “senses”. Gensim (ˇReh˚uˇrek and Sojka, 2010) is a machine learning framewo</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. Wordnet::similarity: Measuring the relatedness of concepts. In Demonstration Papers at HLT-NAACL 2004, HLT-NAACL–Demonstrations ’04, pages 38–41, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin F Porter</author>
</authors>
<title>Snowball: A language for stemming algorithms.</title>
<date>2001</date>
<note>Published online.</note>
<contexts>
<context position="7874" citStr="Porter, 2001" startWordPosition="1193" endWordPosition="1194"> regression analysis. This process is fully automatized, as all features are extracted using OpenNLP and other tools that will be introduced later. For convenience, we set an id for each feature, which has the form f#n, n E {1..}. 3Seehttp://opennlp.sourceforge.net 4A thread-safe, self-contained, Java implementation of some of useful functions over WordNet.See https://code. google.com/p/ws4j/ 185 3.2.1 Lexical Features Some basic similarity metrics are used as features related exclusively with word forms. In this set, we include for each text: the number of stop words, from the Snowball list (Porter, 2001) (f1 and f2 respectively) and the absolute difference of those counts (f3 = |f1− f2|); the number of those words expressing negation (f4 and f5 respectively) and the absolute difference of those counts (f6 = |f4−f5|). In addition, we used the absolute difference of overlapping words for each text pair (f7..10)5. 3.2.2 Syntactic Features The Max Entropy models of OpenNLP were used for tokenization, part-of-speech tagging and text chunking, applied in a pipeline for identifying Noun Phrases (NPs), Verbal Phrases (VPs) and Prepositional Phrases (PPs) of each sentence. Heuristically, these NPs are</context>
</contexts>
<marker>Porter, 2001</marker>
<rawString>Martin F. Porter. 2001. Snowball: A language for stemming algorithms. Published online.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ross J Quinlan</author>
</authors>
<title>Learning with continuous classes.</title>
<date>1992</date>
<booktitle>In 5th Australian Joint Conference on Artificial Intelligence,</booktitle>
<pages>343--348</pages>
<contexts>
<context position="13638" citStr="Quinlan, 1992" startWordPosition="2135" endWordPosition="2136">rning scheme that combines the output of the base learners. The predictions of base learners are used as input to the meta-learner. We used WEKA’s “Stacking” (Wolpert, 1992) meta-classifier in our first run, combining the following base models: three K-Nearest Neighbour (KNN) classifiers (K = 1, K = 3, K = 5) (Aha et al., 1991); a Linear Regression model without an attribute selection method (−S1) and default ridge parameter (1.0−8); three M5P classifiers which implement base routines for generating M5 Model trees and rules with a different minimum number of instances (M = 4, M = 10, M = 20) (Quinlan, 1992; Wang and Witten, 1997). The meta-classifier was a M5P classifier with M = 4. Other ensembles were added for the second and third runs: 1. Stacking combining three base models: KNN classifier (K = 1); Linear Regression model without an attribute selection method (−S1) and default ridge parameter (1.0−8); M5P, with M = 4, being the meta-classifier6. 2. Stacking combining four base models: KNN classifier (K = 1); Linear Regression model without an attribute selection method (−S1) 6A Regression Tree using the M5 algorithm (Quinlan, 1992) and default ridge parameter (1.0−8); ZeroR, a simple rule-</context>
</contexts>
<marker>Quinlan, 1992</marker>
<rawString>Ross J. Quinlan. 1992. Learning with continuous classes. In 5th Australian Joint Conference on Artificial Intelligence, pages 343–348, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radim ˇReh˚uˇrek</author>
<author>Petr Sojka</author>
</authors>
<title>Software Framework for Topic Modelling with Large Corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the Workshop on New Challenges for NLP Frameworks (LREC 2010),</booktitle>
<pages>45--50</pages>
<location>Valletta,</location>
<marker>ˇReh˚uˇrek, Sojka, 2010</marker>
<rawString>Radim ˇReh˚uˇrek and Petr Sojka. 2010. Software Framework for Topic Modelling with Large Corpora. In Proceedings of the Workshop on New Challenges for NLP Frameworks (LREC 2010), pages 45–50, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence - Volume 1, IJCAI’95,</booktitle>
<pages>448--453</pages>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="4899" citStr="Resnik, 1995" startWordPosition="738" endWordPosition="739">gory in WordNet, through the wordnet type property. For instance, CNN is connected to the synset {channel, transmission channel} and Berlusconi to {chancellor, premier, prime minister}. 2.2 Semantic Similarity There are two main approaches to semantic similarity: (i) semantic relatedness is based on cooccurrence statistics, typically over a large corpus; (ii) classic semantic similarity exploits semantic relations in a lexical knowledge base, such as WordNet. Semantic similarity differs from semantic relatedness because it computes proximity between concepts in a given concept hierarchy (see (Resnik, 1995) and (Jiang and Conrath, 1997)), while the former computes the usage of common concepts together (see (Lesk, 1986), in this case on dictionary definitions/glosses). 2.3 Topic Modeling Topic modeling relies on the assumption that documents are mixtures of topics, which, in turn, are probability distributions over words. Latent Dirichlet Allocation (LDA) is a generative probabilistic topic model (Blei et al., 2003) where documents are represented as random mixtures over latent topics, characterized by a distribution over words. Assumptions are not made on the word order, only their frequency is </context>
<context position="10299" citStr="Resnik (1995)" startWordPosition="1587" endWordPosition="1588">ing a Python script for computing baselines available at http://alt.qcri.org/semeval2014/task1/ data/uploads/sick_baseline.zip, which we used as a different setting for stop word removal (from 0 to 3, 4 different combinations) with a suitable synset, the leftmost word of a compound phrase, generally a modifier, is removed until the phrase is empty or a synset is retrieved. If still unsuccessful and the last word ends with an ‘s’, the last character is removed and the word is looked up again. After retrieving a WordNet sense for each phrase, semantic similarity is computed for each pair, using Resnik (1995) (f14), Jiang &amp; Conrath (1997) (f15) and the Adapted Lesk metrics (Banerjee and Pedersen, 2003) (f16) using WS4j tool, where algorithms in the WordNet::Similarity (Pedersen et al., 2004) Perl package are implemented. For part-of-speech tagged words with multiple senses, the one maximizing partial similarity is selected. 3.3 Distributional Features The distribution of topics over documents (in our case, short texts) may contribute to model Semantic Similarity since there is no notion of mutual exclusivity that restricts words to be part of one topic only. This allows topic models to capture pol</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of the 14th International Joint Conference on Artificial Intelligence - Volume 1, IJCAI’95, pages 448–453, San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Christopher Buckley</author>
</authors>
<title>Termweighting approaches in automatic text retrieval.</title>
<date>1988</date>
<journal>Inf. Process. Manage.,</journal>
<volume>24</volume>
<issue>5</issue>
<contexts>
<context position="11363" citStr="Salton and Buckley, 1988" startWordPosition="1754" endWordPosition="1757">del Semantic Similarity since there is no notion of mutual exclusivity that restricts words to be part of one topic only. This allows topic models to capture polysemy. We may thus see topics as natural word sense contexts, as words occur in different topics with distinct “senses”. Gensim (ˇReh˚uˇrek and Sojka, 2010) is a machine learning framework for topic modeling. It includes several pre-processing techniques, such as stop-word removal and TF-IDF, a standard statistical method that combines the frequency of a term in a particular document with its inverse document frequency in general use (Salton and Buckley, 1988). This score is high for rare terms that occur frequently in a document and are therefore more likely to be significant. Gensim computes a distribution of 25 topics over texts with or without using TF-IDF (f17...41). Each feature is the absolute difference of topici (i.e. topic[i] = |topic[i]s1 − topic[i]s2|). The euclidean distance over the difference of topic distribution between text pairs was used as another feature (f42). 3.4 Supervised Learning WEKA (Hall et al., 2009) is a large collection of machine learning algorithms, written in Java, used for learning our STS function from aforement</context>
</contexts>
<marker>Salton, Buckley, 1988</marker>
<rawString>Gerard Salton and Christopher Buckley. 1988. Termweighting approaches in automatic text retrieval. Inf. Process. Manage., 24(5):513–523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander K Seewald</author>
</authors>
<title>How to make stacking better and faster while also taking care of an unknown weakness.</title>
<date>2002</date>
<booktitle>Nineteenth International Conference on Machine Learning,</booktitle>
<pages>554--561</pages>
<editor>In C. Sammut and A. Hoffmann, editors,</editor>
<contexts>
<context position="12632" citStr="Seewald, 2002" startWordPosition="1958" endWordPosition="1959">adopted for building classifier ensembles, each focused on a different level of action. Approach A concerns the different ways of combining the results from the classifiers. Approach B uses different models.At feature level (Approach C), different feature subsets can be used for the classifiers, either if they use the same classification model or not. Finally, datasets can be modified so that each classifier in the ensemble is trained on its own dataset (Approach D) (Kuncheva and Whitaker, 2003). Different methods where applied such as Voting (Franke and Mandler, 1992) (Approach A), Stacking (Seewald, 2002) (Approach B), and variation of the feature subset used (Approach C). Voting is perhaps a simpler approach, as it selects the class with the largest number of votes. Stacking is used to combine different types of classifiers and demands the use of another learning algorithm to predict which of the models would be the most reliable for each case. This is done with a meta-learner, another learning scheme that combines the output of the base learners. The predictions of base learners are used as input to the meta-learner. We used WEKA’s “Stacking” (Wolpert, 1992) meta-classifier in our first run,</context>
</contexts>
<marker>Seewald, 2002</marker>
<rawString>Alexander K. Seewald. 2002. How to make stacking better and faster while also taking care of an unknown weakness. In C. Sammut and A. Hoffmann, editors, Nineteenth International Conference on Machine Learning, pages 554–561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yong Wang</author>
<author>Ian H Witten</author>
</authors>
<title>Induction of model trees for predicting continuous classes.</title>
<date>1997</date>
<booktitle>In Poster papers of the 9th European Conference on Machine Learning.</booktitle>
<contexts>
<context position="13662" citStr="Wang and Witten, 1997" startWordPosition="2137" endWordPosition="2140">at combines the output of the base learners. The predictions of base learners are used as input to the meta-learner. We used WEKA’s “Stacking” (Wolpert, 1992) meta-classifier in our first run, combining the following base models: three K-Nearest Neighbour (KNN) classifiers (K = 1, K = 3, K = 5) (Aha et al., 1991); a Linear Regression model without an attribute selection method (−S1) and default ridge parameter (1.0−8); three M5P classifiers which implement base routines for generating M5 Model trees and rules with a different minimum number of instances (M = 4, M = 10, M = 20) (Quinlan, 1992; Wang and Witten, 1997). The meta-classifier was a M5P classifier with M = 4. Other ensembles were added for the second and third runs: 1. Stacking combining three base models: KNN classifier (K = 1); Linear Regression model without an attribute selection method (−S1) and default ridge parameter (1.0−8); M5P, with M = 4, being the meta-classifier6. 2. Stacking combining four base models: KNN classifier (K = 1); Linear Regression model without an attribute selection method (−S1) 6A Regression Tree using the M5 algorithm (Quinlan, 1992) and default ridge parameter (1.0−8); ZeroR, a simple rule-based classifier which d</context>
</contexts>
<marker>Wang, Witten, 1997</marker>
<rawString>Yong Wang and Ian H. Witten. 1997. Induction of model trees for predicting continuous classes. In Poster papers of the 9th European Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David H Wolpert</author>
</authors>
<title>Stacked generalization.</title>
<date>1992</date>
<journal>Neural Networks,</journal>
<pages>5--241</pages>
<contexts>
<context position="13198" citStr="Wolpert, 1992" startWordPosition="2055" endWordPosition="2056">er, 1992) (Approach A), Stacking (Seewald, 2002) (Approach B), and variation of the feature subset used (Approach C). Voting is perhaps a simpler approach, as it selects the class with the largest number of votes. Stacking is used to combine different types of classifiers and demands the use of another learning algorithm to predict which of the models would be the most reliable for each case. This is done with a meta-learner, another learning scheme that combines the output of the base learners. The predictions of base learners are used as input to the meta-learner. We used WEKA’s “Stacking” (Wolpert, 1992) meta-classifier in our first run, combining the following base models: three K-Nearest Neighbour (KNN) classifiers (K = 1, K = 3, K = 5) (Aha et al., 1991); a Linear Regression model without an attribute selection method (−S1) and default ridge parameter (1.0−8); three M5P classifiers which implement base routines for generating M5 Model trees and rules with a different minimum number of instances (M = 4, M = 10, M = 20) (Quinlan, 1992; Wang and Witten, 1997). The meta-classifier was a M5P classifier with M = 4. Other ensembles were added for the second and third runs: 1. Stacking combining t</context>
</contexts>
<marker>Wolpert, 1992</marker>
<rawString>David H. Wolpert. 1992. Stacked generalization. Neural Networks, 5:241–259.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>