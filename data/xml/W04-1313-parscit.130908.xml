<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000021">
<title confidence="0.996356">
Combining Utterance-Boundary and Predictability Approaches to
Speech Segmentation
</title>
<author confidence="0.960977">
Aris XANTHOS
</author>
<affiliation confidence="0.970786">
Linguistics Department, University of Lausanne
</affiliation>
<address confidence="0.806725666666667">
UNIL - BFSH2
1015 Lausanne
Switzerland
</address>
<email confidence="0.994648">
aris.xanthos@ling.unil.ch
</email>
<sectionHeader confidence="0.993807" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999659058823529">
This paper investigates two approaches to
speech segmentation based on different heuris-
tics: the utterance-boundary strategy, and the
predictability strategy. On the basis of for-
mer empirical results as well as theoretical con-
siderations, it is suggested that the utterance-
boundary approach could be used as a prepro-
cessing step in order to lighten the task of the
predictability approach, without damaging the
resulting segmentation. This intuition leads to
the formulation of an explicit model, which is
empirically evaluated for a task of word segmen-
tation on a child-oriented phonemically tran-
scribed French corpus. The results show that
the hybrid algorithm outperforms its compo-
nent parts while reducing the total memory load
involved.
</bodyText>
<sectionHeader confidence="0.998986" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998165757575758">
The design of speech segmentations methods
has been much studied ever since Harris’ sem-
inal propositions (1955). Research conducted
since the mid 1990’s by cognitive scientists
(Brent and Cartwright, 1996; Saffran et al.,
1996) has established it as a paradigm of its own
in the field of computational models of language
acquisition.
In this paper, we investigate two boundary-
based approaches to speech segmentation. Such
methods “attempt to identify individual word-
boundaries in the input, without reference to
words per se” (Brent and Cartwright, 1996).
The first approach we discuss relies on the
utterance-boundary strategy, which consists in
reusing the information provided by the occur-
rence of specific phoneme sequences at utter-
ance beginnings or endings in order to hypoth-
&apos;To avoid a latent ambiguity, it should be stated that
speech segmentation refers here to a process taking as in-
put a sequence of symbols (usually phonemes) and pro-
ducing as output a sequence of higher-level units (usually
words).
esize boundaries inside utterances (Aslin et al.,
1996; Christiansen et al., 1998; Xanthos, 2004).
The second approach is based on the predictabil-
ity strategy, which assumes that speech should
be segmented at locations where some mea-
sure of the uncertainty about the next symbol
(phoneme or syllable for instance) is high (Har-
ris, 1955; Gammon, 1969; Saffran et al., 1996;
Hutchens and Adler, 1998; Xanthos, 2003).
Our implementation of the utterance-
boundary strategy is based on n-grams
statistics. It was previously found to perform
a “safe” word segmentation, that is with a
rather high precision, but also too conser-
vative as witnessed by a not so high recall
(Xanthos, 2004). As regards the predictability
strategy, we have implemented an incremental
interpretation of the classical successor count
(Harris, 1955). This approach also relies on the
observation of phoneme sequences, the length
of which is however not restricted to a fixed
value. Consequently, the memory load involved
by the successor count algorithm is expected
to be higher than for the utterance-boundary
approach, and its performance substantially
better.
The experiments presented in this paper were
inspired by the intuition that both algorithms
could be combined in order to make the most
of their respective strengths. The utterance-
boundary typicality could be used as a compu-
tationally inexpensive preprocessing step, find-
ing some true boundaries without inducing too
many false alarms; then, the heavier machinery
of the successor count would be used to accu-
rately detect more boundaries, its burden be-
ing lessened as it would process the chunks pro-
duced by the first algorithm rather than whole
utterances. We will show the results obtained
for a word segmentation task on a phonetically
transcribed and child-oriented French corpus,
focusing on the effect of the preprocessing step
on precision and recall, as well as its impact on
</bodyText>
<page confidence="0.997447">
93
</page>
<bodyText confidence="0.999697333333333">
memory load and processing time.
The next section is devoted to the formal def-
inition of both algorithms. Section 3 discusses
some issues related to the space and time com-
plexity they involve. The experimental setup
as well as the results of the simulations are de-
scribed in section 4, and in conclusion we will
summarize our findings and suggest directions
for further research.
</bodyText>
<sectionHeader confidence="0.559134" genericHeader="method">
2 Description of the algorithms
</sectionHeader>
<subsectionHeader confidence="0.998476">
2.1 Segmentation by thresholding
</subsectionHeader>
<bodyText confidence="0.998467027777778">
Many distributional segmentation algorithms
described in the literature can be seen as in-
stances of the following abstract procedure
(Harris, 1955; Gammon, 1969; Saffran et al.,
1996; Hutchens and Adler, 1998; Bavaud and
Xanthos, 2002). Let S be the set of phonemes
(or segments) in a given language. In the most
general case, the input of the algorithm is an
utterance of length l, that is a sequence of l
phonemes u := s1 ... sl (where si denotes the
i-th phoneme of u). Then, for 1 &lt; i &lt; l − 1, we
insert a boundary after si iff D(u, i) &gt; T(u, i),
where the values of the decision variable D(u, i)
and of the threshold T (u, i) may depend on both
the whole sequence and the actual position ex-
amined (Xanthos, 2003).
The output of such algorithms can be evalu-
ated in reference to the segmentation performed
by a human expert, using traditional measures
from the signal detection framework. It is usual
to give evaluations both for word and boundary
detection (Batchelder, 2002). The word preci-
sion is the probability for a word isolated by
the segmentation procedure to be present in
the reference segmentation, and the word recall
is the probability for a word occurring in the
true segmentation to be correctly isolated. Sim-
ilarly, the segmentation precision is the proba-
bility that an inferred boundary actually occurs
in the true segmentation, and the segmentation
recall is the probability for a true boundary to
be detected.
In the remaining of this section, we will use
this framework to show how the two algorithms
we investigate rely on different definitions of
D(u, i) and T(u, i).
</bodyText>
<subsectionHeader confidence="0.998003">
2.2 Frequency estimates
</subsectionHeader>
<bodyText confidence="0.990542272727273">
Let U C_ S* be the set of possible utterances in
the language under examination. Suppose we
are given a corpus C C_ UT made of T successive
utterances.
The absolute frequency of an n-gram w E Sn
in the corpus is given by n(w) := Tt=1 nt(w)
where nt(w) denotes the absolute frequency of w
in the t-th utterance of C. In the same way, we
define the absolute frequency of w in utterance-
initial position as n(w|I) := Tt=1 nt(w|I) where
nt(w|I) denotes the absolute frequency of w in
utterance-initial position in the t-th utterance
of C (which is 1 iff the utterance begins with
w and 0 otherwise). Similarly, the absolute fre-
quency of w in utterance-final position is given
by n(w|F) := Tt=1 nt(w|F).
Accordingly, the relative frequency of w ob-
tains as f(w) := n(w)/ ˜wESn n( ˜w). Its
relative frequencies in utterance-initial and
-final position respectively are given by
f(w|I) := n(w|I)/ ˜wESn n( ˜w|I) and f(w|F) :=
n(w|F)/ ˜wESn n(˜w|F) 2
</bodyText>
<subsectionHeader confidence="0.998575">
2.3 Utterance-boundary typicality
</subsectionHeader>
<bodyText confidence="0.949237303030303">
We use the same implementation of the
utterance-boundary strategy that is described
in more details by Xanthos (2004). Intuitively,
the idea is to segment utterances where se-
quences occur, which are typical of utterance
boundaries. Of course, this implies that the cor-
pus is segmented in utterances, which seems a
reasonable assumption as far as language acqui-
sition is concerned. In this sense, the utterance-
boundary strategy may be viewed as a kind of
learning by generalization.
Probability theory provides us with a
straightforward way of evaluating how much an
n-gram w E Sn is typical of utterance end-
ings. Namely, we know that events “occur-
rence of n-gram w” and “occurrence of an n-
gram in utterance-final position” are indepen-
dent iff p(w fl F) = p(w)p(F) or equivalently
iff p(w|F) = p(w). Thus, using maximum-
likelihood estimates, we may define the typical-
2Note that in general, ˜wESn n( ˜w|F) _
˜wESn n( ˜w|I) _ T˜, where T˜ &lt; T is the number
of utterances in C that have a length greater than or
equal to n.
.
Both algorithms described below process the
input incrementally, one utterance after an-
other. This implies that the frequency measures
defined in this section are in fact evolving all
along the processing of the corpus. In general,
for a given input utterance, we chose to update
n-gram frequencies first (over the whole utter-
ance) before performing the segmentation.
</bodyText>
<page confidence="0.995392">
94
</page>
<bodyText confidence="0.704292">
ity of w in utterance-inal position as:
</bodyText>
<equation confidence="0.991884">
t(w|F) := f(w|F) (1)
f(w)
</equation>
<bodyText confidence="0.9988294">
This measure is higher than 1 iff w is more likely
to occur in utterance-final position (than in any
position), lower iff it is less likely to occur there,
and equal to 1 iff its probability is independent
of its position.
In the context of a segmentation procedure,
this suggest a “natural” constant threshold
T(u, i) := 1 (which can optionally be fine-tuned
in order to obtain a more or less conservative
result). Regarding the decision variable, if we
were dealing with an utterance u of infinite
length, we could simply set the order r &gt; 1
of the typicality computation and define d(u, i)
as t(si−(r−1) ... si|F) (where si denotes the i-
th phoneme of u). Since the algorithm is more
likely to process an utterance of finite length
l, there is a problem when considering a po-
tential boundary close to the beginning of the
utterance, in particular when r &gt; i. In this
case, we can compute the typicality of smaller
sequences, thus defining the decision variable as
t(si−(˜r−1) ... si|F), where r˜ := min(r, i).
As was already suggested by Harris (1955),
our implementation actually combines the typ-
icality in utterance-inal position with its ana-
logue in utterance-initial position. This is done
by taking the average of both statistics, and
we have found empirically efficient to weight it
by the relative lengths of the conditioning se-
quences:
</bodyText>
<equation confidence="0.95912125">
si+1 ... si+˜r/ E S˜r,
h w := si−
, r˜ := min(r, i) and ˜r :=
S˜r _
</equation>
<bodyText confidence="0.99880425">
min(r, l − i). This definition helps compensate
for the asymmetry of arguments when i is either
close to 1 or close to l.
Finally, in the simulations below, we ap-
ply a mechanism that consists in incrementing
n(w|F) and n(w|I) (by one) whenever D(u, i) &gt;
T (u, i). The aim of this is to enable the dis-
covery of new utterance-boundary typical se-
quences. It was found to considerably raise the
recall as more utterances are processed, at the
cost of a slight reduction in precision (Xanthos,
2004).
</bodyText>
<subsectionHeader confidence="0.997754">
2.4 Successor count
</subsectionHeader>
<bodyText confidence="0.999918285714286">
The second algorithm we investigate in this pa-
per is an implementation of Harris’ successor
count (Harris, 1955), the historical source of
all predictability-based approaches to segmen-
tation. It relies on the assumption that in gen-
eral, the diversity of possible phonemes tran-
sitions is high after a word boundary and de-
creases as we consider transitions occurring fur-
ther inside a word.
The diversity of transitions following an n-
gram w E Sn is evaluated by the successor
count (or successor variety), simply defined as
the number of different phonemes that can oc-
cur after it:
</bodyText>
<equation confidence="0.995834">
succ(w) := |fs E S|n(ws) &gt; 0} |(3)
</equation>
<bodyText confidence="0.9893745">
Transposing the indications of Harris in the
terms of section 2.1, for an utterance u :=
</bodyText>
<equation confidence="0.8502434">
s1 ... sl, we define D(u, i) as succ(w) where
w := s1 ... si, and T(u, i) as max[D(u, i −
1), D(u, i + 1)]. Here again a “backward” mea-
sure can be defined, the predecessor count:
predec(w) := |fs E S|n(sw) &gt; 0} |(4)
</equation>
<bodyText confidence="0.993347434782609">
Accordingly, we have D(u, i) = predec(w)
where w := si+1 ... sl, and T(u, i) :=
max[D(u, i − 1), D(u, i + 1)]. In order to com-
bine both statistics, we have found efficient to
use a composite decision rule, where a boundary
is inserted after phoneme si iff D(u, i) &gt; T(u, i)
or D(u, i) &gt; T(u, i).
These decision variables differ from those
used in the utterance-boundary approach in
that there is no fixed bound on the length of
their arguments. As will be discussed in sec-
tion 3, this has important consequences for the
complexity of the algorithm. Also, the thresh-
old used for the successor count depends ex-
plicitely on both u and i: rather than seek-
ing values higher than a given threshold, this
method looks for peaks of the decision variable
monitored over the input, whether the actual
value is high or not. This is a more or less ar-
bitrary feature of this class of algorithms, and
much work remains to be done in order to pro-
vide theoretical justifications rather than mere
empirical evaluations.
</bodyText>
<sectionHeader confidence="0.994323" genericHeader="method">
3 Complexity issues
</sectionHeader>
<bodyText confidence="0.999618333333333">
It is not easy to evaluate the complexity of the
algorithms discussed in this paper, which con-
sist mainly in the space and time needed to store
</bodyText>
<equation confidence="0.9991236">
D(u, i) :=
r˜ t(w|F) +
r˜ + ˜r t(w|I) (2)
˜r
r˜+ ˜r
</equation>
<page confidence="0.961311">
95
</page>
<bodyText confidence="0.999875865384616">
and retrieve the necessary information for the
computation of n-grams frequencies. Of course,
this depends much on the actual implementa-
tion. For instance, in a rather naive approach,
utterances can be stored as such and the mem-
ory load is then roughly equivalent to the size of
the corpus, but computing the frequency of an
n-gram requires scanning the whole memory.
A first optimization is to count utterances
rather than merely store them. Some program-
ming languages have a very convenient and effi-
cient built-in data structure for storing elements
indexed by a string3, such as the frequency as-
sociated with an utterance. However, the actual
gain depends on the redundancy of the corpus at
utterances level, and even in an acquisition cor-
pus, many utterances occur only once. The time
needed to compute the frequency of an n-gram
is reduced accordingly, and due to the average
efficiency of hash coding, the time involved by
the storage of an utterance is approximately as
low as in the naive case above.
It is possible to store not only the frequency
of utterances, but also that of their subparts. In
this approach, storing an n-gram and retrieving
its frequency need comparable time resources,
expected to be low if hashing is performed. Of
course, from the point of view of memory load,
this is much more expensive than the two pre-
vious implementations discussed. However, we
can take advantage of the fact that in an utter-
ance of length l, every n-gram w with 1 &lt; n &lt; l
is the prefix and/or suffix of at least an n + 1-
gram w&apos;. Thus, it is much more compact to
store them in a directed tree, the root of which
is the empty string, and where each node corre-
sponds to a phoneme in a given context4, and
each child of a node to a possible successor of
that phoneme in its context. The frequency of
an n-gram can be stored in a special child of the
node representing the terminal phoneme of the
n-gram.
This implementation (tree storage) will be
used in the simulations described below. It is
not claimed to be more psychologically plausible
than another, but we believe the size in nodes
of the trees built for a given corpus provides
an intuitive and accurate way of comparing the
memory requirements of the algorithms we dis-
cuss. From the point of view of time complexity,
however, the tree structure is less optimal than
a flat hash table since the time needed for the
</bodyText>
<footnote confidence="0.9301005">
3This type of storage is known as hash coding.
4defined by the whole sequence of its parent nodes
</footnote>
<bodyText confidence="0.9908165">
storage or retrieval of an n-gram grows linearly
with n.
</bodyText>
<sectionHeader confidence="0.989298" genericHeader="method">
4 Empirical evaluation
</sectionHeader>
<subsectionHeader confidence="0.964141">
4.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.99999409375">
Both algorithms described above were imple-
mented implemented in Perl5 and evaluated
using a phonemically transcribed and child-
oriented French corpus (Kilani-Schoch corpus6).
We have extracted from the original corpus
all the utterances of Sophie’s parents (mainly
her mother) between ages 1;6.14 and 2;6.25
(year;month.day). These were transcribed
phonemically in a semi-automatic fashion, using
the BRULEX database (Content et al., 1990)
and making the result closer to oral French
with a few hand-crafted rules. Eventually the
first 10’000 utterances were used for simula-
tions. This corresponds to 37’663 words (992
types) and 103’325 phonemes (39 types).
In general, we will compare the results ob-
served for the successor count used on its own
(“SC alone”, on the figures) with those obtained
when the utterance-boundary typicality is used
for preprocessing7. The latter were recorded for
1 &lt; r &lt; 5, where r is the order for the com-
putation of typicalities. The threshold value for
typicality was set to 1 (see section 2.3). The
results of the algorithms for word segmenta-
tion were evaluated by comparing their output
to the segmentation given in the original tran-
scription using precision and recall for word and
boundary detection (computed over the whole
corpus). The memory load is measured by the
number of nodes in the trees built by each al-
gorithm, and the processing time is the number
of seconds needed to process the whole corpus.
</bodyText>
<subsectionHeader confidence="0.999268">
4.2 Segmentation performance
</subsectionHeader>
<bodyText confidence="0.908573125">
When used in isolation, our implementation of
the successor count has a segmentation preci-
sion as high as 82.5%, with a recall of 50.5%;
the word precision and recall are 57% and 40.8%
5Perl was chosen here because of the ease it provides
when it comes to textual statistics; however, execution is
notoriously slower than with C or C++, and this should
be kept in mind when interpreting the large differences
in processing time reported in section 4.4.
6Sophie, a French speaking Swiss child, was recorded
at home by her mother every ten days in situations of
play (Kilani-Schoch and Dressler, 2001). The transcrip-
tion and coding were done according to CHILDES con-
ventions (MacWhinney, 2000).
7Results of the utterance-boundary approach alone
are given in (Xanthos, 2004)
</bodyText>
<page confidence="0.989787">
96
</page>
<figure confidence="0.396994">
Segmentation precision and recall
</figure>
<figureCaption confidence="0.916239">
Figure 1: Segmentation precision and recall ob-
tained with the successor count alone and with
utterance-boundary preprocessing on n-grams.
Word precision and recall
Figure 2: Word precision and recall ob-
tained with the successor count alone and with
utterance-boundary preprocessing on n-grams.
</figureCaption>
<figure confidence="0.995825966666667">
90%
80%
70%
60%
50%
40%
30%
20%
10%
0%
1 2 3 4 5
sc
alone
r
prec
rec
70%
60%
50%
40%
30%
20%
10%
0%
1 2 3 4 5
sc
alone
r
prec
rec
</figure>
<bodyText confidence="0.9999808">
respectively. For comparison, the highest seg-
mentation precision obtained with utterance-
boundary typicality alone is 80.8% (for r = 5),
but the corresponding recall does not exceed
37.6%, and the highest word precision is 44.4%
(r = 4) with a word recall of 31.4%. As ex-
pected, the successor count performs much bet-
ter than the utterance boundary typicality in
isolation.
Using utterance-boundary typicality as a pre-
processing step has a remarkable impact on the
performance of the resulting algorithm. Figure
1 shows the segmentation performance obtained
for boundary detection with the successor count
alone or in combination with preprocessing (for
1 G r G 5). The segmentation precision is al-
ways lower with preprocessing, but the differ-
ence dwindles as r grows: for r = 5, it reaches
79.9%, so only 2.1% are lost. On the contrary,
the segmentation recall is always higher with
preprocessing. It reaches a peak of 79.3% for
r = 3, and stays as high as 71.2% for r = 5 ,
meaning a 20.7% difference with the successor
count alone.
Concerning the detection of whole words, (fig-
ure 2), the word precision is strictly increasing
with r and ranges between 15.2% and 60.2%,
the latter being a 3.2% increase with regard
to the successor count alone. The word recall
is lower when preprocessing is performed with
n = 1 (-18.2%), but higher in all other cases,
with a peak of 56% for n = 4 (+15.2%).
Overall, we can say the segmentation perfor-
mance exhibited by our hybrid algorithm con-
firms our expectations regarding the comple-
mentarity of the two strategies examined: their
combination is clearly superior to each of them
taken independently. There may be a slight loss
in precision, but it is massively counterbalanced
by the gain in recall.
</bodyText>
<subsectionHeader confidence="0.999783">
4.3 Memory load
</subsectionHeader>
<bodyText confidence="0.999592190476191">
The second hypothesis we made was that the
preprocessing step would reduce the memory
load of the successor count algorithm. In our
implementation, the space used by each algo-
rithm can be measured by the number of nodes
of the trees storing the distributions. Five dis-
tinct trees are involved: three for the utterance-
boundary approach (one for the distribution of
n-grams in general and two for their distribu-
tions in utterance-initial and -final position),
and two for the predictability approach (one
for successors and one for predecessors). The
memory load of each algorithm is obtained by
summation of these values.
As can be seen on figure 3, the size of the trees
built by the successor count is drastically re-
duced by preprocessing. Successor count alone
uses as many as 99’405 nodes; after preprocess-
ing, the figures range between 7’965 for n = 1
and 38’786 for n = 5 (SC, on the figure)$. How-
ever, the additional space used by the n-grams
</bodyText>
<footnote confidence="0.485341666666667">
8These values are highly negatively correlated with
the number of boundaries–true or false–inserted by pre-
processing (r = −0.96).
</footnote>
<page confidence="0.997169">
97
</page>
<figure confidence="0.999003628571428">
Memory load Processing time
110
90
100
80
90
70
80
60
70
50
60
50
40
40
30
30
20
20
10
10
0
0
r
r
sc
alone
sc
alone
1 2 3 4 5
1 2 3 4 5
UBT
sc
UBT
sc
</figure>
<figureCaption confidence="0.9823975">
Figure 3: Memory load (in thousands of nodes)
measured with the successor count alone and
with utterance-boundary preprocessing on n-
grams (see text).
</figureCaption>
<bodyText confidence="0.997961">
distributions needed to compute the utterance-
boundary typicality (UBT) grows quickly with
n, and the total number of nodes even exceeds
that of the successor count alone when n = 5.
Still, for lower values of n, preprocessing leads
to a substantial reduction in total memory load.
</bodyText>
<subsectionHeader confidence="0.999952">
4.4 Processing time
</subsectionHeader>
<bodyText confidence="0.999452181818182">
It seems unlikely that the combination of the
two algorithms does not exhibit any drawback.
We have said in section 3 that storing distribu-
tions in a tree was not optimal from the point
of view of time complexity, so we did not have
high expectations on this topic. Nevertheless,
we recorded the time used by the algorithms
for the sake of completeness. CPU times was
measured in seconds, using built-in functions of
Perl, and the durations we report were averaged
over 10 runs of the simulation10.
</bodyText>
<subsectionHeader confidence="0.88482">
What can be seen on figure 4 is that although
</subsectionHeader>
<bodyText confidence="0.999166625">
the time used by the successor count computa-
tion is slightly reduced by preprocessing, this
does not compensate for the additional time re-
quired by the preprocessing itself. On average,
the total time is multiplied by 1.6 when pre-
processing is performed. Again, this is really a
consequence of the chosen implementation, as
this factor could be reduced to 1.15 by storing
</bodyText>
<footnote confidence="0.74521625">
9on a pentium III 700MHz
10This does not give a very accurate evaluation of pro-
cessing time, and we plan to express it in terms of num-
ber of computational steps.
</footnote>
<figureCaption confidence="0.989565">
Figure 4: Processing time (in seconds) mea-
sured with the successor count alone and with
utterance-boundary preprocessing on n-grams.
</figureCaption>
<bodyText confidence="0.927951">
distributions in flat hash tables rather than tree
structures.
</bodyText>
<sectionHeader confidence="0.995796" genericHeader="conclusions">
5 Conclusions and discussion
</sectionHeader>
<bodyText confidence="0.9986418125">
In this paper, we have investigated two ap-
proaches to speech segmentation based on dif-
ferent heuristics: the utterance-boundary strat-
egy, and the predictability strategy. On the ba-
sis of former empirical results as well as theoret-
ical considerations regarding their performance
and complexity, we have suggested that the
utterance-boundary approach could be used as
a preprocessing step in order to lighten the task
of the predictability approach without damag-
ing the segmentation.
This intuition was translated into an explicit
model, then implemented and evaluated for a
task of word segmentation on a child-oriented
phonetically transcribed french corpus. Our re-
sults show that:
</bodyText>
<listItem confidence="0.923065555555555">
• the combined algorithm outperforms its
component parts considered separately;
• the total memory load of the combined al-
gorithm can be substantially reduced by
the preprocessing step;
• however, the processing time of the com-
bined algorithm is generally longer and
possibly much longer depending on the im-
plementation.
</listItem>
<bodyText confidence="0.643882666666667">
These findings are in line with recent research
advocating the integration of various strate-
gies for speech segmentation. In his work on
</bodyText>
<page confidence="0.987661">
98
</page>
<figure confidence="0.994457545454546">
Average successor count
40
35
30
25
20
15
10
5
0 n
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
</figure>
<figureCaption confidence="0.971763">
Figure 5: Average successor count for n-grams
(based on the corpus described in section 4.1).
</figureCaption>
<bodyText confidence="0.999436833333333">
computational morphology, Goldsmith (2001)
uses Harris’ successor count as a means to re-
duce the search space of a more powerful al-
gorithm based on minimum description length
(Marcken, 1996). We go one step further and
show that an utterance-boundary heuristic can
be used in order to reduce the complexity of the
successor count algorithm11.
Besides complexity issues, there is a prob-
lem of data sparseness with the successor count,
as it decreases very quickly while the size n of
the context grows. In the case of our quite re-
dundant child-oriented corpus, the (weighted)
average of the successor count12 for a random
n-gram EwCSn f(w) succ(w) gets lower than 1
for n &gt; 9 (see figure 5). This means that in
most utterances, no more boundary can be in-
serted after the first 9 phonemes (respectively
before the last 9 phonemes) unless we get close
enough to the other extremity of the utter-
ance for the predecessor (respectively successor)
count to operate. As regards the utterance-
boundary typicality, on the other hand, the po-
sition in the utterance makes no difference. As
a consequence, many examples can be found in
our corpus, where the middle part of a long ut-
terance would be undersegmented by the succes-
sor count alone, whereas preprocessing provides
it with more tractable chunks. This is illus-
trated by the following segmentations of the ut-
</bodyText>
<footnote confidence="0.68166775">
terance (Daddy doesn’t
11at least as regards memory load, which could more
restrictive in a developmental perspective
12The predecessor count behaves much the same.
</footnote>
<bodyText confidence="0.997799">
like carrots), where vertical bars denote bound-
aries predicted by the utterance-boundary typ-
icality (for r = 5), and dashes represent bound-
aries inferred by the successor count:
</bodyText>
<table confidence="0.521424333333333">
SC
UBT (r = 5)
UBT + SC
</table>
<bodyText confidence="0.999596576923077">
This suggests that the utterance-boundary
strategy could be more than an additional de-
vice that safely predicts some boundaries that
the successor count alone might have found or
not: it could actually have a functional rela-
tionship with it. If the predictability strategy
has some relevance for speech segmentation in
early infancy (Saffran et al., 1996), then it may
be necessary to counterbalance the data sparse-
ness; this is what these authors implicitely do
by using first-order transition probabilities, and
it would be easy to define an n-th order succes-
sor count in the same way. Yet another possi-
bility would be to “reset” the successor count
after each boundary inserted. Further research
should bring computational and psychological
evidence for or against such ways to address rep-
resentativity issues.
We conclude this paper by raising an issue
that was already discussed by Gammon (1969),
and might well be tackled with our methodol-
ogy. It seems that various segmentation strate-
gies correlate more or less with different segmen-
tation levels. We wonder if these different kinds
of sensitivity could be used to make inferences
about the hierarchical structure of utterances.
</bodyText>
<sectionHeader confidence="0.999457" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999965">
The author is grateful to Marianne Kilani-
Schoch and the mother of Sophie for providing
the acquisition corpus (see p.4), as well as to
Fran¸cois Bavaud, Marianne Kilani-Schoch and
two anonymous reviewers for useful comments
on earlier versions of this paper.
</bodyText>
<sectionHeader confidence="0.999465" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999456909090909">
R.N. Aslin, J.Z. Woodward, N.P. Lamendola,
and T.G. Bever. 1996. Models of word seg-
mentation in fluent maternal speech to in-
fants. In J.L Morgan and Demuth K., ed-
itors, Signal to Syntax: Bootstrapping from
Speech to Grammar in Early Language Ac-
quisition, pages 117–134. Lawrence Erlbaum
Associates, Mahwah (NJ).
E. Batchelder. 2002. Bootstrapping the lexi-
con: A computational model of infant speech
segmentation. Cognition, 83:167–206.
</reference>
<page confidence="0.978669">
99
</page>
<reference confidence="0.987536388888889">
F. Bavaud and A. Xanthos. 2002. Thermody-
namique et statistique textuelle: concepts et
illustrations. In Actes des 6`e Journ´ees Inter-
nationales d’Analyse Statistique des Donn´ees
Textuelles (JADT 2002), pages 101–111.
M.R. Brent and T.A. Cartwright. 1996. Distri-
butional regularity and phonotactics are use-
ful for segmentation. Cognition, 61:93–125.
M.H. Christiansen, J. Allen, and M. Seidenberg.
1998. Learning to segment speech using mul-
tiple cues. Language and Cognitive Processes,
13:221–268.
A. Content, P. Mousty, and M. Radeau. 1990.
Brulex: Une base de donn´ees lexicales in-
formatis´ee pour le fran¸cais ´ecrit et parl´e.
L’Ann´ee Psychologique, 90:551–566.
E. Gammon. 1969. Quantitative approxima-
tions to the word. In Papers presented to the
International Conference on Computational
Linguistics COLING-69.
J. Goldsmith. 2001. Unsupervised learning of
the morphology of a natural language. Com-
putational Linguistics, 27 (2):153–198.
Z.S. Harris. 1955. From phoneme to morpheme.
Language, 31:190–222.
J.L. Hutchens and M.D. Adler. 1998. Find-
ing structure via compression. In Proceedings
of the International Conference on Computa-
tional Natural Language Learning, pages 79–
82.
M. Kilani-Schoch and W.U. Dressler. 2001.
Filler + infinitive and pre- and protomorphol-
ogy demarcation in a french acquisition cor-
pus. Journal of Psycholinguistic Research, 30
(6):653–685.
B. MacWhinney. 2000. The CHILDES Project:
Tools for Analyzing Talk. Third Edition.
Lawrence Erlbaum Associates, Mahwah (NJ).
C.G. de Marcken. 1996. Unsupervised Language
Acquisition. Phd dissertation, Massachusetts
Institute of Technology.
J.R. Saffran, E.L. Newport, and R.N. Aslin.
1996. Word segmentation: The role of distri-
butional cues. Journal of Memory and Lan-
guage, 35:606–621.
A. Xanthos. 2003. Du k-gramme au mot: vari-
ation sur un th`eme distributionnaliste. Bul-
letin de linguistique et des sciences du langage
(BIL), 21.
A. Xanthos. 2004. An incremental implemen-
tation of the utterance-boundary approach to
speech segmentation. To appear in the Pro-
ceedings of Computational Linguistics in the
Netherlands 2003 (CLIN 2003).
</reference>
<page confidence="0.990548">
100
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.247527">
<title confidence="0.997298">Combining Utterance-Boundary and Predictability Approaches to Speech Segmentation</title>
<author confidence="0.338267">Aris</author>
<affiliation confidence="0.758659">Linguistics Department, University of UNIL -</affiliation>
<address confidence="0.811876">1015</address>
<email confidence="0.910119">aris.xanthos@ling.unil.ch</email>
<abstract confidence="0.994180055555555">This paper investigates two approaches to speech segmentation based on different heuristics: the utterance-boundary strategy, and the predictability strategy. On the basis of former empirical results as well as theoretical considerations, it is suggested that the utteranceboundary approach could be used as a preprocessing step in order to lighten the task of the predictability approach, without damaging the resulting segmentation. This intuition leads to the formulation of an explicit model, which is empirically evaluated for a task of word segmentation on a child-oriented phonemically transcribed French corpus. The results show that the hybrid algorithm outperforms its component parts while reducing the total memory load involved.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R N Aslin</author>
<author>J Z Woodward</author>
<author>N P Lamendola</author>
<author>T G Bever</author>
</authors>
<title>Models of word segmentation in fluent maternal speech to infants.</title>
<date>1996</date>
<booktitle>Signal to Syntax: Bootstrapping from Speech to Grammar in Early Language Acquisition,</booktitle>
<pages>117--134</pages>
<editor>In J.L Morgan and Demuth K., editors,</editor>
<contexts>
<context position="2039" citStr="Aslin et al., 1996" startWordPosition="301" endWordPosition="304">fy individual wordboundaries in the input, without reference to words per se” (Brent and Cartwright, 1996). The first approach we discuss relies on the utterance-boundary strategy, which consists in reusing the information provided by the occurrence of specific phoneme sequences at utterance beginnings or endings in order to hypoth&apos;To avoid a latent ambiguity, it should be stated that speech segmentation refers here to a process taking as input a sequence of symbols (usually phonemes) and producing as output a sequence of higher-level units (usually words). esize boundaries inside utterances (Aslin et al., 1996; Christiansen et al., 1998; Xanthos, 2004). The second approach is based on the predictability strategy, which assumes that speech should be segmented at locations where some measure of the uncertainty about the next symbol (phoneme or syllable for instance) is high (Harris, 1955; Gammon, 1969; Saffran et al., 1996; Hutchens and Adler, 1998; Xanthos, 2003). Our implementation of the utteranceboundary strategy is based on n-grams statistics. It was previously found to perform a “safe” word segmentation, that is with a rather high precision, but also too conservative as witnessed by a not so hi</context>
</contexts>
<marker>Aslin, Woodward, Lamendola, Bever, 1996</marker>
<rawString>R.N. Aslin, J.Z. Woodward, N.P. Lamendola, and T.G. Bever. 1996. Models of word segmentation in fluent maternal speech to infants. In J.L Morgan and Demuth K., editors, Signal to Syntax: Bootstrapping from Speech to Grammar in Early Language Acquisition, pages 117–134. Lawrence Erlbaum Associates, Mahwah (NJ).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Batchelder</author>
</authors>
<title>Bootstrapping the lexicon: A computational model of infant speech segmentation.</title>
<date>2002</date>
<journal>Cognition,</journal>
<pages>83--167</pages>
<contexts>
<context position="5328" citStr="Batchelder, 2002" startWordPosition="848" endWordPosition="849">terance of length l, that is a sequence of l phonemes u := s1 ... sl (where si denotes the i-th phoneme of u). Then, for 1 &lt; i &lt; l − 1, we insert a boundary after si iff D(u, i) &gt; T(u, i), where the values of the decision variable D(u, i) and of the threshold T (u, i) may depend on both the whole sequence and the actual position examined (Xanthos, 2003). The output of such algorithms can be evaluated in reference to the segmentation performed by a human expert, using traditional measures from the signal detection framework. It is usual to give evaluations both for word and boundary detection (Batchelder, 2002). The word precision is the probability for a word isolated by the segmentation procedure to be present in the reference segmentation, and the word recall is the probability for a word occurring in the true segmentation to be correctly isolated. Similarly, the segmentation precision is the probability that an inferred boundary actually occurs in the true segmentation, and the segmentation recall is the probability for a true boundary to be detected. In the remaining of this section, we will use this framework to show how the two algorithms we investigate rely on different definitions of D(u, i</context>
</contexts>
<marker>Batchelder, 2002</marker>
<rawString>E. Batchelder. 2002. Bootstrapping the lexicon: A computational model of infant speech segmentation. Cognition, 83:167–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Bavaud</author>
<author>A Xanthos</author>
</authors>
<title>Thermodynamique et statistique textuelle: concepts et illustrations.</title>
<date>2002</date>
<booktitle>In Actes des 6`e Journ´ees Internationales d’Analyse Statistique des Donn´ees Textuelles (JADT</booktitle>
<pages>101--111</pages>
<contexts>
<context position="4584" citStr="Bavaud and Xanthos, 2002" startWordPosition="704" endWordPosition="707">voted to the formal definition of both algorithms. Section 3 discusses some issues related to the space and time complexity they involve. The experimental setup as well as the results of the simulations are described in section 4, and in conclusion we will summarize our findings and suggest directions for further research. 2 Description of the algorithms 2.1 Segmentation by thresholding Many distributional segmentation algorithms described in the literature can be seen as instances of the following abstract procedure (Harris, 1955; Gammon, 1969; Saffran et al., 1996; Hutchens and Adler, 1998; Bavaud and Xanthos, 2002). Let S be the set of phonemes (or segments) in a given language. In the most general case, the input of the algorithm is an utterance of length l, that is a sequence of l phonemes u := s1 ... sl (where si denotes the i-th phoneme of u). Then, for 1 &lt; i &lt; l − 1, we insert a boundary after si iff D(u, i) &gt; T(u, i), where the values of the decision variable D(u, i) and of the threshold T (u, i) may depend on both the whole sequence and the actual position examined (Xanthos, 2003). The output of such algorithms can be evaluated in reference to the segmentation performed by a human expert, using t</context>
</contexts>
<marker>Bavaud, Xanthos, 2002</marker>
<rawString>F. Bavaud and A. Xanthos. 2002. Thermodynamique et statistique textuelle: concepts et illustrations. In Actes des 6`e Journ´ees Internationales d’Analyse Statistique des Donn´ees Textuelles (JADT 2002), pages 101–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Brent</author>
<author>T A Cartwright</author>
</authors>
<title>Distributional regularity and phonotactics are useful for segmentation.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>61--93</pages>
<contexts>
<context position="1177" citStr="Brent and Cartwright, 1996" startWordPosition="164" endWordPosition="167">er to lighten the task of the predictability approach, without damaging the resulting segmentation. This intuition leads to the formulation of an explicit model, which is empirically evaluated for a task of word segmentation on a child-oriented phonemically transcribed French corpus. The results show that the hybrid algorithm outperforms its component parts while reducing the total memory load involved. 1 Introduction The design of speech segmentations methods has been much studied ever since Harris’ seminal propositions (1955). Research conducted since the mid 1990’s by cognitive scientists (Brent and Cartwright, 1996; Saffran et al., 1996) has established it as a paradigm of its own in the field of computational models of language acquisition. In this paper, we investigate two boundarybased approaches to speech segmentation. Such methods “attempt to identify individual wordboundaries in the input, without reference to words per se” (Brent and Cartwright, 1996). The first approach we discuss relies on the utterance-boundary strategy, which consists in reusing the information provided by the occurrence of specific phoneme sequences at utterance beginnings or endings in order to hypoth&apos;To avoid a latent ambi</context>
</contexts>
<marker>Brent, Cartwright, 1996</marker>
<rawString>M.R. Brent and T.A. Cartwright. 1996. Distributional regularity and phonotactics are useful for segmentation. Cognition, 61:93–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M H Christiansen</author>
<author>J Allen</author>
<author>M Seidenberg</author>
</authors>
<title>Learning to segment speech using multiple cues. Language and Cognitive Processes,</title>
<date>1998</date>
<pages>13--221</pages>
<contexts>
<context position="2066" citStr="Christiansen et al., 1998" startWordPosition="305" endWordPosition="308">undaries in the input, without reference to words per se” (Brent and Cartwright, 1996). The first approach we discuss relies on the utterance-boundary strategy, which consists in reusing the information provided by the occurrence of specific phoneme sequences at utterance beginnings or endings in order to hypoth&apos;To avoid a latent ambiguity, it should be stated that speech segmentation refers here to a process taking as input a sequence of symbols (usually phonemes) and producing as output a sequence of higher-level units (usually words). esize boundaries inside utterances (Aslin et al., 1996; Christiansen et al., 1998; Xanthos, 2004). The second approach is based on the predictability strategy, which assumes that speech should be segmented at locations where some measure of the uncertainty about the next symbol (phoneme or syllable for instance) is high (Harris, 1955; Gammon, 1969; Saffran et al., 1996; Hutchens and Adler, 1998; Xanthos, 2003). Our implementation of the utteranceboundary strategy is based on n-grams statistics. It was previously found to perform a “safe” word segmentation, that is with a rather high precision, but also too conservative as witnessed by a not so high recall (Xanthos, 2004). </context>
</contexts>
<marker>Christiansen, Allen, Seidenberg, 1998</marker>
<rawString>M.H. Christiansen, J. Allen, and M. Seidenberg. 1998. Learning to segment speech using multiple cues. Language and Cognitive Processes, 13:221–268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Content</author>
<author>P Mousty</author>
<author>M Radeau</author>
</authors>
<title>Brulex: Une base de donn´ees lexicales informatis´ee pour le fran¸cais ´ecrit et parl´e. L’Ann´ee Psychologique,</title>
<date>1990</date>
<pages>90--551</pages>
<contexts>
<context position="15512" citStr="Content et al., 1990" startWordPosition="2632" endWordPosition="2635">age is known as hash coding. 4defined by the whole sequence of its parent nodes storage or retrieval of an n-gram grows linearly with n. 4 Empirical evaluation 4.1 Experimental setup Both algorithms described above were implemented implemented in Perl5 and evaluated using a phonemically transcribed and childoriented French corpus (Kilani-Schoch corpus6). We have extracted from the original corpus all the utterances of Sophie’s parents (mainly her mother) between ages 1;6.14 and 2;6.25 (year;month.day). These were transcribed phonemically in a semi-automatic fashion, using the BRULEX database (Content et al., 1990) and making the result closer to oral French with a few hand-crafted rules. Eventually the first 10’000 utterances were used for simulations. This corresponds to 37’663 words (992 types) and 103’325 phonemes (39 types). In general, we will compare the results observed for the successor count used on its own (“SC alone”, on the figures) with those obtained when the utterance-boundary typicality is used for preprocessing7. The latter were recorded for 1 &lt; r &lt; 5, where r is the order for the computation of typicalities. The threshold value for typicality was set to 1 (see section 2.3). The result</context>
</contexts>
<marker>Content, Mousty, Radeau, 1990</marker>
<rawString>A. Content, P. Mousty, and M. Radeau. 1990. Brulex: Une base de donn´ees lexicales informatis´ee pour le fran¸cais ´ecrit et parl´e. L’Ann´ee Psychologique, 90:551–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gammon</author>
</authors>
<title>Quantitative approximations to the word.</title>
<date>1969</date>
<booktitle>In Papers presented to the International Conference on Computational Linguistics COLING-69.</booktitle>
<contexts>
<context position="2334" citStr="Gammon, 1969" startWordPosition="351" endWordPosition="352">gs or endings in order to hypoth&apos;To avoid a latent ambiguity, it should be stated that speech segmentation refers here to a process taking as input a sequence of symbols (usually phonemes) and producing as output a sequence of higher-level units (usually words). esize boundaries inside utterances (Aslin et al., 1996; Christiansen et al., 1998; Xanthos, 2004). The second approach is based on the predictability strategy, which assumes that speech should be segmented at locations where some measure of the uncertainty about the next symbol (phoneme or syllable for instance) is high (Harris, 1955; Gammon, 1969; Saffran et al., 1996; Hutchens and Adler, 1998; Xanthos, 2003). Our implementation of the utteranceboundary strategy is based on n-grams statistics. It was previously found to perform a “safe” word segmentation, that is with a rather high precision, but also too conservative as witnessed by a not so high recall (Xanthos, 2004). As regards the predictability strategy, we have implemented an incremental interpretation of the classical successor count (Harris, 1955). This approach also relies on the observation of phoneme sequences, the length of which is however not restricted to a fixed value</context>
<context position="4509" citStr="Gammon, 1969" startWordPosition="694" endWordPosition="695"> on 93 memory load and processing time. The next section is devoted to the formal definition of both algorithms. Section 3 discusses some issues related to the space and time complexity they involve. The experimental setup as well as the results of the simulations are described in section 4, and in conclusion we will summarize our findings and suggest directions for further research. 2 Description of the algorithms 2.1 Segmentation by thresholding Many distributional segmentation algorithms described in the literature can be seen as instances of the following abstract procedure (Harris, 1955; Gammon, 1969; Saffran et al., 1996; Hutchens and Adler, 1998; Bavaud and Xanthos, 2002). Let S be the set of phonemes (or segments) in a given language. In the most general case, the input of the algorithm is an utterance of length l, that is a sequence of l phonemes u := s1 ... sl (where si denotes the i-th phoneme of u). Then, for 1 &lt; i &lt; l − 1, we insert a boundary after si iff D(u, i) &gt; T(u, i), where the values of the decision variable D(u, i) and of the threshold T (u, i) may depend on both the whole sequence and the actual position examined (Xanthos, 2003). The output of such algorithms can be eval</context>
</contexts>
<marker>Gammon, 1969</marker>
<rawString>E. Gammon. 1969. Quantitative approximations to the word. In Papers presented to the International Conference on Computational Linguistics COLING-69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goldsmith</author>
</authors>
<title>Unsupervised learning of the morphology of a natural language.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<pages>2--153</pages>
<contexts>
<context position="23891" citStr="Goldsmith (2001)" startWordPosition="4070" endWordPosition="4071">parately; • the total memory load of the combined algorithm can be substantially reduced by the preprocessing step; • however, the processing time of the combined algorithm is generally longer and possibly much longer depending on the implementation. These findings are in line with recent research advocating the integration of various strategies for speech segmentation. In his work on 98 Average successor count 40 35 30 25 20 15 10 5 0 n 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Figure 5: Average successor count for n-grams (based on the corpus described in section 4.1). computational morphology, Goldsmith (2001) uses Harris’ successor count as a means to reduce the search space of a more powerful algorithm based on minimum description length (Marcken, 1996). We go one step further and show that an utterance-boundary heuristic can be used in order to reduce the complexity of the successor count algorithm11. Besides complexity issues, there is a problem of data sparseness with the successor count, as it decreases very quickly while the size n of the context grows. In the case of our quite redundant child-oriented corpus, the (weighted) average of the successor count12 for a random n-gram EwCSn f(w) suc</context>
</contexts>
<marker>Goldsmith, 2001</marker>
<rawString>J. Goldsmith. 2001. Unsupervised learning of the morphology of a natural language. Computational Linguistics, 27 (2):153–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z S Harris</author>
</authors>
<title>From phoneme to morpheme.</title>
<date>1955</date>
<journal>Language,</journal>
<pages>31--190</pages>
<contexts>
<context position="2320" citStr="Harris, 1955" startWordPosition="348" endWordPosition="350">rance beginnings or endings in order to hypoth&apos;To avoid a latent ambiguity, it should be stated that speech segmentation refers here to a process taking as input a sequence of symbols (usually phonemes) and producing as output a sequence of higher-level units (usually words). esize boundaries inside utterances (Aslin et al., 1996; Christiansen et al., 1998; Xanthos, 2004). The second approach is based on the predictability strategy, which assumes that speech should be segmented at locations where some measure of the uncertainty about the next symbol (phoneme or syllable for instance) is high (Harris, 1955; Gammon, 1969; Saffran et al., 1996; Hutchens and Adler, 1998; Xanthos, 2003). Our implementation of the utteranceboundary strategy is based on n-grams statistics. It was previously found to perform a “safe” word segmentation, that is with a rather high precision, but also too conservative as witnessed by a not so high recall (Xanthos, 2004). As regards the predictability strategy, we have implemented an incremental interpretation of the classical successor count (Harris, 1955). This approach also relies on the observation of phoneme sequences, the length of which is however not restricted to</context>
<context position="4495" citStr="Harris, 1955" startWordPosition="692" endWordPosition="693"> as its impact on 93 memory load and processing time. The next section is devoted to the formal definition of both algorithms. Section 3 discusses some issues related to the space and time complexity they involve. The experimental setup as well as the results of the simulations are described in section 4, and in conclusion we will summarize our findings and suggest directions for further research. 2 Description of the algorithms 2.1 Segmentation by thresholding Many distributional segmentation algorithms described in the literature can be seen as instances of the following abstract procedure (Harris, 1955; Gammon, 1969; Saffran et al., 1996; Hutchens and Adler, 1998; Bavaud and Xanthos, 2002). Let S be the set of phonemes (or segments) in a given language. In the most general case, the input of the algorithm is an utterance of length l, that is a sequence of l phonemes u := s1 ... sl (where si denotes the i-th phoneme of u). Then, for 1 &lt; i &lt; l − 1, we insert a boundary after si iff D(u, i) &gt; T(u, i), where the values of the decision variable D(u, i) and of the threshold T (u, i) may depend on both the whole sequence and the actual position examined (Xanthos, 2003). The output of such algorith</context>
<context position="9461" citStr="Harris (1955)" startWordPosition="1561" endWordPosition="1562">ariable, if we were dealing with an utterance u of infinite length, we could simply set the order r &gt; 1 of the typicality computation and define d(u, i) as t(si−(r−1) ... si|F) (where si denotes the ith phoneme of u). Since the algorithm is more likely to process an utterance of finite length l, there is a problem when considering a potential boundary close to the beginning of the utterance, in particular when r &gt; i. In this case, we can compute the typicality of smaller sequences, thus defining the decision variable as t(si−(˜r−1) ... si|F), where r˜ := min(r, i). As was already suggested by Harris (1955), our implementation actually combines the typicality in utterance-inal position with its analogue in utterance-initial position. This is done by taking the average of both statistics, and we have found empirically efficient to weight it by the relative lengths of the conditioning sequences: si+1 ... si+˜r/ E S˜r, h w := si− , r˜ := min(r, i) and ˜r := S˜r _ min(r, l − i). This definition helps compensate for the asymmetry of arguments when i is either close to 1 or close to l. Finally, in the simulations below, we apply a mechanism that consists in incrementing n(w|F) and n(w|I) (by one) w</context>
</contexts>
<marker>Harris, 1955</marker>
<rawString>Z.S. Harris. 1955. From phoneme to morpheme. Language, 31:190–222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Hutchens</author>
<author>M D Adler</author>
</authors>
<title>Finding structure via compression.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Conference on Computational Natural Language Learning,</booktitle>
<pages>79--82</pages>
<contexts>
<context position="2382" citStr="Hutchens and Adler, 1998" startWordPosition="357" endWordPosition="360">avoid a latent ambiguity, it should be stated that speech segmentation refers here to a process taking as input a sequence of symbols (usually phonemes) and producing as output a sequence of higher-level units (usually words). esize boundaries inside utterances (Aslin et al., 1996; Christiansen et al., 1998; Xanthos, 2004). The second approach is based on the predictability strategy, which assumes that speech should be segmented at locations where some measure of the uncertainty about the next symbol (phoneme or syllable for instance) is high (Harris, 1955; Gammon, 1969; Saffran et al., 1996; Hutchens and Adler, 1998; Xanthos, 2003). Our implementation of the utteranceboundary strategy is based on n-grams statistics. It was previously found to perform a “safe” word segmentation, that is with a rather high precision, but also too conservative as witnessed by a not so high recall (Xanthos, 2004). As regards the predictability strategy, we have implemented an incremental interpretation of the classical successor count (Harris, 1955). This approach also relies on the observation of phoneme sequences, the length of which is however not restricted to a fixed value. Consequently, the memory load involved by the </context>
<context position="4557" citStr="Hutchens and Adler, 1998" startWordPosition="700" endWordPosition="703">me. The next section is devoted to the formal definition of both algorithms. Section 3 discusses some issues related to the space and time complexity they involve. The experimental setup as well as the results of the simulations are described in section 4, and in conclusion we will summarize our findings and suggest directions for further research. 2 Description of the algorithms 2.1 Segmentation by thresholding Many distributional segmentation algorithms described in the literature can be seen as instances of the following abstract procedure (Harris, 1955; Gammon, 1969; Saffran et al., 1996; Hutchens and Adler, 1998; Bavaud and Xanthos, 2002). Let S be the set of phonemes (or segments) in a given language. In the most general case, the input of the algorithm is an utterance of length l, that is a sequence of l phonemes u := s1 ... sl (where si denotes the i-th phoneme of u). Then, for 1 &lt; i &lt; l − 1, we insert a boundary after si iff D(u, i) &gt; T(u, i), where the values of the decision variable D(u, i) and of the threshold T (u, i) may depend on both the whole sequence and the actual position examined (Xanthos, 2003). The output of such algorithms can be evaluated in reference to the segmentation performed</context>
</contexts>
<marker>Hutchens, Adler, 1998</marker>
<rawString>J.L. Hutchens and M.D. Adler. 1998. Finding structure via compression. In Proceedings of the International Conference on Computational Natural Language Learning, pages 79– 82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kilani-Schoch</author>
<author>W U Dressler</author>
</authors>
<title>Filler + infinitive and pre- and protomorphology demarcation in a french acquisition corpus.</title>
<date>2001</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>30</volume>
<pages>6--653</pages>
<contexts>
<context position="17146" citStr="Kilani-Schoch and Dressler, 2001" startWordPosition="2909" endWordPosition="2912">pus. 4.2 Segmentation performance When used in isolation, our implementation of the successor count has a segmentation precision as high as 82.5%, with a recall of 50.5%; the word precision and recall are 57% and 40.8% 5Perl was chosen here because of the ease it provides when it comes to textual statistics; however, execution is notoriously slower than with C or C++, and this should be kept in mind when interpreting the large differences in processing time reported in section 4.4. 6Sophie, a French speaking Swiss child, was recorded at home by her mother every ten days in situations of play (Kilani-Schoch and Dressler, 2001). The transcription and coding were done according to CHILDES conventions (MacWhinney, 2000). 7Results of the utterance-boundary approach alone are given in (Xanthos, 2004) 96 Segmentation precision and recall Figure 1: Segmentation precision and recall obtained with the successor count alone and with utterance-boundary preprocessing on n-grams. Word precision and recall Figure 2: Word precision and recall obtained with the successor count alone and with utterance-boundary preprocessing on n-grams. 90% 80% 70% 60% 50% 40% 30% 20% 10% 0% 1 2 3 4 5 sc alone r prec rec 70% 60% 50% 40% 30% 20% 10%</context>
</contexts>
<marker>Kilani-Schoch, Dressler, 2001</marker>
<rawString>M. Kilani-Schoch and W.U. Dressler. 2001. Filler + infinitive and pre- and protomorphology demarcation in a french acquisition corpus. Journal of Psycholinguistic Research, 30 (6):653–685.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacWhinney</author>
</authors>
<title>The CHILDES Project: Tools for Analyzing Talk. Third Edition. Lawrence Erlbaum Associates,</title>
<date>2000</date>
<location>Mahwah (NJ).</location>
<contexts>
<context position="17238" citStr="MacWhinney, 2000" startWordPosition="2925" endWordPosition="2926">gmentation precision as high as 82.5%, with a recall of 50.5%; the word precision and recall are 57% and 40.8% 5Perl was chosen here because of the ease it provides when it comes to textual statistics; however, execution is notoriously slower than with C or C++, and this should be kept in mind when interpreting the large differences in processing time reported in section 4.4. 6Sophie, a French speaking Swiss child, was recorded at home by her mother every ten days in situations of play (Kilani-Schoch and Dressler, 2001). The transcription and coding were done according to CHILDES conventions (MacWhinney, 2000). 7Results of the utterance-boundary approach alone are given in (Xanthos, 2004) 96 Segmentation precision and recall Figure 1: Segmentation precision and recall obtained with the successor count alone and with utterance-boundary preprocessing on n-grams. Word precision and recall Figure 2: Word precision and recall obtained with the successor count alone and with utterance-boundary preprocessing on n-grams. 90% 80% 70% 60% 50% 40% 30% 20% 10% 0% 1 2 3 4 5 sc alone r prec rec 70% 60% 50% 40% 30% 20% 10% 0% 1 2 3 4 5 sc alone r prec rec respectively. For comparison, the highest segmentation pre</context>
</contexts>
<marker>MacWhinney, 2000</marker>
<rawString>B. MacWhinney. 2000. The CHILDES Project: Tools for Analyzing Talk. Third Edition. Lawrence Erlbaum Associates, Mahwah (NJ).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C G de Marcken</author>
</authors>
<title>Unsupervised Language Acquisition. Phd dissertation,</title>
<date>1996</date>
<institution>Massachusetts Institute of Technology.</institution>
<marker>de Marcken, 1996</marker>
<rawString>C.G. de Marcken. 1996. Unsupervised Language Acquisition. Phd dissertation, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Saffran</author>
<author>E L Newport</author>
<author>R N Aslin</author>
</authors>
<title>Word segmentation: The role of distributional cues.</title>
<date>1996</date>
<journal>Journal of Memory and Language,</journal>
<pages>35--606</pages>
<contexts>
<context position="1200" citStr="Saffran et al., 1996" startWordPosition="168" endWordPosition="171">e predictability approach, without damaging the resulting segmentation. This intuition leads to the formulation of an explicit model, which is empirically evaluated for a task of word segmentation on a child-oriented phonemically transcribed French corpus. The results show that the hybrid algorithm outperforms its component parts while reducing the total memory load involved. 1 Introduction The design of speech segmentations methods has been much studied ever since Harris’ seminal propositions (1955). Research conducted since the mid 1990’s by cognitive scientists (Brent and Cartwright, 1996; Saffran et al., 1996) has established it as a paradigm of its own in the field of computational models of language acquisition. In this paper, we investigate two boundarybased approaches to speech segmentation. Such methods “attempt to identify individual wordboundaries in the input, without reference to words per se” (Brent and Cartwright, 1996). The first approach we discuss relies on the utterance-boundary strategy, which consists in reusing the information provided by the occurrence of specific phoneme sequences at utterance beginnings or endings in order to hypoth&apos;To avoid a latent ambiguity, it should be sta</context>
<context position="4531" citStr="Saffran et al., 1996" startWordPosition="696" endWordPosition="699">load and processing time. The next section is devoted to the formal definition of both algorithms. Section 3 discusses some issues related to the space and time complexity they involve. The experimental setup as well as the results of the simulations are described in section 4, and in conclusion we will summarize our findings and suggest directions for further research. 2 Description of the algorithms 2.1 Segmentation by thresholding Many distributional segmentation algorithms described in the literature can be seen as instances of the following abstract procedure (Harris, 1955; Gammon, 1969; Saffran et al., 1996; Hutchens and Adler, 1998; Bavaud and Xanthos, 2002). Let S be the set of phonemes (or segments) in a given language. In the most general case, the input of the algorithm is an utterance of length l, that is a sequence of l phonemes u := s1 ... sl (where si denotes the i-th phoneme of u). Then, for 1 &lt; i &lt; l − 1, we insert a boundary after si iff D(u, i) &gt; T(u, i), where the values of the decision variable D(u, i) and of the threshold T (u, i) may depend on both the whole sequence and the actual position examined (Xanthos, 2003). The output of such algorithms can be evaluated in reference to </context>
<context position="25923" citStr="Saffran et al., 1996" startWordPosition="4410" endWordPosition="4413">ntal perspective 12The predecessor count behaves much the same. like carrots), where vertical bars denote boundaries predicted by the utterance-boundary typicality (for r = 5), and dashes represent boundaries inferred by the successor count: SC UBT (r = 5) UBT + SC This suggests that the utterance-boundary strategy could be more than an additional device that safely predicts some boundaries that the successor count alone might have found or not: it could actually have a functional relationship with it. If the predictability strategy has some relevance for speech segmentation in early infancy (Saffran et al., 1996), then it may be necessary to counterbalance the data sparseness; this is what these authors implicitely do by using first-order transition probabilities, and it would be easy to define an n-th order successor count in the same way. Yet another possibility would be to “reset” the successor count after each boundary inserted. Further research should bring computational and psychological evidence for or against such ways to address representativity issues. We conclude this paper by raising an issue that was already discussed by Gammon (1969), and might well be tackled with our methodology. It se</context>
</contexts>
<marker>Saffran, Newport, Aslin, 1996</marker>
<rawString>J.R. Saffran, E.L. Newport, and R.N. Aslin. 1996. Word segmentation: The role of distributional cues. Journal of Memory and Language, 35:606–621.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Xanthos</author>
</authors>
<title>Du k-gramme au mot: variation sur un th`eme distributionnaliste. Bulletin de linguistique et des sciences du langage</title>
<date>2003</date>
<journal>(BIL),</journal>
<volume>21</volume>
<contexts>
<context position="2398" citStr="Xanthos, 2003" startWordPosition="361" endWordPosition="362">it should be stated that speech segmentation refers here to a process taking as input a sequence of symbols (usually phonemes) and producing as output a sequence of higher-level units (usually words). esize boundaries inside utterances (Aslin et al., 1996; Christiansen et al., 1998; Xanthos, 2004). The second approach is based on the predictability strategy, which assumes that speech should be segmented at locations where some measure of the uncertainty about the next symbol (phoneme or syllable for instance) is high (Harris, 1955; Gammon, 1969; Saffran et al., 1996; Hutchens and Adler, 1998; Xanthos, 2003). Our implementation of the utteranceboundary strategy is based on n-grams statistics. It was previously found to perform a “safe” word segmentation, that is with a rather high precision, but also too conservative as witnessed by a not so high recall (Xanthos, 2004). As regards the predictability strategy, we have implemented an incremental interpretation of the classical successor count (Harris, 1955). This approach also relies on the observation of phoneme sequences, the length of which is however not restricted to a fixed value. Consequently, the memory load involved by the successor count </context>
<context position="5066" citStr="Xanthos, 2003" startWordPosition="807" endWordPosition="808"> following abstract procedure (Harris, 1955; Gammon, 1969; Saffran et al., 1996; Hutchens and Adler, 1998; Bavaud and Xanthos, 2002). Let S be the set of phonemes (or segments) in a given language. In the most general case, the input of the algorithm is an utterance of length l, that is a sequence of l phonemes u := s1 ... sl (where si denotes the i-th phoneme of u). Then, for 1 &lt; i &lt; l − 1, we insert a boundary after si iff D(u, i) &gt; T(u, i), where the values of the decision variable D(u, i) and of the threshold T (u, i) may depend on both the whole sequence and the actual position examined (Xanthos, 2003). The output of such algorithms can be evaluated in reference to the segmentation performed by a human expert, using traditional measures from the signal detection framework. It is usual to give evaluations both for word and boundary detection (Batchelder, 2002). The word precision is the probability for a word isolated by the segmentation procedure to be present in the reference segmentation, and the word recall is the probability for a word occurring in the true segmentation to be correctly isolated. Similarly, the segmentation precision is the probability that an inferred boundary actually </context>
</contexts>
<marker>Xanthos, 2003</marker>
<rawString>A. Xanthos. 2003. Du k-gramme au mot: variation sur un th`eme distributionnaliste. Bulletin de linguistique et des sciences du langage (BIL), 21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Xanthos</author>
</authors>
<title>An incremental implementation of the utterance-boundary approach to speech segmentation.</title>
<date>2004</date>
<booktitle>the Proceedings of Computational Linguistics in the Netherlands</booktitle>
<note>To appear in</note>
<contexts>
<context position="2082" citStr="Xanthos, 2004" startWordPosition="309" endWordPosition="310">out reference to words per se” (Brent and Cartwright, 1996). The first approach we discuss relies on the utterance-boundary strategy, which consists in reusing the information provided by the occurrence of specific phoneme sequences at utterance beginnings or endings in order to hypoth&apos;To avoid a latent ambiguity, it should be stated that speech segmentation refers here to a process taking as input a sequence of symbols (usually phonemes) and producing as output a sequence of higher-level units (usually words). esize boundaries inside utterances (Aslin et al., 1996; Christiansen et al., 1998; Xanthos, 2004). The second approach is based on the predictability strategy, which assumes that speech should be segmented at locations where some measure of the uncertainty about the next symbol (phoneme or syllable for instance) is high (Harris, 1955; Gammon, 1969; Saffran et al., 1996; Hutchens and Adler, 1998; Xanthos, 2003). Our implementation of the utteranceboundary strategy is based on n-grams statistics. It was previously found to perform a “safe” word segmentation, that is with a rather high precision, but also too conservative as witnessed by a not so high recall (Xanthos, 2004). As regards the p</context>
<context position="7065" citStr="Xanthos (2004)" startWordPosition="1143" endWordPosition="1144">erance-initial position in the t-th utterance of C (which is 1 iff the utterance begins with w and 0 otherwise). Similarly, the absolute frequency of w in utterance-final position is given by n(w|F) := Tt=1 nt(w|F). Accordingly, the relative frequency of w obtains as f(w) := n(w)/ ˜wESn n( ˜w). Its relative frequencies in utterance-initial and -final position respectively are given by f(w|I) := n(w|I)/ ˜wESn n( ˜w|I) and f(w|F) := n(w|F)/ ˜wESn n(˜w|F) 2 2.3 Utterance-boundary typicality We use the same implementation of the utterance-boundary strategy that is described in more details by Xanthos (2004). Intuitively, the idea is to segment utterances where sequences occur, which are typical of utterance boundaries. Of course, this implies that the corpus is segmented in utterances, which seems a reasonable assumption as far as language acquisition is concerned. In this sense, the utteranceboundary strategy may be viewed as a kind of learning by generalization. Probability theory provides us with a straightforward way of evaluating how much an n-gram w E Sn is typical of utterance endings. Namely, we know that events “occurrence of n-gram w” and “occurrence of an ngram in utterance-final posi</context>
<context position="10319" citStr="Xanthos, 2004" startWordPosition="1715" endWordPosition="1716"> relative lengths of the conditioning sequences: si+1 ... si+˜r/ E S˜r, h w := si− , r˜ := min(r, i) and ˜r := S˜r _ min(r, l − i). This definition helps compensate for the asymmetry of arguments when i is either close to 1 or close to l. Finally, in the simulations below, we apply a mechanism that consists in incrementing n(w|F) and n(w|I) (by one) whenever D(u, i) &gt; T (u, i). The aim of this is to enable the discovery of new utterance-boundary typical sequences. It was found to considerably raise the recall as more utterances are processed, at the cost of a slight reduction in precision (Xanthos, 2004). 2.4 Successor count The second algorithm we investigate in this paper is an implementation of Harris’ successor count (Harris, 1955), the historical source of all predictability-based approaches to segmentation. It relies on the assumption that in general, the diversity of possible phonemes transitions is high after a word boundary and decreases as we consider transitions occurring further inside a word. The diversity of transitions following an ngram w E Sn is evaluated by the successor count (or successor variety), simply defined as the number of different phonemes that can occur after it:</context>
<context position="17318" citStr="Xanthos, 2004" startWordPosition="2936" endWordPosition="2937">nd recall are 57% and 40.8% 5Perl was chosen here because of the ease it provides when it comes to textual statistics; however, execution is notoriously slower than with C or C++, and this should be kept in mind when interpreting the large differences in processing time reported in section 4.4. 6Sophie, a French speaking Swiss child, was recorded at home by her mother every ten days in situations of play (Kilani-Schoch and Dressler, 2001). The transcription and coding were done according to CHILDES conventions (MacWhinney, 2000). 7Results of the utterance-boundary approach alone are given in (Xanthos, 2004) 96 Segmentation precision and recall Figure 1: Segmentation precision and recall obtained with the successor count alone and with utterance-boundary preprocessing on n-grams. Word precision and recall Figure 2: Word precision and recall obtained with the successor count alone and with utterance-boundary preprocessing on n-grams. 90% 80% 70% 60% 50% 40% 30% 20% 10% 0% 1 2 3 4 5 sc alone r prec rec 70% 60% 50% 40% 30% 20% 10% 0% 1 2 3 4 5 sc alone r prec rec respectively. For comparison, the highest segmentation precision obtained with utteranceboundary typicality alone is 80.8% (for r = 5), bu</context>
</contexts>
<marker>Xanthos, 2004</marker>
<rawString>A. Xanthos. 2004. An incremental implementation of the utterance-boundary approach to speech segmentation. To appear in the Proceedings of Computational Linguistics in the Netherlands 2003 (CLIN 2003).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>