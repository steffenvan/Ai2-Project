<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000531">
<title confidence="0.990748">
Transition-based Dependency DAG Parsing Using Dynamic Oracles
</title>
<author confidence="0.991861">
Alper Tokg¨oz
</author>
<affiliation confidence="0.940914666666667">
Istanbul Technical University
Department of Computer Engineering
Istanbul, Turkey
</affiliation>
<email confidence="0.995835">
tokgoza@itu.edu.tr
</email>
<author confidence="0.988758">
G¨uls¸en Eryiˇgit
</author>
<affiliation confidence="0.940882">
Istanbul Technical University
Department of Computer Engineering
Istanbul, Turkey
</affiliation>
<email confidence="0.997012">
gulsen.cebiroglu@itu.edu.tr
</email>
<sectionHeader confidence="0.997363" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999291">
In most of the dependency parsing stud-
ies, dependency relations within a sen-
tence are often presented as a tree struc-
ture. Whilst the tree structure is suf-
ficient to represent the surface relations,
deep dependencies which may result to
multi-headed relations require more gen-
eral dependency structures, namely Di-
rected Acyclic Graphs (DAGs). This
study proposes a new dependency DAG
parsing approach which uses a dynamic
oracle within a shift-reduce transition-
based parsing framework. Although there
is still room for improvement on per-
formance with more feature engineer-
ing, we already obtain competitive perfor-
mances compared to static oracles as a re-
sult of our initial experiments conducted
on the ITU-METU-Sabancı Turkish Tree-
bank (IMST).
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999983519230769">
Syntactic parsing is the process of determining
the grammatical structure of a sentence as con-
forming to the grammatical rules of the relevant
natural language. The structure of the sentence
is determined according to the grammar formal-
ism that the parser is built upon. Phrase struc-
ture parsers, also known as constituency parsers,
parse a sentence by splitting it into its smaller
constituents. On the other hand, in dependency
parsers, the structure of the sentence is represented
as dependency trees consisting of directed depen-
dency links between a dependent and a head word.
Data-driven dependency parsing frameworks
have gained increasing popularity in recent years
and been used in a wide range of applications such
as machine translation (Ding and Palmer, 2005),
textual entailment (Yuret et al., 2013) and question
answering (Xu et al., 2014). Most data-driven de-
pendency parsers achieve state-of-the art parsing
performances with a language agnostic approach
on the different syntactic structures of different
languages (Buchholz and Marsi, 2006). Mod-
ern data-driven dependency parsers can be catego-
rized into two groups: graph-based and transition-
based parsers. Graph-based parsers rely on the
global optimization of models aiming to find span-
ning trees over dependency graphs. On the other
hand, transition-based parsers work basically with
greedy local decisions that are deterministically
selected by oracles, which are generic machine
learning models trained to make decisions about
the next transition action. In a recent study,
Zhang and Nivre (2012) propose a new approach
on transition-based parsing that aims to provide
global learning instead of greedy local decisions.
Despite the high performances of both graph-
based and transition-based dependency parsers,
these are generally bounded by the constraint that
each dependent may not have multiple heads.
Therefore, the resulting parsing output is a tree
where words correspond to nodes and dependency
relations correspond to edges. Although depen-
dency trees yield satisfactory performances, they
are inadequate in capturing dependencies at dif-
ferent levels of semantic interpretations or more
complicated linguistic phenomena (e.g. relative
clauses, anaphoric references) which could result
in multi-head dependencies together with exist-
ing surface dependency relations. An example is
given in Figure 1 which is taken from the Turk-
ish IMST Treebank (Sulubacak and Eryi˘git, 2015).
In Figure 1, the dependent token “Umut” depends
</bodyText>
<page confidence="0.975794">
22
</page>
<note confidence="0.848471">
Proceedings of the ACL-IJCNLP 2015 Student Research Workshop, pages 22–27,
Beijing, China, July 28, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999873636363636">
on more than one head token with SUBJECT re-
lations: 1) the verb “kos¸mak” (to run) and 2) the
verb “d¨us¸mek” (to fall). Adding a second rela-
tion (emphasized with a dash-dotted line in the fig-
ure) to the token ”Umut” breaks the condition that
each token may have at most one head, and ren-
ders existing dependency tree parsers incompati-
ble for this setup. It is also worth mentioning that
the deep dependencies in the IMST are not dis-
criminated from surface dependencies by the use
of different labels.
</bodyText>
<subsubsectionHeader confidence="0.368028">
“Umut fell as [he was] running.”
</subsubsectionHeader>
<figureCaption confidence="0.972409">
Figure 1: Example for Turkish multi-head depen-
dencies.
</figureCaption>
<bodyText confidence="0.999936538461539">
In this paper, for the first time in the litera-
ture, we investigate the impact of using dynamic
oracles for parsing multi-head dependency struc-
tures by extending the approach of Goldberg and
Nivre (2012). We provide comparisons with the
replication of the basic shift-reduce DAG pars-
ing algorithm of Sagae and Tsujii (2008) and a
first time implementation of their proposed arc-
eager parsing algorithm. The remainder of the pa-
per first gives a background information about the
topic, then introduces the DAG parsing framework
and the proposed algorithms together with experi-
ments and results.
</bodyText>
<sectionHeader confidence="0.987243" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.99998632">
Although it is possible to discover the syntactic re-
lations with a two stage model by first finding the
regular surface dependencies and then finding the
deep relations with post-processing as in Nivre et
al. (2010), it is not always straightforward to de-
cide which dependencies should be treated as sur-
face relations or deep relations as in the case of
Turkish. Thus, in this study, we focus on single
stage models and aim to discover the entire set of
relations in a single pass. McDonald and Pereira
(2006) use graph-based algorithms for DAG pars-
ing simply using approximate interference in an
edge-factored dependency model starting from de-
pendency trees. On the other hand, Sagae and
Tsujii (2008) propose a transition-based counter-
part for DAG parsing which made available for
parsing multi-headed relations. They modified the
existing shift-reduce bottom-up dependency pars-
ing algorithm of Nivre and Nilsson (2005) to al-
low multiple heads per token by the use of cycle
removal and pseudo-projectivization as a prepro-
cessing stage. They report higher performance
scores on the Danish treebank compared to Mc-
Donald and Pereira (2006).
A standard way of determining transition ac-
tions in a shift-reduce dependency parser is us-
ing static oracles. During the training stage, the
learning instances for the oracle are prepared by
the use of manually annotated gold-standard parse
trees and the current parsing configuration. Dur-
ing the parsing stage, the already trained oracle
decides on the next transition operation. One of
the problems with static oracles lays beneath the
spurious ambiguity, which implies there might be
more than one transition sequence for a given sen-
tence and the sequence proposed by an oracle may
not be the easiest to learn. The second problem oc-
curs when the parser makes a parsing error which
leads to a parser configuration from which the cor-
rect parse tree is not derivable. The algorithm does
not provide any solutions for dealing with the er-
ror propagation caused by such situations. The
idea of dynamic oracles introduced by Goldberg
and Nivre (2012) rises for handling the aforemen-
tioned handicaps of static oracles. Rather than re-
turning a unique transition for a given configura-
tion, a dynamic oracle returns a set of valid tran-
sitions regarding the current configuration, which
would allow the algorithm to explore non-optimal
configurations during the training procedure.
</bodyText>
<sectionHeader confidence="0.967732" genericHeader="method">
3 Transition-Based Solutions for
Dependency DAG Parsing
</sectionHeader>
<bodyText confidence="0.999201285714286">
Transition-based parsing frameworks consider the
transition system to be an abstract machine that
processes input sentences and produces corre-
sponding parsing graphs. The tokens of the in-
put sequence and the partially created dependency
structures are kept within the following data struc-
tures:
</bodyText>
<listItem confidence="0.744024">
1. a buffer β which includes the remaining un-
processed tokens in the input sequence in a
queue,
</listItem>
<figure confidence="0.9829744">
SUBJECT
‘Umut’ ‘[he] runs’ (WHI-) ‘[he] fell’
Umut kos¸ar +ken
SUBJECT DERIV MODIFIER PREDICATE
d¨us¸t¨u
</figure>
<page confidence="0.977947">
23
</page>
<listItem confidence="0.994709333333333">
2. a stack σ which consists of the tokens being
processed,
3. a set A of assigned dependency arcs.
</listItem>
<bodyText confidence="0.999961">
The transition actions explained in the follow-
ing subsections are basic stack and queue opera-
tions that correspond to parsing actions marking
dependency relations. The algorithm starts with a
buffer 0 initialized with all tokens of a sentence
preserving their sequence, and an empty stack σ.
The parsing process finishes when there are no
nodes left in 0 and only the artificial root in σ.
</bodyText>
<subsectionHeader confidence="0.872121">
3.1 Basic shift-reduce parsing with multiple
heads
</subsectionHeader>
<bodyText confidence="0.99968375">
The first algorithm that is capable of parsing DAG
structures is the standard algorithm of Sagae and
Tsujii (2008). The complete list of the transitions
of this algorithm is as follows:
</bodyText>
<listItem confidence="0.999940625">
• Shift: Pops the first item of the buffer and
pushes it onto the top of the stack.
• Left-Reduce: Pops the top two items of the
stack, creates a left arc between them where
the top item is assigned as the head of the
item below, and pushes the head token back
onto the stack.
• Right-Reduce: Pops the top two items of
</listItem>
<bodyText confidence="0.7379155">
the stack, creates a right arc between them,
where the item below is assigned as the head
of the top item, and pushes the head token
back onto the stack.
</bodyText>
<listItem confidence="0.9984062">
• Left-Attach: Creates a left arc between the
top two items of the stack, where the top item
is assigned as the head of the one below. The
stack and the buffer remain unchanged.
• Right-Attach: Creates a right dependency arc
between the two top items on the stack and
assigns the top token as the dependent of the
token below. As the second step, it pops the
top of the stack and places it into the buffer
0.
</listItem>
<subsectionHeader confidence="0.8663405">
3.2 Multi-Head Arc-Eager Parsing
Algorithm
</subsectionHeader>
<bodyText confidence="0.87274675">
The second transition algorithm introduced but not
implemented by Sagae and Tsujii (2008) is a vari-
ation of the Arc-Eager algorithm of Nivre et al.
(2007) and has the following transition operations:
</bodyText>
<listItem confidence="0.998295">
• Shift: Pops the first item of the buffer and
pushes it onto the top token of the stack.
• Left-Arc: Creates a left dependency arc be-
tween the top token of the stack and the first
token of the input buffer, where the first token
in the buffer becomes the head and the one at
the top of the stack becomes the dependent.
It is also worth noticing that the stack and the
input buffer remains unchanged.
• Right-Arc: Creates a right dependency arc
between the top token of the stack and the
first token on the input buffer, where the to-
ken in the stack becomes the head, and the
token which is in front of the buffer becomes
the dependent. It is also worth noticing that
the stack and the input buffer remains un-
changed.
• Reduce: Pops the top item of the stack if and
only if it was previously associated with at
least one head.
</listItem>
<subsectionHeader confidence="0.990542">
3.3 Multi-Head Arc Eager Parsing with a
Dynamic Oracle
</subsectionHeader>
<bodyText confidence="0.99990825">
In order to design a dynamic oracle with the ca-
pability of parsing multi-head dependencies, we
need an efficient method for computing the cost
of each transition. To this end, we extend the
dynamic oracle defined by Goldberg and Nivre
(2012), considering DAG parsing arc-eager sys-
tem of Sagae and Tsujii (2008). Extended arc-
eager transition system will operate in the same
way as previously defined in Section 3.2, within a
dynamic oracle system whose cost function is de-
fined with a transition operation, the current con-
figuration c = (σ|s, b|0, A)1 and the gold parse
of the sentence (Ggold). Differing from Goldberg
and Nivre (2012), for ease of computation, we pre-
fer marking transitions as zero cost or costly in-
stead of computing the exact cost:
</bodyText>
<listItem confidence="0.6385335">
•
Cost(LeftAttach, c, Ggold) Attaching s to b
with a left arc is costly, if there is a right arc
between s and b, or it is already attached with
a left arc.
• Cost(RightAttach, c, Ggold) Attaching s to
</listItem>
<bodyText confidence="0.8669998">
b by creating right arc is costly, if there is a
left arc between s and b, or it is already at-
tached with a right arc.
1In c = (v|s, b1,Q, A), s denotes the top token of the stack
v, b denotes first item of buffer ,Q, A denotes revealed arcs
</bodyText>
<page confidence="0.996184">
24
</page>
<listItem confidence="0.9789737">
• Cost(Reduce, c, Ggold) Popping s from the
stack means it will be no longer possible to
associate it with any head or dependent from
buffer β, therefore it is costly if it has heads
or dependents in the β.
• Cost(Shift, c, Ggold) Pushing b onto the
stack means it will be no longer possible to
associate it with any heads or dependents in
stack σ, therefore it is costly if it has a head
or dependent token in the σ.
</listItem>
<bodyText confidence="0.962994764705882">
Since left attach and right attach operations do
not change the parser configuration (i.e. these op-
erations cannot lead to a parser configuration from
which the gold tree is not reachable), their cost is
measured according to the validity of the attach-
ment. The only difference of our multi-head vari-
ant from the single head arc-eager transition sys-
tem is that the left and right arc operations do not
change the parser state. As such, it is essentially a
relaxed version of the single-head system. There-
fore, since the arc-decomposition property holds
for the single-head system (as proven by Goldberg
and Nivre (2013)), it also holds for our variant.
We use the same online training procedure (with
the perceptron algorithm) as Goldberg and Nivre
(2012) given in Algorithm 1.
Algorithm 1 Online training with dynamic oracle
</bodyText>
<listItem confidence="0.9765174">
1: procedure TRAIN
2: w ← 0
3: for I← 1, ITERATIONS do
4: c ← cs(x)
5: for sentence x do
6: while c is not terminal do
7: tp ← argmaxtw.Φ(c, t)
8: ZC
{t|o(t; c; Ggold) = true}
9: to ← argmaxt,ZCw.Φ(c, t)
10: if tp E� ZC then
11: w ← w + Φ(c, to) −
Φ(c, tp)
12: tn ←NEXT(I, tp, ZC)
13: c ← tn(c)
14: procedure NEXT(I, t, ZC)
15: if t E ZC then
16: return t
17: else
18: RANDOM ELEMENT(ZC)
</listItem>
<bodyText confidence="0.99998375">
The proposed oracle will return a set of zero
cost transition operations (denoted as ZC at line
8) where the costs are calculated according to the
cost function defined above. Feature weights will
be updated only if the perceptron model makes a
transition prediction that does not belong to the
zero cost transitions (lines 10 and 11). After that,
the next transition operation is chosen by the func-
tion NEXT, which returns the transition that is pre-
dicted by the model if it belongs to zero cost tran-
sitions; if not, it returns a random transition which
belongs to the zero cost transition set.
</bodyText>
<sectionHeader confidence="0.999121" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99936519047619">
In order to apply the specified DAG parsing al-
gorithm to non-projective sentences, a pseudo-
projective transformation operation is applied to
the IMST. For that aim, we apply Head scheme2
described by Nivre (2005). Moreover, before the
application of this pseudo-projective transforma-
tion, the cyclic dependency paths are handled as
described by Sagae and Tsujii (2008), by reversing
the shortest arc within the cyclic dependency path
until no cyclic path remains. 99.3% precision and
99.2% recall are acquired on IMST by applying
the pseudo-projective transformation and detrans-
formation operations. As a learning component,
we follow the work of Sagae and Tsujii (2008)
and use a Maximum Entropy model for the clas-
sification with the greedy search algorithm. For
the dynamic oracle experiment, we use an aver-
aged perceptron algorithm iterating 15 times over
the training data.
The following features are used in all of the ex-
periments:
</bodyText>
<listItem confidence="0.995924">
• The POS tag and dependency relations of the
rightmost and leftmost modifiers of the top 2
items on the stack.
• The number of heads and dependents of the
top item of the stack and the first item of the
buffer.
• The dependency relations of the top of the
stack.
</listItem>
<footnote confidence="0.863796">
2Although other schemes could be tried for DAGs for bet-
ter performance, this is left for future work due to time con-
straints.
</footnote>
<bodyText confidence="0.441973333333333">
• The POS tag, lemma and morphological fea-
tures of the top 3 tokens on the stack and the
← first 3 tokens in the buffer.
</bodyText>
<page confidence="0.960883">
25
</page>
<listItem confidence="0.9989083125">
• Whether the top 2 tokens on the stack have a
dependency relation between them or not.
• Whether the top token of the stack and the
first of the buffer have a dependency relation
between them or not, and if so the direction
and the type of the relation.
• Combination of the surface form of the top
token of the stack and its number of left and
right modifiers.
• Combination of the surface form of the first
token of the buffer and its number of left and
right modifiers.
• The surface forms and POS tags of heads of
the top token of the stack and the first token
of the buffer.
• The previous two parsing actions.
</listItem>
<bodyText confidence="0.988677">
For training and evaluation purposes, we use the
IMST with ten-fold cross validation. Experiment
results are given in Table 4.
</bodyText>
<tableCaption confidence="0.987765">
Table 1: Unlabeled scores of experiments with
using IMST.
</tableCaption>
<table confidence="0.914015">
Experiment Precision Recall F1
Static-Standard 79.42 77.56 78.50
Static-Eager 78.90 76.79 77.83
Dynamic-Eager 79.68 81.17 80.42
</table>
<bodyText confidence="0.977040130434783">
As shown in Table 4, the static arc-eager DAG
implementation for Turkish performs slightly
worse than the arc-standard algorithm. This is not
surprising in the light of previous studies (Nivre,
2008; Eryi˘git et al., 2008) reporting that the arc
standard algorithm performs better in tree parsing
due to the smaller number of classes to be learned
by the oracle. However, the proposed multi-head
arc-eager algorithm with dynamic oracle (referred
to as Dynamic-Eager) yields the best precision, re-
call and F1 scores among the three experiments.3
In this study, although there is still room for
improvement on performance with more feature
engineering, we obtain better results on Turkish
IMST treebank between static and dynamic ora-
cles with our newly proposed method for parsing
3The difference of this model from the runner-up models
are found to be statistically significant according to McNe-
mar’s test (p &lt; 0.0001)
DAGs. This encourages us to test our system with
different languages as future work with the expec-
tation that the ameliorations will be much higher
than the reported ones in the single-head scenario.
</bodyText>
<sectionHeader confidence="0.993924" genericHeader="conclusions">
5 Conclusion and Future Works
</sectionHeader>
<bodyText confidence="0.99997955">
In this paper, we experimented with three differ-
ent transition-based algorithms for DAG parsing
which eliminate the single-head constraint of tra-
ditional algorithms and allows multi-head depen-
dency relations to represent more complicated lin-
guistic phenomena along with surface relations.
We present the first results for arc-eager DAG
parsing with static oracles and propose a new arc-
eager DAG parsing algorithm using dynamic ora-
cles. Our initial experiments conducted on Turkish
pave the way for future research on the usage of
the dynamic arc-eager DAG parsing for other lan-
guages. For future work, we will first conduct ex-
periments on how well the Dynamic-Eager algo-
rithm performs on different treebanks, including
multi-head dependencies (such as the Danish tree-
bank (Kromann, 2003)). Secondly, we will con-
duct experiments on previously described static-
oracle parsing algorithms by using different clas-
sifiers such as Support Vector Machines.
</bodyText>
<sectionHeader confidence="0.998385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999952333333333">
We hereby acknowledge that this study is part of
a research project named ”Parsing Web 2.0 Sen-
tences” that is supported by T ¨UB˙ITAK (Turkish
Scientific and Technological Research Council)
1001 program (grant number 112E276) and part
of the ICT COST Action IC 1207.
</bodyText>
<sectionHeader confidence="0.998851" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997858928571429">
Sabine Buchholz and Erwin Marsi. 2006. Conll-x
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, pages 149–164.
Association for Computational Linguistics.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 541–548. Association for Computa-
tional Linguistics.
Gils¸en Eryi˘git, Joakim Nivre, and Kemal Oflazer.
2008. Dependency parsing of Turkish. Computa-
tional Linguistics, 34(3):357–389.
</reference>
<page confidence="0.948628">
26
</page>
<reference confidence="0.999842636363636">
Yoav Goldberg and Joakim Nivre. 2012. A dynamic
oracle for arc-eager dependency parsing. In COL-
ING, pages 959–976.
Yoav Goldberg and Joakim Nivre. 2013. Training
deterministic parsers with non-deterministic oracles.
Transactions of the association for Computational
Linguistics, 1:403–414.
Matthias T Kromann. 2003. The danish dependency
treebank and the underlying linguistic theory. In
Proc. of the Second Workshop on Treebanks and Lin-
guistic Theories (TLT).
Ryan T McDonald and Fernando CN Pereira. 2006.
Online learning of approximate dependency parsing
algorithms. In EACL. Citeseer.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics, pages 99–106. Association for
Computational Linguistics.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(02):95–135.
Joakim Nivre, Laura Rimell, Ryan McDonald, and Car-
los Gomez-Rodriguez. 2010. Evaluation of depen-
dency parsers on unbounded dependencies. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics, pages 833–841. Associ-
ation for Computational Linguistics.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513–553.
Kenji Sagae and Jun’ichi Tsujii. 2008. Shift-reduce
dependency DAG parsing. In Proceedings of the
22nd International Conference on Computational
Linguistics-Volume 1, pages 753–760. Association
for Computational Linguistics.
Umut Sulubacak and G¨uls¸en Eryi˘git. 2015. A rede-
fined Turkish dependency grammar and its imple-
mentations: A new Turkish web treebank &amp; the re-
vised Turkish treebank. under review.
Kun Xu, Sheng Zhang, Yansong Feng, and Dongyan
Zhao. 2014. Answering natural language questions
via phrasal semantic parsing. In Natural Language
Processing and Chinese Computing, pages 333–344.
Springer.
Deniz Yuret, Laura Rimell, and Aydın Han. 2013.
Parser evaluation using textual entailments. Lan-
guage resources and evaluation, 47(3):639–659.
Yue Zhang and Joakim Nivre. 2012. Analyzing
the effect of global learning and beam-search on
transition-based dependency parsing. In COLING
(Posters), pages 1391–1400.
</reference>
<page confidence="0.99881">
27
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.118944">
<title confidence="0.998735">Transition-based Dependency DAG Parsing Using Dynamic Oracles</title>
<author confidence="0.890316">Alper</author>
<affiliation confidence="0.8860905">Istanbul Technical Department of Computer</affiliation>
<address confidence="0.81945">Istanbul,</address>
<email confidence="0.993774">tokgoza@itu.edu.tr</email>
<affiliation confidence="0.821605">Istanbul Technical Department of Computer</affiliation>
<address confidence="0.820102">Istanbul,</address>
<email confidence="0.997907">gulsen.cebiroglu@itu.edu.tr</email>
<abstract confidence="0.968973428571429">In most of the dependency parsing studies, dependency relations within a sentence are often presented as a tree structure. Whilst the tree structure is sufficient to represent the surface relations, deep dependencies which may result to multi-headed relations require more general dependency structures, namely Directed Acyclic Graphs (DAGs). This study proposes a new dependency DAG parsing approach which uses a dynamic oracle within a shift-reduce transitionbased parsing framework. Although there is still room for improvement on performance with more feature engineering, we already obtain competitive performances compared to static oracles as a result of our initial experiments conducted on the ITU-METU-Sabancı Turkish Treebank (IMST).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>Conll-x shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning,</booktitle>
<pages>149--164</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2131" citStr="Buchholz and Marsi, 2006" startWordPosition="307" endWordPosition="310">rsers, the structure of the sentence is represented as dependency trees consisting of directed dependency links between a dependent and a head word. Data-driven dependency parsing frameworks have gained increasing popularity in recent years and been used in a wide range of applications such as machine translation (Ding and Palmer, 2005), textual entailment (Yuret et al., 2013) and question answering (Xu et al., 2014). Most data-driven dependency parsers achieve state-of-the art parsing performances with a language agnostic approach on the different syntactic structures of different languages (Buchholz and Marsi, 2006). Modern data-driven dependency parsers can be categorized into two groups: graph-based and transitionbased parsers. Graph-based parsers rely on the global optimization of models aiming to find spanning trees over dependency graphs. On the other hand, transition-based parsers work basically with greedy local decisions that are deterministically selected by oracles, which are generic machine learning models trained to make decisions about the next transition action. In a recent study, Zhang and Nivre (2012) propose a new approach on transition-based parsing that aims to provide global learning </context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared task on multilingual dependency parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning, pages 149–164. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Ding</author>
<author>Martha Palmer</author>
</authors>
<title>Machine translation using probabilistic synchronous dependency insertion grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>541--548</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1844" citStr="Ding and Palmer, 2005" startWordPosition="267" endWordPosition="270">al language. The structure of the sentence is determined according to the grammar formalism that the parser is built upon. Phrase structure parsers, also known as constituency parsers, parse a sentence by splitting it into its smaller constituents. On the other hand, in dependency parsers, the structure of the sentence is represented as dependency trees consisting of directed dependency links between a dependent and a head word. Data-driven dependency parsing frameworks have gained increasing popularity in recent years and been used in a wide range of applications such as machine translation (Ding and Palmer, 2005), textual entailment (Yuret et al., 2013) and question answering (Xu et al., 2014). Most data-driven dependency parsers achieve state-of-the art parsing performances with a language agnostic approach on the different syntactic structures of different languages (Buchholz and Marsi, 2006). Modern data-driven dependency parsers can be categorized into two groups: graph-based and transitionbased parsers. Graph-based parsers rely on the global optimization of models aiming to find spanning trees over dependency graphs. On the other hand, transition-based parsers work basically with greedy local dec</context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Yuan Ding and Martha Palmer. 2005. Machine translation using probabilistic synchronous dependency insertion grammars. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 541–548. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gils¸en Eryi˘git</author>
<author>Joakim Nivre</author>
<author>Kemal Oflazer</author>
</authors>
<title>Dependency parsing of Turkish.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>3</issue>
<marker>Eryi˘git, Nivre, Oflazer, 2008</marker>
<rawString>Gils¸en Eryi˘git, Joakim Nivre, and Kemal Oflazer. 2008. Dependency parsing of Turkish. Computational Linguistics, 34(3):357–389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Joakim Nivre</author>
</authors>
<title>A dynamic oracle for arc-eager dependency parsing.</title>
<date>2012</date>
<booktitle>In COLING,</booktitle>
<pages>959--976</pages>
<contexts>
<context position="4527" citStr="Goldberg and Nivre (2012)" startWordPosition="677" endWordPosition="680">h-dotted line in the figure) to the token ”Umut” breaks the condition that each token may have at most one head, and renders existing dependency tree parsers incompatible for this setup. It is also worth mentioning that the deep dependencies in the IMST are not discriminated from surface dependencies by the use of different labels. “Umut fell as [he was] running.” Figure 1: Example for Turkish multi-head dependencies. In this paper, for the first time in the literature, we investigate the impact of using dynamic oracles for parsing multi-head dependency structures by extending the approach of Goldberg and Nivre (2012). We provide comparisons with the replication of the basic shift-reduce DAG parsing algorithm of Sagae and Tsujii (2008) and a first time implementation of their proposed arceager parsing algorithm. The remainder of the paper first gives a background information about the topic, then introduces the DAG parsing framework and the proposed algorithms together with experiments and results. 2 Background Although it is possible to discover the syntactic relations with a two stage model by first finding the regular surface dependencies and then finding the deep relations with post-processing as in Ni</context>
<context position="7015" citStr="Goldberg and Nivre (2012)" startWordPosition="1081" endWordPosition="1084">dy trained oracle decides on the next transition operation. One of the problems with static oracles lays beneath the spurious ambiguity, which implies there might be more than one transition sequence for a given sentence and the sequence proposed by an oracle may not be the easiest to learn. The second problem occurs when the parser makes a parsing error which leads to a parser configuration from which the correct parse tree is not derivable. The algorithm does not provide any solutions for dealing with the error propagation caused by such situations. The idea of dynamic oracles introduced by Goldberg and Nivre (2012) rises for handling the aforementioned handicaps of static oracles. Rather than returning a unique transition for a given configuration, a dynamic oracle returns a set of valid transitions regarding the current configuration, which would allow the algorithm to explore non-optimal configurations during the training procedure. 3 Transition-Based Solutions for Dependency DAG Parsing Transition-based parsing frameworks consider the transition system to be an abstract machine that processes input sentences and produces corresponding parsing graphs. The tokens of the input sequence and the partially</context>
<context position="10871" citStr="Goldberg and Nivre (2012)" startWordPosition="1756" endWordPosition="1759">n on the input buffer, where the token in the stack becomes the head, and the token which is in front of the buffer becomes the dependent. It is also worth noticing that the stack and the input buffer remains unchanged. • Reduce: Pops the top item of the stack if and only if it was previously associated with at least one head. 3.3 Multi-Head Arc Eager Parsing with a Dynamic Oracle In order to design a dynamic oracle with the capability of parsing multi-head dependencies, we need an efficient method for computing the cost of each transition. To this end, we extend the dynamic oracle defined by Goldberg and Nivre (2012), considering DAG parsing arc-eager system of Sagae and Tsujii (2008). Extended arceager transition system will operate in the same way as previously defined in Section 3.2, within a dynamic oracle system whose cost function is defined with a transition operation, the current configuration c = (σ|s, b|0, A)1 and the gold parse of the sentence (Ggold). Differing from Goldberg and Nivre (2012), for ease of computation, we prefer marking transitions as zero cost or costly instead of computing the exact cost: • Cost(LeftAttach, c, Ggold) Attaching s to b with a left arc is costly, if there is a ri</context>
<context position="13002" citStr="Goldberg and Nivre (2012)" startWordPosition="2144" endWordPosition="2147">r configuration from which the gold tree is not reachable), their cost is measured according to the validity of the attachment. The only difference of our multi-head variant from the single head arc-eager transition system is that the left and right arc operations do not change the parser state. As such, it is essentially a relaxed version of the single-head system. Therefore, since the arc-decomposition property holds for the single-head system (as proven by Goldberg and Nivre (2013)), it also holds for our variant. We use the same online training procedure (with the perceptron algorithm) as Goldberg and Nivre (2012) given in Algorithm 1. Algorithm 1 Online training with dynamic oracle 1: procedure TRAIN 2: w ← 0 3: for I← 1, ITERATIONS do 4: c ← cs(x) 5: for sentence x do 6: while c is not terminal do 7: tp ← argmaxtw.Φ(c, t) 8: ZC {t|o(t; c; Ggold) = true} 9: to ← argmaxt,ZCw.Φ(c, t) 10: if tp E� ZC then 11: w ← w + Φ(c, to) − Φ(c, tp) 12: tn ←NEXT(I, tp, ZC) 13: c ← tn(c) 14: procedure NEXT(I, t, ZC) 15: if t E ZC then 16: return t 17: else 18: RANDOM ELEMENT(ZC) The proposed oracle will return a set of zero cost transition operations (denoted as ZC at line 8) where the costs are calculated according t</context>
</contexts>
<marker>Goldberg, Nivre, 2012</marker>
<rawString>Yoav Goldberg and Joakim Nivre. 2012. A dynamic oracle for arc-eager dependency parsing. In COLING, pages 959–976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Joakim Nivre</author>
</authors>
<title>Training deterministic parsers with non-deterministic oracles.</title>
<date>2013</date>
<journal>Transactions of the association for Computational Linguistics,</journal>
<pages>1--403</pages>
<contexts>
<context position="12866" citStr="Goldberg and Nivre (2013)" startWordPosition="2122" endWordPosition="2125">he σ. Since left attach and right attach operations do not change the parser configuration (i.e. these operations cannot lead to a parser configuration from which the gold tree is not reachable), their cost is measured according to the validity of the attachment. The only difference of our multi-head variant from the single head arc-eager transition system is that the left and right arc operations do not change the parser state. As such, it is essentially a relaxed version of the single-head system. Therefore, since the arc-decomposition property holds for the single-head system (as proven by Goldberg and Nivre (2013)), it also holds for our variant. We use the same online training procedure (with the perceptron algorithm) as Goldberg and Nivre (2012) given in Algorithm 1. Algorithm 1 Online training with dynamic oracle 1: procedure TRAIN 2: w ← 0 3: for I← 1, ITERATIONS do 4: c ← cs(x) 5: for sentence x do 6: while c is not terminal do 7: tp ← argmaxtw.Φ(c, t) 8: ZC {t|o(t; c; Ggold) = true} 9: to ← argmaxt,ZCw.Φ(c, t) 10: if tp E� ZC then 11: w ← w + Φ(c, to) − Φ(c, tp) 12: tn ←NEXT(I, tp, ZC) 13: c ← tn(c) 14: procedure NEXT(I, t, ZC) 15: if t E ZC then 16: return t 17: else 18: RANDOM ELEMENT(ZC) The p</context>
</contexts>
<marker>Goldberg, Nivre, 2013</marker>
<rawString>Yoav Goldberg and Joakim Nivre. 2013. Training deterministic parsers with non-deterministic oracles. Transactions of the association for Computational Linguistics, 1:403–414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias T Kromann</author>
</authors>
<title>The danish dependency treebank and the underlying linguistic theory.</title>
<date>2003</date>
<booktitle>In Proc. of the Second Workshop on Treebanks and Linguistic Theories (TLT).</booktitle>
<marker>Kromann, 2003</marker>
<rawString>Matthias T Kromann. 2003. The danish dependency treebank and the underlying linguistic theory. In Proc. of the Second Workshop on Treebanks and Linguistic Theories (TLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan T McDonald</author>
<author>Fernando CN Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In EACL. Citeseer.</booktitle>
<contexts>
<context position="5438" citStr="McDonald and Pereira (2006)" startWordPosition="829" endWordPosition="832"> introduces the DAG parsing framework and the proposed algorithms together with experiments and results. 2 Background Although it is possible to discover the syntactic relations with a two stage model by first finding the regular surface dependencies and then finding the deep relations with post-processing as in Nivre et al. (2010), it is not always straightforward to decide which dependencies should be treated as surface relations or deep relations as in the case of Turkish. Thus, in this study, we focus on single stage models and aim to discover the entire set of relations in a single pass. McDonald and Pereira (2006) use graph-based algorithms for DAG parsing simply using approximate interference in an edge-factored dependency model starting from dependency trees. On the other hand, Sagae and Tsujii (2008) propose a transition-based counterpart for DAG parsing which made available for parsing multi-headed relations. They modified the existing shift-reduce bottom-up dependency parsing algorithm of Nivre and Nilsson (2005) to allow multiple heads per token by the use of cycle removal and pseudo-projectivization as a preprocessing stage. They report higher performance scores on the Danish treebank compared t</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan T McDonald and Fernando CN Pereira. 2006. Online learning of approximate dependency parsing algorithms. In EACL. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
</authors>
<title>Pseudoprojective dependency parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>99--106</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5850" citStr="Nivre and Nilsson (2005)" startWordPosition="888" endWordPosition="891">as surface relations or deep relations as in the case of Turkish. Thus, in this study, we focus on single stage models and aim to discover the entire set of relations in a single pass. McDonald and Pereira (2006) use graph-based algorithms for DAG parsing simply using approximate interference in an edge-factored dependency model starting from dependency trees. On the other hand, Sagae and Tsujii (2008) propose a transition-based counterpart for DAG parsing which made available for parsing multi-headed relations. They modified the existing shift-reduce bottom-up dependency parsing algorithm of Nivre and Nilsson (2005) to allow multiple heads per token by the use of cycle removal and pseudo-projectivization as a preprocessing stage. They report higher performance scores on the Danish treebank compared to McDonald and Pereira (2006). A standard way of determining transition actions in a shift-reduce dependency parser is using static oracles. During the training stage, the learning instances for the oracle are prepared by the use of manually annotated gold-standard parse trees and the current parsing configuration. During the parsing stage, the already trained oracle decides on the next transition operation. </context>
</contexts>
<marker>Nivre, Nilsson, 2005</marker>
<rawString>Joakim Nivre and Jens Nilsson. 2005. Pseudoprojective dependency parsing. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 99–106. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>02</issue>
<contexts>
<context position="9702" citStr="Nivre et al. (2007)" startWordPosition="1540" endWordPosition="1543">o the stack. • Left-Attach: Creates a left arc between the top two items of the stack, where the top item is assigned as the head of the one below. The stack and the buffer remain unchanged. • Right-Attach: Creates a right dependency arc between the two top items on the stack and assigns the top token as the dependent of the token below. As the second step, it pops the top of the stack and places it into the buffer 0. 3.2 Multi-Head Arc-Eager Parsing Algorithm The second transition algorithm introduced but not implemented by Sagae and Tsujii (2008) is a variation of the Arc-Eager algorithm of Nivre et al. (2007) and has the following transition operations: • Shift: Pops the first item of the buffer and pushes it onto the top token of the stack. • Left-Arc: Creates a left dependency arc between the top token of the stack and the first token of the input buffer, where the first token in the buffer becomes the head and the one at the top of the stack becomes the dependent. It is also worth noticing that the stack and the input buffer remains unchanged. • Right-Arc: Creates a right dependency arc between the top token of the stack and the first token on the input buffer, where the token in the stack beco</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi. 2007. Maltparser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13(02):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Laura Rimell</author>
<author>Ryan McDonald</author>
<author>Carlos Gomez-Rodriguez</author>
</authors>
<title>Evaluation of dependency parsers on unbounded dependencies.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>833--841</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5144" citStr="Nivre et al. (2010)" startWordPosition="776" endWordPosition="779">2). We provide comparisons with the replication of the basic shift-reduce DAG parsing algorithm of Sagae and Tsujii (2008) and a first time implementation of their proposed arceager parsing algorithm. The remainder of the paper first gives a background information about the topic, then introduces the DAG parsing framework and the proposed algorithms together with experiments and results. 2 Background Although it is possible to discover the syntactic relations with a two stage model by first finding the regular surface dependencies and then finding the deep relations with post-processing as in Nivre et al. (2010), it is not always straightforward to decide which dependencies should be treated as surface relations or deep relations as in the case of Turkish. Thus, in this study, we focus on single stage models and aim to discover the entire set of relations in a single pass. McDonald and Pereira (2006) use graph-based algorithms for DAG parsing simply using approximate interference in an edge-factored dependency model starting from dependency trees. On the other hand, Sagae and Tsujii (2008) propose a transition-based counterpart for DAG parsing which made available for parsing multi-headed relations. </context>
</contexts>
<marker>Nivre, Rimell, McDonald, Gomez-Rodriguez, 2010</marker>
<rawString>Joakim Nivre, Laura Rimell, Ryan McDonald, and Carlos Gomez-Rodriguez. 2010. Evaluation of dependency parsers on unbounded dependencies. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 833–841. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Algorithms for deterministic incremental dependency parsing.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="16650" citStr="Nivre, 2008" startWordPosition="2799" endWordPosition="2800">f the top token of the stack and the first token of the buffer. • The previous two parsing actions. For training and evaluation purposes, we use the IMST with ten-fold cross validation. Experiment results are given in Table 4. Table 1: Unlabeled scores of experiments with using IMST. Experiment Precision Recall F1 Static-Standard 79.42 77.56 78.50 Static-Eager 78.90 76.79 77.83 Dynamic-Eager 79.68 81.17 80.42 As shown in Table 4, the static arc-eager DAG implementation for Turkish performs slightly worse than the arc-standard algorithm. This is not surprising in the light of previous studies (Nivre, 2008; Eryi˘git et al., 2008) reporting that the arc standard algorithm performs better in tree parsing due to the smaller number of classes to be learned by the oracle. However, the proposed multi-head arc-eager algorithm with dynamic oracle (referred to as Dynamic-Eager) yields the best precision, recall and F1 scores among the three experiments.3 In this study, although there is still room for improvement on performance with more feature engineering, we obtain better results on Turkish IMST treebank between static and dynamic oracles with our newly proposed method for parsing 3The difference of </context>
</contexts>
<marker>Nivre, 2008</marker>
<rawString>Joakim Nivre. 2008. Algorithms for deterministic incremental dependency parsing. Computational Linguistics, 34(4):513–553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Shift-reduce dependency DAG parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1,</booktitle>
<pages>753--760</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4647" citStr="Sagae and Tsujii (2008)" startWordPosition="696" endWordPosition="699">rs existing dependency tree parsers incompatible for this setup. It is also worth mentioning that the deep dependencies in the IMST are not discriminated from surface dependencies by the use of different labels. “Umut fell as [he was] running.” Figure 1: Example for Turkish multi-head dependencies. In this paper, for the first time in the literature, we investigate the impact of using dynamic oracles for parsing multi-head dependency structures by extending the approach of Goldberg and Nivre (2012). We provide comparisons with the replication of the basic shift-reduce DAG parsing algorithm of Sagae and Tsujii (2008) and a first time implementation of their proposed arceager parsing algorithm. The remainder of the paper first gives a background information about the topic, then introduces the DAG parsing framework and the proposed algorithms together with experiments and results. 2 Background Although it is possible to discover the syntactic relations with a two stage model by first finding the regular surface dependencies and then finding the deep relations with post-processing as in Nivre et al. (2010), it is not always straightforward to decide which dependencies should be treated as surface relations </context>
<context position="8551" citStr="Sagae and Tsujii (2008)" startWordPosition="1324" endWordPosition="1327">the tokens being processed, 3. a set A of assigned dependency arcs. The transition actions explained in the following subsections are basic stack and queue operations that correspond to parsing actions marking dependency relations. The algorithm starts with a buffer 0 initialized with all tokens of a sentence preserving their sequence, and an empty stack σ. The parsing process finishes when there are no nodes left in 0 and only the artificial root in σ. 3.1 Basic shift-reduce parsing with multiple heads The first algorithm that is capable of parsing DAG structures is the standard algorithm of Sagae and Tsujii (2008). The complete list of the transitions of this algorithm is as follows: • Shift: Pops the first item of the buffer and pushes it onto the top of the stack. • Left-Reduce: Pops the top two items of the stack, creates a left arc between them where the top item is assigned as the head of the item below, and pushes the head token back onto the stack. • Right-Reduce: Pops the top two items of the stack, creates a right arc between them, where the item below is assigned as the head of the top item, and pushes the head token back onto the stack. • Left-Attach: Creates a left arc between the top two i</context>
<context position="10940" citStr="Sagae and Tsujii (2008)" startWordPosition="1767" endWordPosition="1770">nd the token which is in front of the buffer becomes the dependent. It is also worth noticing that the stack and the input buffer remains unchanged. • Reduce: Pops the top item of the stack if and only if it was previously associated with at least one head. 3.3 Multi-Head Arc Eager Parsing with a Dynamic Oracle In order to design a dynamic oracle with the capability of parsing multi-head dependencies, we need an efficient method for computing the cost of each transition. To this end, we extend the dynamic oracle defined by Goldberg and Nivre (2012), considering DAG parsing arc-eager system of Sagae and Tsujii (2008). Extended arceager transition system will operate in the same way as previously defined in Section 3.2, within a dynamic oracle system whose cost function is defined with a transition operation, the current configuration c = (σ|s, b|0, A)1 and the gold parse of the sentence (Ggold). Differing from Goldberg and Nivre (2012), for ease of computation, we prefer marking transitions as zero cost or costly instead of computing the exact cost: • Cost(LeftAttach, c, Ggold) Attaching s to b with a left arc is costly, if there is a right arc between s and b, or it is already attached with a left arc. •</context>
<context position="14439" citStr="Sagae and Tsujii (2008)" startWordPosition="2407" endWordPosition="2410">t transition operation is chosen by the function NEXT, which returns the transition that is predicted by the model if it belongs to zero cost transitions; if not, it returns a random transition which belongs to the zero cost transition set. 4 Experiments In order to apply the specified DAG parsing algorithm to non-projective sentences, a pseudoprojective transformation operation is applied to the IMST. For that aim, we apply Head scheme2 described by Nivre (2005). Moreover, before the application of this pseudo-projective transformation, the cyclic dependency paths are handled as described by Sagae and Tsujii (2008), by reversing the shortest arc within the cyclic dependency path until no cyclic path remains. 99.3% precision and 99.2% recall are acquired on IMST by applying the pseudo-projective transformation and detransformation operations. As a learning component, we follow the work of Sagae and Tsujii (2008) and use a Maximum Entropy model for the classification with the greedy search algorithm. For the dynamic oracle experiment, we use an averaged perceptron algorithm iterating 15 times over the training data. The following features are used in all of the experiments: • The POS tag and dependency re</context>
</contexts>
<marker>Sagae, Tsujii, 2008</marker>
<rawString>Kenji Sagae and Jun’ichi Tsujii. 2008. Shift-reduce dependency DAG parsing. In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 753–760. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Umut Sulubacak</author>
<author>G¨uls¸en Eryi˘git</author>
</authors>
<title>A redefined Turkish dependency grammar and its implementations: A new Turkish web treebank &amp; the revised Turkish treebank. under review.</title>
<date>2015</date>
<marker>Sulubacak, Eryi˘git, 2015</marker>
<rawString>Umut Sulubacak and G¨uls¸en Eryi˘git. 2015. A redefined Turkish dependency grammar and its implementations: A new Turkish web treebank &amp; the revised Turkish treebank. under review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kun Xu</author>
<author>Sheng Zhang</author>
<author>Yansong Feng</author>
<author>Dongyan Zhao</author>
</authors>
<title>Answering natural language questions via phrasal semantic parsing.</title>
<date>2014</date>
<booktitle>In Natural Language Processing and Chinese Computing,</booktitle>
<pages>333--344</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1926" citStr="Xu et al., 2014" startWordPosition="280" endWordPosition="283">ism that the parser is built upon. Phrase structure parsers, also known as constituency parsers, parse a sentence by splitting it into its smaller constituents. On the other hand, in dependency parsers, the structure of the sentence is represented as dependency trees consisting of directed dependency links between a dependent and a head word. Data-driven dependency parsing frameworks have gained increasing popularity in recent years and been used in a wide range of applications such as machine translation (Ding and Palmer, 2005), textual entailment (Yuret et al., 2013) and question answering (Xu et al., 2014). Most data-driven dependency parsers achieve state-of-the art parsing performances with a language agnostic approach on the different syntactic structures of different languages (Buchholz and Marsi, 2006). Modern data-driven dependency parsers can be categorized into two groups: graph-based and transitionbased parsers. Graph-based parsers rely on the global optimization of models aiming to find spanning trees over dependency graphs. On the other hand, transition-based parsers work basically with greedy local decisions that are deterministically selected by oracles, which are generic machine l</context>
</contexts>
<marker>Xu, Zhang, Feng, Zhao, 2014</marker>
<rawString>Kun Xu, Sheng Zhang, Yansong Feng, and Dongyan Zhao. 2014. Answering natural language questions via phrasal semantic parsing. In Natural Language Processing and Chinese Computing, pages 333–344. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
<author>Laura Rimell</author>
<author>Aydın Han</author>
</authors>
<title>Parser evaluation using textual entailments. Language resources and evaluation,</title>
<date>2013</date>
<pages>47--3</pages>
<contexts>
<context position="1885" citStr="Yuret et al., 2013" startWordPosition="273" endWordPosition="276">s determined according to the grammar formalism that the parser is built upon. Phrase structure parsers, also known as constituency parsers, parse a sentence by splitting it into its smaller constituents. On the other hand, in dependency parsers, the structure of the sentence is represented as dependency trees consisting of directed dependency links between a dependent and a head word. Data-driven dependency parsing frameworks have gained increasing popularity in recent years and been used in a wide range of applications such as machine translation (Ding and Palmer, 2005), textual entailment (Yuret et al., 2013) and question answering (Xu et al., 2014). Most data-driven dependency parsers achieve state-of-the art parsing performances with a language agnostic approach on the different syntactic structures of different languages (Buchholz and Marsi, 2006). Modern data-driven dependency parsers can be categorized into two groups: graph-based and transitionbased parsers. Graph-based parsers rely on the global optimization of models aiming to find spanning trees over dependency graphs. On the other hand, transition-based parsers work basically with greedy local decisions that are deterministically selecte</context>
</contexts>
<marker>Yuret, Rimell, Han, 2013</marker>
<rawString>Deniz Yuret, Laura Rimell, and Aydın Han. 2013. Parser evaluation using textual entailments. Language resources and evaluation, 47(3):639–659.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Analyzing the effect of global learning and beam-search on transition-based dependency parsing.</title>
<date>2012</date>
<booktitle>In COLING (Posters),</booktitle>
<pages>1391--1400</pages>
<contexts>
<context position="2642" citStr="Zhang and Nivre (2012)" startWordPosition="383" endWordPosition="386">guage agnostic approach on the different syntactic structures of different languages (Buchholz and Marsi, 2006). Modern data-driven dependency parsers can be categorized into two groups: graph-based and transitionbased parsers. Graph-based parsers rely on the global optimization of models aiming to find spanning trees over dependency graphs. On the other hand, transition-based parsers work basically with greedy local decisions that are deterministically selected by oracles, which are generic machine learning models trained to make decisions about the next transition action. In a recent study, Zhang and Nivre (2012) propose a new approach on transition-based parsing that aims to provide global learning instead of greedy local decisions. Despite the high performances of both graphbased and transition-based dependency parsers, these are generally bounded by the constraint that each dependent may not have multiple heads. Therefore, the resulting parsing output is a tree where words correspond to nodes and dependency relations correspond to edges. Although dependency trees yield satisfactory performances, they are inadequate in capturing dependencies at different levels of semantic interpretations or more co</context>
</contexts>
<marker>Zhang, Nivre, 2012</marker>
<rawString>Yue Zhang and Joakim Nivre. 2012. Analyzing the effect of global learning and beam-search on transition-based dependency parsing. In COLING (Posters), pages 1391–1400.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>