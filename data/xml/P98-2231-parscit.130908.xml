<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000058">
<title confidence="0.994474">
Structural Disambiguation Based on Reliable
Estimation of Strength of Association
</title>
<author confidence="0.941871">
Haodong Wu Eduardo de Paiva Alves
</author>
<affiliation confidence="0.931338333333333">
Teiji Furugori
Department of Computer Science
University of Electro-Communications
</affiliation>
<address confidence="0.833817">
1-5-1, Chofugaoka, Chofu, Tokyo 1828585, JAPAN
</address>
<email confidence="0.998792">
Nu,ealves,furugoril@phaeton.cs.uec.ac.jp
</email>
<sectionHeader confidence="0.997382" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999952785714286">
This paper proposes a new class-based method
to estimate the strength of association in word
co-occurrence for the purpose of structural dis-
ambiguation. To deal with sparseness of data,
we use a conceptual dictionary as the source
for acquiring upper classes of the words related
in the co-occurrence, and then use t-scores to
determine a pair of classes to be employed for
calculating the strength of association. We have
applied our method to determining dependency
relations in Japanese and prepositional phrase
attachments in English. The experimental re-
sults show that the method is sound, effective
and useful in resolving structural ambiguities.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.991799588235294">
The strength of association between words pro-
vides lexical preferences for ambiguity resolu-
tion. It is usually estimated from statistics on
word co-occurrences in large corpora (Hindle
and Rooth, 1993). A problem with this ap-
proach is how to estimate the probability of
word co-occurrences that are not observed in
the training corpus. There are two main ap-
proaches to estimate the probability: smoothing
methods (e.g., Church and Gale, 1991; Jelinek
and Mercer, 1985; Katz, 1987) and class-based
methods (e.g., Brown et al., 1992; Pereira and
Tishby, 1992; Resnik, 1992; Yaxowsky, 1992).
Smoothing methods estimate the probabil-
ity of the unobserved co-occurrences by using
frequencies of the individual words. For exam-
pie, when eat and bread do not co-occur, the
probability of (eat, bread) would be estimated
by using the frequency of (eat) and (bread).
A problem with this approach is that it pays
no attention to the distributional characteris-
tics of the individual words in question. Using
this method, the probability of (eat, bread) and
(eat, cars) would become the same when bread
and cars have the same frequency. It is unac-
ceptable from the linguistic point of view.
Class-based methods, on the other hand, es-
timate the probabilities by associating a class
with each word and collecting statistics on word
class co-occurrences. For instance, instead of
calculating the probability of (eat, bread) di-
rectly, these methods associate eat with the
class [ingest] and bread with the class [food]
and collect statistics on the classes [ingest] and
[food]. The accuracy of the estimation depends
on the choice of classes, however. Some class-
based methods (e.g., Yarowsky, 1992) associate
each word with a single class without consider-
ing the other words in the co-occurrence. How-
ever, a word may need to be replaced by differ-
ent class depending on the co-occurrence. Some
classes may not have enough occurrences to al-
low a reliable estimation, while other classes
may be too general and include too many words
not relevant to the estimation. An alternative is
to obtain various classes associated in a taxon-
omy with the words in question and select the
classes according to a certain criteria.
There are a number of ways to select the
classes used in the estimation. Weischedel et al.
(1993) chose the lowest classes in a taxonomy
</bodyText>
<page confidence="0.986675">
1416
</page>
<bodyText confidence="0.9997654">
for which the association for the co-occurrence
can be estimated. This approach may result in
unreliable estimates, since some of the class co-
occurrences used may be attributed to chance.
Resnik (1993) selected all pairs of classes corre-
sponding to the head of a prepositional phrase
and weighted them to bias the computation
of the association in favor of higher-frequency
co-occurrences which he considered &amp;quot;more reli-
able.&amp;quot; Contrary to this assumption, high fre-
quency co-occurrences are unreliable when the
probability that the co-occurrence may be at-
tributed to chance is high.
In this paper we propose a class-based
method that selects the lowest classes in a tax-
onomy for which the co-occurrence confidence
is above a threshold. We subsequently apply
the method to solving structural ambiguities
in Japanese dependency structures and English
prepositional phrase attachments.
</bodyText>
<sectionHeader confidence="0.887125" genericHeader="method">
2 Class-based Estimation of
</sectionHeader>
<subsectionHeader confidence="0.7121">
Strength of Association
</subsectionHeader>
<bodyText confidence="0.999901384615385">
The strength of association (SA) may be
measured using the frequencies of word co-
occurrences in large corpora. For instance,
Church and Hanks (1990) calculated SA in
terms of mutual information between two words
wi and w2:
here N is the size of the corpus used in the es-
timation, f (wi ,w2) is the frequency of the co-
occurrence, f (wi ) and f (w2) that of each word.
When no co-occurrence is observed, SA may
be estimated using the frequencies of word
classes that contain the words in question. The
mutual information in this case is estimated by:
</bodyText>
<equation confidence="0.906978">
I (Ci , C2) = log2N * f (Ci , C2)
f (Ci) f (C2) (2)
</equation>
<bodyText confidence="0.997451">
here C1 and C2 are the word classes that respec-
tively contain wi and w2, f(C1) and (C2) the
numbers of occurrences of all the words included
in the word classes CI and C2, and f(C1, C2) is
the number of co-occurrences of the word classes
C1 and C2.
Normally, the estimation using word classes
needs to select classes, from a taxonomy, for
which co-occurrences are significant. We use t-
scores for this purpose&apos;.
For a class co-occurrence (Ci , C2), the t-
score may be approximated by:
</bodyText>
<equation confidence="0.993875333333333">
f (Ci C2) - (CO f (C2)
t
f(C1,C2)
</equation>
<bodyText confidence="0.9989914">
We use the lowest class co-occurrence for
which the confidence measured with t-scores is
above a threshold 2. Given a co-occurrence con-
taining the word w, our method selects a class
for w in the following way:
</bodyText>
<equation confidence="0.9556678">
Step 1: Obtain the classes C1, C2,
L., associ-
ated with w in a taxonomy.
Step 2: Set i to 0.
Step 3: Set i to i +1.
Step 4: Compute t using formula (3).
Step 5: If t &lt; threshold.
If i n goto step 3.
Otherwise exit.
Step 6: Select the class C&amp;quot; to replace w.
</equation>
<bodyText confidence="0.997241">
Let us see what this means with an ex-
ample. Suppose we try to estimate SA for
(produce, telephone)3 . See Table 1. Here f (v),
f (n) and f (vn) are the frequencies for the verb
produce, classes for the noun telephone, and co-
occurrences between the verb and the classes for
telephone, respectively: and t is the t-score4.
&apos;The t-score (Church and Mercer, 1993) compares the
hypothesis that a co-occurrence is significant against the
null hypothesis that the co-occurrence can be attributed
to chance.
2The default threshold for t-score is 1.28 which cor-
responds to a confidence level of 90%. t-scores are often
inflated due to certain violations of assumptions.
3The data was obtained from 68,623 verb-noun pairs
in EDR Corpus (EDR, 1993).
&apos;In our theory, we are to use each pair of (C`, C3),
where i=1,2,...m, j=1,2,...,n, to calculate strengths of
lexical associations. But our experiments show that up-
per classes of a verb are very unreliable to be used to
measure the strengths. The reason may be that, unlike
nouns, the verbs would not have a &amp;quot;neat&amp;quot; hierarchy or
that the upper classes of a verb become too general as
they contain too many concepts underneath them. Be-
cause of this observation, we use, for the classes of a
</bodyText>
<equation confidence="0.8883192">
I(w , w2) =log
f (wi)f (w2)
N * f (wi,w2)
(1)
(3)
</equation>
<page confidence="0.7852">
1417
</page>
<bodyText confidence="0.725325857142857">
verb classes for telephone f(v) f(n) f(vn) t-score
produce concrete thing 671 18926 100 -4.6
produce inanimate object 671 5593 69 0.83
produce implement/tool 671 2138 35 1.91
produce machine 671 664 19 2.86
produce communication machine 671 83 1 0.25
produce telephone 671 24 0
</bodyText>
<tableCaption confidence="0.884438">
Table 1 Estimation of (produce telephone)
</tableCaption>
<bodyText confidence="0.999784">
The lowest class co-occurrence (produce,
communication machine) has a low t-score and
produces a bad estimation. The most frequent
co-occurrence (produce, concrete thing) has a
low t-score also reflecting the fact that it may be
attributed to chance. The t-scores for (produce,
machine) and (produce, implement/tool) are
high and show that these co-occurrences are sig-
nificant. Among them, our method selects the
lowest class co-occurrence for which the t-score
is above the threshold: (produce, machine).
</bodyText>
<sectionHeader confidence="0.996313" genericHeader="method">
3 Disambiguation Using
</sectionHeader>
<subsectionHeader confidence="0.973295">
Class-Based Estimation
</subsectionHeader>
<bodyText confidence="0.999933">
We now apply our method to estimate SA for
two different types of syntactic constructions
and use the results in resolving structural am-
biguities.
</bodyText>
<subsectionHeader confidence="0.999502">
3.1 Disambiguation of Dependency
Relations in Japanese
</subsectionHeader>
<bodyText confidence="0.99951">
Identifying the dependency structure of a
Japanese sentence is a difficult problem since
the language allows relatively free word or-
ders. A typical dependency relation in
Japanese appears in the form of modifier-
particle-modificand triplets. When a modifier is
followed by a number of possible modificands,
verb, the verb itself or, when it does not give us a good
result, only the lowest class of the verb in calculating the
strength of association (SA). Thus, for an example, the
verb eat has a sequence of eat ingest—. put something
into body event concept in the class hierarchy,
but we use only eat and ingest for the verb eat when
calculating SA for (eat, apple).
there arise situations in which syntactic rules
may be unable to determine the dependency re-
lation or the modifier-modificand relation. For
instance, in
</bodyText>
<equation confidence="0.9830535">
I
NI /9 ri3A1F ftER*a
</equation>
<bodyText confidence="0.979991666666667">
it Ai) &apos;(vigorous) may modify either &apos;rI
&apos; (middle aged) or &apos; fVVElg &apos; ( health care).
But which one is the modificand of &apos; It.k) &apos;?
We solve the ambiguity comparing the strength
of association for the two or more possible de-
pendency relations.
Calculation of Strength of Association We cal-
culate the Strength of Association (SA) score
for modifier — particle — modificand by:
</bodyText>
<equation confidence="0.9847325">
SA(m .1&amp;quot;; part,mc) = log2 (N * f (Cm f ler, Part Inc))
(4)
</equation>
<bodyText confidence="0.9998841">
where C&apos;T„fier stands for the classes that in-
clude the modifier word, part is the particle fol-
lowing the modifier, in, the content word in the
modificand phrase, and f the frequency.
Let us see the process of obtaining SA score
in an example ( ?V* - - &lt; ) (literally: profes-
sor - subjectmarker - work). To calculate the
frequencies for the classes associated with
&apos;,we obtain from the Co-occurrence Dictionary
(COD)5 the number of occurrences for (w- 751-
</bodyText>
<tableCaption confidence="0.518528666666667">
&apos;COD and CD are provided by Japan Electronic Dic-
tionary Research Institute (EDR, 1993). COD contains
the frequencies of individual words and of the modifier-
</tableCaption>
<table confidence="0.539811">
f (Crn f ier)i m.)(Part
</table>
<page confidence="0.912955">
1418
</page>
<bodyText confidence="0.9801331">
&lt; &gt;, where w can be any modifier. We then
obtain from the Concept Dictionary (CD)6 the
classes that include titt &apos; and then sum up all
the occurrences of words included in the classes.
The relevant portion of CD for &apos;&apos;in ( ectl
- 75/ - t!)&lt; ) is shown in Figure 1. The numbers
in parenthesis here indicate the summed-up fre-
quencies.
We then calculate the t-score between 75/ -
fltb &lt; &apos;and all the classes that include &apos;kV &apos;. See
</bodyText>
<tableCaption confidence="0.8836295">
Table 2.
Classes for the t- particle-
</tableCaption>
<table confidence="0.8625554">
modifier Ii#t score modificand
Al 4.57 blab &lt;
5.14 blift &lt;
t: A RI 1.74 b&apos;gb &lt;
tz A PA 0.74 effib &lt;
</table>
<tableCaption confidence="0.549111">
Table 2 t-scores for ( - 75/ - &lt; )
</tableCaption>
<bodyText confidence="0.990342315789474">
The t-score for the co-occurrence of the
modifier and particle-modificand pair,
and &apos; - &lt; is higher than the threshold
when &apos; tttl &apos;is replaced with [19111-Cla t.:: Ar].
Using (4), the strength of association for the co-
occurrence of ( - - &lt; ) is calculated from
the SA between the class AM-Mk tz AINJ1 and
&apos; &lt;
When the word in question has more than
one sense, we estimate SA corresponding to each
sense and choose the one that results in the
highest SA score. For instance, we estimate SA
between &apos; ttV &apos; and the various senses of
and choose the highest value: in this case the
one corresponding to the sense &apos;to be employed.&apos;
Determination of Most Strongly Associated
Structure After calculating SA for each possible
construction, we choose the construction with
highest SA score as the most probable struc-
</bodyText>
<footnote confidence="0.8530776">
particle-modificand triplets in a corpus that includes
220,000 parsed Japanese sentences.
6 CD provides a hierarchical structure of concepts cor-
responding to all the words in COD. The number of con-
cepts in CD is about 400,000.
</footnote>
<bodyText confidence="0.826688">
ture. See the following example:
</bodyText>
<equation confidence="0.896235333333333">
I 2.7911 6.13
r---t r---t 1
414M*751 k &lt; A OD A I• 1/ A • • •
</equation>
<bodyText confidence="0.998001184210526">
.technical progress work people stress
innovation
Here, the arrows show possible dependency
relations, the numbers on the arrows the esti-
mated SA, and the thick arrows the dependency
with highest mutual information that means the
most probable dependency relation. In the ex-
ample, tniXte &apos; modifies AAA&apos; and j &lt;
&apos; modifies A&apos;. The estimated mutual informa-
tion for ( MN:VE.751, ) is 2.79 and that for
( * &lt; , A ) is 6.13. Thus, we choose &apos; as
the modificand for IONTV751 &apos; and&apos; A&apos; as that
for &apos; &lt;
In the example shown in Figure 2, our
method selects the most likely modifier-
modificand relation.
Experiment Disambiguation of dependency re-
lations was done using 75 ambiguous con-
structions from Fukumoto (1992). Solving
the ambiguity in the constructions involves
choosing among two or more modifier-particle-
modificand relations. The training data con-
sists of all 568,000 modifier-particle-modificand
triplets in COD.
Evaluation We evaluated the performance of
our method comparing its results with those of
other methods using the same test and training
data. Table 3 shows the various results (suc-
cess rates). Here, (1) indicates the performance
obtained using the principle of Closest Attach-
ment (Kimball, 1973); (2) shows the perfor-
mance obtained using the lowest observed class
co-occurrence (Weischedel et at., 1993); (3) is
the result from the maximum mutual informa-
tion over all pairs of classes corresponding to
the words in the co-occurrence (Resnik, 1993;
Alves, 1996); and (4) shows the performance of
our method&apos;.
</bodyText>
<footnote confidence="0.893234">
7The precision is for the 1.28 default threshold. The
precision was 81.2% and 84.1% when we set the threshold
to .84 and .95. In all these cases the coverage was 92.0%.
</footnote>
<note confidence="0.37254">
na 2.86
</note>
<page confidence="0.916355">
1419
</page>
<figure confidence="0.999121">
Atilt tcArsilLigktRitOtt6t4
(42) I human or similar
Aral
(39) human
4.11
RA • MA Afl-c#XktArgri
person defined by race or origin
13 * A
(3) Japanese
ft --eR tc Aral
person defined by position
A
(3) person (3)
(5) person defined by role
:AAR
(2) worker
(I)
ORO
(I) slave (0) professor
</figure>
<figureCaption confidence="0.589811">
Figure 1 An Extract of CD
</figureCaption>
<figure confidence="0.974936666666667">
•1 47
n a
n a
I121L-t r 4.48 I 11 I
oriv&apos;sc Jp:ffloD 5t11/1 IIJ14
national investigation based cause prompt study expect
</figure>
<figureCaption confidence="0.6394394">
Figure 2 An example of parsing a Japanese sentence
method precision any of the strength of association necessary
70.6% for resolving ambiguity. Other errors occurred
81.2% when the decision could not be made without
82.6% surrounding context.
87.0%
(1) closest attachment
(2) lowest classes
(3) maximum MI
(4) our method
</figureCaption>
<bodyText confidence="0.9492475">
Table 3 Results for determining dependency
relations
Closest attachment (1) has a low perfor-
mance since it fails to take into consideration
the identity of the words involved in the deci-
sion. Selecting the lowest classes (2) often pro-
duces unreliable estimates and wrong decisions
due to data sparseness. Selecting the classes
with highest mutual information (3) results in
overgeneralization that may lead to incorrect at-
tachments. Our method avoids both estimating
from unreliable classes and overgeneralization
and results in better estimates and a better per-
formance.
A qualitative analysis of our results shows
two causes of errors, however. Some errors oc-
curred when there were not enough occurrences
of the particle-modificand pattern to estimate
</bodyText>
<subsectionHeader confidence="0.9911065">
3.2 Prepositional Phrase Attachment
in English
</subsectionHeader>
<bodyText confidence="0.9999052">
Prepositional phrase (PP) attachment is a
paradigm case of syntactic ambiguity. The most
probable attachment may be chosen comparing
the SA between the PP and the various attach-
ment elements. Here SA is measured by:
</bodyText>
<equation confidence="0.954882">
S A(v_attachlv,p, n2) &apos;092 (N * f(Cv.p.C„,))
(N* f (Cni,p,C,i2)\
SA(n_attachlni,P, n2) = 1092 f (C.,)f (P. c.2)
</equation>
<bodyText confidence="0.999468166666667">
where Cu, stands for the class that includes
the word w and f is the frequency in a training
data containing verb-nounl-preposition-noun2
constructions.
Our method selects from a taxonomy the
classes to be used to calculate the SA score and
</bodyText>
<equation confidence="0.547947">
f (C.) f (P. C.2)
</equation>
<page confidence="0.913006">
1420
</page>
<bodyText confidence="0.999254277777778">
then chooses the attachment with highest SA
score as the most probable.
Experiment We performed a PP attachment
experiment on the data that consists of all
the 21,046 semantically annotated verb-noun-
preposition-noun constructions found in EDR
English Corpus. We set aside 500 constructions
for test and used the remaining 20,546 as train-
ing data. We first performed the experiment
using various values for the threshold. Table
4 shows the results. The first line here shows
the default which corresponds to the most likely
attachment for each preposition. For instance,
the preposition of is attached to the noun, re-
flecting the fact that PP&apos;s led by of are mostly
attached to nouns in the training data. The
&apos;confidence&apos; values correspond to a binomial dis-
tribution and are given only as a references.
</bodyText>
<table confidence="0.998421428571429">
confidence t coverage precision success
100% 68.0% 68.0%
50% .00 82% 82.2% 79.4%
70% .52 75% 87.3% 83.4%
80% .84 65% 88.6% 84.2%
85% .95 57% 89.6% 84.8%
90% 1.28 50% 91.3% 85.6%
</table>
<tableCaption confidence="0.973359">
Table 4 Results for PP attachment with
various thresholds for t-score
</tableCaption>
<bodyText confidence="0.95743672">
The precision grows with t-scores, while
coverage decreases. In order to improve cov-
erage, when the method cannot find a class
co-occurrence for which the t-score is above
the threshold, we recursively tried to find a
co-occurrence using the threshold immediately
smaller (see Table 4). When the method could
not find co-occurrences with t-score above the
smallest threshold, the default was used. The
overall success rates are shown in &amp;quot;success&amp;quot; col-
urnn in Table 4.
8 As another way of reducing the sparse data problem,
we clustered prepositions using the method described in
Wtt and Furugori (1996). Prepositions like synonyms
and antonyms are clustered into groups and replaced by
a representative preposition (e.g., till and pending are
replaced by until; amongst, amid and amidst are replaced
by among.).
Evaluation We evaluated the performance of
our method comparing its results with those of
other methods with the same test and training
data. The results are given in Table 5. Here, (5)
shows the performance of two native speakers
who were just presented quadruples of four head
words without surrounding contexts.
</bodyText>
<table confidence="0.982648833333333">
Method Success Rate
(1)closest Attachment 59.6%
(2)lowest classes 80.2%
(3)maximum MI 79.0%
(4)our method 85.6%
(5)human (head words only) 87.0%
</table>
<tableCaption confidence="0.999353">
Table 5 Comparison with other methods
</tableCaption>
<bodyText confidence="0.999954823529412">
The lower bound and the upper bound on
the performance of our method seem to be
59.6% scored by the simple heuristic of closest
attachment (1) and 87.0% by human beings (4).
Obviously, the success rate of closest attach-
ment (1) is low as it always attaches a word to
the noun without considering the words in ques-
tion. The unanticipated low success rate of hu-
man judges is partly due to the fact that some-
times constructions were inherently ambiguous
so that their choices differed from the annota-
tion in the corpus.
Our method (4) performed better than the
lowest classes method (2) and maximum MI
method (3). It owes mainly to the fact that
our method makes the estimation from class co-
occurrences that are more reliable.
</bodyText>
<sectionHeader confidence="0.989213" genericHeader="method">
4 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.9999919">
We proposed a class-based method that selects
classes to be used to estimate the strength of as-
sociation for word co-occurrences. The classes
selected by our method can be used to estimate
various types of strength of association in differ-
ent applications. The method differs from other
class-based methods in that it allows identifica-
tion of a reliable and specific class for each co-
occurrence in consideration and can deal with
date sparseness problem more efficiently. It
</bodyText>
<page confidence="0.97365">
1421
</page>
<bodyText confidence="0.990141857142857">
overcame the shortcomings from other meth-
ods: overgeneralization and employment of un-
reliable class co-occurrences.
We applied our method to two structural
disambiguation experiments. In both exper-
iments the performance is significantly better
than those of others.
</bodyText>
<sectionHeader confidence="0.997886" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999977084507042">
[1] Alves, E. 1996. &amp;quot;The Selection of the Most
Probable Dependency Structure in Japanese
Using Mutual Information.&amp;quot; In Proc. of the
34th ACL, pages 372-374.
[2] Brown, P., Della Pietra, V. and Mercer,
R. (1992). &amp;quot;Word Sense Disambiguation Us-
ing Statistical Methods.&amp;quot; Proceedings of the
30th ACL, pages 264-270.
[31 Church, K., and Mercer, R. 1993. &amp;quot;Introduc-
tion to the Special Issue on Computational
Linguistics Using Large Corpora.&amp;quot; Compu-
tational Linguistics, 19(1):1-24.
[4] Church, K., and Hanks, P. 1990. &amp;quot;Word As-
sociation Norms, Mutual Information and
Lexicography.&amp;quot; Computational Linguistics,
16(1):22-29.
[5] Church, K., and Gale, W. 1991. &amp;quot;A Com-
parison of the Enhanced Good-Turing and
Deleted Estimation Methods for Estimat-
ing Probabilities of English Bigrams.&amp;quot; Com-
puter Speech and Language, 5:19-54.
[6] Fukumoto, F., Sano, H., Saitoh, Y. and
Fukumoto J. 1992. &amp;quot;A Framework for De-
pendency Grammar Based on the Word&apos;s
Modifiability Level - Restricted Dependency
Grammar.&amp;quot; Trans. IPS Japan, 33(10):1211-
1223 (in Japanese).
[7] Hindle, D., and Rooth, M. 1993. &amp;quot;Structural
Ambiguity and Lexical Relations.&amp;quot; Compu-
tational Linguistics, 19(1):103-120.
[8] Japan Electronic Dictionary Research Insti-
tute, Ltd. 1993. EDR Electronic Dictionary
Specifications Guide (in Japanese).
[9] Jelinek, F., and Mercer, R. 1985. &amp;quot;Proba-
bility Distribution Estimation from Sparse
Data.&amp;quot; IBM Technical Disclosure Bulletin,
28:2591-2594.
[10] Katz, S. 1987. &amp;quot;Estimation of Probabili-
ties from Sparse Data for Language Model
Component of a Speech Recognizer.&amp;quot; IEEE
Transactions on Acoustics, Speech and Sig-
nal Processing, ASSP-35(3):400-401.
[11] Kimball, J. 1973. &amp;quot;Seven Principles of
Surface Structure Parsing in Natural Lan-
guage.&amp;quot; cognition, 2:15-47.
[12] Pereira, F. and Tishby, N. 1992. &amp;quot;Distribu-
tional Similarity, Phrase Transitions and Hi-
erarchical Clustering.&amp;quot; In Proc. of the 30th
ACL, pages 183-190.
[13] Resnik, P. 1992. &amp;quot;Wordnet and Distribu-
tional Analysis: A Class-Based Approach
to Lexical Discovery.&amp;quot; AAAI Workshop on
Statistically-based Natural Language Pro-
cessing Techniques, pages 56-64.
[14] Resnik, P. 1993. &amp;quot;Selection and Informa-
tion: A Class-Based Approach to Lexical
Relationships.&amp;quot; PhD. thesis, University of
Pennsylvania.
[15] Weischedel, R., Meteer, M., Schwartz, R.,
Ramshaw, L., and Palmucci, J. 1993. &amp;quot;Cop-
ing with Ambiguity and Unknown Words
Through Probabilistic Models.&amp;quot; Computa-
tional Linguistics, 19(2):359-382.
[16] Wu, H. and Furugori, T. 1996. &amp;quot;A Hy-
brid Disambiguation Model for Preposi-
tional Phrase Attachment.&amp;quot; Literary and
Linguistic Computing. 11(4):187-192.
[17] Yarowsky, D. 1992. &amp;quot;Word Sense Disam-
biguation using Statistical Models of Roget&apos;s
Categories Trained on Large Corpora.&amp;quot; Pro-
ceedings of COLING-92, pages 454-460.
</reference>
<page confidence="0.993196">
1422
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.551060">
<title confidence="0.99208">Structural Disambiguation Based on Reliable Estimation of Strength of Association</title>
<author confidence="0.8780215">Eduardo de_Paiva Alves Teiji Furugori</author>
<affiliation confidence="0.999874">Department of Computer Science University of Electro-Communications</affiliation>
<address confidence="0.985053">1-5-1, Chofugaoka, Chofu, Tokyo 1828585, JAPAN</address>
<email confidence="0.748726">Nu,ealves,furugoril@phaeton.cs.uec.ac.jp</email>
<abstract confidence="0.9996576">This paper proposes a new class-based method to estimate the strength of association in word co-occurrence for the purpose of structural disambiguation. To deal with sparseness of data, we use a conceptual dictionary as the source for acquiring upper classes of the words related in the co-occurrence, and then use t-scores to determine a pair of classes to be employed for calculating the strength of association. We have applied our method to determining dependency relations in Japanese and prepositional phrase attachments in English. The experimental results show that the method is sound, effective and useful in resolving structural ambiguities.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Alves</author>
</authors>
<title>The Selection of the Most Probable Dependency Structure in Japanese Using Mutual Information.&amp;quot;</title>
<date>1996</date>
<booktitle>In Proc. of the 34th ACL,</booktitle>
<pages>372--374</pages>
<marker>[1]</marker>
<rawString>Alves, E. 1996. &amp;quot;The Selection of the Most Probable Dependency Structure in Japanese Using Mutual Information.&amp;quot; In Proc. of the 34th ACL, pages 372-374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>Della Pietra</author>
<author>V</author>
<author>R Mercer</author>
</authors>
<title>Word Sense Disambiguation Using Statistical Methods.&amp;quot;</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<booktitle>Proceedings of the 30th ACL,</booktitle>
<pages>264--270</pages>
<marker>[2]</marker>
<rawString>Brown, P., Della Pietra, V. and Mercer, R. (1992). &amp;quot;Word Sense Disambiguation Using Statistical Methods.&amp;quot; Proceedings of the 30th ACL, pages 264-270. [31 Church, K., and Mercer, R. 1993. &amp;quot;Introduction to the Special Issue on Computational Linguistics Using Large Corpora.&amp;quot; Computational Linguistics, 19(1):1-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>P Hanks</author>
</authors>
<title>Word Association Norms, Mutual Information and Lexicography.&amp;quot;</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<pages>16--1</pages>
<marker>[4]</marker>
<rawString>Church, K., and Hanks, P. 1990. &amp;quot;Word Association Norms, Mutual Information and Lexicography.&amp;quot; Computational Linguistics, 16(1):22-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>W Gale</author>
</authors>
<title>A Comparison of the Enhanced Good-Turing and Deleted Estimation Methods for Estimating Probabilities of English Bigrams.&amp;quot;</title>
<date>1991</date>
<journal>Computer Speech and Language,</journal>
<pages>5--19</pages>
<marker>[5]</marker>
<rawString>Church, K., and Gale, W. 1991. &amp;quot;A Comparison of the Enhanced Good-Turing and Deleted Estimation Methods for Estimating Probabilities of English Bigrams.&amp;quot; Computer Speech and Language, 5:19-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Fukumoto</author>
<author>H Sano</author>
<author>Y Saitoh</author>
<author>J Fukumoto</author>
</authors>
<title>A Framework for Dependency Grammar Based on the Word&apos;s Modifiability Level - Restricted Dependency Grammar.&amp;quot; Trans.</title>
<date>1992</date>
<journal>IPS Japan,</journal>
<pages>33--10</pages>
<note>(in Japanese).</note>
<marker>[6]</marker>
<rawString>Fukumoto, F., Sano, H., Saitoh, Y. and Fukumoto J. 1992. &amp;quot;A Framework for Dependency Grammar Based on the Word&apos;s Modifiability Level - Restricted Dependency Grammar.&amp;quot; Trans. IPS Japan, 33(10):1211-1223 (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
<author>M Rooth</author>
</authors>
<title>Structural Ambiguity and Lexical Relations.&amp;quot;</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--1</pages>
<marker>[7]</marker>
<rawString>Hindle, D., and Rooth, M. 1993. &amp;quot;Structural Ambiguity and Lexical Relations.&amp;quot; Computational Linguistics, 19(1):103-120.</rawString>
</citation>
<citation valid="true">
<date>1993</date>
<booktitle>EDR Electronic Dictionary Specifications Guide (in Japanese).</booktitle>
<institution>Japan Electronic Dictionary Research Institute, Ltd.</institution>
<marker>[8]</marker>
<rawString>Japan Electronic Dictionary Research Institute, Ltd. 1993. EDR Electronic Dictionary Specifications Guide (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>R Mercer</author>
</authors>
<title>Probability Distribution Estimation from Sparse Data.&amp;quot;</title>
<date>1985</date>
<journal>IBM Technical Disclosure Bulletin,</journal>
<pages>28--2591</pages>
<marker>[9]</marker>
<rawString>Jelinek, F., and Mercer, R. 1985. &amp;quot;Probability Distribution Estimation from Sparse Data.&amp;quot; IBM Technical Disclosure Bulletin, 28:2591-2594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Katz</author>
</authors>
<title>Estimation of Probabilities from Sparse Data for Language Model Component of a Speech Recognizer.&amp;quot;</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics, Speech and Signal Processing,</journal>
<pages>35--3</pages>
<marker>[10]</marker>
<rawString>Katz, S. 1987. &amp;quot;Estimation of Probabilities from Sparse Data for Language Model Component of a Speech Recognizer.&amp;quot; IEEE Transactions on Acoustics, Speech and Signal Processing, ASSP-35(3):400-401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kimball</author>
</authors>
<title>Seven Principles of Surface Structure Parsing in Natural Language.&amp;quot;</title>
<date>1973</date>
<tech>cognition,</tech>
<pages>2--15</pages>
<marker>[11]</marker>
<rawString>Kimball, J. 1973. &amp;quot;Seven Principles of Surface Structure Parsing in Natural Language.&amp;quot; cognition, 2:15-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>N Tishby</author>
</authors>
<title>Distributional Similarity, Phrase Transitions and Hierarchical Clustering.&amp;quot;</title>
<date>1992</date>
<booktitle>In Proc. of the 30th ACL,</booktitle>
<pages>183--190</pages>
<marker>[12]</marker>
<rawString>Pereira, F. and Tishby, N. 1992. &amp;quot;Distributional Similarity, Phrase Transitions and Hierarchical Clustering.&amp;quot; In Proc. of the 30th ACL, pages 183-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Wordnet and Distributional Analysis: A Class-Based Approach to Lexical Discovery.&amp;quot;</title>
<date>1992</date>
<booktitle>AAAI Workshop on Statistically-based Natural Language Processing Techniques,</booktitle>
<pages>56--64</pages>
<marker>[13]</marker>
<rawString>Resnik, P. 1992. &amp;quot;Wordnet and Distributional Analysis: A Class-Based Approach to Lexical Discovery.&amp;quot; AAAI Workshop on Statistically-based Natural Language Processing Techniques, pages 56-64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Selection and Information: A Class-Based Approach to Lexical Relationships.&amp;quot;</title>
<date>1993</date>
<tech>PhD. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<marker>[14]</marker>
<rawString>Resnik, P. 1993. &amp;quot;Selection and Information: A Class-Based Approach to Lexical Relationships.&amp;quot; PhD. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Weischedel</author>
<author>M Meteer</author>
<author>R Schwartz</author>
<author>L Ramshaw</author>
<author>J Palmucci</author>
</authors>
<title>Coping with Ambiguity and Unknown Words Through Probabilistic Models.&amp;quot;</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<marker>[15]</marker>
<rawString>Weischedel, R., Meteer, M., Schwartz, R., Ramshaw, L., and Palmucci, J. 1993. &amp;quot;Coping with Ambiguity and Unknown Words Through Probabilistic Models.&amp;quot; Computational Linguistics, 19(2):359-382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Wu</author>
<author>T Furugori</author>
</authors>
<title>A Hybrid Disambiguation Model for Prepositional Phrase Attachment.&amp;quot; Literary and Linguistic Computing.</title>
<date>1996</date>
<pages>11--4</pages>
<marker>[16]</marker>
<rawString>Wu, H. and Furugori, T. 1996. &amp;quot;A Hybrid Disambiguation Model for Prepositional Phrase Attachment.&amp;quot; Literary and Linguistic Computing. 11(4):187-192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Word Sense Disambiguation using Statistical Models of Roget&apos;s Categories Trained on Large Corpora.&amp;quot;</title>
<date>1992</date>
<booktitle>Proceedings of COLING-92,</booktitle>
<pages>454--460</pages>
<marker>[17]</marker>
<rawString>Yarowsky, D. 1992. &amp;quot;Word Sense Disambiguation using Statistical Models of Roget&apos;s Categories Trained on Large Corpora.&amp;quot; Proceedings of COLING-92, pages 454-460.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>