<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001089">
<title confidence="0.6320635">
Automatic Identification of Discourse Moves in Scientific Article
Introductions
</title>
<author confidence="0.884634">
Nick Pendar and Elena Cotos
</author>
<affiliation confidence="0.9301885">
Applied Linguistics and Technology Program
Iowa State University
</affiliation>
<address confidence="0.679131">
Ames, IA 50011 USA
</address>
<email confidence="0.996928">
{pendar,ecotos}@iastate.edu
</email>
<sectionHeader confidence="0.996628" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994054">
This paper reports on the first stage of build-
ing an educational tool for international gradu-
ate students to improve their academic writing
skills. Taking a text-categorization approach,
we experimented with several models to au-
tomatically classify sentences in research ar-
ticle introductions into one of three rhetori-
cal moves. The paper begins by situating the
project within the larger framework of intelli-
gent computer-assisted language learning. It
then presents the details of the study with very
encouraging results. The paper then concludes
by commenting on how the system may be im-
proved and how the project is intended to be
pursued and evaluated.
</bodyText>
<sectionHeader confidence="0.969516" genericHeader="categories and subject descriptors">
1 Introduction and Background
</sectionHeader>
<bodyText confidence="0.99996662">
Interest in automated evaluation systems in the field
of language assessment has been growing rapidly
in the last few years. Performance-based and
high-stakes standardized tests (e.g., ACT, GMAT,
TOEFL, etc.) have employed such systems due to
their potential to yield evidence about the learners’
language proficiency and/or subject matter mastery
based on analyses of their constructed responses.
Automated writing evaluation applications are also
beginning to draw the attention of pedagogues who
are much interested in assessment for learning, i.e.,
assessment used as a tool in gaining direction for
remediation. Arguably, these technological innova-
tions open up a wide range of possibilities for high-
quality formative evaluation that can closely match
teaching goals and tailor instruction to individual
learners by providing them with feedback and direc-
tion on their attainment of knowledge.
Traditionally, automated evaluation has been used
for essay grading, but its potential could be success-
fully extrapolated to other genres in both first lan-
guage (L1) and second language (L2) academic con-
texts. Existing scoring systems can assess various
constructs such as topical content, grammar, style,
mechanics, syntactic complexity, and even deviance
or plagiarism (Burstein, 2003; Elliott, 2003; Lan-
dauer et al., 2003; Mitchell et al., 2002; Page, 2003;
Rudner and Liang, 2002). Because learner writing
is generally highly erroneous, an emerging research
trend has focused on automated error detection in
L2 output finding novel approaches to develop in-
telligent ways to assess ill-formed learner responses
(Burstein and Chodorow, 1999; Chodorow et al.,
2007; Han et al., 2006; Leacock and Chodorow,
2003). Various NLP and statistical techniques also
allow for the evaluation of text organization, which
is however limited to recognizing the five-paragraph
essay format, thesis, and topic sentences. At present,
to our knowledge, there is only one automated eval-
uation system, AntMover (Anthony and Lashkia,
2003), that applies intelligent technological possibil-
ities to the genre of research reports—a major chal-
lenge for new non-native speaker (NNS) members of
academia. AntMover is able to automatically iden-
tify the structure of abstracts in various fields and
disciplines.
Academic writing pedagogues have been strug-
gling to find effective ways to teach academic writ-
ing. Frodesen (1995) argues that the writing instruc-
tion for non-native speaker students should “help
</bodyText>
<page confidence="0.985118">
62
</page>
<note confidence="0.7371455">
Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 62–70,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999989830508475">
initiate writers into their field-specific research com-
munities” (p. 333). In support of this opinion,
(Kushner, 1997) reasons that graduate NNS courses
have to combine language and discourse with the
skill of writing within professional norms. Vari-
ous pedagogical approaches have been attempted to
achieve this goal. For instance, (Vann and Myers,
2001) followed the inductive analysis approach, in
which students examined the format, content, gram-
matical, and rhetorical conventions of each section
of research reports. Supplements to this approach
were tasks that required students to write journal en-
tries about the rhetorical conventions of prominent
journals in their disciplines and tasks that placed
the experience of writing up research “in the frame-
work of an interactive, cooperative effort with cross-
cultural interaction” (Vann and Myers, 2001, p. 82).
Later, after having followed a primarily skill-based
approach, in which students wrote field-specific lit-
erature reviews, summaries, paraphrases, data com-
mentaries, and other discipline-specific texts, Levis
and Levis-Muller (2003) reported on transforming
the course into a project-based writing one. The
project consisted of carrying out original research,
the topic of which, for the purpose of coping with
discipline diversity, was the same for all students
and was determined by the instructor. From the
start, the students were provided with a limited set
of articles, for instance, on cross-cultural adjust-
ment, with which they worked to identify potential
research questions for a further investigation and to
write the literature review. This approach placed a
heavy emphasis on collaboration as students worked
in small groups on developing data-collection instru-
ments and on data analysis. Oral presentations on
group-research projects wrapped up the course.
The academic writing course discussed in the
paragraph above is corpus- and genre-based, com-
bining a top-down approach to genre analysis and
a bottom-up approach to the analysis of corpora
(Cortes, 2006). Cortes (2006) explains that the
course was designed to better address the issues of
genre-specificity and disciplinarity since some stu-
dents who took the previous form of the course
claimed that, although they were taught useful
things, they did not learn to write the way re-
searchers in their disciplines generally do. In the
present format of the course, each student is pro-
vided with a corpus of research articles published in
top journals of his/her discipline. Students conduct
class analyses of their corpus according to guide-
lines from empirical findings in applied linguistics
about the discourse tendencies in research article
writing. Their task is to discover organizational and
linguistic patterns characteristic of their particular
discipline, report on their observations, and apply
the knowledge they gain from the corpus analyses
when writing a research article for the final project
in the course.
</bodyText>
<sectionHeader confidence="0.969903" genericHeader="method">
2 Motivation
</sectionHeader>
<bodyText confidence="0.999715117647059">
Although each of the pedagogical approaches men-
tioned in the previous section has its advantages,
they all fail to provide NNS students with sufficient
practice and remediational guidance through exten-
sive individualized feedback during the process of
writing. An NLP-based academic discourse eval-
uation software application could account for this
drawback if implemented as an additional instruc-
tional tool. However, an application with such ca-
pabilities has not yet been developed. Moreover, as
mentioned above, the effects of automated formative
feedback are not fully investigated. The long-term
goal of this research project is the design and imple-
mentation of a new automated discourse evaluation
tool as well as the analysis of its effectiveness for
formative assessment purposes. Named IADE (In-
telligent Academic Discourse Evaluator), this appli-
cation will draw from second language acquisition
models such as interactionist views and Systemic
Functional Linguistics as well as from the Skill Ac-
quisition Theory of learning. Additionally, it will be
informed by empirical research on the provision of
feedback and by Evidence Centered Design princi-
ples (Mislevy et al., 2006).
IADE will evaluate students’ drafts of their aca-
demic writing in accordance with the course materi-
als in terms of an adapted model of Swales’ (Swales,
1990; Swales, 2004) move schema as partially pre-
sented in Table 1. IADE will achieve this by con-
ducting a sentence-level classification of the input
text for rhetorical shifts. Given a draft of a research
article, IADE will identify the discourse moves in
the paper, compare it with other papers in the same
discipline and provide feedback to the user.
</bodyText>
<page confidence="0.999861">
63
</page>
<tableCaption confidence="0.994721">
Table 1: Discourse move model for research article intro-
ductions based on (Swales, 1990; Swales, 2004)
</tableCaption>
<bodyText confidence="0.9997828">
The development of IADE is guided by the prin-
ciples of Evidence Centered Design (ECD), “an
approach to constructing and implementing edu-
cational assessments in terms of evidentiary argu-
ments” (Mislevy et al., 2006, p. 15). This design
allows the program to identify the discourse ele-
ments of students’ work products that constitute ev-
idence and to characterize the strength of this evi-
dence about the writing proficiencies targeted for the
purpose of formative assessment.
</bodyText>
<sectionHeader confidence="0.989253" genericHeader="method">
3 Discourse Move Identification
</sectionHeader>
<subsectionHeader confidence="0.998585">
3.1 Data and Annotation Scheme
</subsectionHeader>
<bodyText confidence="0.99995">
The discussions above imply that the first step in
the development of IADE is automatic identifica-
tion of discourse moves in research articles. We
have approached this task as a classification prob-
</bodyText>
<figure confidence="0.513932363636364">
Discipline Files
1. Accounting 20
2. Aero-space engineering 20
3. Agronomy 21
4. Applied linguistics 20
5. Architecture 20
6. Biology 20
7. Business 20
8. Chemical engineering 20
9. Computer engineering 20
10. Curriculum and instruction 20
11. Economics 20
12. Electrical engineering and power 20
system
13. Environmental engineering 20
14. Food science &amp; food service 20
15. Health &amp; human performance 20
16. Industrial engineering 20
17. Journalism 20
18. Mechanical engineering 20
19. Sociology 20
20. Urban and regional planning 20
</figure>
<tableCaption confidence="0.923707">
Table 2: Disciplines represented in the corpus for article
introductions
</tableCaption>
<bodyText confidence="0.999913476190476">
lem. In other words, given a sentence and a finite set
of moves and steps, what move/step does the sen-
tence signify? This task is very similar to identi-
fying the discourse structure of short argumentative
essays discussed in (Burstein et al., 2003), the dif-
ference being in the genre of the essays and type of
the discourse functions in question.
The corpus used in this study was compiled from
an existing corpus of published research articles in
44 disciplines, used in an academic writing graduate
course for international students. The corpus con-
tains 1,623 articles and 1,322,089 words. The aver-
age length of articles is 814.09 words. We made a
stratified sampling of 401 introduction sections rep-
resentative of 20 academic disciplines (see Table 2)
from this corpus of research articles. The size of this
sub-corpus is 267,029 words; each file is on average
665.91 words long, resulting in 11,149 sentences as
data instances.
The sub-corpus was manually annotated based on
Swales’ framework by one of the authors for moves
</bodyText>
<table confidence="0.721577384615385">
Move 1 Establishing a Territory
Step 1: Claiming Centrality
Step 2: Making topic generalization(s)
and/or
Step 3: Reviewing previous research
Move 2 Establishing a niche
Step 1A: Indicating a gap or
Step 1B: Highlighting a problem or
Step 1C: Question-raising or
Step 1D: Hypothesizing or
Step 1E: Adding to what is known or
Step 1F: Presenting justification
Move 3 Occupying the niche
Step 1A: Announcing present research de-
scriptively or
Step 1: Announcing present research pur-
posefully
Step 2A: Presenting research questions or
Step 2B: Presenting hypotheses
Step 3: Definitional clarifications and/or
Step 4: Summarizing methods and/or
Step 5: Announcing principal outcomes
and/or
Step 6: Stating the value of the present re-
search and/or
Step 7: Outlining the structure of the paper
</table>
<page confidence="0.996429">
64
</page>
<bodyText confidence="0.9585234">
and steps (see Figure 1 for an example). The markup
scheme includes the elements presented in Table 1.
Annotation was performed at sentence level, each
sentence being assigned at least one move and al-
most always a step within that move as specified in
the markup scheme.1 The scheme allowed for mul-
tiple layers of annotation for cases when the same
sentence signified more than one move or more than
one step. This made it possible to capture an array
of the semantic shades rendered by a given sentence.
&lt;intro_m3 step=&amp;quot;description&amp;quot;&gt;
&lt;intro_m3 step=&amp;quot;method&amp;quot;&gt;
&lt;intro_m3 step=&amp;quot;purpose&amp;quot;&gt;
This paper presents an
application of simulation,
multivariate statistics,
and simulation metamodels
to analyze throughput of
multiproduct batch chemical
plants.
</bodyText>
<figure confidence="0.967140333333333">
&lt;/intro_m3&gt;
&lt;/intro_m3&gt;
&lt;/intro_m3&gt;
</figure>
<figureCaption confidence="0.999884">
Figure 1: A sample annotated sentence
</figureCaption>
<subsectionHeader confidence="0.987835">
3.2 Feature Selection
</subsectionHeader>
<bodyText confidence="0.999894764705882">
In order to classify sentences correctly, we first
need to identify features that can reliably indicate
a move/step. We have taken a text-categorization
approach to this problem.2 In this framework each
sentence is treated as a data item to be classified,
and is represented as an n-dimensional vector in the
Rn Euclidean space. More formally, a sentence si
is represented as the vector si = (f1,f2,..., fn)
where each component fj of the vector �si repre-
sents a measure of feature j in the sentence si. The
task of the learning algorithm is to find a function
F : S -4 C that would map the sentences in the cor-
pus S to classes in M = {m1, m2, m3} (where m1,
m2, and m3 stand for Move 1, Move 2, and Move 3,
respectively). In this paper, for simplicity, we are as-
suming that F is a many-to-one function; however,
it should be kept in mind that since sentences may
</bodyText>
<footnote confidence="0.9987055">
1Only in two instances a step was not assigned.
2For an excellent review, see (Sebastiani, 2002).
</footnote>
<bodyText confidence="0.9960655">
signify multiple moves, in reality the relation may
be many-to-many.
An important problem here is choosing features
that would allow us to classify our data instances
into the classes in question properly. In this study
we focused on automatically identifying the major
moves in the introduction section of research articles
(i.e., m1, m2, m3). Due to the sparseness of data, we
have not attempted to identify the steps within the
moves at this time.
We extracted word unigrams, bigrams and tri-
grams (i.e., single words, two word sequences, and
three word sequences) from the annotated corpus.
Subsection 3.5 reports the results of some of our ex-
periments with these feature sets.
The following steps were taken in preprocessing:
</bodyText>
<listItem confidence="0.974606315789474">
1. All tokens were stemmed using the NLTK3
port of the Porter Stemmer algorithm (Porter,
1980). This allows us to represent lexically re-
lated items as the same feature, thus reducing
interdependence among features and also help-
ing with the sparse data problem.
2. All numbers in the texts were replaced by the
string _number_.
3. In case of bigrams and trigrams, the tokens in-
side each n-gram were alphabetized to capture
the semantic similarity among n-grams con-
taining the same words but in a different or-
der. This tactic also reduces interdependence
among features and helps with the sparse data
problem.
4. All n-grams with a frequency of less than five
were excluded. This measure was also taken
to avoid overfitting the classifier to the training
data.
</listItem>
<bodyText confidence="0.9999402">
The total number of each set of n-grams extracted is
shown in Table 3.
To identify which n-grams are better indicators
of moves, odds ratios were calculated for each as
follows:
</bodyText>
<equation confidence="0.9660595">
OR(ti,mj) = p(ti|mj) - (1 − p(ti|�mj)) (1)
(1 − p (ti |mj)) - p (ti  |�mj)
</equation>
<footnote confidence="0.937833">
3http://www.nltk.org
</footnote>
<page confidence="0.990945">
65
</page>
<table confidence="0.9846005">
n-gram Number
unigrams 3,951
bigrams 8,916
trigrams 3,605
</table>
<tableCaption confidence="0.999542">
Table 3: Total number of n-grams extracted
</tableCaption>
<bodyText confidence="0.994777857142857">
where OR(ti, mj) is the odds ratio of the term (n-
gram) ti occurring in move mj; p(ti|mj) is the prob-
ability of seeing the term ti given the move mj;
and p(ti |mj) is the probability of seeing the term
ti given any move other than mj. The above condi-
tional probabilities are calculated as maximum like-
lihood estimates.
</bodyText>
<equation confidence="0.998019333333333">
p(ti|mj) = N count (ti in mj) (2)
Y:
k= count(tk in mj)
</equation>
<bodyText confidence="0.999968833333333">
where N is the total number of n-grams in the cor-
pus of sentences S.
Finally, we selected terms with maximum odds
ratios as features. Subsection 3.5 reports on our ex-
periments with classifiers using n-grams with high-
est odds ratios.
</bodyText>
<subsectionHeader confidence="0.999012">
3.3 Sentence Representation
</subsectionHeader>
<bodyText confidence="0.999966">
As mentioned in the previous subsection, each sen-
tence is represented as a vector, where each vector
component fi represents a measure of feature i in the
sentence. Usually, in text categorization this mea-
sure is calculated as what is commonly known as the
tf.idf (term frequency times the inverse document
frequency), which is a measure of the importance
of a term in a document. However, since our “doc-
uments” are all sentences and therefore very short,
we decided to only record the presence or absence
of terms in the sentences as Boolean values; that is,
a vector component will contain either a 0 for the
absence of the corresponding term or a 1 for its pres-
ence in the sentence.
</bodyText>
<subsectionHeader confidence="0.775255">
3.4 Classifier
</subsectionHeader>
<bodyText confidence="0.999990148148149">
We chose to use Support Vector Machines (SVM)
for our classifier (Basu et al., 2003; Burges, 1998;
Cortes and Vapnik, 1995; Joachims, 1998; Vapnik,
1995). SVMs are commonly used to solve classifica-
tion problems by finding hyperplanes that best clas-
sify data while providing the widest margin possible
between classes. SVMs have proven to be among
the most powerful classifiers provided that the repre-
sentation of the data captures the patterns we are try-
ing to discover and that the parameters of the SVM
classifier itself are properly set.
SVM learning is a supervised learning technique
where the system is provided a set of labeled data
for training. The performance of the system is then
measured by providing the learned model a set of
new (labeled) data, which were not present during
the training phase. The system then applies the
learned model on the new data and provides its own
inferred labels. The labels provided by the system
are then compared with the “true” labels already
available. In this study, we used a common tech-
nique known as v-fold cross validation, in which
data are divided into v equal-sized groups (either by
random sampling or by stratified sampling). Then,
the system is trained on all but one of the groups and
tested on the remaining group. This process is re-
peated v times until all data items have been used
in training and validation. This technique provides
a fairly accurate view of how a model built on the
whole data set will perform when given completely
new data. All the results reported in the following
subsection are based on five-fold cross validation ex-
periments.
We predominantly used the machine learning en-
vironment RAPIDMINER (Mierswa et al., 2006) in
the experimentation phase of the project. The SVMs
were set to use the RBF kernel, which maps samples
into a higher dimensional space allowing for captur-
ing non-linear relationships among the data and la-
bels. The RBF kernel has two parameters, C and -y.
These parameters help against overfitting the clas-
sifier on the training data. The values of these pa-
rameters is not known before hand for each data set
and may be found through an exhaustive search of
different parameter settings (Hsu et al., 2008). In
this study, we used C = 23 and -y = 2−9, which
were arrived at through a search of different param-
eter settings on the feature set with 3,000 unigrams.
The search was performed by performing five-fold
cross validation on the whole data set using models
built with various combinations of C and -y values.
Admittedly, these parameters are not necessarily the
best parameters for the other feature sets on which
exhaustive searches should be performed. This is
</bodyText>
<page confidence="0.973914">
66
</page>
<bodyText confidence="0.916207">
the next step in our project.
</bodyText>
<subsectionHeader confidence="0.578016">
3.5 Evaluation
</subsectionHeader>
<bodyText confidence="0.99850875">
We performed five-fold cross validation on 14 dif-
ferent feature sets as summarized in Table 4. The
results of these experiments are summarized in Fig-
ures 2–4. Accuracy shows the proportion of clas-
sifications that agreed with the manually assigned
labels. The other two performance measures, pre-
cision and recall, are commonly used in information
retrieval, text categorization, and other NLP appli-
cations. For each category, precision measure what
proportion of the items assigned to that category ac-
tually belonged to it, and recall measures what pro-
portion of the items actually belonging to a cate-
gory were labeled correctly. The measures reported
here (macro-precision ˆ�M and macro-recall ˆ�M) are
weighted means of class precision and recall over
the three moves.
</bodyText>
<equation confidence="0.953203777777778">
TP
ˆ�µ = (3)
TP+FP
µ _ TP 4
P TP + FN ( )
irM = EIC|1 wit (5)
ICI
pM = EI°4 wipi (6)
ICI
</equation>
<bodyText confidence="0.999972842105263">
The figures show that the unigram models result
in the best recall and the trigram models, the best
precision. Generally, we attribute lower recall to the
sparseness of the data. Access to more training data
will help improve recall. We should also note the
behavior of the models with respect to bigram fea-
tures. As seen on Figures 3 and 4, increasing the size
of the bigram feature set causes a decline in model
precision and a rise in model recall. Considering that
there are far more frequent bigrams than unigrams or
trigrams (cf. Table 4), this behavior is not surprising.
Including more bigrams will increase recall because
there are more possible phrases to indicate a move,
but that will also result in a decline in precision be-
cause those bigrams may also frequently appear in
other moves. It also seems that a model employing
unigram, bigram and trigrams all will perform bet-
ter than each individual model. We are planning to
experiment with these feature sets, as well.
</bodyText>
<figure confidence="0.9954968">
Terms N
1 Unigrams 1,000
2 2,000
3 3,000
4 Bigrams 1,000
5 2,000
6 3,000
7 4,000
8 5,000
9 6,000
10 7,000
11 8,000
12 Trigrams 1,000
13 2,000
14 3,000
</figure>
<tableCaption confidence="0.922844">
Table 4: Feature sets used in experiments
</tableCaption>
<figureCaption confidence="0.99975">
Figure 2: Model accuracy for different feature sets
</figureCaption>
<bodyText confidence="0.999805083333333">
Error analysis revealed that Move 2 is the hardest
move to identify. It most frequently gets misclassi-
fied as Move 1. In the future, it might be helpful to
make use of the relative position of the sentence in
text in order to disambiguate the move involved. In
addition, further investigation is needed to see what
percentage of Move 2 sentences identified as Move 1
by the system also have been labeled Move 1 by the
annotator. Recall that some of the sentences had
multiple labels and in this study we are only con-
sidering single labels per sentence.
One question that might arise is how much infor-
</bodyText>
<page confidence="0.998545">
67
</page>
<subsectionHeader confidence="0.94895">
3.6 Interannotator agreement
</subsectionHeader>
<bodyText confidence="0.999983571428571">
In order to get a clearer picture of the difficulty of
the problem, we asked a second annotator to anno-
tate a portion of the sub-corpus used in this study.
The second annotations were done on a sample of
files across all the disciplines adding up to 487 sen-
tences. Table 5 contains a summary of the agree-
ments between the two annotators.
</bodyText>
<table confidence="0.99566675">
Move 1 Move 2 Move 3
No. agreed 457 452 480
P(A) 0.938 0.928 0.986
K 0.931 0.919 0.984
</table>
<tableCaption confidence="0.999234">
Table 5: Interannotator agreement on 487 sentences.
</tableCaption>
<figureCaption confidence="0.999814">
Figure 3: Model precision for different feature sets
Figure 4: Model recall for different feature sets
</figureCaption>
<bodyText confidence="0.999945916666667">
mation about the discipline of the article contributes
to classification accuracy. In other words, how
discipline-dependent are our features? We also ran a
set of experiments with the same features plus infor-
mation about the scientific discipline in which each
sentence was written. The change in system perfor-
mance was not significant by any means, which sug-
gests that our extracted features are not discipline-
dependent.
Interannotator agreement K, which is the proba-
bility of agreement minus chance agreement, is cal-
culated as follows:
</bodyText>
<equation confidence="0.9978885">
P(A) − P(E) K �(7)
1 − P(E)
</equation>
<bodyText confidence="0.999986">
where P(A) represents observed probability of
agreement, and P(E) is the expected probabil-
ity of agreement, i.e., chance agreement. Given
three moves and uniform distribution among them,
P(E) _ (3)2. Therefore, the two annotators had an
average K of 0.945 over the three moves.
</bodyText>
<subsectionHeader confidence="0.99356">
3.7 Limitations
</subsectionHeader>
<bodyText confidence="0.9999375">
This research is in its initial stages and naturally it
has many limitations. One issue involves some of
the choices we made in our experiments such as
choosing to alphabetize the n-grams and choosing
particular values for C and -y. We will be experi-
menting with non-alphabetized n-grams and also ex-
perimenting with different kernel parameters to find
optimal models.
</bodyText>
<sectionHeader confidence="0.999738" genericHeader="conclusions">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999953375">
This paper set out to identify rhetorical moves in
research article introductions automatically for the
purpose of developing IADE, an educational tool
for helping international university students in the
United States to improve their academic writing
skills. The results of our models based on a rela-
tively small data set are very encouraging, and re-
search on improving the results is ongoing.
</bodyText>
<page confidence="0.997924">
68
</page>
<bodyText confidence="0.999991390243902">
Apart from system accuracy, there are also some
pedagogical issues that we need to keep in mind
in the development of IADE. Warschauer and
Ware (2006) call for the development of a classroom
research agenda that would help evaluate and guide
the application of automated essay scoring in the
writing pedagogy. Based on a categorization devel-
oped by Long (1984), they propose three directions
for research: product, process, and process/product,
where “product refers to educational outcome (i.e.,
what results from using the software), process refers
to learning and teaching process (i.e., how the soft-
ware is used), and process/product refers to the in-
teraction between use and outcome” (p. 10). On the
level of evaluating technology for language learn-
ing in general, Chapelle (2007) specifies three tar-
gets for evaluation: “what is taught in a com-
plete course”, “what is taught through technology
in a complete course”, and “what is taught through
technology” (p. 30). In the first case, an entire
technology-based course is evaluated, in the second
case, CALL materials used for learning a subset of
course objectives, and in the third case, the use of
technology as support and enhancement of a face-
to-face course.
This project needs to pursue the third direction in
both of these trends by investigating the potential of
the IADE program specifically designed to be im-
plemented as an additional component of a graduate
course to improve non-native speaker students’ aca-
demic writing skills. Since this program will repre-
sent a case of innovative technology, its evaluation,
as well as the evaluation of any other new CALL
applications, according to Chapelle (2007), is “per-
haps the most significant challenge teachers and cur-
riculum developers face when attempting to intro-
duce innovation into language education” (p. 30).
Therefore, the analysis of the effectiveness of IADE
will be conducted based on Chapelle’s (2001) frame-
work, which has proven to provide excellent guid-
ance for research of evaluative nature4.
</bodyText>
<sectionHeader confidence="0.990043" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.979567589285714">
Laurence Anthony and George V. Lashkia. 2003. Mover:
A machine learning tool to assist in the reading and
4see (Jamieson et al., 2005)
writing of technical papers. IEEE Transactions on
Professional Communication, 46(3):185–193.
A. Basu, C. Watters, and M. Shepherd. 2003. Support
vector machines for text categorization. In HICSS ’03:
Proceedings of the 36th Annual Hawaii International
Conference on System Sciences (HICSS’03) - Track 4,
page 103.3, Washington, DC, USA. IEEE Computer
Society.
Christopher J. C. Burges. 1998. A tutorial on support
vector machines for pattern recognition. Data Mining
and Knowledge Discovery, 2(2):121–167.
Jill C. Burstein and Martin Chodorow. 1999. Automated
essay scoring for nonnative english. In Proceedings
of the ACL99 Workshop on Computer-Mediated Lan-
guage Assessment and Evaluation of Natural Lan-
guage Processing, pages 68–75, Joint Symposium of
the Association of Computational Linguistics and the
International Association of Language Learning Tech-
nologies, College Park, Maryland.
Jill C. Burstein, Daniel Marcu, and Kevin Knight. 2003.
Finding the WRITE stuff: automatic identification of
discourse structure in student essays. IEEE Intelligent
Systems, 18(1):32–39.
Jill C. Burstein. 2003. The e-rater text registered scor-
ing engine: Automated essay scoring with natural lan-
guage processing. In Shermis and Burstein (Shermis
and Burstein, 2003), pages 113–121.
Carol Chapelle. 2001. Computer applications in second
language acquisition. Cambridge University Press,
New York.
Carol Chapelle. 2007. Challenges in evaluation of inno-
vation: Observations from technology research. Inno-
vation in Language Learning and Teaching, 1(1):30–
45.
Martin Chodorow, Joel R. Tetreault, and Na-Rae Han.
2007. Detection of grammatical errors involving
prepositions. In Proceedings of the 4th ACL-SIGSEM
Workshop on Prepositions, pages 25–30.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20:273–297.
Viviana Cortes. 2006. Exploring genre and corpora in
the English for academic writing class. Manuscript
submitted for publication. Manuscript submitted for
publication.
Scott Elliott. 2003. IntellimetricTM: From here to valid-
ity. In Shermis and Burstein (Shermis and Burstein,
2003), pages 71–86.
Jan Frodesen. 1995. Negotiating the syllabus: A
learning-centered, interactive approach to ESL grad-
uate writing course design. In Diane Belcher and
George Braine, editors, Academic Writing in a Second
Language: Essays on Research and Pedagogy, pages
331–350. Ablex Publishing Corporation, NJ.
</reference>
<page confidence="0.983834">
69
</page>
<reference confidence="0.99988674025974">
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineering,
2(2):115–129.
Chih-Wei Hsu, Chih-Chung Chang, and Chih-Jen Lin.
2008. A practical guide to support vector classifica-
tion. Unpublished manuscript.
Joanne Jamieson, Carol Chapelle, and Sherry Preiss.
2005. CALL evaluation by developers, a teacher, and
students. CALICO Journal, 23(1):93–138.
T Joachims. 1998. Text categorization with support vec-
tor machines: Learning with many relevant features.
In In Proceedings of ECML-98, 10th European Con-
ference on Machine Learning.
Shimona Kushner. 1997. Tackling the needs of foreign
academic writers: A case study. IEEE Transactions on
Professional Communication, 40:20–25.
Thomas K. Landauer, Darrell Laham, and Peter W. Foltz.
2003. Automated scoring and annotation of essays
with the Intelligent Essay Assessor. In Shermis and
Burstein (Shermis and Burstein, 2003), pages 87–112.
Claudia Leacock and Martin Chodorow. 2003. Auto-
mated grammatical error detection. In Shermis and
Burstein (Shermis and Burstein, 2003), pages 195–
207.
John Levis and Greta Muller-Levis. 2003. A project-
based approach to teaching research writing to nonna-
tive writers. IEEE Transactions on Professional Com-
munication, 46(3):210–220.
Michael Long. 1984. Process and product in ESL pro-
gramme evaluation. TESOL Quarterly, 18(3):409–
425.
I. Mierswa, M. Wurst, R. Klinkenberg, M. Scholz, and T.
Euler. 2006. YALE (now: RAPIDMINER: Rapid pro-
totyping for complex data mining tasks. In Proceed-
ings ofthe ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining (KDD 2006).
R. Mislevy, l. Steinberg, R. Almond, and J. Lukas. 2006.
Concepts, terminology, and basic models of evidence-
centered design. In D. Williamson, R. Mislevy, and I.
Bejar, editors, Automated scoring of complex tasks in
computer-based testing, pages 15–47. Lawrence Erl-
baum Associates, Mahwah, NJ.
Tom Mitchell, Terry Russell, Peter Broomhead, and
Nicola Aldridge. 2002. Towards robust computerised
marking of free-text responses. In Proceedings of the
6th International Computer Assisted Assessment Con-
ference, pages 233–249, Loughborough University.
Ellis Batten Page. 2003. Project Essay Grade. In Sher-
mis and Burstein (Shermis and Burstein, 2003), pages
43–54.
Martin F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130–137.
Lawrence M. Rudner and Tahung Liang. 2002. Auto-
mated essay scoring using Bayes’ theorem. The Jour-
nal of Technology, Learning and Assessment, 1(2):3–
21.
Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM Computing Surveys,
34(1):1–47.
Mark D. Shermis and Jill C. Burstein, editors. 2003. Au-
tomated Essay Scoring: A cross-disciplinary perspec-
tive. Lawrence Erlbaum Associates, Mahwah, NJ.
John Swales. 1990. English in Academic and Research
Settings. Cambridge University Press, Cambridge.
John Swales. 2004. Research Genres: Exploration
and applications. Cambridge University Press, Cam-
bridge.
Roberta Vann and Cynthia Myers. 2001. Academic ESL
options in a large research university. In Ilona Leki,
editor, Academic Writing Programs, Case Studies in
TESOL Practice Series. TESOL, Alexandria, VA.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer, Berlin.
Mark Warschauer and Paige Ware. 2006. Automated
writing evaluation: defining the classroom research
agenda. Language Teaching Research, 10(2):1–24.
</reference>
<page confidence="0.998467">
70
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.845190">
<title confidence="0.84519">Automatic Identification of Discourse Moves in Scientific</title>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Laurence Anthony</author>
<author>George V Lashkia</author>
</authors>
<title>Mover: A machine learning tool to assist in the reading and 4see (Jamieson et al.,</title>
<date>2003</date>
<journal>IEEE Transactions on Professional Communication,</journal>
<volume>46</volume>
<issue>3</issue>
<contexts>
<context position="2935" citStr="Anthony and Lashkia, 2003" startWordPosition="430" endWordPosition="433">learner writing is generally highly erroneous, an emerging research trend has focused on automated error detection in L2 output finding novel approaches to develop intelligent ways to assess ill-formed learner responses (Burstein and Chodorow, 1999; Chodorow et al., 2007; Han et al., 2006; Leacock and Chodorow, 2003). Various NLP and statistical techniques also allow for the evaluation of text organization, which is however limited to recognizing the five-paragraph essay format, thesis, and topic sentences. At present, to our knowledge, there is only one automated evaluation system, AntMover (Anthony and Lashkia, 2003), that applies intelligent technological possibilities to the genre of research reports—a major challenge for new non-native speaker (NNS) members of academia. AntMover is able to automatically identify the structure of abstracts in various fields and disciplines. Academic writing pedagogues have been struggling to find effective ways to teach academic writing. Frodesen (1995) argues that the writing instruction for non-native speaker students should “help 62 Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 62–70, Columbus, Ohio, USA, </context>
</contexts>
<marker>Anthony, Lashkia, 2003</marker>
<rawString>Laurence Anthony and George V. Lashkia. 2003. Mover: A machine learning tool to assist in the reading and 4see (Jamieson et al., 2005) writing of technical papers. IEEE Transactions on Professional Communication, 46(3):185–193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Basu</author>
<author>C Watters</author>
<author>M Shepherd</author>
</authors>
<title>Support vector machines for text categorization.</title>
<date>2003</date>
<booktitle>In HICSS ’03: Proceedings of the 36th Annual Hawaii International Conference on System Sciences (HICSS’03) - Track 4,</booktitle>
<pages>103--3</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="16610" citStr="Basu et al., 2003" startWordPosition="2628" endWordPosition="2631">ually, in text categorization this measure is calculated as what is commonly known as the tf.idf (term frequency times the inverse document frequency), which is a measure of the importance of a term in a document. However, since our “documents” are all sentences and therefore very short, we decided to only record the presence or absence of terms in the sentences as Boolean values; that is, a vector component will contain either a 0 for the absence of the corresponding term or a 1 for its presence in the sentence. 3.4 Classifier We chose to use Support Vector Machines (SVM) for our classifier (Basu et al., 2003; Burges, 1998; Cortes and Vapnik, 1995; Joachims, 1998; Vapnik, 1995). SVMs are commonly used to solve classification problems by finding hyperplanes that best classify data while providing the widest margin possible between classes. SVMs have proven to be among the most powerful classifiers provided that the representation of the data captures the patterns we are trying to discover and that the parameters of the SVM classifier itself are properly set. SVM learning is a supervised learning technique where the system is provided a set of labeled data for training. The performance of the system</context>
</contexts>
<marker>Basu, Watters, Shepherd, 2003</marker>
<rawString>A. Basu, C. Watters, and M. Shepherd. 2003. Support vector machines for text categorization. In HICSS ’03: Proceedings of the 36th Annual Hawaii International Conference on System Sciences (HICSS’03) - Track 4, page 103.3, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher J C Burges</author>
</authors>
<title>A tutorial on support vector machines for pattern recognition.</title>
<date>1998</date>
<journal>Data Mining and Knowledge Discovery,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="16624" citStr="Burges, 1998" startWordPosition="2632" endWordPosition="2633">gorization this measure is calculated as what is commonly known as the tf.idf (term frequency times the inverse document frequency), which is a measure of the importance of a term in a document. However, since our “documents” are all sentences and therefore very short, we decided to only record the presence or absence of terms in the sentences as Boolean values; that is, a vector component will contain either a 0 for the absence of the corresponding term or a 1 for its presence in the sentence. 3.4 Classifier We chose to use Support Vector Machines (SVM) for our classifier (Basu et al., 2003; Burges, 1998; Cortes and Vapnik, 1995; Joachims, 1998; Vapnik, 1995). SVMs are commonly used to solve classification problems by finding hyperplanes that best classify data while providing the widest margin possible between classes. SVMs have proven to be among the most powerful classifiers provided that the representation of the data captures the patterns we are trying to discover and that the parameters of the SVM classifier itself are properly set. SVM learning is a supervised learning technique where the system is provided a set of labeled data for training. The performance of the system is then measu</context>
</contexts>
<marker>Burges, 1998</marker>
<rawString>Christopher J. C. Burges. 1998. A tutorial on support vector machines for pattern recognition. Data Mining and Knowledge Discovery, 2(2):121–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jill C Burstein</author>
<author>Martin Chodorow</author>
</authors>
<title>Automated essay scoring for nonnative english.</title>
<date>1999</date>
<booktitle>In Proceedings of the ACL99 Workshop on Computer-Mediated Language Assessment and Evaluation of Natural Language Processing,</booktitle>
<pages>68--75</pages>
<location>College Park, Maryland.</location>
<contexts>
<context position="2557" citStr="Burstein and Chodorow, 1999" startWordPosition="373" endWordPosition="376">olated to other genres in both first language (L1) and second language (L2) academic contexts. Existing scoring systems can assess various constructs such as topical content, grammar, style, mechanics, syntactic complexity, and even deviance or plagiarism (Burstein, 2003; Elliott, 2003; Landauer et al., 2003; Mitchell et al., 2002; Page, 2003; Rudner and Liang, 2002). Because learner writing is generally highly erroneous, an emerging research trend has focused on automated error detection in L2 output finding novel approaches to develop intelligent ways to assess ill-formed learner responses (Burstein and Chodorow, 1999; Chodorow et al., 2007; Han et al., 2006; Leacock and Chodorow, 2003). Various NLP and statistical techniques also allow for the evaluation of text organization, which is however limited to recognizing the five-paragraph essay format, thesis, and topic sentences. At present, to our knowledge, there is only one automated evaluation system, AntMover (Anthony and Lashkia, 2003), that applies intelligent technological possibilities to the genre of research reports—a major challenge for new non-native speaker (NNS) members of academia. AntMover is able to automatically identify the structure of ab</context>
</contexts>
<marker>Burstein, Chodorow, 1999</marker>
<rawString>Jill C. Burstein and Martin Chodorow. 1999. Automated essay scoring for nonnative english. In Proceedings of the ACL99 Workshop on Computer-Mediated Language Assessment and Evaluation of Natural Language Processing, pages 68–75, Joint Symposium of the Association of Computational Linguistics and the International Association of Language Learning Technologies, College Park, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jill C Burstein</author>
<author>Daniel Marcu</author>
<author>Kevin Knight</author>
</authors>
<title>Finding the WRITE stuff: automatic identification of discourse structure in student essays.</title>
<date>2003</date>
<journal>IEEE Intelligent Systems,</journal>
<volume>18</volume>
<issue>1</issue>
<contexts>
<context position="9948" citStr="Burstein et al., 2003" startWordPosition="1507" endWordPosition="1510">n 20 11. Economics 20 12. Electrical engineering and power 20 system 13. Environmental engineering 20 14. Food science &amp; food service 20 15. Health &amp; human performance 20 16. Industrial engineering 20 17. Journalism 20 18. Mechanical engineering 20 19. Sociology 20 20. Urban and regional planning 20 Table 2: Disciplines represented in the corpus for article introductions lem. In other words, given a sentence and a finite set of moves and steps, what move/step does the sentence signify? This task is very similar to identifying the discourse structure of short argumentative essays discussed in (Burstein et al., 2003), the difference being in the genre of the essays and type of the discourse functions in question. The corpus used in this study was compiled from an existing corpus of published research articles in 44 disciplines, used in an academic writing graduate course for international students. The corpus contains 1,623 articles and 1,322,089 words. The average length of articles is 814.09 words. We made a stratified sampling of 401 introduction sections representative of 20 academic disciplines (see Table 2) from this corpus of research articles. The size of this sub-corpus is 267,029 words; each fil</context>
</contexts>
<marker>Burstein, Marcu, Knight, 2003</marker>
<rawString>Jill C. Burstein, Daniel Marcu, and Kevin Knight. 2003. Finding the WRITE stuff: automatic identification of discourse structure in student essays. IEEE Intelligent Systems, 18(1):32–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jill C Burstein</author>
</authors>
<title>The e-rater text registered scoring engine: Automated essay scoring with natural language processing.</title>
<date>2003</date>
<booktitle>In Shermis and Burstein (Shermis</booktitle>
<pages>113--121</pages>
<contexts>
<context position="2201" citStr="Burstein, 2003" startWordPosition="321" endWordPosition="322">ide range of possibilities for highquality formative evaluation that can closely match teaching goals and tailor instruction to individual learners by providing them with feedback and direction on their attainment of knowledge. Traditionally, automated evaluation has been used for essay grading, but its potential could be successfully extrapolated to other genres in both first language (L1) and second language (L2) academic contexts. Existing scoring systems can assess various constructs such as topical content, grammar, style, mechanics, syntactic complexity, and even deviance or plagiarism (Burstein, 2003; Elliott, 2003; Landauer et al., 2003; Mitchell et al., 2002; Page, 2003; Rudner and Liang, 2002). Because learner writing is generally highly erroneous, an emerging research trend has focused on automated error detection in L2 output finding novel approaches to develop intelligent ways to assess ill-formed learner responses (Burstein and Chodorow, 1999; Chodorow et al., 2007; Han et al., 2006; Leacock and Chodorow, 2003). Various NLP and statistical techniques also allow for the evaluation of text organization, which is however limited to recognizing the five-paragraph essay format, thesis, </context>
</contexts>
<marker>Burstein, 2003</marker>
<rawString>Jill C. Burstein. 2003. The e-rater text registered scoring engine: Automated essay scoring with natural language processing. In Shermis and Burstein (Shermis and Burstein, 2003), pages 113–121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carol Chapelle</author>
</authors>
<title>Computer applications in second language acquisition.</title>
<date>2001</date>
<publisher>Cambridge University Press,</publisher>
<location>New York.</location>
<marker>Chapelle, 2001</marker>
<rawString>Carol Chapelle. 2001. Computer applications in second language acquisition. Cambridge University Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carol Chapelle</author>
</authors>
<title>Challenges in evaluation of innovation: Observations from technology research.</title>
<date>2007</date>
<booktitle>Innovation in Language Learning and Teaching,</booktitle>
<volume>1</volume>
<issue>1</issue>
<pages>45</pages>
<contexts>
<context position="24971" citStr="Chapelle (2007)" startWordPosition="4047" endWordPosition="4048">pment of a classroom research agenda that would help evaluate and guide the application of automated essay scoring in the writing pedagogy. Based on a categorization developed by Long (1984), they propose three directions for research: product, process, and process/product, where “product refers to educational outcome (i.e., what results from using the software), process refers to learning and teaching process (i.e., how the software is used), and process/product refers to the interaction between use and outcome” (p. 10). On the level of evaluating technology for language learning in general, Chapelle (2007) specifies three targets for evaluation: “what is taught in a complete course”, “what is taught through technology in a complete course”, and “what is taught through technology” (p. 30). In the first case, an entire technology-based course is evaluated, in the second case, CALL materials used for learning a subset of course objectives, and in the third case, the use of technology as support and enhancement of a faceto-face course. This project needs to pursue the third direction in both of these trends by investigating the potential of the IADE program specifically designed to be implemented a</context>
</contexts>
<marker>Chapelle, 2007</marker>
<rawString>Carol Chapelle. 2007. Challenges in evaluation of innovation: Observations from technology research. Innovation in Language Learning and Teaching, 1(1):30– 45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Chodorow</author>
<author>Joel R Tetreault</author>
<author>Na-Rae Han</author>
</authors>
<title>Detection of grammatical errors involving prepositions.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th ACL-SIGSEM Workshop on Prepositions,</booktitle>
<pages>25--30</pages>
<contexts>
<context position="2580" citStr="Chodorow et al., 2007" startWordPosition="377" endWordPosition="380">h first language (L1) and second language (L2) academic contexts. Existing scoring systems can assess various constructs such as topical content, grammar, style, mechanics, syntactic complexity, and even deviance or plagiarism (Burstein, 2003; Elliott, 2003; Landauer et al., 2003; Mitchell et al., 2002; Page, 2003; Rudner and Liang, 2002). Because learner writing is generally highly erroneous, an emerging research trend has focused on automated error detection in L2 output finding novel approaches to develop intelligent ways to assess ill-formed learner responses (Burstein and Chodorow, 1999; Chodorow et al., 2007; Han et al., 2006; Leacock and Chodorow, 2003). Various NLP and statistical techniques also allow for the evaluation of text organization, which is however limited to recognizing the five-paragraph essay format, thesis, and topic sentences. At present, to our knowledge, there is only one automated evaluation system, AntMover (Anthony and Lashkia, 2003), that applies intelligent technological possibilities to the genre of research reports—a major challenge for new non-native speaker (NNS) members of academia. AntMover is able to automatically identify the structure of abstracts in various fiel</context>
</contexts>
<marker>Chodorow, Tetreault, Han, 2007</marker>
<rawString>Martin Chodorow, Joel R. Tetreault, and Na-Rae Han. 2007. Detection of grammatical errors involving prepositions. In Proceedings of the 4th ACL-SIGSEM Workshop on Prepositions, pages 25–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Supportvector networks.</title>
<date>1995</date>
<booktitle>Machine Learning,</booktitle>
<pages>20--273</pages>
<contexts>
<context position="16649" citStr="Cortes and Vapnik, 1995" startWordPosition="2634" endWordPosition="2637">s measure is calculated as what is commonly known as the tf.idf (term frequency times the inverse document frequency), which is a measure of the importance of a term in a document. However, since our “documents” are all sentences and therefore very short, we decided to only record the presence or absence of terms in the sentences as Boolean values; that is, a vector component will contain either a 0 for the absence of the corresponding term or a 1 for its presence in the sentence. 3.4 Classifier We chose to use Support Vector Machines (SVM) for our classifier (Basu et al., 2003; Burges, 1998; Cortes and Vapnik, 1995; Joachims, 1998; Vapnik, 1995). SVMs are commonly used to solve classification problems by finding hyperplanes that best classify data while providing the widest margin possible between classes. SVMs have proven to be among the most powerful classifiers provided that the representation of the data captures the patterns we are trying to discover and that the parameters of the SVM classifier itself are properly set. SVM learning is a supervised learning technique where the system is provided a set of labeled data for training. The performance of the system is then measured by providing the lear</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>Corinna Cortes and Vladimir Vapnik. 1995. Supportvector networks. Machine Learning, 20:273–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Viviana Cortes</author>
</authors>
<title>Exploring genre and corpora in the English for academic writing class. Manuscript submitted for publication. Manuscript submitted for publication.</title>
<date>2006</date>
<contexts>
<context position="5633" citStr="Cortes, 2006" startWordPosition="831" endWordPosition="832">t of articles, for instance, on cross-cultural adjustment, with which they worked to identify potential research questions for a further investigation and to write the literature review. This approach placed a heavy emphasis on collaboration as students worked in small groups on developing data-collection instruments and on data analysis. Oral presentations on group-research projects wrapped up the course. The academic writing course discussed in the paragraph above is corpus- and genre-based, combining a top-down approach to genre analysis and a bottom-up approach to the analysis of corpora (Cortes, 2006). Cortes (2006) explains that the course was designed to better address the issues of genre-specificity and disciplinarity since some students who took the previous form of the course claimed that, although they were taught useful things, they did not learn to write the way researchers in their disciplines generally do. In the present format of the course, each student is provided with a corpus of research articles published in top journals of his/her discipline. Students conduct class analyses of their corpus according to guidelines from empirical findings in applied linguistics about the dis</context>
</contexts>
<marker>Cortes, 2006</marker>
<rawString>Viviana Cortes. 2006. Exploring genre and corpora in the English for academic writing class. Manuscript submitted for publication. Manuscript submitted for publication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Elliott</author>
</authors>
<title>IntellimetricTM: From here to validity.</title>
<date>2003</date>
<booktitle>In Shermis and Burstein (Shermis and Burstein,</booktitle>
<pages>71--86</pages>
<contexts>
<context position="2216" citStr="Elliott, 2003" startWordPosition="323" endWordPosition="324">sibilities for highquality formative evaluation that can closely match teaching goals and tailor instruction to individual learners by providing them with feedback and direction on their attainment of knowledge. Traditionally, automated evaluation has been used for essay grading, but its potential could be successfully extrapolated to other genres in both first language (L1) and second language (L2) academic contexts. Existing scoring systems can assess various constructs such as topical content, grammar, style, mechanics, syntactic complexity, and even deviance or plagiarism (Burstein, 2003; Elliott, 2003; Landauer et al., 2003; Mitchell et al., 2002; Page, 2003; Rudner and Liang, 2002). Because learner writing is generally highly erroneous, an emerging research trend has focused on automated error detection in L2 output finding novel approaches to develop intelligent ways to assess ill-formed learner responses (Burstein and Chodorow, 1999; Chodorow et al., 2007; Han et al., 2006; Leacock and Chodorow, 2003). Various NLP and statistical techniques also allow for the evaluation of text organization, which is however limited to recognizing the five-paragraph essay format, thesis, and topic sente</context>
</contexts>
<marker>Elliott, 2003</marker>
<rawString>Scott Elliott. 2003. IntellimetricTM: From here to validity. In Shermis and Burstein (Shermis and Burstein, 2003), pages 71–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Frodesen</author>
</authors>
<title>Negotiating the syllabus: A learning-centered, interactive approach to ESL graduate writing course design.</title>
<date>1995</date>
<booktitle>In Diane Belcher and George Braine, editors, Academic Writing in a Second Language: Essays on Research and Pedagogy,</booktitle>
<pages>331--350</pages>
<publisher>Ablex Publishing Corporation, NJ.</publisher>
<contexts>
<context position="3314" citStr="Frodesen (1995)" startWordPosition="489" endWordPosition="490">f text organization, which is however limited to recognizing the five-paragraph essay format, thesis, and topic sentences. At present, to our knowledge, there is only one automated evaluation system, AntMover (Anthony and Lashkia, 2003), that applies intelligent technological possibilities to the genre of research reports—a major challenge for new non-native speaker (NNS) members of academia. AntMover is able to automatically identify the structure of abstracts in various fields and disciplines. Academic writing pedagogues have been struggling to find effective ways to teach academic writing. Frodesen (1995) argues that the writing instruction for non-native speaker students should “help 62 Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 62–70, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics initiate writers into their field-specific research communities” (p. 333). In support of this opinion, (Kushner, 1997) reasons that graduate NNS courses have to combine language and discourse with the skill of writing within professional norms. Various pedagogical approaches have been attempted to achieve this goal. F</context>
</contexts>
<marker>Frodesen, 1995</marker>
<rawString>Jan Frodesen. 1995. Negotiating the syllabus: A learning-centered, interactive approach to ESL graduate writing course design. In Diane Belcher and George Braine, editors, Academic Writing in a Second Language: Essays on Research and Pedagogy, pages 331–350. Ablex Publishing Corporation, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Na-Rae Han</author>
<author>Martin Chodorow</author>
<author>Claudia Leacock</author>
</authors>
<title>Detecting errors in English article usage by non-native speakers.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="2598" citStr="Han et al., 2006" startWordPosition="381" endWordPosition="384">nd second language (L2) academic contexts. Existing scoring systems can assess various constructs such as topical content, grammar, style, mechanics, syntactic complexity, and even deviance or plagiarism (Burstein, 2003; Elliott, 2003; Landauer et al., 2003; Mitchell et al., 2002; Page, 2003; Rudner and Liang, 2002). Because learner writing is generally highly erroneous, an emerging research trend has focused on automated error detection in L2 output finding novel approaches to develop intelligent ways to assess ill-formed learner responses (Burstein and Chodorow, 1999; Chodorow et al., 2007; Han et al., 2006; Leacock and Chodorow, 2003). Various NLP and statistical techniques also allow for the evaluation of text organization, which is however limited to recognizing the five-paragraph essay format, thesis, and topic sentences. At present, to our knowledge, there is only one automated evaluation system, AntMover (Anthony and Lashkia, 2003), that applies intelligent technological possibilities to the genre of research reports—a major challenge for new non-native speaker (NNS) members of academia. AntMover is able to automatically identify the structure of abstracts in various fields and disciplines</context>
</contexts>
<marker>Han, Chodorow, Leacock, 2006</marker>
<rawString>Na-Rae Han, Martin Chodorow, and Claudia Leacock. 2006. Detecting errors in English article usage by non-native speakers. Natural Language Engineering, 2(2):115–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Wei Hsu</author>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>A practical guide to support vector classification.</title>
<date>2008</date>
<note>Unpublished manuscript.</note>
<contexts>
<context position="18736" citStr="Hsu et al., 2008" startWordPosition="2988" endWordPosition="2991">lidation experiments. We predominantly used the machine learning environment RAPIDMINER (Mierswa et al., 2006) in the experimentation phase of the project. The SVMs were set to use the RBF kernel, which maps samples into a higher dimensional space allowing for capturing non-linear relationships among the data and labels. The RBF kernel has two parameters, C and -y. These parameters help against overfitting the classifier on the training data. The values of these parameters is not known before hand for each data set and may be found through an exhaustive search of different parameter settings (Hsu et al., 2008). In this study, we used C = 23 and -y = 2−9, which were arrived at through a search of different parameter settings on the feature set with 3,000 unigrams. The search was performed by performing five-fold cross validation on the whole data set using models built with various combinations of C and -y values. Admittedly, these parameters are not necessarily the best parameters for the other feature sets on which exhaustive searches should be performed. This is 66 the next step in our project. 3.5 Evaluation We performed five-fold cross validation on 14 different feature sets as summarized in Ta</context>
</contexts>
<marker>Hsu, Chang, Lin, 2008</marker>
<rawString>Chih-Wei Hsu, Chih-Chung Chang, and Chih-Jen Lin. 2008. A practical guide to support vector classification. Unpublished manuscript.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joanne Jamieson</author>
<author>Carol Chapelle</author>
<author>Sherry Preiss</author>
</authors>
<title>CALL evaluation by developers, a teacher, and students.</title>
<date>2005</date>
<journal>CALICO Journal,</journal>
<volume>23</volume>
<issue>1</issue>
<marker>Jamieson, Chapelle, Preiss, 2005</marker>
<rawString>Joanne Jamieson, Carol Chapelle, and Sherry Preiss. 2005. CALL evaluation by developers, a teacher, and students. CALICO Journal, 23(1):93–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text categorization with support vector machines: Learning with many relevant features. In</title>
<date>1998</date>
<booktitle>In Proceedings of ECML-98, 10th European Conference on Machine Learning.</booktitle>
<contexts>
<context position="16665" citStr="Joachims, 1998" startWordPosition="2638" endWordPosition="2639">s what is commonly known as the tf.idf (term frequency times the inverse document frequency), which is a measure of the importance of a term in a document. However, since our “documents” are all sentences and therefore very short, we decided to only record the presence or absence of terms in the sentences as Boolean values; that is, a vector component will contain either a 0 for the absence of the corresponding term or a 1 for its presence in the sentence. 3.4 Classifier We chose to use Support Vector Machines (SVM) for our classifier (Basu et al., 2003; Burges, 1998; Cortes and Vapnik, 1995; Joachims, 1998; Vapnik, 1995). SVMs are commonly used to solve classification problems by finding hyperplanes that best classify data while providing the widest margin possible between classes. SVMs have proven to be among the most powerful classifiers provided that the representation of the data captures the patterns we are trying to discover and that the parameters of the SVM classifier itself are properly set. SVM learning is a supervised learning technique where the system is provided a set of labeled data for training. The performance of the system is then measured by providing the learned model a set </context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T Joachims. 1998. Text categorization with support vector machines: Learning with many relevant features. In In Proceedings of ECML-98, 10th European Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shimona Kushner</author>
</authors>
<title>Tackling the needs of foreign academic writers: A case study.</title>
<date>1997</date>
<journal>IEEE Transactions on Professional Communication,</journal>
<pages>40--20</pages>
<contexts>
<context position="3713" citStr="Kushner, 1997" startWordPosition="546" endWordPosition="547">over is able to automatically identify the structure of abstracts in various fields and disciplines. Academic writing pedagogues have been struggling to find effective ways to teach academic writing. Frodesen (1995) argues that the writing instruction for non-native speaker students should “help 62 Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 62–70, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics initiate writers into their field-specific research communities” (p. 333). In support of this opinion, (Kushner, 1997) reasons that graduate NNS courses have to combine language and discourse with the skill of writing within professional norms. Various pedagogical approaches have been attempted to achieve this goal. For instance, (Vann and Myers, 2001) followed the inductive analysis approach, in which students examined the format, content, grammatical, and rhetorical conventions of each section of research reports. Supplements to this approach were tasks that required students to write journal entries about the rhetorical conventions of prominent journals in their disciplines and tasks that placed the experi</context>
</contexts>
<marker>Kushner, 1997</marker>
<rawString>Shimona Kushner. 1997. Tackling the needs of foreign academic writers: A case study. IEEE Transactions on Professional Communication, 40:20–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Darrell Laham</author>
<author>Peter W Foltz</author>
</authors>
<title>Automated scoring and annotation of essays with the Intelligent Essay Assessor.</title>
<date>2003</date>
<booktitle>In Shermis and Burstein (Shermis</booktitle>
<pages>87--112</pages>
<contexts>
<context position="2239" citStr="Landauer et al., 2003" startWordPosition="325" endWordPosition="329">highquality formative evaluation that can closely match teaching goals and tailor instruction to individual learners by providing them with feedback and direction on their attainment of knowledge. Traditionally, automated evaluation has been used for essay grading, but its potential could be successfully extrapolated to other genres in both first language (L1) and second language (L2) academic contexts. Existing scoring systems can assess various constructs such as topical content, grammar, style, mechanics, syntactic complexity, and even deviance or plagiarism (Burstein, 2003; Elliott, 2003; Landauer et al., 2003; Mitchell et al., 2002; Page, 2003; Rudner and Liang, 2002). Because learner writing is generally highly erroneous, an emerging research trend has focused on automated error detection in L2 output finding novel approaches to develop intelligent ways to assess ill-formed learner responses (Burstein and Chodorow, 1999; Chodorow et al., 2007; Han et al., 2006; Leacock and Chodorow, 2003). Various NLP and statistical techniques also allow for the evaluation of text organization, which is however limited to recognizing the five-paragraph essay format, thesis, and topic sentences. At present, to ou</context>
</contexts>
<marker>Landauer, Laham, Foltz, 2003</marker>
<rawString>Thomas K. Landauer, Darrell Laham, and Peter W. Foltz. 2003. Automated scoring and annotation of essays with the Intelligent Essay Assessor. In Shermis and Burstein (Shermis and Burstein, 2003), pages 87–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>Automated grammatical error detection.</title>
<date>2003</date>
<booktitle>In Shermis and Burstein (Shermis and Burstein,</booktitle>
<pages>195--207</pages>
<contexts>
<context position="2627" citStr="Leacock and Chodorow, 2003" startWordPosition="385" endWordPosition="388"> (L2) academic contexts. Existing scoring systems can assess various constructs such as topical content, grammar, style, mechanics, syntactic complexity, and even deviance or plagiarism (Burstein, 2003; Elliott, 2003; Landauer et al., 2003; Mitchell et al., 2002; Page, 2003; Rudner and Liang, 2002). Because learner writing is generally highly erroneous, an emerging research trend has focused on automated error detection in L2 output finding novel approaches to develop intelligent ways to assess ill-formed learner responses (Burstein and Chodorow, 1999; Chodorow et al., 2007; Han et al., 2006; Leacock and Chodorow, 2003). Various NLP and statistical techniques also allow for the evaluation of text organization, which is however limited to recognizing the five-paragraph essay format, thesis, and topic sentences. At present, to our knowledge, there is only one automated evaluation system, AntMover (Anthony and Lashkia, 2003), that applies intelligent technological possibilities to the genre of research reports—a major challenge for new non-native speaker (NNS) members of academia. AntMover is able to automatically identify the structure of abstracts in various fields and disciplines. Academic writing pedagogues</context>
</contexts>
<marker>Leacock, Chodorow, 2003</marker>
<rawString>Claudia Leacock and Martin Chodorow. 2003. Automated grammatical error detection. In Shermis and Burstein (Shermis and Burstein, 2003), pages 195– 207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Levis</author>
<author>Greta Muller-Levis</author>
</authors>
<title>A projectbased approach to teaching research writing to nonnative writers.</title>
<date>2003</date>
<journal>IEEE Transactions on Professional Communication,</journal>
<volume>46</volume>
<issue>3</issue>
<marker>Levis, Muller-Levis, 2003</marker>
<rawString>John Levis and Greta Muller-Levis. 2003. A projectbased approach to teaching research writing to nonnative writers. IEEE Transactions on Professional Communication, 46(3):210–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Long</author>
</authors>
<title>Process and product in ESL programme evaluation.</title>
<date>1984</date>
<journal>TESOL Quarterly,</journal>
<volume>18</volume>
<issue>3</issue>
<pages>425</pages>
<contexts>
<context position="24546" citStr="Long (1984)" startWordPosition="3983" endWordPosition="3984">ool for helping international university students in the United States to improve their academic writing skills. The results of our models based on a relatively small data set are very encouraging, and research on improving the results is ongoing. 68 Apart from system accuracy, there are also some pedagogical issues that we need to keep in mind in the development of IADE. Warschauer and Ware (2006) call for the development of a classroom research agenda that would help evaluate and guide the application of automated essay scoring in the writing pedagogy. Based on a categorization developed by Long (1984), they propose three directions for research: product, process, and process/product, where “product refers to educational outcome (i.e., what results from using the software), process refers to learning and teaching process (i.e., how the software is used), and process/product refers to the interaction between use and outcome” (p. 10). On the level of evaluating technology for language learning in general, Chapelle (2007) specifies three targets for evaluation: “what is taught in a complete course”, “what is taught through technology in a complete course”, and “what is taught through technolog</context>
</contexts>
<marker>Long, 1984</marker>
<rawString>Michael Long. 1984. Process and product in ESL programme evaluation. TESOL Quarterly, 18(3):409– 425.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mierswa</author>
<author>M Wurst</author>
<author>R Klinkenberg</author>
<author>M Scholz</author>
<author>T Euler</author>
</authors>
<title>YALE (now: RAPIDMINER: Rapid prototyping for complex data mining tasks.</title>
<date>2006</date>
<booktitle>In Proceedings ofthe ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD</booktitle>
<contexts>
<context position="18229" citStr="Mierswa et al., 2006" startWordPosition="2900" endWordPosition="2903">hich data are divided into v equal-sized groups (either by random sampling or by stratified sampling). Then, the system is trained on all but one of the groups and tested on the remaining group. This process is repeated v times until all data items have been used in training and validation. This technique provides a fairly accurate view of how a model built on the whole data set will perform when given completely new data. All the results reported in the following subsection are based on five-fold cross validation experiments. We predominantly used the machine learning environment RAPIDMINER (Mierswa et al., 2006) in the experimentation phase of the project. The SVMs were set to use the RBF kernel, which maps samples into a higher dimensional space allowing for capturing non-linear relationships among the data and labels. The RBF kernel has two parameters, C and -y. These parameters help against overfitting the classifier on the training data. The values of these parameters is not known before hand for each data set and may be found through an exhaustive search of different parameter settings (Hsu et al., 2008). In this study, we used C = 23 and -y = 2−9, which were arrived at through a search of diffe</context>
</contexts>
<marker>Mierswa, Wurst, Klinkenberg, Scholz, Euler, 2006</marker>
<rawString>I. Mierswa, M. Wurst, R. Klinkenberg, M. Scholz, and T. Euler. 2006. YALE (now: RAPIDMINER: Rapid prototyping for complex data mining tasks. In Proceedings ofthe ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mislevy</author>
<author>l Steinberg</author>
<author>R Almond</author>
<author>J Lukas</author>
</authors>
<title>Concepts, terminology, and basic models of evidencecentered design.</title>
<date>2006</date>
<booktitle>Automated scoring of complex tasks in computer-based testing,</booktitle>
<pages>15--47</pages>
<editor>In D. Williamson, R. Mislevy, and I. Bejar, editors,</editor>
<location>Mahwah, NJ.</location>
<contexts>
<context position="7746" citStr="Mislevy et al., 2006" startWordPosition="1150" endWordPosition="1153">not fully investigated. The long-term goal of this research project is the design and implementation of a new automated discourse evaluation tool as well as the analysis of its effectiveness for formative assessment purposes. Named IADE (Intelligent Academic Discourse Evaluator), this application will draw from second language acquisition models such as interactionist views and Systemic Functional Linguistics as well as from the Skill Acquisition Theory of learning. Additionally, it will be informed by empirical research on the provision of feedback and by Evidence Centered Design principles (Mislevy et al., 2006). IADE will evaluate students’ drafts of their academic writing in accordance with the course materials in terms of an adapted model of Swales’ (Swales, 1990; Swales, 2004) move schema as partially presented in Table 1. IADE will achieve this by conducting a sentence-level classification of the input text for rhetorical shifts. Given a draft of a research article, IADE will identify the discourse moves in the paper, compare it with other papers in the same discipline and provide feedback to the user. 63 Table 1: Discourse move model for research article introductions based on (Swales, 1990; Sw</context>
</contexts>
<marker>Mislevy, Steinberg, Almond, Lukas, 2006</marker>
<rawString>R. Mislevy, l. Steinberg, R. Almond, and J. Lukas. 2006. Concepts, terminology, and basic models of evidencecentered design. In D. Williamson, R. Mislevy, and I. Bejar, editors, Automated scoring of complex tasks in computer-based testing, pages 15–47. Lawrence Erlbaum Associates, Mahwah, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Mitchell</author>
<author>Terry Russell</author>
<author>Peter Broomhead</author>
<author>Nicola Aldridge</author>
</authors>
<title>Towards robust computerised marking of free-text responses.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th International Computer Assisted Assessment Conference,</booktitle>
<pages>233--249</pages>
<institution>Loughborough University.</institution>
<contexts>
<context position="2262" citStr="Mitchell et al., 2002" startWordPosition="330" endWordPosition="333">valuation that can closely match teaching goals and tailor instruction to individual learners by providing them with feedback and direction on their attainment of knowledge. Traditionally, automated evaluation has been used for essay grading, but its potential could be successfully extrapolated to other genres in both first language (L1) and second language (L2) academic contexts. Existing scoring systems can assess various constructs such as topical content, grammar, style, mechanics, syntactic complexity, and even deviance or plagiarism (Burstein, 2003; Elliott, 2003; Landauer et al., 2003; Mitchell et al., 2002; Page, 2003; Rudner and Liang, 2002). Because learner writing is generally highly erroneous, an emerging research trend has focused on automated error detection in L2 output finding novel approaches to develop intelligent ways to assess ill-formed learner responses (Burstein and Chodorow, 1999; Chodorow et al., 2007; Han et al., 2006; Leacock and Chodorow, 2003). Various NLP and statistical techniques also allow for the evaluation of text organization, which is however limited to recognizing the five-paragraph essay format, thesis, and topic sentences. At present, to our knowledge, there is o</context>
</contexts>
<marker>Mitchell, Russell, Broomhead, Aldridge, 2002</marker>
<rawString>Tom Mitchell, Terry Russell, Peter Broomhead, and Nicola Aldridge. 2002. Towards robust computerised marking of free-text responses. In Proceedings of the 6th International Computer Assisted Assessment Conference, pages 233–249, Loughborough University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellis Batten Page</author>
</authors>
<title>Project Essay Grade.</title>
<date>2003</date>
<booktitle>In Shermis and Burstein (Shermis and Burstein,</booktitle>
<pages>43--54</pages>
<contexts>
<context position="2274" citStr="Page, 2003" startWordPosition="334" endWordPosition="335">ely match teaching goals and tailor instruction to individual learners by providing them with feedback and direction on their attainment of knowledge. Traditionally, automated evaluation has been used for essay grading, but its potential could be successfully extrapolated to other genres in both first language (L1) and second language (L2) academic contexts. Existing scoring systems can assess various constructs such as topical content, grammar, style, mechanics, syntactic complexity, and even deviance or plagiarism (Burstein, 2003; Elliott, 2003; Landauer et al., 2003; Mitchell et al., 2002; Page, 2003; Rudner and Liang, 2002). Because learner writing is generally highly erroneous, an emerging research trend has focused on automated error detection in L2 output finding novel approaches to develop intelligent ways to assess ill-formed learner responses (Burstein and Chodorow, 1999; Chodorow et al., 2007; Han et al., 2006; Leacock and Chodorow, 2003). Various NLP and statistical techniques also allow for the evaluation of text organization, which is however limited to recognizing the five-paragraph essay format, thesis, and topic sentences. At present, to our knowledge, there is only one auto</context>
</contexts>
<marker>Page, 2003</marker>
<rawString>Ellis Batten Page. 2003. Project Essay Grade. In Shermis and Burstein (Shermis and Burstein, 2003), pages 43–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="14149" citStr="Porter, 1980" startWordPosition="2198" endWordPosition="2199">is study we focused on automatically identifying the major moves in the introduction section of research articles (i.e., m1, m2, m3). Due to the sparseness of data, we have not attempted to identify the steps within the moves at this time. We extracted word unigrams, bigrams and trigrams (i.e., single words, two word sequences, and three word sequences) from the annotated corpus. Subsection 3.5 reports the results of some of our experiments with these feature sets. The following steps were taken in preprocessing: 1. All tokens were stemmed using the NLTK3 port of the Porter Stemmer algorithm (Porter, 1980). This allows us to represent lexically related items as the same feature, thus reducing interdependence among features and also helping with the sparse data problem. 2. All numbers in the texts were replaced by the string _number_. 3. In case of bigrams and trigrams, the tokens inside each n-gram were alphabetized to capture the semantic similarity among n-grams containing the same words but in a different order. This tactic also reduces interdependence among features and helps with the sparse data problem. 4. All n-grams with a frequency of less than five were excluded. This measure was also</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Martin F. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence M Rudner</author>
<author>Tahung Liang</author>
</authors>
<title>Automated essay scoring using Bayes’ theorem.</title>
<date>2002</date>
<journal>The Journal of Technology, Learning and Assessment,</journal>
<volume>1</volume>
<issue>2</issue>
<pages>21</pages>
<contexts>
<context position="2299" citStr="Rudner and Liang, 2002" startWordPosition="336" endWordPosition="339">aching goals and tailor instruction to individual learners by providing them with feedback and direction on their attainment of knowledge. Traditionally, automated evaluation has been used for essay grading, but its potential could be successfully extrapolated to other genres in both first language (L1) and second language (L2) academic contexts. Existing scoring systems can assess various constructs such as topical content, grammar, style, mechanics, syntactic complexity, and even deviance or plagiarism (Burstein, 2003; Elliott, 2003; Landauer et al., 2003; Mitchell et al., 2002; Page, 2003; Rudner and Liang, 2002). Because learner writing is generally highly erroneous, an emerging research trend has focused on automated error detection in L2 output finding novel approaches to develop intelligent ways to assess ill-formed learner responses (Burstein and Chodorow, 1999; Chodorow et al., 2007; Han et al., 2006; Leacock and Chodorow, 2003). Various NLP and statistical techniques also allow for the evaluation of text organization, which is however limited to recognizing the five-paragraph essay format, thesis, and topic sentences. At present, to our knowledge, there is only one automated evaluation system, </context>
</contexts>
<marker>Rudner, Liang, 2002</marker>
<rawString>Lawrence M. Rudner and Tahung Liang. 2002. Automated essay scoring using Bayes’ theorem. The Journal of Technology, Learning and Assessment, 1(2):3– 21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Machine learning in automated text categorization.</title>
<date>2002</date>
<journal>ACM Computing Surveys,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="13323" citStr="Sebastiani, 2002" startWordPosition="2064" endWordPosition="2065"> a sentence si is represented as the vector si = (f1,f2,..., fn) where each component fj of the vector �si represents a measure of feature j in the sentence si. The task of the learning algorithm is to find a function F : S -4 C that would map the sentences in the corpus S to classes in M = {m1, m2, m3} (where m1, m2, and m3 stand for Move 1, Move 2, and Move 3, respectively). In this paper, for simplicity, we are assuming that F is a many-to-one function; however, it should be kept in mind that since sentences may 1Only in two instances a step was not assigned. 2For an excellent review, see (Sebastiani, 2002). signify multiple moves, in reality the relation may be many-to-many. An important problem here is choosing features that would allow us to classify our data instances into the classes in question properly. In this study we focused on automatically identifying the major moves in the introduction section of research articles (i.e., m1, m2, m3). Due to the sparseness of data, we have not attempted to identify the steps within the moves at this time. We extracted word unigrams, bigrams and trigrams (i.e., single words, two word sequences, and three word sequences) from the annotated corpus. Subs</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>Fabrizio Sebastiani. 2002. Machine learning in automated text categorization. ACM Computing Surveys, 34(1):1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark D Shermis</author>
<author>Jill C Burstein</author>
<author>editors</author>
</authors>
<title>Automated Essay Scoring: A cross-disciplinary perspective. Lawrence Erlbaum Associates,</title>
<date>2003</date>
<location>Mahwah, NJ.</location>
<marker>Shermis, Burstein, editors, 2003</marker>
<rawString>Mark D. Shermis and Jill C. Burstein, editors. 2003. Automated Essay Scoring: A cross-disciplinary perspective. Lawrence Erlbaum Associates, Mahwah, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Swales</author>
</authors>
<title>English in Academic and Research Settings.</title>
<date>1990</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="7903" citStr="Swales, 1990" startWordPosition="1179" endWordPosition="1180">sis of its effectiveness for formative assessment purposes. Named IADE (Intelligent Academic Discourse Evaluator), this application will draw from second language acquisition models such as interactionist views and Systemic Functional Linguistics as well as from the Skill Acquisition Theory of learning. Additionally, it will be informed by empirical research on the provision of feedback and by Evidence Centered Design principles (Mislevy et al., 2006). IADE will evaluate students’ drafts of their academic writing in accordance with the course materials in terms of an adapted model of Swales’ (Swales, 1990; Swales, 2004) move schema as partially presented in Table 1. IADE will achieve this by conducting a sentence-level classification of the input text for rhetorical shifts. Given a draft of a research article, IADE will identify the discourse moves in the paper, compare it with other papers in the same discipline and provide feedback to the user. 63 Table 1: Discourse move model for research article introductions based on (Swales, 1990; Swales, 2004) The development of IADE is guided by the principles of Evidence Centered Design (ECD), “an approach to constructing and implementing educational </context>
</contexts>
<marker>Swales, 1990</marker>
<rawString>John Swales. 1990. English in Academic and Research Settings. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Swales</author>
</authors>
<title>Research Genres: Exploration and applications.</title>
<date>2004</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="7918" citStr="Swales, 2004" startWordPosition="1181" endWordPosition="1182">ectiveness for formative assessment purposes. Named IADE (Intelligent Academic Discourse Evaluator), this application will draw from second language acquisition models such as interactionist views and Systemic Functional Linguistics as well as from the Skill Acquisition Theory of learning. Additionally, it will be informed by empirical research on the provision of feedback and by Evidence Centered Design principles (Mislevy et al., 2006). IADE will evaluate students’ drafts of their academic writing in accordance with the course materials in terms of an adapted model of Swales’ (Swales, 1990; Swales, 2004) move schema as partially presented in Table 1. IADE will achieve this by conducting a sentence-level classification of the input text for rhetorical shifts. Given a draft of a research article, IADE will identify the discourse moves in the paper, compare it with other papers in the same discipline and provide feedback to the user. 63 Table 1: Discourse move model for research article introductions based on (Swales, 1990; Swales, 2004) The development of IADE is guided by the principles of Evidence Centered Design (ECD), “an approach to constructing and implementing educational assessments in </context>
</contexts>
<marker>Swales, 2004</marker>
<rawString>John Swales. 2004. Research Genres: Exploration and applications. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberta Vann</author>
<author>Cynthia Myers</author>
</authors>
<title>Academic ESL options in a large research university.</title>
<date>2001</date>
<booktitle>In Ilona Leki, editor, Academic Writing Programs, Case Studies in TESOL Practice Series. TESOL,</booktitle>
<location>Alexandria, VA.</location>
<contexts>
<context position="3949" citStr="Vann and Myers, 2001" startWordPosition="580" endWordPosition="583">he writing instruction for non-native speaker students should “help 62 Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 62–70, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics initiate writers into their field-specific research communities” (p. 333). In support of this opinion, (Kushner, 1997) reasons that graduate NNS courses have to combine language and discourse with the skill of writing within professional norms. Various pedagogical approaches have been attempted to achieve this goal. For instance, (Vann and Myers, 2001) followed the inductive analysis approach, in which students examined the format, content, grammatical, and rhetorical conventions of each section of research reports. Supplements to this approach were tasks that required students to write journal entries about the rhetorical conventions of prominent journals in their disciplines and tasks that placed the experience of writing up research “in the framework of an interactive, cooperative effort with crosscultural interaction” (Vann and Myers, 2001, p. 82). Later, after having followed a primarily skill-based approach, in which students wrote fi</context>
</contexts>
<marker>Vann, Myers, 2001</marker>
<rawString>Roberta Vann and Cynthia Myers. 2001. Academic ESL options in a large research university. In Ilona Leki, editor, Academic Writing Programs, Case Studies in TESOL Practice Series. TESOL, Alexandria, VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<contexts>
<context position="16649" citStr="Vapnik, 1995" startWordPosition="2636" endWordPosition="2637">s calculated as what is commonly known as the tf.idf (term frequency times the inverse document frequency), which is a measure of the importance of a term in a document. However, since our “documents” are all sentences and therefore very short, we decided to only record the presence or absence of terms in the sentences as Boolean values; that is, a vector component will contain either a 0 for the absence of the corresponding term or a 1 for its presence in the sentence. 3.4 Classifier We chose to use Support Vector Machines (SVM) for our classifier (Basu et al., 2003; Burges, 1998; Cortes and Vapnik, 1995; Joachims, 1998; Vapnik, 1995). SVMs are commonly used to solve classification problems by finding hyperplanes that best classify data while providing the widest margin possible between classes. SVMs have proven to be among the most powerful classifiers provided that the representation of the data captures the patterns we are trying to discover and that the parameters of the SVM classifier itself are properly set. SVM learning is a supervised learning technique where the system is provided a set of labeled data for training. The performance of the system is then measured by providing the lear</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Warschauer</author>
<author>Paige Ware</author>
</authors>
<title>Automated writing evaluation: defining the classroom research agenda.</title>
<date>2006</date>
<journal>Language Teaching Research,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="24336" citStr="Warschauer and Ware (2006)" startWordPosition="3947" endWordPosition="3950">enting with different kernel parameters to find optimal models. 4 Discussion This paper set out to identify rhetorical moves in research article introductions automatically for the purpose of developing IADE, an educational tool for helping international university students in the United States to improve their academic writing skills. The results of our models based on a relatively small data set are very encouraging, and research on improving the results is ongoing. 68 Apart from system accuracy, there are also some pedagogical issues that we need to keep in mind in the development of IADE. Warschauer and Ware (2006) call for the development of a classroom research agenda that would help evaluate and guide the application of automated essay scoring in the writing pedagogy. Based on a categorization developed by Long (1984), they propose three directions for research: product, process, and process/product, where “product refers to educational outcome (i.e., what results from using the software), process refers to learning and teaching process (i.e., how the software is used), and process/product refers to the interaction between use and outcome” (p. 10). On the level of evaluating technology for language l</context>
</contexts>
<marker>Warschauer, Ware, 2006</marker>
<rawString>Mark Warschauer and Paige Ware. 2006. Automated writing evaluation: defining the classroom research agenda. Language Teaching Research, 10(2):1–24.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>