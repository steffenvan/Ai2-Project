<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.035849">
<title confidence="0.99203">
Grammaticality Judgement in a Word Completion Task
</title>
<author confidence="0.881403">
Alfred Renaud2 and Fraser Shein1,2 and Vivian Tsang1
</author>
<affiliation confidence="0.49376">
1Bloorview Kids Rehab 2Quillsoft Ltd.
</affiliation>
<address confidence="0.9575065">
150 Kilgour Road 2416 Queen Street East
Toronto, ON M4G 1R8, Canada Toronto, ON M2A 1N1, Canada
</address>
<email confidence="0.997848">
{arenaud,fshein,vtsang}@bloorview.ca
</email>
<sectionHeader confidence="0.995604" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99986264">
In this paper, we present findings from a hu-
man judgement task we conducted on the ef-
fectiveness of syntax filtering in a word com-
pletion task. Human participants were asked
to review a series of incomplete sentences and
identify which words from accompanying lists
extend the expressions in a grammatically ap-
propriate way. The accompanying word lists
were generated by two word completion sys-
tems (our own plus a third-party commercial
system) where the ungrammatical items were
filtered out. Overall, participants agreed more,
to a statistically significant degree, with the
syntax-filtered systems than with baseline.
However, further analysis suggests that syntax
filtering alone does not necessarily improve
the overall acceptability and usability of the
word completion output. Given that word
completion is typically employed in applica-
tions to aid writing, unlike other NLP tasks,
accounting for the role of writer vs. reader be-
comes critical. Evaluating word completion
and, more generally, applications for alterna-
tive and augmentative communication (AAC)
will be discussed.
</bodyText>
<sectionHeader confidence="0.999335" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998355625">
Writers often need help from others to help with
spelling and grammar. For persons with physical
or learning disabilities, writing can be very stress-
ful because of a greater reliance on the assistance
of others. Software tools such as word comple-
tion are now commonly used to reduce the physi-
cal and cognitive load of completing a word or a
sentence and thereby reducing a writer’s depend-
ence on others. But can such tools be as effective
as a human with adequate linguistic knowledge?
While it is hardly possible to completely emulate
a human tutor or a communication partner, the
purpose of this research is to investigate how
much linguistic knowledge is necessary to ensure
the usability of word completion. Here, we will
focus on the grammaticality of word completion.
</bodyText>
<subsectionHeader confidence="0.994894">
1.1 Word Completion
</subsectionHeader>
<bodyText confidence="0.999948083333333">
Word completion facilitates text entry by suggest-
ing a list of words that can follow a given linguis-
tic context. If the desired word is in the list, the
user can select that word with a mouse click or a
keystroke, thereby saving the effort of typing the
remaining letters of the word. Otherwise, the user
can continue typing while the software continues
to display new lists of words based on that input.
For example, consider a user wants to type
“That girl by the benches...” After each letter the
user manually enters, a system would return a list
of potential next words. Say, the next letter the
user enters is “w.” A system may offer the fol-
lowing choices: a) was, b) were, c) with, d) where,
e) wrapped. By suggesting words in any given
context, word completion can assist in the compo-
sition of well-formed text.
Typical word completion systems suggest
words by exploiting n-gram Markov statistical
models (Bentrup, 1987). These systems probabil-
istically determine the current word in a sentence
given the previous n–1 words as context, based on
a pre-generated n-gram language model derived
from a corpus. With n typically being of low or-
</bodyText>
<page confidence="0.979139">
15
</page>
<note confidence="0.966473">
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics and Writing, pages 15–23,
Los Angeles, California, June 2010. @2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999832230769231">
der (two or three, due to sparse data and computa-
tional issues), one consequence is that the appli-
cability of suggested words beyond a certain word
distance may become somewhat arbitrary. Fur-
ther design improvements for word completion
depend on the user population and the intended
use. For example, the demand on the system to
have a sophisticated language model may depend
on whether the intent is to primarily reduce the
physical or cognitive load of entering text.
Evaluation approaches can elucidate on design
and implementation issues for providing meaning-
ful word choices.
</bodyText>
<subsectionHeader confidence="0.874115">
1.2 Evaluation Approaches
</subsectionHeader>
<bodyText confidence="0.99994761627907">
A number of studies have been carried out to
evaluate the efficacy of word completion systems.
Koester (1994) measured time savings, which is
the reduction in time that the user takes to gener-
ate a particular text with the aid of a word comple-
tion system compared to the time taken without it.
The rationale for this measure is that any word
completion system imposes a cognitive load on its
users, whereby they now need to 1) change their
focus between the target document and the word
list display, and possibly between the screen and
keyboard; 2) visually scan the word list to decide
whether their intended word is present; and 3) se-
lect the intended word with the keyboard or
mouse. Others have also examined similar visual-
cognitive issues of using word completion (e.g.,
Tam and Wells, 2009). The overall approach im-
plicitly defines a user-centred approach to evalua-
tion by having human subjects simulate the actual
writing process (usually in a copying, not writing
task). Thus, results depend on the abilities and
preferences of individual subjects.
System-based evaluation measures exist, the
most common of which is keystroke savings. This
measures the reduction in the number of key-
strokes needed to produce a given text with the
aid of a word completion system. Keystroke sav-
ings is an important factor for users with physical
disabilities who have difficulty working with a
keyboard for which it is desirable to keep the
number of keystrokes to a minimum. A comple-
mentary measure, completion keystrokes, deter-
mines how quickly a given word is predicted by
counting the number of characters required to
reach completion. Completion keystrokes differs
from keystroke savings in that the latter counts the
letters remaining in the word.
In contrast to the previous two measures, both
of which measure at the character level, hit rate
measures at the word level by calculating the ratio
of the number of words correctly predicted to the
total number of words predicted. Given a suffi-
ciently large lexicon, hit rate can be as high as
100% if every letter of every word is manually
entered to its completion. As this can be mislead-
ing, hit rate is more typically measured with refer-
ence to the number of characters already typed in
order to assess the system’s demand on the user.
These objective measures address motor load
independent of cognitive load. With the exception
of time savings, these measures can be bench-
marked automatically by simulating the writing
process by using existing texts.
A shortcoming of these objective measures is
that they focus on the reduction on the user’s
physical demand by simulating the entering of an
already written text, and effectively ignore con-
sideration of word choices other than the unique
intended word. In reality, the actual writing proc-
ess depends also on the quality of the entire group
of suggested word choices with respect to the in-
tended content. Renaud (2002) addressed this
shortcoming by arguing that the syntactic and se-
mantic relations between words can impact on
choice-making at the target word. He introduced
two measures, validity and appropriateness,
measuring grammatical consistency and semantic
relevance of all system output, respectively. The
former measure calculates the proportion of a sys-
tem’s suggested words that is syntactically ac-
ceptable. The latter focuses on the proportion of
relevant output based on lexical and domain se-
mantics. Renaud compared a number of commer-
cial systems and found a positive correlation be-
tween the new and existing measures. This find-
ing also lends additional support to Wood’s
(1996) finding that offering syntactically and se-
mantically appropriate choices improves perform-
ance. (Note that the converse may not hold true.)
For the remainder of this paper, we will put our
emphasis on the impact of linguistic content (here,
grammaticality) on the quality of word comple-
tion. The paper is organized as follows. In the
next section, we will describe the need to incorpo-
rate syntactic information in word completion. In
sections 3 and 4, we will describe our human
</bodyText>
<page confidence="0.995175">
16
</page>
<bodyText confidence="0.99989825">
judgement task evaluating the grammaticality of
word completion. Based on our analysis, we will
return to the evaluation issue in section 5 and dis-
cuss how grammaticality alone does not address
the larger usability issue of word completion.
Here, we propose that the word completion task,
unlike traditional NLP tasks, requires both the
reader’s and writer’s perspectives, which impacts
the interpretation of our evaluation, and in turn
impacts design decisions. In section 6, we will
conclude by offering a more inclusive perspective
on AAC.
</bodyText>
<sectionHeader confidence="0.729527" genericHeader="method">
2 The Demand for Syntactic Filtering
</sectionHeader>
<bodyText confidence="0.999990472222222">
As shown earlier, many evaluation methods have
focused on 1) the proportion of key-presses nor-
mally required during a typing session that the
user need not to manually enter and 2) the propor-
tion of text words in a typing session that the sys-
tem is able correctly to predict. For a user with a
learning disability or language difficulties, a
greater concern is that all presented words be
valid, logical, and free of grammatical errors.
Current state-of-the-art systems suffer by suggest-
ing words that are often syntactically implausible
while excluding more justifiable but less probable
suggestions (cf. our example in section 1). A user
may be confused by inappropriate suggestions,
even if correct suggestions are also present.
To quantify the importance of syntax in word
completion, we compare the average hit rate
scores (over all words) with the hit rate scores at
points in sentences we consider as syntactically
critical (see section 3 for their selection). Nantais
et al. (2001) reported an overall hit rate of ap-
proximately 56% using bigram word completion
after entering the first letter of a word across a
large document. However, at the word location
where it is crucial to maintain correct syntactic
relation with the existing sentence fragment, hit
rates are often much lower. In our study situation,
the hit rate is at best 39%—these syntactic chal-
lenges tend to be semantically contentful and thus
present difficulties to human subjects. Likewise,
the systems are expected to struggle with them.
Without a clear understanding of content specific
issues during writing, examining time and key-
stroke savings alone does not reveal the increased
difficulty a user faces at these word positions. We
will return to these issues in section 5.
</bodyText>
<subsectionHeader confidence="0.999105">
2.1 Building Syntactic Knowledge
</subsectionHeader>
<bodyText confidence="0.998209040816327">
Knowledge of syntax can be obtained by first tag-
ging each dictionary word with its part of speech,
such as noun or adjective. This information may
then be used in either a probabilistic or a symbolic
manner. Systems may reason probabilistically by
combining tag n-gram models, where the part-of-
speech tags for the previous n–1 words in a sen-
tence are used to predict the tag for the current
word, with word n-gram models that cue the re-
sulting part(s) of speech to find words proper
(Hunnicutt and Carlberger, 2001). Fazly and
Hirst (2003) introduced two algorithms for com-
bining tag trigrams with word bigrams. The first
algorithm involved conditional independence as-
sumptions between word and tag models, and the
second algorithm involved a weighted linear com-
bination of the two models.
A fundamental limitation to this approach is
that low-order probabilistic language models can
only account for relationships between closely co-
located words. Symbolic syntactic prediction
guided by a grammar, on the other hand, can deal
with long-distance word relationships of arbitrary
depth by applying rules that govern how words
from syntactic categories can be joined, to assign
all sentence words to a category. This approach
uses knowledge of English grammar to analyze
the structure of the sentence in progress and de-
termine the applicable syntactic categories (e.g.,
noun, verb), along with other features (e.g., singu-
lar, past participle), to which the currently
typed/predicted word must belong. In this way a
word completion system is able to suggest words
that are grammatically consistent with the active
sentence fragment.
As such, research closer in nature to our work
involves parsers that process the input sentence
incrementally as each word is entered. Wood’s
(1996) augmented phrase-structure grammar
showed that symbolic syntactic prediction can
improve overall performance when combined
with statistical orthographic prediction. McCoy
(1998) used the augmented transition network or
ATN (Woods, 1970) formalism to find candidate
word categories from which to generate word
lists. Gustavii and Pettersson (2003) used a chart
parser to re-rank, or filter, word lists by gram-
matical value. These parsing algorithms manipu-
late some data structure that represents, and im-
</bodyText>
<page confidence="0.994433">
17
</page>
<bodyText confidence="0.999902666666667">
poses ordering on, syntactic constituents of sen-
tences. Recently, we have been developing a syn-
tax module (Renaud et al., 2010) based on an
ATN-style parser, which can facilitate both in-
creasing the level of correctness in parses through
grammar correction, and modifying the informa-
tion collected during parsing for a particular ap-
plication (Newman, 2007). Specifically, this
system filters words provided by n-gram
completion such that the word list only shows
words that fit an acceptable grammatical structure.
It operates on a longer list of the same frequency-
ranked words our core predictor generates. Under
this setup, our syntax module can influence the
final list shown to the user by demoting
implausible words that otherwise would have been
displayed and replacing them with plausible
words that otherwise would not. Our rationale for
using a symbolic vs. a probabilistic parser in word
completion is beyond the scope of the current pa-
per.
</bodyText>
<sectionHeader confidence="0.997129" genericHeader="method">
3 Grammaticality Judgement Experiment
</sectionHeader>
<bodyText confidence="0.999983714285714">
To evaluate the impact of syntactic filtering on
word completion, we devised a human judgment
task where human subjects were asked to judge
the grammatical acceptability of a word offered
by word completion software, with or without
syntactic filtering. Given a partial sentence and a
leading prefix for the next word, word completion
software presents a number of choices for the po-
tential next word. Although the goal is to assess
the grammaticality of predicted words with or
without syntactic filtering, the intent is to assess
whether the inclusion of syntactic heuristics in the
word completion algorithm improves the quality
of word choices.
</bodyText>
<subsectionHeader confidence="0.997856">
3.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.998568714285714">
In our experiment, we compared three different
word completion systems: our baseline comple-
tion system (WordQ®*, henceforth “baseline”),
our word completion system with syntax filtering
(“System B”). We also included a third-party
commercial word completion system with syntax
filtering built-in (Co:Writer®†, “System C”). In
</bodyText>
<footnote confidence="0.908182666666667">
* http://www.wordq.com; our baseline system uses a bigram
language model trained on a corpus of well-edited text.
† http://www.donjohnston.com/products/cowriter/index.html
</footnote>
<bodyText confidence="0.999936">
each system, we inputted a partial sentence plus
the leading character for the next word. Each sys-
tem returned a list of five choices for the potential
next word. Our subjects were asked to judge the
grammatical acceptability of each word (binary
decision: yes or no).
It is worth noting that the more letters are
manually inserted, the narrower the search space
becomes for the next word. Nantais et al. (2001)
suggested that after inserting two characters, the
hit rate via automatic means can be as high as
72%; the hit rate for humans is likely much
higher. Given that our goal is to examine the
grammaticality of word choices and not hit rate,
providing only one leading letter allows sufficient
ambiguity on what the potential next word is,
which in turn allows for a range of grammatical
choices for our judgement task.
</bodyText>
<subsectionHeader confidence="0.999181">
3.2 Sentence Selection
</subsectionHeader>
<bodyText confidence="0.94121816">
We selected our test sentences from Canadian
news sources (Toronto Star and the Globe and
Mail), which are considered reliably grammatical.
We chose a total of 138 sentences.à Each sen-
tence was truncated into a fragment containing the
first x-1 words and the first character of the xth
word, where x ranges from three to ten inclusive.
The truncation position x was deliberately selected
to include a variety of grammatical challenges.
We divided the sentence fragments into nine
types of grammatical challenges: 1) subject-verb
agreement; 2) subject-verb agreement in question-
asking; 3) subject-verb agreement within a rela-
tive clause; 4) appositives; 5) verb sequence (aux-
iliary verb-main verb agreement); 6) case agree-
ment; 7) non-finite clauses; 8) number agreement;
and 9) others.
For example, the sentence “That girl by the
benches was in my high school” from section 1.1
can be used to test the system’s ability to recog-
nize subject-verb agreement if we truncate the
sentence to produce the fragment “That girl by the
benches w___.” Here, subject-verb agreement
should be decided against the subject “girl” and
not the (tempting) subject “benches.”
</bodyText>
<footnote confidence="0.6224164">
à We did not pick a larger number of sentences due to the
time constraint in our experimental setup. The rationale is to
avoid over-fatiguing our human subjects (approximately an
hour per session). Based on our pilot study, we were able to
fit 140 sentences over three one-hour sessions.
</footnote>
<page confidence="0.997488">
18
</page>
<bodyText confidence="0.999890625">
After the initial selection process, we reduced
our collection to 123 partial sentences. Because
the sentences were not evenly distributed across
the nine categories, we divided the sentences into
three sets such that the frequency distribution of
the sentence types was the same for all three sets
(41 sentences per set). The three word completion
systems were each assigned a different set.§
</bodyText>
<subsectionHeader confidence="0.998308">
3.3 Grammaticality Judgements
</subsectionHeader>
<bodyText confidence="0.999996352941177">
We fed each partial sentence into the correspond-
ing system to produce a word list for grammatical
judgement. Recall our example earlier, given five
word choices per partial sentence, for each word
choice, our subjects were asked to judge its
grammatical acceptability (yes or no).
We recruited 14 human subjects, all native
speakers of English with a university education.
Each subject was presented all 123 sentences cov-
ering the three systems, in a paper-based task.
The sentence order was randomized and the sub-
jects were unaware of which system produced
what list.
Given that each system produced a list of five
options for each partial sentence, each subject
produced 5×41=205 judgements for each system.
There were 14 sets of such judgements in total.
</bodyText>
<sectionHeader confidence="0.999537" genericHeader="method">
4 Results and Analysis
</sectionHeader>
<bodyText confidence="0.99129285915493">
Our primary objective is to examine the subjects’
agreement with the system, and whether the sub-
jects generally agree among themselves. Our ra-
tionale is this. If the subjects generally agree with
one another, then there is an overall agreement on
the perception of grammaticality in word comple-
tion. If this is indeed the case, we then need to
examine how and why our subjects agree or dis-
agree with the systems. Otherwise, if there is low
inter-subject agreement, aside from issues related
to the experimental setup, we need to reconsider
whether offering grammatical word completion
choices is indeed practical and possible.
We first calculated individual participant
agreement with the output of each system (i.e.,
§ We initially to used three different sets, i.e., one set per
system, to avoid a sampling “fluke” of different grammatical
difficulties/categories. However, for exactly the same reason,
we also tested our system using the two sets for the other two
systems for ease of comparison. See section 4.1 for details.
averaged over all participants). The baseline
scored 68%. System B scored 72% and System C
scored 74%. Thus, an early important result was
that syntax assistance in general, independent of
particular approach or algorithm, does appear to
improve subject agreement in a word completion
task. (Note that we treat system C as a black box
as we are not privy to its algorithms, which are
not published.)
Overall, the grammaticality of a given test word
(i.e., averaged over all test words) had an average
agreement of 85%, or by 12 of the 14 participants.
The percentage agreement for each system was
84% for the baseline, 87% for system B, and 86%
for system C. If at least two-thirds of the partici-
pants (10 of 14) agreed on the grammaticality of a
particular test word, we considered the collective
opinion to be consistent for that word and de-
clared a consensus. Participants reached consen-
sus on 77% of the test words for the baseline, 82%
of the test words for system B, and 80% of the test
words for system C.
Next, we calculated consensus participant
agreement for each system. This measure was
different from the previous in that we considered
only those cases where 10 or more of the 14 par-
ticipants agreed with one another on the gram-
maticality of a system’s test word and discarded
all other cases. In 75% of the consensus cases for
the baseline, the subjects agreed with the system
(by approving on the grammaticality); in the other
25% of the consensus cases the subjects disagreed
with the system. System B scored 78% on the
consensus agreement and system C scored 81%.
A repeated-measures analysis of variance
(ANOVA) was performed on the data. For both
individual and consensus participant agreement,
each of Systems B and C outperformed the base-
line system (statistically significant, p&lt;.05), while
the difference between the two systems with syn-
tax awareness was not statistically significant.
To summarize our findings, our subjects gener-
ally found the output grammatically more accept-
able if syntactic assistance was built in (72% and
74% over 68% in raw participant agreement; 78%
and 81% over 75% in consensus agreement). The
behaviour of our System B generally was in line
with the behaviour of the third-party System C.
Finally, the agreement among subjects for all sys-
tems was quite high (~85%) and is considered
reliable.
</bodyText>
<page confidence="0.997998">
19
</page>
<subsectionHeader confidence="0.97132">
4.1 Subject Agreement with Other Systems
</subsectionHeader>
<bodyText confidence="0.999990276923077">
To further understand the behaviour of our own
system (in contrast to our subjects’ judgements),
we create two new systems, A&apos; and C&apos; based on
the output of the baseline system and the third-
party System C. Recall that the sentence set used
in each system is mutually exclusive from the set
used in another system. Therefore, this setup in-
troduces an additional set of 41 sentences × 5 pre-
dicted words × 2 systems = 410 judgements.
Our setup is simple: we feed into our parser
each of the sentence fragments for the correspond-
ing system, along with each predicted word origi-
nally produced. If our parser accepts the word,
the analysis remains unchanged. Otherwise, we
count it as a “negative” result, which we explain
below.
Consider again our earlier example, “The girl
by the benches w___.” Say system C&apos; produces
the following options: a) was, b) were, c) with, d)
where, e) wrapped. We then attempt to generate a
partial parse using the partial sentence with each
predicted word, i.e., “The girl by the benches
was,” “The girl by the benches were,” and so on.
If, for instance, our parser could not generate a
parse for “The girl by the benches where,” then
we would treat the word choice “where” as not
approved for the purpose of recalculating subject
agreement. So if any subjects had approved its
grammaticality (i.e., considered it a grammatical
next word), then we counted it as a disagreement
(between the parser and the human judge), other-
wise, we considered it an agreement.
Consider the following example. One partial
sentence for System C was “Japanese farmers
immediately pick the shoots which a[m]...” Only
1 of 14 judges agreed with it. System C&apos; also
flagged “am” as ungrammatical. Now 13 judges
agreed with it.
On the other hand, consider this partial sen-
tence originally from the baseline system, “The
reason we are doing these i[nclude]...” where 10
judges said yes but our parser could not generate a
parse. In this case, A&apos; scores 4 on agreement.
Overall, A&apos; overrode 10 decisions and scored
71% agreement as a result. That is a 3% im-
provement over the baseline 68% score. Nine of
the 10 reversed consensus in a positive direction
and 1 (example above) reversed consensus in a
negative direction. In comparison, C&apos; overrode 6
decisions, and scored 76% (2.0% improvement
over the original 74%). Five of 6 cases reversed
consensus, all in a positive direction. (The other
case reversed a non-consensus in a positive direc-
tion.) Given that the theoretical maximum agree-
ments for the two systems are 84% and 86% (i.e.,
regardless of polarity), there is considerable in-
crease in the subject agreement.
It is worth noting that many subjects made the
number agreement mistake due to proximity. In
the previous example, “The reason we are doing
these i[nclude]...”, the subjects made the incorrect
agreement linking “include” to “these” instead of
linking to “the reason.” While these cases are not
prevalent, this is one reason (among many) that
the theoretical maximum agreement is not 100%.
</bodyText>
<subsectionHeader confidence="0.998168">
4.2 System’s vs. Subjects’ Perspective
</subsectionHeader>
<bodyText confidence="0.999984148148148">
Although the agreement between the systems and
the subjects were high, no system achieved perfect
agreement—many words were considered un-
grammatical extensions of the partial sentences.
We see two possible explanations: 1) the dis-
agreeable output was erroneous; or 2) the dis-
agreeable output was grammatical but judged as
ungrammatical under certain conditions.
We manually examined the parse trees of the
“disagreeable” cases from our system. Interest-
ingly, in most cases, we found there exists a rea-
sonable parse tree leading to a grammatical sen-
tence. We thus conclude that grammaticality
judgements of partial sentences might not com-
pletely reflect the underlying improvement of the
word completion quality. That is, discrepancies
between human and computer judgement need not
point to a poor quality syntax filter; instead, it
may indicate that the system is exhibiting correct
behaviour but simply disagrees with subjects on
the particular grammatical cases in question. In
such cases, subjects’ disagreement with the sys-
tem does not provide sufficient grounds for mak-
ing modifications to the system’s behaviour.
Rather, it is worth examining the factors leading
to the subjects’ perception of a word as an un-
grammatical extension of a partial sentence.
</bodyText>
<sectionHeader confidence="0.998711" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999745333333333">
Overall, our results indicate that our subjects
agree with the grammaticality of word completion
more when syntactic filtering is used than not.
</bodyText>
<page confidence="0.986622">
20
</page>
<bodyText confidence="0.9915108">
That said, in light of the disagreeable cases, we
believe that the quality of word completion may
not be so straightforwardly evaluated.
ception of grammaticality of the next word (cf. hit
rate issues in section 2).
</bodyText>
<subsectionHeader confidence="0.9872285">
5.2 Evaluation Approach
5.1 Selectional Restriction
</subsectionHeader>
<bodyText confidence="0.999993355932204">
Take this example, “The plane carrying the sol-
diers a___.” The next word “are” was unani-
mously considered ungrammatical by our human
judges. Consider the following full sentence ver-
sion of it: “The plane carrying the soldiers are
contemplating is too difficult a task.” In this case,
the subject is “the plane carrying” (as an activity),
the relative clause is “the soldiers are contemplat-
ing”, and finally, the verb phrase is “is too diffi-
cult a task.” This sentence may be difficult to in-
terpret but a meaningful interpretation is possible
syntactically and semantically.
Consider the following variation, “The political
situation the soldiers a___.” In this case, it is not
difficult to conceive that “are” is a possible next
word, as in “The political situation the soldiers are
discussing is getting worse.” The syntactic con-
struction is [noun phrase] [relative clause] [verb
phrase]. Both partial sentences have a potential
grammatical parse. Why then is one considered
grammatical and the other not?
Sentences that induce midpoint reading diffi-
culties in humans are well known in psycholin-
guistics and are referred to as garden-path sen-
tences (Frazier, 1978). Reading “the plane carry-
ing the soldiers” induces an expectation in the
reader’s mind that the sentence is about the plane
doing the carrying, and not about the carrying of
the plane by the soldiers, leading to a “short cir-
cuit” at the word “are.”
In linguistics and CL, one aspect of this phe-
nomenon, selectional restriction, has been ex-
plored previously (most notably Levin, 1993 and
Resnik, 1995). Selectional restriction is defined
as the semantics of a verb restricting the type of
words and phrases that can occur as its arguments.
Essentially, the meaning of the verb makes an im-
pact on what is possible syntactically and seman-
tically. What we observe here is a generalized
case where it is no longer only about a verb plac-
ing syntactic and semantic restrictions on its sur-
rounding words. Instead, we observe how a word
or a number of words influencing the semantic
interpretation, and in turn impacting on the per-
Although our original intent was to study the
grammaticality of word completion, ultimately the
question is what impacts on the quality of word
completion. It is without a doubt that the gram-
maticality of the next word suggestions impacts
on the perception of the quality of word comple-
tion. However, we believe the key hinges on
whose perspective of quality is considered, which
then becomes a usability issue.
Recall that word completion is designed to aid
the writing process. The curious part of our
evaluation was that we devised it as a grammati-
cality judgement task via reading. Is grammati-
cality different when one is reading vs. writing?
We consider this issue in two ways.
</bodyText>
<subsectionHeader confidence="0.81483">
Partial Sentences vs. Full Sentences
</subsectionHeader>
<bodyText confidence="0.999555380952381">
Let us revisit our garden-path example:
1a. The plane carrying the soldiers a[re]...
1b. The plane carrying the soldiers are
contemplating is not that difficult a task.
2a. The political situation the soldiers a[re]...
2b. The political situation the soldiers are
losing sleep over is getting worse.
In sentences 1a and 2a, readers have no choice but
to judge the grammaticality of “are” based on the
existing partial sentence. Depending on the
reader’s creativity, one may or may not anticipate
potential full sentences such as 1b and 2b. In con-
trast, consider an alternative experimental setup
where the readers were offered full sentences such
as 1b and 2b and were asked to judge the gram-
maticality of “are.” Given the complexity of the
sentences (selectional restriction aside), the read-
ers would have no choice but to consider the exis-
tence of a relative clause, which should increase
the likelihood of evaluating “are” as a grammati-
cal component of the sentence.
</bodyText>
<subsectionHeader confidence="0.507183">
Reading vs. Writing
</subsectionHeader>
<bodyText confidence="0.99984025">
Now we have observed the potential impact on
grammaticality judgements of a potential next
word when reading a partial sentence vs. a full
sentence. That said, it needs emphasizing that the
</bodyText>
<page confidence="0.996821">
21
</page>
<bodyText confidence="0.999987642857143">
key issue is to evaluate the quality of a suggested
next word given a partial sentence, not grammati-
cality in complete isolation. When a user uses
word completion, he/she is actively engaged in the
writing process. No software can truly predict the
intent of the writer; the full sentence is waiting to
be written and cannot be written a priori.
Consider someone who is in the process of
writing the sentence “The plane carrying the sol-
diersÉ” Is this writer likely to be debating in
his/her head whether the sentence is about the
plane that does the carrying or “plane carrying” as
an activity? Clearly, the writer’s intent is clear to
the writer him/herself. In contrast, a sentence may
be perfectly grammatical and semantically rea-
sonable, yet a reader may still find it ambiguous
and/or difficult to read. In other words, the per-
ception of grammaticality of a next word depends
on the task (reading vs. writing). This is not to
say that our evaluation task is compromised as a
result. Despite that the general grammar rules do
not change, our reading judgements depending on
the context (e.g., partial vs. full sentence) suggests
that the reading perspective only provide a partial
picture on the quality of output that is intended for
a writing task. In our case, higher quality syntac-
tic filtering (e.g., our parser here) may not lead to
greater usability.
</bodyText>
<sectionHeader confidence="0.990069" genericHeader="method">
6 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999977140845071">
In this paper, we have shown that the quality of
word completions depends on the perspective one
takes. Considering that AAC is to aid someone in
producing content for communication, i.e., for
third-party consumption, the reading-writing di-
chotomy is too serious an issue to ignore. This
issue has received some CL attention (Morris,
2004, 2010; Hirst, 2008, 2009) but has not been
discussed in the AAC literature (Tsang et al.,
2010). The question remains, how do we then
evaluate, and more generally, design and use an
AAC application?
We believe the issue is far from clear. Take our
current focus—grammaticality of word comple-
tion. If the form of the content produced is un-
grammatical or difficult to read from the perspec-
tive of a reader, you risk having the reader misun-
derstand the writer’s intent. However, from the
writer’s perspective, unless he/she is perceptive of
the interpretation problems with his/her potential
readers, there is no incentive to produce content as
such; the writer can only produce content based
on his/her previous linguistic experience.
One may argue that corpus statistics may best
capture human linguistic behaviour. For example,
hit rate statistics using existing corpora is one
such way of assessing the quality of word comple-
tion. However, corpora tell only one half of the
story—only the writing half is captured, the inter-
pretation issues from the reading side are rarely
captured, if at all.
More important, the design of word completion
is setup in a way that the task consists of both a
reading component and a writing one—the appro-
priateness of suggested words is assessed by the
writer via reading during the writing task. In fact,
this is not merely a case of reading vs. writing, but
rather, an issue of relevance depending on the lin-
guistic context as well as the user’s perception of
it. Traditionally, researchers in CL and psycho-
linguistics have attempted to deal with human
processing of linguistic content at various levels
(cf. the CUNY Conference on Human Sentence
Processing, e.g., Merlo and Stevenson, 2002).
However, no computational means is truly privy
to the content behind the linguistic form. Content,
ultimately, resides in the reader’s or the writer’s
head, i.e., intent. The question remains how best
to design AAC to aid someone to communicate
this content.
In summary, in our grammaticality judgement
task, incorporating syntax in word completion
improves the perceived quality of word choices.
That said, it is unclear how quality relates to us-
ability. Indeed, the evaluation is far from conclu-
sive in that it only captures the reader’s perspec-
tive and not the writer’s. Currently, we are not
aware of the existence of a purely writer-based
evaluation for grammaticality of word completion
(see Lesher et al., 2002 for one curious attempt).
More generally, the reader-writer (or speaker-
listener) dichotomy is unexplored in AAC re-
search and should be considered more seriously
because communication (as text, speech, or oth-
erwise) involves multiple people producing and
consuming content, where the perception of con-
tent differs considerably. The challenge of AAC
may lie in bridging the gap between production
and consumption where communication is neither
only about communicating intent nor making in-
terpretations.
</bodyText>
<page confidence="0.994691">
22
</page>
<sectionHeader confidence="0.998809" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999588">
This project is funded by Quillsoft Ltd. We also
wish to thank Jiafei Niu (University of Toronto)
for conducting our usability study and Frank
Rudzicz (University of Toronto) for providing
helpful comments.
</bodyText>
<sectionHeader confidence="0.999191" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999941240963856">
John Bentrup. 1987. Exploiting Word Frequencies and
their Sequential Dependencies. Proceedings of
RESNA 10th Annual Conference, 121–122.
Afsaneh Fazly and Graeme Hirst. 2003. Testing the
Efficacy of Part-of-Speech Information in Word
Completion. Proceedings of the 2003 EACL Work-
shop on Language Modeling for Text Entry Meth-
ods, 9–16.
Lyn Frazier. 1978. On Comprehending Sentences: Syn-
tactic Parsing Strategies. Ph.D. Thesis, University
of Connecticut.
Ebba Gustavii and Eva Pettersson. 2003. A Swedish
Grammar for Word Prediction. Master’s Thesis,
Department of Linguistics, Uppsala University.
Graeme Hirst. 2008. The Future of Text-Meaning in
Computational Linguistics. In Proceedings of the
11th International Conference on Text, Speech and
Dialogue, 1–9.
Graeme Hirst. 2009. Limitations of the Philosophy of
Language Understanding Implicit in Computational
Linguistics. In Proceedings of the Seventh European
Conference on Computing and Philosophy, 108–
109.
Sheri Hunnicutt and Johan Carlberger. 2001. Improv-
ing Word Prediction Using Markov Models and
Heuristic Methods. Augmentative and Alternative
Communication, 17(4):255–264.
Heidi Koester. 1994. User Performance with Augmen-
tative Communication Systems: Measurements and
Models. Ph.D. thesis, University of Michigan.
Gregory W. Lesher, Bryan J. Moulton, D. Jeffery
Higginbotham, and Brenna Alsofrom. 2002. Limits
of Human Word Prediction Performance. In Pro-
ceedings of 2002 CSUN Conference.
Beth Levin. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. University of
Chicago Press.
Kathleen F. McCoy. 1998. The Intelligent Word Pre-
diction Project. University of Delaware.
http://ww.asel.udel.edu/nli/nlp/wpredict.html
Paola Merlo and Suzanne Stevenson, Eds. 2002. The
Lexical Basis of Sentence Processing: Formal,
Computational and Experimental Issues. John Ben-
jamins Publishing Company.
Jane Morris. 2004. Readers’ Interpretations of Lexical
Cohesion in Text. Conference of the Canadian As-
sociation for Information Science, Winnipeg, Mani-
toba.
Jane Morris. 2010. Individual Differences in the Inter-
pretation of Text: Implications for Information Sci-
ence. Journal of the American Society for Informa-
tion Science and Technology, 61(1):141–149.
Tom Nantais, Fraser Shein, and Mattias Johansson.
2001. Efficacy of the word prediction algorithm in
WordQ. In Proceedings of the 2001 RESNA Annual
Conference, 77–79.
Paula S. Newman. 2007. RH: A Retro Hybrid Parser.
In Proceedings of the 2007 NAACL Conference,
Companion, 121–124.
Alfred Renaud. 2002. Diagnostic Evaluation Measures
for Improving Performance of Word Prediction Sys-
tems. Master’s Thesis, School of Computer Science,
University of Waterloo.
Alfred Renaud, Fraser Shein, and Vivian Tsang. 2010.
A Symbolic Approach to Parsing in the Context of
Word Completion. In Preparation.
Philip Resnik. 1995. Selectional Constraints: An In-
formation-Theoretic Model and its Computational
Realization. Cognition, 61:127–125.
Cynthia Tam and David Wells. 2009. Evaluating the
Benefits of Displaying Word Prediction Lists on a
Personal Digital Assistant at the Keyboard Level.
Assistive Technology, 21:105–114.
Vivian Tsang and Kelvin Leung. 2010. An Ecological
Perspective of Communication With or Without
AAC Use. In Preparation.
Matthew Wood. 1996. Syntactic Pre-Processing in
Single-Word Prediction for Disabled People. Ph.D.
Thesis, Department of Computer Science, Univer-
sity of Bristol.
William Woods. 1970. Transition Network Grammars
for Natural Language Analysis. Communications of
the ACM, 13(10):591–606.
</reference>
<page confidence="0.998936">
23
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.7926545">Grammaticality Judgement in a Word Completion Task and and</title>
<affiliation confidence="0.691978">Kids Rehab Ltd.</affiliation>
<address confidence="0.9601845">150 Kilgour Road 2416 Queen Street East Toronto, ON M4G 1R8, Canada Toronto, ON M2A 1N1,</address>
<email confidence="0.99448">arenaud@bloorview.ca</email>
<email confidence="0.99448">fshein@bloorview.ca</email>
<email confidence="0.99448">vtsang@bloorview.ca</email>
<abstract confidence="0.99828520392157">In this paper, we present findings from a human judgement task we conducted on the effectiveness of syntax filtering in a word completion task. Human participants were asked to review a series of incomplete sentences and identify which words from accompanying lists extend the expressions in a grammatically appropriate way. The accompanying word lists were generated by two word completion systems (our own plus a third-party commercial system) where the ungrammatical items were filtered out. Overall, participants agreed more, to a statistically significant degree, with the syntax-filtered systems than with baseline. However, further analysis suggests that syntax filtering alone does not necessarily improve the overall acceptability and usability of the word completion output. Given that word completion is typically employed in applications to aid writing, unlike other NLP tasks, accounting for the role of writer vs. reader becomes critical. Evaluating word completion and, more generally, applications for alternative and augmentative communication (AAC) will be discussed. Writers often need help from others to help with spelling and grammar. For persons with physical or learning disabilities, writing can be very stressful because of a greater reliance on the assistance of others. Software tools such as word completion are now commonly used to reduce the physical and cognitive load of completing a word or a sentence and thereby reducing a writer’s dependence on others. But can such tools be as effective as a human with adequate linguistic knowledge? While it is hardly possible to completely emulate a human tutor or a communication partner, the purpose of this research is to investigate how much linguistic knowledge is necessary to ensure the usability of word completion. Here, we will focus on the grammaticality of word completion. 1.1 Word Completion Word completion facilitates text entry by suggesting a list of words that can follow a given linguistic context. If the desired word is in the list, the user can select that word with a mouse click or a keystroke, thereby saving the effort of typing the remaining letters of the word. Otherwise, the user can continue typing while the software continues to display new lists of words based on that input. For example, consider a user wants to type “That girl by the benches...” After each letter the user manually enters, a system would return a list of potential next words. Say, the next letter the user enters is “w.” A system may offer the following choices: a) was, b) were, c) with, d) where, e) wrapped. By suggesting words in any given context, word completion can assist in the composition of well-formed text. Typical word completion systems suggest by exploiting Markov statistical models (Bentrup, 1987). These systems probabilistically determine the current word in a sentence the previous words as context, based on pre-generated language model derived a corpus. With being of low or- 15 of the NAACL HLT 2010 Workshop on Computational Linguistics and pages Los Angeles, California, June 2010. @2010 Association for Computational Linguistics der (two or three, due to sparse data and computational issues), one consequence is that the applicability of suggested words beyond a certain word distance may become somewhat arbitrary. Further design improvements for word completion depend on the user population and the intended use. For example, the demand on the system to have a sophisticated language model may depend on whether the intent is to primarily reduce the physical or cognitive load of entering text. Evaluation approaches can elucidate on design and implementation issues for providing meaningful word choices. 1.2 Evaluation Approaches A number of studies have been carried out to evaluate the efficacy of word completion systems. (1994) measured which is the reduction in time that the user takes to generate a particular text with the aid of a word completion system compared to the time taken without it. The rationale for this measure is that any word completion system imposes a cognitive load on its users, whereby they now need to 1) change their focus between the target document and the word list display, and possibly between the screen and keyboard; 2) visually scan the word list to decide whether their intended word is present; and 3) select the intended word with the keyboard or mouse. Others have also examined similar visualcognitive issues of using word completion (e.g., Tam and Wells, 2009). The overall approach implicitly defines a user-centred approach to evaluation by having human subjects simulate the actual writing process (usually in a copying, not writing task). Thus, results depend on the abilities and preferences of individual subjects. System-based evaluation measures exist, the common of which is This measures the reduction in the number of keystrokes needed to produce a given text with the aid of a word completion system. Keystroke savings is an important factor for users with physical disabilities who have difficulty working with a keyboard for which it is desirable to keep the number of keystrokes to a minimum. A complemeasure, determines how quickly a given word is predicted by counting the number of characters required to reach completion. Completion keystrokes differs from keystroke savings in that the latter counts the letters remaining in the word. In contrast to the previous two measures, both which measure at the character level, rate measures at the word level by calculating the ratio of the number of words correctly predicted to the total number of words predicted. Given a sufficiently large lexicon, hit rate can be as high as 100% if every letter of every word is manually entered to its completion. As this can be misleading, hit rate is more typically measured with reference to the number of characters already typed in order to assess the system’s demand on the user. These objective measures address motor load independent of cognitive load. With the exception of time savings, these measures can be benchmarked automatically by simulating the writing process by using existing texts. A shortcoming of these objective measures is that they focus on the reduction on the user’s physical demand by simulating the entering of an already written text, and effectively ignore consideration of word choices other than the unique intended word. In reality, the actual writing process depends also on the quality of the entire group of suggested word choices with respect to the intended content. Renaud (2002) addressed this shortcoming by arguing that the syntactic and semantic relations between words can impact on choice-making at the target word. He introduced measures, measuring grammatical consistency and semantic of output, respectively. The former measure calculates the proportion of a system’s suggested words that is syntactically acceptable. The latter focuses on the proportion of relevant output based on lexical and domain semantics. Renaud compared a number of commercial systems and found a positive correlation between the new and existing measures. This finding also lends additional support to Wood’s (1996) finding that offering syntactically and semantically appropriate choices improves performance. (Note that the converse may not hold true.) For the remainder of this paper, we will put our emphasis on the impact of linguistic content (here, grammaticality) on the quality of word completion. The paper is organized as follows. In the next section, we will describe the need to incorporate syntactic information in word completion. In sections 3 and 4, we will describe our human 16 judgement task evaluating the grammaticality of word completion. Based on our analysis, we will return to the evaluation issue in section 5 and discuss how grammaticality alone does not address the larger usability issue of word completion. Here, we propose that the word completion task, unlike traditional NLP tasks, requires both the reader’s and writer’s perspectives, which impacts the interpretation of our evaluation, and in turn impacts design decisions. In section 6, we will conclude by offering a more inclusive perspective on AAC. Demand for Syntactic Filtering As shown earlier, many evaluation methods have focused on 1) the proportion of key-presses normally required during a typing session that the user need not to manually enter and 2) the proportion of text words in a typing session that the system is able correctly to predict. For a user with a learning disability or language difficulties, a greater concern is that all presented words be valid, logical, and free of grammatical errors. Current state-of-the-art systems suffer by suggesting words that are often syntactically implausible while excluding more justifiable but less probable suggestions (cf. our example in section 1). A user may be confused by inappropriate suggestions, even if correct suggestions are also present. To quantify the importance of syntax in word completion, we compare the average hit rate scores (over all words) with the hit rate scores at points in sentences we consider as syntactically critical (see section 3 for their selection). Nantais et al. (2001) reported an overall hit rate of approximately 56% using bigram word completion after entering the first letter of a word across a large document. However, at the word location where it is crucial to maintain correct syntactic relation with the existing sentence fragment, hit rates are often much lower. In our study situation, the hit rate is at best 39%—these syntactic challenges tend to be semantically contentful and thus present difficulties to human subjects. Likewise, the systems are expected to struggle with them. Without a clear understanding of content specific issues during writing, examining time and keystroke savings alone does not reveal the increased difficulty a user faces at these word positions. We will return to these issues in section 5. 2.1 Building Syntactic Knowledge Knowledge of syntax can be obtained by first tagging each dictionary word with its part of speech, such as noun or adjective. This information may then be used in either a probabilistic or a symbolic manner. Systems may reason probabilistically by tag models, where the part-oftags for the previous words in a sentence are used to predict the tag for the current with word models that cue the resulting part(s) of speech to find words proper (Hunnicutt and Carlberger, 2001). Fazly and Hirst (2003) introduced two algorithms for combining tag trigrams with word bigrams. The first algorithm involved conditional independence assumptions between word and tag models, and the second algorithm involved a weighted linear combination of the two models. A fundamental limitation to this approach is that low-order probabilistic language models can only account for relationships between closely colocated words. Symbolic syntactic prediction guided by a grammar, on the other hand, can deal with long-distance word relationships of arbitrary depth by applying rules that govern how words from syntactic categories can be joined, to assign all sentence words to a category. This approach uses knowledge of English grammar to analyze the structure of the sentence in progress and determine the applicable syntactic categories (e.g., noun, verb), along with other features (e.g., singular, past participle), to which the currently typed/predicted word must belong. In this way a word completion system is able to suggest words that are grammatically consistent with the active sentence fragment. As such, research closer in nature to our work involves parsers that process the input sentence incrementally as each word is entered. Wood’s (1996) augmented phrase-structure grammar showed that symbolic syntactic prediction can improve overall performance when combined with statistical orthographic prediction. McCoy (1998) used the augmented transition network or ATN (Woods, 1970) formalism to find candidate word categories from which to generate word lists. Gustavii and Pettersson (2003) used a chart parser to re-rank, or filter, word lists by grammatical value. These parsing algorithms manipusome data structure that represents, and im- 17 poses ordering on, syntactic constituents of sentences. Recently, we have been developing a syntax module (Renaud et al., 2010) based on an ATN-style parser, which can facilitate both increasing the level of correctness in parses through grammar correction, and modifying the information collected during parsing for a particular application (Newman, 2007). Specifically, this filters words provided by completion such that the word list only shows words that fit an acceptable grammatical structure. It operates on a longer list of the same frequencyranked words our core predictor generates. Under this setup, our syntax module can influence the final list shown to the user by demoting implausible words that otherwise would have been displayed and replacing them with plausible words that otherwise would not. Our rationale for using a symbolic vs. a probabilistic parser in word completion is beyond the scope of the current paper. Judgement Experiment To evaluate the impact of syntactic filtering on word completion, we devised a human judgment task where human subjects were asked to judge the grammatical acceptability of a word offered by word completion software, with or without syntactic filtering. Given a partial sentence and a leading prefix for the next word, word completion software presents a number of choices for the potential next word. Although the goal is to assess the grammaticality of predicted words with or without syntactic filtering, the intent is to assess whether the inclusion of syntactic heuristics in the word completion algorithm improves the quality of word choices. 3.1 Experimental Setup In our experiment, we compared three different word completion systems: our baseline complesystem henceforth “baseline”), our word completion system with syntax filtering (“System B”). We also included a third-party commercial word completion system with syntax built-in “System C”). In our baseline system uses a bigram language model trained on a corpus of well-edited text. each system, we inputted a partial sentence plus the leading character for the next word. Each system returned a list of five choices for the potential next word. Our subjects were asked to judge the grammatical acceptability of each word (binary decision: yes or no). It is worth noting that the more letters are manually inserted, the narrower the search space becomes for the next word. Nantais et al. (2001) suggested that after inserting two characters, the hit rate via automatic means can be as high as 72%; the hit rate for humans is likely much higher. Given that our goal is to examine the grammaticality of word choices and not hit rate, providing only one leading letter allows sufficient ambiguity on what the potential next word is, which in turn allows for a range of grammatical choices for our judgement task. 3.2 Sentence Selection We selected our test sentences from Canadian news sources (Toronto Star and the Globe and Mail), which are considered reliably grammatical. chose a total of 138 Each sentence was truncated into a fragment containing the words and the first character of the where from three to ten inclusive. truncation position deliberately selected to include a variety of grammatical challenges. We divided the sentence fragments into nine types of grammatical challenges: 1) subject-verb agreement; 2) subject-verb agreement in questionasking; 3) subject-verb agreement within a relative clause; 4) appositives; 5) verb sequence (auxiliary verb-main verb agreement); 6) case agreement; 7) non-finite clauses; 8) number agreement; and 9) others. For example, the sentence “That girl by the benches was in my high school” from section 1.1 can be used to test the system’s ability to recognize subject-verb agreement if we truncate the sentence to produce the fragment “That girl by the benches w___.” Here, subject-verb agreement should be decided against the subject “girl” and not the (tempting) subject “benches.” did not pick a larger number of sentences due to the time constraint in our experimental setup. The rationale is to avoid over-fatiguing our human subjects (approximately an hour per session). Based on our pilot study, we were able to fit 140 sentences over three one-hour sessions. 18 After the initial selection process, we reduced our collection to 123 partial sentences. Because the sentences were not evenly distributed across the nine categories, we divided the sentences into three sets such that the frequency distribution of the sentence types was the same for all three sets (41 sentences per set). The three word completion were each assigned a different 3.3 Grammaticality Judgements We fed each partial sentence into the corresponding system to produce a word list for grammatical judgement. Recall our example earlier, given five word choices per partial sentence, for each word choice, our subjects were asked to judge its grammatical acceptability (yes or no). We recruited 14 human subjects, all native speakers of English with a university education. Each subject was presented all 123 sentences covering the three systems, in a paper-based task. The sentence order was randomized and the subjects were unaware of which system produced what list. Given that each system produced a list of five options for each partial sentence, each subject judgements for each system. There were 14 sets of such judgements in total. and Analysis Our primary objective is to examine the subjects’ agreement with the system, and whether the subjects generally agree among themselves. Our rationale is this. If the subjects generally agree with one another, then there is an overall agreement on the perception of grammaticality in word completion. If this is indeed the case, we then need to examine how and why our subjects agree or disagree with the systems. Otherwise, if there is low inter-subject agreement, aside from issues related to the experimental setup, we need to reconsider whether offering grammatical word completion choices is indeed practical and possible. We first calculated individual participant agreement with the output of each system (i.e., initially to used three different sets, i.e., one set per system, to avoid a sampling “fluke” of different grammatical difficulties/categories. However, for exactly the same reason, we also tested our system using the two sets for the other two systems for ease of comparison. See section 4.1 for details. averaged over all participants). The baseline scored 68%. System B scored 72% and System C scored 74%. Thus, an early important result was that syntax assistance in general, independent of particular approach or algorithm, does appear to improve subject agreement in a word completion task. (Note that we treat system C as a black box as we are not privy to its algorithms, which are not published.) Overall, the grammaticality of a given test word (i.e., averaged over all test words) had an average agreement of 85%, or by 12 of the 14 participants. The percentage agreement for each system was 84% for the baseline, 87% for system B, and 86% for system C. If at least two-thirds of the participants (10 of 14) agreed on the grammaticality of a particular test word, we considered the collective opinion to be consistent for that word and declared a consensus. Participants reached consensus on 77% of the test words for the baseline, 82% of the test words for system B, and 80% of the test words for system C. Next, we calculated consensus participant agreement for each system. This measure was different from the previous in that we considered only those cases where 10 or more of the 14 participants agreed with one another on the grammaticality of a system’s test word and discarded all other cases. In 75% of the consensus cases for the baseline, the subjects agreed with the system (by approving on the grammaticality); in the other 25% of the consensus cases the subjects disagreed with the system. System B scored 78% on the consensus agreement and system C scored 81%. A repeated-measures analysis of variance (ANOVA) was performed on the data. For both individual and consensus participant agreement, each of Systems B and C outperformed the basesystem (statistically significant, while the difference between the two systems with syntax awareness was not statistically significant. To summarize our findings, our subjects generally found the output grammatically more acceptable if syntactic assistance was built in (72% and 74% over 68% in raw participant agreement; 78% and 81% over 75% in consensus agreement). The behaviour of our System B generally was in line with the behaviour of the third-party System C. Finally, the agreement among subjects for all systems was quite high (~85%) and is considered reliable. 19 4.1 Subject Agreement with Other Systems To further understand the behaviour of our own system (in contrast to our subjects’ judgements), we create two new systems, A&apos; and C&apos; based on the output of the baseline system and the thirdparty System C. Recall that the sentence set used in each system is mutually exclusive from the set used in another system. Therefore, this setup inan additional set of 41 sentences prewords systems = 410 judgements. Our setup is simple: we feed into our parser each of the sentence fragments for the corresponding system, along with each predicted word originally produced. If our parser accepts the word, the analysis remains unchanged. Otherwise, we count it as a “negative” result, which we explain below. Consider again our earlier example, “The girl by the benches w___.” Say system C&apos; produces the following options: a) was, b) were, c) with, d) where, e) wrapped. We then attempt to generate a partial parse using the partial sentence with each predicted word, i.e., “The girl by the benches was,” “The girl by the benches were,” and so on. If, for instance, our parser could not generate a parse for “The girl by the benches where,” then we would treat the word choice “where” as not approved for the purpose of recalculating subject agreement. So if any subjects had approved its grammaticality (i.e., considered it a grammatical next word), then we counted it as a disagreement (between the parser and the human judge), otherwise, we considered it an agreement. Consider the following example. One partial sentence for System C was “Japanese farmers immediately pick the shoots which a[m]...” Only 1 of 14 judges agreed with it. System C&apos; also flagged “am” as ungrammatical. Now 13 judges agreed with it. On the other hand, consider this partial sentence originally from the baseline system, “The reason we are doing these i[nclude]...” where 10 judges said yes but our parser could not generate a parse. In this case, A&apos; scores 4 on agreement. Overall, A&apos; overrode 10 decisions and scored 71% agreement as a result. That is a 3% improvement over the baseline 68% score. Nine of the 10 reversed consensus in a positive direction and 1 (example above) reversed consensus in a negative direction. In comparison, C&apos; overrode 6 decisions, and scored 76% (2.0% improvement over the original 74%). Five of 6 cases reversed consensus, all in a positive direction. (The other case reversed a non-consensus in a positive direction.) Given that the theoretical maximum agreements for the two systems are 84% and 86% (i.e., regardless of polarity), there is considerable increase in the subject agreement. It is worth noting that many subjects made the number agreement mistake due to proximity. In the previous example, “The reason we are doing these i[nclude]...”, the subjects made the incorrect agreement linking “include” to “these” instead of linking to “the reason.” While these cases are not prevalent, this is one reason (among many) that the theoretical maximum agreement is not 100%. 4.2 System’s vs. Subjects’ Perspective Although the agreement between the systems and the subjects were high, no system achieved perfect agreement—many words were considered ungrammatical extensions of the partial sentences. We see two possible explanations: 1) the disagreeable output was erroneous; or 2) the disagreeable output was grammatical but judged as ungrammatical under certain conditions. We manually examined the parse trees of the “disagreeable” cases from our system. Interestingly, in most cases, we found there exists a reasonable parse tree leading to a grammatical sentence. We thus conclude that grammaticality judgements of partial sentences might not completely reflect the underlying improvement of the word completion quality. That is, discrepancies between human and computer judgement need not point to a poor quality syntax filter; instead, it may indicate that the system is exhibiting correct behaviour but simply disagrees with subjects on the particular grammatical cases in question. In such cases, subjects’ disagreement with the system does not provide sufficient grounds for making modifications to the system’s behaviour. Rather, it is worth examining the factors leading to the subjects’ perception of a word as an ungrammatical extension of a partial sentence. Overall, our results indicate that our subjects agree with the grammaticality of word completion more when syntactic filtering is used than not. 20 That said, in light of the disagreeable cases, we believe that the quality of word completion may not be so straightforwardly evaluated. ception of grammaticality of the next word (cf. hit rate issues in section 2). 5.2 Evaluation Approach 5.1 Selectional Restriction Take this example, “The plane carrying the soldiers a___.” The next word “are” was unanimously considered ungrammatical by our human judges. Consider the following full sentence version of it: “The plane carrying the soldiers are contemplating is too difficult a task.” In this case, the subject is “the plane carrying” (as an activity), the relative clause is “the soldiers are contemplating”, and finally, the verb phrase is “is too difficult a task.” This sentence may be difficult to interpret but a meaningful interpretation is possible syntactically and semantically. Consider the following variation, “The political situation the soldiers a___.” In this case, it is not difficult to conceive that “are” is a possible next word, as in “The political situation the soldiers are discussing is getting worse.” The syntactic construction is [noun phrase] [relative clause] [verb phrase]. Both partial sentences have a potential grammatical parse. Why then is one considered grammatical and the other not? Sentences that induce midpoint reading difficulties in humans are well known in psycholinguistics and are referred to as garden-path sentences (Frazier, 1978). Reading “the plane carrying the soldiers” induces an expectation in the reader’s mind that the sentence is about the plane doing the carrying, and not about the carrying of the plane by the soldiers, leading to a “short circuit” at the word “are.” In linguistics and CL, one aspect of this phenomenon, selectional restriction, has been explored previously (most notably Levin, 1993 and Resnik, 1995). Selectional restriction is defined as the semantics of a verb restricting the type of words and phrases that can occur as its arguments. Essentially, the meaning of the verb makes an impact on what is possible syntactically and semantically. What we observe here is a generalized case where it is no longer only about a verb placing syntactic and semantic restrictions on its surrounding words. Instead, we observe how a word or a number of words influencing the semantic and in turn impacting on the per- Although our original intent was to study the grammaticality of word completion, ultimately the question is what impacts on the quality of word completion. It is without a doubt that the grammaticality of the next word suggestions impacts on the perception of the quality of word completion. However, we believe the key hinges on whose perspective of quality is considered, which then becomes a usability issue. Recall that word completion is designed to aid the writing process. The curious part of our evaluation was that we devised it as a grammaticality judgement task via reading. Is grammaticality different when one is reading vs. writing? We consider this issue in two ways. Partial Sentences vs. Full Sentences Let us revisit our garden-path example: 1a. The plane carrying the soldiers a[re]... 1b. The plane carrying the soldiers are contemplating is not that difficult a task. 2a. The political situation the soldiers a[re]... 2b. The political situation the soldiers are losing sleep over is getting worse. In sentences 1a and 2a, readers have no choice but to judge the grammaticality of “are” based on the existing partial sentence. Depending on the reader’s creativity, one may or may not anticipate potential full sentences such as 1b and 2b. In contrast, consider an alternative experimental setup where the readers were offered full sentences such as 1b and 2b and were asked to judge the grammaticality of “are.” Given the complexity of the sentences (selectional restriction aside), the readers would have no choice but to consider the existence of a relative clause, which should increase the likelihood of evaluating “are” as a grammatical component of the sentence. Reading vs. Writing Now we have observed the potential impact on grammaticality judgements of a potential next word when reading a partial sentence vs. a full sentence. That said, it needs emphasizing that the 21 key issue is to evaluate the quality of a suggested next word given a partial sentence, not grammaticality in complete isolation. When a user uses word completion, he/she is actively engaged in the writing process. No software can truly predict the intent of the writer; the full sentence is waiting to be written and cannot be written a priori. Consider someone who is in the process of writing the sentence “The plane carrying the soldiersÉ” Is this writer likely to be debating in his/her head whether the sentence is about the plane that does the carrying or “plane carrying” as an activity? Clearly, the writer’s intent is clear to the writer him/herself. In contrast, a sentence may be perfectly grammatical and semantically reasonable, yet a reader may still find it ambiguous and/or difficult to read. In other words, the perception of grammaticality of a next word depends the task (reading vs. writing). This is say that our evaluation task is compromised as a result. Despite that the general grammar rules do not change, our reading judgements depending on the context (e.g., partial vs. full sentence) suggests that the reading perspective only provide a partial picture on the quality of output that is intended for a writing task. In our case, higher quality syntactic filtering (e.g., our parser here) may not lead to greater usability. Remarks In this paper, we have shown that the quality of word completions depends on the perspective one takes. Considering that AAC is to aid someone in producing content for communication, i.e., for third-party consumption, the reading-writing dichotomy is too serious an issue to ignore. This issue has received some CL attention (Morris, 2004, 2010; Hirst, 2008, 2009) but has not been discussed in the AAC literature (Tsang et al., 2010). The question remains, how do we then evaluate, and more generally, design and use an AAC application? We believe the issue is far from clear. Take our current focus—grammaticality of word completion. If the form of the content produced is ungrammatical or difficult to read from the perspective of a reader, you risk having the reader misunderstand the writer’s intent. However, from the writer’s perspective, unless he/she is perceptive of the interpretation problems with his/her potential readers, there is no incentive to produce content as such; the writer can only produce content based on his/her previous linguistic experience. One may argue that corpus statistics may best capture human linguistic behaviour. For example, hit rate statistics using existing corpora is one such way of assessing the quality of word completion. However, corpora tell only one half of the story—only the writing half is captured, the interpretation issues from the reading side are rarely captured, if at all. More important, the design of word completion is setup in a way that the task consists of both a reading component and a writing one—the appropriateness of suggested words is assessed by the writer via reading during the writing task. In fact, this is not merely a case of reading vs. writing, but rather, an issue of relevance depending on the linguistic context as well as the user’s perception of it. Traditionally, researchers in CL and psycholinguistics have attempted to deal with human processing of linguistic content at various levels (cf. the CUNY Conference on Human Sentence Processing, e.g., Merlo and Stevenson, 2002). However, no computational means is truly privy to the content behind the linguistic form. Content, ultimately, resides in the reader’s or the writer’s head, i.e., intent. The question remains how best to design AAC to aid someone to communicate this content. In summary, in our grammaticality judgement task, incorporating syntax in word completion improves the perceived quality of word choices. That said, it is unclear how quality relates to usability. Indeed, the evaluation is far from conclusive in that it only captures the reader’s perspective and not the writer’s. Currently, we are not aware of the existence of a purely writer-based evaluation for grammaticality of word completion (see Lesher et al., 2002 for one curious attempt). More generally, the reader-writer (or speakerlistener) dichotomy is unexplored in AAC research and should be considered more seriously because communication (as text, speech, or otherwise) involves multiple people producing and consuming content, where the perception of content differs considerably. The challenge of AAC may lie in bridging the gap between production and consumption where communication is neither only about communicating intent nor making interpretations. 22 Acknowledgments This project is funded by Quillsoft Ltd. We also wish to thank Jiafei Niu (University of Toronto) for conducting our usability study and Frank Rudzicz (University of Toronto) for providing helpful comments.</abstract>
<note confidence="0.910996615384615">References John Bentrup. 1987. Exploiting Word Frequencies and Sequential Dependencies. of Annual 121–122. Afsaneh Fazly and Graeme Hirst. 2003. Testing the Efficacy of Part-of-Speech Information in Word of the 2003 EACL Workshop on Language Modeling for Text Entry Meth- 9–16. Frazier. 1978. Comprehending Sentences: Syn- Parsing Ph.D. Thesis, University of Connecticut. Gustavii and Eva Pettersson. 2003. Swedish</note>
<author confidence="0.569715">Thesis</author>
<affiliation confidence="0.928269">Department of Linguistics, Uppsala University.</affiliation>
<address confidence="0.607994">Graeme Hirst. 2008. The Future of Text-Meaning in</address>
<note confidence="0.865897461538461">Linguistics. In of the 11th International Conference on Text, Speech and 1–9. Graeme Hirst. 2009. Limitations of the Philosophy of Language Understanding Implicit in Computational In of the Seventh European on Computing and 108– 109. Sheri Hunnicutt and Johan Carlberger. 2001. Improving Word Prediction Using Markov Models and Methods. and Alternative 17(4):255–264. Koester. 1994. Performance with Augmen-</note>
<title confidence="0.2802145">tative Communication Systems: Measurements and Ph.D. thesis, University of Michigan.</title>
<author confidence="0.840111">Gregory W Lesher</author>
<author confidence="0.840111">Bryan J Moulton</author>
<author confidence="0.840111">D Jeffery</author>
<note confidence="0.861409142857143">Higginbotham, and Brenna Alsofrom. 2002. Limits Human Word Prediction Performance. In Proceedings of 2002 CSUN Conference. Levin. 1993. Verb Classes and Alterna- A Preliminary Investigation. of Chicago Press. F. McCoy. 1998. Intelligent Word Pre-</note>
<affiliation confidence="0.660483">University of Delaware.</affiliation>
<web confidence="0.976993">http://ww.asel.udel.edu/nli/nlp/wpredict.html</web>
<note confidence="0.765208619047619">Merlo and Suzanne Stevenson, Eds. 2002. Lexical Basis of Sentence Processing: Formal, and Experimental John Benjamins Publishing Company. Jane Morris. 2004. Readers’ Interpretations of Lexical in Text. of the Canadian Asfor Information Winnipeg, Manitoba. Jane Morris. 2010. Individual Differences in the Interpretation of Text: Implications for Information Sciof the American Society for Informa- Science and 61(1):141–149. Tom Nantais, Fraser Shein, and Mattias Johansson. 2001. Efficacy of the word prediction algorithm in In of the 2001 RESNA Annual 77–79. Paula S. Newman. 2007. RH: A Retro Hybrid Parser. of the 2007 NAACL Companion, 121–124. Renaud. 2002. Evaluation Measures for Improving Performance of Word Prediction Sys-</note>
<affiliation confidence="0.8122385">Master’s Thesis, School of Computer Science, University of Waterloo.</affiliation>
<note confidence="0.745587857142857">Alfred Renaud, Fraser Shein, and Vivian Tsang. 2010. A Symbolic Approach to Parsing in the Context of Word Completion. In Preparation. Philip Resnik. 1995. Selectional Constraints: An Information-Theoretic Model and its Computational 61:127–125. Cynthia Tam and David Wells. 2009. Evaluating the Benefits of Displaying Word Prediction Lists on a Personal Digital Assistant at the Keyboard Level. 21:105–114. Vivian Tsang and Kelvin Leung. 2010. An Ecological Perspective of Communication With or Without AAC Use. In Preparation. Wood. 1996. Pre-Processing in Prediction for Disabled Ph.D. Thesis, Department of Computer Science, University of Bristol. William Woods. 1970. Transition Network Grammars Natural Language Analysis. of 13(10):591–606. 23</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Bentrup</author>
</authors>
<title>Exploiting Word Frequencies and their Sequential Dependencies.</title>
<date>1987</date>
<booktitle>Proceedings of RESNA 10th Annual Conference,</booktitle>
<pages>121--122</pages>
<contexts>
<context position="3107" citStr="Bentrup, 1987" startWordPosition="497" endWordPosition="498">nue typing while the software continues to display new lists of words based on that input. For example, consider a user wants to type “That girl by the benches...” After each letter the user manually enters, a system would return a list of potential next words. Say, the next letter the user enters is “w.” A system may offer the following choices: a) was, b) were, c) with, d) where, e) wrapped. By suggesting words in any given context, word completion can assist in the composition of well-formed text. Typical word completion systems suggest words by exploiting n-gram Markov statistical models (Bentrup, 1987). These systems probabilistically determine the current word in a sentence given the previous n–1 words as context, based on a pre-generated n-gram language model derived from a corpus. With n typically being of low or15 Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics and Writing, pages 15–23, Los Angeles, California, June 2010. @2010 Association for Computational Linguistics der (two or three, due to sparse data and computational issues), one consequence is that the applicability of suggested words beyond a certain word distance may become somewhat arbitrary. Further d</context>
</contexts>
<marker>Bentrup, 1987</marker>
<rawString>John Bentrup. 1987. Exploiting Word Frequencies and their Sequential Dependencies. Proceedings of RESNA 10th Annual Conference, 121–122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Afsaneh Fazly</author>
<author>Graeme Hirst</author>
</authors>
<title>Testing the Efficacy of Part-of-Speech Information in Word Completion.</title>
<date>2003</date>
<booktitle>Proceedings of the 2003 EACL Workshop on Language Modeling for Text Entry Methods,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="11105" citStr="Fazly and Hirst (2003)" startWordPosition="1797" endWordPosition="1800">ons. We will return to these issues in section 5. 2.1 Building Syntactic Knowledge Knowledge of syntax can be obtained by first tagging each dictionary word with its part of speech, such as noun or adjective. This information may then be used in either a probabilistic or a symbolic manner. Systems may reason probabilistically by combining tag n-gram models, where the part-ofspeech tags for the previous n–1 words in a sentence are used to predict the tag for the current word, with word n-gram models that cue the resulting part(s) of speech to find words proper (Hunnicutt and Carlberger, 2001). Fazly and Hirst (2003) introduced two algorithms for combining tag trigrams with word bigrams. The first algorithm involved conditional independence assumptions between word and tag models, and the second algorithm involved a weighted linear combination of the two models. A fundamental limitation to this approach is that low-order probabilistic language models can only account for relationships between closely colocated words. Symbolic syntactic prediction guided by a grammar, on the other hand, can deal with long-distance word relationships of arbitrary depth by applying rules that govern how words from syntactic </context>
</contexts>
<marker>Fazly, Hirst, 2003</marker>
<rawString>Afsaneh Fazly and Graeme Hirst. 2003. Testing the Efficacy of Part-of-Speech Information in Word Completion. Proceedings of the 2003 EACL Workshop on Language Modeling for Text Entry Methods, 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lyn Frazier</author>
</authors>
<title>On Comprehending Sentences: Syntactic Parsing Strategies.</title>
<date>1978</date>
<tech>Ph.D. Thesis,</tech>
<institution>University of Connecticut.</institution>
<contexts>
<context position="27830" citStr="Frazier, 1978" startWordPosition="4488" endWordPosition="4489">ally and semantically. Consider the following variation, “The political situation the soldiers a___.” In this case, it is not difficult to conceive that “are” is a possible next word, as in “The political situation the soldiers are discussing is getting worse.” The syntactic construction is [noun phrase] [relative clause] [verb phrase]. Both partial sentences have a potential grammatical parse. Why then is one considered grammatical and the other not? Sentences that induce midpoint reading difficulties in humans are well known in psycholinguistics and are referred to as garden-path sentences (Frazier, 1978). Reading “the plane carrying the soldiers” induces an expectation in the reader’s mind that the sentence is about the plane doing the carrying, and not about the carrying of the plane by the soldiers, leading to a “short circuit” at the word “are.” In linguistics and CL, one aspect of this phenomenon, selectional restriction, has been explored previously (most notably Levin, 1993 and Resnik, 1995). Selectional restriction is defined as the semantics of a verb restricting the type of words and phrases that can occur as its arguments. Essentially, the meaning of the verb makes an impact on what</context>
</contexts>
<marker>Frazier, 1978</marker>
<rawString>Lyn Frazier. 1978. On Comprehending Sentences: Syntactic Parsing Strategies. Ph.D. Thesis, University of Connecticut.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ebba Gustavii</author>
<author>Eva Pettersson</author>
</authors>
<title>A Swedish Grammar for Word Prediction.</title>
<date>2003</date>
<tech>Master’s Thesis,</tech>
<institution>Department of Linguistics, Uppsala University.</institution>
<contexts>
<context position="12690" citStr="Gustavii and Pettersson (2003)" startWordPosition="2031" endWordPosition="2034">ng. In this way a word completion system is able to suggest words that are grammatically consistent with the active sentence fragment. As such, research closer in nature to our work involves parsers that process the input sentence incrementally as each word is entered. Wood’s (1996) augmented phrase-structure grammar showed that symbolic syntactic prediction can improve overall performance when combined with statistical orthographic prediction. McCoy (1998) used the augmented transition network or ATN (Woods, 1970) formalism to find candidate word categories from which to generate word lists. Gustavii and Pettersson (2003) used a chart parser to re-rank, or filter, word lists by grammatical value. These parsing algorithms manipulate some data structure that represents, and im17 poses ordering on, syntactic constituents of sentences. Recently, we have been developing a syntax module (Renaud et al., 2010) based on an ATN-style parser, which can facilitate both increasing the level of correctness in parses through grammar correction, and modifying the information collected during parsing for a particular application (Newman, 2007). Specifically, this system filters words provided by n-gram completion such that the</context>
</contexts>
<marker>Gustavii, Pettersson, 2003</marker>
<rawString>Ebba Gustavii and Eva Pettersson. 2003. A Swedish Grammar for Word Prediction. Master’s Thesis, Department of Linguistics, Uppsala University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
</authors>
<title>The Future of Text-Meaning in Computational Linguistics.</title>
<date>2008</date>
<booktitle>In Proceedings of the 11th International Conference on Text, Speech and Dialogue,</booktitle>
<volume>1</volume>
<contexts>
<context position="32397" citStr="Hirst, 2008" startWordPosition="5255" endWordPosition="5256">sts that the reading perspective only provide a partial picture on the quality of output that is intended for a writing task. In our case, higher quality syntactic filtering (e.g., our parser here) may not lead to greater usability. 6 Concluding Remarks In this paper, we have shown that the quality of word completions depends on the perspective one takes. Considering that AAC is to aid someone in producing content for communication, i.e., for third-party consumption, the reading-writing dichotomy is too serious an issue to ignore. This issue has received some CL attention (Morris, 2004, 2010; Hirst, 2008, 2009) but has not been discussed in the AAC literature (Tsang et al., 2010). The question remains, how do we then evaluate, and more generally, design and use an AAC application? We believe the issue is far from clear. Take our current focus—grammaticality of word completion. If the form of the content produced is ungrammatical or difficult to read from the perspective of a reader, you risk having the reader misunderstand the writer’s intent. However, from the writer’s perspective, unless he/she is perceptive of the interpretation problems with his/her potential readers, there is no incentiv</context>
</contexts>
<marker>Hirst, 2008</marker>
<rawString>Graeme Hirst. 2008. The Future of Text-Meaning in Computational Linguistics. In Proceedings of the 11th International Conference on Text, Speech and Dialogue, 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
</authors>
<title>Limitations of the Philosophy of Language Understanding Implicit in Computational Linguistics.</title>
<date>2009</date>
<booktitle>In Proceedings of the Seventh European Conference on Computing and Philosophy,</booktitle>
<volume>108</volume>
<pages>109</pages>
<marker>Hirst, 2009</marker>
<rawString>Graeme Hirst. 2009. Limitations of the Philosophy of Language Understanding Implicit in Computational Linguistics. In Proceedings of the Seventh European Conference on Computing and Philosophy, 108– 109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sheri Hunnicutt</author>
<author>Johan Carlberger</author>
</authors>
<title>Improving Word Prediction Using Markov Models and Heuristic Methods. Augmentative and Alternative Communication,</title>
<date>2001</date>
<contexts>
<context position="11081" citStr="Hunnicutt and Carlberger, 2001" startWordPosition="1793" endWordPosition="1796">a user faces at these word positions. We will return to these issues in section 5. 2.1 Building Syntactic Knowledge Knowledge of syntax can be obtained by first tagging each dictionary word with its part of speech, such as noun or adjective. This information may then be used in either a probabilistic or a symbolic manner. Systems may reason probabilistically by combining tag n-gram models, where the part-ofspeech tags for the previous n–1 words in a sentence are used to predict the tag for the current word, with word n-gram models that cue the resulting part(s) of speech to find words proper (Hunnicutt and Carlberger, 2001). Fazly and Hirst (2003) introduced two algorithms for combining tag trigrams with word bigrams. The first algorithm involved conditional independence assumptions between word and tag models, and the second algorithm involved a weighted linear combination of the two models. A fundamental limitation to this approach is that low-order probabilistic language models can only account for relationships between closely colocated words. Symbolic syntactic prediction guided by a grammar, on the other hand, can deal with long-distance word relationships of arbitrary depth by applying rules that govern h</context>
</contexts>
<marker>Hunnicutt, Carlberger, 2001</marker>
<rawString>Sheri Hunnicutt and Johan Carlberger. 2001. Improving Word Prediction Using Markov Models and Heuristic Methods. Augmentative and Alternative Communication, 17(4):255–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heidi Koester</author>
</authors>
<title>User Performance with Augmentative Communication Systems: Measurements and Models.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Michigan.</institution>
<contexts>
<context position="4226" citStr="Koester (1994)" startWordPosition="672" endWordPosition="673">lity of suggested words beyond a certain word distance may become somewhat arbitrary. Further design improvements for word completion depend on the user population and the intended use. For example, the demand on the system to have a sophisticated language model may depend on whether the intent is to primarily reduce the physical or cognitive load of entering text. Evaluation approaches can elucidate on design and implementation issues for providing meaningful word choices. 1.2 Evaluation Approaches A number of studies have been carried out to evaluate the efficacy of word completion systems. Koester (1994) measured time savings, which is the reduction in time that the user takes to generate a particular text with the aid of a word completion system compared to the time taken without it. The rationale for this measure is that any word completion system imposes a cognitive load on its users, whereby they now need to 1) change their focus between the target document and the word list display, and possibly between the screen and keyboard; 2) visually scan the word list to decide whether their intended word is present; and 3) select the intended word with the keyboard or mouse. Others have also exam</context>
</contexts>
<marker>Koester, 1994</marker>
<rawString>Heidi Koester. 1994. User Performance with Augmentative Communication Systems: Measurements and Models. Ph.D. thesis, University of Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory W Lesher</author>
<author>Bryan J Moulton</author>
<author>D Jeffery Higginbotham</author>
<author>Brenna Alsofrom</author>
</authors>
<title>Limits of Human Word Prediction Performance.</title>
<date>2002</date>
<booktitle>In Proceedings of 2002 CSUN Conference.</booktitle>
<contexts>
<context position="34825" citStr="Lesher et al., 2002" startWordPosition="5650" endWordPosition="5653"> ultimately, resides in the reader’s or the writer’s head, i.e., intent. The question remains how best to design AAC to aid someone to communicate this content. In summary, in our grammaticality judgement task, incorporating syntax in word completion improves the perceived quality of word choices. That said, it is unclear how quality relates to usability. Indeed, the evaluation is far from conclusive in that it only captures the reader’s perspective and not the writer’s. Currently, we are not aware of the existence of a purely writer-based evaluation for grammaticality of word completion (see Lesher et al., 2002 for one curious attempt). More generally, the reader-writer (or speakerlistener) dichotomy is unexplored in AAC research and should be considered more seriously because communication (as text, speech, or otherwise) involves multiple people producing and consuming content, where the perception of content differs considerably. The challenge of AAC may lie in bridging the gap between production and consumption where communication is neither only about communicating intent nor making interpretations. 22 Acknowledgments This project is funded by Quillsoft Ltd. We also wish to thank Jiafei Niu (Uni</context>
</contexts>
<marker>Lesher, Moulton, Higginbotham, Alsofrom, 2002</marker>
<rawString>Gregory W. Lesher, Bryan J. Moulton, D. Jeffery Higginbotham, and Brenna Alsofrom. 2002. Limits of Human Word Prediction Performance. In Proceedings of 2002 CSUN Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations: A Preliminary Investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="28213" citStr="Levin, 1993" startWordPosition="4554" endWordPosition="4555">ical parse. Why then is one considered grammatical and the other not? Sentences that induce midpoint reading difficulties in humans are well known in psycholinguistics and are referred to as garden-path sentences (Frazier, 1978). Reading “the plane carrying the soldiers” induces an expectation in the reader’s mind that the sentence is about the plane doing the carrying, and not about the carrying of the plane by the soldiers, leading to a “short circuit” at the word “are.” In linguistics and CL, one aspect of this phenomenon, selectional restriction, has been explored previously (most notably Levin, 1993 and Resnik, 1995). Selectional restriction is defined as the semantics of a verb restricting the type of words and phrases that can occur as its arguments. Essentially, the meaning of the verb makes an impact on what is possible syntactically and semantically. What we observe here is a generalized case where it is no longer only about a verb placing syntactic and semantic restrictions on its surrounding words. Instead, we observe how a word or a number of words influencing the semantic interpretation, and in turn impacting on the perAlthough our original intent was to study the grammaticality</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes and Alternations: A Preliminary Investigation. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen F McCoy</author>
</authors>
<title>The Intelligent Word Prediction Project.</title>
<date>1998</date>
<institution>University of Delaware.</institution>
<note>http://ww.asel.udel.edu/nli/nlp/wpredict.html</note>
<contexts>
<context position="12521" citStr="McCoy (1998)" startWordPosition="2008" endWordPosition="2009">actic categories (e.g., noun, verb), along with other features (e.g., singular, past participle), to which the currently typed/predicted word must belong. In this way a word completion system is able to suggest words that are grammatically consistent with the active sentence fragment. As such, research closer in nature to our work involves parsers that process the input sentence incrementally as each word is entered. Wood’s (1996) augmented phrase-structure grammar showed that symbolic syntactic prediction can improve overall performance when combined with statistical orthographic prediction. McCoy (1998) used the augmented transition network or ATN (Woods, 1970) formalism to find candidate word categories from which to generate word lists. Gustavii and Pettersson (2003) used a chart parser to re-rank, or filter, word lists by grammatical value. These parsing algorithms manipulate some data structure that represents, and im17 poses ordering on, syntactic constituents of sentences. Recently, we have been developing a syntax module (Renaud et al., 2010) based on an ATN-style parser, which can facilitate both increasing the level of correctness in parses through grammar correction, and modifying </context>
</contexts>
<marker>McCoy, 1998</marker>
<rawString>Kathleen F. McCoy. 1998. The Intelligent Word Prediction Project. University of Delaware. http://ww.asel.udel.edu/nli/nlp/wpredict.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Merlo</author>
<author>Suzanne Stevenson</author>
<author>Eds</author>
</authors>
<title>The Lexical Basis of Sentence Processing: Formal, Computational and Experimental Issues.</title>
<date>2002</date>
<publisher>John Benjamins Publishing Company.</publisher>
<marker>Merlo, Stevenson, Eds, 2002</marker>
<rawString>Paola Merlo and Suzanne Stevenson, Eds. 2002. The Lexical Basis of Sentence Processing: Formal, Computational and Experimental Issues. John Benjamins Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
</authors>
<date>2004</date>
<booktitle>Readers’ Interpretations of Lexical Cohesion in Text. Conference of the Canadian Association for Information Science,</booktitle>
<location>Winnipeg, Manitoba.</location>
<contexts>
<context position="32378" citStr="Morris, 2004" startWordPosition="5252" endWordPosition="5253">full sentence) suggests that the reading perspective only provide a partial picture on the quality of output that is intended for a writing task. In our case, higher quality syntactic filtering (e.g., our parser here) may not lead to greater usability. 6 Concluding Remarks In this paper, we have shown that the quality of word completions depends on the perspective one takes. Considering that AAC is to aid someone in producing content for communication, i.e., for third-party consumption, the reading-writing dichotomy is too serious an issue to ignore. This issue has received some CL attention (Morris, 2004, 2010; Hirst, 2008, 2009) but has not been discussed in the AAC literature (Tsang et al., 2010). The question remains, how do we then evaluate, and more generally, design and use an AAC application? We believe the issue is far from clear. Take our current focus—grammaticality of word completion. If the form of the content produced is ungrammatical or difficult to read from the perspective of a reader, you risk having the reader misunderstand the writer’s intent. However, from the writer’s perspective, unless he/she is perceptive of the interpretation problems with his/her potential readers, t</context>
</contexts>
<marker>Morris, 2004</marker>
<rawString>Jane Morris. 2004. Readers’ Interpretations of Lexical Cohesion in Text. Conference of the Canadian Association for Information Science, Winnipeg, Manitoba.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
</authors>
<title>Individual Differences in the Interpretation of Text: Implications for Information Science.</title>
<date>2010</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>61</volume>
<issue>1</issue>
<marker>Morris, 2010</marker>
<rawString>Jane Morris. 2010. Individual Differences in the Interpretation of Text: Implications for Information Science. Journal of the American Society for Information Science and Technology, 61(1):141–149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Nantais</author>
<author>Fraser Shein</author>
<author>Mattias Johansson</author>
</authors>
<title>Efficacy of the word prediction algorithm in WordQ.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 RESNA Annual Conference,</booktitle>
<pages>77--79</pages>
<contexts>
<context position="9767" citStr="Nantais et al. (2001)" startWordPosition="1575" endWordPosition="1578">l presented words be valid, logical, and free of grammatical errors. Current state-of-the-art systems suffer by suggesting words that are often syntactically implausible while excluding more justifiable but less probable suggestions (cf. our example in section 1). A user may be confused by inappropriate suggestions, even if correct suggestions are also present. To quantify the importance of syntax in word completion, we compare the average hit rate scores (over all words) with the hit rate scores at points in sentences we consider as syntactically critical (see section 3 for their selection). Nantais et al. (2001) reported an overall hit rate of approximately 56% using bigram word completion after entering the first letter of a word across a large document. However, at the word location where it is crucial to maintain correct syntactic relation with the existing sentence fragment, hit rates are often much lower. In our study situation, the hit rate is at best 39%—these syntactic challenges tend to be semantically contentful and thus present difficulties to human subjects. Likewise, the systems are expected to struggle with them. Without a clear understanding of content specific issues during writing, e</context>
<context position="15419" citStr="Nantais et al. (2001)" startWordPosition="2454" endWordPosition="2457">uilt-in (Co:Writer®†, “System C”). In * http://www.wordq.com; our baseline system uses a bigram language model trained on a corpus of well-edited text. † http://www.donjohnston.com/products/cowriter/index.html each system, we inputted a partial sentence plus the leading character for the next word. Each system returned a list of five choices for the potential next word. Our subjects were asked to judge the grammatical acceptability of each word (binary decision: yes or no). It is worth noting that the more letters are manually inserted, the narrower the search space becomes for the next word. Nantais et al. (2001) suggested that after inserting two characters, the hit rate via automatic means can be as high as 72%; the hit rate for humans is likely much higher. Given that our goal is to examine the grammaticality of word choices and not hit rate, providing only one leading letter allows sufficient ambiguity on what the potential next word is, which in turn allows for a range of grammatical choices for our judgement task. 3.2 Sentence Selection We selected our test sentences from Canadian news sources (Toronto Star and the Globe and Mail), which are considered reliably grammatical. We chose a total of 1</context>
</contexts>
<marker>Nantais, Shein, Johansson, 2001</marker>
<rawString>Tom Nantais, Fraser Shein, and Mattias Johansson. 2001. Efficacy of the word prediction algorithm in WordQ. In Proceedings of the 2001 RESNA Annual Conference, 77–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paula S Newman</author>
</authors>
<title>RH: A Retro Hybrid Parser.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 NAACL Conference, Companion,</booktitle>
<pages>121--124</pages>
<contexts>
<context position="13205" citStr="Newman, 2007" startWordPosition="2116" endWordPosition="2117">o find candidate word categories from which to generate word lists. Gustavii and Pettersson (2003) used a chart parser to re-rank, or filter, word lists by grammatical value. These parsing algorithms manipulate some data structure that represents, and im17 poses ordering on, syntactic constituents of sentences. Recently, we have been developing a syntax module (Renaud et al., 2010) based on an ATN-style parser, which can facilitate both increasing the level of correctness in parses through grammar correction, and modifying the information collected during parsing for a particular application (Newman, 2007). Specifically, this system filters words provided by n-gram completion such that the word list only shows words that fit an acceptable grammatical structure. It operates on a longer list of the same frequencyranked words our core predictor generates. Under this setup, our syntax module can influence the final list shown to the user by demoting implausible words that otherwise would have been displayed and replacing them with plausible words that otherwise would not. Our rationale for using a symbolic vs. a probabilistic parser in word completion is beyond the scope of the current paper. 3 Gra</context>
</contexts>
<marker>Newman, 2007</marker>
<rawString>Paula S. Newman. 2007. RH: A Retro Hybrid Parser. In Proceedings of the 2007 NAACL Conference, Companion, 121–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfred Renaud</author>
</authors>
<title>Diagnostic Evaluation Measures for Improving Performance of Word Prediction Systems.</title>
<date>2002</date>
<tech>Master’s Thesis,</tech>
<institution>School of Computer Science, University of Waterloo.</institution>
<contexts>
<context position="7040" citStr="Renaud (2002)" startWordPosition="1143" endWordPosition="1144">es address motor load independent of cognitive load. With the exception of time savings, these measures can be benchmarked automatically by simulating the writing process by using existing texts. A shortcoming of these objective measures is that they focus on the reduction on the user’s physical demand by simulating the entering of an already written text, and effectively ignore consideration of word choices other than the unique intended word. In reality, the actual writing process depends also on the quality of the entire group of suggested word choices with respect to the intended content. Renaud (2002) addressed this shortcoming by arguing that the syntactic and semantic relations between words can impact on choice-making at the target word. He introduced two measures, validity and appropriateness, measuring grammatical consistency and semantic relevance of all system output, respectively. The former measure calculates the proportion of a system’s suggested words that is syntactically acceptable. The latter focuses on the proportion of relevant output based on lexical and domain semantics. Renaud compared a number of commercial systems and found a positive correlation between the new and ex</context>
</contexts>
<marker>Renaud, 2002</marker>
<rawString>Alfred Renaud. 2002. Diagnostic Evaluation Measures for Improving Performance of Word Prediction Systems. Master’s Thesis, School of Computer Science, University of Waterloo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfred Renaud</author>
<author>Fraser Shein</author>
<author>Vivian Tsang</author>
</authors>
<title>A Symbolic Approach to Parsing in the Context of Word Completion. In Preparation.</title>
<date>2010</date>
<contexts>
<context position="12976" citStr="Renaud et al., 2010" startWordPosition="2079" endWordPosition="2082">rase-structure grammar showed that symbolic syntactic prediction can improve overall performance when combined with statistical orthographic prediction. McCoy (1998) used the augmented transition network or ATN (Woods, 1970) formalism to find candidate word categories from which to generate word lists. Gustavii and Pettersson (2003) used a chart parser to re-rank, or filter, word lists by grammatical value. These parsing algorithms manipulate some data structure that represents, and im17 poses ordering on, syntactic constituents of sentences. Recently, we have been developing a syntax module (Renaud et al., 2010) based on an ATN-style parser, which can facilitate both increasing the level of correctness in parses through grammar correction, and modifying the information collected during parsing for a particular application (Newman, 2007). Specifically, this system filters words provided by n-gram completion such that the word list only shows words that fit an acceptable grammatical structure. It operates on a longer list of the same frequencyranked words our core predictor generates. Under this setup, our syntax module can influence the final list shown to the user by demoting implausible words that o</context>
</contexts>
<marker>Renaud, Shein, Tsang, 2010</marker>
<rawString>Alfred Renaud, Fraser Shein, and Vivian Tsang. 2010. A Symbolic Approach to Parsing in the Context of Word Completion. In Preparation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selectional Constraints: An Information-Theoretic Model and its Computational Realization.</title>
<date>1995</date>
<journal>Cognition,</journal>
<pages>61--127</pages>
<contexts>
<context position="28231" citStr="Resnik, 1995" startWordPosition="4557" endWordPosition="4558">then is one considered grammatical and the other not? Sentences that induce midpoint reading difficulties in humans are well known in psycholinguistics and are referred to as garden-path sentences (Frazier, 1978). Reading “the plane carrying the soldiers” induces an expectation in the reader’s mind that the sentence is about the plane doing the carrying, and not about the carrying of the plane by the soldiers, leading to a “short circuit” at the word “are.” In linguistics and CL, one aspect of this phenomenon, selectional restriction, has been explored previously (most notably Levin, 1993 and Resnik, 1995). Selectional restriction is defined as the semantics of a verb restricting the type of words and phrases that can occur as its arguments. Essentially, the meaning of the verb makes an impact on what is possible syntactically and semantically. What we observe here is a generalized case where it is no longer only about a verb placing syntactic and semantic restrictions on its surrounding words. Instead, we observe how a word or a number of words influencing the semantic interpretation, and in turn impacting on the perAlthough our original intent was to study the grammaticality of word completio</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Selectional Constraints: An Information-Theoretic Model and its Computational Realization. Cognition, 61:127–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Tam</author>
<author>David Wells</author>
</authors>
<title>Evaluating the Benefits of Displaying Word Prediction Lists on a Personal Digital Assistant at the Keyboard Level. Assistive Technology,</title>
<date>2009</date>
<pages>21--105</pages>
<contexts>
<context position="4914" citStr="Tam and Wells, 2009" startWordPosition="791" endWordPosition="794"> takes to generate a particular text with the aid of a word completion system compared to the time taken without it. The rationale for this measure is that any word completion system imposes a cognitive load on its users, whereby they now need to 1) change their focus between the target document and the word list display, and possibly between the screen and keyboard; 2) visually scan the word list to decide whether their intended word is present; and 3) select the intended word with the keyboard or mouse. Others have also examined similar visualcognitive issues of using word completion (e.g., Tam and Wells, 2009). The overall approach implicitly defines a user-centred approach to evaluation by having human subjects simulate the actual writing process (usually in a copying, not writing task). Thus, results depend on the abilities and preferences of individual subjects. System-based evaluation measures exist, the most common of which is keystroke savings. This measures the reduction in the number of keystrokes needed to produce a given text with the aid of a word completion system. Keystroke savings is an important factor for users with physical disabilities who have difficulty working with a keyboard f</context>
</contexts>
<marker>Tam, Wells, 2009</marker>
<rawString>Cynthia Tam and David Wells. 2009. Evaluating the Benefits of Displaying Word Prediction Lists on a Personal Digital Assistant at the Keyboard Level. Assistive Technology, 21:105–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivian Tsang</author>
<author>Kelvin Leung</author>
</authors>
<title>An Ecological Perspective of Communication With or Without AAC Use. In Preparation.</title>
<date>2010</date>
<marker>Tsang, Leung, 2010</marker>
<rawString>Vivian Tsang and Kelvin Leung. 2010. An Ecological Perspective of Communication With or Without AAC Use. In Preparation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Wood</author>
</authors>
<title>Syntactic Pre-Processing in Single-Word Prediction for Disabled People.</title>
<date>1996</date>
<tech>Ph.D. Thesis,</tech>
<institution>Department of Computer Science, University of Bristol.</institution>
<marker>Wood, 1996</marker>
<rawString>Matthew Wood. 1996. Syntactic Pre-Processing in Single-Word Prediction for Disabled People. Ph.D. Thesis, Department of Computer Science, University of Bristol.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Woods</author>
</authors>
<title>Transition Network Grammars for Natural Language Analysis.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<volume>13</volume>
<issue>10</issue>
<contexts>
<context position="12580" citStr="Woods, 1970" startWordPosition="2017" endWordPosition="2018">res (e.g., singular, past participle), to which the currently typed/predicted word must belong. In this way a word completion system is able to suggest words that are grammatically consistent with the active sentence fragment. As such, research closer in nature to our work involves parsers that process the input sentence incrementally as each word is entered. Wood’s (1996) augmented phrase-structure grammar showed that symbolic syntactic prediction can improve overall performance when combined with statistical orthographic prediction. McCoy (1998) used the augmented transition network or ATN (Woods, 1970) formalism to find candidate word categories from which to generate word lists. Gustavii and Pettersson (2003) used a chart parser to re-rank, or filter, word lists by grammatical value. These parsing algorithms manipulate some data structure that represents, and im17 poses ordering on, syntactic constituents of sentences. Recently, we have been developing a syntax module (Renaud et al., 2010) based on an ATN-style parser, which can facilitate both increasing the level of correctness in parses through grammar correction, and modifying the information collected during parsing for a particular a</context>
</contexts>
<marker>Woods, 1970</marker>
<rawString>William Woods. 1970. Transition Network Grammars for Natural Language Analysis. Communications of the ACM, 13(10):591–606.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>