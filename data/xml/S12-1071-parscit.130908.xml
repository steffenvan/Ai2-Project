<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006582">
<title confidence="0.918532">
BUAP: A First Approximation to Relational Similarity Measuring
</title>
<author confidence="0.903641">
Mireya Tovar, J. Alejandro Reyes,
Azucena Montes
</author>
<affiliation confidence="0.959945">
CENIDET, Department of
Computer Science
Int. Internado Palmira S/N, Col. Palmira
</affiliation>
<address confidence="0.57929">
Cuernavaca, Morelos, M´exico
{mtovar, alexreyes06c, amr}
</address>
<email confidence="0.995659">
@cenidet.edu.mx
</email>
<author confidence="0.7987355">
Darnes Vilari˜no, David Pinto,
Saul Le´on
</author>
<affiliation confidence="0.9003645">
B. Universidad Aut´onoma de Puebla,
Faculty of Computer Science
</affiliation>
<address confidence="0.9838935">
14 Sur y Av. San Claudio, CU
Puebla, Puebla, M´exico
</address>
<email confidence="0.9778735">
{darnes, dpinto}@cs.buap.mx
saul.ls@live.com
</email>
<sectionHeader confidence="0.995585" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999890785714286">
We describe a system proposed for measuring
the degree of relational similarity beetwen a
pair of words at the Task #2 of Semeval 2012.
The approach presented is based on a vec-
torial representation using the following fea-
tures: i) the context surrounding the words
with a windows size = 3, ii) knowledge ex-
tracted from WordNet to discover several se-
mantic relationships, such as meronymy, hy-
ponymy, hypernymy, and part-whole between
pair of words, iii) the description of the pairs
with their POS tag, morphological informa-
tion (gender, person), and iv) the average num-
ber of words separating the two words in text.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999344575757576">
The Task # 2 of Semeval 2012 focuses on measuring
the degree of relational similarity between the ref-
erence words pairs (training) and the test pairs for a
given class (Jurgens et al., 2012).
The training data set consists of 10 classes and
the testing data set consists of the 69 classes. These
datasets as well as the particularities of the task are
better described at overview paper (Jurgens et al.,
2012). In this paper we report the approach submit-
ted to the competition, which is based on a vector
space model representation for each pair (Salton et
al., 1975). With respect to the type of features used,
we have observed that Fabio Celli (Celli, 2010) con-
siders that contextual information is useful, as well
the lexical and semantic information are in the ex-
traction of semantic relationships task. Additionally,
in (Chen et al., 2010) and (Negri and Kouylekov,
2010) are proposed WordNet based features with the
same purpose.
In the experiments carried out in this paper, we
use a set of lexical, semantic, WordNet-based and
contextual features which allows to construct the
vectors. Actually, we have tested a subset of the 20
contextual features proposed by Celli (Celli, 2010)
and some of those proposed by Chen (Chen et al.,
2010) and Negri (Negri and Kouylekov, 2010).
The cosine similarity measure is used for deter-
mining the degree of relational similarity (Frakes
and Baeza-Yates, 1992) among the vectors.
The rest of this paper is structured as follows.
Section 2 describes the system employed. Section
3 show the obtained results. Finally, in Section 4 the
final conclusions are given.
</bodyText>
<sectionHeader confidence="0.919576" genericHeader="method">
2 System description
</sectionHeader>
<bodyText confidence="0.99997275">
The approach reported in this paper measures the
relational similarity of a set of word pairs that be-
long to the same semantic relationship. Those word
pairs are represented by means of the vector space
model (Salton et al., 1975). Each value of the vec-
tor represents the average value of the correspond-
ing feature. This average is calculated using 100
samples obtained from Internet by employing the
Google search engine. The search process is car-
ried out assuming that those words co-occurring in
the same context contain some kind of semantic re-
lationship.
Let (w1, w2) be a word pair, then the vectorial
representation of this pair (x-) using semantic, con-
textual, lexical, and WordNet-based features may be
expressed as it can be seen in Eq. (1).
</bodyText>
<page confidence="0.905667">
502
</page>
<note confidence="0.3969955">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 502–505,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<equation confidence="0.990516">
x� = (avg(f1), avg(f2), ..., avg(fn)) (1)
</equation>
<bodyText confidence="0.999601416666667">
where avg(fk) is the average value of the feature fk.
The cardinality of the vector is 42, because we
extracted 4 lexical features, 6 semantic features, 7
WordNet-based features and 25 contextual features
(n = 42). Each word pair is then represented by
a unique vector with values associated to each fea-
ture. In Figure 1, we show the vectorial represen-
tation of the word pair (transportation, bus) using
a unique text sample (s). In this example, the num-
ber and type of features described below is followed,
i.e., the first 4 values are lexical, the following 6 are
semantic and so on.
</bodyText>
<figureCaption confidence="0.9792395">
Figure 1: Example of a feature vector for a word pair and
its corresponding sentence s.
</figureCaption>
<bodyText confidence="0.999932166666667">
The previous example is only illustrative, since
we have gathered 100 sentence per word pair. In
total, we collected a corpus containing 2,054,687 to-
kens, with a average class terms of 26,684 and with
an average class vocabulary of 4,006.
The features extracted are described as follows:
</bodyText>
<subsectionHeader confidence="0.994887">
2.1 Lexical features
</subsectionHeader>
<bodyText confidence="0.999856">
The lexical features describe morphologically and
syntactically the word pair (w1, w2). The lexical
features extracted are the following:
</bodyText>
<listItem confidence="0.99814825">
• Average number of words separating the two
words (w1, w2) in the text.
• The position of w1 with respect to w2 in the
text. If w1 appears before w2 then the feature
value is 1, otherwise, the value is 2.
• The Part of Speech Tag for each word in the
pair (two features). We use the FreeLing PoS-
tagger (Padr´o et al., 2010) for obtaining the
</listItem>
<bodyText confidence="0.84478075">
grammatical category. The possible values are
the following: adjective=1; adverb=2; arti-
cle=3; noun=4; verb=5; pronoun=6; conjunc-
tion=7; preposition=8
</bodyText>
<subsectionHeader confidence="0.99599">
2.2 Semantic features
</subsectionHeader>
<bodyText confidence="0.9986025">
The following four semantic features are boolean
values (true or false) indicating:
</bodyText>
<listItem confidence="0.991328333333333">
• If w1 and w2 are named entities (two features)1.
• If w1 and w2 are entities defined (two fea-
tures)2.
</listItem>
<bodyText confidence="0.855352">
The following two semantic features indicate:
</bodyText>
<listItem confidence="0.874188333333333">
• The type of prepositional phrase in case of
existing for w1 and w2. The feature val-
ues are nominal: about=1; after=2; at=3; be-
</listItem>
<equation confidence="0.993680666666667">
hind=4; between=5; by=6; except=7; from=8;
into=9; near=10; of=11; over=12; through=13;
until=14; under=15; upon=16; without=17;
above=18; among=19; before=20; below=21;
beside=22; but=23; down=24; for=25; in=26;
on=27; since=28; to=29; with=30.
</equation>
<subsectionHeader confidence="0.942258">
2.3 WordNet-based features
</subsectionHeader>
<bodyText confidence="0.98765">
The semantic features are boolean values (true or
false) indicating whether or not w2 is contained in:
</bodyText>
<listItem confidence="0.999971428571429">
• the synonym set of w1
• the antonym set of w1
• the meronymy set of w1
• the hyponymy set of w1
• the hypernymy set of w1
• the part-whole set of w1
• the gloss set of w1
</listItem>
<bodyText confidence="0.9080635">
We used WordNet (Fellbaum, 1998) in order to de-
termine the relationship set for word w1.
</bodyText>
<footnote confidence="0.83392975">
1A named entity is defined by a Proper Noun Phrase, which
was detected using the module NER-Named Entity Recognition
of the FreeLing 2.1 tool.
2A defined sentence is one that begins with a definite article.
</footnote>
<table confidence="0.5610265">
s =“The Toyama Chih Railway is a transporta-
tion company that operates railway, tram, and
bus lines in the eastern part of the prefecture.”
x� = (6, 1, 0, 0, 27, 4, 4, 4, 4, 5, 2, 4, 5, 25, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4,
4, 0, 4, 4, 4, 4)
</table>
<page confidence="0.995606">
503
</page>
<subsectionHeader confidence="0.978437">
2.4 Contextual features
</subsectionHeader>
<bodyText confidence="0.997839">
Contextual features considers values for the words
that occur in the context of w1 and w2 (in a window
size of 3). The description of those features follows.
</bodyText>
<listItem confidence="0.9938715">
• Nominal values indicating the Part of Speech
Tag (adjective=1; adverb=2; article=3; noun=4;
verb=5; pronoun=6; conjunction=7; preposi-
tion=8) for the three words at:
– the left context of w1 (three features).
– the right context of w1 (three features).
– the left context of w2 (three features).
– the right context of w2 (three features).
• A Nominal value indicating number of the fol-
lowing grammatical categories between w1 and
w2: verbs, adjectives and nouns (three fea-
tures).
• Nominal values indicating the frequencies of
the verbs: be, do, have, locate, know, make, use,
become, include, take between w1 and w2 (ten
features).
</listItem>
<subsectionHeader confidence="0.9812">
2.5 Feature selection
</subsectionHeader>
<bodyText confidence="0.999994772727273">
We carried out a feature selection process with the
aim of discarding irrelevant features. In this step,
we apply the attribute selection filter reported in
(Hall, 1999), that evaluates the worth of a subset
of attributes by considering the individual predic-
tive ability of each feature along with the degree of
redundancy between them and an exhaustive search
method.
The following features were obtained as relevant:
the average number of words between w1 and w2;
Named Entity of w1 and w2; phrase defined of w1
and w2; prepositional phrase type w1 and w2; part
of speech tag w1 and w2; part of speech tag of
right context of w1 with a windows size of 3; oc-
currences of verbs between w1 and w2; frequency of
verbs be, do, make, locate, take; synonym, antonym,
meronymy, hyponymy, hypernymy, part-whole and
gloss relationships between w1 and w2.
After applying the aforementioned feature selec-
tion method, we removed 17 features, and the vec-
torial representation of each word pair will be done
with only 25 values (features).
</bodyText>
<subsectionHeader confidence="0.995824">
2.6 Determining the degree of similarity
</subsectionHeader>
<bodyText confidence="0.999932777777778">
We have used the features mentioned before for con-
structing a prototype vector representing a given se-
mantic class. In order to do so, we have employed
the training corpus for gathering samples from Inter-
net and, thereafter, we average the feature values in
order to construct such prototype vector.
For each word pair in the test dataset, we ob-
tained a vector using the same process explained
before. We determined the similarity for each test
feature vector with respect to the prototype of the
given class by using the cosine similarity coefficient
(Frakes and Baeza-Yates, 1992), i.e., measuring the
cosine of the angle between the two vectors.
In this way, we obtain a similarity measure of each
test word pair with respect to its corresponding class.
Finally, we may output a ranking of all the word
pairs at the test dataset by sorting these similarity
values obtained.
</bodyText>
<sectionHeader confidence="0.99101" genericHeader="method">
3 Experimental results
</sectionHeader>
<bodyText confidence="0.9998682">
The approach submitted to the Task #2 of SemEval
2012 obtained very poor results. The Spearman cor-
relation coefficient, which measured the correlation
of the approach with respect to the gold standard, it
is quite low (see Table 1).
</bodyText>
<table confidence="0.99920625">
Team-Algorithm Spearman MaxDiff
UTD-NB 0.23 39.4
UTD-SVM 0.12 34.7
DULUTH-V0 0.05 32.4
DULUTH-V1 0.04 31.5
DULUTH-V2 0.04 31.1
BUAP 0.01 31.7
Random 0.02 31.2
</table>
<tableCaption confidence="0.993755">
Table 1: Spearman and MaxDiff scores obtained at the
Task #2 of Semeval 2012
</tableCaption>
<bodyText confidence="0.999887125">
Actually, it shows that the run submitted does not
correlate with the gold standard. We consider that
this behavior is derived from the nature of the sup-
port corpus used for obtaining the features set. The
number of sentences (100) used for representing the
word pairs was not enough for constructing a real
prototype of both, the semantic class and the word
pairs. A further analysis will confirm this issue.
</bodyText>
<page confidence="0.993026">
504
</page>
<bodyText confidence="0.999813">
Despite this limitation we note that the MaxDiff
score was 31.7% slightly above the baseline (31.2%)
and not far from the best score of the task (39.4%).
That is, we achieved an average of 31.7% of ques-
tions answered correctly.
</bodyText>
<sectionHeader confidence="0.989283" genericHeader="discussions">
4 Discussion and conclusion
</sectionHeader>
<bodyText confidence="0.998925666666667">
In this paper we report the set of features used in
the approach submitted for measuring the degrees of
relational similarity between a given reference word
pair and a variety of other pairs. The results obtained
are not encouraging with a Spearman correlation co-
efficient close to zero, which mean that there are
not correlation between the run submitted and the
gold standard. A deeper analysis of the approach is
needed in order to determine if the limitation of the
system falls in the features used, the similarity mea-
sure, or the support corpus used for extracting the
features.
</bodyText>
<sectionHeader confidence="0.997474" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996599666666667">
This project has been partially supported by projects
CONACYT #106625, VIEP #PIAD-ING11-II and
#VIAD-ING11-II.
</bodyText>
<sectionHeader confidence="0.99812" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999868785714286">
Fabio Celli. 2010. Unitn: Part-of-speech counting in
relation extraction. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, SemEval
’10, pages 198–201, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Yuan Chen, Man Lan, Jian Su, Zhi Min Zhou, and
Yu Xu. 2010. Ecnu: Effective semantic relations
classification without complicated features or multi-
ple external corpora. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, SemEval
’10, pages 226–229, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Christiane Fellbaum. 1998. WordNet: an electronic lexi-
cal database. MIT Press.
William B. Frakes and Ricardo A. Baeza-Yates, editors.
1992. Information Retrieval: Data Structures &amp; Algo-
rithms. Prentice-Hall.
Mark A. Hall. 1999. Correlation-based Feature Sub-
set Selection for Machine Learning. Ph.D. thesis, De-
partment of Computer Science, University of Waikato,
Hamilton, New Zealand.
David A. Jurgens, Saif M. Mohammad, Peter D. Turney,
and Keith J. Holyoak. 2012. Semeval-2012 task 2:
Measuring degrees of relational similarity. In Pro-
ceedings of the 6th International Workshop on Seman-
tic Evaluation (SemEval 2012), Montreal, Canada.
Matteo Negri and Milen Kouylekov. 2010. Fbk nk: A
wordnet-based system for multi-way classification of
semantic relations. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, SemEval
’10, pages 202–205, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Lluis Padr´o, Miquel Collado, Samuel Reese, Marina
Lloberes, and Irene Castell´on. 2010. Freeling
2.1: Five years of open-source language processing
tools. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC’10), Valletta, Malta. European Language Re-
sources Association (ELRA).
G. Salton, A. Wong, and C. S. Yang. 1975. A vector
space model for automatic indexing. Commun. ACM,
18(11):613–620, November.
</reference>
<page confidence="0.998377">
505
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.075034">
<title confidence="0.999817">BUAP: A First Approximation to Relational Similarity Measuring</title>
<author confidence="0.99921">Mireya Tovar</author>
<author confidence="0.99921">J Alejandro</author>
<affiliation confidence="0.86557525">Azucena CENIDET, Department Computer Int. Internado Palmira S/N, Col.</affiliation>
<address confidence="0.742173">Cuernavaca, Morelos,</address>
<email confidence="0.7874895">alexreyes06c,@cenidet.edu.mx</email>
<author confidence="0.8908695">David Saul</author>
<affiliation confidence="0.752821333333333">B. Universidad Aut´onoma de Faculty of Computer 14 Sur y Av. San Claudio,</affiliation>
<address confidence="0.925257">Puebla, Puebla,</address>
<email confidence="0.999938">saul.ls@live.com</email>
<abstract confidence="0.997801733333333">We describe a system proposed for measuring the degree of relational similarity beetwen a pair of words at the Task #2 of Semeval 2012. The approach presented is based on a vectorial representation using the following feacontext surrounding the words a windows extracted from WordNet to discover several semantic relationships, such as meronymy, hyponymy, hypernymy, and part-whole between of words, description of the pairs with their POS tag, morphological informa- (gender, person), and average number of words separating the two words in text.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Fabio Celli</author>
</authors>
<title>Unitn: Part-of-speech counting in relation extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval ’10,</booktitle>
<pages>198--201</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1769" citStr="Celli, 2010" startWordPosition="282" endWordPosition="283">ng the degree of relational similarity between the reference words pairs (training) and the test pairs for a given class (Jurgens et al., 2012). The training data set consists of 10 classes and the testing data set consists of the 69 classes. These datasets as well as the particularities of the task are better described at overview paper (Jurgens et al., 2012). In this paper we report the approach submitted to the competition, which is based on a vector space model representation for each pair (Salton et al., 1975). With respect to the type of features used, we have observed that Fabio Celli (Celli, 2010) considers that contextual information is useful, as well the lexical and semantic information are in the extraction of semantic relationships task. Additionally, in (Chen et al., 2010) and (Negri and Kouylekov, 2010) are proposed WordNet based features with the same purpose. In the experiments carried out in this paper, we use a set of lexical, semantic, WordNet-based and contextual features which allows to construct the vectors. Actually, we have tested a subset of the 20 contextual features proposed by Celli (Celli, 2010) and some of those proposed by Chen (Chen et al., 2010) and Negri (Neg</context>
</contexts>
<marker>Celli, 2010</marker>
<rawString>Fabio Celli. 2010. Unitn: Part-of-speech counting in relation extraction. In Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval ’10, pages 198–201, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Chen</author>
<author>Man Lan</author>
<author>Jian Su</author>
<author>Zhi Min Zhou</author>
<author>Yu Xu</author>
</authors>
<title>Ecnu: Effective semantic relations classification without complicated features or multiple external corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval ’10,</booktitle>
<pages>226--229</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1954" citStr="Chen et al., 2010" startWordPosition="309" endWordPosition="312">10 classes and the testing data set consists of the 69 classes. These datasets as well as the particularities of the task are better described at overview paper (Jurgens et al., 2012). In this paper we report the approach submitted to the competition, which is based on a vector space model representation for each pair (Salton et al., 1975). With respect to the type of features used, we have observed that Fabio Celli (Celli, 2010) considers that contextual information is useful, as well the lexical and semantic information are in the extraction of semantic relationships task. Additionally, in (Chen et al., 2010) and (Negri and Kouylekov, 2010) are proposed WordNet based features with the same purpose. In the experiments carried out in this paper, we use a set of lexical, semantic, WordNet-based and contextual features which allows to construct the vectors. Actually, we have tested a subset of the 20 contextual features proposed by Celli (Celli, 2010) and some of those proposed by Chen (Chen et al., 2010) and Negri (Negri and Kouylekov, 2010). The cosine similarity measure is used for determining the degree of relational similarity (Frakes and Baeza-Yates, 1992) among the vectors. The rest of this pap</context>
</contexts>
<marker>Chen, Lan, Su, Zhou, Xu, 2010</marker>
<rawString>Yuan Chen, Man Lan, Jian Su, Zhi Min Zhou, and Yu Xu. 2010. Ecnu: Effective semantic relations classification without complicated features or multiple external corpora. In Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval ’10, pages 226–229, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: an electronic lexical database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6284" citStr="Fellbaum, 1998" startWordPosition="1029" endWordPosition="1030">ominal: about=1; after=2; at=3; behind=4; between=5; by=6; except=7; from=8; into=9; near=10; of=11; over=12; through=13; until=14; under=15; upon=16; without=17; above=18; among=19; before=20; below=21; beside=22; but=23; down=24; for=25; in=26; on=27; since=28; to=29; with=30. 2.3 WordNet-based features The semantic features are boolean values (true or false) indicating whether or not w2 is contained in: • the synonym set of w1 • the antonym set of w1 • the meronymy set of w1 • the hyponymy set of w1 • the hypernymy set of w1 • the part-whole set of w1 • the gloss set of w1 We used WordNet (Fellbaum, 1998) in order to determine the relationship set for word w1. 1A named entity is defined by a Proper Noun Phrase, which was detected using the module NER-Named Entity Recognition of the FreeLing 2.1 tool. 2A defined sentence is one that begins with a definite article. s =“The Toyama Chih Railway is a transportation company that operates railway, tram, and bus lines in the eastern part of the prefecture.” x� = (6, 1, 0, 0, 27, 4, 4, 4, 4, 5, 2, 4, 5, 25, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 0, 4, 4, 4, 4) 503 2.4 Contextual features Contextual features considers value</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: an electronic lexical database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Frakes</author>
<author>A Ricardo</author>
</authors>
<date>1992</date>
<booktitle>Information Retrieval: Data Structures &amp; Algorithms.</booktitle>
<editor>Baeza-Yates, editors.</editor>
<publisher>Prentice-Hall.</publisher>
<marker>Frakes, Ricardo, 1992</marker>
<rawString>William B. Frakes and Ricardo A. Baeza-Yates, editors. 1992. Information Retrieval: Data Structures &amp; Algorithms. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark A Hall</author>
</authors>
<title>Correlation-based Feature Subset Selection for Machine Learning.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, University of Waikato,</institution>
<location>Hamilton, New Zealand.</location>
<contexts>
<context position="7833" citStr="Hall, 1999" startWordPosition="1308" endWordPosition="1309"> right context of w1 (three features). – the left context of w2 (three features). – the right context of w2 (three features). • A Nominal value indicating number of the following grammatical categories between w1 and w2: verbs, adjectives and nouns (three features). • Nominal values indicating the frequencies of the verbs: be, do, have, locate, know, make, use, become, include, take between w1 and w2 (ten features). 2.5 Feature selection We carried out a feature selection process with the aim of discarding irrelevant features. In this step, we apply the attribute selection filter reported in (Hall, 1999), that evaluates the worth of a subset of attributes by considering the individual predictive ability of each feature along with the degree of redundancy between them and an exhaustive search method. The following features were obtained as relevant: the average number of words between w1 and w2; Named Entity of w1 and w2; phrase defined of w1 and w2; prepositional phrase type w1 and w2; part of speech tag w1 and w2; part of speech tag of right context of w1 with a windows size of 3; occurrences of verbs between w1 and w2; frequency of verbs be, do, make, locate, take; synonym, antonym, meronym</context>
</contexts>
<marker>Hall, 1999</marker>
<rawString>Mark A. Hall. 1999. Correlation-based Feature Subset Selection for Machine Learning. Ph.D. thesis, Department of Computer Science, University of Waikato, Hamilton, New Zealand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Jurgens</author>
<author>Saif M Mohammad</author>
<author>Peter D Turney</author>
<author>Keith J Holyoak</author>
</authors>
<title>Semeval-2012 task 2: Measuring degrees of relational similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="1300" citStr="Jurgens et al., 2012" startWordPosition="198" endWordPosition="201">ing the following features: i) the context surrounding the words with a windows size = 3, ii) knowledge extracted from WordNet to discover several semantic relationships, such as meronymy, hyponymy, hypernymy, and part-whole between pair of words, iii) the description of the pairs with their POS tag, morphological information (gender, person), and iv) the average number of words separating the two words in text. 1 Introduction The Task # 2 of Semeval 2012 focuses on measuring the degree of relational similarity between the reference words pairs (training) and the test pairs for a given class (Jurgens et al., 2012). The training data set consists of 10 classes and the testing data set consists of the 69 classes. These datasets as well as the particularities of the task are better described at overview paper (Jurgens et al., 2012). In this paper we report the approach submitted to the competition, which is based on a vector space model representation for each pair (Salton et al., 1975). With respect to the type of features used, we have observed that Fabio Celli (Celli, 2010) considers that contextual information is useful, as well the lexical and semantic information are in the extraction of semantic re</context>
</contexts>
<marker>Jurgens, Mohammad, Turney, Holyoak, 2012</marker>
<rawString>David A. Jurgens, Saif M. Mohammad, Peter D. Turney, and Keith J. Holyoak. 2012. Semeval-2012 task 2: Measuring degrees of relational similarity. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matteo Negri</author>
<author>Milen Kouylekov</author>
</authors>
<title>Fbk nk: A wordnet-based system for multi-way classification of semantic relations.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval ’10,</booktitle>
<pages>202--205</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1986" citStr="Negri and Kouylekov, 2010" startWordPosition="314" endWordPosition="317">ng data set consists of the 69 classes. These datasets as well as the particularities of the task are better described at overview paper (Jurgens et al., 2012). In this paper we report the approach submitted to the competition, which is based on a vector space model representation for each pair (Salton et al., 1975). With respect to the type of features used, we have observed that Fabio Celli (Celli, 2010) considers that contextual information is useful, as well the lexical and semantic information are in the extraction of semantic relationships task. Additionally, in (Chen et al., 2010) and (Negri and Kouylekov, 2010) are proposed WordNet based features with the same purpose. In the experiments carried out in this paper, we use a set of lexical, semantic, WordNet-based and contextual features which allows to construct the vectors. Actually, we have tested a subset of the 20 contextual features proposed by Celli (Celli, 2010) and some of those proposed by Chen (Chen et al., 2010) and Negri (Negri and Kouylekov, 2010). The cosine similarity measure is used for determining the degree of relational similarity (Frakes and Baeza-Yates, 1992) among the vectors. The rest of this paper is structured as follows. Sec</context>
</contexts>
<marker>Negri, Kouylekov, 2010</marker>
<rawString>Matteo Negri and Milen Kouylekov. 2010. Fbk nk: A wordnet-based system for multi-way classification of semantic relations. In Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval ’10, pages 202–205, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lluis Padr´o</author>
<author>Miquel Collado</author>
<author>Samuel Reese</author>
<author>Marina Lloberes</author>
<author>Irene Castell´on</author>
</authors>
<title>Freeling 2.1: Five years of open-source language processing tools.</title>
<date>2010</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10),</booktitle>
<location>Valletta,</location>
<marker>Padr´o, Collado, Reese, Lloberes, Castell´on, 2010</marker>
<rawString>Lluis Padr´o, Miquel Collado, Samuel Reese, Marina Lloberes, and Irene Castell´on. 2010. Freeling 2.1: Five years of open-source language processing tools. In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10), Valletta, Malta. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Wong</author>
<author>C S Yang</author>
</authors>
<title>A vector space model for automatic indexing.</title>
<date>1975</date>
<journal>Commun. ACM,</journal>
<volume>18</volume>
<issue>11</issue>
<contexts>
<context position="1677" citStr="Salton et al., 1975" startWordPosition="264" endWordPosition="267">rds separating the two words in text. 1 Introduction The Task # 2 of Semeval 2012 focuses on measuring the degree of relational similarity between the reference words pairs (training) and the test pairs for a given class (Jurgens et al., 2012). The training data set consists of 10 classes and the testing data set consists of the 69 classes. These datasets as well as the particularities of the task are better described at overview paper (Jurgens et al., 2012). In this paper we report the approach submitted to the competition, which is based on a vector space model representation for each pair (Salton et al., 1975). With respect to the type of features used, we have observed that Fabio Celli (Celli, 2010) considers that contextual information is useful, as well the lexical and semantic information are in the extraction of semantic relationships task. Additionally, in (Chen et al., 2010) and (Negri and Kouylekov, 2010) are proposed WordNet based features with the same purpose. In the experiments carried out in this paper, we use a set of lexical, semantic, WordNet-based and contextual features which allows to construct the vectors. Actually, we have tested a subset of the 20 contextual features proposed </context>
<context position="2967" citStr="Salton et al., 1975" startWordPosition="475" endWordPosition="478">, 2010) and Negri (Negri and Kouylekov, 2010). The cosine similarity measure is used for determining the degree of relational similarity (Frakes and Baeza-Yates, 1992) among the vectors. The rest of this paper is structured as follows. Section 2 describes the system employed. Section 3 show the obtained results. Finally, in Section 4 the final conclusions are given. 2 System description The approach reported in this paper measures the relational similarity of a set of word pairs that belong to the same semantic relationship. Those word pairs are represented by means of the vector space model (Salton et al., 1975). Each value of the vector represents the average value of the corresponding feature. This average is calculated using 100 samples obtained from Internet by employing the Google search engine. The search process is carried out assuming that those words co-occurring in the same context contain some kind of semantic relationship. Let (w1, w2) be a word pair, then the vectorial representation of this pair (x-) using semantic, contextual, lexical, and WordNet-based features may be expressed as it can be seen in Eq. (1). 502 First Joint Conference on Lexical and Computational Semantics (*SEM), page</context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>G. Salton, A. Wong, and C. S. Yang. 1975. A vector space model for automatic indexing. Commun. ACM, 18(11):613–620, November.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>