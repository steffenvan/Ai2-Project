<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000175">
<title confidence="0.9971505">
janardhan: Semantic Textual Similarity using Universal Networking
Language graph matching
</title>
<author confidence="0.968523">
Janardhan Singh
</author>
<affiliation confidence="0.794088">
IIT Bombay,
Mumbai, India
janardhan
</affiliation>
<email confidence="0.988447">
@cse.iitb.ac.in
</email>
<author confidence="0.905003">
Arindam Bhattacharya
</author>
<affiliation confidence="0.733237333333333">
IIT Bombay,
Mumbai, India
arindamb
</affiliation>
<email confidence="0.986275">
@cse.iitb.ac.in
</email>
<author confidence="0.892706">
Pushpak Bhattacharyya
</author>
<affiliation confidence="0.670245">
IIT Bombay,
Mumbai, India
pb
</affiliation>
<email confidence="0.979757">
@cse.iitb.ac.in
</email>
<sectionHeader confidence="0.995138" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995474923076923">
Sentences that are syntactically quite different
can often have similar or same meaning. The
SemEval 2012 task of Semantic Textual Sim-
ilarity aims at finding the semantic similarity
between two sentences. The semantic repre-
sentation of Universal Networking Language
(UNL), represents only the inherent meaning
in a sentence without any syntactic details.
Thus, comparing the UNL graphs of two sen-
tences can give an insight into how semanti-
cally similar the two sentences are. This paper
presents the UNL graph matching method for
the Semantic Textual Similarity(STS) task.
</bodyText>
<sectionHeader confidence="0.998952" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.986212611111111">
Universal Networking language (UNL) gives the
semantic representation of sentences in a graphi-
cal form. By comparing the similarity of these
graphs, we inherently compare only the semantic
content of the two sentences, rather than compar-
ing the similarities in the syntax. Thus, the UNL
graph matching strategy is a natural choice for the
Semantic Textual Similarity(STS) task of SemEval
2012. UNL graphs are also used in textual en-
tailment and interlingua based machine translation
tasks. We use the UNL enconverter system at:
http://www.cfilt.iitb.ac.in
/UNL enco
to generate the UNL graphs of the sentences. For the
two graphs, generated from the two sentences, we
give a similarity score by matching the two graphs.
In the following sections we describe UNL
matching strategy. section 2 describes the UNL sys-
</bodyText>
<figureCaption confidence="0.998764">
Figure 1: UNL graph for “John eats rice”
</figureCaption>
<bodyText confidence="0.9989026">
tem and why this approach is useful, section 3 de-
scribes the matching algorithm, section 4 describes
the challenges faced in this approach, section 5 gives
the results and finally section 6 gives the conclusion
and the future scope.
</bodyText>
<sectionHeader confidence="0.700532" genericHeader="method">
2 Universal Networking Language
</sectionHeader>
<bodyText confidence="0.999404125">
The Universal Networking Language gives a graph-
ical representation of the semantics of a text in the
form of hypergraphs. The representation is at the
semantic level which allows mapping of the simi-
lar meaning sentences having different syntax to the
same representation. To exemplify this point, con-
sider the UNL graphs generated for the following
sentences:
</bodyText>
<subsubsectionHeader confidence="0.6238965">
Sentence 1: John ate rice.
Sentence 2: Rice was eaten by John.
</subsubsectionHeader>
<bodyText confidence="0.994485333333333">
The UNL graph generated from the system are
given in figures 1 and 2 respectively.
The UNL graph consists of three components:
</bodyText>
<page confidence="0.966425">
662
</page>
<note confidence="0.823017">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 662–666,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<figureCaption confidence="0.993814">
Figure 2: UNL graph for “Rice was eaten by John”
</figureCaption>
<listItem confidence="0.999473666666667">
• Universal Words
• Relations
• Attributes
</listItem>
<subsectionHeader confidence="0.98447">
2.1 Universal Words
</subsectionHeader>
<bodyText confidence="0.999974666666667">
The Universal Words (UWs) form the vocabulary of
the Universal Networking Language. They form the
nodes of the UNL graph. The words are normalized
to their basic lemma, for example, eats becomes eat.
The Universal Word is, usually, followed by a dis-
ambiguating constraint list which is mainly used for
disambiguating the sense of the Universal Word. For
example, John (iof &gt; person), here the word John is
disambiguated as an instance of (iof) a person and
rice is disambiguated to be in the class of (icl) proper
noun. The UNL generation system, uses a Universal
word dictionary created using the wordnet.
</bodyText>
<subsectionHeader confidence="0.992709">
2.2 Relations
</subsectionHeader>
<bodyText confidence="0.9999239">
The UNL manual describes 46 binary semantic re-
lations among the Universal Words as given in UNL
manual. These form the labelled arcs of the UNL
graph. In the example of figures 1 and 2, the rela-
tions agent (agt) and object (obj) are shown. John is
the agent of the action eat and rice is the object of
the action eat. The UNL generation system gener-
ated these relations using complex rules based on the
dependency and constituency parser outputs, Word-
net features and Named Entity recognizer output.
</bodyText>
<subsectionHeader confidence="0.996766">
2.3 Attributes
</subsectionHeader>
<bodyText confidence="0.99990392">
Attributes are attached to the Universal Words to
show the speakers perspective for some subjective
information in the text. For the given example, with
respect to the speaker of the text, the action of eat
happened in the past with respect to the speaker.
This is represented by the attribute @past.
The detailed description of the UNL standard can
be found in the UNL manual available online at
http://www.undl.org/unlsys/unl
/unl2005/.
The two sentences listed above, have the same
semantic content, although their syntax is different.
One sentence is in the active voice, while the other
sentence is in the passive. But if we compare the
UNL graphs of the two sentences, they are almost
identical, with an extra attribute @passive on the
main verb eat in the second graph. The graph match-
ing of the two sentences results in a high score near
to 5. Like voice, most of the syntactic variations are
dropped when we move from syntactic to semantic
representation. Thus, comparing the semantic rep-
resentation of the sentences, is useful, to identify
their semantic similarity. The UNL generation sys-
tem generates the attributes using similar features to
those for relation generation.
</bodyText>
<sectionHeader confidence="0.994543" genericHeader="method">
3 UNL matching
</sectionHeader>
<bodyText confidence="0.899192166666667">
The UNL system available online at:
http://www.cfilt.iitb.ac.in
/UNL enco
produces graphs for the sentences by listing the
binary relations present in the graph. An example
of such a listing is :
</bodyText>
<subsubsectionHeader confidence="0.306594">
Sentence 3: A man is eating a banana by a tree.
</subsubsectionHeader>
<bodyText confidence="0.991359705882353">
[unl:1]
agt ( eat(icl&gt;eat&gt;do, agt&gt;thing,
obj&gt;thing):4.@present.@progress
.@entry,
man(icl&gt;male&gt;thing,
equ&gt;adult_male):2.@indef )
ins ( eat(icl&gt;eat&gt;do, agt&gt;thing,
obj&gt;thing):4.@present.@progress
.@entry,
tree(icl&gt;woody_plant&gt;thing)
:9.@indef )
obj ( eat(icl&gt;eat&gt;do, agt&gt;thing,
obj&gt;thing):4.@present.@progress
.@entry,
banana(icl&gt;herb&gt;thing,
equ&gt;banana_tree):6.@indef )
[\unl]
</bodyText>
<page confidence="0.977084">
663
</page>
<bodyText confidence="0.951593235294118">
Sentence 4 : A man is eating a banana.
[unl:1]
agt ( eat(icl&gt;eat&gt;do, agt&gt;thing,
obj&gt;thing):4.@present.@progress
.@entry,
man(icl&gt;male&gt;thing,
equ&gt;adult_male):2.@indef )
obj ( eat(icl&gt;eat&gt;do, agt&gt;thing,
obj&gt;thing):4.@present.@progress
.@entry,
banana(icl&gt;herb&gt;thing,
equ&gt;banana_tree):6.@indef )
[\unl]
We treat the UNL graph of one sentence as goldunl
and the other as testunl. The matching score
between the two is found using the following
formulation (Mohanty, 2008):
</bodyText>
<equation confidence="0.9877025">
score(testunl, goldunl)
= (2*precision*recall) (1)
(precision+recall)
precision
Erelat%onEtestunl relation score(relation) =(2)
(count(relationsEtestunl))
recall
= Erelat%onEtestunl relation score(relation) (3)
(count(relationsEgoldunl))
relation score(relation)
= avg(rel match, uw1score, uw2score) (4)
= avg(word score, attribute score) (6)
attribute score
= F1score(testunl attr, goldunl attr) (8)
</equation>
<bodyText confidence="0.999947742857143">
The matching scheme is based on the idea of the
F1 score. The two UNL graphs are a list of UNL
relations each. Considering, one as the gold UNL
graph and the other as the test UNL graph, we can
find the precision and recall of the total relations that
have matched. For the example given in section 2.4,
the sentence 3 has three relations while sentence 4
has two relations. A correspondence between the
relations agt of the two graphs and also the relation
obj of the two graphs can be established based on
the universal words that they connect. Each such re-
lation match is given a score, explained later, which
is used in the calculation of the precision and recall.
From the precision and recall the F1 score can be
easily calculated which becomes the total matching
score of the two graphs.
The relation score is obtained by averaging the
scores of relation match, and the score of the two
universal word matches. The universal word match
score has a component of the attributes that match
between the corresponding universal words. This
attribute matching is again the F1 score calculation
similar to relation matching. Matching the attributes
of the universal words, contributes to the score of the
matched universal word, which in turn contributes
to the score of the matched relation. Thus, matching
of the semantic relations has more weight than the
matching of the attributes.
The score obtained by this formulation is between
0 and 1. Another score between 0 and 1 is obtained
by flipping the goldunl graph to testunl and testunl
to goldunl. Average of these two scores is then mul-
tiplied by 5 to give the final score.
By this formulation, the score obtained by match-
ing graphs for sentences 3 and 4 is 4.0
</bodyText>
<sectionHeader confidence="0.916782" genericHeader="method">
4 Challenges in the approach
</sectionHeader>
<bodyText confidence="0.9968955">
In the UNL graph matching startegy we faced the
following challenges:
</bodyText>
<subsectionHeader confidence="0.99973">
4.1 Sentences with grammatical errors
</subsectionHeader>
<bodyText confidence="0.99995675">
Many of the sentences, especially, from the MSRpar
dataset, had minor grammatical errors. The UNL
generation requires grammatical correctness. Some
of the examples of such sentences are:
</bodyText>
<listItem confidence="0.8751245">
• The no-shows were Sens. John Kerry of Mas-
sachusetts and Bob Graham of Florida.
</listItem>
<figure confidence="0.664813333333333">
uwscore
rel match
r 1 if relation name matches
=
0 otherwise (5)
word score
r 1 if universal word matches
=
0 otherwise (7)
</figure>
<page confidence="0.983201">
664
</page>
<listItem confidence="0.999200333333333">
• She countersued for $125 million, saying G+J
broke its contract with her by cutting her out
of key editorial decisions and manipulated the
magazine’s financial figures.
• “She was crying and scared,’ said Isa Yasin, the
owner of the store.
</listItem>
<bodyText confidence="0.99986025">
Here, terms like G+J and punctuation errors as in
the third example lead to the generation of improper
UNL graphs. To handle such cases, the UNL gener-
ation needs to get robust.
</bodyText>
<subsectionHeader confidence="0.999669">
4.2 Scoping errors
</subsectionHeader>
<bodyText confidence="0.999996789473684">
UNL graphs are hypergraphs, in which, a node can
in itself be a UNL graph. Scopes are given iden-
tity numbers like :01,:02 and so on. While matching
two different UNL graphs, this matching of scope
identity numbers cannot be directly achieved. Also,
one graph may have different number of scopes as
compared to the other. Hence, eventhough the UNL
graphs are generated correctly, due to scoping mis-
matches the matching score goes down. To tackle
this problem, the UNL graphs generated are con-
verted into scopeless form before the matching is
performed. Every UNL graph has an entry node,
which is the starting node of the graph. This is de-
noed by an @entry attribute on the node. Every
scope, too, has an entry node. The idea for convert-
ing the UNL graphs into scopeless form is to replace
the scope nodes by the graphs that these nodes repre-
sent, with the connection to the original scope node
going to the entry node of the replacing graph.
</bodyText>
<subsectionHeader confidence="0.987315">
4.3 Incomplete or no graph generation
</subsectionHeader>
<bodyText confidence="0.999902">
It was observed that for some of the sentences,
the UNL generation system did not produce UNL
graphs or the generation was incomplete. Some of
these sentences are:
</bodyText>
<listItem confidence="0.645424">
• The Metropolitan Transportation Authority
was given two weeks to restore the $1.50 fare
and the old commuter railroad rates, York de-
clared.
• Long lines formed outside gas stations and peo-
ple rushed to get money from cash machines
Sunday as Israelis prepared to weather a strike
that threatened to paralyze the country.
</listItem>
<bodyText confidence="0.999760333333333">
These, are due to some internal system errors of
the UNL generation system. To improve on this, the
UNL generation system itself has to improve.
</bodyText>
<sectionHeader confidence="0.999925" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.942180869565217">
By adopting the methodology described in section 3,
the following results were obtained on the different
datasets.
MSRpar 0.1936
MSRvid 0.5504
SMT-eur 0.3755
On-WN 0.2888
SMT-news 0.3387
As observed, the performance is good for the
MSRvid dataset. This dataset consists of small and
simple sentences which are grammatically correct.
The performance on this dataset should further im-
prove by capturing the synonyms of the Univer-
sal words while matching the UNL relations. The
performance for MSRpar dataset is low. The sen-
tences in this dataset are long and sometimes with
minor grammatical errors resulting in incomplete or
no UNL graphs. As the UNL generation system
becomes more robust, the performance is expected
to improve quickly. The overall result over all the
datasets is given in the following table.
ALL ALLnrm Mean
0.3431 0.6878 0.3481
</bodyText>
<sectionHeader confidence="0.996409" genericHeader="conclusions">
6 Conclusion and Future Scope
</sectionHeader>
<bodyText confidence="0.999985555555555">
The UNL graph matching approach works well with
grammatically correct sentences. The approach de-
pends on the accuracy of the UNL generation sys-
tem itself. With the increase in the robustness of the
UNL generation system, this approach seems natu-
ral. Since, the approach is unsupervised, it does not
require any training data. The matching algorithm
can be extended to include the synonyms of the Uni-
versal Words while matching relations.
</bodyText>
<sectionHeader confidence="0.999474" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9990982">
Mohanty, R. and Limaye, S. and Prasad, M.K. and Bhat-
tacharyya, P. 2008. Semantic Graph from English
Sentences, Proceedings of ICON-2008: 6th Inter-
national Conference on Natural Language Processing
Macmillan Publishers, India
</reference>
<page confidence="0.980814">
665
</page>
<reference confidence="0.989022857142857">
UNL Center of UNDL Foundation 2005 Uni-
versal Networking Language (UNL) Spec-
ifications Version 2005 Online URL:
http://www.undl.org/unlsys/unl
/unl2005/
UNL enconversion system. 2012. Online URL:
http://www.cfilt.iitb.ac.in/UNL enco
</reference>
<page confidence="0.99835">
666
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.070622">
<title confidence="0.9955255">janardhan: Semantic Textual Similarity using Universal Language graph matching</title>
<author confidence="0.976881">Janardhan</author>
<affiliation confidence="0.987745">IIT</affiliation>
<address confidence="0.69046">Mumbai,</address>
<email confidence="0.969532">@cse.iitb.ac.in</email>
<author confidence="0.302292">Arindam</author>
<affiliation confidence="0.910675">IIT</affiliation>
<address confidence="0.676702">Mumbai,</address>
<email confidence="0.97373">@cse.iitb.ac.in</email>
<author confidence="0.840292">Pushpak</author>
<affiliation confidence="0.975548">IIT</affiliation>
<address confidence="0.695934">Mumbai,</address>
<email confidence="0.990375">@cse.iitb.ac.in</email>
<abstract confidence="0.993310142857143">Sentences that are syntactically quite different can often have similar or same meaning. The SemEval 2012 task of Semantic Textual Similarity aims at finding the semantic similarity between two sentences. The semantic representation of Universal Networking Language (UNL), represents only the inherent meaning in a sentence without any syntactic details. Thus, comparing the UNL graphs of two sentences can give an insight into how semantically similar the two sentences are. This paper presents the UNL graph matching method for the Semantic Textual Similarity(STS) task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Mohanty</author>
<author>S Limaye</author>
<author>M K Prasad</author>
<author>P Bhattacharyya</author>
</authors>
<title>Semantic Graph from English Sentences,</title>
<date>2008</date>
<booktitle>Proceedings of ICON-2008: 6th International Conference on Natural Language Processing</booktitle>
<publisher>Macmillan Publishers,</publisher>
<location>India</location>
<marker>Mohanty, Limaye, Prasad, Bhattacharyya, 2008</marker>
<rawString>Mohanty, R. and Limaye, S. and Prasad, M.K. and Bhattacharyya, P. 2008. Semantic Graph from English Sentences, Proceedings of ICON-2008: 6th International Conference on Natural Language Processing Macmillan Publishers, India</rawString>
</citation>
<citation valid="true">
<title>of UNDL Foundation</title>
<date>2005</date>
<institution>UNL Center</institution>
<note>Online URL: http://www.undl.org/unlsys/unl /unl2005/</note>
<marker>2005</marker>
<rawString>UNL Center of UNDL Foundation 2005 Universal Networking Language (UNL) Specifications Version 2005 Online URL: http://www.undl.org/unlsys/unl /unl2005/</rawString>
</citation>
<citation valid="true">
<authors>
<author>UNL enconversion system</author>
</authors>
<date>2012</date>
<note>Online URL: http://www.cfilt.iitb.ac.in/UNL enco</note>
<marker>system, 2012</marker>
<rawString>UNL enconversion system. 2012. Online URL: http://www.cfilt.iitb.ac.in/UNL enco</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>