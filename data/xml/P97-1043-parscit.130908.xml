<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9987965">
The Complexity of Recognition
of Linguistically Adequate Dependency Grammars
</title>
<author confidence="0.947654">
Peter Neuhaus
Norbert Broker
</author>
<affiliation confidence="0.870475">
Computational Linguistics Research Group
Freiburg University, FriedrichstraBe 50
</affiliation>
<address confidence="0.906957">
D-79098 Freiburg, Germany
</address>
<email confidence="0.6495775">
email: {neuhaus,nobi}@coling.uni-freiburg.de
eF
</email>
<sectionHeader confidence="0.972899" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999500363636364">
Results of computational complexity exist for
a wide range of phrase structure-based gram-
mar formalisms, while there is an apparent
lack of such results for dependency-based for-
malisms. We here adapt a result on the com-
plexity of ID/LP-grammars to the dependency
framework. Contrary to previous studies on
heavily restricted dependency grammars, we
prove that recognition (and thus, parsing) of
linguistically adequate dependency grammars
is,ArP-complete.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973638888889">
The introduction of dependency grammar (DG) into
modern linguistics is marked by Tesniere (1959). His
conception addressed didactic goals and, thus, did not
aim at formal precision, but rather at an intuitive un-
derstanding of semantically motivated dependency re-
lations. An early formalization was given by Gaifman
(1965), who showed the generative capacity of DG to be
(weakly) equivalent to standard context-free grammars.
Given this equivalence, interest in DG as a linguistic
framework diminished considerably, although many de-
pendency grammarians view Gaifman&apos;s conception as an
unfortunate one (cf. Section 2). To our knowledge, there
has been no other formal study of DG.This is reflected
by a recent study (Lombardo &amp; Lesmo, 1996), which
applies the Earley parsing technique (Earley, 1970) to
DG, and thereby achieves cubic time complexity for the
analysis of DG. In their discussion, Lombardo &amp; Lesmo
express their hope that slight increases in generative ca-
pacity will correspond to equally slight increases in com-
putational complexity. It is this claim that we challenge
here.
After motivating non-projective analyses for DG, we
investigate various variants of DG and identify the sep-
aration of dominance and precedence as a major part of
current DG theorizing. Thus, no current variant of DG
(not even Tesniere&apos;s original formulation) is compatible
with Gaifman s conception, which seems to be motivated
by formal considerations only (viz., the proof of equiva-
lence). Section 3 advances our proposal, which cleanly
separates dominance and precedence relations. This is il-
lustrated in the fourth section, where we give a simple en-
coding of an VP-complete problem in a discontinuous
DG. Our proof of ArT&apos;-completeness, however, does not
rely on discontinuity, but only requires unordered trees.
It is adapted from a similar proof for unordered context-
free grammars (UCFGs) by Barton (1985).
</bodyText>
<sectionHeader confidence="0.915571" genericHeader="method">
2 Versions of Dependency Grammar
</sectionHeader>
<bodyText confidence="0.999760111111111">
The growing interest in the dependency concept (which
roughly corresponds to the 0-roles of GB, subcatego-
rization in HPSG, and the so-called domain of locality
of TAG) again raises the issue whether non-lexical cat-
egories are necessary for linguistic analysis. After re-
viewing several proposals in this section, we argue in the
next section that word order — the description of which
is the most prominent difference between PSGs and DGs
— can adequately be described without reference to non-
lexical categories.
Standard PSG trees are projective, i.e., no branches
cross when the terminal nodes are projected onto the
input string. In contrast to PSG approaches, DG re-
quires non-projective analyses. As DGs are restricted
to lexical nodes, one cannot, e.g., describe the so-called
unbounded dependencies without giving up projectiv-
ity. First, the categorial approach employing partial con-
stituents (Huck, 1988; Hepple, 1990) is not available,
since there are no phrasal categories. Second, the coin-
dexing (Haegeman, 1994) or structure-sharing (Pollard
&amp; Sag, 1994) approaches are not available, since there
are no empty categories.
Consider the extracted NP in &amp;quot;Beans, I know John
likes&amp;quot; (cf. also to Fig.1 in Section 3). A projective tree
would require &amp;quot;Beans&amp;quot; to be connected to either &amp;quot;I&amp;quot; or
&amp;quot;know&amp;quot; – none of which is conceptually directly related
to &amp;quot;Beans&amp;quot;. It is &amp;quot;likes&amp;quot; that determines syntactic fea-
</bodyText>
<page confidence="0.997268">
337
</page>
<bodyText confidence="0.999788772727273">
tures of &amp;quot;Beans&amp;quot; and which provides a semantic role for
it. The only connection between &amp;quot;know&amp;quot; and &amp;quot;Beans&amp;quot; is
that the finite verb allows the extraction of &amp;quot;Beans&amp;quot;, thus
defining order restrictions for the NP. This has led some
DG variants to adopt a general graph structure with mul-
tiple heads instead of trees. We will refer to DGs allow-
ing non-projective analyses as discontinuous DGs.
Tesniere (1959) devised a bipartite grammar theory
which consists of a dependency component and a trans-
lation component (&apos;translation&apos; used in a technical sense
denoting a change of category and grammatical func-
tion). The dependency component defines four main cat-
egories and possible dependencies between them. What
is of interest here is that there is no mentioning of order
in Tesniere&apos; s work. Some practitioneers of DG have al-
lowed word order as a marker for translation, but they do
not prohibit non-projective trees.
Gaifman (1965) designed his DG entirely analogous
to context-free phrase structure grammars. Each word
is associated with a category, which functions like the
non-terminals in CFG. He then defines the following rule
format for dependency grammars:
</bodyText>
<equation confidence="0.993254">
(1) X (Yi • • Yi • • • Yn)
</equation>
<bodyText confidence="0.999712">
This rule states that a word of category X governs words
of category Y1,..., Y, which occur in the given order.
The head (the word of category X) must occur between
the i-th and the (i + 1)-th modifier. The rule can be
viewed as an ordered tree of depth one with node labels.
Trees are combined through the identification of the root
of one tree with a leaf of identical category of another
tree. This formalization is restricted to projective trees
with a completely specified order of sister nodes. As we
have argued above, such a formulation cannot capture se-
mantically motivated dependencies.
</bodyText>
<subsectionHeader confidence="0.998104">
2.1 Current Dependency Grammars
</subsectionHeader>
<bodyText confidence="0.999174087719298">
Today&apos;s DGs differ considerably from Gaifman&apos; s con-
ception, and we will very briefly sketch various order de-
scriptions, showing that DGs generally dissociate dom-
inance and precedence by some mechanism. All vari-
ants share, however, the rejection of phrasal nodes (al-
though phrasal features are sometimes allowed) and the
introduction of edge labels (to distinguish different de-
pendency relations).
Meaning-Text Theory (Mel&apos; euk, 1988) assumes seven
strata of representation. The rules mapping from the un-
ordered dependency trees of surface-syntactic represen-
tations onto the annotated lexeme sequences of deep-
morphological representations include global ordering
rules which allow discontinuities. These rules have not
yet been formally specified (Mel&apos; euk &amp; Pertsov, 1987,
p.1870, but see the proposal by Rambow &amp; Joshi (1994).
Word Grammar (Hudson, 1990) is based on general
graphs. The ordering of two linked words is specified to-
gether with their dependency relation, as in the proposi-
tion &amp;quot;object of verb succeeds it&amp;quot;. Extraction is analyzed
by establishing another dependency, visitor, between the
verb and the extractee, which is required to precede the
verb, as in &amp;quot;visitor of verb precedes it&amp;quot;. Resulting incon-
sistencies, e.g. in case of an extracted object, are not
resolved, however.
Lexicase (Starosta, 1988; 1992) employs complex fea-
ture structures to represent lexical and syntactic enti-
ties. Its word order description is much like that of
Word Grammar (at least at some level of abstraction),
and shares the above inconsistency.
Dependency Unification Grammar (Hellwig, 1988)
defines a tree-like data structure for the representation of
syntactic analyses. Using morphosyntactic features with
special interpretations, a word defines abstract positions
into which modifiers are mapped. Partial orderings and
even discontinuities can thus be described by allowing a
modifier to occupy a position defined by some transitive
head. The approach cannot restrict discontinuities prop-
erly, however.
Slot Grammar (McCord, 1990) employs a number of
rule types, some of which are exclusively concerned with
precedence. So-called head/slot and slot/slot ordering
rules describe the precedence in projective trees, refer-
ring to arbitrary predicates over head and modifiers. Ex-
tractions (i.e., discontinuities) are merely handled by a
mechanism built into the parser.
This brief overview of current DG flavors shows that
various mechanisms (global rules, general graphs, proce-
dural means) are generally employed to lift the limitation
to projective trees. Our own approach presented below
improves on these proposals because it allows the lexi-
calized and declarative formulation of precedence con-
straints. The necessity of non-projective analyses in DG
results from examples like &amp;quot;Beans, I know John likes&amp;quot;
and the restriction to lexical nodes which prohibits gap-
threading and other mechanisms tied to phrasal cate-
gories.
</bodyText>
<sectionHeader confidence="0.7800085" genericHeader="method">
3 A Dependency Grammar with Word
Order Domains
</sectionHeader>
<bodyText confidence="0.99970375">
We now sketch a minimal DG that incorporates only
word classes and word order as descriptional dimensions.
The separation of dominance and precedence presented
here grew out of our work on German, and retains the lo-
cal flavor of dependency specification, while at the same
time covering arbitrary discontinuities. It is based on a
(modal) logic with model-theoretic interpretation, which
is presented in more detail in (Broker, 1997).
</bodyText>
<page confidence="0.984287">
338
</page>
<figure confidence="0.995609285714286">
know
-
likes \I
object &apos; subject/
&apos; John ,&apos;
■
d2
</figure>
<figureCaption confidence="0.928868">
Figure 1: Word order domains in &amp;quot;Beans, I know John
likes&amp;quot;
</figureCaption>
<subsectionHeader confidence="0.993526">
3.1 Order Specification
</subsectionHeader>
<bodyText confidence="0.990274837837838">
Our initial observation is that DG cannot use binary
precedence constraints as PSG does. Since DG analyses
are hierarchically flatter, binary precedence constraints
result in inconsistencies, as the analyses of Word Gram-
mar and Lexicase illustrate. In PSG, on the other hand,
the phrasal hierarchy separates the scope of precedence
restrictions. This effect is achieved in our approach by
defining word order domains as sets of words, where
precedence restrictions apply only to words within the
same domain. Each word defines a sequence of order do-
mains, into which the word and its modifiers are placed.
Several restrictions are placed on domains. First,
the domain sequence must mirror the precedence of the
words included, i.e., words in a prior domain must pre-
cede all words in a subsequent domain. Second, the order
domains must be hierarchically ordered by set inclusion,
i.e., be projective. Third, a domain (e.g., di. in Fig.!)
can be constrained to contain at most one partial depen-
dency tree.&apos; We will write singleton domains as &amp;quot;_&amp;quot;,
while other domains are represented by &amp;quot;E&amp;quot;. The prece-
dence of words within domains is described by binary
precedence restrictions, which must be locally satisfied
in the domain with which they are associated. Consid-
ering Fig.1 again, a precedence restriction for &amp;quot;likes&amp;quot; to
precede its object has no effect, since the two are in dif-
ferent domains. The precedence constraints are formu-
lated as a binary relation &amp;quot;-.&lt;&amp;quot; over dependency labels,
including the special symbol &amp;quot;self&amp;quot; denoting the head.
Discontinuities can easily be characterized, since a word
may be contained in any domain of (nearly) any of its
transitive heads. If a domain of its direct head contains
the modifier, a continuous dependency results. If, how-
ever, a modifier is placed in a domain of some transitive
head (as &amp;quot;Beans&amp;quot; in Fig.1), discontinuities occur. Bound-
ing effects on discontinuities are described by specifying
that certain dependencies may not be crossed.2 For the
&apos;For details, cf. (Broker, 1997).
</bodyText>
<footnote confidence="0.4779915">
2German data exist that cannot be captured by the (more
common) bounding of discontinuities by nodes of a certain
</footnote>
<bodyText confidence="0.997020384615385">
purpose of this paper, we need not formally introduce the
bounding condition, though.
A sample domain structure is given in Fig.1, with two
domains d1 and d2 associated with the governing verb
&amp;quot;know&amp;quot; (solid) and one with the embedded verb &amp;quot;likes&amp;quot;
(dashed). d1 may contain only one partial dependency
tree, the extracted phrase. d2 contains the rest of the sen-
tence. Both domains are described by (2), where the do-
main sequence is represented as &amp;quot;&lt;&amp;quot;. d2 contains two
precedence restrictions which require that &amp;quot;know&amp;quot; (rep-
resented by self) must follow the subject (first precedence
constraint) and precede the object (second precedence
constraint).
</bodyText>
<listItem confidence="0.9348695">
(2) { } &lt;&lt; ( (subject -‹ self), (self -‹ object)}
3.2 Formal Description
</listItem>
<bodyText confidence="0.9790228">
The following notation is used in the proof. A lexicon
Lex maps words from an alphabet E to word classes,
which in turn are associated with valencies and domain
sequences. The set C of word classes is hierarchically
ordered by a subclass relation
</bodyText>
<listItem confidence="0.91542225">
(3) isac c CxC
A word w of class c inherits the valencies (and domain
sequence) from c, which are accessed by
(4) w.valencies
</listItem>
<bodyText confidence="0.999014111111111">
A valency (b, d, c) describes a possible dependency re-
lation by specifying a flag b indicating whether the de-
pendency may be discontinuous, the dependency name d
(a symbol), and the word class c E C of the modifier. A
word h may govern a word m in dependency d if h de-
fines a valency (b, d, c) such that (m isac c) and m can
consistently be inserted into a domain of h (for b = —)
or a domain of a transitive head of h (for b = +). This
condition is written as
</bodyText>
<equation confidence="0.714408">
(5) governs(h, d, m)
A DG is thus characterized by
(6) G= (Lex,C,isac, E)
</equation>
<bodyText confidence="0.9996614">
The language L(G) includes any sequence of words
for which a dependency tree can be constructed such that
for each word h governing a word m in dependency d,
governs(h, d, m) holds. The modifier of h in dependency
d is accessed by
</bodyText>
<figure confidence="0.7293855">
(7) h.mod(d)
category.
Beans
d1
</figure>
<page confidence="0.991123">
339
</page>
<sectionHeader confidence="0.971033" genericHeader="method">
4 The complexity of DG Recognition
</sectionHeader>
<bodyText confidence="0.999955">
Lombardo &amp; Lesmo (1996, p.728) convey their hope that
increasing the flexibility of their conception of DG will
&amp;quot; ... imply the restructuring of some parts of the rec-
ognizer, with a plausible increment of the complexity&amp;quot;.
We will show that adding a little (linguistically required)
flexibility might well render recognition ATP-complete.
To prove this, we will encode the vertex cover problem,
which is known to be .IVP-complete, in a DG.
</bodyText>
<subsectionHeader confidence="0.8426605">
4.1 Encoding the Vertex Cover Problem in
Discontinuous DG
</subsectionHeader>
<bodyText confidence="0.97246275">
A vertex cover of a finite graph is a subset of its ver-
tices such that (at least) one end point of every edge is
a member of that set. The vertex cover problem is to
decide whether for a given graph there exists a vertex
cover with at most k elements. The problem is known to
be .NP-complete (Garey &amp; Johnson, 1983, pp53-56).
Fig. 2 gives a simple example where {c, d} is a vertex
cover.
</bodyText>
<figureCaption confidence="0.99467">
Figure 2: Simple graph with vertex cover 1c, dl.
</figureCaption>
<bodyText confidence="0.999759952380952">
A straightforward encoding of a solution in the DG
formalism introduced in Section 3 defines a root word
s of class S with k valencies for words of class 0. 0
has 1V1 subclasses denoting the nodes of the graph. An
edge is represented by two linked words (one for each
end point) with the governing word corresponding to
the node included in the vertex cover. The subordinated
word is assigned the class R, while the governing word
is assigned the subclass of 0 denoting the node it repre-
sents. The latter word classes define a valency for words
of class R (for the other end point) and a possibly discon-
tinuous valency for another word of the identical class
(representing the end point of another edge which is in-
cluded in the vertex cover). This encoding is summarized
in Table 1.
The input string contains an initial s and for each edge
the words representing its end points, e.g. &amp;quot;saccdadb-
dcb&amp;quot; for our example. If the grammar allows the con-
struction of a complete dependency tree (cf. Fig. 3 for
one solution), this encodes a solution of the vertex cover
problem.
</bodyText>
<figure confidence="0.988780333333333">
\I
I, b
s a C da sdbdcb
</figure>
<figureCaption confidence="0.97073">
Figure 3: Encoding a solution to the vertex cover prob-
lem from Fig. 2.
</figureCaption>
<subsectionHeader confidence="0.882608">
4.2 Formal Proof using Continuous DG
</subsectionHeader>
<bodyText confidence="0.9609691875">
The encoding outlined above uses non-projective trees,
i.e., crossing dependencies. In anticipation of counter
arguments such as that the presented dependency gram-
mar was just too powerful, we will present the proof us-
ing only one feature supplied by most DG formalisms,
namely the free order of modifiers with respect to their
head. Thus, modifiers must be inserted into an order do-
main of their head (i.e., no + mark in valencies). This
version of the proof uses a slightly more complicated en-
coding of the vertex cover problem and resembles the
proof by Barton (1985).
Definition 1 (Measure)
Let 11 • 11 be a measure for the encoded input length of a
computational problem. We require that if S is a set or
string and k E N then 181 &gt; k implies 11811 &gt; 11k11 and
that for any tuple 11(• • • x, • • • )11 &gt; 11x11 holds.
</bodyText>
<subsectionHeader confidence="0.41608">
Definition 2 (Vertex Cover Problem)
</subsectionHeader>
<bodyText confidence="0.948546">
A possible instance of the vertex cover problem is a triple
(V, E, k) where (V, E) is a finite graph and 1V1 &gt; k E
N. The vertex cover problem is the set VC of all in-
stances (V, E, k) for which there exists a subset V&apos; C V
and a function f : E —&gt; V&apos; such that IV&apos; 1 &lt; k and
V(vm, vn) E E ((vm vn)) E {Vm, Viz}. &lt;1
Definition 3 (DG recognition problem)
A possible instance of the DG recognition problem is a
tuple (G, cr) where G = (Lex, C, isac , E) is a depen-
dency grammar as defined in Section 3 and a- E E+. The
DG recognition problem DGR consists of all instances
(G,(r) such that a E L(G).
For an algorithm to decide the VC problem consider a
data structure representing the vertices of the graph (e.g.,
a set). We separate the elements of this data structure
</bodyText>
<page confidence="0.981584">
340
</page>
<figure confidence="0.942186307692308">
classes valencies order domain
S {(— , markt 0), (—, mark2, 0)} a-{ (self -&lt; mark1), (mark1 -&lt; mark2)}
A isac 0 {(—, unmrk, R),(+, same, A)} _{(unmrk --&lt; same), (self -&lt; same)}
B isac 0 {(—, unmrk, R),(+, same, B)) a {(unmrk -‹ same), (self -&lt; same)}
C isac 0 {(—, unmrk, R),(+, same, C)} EE{(unmrk --&lt; same), (self -&lt; same)}
D isac 0 {(—, unmrk, R),(+, same, D)} E{ (unmrk -‹ same), (self -‹ same)}
R {} —{ }
word classes
S {S}
a {A, R}
b {B,R}
C {C, R}
d {D, R}
</figure>
<tableCaption confidence="0.992659">
Table 1: Word classes and lexicon for vertex cover problem from Fig. 2
</tableCaption>
<bodyText confidence="0.97280415">
into the (maximal) vertex cover set and its complement
set. Hence, one end point of every edge is assigned to
the vertex cover (i.e., it is marked). Since (at most) all
1E1 edges might share a common vertex, the data struc-
ture has to be a multiset which contains 1E1 copies of
each vertex. Thus, marking the 1V1 — k complement ver-
tices actually requires marking 1V1 — k times 1E1 iden-
tical vertices. This will leave (k — 1) * 1E1 unmarked
vertices in the input structure. To achieve this algorithm
through recognition of a dependency grammar, the mark-
ing process will be encoded as the filling of appropriate
valencies of a word s by words representing the vertices.
Before we prove that this encoding can be generated in
polynomial time we show that:
Lemma 1
The DG recognition problem is in the complexity class
.A/P. 0
Let G = (Lex, C, isac , E) and a E E. We give
a nondeterministic algorithm for deciding whether a =
(si • sn) is in L(G). Let H be an empty set initially:
</bodyText>
<listItem confidence="0.940365466666667">
1. Repeat until 1H1 = lo-1
(a) i. For every s, E a choose a lexicon entry
c, E Lex(si).
ii. From the c, choose one word as the head
ho.
iii. Let H {h0} and M := fedi E
[I, H.
(b) Repeat until M = 0:
i. Choose a head h E H and a valency
(b,d,c) E h.valencies and a modifier m E
M.
ii. If governs(h, d, m) holds then establish the
dependency relation between h and the m,
and add m to the set H.
iii. Remove in from M.
</listItem>
<bodyText confidence="0.989583375">
The algorithm obviously is (nondeterministically)
polynomial in the length of the input. Given that
(G, E DGR, a dependency tree covering the whole
input exists and the algorithm will be able to guess the
dependents of every head correctly. If, conversely, the
algorithm halts for some input (G, a), then there neces-
sarily must be a dependency tree rooted in ho completely
covering a. Thus, (G, a) E DGR. •
</bodyText>
<subsectionHeader confidence="0.561551">
Lemma 2
</subsectionHeader>
<bodyText confidence="0.959446">
Let (V, E, k) be a possible instance of the vertex cover
problem. Then a grammar G(V, E, k) and an input
</bodyText>
<equation confidence="0.586040333333333">
o-(V,E,k) can be constructed in time polynomial in
11 (V, E, k)11 such that
(V, E, k) E VC &lt;=&gt; (G(V,E,k),a(V,E,k)) E DGR
</equation>
<bodyText confidence="0.926674727272727">
For the proof, we first define the encoding and show
that it can be constructed in polynomial time. Then we
proceed showing that the equivalence claim holds. The
set of classes is C =def IS, R, U) U {Hili E [1, 1E1]} U
V,1i E [1, &apos;VI]). In the isac hierarchy the classes U,
share the superclass U, the classes V, the superclass R.
Valencies are defined for the classes according to Table 2.
Furthermore, we define E =def {S} U {Vili E [1, &apos;VD.
The lexicon Lex associates words with classes as given
in Table 2.
We set
G(V, E, k) =def. (Lex, C, isac , E)
and
o-(V,E,k) =def $ V1 &apos; • • V1 &apos; • • VIVI • • • VIVI
IEl I Ei
For an example, cf. Fig. 4 which shows a dependency
tree for the instance of the vertex cover problem from
Fig. 2. The two dependencies u1 and u2 represent the
complement of the vertex cover.
It is easily seen3 that 11 (G (V, E , k), a (V , E , k))11 is
polynomial in liv lI 11E11 and k. From 1E1 &gt; k and Def-
inition lit follows that 11(V, E, k)11 &gt; 11E11 &gt; 11k11 &gt; k•
</bodyText>
<footnote confidence="0.9373255">
3The construction requires 2 *II/1+1E1 + 3 word classes,
IV&apos; + 1 terminals in at most 1E1 + 2 readings each. S defines
1V1 + k * lEl — k valencies, Ui defines 1E1 — 1 valencies. The
length of a is IVI * 1E1 + 1.
</footnote>
<page confidence="0.989856">
341
</page>
<table confidence="0.993544769230769">
word class valencies order
Vvi E V Vi iSac R { }
Vv i E V Ui iSac U {(--, ri,Vi), - &apos; , (-,riE1-1,Vi)} El }
Vei E E Hi 0 El }
S {(-7 u1) 01* * * ) (-)111171-k I V)) -..=-{ }
(-)111,111))* •• 7 (-1111EI) HIE1))
(-) ri., R), • • • 7 (-) roc-olEi,R)}
word
word classes
vi Vi} U vn E V:
ei = (v,„ vii)A
V i E {vm, vn}}
{S}
</table>
<tableCaption confidence="0.988893">
Table 2: Word classes and lexicon to encode vertex cover problem
</tableCaption>
<figureCaption confidence="0.841847">
Figure 4: Encoding a solution to the vertex cover prob-
lem from Fig. 2.
</figureCaption>
<bodyText confidence="0.989988714285714">
Hence, the construction of (G(V, E , k), a (V, E, k)) can
be done in worst-case time polynomial in 11(V, E,k)11.
We next show the equivalence of the two problems.
Assume (V, E, k) E VC: Then there exists a subset
V&apos; C V and a function f : E -4 V&apos; such that 1V&apos;I &lt; k
and V(v,n, vn) E E f((vm,vn)) E {(vm, v,i)}. A
dependency tree for o-(V, E, k) is constructed by:
</bodyText>
<listItem confidence="0.9727778">
1. For every ei E E, one word f (ei) is assigned class
Hi and governed by s in valency hi.
2. For each vi E V \ VI, lEl - 1 words yi are assigned
class R and governed by the remaining copy of vi
in reading Ui through valencies r1 to r1El- 1.
3. The vi in reading Ui are governed by s through the
valencies u (j E [1, 1V1 - k]).
4. (k - 1) * lEl words remain in (7. These receive
reading R and are governed by s in valencies r3 (j E
[1, (k - 1)1E1]).
</listItem>
<bodyText confidence="0.9878664375">
The dependency tree rooted in s covers the whole in-
put a(V, E, k). Since G(V,E,k) does not give any fur-
ther restrictions this implies a(V,E,k) E L(G(V, E, k))
and, thus, (G(V, E,k),o-(V,E,k)) E DGR.
Conversely assume (G(V,E,k),a(V,E,k)) E DGR:
Then a(V, E, k) E L(G(V, E, k)) holds, i.e., there ex-
ists a dependency tree that covers the whole input. Since
s cannot be governed in any valency, it follows that s
must be the root. The instance s of S has 1E1 valencies
of class H, (k - 1) *1E1 valencies of class R, and 1V1 - k
valencies of class U, whose instances in turn have 1E1-1
valencies of class R. This sums up to 1E1 * 1V1 potential
dependents, which is the number of terminals in a be-
sides s. Thus, all valencies are actually filled. We define
a subset Vo C V by Vo =clef Iv E Vli E [1, 1V1 - k] :
s.mod(ui) = v}. I.e.,
</bodyText>
<equation confidence="0.798203">
(1) 11701 = 1V1- k
</equation>
<bodyText confidence="0.933742285714286">
The dependents of s in valencies hi are from the set V\
Vo. We define a function f : E -4 V \ Vo by f(e) =def
s.mod(hi) for all ei E E. By construction f (ei) is an
end point of edge ei, i.e.
(2) V(vm, vri) E E ((vm,vn)) E {vmovn}
We define a subset V&apos; C V by V&apos; =clef If (e) E E}.
Thus
</bodyText>
<listItem confidence="0.987189333333333">
(3) Ve E E : f (e) E Vi
By construction of V&apos; and by (1) it follows
(4) - k
</listItem>
<bodyText confidence="0.9559015">
From (2), (3), and (4) we induce (V, E, k) E VC. •
Theorem 3
The DG recognition problem is in the complexity class
JVPC. 0
The JVP-completeness of the DG recognition problem
follows directly from lemmata 1 and 2. •
</bodyText>
<sectionHeader confidence="0.999301" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999721">
We have shown that current DG theorizing exhibits a
feature not contained in previous formal studies of DG,
namely the independent specification of dominance and
precedence constraints. This feature leads to a
complete recognition recognition problem. The necessity of this ex-
tension approved by most current DGs relates to the fact
that DG must directly characterize dependencies which
in PSG are captured by a projective structure and addi-
tional processes such as coindexing or structure sharing
(most easily seen in treatments of so-called unbounded
</bodyText>
<figure confidence="0.992871">
h1 113 r1 r2 r3 112 h3 h4 r4 r5
a bc c cc c ddddd
4
a a a a bbbb
</figure>
<page confidence="0.992243">
342
</page>
<bodyText confidence="0.999972315789474">
dependencies). The dissociation of tree structure and
linear order, as we have done in Section 3, nevertheless
seems to be a promising approach for PSG as well; see a
very similar proposal for HPSG (Reape, 1989).
The ./VP-completeness result also holds for the dis-
continuous DG presented in Section 3. This DG can
characterize at least some context-sensitive languages
such as an bn en , i.e., the increase in complexity corre-
sponds to an increase of generative capacity. We conjec-
ture that, provided a proper formalization of the other DG
versions presented in Section 2, their ArP-completeness
can be similarly shown. With respect to parser design,
this result implies that the well known polynomial time
complexity of chart- or tabular-based parsing techniques
cannot be achieved for these DG formalisms in gen-
eral. This is the reason why the PARSETALK text under-
standing system (Neuhaus &amp; Hahn, 1996) utilizes special
heuristics in a heterogeneous chart- and backtracking-
based parsing approach.
</bodyText>
<sectionHeader confidence="0.999438" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994864072463768">
Barton, Jr., G. E. (1985). On the complexity of ID/LP
parsing. Computational Linguistics, 11(4):205-
218.
Broker, N. (1997). Eine Dependenzgrammatik
zur Kopplung heterogener Wissenssysteme auf
modallogischer Basis, (Dissertation). Freiburg,
DE: Philosophische Fakultat, Albert-Ludwigs-
Universitat.
Earley, J. (1970). An efficient context-free parsing algo-
rithm. Communications of the ACM, 13(2):94-102.
Gaifman, H. (1965). Dependency systems and phrase-
structure systems. Information &amp; Control, 8:304-
337.
Garey, M. R. &amp; D. S. Johnson (1983). Computers
and Intractability: A Guide to the Theory of NP-
completeness (2. ed.). New York, NY: Freeman.
Haegeman, L. (1994). Introduction to Government and
Binding. Oxford, UK: Basil Blackwell.
Hellwig, P. (1988). Chart parsing according to the slot
and filler principle. In Proc. of the 12th Int. Conf
on Computational Linguistics. Budapest, HU, 22-
27 Aug 1988, Vol. 1, pp. 242-244.
Hepple, M. (1990). Word order and obliqueness in cat-
egorial grammar. In G. Barry &amp; G. Morill (Eds.),
Studies in categorial grammar, pp. 47-64. Edin-
burgh, UK: Edinburgh University Press.
Huck, G. (1988). Phrasal verbs and the categories of
postponement. In R. Oehrle, E. Bach &amp; D. Wheeler
(Eds.), Categorial Grammars and Natural Lan-
guage Structures, pp. 249-263. Studies in Linguis-
tics and Philosophy 32. Dordrecht, NL: D. Reidel.
Hudson, R. (1990). English Word Grammar. Oxford,
UK: Basil Blackwell.
Lombardo, V. &amp; L. Lesmo (1996). An earley-type recog-
nizer for dependency grammar. In Proc. of the 16th
Int. Conf on Computational Linguistics. Copen-
hagen, DK, 5-9 Aug 1996, Vol. 2, pp. 723-728.
McCord, M. (1990). Slot grammar: A system for simpler
construction of practical natural language gram-
mars. In R. Studer (Ed.), Natural Language and
Logic, pp. 118-145. Berlin, Heidelberg: Springer.
Mel&apos; euk, I. (1988). Dependency Syntax: Theory and
Practice. New York, NY: SUNY State University
Press of New York.
Mel&apos; euk, I. &amp; N. Pertsov (1987). Surface Syntax of En-
glish: A Formal Model within the MIT Framework.
Amsterdam, NL: John Benjamins.
Neuhaus, P. &amp; U. Hahn (1996). Restricted parallelism in
object-oriented lexical parsing. In Proc. of the 16th
Int. Conf on Computational Linguistics. Copen-
hagen, DK, 5-9 Aug 1996, pp. 502-507.
Pollard, C. &amp; I. Sag (1994). Head-Driven Phrase Struc-
ture Grammar. Chicago, IL: University of Chicago
Press.
Rambow, 0. &amp; A. Joshi (1994). A formal look at DGs
and PSGs, with consideration of word-order phe-
nomena. In L. Wanner (Ed.), Current Issues in
Meaning-Text-Theory. London: Pinter.
Reape, M. (1989). A logical treatment of semi-free word
order and discontinuous constituents. In Proc. of the
27th Annual Meeting of the Association for Compu-
tational Linguistics. Vancouver, BC, 1989, pp. 103-
110.
Starosta, S. (1988). The Case for Lexicase. London:
Pinter.
Starosta, S. (1992). Lexicase revisited. Department of
Linguistics, University of Hawaii.
Tesniere, L. ((1969) 1959). Elements de Syntaxe Struc-
turale (2. ed.). Paris, FR: Klincksieck.
</reference>
<page confidence="0.999292">
343
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.715948">
<title confidence="0.999429">The Complexity of Recognition of Linguistically Adequate Dependency Grammars</title>
<author confidence="0.9871525">Peter Neuhaus Norbert Broker</author>
<affiliation confidence="0.907896">Computational Linguistics Research Group Freiburg University, FriedrichstraBe 50</affiliation>
<address confidence="0.999931">D-79098 Freiburg, Germany</address>
<email confidence="0.997844">neuhaus,nobi}@coling.uni-freiburg.de</email>
<abstract confidence="0.991906333333333">Results of computational complexity exist for a wide range of phrase structure-based grammar formalisms, while there is an apparent lack of such results for dependency-based formalisms. We here adapt a result on the complexity of ID/LP-grammars to the dependency framework. Contrary to previous studies on heavily restricted dependency grammars, we prove that recognition (and thus, parsing) of linguistically adequate dependency grammars is,ArP-complete.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G E Barton</author>
</authors>
<title>On the complexity of ID/LP parsing.</title>
<date>1985</date>
<journal>Computational Linguistics,</journal>
<pages>11--4</pages>
<contexts>
<context position="2640" citStr="Barton (1985)" startWordPosition="387" endWordPosition="388">ent variant of DG (not even Tesniere&apos;s original formulation) is compatible with Gaifman s conception, which seems to be motivated by formal considerations only (viz., the proof of equivalence). Section 3 advances our proposal, which cleanly separates dominance and precedence relations. This is illustrated in the fourth section, where we give a simple encoding of an VP-complete problem in a discontinuous DG. Our proof of ArT&apos;-completeness, however, does not rely on discontinuity, but only requires unordered trees. It is adapted from a similar proof for unordered contextfree grammars (UCFGs) by Barton (1985). 2 Versions of Dependency Grammar The growing interest in the dependency concept (which roughly corresponds to the 0-roles of GB, subcategorization in HPSG, and the so-called domain of locality of TAG) again raises the issue whether non-lexical categories are necessary for linguistic analysis. After reviewing several proposals in this section, we argue in the next section that word order — the description of which is the most prominent difference between PSGs and DGs — can adequately be described without reference to nonlexical categories. Standard PSG trees are projective, i.e., no branches </context>
<context position="16218" citStr="Barton (1985)" startWordPosition="2635" endWordPosition="2636">g. 2. 4.2 Formal Proof using Continuous DG The encoding outlined above uses non-projective trees, i.e., crossing dependencies. In anticipation of counter arguments such as that the presented dependency grammar was just too powerful, we will present the proof using only one feature supplied by most DG formalisms, namely the free order of modifiers with respect to their head. Thus, modifiers must be inserted into an order domain of their head (i.e., no + mark in valencies). This version of the proof uses a slightly more complicated encoding of the vertex cover problem and resembles the proof by Barton (1985). Definition 1 (Measure) Let 11 • 11 be a measure for the encoded input length of a computational problem. We require that if S is a set or string and k E N then 181 &gt; k implies 11811 &gt; 11k11 and that for any tuple 11(• • • x, • • • )11 &gt; 11x11 holds. Definition 2 (Vertex Cover Problem) A possible instance of the vertex cover problem is a triple (V, E, k) where (V, E) is a finite graph and 1V1 &gt; k E N. The vertex cover problem is the set VC of all instances (V, E, k) for which there exists a subset V&apos; C V and a function f : E —&gt; V&apos; such that IV&apos; 1 &lt; k and V(vm, vn) E E ((vm vn)) E {Vm, Viz}. &lt;</context>
</contexts>
<marker>Barton, 1985</marker>
<rawString>Barton, Jr., G. E. (1985). On the complexity of ID/LP parsing. Computational Linguistics, 11(4):205-218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Broker</author>
</authors>
<title>Eine Dependenzgrammatik zur Kopplung heterogener Wissenssysteme auf modallogischer Basis,</title>
<date>1997</date>
<location>(Dissertation). Freiburg, DE: Philosophische Fakultat, Albert-LudwigsUniversitat.</location>
<contexts>
<context position="9305" citStr="Broker, 1997" startWordPosition="1430" endWordPosition="1431"> know John likes&amp;quot; and the restriction to lexical nodes which prohibits gapthreading and other mechanisms tied to phrasal categories. 3 A Dependency Grammar with Word Order Domains We now sketch a minimal DG that incorporates only word classes and word order as descriptional dimensions. The separation of dominance and precedence presented here grew out of our work on German, and retains the local flavor of dependency specification, while at the same time covering arbitrary discontinuities. It is based on a (modal) logic with model-theoretic interpretation, which is presented in more detail in (Broker, 1997). 338 know - likes \I object &apos; subject/ &apos; John ,&apos; ■ d2 Figure 1: Word order domains in &amp;quot;Beans, I know John likes&amp;quot; 3.1 Order Specification Our initial observation is that DG cannot use binary precedence constraints as PSG does. Since DG analyses are hierarchically flatter, binary precedence constraints result in inconsistencies, as the analyses of Word Grammar and Lexicase illustrate. In PSG, on the other hand, the phrasal hierarchy separates the scope of precedence restrictions. This effect is achieved in our approach by defining word order domains as sets of words, where precedence restrictio</context>
<context position="11468" citStr="Broker, 1997" startWordPosition="1783" endWordPosition="1784">traints are formulated as a binary relation &amp;quot;-.&lt;&amp;quot; over dependency labels, including the special symbol &amp;quot;self&amp;quot; denoting the head. Discontinuities can easily be characterized, since a word may be contained in any domain of (nearly) any of its transitive heads. If a domain of its direct head contains the modifier, a continuous dependency results. If, however, a modifier is placed in a domain of some transitive head (as &amp;quot;Beans&amp;quot; in Fig.1), discontinuities occur. Bounding effects on discontinuities are described by specifying that certain dependencies may not be crossed.2 For the &apos;For details, cf. (Broker, 1997). 2German data exist that cannot be captured by the (more common) bounding of discontinuities by nodes of a certain purpose of this paper, we need not formally introduce the bounding condition, though. A sample domain structure is given in Fig.1, with two domains d1 and d2 associated with the governing verb &amp;quot;know&amp;quot; (solid) and one with the embedded verb &amp;quot;likes&amp;quot; (dashed). d1 may contain only one partial dependency tree, the extracted phrase. d2 contains the rest of the sentence. Both domains are described by (2), where the domain sequence is represented as &amp;quot;&lt;&amp;quot;. d2 contains two precedence restric</context>
</contexts>
<marker>Broker, 1997</marker>
<rawString>Broker, N. (1997). Eine Dependenzgrammatik zur Kopplung heterogener Wissenssysteme auf modallogischer Basis, (Dissertation). Freiburg, DE: Philosophische Fakultat, Albert-LudwigsUniversitat.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<pages>13--2</pages>
<contexts>
<context position="1538" citStr="Earley, 1970" startWordPosition="213" endWordPosition="214">t rather at an intuitive understanding of semantically motivated dependency relations. An early formalization was given by Gaifman (1965), who showed the generative capacity of DG to be (weakly) equivalent to standard context-free grammars. Given this equivalence, interest in DG as a linguistic framework diminished considerably, although many dependency grammarians view Gaifman&apos;s conception as an unfortunate one (cf. Section 2). To our knowledge, there has been no other formal study of DG.This is reflected by a recent study (Lombardo &amp; Lesmo, 1996), which applies the Earley parsing technique (Earley, 1970) to DG, and thereby achieves cubic time complexity for the analysis of DG. In their discussion, Lombardo &amp; Lesmo express their hope that slight increases in generative capacity will correspond to equally slight increases in computational complexity. It is this claim that we challenge here. After motivating non-projective analyses for DG, we investigate various variants of DG and identify the separation of dominance and precedence as a major part of current DG theorizing. Thus, no current variant of DG (not even Tesniere&apos;s original formulation) is compatible with Gaifman s conception, which see</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Earley, J. (1970). An efficient context-free parsing algorithm. Communications of the ACM, 13(2):94-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Gaifman</author>
</authors>
<title>Dependency systems and phrasestructure systems.</title>
<date>1965</date>
<journal>Information &amp; Control,</journal>
<pages>8--304</pages>
<contexts>
<context position="1062" citStr="Gaifman (1965)" startWordPosition="141" endWordPosition="142">alisms. We here adapt a result on the complexity of ID/LP-grammars to the dependency framework. Contrary to previous studies on heavily restricted dependency grammars, we prove that recognition (and thus, parsing) of linguistically adequate dependency grammars is,ArP-complete. 1 Introduction The introduction of dependency grammar (DG) into modern linguistics is marked by Tesniere (1959). His conception addressed didactic goals and, thus, did not aim at formal precision, but rather at an intuitive understanding of semantically motivated dependency relations. An early formalization was given by Gaifman (1965), who showed the generative capacity of DG to be (weakly) equivalent to standard context-free grammars. Given this equivalence, interest in DG as a linguistic framework diminished considerably, although many dependency grammarians view Gaifman&apos;s conception as an unfortunate one (cf. Section 2). To our knowledge, there has been no other formal study of DG.This is reflected by a recent study (Lombardo &amp; Lesmo, 1996), which applies the Earley parsing technique (Earley, 1970) to DG, and thereby achieves cubic time complexity for the analysis of DG. In their discussion, Lombardo &amp; Lesmo express the</context>
<context position="5012" citStr="Gaifman (1965)" startWordPosition="767" endWordPosition="768">We will refer to DGs allowing non-projective analyses as discontinuous DGs. Tesniere (1959) devised a bipartite grammar theory which consists of a dependency component and a translation component (&apos;translation&apos; used in a technical sense denoting a change of category and grammatical function). The dependency component defines four main categories and possible dependencies between them. What is of interest here is that there is no mentioning of order in Tesniere&apos; s work. Some practitioneers of DG have allowed word order as a marker for translation, but they do not prohibit non-projective trees. Gaifman (1965) designed his DG entirely analogous to context-free phrase structure grammars. Each word is associated with a category, which functions like the non-terminals in CFG. He then defines the following rule format for dependency grammars: (1) X (Yi • • Yi • • • Yn) This rule states that a word of category X governs words of category Y1,..., Y, which occur in the given order. The head (the word of category X) must occur between the i-th and the (i + 1)-th modifier. The rule can be viewed as an ordered tree of depth one with node labels. Trees are combined through the identification of the root of on</context>
</contexts>
<marker>Gaifman, 1965</marker>
<rawString>Gaifman, H. (1965). Dependency systems and phrasestructure systems. Information &amp; Control, 8:304-337.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Garey</author>
<author>D S Johnson</author>
</authors>
<date>1983</date>
<booktitle>Computers and Intractability: A Guide to the Theory of NPcompleteness</booktitle>
<editor>(2. ed.).</editor>
<publisher>Freeman.</publisher>
<location>New York, NY:</location>
<contexts>
<context position="14325" citStr="Garey &amp; Johnson, 1983" startWordPosition="2294" endWordPosition="2297">e increment of the complexity&amp;quot;. We will show that adding a little (linguistically required) flexibility might well render recognition ATP-complete. To prove this, we will encode the vertex cover problem, which is known to be .IVP-complete, in a DG. 4.1 Encoding the Vertex Cover Problem in Discontinuous DG A vertex cover of a finite graph is a subset of its vertices such that (at least) one end point of every edge is a member of that set. The vertex cover problem is to decide whether for a given graph there exists a vertex cover with at most k elements. The problem is known to be .NP-complete (Garey &amp; Johnson, 1983, pp53-56). Fig. 2 gives a simple example where {c, d} is a vertex cover. Figure 2: Simple graph with vertex cover 1c, dl. A straightforward encoding of a solution in the DG formalism introduced in Section 3 defines a root word s of class S with k valencies for words of class 0. 0 has 1V1 subclasses denoting the nodes of the graph. An edge is represented by two linked words (one for each end point) with the governing word corresponding to the node included in the vertex cover. The subordinated word is assigned the class R, while the governing word is assigned the subclass of 0 denoting the nod</context>
</contexts>
<marker>Garey, Johnson, 1983</marker>
<rawString>Garey, M. R. &amp; D. S. Johnson (1983). Computers and Intractability: A Guide to the Theory of NPcompleteness (2. ed.). New York, NY: Freeman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Haegeman</author>
</authors>
<title>Introduction to Government and Binding.</title>
<date>1994</date>
<publisher>Basil Blackwell.</publisher>
<location>Oxford, UK:</location>
<contexts>
<context position="3696" citStr="Haegeman, 1994" startWordPosition="550" endWordPosition="551">difference between PSGs and DGs — can adequately be described without reference to nonlexical categories. Standard PSG trees are projective, i.e., no branches cross when the terminal nodes are projected onto the input string. In contrast to PSG approaches, DG requires non-projective analyses. As DGs are restricted to lexical nodes, one cannot, e.g., describe the so-called unbounded dependencies without giving up projectivity. First, the categorial approach employing partial constituents (Huck, 1988; Hepple, 1990) is not available, since there are no phrasal categories. Second, the coindexing (Haegeman, 1994) or structure-sharing (Pollard &amp; Sag, 1994) approaches are not available, since there are no empty categories. Consider the extracted NP in &amp;quot;Beans, I know John likes&amp;quot; (cf. also to Fig.1 in Section 3). A projective tree would require &amp;quot;Beans&amp;quot; to be connected to either &amp;quot;I&amp;quot; or &amp;quot;know&amp;quot; – none of which is conceptually directly related to &amp;quot;Beans&amp;quot;. It is &amp;quot;likes&amp;quot; that determines syntactic fea337 tures of &amp;quot;Beans&amp;quot; and which provides a semantic role for it. The only connection between &amp;quot;know&amp;quot; and &amp;quot;Beans&amp;quot; is that the finite verb allows the extraction of &amp;quot;Beans&amp;quot;, thus defining order restrictions for the NP. T</context>
</contexts>
<marker>Haegeman, 1994</marker>
<rawString>Haegeman, L. (1994). Introduction to Government and Binding. Oxford, UK: Basil Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hellwig</author>
</authors>
<title>Chart parsing according to the slot and filler principle.</title>
<date>1988</date>
<booktitle>In Proc. of the 12th Int. Conf on Computational Linguistics.</booktitle>
<volume>1</volume>
<pages>242--244</pages>
<location>Budapest, HU,</location>
<contexts>
<context position="7492" citStr="Hellwig, 1988" startWordPosition="1160" endWordPosition="1161"> the proposition &amp;quot;object of verb succeeds it&amp;quot;. Extraction is analyzed by establishing another dependency, visitor, between the verb and the extractee, which is required to precede the verb, as in &amp;quot;visitor of verb precedes it&amp;quot;. Resulting inconsistencies, e.g. in case of an extracted object, are not resolved, however. Lexicase (Starosta, 1988; 1992) employs complex feature structures to represent lexical and syntactic entities. Its word order description is much like that of Word Grammar (at least at some level of abstraction), and shares the above inconsistency. Dependency Unification Grammar (Hellwig, 1988) defines a tree-like data structure for the representation of syntactic analyses. Using morphosyntactic features with special interpretations, a word defines abstract positions into which modifiers are mapped. Partial orderings and even discontinuities can thus be described by allowing a modifier to occupy a position defined by some transitive head. The approach cannot restrict discontinuities properly, however. Slot Grammar (McCord, 1990) employs a number of rule types, some of which are exclusively concerned with precedence. So-called head/slot and slot/slot ordering rules describe the prece</context>
</contexts>
<marker>Hellwig, 1988</marker>
<rawString>Hellwig, P. (1988). Chart parsing according to the slot and filler principle. In Proc. of the 12th Int. Conf on Computational Linguistics. Budapest, HU, 22-27 Aug 1988, Vol. 1, pp. 242-244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hepple</author>
</authors>
<title>Word order and obliqueness in categorial grammar.</title>
<date>1990</date>
<booktitle>In G. Barry &amp; G. Morill (Eds.), Studies in categorial grammar,</booktitle>
<pages>47--64</pages>
<publisher>Edinburgh University Press.</publisher>
<location>Edinburgh, UK:</location>
<contexts>
<context position="3599" citStr="Hepple, 1990" startWordPosition="535" endWordPosition="536"> we argue in the next section that word order — the description of which is the most prominent difference between PSGs and DGs — can adequately be described without reference to nonlexical categories. Standard PSG trees are projective, i.e., no branches cross when the terminal nodes are projected onto the input string. In contrast to PSG approaches, DG requires non-projective analyses. As DGs are restricted to lexical nodes, one cannot, e.g., describe the so-called unbounded dependencies without giving up projectivity. First, the categorial approach employing partial constituents (Huck, 1988; Hepple, 1990) is not available, since there are no phrasal categories. Second, the coindexing (Haegeman, 1994) or structure-sharing (Pollard &amp; Sag, 1994) approaches are not available, since there are no empty categories. Consider the extracted NP in &amp;quot;Beans, I know John likes&amp;quot; (cf. also to Fig.1 in Section 3). A projective tree would require &amp;quot;Beans&amp;quot; to be connected to either &amp;quot;I&amp;quot; or &amp;quot;know&amp;quot; – none of which is conceptually directly related to &amp;quot;Beans&amp;quot;. It is &amp;quot;likes&amp;quot; that determines syntactic fea337 tures of &amp;quot;Beans&amp;quot; and which provides a semantic role for it. The only connection between &amp;quot;know&amp;quot; and &amp;quot;Beans&amp;quot; is that</context>
</contexts>
<marker>Hepple, 1990</marker>
<rawString>Hepple, M. (1990). Word order and obliqueness in categorial grammar. In G. Barry &amp; G. Morill (Eds.), Studies in categorial grammar, pp. 47-64. Edinburgh, UK: Edinburgh University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Huck</author>
</authors>
<title>Phrasal verbs and the categories of postponement. In</title>
<date>1988</date>
<booktitle>Categorial Grammars and Natural Language Structures,</booktitle>
<pages>249--263</pages>
<location>Dordrecht, NL: D. Reidel.</location>
<contexts>
<context position="3584" citStr="Huck, 1988" startWordPosition="533" endWordPosition="534">his section, we argue in the next section that word order — the description of which is the most prominent difference between PSGs and DGs — can adequately be described without reference to nonlexical categories. Standard PSG trees are projective, i.e., no branches cross when the terminal nodes are projected onto the input string. In contrast to PSG approaches, DG requires non-projective analyses. As DGs are restricted to lexical nodes, one cannot, e.g., describe the so-called unbounded dependencies without giving up projectivity. First, the categorial approach employing partial constituents (Huck, 1988; Hepple, 1990) is not available, since there are no phrasal categories. Second, the coindexing (Haegeman, 1994) or structure-sharing (Pollard &amp; Sag, 1994) approaches are not available, since there are no empty categories. Consider the extracted NP in &amp;quot;Beans, I know John likes&amp;quot; (cf. also to Fig.1 in Section 3). A projective tree would require &amp;quot;Beans&amp;quot; to be connected to either &amp;quot;I&amp;quot; or &amp;quot;know&amp;quot; – none of which is conceptually directly related to &amp;quot;Beans&amp;quot;. It is &amp;quot;likes&amp;quot; that determines syntactic fea337 tures of &amp;quot;Beans&amp;quot; and which provides a semantic role for it. The only connection between &amp;quot;know&amp;quot; and </context>
</contexts>
<marker>Huck, 1988</marker>
<rawString>Huck, G. (1988). Phrasal verbs and the categories of postponement. In R. Oehrle, E. Bach &amp; D. Wheeler (Eds.), Categorial Grammars and Natural Language Structures, pp. 249-263. Studies in Linguistics and Philosophy 32. Dordrecht, NL: D. Reidel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hudson</author>
</authors>
<title>English Word Grammar.</title>
<date>1990</date>
<publisher>Basil Blackwell.</publisher>
<location>Oxford, UK:</location>
<contexts>
<context position="6757" citStr="Hudson, 1990" startWordPosition="1045" endWordPosition="1046">on of phrasal nodes (although phrasal features are sometimes allowed) and the introduction of edge labels (to distinguish different dependency relations). Meaning-Text Theory (Mel&apos; euk, 1988) assumes seven strata of representation. The rules mapping from the unordered dependency trees of surface-syntactic representations onto the annotated lexeme sequences of deepmorphological representations include global ordering rules which allow discontinuities. These rules have not yet been formally specified (Mel&apos; euk &amp; Pertsov, 1987, p.1870, but see the proposal by Rambow &amp; Joshi (1994). Word Grammar (Hudson, 1990) is based on general graphs. The ordering of two linked words is specified together with their dependency relation, as in the proposition &amp;quot;object of verb succeeds it&amp;quot;. Extraction is analyzed by establishing another dependency, visitor, between the verb and the extractee, which is required to precede the verb, as in &amp;quot;visitor of verb precedes it&amp;quot;. Resulting inconsistencies, e.g. in case of an extracted object, are not resolved, however. Lexicase (Starosta, 1988; 1992) employs complex feature structures to represent lexical and syntactic entities. Its word order description is much like that of W</context>
</contexts>
<marker>Hudson, 1990</marker>
<rawString>Hudson, R. (1990). English Word Grammar. Oxford, UK: Basil Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Lombardo</author>
<author>L Lesmo</author>
</authors>
<title>An earley-type recognizer for dependency grammar.</title>
<date>1996</date>
<booktitle>In Proc. of the 16th Int. Conf on Computational Linguistics. Copenhagen, DK,</booktitle>
<volume>2</volume>
<pages>5--9</pages>
<contexts>
<context position="1479" citStr="Lombardo &amp; Lesmo, 1996" startWordPosition="203" endWordPosition="206">dressed didactic goals and, thus, did not aim at formal precision, but rather at an intuitive understanding of semantically motivated dependency relations. An early formalization was given by Gaifman (1965), who showed the generative capacity of DG to be (weakly) equivalent to standard context-free grammars. Given this equivalence, interest in DG as a linguistic framework diminished considerably, although many dependency grammarians view Gaifman&apos;s conception as an unfortunate one (cf. Section 2). To our knowledge, there has been no other formal study of DG.This is reflected by a recent study (Lombardo &amp; Lesmo, 1996), which applies the Earley parsing technique (Earley, 1970) to DG, and thereby achieves cubic time complexity for the analysis of DG. In their discussion, Lombardo &amp; Lesmo express their hope that slight increases in generative capacity will correspond to equally slight increases in computational complexity. It is this claim that we challenge here. After motivating non-projective analyses for DG, we investigate various variants of DG and identify the separation of dominance and precedence as a major part of current DG theorizing. Thus, no current variant of DG (not even Tesniere&apos;s original form</context>
<context position="13536" citStr="Lombardo &amp; Lesmo (1996" startWordPosition="2155" endWordPosition="2158"> word m in dependency d if h defines a valency (b, d, c) such that (m isac c) and m can consistently be inserted into a domain of h (for b = —) or a domain of a transitive head of h (for b = +). This condition is written as (5) governs(h, d, m) A DG is thus characterized by (6) G= (Lex,C,isac, E) The language L(G) includes any sequence of words for which a dependency tree can be constructed such that for each word h governing a word m in dependency d, governs(h, d, m) holds. The modifier of h in dependency d is accessed by (7) h.mod(d) category. Beans d1 339 4 The complexity of DG Recognition Lombardo &amp; Lesmo (1996, p.728) convey their hope that increasing the flexibility of their conception of DG will &amp;quot; ... imply the restructuring of some parts of the recognizer, with a plausible increment of the complexity&amp;quot;. We will show that adding a little (linguistically required) flexibility might well render recognition ATP-complete. To prove this, we will encode the vertex cover problem, which is known to be .IVP-complete, in a DG. 4.1 Encoding the Vertex Cover Problem in Discontinuous DG A vertex cover of a finite graph is a subset of its vertices such that (at least) one end point of every edge is a member of </context>
</contexts>
<marker>Lombardo, Lesmo, 1996</marker>
<rawString>Lombardo, V. &amp; L. Lesmo (1996). An earley-type recognizer for dependency grammar. In Proc. of the 16th Int. Conf on Computational Linguistics. Copenhagen, DK, 5-9 Aug 1996, Vol. 2, pp. 723-728.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M McCord</author>
</authors>
<title>Slot grammar: A system for simpler construction of practical natural language grammars. In</title>
<date>1990</date>
<booktitle>Natural Language and Logic,</booktitle>
<pages>118--145</pages>
<editor>R. Studer (Ed.),</editor>
<publisher>Springer.</publisher>
<location>Berlin, Heidelberg:</location>
<contexts>
<context position="7935" citStr="McCord, 1990" startWordPosition="1221" endWordPosition="1222"> description is much like that of Word Grammar (at least at some level of abstraction), and shares the above inconsistency. Dependency Unification Grammar (Hellwig, 1988) defines a tree-like data structure for the representation of syntactic analyses. Using morphosyntactic features with special interpretations, a word defines abstract positions into which modifiers are mapped. Partial orderings and even discontinuities can thus be described by allowing a modifier to occupy a position defined by some transitive head. The approach cannot restrict discontinuities properly, however. Slot Grammar (McCord, 1990) employs a number of rule types, some of which are exclusively concerned with precedence. So-called head/slot and slot/slot ordering rules describe the precedence in projective trees, referring to arbitrary predicates over head and modifiers. Extractions (i.e., discontinuities) are merely handled by a mechanism built into the parser. This brief overview of current DG flavors shows that various mechanisms (global rules, general graphs, procedural means) are generally employed to lift the limitation to projective trees. Our own approach presented below improves on these proposals because it allo</context>
</contexts>
<marker>McCord, 1990</marker>
<rawString>McCord, M. (1990). Slot grammar: A system for simpler construction of practical natural language grammars. In R. Studer (Ed.), Natural Language and Logic, pp. 118-145. Berlin, Heidelberg: Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mel&apos; euk</author>
<author>I</author>
</authors>
<title>Dependency Syntax: Theory and Practice.</title>
<date>1988</date>
<publisher>SUNY State University Press of</publisher>
<location>New York, NY:</location>
<marker>euk, I, 1988</marker>
<rawString>Mel&apos; euk, I. (1988). Dependency Syntax: Theory and Practice. New York, NY: SUNY State University Press of New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mel&apos; euk</author>
<author>I</author>
<author>N Pertsov</author>
</authors>
<title>Surface Syntax of English: A Formal Model within the MIT Framework.</title>
<date>1987</date>
<publisher>John Benjamins.</publisher>
<location>Amsterdam, NL:</location>
<marker>euk, I, Pertsov, 1987</marker>
<rawString>Mel&apos; euk, I. &amp; N. Pertsov (1987). Surface Syntax of English: A Formal Model within the MIT Framework. Amsterdam, NL: John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Neuhaus</author>
<author>U Hahn</author>
</authors>
<title>Restricted parallelism in object-oriented lexical parsing.</title>
<date>1996</date>
<booktitle>In Proc. of the 16th Int. Conf on Computational Linguistics.</booktitle>
<pages>502--507</pages>
<location>Copenhagen, DK,</location>
<marker>Neuhaus, Hahn, 1996</marker>
<rawString>Neuhaus, P. &amp; U. Hahn (1996). Restricted parallelism in object-oriented lexical parsing. In Proc. of the 16th Int. Conf on Computational Linguistics. Copenhagen, DK, 5-9 Aug 1996, pp. 502-507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pollard</author>
<author>I Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press.</publisher>
<location>Chicago, IL:</location>
<contexts>
<context position="3739" citStr="Pollard &amp; Sag, 1994" startWordPosition="554" endWordPosition="557">adequately be described without reference to nonlexical categories. Standard PSG trees are projective, i.e., no branches cross when the terminal nodes are projected onto the input string. In contrast to PSG approaches, DG requires non-projective analyses. As DGs are restricted to lexical nodes, one cannot, e.g., describe the so-called unbounded dependencies without giving up projectivity. First, the categorial approach employing partial constituents (Huck, 1988; Hepple, 1990) is not available, since there are no phrasal categories. Second, the coindexing (Haegeman, 1994) or structure-sharing (Pollard &amp; Sag, 1994) approaches are not available, since there are no empty categories. Consider the extracted NP in &amp;quot;Beans, I know John likes&amp;quot; (cf. also to Fig.1 in Section 3). A projective tree would require &amp;quot;Beans&amp;quot; to be connected to either &amp;quot;I&amp;quot; or &amp;quot;know&amp;quot; – none of which is conceptually directly related to &amp;quot;Beans&amp;quot;. It is &amp;quot;likes&amp;quot; that determines syntactic fea337 tures of &amp;quot;Beans&amp;quot; and which provides a semantic role for it. The only connection between &amp;quot;know&amp;quot; and &amp;quot;Beans&amp;quot; is that the finite verb allows the extraction of &amp;quot;Beans&amp;quot;, thus defining order restrictions for the NP. This has led some DG variants to adopt a gen</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Pollard, C. &amp; I. Sag (1994). Head-Driven Phrase Structure Grammar. Chicago, IL: University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Joshi</author>
</authors>
<title>A formal look at DGs and PSGs, with consideration of word-order phenomena. In</title>
<date>1994</date>
<publisher>Pinter.</publisher>
<location>London:</location>
<contexts>
<context position="6728" citStr="Joshi (1994)" startWordPosition="1041" endWordPosition="1042"> share, however, the rejection of phrasal nodes (although phrasal features are sometimes allowed) and the introduction of edge labels (to distinguish different dependency relations). Meaning-Text Theory (Mel&apos; euk, 1988) assumes seven strata of representation. The rules mapping from the unordered dependency trees of surface-syntactic representations onto the annotated lexeme sequences of deepmorphological representations include global ordering rules which allow discontinuities. These rules have not yet been formally specified (Mel&apos; euk &amp; Pertsov, 1987, p.1870, but see the proposal by Rambow &amp; Joshi (1994). Word Grammar (Hudson, 1990) is based on general graphs. The ordering of two linked words is specified together with their dependency relation, as in the proposition &amp;quot;object of verb succeeds it&amp;quot;. Extraction is analyzed by establishing another dependency, visitor, between the verb and the extractee, which is required to precede the verb, as in &amp;quot;visitor of verb precedes it&amp;quot;. Resulting inconsistencies, e.g. in case of an extracted object, are not resolved, however. Lexicase (Starosta, 1988; 1992) employs complex feature structures to represent lexical and syntactic entities. Its word order descr</context>
</contexts>
<marker>Joshi, 1994</marker>
<rawString>Rambow, 0. &amp; A. Joshi (1994). A formal look at DGs and PSGs, with consideration of word-order phenomena. In L. Wanner (Ed.), Current Issues in Meaning-Text-Theory. London: Pinter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Reape</author>
</authors>
<title>A logical treatment of semi-free word order and discontinuous constituents.</title>
<date>1989</date>
<booktitle>In Proc. of the 27th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>103--110</pages>
<location>Vancouver, BC,</location>
<contexts>
<context position="24587" citStr="Reape, 1989" startWordPosition="4345" endWordPosition="4346">cognition recognition problem. The necessity of this extension approved by most current DGs relates to the fact that DG must directly characterize dependencies which in PSG are captured by a projective structure and additional processes such as coindexing or structure sharing (most easily seen in treatments of so-called unbounded h1 113 r1 r2 r3 112 h3 h4 r4 r5 a bc c cc c ddddd 4 a a a a bbbb 342 dependencies). The dissociation of tree structure and linear order, as we have done in Section 3, nevertheless seems to be a promising approach for PSG as well; see a very similar proposal for HPSG (Reape, 1989). The ./VP-completeness result also holds for the discontinuous DG presented in Section 3. This DG can characterize at least some context-sensitive languages such as an bn en , i.e., the increase in complexity corresponds to an increase of generative capacity. We conjecture that, provided a proper formalization of the other DG versions presented in Section 2, their ArP-completeness can be similarly shown. With respect to parser design, this result implies that the well known polynomial time complexity of chart- or tabular-based parsing techniques cannot be achieved for these DG formalisms in g</context>
</contexts>
<marker>Reape, 1989</marker>
<rawString>Reape, M. (1989). A logical treatment of semi-free word order and discontinuous constituents. In Proc. of the 27th Annual Meeting of the Association for Computational Linguistics. Vancouver, BC, 1989, pp. 103-110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Starosta</author>
</authors>
<title>The Case for Lexicase.</title>
<date>1988</date>
<location>London: Pinter.</location>
<contexts>
<context position="7220" citStr="Starosta, 1988" startWordPosition="1119" endWordPosition="1120">ules have not yet been formally specified (Mel&apos; euk &amp; Pertsov, 1987, p.1870, but see the proposal by Rambow &amp; Joshi (1994). Word Grammar (Hudson, 1990) is based on general graphs. The ordering of two linked words is specified together with their dependency relation, as in the proposition &amp;quot;object of verb succeeds it&amp;quot;. Extraction is analyzed by establishing another dependency, visitor, between the verb and the extractee, which is required to precede the verb, as in &amp;quot;visitor of verb precedes it&amp;quot;. Resulting inconsistencies, e.g. in case of an extracted object, are not resolved, however. Lexicase (Starosta, 1988; 1992) employs complex feature structures to represent lexical and syntactic entities. Its word order description is much like that of Word Grammar (at least at some level of abstraction), and shares the above inconsistency. Dependency Unification Grammar (Hellwig, 1988) defines a tree-like data structure for the representation of syntactic analyses. Using morphosyntactic features with special interpretations, a word defines abstract positions into which modifiers are mapped. Partial orderings and even discontinuities can thus be described by allowing a modifier to occupy a position defined b</context>
</contexts>
<marker>Starosta, 1988</marker>
<rawString>Starosta, S. (1988). The Case for Lexicase. London: Pinter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Starosta</author>
</authors>
<title>Lexicase revisited.</title>
<date>1992</date>
<institution>Department of Linguistics, University of Hawaii.</institution>
<marker>Starosta, 1992</marker>
<rawString>Starosta, S. (1992). Lexicase revisited. Department of Linguistics, University of Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Tesniere</author>
</authors>
<date>1969</date>
<booktitle>Elements de Syntaxe Structurale (2. ed.).</booktitle>
<publisher>Klincksieck.</publisher>
<location>Paris, FR:</location>
<marker>Tesniere, 1969</marker>
<rawString>Tesniere, L. ((1969) 1959). Elements de Syntaxe Structurale (2. ed.). Paris, FR: Klincksieck.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>