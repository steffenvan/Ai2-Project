<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000104">
<title confidence="0.909594">
Never Look Back: An Alternative to Centering
</title>
<author confidence="0.991574">
Michael Strube
</author>
<affiliation confidence="0.996967">
IRCS — Institute for Research in Cognitive Science
University of Pennsylvania
</affiliation>
<address confidence="0.818627">
3401 Walnut Street, Suite 400A
Philadelphia PA 19104
</address>
<email confidence="0.999178">
strube@linc.cis.upenn.edu
</email>
<sectionHeader confidence="0.997394" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999978181818182">
I propose a model for determining the hearer&apos;s at-
tentional state which depends solely on a list of
salient discourse entities (S-list). The ordering
among the elements of the S-list covers also the
function of the backward-looking center in the cen-
tering model. The ranking criteria for the S-list
are based on the distinction between hearer-old and
hearer-new discourse entities and incorporate pref-
erences for inter- and intra-sentential anaphora. The
model is the basis for an algorithm which operates
incrementally, word by word.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999713973684211">
I propose a model for determining the hearer&apos;s at-
tentional state in understanding discourse. My pro-
posal is inspired by the centering model (Grosz
et al., 1983; 1995) and draws on the conclusions of
Strube &amp; Hahn&apos;s (1996) approach for the ranking of
the forward-looking center list for German. Their
approach has been proven as the point of departure
for a new model which is valid for English as well.
The use of the centering transitions in Brennan
et al.&apos;s (1987) algorithm prevents it from being ap-
plied incrementally (cf. Kehler (1997)). In my ap-
proach, I propose to replace the functions of the
backward-looking center and the centering transi-
tions by the order among the elements of the list of
salient discourse entities (S-list). The S-list rank-
ing criteria define a preference for hearer-old over
hearer-new discourse entities (Prince, 1981) gener-
alizing Strube &amp; Hahn&apos;s (1996) approach. Because
of these ranking criteria, I can account for the dif-
ference in salience between definite NPs (mostly
hearer-old) and indefinite NPs (mostly hearer-new).
The S-list is not a local data structure associ-
ated with individual utterances. The S-list rather
describes the attentional state of the hearer at any
given point in processing a discourse. The S-list is
generated incrementally, word by word, and used
immediately. Therefore, the S-list integrates in the
simplest manner preferences for inter- and intra-
sentential anaphora, making further specifications
for processing complex sentences unnecessary.
Section 2 describes the centering model as the
relevant background for my proposal. In Section 3,
I introduce my model, its only data structure, the
S-list, and the accompanying algorithm. In Section
4, I compare the results of my algorithm with the
results of the centering algorithm (Brennan et al.,
1987) with and without specifications for complex
sentences (Kameyama, 1998).
</bodyText>
<sectionHeader confidence="0.989084" genericHeader="method">
2 A Look Back: Centering
</sectionHeader>
<bodyText confidence="0.999951538461539">
The centering model describes the relation between
the focus of attention, the choices of referring ex-
pressions, and the perceived coherence of discourse.
The model has been motivated with evidence from
preferences for the antecedents of pronouns (Grosz
et al., 1983; 1995) and has been applied to pronoun
resolution (Brennan et al. (1987), inter alia, whose
interpretation differs from the original model).
The centering model itself consists of two con-
structs, the backward-looking center and the list
of forward-looking centers, and a few rules and
constraints. Each utterance Ui is assigned a list
of forward-looking centers, Cf (Ui), and a unique
backward-looking center, Cb(Ui). A ranking im-
posed on the elements of the Cf reflects the as-
sumption that the most highly ranked element of
C f (Ui) (the preferred center Cp(Ui)) is most likely
to be the Cb(Ui.4.1). The most highly ranked el-
ement of C f (Ui) that is realized in U2-F1 (i.e., is
associated with an expression that has a valid inter-
pretation in the underlying semantic representation)
is the Cb(Ui+i). Therefore, the ranking on the Cf
plays a crucial role in the model. Grosz et al. (1995)
and Brennan et al. (1987) use grammatical relations
to rank the Cf (i.e., subj obj ...) but state that
other factors might also play a role.
</bodyText>
<page confidence="0.982652">
1251
</page>
<bodyText confidence="0.9999148">
For their centering algorithm, Brennan et al.
(1987, henceforth BFP-algorithm) extend the notion
of centering transition relations, which hold across
adjacent utterances, to differentiate types of shift
(cf. Table 1 taken from Walker et al. (1994)).
</bodyText>
<equation confidence="0.976204">
Cb(U) = Cb(11,-1) Cb(U)
OR no Cb(U,--1) Cb(U2-1)
</equation>
<sectionHeader confidence="0.6159465" genericHeader="method">
CONTINUE SMOOTH-SHIFT
RETAIN ROUGH-SHIFT
</sectionHeader>
<tableCaption confidence="0.523322428571429">
Table 1: Transition Types
Brennan et al. (1987) modify the second of two
rules on center movement and realization which
were defined by Grosz et al. (1983; 1995):
Rule 1: If some element of C f (Ui_i) is realized as
a pronoun in U, then so is Cb(Ui).
Rule 2: Transition states are ordered. CONTINUE is
</tableCaption>
<bodyText confidence="0.6013405">
preferred to RETAIN is preferred to SMOOTH-
SHIFT is preferred to ROUGH-SHIFT.
The BFP-algorithm (cf. Walker et al. (1994)) con-
sists of three basic steps:
</bodyText>
<listItem confidence="0.9993338">
1. GENERATE possible Cb-Cf combinations.
2. FILTER by constraints, e.g., contra-indexing,
sortal predicates, centering rules and con-
straints.
3. RANK by transition orderings.
</listItem>
<bodyText confidence="0.996367230769231">
To illustrate this algorithm, we consider example (1)
(Brennan et al., 1987) which has two different final
utterances (1d) and (1 d&apos;). Utterance (1d) contains
one pronoun, utterance (id&apos;) two pronouns. We look
at the interpretation of (1d) and (1 d&apos;). After step 2,
the algorithm has produced two readings for each
variant which are rated by the corresponding tran-
sitions in step 3. In (1d), the pronoun &amp;quot;she&amp;quot; is
resolved to &amp;quot;her&amp;quot; (= Brennan) because the CON-
TINUE transition is ranked higher than SMOOTH-
SHIFT in the second reading. In (id&apos;), the pronoun
&amp;quot;she&amp;quot; is resolved to &amp;quot;Friedman&amp;quot; because SMOOTH-
SHIFT is preferred over ROUGH-SHIFT.
</bodyText>
<listItem confidence="0.993853">
(1) a. Brennan drives an Alfa Romeo.
b. She drives too fast.
c. Friedman races her on weekends.
d. She goes to Laguna Seca.
d.&apos; She often beats her.
</listItem>
<sectionHeader confidence="0.546171" genericHeader="method">
3 An Alternative to Centering
</sectionHeader>
<subsectionHeader confidence="0.876747">
3.1 The Model
</subsectionHeader>
<bodyText confidence="0.999932">
The realization and the structure of my model de-
parts significantly from the centering model:
</bodyText>
<listItem confidence="0.981360692307692">
• The model consists of one construct with one
operation: the list of salient discourse entities
(S-list) with an insertion operation.
• The S-list describes the attentional state of the
hearer at any given point in processing a dis-
course.
• The S-list contains some (not necessarily all)
discourse entities which are realized in the cur-
rent and the previous utterance.
• The elements of the S-list are ranked according
to their information status. The order among
the elements provides directly the preference
for the interpretation of anaphoric expressions.
</listItem>
<bodyText confidence="0.823535166666667">
In contrast to the centering model, my model does
not need a construct which looks back; it does not
need transitions and transition ranking criteria. In-
stead of using the Cb to account for local coherence,
in my model this is achieved by comparing the first
element of the S-list with the preceding state.
</bodyText>
<subsectionHeader confidence="0.999441">
3.2 S-List Ranking
</subsectionHeader>
<bodyText confidence="0.99994744">
Strube &amp; Hahn (1996) rank the Cf according to the
information status of discourse entities. I here gen-
eralize these ranking criteria by redefining them in
Prince&apos;s (1981; 1992) terms. I distinguish between
three different sets of expressions, hearer-old dis-
course entities (OLD), mediated discourse entities
(MED), and hearer-new discourse entities (NEW).
These sets consist of the elements of Prince&apos;s fa-
miliarity scale (Prince, 1981, p.245). OLD con-
sists of evoked (E) and unused (U) discourse entities
while NEW consists of brand-new (BN) discourse
entities. MED consists of inferrables (I), con-
taining inferrables (IC) and anchored brand-new
(BNA) discourse entities. These discourse entities
are discourse-new but mediated by some hearer-old
discourse entity (cf. Figure 1). I do not assume any
difference between the elements of each set with re-
spect to their information status. E.g., evoked and
unused discourse entities have the same information
status because both belong to OLD.
For an operationalization of Prince&apos;s terms, I stip-
ulate that evoked discourse entitites are co-referring
expressions (pronominal and nominal anaphora,
previously mentioned proper names, relative pro-
nouns, appositives). Unused discourse entities are
</bodyText>
<figure confidence="0.8842255">
Cb(U)
Cp(Ui)
Cb(U)
Cp(Ui)
</figure>
<page confidence="0.682616">
1252
</page>
<figureCaption confidence="0.999796">
Figure 1: S-list Ranking and Familiarity
</figureCaption>
<bodyText confidence="0.931246">
proper names and titles. In texts, brand-new proper
names are usually accompanied by a relative clause
or an appositive which relates them to the hearer&apos;s
knowledge. The corresponding discourse entity is
evoked only after this elaboration. Whenever these
linguistic devices are missing, proper names are
treated as unusedl . I restrict inferrables to the par-
ticular subset defined by Hahn et al. (1996). An-
chored brand-new discourse entities require that the
anchor is either evoked or unused.
I assume the following conventions for the rank-
ing constraints on the elements of the S-list. The
3-tuple (x, uttx, pos z) denotes a discourse entity x
which is evoked in utterance uttx at the text posi-
tion posx. With respect to any two discourse en-
tities (x, uttx , pas x) and (y, utty, posy), uttx and
utty specifying the current utterance Ui or the pre-
ceding utterance U2_1, I set up the following order-
ing constraints on elements in the S-list (Table 2)2.
For any state of the processor/hearer, the ordering
of discourse entities in the S-list that can be derived
from the ordering constraints (1) to (3) is denoted
by the precedence relation
(I) If x E OLD and y E MED, then x y.
If x E OLD and y E NEW, then x y.
If x E MED and y E NEW, then x y.
(2) If x, y E OLD, or x, y E MED, or x, y E NEW,
then if utt. utty, then x y,
if utt. = utty and pos. &lt; posy, then x y.
</bodyText>
<tableCaption confidence="0.964995">
Table 2: Ranking Constraints on the S-list
</tableCaption>
<bodyText confidence="0.943048555555556">
Summarizing Table 2, I state the following pref-
erence ranking for discourse entities in Ui and U2-1:
hearer-old discourse entities in U, hearer-old dis-
course entities in Ui_1, mediated discourse entities
in Ui, mediated discourse entities in Ui_i, hearer-
new discourse entities in U2, hearer-new discourse
entities in Ui_1. By making the distinction in (2)
&apos;For examples of brand-new proper names and their intro-
duction cf., e.g., the &amp;quot;obituaries&amp;quot; section of the New York Times.
2The relations &gt;- and = indicate that the utterance containing
x follows (&gt;-) the utterance containing y or that x and y are
elements of the same utterance (=).
between discourse entities in Ui and discourse enti-
ties in U2_1, I am able to deal with intra-sentential
anaphora. There is no need for further specifications
for complex sentences. A finer grained ordering is
achieved by ranking discourse entities within each
of the sets according to their text position.
</bodyText>
<subsectionHeader confidence="0.998793">
3.3 The Algorithm
</subsectionHeader>
<bodyText confidence="0.997034">
Anaphora resolution is performed with a simple
look-up in the S-list3. The elements of the S-list are
tested in the given order until one test succeeds. Just
after an anaphoric expression is resolved, the S-list
is updated. The algorithm processes a text from left
to right (the unit of processing is the word):
</bodyText>
<listItem confidence="0.993793375">
1. If a referring expression is encountered,
(a) if it is a pronoun, test the elements of the
S-list in the given order until the test suc-
ceeds4;
(b) update S-list; the position of the referring
expression under consideration is deter-
mined by the S-list-ranking criteria which
are used as an insertion algorithm.
</listItem>
<bodyText confidence="0.935786619047619">
2. If the analysis of utterance U5 is finished, re-
move all discourse entities from the S-list,
which are not realized in U.
The analysis for example (1) is given in Table 36.
I show only these steps which are of interest for the
computation of the S-list and the pronoun resolu-
tion. The preferences for pronouns (in bold font)
are given by the S-list immediately above them. The
pronoun &amp;quot;she&amp;quot; in (lb) is resolved to the first el-
ement of the S-list. When the pronoun &amp;quot;her&amp;quot; in
(lc) is encountered, FRIEDMAN is the first element
of the S-list since FRIEDMAN is unused and in the
current utterance. Because of binding restrictions,
&amp;quot;her&amp;quot; cannot be resolved to FRIEDMAN but to the
second element, BRENNAN. In both (1d) and (id&apos;)
the pronoun &amp;quot;she&amp;quot; is resolved to FRIEDMAN.
3The S-list consists of referring expressions which are spec-
ified for text position, agreement, sortal information, and infor-
mation status. Coordinated NPs are collected in a set. The S-
list does not contain predicative NPs, pleonastic &amp;quot;it&amp;quot;, and any
elements of direct speech enclosed in double quotes.
</bodyText>
<footnote confidence="0.949469125">
4The test for pronominal anaphora involves checking agree-
ment criteria, binding and sortal constraints.
5I here define that an utterance is a sentence.
6In the following Tables, discourse entities are represented
by SMALLCAPS, while the corresponding surface expression
appears on the right side of the colon. Discourse entitites are
annotated with their information status. An &amp;quot;e&amp;quot; indicates an
elliptical NP.
</footnote>
<page confidence="0.61188">
1253
</page>
<table confidence="0.999797625">
(la) Brennan drives an Alfa Romeo
S: [BRENNANu: Brennan,
ALFA ROMEOBN: Alfa Romeo]
(lb) She drives too fast.
S: [BRENNANE: she]
Friedman
S: [FRIEDmANu: Friedman, BRENNANE: she]
races her on weekends.
S: [FittEDmANu: Friedman, BRENNANE: her]
She drives to Laguna Seca.
S: [FRIEDMANE: she,
LAGUNA SECAu: Laguna Seca]
(ld&apos;) She
S: [FRIEDMANE: she, BRENNANE: her]
often beats her.
S: [FRIEDMANE: she, BRENNANE: her]
</table>
<tableCaption confidence="0.99303">
Table 3: Analysis for (1)
</tableCaption>
<bodyText confidence="0.9996295625">
The difference between my algorithm and the
BFP-algorithm becomes clearer when the unused
discourse entity &amp;quot;Friedman&amp;quot; is replaced by a brand-
new discourse entity, e.g., &amp;quot;a professional driver&amp;quot;7
(cf. example (2)). In the BFP-algorithm, the rank-
ing of the Cf-list depends on grammatical roles.
Hence, DRIVER is ranked higher than BRENNAN in
the Cft2c). In (2d), the pronoun &amp;quot;she&amp;quot; is resolved
to BRENNAN because of the preference for CON-
TINUE over RETAIN. In (2d&apos;), &amp;quot;she&amp;quot; is resolved to
DRIVER because SMOOTH-SHIFT is preferred over
ROUGH-SHIFT. In my algorithm, at the end of (2c)
the evoked phrase &amp;quot;her&amp;quot; is ranked higher than the
brand-new phrase &amp;quot;a professional driver&amp;quot; (cf. Ta-
ble 4). In both (2d) and (2d&apos;) the pronoun &amp;quot;she&amp;quot; is
resolved to BRENNAN.
</bodyText>
<listItem confidence="0.9952442">
(2) a. Brennan drives an Alfa Romeo.
b. She drives too fast.
c. A professional driver races her on weekends.
d. She goes to Laguna Seca.
(1.1 She often beats her.
</listItem>
<bodyText confidence="0.864206538461538">
Example (3)8 illustrates how the preferences for
intra- and inter-sentential anaphora interact with the
information status of discourse entitites (Table 5).
Sentence (3a) starts a new discourse segment. The
phrase &amp;quot;a judge&amp;quot; is brand-new. &amp;quot;Mr Curtis&amp;quot; is
mentioned several times before in the text, Hence,
71 owe this variant Andrew Kehler. —This example can mis-
direct readers because the phrase &amp;quot;a professional driver&amp;quot; is as-
signed the &amp;quot;default&amp;quot; gender masculine. Anyway, this example
— like the original example — seems not to be felicitous English
and has only illustrative character.
81n: The New York Tunes. Dec. 7, 1997, p.A48 (&amp;quot;Shot in
head, suspect goes free, then to college&amp;quot;).
</bodyText>
<table confidence="0.9978355">
Brennan drives an Alfa Romeo
S: [BRENNANu: Brennan,
ALFA ROMEOBN: Alfa Romeo]
She drives too fast.
S: [BRENNANE: she]
A professional driver
S: [BRENNANE: she, DRIVERBN: Driver]
races her on weekends.
S: [BRENNANE: her, DRivEREN: Driver]
She drives to Laguna Seca.
S: [BRENNANE: she,
LAGUNA SECAu: Laguna Seca]
(2d&apos;) She
S: [BRENNANE: she, DRivERBN: Driver]
often beats her.
S: [BRENNANE: she, DRIVERE: her]
</table>
<tableCaption confidence="0.994552">
Table 4: Analysis for (2)
</tableCaption>
<bodyText confidence="0.999509631578947">
the discourse entity CURTIS is evoked and ranked
higher than the discourse entity JUDGE. In the
next step, the ellipsis refers to JUDGE which is
evoked then. The nouns &amp;quot;request&amp;quot; and &amp;quot;prosecu-
tors&amp;quot; are brand-new9. The pronoun &amp;quot;he&amp;quot; and the
possessive pronoun &amp;quot;his&amp;quot; are resolved to CURTIS.
&amp;quot;Condition&amp;quot; is brand-new but anchored by the pos-
sessive pronoun. For (3b) and (3c) I show only
the steps immediately before the pronouns are re-
solved. In (3b) both &amp;quot;Mr Curtis&amp;quot; and &amp;quot;the judge&amp;quot;
are evoked. However, &amp;quot;Mr Curtis&amp;quot; is the left-most
evoked phrase in this sentence and therefore the
most preferred antecedent for the pronoun &amp;quot;him&amp;quot;.
For my experiments I restricted the length of the
S-list to five elements. Therefore &amp;quot;prosecutors&amp;quot; in
(3b) is not contained in the S-list. The discourse
entity SMIRGA is introduced in (3c). It becomes
evoked after the appositive. Hence SM1RGA is the
most preferred antecedent for the pronoun &amp;quot;he&amp;quot;.
</bodyText>
<listItem confidence="0.937475866666667">
(3) a. A judge ordered that Mr. Curtis be released, but
e agreed with a request from prosecutors that he
be re-examined each year to see if his condition
has improved.
b. But authorities lost contact with Mr. Curtis after
the Connecticut Supreme Court ruled in 1990
that the judge had erred, and that prosecutors
had no right to re-examine him.
c. John Smirga, the assistant state&apos;s attorney in
charge of the original case, said last week that
he always had doubts about the psychiatric re-
ports that said Mr. Curtis would never improve.
91 restrict inferrables to the cases specified by Hahn et al.
(1996). Therefore &amp;quot;prosecutors&amp;quot; is brand-new (cf. Prince
(1992) for a discussion of the form of inferrables).
</listItem>
<page confidence="0.960386">
1254
</page>
<table confidence="0.999861482758621">
A judge
S: [JuDGEBN: judge]
ordered that Mr. Curtis
S: [CuRTISE: Mr. Curtis, JuDGEBN: judge]
be released, but e
S: [CuRTISE: Mr. Curtis, JUDGEE: e]
agreed with a request
S: [CuRTISE: Mr. Curtis, JUDGEE: e, REQUESTBN: request]
from prosecutors
S: [CuRTISE: Mr. Curtis, JUDGEE: e, REQUESTBN: request, PROSECUTORSBN: prosecutors]
that he
S: [CuRTisE: he, JUDGEE: c, REQUESTBN: request, PRosEcuToRsBN: prosecutors]
be re-examined each year
S: [CuRT1sE: he, JUDGEE: E, REQUESTBN: request, PROSECuTORSBN: prosecutors, YEARBN: year]
to see if his
S: [CuRT1SE: his, JUDGEE: c, REQUESTBN: request, PRosEcuToRsBN: prosecutors, YEARBN: year]
condition
S: [CuRTisE: his, JUDGEE: E, CONDITIONBNA : condition, REQUESTBN: request, PRoSEcuTORSBN: prosec.]
has improved.
S: [CuRTBE: his, JUDGEE: c, CoNDITioNBNA : condition, REQUESTBN: request, PRosEcuToRsBN: prosec.]
But authorities lost contact with Mr. Curtis after the Connecticut Supreme Court ruled in 1990 that the judge had
erred, and that prosecutors had no right
S: [CuRTISE: his, CS COURTu: CS Court, JUDGEE: judge, CoNDITIONBNA : condition, AuTH.BN: auth.]
to re-examine him.
S: [CuRTISE: him, CS COURTu: CS Court, JUDGEE: judge, CONDITIONBNA : condition, AuTH.BN: auth.]
John Smirga, the assistant state&apos;s attorney in charge of the original case, said last week
S: [SmiRGAE: attorney, CAsEE: case, CuRTisE: him, CS COURTu: CS Court, JUDGEE: judge]
that he had doubts about the psychiatric reports that said Mr. Curtis would never improve.
S: [SmiRGAE: he, CAsEE: case, REPORTSE: reports, CURTISE: Mr. Curtis, DOUBTsBN: doubts]
</table>
<tableCaption confidence="0.94449">
Table 5: Analysis for (3)
</tableCaption>
<sectionHeader confidence="0.967178" genericHeader="method">
4 Some Empirical Data
</sectionHeader>
<bodyText confidence="0.999868551020408">
In the first experiment, I compare my algorithm with
the BFP-algorithm which was in a second experi-
ment extended by the constraints for complex sen-
tences as described by Kameyama (1998).
Method. I use the following guidelines for the
hand-simulated analysis (Walker, 1989). I do not as-
sume any world knowledge as part of the anaphora
resolution process. Only agreement criteria, bind-
ing and sortal constraints are applied. I do not ac-
count for false positives and error chains. Following
Walker (1989), a segment is defined as a paragraph
unless its first sentence has a pronoun in subject po-
sition or a pronoun where none of the preceding
sentence-internal noun phrases matches its syntactic
features. At the beginning of a segment, anaphora
resolution is preferentially performed within the
same utterance. My algorithm starts with an empty
S-list at the beginning of a segment.
The basic unit for which the centering data struc-
tures are generated is the utterance U. For the BFP-
algorithm, I define U as a simple sentence, a com-
plex sentence, or each full clause of a compound
sentence. Kameyama&apos;s (1998) intra-sentential cen-
tering operates at the clause level. While tensed
clauses are defined as utterances on their own, un-
tensed clauses are processed with the main clause,
so that the Cf-list of the main clause contains
the elements of the untensed embedded clause.
Kameyama distinguishes for tensed clauses further
between sequential and hierarchical centering. Ex-
cept for reported speech (embedded and inaccessi-
ble to the superordinate level), non-report comple-
ments, and relative clauses (both embedded but ac-
cessible to the superordinate level; less salient than
the higher levels), all other types of tensed clauses
build a chain of utterances on the same level.
According to the preference for inter-sentential
candidates in the centering model, I define the fol-
lowing anaphora resolution strategy for the BEP-
algorithm: (1) Test elements of Uj_1. (2) Test el-
ements of Ui left-to-right. (3) Test elements of
Cf (U2_2), Cf (U_3), ... In my algorithm steps (1)
and (2) fall together. (3) is performed using previ-
ous states of the system.
Results. The test set consisted of the beginnings
of three short stories by Hemingway (2785 words,
153 sentences) and three articles from the New
York Times (4546 words, 233 sentences). The re-
sults of my experiments are given in Table 6. The
</bodyText>
<page confidence="0.970851">
1255
</page>
<bodyText confidence="0.999973782608696">
first row gives the number of personal and posses-
sive pronouns. The remainder of the Table shows
the results for the BFP-algorithm, for the BFP-
algorithm extended by Kameyama&apos;s intra-sentential
specifications, and for my algorithm. The overall
error rate of each approach is given in the rows
marked with wrong. The rows marked with wrong
(strat.) give the numbers of errors directly produced
by the algorithms&apos; strategy, the rows marked with
wrong (ambig.) the number of analyses with am-
biguities generated by the BFP-algorithm (my ap-
proach does not generate ambiguities). The rows
marked with wrong (intra) give the number of er-
rors caused by (missing) specifications for intra-
sentential anaphora. Since my algorithm integrates
the specifications for intra-sentential anaphora, I
count these errors as strategic errors. The rows
marked with wrong (chain) give the numbers of er-
rors contained in error chains. The rows marked
with wrong (other) give the numbers of the remain-
ing errors (consisting of pronouns with split an-
tecedents, errors because of segment boundaries,
and missing specifications for event anaphora).
</bodyText>
<table confidence="0.99962419047619">
Hem. NYT E
Pron. and Poss. Pron. 274 302 576
Correct 189 231 420
Wrong 85 71 156
Wrong (strat.) 14 2 16
BFP-Algo. Wrong (ambig.) 9 15 24
Wrong (intra) 17 13 30
Wrong (chain) 29 32 61
Wrong (other) 16 9 25
Correct 193 245 438
Wrong 81 57 138
Wrong (strat.) 3 0 3
BFP/Kam. Wrong (ambig.) 17 8 25
Wrong (intra) 17 27 44
Wrong (chain) 29 15 44
Wrong (other) 15 7 22
Correct 217 275 492
Wrong 57 27 84
My Algo. Wrong (strat.) 21 12 33
Wrong (chain) 22 9 31
Wrong (other) 14 6 20
</table>
<tableCaption confidence="0.999215">
Table 6: Evaluation Results
</tableCaption>
<bodyText confidence="0.98466776">
Interpretation. The results of my experiments
showed not only that my algorithm performed bet-
ter than the centering approaches but also revealed
insight in the interaction between inter- and intra-
sentential preferences for anaphoric antecedents.
Kameyama&apos;s specifications reduce the complexity
in that the Cf-lists in general are shorter after split-
ting up a sentence into clauses. Therefore, the
BFP-algorithm combined with her specifications
has almost no strategic errors while the number of
ambiguities remains constant. But this benefit is
achieved at the expense of more errors caused by the
intra-sentential specifications. These errors occur in
cases like example (3), in which Kameyama&apos;s intra-
sentential strategy makes the correct antecedent less
salient, indicating that a clause-based approach is
too fine-grained and that the hierarchical syntactical
structure as assumed by Kameyama does not have a
great impact on anaphora resolution.
I noted, too, that the BFP-algorithm can gener-
ate ambiguous readings for Ui when the pronoun
in Ui does not co-specify the Cb(Ui_ ). In cases,
where the C1 (U_1) contains more than one possi-
ble antecedent for the pronoun, several ambiguous
readings with the same transitions are generated.
An examplem: There is no Cb(4a) because no ele-
ment of the preceding utterance is realized in (4a).
The pronoun &amp;quot;them&amp;quot; in (4b) co-specifies &amp;quot;deer&amp;quot; but
the BFP-algorithm generates two readings both of
which are marked by a RETAIN transition.
(4) a. Jim pulled the burlap sacks off the deer
b. and Liz looked at them.
In general, the strength of the centering model is
that it is possible to use the Cb(U1) as the most
preferred antecedent for a pronoun in U. In my
model this effect is achieved by the preference for
hearer-old discourse entities. Whenever this prefer-
ence is misleading both approaches give wrong re-
sults. Since the Cb is defined strictly local while
hearer-old discourse entities are defined global, my
model produces less errors. In my model the pref-
erence is available immediately while the BFP-
algorithm can use its preference not before the sec-
ond utterance has been processed. The more global
definition of hearer-old discourse entities leads also
to shorter error chains. — However, the test set is
too small to draw final conclusions, but at least for
the texts analyzed the preference for hearer-old dis-
course entities is more appropriate than the prefer-
ence given by the BFP- algorithm.
</bodyText>
<sectionHeader confidence="0.994328" genericHeader="method">
5 Comparison to Related Approaches
</sectionHeader>
<bodyText confidence="0.9859275">
Kameyama&apos;s (1998) version of centering also omits
the centering transitions. But she uses the Cb and
a ranking over simplified transitions preventing the
incremental application of her model.
</bodyText>
<reference confidence="0.780820666666667">
1°In: Ernest Hemingway. Up in Michigan. In. The Com-
plete Short Stories of Ernest Hemingway New York: Charles
Scribner&apos;s Sons, 1987, p.60.
</reference>
<page confidence="0.994858">
1256
</page>
<bodyText confidence="0.99993425">
The focus model (Sidner, 1983; Suri &amp; McCoy,
1994) accounts for evoked discourse entities explic-
itly because it uses the discourse focus, which is de-
termined by a successful anaphora resolution. In-
cremental processing is not a topic of these papers.
Even models which use salience measures for de-
termining the antecedents of pronoun use the con-
cept of evoked discourse entities. Hajieova et al.
(1992) assign the highest value to an evoked dis-
course entity. Also Lappin &amp; Leass (1994), who
give the subject of the current sentence the high-
est weight, have an implicit notion of evokedness.
The salience weight degrades from one sentence to
another by a factor of two which implies that a re-
peatedly mentioned discourse entity gets a higher
weight than a brand-new subject.
</bodyText>
<sectionHeader confidence="0.999747" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999620333333333">
In this paper, I proposed a model for determining
the hearer&apos;s attentional state which is based on the
distinction between hearer-old and hearer-new dis-
course entities. I showed that my model, though
it omits the backward-looking center and the cen-
tering transitions, does not lose any of the predic-
tive power of the centering model with respect to
anaphora resolution. In contrast to the centering
model, my model includes a treatment for intra-
sentential anaphora and is sufficiently well specified
to be applied to real texts. Its incremental character
seems to be an answer to the question Kehler (1997)
recently raised. Furthermore, it neither has the prob-
lem of inconsistency Kehler mentioned with respect
to the BFP-algorithm nor does it generate unneces-
sary ambiguities.
Future work will address whether the text posi-
tion, which is the weakest grammatical concept, is
sufficient for the order of the elements of the S-list
at the second layer of my ranking constraints. I will
also try to extend my model for the analysis of def-
inite noun phrases for which it is necessary to inte-
grate it into a more global model of discourse pro-
cessing.
Acknowledgments: This work has been funded
by a post-doctoral grant from DFG (Str 545/1-1)
and is supported by a post-doctoral fellowship
award from IRCS. I would like to thank Nobo Ko-
magata, Rashmi Prasad, and Matthew Stone who
commented on earlier drafts of this paper. I am
grateful for valuable comments by Barbara Grosz,
Udo Hahn, Aravind Joshi, Lauri Karttunen, Andrew
Kehler, Ellen Prince, and Bonnie Webber.
</bodyText>
<sectionHeader confidence="0.995788" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999807576271186">
Brennan, S. E., M. W. Friedman &amp; C. J. Pollard (1987). A cen-
tering approach to pronouns. In Proc. of the 25th Annual
Meeting of the Association for Computational Linguis-
tics; Stanford, Cal., 6-9 July 1987, pp. 155-162.
Grosz, B. J., A. K. Joshi &amp; S. Weinstein (1983). Providing
a unified account of definite noun phrases in discourse.
In Proc. of the 21&amp;quot; Annual Meeting of the Association
for Computational Linguistics; Cambridge, Mass., 15-
17 June 1983, pp. 44-50.
Grosz, B. J., A. K. Joshi &amp; S. Weinstein (1995). Centering:
A framework for modeling the local coherence of dis-
course. Computational Linguistics, 21(2):203-225.
Hahn, U., K. Markert &amp; M. Strube (1996). A conceptual rea-
soning approach to textual ellipsis. In Proc. of the 12th
European Conference on Artificial Intelligence (ECAI
&apos;96); Budapest, Hungary, 12-16 August 1996, pp. 572-
576. Chichester: John Wiley.
Hajitova, E., V. Kuboil &amp; P. Kubon (1992). Stock of shared
knowledge: A tool for solving pronominal anaphora. In
Proc. of the lilth Int. Conference on Computational Lin-
guistics; Nantes, France, 23-28 August 1992, Vol. 1, pp.
127-133.
Kameyama, M. (1998). Intrasentential centering: A case study.
In M. Walker, A. Joshi &amp; E. Prince (Eds.), Centering
Theory in Discourse, pp. 89-112. Oxford, U.K.: Oxford
Univ. Pr.
Kehler, A. (1997). Current theories of centering for pronoun
interpretation: A critical evaluation. Computational Lin-
guistics, 23(3):467-475.
Lappin, S. &amp; H. J. Leass (1994). An algorithm for pronom-
inal anaphora resolution. Computational Linguistics,
20(4):535-56I.
Prince, E. F. (1981). Toward a taxonomy of given-new informa-
tion. In P. Cole (Ed.), Radical Pragmatics, pp. 223-255.
New York, N.Y.: Academic Press.
Prince, E. F. (1992). The ZPG letter: Subjects, definiteness, and
information-status. In W. Mann &amp; S. Thompson (Eds.),
Discourse Description. Diverse Linguistic Analyses of a
Fund-Raising Text, pp. 295-325. Amsterdam: John Ben-
j amins.
Sidner, C. L. (1983). Focusing in the comprehension of definite
anaphora. In M. Brady &amp; R. Berwick (Eds.), Compu-
tational Models of Discourse, pp. 267-330. Cambridge,
Mass.: MIT Press.
Strube, M. &amp; U. Hahn (1996). Functional centering. In Proc. of
the 34th Annual Meeting of the Association for Compu-
tational Linguistics; Santa Cruz, Cal., 23-28 June 1996,
pp. 270-277.
Suri, L. Z. &amp; K. F. McCoy (1994). RAFT/RAPR and centering:
A comparison and discussion of problems related to pro-
cessing complex sentences. Computational Linguistics,
20(2):301-317.
Walker, M. A. (1989). Evaluating discourse processing algo-
rithms. In Proc. of the 27th Annual Meeting of the Asso-
ciation for Computational Linguistics; Vancouver, B. C.,
Canada, 26-29 June 1989, pp. 251-261.
Walker, M. A., M. Iida &amp; S. Cote (1994). Japanese discourse
and the process of centering. Computational Linguistics,
20(2):193-233.
</reference>
<page confidence="0.992285">
1257
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.933018">
<title confidence="0.99891">Never Look Back: An Alternative to Centering</title>
<author confidence="0.999977">Michael Strube</author>
<affiliation confidence="0.9994885">for Research in Cognitive Science University of Pennsylvania</affiliation>
<address confidence="0.9785895">3401 Walnut Street, Suite 400A Philadelphia PA 19104</address>
<email confidence="0.999613">strube@linc.cis.upenn.edu</email>
<abstract confidence="0.9979835">I propose a model for determining the hearer&apos;s attentional state which depends solely on a list of salient discourse entities (S-list). The ordering among the elements of the S-list covers also the of the center the centering model. The ranking criteria for the S-list based on the distinction between entities and incorporate preferences for interand intra-sentential anaphora. The model is the basis for an algorithm which operates incrementally, word by word.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>1°In: Ernest Hemingway. Up in Michigan. In. The Complete Short Stories of Ernest Hemingway New York: Charles Scribner&apos;s Sons,</title>
<date>1987</date>
<pages>60</pages>
<contexts>
<context position="1241" citStr="(1987)" startWordPosition="195" endWordPosition="195">or inter- and intra-sentential anaphora. The model is the basis for an algorithm which operates incrementally, word by word. 1 Introduction I propose a model for determining the hearer&apos;s attentional state in understanding discourse. My proposal is inspired by the centering model (Grosz et al., 1983; 1995) and draws on the conclusions of Strube &amp; Hahn&apos;s (1996) approach for the ranking of the forward-looking center list for German. Their approach has been proven as the point of departure for a new model which is valid for English as well. The use of the centering transitions in Brennan et al.&apos;s (1987) algorithm prevents it from being applied incrementally (cf. Kehler (1997)). In my approach, I propose to replace the functions of the backward-looking center and the centering transitions by the order among the elements of the list of salient discourse entities (S-list). The S-list ranking criteria define a preference for hearer-old over hearer-new discourse entities (Prince, 1981) generalizing Strube &amp; Hahn&apos;s (1996) approach. Because of these ranking criteria, I can account for the difference in salience between definite NPs (mostly hearer-old) and indefinite NPs (mostly hearer-new). The S-l</context>
<context position="3028" citStr="(1987)" startWordPosition="469" endWordPosition="469">cture, the S-list, and the accompanying algorithm. In Section 4, I compare the results of my algorithm with the results of the centering algorithm (Brennan et al., 1987) with and without specifications for complex sentences (Kameyama, 1998). 2 A Look Back: Centering The centering model describes the relation between the focus of attention, the choices of referring expressions, and the perceived coherence of discourse. The model has been motivated with evidence from preferences for the antecedents of pronouns (Grosz et al., 1983; 1995) and has been applied to pronoun resolution (Brennan et al. (1987), inter alia, whose interpretation differs from the original model). The centering model itself consists of two constructs, the backward-looking center and the list of forward-looking centers, and a few rules and constraints. Each utterance Ui is assigned a list of forward-looking centers, Cf (Ui), and a unique backward-looking center, Cb(Ui). A ranking imposed on the elements of the Cf reflects the assumption that the most highly ranked element of C f (Ui) (the preferred center Cp(Ui)) is most likely to be the Cb(Ui.4.1). The most highly ranked element of C f (Ui) that is realized in U2-F1 (i</context>
<context position="4379" citStr="(1987)" startWordPosition="690" endWordPosition="690">ranking on the Cf plays a crucial role in the model. Grosz et al. (1995) and Brennan et al. (1987) use grammatical relations to rank the Cf (i.e., subj obj ...) but state that other factors might also play a role. 1251 For their centering algorithm, Brennan et al. (1987, henceforth BFP-algorithm) extend the notion of centering transition relations, which hold across adjacent utterances, to differentiate types of shift (cf. Table 1 taken from Walker et al. (1994)). Cb(U) = Cb(11,-1) Cb(U) OR no Cb(U,--1) Cb(U2-1) CONTINUE SMOOTH-SHIFT RETAIN ROUGH-SHIFT Table 1: Transition Types Brennan et al. (1987) modify the second of two rules on center movement and realization which were defined by Grosz et al. (1983; 1995): Rule 1: If some element of C f (Ui_i) is realized as a pronoun in U, then so is Cb(Ui). Rule 2: Transition states are ordered. CONTINUE is preferred to RETAIN is preferred to SMOOTHSHIFT is preferred to ROUGH-SHIFT. The BFP-algorithm (cf. Walker et al. (1994)) consists of three basic steps: 1. GENERATE possible Cb-Cf combinations. 2. FILTER by constraints, e.g., contra-indexing, sortal predicates, centering rules and constraints. 3. RANK by transition orderings. To illustrate thi</context>
</contexts>
<marker>1987</marker>
<rawString>1°In: Ernest Hemingway. Up in Michigan. In. The Complete Short Stories of Ernest Hemingway New York: Charles Scribner&apos;s Sons, 1987, p.60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Brennan</author>
<author>M W Friedman</author>
<author>C J Pollard</author>
</authors>
<title>A centering approach to pronouns.</title>
<date>1987</date>
<booktitle>In Proc. of the 25th Annual Meeting of the Association for Computational Linguistics;</booktitle>
<pages>155--162</pages>
<location>Stanford, Cal.,</location>
<contexts>
<context position="2591" citStr="Brennan et al., 1987" startWordPosition="399" endWordPosition="402"> hearer at any given point in processing a discourse. The S-list is generated incrementally, word by word, and used immediately. Therefore, the S-list integrates in the simplest manner preferences for inter- and intrasentential anaphora, making further specifications for processing complex sentences unnecessary. Section 2 describes the centering model as the relevant background for my proposal. In Section 3, I introduce my model, its only data structure, the S-list, and the accompanying algorithm. In Section 4, I compare the results of my algorithm with the results of the centering algorithm (Brennan et al., 1987) with and without specifications for complex sentences (Kameyama, 1998). 2 A Look Back: Centering The centering model describes the relation between the focus of attention, the choices of referring expressions, and the perceived coherence of discourse. The model has been motivated with evidence from preferences for the antecedents of pronouns (Grosz et al., 1983; 1995) and has been applied to pronoun resolution (Brennan et al. (1987), inter alia, whose interpretation differs from the original model). The centering model itself consists of two constructs, the backward-looking center and the lis</context>
<context position="3871" citStr="Brennan et al. (1987)" startWordPosition="609" endWordPosition="612">ts. Each utterance Ui is assigned a list of forward-looking centers, Cf (Ui), and a unique backward-looking center, Cb(Ui). A ranking imposed on the elements of the Cf reflects the assumption that the most highly ranked element of C f (Ui) (the preferred center Cp(Ui)) is most likely to be the Cb(Ui.4.1). The most highly ranked element of C f (Ui) that is realized in U2-F1 (i.e., is associated with an expression that has a valid interpretation in the underlying semantic representation) is the Cb(Ui+i). Therefore, the ranking on the Cf plays a crucial role in the model. Grosz et al. (1995) and Brennan et al. (1987) use grammatical relations to rank the Cf (i.e., subj obj ...) but state that other factors might also play a role. 1251 For their centering algorithm, Brennan et al. (1987, henceforth BFP-algorithm) extend the notion of centering transition relations, which hold across adjacent utterances, to differentiate types of shift (cf. Table 1 taken from Walker et al. (1994)). Cb(U) = Cb(11,-1) Cb(U) OR no Cb(U,--1) Cb(U2-1) CONTINUE SMOOTH-SHIFT RETAIN ROUGH-SHIFT Table 1: Transition Types Brennan et al. (1987) modify the second of two rules on center movement and realization which were defined by Gro</context>
</contexts>
<marker>Brennan, Friedman, Pollard, 1987</marker>
<rawString>Brennan, S. E., M. W. Friedman &amp; C. J. Pollard (1987). A centering approach to pronouns. In Proc. of the 25th Annual Meeting of the Association for Computational Linguistics; Stanford, Cal., 6-9 July 1987, pp. 155-162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>A K Joshi</author>
<author>S Weinstein</author>
</authors>
<title>Providing a unified account of definite noun phrases in discourse.</title>
<date>1983</date>
<booktitle>In Proc. of the 21&amp;quot; Annual Meeting of the Association for Computational Linguistics;</booktitle>
<pages>44--50</pages>
<location>Cambridge, Mass.,</location>
<contexts>
<context position="934" citStr="Grosz et al., 1983" startWordPosition="138" endWordPosition="141"> list of salient discourse entities (S-list). The ordering among the elements of the S-list covers also the function of the backward-looking center in the centering model. The ranking criteria for the S-list are based on the distinction between hearer-old and hearer-new discourse entities and incorporate preferences for inter- and intra-sentential anaphora. The model is the basis for an algorithm which operates incrementally, word by word. 1 Introduction I propose a model for determining the hearer&apos;s attentional state in understanding discourse. My proposal is inspired by the centering model (Grosz et al., 1983; 1995) and draws on the conclusions of Strube &amp; Hahn&apos;s (1996) approach for the ranking of the forward-looking center list for German. Their approach has been proven as the point of departure for a new model which is valid for English as well. The use of the centering transitions in Brennan et al.&apos;s (1987) algorithm prevents it from being applied incrementally (cf. Kehler (1997)). In my approach, I propose to replace the functions of the backward-looking center and the centering transitions by the order among the elements of the list of salient discourse entities (S-list). The S-list ranking c</context>
<context position="2955" citStr="Grosz et al., 1983" startWordPosition="454" endWordPosition="457">nt background for my proposal. In Section 3, I introduce my model, its only data structure, the S-list, and the accompanying algorithm. In Section 4, I compare the results of my algorithm with the results of the centering algorithm (Brennan et al., 1987) with and without specifications for complex sentences (Kameyama, 1998). 2 A Look Back: Centering The centering model describes the relation between the focus of attention, the choices of referring expressions, and the perceived coherence of discourse. The model has been motivated with evidence from preferences for the antecedents of pronouns (Grosz et al., 1983; 1995) and has been applied to pronoun resolution (Brennan et al. (1987), inter alia, whose interpretation differs from the original model). The centering model itself consists of two constructs, the backward-looking center and the list of forward-looking centers, and a few rules and constraints. Each utterance Ui is assigned a list of forward-looking centers, Cf (Ui), and a unique backward-looking center, Cb(Ui). A ranking imposed on the elements of the Cf reflects the assumption that the most highly ranked element of C f (Ui) (the preferred center Cp(Ui)) is most likely to be the Cb(Ui.4.1)</context>
<context position="4486" citStr="Grosz et al. (1983" startWordPosition="706" endWordPosition="709">87) use grammatical relations to rank the Cf (i.e., subj obj ...) but state that other factors might also play a role. 1251 For their centering algorithm, Brennan et al. (1987, henceforth BFP-algorithm) extend the notion of centering transition relations, which hold across adjacent utterances, to differentiate types of shift (cf. Table 1 taken from Walker et al. (1994)). Cb(U) = Cb(11,-1) Cb(U) OR no Cb(U,--1) Cb(U2-1) CONTINUE SMOOTH-SHIFT RETAIN ROUGH-SHIFT Table 1: Transition Types Brennan et al. (1987) modify the second of two rules on center movement and realization which were defined by Grosz et al. (1983; 1995): Rule 1: If some element of C f (Ui_i) is realized as a pronoun in U, then so is Cb(Ui). Rule 2: Transition states are ordered. CONTINUE is preferred to RETAIN is preferred to SMOOTHSHIFT is preferred to ROUGH-SHIFT. The BFP-algorithm (cf. Walker et al. (1994)) consists of three basic steps: 1. GENERATE possible Cb-Cf combinations. 2. FILTER by constraints, e.g., contra-indexing, sortal predicates, centering rules and constraints. 3. RANK by transition orderings. To illustrate this algorithm, we consider example (1) (Brennan et al., 1987) which has two different final utterances (1d) a</context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1983</marker>
<rawString>Grosz, B. J., A. K. Joshi &amp; S. Weinstein (1983). Providing a unified account of definite noun phrases in discourse. In Proc. of the 21&amp;quot; Annual Meeting of the Association for Computational Linguistics; Cambridge, Mass., 15-17 June 1983, pp. 44-50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>A K Joshi</author>
<author>S Weinstein</author>
</authors>
<title>Centering: A framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--2</pages>
<contexts>
<context position="3845" citStr="Grosz et al. (1995)" startWordPosition="604" endWordPosition="607"> few rules and constraints. Each utterance Ui is assigned a list of forward-looking centers, Cf (Ui), and a unique backward-looking center, Cb(Ui). A ranking imposed on the elements of the Cf reflects the assumption that the most highly ranked element of C f (Ui) (the preferred center Cp(Ui)) is most likely to be the Cb(Ui.4.1). The most highly ranked element of C f (Ui) that is realized in U2-F1 (i.e., is associated with an expression that has a valid interpretation in the underlying semantic representation) is the Cb(Ui+i). Therefore, the ranking on the Cf plays a crucial role in the model. Grosz et al. (1995) and Brennan et al. (1987) use grammatical relations to rank the Cf (i.e., subj obj ...) but state that other factors might also play a role. 1251 For their centering algorithm, Brennan et al. (1987, henceforth BFP-algorithm) extend the notion of centering transition relations, which hold across adjacent utterances, to differentiate types of shift (cf. Table 1 taken from Walker et al. (1994)). Cb(U) = Cb(11,-1) Cb(U) OR no Cb(U,--1) Cb(U2-1) CONTINUE SMOOTH-SHIFT RETAIN ROUGH-SHIFT Table 1: Transition Types Brennan et al. (1987) modify the second of two rules on center movement and realization</context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>Grosz, B. J., A. K. Joshi &amp; S. Weinstein (1995). Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203-225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Hahn</author>
<author>K Markert</author>
<author>M Strube</author>
</authors>
<title>A conceptual reasoning approach to textual ellipsis.</title>
<date>1996</date>
<booktitle>In Proc. of the 12th European Conference on Artificial Intelligence (ECAI &apos;96);</booktitle>
<pages>572--576</pages>
<publisher>Chichester: John Wiley.</publisher>
<location>Budapest,</location>
<contexts>
<context position="8486" citStr="Hahn et al. (1996)" startWordPosition="1343" endWordPosition="1346">expressions (pronominal and nominal anaphora, previously mentioned proper names, relative pronouns, appositives). Unused discourse entities are Cb(U) Cp(Ui) Cb(U) Cp(Ui) 1252 Figure 1: S-list Ranking and Familiarity proper names and titles. In texts, brand-new proper names are usually accompanied by a relative clause or an appositive which relates them to the hearer&apos;s knowledge. The corresponding discourse entity is evoked only after this elaboration. Whenever these linguistic devices are missing, proper names are treated as unusedl . I restrict inferrables to the particular subset defined by Hahn et al. (1996). Anchored brand-new discourse entities require that the anchor is either evoked or unused. I assume the following conventions for the ranking constraints on the elements of the S-list. The 3-tuple (x, uttx, pos z) denotes a discourse entity x which is evoked in utterance uttx at the text position posx. With respect to any two discourse entities (x, uttx , pas x) and (y, utty, posy), uttx and utty specifying the current utterance Ui or the preceding utterance U2_1, I set up the following ordering constraints on elements in the S-list (Table 2)2. For any state of the processor/hearer, the order</context>
<context position="16563" citStr="Hahn et al. (1996)" startWordPosition="2710" endWordPosition="2713">ronoun &amp;quot;he&amp;quot;. (3) a. A judge ordered that Mr. Curtis be released, but e agreed with a request from prosecutors that he be re-examined each year to see if his condition has improved. b. But authorities lost contact with Mr. Curtis after the Connecticut Supreme Court ruled in 1990 that the judge had erred, and that prosecutors had no right to re-examine him. c. John Smirga, the assistant state&apos;s attorney in charge of the original case, said last week that he always had doubts about the psychiatric reports that said Mr. Curtis would never improve. 91 restrict inferrables to the cases specified by Hahn et al. (1996). Therefore &amp;quot;prosecutors&amp;quot; is brand-new (cf. Prince (1992) for a discussion of the form of inferrables). 1254 A judge S: [JuDGEBN: judge] ordered that Mr. Curtis S: [CuRTISE: Mr. Curtis, JuDGEBN: judge] be released, but e S: [CuRTISE: Mr. Curtis, JUDGEE: e] agreed with a request S: [CuRTISE: Mr. Curtis, JUDGEE: e, REQUESTBN: request] from prosecutors S: [CuRTISE: Mr. Curtis, JUDGEE: e, REQUESTBN: request, PROSECUTORSBN: prosecutors] that he S: [CuRTisE: he, JUDGEE: c, REQUESTBN: request, PRosEcuToRsBN: prosecutors] be re-examined each year S: [CuRT1sE: he, JUDGEE: E, REQUESTBN: request, PROSECu</context>
</contexts>
<marker>Hahn, Markert, Strube, 1996</marker>
<rawString>Hahn, U., K. Markert &amp; M. Strube (1996). A conceptual reasoning approach to textual ellipsis. In Proc. of the 12th European Conference on Artificial Intelligence (ECAI &apos;96); Budapest, Hungary, 12-16 August 1996, pp. 572-576. Chichester: John Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hajitova</author>
<author>V Kuboil</author>
<author>P Kubon</author>
</authors>
<title>Stock of shared knowledge: A tool for solving pronominal anaphora.</title>
<date>1992</date>
<booktitle>In Proc. of the lilth Int. Conference on Computational Linguistics;</booktitle>
<volume>1</volume>
<pages>127--133</pages>
<location>Nantes,</location>
<marker>Hajitova, Kuboil, Kubon, 1992</marker>
<rawString>Hajitova, E., V. Kuboil &amp; P. Kubon (1992). Stock of shared knowledge: A tool for solving pronominal anaphora. In Proc. of the lilth Int. Conference on Computational Linguistics; Nantes, France, 23-28 August 1992, Vol. 1, pp. 127-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kameyama</author>
</authors>
<title>Intrasentential centering: A case study.</title>
<date>1998</date>
<booktitle>In</booktitle>
<pages>89--112</pages>
<publisher>Univ. Pr.</publisher>
<location>Oxford, U.K.: Oxford</location>
<contexts>
<context position="2662" citStr="Kameyama, 1998" startWordPosition="410" endWordPosition="411"> incrementally, word by word, and used immediately. Therefore, the S-list integrates in the simplest manner preferences for inter- and intrasentential anaphora, making further specifications for processing complex sentences unnecessary. Section 2 describes the centering model as the relevant background for my proposal. In Section 3, I introduce my model, its only data structure, the S-list, and the accompanying algorithm. In Section 4, I compare the results of my algorithm with the results of the centering algorithm (Brennan et al., 1987) with and without specifications for complex sentences (Kameyama, 1998). 2 A Look Back: Centering The centering model describes the relation between the focus of attention, the choices of referring expressions, and the perceived coherence of discourse. The model has been motivated with evidence from preferences for the antecedents of pronouns (Grosz et al., 1983; 1995) and has been applied to pronoun resolution (Brennan et al. (1987), inter alia, whose interpretation differs from the original model). The centering model itself consists of two constructs, the backward-looking center and the list of forward-looking centers, and a few rules and constraints. Each utt</context>
<context position="18479" citStr="Kameyama (1998)" startWordPosition="2997" endWordPosition="2998">ondition, AuTH.BN: auth.] John Smirga, the assistant state&apos;s attorney in charge of the original case, said last week S: [SmiRGAE: attorney, CAsEE: case, CuRTisE: him, CS COURTu: CS Court, JUDGEE: judge] that he had doubts about the psychiatric reports that said Mr. Curtis would never improve. S: [SmiRGAE: he, CAsEE: case, REPORTSE: reports, CURTISE: Mr. Curtis, DOUBTsBN: doubts] Table 5: Analysis for (3) 4 Some Empirical Data In the first experiment, I compare my algorithm with the BFP-algorithm which was in a second experiment extended by the constraints for complex sentences as described by Kameyama (1998). Method. I use the following guidelines for the hand-simulated analysis (Walker, 1989). I do not assume any world knowledge as part of the anaphora resolution process. Only agreement criteria, binding and sortal constraints are applied. I do not account for false positives and error chains. Following Walker (1989), a segment is defined as a paragraph unless its first sentence has a pronoun in subject position or a pronoun where none of the preceding sentence-internal noun phrases matches its syntactic features. At the beginning of a segment, anaphora resolution is preferentially performed wit</context>
</contexts>
<marker>Kameyama, 1998</marker>
<rawString>Kameyama, M. (1998). Intrasentential centering: A case study. In M. Walker, A. Joshi &amp; E. Prince (Eds.), Centering Theory in Discourse, pp. 89-112. Oxford, U.K.: Oxford Univ. Pr.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kehler</author>
</authors>
<title>Current theories of centering for pronoun interpretation: A critical evaluation.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--3</pages>
<contexts>
<context position="1315" citStr="Kehler (1997)" startWordPosition="205" endWordPosition="206"> an algorithm which operates incrementally, word by word. 1 Introduction I propose a model for determining the hearer&apos;s attentional state in understanding discourse. My proposal is inspired by the centering model (Grosz et al., 1983; 1995) and draws on the conclusions of Strube &amp; Hahn&apos;s (1996) approach for the ranking of the forward-looking center list for German. Their approach has been proven as the point of departure for a new model which is valid for English as well. The use of the centering transitions in Brennan et al.&apos;s (1987) algorithm prevents it from being applied incrementally (cf. Kehler (1997)). In my approach, I propose to replace the functions of the backward-looking center and the centering transitions by the order among the elements of the list of salient discourse entities (S-list). The S-list ranking criteria define a preference for hearer-old over hearer-new discourse entities (Prince, 1981) generalizing Strube &amp; Hahn&apos;s (1996) approach. Because of these ranking criteria, I can account for the difference in salience between definite NPs (mostly hearer-old) and indefinite NPs (mostly hearer-new). The S-list is not a local data structure associated with individual utterances. T</context>
</contexts>
<marker>Kehler, 1997</marker>
<rawString>Kehler, A. (1997). Current theories of centering for pronoun interpretation: A critical evaluation. Computational Linguistics, 23(3):467-475.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lappin</author>
<author>H J Leass</author>
</authors>
<title>An algorithm for pronominal anaphora resolution.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--4</pages>
<marker>Lappin, Leass, 1994</marker>
<rawString>Lappin, S. &amp; H. J. Leass (1994). An algorithm for pronominal anaphora resolution. Computational Linguistics, 20(4):535-56I.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Prince</author>
</authors>
<title>Toward a taxonomy of given-new information.</title>
<date>1981</date>
<booktitle>In P. Cole (Ed.), Radical Pragmatics,</booktitle>
<pages>223--255</pages>
<publisher>Academic Press.</publisher>
<location>New York, N.Y.:</location>
<contexts>
<context position="1626" citStr="Prince, 1981" startWordPosition="254" endWordPosition="255"> ranking of the forward-looking center list for German. Their approach has been proven as the point of departure for a new model which is valid for English as well. The use of the centering transitions in Brennan et al.&apos;s (1987) algorithm prevents it from being applied incrementally (cf. Kehler (1997)). In my approach, I propose to replace the functions of the backward-looking center and the centering transitions by the order among the elements of the list of salient discourse entities (S-list). The S-list ranking criteria define a preference for hearer-old over hearer-new discourse entities (Prince, 1981) generalizing Strube &amp; Hahn&apos;s (1996) approach. Because of these ranking criteria, I can account for the difference in salience between definite NPs (mostly hearer-old) and indefinite NPs (mostly hearer-new). The S-list is not a local data structure associated with individual utterances. The S-list rather describes the attentional state of the hearer at any given point in processing a discourse. The S-list is generated incrementally, word by word, and used immediately. Therefore, the S-list integrates in the simplest manner preferences for inter- and intrasentential anaphora, making further spe</context>
<context position="7206" citStr="Prince, 1981" startWordPosition="1154" endWordPosition="1155">a. Instead of using the Cb to account for local coherence, in my model this is achieved by comparing the first element of the S-list with the preceding state. 3.2 S-List Ranking Strube &amp; Hahn (1996) rank the Cf according to the information status of discourse entities. I here generalize these ranking criteria by redefining them in Prince&apos;s (1981; 1992) terms. I distinguish between three different sets of expressions, hearer-old discourse entities (OLD), mediated discourse entities (MED), and hearer-new discourse entities (NEW). These sets consist of the elements of Prince&apos;s familiarity scale (Prince, 1981, p.245). OLD consists of evoked (E) and unused (U) discourse entities while NEW consists of brand-new (BN) discourse entities. MED consists of inferrables (I), containing inferrables (IC) and anchored brand-new (BNA) discourse entities. These discourse entities are discourse-new but mediated by some hearer-old discourse entity (cf. Figure 1). I do not assume any difference between the elements of each set with respect to their information status. E.g., evoked and unused discourse entities have the same information status because both belong to OLD. For an operationalization of Prince&apos;s terms,</context>
</contexts>
<marker>Prince, 1981</marker>
<rawString>Prince, E. F. (1981). Toward a taxonomy of given-new information. In P. Cole (Ed.), Radical Pragmatics, pp. 223-255. New York, N.Y.: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Prince</author>
</authors>
<title>The ZPG letter: Subjects, definiteness, and information-status.</title>
<date>1992</date>
<booktitle>In W. Mann &amp; S. Thompson (Eds.), Discourse Description. Diverse Linguistic Analyses of a Fund-Raising Text,</booktitle>
<pages>295--325</pages>
<location>Amsterdam: John Benj amins.</location>
<contexts>
<context position="16620" citStr="Prince (1992)" startWordPosition="2719" endWordPosition="2720">d, but e agreed with a request from prosecutors that he be re-examined each year to see if his condition has improved. b. But authorities lost contact with Mr. Curtis after the Connecticut Supreme Court ruled in 1990 that the judge had erred, and that prosecutors had no right to re-examine him. c. John Smirga, the assistant state&apos;s attorney in charge of the original case, said last week that he always had doubts about the psychiatric reports that said Mr. Curtis would never improve. 91 restrict inferrables to the cases specified by Hahn et al. (1996). Therefore &amp;quot;prosecutors&amp;quot; is brand-new (cf. Prince (1992) for a discussion of the form of inferrables). 1254 A judge S: [JuDGEBN: judge] ordered that Mr. Curtis S: [CuRTISE: Mr. Curtis, JuDGEBN: judge] be released, but e S: [CuRTISE: Mr. Curtis, JUDGEE: e] agreed with a request S: [CuRTISE: Mr. Curtis, JUDGEE: e, REQUESTBN: request] from prosecutors S: [CuRTISE: Mr. Curtis, JUDGEE: e, REQUESTBN: request, PROSECUTORSBN: prosecutors] that he S: [CuRTisE: he, JUDGEE: c, REQUESTBN: request, PRosEcuToRsBN: prosecutors] be re-examined each year S: [CuRT1sE: he, JUDGEE: E, REQUESTBN: request, PROSECuTORSBN: prosecutors, YEARBN: year] to see if his S: [CuRT</context>
</contexts>
<marker>Prince, 1992</marker>
<rawString>Prince, E. F. (1992). The ZPG letter: Subjects, definiteness, and information-status. In W. Mann &amp; S. Thompson (Eds.), Discourse Description. Diverse Linguistic Analyses of a Fund-Raising Text, pp. 295-325. Amsterdam: John Benj amins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L Sidner</author>
</authors>
<title>Focusing in the comprehension of definite anaphora.</title>
<date>1983</date>
<journal>In</journal>
<booktitle>Computational Models of Discourse,</booktitle>
<pages>267--330</pages>
<publisher>MIT Press.</publisher>
<location>Cambridge, Mass.:</location>
<marker>Sidner, 1983</marker>
<rawString>Sidner, C. L. (1983). Focusing in the comprehension of definite anaphora. In M. Brady &amp; R. Berwick (Eds.), Computational Models of Discourse, pp. 267-330. Cambridge, Mass.: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Strube</author>
<author>U Hahn</author>
</authors>
<title>Functional centering.</title>
<date>1996</date>
<booktitle>In Proc. of the 34th Annual Meeting of the Association for Computational Linguistics;</booktitle>
<pages>270--277</pages>
<location>Santa Cruz, Cal.,</location>
<contexts>
<context position="6792" citStr="Strube &amp; Hahn (1996)" startWordPosition="1091" endWordPosition="1094">l) discourse entities which are realized in the current and the previous utterance. • The elements of the S-list are ranked according to their information status. The order among the elements provides directly the preference for the interpretation of anaphoric expressions. In contrast to the centering model, my model does not need a construct which looks back; it does not need transitions and transition ranking criteria. Instead of using the Cb to account for local coherence, in my model this is achieved by comparing the first element of the S-list with the preceding state. 3.2 S-List Ranking Strube &amp; Hahn (1996) rank the Cf according to the information status of discourse entities. I here generalize these ranking criteria by redefining them in Prince&apos;s (1981; 1992) terms. I distinguish between three different sets of expressions, hearer-old discourse entities (OLD), mediated discourse entities (MED), and hearer-new discourse entities (NEW). These sets consist of the elements of Prince&apos;s familiarity scale (Prince, 1981, p.245). OLD consists of evoked (E) and unused (U) discourse entities while NEW consists of brand-new (BN) discourse entities. MED consists of inferrables (I), containing inferrables (I</context>
</contexts>
<marker>Strube, Hahn, 1996</marker>
<rawString>Strube, M. &amp; U. Hahn (1996). Functional centering. In Proc. of the 34th Annual Meeting of the Association for Computational Linguistics; Santa Cruz, Cal., 23-28 June 1996, pp. 270-277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Z Suri</author>
<author>K F McCoy</author>
</authors>
<title>RAFT/RAPR and centering: A comparison and discussion of problems related to processing complex sentences.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--2</pages>
<marker>Suri, McCoy, 1994</marker>
<rawString>Suri, L. Z. &amp; K. F. McCoy (1994). RAFT/RAPR and centering: A comparison and discussion of problems related to processing complex sentences. Computational Linguistics, 20(2):301-317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Walker</author>
</authors>
<title>Evaluating discourse processing algorithms.</title>
<date>1989</date>
<booktitle>In Proc. of the 27th Annual Meeting of the Association for Computational Linguistics;</booktitle>
<pages>251--261</pages>
<location>Vancouver, B. C.,</location>
<contexts>
<context position="18566" citStr="Walker, 1989" startWordPosition="3009" endWordPosition="3010">iginal case, said last week S: [SmiRGAE: attorney, CAsEE: case, CuRTisE: him, CS COURTu: CS Court, JUDGEE: judge] that he had doubts about the psychiatric reports that said Mr. Curtis would never improve. S: [SmiRGAE: he, CAsEE: case, REPORTSE: reports, CURTISE: Mr. Curtis, DOUBTsBN: doubts] Table 5: Analysis for (3) 4 Some Empirical Data In the first experiment, I compare my algorithm with the BFP-algorithm which was in a second experiment extended by the constraints for complex sentences as described by Kameyama (1998). Method. I use the following guidelines for the hand-simulated analysis (Walker, 1989). I do not assume any world knowledge as part of the anaphora resolution process. Only agreement criteria, binding and sortal constraints are applied. I do not account for false positives and error chains. Following Walker (1989), a segment is defined as a paragraph unless its first sentence has a pronoun in subject position or a pronoun where none of the preceding sentence-internal noun phrases matches its syntactic features. At the beginning of a segment, anaphora resolution is preferentially performed within the same utterance. My algorithm starts with an empty S-list at the beginning of a </context>
</contexts>
<marker>Walker, 1989</marker>
<rawString>Walker, M. A. (1989). Evaluating discourse processing algorithms. In Proc. of the 27th Annual Meeting of the Association for Computational Linguistics; Vancouver, B. C., Canada, 26-29 June 1989, pp. 251-261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Walker</author>
<author>M Iida</author>
<author>S Cote</author>
</authors>
<title>Japanese discourse and the process of centering.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--2</pages>
<contexts>
<context position="4239" citStr="Walker et al. (1994)" startWordPosition="667" endWordPosition="670"> U2-F1 (i.e., is associated with an expression that has a valid interpretation in the underlying semantic representation) is the Cb(Ui+i). Therefore, the ranking on the Cf plays a crucial role in the model. Grosz et al. (1995) and Brennan et al. (1987) use grammatical relations to rank the Cf (i.e., subj obj ...) but state that other factors might also play a role. 1251 For their centering algorithm, Brennan et al. (1987, henceforth BFP-algorithm) extend the notion of centering transition relations, which hold across adjacent utterances, to differentiate types of shift (cf. Table 1 taken from Walker et al. (1994)). Cb(U) = Cb(11,-1) Cb(U) OR no Cb(U,--1) Cb(U2-1) CONTINUE SMOOTH-SHIFT RETAIN ROUGH-SHIFT Table 1: Transition Types Brennan et al. (1987) modify the second of two rules on center movement and realization which were defined by Grosz et al. (1983; 1995): Rule 1: If some element of C f (Ui_i) is realized as a pronoun in U, then so is Cb(Ui). Rule 2: Transition states are ordered. CONTINUE is preferred to RETAIN is preferred to SMOOTHSHIFT is preferred to ROUGH-SHIFT. The BFP-algorithm (cf. Walker et al. (1994)) consists of three basic steps: 1. GENERATE possible Cb-Cf combinations. 2. FILTER b</context>
</contexts>
<marker>Walker, Iida, Cote, 1994</marker>
<rawString>Walker, M. A., M. Iida &amp; S. Cote (1994). Japanese discourse and the process of centering. Computational Linguistics, 20(2):193-233.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>