<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000017">
<title confidence="0.988734">
The Hidden TAG Model: Synchronous Grammars for Parsing
Resource-Poor Languages
</title>
<author confidence="0.998305">
David Chiang*
</author>
<affiliation confidence="0.867182333333333">
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
</affiliation>
<address confidence="0.740361">
Marina del Rey, CA 90292, USA
</address>
<email confidence="0.999189">
chiang@isi.edu
</email>
<author confidence="0.996547">
Owen Rambow
</author>
<affiliation confidence="0.9939755">
Center for Computational Learning Systems
Columbia University
</affiliation>
<address confidence="0.908037">
475 Riverside Dr., Suite 850
New York, NY, USA
</address>
<email confidence="0.999795">
rambow@cs.columbia.edu
</email>
<sectionHeader confidence="0.993923" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997909647058823">
This paper discusses a novel probabilis-
tic synchronous TAG formalism, syn-
chronous Tree Substitution Grammar with
sister adjunction (TSG+SA). We use it
to parse a language for which there is
no training data, by leveraging off a sec-
ond, related language for which there is
abundant training data. The grammar for
the resource-rich side is automatically ex-
tracted from a treebank; the grammar on
the resource-poor side and the synchro-
nization are created by handwritten rules.
Our approach thus represents a combina-
tion of grammar-based and empirical nat-
ural language processing. We discuss the
approach using the example of Levantine
Arabic and Standard Arabic.
</bodyText>
<sectionHeader confidence="0.9671515" genericHeader="keywords">
1 Parsing Arabic Dialects and Tree
Adjoining Grammar
</sectionHeader>
<bodyText confidence="0.995902574074074">
The Arabic language is a collection of spoken
dialects and a standard written language. The
standard written language is the same throughout
the Arab world, Modern Standard Arabic (MSA),
which is also used in some scripted spoken com-
munication (news casts, parliamentary debates).
It is based on Classical Arabic and is not a na-
tive language of any Arabic speaking people, i.e.,
children do not learn it from their parents but in
school. Thus most native speakers of Arabic are
unable to produce sustained spontaneous MSA.
The dialects show phonological, morphological,
lexical, and syntactic differences comparable to
*This work was primarily carried out while the first au-
thor was at the University of Maryland Institute for Advanced
Computer Studies.
those among the Romance languages. They vary
not only along a geographical continuum but also
with other sociolinguistic variables such as the ur-
ban/rural/Bedouin dimension.
The multidialectal situation has important neg-
ative consequences for Arabic natural language
processing (NLP): since the spoken dialects are
not officially written and do not have standard or-
thography, it is very costly to obtain adequate cor-
pora, even unannotated corpora, to use for train-
ing NLP tools such as parsers. Furthermore, there
are almost no parallel corpora involving one di-
alect and MSA.
The question thus arises how to create a statisti-
cal parser for an Arabic dialect, when statistical
parsers are typically trained on large corpora of
parse trees. We present one solution to this prob-
lem, based on the assumption that it is easier to
manually create new resources that relate a dialect
to MSA (lexicon and grammar) than it is to man-
ually create syntactically annotated corpora in the
dialect. In this paper, we deal with Levantine Ara-
bic (LA). Our approach does not assume the exis-
tence of any annotated LA corpus (except for de-
velopment and testing), nor of a parallel LA-MSA
corpus.
The approach described in this paper uses a spe-
cial parameterization of stochastic synchronous
TAG (Shieber, 1994) which we call a “hidden TAG
model.” This model couples a model of MSA
trees, learned from the Arabic Treebank, with a
model of MSA-LA translation, which is initial-
ized by hand and then trained in an unsupervised
fashion. Parsing new LA sentences then entails si-
multaneously building a forest of MSA trees and
the corresponding forest of LA trees. Our imple-
mentation uses an extension of our monolingual
parser (Chiang, 2000) based on tree-substitution
</bodyText>
<page confidence="0.832855">
1
</page>
<bodyText confidence="0.89980824">
Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 1–8,
Sydney, July 2006. c�2006 Association for Computational Linguistics
grammar with sister adjunction (TSG+SA).
The main contributions of this paper are as fol-
lows:
1. We introduce the novel concept of a hidden
TAG model.
2. We use this model to combine statistical ap-
proaches with grammar engineering (specif-
ically motivated from the linguistic facts).
Our approach thus exemplifies the specific
strength of a grammar-based approach.
3. We present an implementation of stochas-
tic synchronous TAG that incorporates vari-
ous facilities useful for training on real-world
data: sister-adjunction (needed for generating
the flat structures found in most treebanks),
smoothing, and Inside-Outside reestimation.
This paper is structured as follows. We first
briefly discuss related work (Section 2) and some
of the linguistic facts that motivate this work (Sec-
tion 3). We then present the formalism, probabilis-
tic model, and parsing algorithm (Section 4). Fi-
nally, we discuss the manual grammar engineering
(Section 5) and evaluation (Section 6).
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99974625">
This paper is part of a larger investigation into
parsing Arabic dialects (Rambow et al., 2005; Chi-
ang et al., 2006). In that investigation, we exam-
ined three different approaches:
</bodyText>
<listItem confidence="0.999405222222222">
• Sentence transduction, in which a dialect sen-
tence is roughly translated into one or more
MSA sentences and then parsed by an MSA
parser.
• Treebank transduction, in which the MSA
treebank is transduced into an approximation
of a LA treebank, on which a LA parer is then
trained.
• Grammar transduction, which is the name
</listItem>
<bodyText confidence="0.9026085">
given in the overview papers to the approach
discussed in this paper. The present paper
provides for the first time a complete tech-
nical presentation of this approach.
Overall, grammar transduction outperformed
the other two approaches.
In other work, there has been a fair amount of
interest in parsing one language using another lan-
guage, see for example (Smith and Smith, 2004;
Hwa et al., 2004). Much of this work, like ours,
relies on synchronous grammars (CFGs). How-
ever, these approaches rely on parallel corpora.
For MSA and its dialects, there are no naturally
occurring parallel corpora. It is this fact that has
led us to investigate the use of explicit linguistic
knowledge to complement machine learning.
</bodyText>
<sectionHeader confidence="0.984641" genericHeader="method">
3 Linguistic Facts
</sectionHeader>
<bodyText confidence="0.975205">
We illustrate the differences between LA and
MSA using an example:
</bodyText>
<listItem confidence="0.295007">
(1) a. (LA)
</listItem>
<bodyText confidence="0.953908285714286">
Al$gl hdA
the-work this
the men do not like this work
b. (MSA)
AlrjAl h*A AlEml
the-men this the-work
the men do not like this work
Lexically, we observe that the word for ‘work’
is Al$gl in LA but AlEml in MSA.
In contrast, the word for ‘men’ is the same in both
LA and MSA: AlrjAl. There are typically
also differences in function words, in our example
$ (LA) and lA (MSA) for ‘not’. Morpholog-
ically, we see that LA byHbw has the same
stem as MA yHb, but with two additional
morphemes: the present aspect marker b- which
does not exist in MSA, and the agreement marker
-w, which is used in MSA only in subject-initial
sentences, while in LA it is always used.
Syntactically, we observe three differences.
First, the subject precedes the verb in LA (SVO
order), but follows in MSA (VSO order). This is
in fact not a strict requirement, but a strong pref-
erence: both varieties allow both orders, but in the
dialects, the SVO order is more common, while in
MSA, the VSO order is more common. Second,
we see that the demonstrative determiner follows
the noun in LA, but precedes it in MSA. Finally,
we see that the negation marker follows the verb
in LA, while it precedes the verb in MSA. (Lev-
antine also has other negation markers that pre-
cede the verb, as well as the circumfix m- -$.) The
two phrase structure trees are shown in Figure 1
in the convention of the Linguistic Data Consor-
tium (Maamouri et al., 2004). Unlike the phrase
</bodyText>
<figure confidence="0.9973545">
AlrjAl
the-men
byHbw
like
$
not
lA
not
yHb
like
</figure>
<page confidence="0.535698">
2
</page>
<figure confidence="0.99973324">
‘like’ ‘not’
‘work’ ‘this’
S
VP
‘this’ ‘work’
NP-TPC
‘men’i
V
S
NEG
VP
NP-SBJ
NP-OBJ
DET
ti
N
NP-SBJ
V
DET
‘men’
‘like’
NEG
‘not’
NP-OBJ
N
</figure>
<figureCaption confidence="0.99855">
Figure 1: LDC-style left-to-right phrase structure trees for LA (left) and MSA (right) for sentence (1)
</figureCaption>
<figure confidence="0.9969946">
‘like’
’like’
‘work’ ‘men’ ‘not’ ‘work’ ‘men’ ‘not’
‘this’
‘this’
</figure>
<figureCaption confidence="0.947099">
Figure 2: Unordered dependency trees for LA (left) and MSA (right) for sentence (1)
</figureCaption>
<figure confidence="0.997171214285714">
off
NP
assets
S
NP VP
Qintex VBD
sold
NP
NNP
NP
PRT
RP
NNS
(α1) (α2) (α3) (α4)
</figure>
<figureCaption confidence="0.999926">
Figure 3: Example elementary trees.
</figureCaption>
<bodyText confidence="0.999858">
structure trees, the (unordered) dependency trees
for the MSA and LA sentences are isomorphic, as
shown in Figure 2. They differ only in the node
labels.
</bodyText>
<sectionHeader confidence="0.998791" genericHeader="method">
4 Model
</sectionHeader>
<subsectionHeader confidence="0.999761">
4.1 The synchronous TSG+SA formalism
</subsectionHeader>
<bodyText confidence="0.999288853658537">
Our parser (Chiang, 2000) is based on syn-
chronous tree-substitution grammar with sister-
adjunction (TSG+SA). Tree-substitution grammar
(Schabes, 1990) is TAG without auxiliary trees or
adjunction; instead we include a weaker composi-
tion operation, sister-adjunction (Rambow et al.,
2001), in which an initial tree is inserted between
two sister nodes (see Figure 4). We allow multi-
ple sister-adjunctions at the same site, similar to
how Schabes and Shieber (1994) allow multiple
adjunctions of modifier auxiliary trees.
A synchronous TSG+SA is a set of pairs of el-
ementary trees. In each pair, there is a one-to-one
correspondence between the substitution/sister-
adjunction sites of the two trees, which we repre-
sent using boxed indices (Figure 5). A derivation
then starts with a pair of initial trees and proceeds
by substituting or sister-adjoining elementary tree
pairs at coindexed sites. In this way a set of string
pairs (5, 5&apos;) is generated.
Sister-adjunction presents a special problem
for synchronization: if multiple tree pairs sister-
adjoin at the same site, how should their order on
the source side relate to the order on the target
side? Shieber’s solution (Shieber, 1994) is to al-
low any ordering. We adopt a stricter solution: for
each pair of sites, fix a permutation (either iden-
tity or reversal) for the tree pairs that sister-adjoin
there. Owing to the way we extract trees from the
Treebank, the simplest choice of permutations is:
if the two sites are both to the left of the anchor
or both to the right of the anchor, then multiple
sister-adjoined tree pairs will appear in the same
order on both sides; otherwise, they will appear in
the opposite order. In other words, multiple sister-
adjunction always adds trees from the anchor out-
ward.
A stochastic synchronous TSG+SA adds prob-
abilities to the substitution and sister-adjunction
operations: the probability of substituting an ele-
mentary tree pair (α, α&apos;) at a substitution site pair
</bodyText>
<page confidence="0.943969">
3
</page>
<figure confidence="0.999264038461539">
S
NP VP
NNP
Qintex
VP
VBD
sold
NP
NNS
assets
PRT
S
NP VP
VBD
NP
sold
PRT
RP
NNS
�
NNP
Qintex
assets
off
RP
off
</figure>
<figureCaption confidence="0.970076">
Figure 4: Sister-adjunction, with inserted material shown with shaded background
</figureCaption>
<figure confidence="0.991591">
NP
V
S
VP
V NPI1 NPI2
‘like’
I
S
NPiI 1 VP
NPI 2
ti
‘like’
�
� � � � � � � � � � � � � � �
</figure>
<figureCaption confidence="0.910817">
Figure 5: Example elementary tree pair of a synchronous TSG: the SVO transformation (LA on left,
MSA on right)
</figureCaption>
<page confidence="0.988164">
4
</page>
<bodyText confidence="0.991811">
hn, n0iis Ps(α, α0  |n, n0), and the probability of
sister-adjoining hα, α0i at a sister-adjunction site
pair hn, i, n0, i0i is Psa(α, α0  |n, i, n0, i0), where
i and i0 indicate that the sister-adjunction occurs
between the i and (i + 1)st (or i0 and (i0 + 1)st)
sisters. These parameters must satisfy the normal-
Treebank (Chiang, 2000); the details are not im-
portant here.
As for Pt, in order to obtain better probability
estimates, we further decompose Pt into Pt1 and
Pt2 so they can be estimated separately (as in the
monolingual parsing model):
</bodyText>
<equation confidence="0.963678333333333">
ization conditions Ps(α,α0  |n,n0) = 1 (1) Pt(α0  |α) ≈ Pt1(a0  |a, w0, t0, w, t) ×
E Pt2(w0, t0  |w, t) (5)
a,a/
E Psa(α, α0  |n, i, n0, i0) +
a,a/
Psa(STOP  |n, i, n0, i0) = 1 (2)
</equation>
<subsectionHeader confidence="0.998168">
4.2 Parsing by translation
</subsectionHeader>
<bodyText confidence="0.999995833333333">
We intend to apply a stochastic synchronous
TSG+SA to input sentences 50. This requires pro-
jecting any constraints from the unprimed side of
the synchronous grammar over to the primed side,
and then parsing the sentences 50 using the pro-
jected grammar, using a straightforward general-
ization of the CKY and Viterbi algorithms. This
gives the highest-probability derivation of the syn-
chronous grammar that generates 50 on the primed
side, which includes a parse for 50 and, as a by-
product, a parsed translation of 50.
Suppose that 50 is a sentence of LA. For the
present task we are not actually interested in the
MSA translation of 50, or the parse of the MSA
translation; we are only interested in the parse of
50. The purpose of the MSA side of the grammar
is to provide reliable statistics. Thus, we approxi-
mate the synchronous rewriting probabilities as:
</bodyText>
<equation confidence="0.996413">
Ps(α, α0  |n, n0)
≈ Ps(α  |n)Pt(α0  |α) (3)
Psa(α, α0  |n, i, n0, i0)
≈ Psa(α  |n, i)Pt(α0  |α) (4)
</equation>
<bodyText confidence="0.9995226">
These factors, as we will see shortly, are much eas-
ier to estimate given the available resources.
This factorization is analogous to a hidden
Markov model: the primed derivation is the obser-
vation, the unprimed derivation is the hidden state
sequence (except it is a branching process instead
of a chain); the Ps and Psa are like the transition
probabilities and the Pt are like the observation
probabilities. Hence, we call this model a “hidden
TAG model.”
</bodyText>
<subsectionHeader confidence="0.9954">
4.3 Parameter estimation and smoothing
</subsectionHeader>
<bodyText confidence="0.999795076923077">
Ps and Psa are the parameters of a monolingual
TSG+SA and can be learned from a monolingual
where w and t are the lexical anchor of α and its
POS tag, and α� is the equivalence class of α mod-
ulo lexical anchors and their POS tags. Pt2 repre-
sents the lexical transfer model, and Pt1 the syn-
tactic transfer model. Pt1 and Pt2 are initially as-
signed by hand; Pt1 is then reestimated by EM.
Because the full probability table for Pt1 would
be too large to write by hand, and because our
training data might be too sparse to reestimate it
well, we smooth it by approximating it as a linear
combination of backoff models:
</bodyText>
<equation confidence="0.99881725">
Pt1(a  |6, w0, t0, w, t) ≈
A1Pt11(a0  |a, w0, t0, w, t) +
(1 − A1)(A2Pt12(�α0  |a, w0, t0) +
(1 − A2)Pt13(a0  |a)) (6)
</equation>
<bodyText confidence="0.999744947368421">
where each Ai, unlike in the monolingual parser,
is simply set to 1 if an estimate is available for that
level, so that it completely overrides the further
backed-off models.
The initial estimates for the Pt1i are set by hand.
The availability of three backoff models makes it
easy to specify the initial guesses at an appropri-
ate level of detail: for example, one might give a
general probability of some α� mapping to a0 using
Pt13, but then make special exceptions for partic-
ular lexical anchors using Pt11 or Pt12.
Finally Pt2 is reestimated by EM on some held-
out unannotated sentences of L0, using the same
method as Chiang and Bikel (2002) but on the syn-
tactic transfer probabilities instead of the mono-
lingual parsing model. Another difference is that,
following Bikel (2004), we do not recalculate the
Ai at each iteration, but use the initial values
throughout.
</bodyText>
<sectionHeader confidence="0.994164" genericHeader="method">
5 A Synchronous TSG-SA for Dialectal
Arabic
</sectionHeader>
<bodyText confidence="0.999871">
Just as the probability model discussed in the pre-
ceding section factored the rewriting probabilities
</bodyText>
<page confidence="0.984096">
5
</page>
<bodyText confidence="0.956289666666667">
into three parts, we create a synchronous TSG-SA
and the probabilities of a hidden TAG model in
three steps:
</bodyText>
<listItem confidence="0.887436181818182">
• Ps and Psa are the parameters of a monolin-
gual TSG+SA for MSA. We extract a gram-
mar for the resource-rich language (MSA)
from the Penn Arabic Treebank in a pro-
cess described by Chiang and others (Chiang,
2000; Xia et al., 2000; Chen, 2001).
• For the lexical transfer model Pte, we cre-
ate by hand a probabilistic mapping between
(word, POS tag) pairs in the two languages.
• For the syntactic transfer model Pti, we cre-
ated by hand a grammar for the resource-poor
</listItem>
<bodyText confidence="0.9296038">
language and a mapping between elementary
trees in the two grammars, along with initial
guesses for the mapping probabilities.
We discuss the hand-crafted lexicon and syn-
chronous grammar in the following subsections.
</bodyText>
<subsectionHeader confidence="0.998752">
5.1 Lexical Mapping
</subsectionHeader>
<bodyText confidence="0.999829">
We used a small, hand-crafted lexicon of 100
words which mapped all LA function words and
some of the most common open-class words to
MSA. We assigned uniform probabilities to the
mapping. All other MSA words were assumed
to also be LA words. Unknown LA words were
handled using the standard unknown word mecha-
nism.
</bodyText>
<subsectionHeader confidence="0.999378">
5.2 Syntactic Mapping
</subsectionHeader>
<bodyText confidence="0.959654736842105">
Because of the underlying syntactic similarity be-
tween the two varieties of Arabic, we assume that
every tree in the MSA grammar extracted from the
MSA treebank is also a LA tree. In addition, we
define tree transformations in the Tsurgeon pack-
age (Levy and Andrew, 2006). These consist of
a pattern which matches MSA elementary trees
in the extracted grammar, and a transformation
which produces a LA elementary tree. We per-
form the following tree transformations on all el-
ementary trees which match the underlying MSA
pattern. Thus, each MSA tree corresponds to at
least two LA trees: the original one and the trans-
formed one. If several transformations apply, we
obtain multiple transformed trees.
• Negation (NEG): we insert a $ negation
marker immediately following each verb.
The preverbal marker is generated by a lex-
ical translation of an MSA elementary tree.
</bodyText>
<listItem confidence="0.8778966">
• VSO-SVO Ordering (SVO): Both Verb-
Subject-Object (VSO) and Subject-Verb-
Object (SVO) constructions occur in MSA
and LA treebanks. But pure VSO construc-
tions (without pro-drop) occur in the LA cor-
</listItem>
<bodyText confidence="0.940117142857143">
pus only 10ordering in MSA. Hence, the goal
is to skew the distributions of the SVO con-
structions in the MSA data. Therefore, VSO
constructions are replicated and converted to
SVO constructions. One possible resulting
pair of trees is shown in Figure 5.
• The bd construction (BD): bd is a LA noun
that means ‘want’. It acts like a verb in
verbal constructions yielding VP construc-
tions headed by NN. It is typically followed
by an enclitic possessive pronoun. Accord-
ingly, we defined a transformation that trans-
lated all the verbs meaning ‘want’/‘need’ into
the noun bd and changed their respective
POS tag to NN. The subject clitic is trans-
formed into a possessive pronoun clitic. Note
that this construction is a combination lexical
and syntactic transformation, and thus specif-
ically exploits the extended domain of local-
ity of TAG-like formalisms. One possible re-
sulting pair of trees is shown in Figure 6.
</bodyText>
<sectionHeader confidence="0.992082" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.99972105">
While our approach does not rely on any annotated
corpus for LA, nor on a parallel corpus MSA-
LA, we use a small treebank of LA (Maamouri et
al., 2006) to analyze and test our approach. The
LA treebank is divided into a development corpus
and a test corpus, each about 11,000 tokens (using
the same tokenization scheme as employed in the
MSA treebank).
We first use the development corpus to deter-
mine which of the transformations are useful. We
use two conditions. In the first, the input text is
not tagged, and the parser hypothesizes tags. In
the second, the input text is tagged with the gold
(correct) tag. The results are shown in Table 1.
The baseline is simply the application of a pure
MSA Chiang parser to LA. We see that important
improvements are obtained using the lexical map-
ping. Adding the SVO transformation does not
improve the results, but the NEG and BD trans-
formations help slightly, and their effect is (partly)
</bodyText>
<page confidence="0.971488">
6
</page>
<figure confidence="0.999853428571428">
S
VP
S
V
N
PRP$
‘want’
‘want’
f
S11 1
S11 1
VP
‘my’
1
</figure>
<figureCaption confidence="0.922436">
Figure 6: Example elementary tree pair of a synchronous TSG: the BD transformation (LA on left, MSA
on right)
</figureCaption>
<bodyText confidence="0.988664419354839">
cumulative. (We did not perform these tuning ex-
periments on input without POS tags.)
The evaluation on the test corpus confirms these
results. Using the NEG and BD transformations
and the small lexicon, we obtain a 17.3% error re-
duction relative to the baseline parser (Figure 2).
These results show that the translation lexicon
can be integrated effectively into our synchronous
grammar framework. In addition, some syntac-
tic transformations are useful. The SVO trans-
formation, we assume, turns out not to be useful
because the SVO word order is also possible in
MSA, so that the new trees were not needed and
needlessly introduced new derivations. The BD
transformation shows the importance not of gen-
eral syntactic transformations, but rather of lexi-
cally specific syntactic transformations: varieties
within one language family may differ more in
terms of the lexico-syntactic constructions used
for a specific (semantic or pragmatic) purpose than
in their basic syntactic inventory. Note that our
tree-based synchronous formalism is ideally suited
for expressing such transformations since it is lex-
icalized, and has an extended domain of locality.
Given the impact of the BD transformation, in fu-
ture work we intend to determine more lexico-
structural transformations, rather than pure syntac-
tic transformations. However, one major impedi-
ment to obtaining better results is the disparity in
genre and domain which affects the overall perfor-
mance.
</bodyText>
<sectionHeader confidence="0.998931" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999960333333333">
We have presented a new probabilistic syn-
chronous TAG formalism, synchronous Tree
Substitution Grammar with sister adjunction
(TSG+SA). We have introduced the concept of
a hidden TAG model, analogous to a Hidden
Markov Model. It allows us to parse a resource-
poor language using a treebank-extracted prob-
abilistic grammar for a resource-rich language,
along with a hand-crafted synchronous grammar
for the resource-poor language. Thus, our model
combines statistical approaches with grammar en-
gineering (specifically motivated from the linguis-
tic facts). Our approach thus exemplifies the
specific strength of a grammar-based approach.
While we have applied this approach to two
closely related languages, it would be interesting
to apply this approach to more distantly related
languages in the future.
</bodyText>
<sectionHeader confidence="0.997475" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999807428571429">
This paper is based on work done at the 2005
Johns Hopkins Summer Workshop, which was
partially supported by the National Science Foun-
dation under grant 0121285. The first author
was additionally supported by ONR MURI con-
tract FCPO.810548265 and Department of De-
fense contract RD-02-5700. The second author
was additionally supported by contract HR0011-
06-C-0023 under the GALE program. We wish
to thank the other members of our JHU team (our
co-authors on (Rambow et al., 2005)), especially
Nizar Habash and Mona Diab for their help with
the Arabic examples, and audiences at JHU for
their useful feedback.
</bodyText>
<sectionHeader confidence="0.999468" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9957685">
Daniel M. Bikel. 2004. On the Parameter Space of
Generative Lexicalized Parsing Models. Ph.D. the-
sis, University of Pennsylvania.
John Chen. 2001. Towards Efficient Statistical Parsing
Using Lexicalized Grammatical Information. Ph.D.
thesis, University of Delaware.
</reference>
<page confidence="0.999635">
7
</page>
<table confidence="0.9991985">
LP no tags F1 LP gold tags F1
LR LR
Baseline 59.4 51.9 55.4 64.0 58.3 61.0
Lexical 63.0 60.8 61.9 66.9 67.0 66.9
+ SVO 66.9 66.7 66.8
+ NEG 67.0 67.0 67.0
+ BD 67.4 67.0 67.2
+ NEG + BD 67.4 67.1 67.3
</table>
<tableCaption confidence="0.9492405">
Table 1: Results on development corpus: LP = labeled precision, LR = labeled recall, F1 = balanced
F-measure
</tableCaption>
<table confidence="0.9556715">
Baseline no tags gold tags
Lexical + NEG + BD F1 F1
53.5 60.2
60.2 67.1
</table>
<tableCaption confidence="0.993805">
Table 2: Results on the test corpus: F1 = balanced F-measure
</tableCaption>
<reference confidence="0.999599894736842">
David Chiang and Daniel M. Bikel. 2002. Recovering
latent information in treebanks. In Proceedings of
the Nineteenth International Conference on Compu-
tational Linguistics (COLING), pages 183–189.
David Chiang, Mona Diab, Nizar Habash, Owen Ram-
bow, and Safiullah Shareef. 2006. Parsing Arabic
dialects. In Proceedings ofEACL.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
38th Meeting of the Association for Computational
Linguistics (ACL’00), pages 456–463, Hong Kong,
China.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2004. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering.
Roger Levy and Galen Andrew. 2006. Tregex and
Tsurgeon: tools for querying and manipulating tree
data structures. In Proceedings ofLREC.
Mohamed Maamouri, Ann Bies, and Tim Buckwalter.
2004. The Penn Arabic Treebank: Building a large-
scale annotated Arabic corpus. In NEMLAR Con-
ference on Arabic Language Resources and Tools,
Cairo, Egypt.
Mohamed Maamouri, Ann Bies, Tim Buckwalter,
Mona Diab, Nizar Habash, Owen Rambow, and
Dalila Tabessi. 2006. Developing and using a pilot
dialectal Arabic treebank. In Proceedings ofLREC,
Genoa, Italy.
Owen Rambow, K. Vijay-Shanker, and David Weir.
2001. D-Tree Substitution Grammars. Computa-
tional Linguistics, 27(1).
Owen Rambow, David Chiang, Mona Diab, Nizar
Habash, Rebecca Hwa, Khalil Sima’an, Vincent
Lacey, Roger Levy, Carol Nichols, and Safiullah
Shareef. 2005. Parsing Arabic dialects. Final Re-
port, 2005 JHU Summer Workshop.
Yves Schabes and Stuart Shieber. 1994. An alternative
conception of tree-adjoining derivation. Computa-
tional Linguistics, 1(20):91–124.
Yves Schabes. 1990. Mathematical and Computa-
tional Aspects ofLexicalized Grammars. Ph.D. the-
sis, Department of Computer and Information Sci-
ence, University of Pennsylvania.
Stuart B. Shieber. 1994. Restricting the weak genera-
tive capacity of Synchronous Tree Adjoining Gram-
mar. Computational Intelligence, 10(4):371–385.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using English to
parse Korean. In Proceedings of the 2004 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP04).
Fei Xia, Martha Palmer, and Aravind Joshi. 2000. A
uniform method of grammar extraction and its appli-
cations. In Proceedings of the 2000 Conference on
Empirical Methods in Natural Language Processing
(EMNLP00), Hong Kong.
</reference>
<page confidence="0.998491">
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9913885">The Hidden TAG Model: Synchronous Grammars for Resource-Poor Languages</title>
<affiliation confidence="0.9138435">Information Sciences University of Southern</affiliation>
<address confidence="0.97324">4676 Admiralty Way, Suite Marina del Rey, CA 90292,</address>
<email confidence="0.999604">chiang@isi.edu</email>
<author confidence="0.95201">Owen</author>
<affiliation confidence="0.7816995">Center for Computational Learning Columbia</affiliation>
<address confidence="0.999712">475 Riverside Dr., Suite</address>
<author confidence="0.566024">New York</author>
<author confidence="0.566024">NY</author>
<email confidence="0.999777">rambow@cs.columbia.edu</email>
<abstract confidence="0.99718014673913">This paper discusses a novel probabilistic synchronous TAG formalism, synchronous Tree Substitution Grammar with sister adjunction (TSG+SA). We use it to parse a language for which there is no training data, by leveraging off a second, related language for which there is abundant training data. The grammar for the resource-rich side is automatically extracted from a treebank; the grammar on the resource-poor side and the synchronization are created by handwritten rules. Our approach thus represents a combination of grammar-based and empirical natural language processing. We discuss the approach using the example of Levantine Arabic and Standard Arabic. 1 Parsing Arabic Dialects and Tree Adjoining Grammar The Arabic language is a collection of spoken dialects and a standard written language. The standard written language is the same throughout the Arab world, Modern Standard Arabic (MSA), which is also used in some scripted spoken communication (news casts, parliamentary debates). It is based on Classical Arabic and is not a native language of any Arabic speaking people, i.e., children do not learn it from their parents but in school. Thus most native speakers of Arabic are unable to produce sustained spontaneous MSA. The dialects show phonological, morphological, lexical, and syntactic differences comparable to work was primarily carried out while the first author was at the University of Maryland Institute for Advanced Computer Studies. those among the Romance languages. They vary not only along a geographical continuum but also with other sociolinguistic variables such as the urban/rural/Bedouin dimension. The multidialectal situation has important negative consequences for Arabic natural language processing (NLP): since the spoken dialects are not officially written and do not have standard orthography, it is very costly to obtain adequate corpora, even unannotated corpora, to use for training NLP tools such as parsers. Furthermore, there are almost no parallel corpora involving one dialect and MSA. The question thus arises how to create a statistical parser for an Arabic dialect, when statistical parsers are typically trained on large corpora of parse trees. We present one solution to this problem, based on the assumption that it is easier to manually create new resources that relate a dialect to MSA (lexicon and grammar) than it is to manually create syntactically annotated corpora in the dialect. In this paper, we deal with Levantine Arabic (LA). Our approach does not assume the existence of any annotated LA corpus (except for development and testing), nor of a parallel LA-MSA corpus. The approach described in this paper uses a special parameterization of stochastic synchronous TAG (Shieber, 1994) which we call a “hidden TAG model.” This model couples a model of MSA trees, learned from the Arabic Treebank, with a model of MSA-LA translation, which is initialized by hand and then trained in an unsupervised fashion. Parsing new LA sentences then entails simultaneously building a forest of MSA trees and the corresponding forest of LA trees. Our implementation uses an extension of our monolingual parser (Chiang, 2000) based on tree-substitution 1 of the 8th International Workshop on Tree Adjoining Grammar and Related pages July 2006. Association for Computational Linguistics grammar with sister adjunction (TSG+SA). The main contributions of this paper are as follows: 1. We introduce the novel concept of a hidden TAG model. 2. We use this model to combine statistical approaches with grammar engineering (specifically motivated from the linguistic facts). Our approach thus exemplifies the specific strength of a grammar-based approach. 3. We present an implementation of stochastic synchronous TAG that incorporates various facilities useful for training on real-world data: sister-adjunction (needed for generating the flat structures found in most treebanks), smoothing, and Inside-Outside reestimation. This paper is structured as follows. We first briefly discuss related work (Section 2) and some of the linguistic facts that motivate this work (Section 3). We then present the formalism, probabilistic model, and parsing algorithm (Section 4). Finally, we discuss the manual grammar engineering (Section 5) and evaluation (Section 6). 2 Related Work This paper is part of a larger investigation into parsing Arabic dialects (Rambow et al., 2005; Chiang et al., 2006). In that investigation, we examined three different approaches: • Sentence transduction, in which a dialect sentence is roughly translated into one or more MSA sentences and then parsed by an MSA parser. • Treebank transduction, in which the MSA treebank is transduced into an approximation of a LA treebank, on which a LA parer is then trained. • Grammar transduction, which is the name given in the overview papers to the approach discussed in this paper. The present paper provides for the first time a complete technical presentation of this approach. Overall, grammar transduction outperformed the other two approaches. In other work, there has been a fair amount of interest in parsing one language using another language, see for example (Smith and Smith, 2004; Hwa et al., 2004). Much of this work, like ours, relies on synchronous grammars (CFGs). However, these approaches rely on parallel corpora. For MSA and its dialects, there are no naturally occurring parallel corpora. It is this fact that has led us to investigate the use of explicit linguistic knowledge to complement machine learning. 3 Linguistic Facts We illustrate the differences between LA and MSA using an example: (1) a. (LA) Al$gl hdA the-work this the men do not like this work b. (MSA) AlrjAl h*A AlEml the-men this the-work the men do not like this work Lexically, we observe that the word for ‘work’ LA but In contrast, the word for ‘men’ is the same in both and MSA: There are also differences in function words, in our example and for ‘not’. Morphologwe see that LA the same as MA but with two additional the present aspect marker does not exist in MSA, and the agreement marker which is used in MSA only in subject-initial sentences, while in LA it is always used. Syntactically, we observe three differences. First, the subject precedes the verb in LA (SVO order), but follows in MSA (VSO order). This is in fact not a strict requirement, but a strong preference: both varieties allow both orders, but in the dialects, the SVO order is more common, while in MSA, the VSO order is more common. Second, we see that the demonstrative determiner follows the noun in LA, but precedes it in MSA. Finally, we see that the negation marker follows the verb in LA, while it precedes the verb in MSA. (Levantine also has other negation markers that prethe verb, as well as the circumfix The two phrase structure trees are shown in Figure 1 in the convention of the Linguistic Data Consortium (Maamouri et al., 2004). Unlike the phrase AlrjAl the-men byHbw like $ not lA not yHb like 2 ‘like’ ‘not’ ‘work’ ‘this’ VP ‘this’ ‘work’</abstract>
<title confidence="0.891947818181818">NP-TPC V S NEG VP NP-SBJ NP-OBJ DET N NP-SBJ V</title>
<abstract confidence="0.956902764705882">DET ‘men’ ‘like’ NEG ‘not’ NP-OBJ N Figure 1: LDC-style left-to-right phrase structure trees for LA (left) and MSA (right) for sentence (1) ‘like’ ’like’ ‘work’ ‘men’ ‘not’ ‘work’ ‘men’ ‘not’ ‘this’ ‘this’ Figure 2: Unordered dependency trees for LA (left) and MSA (right) for sentence (1) off NP assets</abstract>
<title confidence="0.7338781">S NP VP Qintex VBD sold NP NNP NP PRT RP NNS</title>
<abstract confidence="0.996929285714286">Figure 3: Example elementary trees. structure trees, the (unordered) dependency trees for the MSA and LA sentences are isomorphic, as shown in Figure 2. They differ only in the node labels. 4 Model 4.1 The synchronous TSG+SA formalism Our parser (Chiang, 2000) is based on synchronous tree-substitution grammar with sisteradjunction (TSG+SA). Tree-substitution grammar (Schabes, 1990) is TAG without auxiliary trees or adjunction; instead we include a weaker composioperation, et al., 2001), in which an initial tree is inserted between two sister nodes (see Figure 4). We allow multiple sister-adjunctions at the same site, similar to how Schabes and Shieber (1994) allow multiple adjunctions of modifier auxiliary trees. is a set of pairs of elementary trees. In each pair, there is a one-to-one correspondence between the substitution/sisteradjunction sites of the two trees, which we represent using boxed indices (Figure 5). A derivation then starts with a pair of initial trees and proceeds by substituting or sister-adjoining elementary tree pairs at coindexed sites. In this way a set of string generated. Sister-adjunction presents a special problem for synchronization: if multiple tree pairs sisteradjoin at the same site, how should their order on the source side relate to the order on the target side? Shieber’s solution (Shieber, 1994) is to allow any ordering. We adopt a stricter solution: for each pair of sites, fix a permutation (either identity or reversal) for the tree pairs that sister-adjoin there. Owing to the way we extract trees from the Treebank, the simplest choice of permutations is: if the two sites are both to the left of the anchor or both to the right of the anchor, then multiple sister-adjoined tree pairs will appear in the same order on both sides; otherwise, they will appear in the opposite order. In other words, multiple sisteradjunction always adds trees from the anchor outward. TSG+SA adds probabilities to the substitution and sister-adjunction operations: the probability of substituting an eletree pair a substitution site pair 3</abstract>
<title confidence="0.813554157894737">S NP VP NNP Qintex VP VBD sold NP NNS assets PRT S NP VP VBD NP sold PRT RP NNS</title>
<abstract confidence="0.987855441176471">NNP Qintex assets off RP off Figure 4: Sister-adjunction, with inserted material shown with shaded background NP V S VP ‘like’ I S ‘like’ � � � � � � � � � � � � � � � � 5: Example elementary tree pair of a synchronous TSG: the (LA on left, MSA on right) 4 and the probability of a sister-adjunction site i, i, where that the sister-adjunction occurs the (or These parameters must satisfy the normal- Treebank (Chiang, 2000); the details are not important here. for in order to obtain better probability we further decompose they can be estimated separately (as in the monolingual parsing model): ization conditions = 1 w, E i, + i, = 1 4.2 Parsing by translation We intend to apply a stochastic synchronous to input sentences This requires projecting any constraints from the unprimed side of the synchronous grammar over to the primed side, then parsing the sentences the projected grammar, using a straightforward generalization of the CKY and Viterbi algorithms. This gives the highest-probability derivation of the syngrammar that generates the primed which includes a parse for as a bya parsed translation of that a sentence of LA. For the present task we are not actually interested in the translation of or the parse of the MSA translation; we are only interested in the parse of The purpose of the MSA side of the grammar is to provide reliable statistics. Thus, we approximate the synchronous rewriting probabilities as: i, These factors, as we will see shortly, are much easier to estimate given the available resources. This factorization is analogous to a hidden Markov model: the primed derivation is the observation, the unprimed derivation is the hidden state sequence (except it is a branching process instead a chain); the like the transition and the like the observation probabilities. Hence, we call this model a “hidden TAG model.” 4.3 Parameter estimation and smoothing the parameters of a monolingual TSG+SA and can be learned from a monolingual the lexical anchor of its tag, and the equivalence class of modlexical anchors and their POS tags. reprethe lexical transfer model, and syntransfer model. initially asby hand; then reestimated by EM. the full probability table for be too large to write by hand, and because our training data might be too sparse to reestimate it well, we smooth it by approximating it as a linear combination of backoff models: w, w, + + each unlike in the monolingual parser, is simply set to 1 if an estimate is available for that level, so that it completely overrides the further backed-off models. initial estimates for the set by hand. The availability of three backoff models makes it easy to specify the initial guesses at an appropriate level of detail: for example, one might give a probability of some to but then make special exceptions for particlexical anchors using reestimated by EM on some heldunannotated sentences of using the same method as Chiang and Bikel (2002) but on the syntactic transfer probabilities instead of the monolingual parsing model. Another difference is that, following Bikel (2004), we do not recalculate the each iteration, but use the initial values throughout. 5 A Synchronous TSG-SA for Dialectal Arabic Just as the probability model discussed in the preceding section factored the rewriting probabilities 5 into three parts, we create a synchronous TSG-SA and the probabilities of a hidden TAG model in three steps: • and are the parameters of a monolingual TSG+SA for MSA. We extract a grammar for the resource-rich language (MSA) from the Penn Arabic Treebank in a process described by Chiang and others (Chiang, 2000; Xia et al., 2000; Chen, 2001). For the lexical transfer model we create by hand a probabilistic mapping between in the two languages. For the syntactic transfer model we created by hand a grammar for the resource-poor language and a mapping between elementary trees in the two grammars, along with initial guesses for the mapping probabilities. We discuss the hand-crafted lexicon and synchronous grammar in the following subsections. 5.1 Lexical Mapping We used a small, hand-crafted lexicon of 100 words which mapped all LA function words and some of the most common open-class words to MSA. We assigned uniform probabilities to the mapping. All other MSA words were assumed to also be LA words. Unknown LA words were handled using the standard unknown word mechanism. 5.2 Syntactic Mapping Because of the underlying syntactic similarity between the two varieties of Arabic, we assume that every tree in the MSA grammar extracted from the MSA treebank is also a LA tree. In addition, we define tree transformations in the Tsurgeon package (Levy and Andrew, 2006). These consist of a pattern which matches MSA elementary trees in the extracted grammar, and a transformation which produces a LA elementary tree. We perform the following tree transformations on all elementary trees which match the underlying MSA pattern. Thus, each MSA tree corresponds to at least two LA trees: the original one and the transformed one. If several transformations apply, we obtain multiple transformed trees. Negation we insert a marker immediately following each verb. The preverbal marker is generated by a lexical translation of an MSA elementary tree. VSO-SVO Ordering Both Verb- Subject-Object (VSO) and Subject-Verb- Object (SVO) constructions occur in MSA and LA treebanks. But pure VSO constructions (without pro-drop) occur in the LA corpus only 10ordering in MSA. Hence, the goal is to skew the distributions of the SVO constructions in the MSA data. Therefore, VSO constructions are replicated and converted to SVO constructions. One possible resulting pair of trees is shown in Figure 5. The a LA noun that means ‘want’. It acts like a verb in verbal constructions yielding VP constructions headed by NN. It is typically followed by an enclitic possessive pronoun. Accordingly, we defined a transformation that translated all the verbs meaning ‘want’/‘need’ into noun changed their respective POS tag to NN. The subject clitic is transformed into a possessive pronoun clitic. Note that this construction is a combination lexical and syntactic transformation, and thus specifically exploits the extended domain of locality of TAG-like formalisms. One possible resulting pair of trees is shown in Figure 6. 6 Experimental Results While our approach does not rely on any annotated corpus for LA, nor on a parallel corpus MSA- LA, we use a small treebank of LA (Maamouri et al., 2006) to analyze and test our approach. The LA treebank is divided into a development corpus and a test corpus, each about 11,000 tokens (using the same tokenization scheme as employed in the MSA treebank). We first use the development corpus to determine which of the transformations are useful. We use two conditions. In the first, the input text is not tagged, and the parser hypothesizes tags. In the second, the input text is tagged with the gold (correct) tag. The results are shown in Table 1. The baseline is simply the application of a pure MSA Chiang parser to LA. We see that important improvements are obtained using the lexical map- Adding the does not the results, but the transformations help slightly, and their effect is (partly) 6</abstract>
<title confidence="0.8508402">S VP S V N</title>
<abstract confidence="0.975653878378378">PRP$ ‘want’ ‘want’ f VP ‘my’ 1 6: Example elementary tree pair of a synchronous TSG: the (LA on left, MSA on right) cumulative. (We did not perform these tuning experiments on input without POS tags.) The evaluation on the test corpus confirms these Using the and the small lexicon, we obtain a 17.3% error reduction relative to the baseline parser (Figure 2). These results show that the translation lexicon can be integrated effectively into our synchronous grammar framework. In addition, some syntactransformations are useful. The transformation, we assume, turns out not to be useful the order is also possible in MSA, so that the new trees were not needed and introduced new derivations. The transformation shows the importance not of general syntactic transformations, but rather of lexically specific syntactic transformations: varieties within one language family may differ more in terms of the lexico-syntactic constructions used for a specific (semantic or pragmatic) purpose than in their basic syntactic inventory. Note that our tree-based synchronous formalism is ideally suited for expressing such transformations since it is lexicalized, and has an extended domain of locality. the impact of the in future work we intend to determine more lexicostructural transformations, rather than pure syntactic transformations. However, one major impediment to obtaining better results is the disparity in genre and domain which affects the overall performance. 7 Conclusion We have presented a new probabilistic synchronous TAG formalism, synchronous Tree Substitution Grammar with sister adjunction (TSG+SA). We have introduced the concept of a hidden TAG model, analogous to a Hidden Markov Model. It allows us to parse a resourcepoor language using a treebank-extracted probabilistic grammar for a resource-rich language, along with a hand-crafted synchronous grammar for the resource-poor language. Thus, our model combines statistical approaches with grammar engineering (specifically motivated from the linguistic facts). Our approach thus exemplifies the specific strength of a grammar-based approach. While we have applied this approach to two closely related languages, it would be interesting to apply this approach to more distantly related languages in the future. Acknowledgments This paper is based on work done at the 2005 Johns Hopkins Summer Workshop, which was partially supported by the National Science Foundation under grant 0121285. The first author was additionally supported by ONR MURI contract FCPO.810548265 and Department of Defense contract RD-02-5700. The second author was additionally supported by contract HR0011- 06-C-0023 under the GALE program. We wish to thank the other members of our JHU team (our co-authors on (Rambow et al., 2005)), especially Nizar Habash and Mona Diab for their help with the Arabic examples, and audiences at JHU for their useful feedback.</abstract>
<title confidence="0.733815">References</title>
<author confidence="0.746939">the Parameter Space of</author>
<affiliation confidence="0.7248285">Lexicalized Parsing Ph.D. thesis, University of Pennsylvania.</affiliation>
<address confidence="0.710116">Chen. 2001. Efficient Statistical Parsing</address>
<note confidence="0.886046666666667">Lexicalized Grammatical Ph.D. thesis, University of Delaware. 7 LP no tags LR F1 LP gold tags LR F1 Baseline 59.4 51.9 55.4 64.0 58.3 61.0 Lexical 63.0 60.8 61.9 66.9 67.0 66.9</note>
<phone confidence="0.80217425">66.9 66.7 66.8 67.0 67.0 67.0 67.4 67.0 67.2 67.4 67.1 67.3</phone>
<note confidence="0.69492365">Table 1: Results on development corpus: LP = labeled precision, LR = labeled recall, F1 = balanced F-measure Baseline no gold 60.2 67.1 Table 2: Results on the test corpus: F1 = balanced F-measure David Chiang and Daniel M. Bikel. 2002. Recovering information in treebanks. In of the Nineteenth International Conference on Compu- Linguistics pages 183–189. David Chiang, Mona Diab, Nizar Habash, Owen Rambow, and Safiullah Shareef. 2006. Parsing Arabic In David Chiang. 2000. Statistical parsing with an automatically-extracted tree adjoining grammar. In 38th Meeting of the Association for Computational pages 456–463, Hong Kong, China. Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2004. Bootstrapping parsers via syntactic projection across parallel texts.</note>
<title confidence="0.683093">Language</title>
<author confidence="0.676431">Tregex</author>
<abstract confidence="0.527573166666667">Tsurgeon: tools for querying and manipulating tree structures. In Mohamed Maamouri, Ann Bies, and Tim Buckwalter. 2004. The Penn Arabic Treebank: Building a largeannotated Arabic corpus. In Conon Arabic Language Resources and</abstract>
<address confidence="0.605829">Cairo, Egypt.</address>
<author confidence="0.780036333333333">Developing</author>
<author confidence="0.780036333333333">using a pilot</author>
<affiliation confidence="0.477146">Arabic treebank. In</affiliation>
<address confidence="0.791223">Genoa, Italy.</address>
<note confidence="0.890490307692308">Owen Rambow, K. Vijay-Shanker, and David Weir. D-Tree Substitution Grammars. Computa- 27(1). Owen Rambow, David Chiang, Mona Diab, Nizar Habash, Rebecca Hwa, Khalil Sima’an, Vincent Lacey, Roger Levy, Carol Nichols, and Safiullah Shareef. 2005. Parsing Arabic dialects. Final Report, 2005 JHU Summer Workshop. Yves Schabes and Stuart Shieber. 1994. An alternative of tree-adjoining derivation. Computa- 1(20):91–124. Schabes. 1990. and Computa- Aspects ofLexicalized Ph.D. the-</note>
<affiliation confidence="0.884477">sis, Department of Computer and Information Science, University of Pennsylvania.</affiliation>
<author confidence="0.604707">Restricting the weak genera-</author>
<affiliation confidence="0.496002">tive capacity of Synchronous Tree Adjoining Gram-</affiliation>
<note confidence="0.721465875">10(4):371–385. David A. Smith and Noah A. Smith. 2004. Bilingual parsing with factored estimation: Using English to Korean. In of the 2004 Conference on Empirical Methods in Natural Language Fei Xia, Martha Palmer, and Aravind Joshi. 2000. A uniform method of grammar extraction and its appli- In of the 2000 Conference on</note>
<title confidence="0.855455">Empirical Methods in Natural Language Processing</title>
<author confidence="0.82318">Hong Kong</author>
<intro confidence="0.843604">8</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
</authors>
<title>On the Parameter Space of Generative Lexicalized Parsing Models.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="14342" citStr="Bikel (2004)" startWordPosition="2443" endWordPosition="2444">els. The initial estimates for the Pt1i are set by hand. The availability of three backoff models makes it easy to specify the initial guesses at an appropriate level of detail: for example, one might give a general probability of some α� mapping to a0 using Pt13, but then make special exceptions for particular lexical anchors using Pt11 or Pt12. Finally Pt2 is reestimated by EM on some heldout unannotated sentences of L0, using the same method as Chiang and Bikel (2002) but on the syntactic transfer probabilities instead of the monolingual parsing model. Another difference is that, following Bikel (2004), we do not recalculate the Ai at each iteration, but use the initial values throughout. 5 A Synchronous TSG-SA for Dialectal Arabic Just as the probability model discussed in the preceding section factored the rewriting probabilities 5 into three parts, we create a synchronous TSG-SA and the probabilities of a hidden TAG model in three steps: • Ps and Psa are the parameters of a monolingual TSG+SA for MSA. We extract a grammar for the resource-rich language (MSA) from the Penn Arabic Treebank in a process described by Chiang and others (Chiang, 2000; Xia et al., 2000; Chen, 2001). • For the l</context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>Daniel M. Bikel. 2004. On the Parameter Space of Generative Lexicalized Parsing Models. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Chen</author>
</authors>
<title>Towards Efficient Statistical Parsing Using Lexicalized Grammatical Information.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Delaware.</institution>
<contexts>
<context position="14929" citStr="Chen, 2001" startWordPosition="2546" endWordPosition="2547">, following Bikel (2004), we do not recalculate the Ai at each iteration, but use the initial values throughout. 5 A Synchronous TSG-SA for Dialectal Arabic Just as the probability model discussed in the preceding section factored the rewriting probabilities 5 into three parts, we create a synchronous TSG-SA and the probabilities of a hidden TAG model in three steps: • Ps and Psa are the parameters of a monolingual TSG+SA for MSA. We extract a grammar for the resource-rich language (MSA) from the Penn Arabic Treebank in a process described by Chiang and others (Chiang, 2000; Xia et al., 2000; Chen, 2001). • For the lexical transfer model Pte, we create by hand a probabilistic mapping between (word, POS tag) pairs in the two languages. • For the syntactic transfer model Pti, we created by hand a grammar for the resource-poor language and a mapping between elementary trees in the two grammars, along with initial guesses for the mapping probabilities. We discuss the hand-crafted lexicon and synchronous grammar in the following subsections. 5.1 Lexical Mapping We used a small, hand-crafted lexicon of 100 words which mapped all LA function words and some of the most common open-class words to MSA.</context>
</contexts>
<marker>Chen, 2001</marker>
<rawString>John Chen. 2001. Towards Efficient Statistical Parsing Using Lexicalized Grammatical Information. Ph.D. thesis, University of Delaware.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Daniel M Bikel</author>
</authors>
<title>Recovering latent information in treebanks.</title>
<date>2002</date>
<booktitle>In Proceedings of the Nineteenth International Conference on Computational Linguistics (COLING),</booktitle>
<pages>183--189</pages>
<contexts>
<context position="14205" citStr="Chiang and Bikel (2002)" startWordPosition="2420" endWordPosition="2423">in the monolingual parser, is simply set to 1 if an estimate is available for that level, so that it completely overrides the further backed-off models. The initial estimates for the Pt1i are set by hand. The availability of three backoff models makes it easy to specify the initial guesses at an appropriate level of detail: for example, one might give a general probability of some α� mapping to a0 using Pt13, but then make special exceptions for particular lexical anchors using Pt11 or Pt12. Finally Pt2 is reestimated by EM on some heldout unannotated sentences of L0, using the same method as Chiang and Bikel (2002) but on the syntactic transfer probabilities instead of the monolingual parsing model. Another difference is that, following Bikel (2004), we do not recalculate the Ai at each iteration, but use the initial values throughout. 5 A Synchronous TSG-SA for Dialectal Arabic Just as the probability model discussed in the preceding section factored the rewriting probabilities 5 into three parts, we create a synchronous TSG-SA and the probabilities of a hidden TAG model in three steps: • Ps and Psa are the parameters of a monolingual TSG+SA for MSA. We extract a grammar for the resource-rich language </context>
</contexts>
<marker>Chiang, Bikel, 2002</marker>
<rawString>David Chiang and Daniel M. Bikel. 2002. Recovering latent information in treebanks. In Proceedings of the Nineteenth International Conference on Computational Linguistics (COLING), pages 183–189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Mona Diab</author>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
<author>Safiullah Shareef</author>
</authors>
<title>Parsing Arabic dialects.</title>
<date>2006</date>
<booktitle>In Proceedings ofEACL.</booktitle>
<contexts>
<context position="4875" citStr="Chiang et al., 2006" startWordPosition="758" endWordPosition="762">ng on real-world data: sister-adjunction (needed for generating the flat structures found in most treebanks), smoothing, and Inside-Outside reestimation. This paper is structured as follows. We first briefly discuss related work (Section 2) and some of the linguistic facts that motivate this work (Section 3). We then present the formalism, probabilistic model, and parsing algorithm (Section 4). Finally, we discuss the manual grammar engineering (Section 5) and evaluation (Section 6). 2 Related Work This paper is part of a larger investigation into parsing Arabic dialects (Rambow et al., 2005; Chiang et al., 2006). In that investigation, we examined three different approaches: • Sentence transduction, in which a dialect sentence is roughly translated into one or more MSA sentences and then parsed by an MSA parser. • Treebank transduction, in which the MSA treebank is transduced into an approximation of a LA treebank, on which a LA parer is then trained. • Grammar transduction, which is the name given in the overview papers to the approach discussed in this paper. The present paper provides for the first time a complete technical presentation of this approach. Overall, grammar transduction outperformed </context>
</contexts>
<marker>Chiang, Diab, Habash, Rambow, Shareef, 2006</marker>
<rawString>David Chiang, Mona Diab, Nizar Habash, Owen Rambow, and Safiullah Shareef. 2006. Parsing Arabic dialects. In Proceedings ofEACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Statistical parsing with an automatically-extracted tree adjoining grammar.</title>
<date>2000</date>
<booktitle>In 38th Meeting of the Association for Computational Linguistics (ACL’00),</booktitle>
<pages>456--463</pages>
<location>Hong Kong, China.</location>
<contexts>
<context position="3570" citStr="Chiang, 2000" startWordPosition="563" endWordPosition="564">us (except for development and testing), nor of a parallel LA-MSA corpus. The approach described in this paper uses a special parameterization of stochastic synchronous TAG (Shieber, 1994) which we call a “hidden TAG model.” This model couples a model of MSA trees, learned from the Arabic Treebank, with a model of MSA-LA translation, which is initialized by hand and then trained in an unsupervised fashion. Parsing new LA sentences then entails simultaneously building a forest of MSA trees and the corresponding forest of LA trees. Our implementation uses an extension of our monolingual parser (Chiang, 2000) based on tree-substitution 1 Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 1–8, Sydney, July 2006. c�2006 Association for Computational Linguistics grammar with sister adjunction (TSG+SA). The main contributions of this paper are as follows: 1. We introduce the novel concept of a hidden TAG model. 2. We use this model to combine statistical approaches with grammar engineering (specifically motivated from the linguistic facts). Our approach thus exemplifies the specific strength of a grammar-based approach. 3. We present an implementation</context>
<context position="8297" citStr="Chiang, 2000" startWordPosition="1366" endWordPosition="1367"> NEG ‘not’ NP-OBJ N Figure 1: LDC-style left-to-right phrase structure trees for LA (left) and MSA (right) for sentence (1) ‘like’ ’like’ ‘work’ ‘men’ ‘not’ ‘work’ ‘men’ ‘not’ ‘this’ ‘this’ Figure 2: Unordered dependency trees for LA (left) and MSA (right) for sentence (1) off NP assets S NP VP Qintex VBD sold NP NNP NP PRT RP NNS (α1) (α2) (α3) (α4) Figure 3: Example elementary trees. structure trees, the (unordered) dependency trees for the MSA and LA sentences are isomorphic, as shown in Figure 2. They differ only in the node labels. 4 Model 4.1 The synchronous TSG+SA formalism Our parser (Chiang, 2000) is based on synchronous tree-substitution grammar with sisteradjunction (TSG+SA). Tree-substitution grammar (Schabes, 1990) is TAG without auxiliary trees or adjunction; instead we include a weaker composition operation, sister-adjunction (Rambow et al., 2001), in which an initial tree is inserted between two sister nodes (see Figure 4). We allow multiple sister-adjunctions at the same site, similar to how Schabes and Shieber (1994) allow multiple adjunctions of modifier auxiliary trees. A synchronous TSG+SA is a set of pairs of elementary trees. In each pair, there is a one-to-one correspond</context>
<context position="10962" citStr="Chiang, 2000" startWordPosition="1832" endWordPosition="1833"> 4: Sister-adjunction, with inserted material shown with shaded background NP V S VP V NPI1 NPI2 ‘like’ I S NPiI 1 VP NPI 2 ti ‘like’ � � � � � � � � � � � � � � � � Figure 5: Example elementary tree pair of a synchronous TSG: the SVO transformation (LA on left, MSA on right) 4 hn, n0iis Ps(α, α0 |n, n0), and the probability of sister-adjoining hα, α0i at a sister-adjunction site pair hn, i, n0, i0i is Psa(α, α0 |n, i, n0, i0), where i and i0 indicate that the sister-adjunction occurs between the i and (i + 1)st (or i0 and (i0 + 1)st) sisters. These parameters must satisfy the normalTreebank (Chiang, 2000); the details are not important here. As for Pt, in order to obtain better probability estimates, we further decompose Pt into Pt1 and Pt2 so they can be estimated separately (as in the monolingual parsing model): ization conditions Ps(α,α0 |n,n0) = 1 (1) Pt(α0 |α) ≈ Pt1(a0 |a, w0, t0, w, t) × E Pt2(w0, t0 |w, t) (5) a,a/ E Psa(α, α0 |n, i, n0, i0) + a,a/ Psa(STOP |n, i, n0, i0) = 1 (2) 4.2 Parsing by translation We intend to apply a stochastic synchronous TSG+SA to input sentences 50. This requires projecting any constraints from the unprimed side of the synchronous grammar over to the primed</context>
<context position="14898" citStr="Chiang, 2000" startWordPosition="2540" endWordPosition="2541">odel. Another difference is that, following Bikel (2004), we do not recalculate the Ai at each iteration, but use the initial values throughout. 5 A Synchronous TSG-SA for Dialectal Arabic Just as the probability model discussed in the preceding section factored the rewriting probabilities 5 into three parts, we create a synchronous TSG-SA and the probabilities of a hidden TAG model in three steps: • Ps and Psa are the parameters of a monolingual TSG+SA for MSA. We extract a grammar for the resource-rich language (MSA) from the Penn Arabic Treebank in a process described by Chiang and others (Chiang, 2000; Xia et al., 2000; Chen, 2001). • For the lexical transfer model Pte, we create by hand a probabilistic mapping between (word, POS tag) pairs in the two languages. • For the syntactic transfer model Pti, we created by hand a grammar for the resource-poor language and a mapping between elementary trees in the two grammars, along with initial guesses for the mapping probabilities. We discuss the hand-crafted lexicon and synchronous grammar in the following subsections. 5.1 Lexical Mapping We used a small, hand-crafted lexicon of 100 words which mapped all LA function words and some of the most </context>
</contexts>
<marker>Chiang, 2000</marker>
<rawString>David Chiang. 2000. Statistical parsing with an automatically-extracted tree adjoining grammar. In 38th Meeting of the Association for Computational Linguistics (ACL’00), pages 456–463, Hong Kong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philip Resnik</author>
<author>Amy Weinberg</author>
<author>Clara Cabezas</author>
<author>Okan Kolak</author>
</authors>
<title>Bootstrapping parsers via syntactic projection across parallel texts. Natural Language Engineering.</title>
<date>2004</date>
<contexts>
<context position="5662" citStr="Hwa et al., 2004" startWordPosition="891" endWordPosition="894"> and then parsed by an MSA parser. • Treebank transduction, in which the MSA treebank is transduced into an approximation of a LA treebank, on which a LA parer is then trained. • Grammar transduction, which is the name given in the overview papers to the approach discussed in this paper. The present paper provides for the first time a complete technical presentation of this approach. Overall, grammar transduction outperformed the other two approaches. In other work, there has been a fair amount of interest in parsing one language using another language, see for example (Smith and Smith, 2004; Hwa et al., 2004). Much of this work, like ours, relies on synchronous grammars (CFGs). However, these approaches rely on parallel corpora. For MSA and its dialects, there are no naturally occurring parallel corpora. It is this fact that has led us to investigate the use of explicit linguistic knowledge to complement machine learning. 3 Linguistic Facts We illustrate the differences between LA and MSA using an example: (1) a. (LA) Al$gl hdA the-work this the men do not like this work b. (MSA) AlrjAl h*A AlEml the-men this the-work the men do not like this work Lexically, we observe that the word for ‘work’ is </context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Cabezas, Kolak, 2004</marker>
<rawString>Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2004. Bootstrapping parsers via syntactic projection across parallel texts. Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>Galen Andrew</author>
</authors>
<title>Tregex and Tsurgeon: tools for querying and manipulating tree data structures.</title>
<date>2006</date>
<booktitle>In Proceedings ofLREC.</booktitle>
<contexts>
<context position="16000" citStr="Levy and Andrew, 2006" startWordPosition="2725" endWordPosition="2728">ical Mapping We used a small, hand-crafted lexicon of 100 words which mapped all LA function words and some of the most common open-class words to MSA. We assigned uniform probabilities to the mapping. All other MSA words were assumed to also be LA words. Unknown LA words were handled using the standard unknown word mechanism. 5.2 Syntactic Mapping Because of the underlying syntactic similarity between the two varieties of Arabic, we assume that every tree in the MSA grammar extracted from the MSA treebank is also a LA tree. In addition, we define tree transformations in the Tsurgeon package (Levy and Andrew, 2006). These consist of a pattern which matches MSA elementary trees in the extracted grammar, and a transformation which produces a LA elementary tree. We perform the following tree transformations on all elementary trees which match the underlying MSA pattern. Thus, each MSA tree corresponds to at least two LA trees: the original one and the transformed one. If several transformations apply, we obtain multiple transformed trees. • Negation (NEG): we insert a $ negation marker immediately following each verb. The preverbal marker is generated by a lexical translation of an MSA elementary tree. • V</context>
</contexts>
<marker>Levy, Andrew, 2006</marker>
<rawString>Roger Levy and Galen Andrew. 2006. Tregex and Tsurgeon: tools for querying and manipulating tree data structures. In Proceedings ofLREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Maamouri</author>
<author>Ann Bies</author>
<author>Tim Buckwalter</author>
</authors>
<title>The Penn Arabic Treebank: Building a largescale annotated Arabic corpus.</title>
<date>2004</date>
<booktitle>In NEMLAR Conference on Arabic Language Resources and Tools,</booktitle>
<location>Cairo, Egypt.</location>
<contexts>
<context position="7495" citStr="Maamouri et al., 2004" startWordPosition="1221" endWordPosition="1224">. This is in fact not a strict requirement, but a strong preference: both varieties allow both orders, but in the dialects, the SVO order is more common, while in MSA, the VSO order is more common. Second, we see that the demonstrative determiner follows the noun in LA, but precedes it in MSA. Finally, we see that the negation marker follows the verb in LA, while it precedes the verb in MSA. (Levantine also has other negation markers that precede the verb, as well as the circumfix m- -$.) The two phrase structure trees are shown in Figure 1 in the convention of the Linguistic Data Consortium (Maamouri et al., 2004). Unlike the phrase AlrjAl the-men byHbw like $ not lA not yHb like 2 ‘like’ ‘not’ ‘work’ ‘this’ S VP ‘this’ ‘work’ NP-TPC ‘men’i V S NEG VP NP-SBJ NP-OBJ DET ti N NP-SBJ V DET ‘men’ ‘like’ NEG ‘not’ NP-OBJ N Figure 1: LDC-style left-to-right phrase structure trees for LA (left) and MSA (right) for sentence (1) ‘like’ ’like’ ‘work’ ‘men’ ‘not’ ‘work’ ‘men’ ‘not’ ‘this’ ‘this’ Figure 2: Unordered dependency trees for LA (left) and MSA (right) for sentence (1) off NP assets S NP VP Qintex VBD sold NP NNP NP PRT RP NNS (α1) (α2) (α3) (α4) Figure 3: Example elementary trees. structure trees, the (</context>
</contexts>
<marker>Maamouri, Bies, Buckwalter, 2004</marker>
<rawString>Mohamed Maamouri, Ann Bies, and Tim Buckwalter. 2004. The Penn Arabic Treebank: Building a largescale annotated Arabic corpus. In NEMLAR Conference on Arabic Language Resources and Tools, Cairo, Egypt.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Maamouri</author>
<author>Ann Bies</author>
<author>Tim Buckwalter</author>
<author>Mona Diab</author>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
<author>Dalila Tabessi</author>
</authors>
<title>Developing and using a pilot dialectal Arabic treebank.</title>
<date>2006</date>
<booktitle>In Proceedings ofLREC,</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="17877" citStr="Maamouri et al., 2006" startWordPosition="3040" endWordPosition="3043">cordingly, we defined a transformation that translated all the verbs meaning ‘want’/‘need’ into the noun bd and changed their respective POS tag to NN. The subject clitic is transformed into a possessive pronoun clitic. Note that this construction is a combination lexical and syntactic transformation, and thus specifically exploits the extended domain of locality of TAG-like formalisms. One possible resulting pair of trees is shown in Figure 6. 6 Experimental Results While our approach does not rely on any annotated corpus for LA, nor on a parallel corpus MSALA, we use a small treebank of LA (Maamouri et al., 2006) to analyze and test our approach. The LA treebank is divided into a development corpus and a test corpus, each about 11,000 tokens (using the same tokenization scheme as employed in the MSA treebank). We first use the development corpus to determine which of the transformations are useful. We use two conditions. In the first, the input text is not tagged, and the parser hypothesizes tags. In the second, the input text is tagged with the gold (correct) tag. The results are shown in Table 1. The baseline is simply the application of a pure MSA Chiang parser to LA. We see that important improvem</context>
</contexts>
<marker>Maamouri, Bies, Buckwalter, Diab, Habash, Rambow, Tabessi, 2006</marker>
<rawString>Mohamed Maamouri, Ann Bies, Tim Buckwalter, Mona Diab, Nizar Habash, Owen Rambow, and Dalila Tabessi. 2006. Developing and using a pilot dialectal Arabic treebank. In Proceedings ofLREC, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Owen Rambow</author>
<author>K Vijay-Shanker</author>
<author>David Weir</author>
</authors>
<title>D-Tree Substitution Grammars.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>1</issue>
<contexts>
<context position="8558" citStr="Rambow et al., 2001" startWordPosition="1399" endWordPosition="1402">t) for sentence (1) off NP assets S NP VP Qintex VBD sold NP NNP NP PRT RP NNS (α1) (α2) (α3) (α4) Figure 3: Example elementary trees. structure trees, the (unordered) dependency trees for the MSA and LA sentences are isomorphic, as shown in Figure 2. They differ only in the node labels. 4 Model 4.1 The synchronous TSG+SA formalism Our parser (Chiang, 2000) is based on synchronous tree-substitution grammar with sisteradjunction (TSG+SA). Tree-substitution grammar (Schabes, 1990) is TAG without auxiliary trees or adjunction; instead we include a weaker composition operation, sister-adjunction (Rambow et al., 2001), in which an initial tree is inserted between two sister nodes (see Figure 4). We allow multiple sister-adjunctions at the same site, similar to how Schabes and Shieber (1994) allow multiple adjunctions of modifier auxiliary trees. A synchronous TSG+SA is a set of pairs of elementary trees. In each pair, there is a one-to-one correspondence between the substitution/sisteradjunction sites of the two trees, which we represent using boxed indices (Figure 5). A derivation then starts with a pair of initial trees and proceeds by substituting or sister-adjoining elementary tree pairs at coindexed s</context>
</contexts>
<marker>Rambow, Vijay-Shanker, Weir, 2001</marker>
<rawString>Owen Rambow, K. Vijay-Shanker, and David Weir. 2001. D-Tree Substitution Grammars. Computational Linguistics, 27(1).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Owen Rambow</author>
<author>David Chiang</author>
</authors>
<location>Mona Diab, Nizar Habash, Rebecca Hwa, Khalil Sima’an, Vincent</location>
<marker>Rambow, Chiang, </marker>
<rawString>Owen Rambow, David Chiang, Mona Diab, Nizar Habash, Rebecca Hwa, Khalil Sima’an, Vincent</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy Lacey</author>
<author>Carol Nichols</author>
<author>Safiullah Shareef</author>
</authors>
<title>Parsing Arabic dialects. Final Report,</title>
<date>2005</date>
<note>JHU Summer Workshop.</note>
<marker>Lacey, Nichols, Shareef, 2005</marker>
<rawString>Lacey, Roger Levy, Carol Nichols, and Safiullah Shareef. 2005. Parsing Arabic dialects. Final Report, 2005 JHU Summer Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Stuart Shieber</author>
</authors>
<title>An alternative conception of tree-adjoining derivation.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>1</volume>
<issue>20</issue>
<contexts>
<context position="8734" citStr="Schabes and Shieber (1994)" startWordPosition="1429" endWordPosition="1432">pendency trees for the MSA and LA sentences are isomorphic, as shown in Figure 2. They differ only in the node labels. 4 Model 4.1 The synchronous TSG+SA formalism Our parser (Chiang, 2000) is based on synchronous tree-substitution grammar with sisteradjunction (TSG+SA). Tree-substitution grammar (Schabes, 1990) is TAG without auxiliary trees or adjunction; instead we include a weaker composition operation, sister-adjunction (Rambow et al., 2001), in which an initial tree is inserted between two sister nodes (see Figure 4). We allow multiple sister-adjunctions at the same site, similar to how Schabes and Shieber (1994) allow multiple adjunctions of modifier auxiliary trees. A synchronous TSG+SA is a set of pairs of elementary trees. In each pair, there is a one-to-one correspondence between the substitution/sisteradjunction sites of the two trees, which we represent using boxed indices (Figure 5). A derivation then starts with a pair of initial trees and proceeds by substituting or sister-adjoining elementary tree pairs at coindexed sites. In this way a set of string pairs (5, 5&apos;) is generated. Sister-adjunction presents a special problem for synchronization: if multiple tree pairs sisteradjoin at the same </context>
</contexts>
<marker>Schabes, Shieber, 1994</marker>
<rawString>Yves Schabes and Stuart Shieber. 1994. An alternative conception of tree-adjoining derivation. Computational Linguistics, 1(20):91–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
</authors>
<title>Mathematical and Computational Aspects ofLexicalized Grammars.</title>
<date>1990</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer and Information Science, University of Pennsylvania.</institution>
<contexts>
<context position="8421" citStr="Schabes, 1990" startWordPosition="1381" endWordPosition="1382"> ‘like’ ’like’ ‘work’ ‘men’ ‘not’ ‘work’ ‘men’ ‘not’ ‘this’ ‘this’ Figure 2: Unordered dependency trees for LA (left) and MSA (right) for sentence (1) off NP assets S NP VP Qintex VBD sold NP NNP NP PRT RP NNS (α1) (α2) (α3) (α4) Figure 3: Example elementary trees. structure trees, the (unordered) dependency trees for the MSA and LA sentences are isomorphic, as shown in Figure 2. They differ only in the node labels. 4 Model 4.1 The synchronous TSG+SA formalism Our parser (Chiang, 2000) is based on synchronous tree-substitution grammar with sisteradjunction (TSG+SA). Tree-substitution grammar (Schabes, 1990) is TAG without auxiliary trees or adjunction; instead we include a weaker composition operation, sister-adjunction (Rambow et al., 2001), in which an initial tree is inserted between two sister nodes (see Figure 4). We allow multiple sister-adjunctions at the same site, similar to how Schabes and Shieber (1994) allow multiple adjunctions of modifier auxiliary trees. A synchronous TSG+SA is a set of pairs of elementary trees. In each pair, there is a one-to-one correspondence between the substitution/sisteradjunction sites of the two trees, which we represent using boxed indices (Figure 5). A </context>
</contexts>
<marker>Schabes, 1990</marker>
<rawString>Yves Schabes. 1990. Mathematical and Computational Aspects ofLexicalized Grammars. Ph.D. thesis, Department of Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart B Shieber</author>
</authors>
<title>Restricting the weak generative capacity of Synchronous Tree Adjoining Grammar.</title>
<date>1994</date>
<journal>Computational Intelligence,</journal>
<volume>10</volume>
<issue>4</issue>
<contexts>
<context position="3145" citStr="Shieber, 1994" startWordPosition="491" endWordPosition="492">parsers are typically trained on large corpora of parse trees. We present one solution to this problem, based on the assumption that it is easier to manually create new resources that relate a dialect to MSA (lexicon and grammar) than it is to manually create syntactically annotated corpora in the dialect. In this paper, we deal with Levantine Arabic (LA). Our approach does not assume the existence of any annotated LA corpus (except for development and testing), nor of a parallel LA-MSA corpus. The approach described in this paper uses a special parameterization of stochastic synchronous TAG (Shieber, 1994) which we call a “hidden TAG model.” This model couples a model of MSA trees, learned from the Arabic Treebank, with a model of MSA-LA translation, which is initialized by hand and then trained in an unsupervised fashion. Parsing new LA sentences then entails simultaneously building a forest of MSA trees and the corresponding forest of LA trees. Our implementation uses an extension of our monolingual parser (Chiang, 2000) based on tree-substitution 1 Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 1–8, Sydney, July 2006. c�2006 Association </context>
<context position="8734" citStr="Shieber (1994)" startWordPosition="1431" endWordPosition="1432">es for the MSA and LA sentences are isomorphic, as shown in Figure 2. They differ only in the node labels. 4 Model 4.1 The synchronous TSG+SA formalism Our parser (Chiang, 2000) is based on synchronous tree-substitution grammar with sisteradjunction (TSG+SA). Tree-substitution grammar (Schabes, 1990) is TAG without auxiliary trees or adjunction; instead we include a weaker composition operation, sister-adjunction (Rambow et al., 2001), in which an initial tree is inserted between two sister nodes (see Figure 4). We allow multiple sister-adjunctions at the same site, similar to how Schabes and Shieber (1994) allow multiple adjunctions of modifier auxiliary trees. A synchronous TSG+SA is a set of pairs of elementary trees. In each pair, there is a one-to-one correspondence between the substitution/sisteradjunction sites of the two trees, which we represent using boxed indices (Figure 5). A derivation then starts with a pair of initial trees and proceeds by substituting or sister-adjoining elementary tree pairs at coindexed sites. In this way a set of string pairs (5, 5&apos;) is generated. Sister-adjunction presents a special problem for synchronization: if multiple tree pairs sisteradjoin at the same </context>
</contexts>
<marker>Shieber, 1994</marker>
<rawString>Stuart B. Shieber. 1994. Restricting the weak generative capacity of Synchronous Tree Adjoining Grammar. Computational Intelligence, 10(4):371–385.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Noah A Smith</author>
</authors>
<title>Bilingual parsing with factored estimation: Using English to parse Korean.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP04).</booktitle>
<contexts>
<context position="5643" citStr="Smith and Smith, 2004" startWordPosition="887" endWordPosition="890">e or more MSA sentences and then parsed by an MSA parser. • Treebank transduction, in which the MSA treebank is transduced into an approximation of a LA treebank, on which a LA parer is then trained. • Grammar transduction, which is the name given in the overview papers to the approach discussed in this paper. The present paper provides for the first time a complete technical presentation of this approach. Overall, grammar transduction outperformed the other two approaches. In other work, there has been a fair amount of interest in parsing one language using another language, see for example (Smith and Smith, 2004; Hwa et al., 2004). Much of this work, like ours, relies on synchronous grammars (CFGs). However, these approaches rely on parallel corpora. For MSA and its dialects, there are no naturally occurring parallel corpora. It is this fact that has led us to investigate the use of explicit linguistic knowledge to complement machine learning. 3 Linguistic Facts We illustrate the differences between LA and MSA using an example: (1) a. (LA) Al$gl hdA the-work this the men do not like this work b. (MSA) AlrjAl h*A AlEml the-men this the-work the men do not like this work Lexically, we observe that the </context>
</contexts>
<marker>Smith, Smith, 2004</marker>
<rawString>David A. Smith and Noah A. Smith. 2004. Bilingual parsing with factored estimation: Using English to parse Korean. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Martha Palmer</author>
<author>Aravind Joshi</author>
</authors>
<title>A uniform method of grammar extraction and its applications.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 Conference on Empirical Methods in Natural Language Processing (EMNLP00),</booktitle>
<location>Hong Kong.</location>
<contexts>
<context position="14916" citStr="Xia et al., 2000" startWordPosition="2542" endWordPosition="2545">difference is that, following Bikel (2004), we do not recalculate the Ai at each iteration, but use the initial values throughout. 5 A Synchronous TSG-SA for Dialectal Arabic Just as the probability model discussed in the preceding section factored the rewriting probabilities 5 into three parts, we create a synchronous TSG-SA and the probabilities of a hidden TAG model in three steps: • Ps and Psa are the parameters of a monolingual TSG+SA for MSA. We extract a grammar for the resource-rich language (MSA) from the Penn Arabic Treebank in a process described by Chiang and others (Chiang, 2000; Xia et al., 2000; Chen, 2001). • For the lexical transfer model Pte, we create by hand a probabilistic mapping between (word, POS tag) pairs in the two languages. • For the syntactic transfer model Pti, we created by hand a grammar for the resource-poor language and a mapping between elementary trees in the two grammars, along with initial guesses for the mapping probabilities. We discuss the hand-crafted lexicon and synchronous grammar in the following subsections. 5.1 Lexical Mapping We used a small, hand-crafted lexicon of 100 words which mapped all LA function words and some of the most common open-class </context>
</contexts>
<marker>Xia, Palmer, Joshi, 2000</marker>
<rawString>Fei Xia, Martha Palmer, and Aravind Joshi. 2000. A uniform method of grammar extraction and its applications. In Proceedings of the 2000 Conference on Empirical Methods in Natural Language Processing (EMNLP00), Hong Kong.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>