<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.998421">
Incremental Reference Resolution: The Task, Metrics for Evaluation, and
a Bayesian Filtering Model that is Sensitive to Disfluencies
</title>
<author confidence="0.990629">
David Schlangen, Timo Baumann, Michaela Atterer
</author>
<affiliation confidence="0.9900665">
Department of Linguistics
University of Potsdam, Germany
</affiliation>
<email confidence="0.997991">
{das|timo|atterer}@ling.uni-potsdam.de
</email>
<sectionHeader confidence="0.998649" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997039">
In this paper we do two things: a) we dis-
cuss in general terms the task of incre-
mental reference resolution (IRR), in par-
ticular resolution of exophoric reference,
and specify metrics for measuring the per-
formance of dialogue system components
tackling this task, and b) we present a sim-
ple Bayesian filtering model of IRR that
performs reasonably well just using words
directly (no structure information and no
hand-coded semantics): it picks the right
referent out of 12 for around 50 % of real-
world dialogue utterances in our test cor-
pus. It is also able to learn to interpret not
only words but also hesitations, just as hu-
mans have shown to do in similar situa-
tions, namely as markers of references to
hard-to-describe entities.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999315">
Like other tasks involved in language comprehen-
sion, reference resolution—that is, the linking of
natural language expressions to contextually given
entities—is performed incrementally by human
listeners. This was shown for example by Tanen-
haus et al. (1995) in a famous experiment where
addressees of utterances containing referring ex-
pressions made eye movements towards target ob-
jects very shortly after the end of the first word
that unambiguously specified the referent, even if
that wasn’t the final word of the phrase. In fact, as
has been shown in later experiments (Brennan and
Schober, 2001; Bailey and Ferreira, 2007; Arnold
et al., 2007), such disambiguating material doesn’t
even have to be lexical: under certain circum-
stances, a speaker’s hesitating already seems to be
understood as increasing the likelihood of subse-
quent reference to hard-to-describe entities.
Recently, efforts have begun to build dialogue
systems that make use of incremental processing
as well (Aist et al., 2006; Skantze and Schlangen,
2009). These efforts have so far focused on as-
pects other than resolution of references ((Stoness
et al., 2004) deals with the interaction of reference
and parsing). In this paper, we discuss in gen-
eral terms the task of incremental reference res-
olution (IRR) and specify metrics for evaluating
incremental components for this task. To make
the discussion more concrete, we also describe a
simple Bayesian filtering model of IRR in a do-
main with a small number of possible referents,
and show that it performs better wrt. our metrics
if given information about hesitations—thus pro-
viding computational support for the rationality of
including observables other than words into mod-
els of dialogue meaning.
The remainder of the paper is structured as fol-
lows: We discuss the IRR task in Section 2, and
suitable evaluation metrics in Section 3. In Sec-
tion 4 we describe and analyse the data for which
we present results with our Bayesian model for
IRR in Section 5.
</bodyText>
<sectionHeader confidence="0.997839" genericHeader="method">
2 Incremental Reference Resolution
</sectionHeader>
<bodyText confidence="0.9999487">
To a first approximation, IRR can be modeled as
the ‘inverse’ as it were of the task of generating re-
ferring expressions (GRE; which is well-studied in
computational linguistics, see e. g. (Dale and Re-
iter, 1995)). Where in GRE words are added that
express features which reduce the size of the set
of possible distractors (with which the object that
the expression is intended to pick out can be con-
fused), in IRR words are encountered that express
features that reduce the size of the set of possible
</bodyText>
<note confidence="0.709996">
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 30–37,
</note>
<affiliation confidence="0.66255">
Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics
</affiliation>
<page confidence="0.998288">
30
</page>
<bodyText confidence="0.999793666666667">
referents. To give a concrete example, for the ex-
pression in (1-a), we could imagine that the logical
representation in (1-b) is built on a word-by-word
basis, and at each step the expression is checked
against the world model to see whether the refer-
ence has become unique.
</bodyText>
<listItem confidence="0.593825">
(1) a. the red cross
b. tx(red(x) n cross(x))
</listItem>
<bodyText confidence="0.966279679245283">
To give an example, in a situation where there
are available for reference only one red cross, one
green circle, and two blue squares, we can say
that after “the red” the referent should have been
found; in a world with two red crosses, we would
need to wait for further restricting information
(e. g. “... on the left”).
This is one way to describe the task, then: a
component for incremental reference resolution
takes expressions as input in a word-by-word fash-
ion and delivers for each new input a set (possibly
a singleton set) as output which collects those dis-
course entities that are compatible with the expres-
sion up to that point. (This description is meant
to be neutral as to whether reference is exophoric,
i. e. directly to entities in the world, or anaphoric,
via previous mentions; we will mainly discuss the
former case, though.)
As we will see below, this does however
not translate directly into a usable metric for
evaluation. While it is easy to identify the
contributions of individual words in simple,
constructed expressions like (1-a), reference in
real conversations is often much more complex,
and is a collaborative process that isn’t confined
to single expressions (Clark and Schaefer, 1987):
referring is a pragmatic action that is not reducible
to denotation. In our corpus (see below), we often
find descriptions as in (2), where the speaker
continuously adds (rather vague) material, typi-
cally until the addressee signals that she identified
the item, or proposes a different way to describe it.
(2) Also das S Teil sieht so aus dass es ein
einzelnes . Teilchen hat . dann . vier am St¨uck
im rechten Winkel .. dazu nee . nee warte ..
dann noch ein einzelnes das guckt auf der an-
deren Seite raus.
well, the S piece looks so that it has a single . piece .
and then. four together in a 90 degree angle.. and also
. no .. wait.. and then a single piece that sticks out on
the other side.
While it’s difficult to say in the individual case
what the appropriate moment is to settle on a hy-
pothesis about the intended referent, and what the
“correct” time-course of the development of hy-
potheses is, it’s easy to say what we want to be true
in general: we want a referent to be found as early
as possible, with as little change of opinion as pos-
sible during the utterance.1 Hence a model that
finds the correct referent earlier and makes fewer
wrong decisions than a competing one will be con-
sidered better. The metrics we develop in the next
section spell out this idea.
</bodyText>
<sectionHeader confidence="0.99331" genericHeader="method">
3 Evaluation Metrics for IRR
</sectionHeader>
<bodyText confidence="0.999151775">
In previous work, we have discussed metrics for
evaluating the performance of incremental speech
recognition (Baumann et al., 2009). There, our
metrics could rely on time-aligned gold-standard
information against which the incremental results
could be measured. For the reasons discussed
in the previous section, we do not assume that
we have such temporally-aligned information for
evaluating IRR. Our measures described here sim-
ply assume that there is one intention behind the
referring utterances (namely to identify a certain
entity), and that this intention is there from the be-
ginning of the utterance and stays constant.2 This
is not to be understood as the claim that it is rea-
sonable to expect an IRR component to pick out a
referent even if the only part of the utterance that
has already been processed for example is “now
take the”—it just facilitates the “earlier is better”
ranking discussed above.
We use two kinds of metrics for IRR: posi-
tional metrics, which measure when (which per-
centage into the utterance) a certain event happens,
and edit metrics which capture the “jumpiness”
of the decision process (how often the component
changes its mind during an utterance).
Figure 1 shows a constructed example that il-
1We leave open here what “as early as possible” means—
a well-trained model might be able to resolve a reference
before the speaker even deems that possible, and hence ap-
pear to do unnatural (or supernatural?) ‘mind reading’. Con-
versely, frequent changes of opinion might be something that
human listeners would exhibit as well (e. g. in their gaze di-
rection). We abstract away from these finer details in our
heuristic.
2Note that our metrics would also work for corpora where
the correct point-of-identification is annotated; this would
simply move the reference point from the beginning of the
utterance to that point. Gallo et al. (2007) describe an anno-
tation effort in a simpler domain where entities can easily be
described which would make such information available.
</bodyText>
<page confidence="0.999945">
31
</page>
<figureCaption confidence="0.9980945">
Figure 1: Simple constructed example that illus-
trates the evaluation measures
</figureCaption>
<bodyText confidence="0.996882956521739">
lustrates these ideas. We assume that reference is
to an object that is internally represented by the
letter F. The example shows two models, no-sil
and sil (what exactly they are doesn’t matter for
now). The former model guesses that reference is
to object X already after the first word, and stays
with this opinion until it encounters the final word,
when it chooses F as most likely referent. (Why
the decision for the items sil is “-” will be ex-
plained below; here this can be read as “repetition
of previous decision”.) The other model changes
its mind more often, but also is correct for the first
time earlier and stays correct earlier. Our metrics
make this observation more precise:
• average fc (first correct): how deep into the ut-
terance do we make the first correct guess? (If the
decision component delivers n-best lists instead of
single guesses, “correct” means here and below “is
member of n-best list”.)
E. g., if the referent is recognised only after the
final word of the expression, the score for this met-
ric would be 1. In our example it is 2/5 for the
sil-model and 1 for the non-sil model.
</bodyText>
<listItem confidence="0.905211538461539">
• fc applicable: since the previous measure can
only be specified for cases where the correct refer-
ent has been found, we also specify for how many
utterances this is the case.
• average ff (first final): how deep into the utter-
ance do we make the correct guess and don’t sub-
sequently change our mind? This would be 4/5 for
the sil-model in our example and 1 for the no-sil-
model.
• ff applicable: again, the previous measure can
only be given where the final guess of the compo-
nent is correct, so we also need to specify how of-
ten this is the case. Note that whenever ff is appli-
</listItem>
<bodyText confidence="0.984059533333333">
cable, fc is applicable as well, so ff applicable&lt;fc
applicable.
• ed-utt (mean edits per utterance): an IRR mod-
ule may still change its mind even after it has al-
ready made a correct guess. This metric measures
how often the module changes its mind before it
comes back to the right guess (if at all). Since such
decision-revisions (edits) may be costly for later
modules, which possibly need to retract their own
hypotheses that they’ve built based on the output
of this module, ideally this number should be low.
In our example the number of edits between fc
and ff is 2 for the sil-model and 0 for the non-sil
model (because here fc and ff are at the same po-
sition).
</bodyText>
<listItem confidence="0.993949833333333">
• eo (edit overhead): ratio unnecessary edits / nec-
essary edits. (In the ideal case, there is exactly one
edit, from “no decision” to the correct guess.)
• correctness: how often the model guesses cor-
rectly. This is 3/5 for the sil-model in the example
and 1/5 for the non-sil-model.
• sil-correctness: how often the model guesses
correctly during hesitations. The correctness mea-
sure applied only to certain data-points; we use
this to investigate whether informing the model
about hesitations is helpful.
• adjusted error: some of our IRR models can re-
</listItem>
<bodyText confidence="0.997507458333333">
turn “undecided” as reply. The correctness mea-
sures defined above would punish this in the same
way as a wrong guess. The adjusted error measure
implements the idea that undecidedness is better
than a wrong guess, at least early in the utterance.
More precisely, it’s defined to be 0 if the guess is
correct, pos l posm. if the reply is “undecided”
(with pos denoting the position in the utterance),
and 1 if the guess is incorrect. That way uncer-
tainty is not punished in the beginning of the utter-
ance and counted like an error towards its end.
Note that these metrics characterise different as-
pects of the performance of a model. In practi-
cal cases, they may not be independent from each
other, and a system designer will have to decide
which one to optimize. If it is helpful to be in-
formed about a likely referent early, for example
to prepare a reaction, and is not terribly costly to
later have to revise hypotheses, then a low first cor-
rect may be the target. If hypothesis revisions are
costly, then a low edit overhead may be preferred
over a low first correct. (first final and ff applicable,
however, are parameters that are useful for global
optimisation.)
</bodyText>
<figure confidence="0.978951">
time
f
F
gold reference F F F F
words take (sil) the (sil)
no−sil−model X − X −
F
F
X F W
sil−model
F
first correct
first final
edit phase
32
slrt
</figure>
<figureCaption confidence="0.99852275">
Figure 2: The Twelve Pentomino Pieces with their
canonical names (which were not known to the di-
alogue participants). The pieces used in the dia-
logues all had the same colour.
</figureCaption>
<bodyText confidence="0.9993528">
In the remaining sections, we describe a prob-
abilistic model of IRR that we have implemented,
and evaluate it in terms of these metrics. We begin
with describing the data from which we learnt our
model.
</bodyText>
<sectionHeader confidence="0.999811" genericHeader="method">
4 Data
</sectionHeader>
<subsectionHeader confidence="0.996234">
4.1 Our Corpora
</subsectionHeader>
<bodyText confidence="0.880952451612903">
As the basis for training and testing of our model
we used data from three corpora of task-oriented
dialogue that differ in some details of the set-up,
but use the same task: an Instruction Giver (IG) in-
structs an Instruction Follower (IF) on which puz-
zle pieces (from the “Pentomino” game, see Fig-
ure 2) to pick up. In detail, the corpora were:
• The Pento Naming corpus described in (Siebert
and Schlangen, 2008). In this variant of the task,
IG records instructions for an absent IF; so these
aren’t fully interactive dialogues. The corpus con-
tained 270 utterances out of which we selected
those 143 that contained descriptions of puzzle
pieces (and not of their position on the game-
board).
• Selections from the FTT/PTT corpus described
in (Fern´andez et al., 2007), where IF and IG are
connected through an audio-only connection, and
in some dialogues a simplex / push-to-talk one.
We selected all utterances from IG that contained
references to puzzle pieces (286 altogether).
• The third part of our corpus was constructed
specifically for the experiments described here.
We set-up a Wizard of Oz experiment where users
were given the task to describe puzzle pieces for
the “dialogue system” to pick up. The system
(i. e. the wizard) had available a limited number
of utterances and hence could conduct only a lim-
ited form of dialogue. We collected 255 utter-
ances containing descriptions of puzzle pieces in
this way.
</bodyText>
<figure confidence="0.985496">
F I L N P T U V W X Y Z
tile
</figure>
<figureCaption confidence="0.9950395">
Figure 3: Silence rate per referent and corpus
(WOz:black, PentoNaming:red, FTT:green)
</figureCaption>
<bodyText confidence="0.999931818181818">
All utterances were hand-transcribed and the
transcriptions were automatically aligned with the
speech data using the MAUS system (Schiel,
2004); this way, we could automatically identify
pauses during utterances and measure their length.
For some experiments (see below), pauses were
“re-ified” through the addition of silence pseudo-
words (one for each 333 ms of silence).
The resulting corpus is not fully balanced in
terms of available material for the various pieces
or contributions by sub-corpora.
</bodyText>
<subsectionHeader confidence="0.978508">
4.2 Descriptive Statistics
</subsectionHeader>
<bodyText confidence="0.9768782">
We were interested to see whether intra-utterance
silences (hesitations) could potentially be used as
an information source in our (more or less) real-
world data in the same way as was shown in
the much more controlled situations described in
the psycholinguistics literature mentioned above
in the introduction (Arnold et al., 2007). Fig-
ure 3 shows the mean ratio of within-utterance si-
lences per word for the different corpora and dif-
ferent referents. We can see that there are clear
differences between the pieces. For example, ref-
erences to the piece whose canonical name is X
contain very few or short hesitations, whereas ref-
erences to Y tend to contain many. We can also
see that the tendencies seem to be remarkably sim-
ilar between corpora, but with relatively stable off-
sets between them, PentoDescr having the longest,
PTT/FTT the shortest silences. We speculate that
this is the result of the differing degrees of inter-
activity (none in PentoDescr, restricted in WOz,
less restricted in PTT, free in FTT) which puts dif-
ferent pressures on speakers to avoid silences. To
balance our data with respect to this difference, we
performed some experiments with adjusted data
0.0 0.1 0.2 0.3 0.4
</bodyText>
<page confidence="0.977014">
33
</page>
<note confidence="0.273871">
intended referent: N
</note>
<bodyText confidence="0.99939425">
where silence lengths in PentoDescr were adjusted
by 0.7 and in PTT/FTT by 1.3. This brings the si-
lence rates in the corpora, if plotted in the style of
Figure 3, almost in congruence.
To test whether the differences in silence rate
between utterances referring to different pieces
are significant, we performed an ANOVA and
found a main effect of silence rate, F(11, 672) =
6.2102,p &lt; 8.714−10. A post-hoc t-test reveals
that there are roughly two groups whose members
are not significantly different within-group, but are
across groups: I, L, U, W and X form one group
with relatively low silence rate, F, N, P, T, V, Y, and
Z another with relatively high silence rate. We will
see in the next section whether our model picked
up on these differences.
</bodyText>
<sectionHeader confidence="0.992786" genericHeader="method">
5 A Bayesian Filtering Model of IRR
</sectionHeader>
<bodyText confidence="0.999980666666667">
To explore incremental reference resolution, and
as part of a larger incremental dialogue system we
are building, we implemented a probabilistic refer-
ence resolver that works in the pentomino domain.
At its base, the resolver has a Bayesian Filtering
model (see e. g. (Thrun et al., 2005)) that with each
new observation (word) computes a belief distri-
bution over the available objects (the twelve puz-
zle pieces); in a second step, a decision for a piece
(or a collection of pieces in the n-best case) is de-
rived from this distribution. This model is incre-
mental in a very natural and direct way: new input
increments are simply treated as new observations
that update the current belief state. Note that this
model does not start with any assumptions about
semantic word classes: whether an observed word
carries information about what is being referred to
will be learnt from data.
</bodyText>
<subsectionHeader confidence="0.992726">
5.1 The Belief-Update Model
</subsectionHeader>
<bodyText confidence="0.999802666666667">
We use a Bayesian model which treats the in-
tended referent as a latent variable generating a
sequence of observations (w1:n is the sequence of
</bodyText>
<equation confidence="0.941085">
words w1, w2, ... , wn):
P(r|w1:n) = α * P(wn|r, w1:n−1) * P(r|w1:n−1)
</equation>
<bodyText confidence="0.991252">
where
</bodyText>
<listItem confidence="0.995389571428571">
• P(wn|r,w1:n−1) is the likelihood of the new
observation (see below for how we approximate
that); and
• the prior P(r|w1:n−1) at step n is the posterior
of the previous step. Before the first observation is
made (i. e., the first word is seen), the prior is sim-
ply a distribution over the possible referents, P(r).
</listItem>
<figure confidence="0.919239">
F I L N P T U V W X Y Z
nimm &lt;sil−0&gt; &lt;sil−1&gt; &lt;sil−2&gt; Cas teil &lt;sil−0&gt; &lt;sil−1&gt; &lt;sil−2&gt; &lt;sil−3&gt; Cas aus einer
</figure>
<figureCaption confidence="0.9668245">
Figure 4: Example of Belief Distribution after Ob-
servation
</figureCaption>
<bodyText confidence="0.979031384615385">
In our experiment, we set this to a uniform distri-
bution, but if there is prior information from other
sources (e. g., because the dialogue state makes
certain pieces more salient), this can be reflected.
• α is a normalising constant, ensuring that the re-
sult is indeed a probability distribution.
The output of the model is a distribution of be-
lief over the 12 available entities, as shown in Fig-
ure 4. Figure 5 shows in a 3D plot the devel-
opment of the belief state (pieces from front to
back, strength of belief as height of the peaks) over
the course of a whole utterance (with observations
from left to right).
</bodyText>
<subsectionHeader confidence="0.994006">
5.2 The Decision Step
</subsectionHeader>
<bodyText confidence="0.999965761904762">
We implemented several ways to derive a decision
for a referent from such a distribution:
i) In the arg max approach, at each state the ref-
erent with the highest posterior probability is cho-
sen. For Figure 4, that would be F (and hence,
a wrong decision). As Figure 5 shows (and the
example is quite representative for the model be-
haviour), there often are various local maxima
over the course of an utterance, and hence a model
that takes as its decision always the maximum can
be expected to perform many edits.
ii) In the adaptive threshold approach, we start
with a default decision for a special 13th class,
“undecided”, and a new decision is only made if
the maximal value at the current step is above a
certain threshold, where this threshold is reset ev-
ery time this condition is met. In other words, this
draws a plane into the belief space and only makes
a new decision when a peak rises above this plane
and hence above the previous peak. In effect, this
approach favours strong convictions and reduces
</bodyText>
<equation confidence="0.590021">
0.0 0.1 0.2 0.3 0.4
</equation>
<page confidence="0.977721">
34
</page>
<note confidence="0.329169">
utterance #: 230 intended referent: N
</note>
<bodyText confidence="0.997386909090909">
the “jitter” in the decision making.
In our example from Figure 4, this would mean
that the maximum, F, would only be the decision
if its value was higher than the threshold and there
was no previous guess that was even higher.
iii) The final model implements a threshold n-
best approach, where not just a single piece is se-
lected but all pieces that are above a certain thresh-
old. Assuming that the threshold is 0.1 for exam-
ple this would select F, I, N, Y, and Z—and hence
would include the correct reference in Figure 4.
</bodyText>
<subsectionHeader confidence="0.995647">
5.3 Implementation
</subsectionHeader>
<bodyText confidence="0.999937741935484">
To learn and query the observation likelihoods
P(wnJr,w1:n_1), we used referent-specific lan-
guage models. More precisely, we computed the
likelihood as P(r, w1:n)/P(r, w1:n_1) (definition
conditional probability), and approximated the
joint probabilities of referent and word sequence
via n-grams with specialised words. E. g., an ut-
terance like “take the long, narrow piece” refer-
ring to piece I (or tested for reference to this piece)
would be rewritten as “take I the I long I narrow I
piece I” and presented to the n-gram learner / in-
ference component. (Both taken from the SRI LM
package, (Stolcke, 2002).)
During evaluation of the models, the test utter-
ances are fed word-by-word to the model and the
decision is evaluated against the known intended
referent. Since we were interested in testing
whether disfluencies contained information that
would be learned, for one variant of the system
we also fed pseudo-words for silences and hesi-
tation markers like uhm, numbered by their posi-
tion (i. e., “take the ..” becomes “take the sil-1 sil-
2”), to both learning and inference for the silence-
sensitive variant; the silence-ignorant variant sim-
ply repeats the previous decision at such points
and does not update its belief state; this way, it
is guaranteed that both variants generate the same
number of decisions and can be compared directly.
(Cf. the dashes in the “no-sil-model” in Figure 1
above: those are points where no real computation
is made in the no-sil case.)
</bodyText>
<subsectionHeader confidence="0.943812">
5.4 Experiments
</subsectionHeader>
<bodyText confidence="0.999859571428571">
All experiments were performed with 10-fold
cross-validation. We always ran both versions, the
one that showed silences to the model and the one
that didn’t. We tested various combinations of lan-
guage model parameters and deciders, of which
the best-performing ones are discussed in the next
section.
</bodyText>
<sectionHeader confidence="0.657422" genericHeader="evaluation">
5.5 Results
</sectionHeader>
<bodyText confidence="0.998299029411765">
Table 1 shows the results for the different deci-
sion methods and for models where silences are
included as observations and where they aren’t,
and, as a baseline, the result for a resolver that
makes a random decision after each observation.
As we can see, the different decision methods
have different characteristics wrt. individual mea-
sures. The threshold n-best approach performs
best across the board—but of course has a slightly
easier job since it does not need to make unam-
biguous decisions. We will look into the develop-
ment of the n-best lists in a second, but for now
we note that this model is for almost all utterances
correct at least once (97 % fc applicable) and if
so, typically very early (after 30 % of the utter-
ance). In over half of the cases (54.68 %), the fi-
nal decision is correct (i. e. is an n-best list that
contains the correct referent), and similarly for a
good third of all silence observations. Interest-
ingly, silence-correctness is decidedly higher for
the silence model (which does actually make new
decisions during silences and hence based on the
information that the speaker is hesitating) than for
the non-sil model (which at these places only re-
peats the previously made decision). The model
performs significantly bettern than a baseline that
randomly selects n-best lists of the same size (see
rnd-nb in Table 1).
As can be expected, the adaptive threshold ap-
proach is more stable with its decisions, as wit-
nessed by the low edit overhead. The fact that it
changes its decision not as often has an impact on
the other measures, though: in more cases, the
model is correct not even once (fc applicable is
</bodyText>
<figureCaption confidence="0.989718">
Figure 5: Belief Update over Course of Utterance
</figureCaption>
<figure confidence="0.975572888888889">
atrix(norm.vect[, 1:12])
Z, Y, X, W, V, U, T, P, N, L, I, F
hast eine lange ule mit drei teilen &lt;sil−0&gt; &lt;sil−1&gt; und eine kurze m
0.8
0.6
0.4
0.2
0.0
1.0
</figure>
<page confidence="0.986292">
35
</page>
<table confidence="0.9393233">
w/ h n-best rnd-nb w/ h adapt w/ h max random
w/o h w/ h w/o h w/o h w/ h
97.22 % 95.03 % 85.38 % 63.15 % 66.67 % 86.55 % 82.89 % 59.94 %
30.43 % 33.73 % 29.61 % 53.87 % 55.25 % 46.55 % 49.31 % 42.60 %
54.68 % 54.24 % 17.54 % 48.68 % 53.07 % 39.77 % 40.64 % 9.65 %
87.74 % 85.01 % 97.08 % 71.24 % 70.89 % 96.08 % 94.28 % 98.44 %
93.49 % 90.65 % 96.65 % 69.61 % 67.66 % 92.57 % 89.44 % 93.16 %
37.81 % 36.81 % 23.37 % 23.01 % 26.61 % 17.83 % 20.23 % 7.83 %
36.60 % 31.09 % 26.39 % 18.71 % 22.58 % 13.67 % 19.34 % 8.63 %
60.07 % 56.96 % 76.63 % 76.29 % 70.90 % 82.17 % 79.42 % 92.16 %
</table>
<figure confidence="0.783946555555556">
Measure / Model
fc applicable
average fc
ff applicable
average ff
edit overhead
correctness
sil-correctness
adjusted error
</figure>
<tableCaption confidence="0.9330435">
Table 1: Results for different decision methods (n-best, adaptive, max arg and random) and for models
with and without silence-observations (w/ h and w/o h, respectively)
</tableCaption>
<bodyText confidence="0.993038590909091">
lower than for the other two models). But it is
still correct with almost half of its final decisions,
and these come even earlier than for the n-best
model. Silence information does not seem to help
this model; this suggests that the information pro-
vided by knowledge about the fact that the speaker
hesitates is too subtle to push through the thresh-
old in order to change decisions.
The arg max approach fares worst. Since nei-
ther the relative strength of the strongest belief (as
compared to that in the competing pieces) nor the
global strength (have I been more convinced be-
fore?) is taken into account, the model changes
its mind too often, as evidenced by the edit over-
head, and does not settle on the correct referent of-
ten (and if, then late). Again, silence information
does not seem to be helpful for this model.
As a more detailed look at what happens dur-
ing silence sequences, Figure 6 plots the average
change in probability from onset of silence to a
point at 1333 ms of silence. (Recall that the un-
derlying Bayesian model is the same for all mod-
els evaluated above, they differ only in how they
derive a decision.) We can see that the gains and
losses are roughly as expected from the analysis of
the corpora: pieces like L and P become more ex-
pected after a silence of that length, pieces like X
less. So the model does indeed seem to learn that
hesitations systematically occur together with cer-
tain pieces. (The reader can convince herself with
the help of Figure 2 that these shapes are indeed
comparatively hard-to-describe; but the interesting
point here is that this categorisation does not have
to be brought to the model but rather is discovered
by it.)
Finally, a look at the distribution and the sizes of
the n-best groupings: the most frequent decision is
Figure 6: Average change in probability from on-
set of silence to 1333 ms into silence
“undecided” (474 times), followed by the group-
ings F N, N Y, and N Y P (343, 342 and 196, re-
spectively). Here again we find groupings that re-
flect the differences w.r.t. hesitation rate. The av-
erage size of the n-best lists is 2.58 (sd = 1.4).
</bodyText>
<sectionHeader confidence="0.997443" genericHeader="conclusions">
6 Conclusions and Further Work
</sectionHeader>
<bodyText confidence="0.999946692307692">
We discussed the task of incremental reference
resolution (IRR), in particular with respect to ex-
ophoric reference. From a theoretical perspective,
it might seem easy to specify what the ideal be-
haviour of an IRR component should be, namely
to always produce the set of entities (the exten-
sion) that is compatible with the part of the ex-
pression seen so far. In practice, however, this is
difficult to annotate, for both practical reasons as
well as theoretical (referring is a pragmatic activ-
ity that is not reducible to denotation). The met-
rics we defined for evaluation of IRR components
account for this in that they do not require a gold
</bodyText>
<figure confidence="0.941587">
F I L N P T U V W X Y Z
−0.05 0.00 0.05
</figure>
<page confidence="0.99222">
36
</page>
<bodyText confidence="0.999768972972973">
standard annotation that fixes the dynamics of the
resolution process; they simply make it possible
to quantify the assumption that “early and with
strong convictions” is best.
We then presented our probabilistic model of
IRR that works directly on word observations
without any further processing (POS tagging,
parsing). It achieves a reasonable success (as mea-
sured with our metrics); for example, in over half
of the cases, the final guess of the model is correct,
and comes before the utterance is over. As an ad-
ditional interesting feature, the model is able to in-
terpret hesitations (silences lifted to pseudo-word
status) in a way shown before only in controlled
psycholinguistic experiments, namely as making
reference to hard-to-describe pieces more likely.3
In future work, we want to explore the model’s
performance on ASR output. It is not clear a
priori that this would degrade performance much,
as it can be expected that the learning components
are quite robust against noise. Connected to
this, we want to explore more complex statis-
tical models, e.g. a hierarchical model where
one level generates parts of the utterance (e. g.
non-referential parts and referential parts) and the
second the actual words. We also want to test how
this approach scales up to worlds with a larger
number of possible referents, where consequently
approximation methods like particle filtering have
to be used. Finally, we will test how the module
contributes to a working dialogue system, where
further decisions (e. g. for clarification requests)
can be built on its output.
Acknowledgments This work was funded by
a grant from DFG in the Emmy Noether Pro-
gramme. We would like to thank the anonymous
reviewers for their detailed comments.
</bodyText>
<sectionHeader confidence="0.999476" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999827926470588">
G.S. Aist, J. Allen, E. Campana, L. Galescu, C.A.
Gomez Gallo, S. Stoness, M. Swift, and M Tanenhaus.
2006. Software architectures for incremental understand-
ing of human speech. In Proceedings of the Interna-
tional Conference on Spoken Language Processing (IC-
SLP), Pittsburgh, PA, USA, September.
Jennifer E. Arnold, Carla L. Hudson Kam, and Michael K.
Tanenhaus. 2007. If you say thee uh you are describ-
ing something hard: The on-line attribution of disfluency
3It is interesting to speculate whether this could have im-
plications for generation of referring expressions as well. It
might be a good strategy to make your planning problems
observable or even to fake planning problems that are under-
standable to humans.
during reference comprehension. Journal of Experimental
Psychology.
Karl Bailey and F. Ferreira. 2007. The processing of filled
pause disfluencies in the visual world. In R. P. G. von
Gompel, M H. Fischer, W. S. Murray, and R. L. Hill,
editors, Eye Movements: A Window on Mind and Brain,
chapter 22. Elsevier.
Timo Baumann, Michaela Atterer, and David Schlangen.
2009. Assessing and Improving the Performance of
Speech Recognition for Incremental Systems. In Proceed-
ings of NAACL-HLT 2009, Boulder, USA.
Susan E. Brennan and Michael F. Schober. 2001. How lis-
teners compensate for disfluencies in spontaneous speech.
Journal of Memory and Language, 44:274–296.
Herbert H. Clark and Edward F. Schaefer. 1987. Collabo-
rating on contributions to conversations. Language and
Cognitive Processes, 2(1):19–41.
Robert Dale and Ehud Reiter. 1995. Computational interpre-
tations of the gricean maxims in the generation of referring
expressions. Cognitive Science, 19:233–263.
Raquel Fern´andez, David Schlangen, and Tatjana Lucht.
2007. Push-to-talk ain’t always bad! comparing differ-
ent interactivity settings in task-oriented dialogue. In Pro-
ceeding of DECALOG (SemDial’07), Trento, Italy, June.
Carlos G´omez Gallo, Gregory Aist, James Allen, William
de Beaumont, Sergio Coria, Whitney Gegg-Harrison,
Joana Paulo Pardal, and Mary Swift. 2007. Annotating
continuous understanding in a multimodal dialogue cor-
pus. In Proceeding of DECALOG (SemDial07), Trento,
Italy, June.
Florian Schiel. 2004. Maus goes iterative. In Proc. of the
IV. International Conference on Language Resources and
Evaluation, Lisbon, Portugal.
Alexander Siebert and David Schlangen. 2008. A simple
method for resolution of definite reference in a shared vi-
sual context. In Procs of SIGdial, Columbus, Ohio.
Gabriel Skantze and David Schlangen. 2009. Incremental
dialogue processing in a micro-domain. In Proceedings of
EACL 2009, Athens, Greece, April.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings Intl. Conf. Spoken Lan-
guage Processing (ICSLP’02), Denver, Colorado, USA,
September.
Scott C. Stoness, Joel Tetreault, and James Allen. 2004. In-
cremental parsing with reference interaction. In Proceed-
ings of the Workshop on Incremental Parsing at the ACL
2004, pages 18–25, Barcelona, Spain, July.
Michael K. Tanenhaus, Michael J. Spivey-Knowlton, Kath-
llen M. Eberhard, and Julie C. Sedivy. 1995. Intergration
of visual and linguistic information in spoken language
comprehension. Science, 268.
Sebastian Thrun, Wolfram Burgard, and Dieter Fox. 2005.
Probabilistic Robotics. MIT Press, Cambridge, Mas-
sachusetts, USA.
</reference>
<page confidence="0.99961">
37
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.977911">
<title confidence="0.997901">Incremental Reference Resolution: The Task, Metrics for Evaluation, a Bayesian Filtering Model that is Sensitive to Disfluencies</title>
<author confidence="0.999464">David Schlangen</author>
<author confidence="0.999464">Timo Baumann</author>
<author confidence="0.999464">Michaela</author>
<affiliation confidence="0.9993095">Department of University of Potsdam,</affiliation>
<abstract confidence="0.999131684210526">In this paper we do two things: a) we discuss in general terms the task of incremental reference resolution (IRR), in particular resolution of exophoric reference, and specify metrics for measuring the performance of dialogue system components tackling this task, and b) we present a simple Bayesian filtering model of IRR that performs reasonably well just using words directly (no structure information and no hand-coded semantics): it picks the right referent out of 12 for around 50 % of realworld dialogue utterances in our test corpus. It is also able to learn to interpret not only words but also hesitations, just as humans have shown to do in similar situations, namely as markers of references to hard-to-describe entities.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G S Aist</author>
<author>J Allen</author>
<author>E Campana</author>
<author>L Galescu</author>
<author>C A Gomez Gallo</author>
<author>S Stoness</author>
<author>M Swift</author>
<author>M Tanenhaus</author>
</authors>
<title>Software architectures for incremental understanding of human speech.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing (ICSLP),</booktitle>
<location>Pittsburgh, PA, USA,</location>
<contexts>
<context position="2035" citStr="Aist et al., 2006" startWordPosition="311" endWordPosition="314">ects very shortly after the end of the first word that unambiguously specified the referent, even if that wasn’t the final word of the phrase. In fact, as has been shown in later experiments (Brennan and Schober, 2001; Bailey and Ferreira, 2007; Arnold et al., 2007), such disambiguating material doesn’t even have to be lexical: under certain circumstances, a speaker’s hesitating already seems to be understood as increasing the likelihood of subsequent reference to hard-to-describe entities. Recently, efforts have begun to build dialogue systems that make use of incremental processing as well (Aist et al., 2006; Skantze and Schlangen, 2009). These efforts have so far focused on aspects other than resolution of references ((Stoness et al., 2004) deals with the interaction of reference and parsing). In this paper, we discuss in general terms the task of incremental reference resolution (IRR) and specify metrics for evaluating incremental components for this task. To make the discussion more concrete, we also describe a simple Bayesian filtering model of IRR in a domain with a small number of possible referents, and show that it performs better wrt. our metrics if given information about hesitations—th</context>
</contexts>
<marker>Aist, Allen, Campana, Galescu, Gallo, Stoness, Swift, Tanenhaus, 2006</marker>
<rawString>G.S. Aist, J. Allen, E. Campana, L. Galescu, C.A. Gomez Gallo, S. Stoness, M. Swift, and M Tanenhaus. 2006. Software architectures for incremental understanding of human speech. In Proceedings of the International Conference on Spoken Language Processing (ICSLP), Pittsburgh, PA, USA, September.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jennifer E Arnold</author>
<author>Carla L Hudson Kam</author>
<author>Michael K Tanenhaus</author>
</authors>
<title>If you say thee uh you are describing something hard: The on-line attribution of disfluency 3It is interesting to speculate whether this could have implications for generation of referring expressions as well. It might be a good strategy to make your planning problems observable or even to fake planning problems that are understandable to humans.</title>
<date>2007</date>
<contexts>
<context position="1684" citStr="Arnold et al., 2007" startWordPosition="259" endWordPosition="262">language comprehension, reference resolution—that is, the linking of natural language expressions to contextually given entities—is performed incrementally by human listeners. This was shown for example by Tanenhaus et al. (1995) in a famous experiment where addressees of utterances containing referring expressions made eye movements towards target objects very shortly after the end of the first word that unambiguously specified the referent, even if that wasn’t the final word of the phrase. In fact, as has been shown in later experiments (Brennan and Schober, 2001; Bailey and Ferreira, 2007; Arnold et al., 2007), such disambiguating material doesn’t even have to be lexical: under certain circumstances, a speaker’s hesitating already seems to be understood as increasing the likelihood of subsequent reference to hard-to-describe entities. Recently, efforts have begun to build dialogue systems that make use of incremental processing as well (Aist et al., 2006; Skantze and Schlangen, 2009). These efforts have so far focused on aspects other than resolution of references ((Stoness et al., 2004) deals with the interaction of reference and parsing). In this paper, we discuss in general terms the task of inc</context>
<context position="15764" citStr="Arnold et al., 2007" startWordPosition="2686" endWordPosition="2689">some experiments (see below), pauses were “re-ified” through the addition of silence pseudowords (one for each 333 ms of silence). The resulting corpus is not fully balanced in terms of available material for the various pieces or contributions by sub-corpora. 4.2 Descriptive Statistics We were interested to see whether intra-utterance silences (hesitations) could potentially be used as an information source in our (more or less) realworld data in the same way as was shown in the much more controlled situations described in the psycholinguistics literature mentioned above in the introduction (Arnold et al., 2007). Figure 3 shows the mean ratio of within-utterance silences per word for the different corpora and different referents. We can see that there are clear differences between the pieces. For example, references to the piece whose canonical name is X contain very few or short hesitations, whereas references to Y tend to contain many. We can also see that the tendencies seem to be remarkably similar between corpora, but with relatively stable offsets between them, PentoDescr having the longest, PTT/FTT the shortest silences. We speculate that this is the result of the differing degrees of interact</context>
</contexts>
<marker>Arnold, Kam, Tanenhaus, 2007</marker>
<rawString>Jennifer E. Arnold, Carla L. Hudson Kam, and Michael K. Tanenhaus. 2007. If you say thee uh you are describing something hard: The on-line attribution of disfluency 3It is interesting to speculate whether this could have implications for generation of referring expressions as well. It might be a good strategy to make your planning problems observable or even to fake planning problems that are understandable to humans.</rawString>
</citation>
<citation valid="false">
<title>during reference comprehension.</title>
<journal>Journal of Experimental Psychology.</journal>
<marker></marker>
<rawString>during reference comprehension. Journal of Experimental Psychology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Bailey</author>
<author>F Ferreira</author>
</authors>
<title>The processing of filled pause disfluencies in the visual world. In</title>
<date>2007</date>
<booktitle>Eye Movements: A Window on Mind and Brain, chapter 22.</booktitle>
<editor>R. P. G. von Gompel, M H. Fischer, W. S. Murray, and R. L. Hill, editors,</editor>
<publisher>Elsevier.</publisher>
<contexts>
<context position="1662" citStr="Bailey and Ferreira, 2007" startWordPosition="255" endWordPosition="258">ke other tasks involved in language comprehension, reference resolution—that is, the linking of natural language expressions to contextually given entities—is performed incrementally by human listeners. This was shown for example by Tanenhaus et al. (1995) in a famous experiment where addressees of utterances containing referring expressions made eye movements towards target objects very shortly after the end of the first word that unambiguously specified the referent, even if that wasn’t the final word of the phrase. In fact, as has been shown in later experiments (Brennan and Schober, 2001; Bailey and Ferreira, 2007; Arnold et al., 2007), such disambiguating material doesn’t even have to be lexical: under certain circumstances, a speaker’s hesitating already seems to be understood as increasing the likelihood of subsequent reference to hard-to-describe entities. Recently, efforts have begun to build dialogue systems that make use of incremental processing as well (Aist et al., 2006; Skantze and Schlangen, 2009). These efforts have so far focused on aspects other than resolution of references ((Stoness et al., 2004) deals with the interaction of reference and parsing). In this paper, we discuss in general</context>
</contexts>
<marker>Bailey, Ferreira, 2007</marker>
<rawString>Karl Bailey and F. Ferreira. 2007. The processing of filled pause disfluencies in the visual world. In R. P. G. von Gompel, M H. Fischer, W. S. Murray, and R. L. Hill, editors, Eye Movements: A Window on Mind and Brain, chapter 22. Elsevier.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timo Baumann</author>
<author>Michaela Atterer</author>
<author>David Schlangen</author>
</authors>
<title>Assessing and Improving the Performance of Speech Recognition for Incremental Systems.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL-HLT 2009,</booktitle>
<location>Boulder, USA.</location>
<contexts>
<context position="6765" citStr="Baumann et al., 2009" startWordPosition="1129" endWordPosition="1132">tended referent, and what the “correct” time-course of the development of hypotheses is, it’s easy to say what we want to be true in general: we want a referent to be found as early as possible, with as little change of opinion as possible during the utterance.1 Hence a model that finds the correct referent earlier and makes fewer wrong decisions than a competing one will be considered better. The metrics we develop in the next section spell out this idea. 3 Evaluation Metrics for IRR In previous work, we have discussed metrics for evaluating the performance of incremental speech recognition (Baumann et al., 2009). There, our metrics could rely on time-aligned gold-standard information against which the incremental results could be measured. For the reasons discussed in the previous section, we do not assume that we have such temporally-aligned information for evaluating IRR. Our measures described here simply assume that there is one intention behind the referring utterances (namely to identify a certain entity), and that this intention is there from the beginning of the utterance and stays constant.2 This is not to be understood as the claim that it is reasonable to expect an IRR component to pick ou</context>
</contexts>
<marker>Baumann, Atterer, Schlangen, 2009</marker>
<rawString>Timo Baumann, Michaela Atterer, and David Schlangen. 2009. Assessing and Improving the Performance of Speech Recognition for Incremental Systems. In Proceedings of NAACL-HLT 2009, Boulder, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan E Brennan</author>
<author>Michael F Schober</author>
</authors>
<title>How listeners compensate for disfluencies in spontaneous speech.</title>
<date>2001</date>
<journal>Journal of Memory and Language,</journal>
<pages>44--274</pages>
<contexts>
<context position="1635" citStr="Brennan and Schober, 2001" startWordPosition="251" endWordPosition="254">entities. 1 Introduction Like other tasks involved in language comprehension, reference resolution—that is, the linking of natural language expressions to contextually given entities—is performed incrementally by human listeners. This was shown for example by Tanenhaus et al. (1995) in a famous experiment where addressees of utterances containing referring expressions made eye movements towards target objects very shortly after the end of the first word that unambiguously specified the referent, even if that wasn’t the final word of the phrase. In fact, as has been shown in later experiments (Brennan and Schober, 2001; Bailey and Ferreira, 2007; Arnold et al., 2007), such disambiguating material doesn’t even have to be lexical: under certain circumstances, a speaker’s hesitating already seems to be understood as increasing the likelihood of subsequent reference to hard-to-describe entities. Recently, efforts have begun to build dialogue systems that make use of incremental processing as well (Aist et al., 2006; Skantze and Schlangen, 2009). These efforts have so far focused on aspects other than resolution of references ((Stoness et al., 2004) deals with the interaction of reference and parsing). In this p</context>
</contexts>
<marker>Brennan, Schober, 2001</marker>
<rawString>Susan E. Brennan and Michael F. Schober. 2001. How listeners compensate for disfluencies in spontaneous speech. Journal of Memory and Language, 44:274–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
<author>Edward F Schaefer</author>
</authors>
<title>Collaborating on contributions to conversations. Language and Cognitive Processes,</title>
<date>1987</date>
<contexts>
<context position="5326" citStr="Clark and Schaefer, 1987" startWordPosition="865" endWordPosition="868">patible with the expression up to that point. (This description is meant to be neutral as to whether reference is exophoric, i. e. directly to entities in the world, or anaphoric, via previous mentions; we will mainly discuss the former case, though.) As we will see below, this does however not translate directly into a usable metric for evaluation. While it is easy to identify the contributions of individual words in simple, constructed expressions like (1-a), reference in real conversations is often much more complex, and is a collaborative process that isn’t confined to single expressions (Clark and Schaefer, 1987): referring is a pragmatic action that is not reducible to denotation. In our corpus (see below), we often find descriptions as in (2), where the speaker continuously adds (rather vague) material, typically until the addressee signals that she identified the item, or proposes a different way to describe it. (2) Also das S Teil sieht so aus dass es ein einzelnes . Teilchen hat . dann . vier am St¨uck im rechten Winkel .. dazu nee . nee warte .. dann noch ein einzelnes das guckt auf der anderen Seite raus. well, the S piece looks so that it has a single . piece . and then. four together in a 90 </context>
</contexts>
<marker>Clark, Schaefer, 1987</marker>
<rawString>Herbert H. Clark and Edward F. Schaefer. 1987. Collaborating on contributions to conversations. Language and Cognitive Processes, 2(1):19–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ehud Reiter</author>
</authors>
<title>Computational interpretations of the gricean maxims in the generation of referring expressions.</title>
<date>1995</date>
<journal>Cognitive Science,</journal>
<pages>19--233</pages>
<contexts>
<context position="3270" citStr="Dale and Reiter, 1995" startWordPosition="517" endWordPosition="521">computational support for the rationality of including observables other than words into models of dialogue meaning. The remainder of the paper is structured as follows: We discuss the IRR task in Section 2, and suitable evaluation metrics in Section 3. In Section 4 we describe and analyse the data for which we present results with our Bayesian model for IRR in Section 5. 2 Incremental Reference Resolution To a first approximation, IRR can be modeled as the ‘inverse’ as it were of the task of generating referring expressions (GRE; which is well-studied in computational linguistics, see e. g. (Dale and Reiter, 1995)). Where in GRE words are added that express features which reduce the size of the set of possible distractors (with which the object that the expression is intended to pick out can be confused), in IRR words are encountered that express features that reduce the size of the set of possible Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 30–37, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 30 referents. To give a concrete example, for the expression in (1-a), we could imagine </context>
</contexts>
<marker>Dale, Reiter, 1995</marker>
<rawString>Robert Dale and Ehud Reiter. 1995. Computational interpretations of the gricean maxims in the generation of referring expressions. Cognitive Science, 19:233–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raquel Fern´andez</author>
<author>David Schlangen</author>
<author>Tatjana Lucht</author>
</authors>
<title>Push-to-talk ain’t always bad! comparing different interactivity settings in task-oriented dialogue.</title>
<date>2007</date>
<booktitle>In Proceeding of DECALOG (SemDial’07),</booktitle>
<location>Trento, Italy,</location>
<marker>Fern´andez, Schlangen, Lucht, 2007</marker>
<rawString>Raquel Fern´andez, David Schlangen, and Tatjana Lucht. 2007. Push-to-talk ain’t always bad! comparing different interactivity settings in task-oriented dialogue. In Proceeding of DECALOG (SemDial’07), Trento, Italy, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos G´omez Gallo</author>
<author>Gregory Aist</author>
<author>James Allen</author>
<author>William de Beaumont</author>
<author>Sergio Coria</author>
<author>Whitney Gegg-Harrison</author>
<author>Joana Paulo Pardal</author>
<author>Mary Swift</author>
</authors>
<title>Annotating continuous understanding in a multimodal dialogue corpus.</title>
<date>2007</date>
<booktitle>In Proceeding of DECALOG (SemDial07),</booktitle>
<location>Trento, Italy,</location>
<marker>Gallo, Aist, Allen, de Beaumont, Coria, Gegg-Harrison, Pardal, Swift, 2007</marker>
<rawString>Carlos G´omez Gallo, Gregory Aist, James Allen, William de Beaumont, Sergio Coria, Whitney Gegg-Harrison, Joana Paulo Pardal, and Mary Swift. 2007. Annotating continuous understanding in a multimodal dialogue corpus. In Proceeding of DECALOG (SemDial07), Trento, Italy, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florian Schiel</author>
</authors>
<title>Maus goes iterative.</title>
<date>2004</date>
<booktitle>In Proc. of the IV. International Conference on Language Resources and Evaluation,</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="15045" citStr="Schiel, 2004" startWordPosition="2578" endWordPosition="2579"> here. We set-up a Wizard of Oz experiment where users were given the task to describe puzzle pieces for the “dialogue system” to pick up. The system (i. e. the wizard) had available a limited number of utterances and hence could conduct only a limited form of dialogue. We collected 255 utterances containing descriptions of puzzle pieces in this way. F I L N P T U V W X Y Z tile Figure 3: Silence rate per referent and corpus (WOz:black, PentoNaming:red, FTT:green) All utterances were hand-transcribed and the transcriptions were automatically aligned with the speech data using the MAUS system (Schiel, 2004); this way, we could automatically identify pauses during utterances and measure their length. For some experiments (see below), pauses were “re-ified” through the addition of silence pseudowords (one for each 333 ms of silence). The resulting corpus is not fully balanced in terms of available material for the various pieces or contributions by sub-corpora. 4.2 Descriptive Statistics We were interested to see whether intra-utterance silences (hesitations) could potentially be used as an information source in our (more or less) realworld data in the same way as was shown in the much more contro</context>
</contexts>
<marker>Schiel, 2004</marker>
<rawString>Florian Schiel. 2004. Maus goes iterative. In Proc. of the IV. International Conference on Language Resources and Evaluation, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Siebert</author>
<author>David Schlangen</author>
</authors>
<title>A simple method for resolution of definite reference in a shared visual context.</title>
<date>2008</date>
<booktitle>In Procs of SIGdial,</booktitle>
<location>Columbus, Ohio.</location>
<contexts>
<context position="13774" citStr="Siebert and Schlangen, 2008" startWordPosition="2364" endWordPosition="2367">ning sections, we describe a probabilistic model of IRR that we have implemented, and evaluate it in terms of these metrics. We begin with describing the data from which we learnt our model. 4 Data 4.1 Our Corpora As the basis for training and testing of our model we used data from three corpora of task-oriented dialogue that differ in some details of the set-up, but use the same task: an Instruction Giver (IG) instructs an Instruction Follower (IF) on which puzzle pieces (from the “Pentomino” game, see Figure 2) to pick up. In detail, the corpora were: • The Pento Naming corpus described in (Siebert and Schlangen, 2008). In this variant of the task, IG records instructions for an absent IF; so these aren’t fully interactive dialogues. The corpus contained 270 utterances out of which we selected those 143 that contained descriptions of puzzle pieces (and not of their position on the gameboard). • Selections from the FTT/PTT corpus described in (Fern´andez et al., 2007), where IF and IG are connected through an audio-only connection, and in some dialogues a simplex / push-to-talk one. We selected all utterances from IG that contained references to puzzle pieces (286 altogether). • The third part of our corpus </context>
</contexts>
<marker>Siebert, Schlangen, 2008</marker>
<rawString>Alexander Siebert and David Schlangen. 2008. A simple method for resolution of definite reference in a shared visual context. In Procs of SIGdial, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Skantze</author>
<author>David Schlangen</author>
</authors>
<title>Incremental dialogue processing in a micro-domain.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL 2009,</booktitle>
<location>Athens, Greece,</location>
<contexts>
<context position="2065" citStr="Skantze and Schlangen, 2009" startWordPosition="315" endWordPosition="318">fter the end of the first word that unambiguously specified the referent, even if that wasn’t the final word of the phrase. In fact, as has been shown in later experiments (Brennan and Schober, 2001; Bailey and Ferreira, 2007; Arnold et al., 2007), such disambiguating material doesn’t even have to be lexical: under certain circumstances, a speaker’s hesitating already seems to be understood as increasing the likelihood of subsequent reference to hard-to-describe entities. Recently, efforts have begun to build dialogue systems that make use of incremental processing as well (Aist et al., 2006; Skantze and Schlangen, 2009). These efforts have so far focused on aspects other than resolution of references ((Stoness et al., 2004) deals with the interaction of reference and parsing). In this paper, we discuss in general terms the task of incremental reference resolution (IRR) and specify metrics for evaluating incremental components for this task. To make the discussion more concrete, we also describe a simple Bayesian filtering model of IRR in a domain with a small number of possible referents, and show that it performs better wrt. our metrics if given information about hesitations—thus providing computational sup</context>
</contexts>
<marker>Skantze, Schlangen, 2009</marker>
<rawString>Gabriel Skantze and David Schlangen. 2009. Incremental dialogue processing in a micro-domain. In Proceedings of EACL 2009, Athens, Greece, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings Intl. Conf. Spoken Language Processing (ICSLP’02),</booktitle>
<location>Denver, Colorado, USA,</location>
<contexts>
<context position="21925" citStr="Stolcke, 2002" startWordPosition="3771" endWordPosition="3772">tion To learn and query the observation likelihoods P(wnJr,w1:n_1), we used referent-specific language models. More precisely, we computed the likelihood as P(r, w1:n)/P(r, w1:n_1) (definition conditional probability), and approximated the joint probabilities of referent and word sequence via n-grams with specialised words. E. g., an utterance like “take the long, narrow piece” referring to piece I (or tested for reference to this piece) would be rewritten as “take I the I long I narrow I piece I” and presented to the n-gram learner / inference component. (Both taken from the SRI LM package, (Stolcke, 2002).) During evaluation of the models, the test utterances are fed word-by-word to the model and the decision is evaluated against the known intended referent. Since we were interested in testing whether disfluencies contained information that would be learned, for one variant of the system we also fed pseudo-words for silences and hesitation markers like uhm, numbered by their position (i. e., “take the ..” becomes “take the sil-1 sil2”), to both learning and inference for the silencesensitive variant; the silence-ignorant variant simply repeats the previous decision at such points and does not </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proceedings Intl. Conf. Spoken Language Processing (ICSLP’02), Denver, Colorado, USA, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott C Stoness</author>
<author>Joel Tetreault</author>
<author>James Allen</author>
</authors>
<title>Incremental parsing with reference interaction.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Incremental Parsing at the ACL 2004,</booktitle>
<pages>18--25</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="2171" citStr="Stoness et al., 2004" startWordPosition="333" endWordPosition="336">he phrase. In fact, as has been shown in later experiments (Brennan and Schober, 2001; Bailey and Ferreira, 2007; Arnold et al., 2007), such disambiguating material doesn’t even have to be lexical: under certain circumstances, a speaker’s hesitating already seems to be understood as increasing the likelihood of subsequent reference to hard-to-describe entities. Recently, efforts have begun to build dialogue systems that make use of incremental processing as well (Aist et al., 2006; Skantze and Schlangen, 2009). These efforts have so far focused on aspects other than resolution of references ((Stoness et al., 2004) deals with the interaction of reference and parsing). In this paper, we discuss in general terms the task of incremental reference resolution (IRR) and specify metrics for evaluating incremental components for this task. To make the discussion more concrete, we also describe a simple Bayesian filtering model of IRR in a domain with a small number of possible referents, and show that it performs better wrt. our metrics if given information about hesitations—thus providing computational support for the rationality of including observables other than words into models of dialogue meaning. The re</context>
</contexts>
<marker>Stoness, Tetreault, Allen, 2004</marker>
<rawString>Scott C. Stoness, Joel Tetreault, and James Allen. 2004. Incremental parsing with reference interaction. In Proceedings of the Workshop on Incremental Parsing at the ACL 2004, pages 18–25, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael K Tanenhaus</author>
<author>Michael J Spivey-Knowlton</author>
<author>Kathllen M Eberhard</author>
<author>Julie C Sedivy</author>
</authors>
<title>Intergration of visual and linguistic information in spoken language comprehension.</title>
<date>1995</date>
<journal>Science,</journal>
<volume>268</volume>
<contexts>
<context position="1293" citStr="Tanenhaus et al. (1995)" startWordPosition="194" endWordPosition="198">tructure information and no hand-coded semantics): it picks the right referent out of 12 for around 50 % of realworld dialogue utterances in our test corpus. It is also able to learn to interpret not only words but also hesitations, just as humans have shown to do in similar situations, namely as markers of references to hard-to-describe entities. 1 Introduction Like other tasks involved in language comprehension, reference resolution—that is, the linking of natural language expressions to contextually given entities—is performed incrementally by human listeners. This was shown for example by Tanenhaus et al. (1995) in a famous experiment where addressees of utterances containing referring expressions made eye movements towards target objects very shortly after the end of the first word that unambiguously specified the referent, even if that wasn’t the final word of the phrase. In fact, as has been shown in later experiments (Brennan and Schober, 2001; Bailey and Ferreira, 2007; Arnold et al., 2007), such disambiguating material doesn’t even have to be lexical: under certain circumstances, a speaker’s hesitating already seems to be understood as increasing the likelihood of subsequent reference to hard-t</context>
</contexts>
<marker>Tanenhaus, Spivey-Knowlton, Eberhard, Sedivy, 1995</marker>
<rawString>Michael K. Tanenhaus, Michael J. Spivey-Knowlton, Kathllen M. Eberhard, and Julie C. Sedivy. 1995. Intergration of visual and linguistic information in spoken language comprehension. Science, 268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Thrun</author>
<author>Wolfram Burgard</author>
<author>Dieter Fox</author>
</authors>
<title>Probabilistic Robotics.</title>
<date>2005</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts, USA.</location>
<contexts>
<context position="17733" citStr="Thrun et al., 2005" startWordPosition="3024" endWordPosition="3027">se members are not significantly different within-group, but are across groups: I, L, U, W and X form one group with relatively low silence rate, F, N, P, T, V, Y, and Z another with relatively high silence rate. We will see in the next section whether our model picked up on these differences. 5 A Bayesian Filtering Model of IRR To explore incremental reference resolution, and as part of a larger incremental dialogue system we are building, we implemented a probabilistic reference resolver that works in the pentomino domain. At its base, the resolver has a Bayesian Filtering model (see e. g. (Thrun et al., 2005)) that with each new observation (word) computes a belief distribution over the available objects (the twelve puzzle pieces); in a second step, a decision for a piece (or a collection of pieces in the n-best case) is derived from this distribution. This model is incremental in a very natural and direct way: new input increments are simply treated as new observations that update the current belief state. Note that this model does not start with any assumptions about semantic word classes: whether an observed word carries information about what is being referred to will be learnt from data. 5.1 </context>
</contexts>
<marker>Thrun, Burgard, Fox, 2005</marker>
<rawString>Sebastian Thrun, Wolfram Burgard, and Dieter Fox. 2005. Probabilistic Robotics. MIT Press, Cambridge, Massachusetts, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>