<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004846">
<title confidence="0.983865">
Precision-focused Textual Inference
</title>
<author confidence="0.989359">
D. G. Bobrow, C. Condoravdi, R. Crouch, V. de Paiva, L. Karttunen, T. H. King, R. Nairn, L. Price, A. Zaenen
</author>
<affiliation confidence="0.991633">
Palo Alto Research Center
</affiliation>
<sectionHeader confidence="0.989268" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99880625">
This paper describes our system as used in
the RTE3 task. The system maps premise and
hypothesis pairs into an abstract knowledge
representation (AKR) and then performs en-
tailment and contradiction detection (ECD)
on the resulting AKRs. Two versions of ECD
were used in RTE3, one with strict ECD and
one with looser ECD.
</bodyText>
<figure confidence="0.863800222222222">
Process Output
Text-Breaking Delimited sentences
Named-entity recognition Type-marked Entities
Morphological Analysis Word stems plus features
LFG Parsing Functional Structure
Semantic processing Scope, Predicate-
argument structure
AKR rules Conceptual, Contextual,
Temporal Structure
</figure>
<sectionHeader confidence="0.985296" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999881137931035">
In the RTE textual entailment challenge, one is given
a source text T and a hypothesis H, and the task is to
decide whether H can be inferred from T. Our sys-
tem interprets inference in a strict way. Given the
knowledge of the language embedded in the system,
does the hypothesis logically follow from the infor-
mation embedded in the text? Thus we are empha-
sizing precision, particularly in question-answering.
This was reflected in our results in the RTE3 chal-
lenge. We responded correctly with YES to relatively
few of the examples, but on the QA-type examples,
we achieved 90-95% average precision.
The methodology employed is to use the linguis-
tic information to map T and H onto a logical form in
AKR, our Abstract Knowledge Representation. The
AKR is designed to capture the propositions the au-
thor of a statement is committed to. For the sake of
ECD, the representation of T may include elements
that are not directly expressed in the text. For ex-
ample, in the AKR of John bought a car includes the
fact that the car was sold. The AKR of John forgot to
buy milk includes the fact that John did not buy milk.
Our reasoning algorithm tries to determine whether
the AKR of H is subsumed by the AKR of T and detect
cases when they are in conflict.
The Entailment and Contradiction Detection
(ECD) algorithm makes a distinction that is not part
of the basic RTE challenge. If T entails the negation
of H, we answer NO (Contradiction). On the other
</bodyText>
<figureCaption confidence="0.964707">
Figure 1: The processing pipeline: processes with
their ambiguity-enabled packed outputs
</figureCaption>
<bodyText confidence="0.999916769230769">
hand, if there is no direct entailment we answer UN-
KNOWN. We do not try to construct a likely scenario
that would link T and H. Nor have we tried to col-
lect data on phrases that would tend to indicate such
likely associations between T and H. That approach
is clearly very useful (e.g. (Hickl et al., 2006)), and
could be used as a backup strategy with our more
formal entailment approach. We have chosen to fo-
cus on strict structural and lexical entailments.
This paper describes the processing pipeline for
mapping to AKR, the ECD algorithm, the challenges
we faced in processing the RTE data and a summary
of our results on RTE3.
</bodyText>
<sectionHeader confidence="0.983347" genericHeader="method">
2 Process Pipeline
</sectionHeader>
<bodyText confidence="0.9886975">
Figure 1 shows the processing pipeline for mapping
texts to AKR. The input is a text of one or more
sentences.
All components of the system are “ambiguity en-
abled” (Maxwell and Kaplan, 1991). This allows
each component to accept ambiguous input in a
“packed” format, process it without unpacking the
ambiguities, and then pass packed input to the next
stage. The syntactic component, LFG Parsing, also
has a stochastic disambiguation system which al-
lows us to pass the n-best on to the semantics (Rie-
zler et al., 2002); for the RTE3 challenge, we used
</bodyText>
<page confidence="0.961671">
16
</page>
<bodyText confidence="0.972073528301887">
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 16–21,
Prague, June 2007. @2007 Association for Computational Linguistics
n=50.
The parser takes the output of the morphology
(i.e. a series of lemmata with their tags) and pro-
duces a tree (constituent-structure) and a depen-
dency structure (functional-structure) represented as
an attribute-value matrix. The functional-structure
is of primary importance for the semantics and
AKR. In particular, it encodes predicate-argument
relations, including long-distance dependencies, and
provides other syntactic features (e.g. number, tense,
noun type).
The output of the syntax is input for the seman-
tics that is produced by an ambiguity enabled packed
rewriting system. The semantics is described in de-
tail in (Crouch and King, 2006). Semantic process-
ing assigns scope to scope-bearing elements such as
negation and normalizes the output of the syntax.
This normalization includes reformulating syntactic
passives as actives (e.g. The cake was eaten by Mary.
/ Mary ate the cake.), resolving many null pronouns
(e.g. Laughing, John entered the room / Johni laugh-
ing, Johni entered the room.), and canonicalizing
measure phrases, comparatives, and dates. More
complex normalizations involve converting nominal
deverbals into the equivalent verbal form, identify-
ing arguments of the verb from the arguments of
the nominal (Gurevich et al., 2006). For example,
the semantic representation of Iraq’s destruction of
its WMD is similar to the representation of Iraq de-
stroyed its WMD.
The final main task of the semantics rules is to
convert words into concepts and syntactic grammat-
ical functions into roles. The mapping onto concepts
uses WordNet (Fellbaum, 1998) to map words into
lists of synsets. The named entity types provided by
the morphology and syntax are used to create more
accurate mapping of proper nouns since these are
not systematically represented in WordNet. The se-
mantic rules use the grammatical function subcat-
egorization information from the verb and the role
information found in extended VerbNet (Kipper et
al., 2000) to map syntactic subjects, objects, and
obliques into more abstract thematic roles such as
Agent, Theme, and Goal (Crouch and King, 2005).
This mapping into thematic-style roles allows the
system to correctly align the arguments in pairs like
(1) and (2), something which is impossible using just
syntactic functions. In the first, the object and sub-
ject have a common thematic role in the alternation
between transitive and intransitive; while in the sec-
ond, the common role is shared by the subjects.
</bodyText>
<listItem confidence="0.96642775">
(1) John broke the vasesyn:object,sem:patient.
The vasesyn:subject,sem:patient broke.
(2) Johnsyn:subject,sem:agent ate the cake.
Johnsyn:subject,sem:agent ate.
</listItem>
<bodyText confidence="0.999993">
The goal of these semantic normalizations is to
abstract away from the syntactic representation so
that sentences with similar meaning have similar se-
mantic representations. However, the semantics is
still fundamentally a linguistic level of representa-
tion; further abstraction towards the meaning is done
in the mapping from semantics to AKR. The AKR
is the level of representation that is used to deter-
mine entailment and contradiction in our RTE3 sys-
tem. A preliminary description of its logic was pro-
vided in (Bobrow et al., 2005). The AKR mapping
converts grammatical tense and temporal modifiers
into temporal relations, identifies anaphoric refer-
ents and makes explicit the implied relation between
complement clauses and the main verb (e.g. for
manage, fail) (Nairn et al., 2006). AKR also deals
with standard phrases that are equivalent to simple
vocabulary terms. For example, take a flight to New
York is equivalent to fly to New York. These uses
of “light” verbs (e.g. take, give) are not included
in synonyms found in WordNet. Another class of
phrasal synonyms involve inchoatives (e.g. take a
turn for the worse/worsen). We included a special
set of transformation rules for phrasal synonyms:
some of the rules are part of the mapping from se-
mantics to AKR while others are part of the ECD
module. The mapping to AKR is done using the same
ambiguity-enabled ordered rewriting system that the
semantics uses, allowing the AKR mapping system
to efficiently process the packed output of the se-
mantics.
The AKR for a sentence like Bush claimed that
Iraq possessed WMDs in Figure 2 introduces two
contexts: a top level context t, representing the com-
mitments of the speaker of sentence, and an embed-
ded context claim cx:37 representing the state of af-
fairs according to Bush’s claim. The two contexts
are related via the Topic role of the claim event.
The representation contains terms like claim:37 or
</bodyText>
<page confidence="0.990618">
17
</page>
<subsectionHeader confidence="0.396949">
Conceptual Structure
</subsectionHeader>
<equation confidence="0.918490285714286">
subconcept(claim:37,[claim-1,. . .,claim-5])
role(Topic,claim:37,claim cx:37)
role(Agent,claim:37,Bush:1)
subconcept(Bush:1,[person-1])
alias(Bush:1,[Bush])
role(cardinality restriction,Bush:1,sg)
subconcept(possess:24,[possess-1,own-1,possess-3])
role(Destination,possess:24,wmd:34)
role(Agent,possess:24,Iraq:19)
subconcept(Iraq:19,[location-1,location-4])
alias(Iraq:19,[Iraq])
role(cardinality restriction,Iraq:19,sg)
subconcept(wmd:34,
[weapon of mass destruction-1])
role(cardinality restriction,wmd:34,pl)
Contextual Structure
context(t)
context(claim cx:37)
context relation(t,claim cx:37,crel(Topic,claim:37))
instantiable(Bush:1,t)
instantiable(Iraq:19,t)
instantiable(claim:37,t)
instantiable(Iraq:19,claim cx:37)
instantiable(possess:24,claim cx:37)
instantiable(wmd:34,claim cx:37)
Temporal Structure
temporalRel(After,Now,claim:37)
temporalRel(After,claim:37,possess:24)
</equation>
<figureCaption confidence="0.944036">
Figure 2: AKR for Bush claimed that Iraq possessed
WMDs.
</figureCaption>
<bodyText confidence="0.999933911764706">
Bush:1 which refer to the kinds of object that the
sentence is talking about. The subconcept facts ex-
plicitly link these terms to their concepts in Word-
Net. Thus claim:37 is stated to be some subkind
of the type claim-1, etc., and wmd:34 to be some
subkind of the type weapon of mass destruction-
1. Terms like claim:37 and wmd:34 do not refer
to individuals, but to concepts (or types or kinds).
Saying that there is some subconcept of the kind
weapon of mass destruction-1, where this subcon-
cept is further restricted to be a kind of WMD pos-
sessed by Iraq, does not commit you to saying that
there are any instances of this subconcept.
The instantiable assertions capture the commit-
ments about the existence of the kinds of object de-
scribed. In the top-level context t, there is a com-
mitment to an instance of Bush and of a claim:37
event made by him. However, there is no top-level
commitment to any instances of wmd:34 possessed
by Iraq:19. These commitments are only made in
the embedded claim cx:37 context. It is left open
whether these embedded commitments correspond,
or not, to the beliefs of the speaker. Two distinct
levels of structure can thus be discerned in AKR: a
conceptual structure and a contextual structure. The
conceptual structure, through use of subconcept and
role assertions, indicates the subject matter. The
contextual structure indicates commitments as to the
existence of the subject matter via instantiability as-
sertions linking concepts to contexts, and via context
relations linking contexts to contexts. In addition,
there is a temporal structure that situates the events
described with respect to the time of utterance and
temporally relates them to one another.
</bodyText>
<sectionHeader confidence="0.995491" genericHeader="method">
3 Entailment and Contradiction Detection
</sectionHeader>
<bodyText confidence="0.9999655">
ECD is implemented as another set of rewrite rules,
running on the same packed rewrite system used to
generate the AKR representations. The rules (i) align
concept and context terms in text (T) and hypoth-
esis (H) AKRs, (ii) calculate concept subsumption
orderings between aligned T and H terms, and (iii)
check instantiability and uninstantiability claims in
the light of subsumption orderings to determine
whether T entails H, T contradicts H, or T neither
entails not contradicts H. For the purposes of RTE3,
both contradiction and neither contradiction nor en-
tailment are collapsed into a NO (does not follow)
judgment.
One of the novel features of this approach is that
T and H representations do not need to be disam-
biguated before checking for entailment or contra-
diction. The approach is able to detect if there is one
reading of T that entails (or contradicts) one reading
of H. The T and H passages can in effect mutually
disambiguate one another through the ECD. For ex-
ample, although plane and level both have multiple
readings, they can both refer to a horizontal surface,
and in that sense The plane is dry entails The level is
dry, and vice versa.
The first phase of ECD aligns concepts and con-
text terms in the T and H AKRs. Concepts are repre-
</bodyText>
<page confidence="0.995999">
18
</page>
<bodyText confidence="0.999963365853659">
sented as lists of WordNet hypernym lists, in Word-
Net sense order. Two concept terms can be aligned
if a sense synset of one term (i.e. the first element
of one of the term’s hypernym lists) is contained in
a hypernym list of the other term. The alignment
can be weighted according to word sense; so a con-
cept overlap on the first senses of a T and H term
counts for more than a concept overlap on the n and
mth senses. However, no weightings were used in
RTE3. For named entities, alignment demands not
only a concept overlap, but also an intersection in
the “alias” forms of the proper nouns. For exam-
ple,“George Bush” may be aligned with “George”
or with “Bush”. Context alignment relies on associ-
ating each context with an indexing concept, usually
the concept for the main verb in the clause heading
the context. Contexts are then aligned on the basis
of these concept indices.
Typically, an H term can align with more than one
T term. In such cases all possible alignments are
proposed, but the alignment rules put the alternative
alignments in different parts of the choice space.
Having aligned T and H terms, rules are applied to
determine concept specificity and subsumption rela-
tions between aligned terms. Preliminary judgments
of specificity are made by looking for hypernym in-
clusion. For example, an H term denoting the con-
cept “person” is less specific than a T term denot-
ing “woman”. These preliminary judgments need to
be revised in the light of role restrictions modifying
the terms: a “tall person” is neither more nor less
specific than a ”woman”. Revisions to specificity
judgments also take into account cardinality modi-
fiers: while “person” is less specific than “woman”,
“all persons” is judged to be more specific than “all
women”.
With judgments of concept specificity in place,
it is possible to determine entailment relations on
the basis of (un)instantiability claims in the T and
H AKRs. For example, suppose the T and H AKRs
contain the facts in (3).
</bodyText>
<listItem confidence="0.9706145">
(3) T: instantiable(C T, Ctx T)
H: instantiable(C H, Ctx H)
</listItem>
<bodyText confidence="0.9159605">
where concept C T is aligned with C H, C T is
judged to be more specific than C H, and context
Ctx T is aligned with context Ctx H. In this case,
the hypothesis instantiability claim is entailed by
the text instantiability claim (existence of something
more specific entails existence of something more
general). This being so, the H instantiability claim
can be deleted without loss of information.
If instead we had the (un)instantiability claims in
(4) for the same alignments and specificity relations,
</bodyText>
<listItem confidence="0.9973815">
(4) T: instantiable(C T, Ctx T)
H: uninstantiable(C H, Ctx H)
</listItem>
<bodyText confidence="0.999863428571429">
we would have a contradiction: the text says that
there is something of the more specific type C T,
whereas the hypothesis says there are no things of
the more general type C H. In this case, the rules
explicitly flag a contradiction.
Once all (un)instantiability claims have been
compared, it is possible to judge whether the text en-
tails or contradicts the hypothesis. Entailed hypothe-
sis (un)instantiability assertions are deleted from the
representation. Consequently, if there is one T and H
AKR readings and one set of alignments under which
all the H (un)instantiability assertions have been re-
moved, then there is an entailment of H by T. If
there is a pair of readings and a set of alignments
under which a contradiction is flagged, then there
is a contradiction. If there is no pair of readings or
set of alignments under which there is either an en-
tailment or a contradiction, then T and H are merely
consistent with one another. There are exceptional
cases such as (5) where one reading of T entails H
and another reading contradicts it.
</bodyText>
<listItem confidence="0.9321555">
(5) T: John did not wait to call for help.
H: John called for help.
</listItem>
<bodyText confidence="0.997701066666667">
Our ECD rules detect such cases.
WordNet often misses synonyms needed for the
alignment in the ECD. In particular, the hierarchy
and synsets for verbs are one of WordNet’s least de-
veloped parts. To test the impact of the missing syn-
onyms, we developed a variation on the ECD algo-
rithm that allows loose matching.
First, in concept alignment, if a verb concept in H
does not align with any verb concept in T, then we
permit it to (separately) align with all the text verb
concepts. We do not permit the same loose align-
ment for noun concepts, since we judge WordNet
information to be more reliable for nouns. This free
alignment of verbs might sound risky, but in gen-
eral these alignments will not lead to useful concept
</bodyText>
<page confidence="0.99817">
19
</page>
<bodyText confidence="0.999934058823529">
specificity judgments unless the T and H verbs have
very similar arguments / role restrictions.
When such a loose verb alignment is made, we
explicitly record this fact in a justification term in-
cluded in the alignment fact. Similarly, when judg-
ing concept specificity, each rule that applies adds a
term to a list of justifications recorded as part of the
fact indicating the specificity relation. This means
that when the final specificity judgments are deter-
mined, each judgment has a record of the sequence
of decisions made to reach it.
(Un)instantiability comparisons are made as in
strict matching. However, the criteria for detect-
ing an entailment are selectively loosened. If no
contradiction is flagged, and there is a pairing of
readings and alignments under which just a single
H instantiability assertion is left standing, then this
is allowed through as a loose entailment. However,
further rules are applied to block those loose entail-
ments that are deemed inappropriate. These block-
ing rules look at the form of the justification terms
gathered based on specificity judgments.
These blocking rules are manually selected. First,
a loose matching run is made without any block-
ing rules. Results are dumped for each T-H pair,
recording the expected logical relation and the jus-
tifications collected. Blocking rules are created by
detecting patterns of justification that are associated
with labeled non-entailments. One such blocking
rule says that if you have just a single H instantia-
bility left, but the specificity justifications leading to
this have been shown to be reliable on training data,
then the instantiability should not be eliminated as a
loose entailment.
</bodyText>
<sectionHeader confidence="0.962542" genericHeader="method">
4 Challenges in Processing the RTE Data
</sectionHeader>
<bodyText confidence="0.9994845">
The RTE3 data set contains inconsistencies in
spelling and punctuation between the text and the
hypothesis. To handle these, we did an automatic
prepass where we compared the strings in the pas-
sage text to those in the hypothesis. Some of the
special cases that we handled include:
</bodyText>
<listItem confidence="0.9999635">
• Normalize capitalization and spacing
• Identify acronyms and shorten names
• Title identification
• Spelling correction
</listItem>
<bodyText confidence="0.999588481481481">
Role names in VerbNet are in part intended to cap-
ture the relation of the argument to the event be-
ing described by the verb. For example, an object
playing an Agent role is causally involved in the
event, while an object playing a Theme or Patient
role is only supposed to be affected. This allows
participants in an action to be identified regardless
of the syntactic frame chosen to represent the verb;
this was seen in (1) and (2). Sometimes the roles
from VerbNet are not assigned in such a way as to
allow such transparent identification across frames
or related verbs. Consider an example. In Ed trav-
els/goes to Boston VerbNet identifies Ed as playing a
Theme role. However, in Ed flies to Boston VerbNet
assigns Ed an Agent role; this difference can make
determining contradiction and entailment between T
and H difficult. We have tried to compensate in our
ECD, by using a backoff strategy where fewer role
names are used (by projecting down role names to
the smaller set). As we develop the system further,
we continue to experiment with which set of roles
works best for which tasks.
Another open issue involves identifying alterna-
tive ways vague relations among objects appear in
text. We do not match the expression the Boston
team with the team from Boston. To improve our re-
call, we are considering loose matching techniques.
</bodyText>
<sectionHeader confidence="0.919755" genericHeader="conclusions">
5 Summary of our results on RTE3
</sectionHeader>
<bodyText confidence="0.999564625">
We participated in the RTE challenge as a way to
understand what our particular techniques could do
with respect to a more general version of textual en-
tailment. The overall experiment was quite enlight-
ening. Tables 1 and 2 summarize how we did on the
RTE3 challenge. System 1 is our standard system
with strict ECD. System 2 used the looser set of ECD
rules.
</bodyText>
<table confidence="0.998245857142857">
Gold Sys Cor- R P F
YES YES rect
IE 105 6 5 0.048 0.83 0.20
IR 87 4 4 0.046 1.00 0.21
QA 106 10 9 0.085 0.90 0.28
SUM 112 11 7 0.063 0.64 0.20
Total 410 31 25 0.060 0.84 0.22
</table>
<tableCaption confidence="0.999166">
Table 1: System 1 with Strict ECD
</tableCaption>
<page confidence="0.724347">
20
</page>
<table confidence="0.999062714285714">
Gold Sys Cor- R P F
YES YES rect
IE 105 15 10 0.095 0.67 0.25
IR 87 6 4 0.046 0.67 0.18
QA 106 14 13 0.12 0.93 0.34
SUM 112 17 10 0.089 0.59 0.23
Total 410 52 37 0.088 0.71 0.25
</table>
<tableCaption confidence="0.999208">
Table 2: System 2 with Loose ECD
</tableCaption>
<bodyText confidence="0.999767615384615">
As can be seen, we answered very few of the ques-
tions; only 31 of the possible 410 with a YES answer.
However, for those we did answer (requiring only
linguistic, and not world knowledge), we achieved
high precision: up to 90% on QA. However, we were
not perfect even from this perspective. Here are sim-
plified versions of the errors where our system an-
swered YES, and the answer should be NO with an
analysis of what is needed in the system to correct
the error.
The wrong result in (6) is due to our incomplete
coverage of intensional verbs (seek, want, look for,
need, etc.).
</bodyText>
<listItem confidence="0.928257">
(6) T: The US sought the release of hostages.
H: Hostages were released.
</listItem>
<bodyText confidence="0.9999096">
The object of an intensional verb cannot be assumed
to exist or to occur. Intensional verbs need to be
marked systematically in our lexicon.
The problem with (7) lies in the lack of treatment
for generic sentences.
</bodyText>
<listItem confidence="0.929787666666667">
(7) T: Girls and boys are segregated in high school
during sex education class.
H: Girls and boys are segregated in high school.
</listItem>
<bodyText confidence="0.999946578947369">
The natural interpretation of H is that girls and boys
are segregated in high school ALL THE TIME. Be-
cause we do not yet handle generic sentences prop-
erly, our algorithm for calculating specificity pro-
duces the wrong result here. It judges segregation in
H to be less specific than in T whereas the opposite
is in fact the case. Adding the word “sometimes” to
H would make our YES the correct answer.
The distinction between generic and episodic
readings is difficult to make but crucial for the in-
terpretation of bare plural noun phrases such as girls
and boys. For example, the most likely interpreta-
tion of Counselors are available is episodic: SOME
counselors are available. But Experts are highly
paid is weighted towards a generic reading: MOST
IF NOT ALL experts get a good salary.
These examples are indicative of the subtlety of
analysis necessary for high precision textual infer-
ence.
</bodyText>
<sectionHeader confidence="0.999429" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999507809523809">
Danny Bobrow, Cleo Condoravdi, Richard Crouch,
Ronald Kaplan, Lauri Karttunen, Tracy Holloway
King, Valeria de Paiva, and Annie Zaenen. 2005. A
basic logic for textual inference. In Proceedings of the
AAAI Workshop on Inference for Textual Question An-
swering.
Dick Crouch and Tracy Holloway King. 2005. Unify-
ing lexical resources. In Proceedings of the Interdisci-
plinary Workshop on the Identification and Represen-
tation of Verb Features and Verb Classes.
Dick Crouch and Tracy Holloway King. 2006. Se-
mantics via F-structure rewriting. In Proceedings of
LFG06. CSLI On-line Publications.
Dick Crouch, Mary Dalrymple, Ron Kaplan, Tracy King,
John Maxwell, and Paula Newman. 2007. XLE docu-
mentation. Available on-line.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press.
Olga Gurevich, Richard Crouch, Tracy Holloway King,
and Valeria de Paiva. 2006. Deverbal nouns in knowl-
edge representation. In Proceedings of FLAIRS 2006.
Andres Hickl, John Williams, Jeremy Bensley, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Recog-
nizing textual entailment with LCC’s GROUNDHOG
system. In The Second PASCAL Recognising Textual
Entailment Challenge.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class-based construction of a verb lexicon. In
AAAI-200017th National Conference on Artificial In-
telligence.
John Maxwell and Ron Kaplan. 1991. A method for
disjunctive constraint satisfaction. Current Issues in
Parsing Technologies.
Rowan Nairn, Cleo Condoravdi, and Lauri Karttunen.
2006. Computing relative polarity for textual infer-
ence. In Proceedings of ICoS-5.
Stefan Riezler, Tracy Holloway King, Ron Kaplan, Dick
Crouch, John Maxwell, and Mark Johnson. 2002.
Parsing the Wall Street Journal using a Lexical-
Functional Grammar and discriminative estimation
techniques. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics.
</reference>
<page confidence="0.999438">
21
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.133516">
<title confidence="0.998624">Precision-focused Textual Inference</title>
<author confidence="0.999493">D G Bobrow</author>
<author confidence="0.999493">C Condoravdi</author>
<author confidence="0.999493">R Crouch</author>
<author confidence="0.999493">V de_Paiva</author>
<author confidence="0.999493">L Karttunen</author>
<author confidence="0.999493">T H King</author>
<author confidence="0.999493">R Nairn</author>
<author confidence="0.999493">L Price</author>
<author confidence="0.999493">A Zaenen</author>
<affiliation confidence="0.638143">Palo Alto Research Center</affiliation>
<abstract confidence="0.976521888888889">This paper describes our system as used in The system maps premise and hypothesis pairs into an abstract knowledge and then performs enand contradiction detection the resulting Two versions of used in one with strict with looser</abstract>
<title confidence="0.9743865">Process Output Text-Breaking Delimited sentences Named-entity recognition Type-marked Entities Morphological Analysis Word stems plus features</title>
<author confidence="0.539982">Functional Structure</author>
<keyword confidence="0.751629333333333">processing Scope, Predicateargument structure Conceptual, Contextual,</keyword>
<intro confidence="0.792244">Temporal Structure</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Danny Bobrow</author>
<author>Cleo Condoravdi</author>
<author>Richard Crouch</author>
<author>Ronald Kaplan</author>
<author>Lauri Karttunen</author>
<author>Tracy Holloway King</author>
<author>Valeria de Paiva</author>
<author>Annie Zaenen</author>
</authors>
<title>A basic logic for textual inference.</title>
<date>2005</date>
<booktitle>In Proceedings of the AAAI Workshop on Inference for Textual Question Answering.</booktitle>
<marker>Bobrow, Condoravdi, Crouch, Kaplan, Karttunen, King, de Paiva, Zaenen, 2005</marker>
<rawString>Danny Bobrow, Cleo Condoravdi, Richard Crouch, Ronald Kaplan, Lauri Karttunen, Tracy Holloway King, Valeria de Paiva, and Annie Zaenen. 2005. A basic logic for textual inference. In Proceedings of the AAAI Workshop on Inference for Textual Question Answering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dick Crouch</author>
<author>Tracy Holloway King</author>
</authors>
<title>Unifying lexical resources.</title>
<date>2005</date>
<booktitle>In Proceedings of the Interdisciplinary Workshop on the Identification and Representation of Verb Features and Verb Classes.</booktitle>
<contexts>
<context position="5789" citStr="Crouch and King, 2005" startWordPosition="938" endWordPosition="941">ts and syntactic grammatical functions into roles. The mapping onto concepts uses WordNet (Fellbaum, 1998) to map words into lists of synsets. The named entity types provided by the morphology and syntax are used to create more accurate mapping of proper nouns since these are not systematically represented in WordNet. The semantic rules use the grammatical function subcategorization information from the verb and the role information found in extended VerbNet (Kipper et al., 2000) to map syntactic subjects, objects, and obliques into more abstract thematic roles such as Agent, Theme, and Goal (Crouch and King, 2005). This mapping into thematic-style roles allows the system to correctly align the arguments in pairs like (1) and (2), something which is impossible using just syntactic functions. In the first, the object and subject have a common thematic role in the alternation between transitive and intransitive; while in the second, the common role is shared by the subjects. (1) John broke the vasesyn:object,sem:patient. The vasesyn:subject,sem:patient broke. (2) Johnsyn:subject,sem:agent ate the cake. Johnsyn:subject,sem:agent ate. The goal of these semantic normalizations is to abstract away from the sy</context>
</contexts>
<marker>Crouch, King, 2005</marker>
<rawString>Dick Crouch and Tracy Holloway King. 2005. Unifying lexical resources. In Proceedings of the Interdisciplinary Workshop on the Identification and Representation of Verb Features and Verb Classes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dick Crouch</author>
<author>Tracy Holloway King</author>
</authors>
<title>Semantics via F-structure rewriting.</title>
<date>2006</date>
<booktitle>In Proceedings of LFG06.</booktitle>
<publisher>CSLI On-line Publications.</publisher>
<contexts>
<context position="4347" citStr="Crouch and King, 2006" startWordPosition="713" endWordPosition="716">t of the morphology (i.e. a series of lemmata with their tags) and produces a tree (constituent-structure) and a dependency structure (functional-structure) represented as an attribute-value matrix. The functional-structure is of primary importance for the semantics and AKR. In particular, it encodes predicate-argument relations, including long-distance dependencies, and provides other syntactic features (e.g. number, tense, noun type). The output of the syntax is input for the semantics that is produced by an ambiguity enabled packed rewriting system. The semantics is described in detail in (Crouch and King, 2006). Semantic processing assigns scope to scope-bearing elements such as negation and normalizes the output of the syntax. This normalization includes reformulating syntactic passives as actives (e.g. The cake was eaten by Mary. / Mary ate the cake.), resolving many null pronouns (e.g. Laughing, John entered the room / Johni laughing, Johni entered the room.), and canonicalizing measure phrases, comparatives, and dates. More complex normalizations involve converting nominal deverbals into the equivalent verbal form, identifying arguments of the verb from the arguments of the nominal (Gurevich et </context>
</contexts>
<marker>Crouch, King, 2006</marker>
<rawString>Dick Crouch and Tracy Holloway King. 2006. Semantics via F-structure rewriting. In Proceedings of LFG06. CSLI On-line Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dick Crouch</author>
<author>Mary Dalrymple</author>
<author>Ron Kaplan</author>
<author>Tracy King</author>
<author>John Maxwell</author>
<author>Paula Newman</author>
</authors>
<title>XLE documentation. Available on-line.</title>
<date>2007</date>
<marker>Crouch, Dalrymple, Kaplan, King, Maxwell, Newman, 2007</marker>
<rawString>Dick Crouch, Mary Dalrymple, Ron Kaplan, Tracy King, John Maxwell, and Paula Newman. 2007. XLE documentation. Available on-line.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>The MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Gurevich</author>
<author>Richard Crouch</author>
<author>Tracy Holloway King</author>
<author>Valeria de Paiva</author>
</authors>
<title>Deverbal nouns in knowledge representation.</title>
<date>2006</date>
<booktitle>In Proceedings of FLAIRS</booktitle>
<marker>Gurevich, Crouch, King, de Paiva, 2006</marker>
<rawString>Olga Gurevich, Richard Crouch, Tracy Holloway King, and Valeria de Paiva. 2006. Deverbal nouns in knowledge representation. In Proceedings of FLAIRS 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andres Hickl</author>
<author>John Williams</author>
<author>Jeremy Bensley</author>
<author>Kirk Roberts</author>
<author>Bryan Rink</author>
<author>Ying Shi</author>
</authors>
<title>Recognizing textual entailment with LCC’s GROUNDHOG system.</title>
<date>2006</date>
<booktitle>In The Second PASCAL Recognising Textual Entailment Challenge.</booktitle>
<contexts>
<context position="2644" citStr="Hickl et al., 2006" startWordPosition="442" endWordPosition="445">en they are in conflict. The Entailment and Contradiction Detection (ECD) algorithm makes a distinction that is not part of the basic RTE challenge. If T entails the negation of H, we answer NO (Contradiction). On the other Figure 1: The processing pipeline: processes with their ambiguity-enabled packed outputs hand, if there is no direct entailment we answer UNKNOWN. We do not try to construct a likely scenario that would link T and H. Nor have we tried to collect data on phrases that would tend to indicate such likely associations between T and H. That approach is clearly very useful (e.g. (Hickl et al., 2006)), and could be used as a backup strategy with our more formal entailment approach. We have chosen to focus on strict structural and lexical entailments. This paper describes the processing pipeline for mapping to AKR, the ECD algorithm, the challenges we faced in processing the RTE data and a summary of our results on RTE3. 2 Process Pipeline Figure 1 shows the processing pipeline for mapping texts to AKR. The input is a text of one or more sentences. All components of the system are “ambiguity enabled” (Maxwell and Kaplan, 1991). This allows each component to accept ambiguous input in a “pac</context>
</contexts>
<marker>Hickl, Williams, Bensley, Roberts, Rink, Shi, 2006</marker>
<rawString>Andres Hickl, John Williams, Jeremy Bensley, Kirk Roberts, Bryan Rink, and Ying Shi. 2006. Recognizing textual entailment with LCC’s GROUNDHOG system. In The Second PASCAL Recognising Textual Entailment Challenge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper</author>
<author>Hoa Trang Dang</author>
<author>Martha Palmer</author>
</authors>
<title>Class-based construction of a verb lexicon.</title>
<date>2000</date>
<booktitle>In AAAI-200017th National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="5651" citStr="Kipper et al., 2000" startWordPosition="916" endWordPosition="919">D is similar to the representation of Iraq destroyed its WMD. The final main task of the semantics rules is to convert words into concepts and syntactic grammatical functions into roles. The mapping onto concepts uses WordNet (Fellbaum, 1998) to map words into lists of synsets. The named entity types provided by the morphology and syntax are used to create more accurate mapping of proper nouns since these are not systematically represented in WordNet. The semantic rules use the grammatical function subcategorization information from the verb and the role information found in extended VerbNet (Kipper et al., 2000) to map syntactic subjects, objects, and obliques into more abstract thematic roles such as Agent, Theme, and Goal (Crouch and King, 2005). This mapping into thematic-style roles allows the system to correctly align the arguments in pairs like (1) and (2), something which is impossible using just syntactic functions. In the first, the object and subject have a common thematic role in the alternation between transitive and intransitive; while in the second, the common role is shared by the subjects. (1) John broke the vasesyn:object,sem:patient. The vasesyn:subject,sem:patient broke. (2) Johnsy</context>
</contexts>
<marker>Kipper, Dang, Palmer, 2000</marker>
<rawString>Karin Kipper, Hoa Trang Dang, and Martha Palmer. 2000. Class-based construction of a verb lexicon. In AAAI-200017th National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Maxwell</author>
<author>Ron Kaplan</author>
</authors>
<title>A method for disjunctive constraint satisfaction. Current Issues in Parsing Technologies.</title>
<date>1991</date>
<contexts>
<context position="3180" citStr="Maxwell and Kaplan, 1991" startWordPosition="535" endWordPosition="538">ciations between T and H. That approach is clearly very useful (e.g. (Hickl et al., 2006)), and could be used as a backup strategy with our more formal entailment approach. We have chosen to focus on strict structural and lexical entailments. This paper describes the processing pipeline for mapping to AKR, the ECD algorithm, the challenges we faced in processing the RTE data and a summary of our results on RTE3. 2 Process Pipeline Figure 1 shows the processing pipeline for mapping texts to AKR. The input is a text of one or more sentences. All components of the system are “ambiguity enabled” (Maxwell and Kaplan, 1991). This allows each component to accept ambiguous input in a “packed” format, process it without unpacking the ambiguities, and then pass packed input to the next stage. The syntactic component, LFG Parsing, also has a stochastic disambiguation system which allows us to pass the n-best on to the semantics (Riezler et al., 2002); for the RTE3 challenge, we used 16 Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 16–21, Prague, June 2007. @2007 Association for Computational Linguistics n=50. The parser takes the output of the morphology (i.e. a series of lemmata with thei</context>
</contexts>
<marker>Maxwell, Kaplan, 1991</marker>
<rawString>John Maxwell and Ron Kaplan. 1991. A method for disjunctive constraint satisfaction. Current Issues in Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rowan Nairn</author>
<author>Cleo Condoravdi</author>
<author>Lauri Karttunen</author>
</authors>
<title>Computing relative polarity for textual inference.</title>
<date>2006</date>
<booktitle>In Proceedings of ICoS-5.</booktitle>
<contexts>
<context position="7103" citStr="Nairn et al., 2006" startWordPosition="1136" endWordPosition="1139">ions. However, the semantics is still fundamentally a linguistic level of representation; further abstraction towards the meaning is done in the mapping from semantics to AKR. The AKR is the level of representation that is used to determine entailment and contradiction in our RTE3 system. A preliminary description of its logic was provided in (Bobrow et al., 2005). The AKR mapping converts grammatical tense and temporal modifiers into temporal relations, identifies anaphoric referents and makes explicit the implied relation between complement clauses and the main verb (e.g. for manage, fail) (Nairn et al., 2006). AKR also deals with standard phrases that are equivalent to simple vocabulary terms. For example, take a flight to New York is equivalent to fly to New York. These uses of “light” verbs (e.g. take, give) are not included in synonyms found in WordNet. Another class of phrasal synonyms involve inchoatives (e.g. take a turn for the worse/worsen). We included a special set of transformation rules for phrasal synonyms: some of the rules are part of the mapping from semantics to AKR while others are part of the ECD module. The mapping to AKR is done using the same ambiguity-enabled ordered rewriti</context>
</contexts>
<marker>Nairn, Condoravdi, Karttunen, 2006</marker>
<rawString>Rowan Nairn, Cleo Condoravdi, and Lauri Karttunen. 2006. Computing relative polarity for textual inference. In Proceedings of ICoS-5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Tracy Holloway King</author>
<author>Ron Kaplan</author>
<author>Dick Crouch</author>
<author>John Maxwell</author>
<author>Mark Johnson</author>
</authors>
<title>Parsing the Wall Street Journal using a LexicalFunctional Grammar and discriminative estimation techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3508" citStr="Riezler et al., 2002" startWordPosition="589" endWordPosition="593">hallenges we faced in processing the RTE data and a summary of our results on RTE3. 2 Process Pipeline Figure 1 shows the processing pipeline for mapping texts to AKR. The input is a text of one or more sentences. All components of the system are “ambiguity enabled” (Maxwell and Kaplan, 1991). This allows each component to accept ambiguous input in a “packed” format, process it without unpacking the ambiguities, and then pass packed input to the next stage. The syntactic component, LFG Parsing, also has a stochastic disambiguation system which allows us to pass the n-best on to the semantics (Riezler et al., 2002); for the RTE3 challenge, we used 16 Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 16–21, Prague, June 2007. @2007 Association for Computational Linguistics n=50. The parser takes the output of the morphology (i.e. a series of lemmata with their tags) and produces a tree (constituent-structure) and a dependency structure (functional-structure) represented as an attribute-value matrix. The functional-structure is of primary importance for the semantics and AKR. In particular, it encodes predicate-argument relations, including long-distance dependencies, and provides </context>
</contexts>
<marker>Riezler, King, Kaplan, Crouch, Maxwell, Johnson, 2002</marker>
<rawString>Stefan Riezler, Tracy Holloway King, Ron Kaplan, Dick Crouch, John Maxwell, and Mark Johnson. 2002. Parsing the Wall Street Journal using a LexicalFunctional Grammar and discriminative estimation techniques. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>