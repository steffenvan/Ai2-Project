<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.99586">
Better Informed Training of Latent Syntactic Features
</title>
<author confidence="0.998292">
Markus Dreyer and Jason Eisner
</author>
<affiliation confidence="0.815775333333333">
Department of Computer Science / Center for Language and Speech Processing
Johns Hopkins University
3400 North Charles Street, Baltimore, MD 21218 USA
</affiliation>
<email confidence="0.998141">
{markus,jason}@clsp.jhu.edu
</email>
<sectionHeader confidence="0.99738" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999976434782609">
We study unsupervised methods for learn-
ing refinements of the nonterminals in
a treebank. Following Matsuzaki et al.
(2005) and Prescher (2005), we may for
example split NP without supervision into
NP[0] and NP[1], which behave differently.
We first propose to learn a PCFG that adds
such features to nonterminals in such a
way that they respect patterns of linguis-
tic feature passing: each node’s nontermi-
nal features are either identical to, or inde-
pendent of, those of its parent. This lin-
guistic constraint reduces runtime and the
number of parameters to be learned. How-
ever, it did not yield improvements when
training on the Penn Treebank. An orthog-
onal strategy was more successful: to im-
prove the performance of the EM learner
by treebank preprocessing and by anneal-
ing methods that split nonterminals selec-
tively. Using these methods, we can main-
tain high parsing accuracy while dramati-
cally reducing the model size.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999891733333333">
Treebanks never contain enough information; thus
PCFGs estimated straightforwardly from the Penn
Treebank (Bies et al., 1995) work only moderately
well (Charniak, 1996). To address this problem,
researchers have used heuristics to add more infor-
mation. Eisner (1996), Charniak (1997), Collins
(1997), and many subsequent researchers1 anno-
tated every node with lexical features passed up
from its “head child,” in order to more precisely re-
flect the node’s “inside” contents. Charniak (1997)
and Johnson (1998) annotated each node with its
parent and grandparent nonterminals, to more pre-
cisely reflect its “outside” context. Collins (1996)
split the sentence label S into two versions, repre-
senting sentences with and without subjects. He
</bodyText>
<footnote confidence="0.504734">
1Not to mention earlier non-PCFG lexicalized statistical
parsers, notably Magerman (1995) for the Penn Treebank.
</footnote>
<bodyText confidence="0.999937025641025">
also modified the treebank to contain different la-
bels for standard and for base noun phrases. Klein
and Manning (2003) identified nonterminals that
could valuably be split into fine-grained ones us-
ing hand-written linguistic rules. Their unlexical-
ized parser combined several such heuristics with
rule markovization and reached a performance
similar to early lexicalized parsers.
In all these cases, choosing which nonterminals
to split, and how, was a matter of art. Ideally
such splits would be learned automatically from
the given treebank itself. This would be less costly
and more portable to treebanks for new domains
and languages. One might also hope that the auto-
matically learned splits would be more effective.
Matsuzaki et al. (2005) introduced a model for
such learning: PCFG-LA.2 They used EM to in-
duce fine-grained versions of a given treebank’s
nonterminals and rules. We present models that
similarly learn to propagate fine-grained features
through the tree, but only in certain linguistically
motivated ways. Our models therefore allocate
a supply of free parameters differently, allow-
ing more fine-grained nonterminals but less fine-
grained control over the probabilities of rewriting
them. We also present simple methods for decid-
ing selectively (during training) which nontermi-
nals to split and how.
Section 2 describes previous work in finding
hidden information in treebanks. Section 3 de-
scribes automatically induced feature grammars.
We start by describing the PCFG-LA model, then
introduce new models that use specific agreement
patterns to propagate features through the tree.
Section 4 describes annealing-like procedures for
training latent-annotation models. Section 5 de-
scribes the motivation and results of our experi-
ments. We finish by discussing future work and
conclusions in sections 6–7.
</bodyText>
<footnote confidence="0.8105825">
2Probabilistic context-free grammar with latent annota-
tions.
</footnote>
<page confidence="0.934779">
317
</page>
<note confidence="0.993889846153846">
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 317–326,
Sydney, July 2006. c�2006 Association for Computational Linguistics
Citation Observed data Hidden data
Collins (1997) Treebank tree with head child an- No hidden data. Degenerate EM
notated on each nonterminal case.
Lari and Young (1990) Words Parse tree
Pereira and Schabes (1992) Words and partial brackets Parse tree
Klein and Manning (2001) Part-of-speech tags Parse tree
Chiang and Bikel (2002) Treebank tree Head child on each nonterminal
Matsuzaki et al. (2005) Treebank tree Integer feature on each nontermi-
nal
INHERIT model (this paper) Treebank tree and head child Integer feature on each nontermi-
heuristics nal
</note>
<tableCaption confidence="0.996678">
Table 1: Observed and hidden data in PCFG grammar learning.
</tableCaption>
<sectionHeader confidence="0.909412" genericHeader="method">
2 Partially supervised EM learning
</sectionHeader>
<bodyText confidence="0.999871809523809">
The parameters of a PCFG can be learned with
or without supervision. In the supervised case,
the complete tree is observed, and the rewrite rule
probabilities can be estimated directly from the
observed rule counts. In the unsupervised case,
only the words are observed, and the learning
method must induce the whole structure above
them. (See Table 1.)
In the partially supervised case we will con-
sider, some part of the tree is observed, and
the remaining information has to be induced.
Pereira and Schabes (1992) estimate PCFG pa-
rameters from partially bracketed sentences, using
the inside-outside algorithm to induce the miss-
ing brackets and the missing node labels. Some
authors define a complete tree as one that speci-
fies not only a label but also a “head child” for
each node. Chiang and Bikel (2002) induces the
missing head-child information; Prescher (2005)
induces both the head-child information and the
latent annotations we will now discuss.
</bodyText>
<sectionHeader confidence="0.97778" genericHeader="method">
3 Feature Grammars
</sectionHeader>
<subsectionHeader confidence="0.980409">
3.1 The PCFG-LA Model
</subsectionHeader>
<bodyText confidence="0.999913136363636">
Staying in the partially supervised paradigm, the
PCFG-LA model described in Matsuzaki et al.
(2005) observe whole treebank trees, but learn
an “annotation” on each nonterminal token—an
unspecified and uninterpreted integer that distin-
guishes otherwise identical nonterminals. Just as
Collins manually split the S nonterminal label into
S and SG for sentences with and without subjects,
Matsuzaki et al. (2005) split S into S[1], S[2], ... ,
S[L] where L is a predefined number—but they do
it automatically and systematically, and not only
for S but for every nonterminal. Their partially
supervised learning procedure observes trees that
are fully bracketed and fully labeled, except for
the integer subscript used to annotate each node.
After automatically inducing the annotations with
EM, their resulting parser performs just as well as
one learned from a treebank whose nonterminals
were manually refined through linguistic and error
analysis (Klein and Manning, 2003).
In Matsuzaki’s PCFG-LA model, rewrite rules
take the form
</bodyText>
<equation confidence="0.971739">
X[α] —* Y [Q] Z[-y] (1)
in the binary case, and
X[α] —* w (2)
</equation>
<bodyText confidence="0.9991438">
in the lexical case. The probability of a tree con-
sisting of rules r1, r2,... is given by the probabil-
ity of its root symbol times the conditional prob-
abilities of the rules. The annotated tree T1 in
Fig. 1, for example, has the following probability:
</bodyText>
<equation confidence="0.999393">
P(T1) = P(ROOT S[2])
xP(S[2] NP[1] VP[3])
xP(NP[1] —*� He)
xP(VP[3] —*� loves cookies)
</equation>
<bodyText confidence="0.9967385">
where, to simplify the notation, we use
P(X —* Y Z) to denote the conditional probabil-
ity P(Y Z  |X) that a given node with label X
will have children Y Z.
Degrees of freedom. We will want to compare
models that have about the same size. Models with
more free parameters have an inherent advantage
on modeling copious data because of their greater
</bodyText>
<page confidence="0.998517">
318
</page>
<figureCaption confidence="0.999812">
Figure 1: Treebank tree with annotations.
</figureCaption>
<bodyText confidence="0.931226869565217">
expressiveness. Models with fewer free parame-
ters are easier to train accurately on sparse data,
as well as being more efficient in space and often
in time. Our question is therefore what can be ac-
complished with a given number of parameters.
How many free parameters in a PCFG-LA
model? Such a model is created by annotating
the nonterminals of a standard PCFG (extracted
from the given treebank) with the various integers
from 1 to L. If the original, “backbone” grammar
has R3 binary rules of the form X —* Y Z, then
the resulting PCFG-LA model has L3 x R3 such
rules: X[1] —* Y [1] Z[1], X[1] —* Y [1] Z[2],
X[1] —* Y [2] Z[1], ..., X[L] —* Y [L] Z[L]. Sim-
ilarly, if the backbone grammar has R2 rules of
the form X —* Y the PCFG-LA model has L2 x
R2 such rules.3 The number of R1 terminal rules
X —* w is just multiplied by L.
The PCFG-LA has as many parameters to learn
as rules: one probability per rule. However, not
all these parameters are free, as there are L x N
sum-to-one constraints, where N is the number of
backbone nonterminals. Thus we have
</bodyText>
<equation confidence="0.996044">
L3R3 + L2R2 + LR1 − LN (3)
</equation>
<bodyText confidence="0.958649538461538">
degrees of freedom.
We note that Goodman (1997) mentioned possi-
ble ways to factor the probability 1, making inde-
pendence assumptions in order to reduce the num-
ber of parameters.
Runtime. Assuming there are no unary rule cy-
cles in the backbone grammar, bottom-up chart
parsing of a length-n sentence at test time takes
time proportional to n3L3R3 + n2L2R2 + nLR1,
by attempting to apply each rule everywhere in the
sentence. (The dominating term comes from equa-
tion (4) of Table 2: we must loop over all n3 triples
i, j, k and all R3 backbone rules X —* Y Z and all
</bodyText>
<footnote confidence="0.887532">
3We use unary rules of this form (e.g. the Treebank’s S �
NP) in our reimplementation of Matsuzaki’s algorithm.
</footnote>
<equation confidence="0.5564925">
L3 triples α, 0, -y.) As a function of n and L only,
this is 0(n3L3).
</equation>
<bodyText confidence="0.999917846153846">
At training time, to induce the annotations on
a given backbone tree with n nodes, one can run
a constrained version of this algorithm that loops
over only the n triples i, j, k that are consistent
with the given tree (and considers only the single
consistent backbone rule for each one). This takes
time 0(nL3), as does the inside-outside version
we actually use to collect expected PCFG-LA rule
counts for EM training.
We now introduce a model that is smaller, and
has a lower runtime complexity, because it adheres
to specified ways of propagating features through
the tree.
</bodyText>
<subsectionHeader confidence="0.995248">
3.2 Feature Passing: The INHERIT Model
</subsectionHeader>
<bodyText confidence="0.999697875">
Many linguistic theories assume that features get
passed from the mother node to their children or
some of their children. In many cases it is the
head child that gets passed its feature value from
its mother (e.g., Kaplan and Bresnan (1982), Pol-
lard and Sag (1994)). In some cases the feature is
passed to both the head and the non-head child, or
perhaps even to the non-head alone.
</bodyText>
<figureCaption confidence="0.6802725">
Figure 2: Features are passed to different children
at different positions in the tree.
</figureCaption>
<bodyText confidence="0.999098142857143">
In the example in Fig. 2, the tense feature (pres)
is always passed to the head child (underlined).
How the number feature (sg/pl) is passed depends
on the rewrite rule: S —* NP VP passes it to both
children, to enforce subject-verb agreement, while
VP —* V NP only passes it to the head child, since
the object NP is free not to agree with the verb.
A feature grammar can incorporate such pat-
terns of feature passing. We introduce additional
parameters that define the probability of passing a
feature to certain children. The head child of each
node is given deterministically by the head rules
of (Collins, 1996).
Under the INHERIT model that we propose, the
</bodyText>
<page confidence="0.997283">
319
</page>
<table confidence="0.99908152173913">
Model Runtime and d.f. Simplified equation for inside probabilities (ignores unary rules)
Matsuzaki test: O(n3L3) train: O(nL3)
et al. (2005) d.f.: L3R3 + BX[α](i, k) = P(X[α] → Y [β] Z[γ]) (4)
L2R2 +LR1 −LN �
Y,β,Z,γ,j
×BY [β](i, j) × BZ[γ](j, k)
INHERIT test: O(n3L) BX[α](i, k) = P(X[α] → Y Z) (5)
model train: O(nL) �
(this paper) d.f.: L(R3 + R2 + Y,Z,j
R1) + 3R3 − N P(neither  |X, Y, Z) × BY (i, j) × BZ(j, k))
0 1
�+ P(left  |X, Y, Z) × BY [α](i, j) × BZ(j, k)) �
+ P(right  |X, Y, Z) × BY (i, j) × BZ[α](j, k))
× � �
+ P(both  |X, Y, Z) × BY [α]Y (i, j) × BZ[α](j, k))
BX(i, j) = Pann(α  |X) × BX[α](i, j) (6)
�
α �P(head  |X, Y, Z) if Y heads X → Y Z
P(left  |X, Y, Z) = (7)
P(nonhead  |X, Y, Z) otherwise
� P(head  |X, Y, Z) if Z heads X → Y Z
P(right  |X, Y, Z) = (8)
P(nonhead  |X, Y, Z) otherwise
</table>
<tableCaption confidence="0.995672">
Table 2: Comparison of the PCFG-LA model with the INHERIT model proposed in this paper. “d.f.”
</tableCaption>
<bodyText confidence="0.93747825">
stands for “degrees of freedom” (i.e., free parameters). The B terms are inside probabilities; to compute
Viterbi parse probabilities instead, replace summation by maximization. Note the use of the intermediate
quantity BX(i, j) to improve runtime complexity by moving some summations out of the inner loop;
this is an instance of a “folding transformation” (Blatz and Eisner, 2006).
Figure 3: Two passpatterns. Left: T2. The feature
is passed to the head child (underlined). Right: T3.
The feature is passed to both children.
probabilities of tree T2 in Fig. 3 are calculated as
follows, with Pann(1  |NP) being the probability
of annotating an NP with feature 1 if it does not
inherit its parent’s feature. The VP is boldfaced to
indicate that it is the head child of this rule.
</bodyText>
<equation confidence="0.999037181818182">
P(T2) = P(ROOT → S[2])
×P(S[2] → NP VP)
×P(pass to head  |S → NP VP)
×Pann(1  |NP) × P(NP[1] →* He)
×P(VP[2] →* loves cookies)
Tree T3 in Fig. 3 has the following probability:
P(T3) = P(ROOT → S[2])
×P(S[2] → NP VP)
×P(pass to both  |S → NP VP)
×P(NP[2] →� He)
×P(VP[2] →� loves cookies)
</equation>
<bodyText confidence="0.951272103448276">
In T2, the subject NP chose feature 1 or 2 indepen-
dent of its parent S, according to the distribution
Pann(·  |NP). In T3, it was constrained to inherit
its parent’s feature 2.
Degrees of freedom. The INHERIT model may
be regarded as containing all the same rules
(see (1)) as the PCFG-LA model. However, these
rules’ probabilities are now collectively deter-
mined by a smaller set of shared parameters.4 That
is because the distribution of the child features Q
and -y no longer depends arbitrarily on the rest of
the rule. Q is either equal to α, or chosen indepen-
dently of everything but Y .
The model needs probabilities for L × R3
binary-rule parameters like P(S[2] → NP VP)
above, as well as L × R2 unary-rule and L × R1
lexical-rule parameters. None of these consider
the annotations on the children. They are subject
to L × N sum-to-one constraints.
The model also needs 4 × R3 passpattern prob-
abilities like P(pass to head  |X → Y Z) above,
with R3 sum-to-one constraints, and L × N non-
inherited annotation parameters Pann(α|X), with
N sum-to-one constraints.
Adding these up and canceling the two L × N
4The reader may find it useful to write out the probability
P(X[α] → Y [β] Z[γ]) in terms of the parameters described
below. Like equation (5), it is P(X[α] → Y Z) times a sum
of up to 4 products, corresponding to the 4 passpattern cases.
</bodyText>
<page confidence="0.955814">
320
</page>
<bodyText confidence="0.818525">
terms, the INHERIT model has
</bodyText>
<equation confidence="0.995482">
L(R3 + R2 + R1) + 3R3 — N (9)
</equation>
<bodyText confidence="0.994668818181818">
degrees of freedom. Thus for a typical grammar
where R3 dominates, we have reduced the number
of free parameters from about L3R3 to only about
LR3.
Runtime. We may likewise reduce an L3 factor
to L in the runtime. Table 2 shows dynamic pro-
gramming equations for the INHERIT model. By
exercising care, they are able to avoid summing
over all possible values of β and γ within the in-
ner loop. This is possible because when they are
not inherited, they do not depend on X, Y, Z, or α.
</bodyText>
<subsectionHeader confidence="0.996134">
3.3 Multiple Features
</subsectionHeader>
<bodyText confidence="0.999955235294118">
The INHERIT model described above is linguisti-
cally naive in several ways. One problem (see sec-
tion 6 for others) is that each nonterminal has only
a single feature to pass. Linguists, however, usu-
ally annotate each phrase with multiple features.
Our example tree in Fig. 2 was annotated with both
tense and number features, with different inheri-
tance patterns.
As a step up from INHERIT, we propose an
INHERIT2 model where each nonterminal carries
two features. Thus, we will have L6R3 binary
rules instead of L3R3. However, we assume that
the two features choose their passpatterns inde-
pendently, and that when a feature is not inher-
ited, it is chosen independently of the other fea-
ture. This keeps the number of parameters down.
In effect, we are defining
</bodyText>
<equation confidence="0.9995335">
P(X[α][ρ] —* Y [β][σ] Z[γ][τ])
� P(X[α][ρ] —* Y Z)
xP1(β, γ  |X[α] —* Y Z)
xP2(σ,τ  |X[ρ] —* Y Z)
</equation>
<bodyText confidence="0.999951428571429">
where P1 and P2 choose child features as if they
were separate single-feature INHERIT models.
We omit discussion of dynamic programming
speedups for INHERIT2. Empirically, the hope is
that the two features when learned with the EM
algorithm will pick out different linguistic proper-
ties of the constituents in the treebank tree.
</bodyText>
<sectionHeader confidence="0.938053" genericHeader="method">
4 Annealing-Like Training Approaches
</sectionHeader>
<bodyText confidence="0.999008039215687">
Training latent PCFG models, like training most
other unsupervised models, requires non-convex
optimization. To find good parameter values, it
is often helpful to train a simpler model first and
use its parameters to derive a starting guess for the
harder optimization problem. A well-known ex-
ample is the training of the IBM models for statis-
tical machine translation (Berger et al., 1994).
In this vein, we did an experiment in which we
gradually increased L during EM training of the
PCFG-LA and INHERIT models. Whenever the
training likelihood began to converge, we man-
ually and globally increased L, simply doubling
or tripling it (see “clone all” in Table 3 and
Fig. 5). The probability of X[α] —* Y [β]Z[γ]
under the new model was initialized to be pro-
portional to the probability of X[α mod L] —*
Y [β mod L]Z[γ mod L] (where L refers to the
old L),5 times a random ”jitter” to break symme-
try.
In a second annealing experiment (“clone
some”) we addressed a weakness of the PCFG-
LA and INHERIT models: They give every non-
terminal the same number of latent annotations.
It would seem that different coarse-grained non-
terminals in the original Penn Treebank have dif-
ferent degrees of impurity (Klein and Manning,
2003). There are linguistically many kinds of
NP, which are differentially selected for by vari-
ous contexts and hence are worth distinguishing.
By contrast, -LRB- is almost always realized as
a left parenthesis and may not need further refine-
ment. Our “clone some” annealing starts by train-
ing a model with L=2 to convergence. Then, in-
stead of cloning all nonterminals as in the previ-
ous annealing experiments, we clone only those
that have seemed to benefit most from their previ-
ous refinement. This benefit is measured by the
Jensen-Shannon divergence of the two distribu-
tions P(X[0] —* · · · ) and P(X[1] —* · · · ). The
5Notice that as well as cloning X[α], this procedure mul-
tiplies by 4, 2, and 1 the number of binary, unary, and lex-
ical rules that rewrite X[α]. To leave the backbone gram-
mar unchanged, we should have scaled down the probabili-
ties of such rules by 1/4, 1/2, and 1 respectively. Instead, we
simply scaled them all down by the same proportion. While
this temporarily changes the balance of probability among the
three kinds of rules, EM immediately corrects this balance on
the next training iteration to match the observed balance on
the treebank trees—hence the one-iteration downtick in Fig-
ure 5).
</bodyText>
<page confidence="0.954729">
321
</page>
<bodyText confidence="0.583713">
Jensen-Shannon divergence is defined as
</bodyText>
<equation confidence="0.969973">
D(q,r) = 2 (D (q q2r) + D (r  ||q + r)/
2
</equation>
<bodyText confidence="0.983567085714286">
These experiments are a kind of
of the deterministic annealing cluster-
ing algorithm (Pereira et al., 1993; Rose, 1998),
which gradually increases the number of clus-
ters during the clustering process. In determinis-
tic annealing, one starts in principle with a very
large number of clusters, but maximizes likeli-
hood only under a constraint that the joint distri-
bution p(point, cluster) must have very high en-
tropy. This drives all of the cluster centroids to co-
incide exactly, redundantly representing just one
effective cluster. As the entropy is permitted to de-
crease, some of the cluster centroids find it worth-
while to drift apart.6 In future work, we would
like to apply this technique to split nonterminals
gradually, by initially requiring high-entropy parse
forests on the training data an
“poorman’sversion”
d slowly relaxing this
constraint.
We ran several experiments to compare the
with the PCFG-LA model and look into the
effect of different Treebank preprocessing and the
annealing-like procedures.
We used sections
of the Penn Treebank 2
Wall Street Journal corpus (Marcus et al., 1993)
for training, section 22 as development set and
section 23 for testing. Following Matsuzaki et al.
(2005), words occurring fewer than 4 times in the
training corpus were replaced by unknown-word
symbols that encoded certain suffix and capitaliza-
tion information.
All experiments used simple add-lambda
smoothing (A
</bodyText>
<equation confidence="0.256812333333333">
IN-
HERIT
2–20
</equation>
<bodyText confidence="0.803675846153846">
=0.1) during the reestimation step
(M step) of training.
322 Figure 4: Horizontal and vertical markovization
all but the first
experiments, we also
enriched the nonterminals with order-1 horizon-
tal and order-2 vertical markovization (Klein and
Manning,
Figure 4 shows what amultiple-
child structure
A B H C D looks like
after binarization and markovization. The bina-
Each auxiliary
</bodyText>
<equation confidence="0.913618333333333">
(“Basic”)
2003).7
X →
</equation>
<bodyText confidence="0.968136733333333">
node consists of the parent label,
the direction (L or R) and the label of the child
just picked up.
rization process starts at the head of the sentence
and moves to the right, inserting an auxiliary node
for each picked up child, then moving to the left.
an
d center-parent binarization of the rule X →
A B H C D where H is the head child.
to break symmetry
[.9999,1.0001]
.
ing the rules at most binary
, this preprocessing also
helpfully enriched the backbone nonterminals. For
</bodyText>
<footnote confidence="0.856851857142857">
practice, each very large group of centroids (effective
cluster) is repre
6In
sented by just two, until such time as those
two drift apart to represent separate effective clusters—then
each is cloned.
dard Treebank parses.
</footnote>
<sectionHeader confidence="0.999638" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.995417">
5.1 Setup
</subsectionHeader>
<bodyText confidence="0.999859833333333">
Binarization and Markovization. Before ex-
tracting the backbone PCFG and running the con-
strained inside-outside (EM) training algorithm,
we preprocessed the Treebank using center-parent
binarization Matsuzaki et al. (2005). Besides mak-
Initialization. The backbone PCFG grammar
was read off the altered Treebank, and the initial
annotated grammar was created by creating sev-
eral versions of every rewrite rule. The proba-
bilities of these newly created rules are uniform
and proportional to the original rule, multiplied by
a random epsilon factor uniformly sampled from
</bodyText>
<subsectionHeader confidence="0.997262">
5.2 Decoding
</subsectionHeader>
<bodyText confidence="0.9956813">
To test the PCFG learned by a given method,
we attempted to recover the unannotated parse
of each sentence in the development set. We
then scored these parses by debinarizing or de-
markovizing them, then measuring their precision
and recall of the labeled constituents from the
gold-stan
vertical markovization was applied before binariza-
tion. – Matsuzaki et al. (2005) used a markovized grammar
to get a better unannotated parse fore
</bodyText>
<footnote confidence="0.470486">
7The
st during decoding, but
they did not markovize the training data.
</footnote>
<figureCaption confidence="0.843274">
Figure 5: Loge-likelihood during training. The
</figureCaption>
<bodyText confidence="0.99359392">
two “anneal” curves use the “clone all” method.
We increased L after iteration 50 and, for the IN-
HERIT model, iteration 110. The downward spikes
in the two annealed cases are due to perturbation
of the model parameters (footnote 5).
An unannotated parse’s probability is the total
probability, under our learned PCFG, of all of its
annotated refinements. This total can be efficiently
computed by the constrained version of the inside
algorithm in Table 2.
How do we obtain the unannotated parse whose
total probability is greatest? It does not suffice to
find the single best annotated parse and then strip
off the annotations. Matsuzaki et al. (2005) note
that the best annotated parse is in fact NP-hard to
find. We use their reranking approximation. A
1000-best list for each sentence in the decoding
set was created by parsing with our markovized
unannotated grammar and extracting the 1000 best
parses using the k-best algorithm 3 described in
Huang and Chiang (2005). Then we chose the
most probable of these 1000 unannotated parses
under our PCFG, first finding the total probability
of each by using the the constrained inside algo-
rithm as explained above.8
</bodyText>
<subsectionHeader confidence="0.944948">
5.3 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.98370925">
Table 3 summarizes the results on development
and test data. 9 Figure 5 shows the training log-
likelihoods.
First, markovization of the Treebank leads to
</bodyText>
<footnote confidence="0.9983132">
8For the first set of experiments, in which the models were
trained on a simple non-markovized grammar, the 1000-best
trees had to be “demarkovized” before our PCFG was able to
rescore them.
9All results are reported on sentences of 40 words or less.
</footnote>
<bodyText confidence="0.999923386363637">
striking improvements. The “Basic” block of ex-
periments in Table 3 used non-markovized gram-
mars, as in Matsuzaki et al. (2005). The next block
of experiments, introducing markovized gram-
mars, shows a considerable improvement. This
is not simply because markovization increases the
number of parameters: markovization with L = 2
already beats basic models that have much higher
L and far more parameters.
Evidently, markovization pre-splits the labels
in the trees in a reasonable way, so EM has less
work to do. This is not to say that markovization
eliminates the need for hidden annotations: with
markovization, going from L=1 to L=2 increases
the parsing accuracy even more than without it.
Second, our “clone all” training technique
(shown in the next block of Table 3) did not
help performance and may even have hurt slightly.
Here we initialized the L=2x2 model with the
trained L=2 model for PCFG-LA, and the L=3x3
model with the L=3 and the L=3x3x3 model with
the L=3x3 model.
Third, our “clone some” training technique ap-
peared to work. On PCFG-LA, the L&lt;2x2 con-
dition (i.e., train with L=2 and then clone some)
matched the performance of L=4 with 30% fewer
parameters. On INHERIT, L&lt;2x2 beat L=4 with
8% fewer parameters. In these experiments, we
used the average divergence as a threshold: x[0]
and X[1] are split again if the divergence of their
rewrite distributions is higher than average.
Fourth, our INHERIT model was a disappoint-
ment. It generally performed slightly worse than
PCFG-LA when given about as many degrees
of freedom. This was also the case on some
cursory experiments on smaller training corpora.
It is tempting to conclude that INHERIT simply
adopted overly strong linguistic constraints, but
relaxing those constraints by moving to the IN-
HERIT2 model did not seem to help. In our
one experiment with INHERIT2 (not shown in Ta-
ble 3), using 2 features that can each take L=2
values (d.f.: 212,707) obtains an F1 score of only
83.67—worse than 1 feature taking L=4 values.
</bodyText>
<subsectionHeader confidence="0.992261">
5.4 Analysis: What was learned by INHERIT?
</subsectionHeader>
<bodyText confidence="0.99887">
INHERIT did seem to discover “linguistic” fea-
tures, as intended, even though this did not im-
prove parse accuracy. We trained INHERIT and
PCFG-LA models (both L=2, non-markovized)
and noticed the following.
</bodyText>
<page confidence="0.997491">
323
</page>
<table confidence="0.999843388888889">
L PCFG -LA LR F1 L d.f. INHERIT LR F1
d.f. LP LP
Basic 1 24,226 76.99 74.51 75.73 1 35,956 76.99 74.51 75.73
2 72,392 81.22 80.67 80.94 2 60,902 79.42 77.58 78.49
4 334,384 83.53 83.39 83.46 12 303,162 82.41 81.55 81.98
8 2,177,888 85.43 85.05 85.24 80 1,959,053 83.99 83.02 83.50
Markov. 1 41,027 79.95 78.43 79.18 1 88,385 79.95 78.43 79.18
2 132,371 83.85 82.23 83.03
2 178,264 85.70 84.37 85.03 3 176,357 85.04 83.60 84.31
4 220,343 85.30 84.06 84.68
3 506,427 86.44 85.19 85.81 9 440,273 86.16 85.12 85.64
4 1,120,232 87.09 85.71 86.39 26 1,188,035 86.55 85.55 86.05
Clone all 2 178,264 85.70 84.37 85.03 3 176,357 85.04 83.60 84.31
3x3 440,273 85.99 84.88 85.43
2x2 1,120,232 87.06 85.49 86.27 3x3x3 1,232,021 86.65 85.70 86.17
Cl.some 2 178,264 85.70 84.37 85.03 2 132,371 83.85 82.23 83.03
&lt;2x2 789,279 87.17 85.71 86.43 &lt;2x2 203,673 85.49 84.45 84.97
&lt;2x2x2 314,999 85.57 84.60 85.08
</table>
<tableCaption confidence="0.7790115">
Table 3: Results on the development set: labeled precision (LP), labeled recall (LR), and their harmonic
mean (F1). “Basic” models are trained on a non-markovized treebank (as in Matsuzaki et al. (2005)); all
others are trained on a markovized treebank. The best model (PCFG-LA with “clone some” annealing,
F1=86.43) has also been decoded on the final test set, reaching P/R=86.94/85.40 (F1=86.17).
</tableCaption>
<bodyText confidence="0.999963902439024">
We used both models to assign the most-
probable annotations to the gold parses of the de-
velopment set. Under the INHERIT model, NP[0]
vs. NP[1] constituents were 21% plural vs. 41%
plural. Under PCFG-LA this effect was weaker
(30% vs. 39%), although it was significant in both
(Fisher’s exact test, p &lt; 0.001). Strikingly, un-
der the INHERIT model, the NP’s were 10 times
more likely to pass this feature to both children
(Fisher’s, p &lt; 0.001)—just as we would expect
for a number feature, since the determiner and
head noun of an NP must agree.
The INHERIT model also learned to use feature
value 1 for “tensed auxiliary.” The VP[1] nonter-
minal was far more likely than VP[0] to expand as
V VP, where V represents any of the tensed verb
preterminals VBZ, VBG, VBN, VBD, VBP. Further-
more, these expansion rules had a very strong pref-
erence for “pass to head,” so that the left child
would also be annotated as a tensed auxiliary, typ-
ically causing it to expand as a form of be, have,
or do. In short, the feature ensured that it was gen-
uine auxiliary verbs that subcategorized for VP&apos;s.
(The PCFG-LA model actually arranged the
same behavior, e.g. similarly preferring VBZ[1] in
the auxiliary expansion rule VP -* VBZ VP. The
difference is that the PCFG-LA model was able
to express this preference directly without prop-
agating the [1] up to the VP parent. Hence neither
VP[0] nor VP[1] became strongly associated with
the auxiliary rule.)
Many things are equally learned by both mod-
els: They learn the difference between subordinat-
ing conjunctions (while, if) and prepositions (un-
der, after), putting them in distinct groups of the
original IN tag, which typically combine with sen-
tences and noun phrases, respectively. Both mod-
els also split the conjunction CC into two distinct
groups: a group of conjunctions starting with an
upper-case letter at the beginning of the sentence
and a group containing all other conjunctions.
</bodyText>
<sectionHeader confidence="0.998113" genericHeader="method">
6 Future Work: Log-Linear Modeling
</sectionHeader>
<bodyText confidence="0.999756333333333">
Our approach in the INHERIT model made certain
strict independence assumptions, with no backoff.
The choice of a particular passpattern, for exam-
ple, depends on all and only the three nontermi-
nals X, Y, Z. However, given sparse training data,
sometimes it is advantageous to back off to smaller
amounts of contextual information; the nontermi-
nal X or Y might alone be sufficient to predict the
passpattern.
</bodyText>
<page confidence="0.996381">
324
</page>
<bodyText confidence="0.999734696969697">
A very reasonable framework for handling this
issue is to model P(X[α] — Y [Q] Z[-y]) with
a log-linear model.10 Feature functions would
consider the values of variously sized, over-
lapping subsets of X, Y, Z, α, Q, -y. For exam-
ple, a certain feature might fire when X[α] =
NP[1] and Z[-y] = N[2]. This approach can be ex-
tended to the multi-feature case, as in INHERIT2.
Inheritance as in the INHERIT model can then
be expressed by features like α = Q, or α =
Q and X = VP. During early iterations, we could
use a prior to encourage a strong positive weight
on these inheritance features, and gradually re-
lax this bias—akin to the “structural annealing” of
(Smith and Eisner, 2006).
When modeling the lexical rule P(X[α] -* w),
we could use features that consider the spelling
of the word w in conjunction with the value of
α. Thus, we might learn that V [1] is particularly
likely to rewrite as a word ending in -s. Spelling
features that are predictable from string context
are important clues to the existence and behavior
of the hidden annotations we wish to induce.
A final remark is that “inheritance” does not
necessarily have to mean that α = Q. It is enough
that α and Q should have high mutual informa-
tion, so that one can be predicted from the other;
they do not actually have to be represented by the
same integer. More broadly, we might like α to
have high mutual information with the pair (Q, -y).
One might try using this sort of intuition directly
in an unsupervised learning procedure (Elidan and
Friedman, 2003).
</bodyText>
<sectionHeader confidence="0.999459" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999919">
We have discussed “informed” techniques for in-
ducing latent syntactic features. Our INHERIT
model tries to constrain the way in which features
are passed through the tree. The motivation for
this approach is twofold: First, we wanted to cap-
ture the linguistic insight that features follow cer-
tain patterns in propagating through the tree. Sec-
ond, we wanted to make it statistically feasible and
computationally tractable to increase L to higher
values than in the PCFG-LA model. The hope was
that the learning process could then make finer dis-
tinctions and learn more fine-grained information.
However, it turned out that the higher values of
L did not compensate for the perhaps overly con-
</bodyText>
<footnote confidence="0.496874">
10This affects EM training only by requiring a convex op-
timization at the M step (Riezler, 1998).
</footnote>
<bodyText confidence="0.999893692307692">
strained model. The results on English parsing
rather suggest that it is the similarity in degrees of
freedom (e.g., INHERIT with L=3x3x3 and PCFG-
LA with L=2x2) that produces comparable results.
Substantial gains were achieved by using
markovization and splitting only selected nonter-
minals. With these techniques we reach a pars-
ing accuracy similar to Matsuzaki et al. (2005),
but with an order of magnitude less parameters,
resulting in more efficient parsing. We hope to
get more wins in future by using more sophisti-
cated annealing techniques and log-linear model-
ing techniques.
</bodyText>
<sectionHeader confidence="0.999158" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999740666666667">
This paper is based upon work supported by the
National Science Foundation under Grant No.
0313193. We are grateful to Takuya Matsuzaki
for providing details about his implementation of
PCFG-LA, and to Noah Smith and the anonymous
reviewers for helpful comments.
</bodyText>
<sectionHeader confidence="0.998934" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99981962962963">
A. Berger, P. Brown, S. Pietra, V. Pietra, J. Lafferty,
H. Printz, and L. Ures. 1994. The CANDIDE sys-
tem for machine translation.
Ann Bies, Mark Ferguson, Karen Katz, Robert Mac-
Intyre, Victoria Tredinnick, Grace Kim, Mary Ann
Marcinkiewicz, and Britta Schasberger. 1995.
Bracketing guidelines for Treebank II style: Penn
Treebank project. Technical Report MS-CIS-95-06,
University of Pennsylvania, January.
John Blatz and Jason Eisner. 2006. Transforming pars-
ing algorithms and other weighted logic programs.
In Proceedings of the 11th Conference on Formal
Grammar.
Eugene Charniak. 1996. Tree-bank grammars. In Pro-
ceedings of the 13th National Conference on Artifi-
cial Intelligence.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Pro-
ceedings of the Fourteenth National Conference on
Artificial Intelligence, pages 598–603.
David Chiang and Daniel M. Bikel. 2002. Recov-
ering latent information in treebanks. In COLING
2002: The 17th International Conference on Com-
putational Linguistics, Taipei.
Michael John Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In ACL-96,
pages 184–191, Santa Cruz, CA. ACL.
</reference>
<page confidence="0.987102">
325
</page>
<reference confidence="0.999941147727273">
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Compu-
tational Linguistics and 8th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 16–23, Madrid. Association for
Computational Linguistics.
Jason Eisner, Eric Goldlust, and Noah A. Smith. 2005.
Compiling comp ling: Weighted dynamic program-
ming and the Dyna language. In Proceedings of
Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Language
Processing, pages 281–290, Vancouver, October.
Jason Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In COL-
ING96: Proceedings of the 16th International Con-
ference on Computational Linguistics, pages 340–
345, Copenhagen. Center for Sprogteknologi.
Gal Elidan and Nir Friedman. 2003. The information
bottleneck EM algorithm. In Proceedings of UAI.
Joshua Goodman. 1997. Probabilistic feature gram-
mars. In Proceedings of the 5th International Work-
shop on Parsing Technologies, pages 89–100, MIT,
Cambridge, MA, September.
L. Huang and D. Chiang. 2005. Parsing and k-best
algorithms. In Proc. ofIWPT.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613–632.
Ronald M. Kaplan and Joan Bresnan. 1982. Lexical-
functional grammar: A formal system for grammat-
ical representation. In Joan Bresnan, editor, The
Mental Representation of Grammatical Relations,
pages 173–281. MIT Press, Cambridge, MA.
Dan Klein and Christopher D. Manning. 2001. Dis-
tributional phrase structure induction. In The Fifth
Conference on Natural Language Learning.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Erhard Hinrichs and
Dan Roth, editors, Proceedings of the 41st Annual
Meeting of the Association for Computational Lin-
guistics, pages 423–430, Sapporo, Japan.
K. Lari and S. Young. 1990. The estimation of stochas-
tic context-free grammars using the inside-outside
algorithm. Computer Speech and Language, 4:35–
56.
David M. Magerman. 1995. Statistical Decision-Tree
models for parsing. In Proceedings of the 33rd An-
nual Meeting of the Association for Computational
Linguistics, pages 276–283, Cambridge, Mass.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn treebank. Computa-
tional Linguistics, 19(2):313–330, June.
Takuya Matsuzaki, Yusuke Miyao, and Junichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL’05),
University of Michigan.
Fernando Pereira and Yves Schabes. 1992. Inside-
Outside reestimation from partially bracketed cor-
pora. In Proceedings of the 30th Meeting of the As-
sociation for Computational Linguistics, pages 128–
135, Newark. University of Delaware.
Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of English words. In
Proceedings ofACL, Ohio State University.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press, Chicago.
Detlef Prescher. 2005. Head-driven PCFGs with
latent-head statistics. In Proceedings of the 9th
International Workshop on Parsing Technologies,
pages 115–124, Vancouver, BC, Canada, October.
Stefan Riezler. 1998. Statistical inference and prob-
abilistic modeling for constraint-based NLP. In
B. Schr¨oder, W. Lenders, W. Hess, and T. Portele,
editors, Computers, Linguistics, and Phonetics be-
tween Language and Speech: Proceedings of the 4th
Conference on Natural Language Processing (KON-
VENS’98), pages 111–124, Bonn. Lang.
Kenneth Rose. 1998. Deterministic annealing for clus-
tering, compression, classification, regression, and
related optimization problems. Proceedings of the
IEEE, 80:2210–2239, November.
Noah A. Smith and Jason Eisner. 2006. Annealing
structural bias in multilingual weighted grammar in-
duction. In Proceedings ofACL.
</reference>
<page confidence="0.999117">
326
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.697119">
<title confidence="0.998924">Better Informed Training of Latent Syntactic Features</title>
<author confidence="0.973328">Dreyer</author>
<affiliation confidence="0.996957">Department of Computer Science / Center for Language and Speech</affiliation>
<address confidence="0.8685625">Johns Hopkins 3400 North Charles Street, Baltimore, MD 21218</address>
<abstract confidence="0.998744125">We study unsupervised methods for learning refinements of the nonterminals in a treebank. Following Matsuzaki et al. (2005) and Prescher (2005), we may for split supervision into which behave differently. We first propose to learn a PCFG that adds such features to nonterminals in such a way that they respect patterns of linguistic feature passing: each node’s nonterminal features are either identical to, or independent of, those of its parent. This linguistic constraint reduces runtime and the number of parameters to be learned. However, it did not yield improvements when training on the Penn Treebank. An orthogonal strategy was more successful: to improve the performance of the EM learner by treebank preprocessing and by annealing methods that split nonterminals selectively. Using these methods, we can maintain high parsing accuracy while dramatically reducing the model size.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>P Brown</author>
<author>S Pietra</author>
<author>V Pietra</author>
<author>J Lafferty</author>
<author>H Printz</author>
<author>L Ures</author>
</authors>
<title>The CANDIDE system for machine translation.</title>
<date>1994</date>
<contexts>
<context position="16690" citStr="Berger et al., 1994" startWordPosition="2849" endWordPosition="2852">speedups for INHERIT2. Empirically, the hope is that the two features when learned with the EM algorithm will pick out different linguistic properties of the constituents in the treebank tree. 4 Annealing-Like Training Approaches Training latent PCFG models, like training most other unsupervised models, requires non-convex optimization. To find good parameter values, it is often helpful to train a simpler model first and use its parameters to derive a starting guess for the harder optimization problem. A well-known example is the training of the IBM models for statistical machine translation (Berger et al., 1994). In this vein, we did an experiment in which we gradually increased L during EM training of the PCFG-LA and INHERIT models. Whenever the training likelihood began to converge, we manually and globally increased L, simply doubling or tripling it (see “clone all” in Table 3 and Fig. 5). The probability of X[α] —* Y [β]Z[γ] under the new model was initialized to be proportional to the probability of X[α mod L] —* Y [β mod L]Z[γ mod L] (where L refers to the old L),5 times a random ”jitter” to break symmetry. In a second annealing experiment (“clone some”) we addressed a weakness of the PCFGLA an</context>
</contexts>
<marker>Berger, Brown, Pietra, Pietra, Lafferty, Printz, Ures, 1994</marker>
<rawString>A. Berger, P. Brown, S. Pietra, V. Pietra, J. Lafferty, H. Printz, and L. Ures. 1994. The CANDIDE system for machine translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Robert MacIntyre</author>
<author>Victoria Tredinnick</author>
<author>Grace Kim</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Britta Schasberger</author>
</authors>
<title>Bracketing guidelines for Treebank II style: Penn Treebank project.</title>
<date>1995</date>
<tech>Technical Report MS-CIS-95-06,</tech>
<institution>University of Pennsylvania,</institution>
<contexts>
<context position="1339" citStr="Bies et al., 1995" startWordPosition="204" endWordPosition="207">ndependent of, those of its parent. This linguistic constraint reduces runtime and the number of parameters to be learned. However, it did not yield improvements when training on the Penn Treebank. An orthogonal strategy was more successful: to improve the performance of the EM learner by treebank preprocessing and by annealing methods that split nonterminals selectively. Using these methods, we can maintain high parsing accuracy while dramatically reducing the model size. 1 Introduction Treebanks never contain enough information; thus PCFGs estimated straightforwardly from the Penn Treebank (Bies et al., 1995) work only moderately well (Charniak, 1996). To address this problem, researchers have used heuristics to add more information. Eisner (1996), Charniak (1997), Collins (1997), and many subsequent researchers1 annotated every node with lexical features passed up from its “head child,” in order to more precisely reflect the node’s “inside” contents. Charniak (1997) and Johnson (1998) annotated each node with its parent and grandparent nonterminals, to more precisely reflect its “outside” context. Collins (1996) split the sentence label S into two versions, representing sentences with and without</context>
</contexts>
<marker>Bies, Ferguson, Katz, MacIntyre, Tredinnick, Kim, Marcinkiewicz, Schasberger, 1995</marker>
<rawString>Ann Bies, Mark Ferguson, Karen Katz, Robert MacIntyre, Victoria Tredinnick, Grace Kim, Mary Ann Marcinkiewicz, and Britta Schasberger. 1995. Bracketing guidelines for Treebank II style: Penn Treebank project. Technical Report MS-CIS-95-06, University of Pennsylvania, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blatz</author>
<author>Jason Eisner</author>
</authors>
<title>Transforming parsing algorithms and other weighted logic programs.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference on Formal Grammar.</booktitle>
<contexts>
<context position="12481" citStr="Blatz and Eisner, 2006" startWordPosition="2096" endWordPosition="2099"> P(left |X, Y, Z) = (7) P(nonhead |X, Y, Z) otherwise � P(head |X, Y, Z) if Z heads X → Y Z P(right |X, Y, Z) = (8) P(nonhead |X, Y, Z) otherwise Table 2: Comparison of the PCFG-LA model with the INHERIT model proposed in this paper. “d.f.” stands for “degrees of freedom” (i.e., free parameters). The B terms are inside probabilities; to compute Viterbi parse probabilities instead, replace summation by maximization. Note the use of the intermediate quantity BX(i, j) to improve runtime complexity by moving some summations out of the inner loop; this is an instance of a “folding transformation” (Blatz and Eisner, 2006). Figure 3: Two passpatterns. Left: T2. The feature is passed to the head child (underlined). Right: T3. The feature is passed to both children. probabilities of tree T2 in Fig. 3 are calculated as follows, with Pann(1 |NP) being the probability of annotating an NP with feature 1 if it does not inherit its parent’s feature. The VP is boldfaced to indicate that it is the head child of this rule. P(T2) = P(ROOT → S[2]) ×P(S[2] → NP VP) ×P(pass to head |S → NP VP) ×Pann(1 |NP) × P(NP[1] →* He) ×P(VP[2] →* loves cookies) Tree T3 in Fig. 3 has the following probability: P(T3) = P(ROOT → S[2]) ×P(S[</context>
</contexts>
<marker>Blatz, Eisner, 2006</marker>
<rawString>John Blatz and Jason Eisner. 2006. Transforming parsing algorithms and other weighted logic programs. In Proceedings of the 11th Conference on Formal Grammar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Tree-bank grammars.</title>
<date>1996</date>
<booktitle>In Proceedings of the 13th National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="1382" citStr="Charniak, 1996" startWordPosition="212" endWordPosition="213">istic constraint reduces runtime and the number of parameters to be learned. However, it did not yield improvements when training on the Penn Treebank. An orthogonal strategy was more successful: to improve the performance of the EM learner by treebank preprocessing and by annealing methods that split nonterminals selectively. Using these methods, we can maintain high parsing accuracy while dramatically reducing the model size. 1 Introduction Treebanks never contain enough information; thus PCFGs estimated straightforwardly from the Penn Treebank (Bies et al., 1995) work only moderately well (Charniak, 1996). To address this problem, researchers have used heuristics to add more information. Eisner (1996), Charniak (1997), Collins (1997), and many subsequent researchers1 annotated every node with lexical features passed up from its “head child,” in order to more precisely reflect the node’s “inside” contents. Charniak (1997) and Johnson (1998) annotated each node with its parent and grandparent nonterminals, to more precisely reflect its “outside” context. Collins (1996) split the sentence label S into two versions, representing sentences with and without subjects. He 1Not to mention earlier non-P</context>
</contexts>
<marker>Charniak, 1996</marker>
<rawString>Eugene Charniak. 1996. Tree-bank grammars. In Proceedings of the 13th National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth National Conference on Artificial Intelligence,</booktitle>
<pages>598--603</pages>
<contexts>
<context position="1497" citStr="Charniak (1997)" startWordPosition="229" endWordPosition="230"> when training on the Penn Treebank. An orthogonal strategy was more successful: to improve the performance of the EM learner by treebank preprocessing and by annealing methods that split nonterminals selectively. Using these methods, we can maintain high parsing accuracy while dramatically reducing the model size. 1 Introduction Treebanks never contain enough information; thus PCFGs estimated straightforwardly from the Penn Treebank (Bies et al., 1995) work only moderately well (Charniak, 1996). To address this problem, researchers have used heuristics to add more information. Eisner (1996), Charniak (1997), Collins (1997), and many subsequent researchers1 annotated every node with lexical features passed up from its “head child,” in order to more precisely reflect the node’s “inside” contents. Charniak (1997) and Johnson (1998) annotated each node with its parent and grandparent nonterminals, to more precisely reflect its “outside” context. Collins (1996) split the sentence label S into two versions, representing sentences with and without subjects. He 1Not to mention earlier non-PCFG lexicalized statistical parsers, notably Magerman (1995) for the Penn Treebank. also modified the treebank to c</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Eugene Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In Proceedings of the Fourteenth National Conference on Artificial Intelligence, pages 598–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Daniel M Bikel</author>
</authors>
<title>Recovering latent information in treebanks.</title>
<date>2002</date>
<booktitle>In COLING 2002: The 17th International Conference on Computational Linguistics,</booktitle>
<location>Taipei.</location>
<contexts>
<context position="4472" citStr="Chiang and Bikel (2002)" startWordPosition="671" endWordPosition="674">ture work and conclusions in sections 6–7. 2Probabilistic context-free grammar with latent annotations. 317 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 317–326, Sydney, July 2006. c�2006 Association for Computational Linguistics Citation Observed data Hidden data Collins (1997) Treebank tree with head child an- No hidden data. Degenerate EM notated on each nonterminal case. Lari and Young (1990) Words Parse tree Pereira and Schabes (1992) Words and partial brackets Parse tree Klein and Manning (2001) Part-of-speech tags Parse tree Chiang and Bikel (2002) Treebank tree Head child on each nonterminal Matsuzaki et al. (2005) Treebank tree Integer feature on each nonterminal INHERIT model (this paper) Treebank tree and head child Integer feature on each nontermiheuristics nal Table 1: Observed and hidden data in PCFG grammar learning. 2 Partially supervised EM learning The parameters of a PCFG can be learned with or without supervision. In the supervised case, the complete tree is observed, and the rewrite rule probabilities can be estimated directly from the observed rule counts. In the unsupervised case, only the words are observed, and the lea</context>
</contexts>
<marker>Chiang, Bikel, 2002</marker>
<rawString>David Chiang and Daniel M. Bikel. 2002. Recovering latent information in treebanks. In COLING 2002: The 17th International Conference on Computational Linguistics, Taipei.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael John Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In ACL-96,</booktitle>
<pages>184--191</pages>
<publisher>ACL.</publisher>
<location>Santa Cruz, CA.</location>
<contexts>
<context position="1853" citStr="Collins (1996)" startWordPosition="283" endWordPosition="284">nough information; thus PCFGs estimated straightforwardly from the Penn Treebank (Bies et al., 1995) work only moderately well (Charniak, 1996). To address this problem, researchers have used heuristics to add more information. Eisner (1996), Charniak (1997), Collins (1997), and many subsequent researchers1 annotated every node with lexical features passed up from its “head child,” in order to more precisely reflect the node’s “inside” contents. Charniak (1997) and Johnson (1998) annotated each node with its parent and grandparent nonterminals, to more precisely reflect its “outside” context. Collins (1996) split the sentence label S into two versions, representing sentences with and without subjects. He 1Not to mention earlier non-PCFG lexicalized statistical parsers, notably Magerman (1995) for the Penn Treebank. also modified the treebank to contain different labels for standard and for base noun phrases. Klein and Manning (2003) identified nonterminals that could valuably be split into fine-grained ones using hand-written linguistic rules. Their unlexicalized parser combined several such heuristics with rule markovization and reached a performance similar to early lexicalized parsers. In all</context>
<context position="11153" citStr="Collins, 1996" startWordPosition="1834" endWordPosition="1835">n the example in Fig. 2, the tense feature (pres) is always passed to the head child (underlined). How the number feature (sg/pl) is passed depends on the rewrite rule: S —* NP VP passes it to both children, to enforce subject-verb agreement, while VP —* V NP only passes it to the head child, since the object NP is free not to agree with the verb. A feature grammar can incorporate such patterns of feature passing. We introduce additional parameters that define the probability of passing a feature to certain children. The head child of each node is given deterministically by the head rules of (Collins, 1996). Under the INHERIT model that we propose, the 319 Model Runtime and d.f. Simplified equation for inside probabilities (ignores unary rules) Matsuzaki test: O(n3L3) train: O(nL3) et al. (2005) d.f.: L3R3 + BX[α](i, k) = P(X[α] → Y [β] Z[γ]) (4) L2R2 +LR1 −LN � Y,β,Z,γ,j ×BY [β](i, j) × BZ[γ](j, k) INHERIT test: O(n3L) BX[α](i, k) = P(X[α] → Y Z) (5) model train: O(nL) � (this paper) d.f.: L(R3 + R2 + Y,Z,j R1) + 3R3 − N P(neither |X, Y, Z) × BY (i, j) × BZ(j, k)) 0 1 �+ P(left |X, Y, Z) × BY [α](i, j) × BZ(j, k)) � + P(right |X, Y, Z) × BY (i, j) × BZ[α](j, k)) × � � + P(both |X, Y, Z) × BY [α</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>Michael John Collins. 1996. A new statistical parser based on bigram lexical dependencies. In ACL-96, pages 184–191, Santa Cruz, CA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>16--23</pages>
<institution>Madrid. Association for Computational Linguistics.</institution>
<contexts>
<context position="1513" citStr="Collins (1997)" startWordPosition="231" endWordPosition="232"> the Penn Treebank. An orthogonal strategy was more successful: to improve the performance of the EM learner by treebank preprocessing and by annealing methods that split nonterminals selectively. Using these methods, we can maintain high parsing accuracy while dramatically reducing the model size. 1 Introduction Treebanks never contain enough information; thus PCFGs estimated straightforwardly from the Penn Treebank (Bies et al., 1995) work only moderately well (Charniak, 1996). To address this problem, researchers have used heuristics to add more information. Eisner (1996), Charniak (1997), Collins (1997), and many subsequent researchers1 annotated every node with lexical features passed up from its “head child,” in order to more precisely reflect the node’s “inside” contents. Charniak (1997) and Johnson (1998) annotated each node with its parent and grandparent nonterminals, to more precisely reflect its “outside” context. Collins (1996) split the sentence label S into two versions, representing sentences with and without subjects. He 1Not to mention earlier non-PCFG lexicalized statistical parsers, notably Magerman (1995) for the Penn Treebank. also modified the treebank to contain different</context>
<context position="4190" citStr="Collins (1997)" startWordPosition="628" endWordPosition="629">roduce new models that use specific agreement patterns to propagate features through the tree. Section 4 describes annealing-like procedures for training latent-annotation models. Section 5 describes the motivation and results of our experiments. We finish by discussing future work and conclusions in sections 6–7. 2Probabilistic context-free grammar with latent annotations. 317 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 317–326, Sydney, July 2006. c�2006 Association for Computational Linguistics Citation Observed data Hidden data Collins (1997) Treebank tree with head child an- No hidden data. Degenerate EM notated on each nonterminal case. Lari and Young (1990) Words Parse tree Pereira and Schabes (1992) Words and partial brackets Parse tree Klein and Manning (2001) Part-of-speech tags Parse tree Chiang and Bikel (2002) Treebank tree Head child on each nonterminal Matsuzaki et al. (2005) Treebank tree Integer feature on each nonterminal INHERIT model (this paper) Treebank tree and head child Integer feature on each nontermiheuristics nal Table 1: Observed and hidden data in PCFG grammar learning. 2 Partially supervised EM learning </context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics, pages 16–23, Madrid. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Eric Goldlust</author>
<author>Noah A Smith</author>
</authors>
<title>Compiling comp ling: Weighted dynamic programming and the Dyna language.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>281--290</pages>
<location>Vancouver,</location>
<marker>Eisner, Goldlust, Smith, 2005</marker>
<rawString>Jason Eisner, Eric Goldlust, and Noah A. Smith. 2005. Compiling comp ling: Weighted dynamic programming and the Dyna language. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 281–290, Vancouver, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In COLING96: Proceedings of the 16th International Conference on Computational Linguistics,</booktitle>
<pages>340--345</pages>
<institution>Copenhagen. Center for Sprogteknologi.</institution>
<contexts>
<context position="1480" citStr="Eisner (1996)" startWordPosition="227" endWordPosition="228">ld improvements when training on the Penn Treebank. An orthogonal strategy was more successful: to improve the performance of the EM learner by treebank preprocessing and by annealing methods that split nonterminals selectively. Using these methods, we can maintain high parsing accuracy while dramatically reducing the model size. 1 Introduction Treebanks never contain enough information; thus PCFGs estimated straightforwardly from the Penn Treebank (Bies et al., 1995) work only moderately well (Charniak, 1996). To address this problem, researchers have used heuristics to add more information. Eisner (1996), Charniak (1997), Collins (1997), and many subsequent researchers1 annotated every node with lexical features passed up from its “head child,” in order to more precisely reflect the node’s “inside” contents. Charniak (1997) and Johnson (1998) annotated each node with its parent and grandparent nonterminals, to more precisely reflect its “outside” context. Collins (1996) split the sentence label S into two versions, representing sentences with and without subjects. He 1Not to mention earlier non-PCFG lexicalized statistical parsers, notably Magerman (1995) for the Penn Treebank. also modified </context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In COLING96: Proceedings of the 16th International Conference on Computational Linguistics, pages 340– 345, Copenhagen. Center for Sprogteknologi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gal Elidan</author>
<author>Nir Friedman</author>
</authors>
<title>The information bottleneck EM algorithm.</title>
<date>2003</date>
<booktitle>In Proceedings of UAI.</booktitle>
<contexts>
<context position="31563" citStr="Elidan and Friedman, 2003" startWordPosition="5358" endWordPosition="5361">ending in -s. Spelling features that are predictable from string context are important clues to the existence and behavior of the hidden annotations we wish to induce. A final remark is that “inheritance” does not necessarily have to mean that α = Q. It is enough that α and Q should have high mutual information, so that one can be predicted from the other; they do not actually have to be represented by the same integer. More broadly, we might like α to have high mutual information with the pair (Q, -y). One might try using this sort of intuition directly in an unsupervised learning procedure (Elidan and Friedman, 2003). 7 Conclusions We have discussed “informed” techniques for inducing latent syntactic features. Our INHERIT model tries to constrain the way in which features are passed through the tree. The motivation for this approach is twofold: First, we wanted to capture the linguistic insight that features follow certain patterns in propagating through the tree. Second, we wanted to make it statistically feasible and computationally tractable to increase L to higher values than in the PCFG-LA model. The hope was that the learning process could then make finer distinctions and learn more fine-grained inf</context>
</contexts>
<marker>Elidan, Friedman, 2003</marker>
<rawString>Gal Elidan and Nir Friedman. 2003. The information bottleneck EM algorithm. In Proceedings of UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Probabilistic feature grammars.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th International Workshop on Parsing Technologies,</booktitle>
<pages>89--100</pages>
<location>MIT, Cambridge, MA,</location>
<contexts>
<context position="8749" citStr="Goodman (1997)" startWordPosition="1406" endWordPosition="1407"> the resulting PCFG-LA model has L3 x R3 such rules: X[1] —* Y [1] Z[1], X[1] —* Y [1] Z[2], X[1] —* Y [2] Z[1], ..., X[L] —* Y [L] Z[L]. Similarly, if the backbone grammar has R2 rules of the form X —* Y the PCFG-LA model has L2 x R2 such rules.3 The number of R1 terminal rules X —* w is just multiplied by L. The PCFG-LA has as many parameters to learn as rules: one probability per rule. However, not all these parameters are free, as there are L x N sum-to-one constraints, where N is the number of backbone nonterminals. Thus we have L3R3 + L2R2 + LR1 − LN (3) degrees of freedom. We note that Goodman (1997) mentioned possible ways to factor the probability 1, making independence assumptions in order to reduce the number of parameters. Runtime. Assuming there are no unary rule cycles in the backbone grammar, bottom-up chart parsing of a length-n sentence at test time takes time proportional to n3L3R3 + n2L2R2 + nLR1, by attempting to apply each rule everywhere in the sentence. (The dominating term comes from equation (4) of Table 2: we must loop over all n3 triples i, j, k and all R3 backbone rules X —* Y Z and all 3We use unary rules of this form (e.g. the Treebank’s S � NP) in our reimplementat</context>
</contexts>
<marker>Goodman, 1997</marker>
<rawString>Joshua Goodman. 1997. Probabilistic feature grammars. In Proceedings of the 5th International Workshop on Parsing Technologies, pages 89–100, MIT, Cambridge, MA, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>D Chiang</author>
</authors>
<title>Parsing and k-best algorithms.</title>
<date>2005</date>
<booktitle>In Proc. ofIWPT.</booktitle>
<contexts>
<context position="23490" citStr="Huang and Chiang (2005)" startWordPosition="3978" endWordPosition="3981">nts. This total can be efficiently computed by the constrained version of the inside algorithm in Table 2. How do we obtain the unannotated parse whose total probability is greatest? It does not suffice to find the single best annotated parse and then strip off the annotations. Matsuzaki et al. (2005) note that the best annotated parse is in fact NP-hard to find. We use their reranking approximation. A 1000-best list for each sentence in the decoding set was created by parsing with our markovized unannotated grammar and extracting the 1000 best parses using the k-best algorithm 3 described in Huang and Chiang (2005). Then we chose the most probable of these 1000 unannotated parses under our PCFG, first finding the total probability of each by using the the constrained inside algorithm as explained above.8 5.3 Results and Discussion Table 3 summarizes the results on development and test data. 9 Figure 5 shows the training loglikelihoods. First, markovization of the Treebank leads to 8For the first set of experiments, in which the models were trained on a simple non-markovized grammar, the 1000-best trees had to be “demarkovized” before our PCFG was able to rescore them. 9All results are reported on senten</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>L. Huang and D. Chiang. 2005. Parsing and k-best algorithms. In Proc. ofIWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFG models of linguistic tree representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="1723" citStr="Johnson (1998)" startWordPosition="264" endWordPosition="265">ethods, we can maintain high parsing accuracy while dramatically reducing the model size. 1 Introduction Treebanks never contain enough information; thus PCFGs estimated straightforwardly from the Penn Treebank (Bies et al., 1995) work only moderately well (Charniak, 1996). To address this problem, researchers have used heuristics to add more information. Eisner (1996), Charniak (1997), Collins (1997), and many subsequent researchers1 annotated every node with lexical features passed up from its “head child,” in order to more precisely reflect the node’s “inside” contents. Charniak (1997) and Johnson (1998) annotated each node with its parent and grandparent nonterminals, to more precisely reflect its “outside” context. Collins (1996) split the sentence label S into two versions, representing sentences with and without subjects. He 1Not to mention earlier non-PCFG lexicalized statistical parsers, notably Magerman (1995) for the Penn Treebank. also modified the treebank to contain different labels for standard and for base noun phrases. Klein and Manning (2003) identified nonterminals that could valuably be split into fine-grained ones using hand-written linguistic rules. Their unlexicalized pars</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998. PCFG models of linguistic tree representations. Computational Linguistics, 24(4):613–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
<author>Joan Bresnan</author>
</authors>
<title>Lexicalfunctional grammar: A formal system for grammatical representation.</title>
<date>1982</date>
<booktitle>The Mental Representation of Grammatical Relations,</booktitle>
<pages>173--281</pages>
<editor>In Joan Bresnan, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="10307" citStr="Kaplan and Bresnan (1982)" startWordPosition="1682" endWordPosition="1685">only the single consistent backbone rule for each one). This takes time 0(nL3), as does the inside-outside version we actually use to collect expected PCFG-LA rule counts for EM training. We now introduce a model that is smaller, and has a lower runtime complexity, because it adheres to specified ways of propagating features through the tree. 3.2 Feature Passing: The INHERIT Model Many linguistic theories assume that features get passed from the mother node to their children or some of their children. In many cases it is the head child that gets passed its feature value from its mother (e.g., Kaplan and Bresnan (1982), Pollard and Sag (1994)). In some cases the feature is passed to both the head and the non-head child, or perhaps even to the non-head alone. Figure 2: Features are passed to different children at different positions in the tree. In the example in Fig. 2, the tense feature (pres) is always passed to the head child (underlined). How the number feature (sg/pl) is passed depends on the rewrite rule: S —* NP VP passes it to both children, to enforce subject-verb agreement, while VP —* V NP only passes it to the head child, since the object NP is free not to agree with the verb. A feature grammar </context>
</contexts>
<marker>Kaplan, Bresnan, 1982</marker>
<rawString>Ronald M. Kaplan and Joan Bresnan. 1982. Lexicalfunctional grammar: A formal system for grammatical representation. In Joan Bresnan, editor, The Mental Representation of Grammatical Relations, pages 173–281. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Distributional phrase structure induction.</title>
<date>2001</date>
<booktitle>In The Fifth Conference on Natural Language Learning.</booktitle>
<contexts>
<context position="4417" citStr="Klein and Manning (2001)" startWordPosition="663" endWordPosition="666">d results of our experiments. We finish by discussing future work and conclusions in sections 6–7. 2Probabilistic context-free grammar with latent annotations. 317 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 317–326, Sydney, July 2006. c�2006 Association for Computational Linguistics Citation Observed data Hidden data Collins (1997) Treebank tree with head child an- No hidden data. Degenerate EM notated on each nonterminal case. Lari and Young (1990) Words Parse tree Pereira and Schabes (1992) Words and partial brackets Parse tree Klein and Manning (2001) Part-of-speech tags Parse tree Chiang and Bikel (2002) Treebank tree Head child on each nonterminal Matsuzaki et al. (2005) Treebank tree Integer feature on each nonterminal INHERIT model (this paper) Treebank tree and head child Integer feature on each nontermiheuristics nal Table 1: Observed and hidden data in PCFG grammar learning. 2 Partially supervised EM learning The parameters of a PCFG can be learned with or without supervision. In the supervised case, the complete tree is observed, and the rewrite rule probabilities can be estimated directly from the observed rule counts. In the unsu</context>
</contexts>
<marker>Klein, Manning, 2001</marker>
<rawString>Dan Klein and Christopher D. Manning. 2001. Distributional phrase structure induction. In The Fifth Conference on Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<editor>In Erhard Hinrichs and Dan Roth, editors,</editor>
<location>Sapporo, Japan.</location>
<contexts>
<context position="2185" citStr="Klein and Manning (2003)" startWordPosition="332" endWordPosition="335">ry node with lexical features passed up from its “head child,” in order to more precisely reflect the node’s “inside” contents. Charniak (1997) and Johnson (1998) annotated each node with its parent and grandparent nonterminals, to more precisely reflect its “outside” context. Collins (1996) split the sentence label S into two versions, representing sentences with and without subjects. He 1Not to mention earlier non-PCFG lexicalized statistical parsers, notably Magerman (1995) for the Penn Treebank. also modified the treebank to contain different labels for standard and for base noun phrases. Klein and Manning (2003) identified nonterminals that could valuably be split into fine-grained ones using hand-written linguistic rules. Their unlexicalized parser combined several such heuristics with rule markovization and reached a performance similar to early lexicalized parsers. In all these cases, choosing which nonterminals to split, and how, was a matter of art. Ideally such splits would be learned automatically from the given treebank itself. This would be less costly and more portable to treebanks for new domains and languages. One might also hope that the automatically learned splits would be more effecti</context>
<context position="6762" citStr="Klein and Manning, 2003" startWordPosition="1031" endWordPosition="1034"> sentences with and without subjects, Matsuzaki et al. (2005) split S into S[1], S[2], ... , S[L] where L is a predefined number—but they do it automatically and systematically, and not only for S but for every nonterminal. Their partially supervised learning procedure observes trees that are fully bracketed and fully labeled, except for the integer subscript used to annotate each node. After automatically inducing the annotations with EM, their resulting parser performs just as well as one learned from a treebank whose nonterminals were manually refined through linguistic and error analysis (Klein and Manning, 2003). In Matsuzaki’s PCFG-LA model, rewrite rules take the form X[α] —* Y [Q] Z[-y] (1) in the binary case, and X[α] —* w (2) in the lexical case. The probability of a tree consisting of rules r1, r2,... is given by the probability of its root symbol times the conditional probabilities of the rules. The annotated tree T1 in Fig. 1, for example, has the following probability: P(T1) = P(ROOT S[2]) xP(S[2] NP[1] VP[3]) xP(NP[1] —*� He) xP(VP[3] —*� loves cookies) where, to simplify the notation, we use P(X —* Y Z) to denote the conditional probability P(Y Z |X) that a given node with label X will hav</context>
<context position="17522" citStr="Klein and Manning, 2003" startWordPosition="2997" endWordPosition="3000">sed L, simply doubling or tripling it (see “clone all” in Table 3 and Fig. 5). The probability of X[α] —* Y [β]Z[γ] under the new model was initialized to be proportional to the probability of X[α mod L] —* Y [β mod L]Z[γ mod L] (where L refers to the old L),5 times a random ”jitter” to break symmetry. In a second annealing experiment (“clone some”) we addressed a weakness of the PCFGLA and INHERIT models: They give every nonterminal the same number of latent annotations. It would seem that different coarse-grained nonterminals in the original Penn Treebank have different degrees of impurity (Klein and Manning, 2003). There are linguistically many kinds of NP, which are differentially selected for by various contexts and hence are worth distinguishing. By contrast, -LRB- is almost always realized as a left parenthesis and may not need further refinement. Our “clone some” annealing starts by training a model with L=2 to convergence. Then, instead of cloning all nonterminals as in the previous annealing experiments, we clone only those that have seemed to benefit most from their previous refinement. This benefit is measured by the Jensen-Shannon divergence of the two distributions P(X[0] —* · · · ) and P(X[</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Erhard Hinrichs and Dan Roth, editors, Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 423–430, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lari</author>
<author>S Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the inside-outside algorithm.</title>
<date>1990</date>
<journal>Computer Speech and Language,</journal>
<volume>4</volume>
<pages>56</pages>
<contexts>
<context position="4310" citStr="Lari and Young (1990)" startWordPosition="646" endWordPosition="649">s annealing-like procedures for training latent-annotation models. Section 5 describes the motivation and results of our experiments. We finish by discussing future work and conclusions in sections 6–7. 2Probabilistic context-free grammar with latent annotations. 317 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 317–326, Sydney, July 2006. c�2006 Association for Computational Linguistics Citation Observed data Hidden data Collins (1997) Treebank tree with head child an- No hidden data. Degenerate EM notated on each nonterminal case. Lari and Young (1990) Words Parse tree Pereira and Schabes (1992) Words and partial brackets Parse tree Klein and Manning (2001) Part-of-speech tags Parse tree Chiang and Bikel (2002) Treebank tree Head child on each nonterminal Matsuzaki et al. (2005) Treebank tree Integer feature on each nonterminal INHERIT model (this paper) Treebank tree and head child Integer feature on each nontermiheuristics nal Table 1: Observed and hidden data in PCFG grammar learning. 2 Partially supervised EM learning The parameters of a PCFG can be learned with or without supervision. In the supervised case, the complete tree is observ</context>
</contexts>
<marker>Lari, Young, 1990</marker>
<rawString>K. Lari and S. Young. 1990. The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language, 4:35– 56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
</authors>
<title>Statistical Decision-Tree models for parsing.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>276--283</pages>
<location>Cambridge, Mass.</location>
<contexts>
<context position="2042" citStr="Magerman (1995)" startWordPosition="310" endWordPosition="311">sed heuristics to add more information. Eisner (1996), Charniak (1997), Collins (1997), and many subsequent researchers1 annotated every node with lexical features passed up from its “head child,” in order to more precisely reflect the node’s “inside” contents. Charniak (1997) and Johnson (1998) annotated each node with its parent and grandparent nonterminals, to more precisely reflect its “outside” context. Collins (1996) split the sentence label S into two versions, representing sentences with and without subjects. He 1Not to mention earlier non-PCFG lexicalized statistical parsers, notably Magerman (1995) for the Penn Treebank. also modified the treebank to contain different labels for standard and for base noun phrases. Klein and Manning (2003) identified nonterminals that could valuably be split into fine-grained ones using hand-written linguistic rules. Their unlexicalized parser combined several such heuristics with rule markovization and reached a performance similar to early lexicalized parsers. In all these cases, choosing which nonterminals to split, and how, was a matter of art. Ideally such splits would be learned automatically from the given treebank itself. This would be less costl</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>David M. Magerman. 1995. Statistical Decision-Tree models for parsing. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 276–283, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="19925" citStr="Marcus et al., 1993" startWordPosition="3403" endWordPosition="3406">ndantly representing just one effective cluster. As the entropy is permitted to decrease, some of the cluster centroids find it worthwhile to drift apart.6 In future work, we would like to apply this technique to split nonterminals gradually, by initially requiring high-entropy parse forests on the training data an “poorman’sversion” d slowly relaxing this constraint. We ran several experiments to compare the with the PCFG-LA model and look into the effect of different Treebank preprocessing and the annealing-like procedures. We used sections of the Penn Treebank 2 Wall Street Journal corpus (Marcus et al., 1993) for training, section 22 as development set and section 23 for testing. Following Matsuzaki et al. (2005), words occurring fewer than 4 times in the training corpus were replaced by unknown-word symbols that encoded certain suffix and capitalization information. All experiments used simple add-lambda smoothing (A INHERIT 2–20 =0.1) during the reestimation step (M step) of training. 322 Figure 4: Horizontal and vertical markovization all but the first experiments, we also enriched the nonterminals with order-1 horizontal and order-2 vertical markovization (Klein and Manning, Figure 4 shows wha</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn treebank. Computational Linguistics, 19(2):313–330, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Junichi Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<institution>University of Michigan.</institution>
<contexts>
<context position="2812" citStr="Matsuzaki et al. (2005)" startWordPosition="428" endWordPosition="431">ntified nonterminals that could valuably be split into fine-grained ones using hand-written linguistic rules. Their unlexicalized parser combined several such heuristics with rule markovization and reached a performance similar to early lexicalized parsers. In all these cases, choosing which nonterminals to split, and how, was a matter of art. Ideally such splits would be learned automatically from the given treebank itself. This would be less costly and more portable to treebanks for new domains and languages. One might also hope that the automatically learned splits would be more effective. Matsuzaki et al. (2005) introduced a model for such learning: PCFG-LA.2 They used EM to induce fine-grained versions of a given treebank’s nonterminals and rules. We present models that similarly learn to propagate fine-grained features through the tree, but only in certain linguistically motivated ways. Our models therefore allocate a supply of free parameters differently, allowing more fine-grained nonterminals but less finegrained control over the probabilities of rewriting them. We also present simple methods for deciding selectively (during training) which nonterminals to split and how. Section 2 describes prev</context>
<context position="4541" citStr="Matsuzaki et al. (2005)" startWordPosition="682" endWordPosition="685">e grammar with latent annotations. 317 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 317–326, Sydney, July 2006. c�2006 Association for Computational Linguistics Citation Observed data Hidden data Collins (1997) Treebank tree with head child an- No hidden data. Degenerate EM notated on each nonterminal case. Lari and Young (1990) Words Parse tree Pereira and Schabes (1992) Words and partial brackets Parse tree Klein and Manning (2001) Part-of-speech tags Parse tree Chiang and Bikel (2002) Treebank tree Head child on each nonterminal Matsuzaki et al. (2005) Treebank tree Integer feature on each nonterminal INHERIT model (this paper) Treebank tree and head child Integer feature on each nontermiheuristics nal Table 1: Observed and hidden data in PCFG grammar learning. 2 Partially supervised EM learning The parameters of a PCFG can be learned with or without supervision. In the supervised case, the complete tree is observed, and the rewrite rule probabilities can be estimated directly from the observed rule counts. In the unsupervised case, only the words are observed, and the learning method must induce the whole structure above them. (See Table 1</context>
<context position="5889" citStr="Matsuzaki et al. (2005)" startWordPosition="899" endWordPosition="902"> be induced. Pereira and Schabes (1992) estimate PCFG parameters from partially bracketed sentences, using the inside-outside algorithm to induce the missing brackets and the missing node labels. Some authors define a complete tree as one that specifies not only a label but also a “head child” for each node. Chiang and Bikel (2002) induces the missing head-child information; Prescher (2005) induces both the head-child information and the latent annotations we will now discuss. 3 Feature Grammars 3.1 The PCFG-LA Model Staying in the partially supervised paradigm, the PCFG-LA model described in Matsuzaki et al. (2005) observe whole treebank trees, but learn an “annotation” on each nonterminal token—an unspecified and uninterpreted integer that distinguishes otherwise identical nonterminals. Just as Collins manually split the S nonterminal label into S and SG for sentences with and without subjects, Matsuzaki et al. (2005) split S into S[1], S[2], ... , S[L] where L is a predefined number—but they do it automatically and systematically, and not only for S but for every nonterminal. Their partially supervised learning procedure observes trees that are fully bracketed and fully labeled, except for the integer</context>
<context position="20031" citStr="Matsuzaki et al. (2005)" startWordPosition="3420" endWordPosition="3423">luster centroids find it worthwhile to drift apart.6 In future work, we would like to apply this technique to split nonterminals gradually, by initially requiring high-entropy parse forests on the training data an “poorman’sversion” d slowly relaxing this constraint. We ran several experiments to compare the with the PCFG-LA model and look into the effect of different Treebank preprocessing and the annealing-like procedures. We used sections of the Penn Treebank 2 Wall Street Journal corpus (Marcus et al., 1993) for training, section 22 as development set and section 23 for testing. Following Matsuzaki et al. (2005), words occurring fewer than 4 times in the training corpus were replaced by unknown-word symbols that encoded certain suffix and capitalization information. All experiments used simple add-lambda smoothing (A INHERIT 2–20 =0.1) during the reestimation step (M step) of training. 322 Figure 4: Horizontal and vertical markovization all but the first experiments, we also enriched the nonterminals with order-1 horizontal and order-2 vertical markovization (Klein and Manning, Figure 4 shows what amultiplechild structure A B H C D looks like after binarization and markovization. The binaEach auxilia</context>
<context position="21605" citStr="Matsuzaki et al. (2005)" startWordPosition="3673" endWordPosition="3676">he head child. to break symmetry [.9999,1.0001] . ing the rules at most binary , this preprocessing also helpfully enriched the backbone nonterminals. For practice, each very large group of centroids (effective cluster) is repre 6In sented by just two, until such time as those two drift apart to represent separate effective clusters—then each is cloned. dard Treebank parses. 5 Experiments 5.1 Setup Binarization and Markovization. Before extracting the backbone PCFG and running the constrained inside-outside (EM) training algorithm, we preprocessed the Treebank using center-parent binarization Matsuzaki et al. (2005). Besides makInitialization. The backbone PCFG grammar was read off the altered Treebank, and the initial annotated grammar was created by creating several versions of every rewrite rule. The probabilities of these newly created rules are uniform and proportional to the original rule, multiplied by a random epsilon factor uniformly sampled from 5.2 Decoding To test the PCFG learned by a given method, we attempted to recover the unannotated parse of each sentence in the development set. We then scored these parses by debinarizing or demarkovizing them, then measuring their precision and recall </context>
<context position="23169" citStr="Matsuzaki et al. (2005)" startWordPosition="3925" endWordPosition="3928">clone all” method. We increased L after iteration 50 and, for the INHERIT model, iteration 110. The downward spikes in the two annealed cases are due to perturbation of the model parameters (footnote 5). An unannotated parse’s probability is the total probability, under our learned PCFG, of all of its annotated refinements. This total can be efficiently computed by the constrained version of the inside algorithm in Table 2. How do we obtain the unannotated parse whose total probability is greatest? It does not suffice to find the single best annotated parse and then strip off the annotations. Matsuzaki et al. (2005) note that the best annotated parse is in fact NP-hard to find. We use their reranking approximation. A 1000-best list for each sentence in the decoding set was created by parsing with our markovized unannotated grammar and extracting the 1000 best parses using the k-best algorithm 3 described in Huang and Chiang (2005). Then we chose the most probable of these 1000 unannotated parses under our PCFG, first finding the total probability of each by using the the constrained inside algorithm as explained above.8 5.3 Results and Discussion Table 3 summarizes the results on development and test dat</context>
<context position="27466" citStr="Matsuzaki et al. (2005)" startWordPosition="4646" endWordPosition="4649">5.19 85.81 9 440,273 86.16 85.12 85.64 4 1,120,232 87.09 85.71 86.39 26 1,188,035 86.55 85.55 86.05 Clone all 2 178,264 85.70 84.37 85.03 3 176,357 85.04 83.60 84.31 3x3 440,273 85.99 84.88 85.43 2x2 1,120,232 87.06 85.49 86.27 3x3x3 1,232,021 86.65 85.70 86.17 Cl.some 2 178,264 85.70 84.37 85.03 2 132,371 83.85 82.23 83.03 &lt;2x2 789,279 87.17 85.71 86.43 &lt;2x2 203,673 85.49 84.45 84.97 &lt;2x2x2 314,999 85.57 84.60 85.08 Table 3: Results on the development set: labeled precision (LP), labeled recall (LR), and their harmonic mean (F1). “Basic” models are trained on a non-markovized treebank (as in Matsuzaki et al. (2005)); all others are trained on a markovized treebank. The best model (PCFG-LA with “clone some” annealing, F1=86.43) has also been decoded on the final test set, reaching P/R=86.94/85.40 (F1=86.17). We used both models to assign the mostprobable annotations to the gold parses of the development set. Under the INHERIT model, NP[0] vs. NP[1] constituents were 21% plural vs. 41% plural. Under PCFG-LA this effect was weaker (30% vs. 39%), although it was significant in both (Fisher’s exact test, p &lt; 0.001). Strikingly, under the INHERIT model, the NP’s were 10 times more likely to pass this feature </context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Junichi Tsujii. 2005. Probabilistic CFG with latent annotations. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), University of Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Yves Schabes</author>
</authors>
<title>InsideOutside reestimation from partially bracketed corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th Meeting of the Association for Computational Linguistics,</booktitle>
<pages>128--135</pages>
<institution>Newark. University of Delaware.</institution>
<contexts>
<context position="4354" citStr="Pereira and Schabes (1992)" startWordPosition="653" endWordPosition="656">g latent-annotation models. Section 5 describes the motivation and results of our experiments. We finish by discussing future work and conclusions in sections 6–7. 2Probabilistic context-free grammar with latent annotations. 317 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 317–326, Sydney, July 2006. c�2006 Association for Computational Linguistics Citation Observed data Hidden data Collins (1997) Treebank tree with head child an- No hidden data. Degenerate EM notated on each nonterminal case. Lari and Young (1990) Words Parse tree Pereira and Schabes (1992) Words and partial brackets Parse tree Klein and Manning (2001) Part-of-speech tags Parse tree Chiang and Bikel (2002) Treebank tree Head child on each nonterminal Matsuzaki et al. (2005) Treebank tree Integer feature on each nonterminal INHERIT model (this paper) Treebank tree and head child Integer feature on each nontermiheuristics nal Table 1: Observed and hidden data in PCFG grammar learning. 2 Partially supervised EM learning The parameters of a PCFG can be learned with or without supervision. In the supervised case, the complete tree is observed, and the rewrite rule probabilities can b</context>
</contexts>
<marker>Pereira, Schabes, 1992</marker>
<rawString>Fernando Pereira and Yves Schabes. 1992. InsideOutside reestimation from partially bracketed corpora. In Proceedings of the 30th Meeting of the Association for Computational Linguistics, pages 128– 135, Newark. University of Delaware.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Naftali Tishby</author>
<author>Lillian Lee</author>
</authors>
<title>Distributional clustering of English words.</title>
<date>1993</date>
<booktitle>In Proceedings ofACL,</booktitle>
<institution>Ohio State University.</institution>
<contexts>
<context position="18930" citStr="Pereira et al., 1993" startWordPosition="3245" endWordPosition="3248">ammar unchanged, we should have scaled down the probabilities of such rules by 1/4, 1/2, and 1 respectively. Instead, we simply scaled them all down by the same proportion. While this temporarily changes the balance of probability among the three kinds of rules, EM immediately corrects this balance on the next training iteration to match the observed balance on the treebank trees—hence the one-iteration downtick in Figure 5). 321 Jensen-Shannon divergence is defined as D(q,r) = 2 (D (q q2r) + D (r ||q + r)/ 2 These experiments are a kind of of the deterministic annealing clustering algorithm (Pereira et al., 1993; Rose, 1998), which gradually increases the number of clusters during the clustering process. In deterministic annealing, one starts in principle with a very large number of clusters, but maximizes likelihood only under a constraint that the joint distribution p(point, cluster) must have very high entropy. This drives all of the cluster centroids to coincide exactly, redundantly representing just one effective cluster. As the entropy is permitted to decrease, some of the cluster centroids find it worthwhile to drift apart.6 In future work, we would like to apply this technique to split nonter</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of English words. In Proceedings ofACL, Ohio State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="10331" citStr="Pollard and Sag (1994)" startWordPosition="1686" endWordPosition="1690">backbone rule for each one). This takes time 0(nL3), as does the inside-outside version we actually use to collect expected PCFG-LA rule counts for EM training. We now introduce a model that is smaller, and has a lower runtime complexity, because it adheres to specified ways of propagating features through the tree. 3.2 Feature Passing: The INHERIT Model Many linguistic theories assume that features get passed from the mother node to their children or some of their children. In many cases it is the head child that gets passed its feature value from its mother (e.g., Kaplan and Bresnan (1982), Pollard and Sag (1994)). In some cases the feature is passed to both the head and the non-head child, or perhaps even to the non-head alone. Figure 2: Features are passed to different children at different positions in the tree. In the example in Fig. 2, the tense feature (pres) is always passed to the head child (underlined). How the number feature (sg/pl) is passed depends on the rewrite rule: S —* NP VP passes it to both children, to enforce subject-verb agreement, while VP —* V NP only passes it to the head child, since the object NP is free not to agree with the verb. A feature grammar can incorporate such pat</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Detlef Prescher</author>
</authors>
<title>Head-driven PCFGs with latent-head statistics.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th International Workshop on Parsing Technologies,</booktitle>
<pages>115--124</pages>
<location>Vancouver, BC, Canada,</location>
<contexts>
<context position="5659" citStr="Prescher (2005)" startWordPosition="866" endWordPosition="867">ds are observed, and the learning method must induce the whole structure above them. (See Table 1.) In the partially supervised case we will consider, some part of the tree is observed, and the remaining information has to be induced. Pereira and Schabes (1992) estimate PCFG parameters from partially bracketed sentences, using the inside-outside algorithm to induce the missing brackets and the missing node labels. Some authors define a complete tree as one that specifies not only a label but also a “head child” for each node. Chiang and Bikel (2002) induces the missing head-child information; Prescher (2005) induces both the head-child information and the latent annotations we will now discuss. 3 Feature Grammars 3.1 The PCFG-LA Model Staying in the partially supervised paradigm, the PCFG-LA model described in Matsuzaki et al. (2005) observe whole treebank trees, but learn an “annotation” on each nonterminal token—an unspecified and uninterpreted integer that distinguishes otherwise identical nonterminals. Just as Collins manually split the S nonterminal label into S and SG for sentences with and without subjects, Matsuzaki et al. (2005) split S into S[1], S[2], ... , S[L] where L is a predefined</context>
</contexts>
<marker>Prescher, 2005</marker>
<rawString>Detlef Prescher. 2005. Head-driven PCFGs with latent-head statistics. In Proceedings of the 9th International Workshop on Parsing Technologies, pages 115–124, Vancouver, BC, Canada, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
</authors>
<title>Statistical inference and probabilistic modeling for constraint-based NLP.</title>
<date>1998</date>
<booktitle>Computers, Linguistics, and Phonetics between Language and Speech: Proceedings of the 4th Conference on Natural Language Processing (KONVENS’98),</booktitle>
<pages>111--124</pages>
<editor>In B. Schr¨oder, W. Lenders, W. Hess, and T. Portele, editors,</editor>
<location>Bonn. Lang.</location>
<contexts>
<context position="32365" citStr="Riezler, 1998" startWordPosition="5492" endWordPosition="5493">The motivation for this approach is twofold: First, we wanted to capture the linguistic insight that features follow certain patterns in propagating through the tree. Second, we wanted to make it statistically feasible and computationally tractable to increase L to higher values than in the PCFG-LA model. The hope was that the learning process could then make finer distinctions and learn more fine-grained information. However, it turned out that the higher values of L did not compensate for the perhaps overly con10This affects EM training only by requiring a convex optimization at the M step (Riezler, 1998). strained model. The results on English parsing rather suggest that it is the similarity in degrees of freedom (e.g., INHERIT with L=3x3x3 and PCFGLA with L=2x2) that produces comparable results. Substantial gains were achieved by using markovization and splitting only selected nonterminals. With these techniques we reach a parsing accuracy similar to Matsuzaki et al. (2005), but with an order of magnitude less parameters, resulting in more efficient parsing. We hope to get more wins in future by using more sophisticated annealing techniques and log-linear modeling techniques. Acknowledgments</context>
</contexts>
<marker>Riezler, 1998</marker>
<rawString>Stefan Riezler. 1998. Statistical inference and probabilistic modeling for constraint-based NLP. In B. Schr¨oder, W. Lenders, W. Hess, and T. Portele, editors, Computers, Linguistics, and Phonetics between Language and Speech: Proceedings of the 4th Conference on Natural Language Processing (KONVENS’98), pages 111–124, Bonn. Lang.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Rose</author>
</authors>
<title>Deterministic annealing for clustering, compression, classification, regression, and related optimization problems.</title>
<date>1998</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>80--2210</pages>
<contexts>
<context position="18943" citStr="Rose, 1998" startWordPosition="3249" endWordPosition="3250">ould have scaled down the probabilities of such rules by 1/4, 1/2, and 1 respectively. Instead, we simply scaled them all down by the same proportion. While this temporarily changes the balance of probability among the three kinds of rules, EM immediately corrects this balance on the next training iteration to match the observed balance on the treebank trees—hence the one-iteration downtick in Figure 5). 321 Jensen-Shannon divergence is defined as D(q,r) = 2 (D (q q2r) + D (r ||q + r)/ 2 These experiments are a kind of of the deterministic annealing clustering algorithm (Pereira et al., 1993; Rose, 1998), which gradually increases the number of clusters during the clustering process. In deterministic annealing, one starts in principle with a very large number of clusters, but maximizes likelihood only under a constraint that the joint distribution p(point, cluster) must have very high entropy. This drives all of the cluster centroids to coincide exactly, redundantly representing just one effective cluster. As the entropy is permitted to decrease, some of the cluster centroids find it worthwhile to drift apart.6 In future work, we would like to apply this technique to split nonterminals gradua</context>
</contexts>
<marker>Rose, 1998</marker>
<rawString>Kenneth Rose. 1998. Deterministic annealing for clustering, compression, classification, regression, and related optimization problems. Proceedings of the IEEE, 80:2210–2239, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Annealing structural bias in multilingual weighted grammar induction.</title>
<date>2006</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="30715" citStr="Smith and Eisner, 2006" startWordPosition="5205" endWordPosition="5208"> model P(X[α] — Y [Q] Z[-y]) with a log-linear model.10 Feature functions would consider the values of variously sized, overlapping subsets of X, Y, Z, α, Q, -y. For example, a certain feature might fire when X[α] = NP[1] and Z[-y] = N[2]. This approach can be extended to the multi-feature case, as in INHERIT2. Inheritance as in the INHERIT model can then be expressed by features like α = Q, or α = Q and X = VP. During early iterations, we could use a prior to encourage a strong positive weight on these inheritance features, and gradually relax this bias—akin to the “structural annealing” of (Smith and Eisner, 2006). When modeling the lexical rule P(X[α] -* w), we could use features that consider the spelling of the word w in conjunction with the value of α. Thus, we might learn that V [1] is particularly likely to rewrite as a word ending in -s. Spelling features that are predictable from string context are important clues to the existence and behavior of the hidden annotations we wish to induce. A final remark is that “inheritance” does not necessarily have to mean that α = Q. It is enough that α and Q should have high mutual information, so that one can be predicted from the other; they do not actuall</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>Noah A. Smith and Jason Eisner. 2006. Annealing structural bias in multilingual weighted grammar induction. In Proceedings ofACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>