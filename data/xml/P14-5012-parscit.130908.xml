<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000316">
<title confidence="0.997938">
WoSIT: A Word Sense Induction Toolkit
for Search Result Clustering and Diversification
</title>
<author confidence="0.842176">
Daniele Vannella, Tiziano Flati and Roberto Navigli
</author>
<affiliation confidence="0.645526">
Dipartimento di Informatica
</affiliation>
<address confidence="0.512638">
Sapienza Universit`a di Roma
</address>
<email confidence="0.994453">
{vannella,flati,navigli}@di.uniroma1.it
</email>
<sectionHeader confidence="0.997329" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999771545454546">
In this demonstration we present WoSIT,
an API for Word Sense Induction (WSI)
algorithms. The toolkit provides imple-
mentations of existing graph-based WSI
algorithms, but can also be extended with
new algorithms. The main mission of
WoSIT is to provide a framework for the
extrinsic evaluation of WSI algorithms,
also within end-user applications such as
Web search result clustering and diversifi-
cation.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99980184">
The Web is by far the world’s largest information
archive, whose content – made up of billions of
Web pages – is growing exponentially. Unfortu-
nately the retrieval of any given piece of infor-
mation is an arduous task which challenges even
prominent search engines such as those developed
by Google, Yahoo! and Microsoft. Even today,
such systems still find themselves up against the
lexical ambiguity issue, that is, the linguistic prop-
erty due to which a single word may convey dif-
ferent meanings.
It has been estimated that around 4% of Web
queries and 16% of the most frequent queries are
ambiguous (Sanderson, 2008). A major issue as-
sociated with the lexical ambiguity phenomenon
on the Web is the low number of query words sub-
mitted by Web users to search engines. A pos-
sible solution to this issue is the diversification of
search results obtained by maximizing the dissimi-
larity of the top-ranking Web pages returned to the
user (Agrawal et al., 2009; Ashwin Swaminathan
and Kirovski, 2009). Another solution consists of
clustering Web search results by way of clustering
engines such as Carrot1 and Yippy2 and presenting
them to the user grouped by topic.
</bodyText>
<footnote confidence="0.999941">
1http://search.carrot2.org
2http://yippy.com
</footnote>
<bodyText confidence="0.999216173913044">
Diversification and Web clustering algorithms,
however, do not perform any semantic analysis of
search results, clustering them solely on the basis
of their lexical similarity. Recently, it has been
shown that the automatic acquisition of the mean-
ings of a word of interest, a task referred to as
Word Sense Induction, can be successfully inte-
grated into search result clustering and diversifica-
tion (Navigli and Crisafulli, 2010; Di Marco and
Navigli, 2013) so as to outperform non-semantic
state-of-the-art Web clustering systems.
In this demonstration we describe a new toolkit
for Word Sense Induction, called WoSIT, which
i) provides ready implementations of existing
WSI algorithms; ii) can be extended with addi-
tional WSI algorithms; iii) enables the integration
of WSI algorithms into search result clustering
and diversification, thereby providing an extrinsic
evaluation tool. As a result the toolkit enables the
objective comparison of WSI algorithms within an
end-user application in terms of the degree of di-
versification of the search results of a given am-
biguous query.
</bodyText>
<sectionHeader confidence="0.999099" genericHeader="method">
2 WoSIT
</sectionHeader>
<bodyText confidence="0.999744882352941">
In Figure 1 we show the workflow of the WoSIT
toolkit, composed of three main phases: WSI;
semantically-enhanced search result clustering
and diversification; evaluation. Given a target
query q whose meanings we want to automati-
cally acquire, the toolkit first builds a graph for q,
obtained either from a co-occurrence database, or
constructed programmatically by using any user-
provided input. The co-occurrence graph is then
input to a WSI algorithm, chosen from among
those available in the toolkit or implemented by
the user. As a result, a set of word clusters
is produced. This concludes the first phase of
the WoSIT workflow. Then, the word clusters
produced are used for assigning meanings to the
search results returned by a search engine for the
query q, i.e. search result disambiguation. The
</bodyText>
<page confidence="0.994567">
67
</page>
<affiliation confidence="0.4168595">
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 67–72,
Baltimore, Maryland USA, June 23-24, 2014. c�2014 Association for Computational Linguistics
</affiliation>
<figure confidence="0.999268714285714">
Co-occurrence Information
DBDB
Co-occurrence WSI
graph
WSI
Word Assignment of Search result
Algorithm Clusters results to clusters disambiguation WSI Evaluator
w1w3
w2w4w5 s5s4
w6 s
1
Web
search engine
Semantically Enhanced
Search Result Clustering
Dataset
s3
+ MANUAL ANNOTATIONS
s2
Evaluation
Eval Results
</figure>
<figureCaption confidence="0.999996">
Figure 1: The WoSIT workflow.
</figureCaption>
<bodyText confidence="0.997445714285714">
outcome is that we obtain a clustering of search
results. Finally, during the third phase, we apply
the evaluation module which performs an evalua-
tion of the search result clustering quality and the
diversification performance.
We now describe in detail the three main phases
of WoSIT.
</bodyText>
<subsectionHeader confidence="0.999076">
2.1 Word Sense Induction
</subsectionHeader>
<bodyText confidence="0.999919818181818">
The first phase of WoSIT consists of the automatic
identification of the senses of a query of inter-
est, i.e. the task of Word Sense Induction. Al-
though WoSIT enables the integration of custom
implementations which can potentially work with
any WSI paradigm, the toolkit provides ready-to-
use implementations of several graph-based algo-
rithms that work with word co-occurrences. All
these algorithms carry out WSI in two steps: co-
occurrence graph construction (Section 2.1.1) and
discovery of word senses (Section 2.1.2).
</bodyText>
<subsectionHeader confidence="0.977998">
2.1.1 Co-occurrence graph construction
</subsectionHeader>
<bodyText confidence="0.97533528125">
Given a target query q, we build a co-occurrence
graph GQ = (V, E) such that V is the set of
words co-occurring with q and E is the set of undi-
rected edges, each denoting a co-occurrence be-
tween pairs of words in V . In Figure 2 we show
an example of a co-occurrence graph for the target
word excalibur.
WoSIT enables the creation of the co-
occurrence graph either programmatically, by
adding edges and vertices according to any user-
specific algorithm, or starting from the statis-
tics for co-occurring words obtained from a co-
occurrence database (created, e.g., from a text cor-
pus, as was done by Di Marco and Navigli (2013)).
In either case, weights for edges have to be pro-
vided in terms of the correlation strength between
pairs of words (e.g. using Dice, Jaccard or other
co-occurrence measures).
The information about the co-occurrence
database, e.g. a MySQL database, is provided
programmatically or via parameters in the prop-
erties configuration file (db.properties).
The co-occurrence database has to follow a
given schema provided in the toolkit docu-
mentation. An additional configuration file
(wosit.properties) also allows the user
to specify additional constraints, e.g. the
minimum weight value of co-occurrence (the
wordGraph.minWeight parameter) to be
added as edges to the graph.
The graphs produced can also be saved to binary
(i.e. serialized) or text file:
</bodyText>
<equation confidence="0.995547">
g.saveToSer(fileName);
g = WordGraph.loadFromSer(fileName);
g.saveToTxt(fileName);
g = WordGraph.loadFromTxt(fileName);
</equation>
<bodyText confidence="0.999931">
We are now ready to provide our co-occurrence
graph, created with just a few lines of code, as in-
put to a WSI algorithm, as will be explained in the
next section.
</bodyText>
<subsectionHeader confidence="0.966979">
2.1.2 Discovery of Word Senses
</subsectionHeader>
<bodyText confidence="0.9999436">
Once the co-occurrence graph for the query q is
built, it can be input to any WSI algorithm which
extends the GraphClusteringAlgorithm
class in the toolkit. WoSIT comes with a number
of ready-to-use such algorithms, among which:
</bodyText>
<page confidence="0.998559">
68
</page>
<figureCaption confidence="0.9780735">
Figure 2: Example of a co-occurrence graph for
the word excalibur.
</figureCaption>
<listItem confidence="0.970741782608695">
• Balanced Maximum Spanning Tree (B-
MST) (Di Marco and Navigli, 2013), an ex-
tension of a WSI algorithm based on the
calculation of a Maximum Spanning Tree
(Di Marco and Navigli, 2011) aimed at bal-
ancing the number of co-occurrences in each
sense cluster.
• HyperLex (V´eronis, 2004), an algorithm
which identifies hubs in co-occurrence
graphs, thereby identifying basic meanings
for the input query.
• Chinese Whispers (Biemann, 2006), a ran-
domized algorithm which partitions nodes by
means of the iterative transfer of word sense
information across the co-occurrence graph
(Biemann, 2006).
• Squares, Triangles and Diamonds
(SquaT++) (Di Marco and Navigli, 2013),
an extension of the SquaT algorithm (Navigli
and Crisafulli, 2010) which exploits three
cyclic graph patterns to determine and
discard those vertices (or edges) with weak
degree of connectivity in the graph.
</listItem>
<bodyText confidence="0.973368766666667">
We also provide an implementation of a word
clustering algorithm, i.e. Lin98 (Lin, 1998),
which does not rely on co-occurrence graphs, but
just on the word co-occurrence information to it-
eratively refine word clusters on the basis of their
“semantic” relationships.
A programmatic example of use of the B-MST
WSI algorithm is as follows:
BMST mst = new BMST(g);
mst.makeClustering();
Clustering wordClusters =
mst.getClustering();
where g is a co-occurrence graph created as ex-
plained in Section 2.1.1, provided as input to
the constructor of the algorithm’s class. The
makeClustering method implements the in-
duction algorithm and creates the word clus-
ters, which can then be retrieved calling the
getClustering method. As a result an in-
stance of the Clustering class is provided.
As mentioned above, WoSIT also enables
the creation of custom WSI implementa-
tions. This can be done by extending the
GraphClusteringAlgorihm abstract class.
The new algorithm just has to implement two
methods:
public void makeClustering();
public Clustering getClustering();
As a result, the new algorithm is readily inte-
grated into the WoSIT toolkit.
</bodyText>
<subsectionHeader confidence="0.816034">
2.2 Semantically-enhanced Search Result
Clustering and Diversification
</subsectionHeader>
<bodyText confidence="0.994488">
We now move to the use of the induced senses of
our target query q within an application, i.e. search
result clustering and diversification.
Search result clustering. The next step (cf. Fig-
ure 1) is the association of the search results re-
turned by a search engine for query q with the most
suitable word cluster (i.e. meaning of q). This can
be done in two lines:
SnippetAssociator associator =
SnippetAssociator.getInstance();
SnippetClustering clustering =
associator.associateSnippet(
targetWord,
searchResults,
wordClusters,
AssociationMetric.DEGREE_OVERLAP);
The first line obtains an instance of the class
which performs the association between search re-
sult snippets and the word clusters obtained from
the WSI algorithm. The second line calls the asso-
ciation method associateSnippet which in-
puts the target word, the search results obtained
from the search engine, the word clusters and, fi-
nally, the kind of metric to use for the associa-
tion. Three different association metrics are im-
plemented in the toolkit:
</bodyText>
<listItem confidence="0.994275833333333">
• WORD OVERLAP performs the association by
maximizing the size of the intersection be-
tween the word sets in each snippet and the
word clusters;
• DEGREE OVERLAP performs the association
by calculating for each word cluster the sum
</listItem>
<figure confidence="0.999242333333334">
King Arthur
0.007
Car
Fantasy
0.015
0.04
0.01
0.02
0.025
Excalibur
0.02
0.012
Limousine
0.013
Book
0.005
0.006
Film
</figure>
<page confidence="0.980469">
69
</page>
<bodyText confidence="0.659469">
of the vertex degrees in the co-occurrence
graph of the words occurring in each snippet;
</bodyText>
<listItem confidence="0.929885666666667">
• TOKEN OVERLAP is similar in spirit to
WORD OVERLAP, but takes into account each
token occurrence in the snippet bag of words.
</listItem>
<bodyText confidence="0.996917">
Search result diversification. The above two
lines of code return a set of snippet clusters and, as
a result, semantically-enhanced search result clus-
tering is performed. At the end, the resulting clus-
tering can be used to provide a diversified rerank-
ing of the results:
</bodyText>
<equation confidence="0.9511845">
List&lt;Snippet&gt; snippets =
clustering.diversify(sorter);
</equation>
<bodyText confidence="0.999966615384616">
The diversify method returns a flat list of
snippet results obtained according to the Sorter
object provided in input. The Sorter abstract
class is designed to rerank the snippet clusters ac-
cording to some predefined rule. For instance, the
CardinalitySorter class, included in the
toolkit, sorts the clusters according to the size of
each cluster. Once a sorting order has been es-
tablished, an element from each snippet cluster is
added to an initially-empty list; next, a second el-
ement from each cluster is added, and so on, until
all snippets are added to the list.
The sorting rules implemented in the toolkit are:
</bodyText>
<listItem confidence="0.99984825">
• CardinalitySorter: sorts the clusters
according to their size, i.e. the number of ver-
tices in the cluster;
• MeanSimilaritySorter: sorts the clus-
</listItem>
<bodyText confidence="0.928439857142857">
ters according to the average association
score between the snippets in the cluster and
the backing word cluster (defined by the se-
lected association metrics).
Notably, the end user can then implement his or
her own custom sorting procedure by simply ex-
tending the Sorter class.
</bodyText>
<subsectionHeader confidence="0.922798">
2.2.1 Search Result Datasets
</subsectionHeader>
<bodyText confidence="0.9989955">
The framework comes with two search result
datasets of ambiguous queries: the AMBI-
ENT+MORESQUE dataset made available by
Bernardini et al. (2009) and Navigli and Crisa-
fulli (2010), respectively, and the SemEval-2013-
Task11 dataset.3 New result datasets can be pro-
vided by users complying with the dataset format
described below.
</bodyText>
<footnote confidence="0.969254">
3For details visit http://lcl.uniroma1.it/
wosit/.
</footnote>
<bodyText confidence="0.9974275">
A search result dataset in WoSIT is made up of
at least two files:
</bodyText>
<listItem confidence="0.987093666666667">
• topics.txt, which contains the queries
(topics) of interest together with their nu-
meric ids. For instance:
</listItem>
<figure confidence="0.9860468">
id description
1 polaroid
2 kangaroo
3 shakira
... ...
</figure>
<listItem confidence="0.644757">
• results.txt, which lists the search re-
sults for each given query, in terms of URL,
page title and page snippet:
</listItem>
<footnote confidence="0.808469">
IDurl title snippet
1.1 http://www.polaroid.com/ Polaroid  |Home ...
1.2 http://www.polaroid.com/products products...
1.3 http://en.wikipedia.org/wiki/Polaroid_Cor...
</footnote>
<bodyText confidence="0.927932833333333">
... ...
Therefore, the two files provide the queries and the
corresponding search results returned by a search
engine. In order to enable an automatic evaluation
of the search result clustering and diversification
output, two additional files have to be provided:
</bodyText>
<listItem confidence="0.989650285714286">
• subTopics.txt, which for each query
provides the list of meanings for that query,
e.g.:
• STRel.txt, which provides the manual as-
sociations between each search result and the
most suitable meaning as provided in the
subTopics.txt file. For instance:
</listItem>
<figure confidence="0.5828338">
subTopicID resultID
1.1 1.1
1.1 1.2
1.1 1.3
... ...
</figure>
<subsectionHeader confidence="0.99886">
2.3 WSI Evaluator
</subsectionHeader>
<bodyText confidence="0.9999711">
As shown in Figure 1 the final component of our
workflow is the evaluation of WSI when integrated
into search result clustering and diversification (al-
ready used by Navigli and Vannella (2013)). This
component, called the WSI Evaluator, takes as
input the snippet clusters obtained for a given
query together with the fully annotated search re-
sult dataset, as described in the previous section.
Two kinds of evaluations are carried out, described
in what follows.
</bodyText>
<figure confidence="0.987298263157895">
ID description
1.1 Polaroid Corporation, a multinational con...
1.2 Instant film photographs are sometimes kn...
1.3 Instant camera (or Land camera), sometime...
... ...
70
1 Dataset searchResults = Dataset.getInstance();
2 DBConfiguration db = DBConfiguration.getInstance();
3 for(String targetWord : dataset.getQueries())
4 {
5 WordGraph g = WordGraph.createWordGraph(targetWord, searchResults, db);
6 BMST mst = new BMST(g);
7 mst.makeClustering();
8 SnippetAssociator snippetAssociator = SnippetAssociator.getInstance();
9 SnippetClustering snippetClustering = snippetAssociator.associateSnippet(
10 targetWord, searchResults, mst.getClustering(), AssociationMetric.WORD_OVERLAP);
11 snippetClustering.export(&amp;quot;output/outputMST.txt&amp;quot;, true);
12 }
13 WSIEvaluator.evaluate(searchResults, &amp;quot;output/outputMST.txt&amp;quot;);
</figure>
<figureCaption confidence="0.998965">
Figure 3: An example of evaluation code for the B-MST clustering algorithm.
</figureCaption>
<subsectionHeader confidence="0.660818">
2.3.1 Evaluation of the clustering quality
</subsectionHeader>
<bodyText confidence="0.999865285714286">
The quality of the output produced by
semantically-enhanced search result cluster-
ing is evaluated in terms of Rand Index (Rand,
1971, RI), Adjusted Rand Index (Hubert and
Arabie, 1985, ARI), Jaccard Index (JI) and,
finally, precision and recall as done by Crabtree et
al. (2005), together with their F1 harmonic mean.
</bodyText>
<subsectionHeader confidence="0.919187">
2.3.2 Evaluation of the clustering diversity
</subsectionHeader>
<bodyText confidence="0.999880375">
To evaluate the snippet clustering diversity the
measures of S-recall@K and S-precision@r (Zhai
et al., 2003) are calculated. These measures de-
termine how many different meanings of a query
are covered in the top-ranking results shown to the
user. We calculate these measures on the output of
the three different association metrics illustrated in
Section 2.2.
</bodyText>
<sectionHeader confidence="0.989577" genericHeader="method">
3 A Full Example
</sectionHeader>
<bodyText confidence="0.999985857142857">
We now show a full example of usage of the
WoSIT API. The code shown in Figure 3 initially
obtains a search result dataset (line 1), selects a
database (line 2) and iterates over its queries (line
3). Next, a co-occurrence graph for the current
query is created from a co-occurrence database
(line 5) and an instance of the B-MST WSI algo-
rithm is created with the graph as input (line 6).
After executing the algorithm (line 7), the snippets
for the given query are clustered (lines 8-10). The
resulting snippet clustering is appended to an out-
put file (line 11). Finally, the WSI evaluator is run
on the resulting snippet clustering using the given
dataset (line 13).
</bodyText>
<subsectionHeader confidence="0.985778">
3.1 Experiments
</subsectionHeader>
<bodyText confidence="0.973501">
We applied the WoSIT API to the AMBI-
ENT+MORESQUE dataset using 4 induction al-
</bodyText>
<table confidence="0.999746642857143">
Algorithm Assoc. ARI Web1T # cl.
metr. JI F1
WO 69.65 75.69 59.19 2.1
SquaT++ DO 69.21 75.45 59.19 2.1
TO 69.67 75.69 59.19 2.1
WO 60.76 71.51 64.56 5.0
B-MST DO 66.48 69.37 64.84 5.0
TO 63.17 71.21 64.04 5.0
WO 60.86 72.05 65.41 13.0
HyperLex DO 66.27 68.00 71.91 13.0
TO 62.82 70.87 65.08 13.0
WO 67.75 75.37 60.25 12.5
Chinese Whispers DO 65.95 69.49 70.33 12.5
TO 67.57 74.69 60.50 12.5
</table>
<tableCaption confidence="0.998803">
Table 1: Results of WSI algorithms with a Web1T
</tableCaption>
<bodyText confidence="0.998608">
co-occurrence database and the three association
metrics (Word Overlap, Degree Overlap and To-
ken Overlap). The reported measures are Ad-
justed Rand Index (ARI), Jaccard Index (JI) and
F1. We also show the average number of clusters
per query produced by each algorithm.
gorithms among those available in the toolkit,
where co-occurrences were obtained from the
Google Web1T corpus (Brants and Franz, 2006).
In Table 1 we show the clustering quality results
output by the WoSIT evaluator, whereas in Fig-
ure 4 we show the diversification performance in
terms of S-recall@K.
</bodyText>
<subsectionHeader confidence="0.994668">
3.2 Conclusions
</subsectionHeader>
<bodyText confidence="0.999856333333333">
In this demonstration we presented WoSIT, a full-
fledged toolkit for Word Sense Induction algo-
rithms and their integration into search result clus-
tering and diversification. The main contributions
are as follows: first, we release a Java API for
performing Word Sense Induction which includes
several ready-to-use implementations of existing
algorithms; second, the API enables the use of the
acquired senses for a given query for enhancing
</bodyText>
<page confidence="0.989786">
71
</page>
<figure confidence="0.977927333333333">
HyperLex
BMST
ChineseW
SquaT++
2.0 4.0 6.0 8.0 10.0 12.0 14.0 16.0 18.0 20.0
K
</figure>
<figureCaption confidence="0.999959">
Figure 4: S-recall@K performance.
</figureCaption>
<bodyText confidence="0.999794578947368">
search result clustering and diversification; third,
we provide an evaluation component which, given
an annotated dataset of search results, carries out
different kinds of evaluation of the snippet cluster-
ing quality and diversity.
WoSIT is the first available toolkit which pro-
vides an end-to-end approach to the integration of
WSI into a real-world application. The toolkit en-
ables an objective comparison of WSI algorithms
as well as an evaluation of the impact of apply-
ing WSI to clustering and diversifying search re-
sults. As shown by Di Marco and Navigli (2013),
this integration is beneficial and allows outperfor-
mance of non-semantic state-of-the-art Web clus-
tering systems.
The toolkit, licensed under a Creative Com-
mons Attribution-Non Commercial-Share Alike
3.0 License, is available at http://lcl.
uniroma1.it/wosit/.
</bodyText>
<sectionHeader confidence="0.999152" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999004106666667">
Rakesh Agrawal, Sreenivas Gollapudi, Alan Halver-
son, and Samuel Ieong. 2009. Diversifying search
results. In Proc. of the Second International Confer-
ence on Web Search and Web Data Mining (WSDM
2009), pages 5–14, Barcelona, Spain.
Cherian V. Mathew Ashwin Swaminathan and Darko
Kirovski. 2009. Essential Pages. In Proc. of the
2009 IEEE/WIC/ACM International Joint Confer-
ence on Web Intelligence and IntelligentAgent Tech-
nology, volume 1, pages 173–182.
Andrea Bernardini, Claudio Carpineto, and Massim-
iliano D’Amico. 2009. Full-Subtopic Retrieval
with Keyphrase-Based Search Results Clustering.
In Proc. of Web Intelligence 2009, volume 1, pages
206–213, Los Alamitos, CA, USA.
Chris Biemann. 2006. Chinese Whispers - an Effi-
cient Graph Clustering Algorithm and its Applica-
tion to Natural Language Processing Problems. In
Proc. of TextGraphs: the First Workshop on Graph
Based Methods for Natural Language Processing,
pages 73–80, New York City.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram, ver. 1, LDC2006T13. In Linguistic Data Con-
sortium, Philadelphia, USA.
Daniel Crabtree, Xiaoying Gao, and Peter Andreae.
2005. Improving web clustering by cluster selec-
tion. In Proc. of the 2005 IEEE/WIC/ACM Interna-
tional Conference on Web Intelligence, pages 172–
178, Washington, DC, USA.
Antonio Di Marco and Roberto Navigli. 2011. Clus-
tering Web Search Results with Maximum Spanning
Trees. In Proc. of the XIIth International Confer-
ence of the Italian Association for Artificial Intelli-
gence (AI*IA), pages 201–212, Palermo, Italy.
Antonio Di Marco and Roberto Navigli. 2013. Clus-
tering and Diversifying Web Search Results with
Graph-Based Word Sense Induction. Computa-
tional Linguistics, 39(3):709–754.
Lawrence Hubert and Phipps Arabie. 1985. Compar-
ing Partitions. Journal of Classification, 2(1):193–
218.
Dekang Lin. 1998. Automatic Retrieval and Cluster-
ing of Similar Words. In Proc. of the 171h Inter-
national Conference on Computational linguistics
(COLING), pages 768–774, Montreal, Canada.
Roberto Navigli and Giuseppe Crisafulli. 2010. In-
ducing Word Senses to Improve Web Search Result
Clustering. In Proc. of the 2010 Conference on Em-
pirical Methods in Natural Language Processing,
pages 116–126, Boston, USA.
Roberto Navigli and Daniele Vannella. 2013.
SemEval-2013 Task 11: Evaluating Word Sense In-
duction &amp; Disambiguation within An End-User Ap-
plication. In Proc. of the 71h International Work-
shop on Semantic Evaluation (SemEval 2013), in
conjunction with the Second Joint Conference on
Lexical and Computational Semantics (*SEM 2013),
pages 193–201, Atlanta, USA.
William M. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of the Ameri-
can Statistical association, 66(336):846–850.
Mark Sanderson. 2008. Ambiguous queries: test col-
lections need more sense. In Proc. of the 31st an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval,
pages 499–506, Singapore.
Jean V´eronis. 2004. HyperLex: lexical cartography
for information retrieval. Computer, Speech and
Language, 18(3):223–252.
ChengXiang Zhai, William W. Cohen, and John Laf-
ferty. 2003. Beyond independent relevance: Meth-
ods and evaluation metrics for subtopic retrieval. In
Proc. of the 26th annual international ACM SIGIR
conference on Research and development in infor-
mation retrieval, pages 10–17, Toronto, Canada.
</reference>
<figure confidence="0.999142888888889">
S-recall-at-K 1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
</figure>
<page confidence="0.953218">
72
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.477493">
<title confidence="0.961209">WoSIT: A Word Sense Induction for Search Result Clustering and Diversification Vannella, Tiziano Flati</title>
<author confidence="0.8471365">Dipartimento di_Sapienza Universit`a di</author>
<abstract confidence="0.97388">In this demonstration we present WoSIT, an API for Word Sense Induction (WSI) algorithms. The toolkit provides implementations of existing graph-based WSI algorithms, but can also be extended with new algorithms. The main mission of WoSIT is to provide a framework for the extrinsic evaluation of WSI algorithms, also within end-user applications such as Web search result clustering and diversification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rakesh Agrawal</author>
<author>Sreenivas Gollapudi</author>
<author>Alan Halverson</author>
<author>Samuel Ieong</author>
</authors>
<title>Diversifying search results.</title>
<date>2009</date>
<booktitle>In Proc. of the Second International Conference on Web Search and Web Data Mining (WSDM</booktitle>
<pages>5--14</pages>
<location>Barcelona,</location>
<contexts>
<context position="1622" citStr="Agrawal et al., 2009" startWordPosition="253" endWordPosition="256"> systems still find themselves up against the lexical ambiguity issue, that is, the linguistic property due to which a single word may convey different meanings. It has been estimated that around 4% of Web queries and 16% of the most frequent queries are ambiguous (Sanderson, 2008). A major issue associated with the lexical ambiguity phenomenon on the Web is the low number of query words submitted by Web users to search engines. A possible solution to this issue is the diversification of search results obtained by maximizing the dissimilarity of the top-ranking Web pages returned to the user (Agrawal et al., 2009; Ashwin Swaminathan and Kirovski, 2009). Another solution consists of clustering Web search results by way of clustering engines such as Carrot1 and Yippy2 and presenting them to the user grouped by topic. 1http://search.carrot2.org 2http://yippy.com Diversification and Web clustering algorithms, however, do not perform any semantic analysis of search results, clustering them solely on the basis of their lexical similarity. Recently, it has been shown that the automatic acquisition of the meanings of a word of interest, a task referred to as Word Sense Induction, can be successfully integrate</context>
</contexts>
<marker>Agrawal, Gollapudi, Halverson, Ieong, 2009</marker>
<rawString>Rakesh Agrawal, Sreenivas Gollapudi, Alan Halverson, and Samuel Ieong. 2009. Diversifying search results. In Proc. of the Second International Conference on Web Search and Web Data Mining (WSDM 2009), pages 5–14, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cherian V Mathew Ashwin Swaminathan</author>
<author>Darko Kirovski</author>
</authors>
<title>Essential Pages.</title>
<date>2009</date>
<booktitle>In Proc. of the 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and IntelligentAgent Technology,</booktitle>
<volume>1</volume>
<pages>173--182</pages>
<contexts>
<context position="1662" citStr="Swaminathan and Kirovski, 2009" startWordPosition="258" endWordPosition="261">s up against the lexical ambiguity issue, that is, the linguistic property due to which a single word may convey different meanings. It has been estimated that around 4% of Web queries and 16% of the most frequent queries are ambiguous (Sanderson, 2008). A major issue associated with the lexical ambiguity phenomenon on the Web is the low number of query words submitted by Web users to search engines. A possible solution to this issue is the diversification of search results obtained by maximizing the dissimilarity of the top-ranking Web pages returned to the user (Agrawal et al., 2009; Ashwin Swaminathan and Kirovski, 2009). Another solution consists of clustering Web search results by way of clustering engines such as Carrot1 and Yippy2 and presenting them to the user grouped by topic. 1http://search.carrot2.org 2http://yippy.com Diversification and Web clustering algorithms, however, do not perform any semantic analysis of search results, clustering them solely on the basis of their lexical similarity. Recently, it has been shown that the automatic acquisition of the meanings of a word of interest, a task referred to as Word Sense Induction, can be successfully integrated into search result clustering and dive</context>
</contexts>
<marker>Swaminathan, Kirovski, 2009</marker>
<rawString>Cherian V. Mathew Ashwin Swaminathan and Darko Kirovski. 2009. Essential Pages. In Proc. of the 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and IntelligentAgent Technology, volume 1, pages 173–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Bernardini</author>
<author>Claudio Carpineto</author>
<author>Massimiliano D’Amico</author>
</authors>
<title>Full-Subtopic Retrieval with Keyphrase-Based Search Results Clustering.</title>
<date>2009</date>
<booktitle>In Proc. of Web Intelligence</booktitle>
<volume>1</volume>
<pages>206--213</pages>
<location>Los Alamitos, CA, USA.</location>
<marker>Bernardini, Carpineto, D’Amico, 2009</marker>
<rawString>Andrea Bernardini, Claudio Carpineto, and Massimiliano D’Amico. 2009. Full-Subtopic Retrieval with Keyphrase-Based Search Results Clustering. In Proc. of Web Intelligence 2009, volume 1, pages 206–213, Los Alamitos, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
</authors>
<title>Chinese Whispers - an Efficient Graph Clustering Algorithm and its Application to Natural Language Processing Problems.</title>
<date>2006</date>
<booktitle>In Proc. of TextGraphs: the First Workshop on Graph Based Methods for Natural Language Processing,</booktitle>
<pages>73--80</pages>
<location>New York City.</location>
<contexts>
<context position="7616" citStr="Biemann, 2006" startWordPosition="1188" endWordPosition="1189">steringAlgorithm class in the toolkit. WoSIT comes with a number of ready-to-use such algorithms, among which: 68 Figure 2: Example of a co-occurrence graph for the word excalibur. • Balanced Maximum Spanning Tree (BMST) (Di Marco and Navigli, 2013), an extension of a WSI algorithm based on the calculation of a Maximum Spanning Tree (Di Marco and Navigli, 2011) aimed at balancing the number of co-occurrences in each sense cluster. • HyperLex (V´eronis, 2004), an algorithm which identifies hubs in co-occurrence graphs, thereby identifying basic meanings for the input query. • Chinese Whispers (Biemann, 2006), a randomized algorithm which partitions nodes by means of the iterative transfer of word sense information across the co-occurrence graph (Biemann, 2006). • Squares, Triangles and Diamonds (SquaT++) (Di Marco and Navigli, 2013), an extension of the SquaT algorithm (Navigli and Crisafulli, 2010) which exploits three cyclic graph patterns to determine and discard those vertices (or edges) with weak degree of connectivity in the graph. We also provide an implementation of a word clustering algorithm, i.e. Lin98 (Lin, 1998), which does not rely on co-occurrence graphs, but just on the word co-oc</context>
</contexts>
<marker>Biemann, 2006</marker>
<rawString>Chris Biemann. 2006. Chinese Whispers - an Efficient Graph Clustering Algorithm and its Application to Natural Language Processing Problems. In Proc. of TextGraphs: the First Workshop on Graph Based Methods for Natural Language Processing, pages 73–80, New York City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<date>2006</date>
<journal>Web</journal>
<booktitle>In Linguistic Data Consortium,</booktitle>
<volume>1</volume>
<pages>2006--13</pages>
<location>Philadelphia, USA.</location>
<contexts>
<context position="17469" citStr="Brants and Franz, 2006" startWordPosition="2705" endWordPosition="2708">.05 65.41 13.0 HyperLex DO 66.27 68.00 71.91 13.0 TO 62.82 70.87 65.08 13.0 WO 67.75 75.37 60.25 12.5 Chinese Whispers DO 65.95 69.49 70.33 12.5 TO 67.57 74.69 60.50 12.5 Table 1: Results of WSI algorithms with a Web1T co-occurrence database and the three association metrics (Word Overlap, Degree Overlap and Token Overlap). The reported measures are Adjusted Rand Index (ARI), Jaccard Index (JI) and F1. We also show the average number of clusters per query produced by each algorithm. gorithms among those available in the toolkit, where co-occurrences were obtained from the Google Web1T corpus (Brants and Franz, 2006). In Table 1 we show the clustering quality results output by the WoSIT evaluator, whereas in Figure 4 we show the diversification performance in terms of S-recall@K. 3.2 Conclusions In this demonstration we presented WoSIT, a fullfledged toolkit for Word Sense Induction algorithms and their integration into search result clustering and diversification. The main contributions are as follows: first, we release a Java API for performing Word Sense Induction which includes several ready-to-use implementations of existing algorithms; second, the API enables the use of the acquired senses for a giv</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram, ver. 1, LDC2006T13. In Linguistic Data Consortium, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Crabtree</author>
<author>Xiaoying Gao</author>
<author>Peter Andreae</author>
</authors>
<title>Improving web clustering by cluster selection.</title>
<date>2005</date>
<booktitle>In Proc. of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence,</booktitle>
<pages>172--178</pages>
<location>Washington, DC, USA.</location>
<contexts>
<context position="15401" citStr="Crabtree et al. (2005)" startWordPosition="2353" endWordPosition="2356">ateSnippet( 10 targetWord, searchResults, mst.getClustering(), AssociationMetric.WORD_OVERLAP); 11 snippetClustering.export(&amp;quot;output/outputMST.txt&amp;quot;, true); 12 } 13 WSIEvaluator.evaluate(searchResults, &amp;quot;output/outputMST.txt&amp;quot;); Figure 3: An example of evaluation code for the B-MST clustering algorithm. 2.3.1 Evaluation of the clustering quality The quality of the output produced by semantically-enhanced search result clustering is evaluated in terms of Rand Index (Rand, 1971, RI), Adjusted Rand Index (Hubert and Arabie, 1985, ARI), Jaccard Index (JI) and, finally, precision and recall as done by Crabtree et al. (2005), together with their F1 harmonic mean. 2.3.2 Evaluation of the clustering diversity To evaluate the snippet clustering diversity the measures of S-recall@K and S-precision@r (Zhai et al., 2003) are calculated. These measures determine how many different meanings of a query are covered in the top-ranking results shown to the user. We calculate these measures on the output of the three different association metrics illustrated in Section 2.2. 3 A Full Example We now show a full example of usage of the WoSIT API. The code shown in Figure 3 initially obtains a search result dataset (line 1), sele</context>
</contexts>
<marker>Crabtree, Gao, Andreae, 2005</marker>
<rawString>Daniel Crabtree, Xiaoying Gao, and Peter Andreae. 2005. Improving web clustering by cluster selection. In Proc. of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence, pages 172– 178, Washington, DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Di Marco</author>
<author>Roberto Navigli</author>
</authors>
<title>Clustering Web Search Results with Maximum Spanning Trees.</title>
<date>2011</date>
<booktitle>In Proc. of the XIIth International Conference of the Italian Association for Artificial Intelligence (AI*IA),</booktitle>
<pages>201--212</pages>
<location>Palermo, Italy.</location>
<marker>Di Marco, Navigli, 2011</marker>
<rawString>Antonio Di Marco and Roberto Navigli. 2011. Clustering Web Search Results with Maximum Spanning Trees. In Proc. of the XIIth International Conference of the Italian Association for Artificial Intelligence (AI*IA), pages 201–212, Palermo, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Di Marco</author>
<author>Roberto Navigli</author>
</authors>
<title>Clustering and Diversifying Web Search Results with Graph-Based Word Sense Induction.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>3</issue>
<marker>Di Marco, Navigli, 2013</marker>
<rawString>Antonio Di Marco and Roberto Navigli. 2013. Clustering and Diversifying Web Search Results with Graph-Based Word Sense Induction. Computational Linguistics, 39(3):709–754.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Hubert</author>
<author>Phipps Arabie</author>
</authors>
<title>Comparing Partitions.</title>
<date>1985</date>
<journal>Journal of Classification,</journal>
<volume>2</volume>
<issue>1</issue>
<pages>218</pages>
<contexts>
<context position="15306" citStr="Hubert and Arabie, 1985" startWordPosition="2337" endWordPosition="2340">nippetAssociator.getInstance(); 9 SnippetClustering snippetClustering = snippetAssociator.associateSnippet( 10 targetWord, searchResults, mst.getClustering(), AssociationMetric.WORD_OVERLAP); 11 snippetClustering.export(&amp;quot;output/outputMST.txt&amp;quot;, true); 12 } 13 WSIEvaluator.evaluate(searchResults, &amp;quot;output/outputMST.txt&amp;quot;); Figure 3: An example of evaluation code for the B-MST clustering algorithm. 2.3.1 Evaluation of the clustering quality The quality of the output produced by semantically-enhanced search result clustering is evaluated in terms of Rand Index (Rand, 1971, RI), Adjusted Rand Index (Hubert and Arabie, 1985, ARI), Jaccard Index (JI) and, finally, precision and recall as done by Crabtree et al. (2005), together with their F1 harmonic mean. 2.3.2 Evaluation of the clustering diversity To evaluate the snippet clustering diversity the measures of S-recall@K and S-precision@r (Zhai et al., 2003) are calculated. These measures determine how many different meanings of a query are covered in the top-ranking results shown to the user. We calculate these measures on the output of the three different association metrics illustrated in Section 2.2. 3 A Full Example We now show a full example of usage of the</context>
</contexts>
<marker>Hubert, Arabie, 1985</marker>
<rawString>Lawrence Hubert and Phipps Arabie. 1985. Comparing Partitions. Journal of Classification, 2(1):193– 218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic Retrieval and Clustering of Similar Words.</title>
<date>1998</date>
<booktitle>In Proc. of the 171h International Conference on Computational linguistics (COLING),</booktitle>
<pages>768--774</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="8143" citStr="Lin, 1998" startWordPosition="1268" endWordPosition="1269"> identifying basic meanings for the input query. • Chinese Whispers (Biemann, 2006), a randomized algorithm which partitions nodes by means of the iterative transfer of word sense information across the co-occurrence graph (Biemann, 2006). • Squares, Triangles and Diamonds (SquaT++) (Di Marco and Navigli, 2013), an extension of the SquaT algorithm (Navigli and Crisafulli, 2010) which exploits three cyclic graph patterns to determine and discard those vertices (or edges) with weak degree of connectivity in the graph. We also provide an implementation of a word clustering algorithm, i.e. Lin98 (Lin, 1998), which does not rely on co-occurrence graphs, but just on the word co-occurrence information to iteratively refine word clusters on the basis of their “semantic” relationships. A programmatic example of use of the B-MST WSI algorithm is as follows: BMST mst = new BMST(g); mst.makeClustering(); Clustering wordClusters = mst.getClustering(); where g is a co-occurrence graph created as explained in Section 2.1.1, provided as input to the constructor of the algorithm’s class. The makeClustering method implements the induction algorithm and creates the word clusters, which can then be retrieved ca</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic Retrieval and Clustering of Similar Words. In Proc. of the 171h International Conference on Computational linguistics (COLING), pages 768–774, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Giuseppe Crisafulli</author>
</authors>
<title>Inducing Word Senses to Improve Web Search Result Clustering.</title>
<date>2010</date>
<booktitle>In Proc. of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>116--126</pages>
<location>Boston, USA.</location>
<contexts>
<context position="2303" citStr="Navigli and Crisafulli, 2010" startWordPosition="354" endWordPosition="357">ution consists of clustering Web search results by way of clustering engines such as Carrot1 and Yippy2 and presenting them to the user grouped by topic. 1http://search.carrot2.org 2http://yippy.com Diversification and Web clustering algorithms, however, do not perform any semantic analysis of search results, clustering them solely on the basis of their lexical similarity. Recently, it has been shown that the automatic acquisition of the meanings of a word of interest, a task referred to as Word Sense Induction, can be successfully integrated into search result clustering and diversification (Navigli and Crisafulli, 2010; Di Marco and Navigli, 2013) so as to outperform non-semantic state-of-the-art Web clustering systems. In this demonstration we describe a new toolkit for Word Sense Induction, called WoSIT, which i) provides ready implementations of existing WSI algorithms; ii) can be extended with additional WSI algorithms; iii) enables the integration of WSI algorithms into search result clustering and diversification, thereby providing an extrinsic evaluation tool. As a result the toolkit enables the objective comparison of WSI algorithms within an end-user application in terms of the degree of diversific</context>
<context position="7913" citStr="Navigli and Crisafulli, 2010" startWordPosition="1230" endWordPosition="1233"> based on the calculation of a Maximum Spanning Tree (Di Marco and Navigli, 2011) aimed at balancing the number of co-occurrences in each sense cluster. • HyperLex (V´eronis, 2004), an algorithm which identifies hubs in co-occurrence graphs, thereby identifying basic meanings for the input query. • Chinese Whispers (Biemann, 2006), a randomized algorithm which partitions nodes by means of the iterative transfer of word sense information across the co-occurrence graph (Biemann, 2006). • Squares, Triangles and Diamonds (SquaT++) (Di Marco and Navigli, 2013), an extension of the SquaT algorithm (Navigli and Crisafulli, 2010) which exploits three cyclic graph patterns to determine and discard those vertices (or edges) with weak degree of connectivity in the graph. We also provide an implementation of a word clustering algorithm, i.e. Lin98 (Lin, 1998), which does not rely on co-occurrence graphs, but just on the word co-occurrence information to iteratively refine word clusters on the basis of their “semantic” relationships. A programmatic example of use of the B-MST WSI algorithm is as follows: BMST mst = new BMST(g); mst.makeClustering(); Clustering wordClusters = mst.getClustering(); where g is a co-occurrence </context>
<context position="12427" citStr="Navigli and Crisafulli (2010)" startWordPosition="1938" endWordPosition="1942">dinalitySorter: sorts the clusters according to their size, i.e. the number of vertices in the cluster; • MeanSimilaritySorter: sorts the clusters according to the average association score between the snippets in the cluster and the backing word cluster (defined by the selected association metrics). Notably, the end user can then implement his or her own custom sorting procedure by simply extending the Sorter class. 2.2.1 Search Result Datasets The framework comes with two search result datasets of ambiguous queries: the AMBIENT+MORESQUE dataset made available by Bernardini et al. (2009) and Navigli and Crisafulli (2010), respectively, and the SemEval-2013- Task11 dataset.3 New result datasets can be provided by users complying with the dataset format described below. 3For details visit http://lcl.uniroma1.it/ wosit/. A search result dataset in WoSIT is made up of at least two files: • topics.txt, which contains the queries (topics) of interest together with their numeric ids. For instance: id description 1 polaroid 2 kangaroo 3 shakira ... ... • results.txt, which lists the search results for each given query, in terms of URL, page title and page snippet: IDurl title snippet 1.1 http://www.polaroid.com/ Pola</context>
</contexts>
<marker>Navigli, Crisafulli, 2010</marker>
<rawString>Roberto Navigli and Giuseppe Crisafulli. 2010. Inducing Word Senses to Improve Web Search Result Clustering. In Proc. of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 116–126, Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Daniele Vannella</author>
</authors>
<title>SemEval-2013 Task 11: Evaluating Word Sense Induction &amp; Disambiguation within An End-User Application.</title>
<date>2013</date>
<booktitle>In Proc. of the 71h International Workshop on Semantic Evaluation (SemEval 2013), in conjunction with the Second Joint Conference on Lexical and Computational Semantics (*SEM</booktitle>
<pages>193--201</pages>
<location>Atlanta, USA.</location>
<contexts>
<context position="13918" citStr="Navigli and Vannella (2013)" startWordPosition="2167" endWordPosition="2170">evaluation of the search result clustering and diversification output, two additional files have to be provided: • subTopics.txt, which for each query provides the list of meanings for that query, e.g.: • STRel.txt, which provides the manual associations between each search result and the most suitable meaning as provided in the subTopics.txt file. For instance: subTopicID resultID 1.1 1.1 1.1 1.2 1.1 1.3 ... ... 2.3 WSI Evaluator As shown in Figure 1 the final component of our workflow is the evaluation of WSI when integrated into search result clustering and diversification (already used by Navigli and Vannella (2013)). This component, called the WSI Evaluator, takes as input the snippet clusters obtained for a given query together with the fully annotated search result dataset, as described in the previous section. Two kinds of evaluations are carried out, described in what follows. ID description 1.1 Polaroid Corporation, a multinational con... 1.2 Instant film photographs are sometimes kn... 1.3 Instant camera (or Land camera), sometime... ... ... 70 1 Dataset searchResults = Dataset.getInstance(); 2 DBConfiguration db = DBConfiguration.getInstance(); 3 for(String targetWord : dataset.getQueries()) 4 { </context>
</contexts>
<marker>Navigli, Vannella, 2013</marker>
<rawString>Roberto Navigli and Daniele Vannella. 2013. SemEval-2013 Task 11: Evaluating Word Sense Induction &amp; Disambiguation within An End-User Application. In Proc. of the 71h International Workshop on Semantic Evaluation (SemEval 2013), in conjunction with the Second Joint Conference on Lexical and Computational Semantics (*SEM 2013), pages 193–201, Atlanta, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William M Rand</author>
</authors>
<title>Objective criteria for the evaluation of clustering methods.</title>
<date>1971</date>
<journal>Journal of the American Statistical association,</journal>
<pages>66--336</pages>
<contexts>
<context position="15255" citStr="Rand, 1971" startWordPosition="2331" endWordPosition="2332">nippetAssociator snippetAssociator = SnippetAssociator.getInstance(); 9 SnippetClustering snippetClustering = snippetAssociator.associateSnippet( 10 targetWord, searchResults, mst.getClustering(), AssociationMetric.WORD_OVERLAP); 11 snippetClustering.export(&amp;quot;output/outputMST.txt&amp;quot;, true); 12 } 13 WSIEvaluator.evaluate(searchResults, &amp;quot;output/outputMST.txt&amp;quot;); Figure 3: An example of evaluation code for the B-MST clustering algorithm. 2.3.1 Evaluation of the clustering quality The quality of the output produced by semantically-enhanced search result clustering is evaluated in terms of Rand Index (Rand, 1971, RI), Adjusted Rand Index (Hubert and Arabie, 1985, ARI), Jaccard Index (JI) and, finally, precision and recall as done by Crabtree et al. (2005), together with their F1 harmonic mean. 2.3.2 Evaluation of the clustering diversity To evaluate the snippet clustering diversity the measures of S-recall@K and S-precision@r (Zhai et al., 2003) are calculated. These measures determine how many different meanings of a query are covered in the top-ranking results shown to the user. We calculate these measures on the output of the three different association metrics illustrated in Section 2.2. 3 A Full</context>
</contexts>
<marker>Rand, 1971</marker>
<rawString>William M. Rand. 1971. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical association, 66(336):846–850.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Sanderson</author>
</authors>
<title>Ambiguous queries: test collections need more sense.</title>
<date>2008</date>
<booktitle>In Proc. of the 31st annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>499--506</pages>
<contexts>
<context position="1284" citStr="Sanderson, 2008" startWordPosition="195" endWordPosition="196"> Web is by far the world’s largest information archive, whose content – made up of billions of Web pages – is growing exponentially. Unfortunately the retrieval of any given piece of information is an arduous task which challenges even prominent search engines such as those developed by Google, Yahoo! and Microsoft. Even today, such systems still find themselves up against the lexical ambiguity issue, that is, the linguistic property due to which a single word may convey different meanings. It has been estimated that around 4% of Web queries and 16% of the most frequent queries are ambiguous (Sanderson, 2008). A major issue associated with the lexical ambiguity phenomenon on the Web is the low number of query words submitted by Web users to search engines. A possible solution to this issue is the diversification of search results obtained by maximizing the dissimilarity of the top-ranking Web pages returned to the user (Agrawal et al., 2009; Ashwin Swaminathan and Kirovski, 2009). Another solution consists of clustering Web search results by way of clustering engines such as Carrot1 and Yippy2 and presenting them to the user grouped by topic. 1http://search.carrot2.org 2http://yippy.com Diversific</context>
</contexts>
<marker>Sanderson, 2008</marker>
<rawString>Mark Sanderson. 2008. Ambiguous queries: test collections need more sense. In Proc. of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, pages 499–506, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean V´eronis</author>
</authors>
<title>HyperLex: lexical cartography for information retrieval.</title>
<date>2004</date>
<journal>Computer, Speech and Language,</journal>
<volume>18</volume>
<issue>3</issue>
<marker>V´eronis, 2004</marker>
<rawString>Jean V´eronis. 2004. HyperLex: lexical cartography for information retrieval. Computer, Speech and Language, 18(3):223–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ChengXiang Zhai</author>
<author>William W Cohen</author>
<author>John Lafferty</author>
</authors>
<title>Beyond independent relevance: Methods and evaluation metrics for subtopic retrieval.</title>
<date>2003</date>
<booktitle>In Proc. of the 26th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>10--17</pages>
<location>Toronto, Canada.</location>
<contexts>
<context position="15595" citStr="Zhai et al., 2003" startWordPosition="2381" endWordPosition="2384">, &amp;quot;output/outputMST.txt&amp;quot;); Figure 3: An example of evaluation code for the B-MST clustering algorithm. 2.3.1 Evaluation of the clustering quality The quality of the output produced by semantically-enhanced search result clustering is evaluated in terms of Rand Index (Rand, 1971, RI), Adjusted Rand Index (Hubert and Arabie, 1985, ARI), Jaccard Index (JI) and, finally, precision and recall as done by Crabtree et al. (2005), together with their F1 harmonic mean. 2.3.2 Evaluation of the clustering diversity To evaluate the snippet clustering diversity the measures of S-recall@K and S-precision@r (Zhai et al., 2003) are calculated. These measures determine how many different meanings of a query are covered in the top-ranking results shown to the user. We calculate these measures on the output of the three different association metrics illustrated in Section 2.2. 3 A Full Example We now show a full example of usage of the WoSIT API. The code shown in Figure 3 initially obtains a search result dataset (line 1), selects a database (line 2) and iterates over its queries (line 3). Next, a co-occurrence graph for the current query is created from a co-occurrence database (line 5) and an instance of the B-MST W</context>
</contexts>
<marker>Zhai, Cohen, Lafferty, 2003</marker>
<rawString>ChengXiang Zhai, William W. Cohen, and John Lafferty. 2003. Beyond independent relevance: Methods and evaluation metrics for subtopic retrieval. In Proc. of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10–17, Toronto, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>