<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.965301">
UTDHLT: COPACETIC System for Choosing Plausible Alternatives
</title>
<author confidence="0.997471">
Travis Goodwin, Bryan Rink, Kirk Roberts, Sanda M. Harabagiu
</author>
<affiliation confidence="0.996829">
Human Language Technology Research Institute
University of Texas Dallas
</affiliation>
<address confidence="0.971208">
Richardson TX, 75080
</address>
<email confidence="0.999819">
{travis,bryan,kirk,sanda}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.996134" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.98033825">
The Choice of Plausible Alternatives (COPA)
task in SemEval-2012 presents a series of
forced-choice questions wherein each question
provides a premise and two viable cause or ef-
fect scenarios. The correct answer is the cause
or effect that is the most plausible. This paper
describes the COPACETIC system developed
by the University of Texas at Dallas (UTD) for
this task. We approach this task by casting it
as a classification problem and using features
derived from bigram co-occurrences, TimeML
temporal links between events, single-word po-
larities from the Harvard General Inquirer, and
causal syntactic dependency structures within
the gigaword corpus. Additionally, we show
that although each of these components im-
proves our score for this evaluation, the dif-
ference in accuracy between using all of these
features and using bigram co-occurrence infor-
mation alone is not statistically significant.
</bodyText>
<sectionHeader confidence="0.977986" genericHeader="method">
1 The Problem
</sectionHeader>
<bodyText confidence="0.998943363636364">
“The surfer caught the wave.” This statement, al-
though almost tautological for human understanding,
requires a considerable depth of semantic reasoning.
What is a surfer? What does it mean to “catch a
wave”? How are these concepts related? What if
we want to ascertain, given that the surfer caught the
wave, whether the most likely next event is that “the
wave carried her to the shore” or that “she paddled her
board into the ocean”? This type of causal and tempo-
ral reasoning requires a breadth of world-knowledge,
often called commonsense understanding.
</bodyText>
<figure confidence="0.197929">
Question 15 (Find the EFFECT)
</figure>
<figureCaption confidence="0.532614333333333">
Premise: I poured water on my sleeping friend.
Alternative 1: My friend awoke.
Alternative 2: My friend snored.
Question 379 (Find the CAUSE)
Premise: The man closed the umbrella.
Alternative 1: He got out of the car.
Alternative 2: He approached the building.
Figure 1: An example of each type of question, one target-
ing an effect, and another targeting a cause.
</figureCaption>
<bodyText confidence="0.999357615384615">
The seventh task of SemEval-2012 evaluates pre-
cisely this type of cogitation. COPA: Choice of Plau-
sible Alternatives presents 1,0001 sets of two-choice
questions (presented as a premise and two alterna-
tives) provided in simple English sentences. The
goal for each question is to choose the most plausible
cause or effect entailed by the premise (the dataset
provided an equal distribution of cause and effect
targetting questions). Additionally, each question is
labeled so as to describe whether the answer should
be a cause or an effect, as indicated in Figure 1.
The topics of these questions were drawn from two
sources:
</bodyText>
<listItem confidence="0.999916166666667">
1. Randomly selected accounts of personal stories
taken from a collection of Internet weblogs (Gor-
don and Swanson, 2009).
2. Randomly selected subject terms from the Li-
brary of Congress Thesaurus for Graphic Mate-
rials (of Congress. Prints et al., 1980).
</listItem>
<bodyText confidence="0.726864">
Additionally, the incorrect alternatives were authored
</bodyText>
<footnote confidence="0.9660055">
1This data set was split into a 500 question development (or
training) set and a 500 question test set.
</footnote>
<page confidence="0.965091">
461
</page>
<note confidence="0.824793">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 461–466,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999848">
Figure 2: Architecture of the COPACETIC System
</figureCaption>
<bodyText confidence="0.903893">
with the intent of impeding “purely associative meth-
ods” (Roemmele et al., 2011). The task aims to
evaluate the state of commonsense causal reasoning
(Roemmele et al., 2011).
</bodyText>
<sectionHeader confidence="0.905265" genericHeader="method">
2 System Architecture
</sectionHeader>
<bodyText confidence="0.999786666666667">
Given a question, such as Question 15 (as shown
in Figure 1), our system selects the most plausible
alternative by using the output of an SVM classifier,
trained on the 500 provided development questions
and tested on the 500 provided test questions. The
classifier operates with features describing informa-
tion extracted from the processing of the question’s
premise and alternatives. As illustrated by Figure 2,
the preprocessing involves part of speech (POS) tag-
ging, and syntactic dependency parsing provided
by the Stanford parser (Klein and Manning, 2003;
Toutanova et al., 2003), multi-word expression detec-
tion using Wikipedia, automatic TimeML annotation
using TARSQI (Verhagen et al., 2005; Pustejovsky
et al., 2003), and Brown clustering as provided in
(Turian, 2010).
The architecture of the COPACETIC system is di-
vided into offline (independent of any question) and
online (question dependent) processing. The online
aspect of our system inspects each question using
an SVM and selects the most likely alternative. Our
system’s offline functions focus on pre-processing
resources so that they may be used by components
of the online aspect of our system. In the next sec-
tion, we describe the offline processing upon which
our system is built, and in the following section, the
online manner in which we evaluate each question.
</bodyText>
<subsectionHeader confidence="0.908842">
2.1 Offline Processing
</subsectionHeader>
<bodyText confidence="0.999983952380953">
Because the questions presented in this task require
a wealth of commonsense knowledge, we first ex-
tracted commonsense and temporal facts. This sub-
section describes the process of mining this informa-
tion from the fourth edition of the English Gigaword
corpus2 (Parker et al., 2009).
We collected commonsense facts by extracting
cause and effect pairs using twenty-four hand-crafted
patterns. Rather than lexical patterns, we used pat-
terns over syntactic dependency structures in order
to capture the syntactic role each word plays. Fig-
ure 3 illuminates two examples of the dependency
structures encoded by our causal patterns. Causal
Pattern 1 captures all cases of causality indicated by
the verb causes, while Causal Pattern 2 illustrates a
more sophisticated pattern, in which the phrasal verb
brought on indicates causality.
In order to extract this information, we first parsed
the syntactic dependence structure of each sentence
using the Stanford parser (Klein and Manning, 2003).
Next, we loaded each sentence’s dependence tree
</bodyText>
<footnote confidence="0.9975465">
2The LDC Catalog number of the English Gigaword Fourth
Edition corpus is LDC2009T13.
</footnote>
<page confidence="0.997423">
462
</page>
<figureCaption confidence="0.997422666666667">
Figure 3: The dependency structures associated with
the causal patterns: ?cause “causes” ?effect, and
?cause “brought on” ?effect.
</figureCaption>
<bodyText confidence="0.999968933333333">
into the RDF3X (Neumann and Weikum, 2008)
implementation of an RDF3 database. Then, we
represented our dependency structures using in the
SPARQL4query language and extracted cause and
effect pairs by issuing SPARQL queries against the
RDF3X database. We used SPARQL and RDF repre-
sentations because they allowed us to easily represent
and reason over graphical structures, such as those of
our dependency trees.
It has been shown that causality often manifests as
a temporal relation (Bethard, 2008; Bethard and Mar-
tin, 2008). The questions presented in this task are
no exception: many of the alternative-premise pairs
necessitate temporal understanding. For example,
consider question 63 provided in Figure 4.
</bodyText>
<figure confidence="0.17795">
Question 63 (Find the EFFECT)
</figure>
<figureCaption confidence="0.7648076">
Premise: The man removed his coat.
Alternative 1: He entered the house.
Alternative 2: He loosened his tie.
Figure 4: Example question 63, which illustrates the ne-
cessity for temporal reasoning.
</figureCaption>
<footnote confidence="0.795257428571429">
3The Resource Description Framework (RDF) is is a spec-
ification from the W3C. Information on RDF is available at
http://www.w3.org/RDF/.
3The SPARQL Query Language is defined at http://www.
w3.org/TR/rdf-sparql-query/. An examples of the
WHERE clause for a SPARQL query associated with the brought
on pattern from Figure 3 is provided below:
</footnote>
<equation confidence="0.6185982">
I ?a &lt;nsubj&gt; ?cause ;
&lt;token&gt; &amp;quot;brought&amp;quot; ;
&lt;prep&gt; ?b .
?b &lt;token&gt; &amp;quot;on&amp;quot; ;
&lt;pobj&gt; ?effect . }
</equation>
<bodyText confidence="0.999964333333333">
In order to extract this temporal information, we
automatically annotated our corpus with TimeML
annotations using the TARSQI Toolkit (Verhagen
et al., 2005). Unfortunately, the events represented
in this corpus were too sparse to use directly. To
mitigate this sparsity, we clustered events using the
3,200 Brown clusters5 described in (Turian, 2010).
After all such offline processing has been com-
pleted, we incorporate the knowledge encoded by
this processing in the online components of our sys-
tem (online preprocessing, and feature extraction) as
described in the following section.
</bodyText>
<subsectionHeader confidence="0.97679">
2.2 Online Processing
</subsectionHeader>
<bodyText confidence="0.999899384615385">
We cast the task of selecting the most plausible al-
ternative as a classification problem, using a support
vector machine (SVM) supervised classifier (using
a linear kernel). To this end, we pre-process each
question for lexical information. We extract parts
of speech (POS) and syntactic dependencies using
the Stanford CoreNLP parser (Klein and Manning,
2003; Toutanova et al., 2003). Stopwords are re-
moved using a manually curated list of one hundred
and one common stopwords; non-content words (de-
fined as words whose POS is not a noun, verb, or
adjective) are also discarded. Additionally, we ex-
tract multi-word expressions (noun collocations6 and
phrasal verbs7). Finally, in order to utilize our of-
fline TimeML annotations, we extract events using
POS. Examples of the retained content words are
underlined in Figures 5, 6, 7 and 8.
After preprocessing each question, we convert
it into two premise-alternative pairs (PREMISE-
ALTERNATIVE1, and PREMISE-ALTERNATIVE2).
For each of these pairs, we attempt to form a bridge
from the causal sentence to the effect sentence, with-
out distinction over whether the cause or effect origi-
nated from the premise or the alternative. This bridge
is provided by four measures, or features, described
in the following section.
</bodyText>
<footnote confidence="0.97259475">
5These clusters are available at http://metaoptimize.
com/projects/wordreprs/.
6These were detected using a list of English Wikipedia ar-
ticle titles available at http://dumps.wikimedia.org/
backup-index.html.
7Phrasal verbs were determined using a list avail-
able at http://www.learn-english-today.com/
phrasal-verbs/phrasal-verb-list.htm.
</footnote>
<figure confidence="0.983935357142857">
CAUSAL PATTERN 1:
&amp;quot;causes&amp;quot;
nsubj
?cause
dobj
?effect
CAUSAL PATTERN 2:
?cause
&amp;quot;brought&amp;quot;
nsubj
?effect
prep
&amp;quot;on&amp;quot;
pobj
</figure>
<page confidence="0.999343">
463
</page>
<sectionHeader confidence="0.938963" genericHeader="method">
3 The Features of the COPACETIC
System
</sectionHeader>
<bodyText confidence="0.999987428571429">
In determining the causal relatedness between a cause
and an effect sentence, we utilize four features. Each
feature calculates a value indicating the perceived
strength of the causal relationship between a cause
and an effect using a different measure of causality.
The four features used by our COPACETIC system
are described in the following subsections.
</bodyText>
<subsectionHeader confidence="0.99833">
3.1 Bigram Relatedness
</subsectionHeader>
<bodyText confidence="0.999807">
Our first feature measures the degree of relatedness
between all pairs of bigrams (at the token level) in the
cause and effect pair. We do this by calculating the
point-wise mutual Information (PMI) (Fano, 1961)
for all bigram combinations between the candidate
alternative and its premise in the English Gigaword
corpus (Parker et al., 2009) as shown in Equation 1.
</bodyText>
<equation confidence="0.9965205">
PMI(x; y) - log p(x, y) (1)
p(x)p(y)
</equation>
<bodyText confidence="0.999424875">
Under the assumption that distance words are un-
likely to causally influence each other, we only con-
sider co-occurrences within a window of one hundred
tokens when calculating the joint probability of the
PMI. Additionally, we allow for up to two tokens
to occur within a single bigram’s occurrence (e.g.
the phrase pierced her ears would be considered a
match for the bigram pierced ears ). Although these
relaxations skew the values of our calculated PMIs
by artificially lowering the joint probability, we are
only concerned with how the values compare to each
other. Note that because we employ no smoothing,
the PMI of an unseen bigram is set to zero. The max-
imum PMI over all pairs of bigrams is retained as the
value for this feature. Figure 5 illustrates this feature
for Question 495.
</bodyText>
<subsectionHeader confidence="0.998452">
3.2 Temporal Relatedness
</subsectionHeader>
<bodyText confidence="0.999372625">
Although most of the questions in this task focus on
causal relationships, for many questions, the nature
of this causal relationship manifests instead as a tem-
poral one (Bethard and Martin, 2008; Bethard, 2008).
We use temporal link information from TimeML
(Pustejovsky et al., 2005; Pustejovsky et al., 2003)
annotations on our corpus to determine how tempo-
rally related a given cause and effect sentence are.
</bodyText>
<table confidence="0.9688379">
Question 495 (Find the EFFECT)
Premise: The girl wanted to wear earrings.
Alternative 1: She got her ears pierced.
Alternative 2: She got a tattoo.
Alternative 1 Alternative 2
PMI(wear earrings, pierced ears) = -10.928 PMI(wear earrings, tattoo) = -12.77
PMI(wanted wear, pierced ears) = -13.284 PMI(wanted wear, tattoo) = -14.284
PMI(girl wanted, pierced ears) = -13.437 PMI(girl wanted, tattoo) = -14.762
PMI(girl, pierced ears) = -15.711 PMI(girl, tattoo) = -14.859
Maximum PMI = -10.928 Maximum PMI = -12.77
</table>
<figureCaption confidence="0.755968333333333">
Figure 5: Example PMI values for bigrams and unigrams
(with content words underlined). Alternative 1 is correctly
chosen as it has largest maxi mum PMI.
</figureCaption>
<bodyText confidence="0.99881075">
This is accomplished by using the point-wise mutual
information (PMI) between all pairs of events from
the cause to the effect (see Equation 1). We define
the relevant probabilities as follows:
</bodyText>
<listItem confidence="0.9571677">
• The joint probability (P(x, y)) of a cause and
effect event is defined as the number of times
the cause event participates in a temporal link
ending with the effect event.
• The probability of a cause event (P(x)) is de-
fined as the number of times the cause event
precipitates a temporal link to any event.
• The probability of an effect event (P(y)) is de-
fined as the number of times the effect event
ends a temporal link begun by any event.
</listItem>
<bodyText confidence="0.9993438">
We define the PMI to be zero for any unseen pair of
events (and for any pairs involving an unseen event).
The summation of all pairs of PMIs is used as the
value of this feature. Figure 6 shows how this feature
behaves.
</bodyText>
<figure confidence="0.283467">
Question 468 (Find the CAUSE)
Premise: The dog barked.
</figure>
<figureCaption confidence="0.818896">
Alternative 1: The cat lounged on the couch.
Alternative 2: A knock sounded at the door.
</figureCaption>
<figure confidence="0.692536">
Alternative 1 Alternative 2
PMI(lounge, bark) = 5.60436 PMI(knock, bark) = 5.77867
PMI(sound, bark) = 5.26971
</figure>
<figureCaption confidence="0.99103">
Figure 6: Example temporal PMI values (with content
words underlined). Alternative 2 is correctly chosen as it
has the highest summation.
</figureCaption>
<subsectionHeader confidence="0.999671">
3.3 Causal Dependency Structures
</subsectionHeader>
<bodyText confidence="0.999837666666667">
We attempted to capture the degree of direct causal re-
latedness between a cause sentence and an effect sen-
tence. To determine the strength of this relationship,
</bodyText>
<page confidence="0.999012">
464
</page>
<bodyText confidence="0.9971185">
we considered how often phrases from the cause and
effect sentences occur within a causal dependency
structure. We detect this through the use of twenty-
four8 manually crafted causal patterns (described in
Section 2.1). The alternative that has the maximum
number of matched dependency structures with the
premise is retained as the correct choice. Figure 7
illustrates this feature.
</bodyText>
<table confidence="0.718768833333333">
Question 490 (Find the EFFECT)
Premise: The man won the lottery.
Alternative 1: He became rich.
Alternative 2: He owed money.
Alternative 1 Alternative 2
won → rich = 15 won → owed = 5
</table>
<figureCaption confidence="0.889783">
Figure 7: Example casual dependency matches (with con-
tent words underlined). Alternative 1 is correctly selected
because more patterns extracted “won” causing “rich” than
“won” causing “owed”.
</figureCaption>
<subsectionHeader confidence="0.995694">
3.4 Polarity Comparison
</subsectionHeader>
<bodyText confidence="0.999863541666667">
We observed that many of the questions involve the
dilemma of determining whether a positive premise
is more related to a positive or negative alternative
(and vice-versa). This differs from sentiment analysis
in that rather than determining if a sentence expresses
a negative statement or view, we instead desire the
overall sentimental connotation of a sentence (and
thus of each word). For example, the premise from
Question 494 (Figure 8) is “the woman became fa-
mous.” Although this sentence makes no positive or
negative claims about the woman, the word “famous”
– when considered on its own – implies positive con-
notations.
We capture this information using the Harvard
General Inquirer (Stone et al., 1966). Originally de-
veloped in 1966, the Harvard General Inquirer pro-
vides a mapping from English words to their polarity
(POSITIVE, or NEGATIVE). For example, it de-
notes the word “abandon” as NEGATIVE, and the
word “abound” as POSITIVE. We use this informa-
tion by summing the score for all words in a sen-
tence (assigning POSITIVE words a score of 1.0,
NEGATIVE words a score of -1.0, and NEUTRAL or
unseen words a score of 0.0). The difference between
</bodyText>
<footnote confidence="0.628078">
8Twenty-four patterns was deemed sufficient due to time
constraints.
</footnote>
<bodyText confidence="0.993367666666667">
these scores between the cause sentence and the ef-
fect sentence is used as the value of this feature. This
feature is illustrated in Figure 8.
</bodyText>
<table confidence="0.959603625">
Question 494 (Find the CAUSE)
Premise: The woman became famous.
Alternative 1: Photographers followed her.
Alternative 2: Her family avoided her.
Premise Alternative 1 Alternative 2
famous POSITIVE 1.0 follow NEUTRAL 0.0
photographer NEUTRAL 0.0
Sum 1.0 Sum 0.0
</table>
<figureCaption confidence="0.990315333333333">
Figure 8: Example polarity comparison (with content
words underlined). Alternative 1 is correctly chosen as it
has the least difference from the score of the premise.
</figureCaption>
<sectionHeader confidence="0.999928" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999830947368421">
The COPA task of SemEval-2012 provided partici-
pants with 1,000 causal questions, divided into 500
questions for development or training, and 500 ques-
tions for testing. We submitted two systems to the
COPA Evaluation for SemEval-2012, both of which
are trained on the 500 development questions. Our
first system uses only the bigram PMI feature and is
denoted as bigram pmi. Our second system uses
all four features and is denoted as svm combined.
The accuracy of our two systems on the 500 provided
test questions is provided in Table 1 (Gordon et al.,
2012). On this task, accuracy is defined as the quo-
tient of dividing the number of questions for which
the correct alternative was chosen by the number of
questions. Although multiple groups registered, ours
were the only submitted results. Note that the differ-
ence in performance between our two systems is not
statistically significant (p = 0.411) (Gordon et al.,
2012).
</bodyText>
<table confidence="0.997012333333333">
Team ID System ID Score
UTDHLT bigram pmi 0.618
UTDHLT svm combined 0.634
</table>
<tableCaption confidence="0.999909">
Table 1: Accuracy of submitted systems
</tableCaption>
<bodyText confidence="0.9999035">
The primary hindrance to our approach is in com-
bining each feature – that is, determining the con-
fidence of each feature’s judgement. Because the
questions vary significantly in their subject matter
and the nature of the causal relationship between
given causes and effects, a single approach is unlikely
</bodyText>
<figure confidence="0.907562">
avoid NEGATIVE −1.0
family NEUTRAL 0.0
Sum −1.0
</figure>
<page confidence="0.999308">
465
</page>
<bodyText confidence="0.99971075">
to satisfy all scenarios. Unfortunately, the problem
of determining which feature best applies to a give
question requires non-trivial reasoning over implicit
semantics between the premise and alternatives.
</bodyText>
<sectionHeader confidence="0.997707" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999929153846154">
This evaluation has shown that although common-
sense causal reasoning is trivial for humans, it belies
deep semantic reasoning and necessitates a breadth of
world knowledge. Additional progress towards cap-
turing world knowledge by leveraging a large number
of cross-domain knowledge resources is necessary.
Moreover, distilling information not specific to any
domain – that is, a means of inferring basic and fun-
damental information about the world – is not only
necessary but paramount to the success of any fu-
ture system desiring to build chains of commonsense
or causal reasoning. At this point, we are merely
approximating such possible distillation.
</bodyText>
<sectionHeader confidence="0.998745" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999964666666667">
We would like to thank the organizers of SemEval-
2012 task 7 for their work constructing the dataset
and overseeing the task.
</bodyText>
<sectionHeader confidence="0.997537" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999087461538462">
[Bethard and Martin2008] S. Bethard and J.H. Martin.
2008. Learning semantic links from a corpus of parallel
temporal and causal relations. Proceedings of the 46th
Annual Meeting of the ACL-HLT.
[Bethard2008] S Bethard. 2008. Building a corpus of
temporal-causal structure. Proceedings of the Sixth
LREC.
[Fano1961] RM Fano. 1961. Transmission of Information:
A Statistical Theory of Communication.
[Gordon and Swanson2009] A. Gordon and R. Swanson.
2009. Identifying personal stories in millions of weblog
entries. In Third International Conference on Weblogs
and Social Media, Data Challenge Workshop, San Jose,
CA.
[Gordon et al.2012] Andrew Gordon, Zornitsa Kozareva,
and Melissa Roemmele. 2012. (2012) SemEval-2012
Task 7: Choice of Plausible Alternatives: An Evalua-
tion of Commonsense Causal Reasoning. In Proceed-
ings of the 6th International Workshop on Semantic
Evaluation (SemEval 2012), Montreal.
[Klein and Manning2003] D. Klein and C.D. Manning.
2003. Accurate unlexicalized parsing. In Proceed-
ings of the 41st Annual Meeting on Association for
Computational Linguistics-Volume 1, pages 423–430.
Association for Computational Linguistics.
[Neumann and Weikum2008] Thomas Neumann and Ger-
hard Weikum. 2008. RDF-3X: a RISC-style engine for
RDF. Proceedings of the VLDB Endowment.
[of Congress. Prints et al.1980] Library
of Congress. Prints, Photographs Division, and
E.B. Parker. 1980. Subject headings used in the library
of congress prints and photographs division. Prints and
Photographs Division, Library of Congress.
[Parker et al.2009] Robert Parker, David Graff, Junbo
Kong, Ke Chen, and Kazuaki Maeda. 2009. English
Gigaword Fourth Edition.
[Pustejovsky et al.2003] J Pustejovsky, J Castano, and
R Ingria. 2003. TimeML: Robust specification of
event and temporal expressions in text. AAAI Spring
Symposium on New Directions in Question-Answering.
[Pustejovsky et al.2005] J Pustejovsky, Bob Ingria, Roser
Sauri, Jose Castano, Jessica Littman, Rob Gaizauskas,
Andrea Setzer, G. Katz, and I. Mani. 2005. The speci-
fication language TimeML. The Language of Time: A
Reader.
[Roemmele et al.2011] Melissa Roemmele, Cos-
min Adrian Bejan, and Andrew S. Gordon. 2011.
Choice of Plausible Alternatives: An Evaluation of
Commonsense Causal Reasoning. 2011 AAAI Spring
Symposium Series.
[Stone et al.1966] P. J. Stone, D.C. Dunphy, and M. S.
Smith. 1966. The General Inquirer: A Computer
Approach to Content Analysis. MIT Press.
[Toutanova et al.2003] K. Toutanova, D. Klein, C.D. Man-
ning, and Y. Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proceed-
ings of the 2003 Conference of NAACL-HLT, pages
173–180. Association for Computational Linguistics.
[Turian2010] J Turian. 2010. Word representations: a sim-
ple and general method for semi-supervised learning.
Proceedings of the 48th Annual Meeting of the ACL,
pages 384–394.
[Verhagen et al.2005] M Verhagen, I Mani, and R Sauri.
2005. Automating Temporal Annotation with TARSQI.
In Proceedings of the ACL 2005, pages 81–84.
</reference>
<page confidence="0.999596">
466
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000682">
<title confidence="0.999796">UTDHLT: COPACETIC System for Choosing Plausible Alternatives</title>
<author confidence="0.991269">Travis Goodwin</author>
<author confidence="0.991269">Bryan Rink</author>
<author confidence="0.991269">Kirk Roberts</author>
<author confidence="0.991269">M Sanda</author>
<affiliation confidence="0.859186666666667">Human Language Technology Research University of Texas Richardson TX,</affiliation>
<abstract confidence="0.964293153846154">The Choice of Plausible Alternatives (COPA) task in SemEval-2012 presents a series of forced-choice questions wherein each question provides a premise and two viable cause or effect scenarios. The correct answer is the cause or effect that is the most plausible. This paper describes the COPACETIC system developed by the University of Texas at Dallas (UTD) for this task. We approach this task by casting it as a classification problem and using features derived from bigram co-occurrences, TimeML temporal links between events, single-word polarities from the Harvard General Inquirer, and causal syntactic dependency structures within the gigaword corpus. Additionally, we show that although each of these components improves our score for this evaluation, the difference in accuracy between using all of these features and using bigram co-occurrence information alone is not statistically significant. 1 The Problem “The surfer caught the wave.” This statement, although almost tautological for human understanding, requires a considerable depth of semantic reasoning. What is a surfer? What does it mean to “catch a wave”? How are these concepts related? What if we want to ascertain, given that the surfer caught the wave, whether the most likely next event is that “the wave carried her to the shore” or that “she paddled her board into the ocean”? This type of causal and temporal reasoning requires a breadth of world-knowledge, often called commonsense understanding. 15 (Find the Premise: I poured water on my sleeping Alternative 1: My friend awoke. Alternative 2: My friend snored. 379 (Find the Premise: The man closed the Alternative 1: He got out of the car. Alternative 2: He approached the building. Figure 1: An example of each type of question, one targeting an effect, and another targeting a cause. The seventh task of SemEval-2012 evaluates precisely this type of cogitation. COPA: Choice of Plau- Alternatives presents sets of two-choice questions (presented as a premise and two alternatives) provided in simple English sentences. The goal for each question is to choose the most plausible cause or effect entailed by the premise (the dataset provided an equal distribution of cause and effect targetting questions). Additionally, each question is labeled so as to describe whether the answer should be a cause or an effect, as indicated in Figure 1. The topics of these questions were drawn from two sources: 1. Randomly selected accounts of personal stories taken from a collection of Internet weblogs (Gordon and Swanson, 2009). 2. Randomly selected subject terms from the Library of Congress Thesaurus for Graphic Materials (of Congress. Prints et al., 1980). Additionally, the incorrect alternatives were authored data set was split into a 500 question development (or training) set and a 500 question test set.</abstract>
<note confidence="0.565191">461 Joint Conference on Lexical and Computational Semantics pages 461–466, Canada, June 7-8, 2012. Association for Computational Linguistics Figure 2: Architecture of the COPACETIC System</note>
<abstract confidence="0.982069963585435">with the intent of impeding “purely associative methods” (Roemmele et al., 2011). The task aims to evaluate the state of commonsense causal reasoning (Roemmele et al., 2011). 2 System Architecture Given a question, such as Question 15 (as shown in Figure 1), our system selects the most plausible alternative by using the output of an SVM classifier, trained on the 500 provided development questions and tested on the 500 provided test questions. The classifier operates with features describing information extracted from the processing of the question’s premise and alternatives. As illustrated by Figure 2, the preprocessing involves part of speech (POS) tagging, and syntactic dependency parsing provided by the Stanford parser (Klein and Manning, 2003; Toutanova et al., 2003), multi-word expression detection using Wikipedia, automatic TimeML annotation using TARSQI (Verhagen et al., 2005; Pustejovsky et al., 2003), and Brown clustering as provided in (Turian, 2010). The architecture of the COPACETIC system is divided into offline (independent of any question) and online (question dependent) processing. The online aspect of our system inspects each question using an SVM and selects the most likely alternative. Our system’s offline functions focus on pre-processing resources so that they may be used by components of the online aspect of our system. In the next section, we describe the offline processing upon which our system is built, and in the following section, the online manner in which we evaluate each question. 2.1 Offline Processing Because the questions presented in this task require a wealth of commonsense knowledge, we first extracted commonsense and temporal facts. This subsection describes the process of mining this information from the fourth edition of the English Gigaword (Parker et al., 2009). We collected commonsense facts by extracting cause and effect pairs using twenty-four hand-crafted patterns. Rather than lexical patterns, we used patterns over syntactic dependency structures in order to capture the syntactic role each word plays. Figure 3 illuminates two examples of the dependency structures encoded by our causal patterns. Causal Pattern 1 captures all cases of causality indicated by verb while Causal Pattern 2 illustrates a more sophisticated pattern, in which the phrasal verb on causality. In order to extract this information, we first parsed the syntactic dependence structure of each sentence using the Stanford parser (Klein and Manning, 2003). Next, we loaded each sentence’s dependence tree LDC Catalog number of the English Gigaword Fourth Edition corpus is LDC2009T13. 462 Figure 3: The dependency structures associated with causal patterns: and on” into the RDF3X (Neumann and Weikum, 2008) of an database. Then, we represented our dependency structures using in the language and extracted cause and effect pairs by issuing SPARQL queries against the RDF3X database. We used SPARQL and RDF representations because they allowed us to easily represent and reason over graphical structures, such as those of our dependency trees. It has been shown that causality often manifests as a temporal relation (Bethard, 2008; Bethard and Martin, 2008). The questions presented in this task are no exception: many of the alternative-premise pairs necessitate temporal understanding. For example, consider question 63 provided in Figure 4. 63 (Find the Premise: The man removed his Alternative 1: He entered the house. Alternative 2: He loosened his tie. Figure 4: Example question 63, which illustrates the necessity for temporal reasoning. Resource Description Framework (RDF) is is a specification from the W3C. Information on RDF is available at http://www.w3.org/RDF/. SPARQL Query Language is defined at An examples of the for a SPARQL query associated with the from Figure 3 is provided below: &lt;nsubj&gt; ?cause ; &lt;token&gt; &amp;quot;brought&amp;quot; ; &lt;prep&gt; ?b . ?b &lt;token&gt; &amp;quot;on&amp;quot; ; ?effect . In order to extract this temporal information, we automatically annotated our corpus with TimeML annotations using the TARSQI Toolkit (Verhagen et al., 2005). Unfortunately, the events represented in this corpus were too sparse to use directly. To mitigate this sparsity, we clustered events using the Brown described in (Turian, 2010). After all such offline processing has been completed, we incorporate the knowledge encoded by this processing in the online components of our system (online preprocessing, and feature extraction) as described in the following section. 2.2 Online Processing We cast the task of selecting the most plausible alternative as a classification problem, using a support vector machine (SVM) supervised classifier (using a linear kernel). To this end, we pre-process each question for lexical information. We extract parts of speech (POS) and syntactic dependencies using the Stanford CoreNLP parser (Klein and Manning, 2003; Toutanova et al., 2003). Stopwords are removed using a manually curated list of one hundred and one common stopwords; non-content words (defined as words whose POS is not a noun, verb, or adjective) are also discarded. Additionally, we exmulti-word expressions (noun and Finally, in order to utilize our offline TimeML annotations, we extract events using POS. Examples of the retained content words are underlined in Figures 5, 6, 7 and 8. After preprocessing each question, we convert into two premise-alternative pairs and For each of these pairs, we attempt to form a bridge from the causal sentence to the effect sentence, without distinction over whether the cause or effect originated from the premise or the alternative. This bridge is provided by four measures, or features, described in the following section. clusters are available at were detected using a list of English Wikipedia artitles available at verbs were determined using a list availat &amp;quot;causes&amp;quot; nsubj ?cause dobj ?effect ?cause &amp;quot;brought&amp;quot; nsubj ?effect prep &amp;quot;on&amp;quot; pobj 463 3 The Features of the COPACETIC System In determining the causal relatedness between a cause and an effect sentence, we utilize four features. Each feature calculates a value indicating the perceived strength of the causal relationship between a cause and an effect using a different measure of causality. The four features used by our COPACETIC system are described in the following subsections. 3.1 Bigram Relatedness Our first feature measures the degree of relatedness between all pairs of bigrams (at the token level) in the cause and effect pair. We do this by calculating the point-wise mutual Information (PMI) (Fano, 1961) for all bigram combinations between the candidate alternative and its premise in the English Gigaword corpus (Parker et al., 2009) as shown in Equation 1. (1) Under the assumption that distance words are unlikely to causally influence each other, we only consider co-occurrences within a window of one hundred tokens when calculating the joint probability of the PMI. Additionally, we allow for up to two tokens to occur within a single bigram’s occurrence (e.g. phrase her ears be considered a for the bigram ears Although these relaxations skew the values of our calculated PMIs by artificially lowering the joint probability, we are only concerned with how the values compare to each other. Note that because we employ no smoothing, the PMI of an unseen bigram is set to zero. The maximum PMI over all pairs of bigrams is retained as the value for this feature. Figure 5 illustrates this feature for Question 495. 3.2 Temporal Relatedness Although most of the questions in this task focus on causal relationships, for many questions, the nature of this causal relationship manifests instead as a temporal one (Bethard and Martin, 2008; Bethard, 2008). We use temporal link information from TimeML (Pustejovsky et al., 2005; Pustejovsky et al., 2003) annotations on our corpus to determine how temporally related a given cause and effect sentence are. 495 (Find the The wantedto 1: She got her pierced. 2: She got a tattoo. Alternative 1 Alternative 2 PMI(wear earrings, pierced ears) = -10.928 PMI(wanted wear, pierced ears) = -13.284 PMI(wear earrings, tattoo) = -12.77 PMI(wanted wear, tattoo) = -14.284 PMI(girl wanted, pierced ears) = -13.437 PMI(girl wanted, tattoo) = -14.762 PMI(girl, pierced ears) = -15.711 PMI(girl, tattoo) = -14.859 Maximum PMI = -10.928 Maximum PMI = -12.77 Figure 5: Example PMI values for bigrams and unigrams (with content words underlined). Alternative 1 is correctly chosen as it has largest maxi mum PMI. This is accomplished by using the point-wise mutual information (PMI) between all pairs of events from the cause to the effect (see Equation 1). We define the relevant probabilities as follows: The joint probability of a cause and effect event is defined as the number of times the cause event participates in a temporal link ending with the effect event. The probability of a cause event is defined as the number of times the cause event precipitates a temporal link to any event. The probability of an effect event is defined as the number of times the effect event ends a temporal link begun by any event. We define the PMI to be zero for any unseen pair of events (and for any pairs involving an unseen event). The summation of all pairs of PMIs is used as the value of this feature. Figure 6 shows how this feature behaves. 468 (Find the The barked. 1: The loungedon the Alternative 2: A knock sounded at the door. Alternative 1 Alternative 2 PMI(lounge, bark) = 5.60436 PMI(knock, bark) = 5.77867 PMI(sound, bark) = 5.26971 Figure 6: Example temporal PMI values (with content words underlined). Alternative 2 is correctly chosen as it has the highest summation. 3.3 Causal Dependency Structures We attempted to capture the degree of direct causal relatedness between a cause sentence and an effect sentence. To determine the strength of this relationship, 464 we considered how often phrases from the cause and effect sentences occur within a causal dependency structure. We detect this through the use of twentymanually crafted causal patterns (described in Section 2.1). The alternative that has the maximum number of matched dependency structures with the premise is retained as the correct choice. Figure 7 illustrates this feature. 490 (Find the The man wonthe Alternative 1: He became rich. 2: He money. Alternative 1 Alternative 2 = = Figure 7: Example casual dependency matches (with content words underlined). Alternative 1 is correctly selected because more patterns extracted “won” causing “rich” than “won” causing “owed”. 3.4 Polarity Comparison We observed that many of the questions involve the dilemma of determining whether a positive premise is more related to a positive or negative alternative (and vice-versa). This differs from sentiment analysis in that rather than determining if a sentence expresses a negative statement or view, we instead desire the overall sentimental connotation of a sentence (and thus of each word). For example, the premise from Question 494 (Figure 8) is “the woman became famous.” Although this sentence makes no positive or negative claims about the woman, the word “famous” – when considered on its own – implies positive connotations. We capture this information using the Harvard General Inquirer (Stone et al., 1966). Originally developed in 1966, the Harvard General Inquirer provides a mapping from English words to their polarity or For example, it dethe word “abandon” as and the “abound” as We use this information by summing the score for all words in a sen- (assigning a score of 1.0, a score of -1.0, and unseen words a score of 0.0). The difference between patterns was deemed sufficient due to time constraints. these scores between the cause sentence and the effect sentence is used as the value of this feature. This feature is illustrated in Figure 8. 494 (Find the Premise: The woman became famous. 1: followedher. 2: Her avoidedher. Premise Alternative 1 Alternative 2 follow Sum Figure 8: Example polarity comparison (with content words underlined). Alternative 1 is correctly chosen as it has the least difference from the score of the premise. 4 Results The COPA task of SemEval-2012 provided participants with 1,000 causal questions, divided into 500 questions for development or training, and 500 questions for testing. We submitted two systems to the COPA Evaluation for SemEval-2012, both of which are trained on the 500 development questions. Our first system uses only the bigram PMI feature and is as Our second system uses four features and is denoted as The accuracy of our two systems on the 500 provided test questions is provided in Table 1 (Gordon et al., 2012). On this task, accuracy is defined as the quotient of dividing the number of questions for which the correct alternative was chosen by the number of questions. Although multiple groups registered, ours were the only submitted results. Note that the difference in performance between our two systems is not significant (Gordon et al., 2012). Team ID System ID Score UTDHLT bigram pmi UTDHLT svm combined Table 1: Accuracy of submitted systems The primary hindrance to our approach is in combining each feature – that is, determining the confidence of each feature’s judgement. Because the questions vary significantly in their subject matter and the nature of the causal relationship between given causes and effects, a single approach is unlikely 465 to satisfy all scenarios. Unfortunately, the problem of determining which feature best applies to a give question requires non-trivial reasoning over implicit semantics between the premise and alternatives. 5 Conclusion This evaluation has shown that although commonsense causal reasoning is trivial for humans, it belies deep semantic reasoning and necessitates a breadth of world knowledge. Additional progress towards capturing world knowledge by leveraging a large number of cross-domain knowledge resources is necessary. Moreover, distilling information not specific to any domain – that is, a means of inferring basic and fundamental information about the world – is not only necessary but paramount to the success of any future system desiring to build chains of commonsense or causal reasoning. At this point, we are merely approximating such possible distillation. 6 Acknowledgements We would like to thank the organizers of SemEval- 2012 task 7 for their work constructing the dataset and overseeing the task. References [Bethard and Martin2008] S. Bethard and J.H. Martin. 2008. Learning semantic links from a corpus of parallel and causal relations. of the 46th Meeting of the [Bethard2008] S Bethard. 2008. Building a corpus of structure. of the Sixth</abstract>
<note confidence="0.808436303571429">[Fano1961] RM Fano. 1961. Transmission of Information: A Statistical Theory of Communication. [Gordon and Swanson2009] A. Gordon and R. Swanson. 2009. Identifying personal stories in millions of weblog In International Conference on Weblogs and Social Media, Data Challenge Workshop, San Jose, [Gordon et al.2012] Andrew Gordon, Zornitsa Kozareva, and Melissa Roemmele. 2012. (2012) SemEval-2012 Task 7: Choice of Plausible Alternatives: An Evaluaof Commonsense Causal Reasoning. In Proceedings of the 6th International Workshop on Semantic (SemEval Montreal. [Klein and Manning2003] D. Klein and C.D. Manning. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Linguistics-Volume pages 423–430. Association for Computational Linguistics. [Neumann and Weikum2008] Thomas Neumann and Gerhard Weikum. 2008. RDF-3X: a RISC-style engine for of the VLDB [of Congress. Prints et al.1980] Library of Congress. Prints, Photographs Division, and E.B. Parker. 1980. Subject headings used in the library of congress prints and photographs division. Prints and Photographs Division, Library of Congress. [Parker et al.2009] Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2009. English Gigaword Fourth Edition. [Pustejovsky et al.2003] J Pustejovsky, J Castano, and R Ingria. 2003. TimeML: Robust specification of and temporal expressions in text. Spring on New Directions in [Pustejovsky et al.2005] J Pustejovsky, Bob Ingria, Roser Sauri, Jose Castano, Jessica Littman, Rob Gaizauskas, Andrea Setzer, G. Katz, and I. Mani. 2005. The specilanguage TimeML. Language of Time: A et al.2011] Melissa Roemmele, min Adrian Bejan, and Andrew S. Gordon. 2011. Choice of Plausible Alternatives: An Evaluation of Causal Reasoning. AAAI Spring [Stone et al.1966] P. J. Stone, D.C. Dunphy, and M. S. 1966. General Inquirer: A Computer to Content Analysis. Press. [Toutanova et al.2003] K. Toutanova, D. Klein, C.D. Manning, and Y. Singer. 2003. Feature-rich part-of-speech with a cyclic dependency network. In Proceedof the 2003 Conference of pages 173–180. Association for Computational Linguistics. [Turian2010] J Turian. 2010. Word representations: a simple and general method for semi-supervised learning. of the 48th Annual Meeting of the pages 384–394. [Verhagen et al.2005] M Verhagen, I Mani, and R Sauri. 2005. Automating Temporal Annotation with TARSQI. of the ACL pages 81–84. 466</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bethard</author>
<author>J H Martin</author>
</authors>
<title>Learning semantic links from a corpus of parallel temporal and causal relations.</title>
<date>2008</date>
<booktitle>Proceedings of the 46th Annual Meeting of the ACL-HLT.</booktitle>
<marker>[Bethard and Martin2008]</marker>
<rawString>S. Bethard and J.H. Martin. 2008. Learning semantic links from a corpus of parallel temporal and causal relations. Proceedings of the 46th Annual Meeting of the ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bethard</author>
</authors>
<title>Building a corpus of temporal-causal structure.</title>
<date>2008</date>
<booktitle>Proceedings of the Sixth LREC.</booktitle>
<marker>[Bethard2008]</marker>
<rawString>S Bethard. 2008. Building a corpus of temporal-causal structure. Proceedings of the Sixth LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>RM Fano</author>
</authors>
<title>Transmission of Information: A Statistical Theory of Communication.</title>
<date>1961</date>
<marker>[Fano1961]</marker>
<rawString>RM Fano. 1961. Transmission of Information: A Statistical Theory of Communication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gordon</author>
<author>R Swanson</author>
</authors>
<title>Identifying personal stories in millions of weblog entries.</title>
<date>2009</date>
<booktitle>In Third International Conference on Weblogs and Social Media, Data Challenge Workshop,</booktitle>
<location>San Jose, CA.</location>
<marker>[Gordon and Swanson2009]</marker>
<rawString>A. Gordon and R. Swanson. 2009. Identifying personal stories in millions of weblog entries. In Third International Conference on Weblogs and Social Media, Data Challenge Workshop, San Jose, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Gordon</author>
<author>Zornitsa Kozareva</author>
<author>Melissa Roemmele</author>
</authors>
<title>SemEval-2012 Task 7: Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012),</booktitle>
<location>Montreal.</location>
<marker>[Gordon et al.2012]</marker>
<rawString>Andrew Gordon, Zornitsa Kozareva, and Melissa Roemmele. 2012. (2012) SemEval-2012 Task 7: Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>[Klein and Manning2003]</marker>
<rawString>D. Klein and C.D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 423–430. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Neumann</author>
<author>Gerhard Weikum</author>
</authors>
<title>RDF-3X: a RISC-style engine for RDF.</title>
<date>2008</date>
<booktitle>Proceedings of the VLDB Endowment.</booktitle>
<marker>[Neumann and Weikum2008]</marker>
<rawString>Thomas Neumann and Gerhard Weikum. 2008. RDF-3X: a RISC-style engine for RDF. Proceedings of the VLDB Endowment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Photographs Division Prints</author>
<author>E B Parker</author>
</authors>
<title>Subject headings used in the library of congress prints and photographs division. Prints and Photographs Division, Library of Congress.</title>
<date>1980</date>
<marker>[of Congress. Prints et al.1980]</marker>
<rawString>Library of Congress. Prints, Photographs Division, and E.B. Parker. 1980. Subject headings used in the library of congress prints and photographs division. Prints and Photographs Division, Library of Congress.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Parker</author>
<author>David Graff</author>
<author>Junbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<title>English Gigaword Fourth Edition.</title>
<date>2009</date>
<marker>[Parker et al.2009]</marker>
<rawString>Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2009. English Gigaword Fourth Edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>J Castano</author>
<author>R Ingria</author>
</authors>
<title>TimeML: Robust specification of event and temporal expressions in text.</title>
<date>2003</date>
<booktitle>Symposium on New Directions in Question-Answering.</booktitle>
<publisher>AAAI Spring</publisher>
<marker>[Pustejovsky et al.2003]</marker>
<rawString>J Pustejovsky, J Castano, and R Ingria. 2003. TimeML: Robust specification of event and temporal expressions in text. AAAI Spring Symposium on New Directions in Question-Answering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>Bob Ingria</author>
<author>Roser Sauri</author>
<author>Jose Castano</author>
<author>Jessica Littman</author>
<author>Rob Gaizauskas</author>
<author>Andrea Setzer</author>
<author>G Katz</author>
<author>I Mani</author>
</authors>
<title>The specification language TimeML. The Language of Time: A Reader.</title>
<date>2005</date>
<marker>[Pustejovsky et al.2005]</marker>
<rawString>J Pustejovsky, Bob Ingria, Roser Sauri, Jose Castano, Jessica Littman, Rob Gaizauskas, Andrea Setzer, G. Katz, and I. Mani. 2005. The specification language TimeML. The Language of Time: A Reader.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melissa Roemmele</author>
<author>Cosmin Adrian Bejan</author>
<author>Andrew S Gordon</author>
</authors>
<title>Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning.</title>
<date>2011</date>
<publisher>AAAI Spring Symposium Series.</publisher>
<marker>[Roemmele et al.2011]</marker>
<rawString>Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S. Gordon. 2011. Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning. 2011 AAAI Spring Symposium Series.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Stone</author>
<author>D C Dunphy</author>
<author>M S Smith</author>
</authors>
<title>The General Inquirer: A Computer Approach to Content Analysis.</title>
<date>1966</date>
<publisher>MIT Press.</publisher>
<marker>[Stone et al.1966]</marker>
<rawString>P. J. Stone, D.C. Dunphy, and M. S. Smith. 1966. The General Inquirer: A Computer Approach to Content Analysis. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>D Klein</author>
<author>C D Manning</author>
<author>Y Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of NAACL-HLT,</booktitle>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>[Toutanova et al.2003]</marker>
<rawString>K. Toutanova, D. Klein, C.D. Manning, and Y. Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of NAACL-HLT, pages 173–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turian</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>Proceedings of the 48th Annual Meeting of the ACL,</booktitle>
<pages>384--394</pages>
<marker>[Turian2010]</marker>
<rawString>J Turian. 2010. Word representations: a simple and general method for semi-supervised learning. Proceedings of the 48th Annual Meeting of the ACL, pages 384–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Verhagen</author>
<author>I Mani</author>
<author>R Sauri</author>
</authors>
<title>Automating Temporal Annotation with TARSQI.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL</booktitle>
<pages>81--84</pages>
<marker>[Verhagen et al.2005]</marker>
<rawString>M Verhagen, I Mani, and R Sauri. 2005. Automating Temporal Annotation with TARSQI. In Proceedings of the ACL 2005, pages 81–84.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>