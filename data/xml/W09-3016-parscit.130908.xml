<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000338">
<title confidence="0.772579">
High-Performance High-Volume Layered Corpora Annotation
</title>
<note confidence="0.770737333333333">
Tiago Luis and David Martins de Matos
L2F - INESC-ID
R. Alves Redol 9, Lisboa, Portugal
</note>
<email confidence="0.917058">
{tiago.luis,david.matos}@l2f.inesc-id.pt
</email>
<sectionHeader confidence="0.996459" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999884407407407">
NLP systems that deal with large collec-
tions of text require significant computa-
tional resources, both in terms of space
and processing time. Moreover, these sys-
tems typically add new layers of linguis-
tic information with references to another
layer. The spreading of these layered an-
notations across different files makes them
more difficult to process and access the
data. As the amount of input increases, so
does the difficulty to process it. One ap-
proach is to use distributed parallel com-
puting for solving these larger problems
and save time.
We propose a framework that simplifies
the integration of independently existing
NLP tools to build language-independent
NLP systems capable of creating layered
annotations. Moreover, it allows the devel-
opment of scalable NLP systems, that exe-
cutes NLP tools in parallel, while offering
an easy-to-use programming environment
and a transparent handling of distributed
computing problems. With this framework
the execution time was decreased to 40
times less than the original one on a cluster
with 80 cores.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971392156863">
Linguistic information can be automatically cre-
ated by NLP systems. These systems are com-
posed by several NLP tools that are typically ex-
ecuted in a pipeline, where each tool performs a
processing step. Therefore, each tool uses the re-
sults produced by the previous processing steps
and produces new linguistic information that can
be later used by other tools. The addition of new
layers of linguistic information (layered annota-
tions) by NLP tools makes the processing and ac-
cess to data difficult due to the spreading of the
layered annotations across different files. More-
over, whenever these tools are integrated, several
problems related with information flow between
them may arise. A given tool may need an annota-
tion previously produced by another tool but some
of the information in annotation can be lost in con-
versions between the different tool data formats,
because the expressiveness of each format may be
different and not completely convertible into other
formats.
Besides tool integration problems, there is also
another problem related with the data-intensive
nature of NLP and the computation power needed
to produce the linguistic information. The wealth
of annotations has increased the amount of data to
process. Therefore, the processing of this linguis-
tic information is a computation-heavy process
and some algorithms continue to take a long time
(hours or days) to produce their results. This kind
of processing can benefit from distributed parallel
computing but it may create other problems, such
as fault tolerance to machine failures. Because
some NLP algorithms can take long time to pro-
duce their results, it is important to automatically
recover from these failures, in order not to lose the
results of computations already performed. Task
scheduling is also a problem due to data-intensive
nature of NLP. Data-driven scheduling (based on
data location) improves performance because it re-
duces bandwidth usage.
Our framework aims to simplify the integration
of independently developed NLP tools, while pro-
viding an easy-to-use programming environment,
and transparent handling of distributed computing
problems, such as fault tolerance and task schedul-
ing, when executing the NLP tools in parallel.
Moreover, NLP systems built on top of the frame-
work are language-independent and produce lay-
ered annotations. We also measured the gains that
can be achieved with the parallel execution of NLP
</bodyText>
<page confidence="0.98341">
99
</page>
<note confidence="0.996811">
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 99–107,
Suntec, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.997485375">
tools and the merging of the layered annotations.
Section 2 discusses related work, Section 3
present the framework’s architecture and a de-
tailed description of its components, Section 4
shows the integrated tools, Section 5 explains how
the information produced by tools is merged, and
Section 6 presents the achieved results. Finally,
Section 7 presents concluding remarks.
</bodyText>
<sectionHeader confidence="0.999921" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9988566">
GATE (Cunningham et al., 2002) is one of the
most used framework for building NLP systems.
However, it does not provide a controller for paral-
lel execution, it only supports the execution of ap-
plications on different machines over data shared
on the server (Bontcheva et al., 2004). However,
this solution cannot be applied in a large-scale dis-
tributed environment because the shared reposi-
tory becomes a bottleneck in computation due to
the accesses from all the machines making com-
putations.
UIMA (Ferrucci and Lally, 2004) is also used
to build NLP systems, and this framework sup-
ports replication of pipeline components to im-
prove throughput on multi-processor or multi-
machine platforms. However, we did not find
any published results regarding the parallel execu-
tion. The UIMA framework has been successfully
leveraged (Egner et al., 2007) with Condor1, a
manager of loosely coupled compute resources, al-
lowing the parallel execution of multiple instances
of the NLP system built with UIMA. The Condor
scheduler allows to solve problems where there is
no communication between tasks and complicates
the development of parallel applications when this
interaction is needed, like in our case, where it
is necessary to merge multiple layers of annota-
tions. Also, the Condor does not move computa-
tions closer to their input data, like the MapReduce
approach.
The MapReduce paradigm has already been
successfully adopted by the Ontea semantic anno-
tator (Laclavik et al., 2008). We think that GATE
and UIMA frameworks could also benefit with
the MapReduce. For example, the NLTK (Loper
and Bird, 2002) adopted this paradigm, and al-
ready have implementations of some algorithms
like term frequency-inverse document frequency
(tf-idf) or expectation-maximization (EM).
There are already tools for merging of layered
</bodyText>
<footnote confidence="0.984142">
1http://www.cs.wisc.edu/condor/
</footnote>
<figureCaption confidence="0.999839">
Figure 1: Framework architecture.
</figureCaption>
<bodyText confidence="0.999480103448276">
annotations, like the ANC tool (Ide and Suder-
man, 2006). However, we did not find any ap-
proach to this task in a scalable manner.
Concerning the parallel programming
approaches, Message Passing Interface
(MPI) (Gropp, 2001) continues to be widely
used in parallel programming and therefore there
are currently many libraries built based on this
programming model. However, this approach
provides very low level routines that are difficult
to use and make for obscure algorithm imple-
mentation, making code reuse and maintenance
difficult and time consuming. MPI programming
can be difficult because it is necessary to divide
the problem among processes with separate
address spaces and coordinate these processes
with communication routines.
MapReduce (Dean and Ghemawat, 2008)
forces the programmer to consider the data par-
allelism of the computation. Also, this frame-
work automatically schedules and distributes data
to tasks. The simple API provided by the system
allows programmers to write simple serial pro-
grams that are run in a distributed way, while hid-
ing several parallel programming details. There-
fore, this framework is accessible to a wide range
of developers and allows them to write their ap-
plications at a higher level of abstraction than the
MPI approach.
</bodyText>
<sectionHeader confidence="0.985911" genericHeader="method">
3 Framework Architecture
</sectionHeader>
<bodyText confidence="0.983967285714286">
Our framework aims to simplify the integration of
independently developed NLP tools, while execut-
ing the NLP tools in a parallel manner. Our archi-
tecture is composed by: Stage, Tool, and Unit (see
Figure 1). Stages represent phases in the annota-
tion process of the NLP system. They can be in-
terconnected in order to form an NLP system. The
</bodyText>
<figure confidence="0.999346">
Text
TEI
Fragmentation
Annotations
LAF
Input
Unit
Input
Queue
Unit Unit
Stage
Tool
Framework
Output
Queue
Unit
Annotations
LAF
Output
</figure>
<page confidence="0.964132">
100
</page>
<bodyText confidence="0.999980038461538">
Tool interacts with the existing NLP tool and can
receive as input an annotation previously created
or a text file (structured or unstructured). The text
files received are divided into sets of independent
Units. Units are used to represent the input file
fragmentation (each fragment is represented by a
Unit). These independent Units are then processed
in parallel. Previously created annotations are al-
ready divided, since they correspond to an annota-
tion that refers the corresponding input fragment.
Tools are wrapped in Stage components. Stages
have two queues: an input and an output queue of
Units. Stages are responsible for consuming input
queue Units, pass them to the Tool and, after their
processing, put the result on output queue. These
queues allow multithreaded consumption and pro-
duction of the Units.
The framework was implemented using
the MapReduce (Dean and Ghemawat, 2008)
paradigm due to its scalability when dealing with
large data volumes. The Hadoop2 framework (de-
scribed in the next section) was used as the base
for implementation. The next sections describe
the representation format used for annotations, the
input accepted, and the framework components in
more detail.
</bodyText>
<subsectionHeader confidence="0.99562">
3.1 Hadoop
</subsectionHeader>
<bodyText confidence="0.999968227272727">
Hadoop is a MapReduce implementation written
in Java. One of the main advantages of using the
MapReduce paradigm is task scheduling. When
dealing with large datasets in a distributed man-
ner, bandwidth to data becomes a problem. The
MapReduce paradigm and the Hadoop Distributed
File System (HDFS) allows to reduce bandwidth
consumption because tasks are scheduled close to
their inputs whenever possible.
Another advantage is fault tolerance and task
synchronization handling. These problems, inher-
ent to distributed systems, are transparently solved
by the Hadoop framework, facilitating program-
ming of distributed applications.
The MapReduce framework operates exclu-
sively on key/value pairs, i.e., the framework
views the input to the job as a set of key/value pairs
and produces a set of key/value pairs as the output
of the job. These key and value elements can be
any user defined data type.
The main tasks of MapReduce are the map and
the reduce task. The map task produces a set of
</bodyText>
<footnote confidence="0.942671">
2http://hadoop.apache.org/core/
</footnote>
<figureCaption confidence="0.998412">
Figure 2: Graph-based model annotation example.
</figureCaption>
<bodyText confidence="0.999901">
intermediate key/value pairs from input key/value
pairs. Each map is an individual task that runs on a
machine. The reduce phase creates a smaller set of
key/value pairs from a set of intermediate values
that have the same key. Since different mappers
can output the same key, the framework groups
reducer input key/value pairs with the same key.
This grouping capability is used to merge annota-
tions produced by different tools and that are re-
lated with each other, as shown is the Section 5.
</bodyText>
<subsectionHeader confidence="0.999508">
3.2 Representation Format
</subsectionHeader>
<bodyText confidence="0.999505821428572">
In order to represent linguistic information gener-
ated by the tools, we chose the Linguistic Anno-
tation Framework (LAF) (Ide and Romary, 2006)
format, that uses a graph model to store annota-
tions.
An annotation can be viewed as a set of linguis-
tic information items that are associated with some
data (a part of a text or speech signal, for example),
called primary data. Primary data objects are rep-
resented by locations in the input. These locations
can be the offset of a character comprising a sen-
tence or word, in the case of a text input, or a point
at which a given temporal event begins or ends, in
the case of a speech signal input. As such, primary
data objects have a simple structure. However, it is
possible to build more complex data objects, com-
posed by sets of contiguous or noncontiguous lo-
cations. Primary data objects are used to build seg-
mentations over data. A segmentation represents a
list of ordered segments, where each segment rep-
resents a linguistic element. A segment is repre-
sented by an edge between virtual nodes located
between each character in the primary data (see
Figure 2). It is possible to define multiple segmen-
tations over the same primary data, and multiple
annotations may refer to the same segmentation.
An annotation is defined as a label and a feature
structure. A feature structure is itself a graph in
</bodyText>
<figure confidence="0.99793325">
POS Tagger
Annotation
POS Tagger
Annotation
NER
Annotation
Parser
Annotation
POS Tagger
Annotation
n1
Great Britain
0
5 6 13
n3
n2
</figure>
<page confidence="0.782168">
101
</page>
<table confidence="0.887437846153846">
&lt;laf&gt;
&lt;edgeSet&gt;
&lt;edge id=“e1&amp;quot; from=“0&amp;quot; to=“5&amp;quot;/&gt;
&lt;edge id=“e2&amp;quot; from=“6&amp;quot; to=&amp;quot;13&amp;quot;/&gt;
&lt;/edgeSet&gt;
&lt;nodeSet&gt;
&lt;node id=“n1“ edgesTo=“e1&amp;quot;&gt;
&lt;fs type=“segment“&gt;
&lt;f name=“SEGMENT“ value=“Great&amp;quot;/&gt;
&lt;f name=“POS&amp;quot; value=“NNP&amp;quot;/&gt;
&lt;/fs&gt;
&lt;/node&gt;
&lt;node id=“n2“ edgesTo=“e2&amp;quot;&gt;
&lt;fs type=“segment“&gt;
&lt;f name=“SEGMENT“ value=“Britain&amp;quot;/&gt;
&lt;f name=“POS&amp;quot; value=“NNP&amp;quot;/&gt;
&lt;/fs&gt;
&lt;/node&gt;
&lt;node id=“n3“ edgesTo=“e1 e2&amp;quot;&gt;
&lt;fs type=“segment“&gt;
&lt;f name=“SEGMENT“ value=“Great Britain&amp;quot;/&gt;
&lt;f name=“POS&amp;quot; value=“NNP&amp;quot;/&gt;
&lt;/fs&gt;
&lt;/node&gt;
&lt;/nodeSet&gt;
&lt;/laf&gt;
</table>
<figureCaption confidence="0.9507795">
Figure 3: Morphosyntactic LAF annotation exam- Figure 4: TEI file LAF annotation example.
ple.
</figureCaption>
<bodyText confidence="0.999730777777778">
which nodes are labeled with feature/value pairs or
other feature structures. Hence, a morphosyntac-
tic annotation is represented by a graph in which
nodes are labeled with feature/value pairs. These
pairs contain the morphosyntactic information.
Figure 3 shows how the two possible segmenta-
tions in the POS tagger annotation in Figure 2 can
be represented: the segment “Great Britain” has a
total of 13 characters; the edges use the character
offsets to delimit the segment; the nodes built on
top of these edges contain the morphosyntactic in-
formation, such as the POS, and the text pointed to
by the segment. As shown in the third node (with
identifier “n3”), it is possible to have a node refer-
ring to multiple edges. A node can also refer to
other nodes to add other kinds of linguistic infor-
mation, such as dependencies between segments
or syntactic annotations.
</bodyText>
<subsectionHeader confidence="0.951462">
3.3 Input
</subsectionHeader>
<bodyText confidence="0.999986625">
Currently, the Tools integrated in our framework
can process three kinds of input files: structured
and unstructured text, and previously created an-
notations. The structured text format currently
supported is TEI (Text Encoding Initiative)3. Both
structured and unstructured text are fragmented
into a set of Units. The division is currently
paragraph-based, in the case of the unstructured
</bodyText>
<footnote confidence="0.805663">
3http://www.tei-c.org/
</footnote>
<bodyText confidence="0.997676222222222">
text, and on XML textual elements, in the case of
the TEI input. However, it is possible to create
Units with other user-defined granularity.
In order to make references to locations in the
TEI input, we adopted the XPointer4 format (see
Figure 4). Assuming that each text element in the
TEI file has a unique identifier, the XPointer of
the start and end tag will refer this identifier and
the word character offset.
</bodyText>
<subsectionHeader confidence="0.882095">
3.4 Unit
</subsectionHeader>
<bodyText confidence="0.999818375">
When processing large files, with several giga-
bytes, it is not efficient to process them in serial
mode due to memory constraints. Therefore, we
divide them into sets of Units that are processed
independently.
Each Unit is associated with a portion of the
input file and contains the linguistic information
generated by a tool in a stage. The Unit has a
unique identifier, a set of dependencies (contains
information about other Units that the Unit de-
pends on), the identifier of the Stage that produced
the unit and the annotation (linguistic informa-
tion produced by Tool). Besides these elements, it
also has a common identifier that is shared across
the layered annotations that are related with each
other.
</bodyText>
<footnote confidence="0.980298">
4http://www.w3.org/TR/xptr/
</footnote>
<page confidence="0.992935">
102
</page>
<subsectionHeader confidence="0.722607">
3.5 Stage
</subsectionHeader>
<bodyText confidence="0.999962684210526">
Stages represent a phase in the annotation process.
Each Stage has two queues: an input and an output
queue of Units. This component is responsible for
consuming input units, pass them to the Tool and,
after their processing, putting them on the output
queue. The Units in the output queue can later be
used by another Stage (by connecting the output
queue to the input queue of the next stage) or writ-
ten to a file.
An NLP system can be composed of several
Stages that are responsible for a specific annota-
tion task. The framework allows the composition
of various Tools to form a complete NLP system:
each Tool receives the information produced by
the Tools in the previous Stages and produces a
Unit with the annotation created with references to
the previous ones. This information is maintained
in memory, along the created tool pipeline, and is
only written to disk at the end of the NLP system.
</bodyText>
<subsectionHeader confidence="0.920785">
3.6 Tool
</subsectionHeader>
<bodyText confidence="0.999943892857143">
Tools are responsible for specific linguistic tasks.
Currently, these Tools include (without limitation)
Tokenizers and Classifiers. Tokenizers receive the
input text and produce segmentations (list of seg-
ments) that refer to the input, i.e., divide the in-
put sentences into words. Classifiers produce sets
of classifications for a given segmentation. These
classifications can be, for example, the grammar
class of each word. These tools accept two kinds
of inputs: an input text or a previously created an-
notation with a segmentation.
In order to add new tools, it is necessary to ex-
tend the previous classes and add the necessary
code in order to add the information produced by
the existing NLP tool in the LAF format.
Because the framework is written in Java, and
the tools could have been developed in a different
language, such as C++ or Perl, it was necessary to
find a way to interact with other programming lan-
guages. Hence, an existing tool can be integrated
in various ways. If a tool provides an API, we cur-
rently provide an Remote Procedure Call (RPC)
mechanism with the Thrift5 software library. If
the API can be used in a C/C++ program, it is also
possible to use the existing tool API with Java Na-
tive Interface (JNI) (Liang, 1999). The framework
also supports tools that can only be executed from
the command line.
</bodyText>
<footnote confidence="0.905369">
5http://incubator.apache.org/thrift/
</footnote>
<sectionHeader confidence="0.994107" genericHeader="method">
4 Applications
</sectionHeader>
<bodyText confidence="0.999931790697674">
The tools that have been integrated can be divided
into two classes: those capable of producing first
level annotations and those capable of producing
second level annotations. The first level tools pro-
duce morphosyntactic annotation from an input
text. Second level tools receive morphosyntac-
tic information as input and produce morphosyn-
tactic annotations. To show the language inde-
pendence of the framework, we integrated tools
from four different languages: Arabic, Japanese,
English, and Portuguese. One of the tools capa-
ble of analyzing Arabic texts is AraMorph (Buck-
walter, 2002), a Java-based Arabic morpholog-
ical analyzer. For the Japanese language we
chose Chasen (Matsumoto et al., 1999), a morpho-
logical analyzer capable of processing Japanese
texts. The Stanford POS Tagger (Toutanova, 2000;
Toutanova et al., 2003) is only being used to pro-
cess English texts but it can be easily adapted (by
changing its input dictionary) to process other lan-
guages, like Chinese or German. For processing
Portuguese, we chose the Palavroso morphologi-
cal analyzer (Medeiros, 1995). The morphological
analyzers previously described produce first level
annotations, i.e., they receive text as input and pro-
duce annotations.
Besides these tools, we also integrated types
of tools for testing second level annotations:
RuDriCo (Paulo, 2001) and JMARv (Ribeiro et
al., 2003). RuDriCo is a post-morphological an-
alyzer that rewrites the results of a morphologi-
cal analyzer. RuDriCo uses declarative transfor-
mation rules based on pattern matching. JMARv
is a tool that performs morphosyntactic disam-
biguation (selects a classification from the possi-
ble classifications in each segment from the in-
put sequence). The two previous tools were
used for processing Portuguese morphosyntactic
information, but can be easily adapted to pro-
cess other languages. For example, JMARv could
be used to disambiguate AraMorph classifications
and RuDriCo could translate Chasen’s Japanese
POS information into other formats.
</bodyText>
<sectionHeader confidence="0.970344" genericHeader="method">
5 Merging Layered Annotations
</sectionHeader>
<bodyText confidence="0.9999926">
The stand-off annotation provided by the LAF for-
mat allows to add new layers of linguistic informa-
tion by creating a tree whose nodes are references
to another layer. This approach offers many ad-
vantages, like the possibility to distribute the an-
</bodyText>
<page confidence="0.997036">
103
</page>
<figure confidence="0.885878333333333">
LAF
LAF
Annotations
</figure>
<figureCaption confidence="0.9962595">
Figure 5: Illustration of the dependencies between
annotation layers.
</figureCaption>
<bodyText confidence="0.994657653846154">
notations without the source text, and the possi-
bility to annotate discontinuous segments of text.
However, these layers, although separate, depend
on each other, and their information can be diffi-
cult to access, because these layers can be spread
across different files. There is a naive approach
to do this merge, that consists in loading all anno-
tations from all files to memory and then resolve
their dependencies. However, these dependencies
can be dispersed across several large files (see Fig-
ure 5. Thus, the machine memory constraints be-
come a problem for this solution.
Therefore, we propose a novel solution to solve
the merging problem in a efficient manner us-
ing the MapReduce programming paradigm. The
grouping capability offered by the Hadoop frame-
work – a Java implementation of the MapReduce
paradigm – allows to efficiently merge the annota-
tions produced by the different tools, i.e., the lay-
ered annotations. This operation is performed as
follows:
Map - this phase produces key/value pairs with a
key equal to the identifier that is shared by
annotations that depend on one another (see
Figure 6). Thus, all related annotations are
grouped by the framework after this phase.
</bodyText>
<footnote confidence="0.398192">
Reduce - before the creation of the new anno-
</footnote>
<figureCaption confidence="0.9889755">
Figure 6: Codification of the layered annotations
dependencies.
</figureCaption>
<bodyText confidence="0.9986505">
tation, merge the previously created annota-
tions. This merging process creates a single
annotation that contains all the annotations
that were combined. This unified annotation
is then passed to the Tool. The Tool pro-
cesses the annotation and produces another
one sharing a common identifier. The new
annotation is written at the end of this phase.
The serialization of the intermediate key and value
elements from a pair in a binary format allows us
to reduce bandwidth usage due to the more com-
pact representation of the key and value compared
to the LAF (XML-based format representation of
the input file).
</bodyText>
<sectionHeader confidence="0.999904" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999914545454546">
The tests were performed on a cluster with 20 ma-
chines. Each machine had an Intel Quad-Core
Q6600 2.4 GHz processor, 8 GB of DDR2 RAM at
667 MHz and was connected to a gigabit ethernet.
To measure the amount of achieved parallelism,
we used the speedup formula shown in Equation 1:
T3 is the execution time of the sequential algo-
rithm; TP is the execution time of the parallel al-
gorithm. Speedup refers to how much a parallel
solution is faster than the corresponding sequen-
tial solution.
</bodyText>
<equation confidence="0.97657">
S = T (1)
p
</equation>
<bodyText confidence="0.999003833333333">
The Hadoop framework was installed on all ma-
chines and each one was configured to run 4 map
and 4 reduce tasks simultaneously. The Hadoop
uses HDFS as storage. This file system was con-
figured to split each file into 64 MB chunks. These
blocks are replicated across the machines in the
</bodyText>
<figure confidence="0.864951422222222">
depends
refers
Input
File
Tool
Tool
�����������
���
LAF��
LAF
Stage 1
Stage 2
�����������
���
LAF
Annotations
LAF
104
Data [MB] Stanford POS Tagger
Serial Time [s]
1 308
2 606
5 1531
10 3055
20 6021
50 15253
Stanford POS Tagger (English)
50 MB
Reducers
120
100
80
60
40
20
40
35
30
25
20
5
0
15
10
Speedup
</figure>
<tableCaption confidence="0.9128175">
Table 1: Serial processing time of the Stanford
POS Tagger
</tableCaption>
<bodyText confidence="0.999564346153846">
cluster in order to tolerate machine failures. We
used a HDFS replication factor of three.
To test the system, we used the speedup formula
and selected the Stanford POS Tagger. Table 1
shows the serial execution time of the Stanford
POS Tagger. This time corresponds to the stan-
dalone execution of the tool (without being inte-
grated in the framework) on a single computer, for
various sizes of input data (from 1 MB to 50 MB).
Input and output were read/written from/to the lo-
cal disk.
In addition to the previous tool, we also tested
JMARv in order to assess the impact of annotation
merging at execution time. Unlike the other tools,
this tool receives annotations as input.
We must also consider the setup time for
Hadoop. When executing the tools on top of
Hadoop, it is necessary to store the input data on
HDFS. However, these files are, in many cases,
rarely updated. Therefore, they are perfect for
the write-once read-many nature of HDFS and the
copy times of the input data files were not consid-
ered (the HDFS write speed was around 22 MB/s).
Section 6.1 shows the speedups achieved with
the Stanford POS Tagger, and Section 6.2 the an-
notation merging results, with the JMARv tool.
</bodyText>
<subsectionHeader confidence="0.998335">
6.1 Stanford POS Tagger Results
</subsectionHeader>
<bodyText confidence="0.995230153846154">
Figure 7 shows the speedup values when consider-
ing various values for the number of mappers and
reducers, without any compression of the final out-
put, for an input of 50 MB. The large standalone
times show that this tool is computationally heavy.
With this tool it was possible to achieve a speedup
value of approximately 40.
The horizontal progression of the speedup is ex-
plained by the heavy computation performed by
the tool. Since processing from the Stanford POS
Tagger is performed in mappers, the increase in
the number of mappers improves speedup values.
The execution time of this tool is around 400
</bodyText>
<figure confidence="0.850014">
50 100 150 200 250
Mappers
</figure>
<figureCaption confidence="0.988745">
Figure 7: Stanford POS Tagger speedup results
</figureCaption>
<table confidence="0.999598625">
Input [MB] Compressed Uncompressed
Output [MB] Time [s] Output [MB] Time [s]
1 3 56 24 57
2 6 66 48 67
5 15 91 119 94
10 31 140 238 140
20 62 235 476 226
50 155 534 1192 529
</table>
<tableCaption confidence="0.888585666666667">
Table 2: Stanford POS tagger output compression
evaluation with a fixed number of 64 mappers and
64 reducers.
</tableCaption>
<bodyText confidence="0.998272">
seconds, on the yellow (light gray) portion of Fig-
ure 7 and 1700 seconds on the dark blue (black)
portions. On the intermediate values the execution
time is approximately 1000 seconds.
The top right corner of the graph shows a small
speedup decrease. This can be explained by the
large number of queued map and reduce tasks.
</bodyText>
<subsubsectionHeader confidence="0.66134">
6.1.1 Compression Evaluation
</subsubsectionHeader>
<bodyText confidence="0.999752777777778">
Table 2 shows how output compression influences
execution times values. As shown in Table 2, this
tool produces, approximately, an output 24 times
larger than the input, without compression, and
3 times larger with compression. However, out-
put compression does not improve execution times
due to heavy computation performed by the tool.
Hence, processing time dominates output writing
time.
</bodyText>
<subsectionHeader confidence="0.998473">
6.2 Annotation Merging Results
</subsectionHeader>
<bodyText confidence="0.998906428571429">
Unlike the previous tool, JMARv does not process
text as input. This tool receives an annotation as
input that, in this case, was previously created by
Palavroso.
In order to test the parallel annotation merging
on top of Hadoop, we measured three kinds of
times: Palavroso execution time with output cre-
</bodyText>
<page confidence="0.993754">
105
</page>
<table confidence="0.991393">
Palavroso Palavroso + JMARv JMARv
Time [s] 171 s 323 s 179 s
</table>
<tableCaption confidence="0.718385666666667">
Table 3: Annotation merging time evaluation with
a fixed number of 64 mappers and 64 reducers, for
an input of 100 MB of text.
</tableCaption>
<bodyText confidence="0.99899405">
ation time, execution time of JMARv with the pre-
viously written Palavroso output and the time of
Palavroso and JMARv executed in a pipeline (in-
termediate data is maintained in memory and the
output produced is only written to disk after the
execution of the two tools). The results are pre-
sented in Table 3. The first column shows the exe-
cution time of the Palavroso tool. The second col-
umn shows the time of Palavroso and JMARv exe-
cution in a pipeline. Finally, the last column shows
the execution time of JMARv with the previously
created Palavroso output.
In order to execute JMARv after Palavroso, it
was necessary to handle about 8 GB of output
produced by the previous tool (already stored on
disk). However, these results show that running
JMARv with this amount of data is practically the
same as running both tools in pipeline with the
original input (100 MB) and only write their out-
put at the end.
</bodyText>
<sectionHeader confidence="0.999459" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999974119047619">
This framework allowed us to build scalable NLP
systems that achieve significant speedups: in
the case of computation heavy tools, speedups
reached values of approximately 40. In this case,
an increase on the number of map tasks improves
speedups, because processing time dominates the
output writing time.
In addition, the framework supports a wide
range of linguistic annotations, thanks to the adop-
tion of LAF. The integration of tools does not con-
sider any aspect related with the parallel execu-
tion on top of the Hadoop. Thus, the programmer
focuses only on representing the linguistic infor-
mation produced by the tool for a given input text
or previously created annotations. In addition, the
programming ease offered by the Hadoop frame-
work allows to focus only on the problem we are
solving, i.e., linguistic annotation. All the prob-
lems inherent to distributed computing are trans-
parently solved by the platform. The MapRe-
duce sort/grouping capabilities has been used to
efficiently merge layered annotations produced by
tools integrated in the framework. Regarding fu-
ture work, on the linguistic part, we plan to inte-
grate tools that produce syntactic annotations (the
LAF format already supports these annotations).
This linguistic information can be merged with the
current tree by simply adding more nodes above
the nodes that contain the morphosyntactic anno-
tations. Also, this work did not focus on informa-
tion normalization. The Data Category Registry
(DCR) (Wright, 2004) could be explored in the fu-
ture, in order to improve interoperability between
linguistic resources.
Finally, the creation of NLP systems can be sim-
plified by an XML parametrization. This way it is
possible to compose a tool pipeline by simply edit-
ing an XML file. An graphical environment for vi-
sualization and editing of LAF annotations is also
useful.
Our code is available at http://code.
google.com/p/anota/.
</bodyText>
<sectionHeader confidence="0.999075" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99943">
This work was supported by the partnership be-
tween Carnegie Mellon University and Portu-
gal’s National Science and Technology Founda-
tion (FCT – Fundac¸˜ao para a Ciˆencia e a Tecnolo-
gia).
</bodyText>
<sectionHeader confidence="0.999535" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999668409090909">
Kalina Bontcheva, Valentin Tablan, Diana Maynard,
and Hamish Cunningham. 2004. Evolving gate to
meet new challenges in language engineering. Nat.
Lang. Eng., 10(3-4):349–373.
Tim Buckwalter. 2002. Buckwalter Arabic Morpho-
logical Analyzer Version 1.0. Linguistic Data Con-
sortium, catalog number LDC2002L49, ISBN 1-
58563-257-0.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. Gate: A framework and graph-
ical development environment for robust nlp tools
and applications. In Proceedings of the 40th Annual
Meeting of the ACL.
Jeffrey Dean and Sanjay Ghemawat. 2008. MapRe-
duce: simplified data processing on large clusters.
Commun. ACM, 51(1):107–113, January.
Michael Thomas Egner, Markus Lorch, and Edd Bid-
dle. 2007. Uima grid: Distributed large-scale
text analysis. In CCGRID ’07: Proceedings of the
Seventh IEEE International Symposium on Cluster
Computing and the Grid, pages 317–326, Washing-
ton, DC, USA. IEEE Computer Society.
</reference>
<page confidence="0.935262">
106
</page>
<reference confidence="0.999602984848485">
David Ferrucci and Adam Lally. 2004. Uima: an
architectural approach to unstructured information
processing in the corporate research environment.
Nat. Lang. Eng., 10(3-4):327–348.
William Gropp. 2001. Learning from the Success of
MPI. In Burkhard Monien, Viktor K. Prasanna, and
Sriram Vajapeyam, editors, HiPC, volume 2228 of
Lecture Notes in Computer Science, pages 81–94.
Springer.
Nancy Ide and Laurent Romary. 2006. Representing
linguistic corpora and their annotations. In Proceed-
ings of the Fifth Language Resources and Evalua-
tion Conference (LREC.
Nancy Ide and Keith Suderman. 2006. Merging lay-
ered annotations. In Proceedings of Merging and
Layering Linguistic Information, Genoa, Italy.
Michal Laclavik, Martin ˇSeleng, and Ladislav Hluch´y.
2008. Towards large scale semantic annotation built
on mapreduce architecture. In ICCS ’08: Proceed-
ings of the 8th international conference on Compu-
tational Science, Part III, pages 331–338, Berlin,
Heidelberg. Springer-Verlag.
Sheng Liang. 1999. Java Native Interface: Program-
mer’s Guide and Reference. Addison-Wesley Long-
man Publishing Co., Inc., Boston, MA, USA.
Edward Loper and Steven Bird. 2002. Nltk: the natu-
ral language toolkit. In Proceedings of the ACL-02
Workshop on Effective tools and methodologies for
teaching natural language processing and compu-
tational linguistics, pages 63–70, Morristown, NJ,
USA. Association for Computational Linguistics.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Y. Hirano, Hiroshi Matsuda, and Masayuki Asa-
hara, 1999. Japanese morphological analysis sys-
tem ChaSen version 2.0 manual 2nd edition. Nara
Institute of Science and Technology, technical report
naist-istr99009 edition.
Jos Carlos Medeiros. 1995. Processamento morfol-
gico e correco ortogrfica do portugułs. Master’s the-
sis, Insituto Superior Tcnico – Universidade Tcnica
de Lisboa, Portugal.
Joana Lcio Paulo. 2001. PAsMo - Ps Analisador Mor-
folgico. Master’s thesis, Insituto Superior Tcnico –
Universidade Tcnica de Lisboa, Portugal.
Ricardo Ribeiro, Nuno J. Mamede, and Isabel Tran-
coso. 2003. Using Morphossyntactic Information
in TTS Systems: comparing strategies for European
Portuguese. In Computational Processing of the
Portuguese Language: 6th International Workshop,
PROPOR 2003, Faro, Portugal, June 26-27, 2003.
Proceedings, volume 2721 of Lecture Notes in Com-
puter Science. Springer.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of HLT-NAACL 2003, pages 252–
259.
Kristina Toutanova. 2000. Enriching the knowledge
sources used in a maximum entropy part-of-speech
tagger. In Proceedings ofEMNLP/VLC 2000, pages
63–70.
S. E. Wright. 2004. A global data category registry for
interoperable language resources. In Proceedings
of the Fourth Language Resources and Evaluation
Conference – LREC 2004, pages 123–126. ELRA
European Language Resources Association.
</reference>
<page confidence="0.998666">
107
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.191849">
<title confidence="0.999976">High-Performance High-Volume Layered Corpora Annotation</title>
<author confidence="0.867206">Tiago Luis</author>
<author confidence="0.867206">David Martins de</author>
<note confidence="0.366961">R. Alves Redol 9, Lisboa,</note>
<abstract confidence="0.998385142857143">NLP systems that deal with large collections of text require significant computational resources, both in terms of space and processing time. Moreover, these systems typically add new layers of linguistic information with references to another layer. The spreading of these layered annotations across different files makes them more difficult to process and access the data. As the amount of input increases, so does the difficulty to process it. One approach is to use distributed parallel computing for solving these larger problems and save time. We propose a framework that simplifies the integration of independently existing NLP tools to build language-independent NLP systems capable of creating layered annotations. Moreover, it allows the development of scalable NLP systems, that executes NLP tools in parallel, while offering an easy-to-use programming environment and a transparent handling of distributed computing problems. With this framework the execution time was decreased to 40 times less than the original one on a cluster with 80 cores.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kalina Bontcheva</author>
<author>Valentin Tablan</author>
<author>Diana Maynard</author>
<author>Hamish Cunningham</author>
</authors>
<title>Evolving gate to meet new challenges in language engineering.</title>
<date>2004</date>
<journal>Nat. Lang. Eng.,</journal>
<pages>10--3</pages>
<contexts>
<context position="4513" citStr="Bontcheva et al., 2004" startWordPosition="697" endWordPosition="700"> Section 2 discusses related work, Section 3 present the framework’s architecture and a detailed description of its components, Section 4 shows the integrated tools, Section 5 explains how the information produced by tools is merged, and Section 6 presents the achieved results. Finally, Section 7 presents concluding remarks. 2 Related Work GATE (Cunningham et al., 2002) is one of the most used framework for building NLP systems. However, it does not provide a controller for parallel execution, it only supports the execution of applications on different machines over data shared on the server (Bontcheva et al., 2004). However, this solution cannot be applied in a large-scale distributed environment because the shared repository becomes a bottleneck in computation due to the accesses from all the machines making computations. UIMA (Ferrucci and Lally, 2004) is also used to build NLP systems, and this framework supports replication of pipeline components to improve throughput on multi-processor or multimachine platforms. However, we did not find any published results regarding the parallel execution. The UIMA framework has been successfully leveraged (Egner et al., 2007) with Condor1, a manager of loosely c</context>
</contexts>
<marker>Bontcheva, Tablan, Maynard, Cunningham, 2004</marker>
<rawString>Kalina Bontcheva, Valentin Tablan, Diana Maynard, and Hamish Cunningham. 2004. Evolving gate to meet new challenges in language engineering. Nat. Lang. Eng., 10(3-4):349–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Buckwalter</author>
</authors>
<title>Buckwalter Arabic Morphological Analyzer Version 1.0. Linguistic Data Consortium, catalog number LDC2002L49,</title>
<date>2002</date>
<journal>ISBN</journal>
<pages>1--58563</pages>
<contexts>
<context position="18129" citStr="Buckwalter, 2002" startWordPosition="2878" endWordPosition="2880">che.org/thrift/ 4 Applications The tools that have been integrated can be divided into two classes: those capable of producing first level annotations and those capable of producing second level annotations. The first level tools produce morphosyntactic annotation from an input text. Second level tools receive morphosyntactic information as input and produce morphosyntactic annotations. To show the language independence of the framework, we integrated tools from four different languages: Arabic, Japanese, English, and Portuguese. One of the tools capable of analyzing Arabic texts is AraMorph (Buckwalter, 2002), a Java-based Arabic morphological analyzer. For the Japanese language we chose Chasen (Matsumoto et al., 1999), a morphological analyzer capable of processing Japanese texts. The Stanford POS Tagger (Toutanova, 2000; Toutanova et al., 2003) is only being used to process English texts but it can be easily adapted (by changing its input dictionary) to process other languages, like Chinese or German. For processing Portuguese, we chose the Palavroso morphological analyzer (Medeiros, 1995). The morphological analyzers previously described produce first level annotations, i.e., they receive text </context>
</contexts>
<marker>Buckwalter, 2002</marker>
<rawString>Tim Buckwalter. 2002. Buckwalter Arabic Morphological Analyzer Version 1.0. Linguistic Data Consortium, catalog number LDC2002L49, ISBN 1-58563-257-0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cunningham</author>
<author>D Maynard</author>
<author>K Bontcheva</author>
<author>V Tablan</author>
</authors>
<title>Gate: A framework and graphical development environment for robust nlp tools and applications.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="4262" citStr="Cunningham et al., 2002" startWordPosition="654" endWordPosition="657">hat can be achieved with the parallel execution of NLP 99 Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 99–107, Suntec, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP tools and the merging of the layered annotations. Section 2 discusses related work, Section 3 present the framework’s architecture and a detailed description of its components, Section 4 shows the integrated tools, Section 5 explains how the information produced by tools is merged, and Section 6 presents the achieved results. Finally, Section 7 presents concluding remarks. 2 Related Work GATE (Cunningham et al., 2002) is one of the most used framework for building NLP systems. However, it does not provide a controller for parallel execution, it only supports the execution of applications on different machines over data shared on the server (Bontcheva et al., 2004). However, this solution cannot be applied in a large-scale distributed environment because the shared repository becomes a bottleneck in computation due to the accesses from all the machines making computations. UIMA (Ferrucci and Lally, 2004) is also used to build NLP systems, and this framework supports replication of pipeline components to imp</context>
</contexts>
<marker>Cunningham, Maynard, Bontcheva, Tablan, 2002</marker>
<rawString>H. Cunningham, D. Maynard, K. Bontcheva, and V. Tablan. 2002. Gate: A framework and graphical development environment for robust nlp tools and applications. In Proceedings of the 40th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Dean</author>
<author>Sanjay Ghemawat</author>
</authors>
<title>MapReduce: simplified data processing on large clusters.</title>
<date>2008</date>
<journal>Commun. ACM,</journal>
<volume>51</volume>
<issue>1</issue>
<contexts>
<context position="6887" citStr="Dean and Ghemawat, 2008" startWordPosition="1056" endWordPosition="1059">l programming approaches, Message Passing Interface (MPI) (Gropp, 2001) continues to be widely used in parallel programming and therefore there are currently many libraries built based on this programming model. However, this approach provides very low level routines that are difficult to use and make for obscure algorithm implementation, making code reuse and maintenance difficult and time consuming. MPI programming can be difficult because it is necessary to divide the problem among processes with separate address spaces and coordinate these processes with communication routines. MapReduce (Dean and Ghemawat, 2008) forces the programmer to consider the data parallelism of the computation. Also, this framework automatically schedules and distributes data to tasks. The simple API provided by the system allows programmers to write simple serial programs that are run in a distributed way, while hiding several parallel programming details. Therefore, this framework is accessible to a wide range of developers and allows them to write their applications at a higher level of abstraction than the MPI approach. 3 Framework Architecture Our framework aims to simplify the integration of independently developed NLP </context>
<context position="8782" citStr="Dean and Ghemawat, 2008" startWordPosition="1363" endWordPosition="1366">each fragment is represented by a Unit). These independent Units are then processed in parallel. Previously created annotations are already divided, since they correspond to an annotation that refers the corresponding input fragment. Tools are wrapped in Stage components. Stages have two queues: an input and an output queue of Units. Stages are responsible for consuming input queue Units, pass them to the Tool and, after their processing, put the result on output queue. These queues allow multithreaded consumption and production of the Units. The framework was implemented using the MapReduce (Dean and Ghemawat, 2008) paradigm due to its scalability when dealing with large data volumes. The Hadoop2 framework (described in the next section) was used as the base for implementation. The next sections describe the representation format used for annotations, the input accepted, and the framework components in more detail. 3.1 Hadoop Hadoop is a MapReduce implementation written in Java. One of the main advantages of using the MapReduce paradigm is task scheduling. When dealing with large datasets in a distributed manner, bandwidth to data becomes a problem. The MapReduce paradigm and the Hadoop Distributed File </context>
</contexts>
<marker>Dean, Ghemawat, 2008</marker>
<rawString>Jeffrey Dean and Sanjay Ghemawat. 2008. MapReduce: simplified data processing on large clusters. Commun. ACM, 51(1):107–113, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Thomas Egner</author>
<author>Markus Lorch</author>
<author>Edd Biddle</author>
</authors>
<title>Uima grid: Distributed large-scale text analysis.</title>
<date>2007</date>
<booktitle>In CCGRID ’07: Proceedings of the Seventh IEEE International Symposium on Cluster Computing and the Grid,</booktitle>
<pages>317--326</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="5076" citStr="Egner et al., 2007" startWordPosition="785" endWordPosition="788"> over data shared on the server (Bontcheva et al., 2004). However, this solution cannot be applied in a large-scale distributed environment because the shared repository becomes a bottleneck in computation due to the accesses from all the machines making computations. UIMA (Ferrucci and Lally, 2004) is also used to build NLP systems, and this framework supports replication of pipeline components to improve throughput on multi-processor or multimachine platforms. However, we did not find any published results regarding the parallel execution. The UIMA framework has been successfully leveraged (Egner et al., 2007) with Condor1, a manager of loosely coupled compute resources, allowing the parallel execution of multiple instances of the NLP system built with UIMA. The Condor scheduler allows to solve problems where there is no communication between tasks and complicates the development of parallel applications when this interaction is needed, like in our case, where it is necessary to merge multiple layers of annotations. Also, the Condor does not move computations closer to their input data, like the MapReduce approach. The MapReduce paradigm has already been successfully adopted by the Ontea semantic a</context>
</contexts>
<marker>Egner, Lorch, Biddle, 2007</marker>
<rawString>Michael Thomas Egner, Markus Lorch, and Edd Biddle. 2007. Uima grid: Distributed large-scale text analysis. In CCGRID ’07: Proceedings of the Seventh IEEE International Symposium on Cluster Computing and the Grid, pages 317–326, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Ferrucci</author>
<author>Adam Lally</author>
</authors>
<title>Uima: an architectural approach to unstructured information processing in the corporate research environment.</title>
<date>2004</date>
<journal>Nat. Lang. Eng.,</journal>
<pages>10--3</pages>
<contexts>
<context position="4757" citStr="Ferrucci and Lally, 2004" startWordPosition="735" endWordPosition="738">ion 6 presents the achieved results. Finally, Section 7 presents concluding remarks. 2 Related Work GATE (Cunningham et al., 2002) is one of the most used framework for building NLP systems. However, it does not provide a controller for parallel execution, it only supports the execution of applications on different machines over data shared on the server (Bontcheva et al., 2004). However, this solution cannot be applied in a large-scale distributed environment because the shared repository becomes a bottleneck in computation due to the accesses from all the machines making computations. UIMA (Ferrucci and Lally, 2004) is also used to build NLP systems, and this framework supports replication of pipeline components to improve throughput on multi-processor or multimachine platforms. However, we did not find any published results regarding the parallel execution. The UIMA framework has been successfully leveraged (Egner et al., 2007) with Condor1, a manager of loosely coupled compute resources, allowing the parallel execution of multiple instances of the NLP system built with UIMA. The Condor scheduler allows to solve problems where there is no communication between tasks and complicates the development of pa</context>
</contexts>
<marker>Ferrucci, Lally, 2004</marker>
<rawString>David Ferrucci and Adam Lally. 2004. Uima: an architectural approach to unstructured information processing in the corporate research environment. Nat. Lang. Eng., 10(3-4):327–348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Gropp</author>
</authors>
<title>Learning from the Success of MPI.</title>
<date>2001</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>2228</volume>
<pages>81--94</pages>
<editor>In Burkhard Monien, Viktor K. Prasanna, and Sriram Vajapeyam, editors, HiPC,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="6334" citStr="Gropp, 2001" startWordPosition="977" endWordPosition="978">t GATE and UIMA frameworks could also benefit with the MapReduce. For example, the NLTK (Loper and Bird, 2002) adopted this paradigm, and already have implementations of some algorithms like term frequency-inverse document frequency (tf-idf) or expectation-maximization (EM). There are already tools for merging of layered 1http://www.cs.wisc.edu/condor/ Figure 1: Framework architecture. annotations, like the ANC tool (Ide and Suderman, 2006). However, we did not find any approach to this task in a scalable manner. Concerning the parallel programming approaches, Message Passing Interface (MPI) (Gropp, 2001) continues to be widely used in parallel programming and therefore there are currently many libraries built based on this programming model. However, this approach provides very low level routines that are difficult to use and make for obscure algorithm implementation, making code reuse and maintenance difficult and time consuming. MPI programming can be difficult because it is necessary to divide the problem among processes with separate address spaces and coordinate these processes with communication routines. MapReduce (Dean and Ghemawat, 2008) forces the programmer to consider the data par</context>
</contexts>
<marker>Gropp, 2001</marker>
<rawString>William Gropp. 2001. Learning from the Success of MPI. In Burkhard Monien, Viktor K. Prasanna, and Sriram Vajapeyam, editors, HiPC, volume 2228 of Lecture Notes in Computer Science, pages 81–94. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Laurent Romary</author>
</authors>
<title>Representing linguistic corpora and their annotations.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth Language Resources and Evaluation Conference (LREC.</booktitle>
<contexts>
<context position="10831" citStr="Ide and Romary, 2006" startWordPosition="1689" endWordPosition="1692">ey/value pairs. Each map is an individual task that runs on a machine. The reduce phase creates a smaller set of key/value pairs from a set of intermediate values that have the same key. Since different mappers can output the same key, the framework groups reducer input key/value pairs with the same key. This grouping capability is used to merge annotations produced by different tools and that are related with each other, as shown is the Section 5. 3.2 Representation Format In order to represent linguistic information generated by the tools, we chose the Linguistic Annotation Framework (LAF) (Ide and Romary, 2006) format, that uses a graph model to store annotations. An annotation can be viewed as a set of linguistic information items that are associated with some data (a part of a text or speech signal, for example), called primary data. Primary data objects are represented by locations in the input. These locations can be the offset of a character comprising a sentence or word, in the case of a text input, or a point at which a given temporal event begins or ends, in the case of a speech signal input. As such, primary data objects have a simple structure. However, it is possible to build more complex</context>
</contexts>
<marker>Ide, Romary, 2006</marker>
<rawString>Nancy Ide and Laurent Romary. 2006. Representing linguistic corpora and their annotations. In Proceedings of the Fifth Language Resources and Evaluation Conference (LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Keith Suderman</author>
</authors>
<title>Merging layered annotations.</title>
<date>2006</date>
<booktitle>In Proceedings of Merging and Layering Linguistic Information,</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="6166" citStr="Ide and Suderman, 2006" startWordPosition="948" endWordPosition="952">o their input data, like the MapReduce approach. The MapReduce paradigm has already been successfully adopted by the Ontea semantic annotator (Laclavik et al., 2008). We think that GATE and UIMA frameworks could also benefit with the MapReduce. For example, the NLTK (Loper and Bird, 2002) adopted this paradigm, and already have implementations of some algorithms like term frequency-inverse document frequency (tf-idf) or expectation-maximization (EM). There are already tools for merging of layered 1http://www.cs.wisc.edu/condor/ Figure 1: Framework architecture. annotations, like the ANC tool (Ide and Suderman, 2006). However, we did not find any approach to this task in a scalable manner. Concerning the parallel programming approaches, Message Passing Interface (MPI) (Gropp, 2001) continues to be widely used in parallel programming and therefore there are currently many libraries built based on this programming model. However, this approach provides very low level routines that are difficult to use and make for obscure algorithm implementation, making code reuse and maintenance difficult and time consuming. MPI programming can be difficult because it is necessary to divide the problem among processes wit</context>
</contexts>
<marker>Ide, Suderman, 2006</marker>
<rawString>Nancy Ide and Keith Suderman. 2006. Merging layered annotations. In Proceedings of Merging and Layering Linguistic Information, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michal Laclavik</author>
<author>Martin ˇSeleng</author>
<author>Ladislav Hluch´y</author>
</authors>
<title>Towards large scale semantic annotation built on mapreduce architecture.</title>
<date>2008</date>
<booktitle>In ICCS ’08: Proceedings of the 8th international conference on Computational Science, Part III,</booktitle>
<pages>331--338</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<marker>Laclavik, ˇSeleng, Hluch´y, 2008</marker>
<rawString>Michal Laclavik, Martin ˇSeleng, and Ladislav Hluch´y. 2008. Towards large scale semantic annotation built on mapreduce architecture. In ICCS ’08: Proceedings of the 8th international conference on Computational Science, Part III, pages 331–338, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sheng Liang</author>
</authors>
<title>Java Native Interface: Programmer’s Guide and Reference.</title>
<date>1999</date>
<publisher>Addison-Wesley Longman Publishing Co., Inc.,</publisher>
<location>Boston, MA, USA.</location>
<contexts>
<context position="17406" citStr="Liang, 1999" startWordPosition="2773" endWordPosition="2774"> in order to add the information produced by the existing NLP tool in the LAF format. Because the framework is written in Java, and the tools could have been developed in a different language, such as C++ or Perl, it was necessary to find a way to interact with other programming languages. Hence, an existing tool can be integrated in various ways. If a tool provides an API, we currently provide an Remote Procedure Call (RPC) mechanism with the Thrift5 software library. If the API can be used in a C/C++ program, it is also possible to use the existing tool API with Java Native Interface (JNI) (Liang, 1999). The framework also supports tools that can only be executed from the command line. 5http://incubator.apache.org/thrift/ 4 Applications The tools that have been integrated can be divided into two classes: those capable of producing first level annotations and those capable of producing second level annotations. The first level tools produce morphosyntactic annotation from an input text. Second level tools receive morphosyntactic information as input and produce morphosyntactic annotations. To show the language independence of the framework, we integrated tools from four different languages: A</context>
</contexts>
<marker>Liang, 1999</marker>
<rawString>Sheng Liang. 1999. Java Native Interface: Programmer’s Guide and Reference. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Loper</author>
<author>Steven Bird</author>
</authors>
<title>Nltk: the natural language toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 Workshop on Effective tools and methodologies for teaching natural language processing and computational linguistics,</booktitle>
<pages>63--70</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="5832" citStr="Loper and Bird, 2002" startWordPosition="906" endWordPosition="909">built with UIMA. The Condor scheduler allows to solve problems where there is no communication between tasks and complicates the development of parallel applications when this interaction is needed, like in our case, where it is necessary to merge multiple layers of annotations. Also, the Condor does not move computations closer to their input data, like the MapReduce approach. The MapReduce paradigm has already been successfully adopted by the Ontea semantic annotator (Laclavik et al., 2008). We think that GATE and UIMA frameworks could also benefit with the MapReduce. For example, the NLTK (Loper and Bird, 2002) adopted this paradigm, and already have implementations of some algorithms like term frequency-inverse document frequency (tf-idf) or expectation-maximization (EM). There are already tools for merging of layered 1http://www.cs.wisc.edu/condor/ Figure 1: Framework architecture. annotations, like the ANC tool (Ide and Suderman, 2006). However, we did not find any approach to this task in a scalable manner. Concerning the parallel programming approaches, Message Passing Interface (MPI) (Gropp, 2001) continues to be widely used in parallel programming and therefore there are currently many librar</context>
</contexts>
<marker>Loper, Bird, 2002</marker>
<rawString>Edward Loper and Steven Bird. 2002. Nltk: the natural language toolkit. In Proceedings of the ACL-02 Workshop on Effective tools and methodologies for teaching natural language processing and computational linguistics, pages 63–70, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuji Matsumoto</author>
<author>Akira Kitauchi</author>
<author>Tatsuo Yamashita</author>
<author>Y Hirano</author>
<author>Hiroshi Matsuda</author>
<author>Masayuki Asahara</author>
</authors>
<date>1999</date>
<booktitle>Japanese morphological analysis system ChaSen version 2.0 manual 2nd edition. Nara Institute of Science and Technology, technical report naist-istr99009</booktitle>
<pages>edition.</pages>
<contexts>
<context position="18241" citStr="Matsumoto et al., 1999" startWordPosition="2894" endWordPosition="2897">capable of producing first level annotations and those capable of producing second level annotations. The first level tools produce morphosyntactic annotation from an input text. Second level tools receive morphosyntactic information as input and produce morphosyntactic annotations. To show the language independence of the framework, we integrated tools from four different languages: Arabic, Japanese, English, and Portuguese. One of the tools capable of analyzing Arabic texts is AraMorph (Buckwalter, 2002), a Java-based Arabic morphological analyzer. For the Japanese language we chose Chasen (Matsumoto et al., 1999), a morphological analyzer capable of processing Japanese texts. The Stanford POS Tagger (Toutanova, 2000; Toutanova et al., 2003) is only being used to process English texts but it can be easily adapted (by changing its input dictionary) to process other languages, like Chinese or German. For processing Portuguese, we chose the Palavroso morphological analyzer (Medeiros, 1995). The morphological analyzers previously described produce first level annotations, i.e., they receive text as input and produce annotations. Besides these tools, we also integrated types of tools for testing second leve</context>
</contexts>
<marker>Matsumoto, Kitauchi, Yamashita, Hirano, Matsuda, Asahara, 1999</marker>
<rawString>Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, Y. Hirano, Hiroshi Matsuda, and Masayuki Asahara, 1999. Japanese morphological analysis system ChaSen version 2.0 manual 2nd edition. Nara Institute of Science and Technology, technical report naist-istr99009 edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos Carlos Medeiros</author>
</authors>
<title>Processamento morfolgico e correco ortogrfica do portugułs. Master’s thesis, Insituto Superior Tcnico – Universidade Tcnica de</title>
<date>1995</date>
<location>Lisboa, Portugal.</location>
<contexts>
<context position="18621" citStr="Medeiros, 1995" startWordPosition="2956" endWordPosition="2957">rabic, Japanese, English, and Portuguese. One of the tools capable of analyzing Arabic texts is AraMorph (Buckwalter, 2002), a Java-based Arabic morphological analyzer. For the Japanese language we chose Chasen (Matsumoto et al., 1999), a morphological analyzer capable of processing Japanese texts. The Stanford POS Tagger (Toutanova, 2000; Toutanova et al., 2003) is only being used to process English texts but it can be easily adapted (by changing its input dictionary) to process other languages, like Chinese or German. For processing Portuguese, we chose the Palavroso morphological analyzer (Medeiros, 1995). The morphological analyzers previously described produce first level annotations, i.e., they receive text as input and produce annotations. Besides these tools, we also integrated types of tools for testing second level annotations: RuDriCo (Paulo, 2001) and JMARv (Ribeiro et al., 2003). RuDriCo is a post-morphological analyzer that rewrites the results of a morphological analyzer. RuDriCo uses declarative transformation rules based on pattern matching. JMARv is a tool that performs morphosyntactic disambiguation (selects a classification from the possible classifications in each segment fro</context>
</contexts>
<marker>Medeiros, 1995</marker>
<rawString>Jos Carlos Medeiros. 1995. Processamento morfolgico e correco ortogrfica do portugułs. Master’s thesis, Insituto Superior Tcnico – Universidade Tcnica de Lisboa, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joana Lcio Paulo</author>
</authors>
<title>PAsMo - Ps Analisador Morfolgico. Master’s thesis, Insituto Superior Tcnico – Universidade Tcnica de</title>
<date>2001</date>
<location>Lisboa, Portugal.</location>
<contexts>
<context position="18877" citStr="Paulo, 2001" startWordPosition="2992" endWordPosition="2993">er capable of processing Japanese texts. The Stanford POS Tagger (Toutanova, 2000; Toutanova et al., 2003) is only being used to process English texts but it can be easily adapted (by changing its input dictionary) to process other languages, like Chinese or German. For processing Portuguese, we chose the Palavroso morphological analyzer (Medeiros, 1995). The morphological analyzers previously described produce first level annotations, i.e., they receive text as input and produce annotations. Besides these tools, we also integrated types of tools for testing second level annotations: RuDriCo (Paulo, 2001) and JMARv (Ribeiro et al., 2003). RuDriCo is a post-morphological analyzer that rewrites the results of a morphological analyzer. RuDriCo uses declarative transformation rules based on pattern matching. JMARv is a tool that performs morphosyntactic disambiguation (selects a classification from the possible classifications in each segment from the input sequence). The two previous tools were used for processing Portuguese morphosyntactic information, but can be easily adapted to process other languages. For example, JMARv could be used to disambiguate AraMorph classifications and RuDriCo could</context>
</contexts>
<marker>Paulo, 2001</marker>
<rawString>Joana Lcio Paulo. 2001. PAsMo - Ps Analisador Morfolgico. Master’s thesis, Insituto Superior Tcnico – Universidade Tcnica de Lisboa, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ricardo Ribeiro</author>
<author>Nuno J Mamede</author>
<author>Isabel Trancoso</author>
</authors>
<title>Using Morphossyntactic Information in TTS Systems: comparing strategies for European Portuguese.</title>
<date>2003</date>
<booktitle>In Computational Processing of the Portuguese Language: 6th International Workshop, PROPOR 2003,</booktitle>
<volume>2721</volume>
<publisher>Springer.</publisher>
<location>Faro, Portugal,</location>
<contexts>
<context position="18910" citStr="Ribeiro et al., 2003" startWordPosition="2996" endWordPosition="2999"> Japanese texts. The Stanford POS Tagger (Toutanova, 2000; Toutanova et al., 2003) is only being used to process English texts but it can be easily adapted (by changing its input dictionary) to process other languages, like Chinese or German. For processing Portuguese, we chose the Palavroso morphological analyzer (Medeiros, 1995). The morphological analyzers previously described produce first level annotations, i.e., they receive text as input and produce annotations. Besides these tools, we also integrated types of tools for testing second level annotations: RuDriCo (Paulo, 2001) and JMARv (Ribeiro et al., 2003). RuDriCo is a post-morphological analyzer that rewrites the results of a morphological analyzer. RuDriCo uses declarative transformation rules based on pattern matching. JMARv is a tool that performs morphosyntactic disambiguation (selects a classification from the possible classifications in each segment from the input sequence). The two previous tools were used for processing Portuguese morphosyntactic information, but can be easily adapted to process other languages. For example, JMARv could be used to disambiguate AraMorph classifications and RuDriCo could translate Chasen’s Japanese POS </context>
</contexts>
<marker>Ribeiro, Mamede, Trancoso, 2003</marker>
<rawString>Ricardo Ribeiro, Nuno J. Mamede, and Isabel Trancoso. 2003. Using Morphossyntactic Information in TTS Systems: comparing strategies for European Portuguese. In Computational Processing of the Portuguese Language: 6th International Workshop, PROPOR 2003, Faro, Portugal, June 26-27, 2003. Proceedings, volume 2721 of Lecture Notes in Computer Science. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<contexts>
<context position="18371" citStr="Toutanova et al., 2003" startWordPosition="2913" endWordPosition="2916">e morphosyntactic annotation from an input text. Second level tools receive morphosyntactic information as input and produce morphosyntactic annotations. To show the language independence of the framework, we integrated tools from four different languages: Arabic, Japanese, English, and Portuguese. One of the tools capable of analyzing Arabic texts is AraMorph (Buckwalter, 2002), a Java-based Arabic morphological analyzer. For the Japanese language we chose Chasen (Matsumoto et al., 1999), a morphological analyzer capable of processing Japanese texts. The Stanford POS Tagger (Toutanova, 2000; Toutanova et al., 2003) is only being used to process English texts but it can be easily adapted (by changing its input dictionary) to process other languages, like Chinese or German. For processing Portuguese, we chose the Palavroso morphological analyzer (Medeiros, 1995). The morphological analyzers previously described produce first level annotations, i.e., they receive text as input and produce annotations. Besides these tools, we also integrated types of tools for testing second level annotations: RuDriCo (Paulo, 2001) and JMARv (Ribeiro et al., 2003). RuDriCo is a post-morphological analyzer that rewrites the </context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of HLT-NAACL 2003,</booktitle>
<pages>252--259</pages>
<marker></marker>
<rawString>In Proceedings of HLT-NAACL 2003, pages 252– 259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
</authors>
<title>Enriching the knowledge sources used in a maximum entropy part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings ofEMNLP/VLC</booktitle>
<pages>63--70</pages>
<contexts>
<context position="18346" citStr="Toutanova, 2000" startWordPosition="2911" endWordPosition="2912">evel tools produce morphosyntactic annotation from an input text. Second level tools receive morphosyntactic information as input and produce morphosyntactic annotations. To show the language independence of the framework, we integrated tools from four different languages: Arabic, Japanese, English, and Portuguese. One of the tools capable of analyzing Arabic texts is AraMorph (Buckwalter, 2002), a Java-based Arabic morphological analyzer. For the Japanese language we chose Chasen (Matsumoto et al., 1999), a morphological analyzer capable of processing Japanese texts. The Stanford POS Tagger (Toutanova, 2000; Toutanova et al., 2003) is only being used to process English texts but it can be easily adapted (by changing its input dictionary) to process other languages, like Chinese or German. For processing Portuguese, we chose the Palavroso morphological analyzer (Medeiros, 1995). The morphological analyzers previously described produce first level annotations, i.e., they receive text as input and produce annotations. Besides these tools, we also integrated types of tools for testing second level annotations: RuDriCo (Paulo, 2001) and JMARv (Ribeiro et al., 2003). RuDriCo is a post-morphological an</context>
</contexts>
<marker>Toutanova, 2000</marker>
<rawString>Kristina Toutanova. 2000. Enriching the knowledge sources used in a maximum entropy part-of-speech tagger. In Proceedings ofEMNLP/VLC 2000, pages 63–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Wright</author>
</authors>
<title>A global data category registry for interoperable language resources.</title>
<date>2004</date>
<journal>ELRA European Language Resources Association.</journal>
<booktitle>In Proceedings of the Fourth Language Resources and Evaluation Conference – LREC 2004,</booktitle>
<pages>123--126</pages>
<contexts>
<context position="28858" citStr="Wright, 2004" startWordPosition="4684" endWordPosition="4685">ting are transparently solved by the platform. The MapReduce sort/grouping capabilities has been used to efficiently merge layered annotations produced by tools integrated in the framework. Regarding future work, on the linguistic part, we plan to integrate tools that produce syntactic annotations (the LAF format already supports these annotations). This linguistic information can be merged with the current tree by simply adding more nodes above the nodes that contain the morphosyntactic annotations. Also, this work did not focus on information normalization. The Data Category Registry (DCR) (Wright, 2004) could be explored in the future, in order to improve interoperability between linguistic resources. Finally, the creation of NLP systems can be simplified by an XML parametrization. This way it is possible to compose a tool pipeline by simply editing an XML file. An graphical environment for visualization and editing of LAF annotations is also useful. Our code is available at http://code. google.com/p/anota/. Acknowledgments This work was supported by the partnership between Carnegie Mellon University and Portugal’s National Science and Technology Foundation (FCT – Fundac¸˜ao para a Ciˆencia </context>
</contexts>
<marker>Wright, 2004</marker>
<rawString>S. E. Wright. 2004. A global data category registry for interoperable language resources. In Proceedings of the Fourth Language Resources and Evaluation Conference – LREC 2004, pages 123–126. ELRA European Language Resources Association.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>