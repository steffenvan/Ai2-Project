<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002938">
<title confidence="0.997244">
A Robust Approach to Aligning Heterogeneous Lexical Resources
</title>
<author confidence="0.892747">
Mohammad Taher Pilehvar and Roberto Navigli
</author>
<affiliation confidence="0.9981595">
Department of Computer Science
Sapienza University of Rome
</affiliation>
<email confidence="0.982122">
{pilehvar,navigli}@di.uniroma1.it
</email>
<sectionHeader confidence="0.997121" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999632111111111">
Lexical resource alignment has been an
active field of research over the last
decade. However, prior methods for align-
ing lexical resources have been either spe-
cific to a particular pair of resources, or
heavily dependent on the availability of
hand-crafted alignment data for the pair of
resources to be aligned. Here we present a
unified approach that can be applied to an
arbitrary pair of lexical resources, includ-
ing machine-readable dictionaries with no
network structure. Our approach leverages
a similarity measure that enables the struc-
tural comparison of senses across lexical
resources, achieving state-of-the-art per-
formance on the task of aligning WordNet
to three different collaborative resources:
Wikipedia, Wiktionary and OmegaWiki.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971048387097">
Lexical resources are repositories of machine-
readable knowledge that can be used in virtually
any Natural Language Processing task. Notable
examples are WordNet, Wikipedia and, more re-
cently, collaboratively-curated resources such as
OmegaWiki and Wiktionary (Hovy et al., 2013).
On the one hand, these resources are heteroge-
neous in design, structure and content, but, on
the other hand, they often provide complemen-
tary knowledge which we would like to see inte-
grated. Given the large scale this intrinsic issue
can only be addressed automatically, by means of
lexical resource alignment algorithms. Owing to
its ability to bring together features like multilin-
guality and increasing coverage, over the past few
years resource alignment has proven beneficial to
a wide spectrum of tasks, such as Semantic Pars-
ing (Shi and Mihalcea, 2005), Semantic Role La-
beling (Palmer et al., 2010), and Word Sense Dis-
ambiguation (Navigli and Ponzetto, 2012).
Nevertheless, when it comes to aligning textual
definitions in different resources, the lexical ap-
proach (Ruiz-Casado et al., 2005; de Melo and
Weikum, 2010; Henrich et al., 2011) falls short
because of the potential use of totally different
wordings to define the same concept. Deeper ap-
proaches leverage semantic similarity to go be-
yond the surface realization of definitions (Nav-
igli, 2006; Meyer and Gurevych, 2011; Niemann
and Gurevych, 2011). While providing good re-
sults in general, these approaches fail when the
definitions of a given word are not of adequate
quality and expressiveness to be distinguishable
from one another. When a lexical resource can be
viewed as a semantic graph, as with WordNet or
Wikipedia, this limit can be overcome by means
of alignment algorithms that exploit the network
structure to determine the similarity of concept
pairs. However, not all lexical resources provide
explicit semantic relations between concepts and,
hence, machine-readable dictionaries like Wik-
tionary have first to be transformed into semantic
graphs before such graph-based approaches can be
applied to them. To do this, recent work has pro-
posed graph construction by monosemous linking,
where a concept is linked to all the concepts asso-
ciated with the monosemous words in its definition
(Matuschek and Gurevych, 2013). However, this
alignment method still involves tuning of parame-
ters which are highly dependent on the character-
istics of the generated graphs and, hence, requires
hand-crafted sense alignments for the specific pair
of resources to be aligned, a task which has to be
replicated every time the resources are updated.
In this paper we propose a unified approach
to aligning arbitrary pairs of lexical resources
which is independent of their specific structure.
Thanks to a novel modeling of the sense entries
and an effective ontologization algorithm, our ap-
proach also fares well when resources lack rela-
tional structure or pair-specific training data is ab-
sent, meaning that it is applicable to arbitrary pairs
</bodyText>
<page confidence="0.988862">
468
</page>
<note confidence="0.867257">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 468–478,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.674696666666667">
without adaptation. We report state-of-the-art per-
formance when aligning WordNet to Wikipedia,
OmegaWiki and Wiktionary.
</bodyText>
<sectionHeader confidence="0.993363" genericHeader="method">
2 Resource Alignment
</sectionHeader>
<bodyText confidence="0.979526862745098">
Preliminaries. Our approach for aligning lexi-
cal resources exploits the graph structure of each
resource. Therefore, we assume that a lexical
resource L can be represented as an undirected
graph G = (V, E) where V is the set of nodes,
i.e., the concepts defined in the resource, and
E is the set of undirected edges, i.e., seman-
tic relations between concepts. Each concept
c E V is associated with a set of lexicalizations
LG(c) = {w1, w2,..., wnj. For instance, Word-
Net can be readily represented as an undirected
graph G whose nodes are synsets and edges are
modeled after the relations between synsets de-
fined in WordNet (e.g., hypernymy, meronymy,
etc.), and LG is the mapping between each synset
node and the set of synonyms which express the
concept. However, other resources such as Wik-
tionary do not provide semantic relations between
concepts and, therefore, have first to be trans-
formed into semantic networks before they can be
aligned using our alignment algorithm. We ex-
plain in Section 3 how a semi-structured resource
which does not exhibit a graph structure can be
transformed into a semantic network.
Alignment algorithm. Given a pair of lexical
resources L1 and L2, we align each concept in L1
by mapping it to its corresponding concept(s) in
the target lexicon L2. Algorithm 1 formalizes the
alignment process: the algorithm takes as input the
semantic graphs G1 and G2 corresponding to the
two resources, as explained above, and produces
as output an alignment in the form of a set A of
concept pairs. The algorithm iterates over all con-
cepts c1 E V1 and, for each of them, obtains the set
of concepts C C V2, which can be considered as
alignment candidates for c1 (line 3). For a concept
c1, alignment candidates in G2 usually consist of
every concept c2 E V2 that shares at least one lex-
icalization with c1 in the same part of speech tag,
i.e., LG1(c1) n LG2(c2) =� 0 (Reiter et al., 2008;
Meyer and Gurevych, 2011). Once the set of target
candidates C for a source concept c1 is obtained,
the alignment task can be cast as that of identifying
those concepts in C to which c1 should be aligned.
To do this, the algorithm calculates the similarity
between c1 and each c2 E C (line 5). If their sim-
ilarity score exceeds a certain value denoted by θ
Algorithm 1 Lexical Resource Aligner
Input: graphs H = (VH, EH), G1 = (V1, E1) and G2 =
(V2, E2), the similarity threshold B, and the combination
parameter β
</bodyText>
<listItem confidence="0.942248888888889">
Output: A, the set of all aligned concept pairs
1: A +— 0
2: for each concept c1 E V1
3: C +— getCandidates(c1, V2)
4: for each concept c2 E C
5: sim +— calculateSimilarity(H, G1, G2, c1, c2, β)
6: if sim &gt; B then
7: A +— A U {(c1, c2)}
8: return A
</listItem>
<bodyText confidence="0.9952345">
(line 6), the two concepts c1 and c2 are aligned and
the pair (c1, c2) is added to A (line 7).
Different resource alignment techniques usually
vary in the way they compute the similarity of a
pair of concepts across two resources (line 5 in Al-
gorithm 1). In the following, we present our novel
approach for measuring the similarity of concept
pairs.
</bodyText>
<subsectionHeader confidence="0.996606">
2.1 Measuring the Similarity of Concepts
</subsectionHeader>
<bodyText confidence="0.999914967741936">
Figure 1 illustrates the procedure underlying our
cross-resource concept similarity measurement
technique. As can be seen, the approach consists
of two main components: definitional similarity
and structural similarity. Each of these compo-
nents gets, as its input, a pair of concepts belong-
ing to two different semantic networks and pro-
duces a similarity score. These two scores are then
combined into an overall score (part (e) of Figure
1) which quantifies the semantic similarity of the
two input concepts c1 and c2.
The definitional similarity component computes
the similarity of two concepts in terms of the simi-
larity of their definitions, a method that has also
been used in previous work for aligning lexical
resources (Niemann and Gurevych, 2011; Hen-
rich et al., 2012). In spite of its simplicity, the
mere calculation of the similarity of concept defi-
nitions provides a strong baseline, especially for
cases where the definitional texts for a pair of
concepts to be aligned are lexically similar, yet
distinguishable from the other definitions. How-
ever, as mentioned in the introduction, definition
similarity-based techniques fail at identifying the
correct alignments in cases where different word-
ings are used or definitions are not of high qual-
ity. The structural similarity component, instead,
is a novel graph-based similarity measurement
technique which calculates the similarity between
a pair of concepts across the semantic networks
of the two resources by leveraging the semantic
</bodyText>
<page confidence="0.999748">
469
</page>
<figureCaption confidence="0.943726666666667">
Figure 1: The process of measuring the similarity of a pair of concepts across two resources. The method
consists of two components: definitional and structural similarities, each measuring a similarity score for
the given concept pair. The two scores are combined by means of parameter Q in the last stage.
</figureCaption>
<bodyText confidence="0.9984275">
structure of those networks. This component goes
beyond the surface realization of concepts, thus
providing a deeper measure of concept similarity.
The two components share the same backbone
(parts (b) and (d) of Figure 1), but differ in some
stages (parts (a) and (c) in Figure 1). In the follow-
ing, we explain all the stages involved in the two
components (gray blocks in the figure).
</bodyText>
<subsectionHeader confidence="0.933505">
2.1.1 Semantic signature generation
</subsectionHeader>
<bodyText confidence="0.999973328125">
The aim of this stage is to model a given concept
or set of concepts through a vectorial semantic
representation, which we refer to as the seman-
tic signature of the input. We utilized Person-
alized PageRank (Haveliwala, 2002, PPR), a ran-
dom walk graph algorithm, for calculating seman-
tic signatures. The original PageRank (PR) algo-
rithm (Brin and Page, 1998) computes, for a given
graph, a single vector wherein each node is as-
sociated with a weight denoting its structural im-
portance in that graph. PPR is a variation of PR
where the computation is biased towards a set of
initial nodes in order to capture the notion of im-
portance with respect to those particular nodes.
PPR has been previously used in a wide variety of
tasks such as definition similarity-based resource
alignment (Niemann and Gurevych, 2011), textual
semantic similarity (Hughes and Ramage, 2007;
Pilehvar et al., 2013), Word Sense Disambigua-
tion (Agirre and Soroa, 2009; Faralli and Navigli,
2012) and semantic text categorization (Navigli et
al., 2011). When applied to a semantic graph by
initializing the random walks from a set of con-
cepts (nodes), PPR yields a vector in which each
concept is associated with a weight denoting its
semantic relevance to the initial concepts.
Formally, we first represent a semantic network
consisting of N concepts as a row-stochastic tran-
sition matrix M E RN×N. The cell (i, j) in the
matrix denotes the probability of moving from a
concept i to j in the graph: 0 if no edge exists
from i to j and 1/degree(i) otherwise. Then the
PPR vector, hence the semantic signature Sv of
vector v is the unique solution to the linear sys-
tem: Sv = (1 − α) v + α M Sv, where v is the
personalization vector of size N in which all the
probability mass is put on the concepts for which
a semantic signature is to be computed and α is the
damping factor, which is usually set to 0.85 (Brin
and Page, 1998). We used the UKB1 off-the-shelf
implementation of PPR.
Definitional similarity signature. In the defini-
tional similarity component, the two concepts c1
and c2 are first represented by their corresponding
definitions d1 and d2 in the respective resources L1
and L2 (Figure 1(a), top). To improve expressive-
ness, we follow Niemann and Gurevych (2011)
and further extend di with all the word forms asso-
ciated with concept ci and its neighbours, i.e., the
union of all lexicalizations LGi(x) for all concepts
x E {c� E Vi : (c, c&apos;) E Ei} U {c}, where Ei is the
set of edges in Gi. In this component the person-
alization vector vi is set by uniformly distributing
the probability mass over the nodes correspond-
ing to the senses of all the content words in the
extended definition of di according to the sense
inventory of a semantic network H. We use the
same semantic graph H for computing the seman-
tic signatures of both definitions. Any semantic
network with a dense relational structure, provid-
ing good coverage of the words appearing in the
definitions, is a suitable candidate for H. For this
purpose we used the WordNet (Fellbaum, 1998)
graph which was further enriched by connecting
</bodyText>
<footnote confidence="0.988217">
1http://ixa2.si.ehu.es/ukb/
</footnote>
<page confidence="0.996412">
470
</page>
<bodyText confidence="0.9961215">
each concept to all the concepts appearing in its
disambiguated gloss.2
Structural similarity signature. In the struc-
tural similarity component (Figure 1(b), bottom),
the semantic signature for each concept ci is com-
puted by running the PPR algorithm on its corre-
sponding graph Gi, hence a different Mi is built
for each of the two concepts.
</bodyText>
<subsectionHeader confidence="0.690581">
2.1.2 Signature unification
</subsectionHeader>
<bodyText confidence="0.999889769230769">
As mentioned earlier, semantic signatures are vec-
tors with dimension equal to the number of nodes
in the semantic graph. Since the structural similar-
ity signatures Sv1 and Sv2 are calculated on differ-
ent graphs and thus have different dimensions, we
need to make them comparable by unifying them.
We therefore propose an approach (part (c) of Fig-
ure 1) that finds a common ground between the
two signatures: to this end we consider all the
concepts associated with monosemous words in
the two signatures as landmarks and restrict the
two signatures exclusively to those common con-
cepts. Leveraging monosemous words as bridges
between two signatures is a particularly reliable
technique as typically a significant portion of all
words in a lexicon are monosemous.3
Formally, let IG(w) be an inventory mapping
function that maps a term w to the set of con-
cepts which are expressed by w in graph G. Then,
given two signatures Sv1 and Sv2, computed on
the respective graphs G1 and G2, we first obtain
the set M of words that are monosemous accord-
ing to both semantic networks, i.e., M = {w :
|IG1(w)|=1 n |IG2(w)|=1}. We then transform
each of the two signatures Svi into a new sub-
signature S0vi whose dimension is |M|: the kth
component of S0vi corresponds to the weight in Svi
of the only concept of wk in IGi(wk). As an exam-
ple, assume we are given two semantic signatures
computed for two concepts in WordNet and Wik-
tionary. Also, consider the noun tradeoff which
is monosemous according to both these resources.
Then, each of the two unified sub-signatures will
contain a component whose weight is determined
by the weight of the only concept associated with
tradeoffn in the corresponding semantic signature.
As a result of the unification process, we obtain
a pair of equally-sized semantic signatures with
comparable components.
</bodyText>
<footnote confidence="0.9583545">
2http://wordnet.princeton.edu
3For instance, we calculated that more than 80% of the
words in WordNet are monosemous, with over 60% of all the
synsets containing at least one of them.
</footnote>
<subsectionHeader confidence="0.668371">
2.1.3 Signature comparison
</subsectionHeader>
<bodyText confidence="0.999983375">
Having at hand the semantic signatures for the
two input concepts, we proceed to comparing
them (part (d) in Figure 1). We leverage a non-
parametric measure proposed by Pilehvar et al.
(2013) which first transforms each signature into
a list of sorted elements and then calculates the
similarity on the basis of the average ranking of
elements across the two lists:
</bodyText>
<equation confidence="0.994402">
M|T |i=1(r1 i + r2 i )−1
Sim(Sv1, Sv2) = �|T  |(1)
i=1(2i)−1
</equation>
<bodyText confidence="0.999962454545454">
where T is the intersection of all concepts with
non-zero probability in the two signatures and rji
is the rank of the ith entry in the jth sorted list.
The denominator is a normalization factor to guar-
antee a maximum value of one. The method pe-
nalizes the differences in the higher rankings more
than it does for the lower ones. The measure was
shown to outperform the conventional cosine dis-
tance when comparing different semantic signa-
tures in multiple textual similarity tasks (Pilehvar
et al., 2013).
</bodyText>
<subsectionHeader confidence="0.709092">
2.1.4 Score combination
</subsectionHeader>
<bodyText confidence="0.99993275">
Finally (part (e) of Figure 1), we calculate the
overall similarity between two concepts as a lin-
ear combination of their definitional and struc-
tural similarities: Q Simdef(Sv1, Sv2) + (1 −
Q) Simstr(Sv1, Sv2). In Section 4.2.1, we explain
how we set, in our experiments, the values of Q
and the similarity threshold θ (cf. alignment algo-
rithm in Section 2).
</bodyText>
<sectionHeader confidence="0.979391" genericHeader="method">
3 Lexical Resource Ontologization
</sectionHeader>
<bodyText confidence="0.999820235294118">
In Section 2, we presented our approach for align-
ing lexical resources. However, the approach as-
sumes that the input resources can be viewed as
semantic networks, which seems to limit its ap-
plicability to structured resources only. In or-
der to address this issue and hence generalize our
alignment approach to any given lexical resource,
we propose a method for transforming a given
machine-readable dictionary into a semantic net-
work, a process we refer to as ontologization.
Our ontologization algorithm takes as input a
lexicon L and outputs a semantic graph G =
(V, E) where, as already defined in Section 2, V is
the set of concepts in L and E is the set of seman-
tic relations between these concepts. Introducing
relational links into a lexicon can be achieved in
different ways. A first option is to extract binary
</bodyText>
<page confidence="0.994016">
471
</page>
<bodyText confidence="0.997918888888889">
relations between pairs of words from raw text.
Both words in these relations, however, should
be disambiguated according to the given lexicon
(Pantel and Pennacchiotti, 2008), making the task
particularly prone to mistakes due to the high num-
ber of possible sense pairings.
Here, we take an alternative approach which
requires disambiguation on the target side only,
hence reducing the size of the search space sig-
nificantly. We first create the empty undirected
graph GL = (V, E) such that V is the set of con-
cepts in L and E = ∅. For each source con-
cept c E V we create a bag of content words
W = {wi, ... , wn} which includes all the con-
tent words in its definition d and, if available, ad-
ditional related words obtained from lexicon rela-
tions (e.g., synonyms in Wiktionary). The prob-
lem is then cast as a disambiguation task whose
goal is to identify the intended sense of each word
wi E W according to the sense inventory of L: if
wi is monosemous, i.e., |{IGL(wi)} |= 1, we con-
nect our source concept c to the only sense cwi of
wi and set E := E U {{c, cwi}}; else, wi has mul-
tiple senses in L. In this latter case, we choose the
most appropriate concept ci E IGL(wi) by finding
the maximal similarity between the definition of c
and the definitions of each sense of wi. To do this,
we apply our definitional similarity measure intro-
duced in Section 2.1. Having found the intended
sense ˆcwi of wi, we add the edge {c, ˆcwi} to E.
As a result of this procedure, we obtain a semantic
graph representation G for the lexicon L.
As an example, consider the 4th sense of the
noun cone in Wiktionary (i.e., cone n) which is de-
fined as “The fruit of a conifer”. The definition
contains two content words: fruitn and conifern.
The latter word is monosemous in Wiktionary,
hence we directly connect conen to the only sense
of conifern. The noun fruit, however, has 5 senses
in Wiktionary. We therefore measure the similar-
ity between the definition of conen and all the 5
definitions offruit and introduce a link from cone4n
to the sense of fruit which yields the maximal
similarity value (defined as “(botany) The seed-
bearing part of a plant...”).
</bodyText>
<sectionHeader confidence="0.999873" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999095625">
Lexical resources. To enable a comparison with
the state of the art, we followed Matuschek
and Gurevych (2013) and performed an align-
ment of WordNet synsets (WN) to three different
collaboratively-constructed resources: Wikipedia
(WP), Wiktionary (WT), and OmegaWiki (OW).
We utilized the DKPro software (Zesch et al.,
2008; Gurevych et al., 2012) to access the infor-
mation in the foregoing three resources. For WP,
WT, OW we used the dump versions 20090822,
20131002, and 20131115, respectively.
Evaluation measures. We followed previous
work (Navigli and Ponzetto, 2012; Matuschek and
Gurevych, 2013) and evaluated the alignment per-
formance in terms of four measures: precision, re-
call, F1, and accuracy. Precision is the fraction of
correct alignment judgments returned by the sys-
tem and recall is the fraction of alignment judg-
ments in the gold standard dataset that are cor-
rectly returned by the system. F1 is the harmonic
mean of precision and recall. We also report re-
sults for accuracy which, in addition to true posi-
tives, takes into account true negatives, i.e., pairs
which are correctly judged as unaligned.
Lexicons and semantic graphs. Here, we de-
scribe how the four semantic graphs for our four
lexical resources (i.e., WN, WP, WT, OW) were
constructed. As mentioned in Section 2.1.1, we
build the WN graph by including all the synsets
and semantic relations defined in WordNet (e.g.,
hypernymy and meronymy) and further populate
the relation set by connecting a synset to all the
other synsets that appear in its disambiguated
gloss. For WP, we used the graph provided by
Matuschek and Gurevych (2013), constructed by
directly connecting an article (concept) to all the
hyperlinks in its first paragraph, together with the
category links. Our WN and WP graphs have 118K
and 2.8M nodes, respectively, with the average
node degree being roughly 9 in both resources.
The other two resources, i.e., WT and OW, do
not provide a reliable network of semantic rela-
tions, therefore we used our ontologization ap-
proach to construct their corresponding semantic
graphs. We report, in the following subsection,
the experiments carried out to assess the accuracy
of our ontologization method, together with the
statistics of the obtained graphs for WT and OW.
</bodyText>
<subsectionHeader confidence="0.988878">
4.1 Ontologization Experiments
</subsectionHeader>
<bodyText confidence="0.999930857142857">
For ontologizing WT and OW, the bag of con-
tent words W is given by the content words in
sense definitions and, if available, additional re-
lated words obtained from lexicon relations (see
Section 3). In WT, both of these are in word sur-
face form and hence had to be disambiguated. For
OW, however, the encoded relations, though rela-
</bodyText>
<page confidence="0.987001">
472
</page>
<table confidence="0.999491">
Source Type WT OW
Definition Ambiguous 76.6% 50.7%
Unambiguous 18.3% 32.9%
Relation Ambiguous 2.8% -
Unambiguous 2.3% 16.4%
Total number of edges 2.1M 255K
</table>
<tableCaption confidence="0.999573">
Table 1: The statistics of the generated graphs
</tableCaption>
<bodyText confidence="0.996680069767442">
for WT and OW. We report the distribution of
the edges across types (i.e., ambiguous and un-
ambiguous) and sources (i.e., definitions and rela-
tions) from which candidate words were obtained.
tively small in number, are already disambiguated
and, therefore, the ontologization was just per-
formed on the definition’s content words.
The resulting graphs for WT and OW contain
430K and 48K nodes, respectively, each provid-
ing more than 95% coverage of concepts, with the
average node degree being around 10 for both re-
sources. We present in Table 1, for WT and OW,
the total number of edges together with their dis-
tribution across types (i.e., ambiguous and unam-
biguous) and sources (i.e., definitions and rela-
tions) from which candidate words were obtained.
The edges obtained from unambiguous entries
are essentially sense disambiguated on both sides
whereas those obtained from ambiguous terms
are a result of our similarity-based disambigua-
tion. Hence, given that a large portion of edges
came from ambiguous words (see Table 1), we
carried out an experiment to evaluate the accu-
racy of our disambiguation method. To this end,
we took as our benchmark the dataset provided
by Meyer and Gurevych (2010) for evaluating re-
lation disambiguation in WT. The dataset con-
tains 394 manually-disambiguated relations. We
compared our similarity-based disambiguation ap-
proach against the state of the art on this dataset,
i.e., the WKTWSD system, which is a WT rela-
tion disambiguation algorithm based on a series of
rules (Meyer and Gurevych, 2012b).
Table 2 shows the performance of our disam-
biguation method, together with that of WKTWSD,
in terms of Precision (P), Recall (R), F1, and ac-
curacy. The “Human” row corresponds to the
inter-rater F1 and accuracy scores, i.e., the upper-
bound performance on this dataset, as calculated
by Meyer and Gurevych (2010). As can be seen,
our method proves to be very accurate, surpassing
the performance of the WKTWSD system in terms
of precision, F1, and accuracy. This is particularly
</bodyText>
<table confidence="0.999612">
Approach P R F1 A
WKTWSD 0.780 0.800 0.790 0.840
Our method 0.852 0.767 0.807 0.857
Human - - 0.890 0.910
</table>
<tableCaption confidence="0.996943">
Table 2: The performance of relation disam-
</tableCaption>
<bodyText confidence="0.989873346153846">
biguation for our similarity-based disambiguation
method, as well as for the WKTWSD system.
interesting as the WKTWSD system uses a rule-
based technique specific to relation disambigua-
tion in WT, whereas our method is resource inde-
pendent and can be applied to arbitrary words in
the definition of any concept. We also note that the
graph constructed by Meyer and Gurevych (2010)
had an average node degree of around 1.
More recently, Matuschek and Gurevych (2013)
leveraged monosemous linking (cf. Section 5) in
order to create denser semantic graphs for OW and
WT. Our approach, however, thanks to the con-
nections obtained through ambiguous words, can
provide graphs with significantly higher coverage.
As an example, for WT, Matuschek and Gurevych
(2013) generated a graph where around 30% of
the nodes were in isolation, whereas this number
drops to around 5% in our corresponding graph.
These results show that our ontologization ap-
proach can be used to obtain dense semantic graph
representations of lexical resources, while at the
same time preserving a high level of accuracy.
Now that all the four resources are transformed
into semantic graphs, we move to our alignment
experiments.
</bodyText>
<subsectionHeader confidence="0.9910135">
4.2 Alignment Experiments
4.2.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999935133333333">
Datasets. As our benchmark we tested on
the gold standard datasets used in Matuschek
and Gurevych (2013) for three alignment
tasks: WordNet-Wikipedia (WN-WP), WordNet-
Wiktionary (WN-WT), and WordNet-OmegaWiki
(WN-OW). However, the dataset for WN-OW was
originally built for the German language and,
hence, was missing many English OW concepts
that could be considered as candidate target
alignments. We therefore fixed the dataset for the
English language and reproduced the performance
of previous work on the new dataset. The three
datasets contained 320, 484, and 315 WN concepts
that were manually mapped to their corresponding
concepts in WP, WT, and OW, respectively.
</bodyText>
<page confidence="0.998564">
473
</page>
<table confidence="0.999945181818182">
Approach Training type WN-WP WN-WT WN-OW
P R F1 A P R F1 A P R F1 A
SB Cross-val. 0.780 0.780 0.780 0.950 0.670 0.650 0.660 0.910 0.749 0.691 0.716 0.886
DWSA Tuning on subset 0.750 0.670 0.710 0.930 0.680 0.270 0.390 0.890 0.651 0.372 0.473 0.830
SB+DWSA Cross-val. + tuning 0.750 0.870 0.810 0.950 0.680 0.710 0.690 0.920 0.794 0.688 0.735 0.898
Unsupervised 0.709 0.929 0.805 0.943 0.642 0.799 0.712 0.923 0.664 0.761 0.709 0.872
Tuning on subset 0.877 0.792 0.833 0.960 0.672 0.799 0.730 0.930 0.750 0.717 0.733 0.893
SemAlign Cross-val. 0.852 0.835 0.840 0.965 0.680 0.769 0.722 0.931 0.778 0.725 0.749 0.900
Tuning on WN-WP - - - - 0.754 0.627 0.684 0.931 0.825 0.584 0.684 0.889
Tuning on WN-WT 0.738 0.934 0.824 0.950 - - - - 0.805 0.677 0.736 0.900
Tuning on WN-OW 0.744 0.925 0.824 0.950 0.684 0.766 0.723 0.930 - - - -
</table>
<tableCaption confidence="0.999496">
Table 3: The performance of different systems on the task of aligning WordNet to Wikipedia (WN-WP),
</tableCaption>
<bodyText confidence="0.991153222222222">
Wiktionary (WN-WT), and OmegaWiki (WN-OW) in terms of Precision (P), Recall (R), F1, and Accuracy
(A). We present results for different configurations of our system (SemAlign), together with the state of
the art in definition similarity-based alignment approaches (SB) and the best configuration of the state-
of-the-art graph-based system, Dijkstra-WSA (Matuschek and Gurevych, 2013, DWSA).
Configurations. Recall from Section 2 that our
resource alignment technique has two parameters:
the similarity threshold θ and the combination pa-
rameter β, both defined in [0, 1]. We performed
experiments with three different configurations:
</bodyText>
<listItem confidence="0.997641666666667">
• Unsupervised, where the two parameters are
set to their middle values (i.e., 0.5), hence,
no tuning is performed for either of the pa-
rameters. In this case, both the definitional
and structural similarity scores are treated
as equally important and two concepts are
aligned if their overall similarity exceeds the
middle point of the similarity scale.
• Tuning, where we follow Matuschek and
Gurevych (2013) and tune the parameters on
a subset of the dataset comprising 100 items.
• Cross-validation, where a 5-fold cross vali-
</listItem>
<bodyText confidence="0.5070694">
dation is carried out to find the optimal values
for the parameters, a technique used in most
of the recent alignment methods (Niemann
and Gurevych, 2011; Meyer and Gurevych,
2012a; Matuschek and Gurevych, 2013).
</bodyText>
<sectionHeader confidence="0.786199" genericHeader="evaluation">
4.2.2 Results
</sectionHeader>
<bodyText confidence="0.999832622222222">
We show in Table 3 the alignment performance of
different systems on the task of aligning WN-WP,
WN-WT, and WN-OW in terms of Precision (P), Re-
call (R), F1, and Accuracy. The SB system corre-
sponds to the state-of-the-art definition similarity
approaches for WN-WP (Niemann and Gurevych,
2011), WN-WT (Meyer and Gurevych, 2011), and
WN-OW (Gurevych et al., 2012). DWSA stands
for Dijkstra-WSA, the state-of-the-art graph-based
alignment approach of Matuschek and Gurevych
(2013). The authors also provided results for
SB+Dijkstra-WSA, a hybrid system where DWSA
was tuned for high precision and, in the case when
no alignment target could be found, the algorithm
fell back on SB judgments. We also show the re-
sults for this system as SB+DWSA in the table.
For our approach (SemAlign) we show the re-
sults of six different runs each corresponding to a
different setting. The first three (middle part of the
table) correspond to the results obtained with the
three configurations of SemAlign: unsupervised,
with tuning on subset, and cross-validation (see
Section 4.2.1). In addition to these, we performed
experiments where the two parameters of SemA-
lign were tuned on pair-independent training data,
i.e., a training dataset for a pair of resources dif-
ferent from the one being aligned. For this setting,
we used the whole dataset of the corresponding re-
source pair to tune the two parameters of our sys-
tem. We show the results for this setting in the
bottom part of the table (last three lines).
The main feature worth remarking upon is the
consistency in the results across different resource
pairs: the unsupervised system gains the best re-
call among the three configurations (with the im-
provement over SB+DWSA being always statisti-
cally significant4) whereas tuning, both on a subset
or through cross-validation, consistently leads to
the best performance in terms of F1 and accuracy
(with the latter being statistically significant with
respect to SB+DWSA on WN-WP and WN-WT).
Moreover, the unsupervised system proves to be
very robust inasmuch as it provides competitive
results on all the three datasets, while it surpasses
the performance of SB+DWSA on WN-WT. This
</bodyText>
<footnote confidence="0.953596">
4All significance tests are done using z-test at p &lt; 0.05.
</footnote>
<page confidence="0.992144">
474
</page>
<table confidence="0.99984225">
Approach WN-WP WN-WT WN-OW
P R F1 A P R F1 A P R F1 A
Dijkstra-WSA 0.750 0.670 0.710 0.930 0.680 0.270 0.390 0.890 0.651 0.372 0.473 0.830
SemAlignstr 0.877 0.788 0.830 0.959 0.604 0.643 0.623 0.907 0.654 0.602 0.627 0.853
</table>
<tableCaption confidence="0.99031">
Table 4: Performance of SemAlign when using only the structural similarity component (SemAlignstr)
</tableCaption>
<bodyText confidence="0.97334492">
compared to the state-of-the-art graph-based alignment approach, Dijkstra-WSA (Matuschek and
Gurevych, 2013) for our three resource pairs: WordNet to Wikipedia (WN-WP), Wiktionary (WN-WT),
and OmegaWiki (WN-OW).
is particularly interesting as the latter system in-
volves tuning of several parameters, whereas Se-
mAlign, in its unsupervised configuration, does
not need any training data nor does it involve any
tuning. In addition, as can be seen in the table,
SemAlign benefits from pair-independent training
data in most cases across the three resource pairs
with performance surpassing that of SB+DWSA, a
system which is dependent on pair-specific train-
ing data. The consistency in the performance of
SemAlign in its different configurations and across
different resource pairs indicates its robustness
and shows that our system can be utilized effec-
tively for aligning any pair of lexical resources, ir-
respective of their structure or availability of train-
ing data.
The system performance is generally higher on
the alignment task for WP compared to WT and
OW. We attribute this difference to the dictionary
nature of the latter two, where sense distinctions
are more fine-grained, as opposed to the relatively
concrete concepts in the WP encyclopedia.
</bodyText>
<subsectionHeader confidence="0.999372">
4.3 Similarity Measure Analysis
</subsectionHeader>
<bodyText confidence="0.999991586956522">
We explained in Section 2.1 that our concept sim-
ilarity measure consists of two components: the
definitional and the structural similarities. Mea-
suring the similarity of two concepts in terms of
their definitions has been investigated in previ-
ous work (Niemann and Gurevych, 2011; Hen-
rich et al., 2012). The structural similarity compo-
nent of our approach, however, is novel, but at the
same time one of the very few measures which en-
ables the computation of the similarity of concepts
across two resources directly and independently of
the similarity of their definitions. A comparable
approach is the Dijkstra-WSA proposed by Ma-
tuschek and Gurevych (2013) which, as also men-
tioned earlier in the Introduction, first connects the
two resources’ graphs by leveraging monosemous
linking and then aligns two concepts across the
two graphs on the basis of their shortest distance.
To gain more insight into the effectiveness of our
structural similarity measure in comparison to the
Dijkstra-WSA method, we carried out an experi-
ment where our alignment system used only the
structural similarity component, a variant of our
system we refer to as SemAlignstr. Both systems
(i.e., SemAlignstr and Dijkstra-WSA) were tuned
on 100-item subsets of the corresponding datasets.
We show in Table 4 the performance of the two
systems on our three datasets. As can be seen in
the table, SemAlignstr consistently improves over
Dijkstra-WSA according to recall, F1 and accu-
racy with all the differences in recall and accu-
racy being statistically significant (p &lt; 0.05). The
improvement is especially noticeable for pairs in-
volving either WT or OW where, thanks to the rel-
atively denser semantic graphs obtained by means
of our ontologization technique, the gap in F1 is
about 0.23 (WN-WT) and 0.15 (WN-OW).
In addition, as we mentioned earlier, for WN-WP
we used the same graph as that of Dijkstra-WSA,
since both WN and WP provide a full-fledged se-
mantic network and thus neither needed to be
ontologized. Therefore, the considerable perfor-
mance improvement over Dijkstra-WSA on this
resource pair shows the effectiveness of our novel
concept similarity measure independently of the
underlying semantic network.
</bodyText>
<sectionHeader confidence="0.999924" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999622714285714">
Resource ontologization. Having lexical re-
sources represented as semantic networks is
highly beneficial. A good example is WordNet,
which has been exploited as a semantic network
in dozens of NLP tasks (Fellbaum, 1998). A re-
cent prominent case is Wikipedia (Medelyan et
al., 2009; Hovy et al., 2013) which, thanks to
its inter-article hyperlink structure, provides a rich
backbone for structuring additional information
(Auer et al., 2007; Suchanek et al., 2008; Moro
and Navigli, 2013; Flati et al., 2014). How-
ever, there are many large-scale resources, such
as Wiktionary for instance, which by their very
nature are not in the form of a graph. This is
</bodyText>
<page confidence="0.998202">
475
</page>
<bodyText confidence="0.999984861111111">
usually the case with machine-readable dictionar-
ies, where structuring the resource involves the
arduous task of connecting lexicographic senses
by means of semantic relations. Surprisingly,
despite their vast potential, little research has
been conducted on the automatic ontologization of
collaboratively-constructed dictionaries like Wik-
tionary and OmegaWiki. Meyer and Gurevych
(2012a) and Matuschek and Gurevych (2013) pro-
vided approaches for building graph representa-
tions of Wiktionary and OmegaWiki. The result-
ing graphs, however, were either sparse or had a
considerable portion of the nodes left in isolation.
Our approach, in contrast, aims at transforming a
lexical resource into a full-fledged semantic net-
work, hence providing a denser graph with most
of its nodes connected.
Resource alignment. Aligning lexical resources
has been a very active field of research in the
last decade. One of the main objectives in this
area has been to enrich existing ontologies by
means of complementary information from other
resources. As a matter of fact, most efforts have
been concentrated on aligning the de facto com-
munity standard sense inventory, i.e. WordNet, to
other resources. These include: the Roget’s the-
saurus and Longman Dictionary of Contemporary
English (Kwong, 1998), FrameNet (Laparra and
Rigau, 2009), VerbNet (Shi and Mihalcea, 2005)
or domain-specific terminologies such as the Uni-
fied Medical Language System (Burgun and Bo-
denreider, 2001). More recently, the growth
of collaboratively-constructed resources has seen
the development of alignment approaches with
Wikipedia (Ruiz-Casado et al., 2005; Auer et al.,
2007; Suchanek et al., 2008; Reiter et al., 2008;
Navigli and Ponzetto, 2012), Wiktionary (Meyer
and Gurevych, 2011) and OmegaWiki (Gurevych
et al., 2012). Last year Matuschek and Gurevych
(2013) proposed Dijkstra-WSA, a graph-based ap-
proach relying on shortest paths between two
concepts when the two corresponding resources
graphs were combined by leveraging monosemous
linking. Their method when backed off with other
definition similarity based approaches (Niemann
and Gurevych, 2011; Meyer and Gurevych, 2011),
achieved state-of-the-art results on the mapping of
WordNet to different collaboratively-constructed
resources. This approach, however, in addition to
setting the threshold for the definition similarity
component by means of cross validation, also re-
quired other parameters to be tuned, such as the
allowed path length (λ) and the maximum num-
ber of edges in a graph. The optimal value for the
λ parameter varied from one resource pair to an-
other, and even for a specific resource pair it had
to be tuned for each configuration. This made the
approach dependent on the training data for the
specific pair of resources that were to be aligned.
Instead of measuring the similarity of two con-
cepts on the basis of their distance in the com-
bined graph, our approach models each concept
through a rich vectorial representation we refer to
as semantic signature and compares the two con-
cepts in terms of the similarity of their semantic
signatures. This rich representation leads to our
approach having a good degree of robustness such
that it can achieve competitive results even in the
absence of training data. This enables our system
to be applied effectively for aligning new pairs of
resources for which no training data is available,
with state-of-the-art performance.
</bodyText>
<sectionHeader confidence="0.999793" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999975">
This paper presents a unified approach for align-
ing lexical resources. Our method leverages
a novel similarity measure which enables a di-
rect structural comparison of concepts across dif-
ferent lexical resources. Thanks to an effec-
tive ontologization method, our alignment ap-
proach can be applied to any pair of lexical re-
sources independently of whether they provide
a full-fledged network structure. We demon-
strate that our approach achieves state-of-the-
art performance on aligning WordNet to three
collaboratively-constructed resources with differ-
ent characteristics, i.e., Wikipedia, Wiktionary,
and OmegaWiki. We also show that our approach
is robust across its different configurations, even
when the training data is absent, enabling it to be
used effectively for aligning new pairs of lexical
resources for which no resource-specific training
data is available. In future work, we plan to ex-
tend our concept similarity measure across differ-
ent natural languages. We release all our data at
http://lcl.uniroma1.it/semalign.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999481333333333">
The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.
We would like to thank Michael Matuschek for
providing us with Wikipedia graphs and alignment
datasets.
</bodyText>
<page confidence="0.998762">
476
</page>
<sectionHeader confidence="0.995269" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998593146788991">
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for Word Sense Disambiguation. In Pro-
ceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 33–41, Athens, Greece.
S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ive.
2007. DBpedia: A nucleus for a web of open data.
In Proceedings of 6th International Semantic Web
Conference joint with 2nd Asian Semantic Web Con-
ference (ISWC+ASWC 2007), pages 722–735, Bu-
san, Korea.
Sergey Brin and Michael Page. 1998. Anatomy of a
large-scale hypertextual Web search engine. In Pro-
ceedings of the 7th Conference on World Wide Web,
pages 107–117, Brisbane, Australia.
Anita Burgun and Olivier Bodenreider. 2001. Compar-
ing terms, concepts and semantic classes in WordNet
and the Unified Medical Language System. In Pro-
ceedings of NAACL Workshop, WordNet and Other
Lexical Resources: Applications, Extensions and
Customizations, pages 77–82, Pittsburgh, USA.
Gerard de Melo and Gerhard Weikum. 2010. Pro-
viding multilingual, multimodal answers to lexical
database queries. In Proceedings of the Seventh
International Conference on Language Resources
and Evaluation (LREC’10), pages 348–355, Val-
letta, Malta.
Stefano Faralli and Roberto Navigli. 2012. A
New Minimally-supervised Framework for Domain
Word Sense Disambiguation. In Proceedings of
the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Compu-
tational Natural Language Learning, pages 1411–
1422, Jeju, Korea.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
Tiziano Flati, Daniele Vannella, Tommaso Pasini, and
Roberto Navigli. 2014. Two is bigger (and bet-
ter) than one: the Wikipedia Bitaxonomy Project.
In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics (ACL
2014), Baltimore, Maryland.
Iryna Gurevych, Judith Eckle-Kohler, Silvana Hart-
mann, Michael Matuschek, Christian M. Meyer, and
Christian Wirth. 2012. UBY - a large-scale uni-
fied lexical-semantic resource based on LMF. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 580–590, Avignon, France.
Taher H. Haveliwala. 2002. Topic-sensitive PageRank.
In Proceedings of the 11th international conference
on World Wide Web, pages 517–526, Hawaii, USA.
Verena Henrich, Erhard Hinrichs, and Tatiana Vodola-
zova. 2011. Semi-automatic extension of GermaNet
with sense definitions from Wiktionary. In Pro-
ceedings of 5th Language &amp; Technology Conference
(LTC 2011), pages 126–130, Pozna, Poland.
Verena Henrich, Erhard W. Hinrichs, and Klaus Sut-
tner. 2012. Automatically linking GermaNet to
Wikipedia for harvesting corpus examples for Ger-
maNet senses. In Journal for Language Technology
and Computational Linguistics (JLCL), 27(1):1–19.
Eduard H. Hovy, Roberto Navigli, and Simone Paolo
Ponzetto. 2013. Collaboratively built semi-
structured content and Artificial Intelligence: The
story so far. Artificial Intelligence, 194:2–27.
Thad Hughes and Daniel Ramage. 2007. Lexical se-
mantic relatedness with random graph walks. In
Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing,
pages 581–589, Prague, Czech Republic.
Oi Yee Kwong. 1998. Aligning WordNet with
additional lexical resources. In COLING-ACL98
Workshop on Usage of WordNet in Natural Lan-
guage Processing Systems, pages 73–79, Montreal,
Canada.
Egoitzand Laparra and German Rigau. 2009. Inte-
grating WordNet and FrameNet using a knowledge-
based Word Sense Disambiguation algorithm. In
Proceedings of Recent Advances in Natural Lan-
guage Processing (RANLP09), pages 1–6, Borovets,
Bulgaria.
Michael Matuschek and Iryna Gurevych. 2013.
Dijkstra-WSA: A graph-based approach to word
sense alignment. Transactions of the Association for
Computational Linguistics (TACL), 1:151–164.
Olena Medelyan, David Milne, Catherine Legg, and
Ian H. Witten. 2009. Mining meaning from
Wikipedia. International Journal of Human-
Computer Studies, 67(9):716–754.
Christian M. Meyer and Iryna Gurevych. 2010. “worth
its weight in gold or yet another resource”; a com-
parative study of Wiktionary, OpenThesaurus and
GermaNet. In Proceedings of the 11th International
Conference on Computational Linguistics and Intel-
ligent Text Processing, CICLing’10, pages 38–49,
Iasi, Romania.
Christian M. Meyer and Iryna Gurevych. 2011. What
psycholinguists know about Chemistry: Aligning
Wiktionary and WordNet for increased domain cov-
erage. In Proceedings of the 5th International Joint
Conference on Natural Language Processing, pages
883–892, Chiang Mai, Thailand.
Christian M. Meyer and Iryna Gurevych. 2012a. On-
toWiktionary: Constructing an ontology from the
collaborative online dictionary Wiktionary. In Semi-
Automatic Ontology Development: Processes and
Resources, pages 131–161. IGI Global.
</reference>
<page confidence="0.980835">
477
</page>
<reference confidence="0.999774604938272">
Christian M. Meyer and Iryna Gurevych. 2012b.
To exhibit is not to loiter: A multilingual, sense-
disambiguated Wiktionary for measuring verb sim-
ilarity. In Proceedings of the 24th International
Conference on Computational Linguistics (COLING
2012), pages 1763–1780, Mumbai, India.
Andrea Moro and Roberto Navigli. 2013. Integrating
syntactic and semantic analysis into the Open Infor-
mation Extraction paradigm. In Proceedings of the
23rd International Joint Conference on Artificial In-
telligence (IJCAI 2013), pages 2148–2154, Beijing,
China.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217–
250.
Roberto Navigli, Stefano Faralli, Aitor Soroa, Oier
de Lacalle, and Eneko Agirre. 2011. Two birds
with one stone: Learning semantic models for text
categorization and Word Sense Disambiguation. In
Proceedings of the 20th ACM Conference on Infor-
mation and Knowledge Management (CIKM), pages
2317–2320, Glasgow, UK.
Roberto Navigli. 2006. Meaningful clustering of
senses helps boost word sense disambiguation per-
formance. In Proceedings of the 44th Annual Meet-
ing of the Association for Computational Linguis-
tics joint with the 21st International Conference on
Computational Linguistics (COLING-ACL 2006),
pages 105–112, Sydney, Australia.
Elisabeth Niemann and Iryna Gurevych. 2011. The
people’s web meets linguistic knowledge: Auto-
matic sense alignment of Wikipedia and WordNet.
In Proceedings of the Ninth International Confer-
ence on Computational Semantics, pages 205–214,
Oxford, United Kingdom.
Martha Palmer, Daniel Gildea, and Nianwen Xue.
2010. Semantic Role Labeling. Synthesis Lectures
on Human Language Technologies. Morgan &amp; Clay-
pool Publishers.
Patrick Pantel and Marco Pennacchiotti. 2008. Auto-
matically harvesting and ontologizing semantic rela-
tions. In Proceedings of the 2008 Conference on On-
tology Learning and Population: Bridging the Gap
Between Text and Knowledge, pages 171–195, Am-
sterdam, The Netherlands.
Mohammad Taher Pilehvar, David Jurgens, and
Roberto Navigli. 2013. Align, Disambiguate and
Walk: a Unified Approach for Measuring Semantic
Similarity. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1341–1351, Sofia, Bulgaria.
Nils Reiter, Matthias Hartung, and Anette Frank.
2008. A resource-poor approach for linking ontol-
ogy classes to Wikipedia articles. In Johan Bos and
Rodolfo Delmonte, editors, Semantics in Text Pro-
cessing, volume 1 of Research in Computational Se-
mantics, pages 381–387. College Publications, Lon-
don, England.
Maria Ruiz-Casado, Enrique Alfonseca, and Pablo
Castells. 2005. Automatic assignment of Wikipedia
encyclopedic entries to WordNet synsets. In Pro-
ceedings of the Third International Conference on
Advances in Web Intelligence, pages 380–386, Lodz,
Poland.
Lei Shi and Rada Mihalcea. 2005. Putting pieces to-
gether: Combining FrameNet, VerbNet and Word-
Net for robust semantic parsing. In Proceedings of
the 6th International Conference on Computational
Linguistics and Intelligent Text Processing, pages
100–111, Mexico City, Mexico.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. Yago: A large ontology from
Wikipedia and WordNet. Journal of Web Semantics,
6(3):203–217.
Torsten Zesch, Christof M¨uller, and Iryna Gurevych.
2008. Using Wiktionary for computing semantic re-
latedness. In Proceedings of the 23rd national con-
ference on Artificial intelligence - Volume 2, pages
861–866, Chicago, Illinois.
</reference>
<page confidence="0.99802">
478
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.404817">
<title confidence="0.999843">A Robust Approach to Aligning Heterogeneous Lexical Resources</title>
<author confidence="0.990387">Taher Pilehvar</author>
<affiliation confidence="0.9880095">Department of Computer Sapienza University of</affiliation>
<abstract confidence="0.964922105263158">Lexical resource alignment has been an active field of research over the last decade. However, prior methods for aligning lexical resources have been either specific to a particular pair of resources, or heavily dependent on the availability of hand-crafted alignment data for the pair of resources to be aligned. Here we present a unified approach that can be applied to an arbitrary pair of lexical resources, including machine-readable dictionaries with no network structure. Our approach leverages a similarity measure that enables the structural comparison of senses across lexical resources, achieving state-of-the-art performance on the task of aligning WordNet to three different collaborative resources: Wikipedia, Wiktionary and OmegaWiki.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Personalizing PageRank for Word Sense Disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>33--41</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="10548" citStr="Agirre and Soroa, 2009" startWordPosition="1714" endWordPosition="1717">nk (PR) algorithm (Brin and Page, 1998) computes, for a given graph, a single vector wherein each node is associated with a weight denoting its structural importance in that graph. PPR is a variation of PR where the computation is biased towards a set of initial nodes in order to capture the notion of importance with respect to those particular nodes. PPR has been previously used in a wide variety of tasks such as definition similarity-based resource alignment (Niemann and Gurevych, 2011), textual semantic similarity (Hughes and Ramage, 2007; Pilehvar et al., 2013), Word Sense Disambiguation (Agirre and Soroa, 2009; Faralli and Navigli, 2012) and semantic text categorization (Navigli et al., 2011). When applied to a semantic graph by initializing the random walks from a set of concepts (nodes), PPR yields a vector in which each concept is associated with a weight denoting its semantic relevance to the initial concepts. Formally, we first represent a semantic network consisting of N concepts as a row-stochastic transition matrix M E RN×N. The cell (i, j) in the matrix denotes the probability of moving from a concept i to j in the graph: 0 if no edge exists from i to j and 1/degree(i) otherwise. Then the </context>
</contexts>
<marker>Agirre, Soroa, 2009</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2009. Personalizing PageRank for Word Sense Disambiguation. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 33–41, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S¨oren Auer</author>
<author>Christian Bizer</author>
<author>Georgi Kobilarov</author>
<author>Jens Lehmann</author>
<author>Richard Cyganiak</author>
<author>Zachary Ive</author>
</authors>
<title>DBpedia: A nucleus for a web of open data.</title>
<date>2007</date>
<booktitle>In Proceedings of 6th International Semantic Web Conference joint with 2nd Asian Semantic Web Conference (ISWC+ASWC</booktitle>
<pages>722--735</pages>
<location>Busan,</location>
<contexts>
<context position="35199" citStr="Auer et al., 2007" startWordPosition="5797" endWordPosition="5800">provement over Dijkstra-WSA on this resource pair shows the effectiveness of our novel concept similarity measure independently of the underlying semantic network. 5 Related Work Resource ontologization. Having lexical resources represented as semantic networks is highly beneficial. A good example is WordNet, which has been exploited as a semantic network in dozens of NLP tasks (Fellbaum, 1998). A recent prominent case is Wikipedia (Medelyan et al., 2009; Hovy et al., 2013) which, thanks to its inter-article hyperlink structure, provides a rich backbone for structuring additional information (Auer et al., 2007; Suchanek et al., 2008; Moro and Navigli, 2013; Flati et al., 2014). However, there are many large-scale resources, such as Wiktionary for instance, which by their very nature are not in the form of a graph. This is 475 usually the case with machine-readable dictionaries, where structuring the resource involves the arduous task of connecting lexicographic senses by means of semantic relations. Surprisingly, despite their vast potential, little research has been conducted on the automatic ontologization of collaboratively-constructed dictionaries like Wiktionary and OmegaWiki. Meyer and Gurevy</context>
<context position="37061" citStr="Auer et al., 2007" startWordPosition="6078" endWordPosition="6081">r resources. As a matter of fact, most efforts have been concentrated on aligning the de facto community standard sense inventory, i.e. WordNet, to other resources. These include: the Roget’s thesaurus and Longman Dictionary of Contemporary English (Kwong, 1998), FrameNet (Laparra and Rigau, 2009), VerbNet (Shi and Mihalcea, 2005) or domain-specific terminologies such as the Unified Medical Language System (Burgun and Bodenreider, 2001). More recently, the growth of collaboratively-constructed resources has seen the development of alignment approaches with Wikipedia (Ruiz-Casado et al., 2005; Auer et al., 2007; Suchanek et al., 2008; Reiter et al., 2008; Navigli and Ponzetto, 2012), Wiktionary (Meyer and Gurevych, 2011) and OmegaWiki (Gurevych et al., 2012). Last year Matuschek and Gurevych (2013) proposed Dijkstra-WSA, a graph-based approach relying on shortest paths between two concepts when the two corresponding resources graphs were combined by leveraging monosemous linking. Their method when backed off with other definition similarity based approaches (Niemann and Gurevych, 2011; Meyer and Gurevych, 2011), achieved state-of-the-art results on the mapping of WordNet to different collaboratively</context>
</contexts>
<marker>Auer, Bizer, Kobilarov, Lehmann, Cyganiak, Ive, 2007</marker>
<rawString>S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ive. 2007. DBpedia: A nucleus for a web of open data. In Proceedings of 6th International Semantic Web Conference joint with 2nd Asian Semantic Web Conference (ISWC+ASWC 2007), pages 722–735, Busan, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
<author>Michael Page</author>
</authors>
<title>Anatomy of a large-scale hypertextual Web search engine.</title>
<date>1998</date>
<booktitle>In Proceedings of the 7th Conference on World Wide Web,</booktitle>
<pages>107--117</pages>
<location>Brisbane, Australia.</location>
<contexts>
<context position="9965" citStr="Brin and Page, 1998" startWordPosition="1617" endWordPosition="1620">onents share the same backbone (parts (b) and (d) of Figure 1), but differ in some stages (parts (a) and (c) in Figure 1). In the following, we explain all the stages involved in the two components (gray blocks in the figure). 2.1.1 Semantic signature generation The aim of this stage is to model a given concept or set of concepts through a vectorial semantic representation, which we refer to as the semantic signature of the input. We utilized Personalized PageRank (Haveliwala, 2002, PPR), a random walk graph algorithm, for calculating semantic signatures. The original PageRank (PR) algorithm (Brin and Page, 1998) computes, for a given graph, a single vector wherein each node is associated with a weight denoting its structural importance in that graph. PPR is a variation of PR where the computation is biased towards a set of initial nodes in order to capture the notion of importance with respect to those particular nodes. PPR has been previously used in a wide variety of tasks such as definition similarity-based resource alignment (Niemann and Gurevych, 2011), textual semantic similarity (Hughes and Ramage, 2007; Pilehvar et al., 2013), Word Sense Disambiguation (Agirre and Soroa, 2009; Faralli and Nav</context>
<context position="11507" citStr="Brin and Page, 1998" startWordPosition="1894" endWordPosition="1897"> a semantic network consisting of N concepts as a row-stochastic transition matrix M E RN×N. The cell (i, j) in the matrix denotes the probability of moving from a concept i to j in the graph: 0 if no edge exists from i to j and 1/degree(i) otherwise. Then the PPR vector, hence the semantic signature Sv of vector v is the unique solution to the linear system: Sv = (1 − α) v + α M Sv, where v is the personalization vector of size N in which all the probability mass is put on the concepts for which a semantic signature is to be computed and α is the damping factor, which is usually set to 0.85 (Brin and Page, 1998). We used the UKB1 off-the-shelf implementation of PPR. Definitional similarity signature. In the definitional similarity component, the two concepts c1 and c2 are first represented by their corresponding definitions d1 and d2 in the respective resources L1 and L2 (Figure 1(a), top). To improve expressiveness, we follow Niemann and Gurevych (2011) and further extend di with all the word forms associated with concept ci and its neighbours, i.e., the union of all lexicalizations LGi(x) for all concepts x E {c� E Vi : (c, c&apos;) E Ei} U {c}, where Ei is the set of edges in Gi. In this component the </context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>Sergey Brin and Michael Page. 1998. Anatomy of a large-scale hypertextual Web search engine. In Proceedings of the 7th Conference on World Wide Web, pages 107–117, Brisbane, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anita Burgun</author>
<author>Olivier Bodenreider</author>
</authors>
<title>Comparing terms, concepts and semantic classes in WordNet and the Unified Medical Language System.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL Workshop, WordNet and Other Lexical Resources: Applications, Extensions and Customizations,</booktitle>
<pages>77--82</pages>
<location>Pittsburgh, USA.</location>
<contexts>
<context position="36884" citStr="Burgun and Bodenreider, 2001" startWordPosition="6053" endWordPosition="6057">s has been a very active field of research in the last decade. One of the main objectives in this area has been to enrich existing ontologies by means of complementary information from other resources. As a matter of fact, most efforts have been concentrated on aligning the de facto community standard sense inventory, i.e. WordNet, to other resources. These include: the Roget’s thesaurus and Longman Dictionary of Contemporary English (Kwong, 1998), FrameNet (Laparra and Rigau, 2009), VerbNet (Shi and Mihalcea, 2005) or domain-specific terminologies such as the Unified Medical Language System (Burgun and Bodenreider, 2001). More recently, the growth of collaboratively-constructed resources has seen the development of alignment approaches with Wikipedia (Ruiz-Casado et al., 2005; Auer et al., 2007; Suchanek et al., 2008; Reiter et al., 2008; Navigli and Ponzetto, 2012), Wiktionary (Meyer and Gurevych, 2011) and OmegaWiki (Gurevych et al., 2012). Last year Matuschek and Gurevych (2013) proposed Dijkstra-WSA, a graph-based approach relying on shortest paths between two concepts when the two corresponding resources graphs were combined by leveraging monosemous linking. Their method when backed off with other defini</context>
</contexts>
<marker>Burgun, Bodenreider, 2001</marker>
<rawString>Anita Burgun and Olivier Bodenreider. 2001. Comparing terms, concepts and semantic classes in WordNet and the Unified Medical Language System. In Proceedings of NAACL Workshop, WordNet and Other Lexical Resources: Applications, Extensions and Customizations, pages 77–82, Pittsburgh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard de Melo</author>
<author>Gerhard Weikum</author>
</authors>
<title>Providing multilingual, multimodal answers to lexical database queries.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10),</booktitle>
<pages>348--355</pages>
<location>Valletta,</location>
<marker>de Melo, Weikum, 2010</marker>
<rawString>Gerard de Melo and Gerhard Weikum. 2010. Providing multilingual, multimodal answers to lexical database queries. In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10), pages 348–355, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefano Faralli</author>
<author>Roberto Navigli</author>
</authors>
<title>A New Minimally-supervised Framework for Domain Word Sense Disambiguation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1411--1422</pages>
<location>Jeju,</location>
<contexts>
<context position="10576" citStr="Faralli and Navigli, 2012" startWordPosition="1718" endWordPosition="1721">and Page, 1998) computes, for a given graph, a single vector wherein each node is associated with a weight denoting its structural importance in that graph. PPR is a variation of PR where the computation is biased towards a set of initial nodes in order to capture the notion of importance with respect to those particular nodes. PPR has been previously used in a wide variety of tasks such as definition similarity-based resource alignment (Niemann and Gurevych, 2011), textual semantic similarity (Hughes and Ramage, 2007; Pilehvar et al., 2013), Word Sense Disambiguation (Agirre and Soroa, 2009; Faralli and Navigli, 2012) and semantic text categorization (Navigli et al., 2011). When applied to a semantic graph by initializing the random walks from a set of concepts (nodes), PPR yields a vector in which each concept is associated with a weight denoting its semantic relevance to the initial concepts. Formally, we first represent a semantic network consisting of N concepts as a row-stochastic transition matrix M E RN×N. The cell (i, j) in the matrix denotes the probability of moving from a concept i to j in the graph: 0 if no edge exists from i to j and 1/degree(i) otherwise. Then the PPR vector, hence the semant</context>
</contexts>
<marker>Faralli, Navigli, 2012</marker>
<rawString>Stefano Faralli and Roberto Navigli. 2012. A New Minimally-supervised Framework for Domain Word Sense Disambiguation. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1411– 1422, Jeju, Korea.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tiziano Flati</author>
<author>Daniele Vannella</author>
<author>Tommaso Pasini</author>
<author>Roberto Navigli</author>
</authors>
<title>Two is bigger (and better) than one: the Wikipedia Bitaxonomy Project.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014),</booktitle>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="35267" citStr="Flati et al., 2014" startWordPosition="5809" endWordPosition="5812">iveness of our novel concept similarity measure independently of the underlying semantic network. 5 Related Work Resource ontologization. Having lexical resources represented as semantic networks is highly beneficial. A good example is WordNet, which has been exploited as a semantic network in dozens of NLP tasks (Fellbaum, 1998). A recent prominent case is Wikipedia (Medelyan et al., 2009; Hovy et al., 2013) which, thanks to its inter-article hyperlink structure, provides a rich backbone for structuring additional information (Auer et al., 2007; Suchanek et al., 2008; Moro and Navigli, 2013; Flati et al., 2014). However, there are many large-scale resources, such as Wiktionary for instance, which by their very nature are not in the form of a graph. This is 475 usually the case with machine-readable dictionaries, where structuring the resource involves the arduous task of connecting lexicographic senses by means of semantic relations. Surprisingly, despite their vast potential, little research has been conducted on the automatic ontologization of collaboratively-constructed dictionaries like Wiktionary and OmegaWiki. Meyer and Gurevych (2012a) and Matuschek and Gurevych (2013) provided approaches for</context>
</contexts>
<marker>Flati, Vannella, Pasini, Navigli, 2014</marker>
<rawString>Tiziano Flati, Daniele Vannella, Tommaso Pasini, and Roberto Navigli. 2014. Two is bigger (and better) than one: the Wikipedia Bitaxonomy Project. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014), Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iryna Gurevych</author>
<author>Judith Eckle-Kohler</author>
<author>Silvana Hartmann</author>
<author>Michael Matuschek</author>
<author>Christian M Meyer</author>
<author>Christian Wirth</author>
</authors>
<title>UBY - a large-scale unified lexical-semantic resource based on LMF.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>580--590</pages>
<location>Avignon, France.</location>
<contexts>
<context position="19819" citStr="Gurevych et al., 2012" startWordPosition="3322" endWordPosition="3325">. We therefore measure the similarity between the definition of conen and all the 5 definitions offruit and introduce a link from cone4n to the sense of fruit which yields the maximal similarity value (defined as “(botany) The seedbearing part of a plant...”). 4 Experiments Lexical resources. To enable a comparison with the state of the art, we followed Matuschek and Gurevych (2013) and performed an alignment of WordNet synsets (WN) to three different collaboratively-constructed resources: Wikipedia (WP), Wiktionary (WT), and OmegaWiki (OW). We utilized the DKPro software (Zesch et al., 2008; Gurevych et al., 2012) to access the information in the foregoing three resources. For WP, WT, OW we used the dump versions 20090822, 20131002, and 20131115, respectively. Evaluation measures. We followed previous work (Navigli and Ponzetto, 2012; Matuschek and Gurevych, 2013) and evaluated the alignment performance in terms of four measures: precision, recall, F1, and accuracy. Precision is the fraction of correct alignment judgments returned by the system and recall is the fraction of alignment judgments in the gold standard dataset that are correctly returned by the system. F1 is the harmonic mean of precision a</context>
<context position="29066" citStr="Gurevych et al., 2012" startWordPosition="4824" endWordPosition="4827">idation, where a 5-fold cross validation is carried out to find the optimal values for the parameters, a technique used in most of the recent alignment methods (Niemann and Gurevych, 2011; Meyer and Gurevych, 2012a; Matuschek and Gurevych, 2013). 4.2.2 Results We show in Table 3 the alignment performance of different systems on the task of aligning WN-WP, WN-WT, and WN-OW in terms of Precision (P), Recall (R), F1, and Accuracy. The SB system corresponds to the state-of-the-art definition similarity approaches for WN-WP (Niemann and Gurevych, 2011), WN-WT (Meyer and Gurevych, 2011), and WN-OW (Gurevych et al., 2012). DWSA stands for Dijkstra-WSA, the state-of-the-art graph-based alignment approach of Matuschek and Gurevych (2013). The authors also provided results for SB+Dijkstra-WSA, a hybrid system where DWSA was tuned for high precision and, in the case when no alignment target could be found, the algorithm fell back on SB judgments. We also show the results for this system as SB+DWSA in the table. For our approach (SemAlign) we show the results of six different runs each corresponding to a different setting. The first three (middle part of the table) correspond to the results obtained with the three </context>
<context position="37211" citStr="Gurevych et al., 2012" startWordPosition="6101" endWordPosition="6104">to other resources. These include: the Roget’s thesaurus and Longman Dictionary of Contemporary English (Kwong, 1998), FrameNet (Laparra and Rigau, 2009), VerbNet (Shi and Mihalcea, 2005) or domain-specific terminologies such as the Unified Medical Language System (Burgun and Bodenreider, 2001). More recently, the growth of collaboratively-constructed resources has seen the development of alignment approaches with Wikipedia (Ruiz-Casado et al., 2005; Auer et al., 2007; Suchanek et al., 2008; Reiter et al., 2008; Navigli and Ponzetto, 2012), Wiktionary (Meyer and Gurevych, 2011) and OmegaWiki (Gurevych et al., 2012). Last year Matuschek and Gurevych (2013) proposed Dijkstra-WSA, a graph-based approach relying on shortest paths between two concepts when the two corresponding resources graphs were combined by leveraging monosemous linking. Their method when backed off with other definition similarity based approaches (Niemann and Gurevych, 2011; Meyer and Gurevych, 2011), achieved state-of-the-art results on the mapping of WordNet to different collaboratively-constructed resources. This approach, however, in addition to setting the threshold for the definition similarity component by means of cross validat</context>
</contexts>
<marker>Gurevych, Eckle-Kohler, Hartmann, Matuschek, Meyer, Wirth, 2012</marker>
<rawString>Iryna Gurevych, Judith Eckle-Kohler, Silvana Hartmann, Michael Matuschek, Christian M. Meyer, and Christian Wirth. 2012. UBY - a large-scale unified lexical-semantic resource based on LMF. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 580–590, Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taher H Haveliwala</author>
</authors>
<title>Topic-sensitive PageRank.</title>
<date>2002</date>
<booktitle>In Proceedings of the 11th international conference on World Wide Web,</booktitle>
<pages>517--526</pages>
<location>Hawaii, USA.</location>
<contexts>
<context position="9831" citStr="Haveliwala, 2002" startWordPosition="1597" endWordPosition="1598">his component goes beyond the surface realization of concepts, thus providing a deeper measure of concept similarity. The two components share the same backbone (parts (b) and (d) of Figure 1), but differ in some stages (parts (a) and (c) in Figure 1). In the following, we explain all the stages involved in the two components (gray blocks in the figure). 2.1.1 Semantic signature generation The aim of this stage is to model a given concept or set of concepts through a vectorial semantic representation, which we refer to as the semantic signature of the input. We utilized Personalized PageRank (Haveliwala, 2002, PPR), a random walk graph algorithm, for calculating semantic signatures. The original PageRank (PR) algorithm (Brin and Page, 1998) computes, for a given graph, a single vector wherein each node is associated with a weight denoting its structural importance in that graph. PPR is a variation of PR where the computation is biased towards a set of initial nodes in order to capture the notion of importance with respect to those particular nodes. PPR has been previously used in a wide variety of tasks such as definition similarity-based resource alignment (Niemann and Gurevych, 2011), textual se</context>
</contexts>
<marker>Haveliwala, 2002</marker>
<rawString>Taher H. Haveliwala. 2002. Topic-sensitive PageRank. In Proceedings of the 11th international conference on World Wide Web, pages 517–526, Hawaii, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Henrich</author>
<author>Erhard Hinrichs</author>
<author>Tatiana Vodolazova</author>
</authors>
<title>Semi-automatic extension of GermaNet with sense definitions from Wiktionary.</title>
<date>2011</date>
<booktitle>In Proceedings of 5th Language &amp; Technology Conference (LTC 2011),</booktitle>
<pages>126--130</pages>
<location>Pozna,</location>
<contexts>
<context position="2099" citStr="Henrich et al., 2011" startWordPosition="309" endWordPosition="312">ic issue can only be addressed automatically, by means of lexical resource alignment algorithms. Owing to its ability to bring together features like multilinguality and increasing coverage, over the past few years resource alignment has proven beneficial to a wide spectrum of tasks, such as Semantic Parsing (Shi and Mihalcea, 2005), Semantic Role Labeling (Palmer et al., 2010), and Word Sense Disambiguation (Navigli and Ponzetto, 2012). Nevertheless, when it comes to aligning textual definitions in different resources, the lexical approach (Ruiz-Casado et al., 2005; de Melo and Weikum, 2010; Henrich et al., 2011) falls short because of the potential use of totally different wordings to define the same concept. Deeper approaches leverage semantic similarity to go beyond the surface realization of definitions (Navigli, 2006; Meyer and Gurevych, 2011; Niemann and Gurevych, 2011). While providing good results in general, these approaches fail when the definitions of a given word are not of adequate quality and expressiveness to be distinguishable from one another. When a lexical resource can be viewed as a semantic graph, as with WordNet or Wikipedia, this limit can be overcome by means of alignment algor</context>
</contexts>
<marker>Henrich, Hinrichs, Vodolazova, 2011</marker>
<rawString>Verena Henrich, Erhard Hinrichs, and Tatiana Vodolazova. 2011. Semi-automatic extension of GermaNet with sense definitions from Wiktionary. In Proceedings of 5th Language &amp; Technology Conference (LTC 2011), pages 126–130, Pozna, Poland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Henrich</author>
<author>Erhard W Hinrichs</author>
<author>Klaus Suttner</author>
</authors>
<title>Automatically linking GermaNet to Wikipedia for harvesting corpus examples for GermaNet senses.</title>
<date>2012</date>
<booktitle>In Journal for Language Technology and Computational Linguistics (JLCL),</booktitle>
<pages>27--1</pages>
<contexts>
<context position="8149" citStr="Henrich et al., 2012" startWordPosition="1326" endWordPosition="1330">components: definitional similarity and structural similarity. Each of these components gets, as its input, a pair of concepts belonging to two different semantic networks and produces a similarity score. These two scores are then combined into an overall score (part (e) of Figure 1) which quantifies the semantic similarity of the two input concepts c1 and c2. The definitional similarity component computes the similarity of two concepts in terms of the similarity of their definitions, a method that has also been used in previous work for aligning lexical resources (Niemann and Gurevych, 2011; Henrich et al., 2012). In spite of its simplicity, the mere calculation of the similarity of concept definitions provides a strong baseline, especially for cases where the definitional texts for a pair of concepts to be aligned are lexically similar, yet distinguishable from the other definitions. However, as mentioned in the introduction, definition similarity-based techniques fail at identifying the correct alignments in cases where different wordings are used or definitions are not of high quality. The structural similarity component, instead, is a novel graph-based similarity measurement technique which calcul</context>
<context position="32850" citStr="Henrich et al., 2012" startWordPosition="5424" endWordPosition="5428">The system performance is generally higher on the alignment task for WP compared to WT and OW. We attribute this difference to the dictionary nature of the latter two, where sense distinctions are more fine-grained, as opposed to the relatively concrete concepts in the WP encyclopedia. 4.3 Similarity Measure Analysis We explained in Section 2.1 that our concept similarity measure consists of two components: the definitional and the structural similarities. Measuring the similarity of two concepts in terms of their definitions has been investigated in previous work (Niemann and Gurevych, 2011; Henrich et al., 2012). The structural similarity component of our approach, however, is novel, but at the same time one of the very few measures which enables the computation of the similarity of concepts across two resources directly and independently of the similarity of their definitions. A comparable approach is the Dijkstra-WSA proposed by Matuschek and Gurevych (2013) which, as also mentioned earlier in the Introduction, first connects the two resources’ graphs by leveraging monosemous linking and then aligns two concepts across the two graphs on the basis of their shortest distance. To gain more insight int</context>
</contexts>
<marker>Henrich, Hinrichs, Suttner, 2012</marker>
<rawString>Verena Henrich, Erhard W. Hinrichs, and Klaus Suttner. 2012. Automatically linking GermaNet to Wikipedia for harvesting corpus examples for GermaNet senses. In Journal for Language Technology and Computational Linguistics (JLCL), 27(1):1–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard H Hovy</author>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>Collaboratively built semistructured content and Artificial Intelligence: The story so far.</title>
<date>2013</date>
<journal>Artificial Intelligence,</journal>
<pages>194--2</pages>
<contexts>
<context position="1251" citStr="Hovy et al., 2013" startWordPosition="173" endWordPosition="176">hine-readable dictionaries with no network structure. Our approach leverages a similarity measure that enables the structural comparison of senses across lexical resources, achieving state-of-the-art performance on the task of aligning WordNet to three different collaborative resources: Wikipedia, Wiktionary and OmegaWiki. 1 Introduction Lexical resources are repositories of machinereadable knowledge that can be used in virtually any Natural Language Processing task. Notable examples are WordNet, Wikipedia and, more recently, collaboratively-curated resources such as OmegaWiki and Wiktionary (Hovy et al., 2013). On the one hand, these resources are heterogeneous in design, structure and content, but, on the other hand, they often provide complementary knowledge which we would like to see integrated. Given the large scale this intrinsic issue can only be addressed automatically, by means of lexical resource alignment algorithms. Owing to its ability to bring together features like multilinguality and increasing coverage, over the past few years resource alignment has proven beneficial to a wide spectrum of tasks, such as Semantic Parsing (Shi and Mihalcea, 2005), Semantic Role Labeling (Palmer et al.</context>
<context position="35060" citStr="Hovy et al., 2013" startWordPosition="5778" endWordPosition="5781">both WN and WP provide a full-fledged semantic network and thus neither needed to be ontologized. Therefore, the considerable performance improvement over Dijkstra-WSA on this resource pair shows the effectiveness of our novel concept similarity measure independently of the underlying semantic network. 5 Related Work Resource ontologization. Having lexical resources represented as semantic networks is highly beneficial. A good example is WordNet, which has been exploited as a semantic network in dozens of NLP tasks (Fellbaum, 1998). A recent prominent case is Wikipedia (Medelyan et al., 2009; Hovy et al., 2013) which, thanks to its inter-article hyperlink structure, provides a rich backbone for structuring additional information (Auer et al., 2007; Suchanek et al., 2008; Moro and Navigli, 2013; Flati et al., 2014). However, there are many large-scale resources, such as Wiktionary for instance, which by their very nature are not in the form of a graph. This is 475 usually the case with machine-readable dictionaries, where structuring the resource involves the arduous task of connecting lexicographic senses by means of semantic relations. Surprisingly, despite their vast potential, little research has</context>
</contexts>
<marker>Hovy, Navigli, Ponzetto, 2013</marker>
<rawString>Eduard H. Hovy, Roberto Navigli, and Simone Paolo Ponzetto. 2013. Collaboratively built semistructured content and Artificial Intelligence: The story so far. Artificial Intelligence, 194:2–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thad Hughes</author>
<author>Daniel Ramage</author>
</authors>
<title>Lexical semantic relatedness with random graph walks.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>581--589</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="10473" citStr="Hughes and Ramage, 2007" startWordPosition="1702" endWordPosition="1705">lk graph algorithm, for calculating semantic signatures. The original PageRank (PR) algorithm (Brin and Page, 1998) computes, for a given graph, a single vector wherein each node is associated with a weight denoting its structural importance in that graph. PPR is a variation of PR where the computation is biased towards a set of initial nodes in order to capture the notion of importance with respect to those particular nodes. PPR has been previously used in a wide variety of tasks such as definition similarity-based resource alignment (Niemann and Gurevych, 2011), textual semantic similarity (Hughes and Ramage, 2007; Pilehvar et al., 2013), Word Sense Disambiguation (Agirre and Soroa, 2009; Faralli and Navigli, 2012) and semantic text categorization (Navigli et al., 2011). When applied to a semantic graph by initializing the random walks from a set of concepts (nodes), PPR yields a vector in which each concept is associated with a weight denoting its semantic relevance to the initial concepts. Formally, we first represent a semantic network consisting of N concepts as a row-stochastic transition matrix M E RN×N. The cell (i, j) in the matrix denotes the probability of moving from a concept i to j in the </context>
</contexts>
<marker>Hughes, Ramage, 2007</marker>
<rawString>Thad Hughes and Daniel Ramage. 2007. Lexical semantic relatedness with random graph walks. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing, pages 581–589, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oi Yee Kwong</author>
</authors>
<title>Aligning WordNet with additional lexical resources.</title>
<date>1998</date>
<booktitle>In COLING-ACL98 Workshop on Usage of WordNet in Natural Language Processing Systems,</booktitle>
<pages>73--79</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="36706" citStr="Kwong, 1998" startWordPosition="6030" endWordPosition="6031">cal resource into a full-fledged semantic network, hence providing a denser graph with most of its nodes connected. Resource alignment. Aligning lexical resources has been a very active field of research in the last decade. One of the main objectives in this area has been to enrich existing ontologies by means of complementary information from other resources. As a matter of fact, most efforts have been concentrated on aligning the de facto community standard sense inventory, i.e. WordNet, to other resources. These include: the Roget’s thesaurus and Longman Dictionary of Contemporary English (Kwong, 1998), FrameNet (Laparra and Rigau, 2009), VerbNet (Shi and Mihalcea, 2005) or domain-specific terminologies such as the Unified Medical Language System (Burgun and Bodenreider, 2001). More recently, the growth of collaboratively-constructed resources has seen the development of alignment approaches with Wikipedia (Ruiz-Casado et al., 2005; Auer et al., 2007; Suchanek et al., 2008; Reiter et al., 2008; Navigli and Ponzetto, 2012), Wiktionary (Meyer and Gurevych, 2011) and OmegaWiki (Gurevych et al., 2012). Last year Matuschek and Gurevych (2013) proposed Dijkstra-WSA, a graph-based approach relying</context>
</contexts>
<marker>Kwong, 1998</marker>
<rawString>Oi Yee Kwong. 1998. Aligning WordNet with additional lexical resources. In COLING-ACL98 Workshop on Usage of WordNet in Natural Language Processing Systems, pages 73–79, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Egoitzand Laparra</author>
<author>German Rigau</author>
</authors>
<title>Integrating WordNet and FrameNet using a knowledgebased Word Sense Disambiguation algorithm.</title>
<date>2009</date>
<booktitle>In Proceedings of Recent Advances in Natural Language Processing (RANLP09),</booktitle>
<pages>1--6</pages>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="36742" citStr="Laparra and Rigau, 2009" startWordPosition="6033" endWordPosition="6036">-fledged semantic network, hence providing a denser graph with most of its nodes connected. Resource alignment. Aligning lexical resources has been a very active field of research in the last decade. One of the main objectives in this area has been to enrich existing ontologies by means of complementary information from other resources. As a matter of fact, most efforts have been concentrated on aligning the de facto community standard sense inventory, i.e. WordNet, to other resources. These include: the Roget’s thesaurus and Longman Dictionary of Contemporary English (Kwong, 1998), FrameNet (Laparra and Rigau, 2009), VerbNet (Shi and Mihalcea, 2005) or domain-specific terminologies such as the Unified Medical Language System (Burgun and Bodenreider, 2001). More recently, the growth of collaboratively-constructed resources has seen the development of alignment approaches with Wikipedia (Ruiz-Casado et al., 2005; Auer et al., 2007; Suchanek et al., 2008; Reiter et al., 2008; Navigli and Ponzetto, 2012), Wiktionary (Meyer and Gurevych, 2011) and OmegaWiki (Gurevych et al., 2012). Last year Matuschek and Gurevych (2013) proposed Dijkstra-WSA, a graph-based approach relying on shortest paths between two conce</context>
</contexts>
<marker>Laparra, Rigau, 2009</marker>
<rawString>Egoitzand Laparra and German Rigau. 2009. Integrating WordNet and FrameNet using a knowledgebased Word Sense Disambiguation algorithm. In Proceedings of Recent Advances in Natural Language Processing (RANLP09), pages 1–6, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Matuschek</author>
<author>Iryna Gurevych</author>
</authors>
<title>Dijkstra-WSA: A graph-based approach to word sense alignment.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics (TACL),</journal>
<pages>1--151</pages>
<contexts>
<context position="3251" citStr="Matuschek and Gurevych, 2013" startWordPosition="489" endWordPosition="492"> WordNet or Wikipedia, this limit can be overcome by means of alignment algorithms that exploit the network structure to determine the similarity of concept pairs. However, not all lexical resources provide explicit semantic relations between concepts and, hence, machine-readable dictionaries like Wiktionary have first to be transformed into semantic graphs before such graph-based approaches can be applied to them. To do this, recent work has proposed graph construction by monosemous linking, where a concept is linked to all the concepts associated with the monosemous words in its definition (Matuschek and Gurevych, 2013). However, this alignment method still involves tuning of parameters which are highly dependent on the characteristics of the generated graphs and, hence, requires hand-crafted sense alignments for the specific pair of resources to be aligned, a task which has to be replicated every time the resources are updated. In this paper we propose a unified approach to aligning arbitrary pairs of lexical resources which is independent of their specific structure. Thanks to a novel modeling of the sense entries and an effective ontologization algorithm, our approach also fares well when resources lack r</context>
<context position="19582" citStr="Matuschek and Gurevych (2013)" startWordPosition="3288" endWordPosition="3291">fruit of a conifer”. The definition contains two content words: fruitn and conifern. The latter word is monosemous in Wiktionary, hence we directly connect conen to the only sense of conifern. The noun fruit, however, has 5 senses in Wiktionary. We therefore measure the similarity between the definition of conen and all the 5 definitions offruit and introduce a link from cone4n to the sense of fruit which yields the maximal similarity value (defined as “(botany) The seedbearing part of a plant...”). 4 Experiments Lexical resources. To enable a comparison with the state of the art, we followed Matuschek and Gurevych (2013) and performed an alignment of WordNet synsets (WN) to three different collaboratively-constructed resources: Wikipedia (WP), Wiktionary (WT), and OmegaWiki (OW). We utilized the DKPro software (Zesch et al., 2008; Gurevych et al., 2012) to access the information in the foregoing three resources. For WP, WT, OW we used the dump versions 20090822, 20131002, and 20131115, respectively. Evaluation measures. We followed previous work (Navigli and Ponzetto, 2012; Matuschek and Gurevych, 2013) and evaluated the alignment performance in terms of four measures: precision, recall, F1, and accuracy. Pre</context>
<context position="21089" citStr="Matuschek and Gurevych (2013)" startWordPosition="3531" endWordPosition="3534">uracy which, in addition to true positives, takes into account true negatives, i.e., pairs which are correctly judged as unaligned. Lexicons and semantic graphs. Here, we describe how the four semantic graphs for our four lexical resources (i.e., WN, WP, WT, OW) were constructed. As mentioned in Section 2.1.1, we build the WN graph by including all the synsets and semantic relations defined in WordNet (e.g., hypernymy and meronymy) and further populate the relation set by connecting a synset to all the other synsets that appear in its disambiguated gloss. For WP, we used the graph provided by Matuschek and Gurevych (2013), constructed by directly connecting an article (concept) to all the hyperlinks in its first paragraph, together with the category links. Our WN and WP graphs have 118K and 2.8M nodes, respectively, with the average node degree being roughly 9 in both resources. The other two resources, i.e., WT and OW, do not provide a reliable network of semantic relations, therefore we used our ontologization approach to construct their corresponding semantic graphs. We report, in the following subsection, the experiments carried out to assess the accuracy of our ontologization method, together with the sta</context>
<context position="24931" citStr="Matuschek and Gurevych (2013)" startWordPosition="4162" endWordPosition="4165">s is particularly Approach P R F1 A WKTWSD 0.780 0.800 0.790 0.840 Our method 0.852 0.767 0.807 0.857 Human - - 0.890 0.910 Table 2: The performance of relation disambiguation for our similarity-based disambiguation method, as well as for the WKTWSD system. interesting as the WKTWSD system uses a rulebased technique specific to relation disambiguation in WT, whereas our method is resource independent and can be applied to arbitrary words in the definition of any concept. We also note that the graph constructed by Meyer and Gurevych (2010) had an average node degree of around 1. More recently, Matuschek and Gurevych (2013) leveraged monosemous linking (cf. Section 5) in order to create denser semantic graphs for OW and WT. Our approach, however, thanks to the connections obtained through ambiguous words, can provide graphs with significantly higher coverage. As an example, for WT, Matuschek and Gurevych (2013) generated a graph where around 30% of the nodes were in isolation, whereas this number drops to around 5% in our corresponding graph. These results show that our ontologization approach can be used to obtain dense semantic graph representations of lexical resources, while at the same time preserving a hig</context>
<context position="27699" citStr="Matuschek and Gurevych, 2013" startWordPosition="4607" endWordPosition="4610">89 Tuning on WN-WT 0.738 0.934 0.824 0.950 - - - - 0.805 0.677 0.736 0.900 Tuning on WN-OW 0.744 0.925 0.824 0.950 0.684 0.766 0.723 0.930 - - - - Table 3: The performance of different systems on the task of aligning WordNet to Wikipedia (WN-WP), Wiktionary (WN-WT), and OmegaWiki (WN-OW) in terms of Precision (P), Recall (R), F1, and Accuracy (A). We present results for different configurations of our system (SemAlign), together with the state of the art in definition similarity-based alignment approaches (SB) and the best configuration of the stateof-the-art graph-based system, Dijkstra-WSA (Matuschek and Gurevych, 2013, DWSA). Configurations. Recall from Section 2 that our resource alignment technique has two parameters: the similarity threshold θ and the combination parameter β, both defined in [0, 1]. We performed experiments with three different configurations: • Unsupervised, where the two parameters are set to their middle values (i.e., 0.5), hence, no tuning is performed for either of the parameters. In this case, both the definitional and structural similarity scores are treated as equally important and two concepts are aligned if their overall similarity exceeds the middle point of the similarity sc</context>
<context position="29182" citStr="Matuschek and Gurevych (2013)" startWordPosition="4838" endWordPosition="4841">nique used in most of the recent alignment methods (Niemann and Gurevych, 2011; Meyer and Gurevych, 2012a; Matuschek and Gurevych, 2013). 4.2.2 Results We show in Table 3 the alignment performance of different systems on the task of aligning WN-WP, WN-WT, and WN-OW in terms of Precision (P), Recall (R), F1, and Accuracy. The SB system corresponds to the state-of-the-art definition similarity approaches for WN-WP (Niemann and Gurevych, 2011), WN-WT (Meyer and Gurevych, 2011), and WN-OW (Gurevych et al., 2012). DWSA stands for Dijkstra-WSA, the state-of-the-art graph-based alignment approach of Matuschek and Gurevych (2013). The authors also provided results for SB+Dijkstra-WSA, a hybrid system where DWSA was tuned for high precision and, in the case when no alignment target could be found, the algorithm fell back on SB judgments. We also show the results for this system as SB+DWSA in the table. For our approach (SemAlign) we show the results of six different runs each corresponding to a different setting. The first three (middle part of the table) correspond to the results obtained with the three configurations of SemAlign: unsupervised, with tuning on subset, and cross-validation (see Section 4.2.1). In additi</context>
<context position="31369" citStr="Matuschek and Gurevych, 2013" startWordPosition="5194" endWordPosition="5197">ust inasmuch as it provides competitive results on all the three datasets, while it surpasses the performance of SB+DWSA on WN-WT. This 4All significance tests are done using z-test at p &lt; 0.05. 474 Approach WN-WP WN-WT WN-OW P R F1 A P R F1 A P R F1 A Dijkstra-WSA 0.750 0.670 0.710 0.930 0.680 0.270 0.390 0.890 0.651 0.372 0.473 0.830 SemAlignstr 0.877 0.788 0.830 0.959 0.604 0.643 0.623 0.907 0.654 0.602 0.627 0.853 Table 4: Performance of SemAlign when using only the structural similarity component (SemAlignstr) compared to the state-of-the-art graph-based alignment approach, Dijkstra-WSA (Matuschek and Gurevych, 2013) for our three resource pairs: WordNet to Wikipedia (WN-WP), Wiktionary (WN-WT), and OmegaWiki (WN-OW). is particularly interesting as the latter system involves tuning of several parameters, whereas SemAlign, in its unsupervised configuration, does not need any training data nor does it involve any tuning. In addition, as can be seen in the table, SemAlign benefits from pair-independent training data in most cases across the three resource pairs with performance surpassing that of SB+DWSA, a system which is dependent on pair-specific training data. The consistency in the performance of SemAli</context>
<context position="33205" citStr="Matuschek and Gurevych (2013)" startWordPosition="5481" endWordPosition="5485">that our concept similarity measure consists of two components: the definitional and the structural similarities. Measuring the similarity of two concepts in terms of their definitions has been investigated in previous work (Niemann and Gurevych, 2011; Henrich et al., 2012). The structural similarity component of our approach, however, is novel, but at the same time one of the very few measures which enables the computation of the similarity of concepts across two resources directly and independently of the similarity of their definitions. A comparable approach is the Dijkstra-WSA proposed by Matuschek and Gurevych (2013) which, as also mentioned earlier in the Introduction, first connects the two resources’ graphs by leveraging monosemous linking and then aligns two concepts across the two graphs on the basis of their shortest distance. To gain more insight into the effectiveness of our structural similarity measure in comparison to the Dijkstra-WSA method, we carried out an experiment where our alignment system used only the structural similarity component, a variant of our system we refer to as SemAlignstr. Both systems (i.e., SemAlignstr and Dijkstra-WSA) were tuned on 100-item subsets of the corresponding</context>
<context position="35843" citStr="Matuschek and Gurevych (2013)" startWordPosition="5892" endWordPosition="5895">l., 2008; Moro and Navigli, 2013; Flati et al., 2014). However, there are many large-scale resources, such as Wiktionary for instance, which by their very nature are not in the form of a graph. This is 475 usually the case with machine-readable dictionaries, where structuring the resource involves the arduous task of connecting lexicographic senses by means of semantic relations. Surprisingly, despite their vast potential, little research has been conducted on the automatic ontologization of collaboratively-constructed dictionaries like Wiktionary and OmegaWiki. Meyer and Gurevych (2012a) and Matuschek and Gurevych (2013) provided approaches for building graph representations of Wiktionary and OmegaWiki. The resulting graphs, however, were either sparse or had a considerable portion of the nodes left in isolation. Our approach, in contrast, aims at transforming a lexical resource into a full-fledged semantic network, hence providing a denser graph with most of its nodes connected. Resource alignment. Aligning lexical resources has been a very active field of research in the last decade. One of the main objectives in this area has been to enrich existing ontologies by means of complementary information from oth</context>
<context position="37252" citStr="Matuschek and Gurevych (2013)" startWordPosition="6107" endWordPosition="6110"> the Roget’s thesaurus and Longman Dictionary of Contemporary English (Kwong, 1998), FrameNet (Laparra and Rigau, 2009), VerbNet (Shi and Mihalcea, 2005) or domain-specific terminologies such as the Unified Medical Language System (Burgun and Bodenreider, 2001). More recently, the growth of collaboratively-constructed resources has seen the development of alignment approaches with Wikipedia (Ruiz-Casado et al., 2005; Auer et al., 2007; Suchanek et al., 2008; Reiter et al., 2008; Navigli and Ponzetto, 2012), Wiktionary (Meyer and Gurevych, 2011) and OmegaWiki (Gurevych et al., 2012). Last year Matuschek and Gurevych (2013) proposed Dijkstra-WSA, a graph-based approach relying on shortest paths between two concepts when the two corresponding resources graphs were combined by leveraging monosemous linking. Their method when backed off with other definition similarity based approaches (Niemann and Gurevych, 2011; Meyer and Gurevych, 2011), achieved state-of-the-art results on the mapping of WordNet to different collaboratively-constructed resources. This approach, however, in addition to setting the threshold for the definition similarity component by means of cross validation, also required other parameters to be</context>
</contexts>
<marker>Matuschek, Gurevych, 2013</marker>
<rawString>Michael Matuschek and Iryna Gurevych. 2013. Dijkstra-WSA: A graph-based approach to word sense alignment. Transactions of the Association for Computational Linguistics (TACL), 1:151–164.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Olena Medelyan</author>
</authors>
<location>David Milne, Catherine Legg, and</location>
<marker>Medelyan, </marker>
<rawString>Olena Medelyan, David Milne, Catherine Legg, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
</authors>
<title>Mining meaning from Wikipedia.</title>
<date>2009</date>
<journal>International Journal of HumanComputer Studies,</journal>
<volume>67</volume>
<issue>9</issue>
<marker>Witten, 2009</marker>
<rawString>Ian H. Witten. 2009. Mining meaning from Wikipedia. International Journal of HumanComputer Studies, 67(9):716–754.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian M Meyer</author>
<author>Iryna Gurevych</author>
</authors>
<title>worth its weight in gold or yet another resource”; a comparative study of Wiktionary, OpenThesaurus and GermaNet.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th International Conference on Computational Linguistics and Intelligent Text Processing, CICLing’10,</booktitle>
<pages>38--49</pages>
<location>Iasi, Romania.</location>
<contexts>
<context position="23505" citStr="Meyer and Gurevych (2010)" startWordPosition="3926" endWordPosition="3929">r of edges together with their distribution across types (i.e., ambiguous and unambiguous) and sources (i.e., definitions and relations) from which candidate words were obtained. The edges obtained from unambiguous entries are essentially sense disambiguated on both sides whereas those obtained from ambiguous terms are a result of our similarity-based disambiguation. Hence, given that a large portion of edges came from ambiguous words (see Table 1), we carried out an experiment to evaluate the accuracy of our disambiguation method. To this end, we took as our benchmark the dataset provided by Meyer and Gurevych (2010) for evaluating relation disambiguation in WT. The dataset contains 394 manually-disambiguated relations. We compared our similarity-based disambiguation approach against the state of the art on this dataset, i.e., the WKTWSD system, which is a WT relation disambiguation algorithm based on a series of rules (Meyer and Gurevych, 2012b). Table 2 shows the performance of our disambiguation method, together with that of WKTWSD, in terms of Precision (P), Recall (R), F1, and accuracy. The “Human” row corresponds to the inter-rater F1 and accuracy scores, i.e., the upperbound performance on this dat</context>
<context position="24846" citStr="Meyer and Gurevych (2010)" startWordPosition="4148" endWordPosition="4151">the performance of the WKTWSD system in terms of precision, F1, and accuracy. This is particularly Approach P R F1 A WKTWSD 0.780 0.800 0.790 0.840 Our method 0.852 0.767 0.807 0.857 Human - - 0.890 0.910 Table 2: The performance of relation disambiguation for our similarity-based disambiguation method, as well as for the WKTWSD system. interesting as the WKTWSD system uses a rulebased technique specific to relation disambiguation in WT, whereas our method is resource independent and can be applied to arbitrary words in the definition of any concept. We also note that the graph constructed by Meyer and Gurevych (2010) had an average node degree of around 1. More recently, Matuschek and Gurevych (2013) leveraged monosemous linking (cf. Section 5) in order to create denser semantic graphs for OW and WT. Our approach, however, thanks to the connections obtained through ambiguous words, can provide graphs with significantly higher coverage. As an example, for WT, Matuschek and Gurevych (2013) generated a graph where around 30% of the nodes were in isolation, whereas this number drops to around 5% in our corresponding graph. These results show that our ontologization approach can be used to obtain dense semanti</context>
</contexts>
<marker>Meyer, Gurevych, 2010</marker>
<rawString>Christian M. Meyer and Iryna Gurevych. 2010. “worth its weight in gold or yet another resource”; a comparative study of Wiktionary, OpenThesaurus and GermaNet. In Proceedings of the 11th International Conference on Computational Linguistics and Intelligent Text Processing, CICLing’10, pages 38–49, Iasi, Romania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian M Meyer</author>
<author>Iryna Gurevych</author>
</authors>
<title>What psycholinguists know about Chemistry: Aligning Wiktionary and WordNet for increased domain coverage.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>883--892</pages>
<location>Chiang Mai, Thailand.</location>
<contexts>
<context position="2338" citStr="Meyer and Gurevych, 2011" startWordPosition="347" endWordPosition="350">s proven beneficial to a wide spectrum of tasks, such as Semantic Parsing (Shi and Mihalcea, 2005), Semantic Role Labeling (Palmer et al., 2010), and Word Sense Disambiguation (Navigli and Ponzetto, 2012). Nevertheless, when it comes to aligning textual definitions in different resources, the lexical approach (Ruiz-Casado et al., 2005; de Melo and Weikum, 2010; Henrich et al., 2011) falls short because of the potential use of totally different wordings to define the same concept. Deeper approaches leverage semantic similarity to go beyond the surface realization of definitions (Navigli, 2006; Meyer and Gurevych, 2011; Niemann and Gurevych, 2011). While providing good results in general, these approaches fail when the definitions of a given word are not of adequate quality and expressiveness to be distinguishable from one another. When a lexical resource can be viewed as a semantic graph, as with WordNet or Wikipedia, this limit can be overcome by means of alignment algorithms that exploit the network structure to determine the similarity of concept pairs. However, not all lexical resources provide explicit semantic relations between concepts and, hence, machine-readable dictionaries like Wiktionary have f</context>
<context position="6241" citStr="Meyer and Gurevych, 2011" startWordPosition="985" endWordPosition="988">lignment process: the algorithm takes as input the semantic graphs G1 and G2 corresponding to the two resources, as explained above, and produces as output an alignment in the form of a set A of concept pairs. The algorithm iterates over all concepts c1 E V1 and, for each of them, obtains the set of concepts C C V2, which can be considered as alignment candidates for c1 (line 3). For a concept c1, alignment candidates in G2 usually consist of every concept c2 E V2 that shares at least one lexicalization with c1 in the same part of speech tag, i.e., LG1(c1) n LG2(c2) =� 0 (Reiter et al., 2008; Meyer and Gurevych, 2011). Once the set of target candidates C for a source concept c1 is obtained, the alignment task can be cast as that of identifying those concepts in C to which c1 should be aligned. To do this, the algorithm calculates the similarity between c1 and each c2 E C (line 5). If their similarity score exceeds a certain value denoted by θ Algorithm 1 Lexical Resource Aligner Input: graphs H = (VH, EH), G1 = (V1, E1) and G2 = (V2, E2), the similarity threshold B, and the combination parameter β Output: A, the set of all aligned concept pairs 1: A +— 0 2: for each concept c1 E V1 3: C +— getCandidates(c1</context>
<context position="29031" citStr="Meyer and Gurevych, 2011" startWordPosition="4818" endWordPosition="4821">aset comprising 100 items. • Cross-validation, where a 5-fold cross validation is carried out to find the optimal values for the parameters, a technique used in most of the recent alignment methods (Niemann and Gurevych, 2011; Meyer and Gurevych, 2012a; Matuschek and Gurevych, 2013). 4.2.2 Results We show in Table 3 the alignment performance of different systems on the task of aligning WN-WP, WN-WT, and WN-OW in terms of Precision (P), Recall (R), F1, and Accuracy. The SB system corresponds to the state-of-the-art definition similarity approaches for WN-WP (Niemann and Gurevych, 2011), WN-WT (Meyer and Gurevych, 2011), and WN-OW (Gurevych et al., 2012). DWSA stands for Dijkstra-WSA, the state-of-the-art graph-based alignment approach of Matuschek and Gurevych (2013). The authors also provided results for SB+Dijkstra-WSA, a hybrid system where DWSA was tuned for high precision and, in the case when no alignment target could be found, the algorithm fell back on SB judgments. We also show the results for this system as SB+DWSA in the table. For our approach (SemAlign) we show the results of six different runs each corresponding to a different setting. The first three (middle part of the table) correspond to t</context>
<context position="37173" citStr="Meyer and Gurevych, 2011" startWordPosition="6095" endWordPosition="6098"> standard sense inventory, i.e. WordNet, to other resources. These include: the Roget’s thesaurus and Longman Dictionary of Contemporary English (Kwong, 1998), FrameNet (Laparra and Rigau, 2009), VerbNet (Shi and Mihalcea, 2005) or domain-specific terminologies such as the Unified Medical Language System (Burgun and Bodenreider, 2001). More recently, the growth of collaboratively-constructed resources has seen the development of alignment approaches with Wikipedia (Ruiz-Casado et al., 2005; Auer et al., 2007; Suchanek et al., 2008; Reiter et al., 2008; Navigli and Ponzetto, 2012), Wiktionary (Meyer and Gurevych, 2011) and OmegaWiki (Gurevych et al., 2012). Last year Matuschek and Gurevych (2013) proposed Dijkstra-WSA, a graph-based approach relying on shortest paths between two concepts when the two corresponding resources graphs were combined by leveraging monosemous linking. Their method when backed off with other definition similarity based approaches (Niemann and Gurevych, 2011; Meyer and Gurevych, 2011), achieved state-of-the-art results on the mapping of WordNet to different collaboratively-constructed resources. This approach, however, in addition to setting the threshold for the definition similari</context>
</contexts>
<marker>Meyer, Gurevych, 2011</marker>
<rawString>Christian M. Meyer and Iryna Gurevych. 2011. What psycholinguists know about Chemistry: Aligning Wiktionary and WordNet for increased domain coverage. In Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 883–892, Chiang Mai, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian M Meyer</author>
<author>Iryna Gurevych</author>
</authors>
<title>OntoWiktionary: Constructing an ontology from the collaborative online dictionary Wiktionary.</title>
<date>2012</date>
<booktitle>In SemiAutomatic Ontology Development: Processes and Resources,</booktitle>
<pages>131--161</pages>
<publisher>IGI Global.</publisher>
<contexts>
<context position="23839" citStr="Meyer and Gurevych, 2012" startWordPosition="3978" endWordPosition="3981">r similarity-based disambiguation. Hence, given that a large portion of edges came from ambiguous words (see Table 1), we carried out an experiment to evaluate the accuracy of our disambiguation method. To this end, we took as our benchmark the dataset provided by Meyer and Gurevych (2010) for evaluating relation disambiguation in WT. The dataset contains 394 manually-disambiguated relations. We compared our similarity-based disambiguation approach against the state of the art on this dataset, i.e., the WKTWSD system, which is a WT relation disambiguation algorithm based on a series of rules (Meyer and Gurevych, 2012b). Table 2 shows the performance of our disambiguation method, together with that of WKTWSD, in terms of Precision (P), Recall (R), F1, and accuracy. The “Human” row corresponds to the inter-rater F1 and accuracy scores, i.e., the upperbound performance on this dataset, as calculated by Meyer and Gurevych (2010). As can be seen, our method proves to be very accurate, surpassing the performance of the WKTWSD system in terms of precision, F1, and accuracy. This is particularly Approach P R F1 A WKTWSD 0.780 0.800 0.790 0.840 Our method 0.852 0.767 0.807 0.857 Human - - 0.890 0.910 Table 2: The </context>
<context position="28657" citStr="Meyer and Gurevych, 2012" startWordPosition="4759" endWordPosition="4762">, hence, no tuning is performed for either of the parameters. In this case, both the definitional and structural similarity scores are treated as equally important and two concepts are aligned if their overall similarity exceeds the middle point of the similarity scale. • Tuning, where we follow Matuschek and Gurevych (2013) and tune the parameters on a subset of the dataset comprising 100 items. • Cross-validation, where a 5-fold cross validation is carried out to find the optimal values for the parameters, a technique used in most of the recent alignment methods (Niemann and Gurevych, 2011; Meyer and Gurevych, 2012a; Matuschek and Gurevych, 2013). 4.2.2 Results We show in Table 3 the alignment performance of different systems on the task of aligning WN-WP, WN-WT, and WN-OW in terms of Precision (P), Recall (R), F1, and Accuracy. The SB system corresponds to the state-of-the-art definition similarity approaches for WN-WP (Niemann and Gurevych, 2011), WN-WT (Meyer and Gurevych, 2011), and WN-OW (Gurevych et al., 2012). DWSA stands for Dijkstra-WSA, the state-of-the-art graph-based alignment approach of Matuschek and Gurevych (2013). The authors also provided results for SB+Dijkstra-WSA, a hybrid system wh</context>
<context position="35807" citStr="Meyer and Gurevych (2012" startWordPosition="5887" endWordPosition="5890">uer et al., 2007; Suchanek et al., 2008; Moro and Navigli, 2013; Flati et al., 2014). However, there are many large-scale resources, such as Wiktionary for instance, which by their very nature are not in the form of a graph. This is 475 usually the case with machine-readable dictionaries, where structuring the resource involves the arduous task of connecting lexicographic senses by means of semantic relations. Surprisingly, despite their vast potential, little research has been conducted on the automatic ontologization of collaboratively-constructed dictionaries like Wiktionary and OmegaWiki. Meyer and Gurevych (2012a) and Matuschek and Gurevych (2013) provided approaches for building graph representations of Wiktionary and OmegaWiki. The resulting graphs, however, were either sparse or had a considerable portion of the nodes left in isolation. Our approach, in contrast, aims at transforming a lexical resource into a full-fledged semantic network, hence providing a denser graph with most of its nodes connected. Resource alignment. Aligning lexical resources has been a very active field of research in the last decade. One of the main objectives in this area has been to enrich existing ontologies by means o</context>
</contexts>
<marker>Meyer, Gurevych, 2012</marker>
<rawString>Christian M. Meyer and Iryna Gurevych. 2012a. OntoWiktionary: Constructing an ontology from the collaborative online dictionary Wiktionary. In SemiAutomatic Ontology Development: Processes and Resources, pages 131–161. IGI Global.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian M Meyer</author>
<author>Iryna Gurevych</author>
</authors>
<title>To exhibit is not to loiter: A multilingual, sensedisambiguated Wiktionary for measuring verb similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012),</booktitle>
<pages>1763--1780</pages>
<location>Mumbai, India.</location>
<contexts>
<context position="23839" citStr="Meyer and Gurevych, 2012" startWordPosition="3978" endWordPosition="3981">r similarity-based disambiguation. Hence, given that a large portion of edges came from ambiguous words (see Table 1), we carried out an experiment to evaluate the accuracy of our disambiguation method. To this end, we took as our benchmark the dataset provided by Meyer and Gurevych (2010) for evaluating relation disambiguation in WT. The dataset contains 394 manually-disambiguated relations. We compared our similarity-based disambiguation approach against the state of the art on this dataset, i.e., the WKTWSD system, which is a WT relation disambiguation algorithm based on a series of rules (Meyer and Gurevych, 2012b). Table 2 shows the performance of our disambiguation method, together with that of WKTWSD, in terms of Precision (P), Recall (R), F1, and accuracy. The “Human” row corresponds to the inter-rater F1 and accuracy scores, i.e., the upperbound performance on this dataset, as calculated by Meyer and Gurevych (2010). As can be seen, our method proves to be very accurate, surpassing the performance of the WKTWSD system in terms of precision, F1, and accuracy. This is particularly Approach P R F1 A WKTWSD 0.780 0.800 0.790 0.840 Our method 0.852 0.767 0.807 0.857 Human - - 0.890 0.910 Table 2: The </context>
<context position="28657" citStr="Meyer and Gurevych, 2012" startWordPosition="4759" endWordPosition="4762">, hence, no tuning is performed for either of the parameters. In this case, both the definitional and structural similarity scores are treated as equally important and two concepts are aligned if their overall similarity exceeds the middle point of the similarity scale. • Tuning, where we follow Matuschek and Gurevych (2013) and tune the parameters on a subset of the dataset comprising 100 items. • Cross-validation, where a 5-fold cross validation is carried out to find the optimal values for the parameters, a technique used in most of the recent alignment methods (Niemann and Gurevych, 2011; Meyer and Gurevych, 2012a; Matuschek and Gurevych, 2013). 4.2.2 Results We show in Table 3 the alignment performance of different systems on the task of aligning WN-WP, WN-WT, and WN-OW in terms of Precision (P), Recall (R), F1, and Accuracy. The SB system corresponds to the state-of-the-art definition similarity approaches for WN-WP (Niemann and Gurevych, 2011), WN-WT (Meyer and Gurevych, 2011), and WN-OW (Gurevych et al., 2012). DWSA stands for Dijkstra-WSA, the state-of-the-art graph-based alignment approach of Matuschek and Gurevych (2013). The authors also provided results for SB+Dijkstra-WSA, a hybrid system wh</context>
<context position="35807" citStr="Meyer and Gurevych (2012" startWordPosition="5887" endWordPosition="5890">uer et al., 2007; Suchanek et al., 2008; Moro and Navigli, 2013; Flati et al., 2014). However, there are many large-scale resources, such as Wiktionary for instance, which by their very nature are not in the form of a graph. This is 475 usually the case with machine-readable dictionaries, where structuring the resource involves the arduous task of connecting lexicographic senses by means of semantic relations. Surprisingly, despite their vast potential, little research has been conducted on the automatic ontologization of collaboratively-constructed dictionaries like Wiktionary and OmegaWiki. Meyer and Gurevych (2012a) and Matuschek and Gurevych (2013) provided approaches for building graph representations of Wiktionary and OmegaWiki. The resulting graphs, however, were either sparse or had a considerable portion of the nodes left in isolation. Our approach, in contrast, aims at transforming a lexical resource into a full-fledged semantic network, hence providing a denser graph with most of its nodes connected. Resource alignment. Aligning lexical resources has been a very active field of research in the last decade. One of the main objectives in this area has been to enrich existing ontologies by means o</context>
</contexts>
<marker>Meyer, Gurevych, 2012</marker>
<rawString>Christian M. Meyer and Iryna Gurevych. 2012b. To exhibit is not to loiter: A multilingual, sensedisambiguated Wiktionary for measuring verb similarity. In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012), pages 1763–1780, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Moro</author>
<author>Roberto Navigli</author>
</authors>
<title>Integrating syntactic and semantic analysis into the Open Information Extraction paradigm.</title>
<date>2013</date>
<booktitle>In Proceedings of the 23rd International Joint Conference on Artificial Intelligence (IJCAI</booktitle>
<pages>2148--2154</pages>
<location>Beijing, China.</location>
<contexts>
<context position="35246" citStr="Moro and Navigli, 2013" startWordPosition="5805" endWordPosition="5808">ce pair shows the effectiveness of our novel concept similarity measure independently of the underlying semantic network. 5 Related Work Resource ontologization. Having lexical resources represented as semantic networks is highly beneficial. A good example is WordNet, which has been exploited as a semantic network in dozens of NLP tasks (Fellbaum, 1998). A recent prominent case is Wikipedia (Medelyan et al., 2009; Hovy et al., 2013) which, thanks to its inter-article hyperlink structure, provides a rich backbone for structuring additional information (Auer et al., 2007; Suchanek et al., 2008; Moro and Navigli, 2013; Flati et al., 2014). However, there are many large-scale resources, such as Wiktionary for instance, which by their very nature are not in the form of a graph. This is 475 usually the case with machine-readable dictionaries, where structuring the resource involves the arduous task of connecting lexicographic senses by means of semantic relations. Surprisingly, despite their vast potential, little research has been conducted on the automatic ontologization of collaboratively-constructed dictionaries like Wiktionary and OmegaWiki. Meyer and Gurevych (2012a) and Matuschek and Gurevych (2013) pr</context>
</contexts>
<marker>Moro, Navigli, 2013</marker>
<rawString>Andrea Moro and Roberto Navigli. 2013. Integrating syntactic and semantic analysis into the Open Information Extraction paradigm. In Proceedings of the 23rd International Joint Conference on Artificial Intelligence (IJCAI 2013), pages 2148–2154, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network.</title>
<date>2012</date>
<journal>Artificial Intelligence,</journal>
<volume>193</volume>
<pages>250</pages>
<contexts>
<context position="1918" citStr="Navigli and Ponzetto, 2012" startWordPosition="281" endWordPosition="284">eterogeneous in design, structure and content, but, on the other hand, they often provide complementary knowledge which we would like to see integrated. Given the large scale this intrinsic issue can only be addressed automatically, by means of lexical resource alignment algorithms. Owing to its ability to bring together features like multilinguality and increasing coverage, over the past few years resource alignment has proven beneficial to a wide spectrum of tasks, such as Semantic Parsing (Shi and Mihalcea, 2005), Semantic Role Labeling (Palmer et al., 2010), and Word Sense Disambiguation (Navigli and Ponzetto, 2012). Nevertheless, when it comes to aligning textual definitions in different resources, the lexical approach (Ruiz-Casado et al., 2005; de Melo and Weikum, 2010; Henrich et al., 2011) falls short because of the potential use of totally different wordings to define the same concept. Deeper approaches leverage semantic similarity to go beyond the surface realization of definitions (Navigli, 2006; Meyer and Gurevych, 2011; Niemann and Gurevych, 2011). While providing good results in general, these approaches fail when the definitions of a given word are not of adequate quality and expressiveness to</context>
<context position="20043" citStr="Navigli and Ponzetto, 2012" startWordPosition="3356" endWordPosition="3359">y) The seedbearing part of a plant...”). 4 Experiments Lexical resources. To enable a comparison with the state of the art, we followed Matuschek and Gurevych (2013) and performed an alignment of WordNet synsets (WN) to three different collaboratively-constructed resources: Wikipedia (WP), Wiktionary (WT), and OmegaWiki (OW). We utilized the DKPro software (Zesch et al., 2008; Gurevych et al., 2012) to access the information in the foregoing three resources. For WP, WT, OW we used the dump versions 20090822, 20131002, and 20131115, respectively. Evaluation measures. We followed previous work (Navigli and Ponzetto, 2012; Matuschek and Gurevych, 2013) and evaluated the alignment performance in terms of four measures: precision, recall, F1, and accuracy. Precision is the fraction of correct alignment judgments returned by the system and recall is the fraction of alignment judgments in the gold standard dataset that are correctly returned by the system. F1 is the harmonic mean of precision and recall. We also report results for accuracy which, in addition to true positives, takes into account true negatives, i.e., pairs which are correctly judged as unaligned. Lexicons and semantic graphs. Here, we describe how</context>
<context position="37134" citStr="Navigli and Ponzetto, 2012" startWordPosition="6090" endWordPosition="6093">trated on aligning the de facto community standard sense inventory, i.e. WordNet, to other resources. These include: the Roget’s thesaurus and Longman Dictionary of Contemporary English (Kwong, 1998), FrameNet (Laparra and Rigau, 2009), VerbNet (Shi and Mihalcea, 2005) or domain-specific terminologies such as the Unified Medical Language System (Burgun and Bodenreider, 2001). More recently, the growth of collaboratively-constructed resources has seen the development of alignment approaches with Wikipedia (Ruiz-Casado et al., 2005; Auer et al., 2007; Suchanek et al., 2008; Reiter et al., 2008; Navigli and Ponzetto, 2012), Wiktionary (Meyer and Gurevych, 2011) and OmegaWiki (Gurevych et al., 2012). Last year Matuschek and Gurevych (2013) proposed Dijkstra-WSA, a graph-based approach relying on shortest paths between two concepts when the two corresponding resources graphs were combined by leveraging monosemous linking. Their method when backed off with other definition similarity based approaches (Niemann and Gurevych, 2011; Meyer and Gurevych, 2011), achieved state-of-the-art results on the mapping of WordNet to different collaboratively-constructed resources. This approach, however, in addition to setting th</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2012. BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network. Artificial Intelligence, 193:217– 250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Stefano Faralli</author>
</authors>
<title>Aitor Soroa, Oier de Lacalle, and Eneko Agirre.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th ACM Conference on Information and Knowledge Management (CIKM),</booktitle>
<pages>2317--2320</pages>
<location>Glasgow, UK.</location>
<marker>Navigli, Faralli, 2011</marker>
<rawString>Roberto Navigli, Stefano Faralli, Aitor Soroa, Oier de Lacalle, and Eneko Agirre. 2011. Two birds with one stone: Learning semantic models for text categorization and Word Sense Disambiguation. In Proceedings of the 20th ACM Conference on Information and Knowledge Management (CIKM), pages 2317–2320, Glasgow, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Meaningful clustering of senses helps boost word sense disambiguation performance.</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics joint with the 21st International Conference on Computational Linguistics (COLING-ACL</booktitle>
<pages>105--112</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="2312" citStr="Navigli, 2006" startWordPosition="344" endWordPosition="346">ce alignment has proven beneficial to a wide spectrum of tasks, such as Semantic Parsing (Shi and Mihalcea, 2005), Semantic Role Labeling (Palmer et al., 2010), and Word Sense Disambiguation (Navigli and Ponzetto, 2012). Nevertheless, when it comes to aligning textual definitions in different resources, the lexical approach (Ruiz-Casado et al., 2005; de Melo and Weikum, 2010; Henrich et al., 2011) falls short because of the potential use of totally different wordings to define the same concept. Deeper approaches leverage semantic similarity to go beyond the surface realization of definitions (Navigli, 2006; Meyer and Gurevych, 2011; Niemann and Gurevych, 2011). While providing good results in general, these approaches fail when the definitions of a given word are not of adequate quality and expressiveness to be distinguishable from one another. When a lexical resource can be viewed as a semantic graph, as with WordNet or Wikipedia, this limit can be overcome by means of alignment algorithms that exploit the network structure to determine the similarity of concept pairs. However, not all lexical resources provide explicit semantic relations between concepts and, hence, machine-readable dictionar</context>
</contexts>
<marker>Navigli, 2006</marker>
<rawString>Roberto Navigli. 2006. Meaningful clustering of senses helps boost word sense disambiguation performance. In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics joint with the 21st International Conference on Computational Linguistics (COLING-ACL 2006), pages 105–112, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elisabeth Niemann</author>
<author>Iryna Gurevych</author>
</authors>
<title>The people’s web meets linguistic knowledge: Automatic sense alignment of Wikipedia and WordNet.</title>
<date>2011</date>
<booktitle>In Proceedings of the Ninth International Conference on Computational Semantics,</booktitle>
<pages>205--214</pages>
<location>Oxford, United Kingdom.</location>
<contexts>
<context position="2367" citStr="Niemann and Gurevych, 2011" startWordPosition="351" endWordPosition="354">ide spectrum of tasks, such as Semantic Parsing (Shi and Mihalcea, 2005), Semantic Role Labeling (Palmer et al., 2010), and Word Sense Disambiguation (Navigli and Ponzetto, 2012). Nevertheless, when it comes to aligning textual definitions in different resources, the lexical approach (Ruiz-Casado et al., 2005; de Melo and Weikum, 2010; Henrich et al., 2011) falls short because of the potential use of totally different wordings to define the same concept. Deeper approaches leverage semantic similarity to go beyond the surface realization of definitions (Navigli, 2006; Meyer and Gurevych, 2011; Niemann and Gurevych, 2011). While providing good results in general, these approaches fail when the definitions of a given word are not of adequate quality and expressiveness to be distinguishable from one another. When a lexical resource can be viewed as a semantic graph, as with WordNet or Wikipedia, this limit can be overcome by means of alignment algorithms that exploit the network structure to determine the similarity of concept pairs. However, not all lexical resources provide explicit semantic relations between concepts and, hence, machine-readable dictionaries like Wiktionary have first to be transformed into s</context>
<context position="8126" citStr="Niemann and Gurevych, 2011" startWordPosition="1322" endWordPosition="1325">proach consists of two main components: definitional similarity and structural similarity. Each of these components gets, as its input, a pair of concepts belonging to two different semantic networks and produces a similarity score. These two scores are then combined into an overall score (part (e) of Figure 1) which quantifies the semantic similarity of the two input concepts c1 and c2. The definitional similarity component computes the similarity of two concepts in terms of the similarity of their definitions, a method that has also been used in previous work for aligning lexical resources (Niemann and Gurevych, 2011; Henrich et al., 2012). In spite of its simplicity, the mere calculation of the similarity of concept definitions provides a strong baseline, especially for cases where the definitional texts for a pair of concepts to be aligned are lexically similar, yet distinguishable from the other definitions. However, as mentioned in the introduction, definition similarity-based techniques fail at identifying the correct alignments in cases where different wordings are used or definitions are not of high quality. The structural similarity component, instead, is a novel graph-based similarity measurement</context>
<context position="10419" citStr="Niemann and Gurevych, 2011" startWordPosition="1695" endWordPosition="1698">Personalized PageRank (Haveliwala, 2002, PPR), a random walk graph algorithm, for calculating semantic signatures. The original PageRank (PR) algorithm (Brin and Page, 1998) computes, for a given graph, a single vector wherein each node is associated with a weight denoting its structural importance in that graph. PPR is a variation of PR where the computation is biased towards a set of initial nodes in order to capture the notion of importance with respect to those particular nodes. PPR has been previously used in a wide variety of tasks such as definition similarity-based resource alignment (Niemann and Gurevych, 2011), textual semantic similarity (Hughes and Ramage, 2007; Pilehvar et al., 2013), Word Sense Disambiguation (Agirre and Soroa, 2009; Faralli and Navigli, 2012) and semantic text categorization (Navigli et al., 2011). When applied to a semantic graph by initializing the random walks from a set of concepts (nodes), PPR yields a vector in which each concept is associated with a weight denoting its semantic relevance to the initial concepts. Formally, we first represent a semantic network consisting of N concepts as a row-stochastic transition matrix M E RN×N. The cell (i, j) in the matrix denotes t</context>
<context position="11856" citStr="Niemann and Gurevych (2011)" startWordPosition="1947" endWordPosition="1950"> the linear system: Sv = (1 − α) v + α M Sv, where v is the personalization vector of size N in which all the probability mass is put on the concepts for which a semantic signature is to be computed and α is the damping factor, which is usually set to 0.85 (Brin and Page, 1998). We used the UKB1 off-the-shelf implementation of PPR. Definitional similarity signature. In the definitional similarity component, the two concepts c1 and c2 are first represented by their corresponding definitions d1 and d2 in the respective resources L1 and L2 (Figure 1(a), top). To improve expressiveness, we follow Niemann and Gurevych (2011) and further extend di with all the word forms associated with concept ci and its neighbours, i.e., the union of all lexicalizations LGi(x) for all concepts x E {c� E Vi : (c, c&apos;) E Ei} U {c}, where Ei is the set of edges in Gi. In this component the personalization vector vi is set by uniformly distributing the probability mass over the nodes corresponding to the senses of all the content words in the extended definition of di according to the sense inventory of a semantic network H. We use the same semantic graph H for computing the semantic signatures of both definitions. Any semantic netwo</context>
<context position="28631" citStr="Niemann and Gurevych, 2011" startWordPosition="4755" endWordPosition="4758">ir middle values (i.e., 0.5), hence, no tuning is performed for either of the parameters. In this case, both the definitional and structural similarity scores are treated as equally important and two concepts are aligned if their overall similarity exceeds the middle point of the similarity scale. • Tuning, where we follow Matuschek and Gurevych (2013) and tune the parameters on a subset of the dataset comprising 100 items. • Cross-validation, where a 5-fold cross validation is carried out to find the optimal values for the parameters, a technique used in most of the recent alignment methods (Niemann and Gurevych, 2011; Meyer and Gurevych, 2012a; Matuschek and Gurevych, 2013). 4.2.2 Results We show in Table 3 the alignment performance of different systems on the task of aligning WN-WP, WN-WT, and WN-OW in terms of Precision (P), Recall (R), F1, and Accuracy. The SB system corresponds to the state-of-the-art definition similarity approaches for WN-WP (Niemann and Gurevych, 2011), WN-WT (Meyer and Gurevych, 2011), and WN-OW (Gurevych et al., 2012). DWSA stands for Dijkstra-WSA, the state-of-the-art graph-based alignment approach of Matuschek and Gurevych (2013). The authors also provided results for SB+Dijkst</context>
<context position="32827" citStr="Niemann and Gurevych, 2011" startWordPosition="5420" endWordPosition="5423">ilability of training data. The system performance is generally higher on the alignment task for WP compared to WT and OW. We attribute this difference to the dictionary nature of the latter two, where sense distinctions are more fine-grained, as opposed to the relatively concrete concepts in the WP encyclopedia. 4.3 Similarity Measure Analysis We explained in Section 2.1 that our concept similarity measure consists of two components: the definitional and the structural similarities. Measuring the similarity of two concepts in terms of their definitions has been investigated in previous work (Niemann and Gurevych, 2011; Henrich et al., 2012). The structural similarity component of our approach, however, is novel, but at the same time one of the very few measures which enables the computation of the similarity of concepts across two resources directly and independently of the similarity of their definitions. A comparable approach is the Dijkstra-WSA proposed by Matuschek and Gurevych (2013) which, as also mentioned earlier in the Introduction, first connects the two resources’ graphs by leveraging monosemous linking and then aligns two concepts across the two graphs on the basis of their shortest distance. T</context>
<context position="37544" citStr="Niemann and Gurevych, 2011" startWordPosition="6147" endWordPosition="6150">laboratively-constructed resources has seen the development of alignment approaches with Wikipedia (Ruiz-Casado et al., 2005; Auer et al., 2007; Suchanek et al., 2008; Reiter et al., 2008; Navigli and Ponzetto, 2012), Wiktionary (Meyer and Gurevych, 2011) and OmegaWiki (Gurevych et al., 2012). Last year Matuschek and Gurevych (2013) proposed Dijkstra-WSA, a graph-based approach relying on shortest paths between two concepts when the two corresponding resources graphs were combined by leveraging monosemous linking. Their method when backed off with other definition similarity based approaches (Niemann and Gurevych, 2011; Meyer and Gurevych, 2011), achieved state-of-the-art results on the mapping of WordNet to different collaboratively-constructed resources. This approach, however, in addition to setting the threshold for the definition similarity component by means of cross validation, also required other parameters to be tuned, such as the allowed path length (λ) and the maximum number of edges in a graph. The optimal value for the λ parameter varied from one resource pair to another, and even for a specific resource pair it had to be tuned for each configuration. This made the approach dependent on the tra</context>
</contexts>
<marker>Niemann, Gurevych, 2011</marker>
<rawString>Elisabeth Niemann and Iryna Gurevych. 2011. The people’s web meets linguistic knowledge: Automatic sense alignment of Wikipedia and WordNet. In Proceedings of the Ninth International Conference on Computational Semantics, pages 205–214, Oxford, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Nianwen Xue</author>
</authors>
<title>Semantic Role Labeling. Synthesis Lectures on Human Language Technologies.</title>
<date>2010</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="1858" citStr="Palmer et al., 2010" startWordPosition="272" endWordPosition="275">et al., 2013). On the one hand, these resources are heterogeneous in design, structure and content, but, on the other hand, they often provide complementary knowledge which we would like to see integrated. Given the large scale this intrinsic issue can only be addressed automatically, by means of lexical resource alignment algorithms. Owing to its ability to bring together features like multilinguality and increasing coverage, over the past few years resource alignment has proven beneficial to a wide spectrum of tasks, such as Semantic Parsing (Shi and Mihalcea, 2005), Semantic Role Labeling (Palmer et al., 2010), and Word Sense Disambiguation (Navigli and Ponzetto, 2012). Nevertheless, when it comes to aligning textual definitions in different resources, the lexical approach (Ruiz-Casado et al., 2005; de Melo and Weikum, 2010; Henrich et al., 2011) falls short because of the potential use of totally different wordings to define the same concept. Deeper approaches leverage semantic similarity to go beyond the surface realization of definitions (Navigli, 2006; Meyer and Gurevych, 2011; Niemann and Gurevych, 2011). While providing good results in general, these approaches fail when the definitions of a </context>
</contexts>
<marker>Palmer, Gildea, Xue, 2010</marker>
<rawString>Martha Palmer, Daniel Gildea, and Nianwen Xue. 2010. Semantic Role Labeling. Synthesis Lectures on Human Language Technologies. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Automatically harvesting and ontologizing semantic relations.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Ontology Learning and Population: Bridging the Gap Between Text and Knowledge,</booktitle>
<pages>171--195</pages>
<location>Amsterdam, The Netherlands.</location>
<contexts>
<context position="17487" citStr="Pantel and Pennacchiotti, 2008" startWordPosition="2900" endWordPosition="2903">rming a given machine-readable dictionary into a semantic network, a process we refer to as ontologization. Our ontologization algorithm takes as input a lexicon L and outputs a semantic graph G = (V, E) where, as already defined in Section 2, V is the set of concepts in L and E is the set of semantic relations between these concepts. Introducing relational links into a lexicon can be achieved in different ways. A first option is to extract binary 471 relations between pairs of words from raw text. Both words in these relations, however, should be disambiguated according to the given lexicon (Pantel and Pennacchiotti, 2008), making the task particularly prone to mistakes due to the high number of possible sense pairings. Here, we take an alternative approach which requires disambiguation on the target side only, hence reducing the size of the search space significantly. We first create the empty undirected graph GL = (V, E) such that V is the set of concepts in L and E = ∅. For each source concept c E V we create a bag of content words W = {wi, ... , wn} which includes all the content words in its definition d and, if available, additional related words obtained from lexicon relations (e.g., synonyms in Wiktiona</context>
</contexts>
<marker>Pantel, Pennacchiotti, 2008</marker>
<rawString>Patrick Pantel and Marco Pennacchiotti. 2008. Automatically harvesting and ontologizing semantic relations. In Proceedings of the 2008 Conference on Ontology Learning and Population: Bridging the Gap Between Text and Knowledge, pages 171–195, Amsterdam, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad Taher Pilehvar</author>
<author>David Jurgens</author>
<author>Roberto Navigli</author>
</authors>
<title>Align, Disambiguate and Walk: a Unified Approach for Measuring Semantic Similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1341--1351</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="10497" citStr="Pilehvar et al., 2013" startWordPosition="1706" endWordPosition="1709">alculating semantic signatures. The original PageRank (PR) algorithm (Brin and Page, 1998) computes, for a given graph, a single vector wherein each node is associated with a weight denoting its structural importance in that graph. PPR is a variation of PR where the computation is biased towards a set of initial nodes in order to capture the notion of importance with respect to those particular nodes. PPR has been previously used in a wide variety of tasks such as definition similarity-based resource alignment (Niemann and Gurevych, 2011), textual semantic similarity (Hughes and Ramage, 2007; Pilehvar et al., 2013), Word Sense Disambiguation (Agirre and Soroa, 2009; Faralli and Navigli, 2012) and semantic text categorization (Navigli et al., 2011). When applied to a semantic graph by initializing the random walks from a set of concepts (nodes), PPR yields a vector in which each concept is associated with a weight denoting its semantic relevance to the initial concepts. Formally, we first represent a semantic network consisting of N concepts as a row-stochastic transition matrix M E RN×N. The cell (i, j) in the matrix denotes the probability of moving from a concept i to j in the graph: 0 if no edge exis</context>
<context position="15329" citStr="Pilehvar et al. (2013)" startWordPosition="2533" endWordPosition="2536"> the weight of the only concept associated with tradeoffn in the corresponding semantic signature. As a result of the unification process, we obtain a pair of equally-sized semantic signatures with comparable components. 2http://wordnet.princeton.edu 3For instance, we calculated that more than 80% of the words in WordNet are monosemous, with over 60% of all the synsets containing at least one of them. 2.1.3 Signature comparison Having at hand the semantic signatures for the two input concepts, we proceed to comparing them (part (d) in Figure 1). We leverage a nonparametric measure proposed by Pilehvar et al. (2013) which first transforms each signature into a list of sorted elements and then calculates the similarity on the basis of the average ranking of elements across the two lists: M|T |i=1(r1 i + r2 i )−1 Sim(Sv1, Sv2) = �|T |(1) i=1(2i)−1 where T is the intersection of all concepts with non-zero probability in the two signatures and rji is the rank of the ith entry in the jth sorted list. The denominator is a normalization factor to guarantee a maximum value of one. The method penalizes the differences in the higher rankings more than it does for the lower ones. The measure was shown to outperform</context>
</contexts>
<marker>Pilehvar, Jurgens, Navigli, 2013</marker>
<rawString>Mohammad Taher Pilehvar, David Jurgens, and Roberto Navigli. 2013. Align, Disambiguate and Walk: a Unified Approach for Measuring Semantic Similarity. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1341–1351, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nils Reiter</author>
<author>Matthias Hartung</author>
<author>Anette Frank</author>
</authors>
<title>A resource-poor approach for linking ontology classes to Wikipedia articles.</title>
<date>2008</date>
<booktitle>Semantics in Text Processing,</booktitle>
<volume>1</volume>
<pages>381--387</pages>
<editor>In Johan Bos and Rodolfo Delmonte, editors,</editor>
<publisher>College Publications,</publisher>
<location>London, England.</location>
<contexts>
<context position="6214" citStr="Reiter et al., 2008" startWordPosition="981" endWordPosition="984">hm 1 formalizes the alignment process: the algorithm takes as input the semantic graphs G1 and G2 corresponding to the two resources, as explained above, and produces as output an alignment in the form of a set A of concept pairs. The algorithm iterates over all concepts c1 E V1 and, for each of them, obtains the set of concepts C C V2, which can be considered as alignment candidates for c1 (line 3). For a concept c1, alignment candidates in G2 usually consist of every concept c2 E V2 that shares at least one lexicalization with c1 in the same part of speech tag, i.e., LG1(c1) n LG2(c2) =� 0 (Reiter et al., 2008; Meyer and Gurevych, 2011). Once the set of target candidates C for a source concept c1 is obtained, the alignment task can be cast as that of identifying those concepts in C to which c1 should be aligned. To do this, the algorithm calculates the similarity between c1 and each c2 E C (line 5). If their similarity score exceeds a certain value denoted by θ Algorithm 1 Lexical Resource Aligner Input: graphs H = (VH, EH), G1 = (V1, E1) and G2 = (V2, E2), the similarity threshold B, and the combination parameter β Output: A, the set of all aligned concept pairs 1: A +— 0 2: for each concept c1 E </context>
<context position="37105" citStr="Reiter et al., 2008" startWordPosition="6086" endWordPosition="6089">orts have been concentrated on aligning the de facto community standard sense inventory, i.e. WordNet, to other resources. These include: the Roget’s thesaurus and Longman Dictionary of Contemporary English (Kwong, 1998), FrameNet (Laparra and Rigau, 2009), VerbNet (Shi and Mihalcea, 2005) or domain-specific terminologies such as the Unified Medical Language System (Burgun and Bodenreider, 2001). More recently, the growth of collaboratively-constructed resources has seen the development of alignment approaches with Wikipedia (Ruiz-Casado et al., 2005; Auer et al., 2007; Suchanek et al., 2008; Reiter et al., 2008; Navigli and Ponzetto, 2012), Wiktionary (Meyer and Gurevych, 2011) and OmegaWiki (Gurevych et al., 2012). Last year Matuschek and Gurevych (2013) proposed Dijkstra-WSA, a graph-based approach relying on shortest paths between two concepts when the two corresponding resources graphs were combined by leveraging monosemous linking. Their method when backed off with other definition similarity based approaches (Niemann and Gurevych, 2011; Meyer and Gurevych, 2011), achieved state-of-the-art results on the mapping of WordNet to different collaboratively-constructed resources. This approach, howev</context>
</contexts>
<marker>Reiter, Hartung, Frank, 2008</marker>
<rawString>Nils Reiter, Matthias Hartung, and Anette Frank. 2008. A resource-poor approach for linking ontology classes to Wikipedia articles. In Johan Bos and Rodolfo Delmonte, editors, Semantics in Text Processing, volume 1 of Research in Computational Semantics, pages 381–387. College Publications, London, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Ruiz-Casado</author>
<author>Enrique Alfonseca</author>
<author>Pablo Castells</author>
</authors>
<title>Automatic assignment of Wikipedia encyclopedic entries to WordNet synsets.</title>
<date>2005</date>
<booktitle>In Proceedings of the Third International Conference on Advances in Web Intelligence,</booktitle>
<pages>380--386</pages>
<location>Lodz,</location>
<contexts>
<context position="2050" citStr="Ruiz-Casado et al., 2005" startWordPosition="300" endWordPosition="303">o see integrated. Given the large scale this intrinsic issue can only be addressed automatically, by means of lexical resource alignment algorithms. Owing to its ability to bring together features like multilinguality and increasing coverage, over the past few years resource alignment has proven beneficial to a wide spectrum of tasks, such as Semantic Parsing (Shi and Mihalcea, 2005), Semantic Role Labeling (Palmer et al., 2010), and Word Sense Disambiguation (Navigli and Ponzetto, 2012). Nevertheless, when it comes to aligning textual definitions in different resources, the lexical approach (Ruiz-Casado et al., 2005; de Melo and Weikum, 2010; Henrich et al., 2011) falls short because of the potential use of totally different wordings to define the same concept. Deeper approaches leverage semantic similarity to go beyond the surface realization of definitions (Navigli, 2006; Meyer and Gurevych, 2011; Niemann and Gurevych, 2011). While providing good results in general, these approaches fail when the definitions of a given word are not of adequate quality and expressiveness to be distinguishable from one another. When a lexical resource can be viewed as a semantic graph, as with WordNet or Wikipedia, this </context>
<context position="37042" citStr="Ruiz-Casado et al., 2005" startWordPosition="6074" endWordPosition="6077">tary information from other resources. As a matter of fact, most efforts have been concentrated on aligning the de facto community standard sense inventory, i.e. WordNet, to other resources. These include: the Roget’s thesaurus and Longman Dictionary of Contemporary English (Kwong, 1998), FrameNet (Laparra and Rigau, 2009), VerbNet (Shi and Mihalcea, 2005) or domain-specific terminologies such as the Unified Medical Language System (Burgun and Bodenreider, 2001). More recently, the growth of collaboratively-constructed resources has seen the development of alignment approaches with Wikipedia (Ruiz-Casado et al., 2005; Auer et al., 2007; Suchanek et al., 2008; Reiter et al., 2008; Navigli and Ponzetto, 2012), Wiktionary (Meyer and Gurevych, 2011) and OmegaWiki (Gurevych et al., 2012). Last year Matuschek and Gurevych (2013) proposed Dijkstra-WSA, a graph-based approach relying on shortest paths between two concepts when the two corresponding resources graphs were combined by leveraging monosemous linking. Their method when backed off with other definition similarity based approaches (Niemann and Gurevych, 2011; Meyer and Gurevych, 2011), achieved state-of-the-art results on the mapping of WordNet to differ</context>
</contexts>
<marker>Ruiz-Casado, Alfonseca, Castells, 2005</marker>
<rawString>Maria Ruiz-Casado, Enrique Alfonseca, and Pablo Castells. 2005. Automatic assignment of Wikipedia encyclopedic entries to WordNet synsets. In Proceedings of the Third International Conference on Advances in Web Intelligence, pages 380–386, Lodz, Poland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Shi</author>
<author>Rada Mihalcea</author>
</authors>
<title>Putting pieces together: Combining FrameNet, VerbNet and WordNet for robust semantic parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 6th International Conference on Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>100--111</pages>
<location>Mexico City, Mexico.</location>
<contexts>
<context position="1812" citStr="Shi and Mihalcea, 2005" startWordPosition="264" endWordPosition="267">resources such as OmegaWiki and Wiktionary (Hovy et al., 2013). On the one hand, these resources are heterogeneous in design, structure and content, but, on the other hand, they often provide complementary knowledge which we would like to see integrated. Given the large scale this intrinsic issue can only be addressed automatically, by means of lexical resource alignment algorithms. Owing to its ability to bring together features like multilinguality and increasing coverage, over the past few years resource alignment has proven beneficial to a wide spectrum of tasks, such as Semantic Parsing (Shi and Mihalcea, 2005), Semantic Role Labeling (Palmer et al., 2010), and Word Sense Disambiguation (Navigli and Ponzetto, 2012). Nevertheless, when it comes to aligning textual definitions in different resources, the lexical approach (Ruiz-Casado et al., 2005; de Melo and Weikum, 2010; Henrich et al., 2011) falls short because of the potential use of totally different wordings to define the same concept. Deeper approaches leverage semantic similarity to go beyond the surface realization of definitions (Navigli, 2006; Meyer and Gurevych, 2011; Niemann and Gurevych, 2011). While providing good results in general, th</context>
<context position="36776" citStr="Shi and Mihalcea, 2005" startWordPosition="6038" endWordPosition="6041">oviding a denser graph with most of its nodes connected. Resource alignment. Aligning lexical resources has been a very active field of research in the last decade. One of the main objectives in this area has been to enrich existing ontologies by means of complementary information from other resources. As a matter of fact, most efforts have been concentrated on aligning the de facto community standard sense inventory, i.e. WordNet, to other resources. These include: the Roget’s thesaurus and Longman Dictionary of Contemporary English (Kwong, 1998), FrameNet (Laparra and Rigau, 2009), VerbNet (Shi and Mihalcea, 2005) or domain-specific terminologies such as the Unified Medical Language System (Burgun and Bodenreider, 2001). More recently, the growth of collaboratively-constructed resources has seen the development of alignment approaches with Wikipedia (Ruiz-Casado et al., 2005; Auer et al., 2007; Suchanek et al., 2008; Reiter et al., 2008; Navigli and Ponzetto, 2012), Wiktionary (Meyer and Gurevych, 2011) and OmegaWiki (Gurevych et al., 2012). Last year Matuschek and Gurevych (2013) proposed Dijkstra-WSA, a graph-based approach relying on shortest paths between two concepts when the two corresponding res</context>
</contexts>
<marker>Shi, Mihalcea, 2005</marker>
<rawString>Lei Shi and Rada Mihalcea. 2005. Putting pieces together: Combining FrameNet, VerbNet and WordNet for robust semantic parsing. In Proceedings of the 6th International Conference on Computational Linguistics and Intelligent Text Processing, pages 100–111, Mexico City, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: A large ontology from Wikipedia and WordNet.</title>
<date>2008</date>
<journal>Journal of Web Semantics,</journal>
<volume>6</volume>
<issue>3</issue>
<contexts>
<context position="35222" citStr="Suchanek et al., 2008" startWordPosition="5801" endWordPosition="5804">stra-WSA on this resource pair shows the effectiveness of our novel concept similarity measure independently of the underlying semantic network. 5 Related Work Resource ontologization. Having lexical resources represented as semantic networks is highly beneficial. A good example is WordNet, which has been exploited as a semantic network in dozens of NLP tasks (Fellbaum, 1998). A recent prominent case is Wikipedia (Medelyan et al., 2009; Hovy et al., 2013) which, thanks to its inter-article hyperlink structure, provides a rich backbone for structuring additional information (Auer et al., 2007; Suchanek et al., 2008; Moro and Navigli, 2013; Flati et al., 2014). However, there are many large-scale resources, such as Wiktionary for instance, which by their very nature are not in the form of a graph. This is 475 usually the case with machine-readable dictionaries, where structuring the resource involves the arduous task of connecting lexicographic senses by means of semantic relations. Surprisingly, despite their vast potential, little research has been conducted on the automatic ontologization of collaboratively-constructed dictionaries like Wiktionary and OmegaWiki. Meyer and Gurevych (2012a) and Matusche</context>
<context position="37084" citStr="Suchanek et al., 2008" startWordPosition="6082" endWordPosition="6085">atter of fact, most efforts have been concentrated on aligning the de facto community standard sense inventory, i.e. WordNet, to other resources. These include: the Roget’s thesaurus and Longman Dictionary of Contemporary English (Kwong, 1998), FrameNet (Laparra and Rigau, 2009), VerbNet (Shi and Mihalcea, 2005) or domain-specific terminologies such as the Unified Medical Language System (Burgun and Bodenreider, 2001). More recently, the growth of collaboratively-constructed resources has seen the development of alignment approaches with Wikipedia (Ruiz-Casado et al., 2005; Auer et al., 2007; Suchanek et al., 2008; Reiter et al., 2008; Navigli and Ponzetto, 2012), Wiktionary (Meyer and Gurevych, 2011) and OmegaWiki (Gurevych et al., 2012). Last year Matuschek and Gurevych (2013) proposed Dijkstra-WSA, a graph-based approach relying on shortest paths between two concepts when the two corresponding resources graphs were combined by leveraging monosemous linking. Their method when backed off with other definition similarity based approaches (Niemann and Gurevych, 2011; Meyer and Gurevych, 2011), achieved state-of-the-art results on the mapping of WordNet to different collaboratively-constructed resources.</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2008</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2008. Yago: A large ontology from Wikipedia and WordNet. Journal of Web Semantics, 6(3):203–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
<author>Christof M¨uller</author>
<author>Iryna Gurevych</author>
</authors>
<title>Using Wiktionary for computing semantic relatedness.</title>
<date>2008</date>
<booktitle>In Proceedings of the 23rd national conference on Artificial intelligence -</booktitle>
<volume>2</volume>
<pages>861--866</pages>
<location>Chicago, Illinois.</location>
<marker>Zesch, M¨uller, Gurevych, 2008</marker>
<rawString>Torsten Zesch, Christof M¨uller, and Iryna Gurevych. 2008. Using Wiktionary for computing semantic relatedness. In Proceedings of the 23rd national conference on Artificial intelligence - Volume 2, pages 861–866, Chicago, Illinois.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>