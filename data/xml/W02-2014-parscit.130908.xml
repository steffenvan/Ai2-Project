<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000068">
<title confidence="0.995691">
On the Robustness of Entropy-Based Similarity Measures in
Evaluation of Subcategorization Acquisition Systems
</title>
<author confidence="0.998759">
Anna Korhonen
</author>
<affiliation confidence="0.9768805">
University of Cambridge
Computer Laboratory
</affiliation>
<address confidence="0.9909305">
15 JJ Thomson Avenue
Cambridge CB3 OFD, UK
</address>
<email confidence="0.998464">
Anna.Korhonen@cl.cam.ac.uk
</email>
<author confidence="0.99443">
Yuval Krymolowski
</author>
<affiliation confidence="0.9990755">
Bar-Ilan University
Department of Computer Science
</affiliation>
<address confidence="0.888467">
Ramat Gan 52900, Israel
</address>
<email confidence="0.998949">
yuvalk@cs.biu.ac.il
</email>
<sectionHeader confidence="0.980154" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999426066666667">
Some statistical learning systems are evaluated
using measures of distributional similarity. To
deal with the problem of zero events in the dis-
tributions under comparison, smoothing is fre-
quently performed before similarity measures
are applied. Smoothing alters the information
in the original distribution, and may add noise
to the results. Here, we investigate the sensi-
tivity of entropy-based similarity measures to
noise from uninformative smoothing. Our ex-
periments with two subcategorization acquisi-
tion systems show that similarity measures vary
in their robustness. While some are led astray
by noise from smoothing, others are more re-
silient.
</bodyText>
<sectionHeader confidence="0.996306" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.991246894117647">
Many natural language processing (NLP) tasks
involve measuring distributional similarity.
Some examples are the estimation of word co-
occurrence probabilities (Dagan et al., 1999),
automatic construction of thesauri (Lin, 1998),
automatic detection of diathesis alternations
(McCarthy, 2000), disambiguation of nominal-
izations (Lapata, 2002), and evaluation of sta-
tistical NLP learners (Carroll and Rooth, 1998;
Korhonen, 2002b).
Various similarity measures have been pro-
posed and used for NLP purposes, including the
Kullback-Leibler distance (Cover and Thomas,
1991), cross entropy (Cover and Thomas, 1991),
the Jensen-Shannon divergence (Lin, 1991), the
skew divergence (Lee, 1999), cosine (Frakes and
Baeza-Yates, 1992), Jaccard&apos;s coefficient, Ll
norm, and the confusion probability.
In this paper, we discuss the use of similarity
measures in evaluation of the type of statistical
language learners which deliver as an output a
distribution of events. A typical example is an
automatic subcategorization acquisition system
(e.g. (Briscoe and Carroll, 1997; Carroll and
Rooth, 1998; Korhonen, 2002b)), which learns,
from corpus data, a distribution of subcatego-
rization frames (scFs) specific to a certain pred-
icate (p(scf,lpredicate3)). These learners are
frequently evaluated using the standard preci-
sion, recall and accuracy measures (Manning
and Schiitze, 1999). However, similarity mea-
sures provide an important means to evaluate
the actual acquired frequencies.
In similarity-based evaluation, a learned dis-
tribution is compared with a gold standard dis-
tribution in order to determine how closely the
two correlate. Such evaluation is complicated
by cases where the distributions under compar-
ison have different supports, i.e. regions of pos-
itive probability. Due to the sparse data prob-
lem, zero events typically make up a substantial
portion of joint data.
To allow the comparison of all events,
smoothing is frequently performed before simi-
larity measures are applied. Smoothing tackles
the problem by assigning non-zero values to zero
events. It is usually done in an uninformative
way (by assigning a uniform prior probababil-
ity to events; e.g. (Laplace, 1814; Witten and
Bell, 1991)), rather than in an informative way
(by assigning an informative prior probability
e.g. by backing-off (Katz, 1987)), as it is desir-
able — from the evaluation point of view — to
preserve as much of the original, learned distri-
bution as possible&apos;.
Although uninformative smoothing makes
the simplest possible assumption regarding the
probability of unseen events, it involves also al-
tering the information included in the original
1-See Manning and Schiitze (1999) for both informa-
tive and uninformative smoothing methods.
distribution. It can add &apos;noise&apos; to the origi-
nal data. This noise may, in turn, affect dis-
tributional similarity and potentially obscure
similarity-based evaluation.
Here, we investigate the sensitivity of widely-
used entropy-based similarity measures to the
noise from uninformative smoothing. We do
this in the context of evaluation, by using the
measures to evaluate the accuracy of verbal SCF
distributions learned automatically from cor-
pus data. By controlling the number of scFs
smoothed and examining the effect on distribu-
tional similarity, we observe differences in the
robustness of various measures. Our results
show that some entropy-based similarity mea-
sures are led astray by noise from smoothing,
while others are more resilient and thus better
suited for evaluation purposes.
Section 2 introduces the subcategorization
learners employed. The similarity measures are
described in section 3 and the smoothing meth-
ods in section 4. Section 5 reports our experi-
ments. We discuss our findings in section 6 and
present our conclusions in section 7.
</bodyText>
<sectionHeader confidence="0.904518" genericHeader="introduction">
2 Subcategorization Learners
</sectionHeader>
<bodyText confidence="0.973694465753424">
We used two subcategorization learners pro-
posed by Korhonen (2002b) to obtain the SCF
distributions employed in our experiments. The
learners are variations of the subcategoriza-
tion acquisition system of Briscoe and Carroll
(1997). The system uses a shallow parser to
obtain the subcategorization information from
corpus data. It distinguishes 163 verbal SCFS
and returns relative frequencies for each SCF
found for a given verb. The resulting puta-
tive SCF distributions are processed by the two
learners as follows:
Learner 1: The scFs in putative distributions
are simply ranked in the order of the prob-
ability of their occurrence with the verb.
The probabilities are estimated using a
maximum likelihood estimate (mLE) from
the observed relative frequencies.
Learner 2: The SCF distributions obtained us-
ing Learner 1 are smoothed using lin-
ear interpolation (Chen and Goodman,
1996). The informative back-off distribu-
tion employed in smoothing is based on
the semantic class of the verb in ques-
tion (p(scLisemanticclassi)), chosen ac-
cording to the verb&apos;s most frequent sense
in WordNet (Miller, 1990). For instance,
the predominant sense of the verb fly in
WordNet belongs to the semantic class of
&amp;quot;Motion&amp;quot; verbs - hence, the distribution of
&amp;quot;Motion&amp;quot; verbs is employed as a back-off
distribution in smoothing.
The semantic classes are based on Levin
classes (Levin, 1993) and the back-off dis-
tribution for each class is obtained by merg-
ing SCF distributions of a few verbs in
the same class. The parameters used in
smoothing are obtained by optimising SCF
acquisition performance on held-out train-
ing data so that most of the smoothed
probability is determined by the MLE from
the subcategorization acquisition system2.
Learner 2 tends to perform better than
learner 1, since it involves using a priori knowl-
edge about generalizations of verb semantics to
guide subcategorization acquisition. It corrects
the putative SCF distribution and deals better
with sparse data.
Korhonen (2002a) used an empirically deter-
mined threshold on the probability estimates to
filter noisy SCFS out, and then evaluated the two
learners on a test set of 91 verbs using precision
and recall -based evaluation. The gold stan-
dard employed in the evaluation was obtained
by manually analysing an average of 300 occur-
rences of each test verb in corpus data.
Learner 1 yielded 82% type precision (the per-
centage of SCF types that the method proposes
which are correct) and 49% type recall (the
percentage of SCF types in the gold standard
that the method proposes), while type precision
was 81% and type recall 73% for learner 2. F-
measure3 was thus 61 for learner 1 and 76 for
learner 2.
While this evaluation allows us to evaluate
the set of acquired SCF types, similarity-based
evaluation is needed to evaluate the frequen-
2h is important to note that we regard the infor-
mative smoothing employed by learner 2 as part of the
method for SCF acquisition. When we discuss the effect
of smoothing on similarity measures, we essentially mean
the uninformative smoothing performed on the output of
learner 2 for evaluation purposes, not the informative
</bodyText>
<footnote confidence="0.597300333333333">
smoothing performed for SCF acquisition.
3F _ 2.precision•recall
precision+recall
</footnote>
<bodyText confidence="0.9997895">
cies associated with the types. For example, a
learner may correctly acquire a sentential com-
plement frame for believe (I believe that our the-
ory is correct) but incorrectly assign this fre-
quently occurring frame a very small probabil-
ity.
</bodyText>
<sectionHeader confidence="0.980844" genericHeader="method">
3 Similarity Measures
</sectionHeader>
<bodyText confidence="0.999910166666667">
We used the following set of similarity measures
to evaluate the accuracy of a learned SCF dis-
tribution q = {q,} with respect to a gold stan-
dard distribution p = fp, 1. q, and p, denote
the probability of scf, in the two distributions,
respectively.
</bodyText>
<listItem confidence="0.94507">
1. IS: The intersection measure (Lin, 1998)
</listItem>
<equation confidence="0.989389333333333">
2 I com(p, q) I ,
is(p, q) =
IsuPP(P)I + IsuPP(q) I
</equation>
<bodyText confidence="0.999362">
where supp(p) and supp(q) are the sets of
scFs with non-zero probability in p and q,
and com(p, q) is the intersection of these
two sets.
</bodyText>
<listItem confidence="0.866540333333333">
2. RC: The Spearman rank correlation coef-
ficient (Spearman, 1904). It involves (i)
calculating the ranks rP and rq for each
of the SCF variables separately, using aver-
aged ranks for tied values, and (ii) finding
RC by calculating the Pearson correlation
coefficient for the ranks:
RC (p, q) = con (rP , rq).
RC lies in the range [-1, 1], with values near
0 denoting a low degree of association and
values near -1 and 1 denoting strong asso-
ciation.
3. CE: Cross entropy — a measure of the in-
formation needed to describe a true distri-
bution p using a model distribution q:
</listItem>
<equation confidence="0.712981">
cE(p,q) = E —p, log(q,).
</equation>
<bodyText confidence="0.886848666666667">
CE is minimal when p and q are identical.
In this case cE(p, q) = H(p) is the Shannon
entropy of p.
</bodyText>
<listItem confidence="0.990236777777778">
4. KL: Kullback-Leibler distance — a measure
of the additional information needed to de-
scribe p using q:
D (PIlq) = CE(P q) H (P) = E Pi log (7) •
KL is always &gt; 0 and = 0 only when p q.
5. JS: The Jensen-Shannon divergence — a
measure which relies on the assumption
that if p and q are similar, they are close to
their average.
</listItem>
<equation confidence="0.8861285">
1
Js(p,q) = [D(pl
</equation>
<listItem confidence="0.981458">
6. SD: The skew divergence. It smooths q by
mixing it with p:
</listItem>
<bodyText confidence="0.8835127">
sp(p, q) = D(plla • q + (1 — • p).
sD(p, q) approximates KL as a —&gt; 1. Lee
(1999) reports the best results with a =
0.99. We adopted the same value.
All these other measures, except IS and RC
(which are included in our experiment primarily
for comparison), are entropy-based measures of
distributional similarity. CE and KL are asym-
metric and, unlike JS and SD, undefined if there
exists a SCF for which p, &gt; 0 but q, = 0.
</bodyText>
<sectionHeader confidence="0.994443" genericHeader="method">
4 Smoothing
</sectionHeader>
<bodyText confidence="0.999918285714286">
Two different uninformative smoothing meth-
ods were selected for investigation: the add-one
and Witten-Bell (Witten and Bell, 1991) meth-
ods. They both work by distributing a certain
probability mass among unseen events and dis-
counting the observed distribution accordingly,
but differ in the way they estimate the discount.
</bodyText>
<subsectionHeader confidence="0.979122">
4.1 Add-One
</subsectionHeader>
<bodyText confidence="0.999795285714286">
Add-one smoothing involves assigning a uni-
form prior probability to all events so that
q, &gt; 0 for all i. Let c(scf,) be the frequency
of a SCF (given a verb) in q, N the total num-
ber of SCF tokens for this verb in q, and n„f
the total number of SCF types4 considered. The
estimated probability of the SCF is:
</bodyText>
<footnote confidence="0.863568666666667">
4While &apos;types&apos; are the set of SCFS assumed e.g. in
a dictionary, &apos;tokens&apos; are the individual occurrences of
SCFS e.g. in corpus data.
</footnote>
<equation confidence="0.671269666666667">
P + q\
) D(q1
2
P(scf= c(scf,) ± 1
,)
N n„f
</equation>
<subsectionHeader confidence="0.662271">
4.2 Witten-Bell
</subsectionHeader>
<bodyText confidence="0.998867375">
Witten and Bell (1991) present a set of smooth-
ing methods which involve estimating the dis-
count from the observed distribution. We adopt
their method &amp;quot;C&amp;quot;, the so-called Witten-Bell
method. It considers each unseen SCF type as
an event in addition to the N seen SCF tokens.
Accordingly, the probability of an unseen SCF is
estimated by
</bodyText>
<equation confidence="0.939009333333333">
nscf
Pwb - •
N nscf
</equation>
<sectionHeader confidence="0.990947" genericHeader="method">
5 Experiment
</sectionHeader>
<subsectionHeader confidence="0.999868">
5.1 Test Data and Methods
</subsectionHeader>
<bodyText confidence="0.989965615384615">
We selected 17 test verbs for our experiments.
Sentences containing an occurrence of one of
these verbs were first extracted from 20 mil-
lion words of the British National Corpus (BNc)
(Leech, 1992), an average of 1000 citations of
each, and then processed using the two subcat-
egorization learners. This yielded two SCF dis-
tributions per test verb.
The similarity measures introduced in the
previous section were then applied to evaluate
the accuracy of these acquired distributions (q)
against gold standard distributions (p). The lat-
ter were obtained by manually analysing an av-
erage of 300 occurrences of each test verb in the
BNC.
Prior to calculating the similarity, we
smoothed the distributions under comparison
using the two uninformative methods intro-
duced in section 4. To investigate in detail the
effect of smoothing, we varied the number of
scFs considered, i.e. the number of scFs which
enter into the similarity measurement. This al-
lowed us to control the number of scFs which
require smoothing. Three options were explored
which involved considering only those scFs, for
which
</bodyText>
<construct confidence="0.580446666666667">
Option 1: p, &gt; 0 and q, &gt; 0,
Option 2: p, &gt; 0,
Option 3: either p,&gt; 0 or q, &gt; 0.
</construct>
<bodyText confidence="0.9996566">
Option 1 involves considering the smallest
number of scFs - only those common for p,
and q,- and never requires smoothing. Options
2 and 3 involve smoothing the gold standard
scFs absent in q,. Option 3 involves, in addi-
tion, smoothing the scFs which occur in q, but
are absent in A.
Option 2 is perhaps the most conventional
one when similarity measures are used in evalu-
ation. It involves evaluating the gold standard
events only. Option 3 takes into account the
false positive events in q, as well. It involves
assigning them a very small probability in p„
accounting for the fact that these events are ex-
tremely unlikely to appear in the gold standard.
</bodyText>
<sectionHeader confidence="0.819174" genericHeader="method">
5.2 Results
</sectionHeader>
<table confidence="0.999786692307692">
Add-One W-B
Li L2 Li L2
IS
1.00 1.00 1.00 1.00
0.87 0.96 0.87 0.96
0.36 0.49 0.36 0.49
RC
0.51 0.78 0.51 0.78
0.48 0.72 0.49 0.72
0.31 0.68 0.31 0.68
CE
2.01 1.81 2.01 1.81
3.24 2.23 2.38 1.99
3.42 2.30 2.57 2.21
KL
0.56 0.21 0.56 0.21
1.56 0.56 0.70 0.31
1.74 0.62 0.64 0.31
JS
0.11 0.05 0.11 0.05
0.14 0.06 0.13 0.06
0.20 0.08 0.13 0.07
SD
0.53 0.20 0.53 0.20
0.90 0.33 0.66 0.29
1.08 0.40 0.61 0.30
</table>
<tableCaption confidence="0.88787375">
Table 1: Results for the two learners, with the
different similarity measures and options 1-3.
Results are reported separately for add-one and
Witten-Bell smoothing methods.
</tableCaption>
<bodyText confidence="0.992640436363637">
Table 1 shows the average results for the 17
verbs with each similarity measure and smooth-
ing option (the options 1-3) combination. Ac-
cording to all results reported, learner 2 (L2) is
more accurate than learner 1 (L1). The results
with Is with option 2 indicate that learner 2 is
good in detecting scFs, finding 92% of the gold
standard scFs, while learner 1 finds only 77% of
the scF55. Thus the number of scFs smoothed
is always higher for learner 1 than for learner 2,
and therefore we expect the effect of smoothing
to be always stronger for learner 1.
With add-one smoothing, all the entropy-
based similarity measures (CE, KL, JS, and SD)
show worse results when the number of scFs
smoothed increases. Thus option 1 yields the
best results and option 3 the worst. The ef-
fect of smoothing is indeed always stronger for
learner 1 than learner 2. Interestingly, it also
varies largely from one entropy-based measure
to another.
KL and CE prove the measures most sensitive
to add-one smoothing. When we consider the
results for learner 1 and observe the decline in
results from option 1 to option 3, KL worsens
by a factor of 3.1. CE proves nearly as sensitive.
SD worsens by a factor of 2 and JS by a factor
of 1.8. From the entropy-based measures, JS is
thus the one most resistant to the effect of add-
one smoothing. It shows results consistent with
those obtained using RC. Similar observations
regarding the robustness of the measures can be
made with the more accurate learner 2.
With Witten-Bell method, option 1 yields the
best results as well. The results for option 2 are
not, however, considerably better than those for
option 3. In fact with learner 1, one measure —
KL — shows the best results with option 3.
This happens because Witten-Bell smoothing
tends to assign a higher probability to unseen
scFs than add-one smoothing. Whenever an un-
seen SCF has a high probability in the distribu-
tion where it is seen, Witten-Bell method makes
a better guess. This affects the distributional
similarity favourably. The converse happens
whenever an unseen SCF has a low probability.
Although most unseen scFs have a low probabil-
ity, the few high probability scFs have more ef-
fect, as entropy-based measures give them more
weight. Hence the small differences in results
between options 1-3.
Whether or not this indicates that Witten-
Bell method is more suitable for our task than
add-one method is difficult to judge. However,
the results do show that KL is the measure most
</bodyText>
<footnote confidence="0.967694">
5With option 2, the fraction of detected frames is
2-is •
</footnote>
<bodyText confidence="0.976970761904762">
effected by Witten-Bell smoothing.
Overall, the more sensitive measures (KT, and
CE) behave similarly with the more robust mea-
sures (SD and JS) when option 1 is used. This
seems to suggest that from all options, option 1
is the most suitable for these measures when a
learned distribution is sparse and a high num-
ber of scFs require smoothing (this is frequently
the case with learner 1). Although considering
only the scFs for which p &gt; 0 and q, &gt; 0 means
ignoring a number of events in A, it avoids the
noise from smoothing and yields more reliable
results.
Whether or not the latter unconventional use
of similarity measures is generally applicable is a
matter which requires further investigation. We
conducted our experiments by comparing distri-
butions that share, on average, around half of
their scFs. Further research is required to inves-
tigate the effect of smoothing on distributions
that share fewer of their scFs.
</bodyText>
<sectionHeader confidence="0.996578" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999777629032258">
In our experiment, entropy-based similarity
measures which require q, &gt; 0 (KT, and CE)
proved more sensitive to the noise from unin-
formative smoothing, while those which do not
require q, &gt; 0 (SD and JS) proved more robust.
Interestingly, Carroll and Rooth (1998) made
similar observations when using CE in evalua-
tion of their subcategorization learner. They
noted that uninformative smoothing (using the
Poisson model (Witten and Bell, 1991)) intro-
duces a high penalty on CE. They did not inves-
tigate other similarity measures or smoothing
methods.
Lee (1999) who compared the performance of
a variety of similarity measures on a pair co-
occurrence task, also reported best results with
measures which concentrate effort on events
for which both probability estimates are non-
zero. She only considered two entropy-based
measures — JS and SD — from which SD proved
more accurate. Lee (2001) reported better re-
sults with SD than KL, even when highly so-
phisticated (informative) methods were used for
smoothing.
We restricted our investigation to entropy-
based similarity measures. In the future, it
would be interesting to examine the effect
of smoothing on commonly used non-entropy
based similarity measures as well. For example,
L1 (Manhattan) norm and Jaccard&apos;s coefficient
seem promising candidates on the basis of Lee&apos;s
(1999) evaluation.
In the future, we also plan to investigate other
uninformative smoothing methods (e.g. Pois-
son and Good-Turing (Good, 1953)), and carry
out experiments with a wider range of learners
with varying degree of accuracy. Our experi-
ments with the two learners substantially dif-
ferent in their accuracy were inadequate to es-
tablish whether the noise from smoothing can
actually obscure inter-system comparison.
In this paper, we have examined the use of
similarity measures in evaluation of (particu-
lar type of) language learners. The results re-
ported suggest that it is best to either eval-
uate these language learners using similarity
measures known to be resistant to noise from
smoothing, or possibly employ measures such
as KL and CE in an unconventional way, with-
out smoothing: by considering only the non-
zero gold standard events in learned distribu-
tions.
The SCF distributions we experimented with
are typical zipf like ones, which we encounter
frequently in natural language. Also, the topic
we have investigated is not specific to evalua-
tion: similarity measures are frequently applied
to smoothed estimates in other domains as well.
Therefore, our observations are likely to be of an
interest to the range of NLP tasks which — one
way or the other — involve measuring distribu-
tional similarity.
</bodyText>
<sectionHeader confidence="0.996947" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999886647058823">
Uninformative smoothing is frequently per-
formed before the accuracy of automatically ac-
quired SCF distributions is evaluated using mea-
sures of distributional similarity. Smoothing al-
lows the comparison of unseen events but adds
noise to the original data. In this paper, we in-
vestigated the effect of uninformative smooth-
ing on entropy-based similarity measures. We
applied these measures to evaluate the accuracy
of two subcategorization learners, and studied
the effect of smoothing by controlling the num-
ber of scFs considered. We observed varia-
tion in the robustness of the different measures.
Some measures proved highly sensitive to the
noise from smoothing, while others proved more
robust and thus preferable for evaluation pur-
poses.
</bodyText>
<sectionHeader confidence="0.986667" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9660266">
We thank Ido Dagan and Diana McCarthy for
useful comments on this paper. This work
was partly supported by UK EPSRC project
GR/N36462/93: &apos;Robust Accurate Statistical
Parsing (RASP)&apos;.
</bodyText>
<sectionHeader confidence="0.995197" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995366166666667">
E. J. Briscoe and J. Carroll. 1997. Automatic
extraction of subcategorization from corpora.
In Proceedings of the 5th ACL Conference on
Applied Natural Language Processing, pages
356-363, Washington DC.
Glenn Carroll and Mats Rooth. 1998. Valence
induction with a head-lexicalized PCFG. In
Proceedings of the 3rd Conference on Empir-
ical Methods in Natural Language Processing,
Granada, Spain.
Stanley F. Chen and Joshua Goodman. 1996.
An empirical study of smoothing techniques
for language modeling. In Proceedings of the
ACL-96, pages 310-318, Santa Cruz, CA.
Thomas M. Cover and Joy A. Thomas. 1991.
Elements of information theory. Wiley, New
York.
Ido Dagan, Lillian Lee, and Fernando Pereira.
1999. Similarity-based models of cooccur-
rence probabilities. Machine Learning, 34(1-
3):43-69.
William B. Frakes and Ricardo Baeza-Yates.
1992. Information Retrieval: Data Structures
and Algorithms. Prentice Hall, Englewood
Cliffs, NJ.
I. J. Good. 1953. The population frequencies
of species and the estimation of population
parameters. Biometrika, 40:16-264.
Slava M. Katz. 1987. Estimation of probabili-
ties from sparse data for the language model
component of a speech recogniser. IEEE
Transactions on Acoustics, Speech, and Sig-
nal Processing, 35(3):400-401.
Anna Korhonen. 2002a. Semantically moti-
vated subcategorization acquisition. In Pro-
ceedings of the ACL Workshop on Unsuper-
vised Lexical Acquisition. To appear.
Anna Korhonen. 2002b. Subcategorization Ac-
quisition. Ph.D. thesis, University of Cam-
bridge, UK.
Maria Lapata. 2002. The disambiguation of
nominalisations. Computational Linguistics,
28(3).
P. S. Laplace. 1814. Essai philosophique stir les
pro babilites. Mme. Ve. Courcier.
Lillian Lee. 1999. Measures of distributional
similarity. In Proceedings of the 37th Annual
Meeting of the ACL, pages 25-32.
Lillian Lee. 2001. On the effectiveness of the
skew divergence for statistical language anal-
ysis. In Artificial Intelligence and Statistics
2001, pages 65-72.
Geoff Leech. 1992. 100 million words of En-
glish: the British National Corpus. Language
Research, 28(1):1-13.
Beth Levin. 1993. English Verb Classes
and Alternations. Chicago University Press,
Chicago.
Jianhua Lin. 1991. Divergence measures based
on the Shannon entropy. IEEE Transactions
on Information Theory, 37(1):145-151.
Dekang Lin. 1998. Automatic retrieval and
clustering of similar words. In Proceedings of
the COLING-ACL&apos;98, pages 768-773, Mon-
treal, Canada.
Christopher D. Manning and Hinrich Schiitze.
1999. Foundations of Statistical Natural Lan-
guage Processing. MIT Press, Cambridge,
MA.
Diana McCarthy. 2000. Using semantic prefer-
ences to identify verbal participation in role
switching alternations. In Proceedings of the
NAACL, pages 162-169, Seattle, WA.
George A. Miller. 1990. WordNet: An on-
line lexical database. International Journal
of Lexicography, 3(4):235-312.
C. Spearman. 1904. The proof and mea-
surement of association between two things.
American Journal of Psychology, 15:72-101.
I. H. Witten and T. C. Bell. 1991. The zero-
frequency problem: Estimating the probabil-
ities of novel events in adaptive text com-
pression. IEEE Transactions on Information
Theory, 37(4):1085-1094.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.096879">
<title confidence="0.997727">On the Robustness of Entropy-Based Similarity Measures Evaluation of Subcategorization Acquisition Systems</title>
<author confidence="0.929157">Anna</author>
<affiliation confidence="0.9946515">University of Computer</affiliation>
<address confidence="0.925092">15 JJ Thomson Cambridge CB3 OFD,</address>
<email confidence="0.751386">Anna.Korhonen@cl.cam.ac.uk</email>
<title confidence="0.397405">Yuval</title>
<author confidence="0.732601">Bar-Ilan</author>
<affiliation confidence="0.916953">Department of Computer</affiliation>
<address confidence="0.621315">Ramat Gan 52900,</address>
<email confidence="0.997557">yuvalk@cs.biu.ac.il</email>
<abstract confidence="0.9905269375">Some statistical learning systems are evaluated using measures of distributional similarity. To deal with the problem of zero events in the distributions under comparison, smoothing is frequently performed before similarity measures are applied. Smoothing alters the information in the original distribution, and may add noise to the results. Here, we investigate the sensitivity of entropy-based similarity measures to noise from uninformative smoothing. Our experiments with two subcategorization acquisition systems show that similarity measures vary in their robustness. While some are led astray by noise from smoothing, others are more resilient.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E J Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Automatic extraction of subcategorization from corpora.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th ACL Conference on Applied Natural Language Processing,</booktitle>
<pages>356--363</pages>
<location>Washington DC.</location>
<contexts>
<context position="2087" citStr="Briscoe and Carroll, 1997" startWordPosition="284" endWordPosition="287">us similarity measures have been proposed and used for NLP purposes, including the Kullback-Leibler distance (Cover and Thomas, 1991), cross entropy (Cover and Thomas, 1991), the Jensen-Shannon divergence (Lin, 1991), the skew divergence (Lee, 1999), cosine (Frakes and Baeza-Yates, 1992), Jaccard&apos;s coefficient, Ll norm, and the confusion probability. In this paper, we discuss the use of similarity measures in evaluation of the type of statistical language learners which deliver as an output a distribution of events. A typical example is an automatic subcategorization acquisition system (e.g. (Briscoe and Carroll, 1997; Carroll and Rooth, 1998; Korhonen, 2002b)), which learns, from corpus data, a distribution of subcategorization frames (scFs) specific to a certain predicate (p(scf,lpredicate3)). These learners are frequently evaluated using the standard precision, recall and accuracy measures (Manning and Schiitze, 1999). However, similarity measures provide an important means to evaluate the actual acquired frequencies. In similarity-based evaluation, a learned distribution is compared with a gold standard distribution in order to determine how closely the two correlate. Such evaluation is complicated by </context>
<context position="5065" citStr="Briscoe and Carroll (1997)" startWordPosition="735" endWordPosition="738">ise from smoothing, while others are more resilient and thus better suited for evaluation purposes. Section 2 introduces the subcategorization learners employed. The similarity measures are described in section 3 and the smoothing methods in section 4. Section 5 reports our experiments. We discuss our findings in section 6 and present our conclusions in section 7. 2 Subcategorization Learners We used two subcategorization learners proposed by Korhonen (2002b) to obtain the SCF distributions employed in our experiments. The learners are variations of the subcategorization acquisition system of Briscoe and Carroll (1997). The system uses a shallow parser to obtain the subcategorization information from corpus data. It distinguishes 163 verbal SCFS and returns relative frequencies for each SCF found for a given verb. The resulting putative SCF distributions are processed by the two learners as follows: Learner 1: The scFs in putative distributions are simply ranked in the order of the probability of their occurrence with the verb. The probabilities are estimated using a maximum likelihood estimate (mLE) from the observed relative frequencies. Learner 2: The SCF distributions obtained using Learner 1 are smooth</context>
</contexts>
<marker>Briscoe, Carroll, 1997</marker>
<rawString>E. J. Briscoe and J. Carroll. 1997. Automatic extraction of subcategorization from corpora. In Proceedings of the 5th ACL Conference on Applied Natural Language Processing, pages 356-363, Washington DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glenn Carroll</author>
<author>Mats Rooth</author>
</authors>
<title>Valence induction with a head-lexicalized PCFG.</title>
<date>1998</date>
<booktitle>In Proceedings of the 3rd Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Granada,</location>
<contexts>
<context position="1437" citStr="Carroll and Rooth, 1998" startWordPosition="191" endWordPosition="194">. Our experiments with two subcategorization acquisition systems show that similarity measures vary in their robustness. While some are led astray by noise from smoothing, others are more resilient. 1 Introduction Many natural language processing (NLP) tasks involve measuring distributional similarity. Some examples are the estimation of word cooccurrence probabilities (Dagan et al., 1999), automatic construction of thesauri (Lin, 1998), automatic detection of diathesis alternations (McCarthy, 2000), disambiguation of nominalizations (Lapata, 2002), and evaluation of statistical NLP learners (Carroll and Rooth, 1998; Korhonen, 2002b). Various similarity measures have been proposed and used for NLP purposes, including the Kullback-Leibler distance (Cover and Thomas, 1991), cross entropy (Cover and Thomas, 1991), the Jensen-Shannon divergence (Lin, 1991), the skew divergence (Lee, 1999), cosine (Frakes and Baeza-Yates, 1992), Jaccard&apos;s coefficient, Ll norm, and the confusion probability. In this paper, we discuss the use of similarity measures in evaluation of the type of statistical language learners which deliver as an output a distribution of events. A typical example is an automatic subcategorization a</context>
<context position="17716" citStr="Carroll and Rooth (1998)" startWordPosition="2946" endWordPosition="2949"> latter unconventional use of similarity measures is generally applicable is a matter which requires further investigation. We conducted our experiments by comparing distributions that share, on average, around half of their scFs. Further research is required to investigate the effect of smoothing on distributions that share fewer of their scFs. 6 Discussion In our experiment, entropy-based similarity measures which require q, &gt; 0 (KT, and CE) proved more sensitive to the noise from uninformative smoothing, while those which do not require q, &gt; 0 (SD and JS) proved more robust. Interestingly, Carroll and Rooth (1998) made similar observations when using CE in evaluation of their subcategorization learner. They noted that uninformative smoothing (using the Poisson model (Witten and Bell, 1991)) introduces a high penalty on CE. They did not investigate other similarity measures or smoothing methods. Lee (1999) who compared the performance of a variety of similarity measures on a pair cooccurrence task, also reported best results with measures which concentrate effort on events for which both probability estimates are nonzero. She only considered two entropy-based measures — JS and SD — from which SD proved </context>
</contexts>
<marker>Carroll, Rooth, 1998</marker>
<rawString>Glenn Carroll and Mats Rooth. 1998. Valence induction with a head-lexicalized PCFG. In Proceedings of the 3rd Conference on Empirical Methods in Natural Language Processing, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of the ACL-96,</booktitle>
<pages>310--318</pages>
<location>Santa Cruz, CA.</location>
<contexts>
<context position="5719" citStr="Chen and Goodman, 1996" startWordPosition="838" endWordPosition="841">ser to obtain the subcategorization information from corpus data. It distinguishes 163 verbal SCFS and returns relative frequencies for each SCF found for a given verb. The resulting putative SCF distributions are processed by the two learners as follows: Learner 1: The scFs in putative distributions are simply ranked in the order of the probability of their occurrence with the verb. The probabilities are estimated using a maximum likelihood estimate (mLE) from the observed relative frequencies. Learner 2: The SCF distributions obtained using Learner 1 are smoothed using linear interpolation (Chen and Goodman, 1996). The informative back-off distribution employed in smoothing is based on the semantic class of the verb in question (p(scLisemanticclassi)), chosen according to the verb&apos;s most frequent sense in WordNet (Miller, 1990). For instance, the predominant sense of the verb fly in WordNet belongs to the semantic class of &amp;quot;Motion&amp;quot; verbs - hence, the distribution of &amp;quot;Motion&amp;quot; verbs is employed as a back-off distribution in smoothing. The semantic classes are based on Levin classes (Levin, 1993) and the back-off distribution for each class is obtained by merging SCF distributions of a few verbs in the sa</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proceedings of the ACL-96, pages 310-318, Santa Cruz, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas M Cover</author>
<author>Joy A Thomas</author>
</authors>
<title>Elements of information theory.</title>
<date>1991</date>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="1595" citStr="Cover and Thomas, 1991" startWordPosition="213" endWordPosition="216">om smoothing, others are more resilient. 1 Introduction Many natural language processing (NLP) tasks involve measuring distributional similarity. Some examples are the estimation of word cooccurrence probabilities (Dagan et al., 1999), automatic construction of thesauri (Lin, 1998), automatic detection of diathesis alternations (McCarthy, 2000), disambiguation of nominalizations (Lapata, 2002), and evaluation of statistical NLP learners (Carroll and Rooth, 1998; Korhonen, 2002b). Various similarity measures have been proposed and used for NLP purposes, including the Kullback-Leibler distance (Cover and Thomas, 1991), cross entropy (Cover and Thomas, 1991), the Jensen-Shannon divergence (Lin, 1991), the skew divergence (Lee, 1999), cosine (Frakes and Baeza-Yates, 1992), Jaccard&apos;s coefficient, Ll norm, and the confusion probability. In this paper, we discuss the use of similarity measures in evaluation of the type of statistical language learners which deliver as an output a distribution of events. A typical example is an automatic subcategorization acquisition system (e.g. (Briscoe and Carroll, 1997; Carroll and Rooth, 1998; Korhonen, 2002b)), which learns, from corpus data, a distribution of subcategoriz</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>Thomas M. Cover and Joy A. Thomas. 1991. Elements of information theory. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Lillian Lee</author>
<author>Fernando Pereira</author>
</authors>
<title>Similarity-based models of cooccurrence probabilities.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="1206" citStr="Dagan et al., 1999" startWordPosition="161" endWordPosition="164">sures are applied. Smoothing alters the information in the original distribution, and may add noise to the results. Here, we investigate the sensitivity of entropy-based similarity measures to noise from uninformative smoothing. Our experiments with two subcategorization acquisition systems show that similarity measures vary in their robustness. While some are led astray by noise from smoothing, others are more resilient. 1 Introduction Many natural language processing (NLP) tasks involve measuring distributional similarity. Some examples are the estimation of word cooccurrence probabilities (Dagan et al., 1999), automatic construction of thesauri (Lin, 1998), automatic detection of diathesis alternations (McCarthy, 2000), disambiguation of nominalizations (Lapata, 2002), and evaluation of statistical NLP learners (Carroll and Rooth, 1998; Korhonen, 2002b). Various similarity measures have been proposed and used for NLP purposes, including the Kullback-Leibler distance (Cover and Thomas, 1991), cross entropy (Cover and Thomas, 1991), the Jensen-Shannon divergence (Lin, 1991), the skew divergence (Lee, 1999), cosine (Frakes and Baeza-Yates, 1992), Jaccard&apos;s coefficient, Ll norm, and the confusion prob</context>
</contexts>
<marker>Dagan, Lee, Pereira, 1999</marker>
<rawString>Ido Dagan, Lillian Lee, and Fernando Pereira. 1999. Similarity-based models of cooccurrence probabilities. Machine Learning, 34(1-3):43-69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Frakes</author>
<author>Ricardo Baeza-Yates</author>
</authors>
<title>Information Retrieval: Data Structures and Algorithms.</title>
<date>1992</date>
<publisher>Prentice Hall,</publisher>
<location>Englewood Cliffs, NJ.</location>
<contexts>
<context position="1750" citStr="Frakes and Baeza-Yates, 1992" startWordPosition="234" endWordPosition="237">e examples are the estimation of word cooccurrence probabilities (Dagan et al., 1999), automatic construction of thesauri (Lin, 1998), automatic detection of diathesis alternations (McCarthy, 2000), disambiguation of nominalizations (Lapata, 2002), and evaluation of statistical NLP learners (Carroll and Rooth, 1998; Korhonen, 2002b). Various similarity measures have been proposed and used for NLP purposes, including the Kullback-Leibler distance (Cover and Thomas, 1991), cross entropy (Cover and Thomas, 1991), the Jensen-Shannon divergence (Lin, 1991), the skew divergence (Lee, 1999), cosine (Frakes and Baeza-Yates, 1992), Jaccard&apos;s coefficient, Ll norm, and the confusion probability. In this paper, we discuss the use of similarity measures in evaluation of the type of statistical language learners which deliver as an output a distribution of events. A typical example is an automatic subcategorization acquisition system (e.g. (Briscoe and Carroll, 1997; Carroll and Rooth, 1998; Korhonen, 2002b)), which learns, from corpus data, a distribution of subcategorization frames (scFs) specific to a certain predicate (p(scf,lpredicate3)). These learners are frequently evaluated using the standard precision, recall and </context>
</contexts>
<marker>Frakes, Baeza-Yates, 1992</marker>
<rawString>William B. Frakes and Ricardo Baeza-Yates. 1992. Information Retrieval: Data Structures and Algorithms. Prentice Hall, Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I J Good</author>
</authors>
<title>The population frequencies of species and the estimation of population parameters.</title>
<date>1953</date>
<journal>Biometrika,</journal>
<pages>40--16</pages>
<contexts>
<context position="18917" citStr="Good, 1953" startWordPosition="3135" endWordPosition="3136">SD proved more accurate. Lee (2001) reported better results with SD than KL, even when highly sophisticated (informative) methods were used for smoothing. We restricted our investigation to entropybased similarity measures. In the future, it would be interesting to examine the effect of smoothing on commonly used non-entropy based similarity measures as well. For example, L1 (Manhattan) norm and Jaccard&apos;s coefficient seem promising candidates on the basis of Lee&apos;s (1999) evaluation. In the future, we also plan to investigate other uninformative smoothing methods (e.g. Poisson and Good-Turing (Good, 1953)), and carry out experiments with a wider range of learners with varying degree of accuracy. Our experiments with the two learners substantially different in their accuracy were inadequate to establish whether the noise from smoothing can actually obscure inter-system comparison. In this paper, we have examined the use of similarity measures in evaluation of (particular type of) language learners. The results reported suggest that it is best to either evaluate these language learners using similarity measures known to be resistant to noise from smoothing, or possibly employ measures such as KL</context>
</contexts>
<marker>Good, 1953</marker>
<rawString>I. J. Good. 1953. The population frequencies of species and the estimation of population parameters. Biometrika, 40:16-264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava M Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recogniser.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics, Speech, and Signal Processing,</journal>
<pages>35--3</pages>
<contexts>
<context position="3341" citStr="Katz, 1987" startWordPosition="475" endWordPosition="476">son have different supports, i.e. regions of positive probability. Due to the sparse data problem, zero events typically make up a substantial portion of joint data. To allow the comparison of all events, smoothing is frequently performed before similarity measures are applied. Smoothing tackles the problem by assigning non-zero values to zero events. It is usually done in an uninformative way (by assigning a uniform prior probabability to events; e.g. (Laplace, 1814; Witten and Bell, 1991)), rather than in an informative way (by assigning an informative prior probability e.g. by backing-off (Katz, 1987)), as it is desirable — from the evaluation point of view — to preserve as much of the original, learned distribution as possible&apos;. Although uninformative smoothing makes the simplest possible assumption regarding the probability of unseen events, it involves also altering the information included in the original 1-See Manning and Schiitze (1999) for both informative and uninformative smoothing methods. distribution. It can add &apos;noise&apos; to the original data. This noise may, in turn, affect distributional similarity and potentially obscure similarity-based evaluation. Here, we investigate the se</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Slava M. Katz. 1987. Estimation of probabilities from sparse data for the language model component of a speech recogniser. IEEE Transactions on Acoustics, Speech, and Signal Processing, 35(3):400-401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Korhonen</author>
</authors>
<title>Semantically motivated subcategorization acquisition.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL Workshop on Unsupervised Lexical Acquisition.</booktitle>
<note>To appear.</note>
<contexts>
<context position="1453" citStr="Korhonen, 2002" startWordPosition="195" endWordPosition="196">o subcategorization acquisition systems show that similarity measures vary in their robustness. While some are led astray by noise from smoothing, others are more resilient. 1 Introduction Many natural language processing (NLP) tasks involve measuring distributional similarity. Some examples are the estimation of word cooccurrence probabilities (Dagan et al., 1999), automatic construction of thesauri (Lin, 1998), automatic detection of diathesis alternations (McCarthy, 2000), disambiguation of nominalizations (Lapata, 2002), and evaluation of statistical NLP learners (Carroll and Rooth, 1998; Korhonen, 2002b). Various similarity measures have been proposed and used for NLP purposes, including the Kullback-Leibler distance (Cover and Thomas, 1991), cross entropy (Cover and Thomas, 1991), the Jensen-Shannon divergence (Lin, 1991), the skew divergence (Lee, 1999), cosine (Frakes and Baeza-Yates, 1992), Jaccard&apos;s coefficient, Ll norm, and the confusion probability. In this paper, we discuss the use of similarity measures in evaluation of the type of statistical language learners which deliver as an output a distribution of events. A typical example is an automatic subcategorization acquisition syste</context>
<context position="4900" citStr="Korhonen (2002" startWordPosition="713" endWordPosition="714">milarity, we observe differences in the robustness of various measures. Our results show that some entropy-based similarity measures are led astray by noise from smoothing, while others are more resilient and thus better suited for evaluation purposes. Section 2 introduces the subcategorization learners employed. The similarity measures are described in section 3 and the smoothing methods in section 4. Section 5 reports our experiments. We discuss our findings in section 6 and present our conclusions in section 7. 2 Subcategorization Learners We used two subcategorization learners proposed by Korhonen (2002b) to obtain the SCF distributions employed in our experiments. The learners are variations of the subcategorization acquisition system of Briscoe and Carroll (1997). The system uses a shallow parser to obtain the subcategorization information from corpus data. It distinguishes 163 verbal SCFS and returns relative frequencies for each SCF found for a given verb. The resulting putative SCF distributions are processed by the two learners as follows: Learner 1: The scFs in putative distributions are simply ranked in the order of the probability of their occurrence with the verb. The probabilities</context>
<context position="6821" citStr="Korhonen (2002" startWordPosition="1015" endWordPosition="1016">) and the back-off distribution for each class is obtained by merging SCF distributions of a few verbs in the same class. The parameters used in smoothing are obtained by optimising SCF acquisition performance on held-out training data so that most of the smoothed probability is determined by the MLE from the subcategorization acquisition system2. Learner 2 tends to perform better than learner 1, since it involves using a priori knowledge about generalizations of verb semantics to guide subcategorization acquisition. It corrects the putative SCF distribution and deals better with sparse data. Korhonen (2002a) used an empirically determined threshold on the probability estimates to filter noisy SCFS out, and then evaluated the two learners on a test set of 91 verbs using precision and recall -based evaluation. The gold standard employed in the evaluation was obtained by manually analysing an average of 300 occurrences of each test verb in corpus data. Learner 1 yielded 82% type precision (the percentage of SCF types that the method proposes which are correct) and 49% type recall (the percentage of SCF types in the gold standard that the method proposes), while type precision was 81% and type reca</context>
</contexts>
<marker>Korhonen, 2002</marker>
<rawString>Anna Korhonen. 2002a. Semantically motivated subcategorization acquisition. In Proceedings of the ACL Workshop on Unsupervised Lexical Acquisition. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Korhonen</author>
</authors>
<title>Subcategorization Acquisition.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Cambridge, UK.</institution>
<contexts>
<context position="1453" citStr="Korhonen, 2002" startWordPosition="195" endWordPosition="196">o subcategorization acquisition systems show that similarity measures vary in their robustness. While some are led astray by noise from smoothing, others are more resilient. 1 Introduction Many natural language processing (NLP) tasks involve measuring distributional similarity. Some examples are the estimation of word cooccurrence probabilities (Dagan et al., 1999), automatic construction of thesauri (Lin, 1998), automatic detection of diathesis alternations (McCarthy, 2000), disambiguation of nominalizations (Lapata, 2002), and evaluation of statistical NLP learners (Carroll and Rooth, 1998; Korhonen, 2002b). Various similarity measures have been proposed and used for NLP purposes, including the Kullback-Leibler distance (Cover and Thomas, 1991), cross entropy (Cover and Thomas, 1991), the Jensen-Shannon divergence (Lin, 1991), the skew divergence (Lee, 1999), cosine (Frakes and Baeza-Yates, 1992), Jaccard&apos;s coefficient, Ll norm, and the confusion probability. In this paper, we discuss the use of similarity measures in evaluation of the type of statistical language learners which deliver as an output a distribution of events. A typical example is an automatic subcategorization acquisition syste</context>
<context position="4900" citStr="Korhonen (2002" startWordPosition="713" endWordPosition="714">milarity, we observe differences in the robustness of various measures. Our results show that some entropy-based similarity measures are led astray by noise from smoothing, while others are more resilient and thus better suited for evaluation purposes. Section 2 introduces the subcategorization learners employed. The similarity measures are described in section 3 and the smoothing methods in section 4. Section 5 reports our experiments. We discuss our findings in section 6 and present our conclusions in section 7. 2 Subcategorization Learners We used two subcategorization learners proposed by Korhonen (2002b) to obtain the SCF distributions employed in our experiments. The learners are variations of the subcategorization acquisition system of Briscoe and Carroll (1997). The system uses a shallow parser to obtain the subcategorization information from corpus data. It distinguishes 163 verbal SCFS and returns relative frequencies for each SCF found for a given verb. The resulting putative SCF distributions are processed by the two learners as follows: Learner 1: The scFs in putative distributions are simply ranked in the order of the probability of their occurrence with the verb. The probabilities</context>
<context position="6821" citStr="Korhonen (2002" startWordPosition="1015" endWordPosition="1016">) and the back-off distribution for each class is obtained by merging SCF distributions of a few verbs in the same class. The parameters used in smoothing are obtained by optimising SCF acquisition performance on held-out training data so that most of the smoothed probability is determined by the MLE from the subcategorization acquisition system2. Learner 2 tends to perform better than learner 1, since it involves using a priori knowledge about generalizations of verb semantics to guide subcategorization acquisition. It corrects the putative SCF distribution and deals better with sparse data. Korhonen (2002a) used an empirically determined threshold on the probability estimates to filter noisy SCFS out, and then evaluated the two learners on a test set of 91 verbs using precision and recall -based evaluation. The gold standard employed in the evaluation was obtained by manually analysing an average of 300 occurrences of each test verb in corpus data. Learner 1 yielded 82% type precision (the percentage of SCF types that the method proposes which are correct) and 49% type recall (the percentage of SCF types in the gold standard that the method proposes), while type precision was 81% and type reca</context>
</contexts>
<marker>Korhonen, 2002</marker>
<rawString>Anna Korhonen. 2002b. Subcategorization Acquisition. Ph.D. thesis, University of Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Lapata</author>
</authors>
<title>The disambiguation of nominalisations.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="1368" citStr="Lapata, 2002" startWordPosition="182" endWordPosition="183">d similarity measures to noise from uninformative smoothing. Our experiments with two subcategorization acquisition systems show that similarity measures vary in their robustness. While some are led astray by noise from smoothing, others are more resilient. 1 Introduction Many natural language processing (NLP) tasks involve measuring distributional similarity. Some examples are the estimation of word cooccurrence probabilities (Dagan et al., 1999), automatic construction of thesauri (Lin, 1998), automatic detection of diathesis alternations (McCarthy, 2000), disambiguation of nominalizations (Lapata, 2002), and evaluation of statistical NLP learners (Carroll and Rooth, 1998; Korhonen, 2002b). Various similarity measures have been proposed and used for NLP purposes, including the Kullback-Leibler distance (Cover and Thomas, 1991), cross entropy (Cover and Thomas, 1991), the Jensen-Shannon divergence (Lin, 1991), the skew divergence (Lee, 1999), cosine (Frakes and Baeza-Yates, 1992), Jaccard&apos;s coefficient, Ll norm, and the confusion probability. In this paper, we discuss the use of similarity measures in evaluation of the type of statistical language learners which deliver as an output a distribu</context>
</contexts>
<marker>Lapata, 2002</marker>
<rawString>Maria Lapata. 2002. The disambiguation of nominalisations. Computational Linguistics, 28(3).</rawString>
</citation>
<citation valid="false">
<title>Essai philosophique stir les pro babilites.</title>
<location>Mme. Ve. Courcier.</location>
<marker></marker>
<rawString>P. S. Laplace. 1814. Essai philosophique stir les pro babilites. Mme. Ve. Courcier.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>Measures of distributional similarity.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the ACL,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="1711" citStr="Lee, 1999" startWordPosition="231" endWordPosition="232">onal similarity. Some examples are the estimation of word cooccurrence probabilities (Dagan et al., 1999), automatic construction of thesauri (Lin, 1998), automatic detection of diathesis alternations (McCarthy, 2000), disambiguation of nominalizations (Lapata, 2002), and evaluation of statistical NLP learners (Carroll and Rooth, 1998; Korhonen, 2002b). Various similarity measures have been proposed and used for NLP purposes, including the Kullback-Leibler distance (Cover and Thomas, 1991), cross entropy (Cover and Thomas, 1991), the Jensen-Shannon divergence (Lin, 1991), the skew divergence (Lee, 1999), cosine (Frakes and Baeza-Yates, 1992), Jaccard&apos;s coefficient, Ll norm, and the confusion probability. In this paper, we discuss the use of similarity measures in evaluation of the type of statistical language learners which deliver as an output a distribution of events. A typical example is an automatic subcategorization acquisition system (e.g. (Briscoe and Carroll, 1997; Carroll and Rooth, 1998; Korhonen, 2002b)), which learns, from corpus data, a distribution of subcategorization frames (scFs) specific to a certain predicate (p(scf,lpredicate3)). These learners are frequently evaluated us</context>
<context position="9974" citStr="Lee (1999)" startWordPosition="1593" endWordPosition="1594">q) = E —p, log(q,). CE is minimal when p and q are identical. In this case cE(p, q) = H(p) is the Shannon entropy of p. 4. KL: Kullback-Leibler distance — a measure of the additional information needed to describe p using q: D (PIlq) = CE(P q) H (P) = E Pi log (7) • KL is always &gt; 0 and = 0 only when p q. 5. JS: The Jensen-Shannon divergence — a measure which relies on the assumption that if p and q are similar, they are close to their average. 1 Js(p,q) = [D(pl 6. SD: The skew divergence. It smooths q by mixing it with p: sp(p, q) = D(plla • q + (1 — • p). sD(p, q) approximates KL as a —&gt; 1. Lee (1999) reports the best results with a = 0.99. We adopted the same value. All these other measures, except IS and RC (which are included in our experiment primarily for comparison), are entropy-based measures of distributional similarity. CE and KL are asymmetric and, unlike JS and SD, undefined if there exists a SCF for which p, &gt; 0 but q, = 0. 4 Smoothing Two different uninformative smoothing methods were selected for investigation: the add-one and Witten-Bell (Witten and Bell, 1991) methods. They both work by distributing a certain probability mass among unseen events and discounting the observed</context>
<context position="18013" citStr="Lee (1999)" startWordPosition="2994" endWordPosition="2995">ions that share fewer of their scFs. 6 Discussion In our experiment, entropy-based similarity measures which require q, &gt; 0 (KT, and CE) proved more sensitive to the noise from uninformative smoothing, while those which do not require q, &gt; 0 (SD and JS) proved more robust. Interestingly, Carroll and Rooth (1998) made similar observations when using CE in evaluation of their subcategorization learner. They noted that uninformative smoothing (using the Poisson model (Witten and Bell, 1991)) introduces a high penalty on CE. They did not investigate other similarity measures or smoothing methods. Lee (1999) who compared the performance of a variety of similarity measures on a pair cooccurrence task, also reported best results with measures which concentrate effort on events for which both probability estimates are nonzero. She only considered two entropy-based measures — JS and SD — from which SD proved more accurate. Lee (2001) reported better results with SD than KL, even when highly sophisticated (informative) methods were used for smoothing. We restricted our investigation to entropybased similarity measures. In the future, it would be interesting to examine the effect of smoothing on common</context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>Lillian Lee. 1999. Measures of distributional similarity. In Proceedings of the 37th Annual Meeting of the ACL, pages 25-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>On the effectiveness of the skew divergence for statistical language analysis.</title>
<date>2001</date>
<booktitle>In Artificial Intelligence and Statistics</booktitle>
<pages>65--72</pages>
<contexts>
<context position="18341" citStr="Lee (2001)" startWordPosition="3048" endWordPosition="3049">bservations when using CE in evaluation of their subcategorization learner. They noted that uninformative smoothing (using the Poisson model (Witten and Bell, 1991)) introduces a high penalty on CE. They did not investigate other similarity measures or smoothing methods. Lee (1999) who compared the performance of a variety of similarity measures on a pair cooccurrence task, also reported best results with measures which concentrate effort on events for which both probability estimates are nonzero. She only considered two entropy-based measures — JS and SD — from which SD proved more accurate. Lee (2001) reported better results with SD than KL, even when highly sophisticated (informative) methods were used for smoothing. We restricted our investigation to entropybased similarity measures. In the future, it would be interesting to examine the effect of smoothing on commonly used non-entropy based similarity measures as well. For example, L1 (Manhattan) norm and Jaccard&apos;s coefficient seem promising candidates on the basis of Lee&apos;s (1999) evaluation. In the future, we also plan to investigate other uninformative smoothing methods (e.g. Poisson and Good-Turing (Good, 1953)), and carry out experim</context>
</contexts>
<marker>Lee, 2001</marker>
<rawString>Lillian Lee. 2001. On the effectiveness of the skew divergence for statistical language analysis. In Artificial Intelligence and Statistics 2001, pages 65-72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoff Leech</author>
</authors>
<title>100 million words of English:</title>
<date>1992</date>
<journal>the British National Corpus. Language Research,</journal>
<pages>28--1</pages>
<contexts>
<context position="11757" citStr="Leech, 1992" startWordPosition="1910" endWordPosition="1911"> n„f 4.2 Witten-Bell Witten and Bell (1991) present a set of smoothing methods which involve estimating the discount from the observed distribution. We adopt their method &amp;quot;C&amp;quot;, the so-called Witten-Bell method. It considers each unseen SCF type as an event in addition to the N seen SCF tokens. Accordingly, the probability of an unseen SCF is estimated by nscf Pwb - • N nscf 5 Experiment 5.1 Test Data and Methods We selected 17 test verbs for our experiments. Sentences containing an occurrence of one of these verbs were first extracted from 20 million words of the British National Corpus (BNc) (Leech, 1992), an average of 1000 citations of each, and then processed using the two subcategorization learners. This yielded two SCF distributions per test verb. The similarity measures introduced in the previous section were then applied to evaluate the accuracy of these acquired distributions (q) against gold standard distributions (p). The latter were obtained by manually analysing an average of 300 occurrences of each test verb in the BNC. Prior to calculating the similarity, we smoothed the distributions under comparison using the two uninformative methods introduced in section 4. To investigate in </context>
</contexts>
<marker>Leech, 1992</marker>
<rawString>Geoff Leech. 1992. 100 million words of English: the British National Corpus. Language Research, 28(1):1-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations.</title>
<date>1993</date>
<publisher>Chicago University Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="6208" citStr="Levin, 1993" startWordPosition="918" endWordPosition="919"> Learner 2: The SCF distributions obtained using Learner 1 are smoothed using linear interpolation (Chen and Goodman, 1996). The informative back-off distribution employed in smoothing is based on the semantic class of the verb in question (p(scLisemanticclassi)), chosen according to the verb&apos;s most frequent sense in WordNet (Miller, 1990). For instance, the predominant sense of the verb fly in WordNet belongs to the semantic class of &amp;quot;Motion&amp;quot; verbs - hence, the distribution of &amp;quot;Motion&amp;quot; verbs is employed as a back-off distribution in smoothing. The semantic classes are based on Levin classes (Levin, 1993) and the back-off distribution for each class is obtained by merging SCF distributions of a few verbs in the same class. The parameters used in smoothing are obtained by optimising SCF acquisition performance on held-out training data so that most of the smoothed probability is determined by the MLE from the subcategorization acquisition system2. Learner 2 tends to perform better than learner 1, since it involves using a priori knowledge about generalizations of verb semantics to guide subcategorization acquisition. It corrects the putative SCF distribution and deals better with sparse data. K</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes and Alternations. Chicago University Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianhua Lin</author>
</authors>
<title>Divergence measures based on the Shannon entropy.</title>
<date>1991</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>37--1</pages>
<contexts>
<context position="1678" citStr="Lin, 1991" startWordPosition="226" endWordPosition="227">asks involve measuring distributional similarity. Some examples are the estimation of word cooccurrence probabilities (Dagan et al., 1999), automatic construction of thesauri (Lin, 1998), automatic detection of diathesis alternations (McCarthy, 2000), disambiguation of nominalizations (Lapata, 2002), and evaluation of statistical NLP learners (Carroll and Rooth, 1998; Korhonen, 2002b). Various similarity measures have been proposed and used for NLP purposes, including the Kullback-Leibler distance (Cover and Thomas, 1991), cross entropy (Cover and Thomas, 1991), the Jensen-Shannon divergence (Lin, 1991), the skew divergence (Lee, 1999), cosine (Frakes and Baeza-Yates, 1992), Jaccard&apos;s coefficient, Ll norm, and the confusion probability. In this paper, we discuss the use of similarity measures in evaluation of the type of statistical language learners which deliver as an output a distribution of events. A typical example is an automatic subcategorization acquisition system (e.g. (Briscoe and Carroll, 1997; Carroll and Rooth, 1998; Korhonen, 2002b)), which learns, from corpus data, a distribution of subcategorization frames (scFs) specific to a certain predicate (p(scf,lpredicate3)). These lea</context>
</contexts>
<marker>Lin, 1991</marker>
<rawString>Jianhua Lin. 1991. Divergence measures based on the Shannon entropy. IEEE Transactions on Information Theory, 37(1):145-151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the COLING-ACL&apos;98,</booktitle>
<pages>768--773</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="1254" citStr="Lin, 1998" startWordPosition="169" endWordPosition="170">e original distribution, and may add noise to the results. Here, we investigate the sensitivity of entropy-based similarity measures to noise from uninformative smoothing. Our experiments with two subcategorization acquisition systems show that similarity measures vary in their robustness. While some are led astray by noise from smoothing, others are more resilient. 1 Introduction Many natural language processing (NLP) tasks involve measuring distributional similarity. Some examples are the estimation of word cooccurrence probabilities (Dagan et al., 1999), automatic construction of thesauri (Lin, 1998), automatic detection of diathesis alternations (McCarthy, 2000), disambiguation of nominalizations (Lapata, 2002), and evaluation of statistical NLP learners (Carroll and Rooth, 1998; Korhonen, 2002b). Various similarity measures have been proposed and used for NLP purposes, including the Kullback-Leibler distance (Cover and Thomas, 1991), cross entropy (Cover and Thomas, 1991), the Jensen-Shannon divergence (Lin, 1991), the skew divergence (Lee, 1999), cosine (Frakes and Baeza-Yates, 1992), Jaccard&apos;s coefficient, Ll norm, and the confusion probability. In this paper, we discuss the use of si</context>
<context position="8597" citStr="Lin, 1998" startWordPosition="1315" endWordPosition="1316">F acquisition. 3F _ 2.precision•recall precision+recall cies associated with the types. For example, a learner may correctly acquire a sentential complement frame for believe (I believe that our theory is correct) but incorrectly assign this frequently occurring frame a very small probability. 3 Similarity Measures We used the following set of similarity measures to evaluate the accuracy of a learned SCF distribution q = {q,} with respect to a gold standard distribution p = fp, 1. q, and p, denote the probability of scf, in the two distributions, respectively. 1. IS: The intersection measure (Lin, 1998) 2 I com(p, q) I , is(p, q) = IsuPP(P)I + IsuPP(q) I where supp(p) and supp(q) are the sets of scFs with non-zero probability in p and q, and com(p, q) is the intersection of these two sets. 2. RC: The Spearman rank correlation coefficient (Spearman, 1904). It involves (i) calculating the ranks rP and rq for each of the SCF variables separately, using averaged ranks for tied values, and (ii) finding RC by calculating the Pearson correlation coefficient for the ranks: RC (p, q) = con (rP , rq). RC lies in the range [-1, 1], with values near 0 denoting a low degree of association and values near</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the COLING-ACL&apos;98, pages 768-773, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Schiitze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2396" citStr="Manning and Schiitze, 1999" startWordPosition="327" endWordPosition="330">ient, Ll norm, and the confusion probability. In this paper, we discuss the use of similarity measures in evaluation of the type of statistical language learners which deliver as an output a distribution of events. A typical example is an automatic subcategorization acquisition system (e.g. (Briscoe and Carroll, 1997; Carroll and Rooth, 1998; Korhonen, 2002b)), which learns, from corpus data, a distribution of subcategorization frames (scFs) specific to a certain predicate (p(scf,lpredicate3)). These learners are frequently evaluated using the standard precision, recall and accuracy measures (Manning and Schiitze, 1999). However, similarity measures provide an important means to evaluate the actual acquired frequencies. In similarity-based evaluation, a learned distribution is compared with a gold standard distribution in order to determine how closely the two correlate. Such evaluation is complicated by cases where the distributions under comparison have different supports, i.e. regions of positive probability. Due to the sparse data problem, zero events typically make up a substantial portion of joint data. To allow the comparison of all events, smoothing is frequently performed before similarity measures </context>
<context position="3689" citStr="Manning and Schiitze (1999)" startWordPosition="528" endWordPosition="531">alues to zero events. It is usually done in an uninformative way (by assigning a uniform prior probabability to events; e.g. (Laplace, 1814; Witten and Bell, 1991)), rather than in an informative way (by assigning an informative prior probability e.g. by backing-off (Katz, 1987)), as it is desirable — from the evaluation point of view — to preserve as much of the original, learned distribution as possible&apos;. Although uninformative smoothing makes the simplest possible assumption regarding the probability of unseen events, it involves also altering the information included in the original 1-See Manning and Schiitze (1999) for both informative and uninformative smoothing methods. distribution. It can add &apos;noise&apos; to the original data. This noise may, in turn, affect distributional similarity and potentially obscure similarity-based evaluation. Here, we investigate the sensitivity of widelyused entropy-based similarity measures to the noise from uninformative smoothing. We do this in the context of evaluation, by using the measures to evaluate the accuracy of verbal SCF distributions learned automatically from corpus data. By controlling the number of scFs smoothed and examining the effect on distributional simil</context>
</contexts>
<marker>Manning, Schiitze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Schiitze. 1999. Foundations of Statistical Natural Language Processing. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
</authors>
<title>Using semantic preferences to identify verbal participation in role switching alternations.</title>
<date>2000</date>
<booktitle>In Proceedings of the NAACL,</booktitle>
<pages>162--169</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="1318" citStr="McCarthy, 2000" startWordPosition="176" endWordPosition="177">Here, we investigate the sensitivity of entropy-based similarity measures to noise from uninformative smoothing. Our experiments with two subcategorization acquisition systems show that similarity measures vary in their robustness. While some are led astray by noise from smoothing, others are more resilient. 1 Introduction Many natural language processing (NLP) tasks involve measuring distributional similarity. Some examples are the estimation of word cooccurrence probabilities (Dagan et al., 1999), automatic construction of thesauri (Lin, 1998), automatic detection of diathesis alternations (McCarthy, 2000), disambiguation of nominalizations (Lapata, 2002), and evaluation of statistical NLP learners (Carroll and Rooth, 1998; Korhonen, 2002b). Various similarity measures have been proposed and used for NLP purposes, including the Kullback-Leibler distance (Cover and Thomas, 1991), cross entropy (Cover and Thomas, 1991), the Jensen-Shannon divergence (Lin, 1991), the skew divergence (Lee, 1999), cosine (Frakes and Baeza-Yates, 1992), Jaccard&apos;s coefficient, Ll norm, and the confusion probability. In this paper, we discuss the use of similarity measures in evaluation of the type of statistical langu</context>
</contexts>
<marker>McCarthy, 2000</marker>
<rawString>Diana McCarthy. 2000. Using semantic preferences to identify verbal participation in role switching alternations. In Proceedings of the NAACL, pages 162-169, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: An online lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<pages>3--4</pages>
<contexts>
<context position="5937" citStr="Miller, 1990" startWordPosition="874" endWordPosition="875">he two learners as follows: Learner 1: The scFs in putative distributions are simply ranked in the order of the probability of their occurrence with the verb. The probabilities are estimated using a maximum likelihood estimate (mLE) from the observed relative frequencies. Learner 2: The SCF distributions obtained using Learner 1 are smoothed using linear interpolation (Chen and Goodman, 1996). The informative back-off distribution employed in smoothing is based on the semantic class of the verb in question (p(scLisemanticclassi)), chosen according to the verb&apos;s most frequent sense in WordNet (Miller, 1990). For instance, the predominant sense of the verb fly in WordNet belongs to the semantic class of &amp;quot;Motion&amp;quot; verbs - hence, the distribution of &amp;quot;Motion&amp;quot; verbs is employed as a back-off distribution in smoothing. The semantic classes are based on Levin classes (Levin, 1993) and the back-off distribution for each class is obtained by merging SCF distributions of a few verbs in the same class. The parameters used in smoothing are obtained by optimising SCF acquisition performance on held-out training data so that most of the smoothed probability is determined by the MLE from the subcategorization a</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>George A. Miller. 1990. WordNet: An online lexical database. International Journal of Lexicography, 3(4):235-312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Spearman</author>
</authors>
<title>The proof and measurement of association between two things.</title>
<date>1904</date>
<journal>American Journal of Psychology,</journal>
<pages>15--72</pages>
<contexts>
<context position="8853" citStr="Spearman, 1904" startWordPosition="1364" endWordPosition="1365">y occurring frame a very small probability. 3 Similarity Measures We used the following set of similarity measures to evaluate the accuracy of a learned SCF distribution q = {q,} with respect to a gold standard distribution p = fp, 1. q, and p, denote the probability of scf, in the two distributions, respectively. 1. IS: The intersection measure (Lin, 1998) 2 I com(p, q) I , is(p, q) = IsuPP(P)I + IsuPP(q) I where supp(p) and supp(q) are the sets of scFs with non-zero probability in p and q, and com(p, q) is the intersection of these two sets. 2. RC: The Spearman rank correlation coefficient (Spearman, 1904). It involves (i) calculating the ranks rP and rq for each of the SCF variables separately, using averaged ranks for tied values, and (ii) finding RC by calculating the Pearson correlation coefficient for the ranks: RC (p, q) = con (rP , rq). RC lies in the range [-1, 1], with values near 0 denoting a low degree of association and values near -1 and 1 denoting strong association. 3. CE: Cross entropy — a measure of the information needed to describe a true distribution p using a model distribution q: cE(p,q) = E —p, log(q,). CE is minimal when p and q are identical. In this case cE(p, q) = H(p</context>
</contexts>
<marker>Spearman, 1904</marker>
<rawString>C. Spearman. 1904. The proof and measurement of association between two things. American Journal of Psychology, 15:72-101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>T C Bell</author>
</authors>
<title>The zerofrequency problem: Estimating the probabilities of novel events in adaptive text compression.</title>
<date>1991</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>37--4</pages>
<contexts>
<context position="3225" citStr="Witten and Bell, 1991" startWordPosition="456" endWordPosition="459">order to determine how closely the two correlate. Such evaluation is complicated by cases where the distributions under comparison have different supports, i.e. regions of positive probability. Due to the sparse data problem, zero events typically make up a substantial portion of joint data. To allow the comparison of all events, smoothing is frequently performed before similarity measures are applied. Smoothing tackles the problem by assigning non-zero values to zero events. It is usually done in an uninformative way (by assigning a uniform prior probabability to events; e.g. (Laplace, 1814; Witten and Bell, 1991)), rather than in an informative way (by assigning an informative prior probability e.g. by backing-off (Katz, 1987)), as it is desirable — from the evaluation point of view — to preserve as much of the original, learned distribution as possible&apos;. Although uninformative smoothing makes the simplest possible assumption regarding the probability of unseen events, it involves also altering the information included in the original 1-See Manning and Schiitze (1999) for both informative and uninformative smoothing methods. distribution. It can add &apos;noise&apos; to the original data. This noise may, in tur</context>
<context position="10458" citStr="Witten and Bell, 1991" startWordPosition="1673" endWordPosition="1676">: The skew divergence. It smooths q by mixing it with p: sp(p, q) = D(plla • q + (1 — • p). sD(p, q) approximates KL as a —&gt; 1. Lee (1999) reports the best results with a = 0.99. We adopted the same value. All these other measures, except IS and RC (which are included in our experiment primarily for comparison), are entropy-based measures of distributional similarity. CE and KL are asymmetric and, unlike JS and SD, undefined if there exists a SCF for which p, &gt; 0 but q, = 0. 4 Smoothing Two different uninformative smoothing methods were selected for investigation: the add-one and Witten-Bell (Witten and Bell, 1991) methods. They both work by distributing a certain probability mass among unseen events and discounting the observed distribution accordingly, but differ in the way they estimate the discount. 4.1 Add-One Add-one smoothing involves assigning a uniform prior probability to all events so that q, &gt; 0 for all i. Let c(scf,) be the frequency of a SCF (given a verb) in q, N the total number of SCF tokens for this verb in q, and n„f the total number of SCF types4 considered. The estimated probability of the SCF is: 4While &apos;types&apos; are the set of SCFS assumed e.g. in a dictionary, &apos;tokens&apos; are the indi</context>
<context position="17895" citStr="Witten and Bell, 1991" startWordPosition="2972" endWordPosition="2975">hat share, on average, around half of their scFs. Further research is required to investigate the effect of smoothing on distributions that share fewer of their scFs. 6 Discussion In our experiment, entropy-based similarity measures which require q, &gt; 0 (KT, and CE) proved more sensitive to the noise from uninformative smoothing, while those which do not require q, &gt; 0 (SD and JS) proved more robust. Interestingly, Carroll and Rooth (1998) made similar observations when using CE in evaluation of their subcategorization learner. They noted that uninformative smoothing (using the Poisson model (Witten and Bell, 1991)) introduces a high penalty on CE. They did not investigate other similarity measures or smoothing methods. Lee (1999) who compared the performance of a variety of similarity measures on a pair cooccurrence task, also reported best results with measures which concentrate effort on events for which both probability estimates are nonzero. She only considered two entropy-based measures — JS and SD — from which SD proved more accurate. Lee (2001) reported better results with SD than KL, even when highly sophisticated (informative) methods were used for smoothing. We restricted our investigation to</context>
</contexts>
<marker>Witten, Bell, 1991</marker>
<rawString>I. H. Witten and T. C. Bell. 1991. The zerofrequency problem: Estimating the probabilities of novel events in adaptive text compression. IEEE Transactions on Information Theory, 37(4):1085-1094.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>