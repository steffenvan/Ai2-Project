<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000026">
<title confidence="0.997027">
Paraphrase-Driven Learning for Open Question Answering
</title>
<author confidence="0.997754">
Anthony Fader Luke Zettlemoyer Oren Etzioni
</author>
<affiliation confidence="0.9947175">
Computer Science &amp; Engineering
University of Washington
</affiliation>
<address confidence="0.891893">
Seattle, WA 98195
</address>
<email confidence="0.996862">
{afader, lsz, etzioni}@cs.washington.edu
</email>
<sectionHeader confidence="0.993848" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999951125">
We study question answering as a ma-
chine learning problem, and induce a func-
tion that maps open-domain questions to
queries over a database of web extrac-
tions. Given a large, community-authored,
question-paraphrase corpus, we demon-
strate that it is possible to learn a se-
mantic lexicon and linear ranking func-
tion without manually annotating ques-
tions. Our approach automatically gener-
alizes a seed lexicon and includes a scal-
able, parallelized perceptron parameter es-
timation scheme. Experiments show that
our approach more than quadruples the re-
call of the seed lexicon, with only an 8%
loss in precision.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.980760303030303">
Open-domain question answering (QA) is a long-
standing, unsolved problem. The central challenge
is to automate every step of QA system construc-
tion, including gathering large databases and an-
swering questions against these databases. While
there has been significant work on large-scale in-
formation extraction (IE) from unstructured text
(Banko et al., 2007; Hoffmann et al., 2010; Riedel
et al., 2010), the problem of answering questions
with the noisy knowledge bases that IE systems
produce has received less attention. In this paper,
we present an approach for learning to map ques-
tions to formal queries over a large, open-domain
database of extracted facts (Fader et al., 2011).
Our system learns from a large, noisy, question-
paraphrase corpus, where question clusters have
a common but unknown query, and can span
a diverse set of topics. Table 1 shows exam-
ple paraphrase clusters for a set of factual ques-
tions. Such data provides strong signal for learn-
ing about lexical variation, but there are a number
Who wrote the Winnie the Pooh books?
Who is the author of winnie the pooh?
What was the name of the authur of winnie the pooh?
Who wrote the series of books for Winnie the poo?
Who wrote the children’s storybook ‘Winnie the Pooh’?
Who is poohs creator?
What relieves a hangover?
What is the best cure for a hangover?
The best way to recover from a hangover?
Best remedy for a hangover?
What takes away a hangover?
How do you lose a hangover?
What helps hangover symptoms?
What are social networking sites used for?
Why do people use social networking sites worldwide?
Advantages of using social network sites?
Why do people use social networks a lot?
Why do people communicate on social networking sites?
What are the pros and cons of social networking sites?
How do you say Santa Claus in Sweden?
Say santa clause in sweden?
How do you say santa clause in swedish?
How do they say santa in Sweden?
In Sweden what is santa called?
Who is sweden santa?
Table 1: Examples of paraphrase clusters from the
WikiAnswers corpus. Within each cluster, there is
a wide range of syntactic and lexical variations.
of challenges. Given that the data is community-
authored, it will inevitably be incomplete, contain
incorrectly tagged paraphrases, non-factual ques-
tions, and other sources of noise.
Our core contribution is a new learning ap-
proach that scalably sifts through this para-
phrase noise, learning to answer a broad class
of factual questions. We focus on answer-
ing open-domain questions that can be answered
with single-relation queries, e.g. all of the para-
phrases of “Who wrote Winnie the Pooh?” and
“What cures a hangover?” in Table 1. The
algorithm answers such questions by mapping
them to executable queries over a tuple store
containing relations such as authored(milne,
winnie-the-pooh) and treat(bloody-mary,
hangover-symptoms).
</bodyText>
<page confidence="0.938955">
1608
</page>
<note confidence="0.913235">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1608–1618,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999821851851852">
The approach automatically induces lexical
structures, which are combined to build queries for
unseen questions. It learns lexical equivalences for
relations (e.g., wrote, authored, and creator), en-
tities (e.g., Winnie the Pooh or Pooh Bear), and
question templates (e.g., Who r the e books? and
Who is the r of e?). Crucially, the approach
does not require any explicit labeling of the ques-
tions in our paraphrase corpus. Instead, we use
16 seed question templates and string-matching to
find high-quality queries for a small subset of the
questions. The algorithm uses learned word align-
ments to aggressively generalize the seeds, pro-
ducing a large set of possible lexical equivalences.
We then learn a linear ranking model to filter the
learned lexical equivalences, keeping only those
that are likely to answer questions well in practice.
Experimental results on 18 million paraphrase
pairs gathered from WikiAnswers1 demonstrate
the effectiveness of the overall approach. We
performed an end-to-end evaluation against a
database of 15 million facts automatically ex-
tracted from general web text (Fader et al., 2011).
On known-answerable questions, the approach
achieved 42% recall, with 77% precision, more
than quadrupling the recall over a baseline system.
In sum, we make the following contributions:
</bodyText>
<listItem confidence="0.9931990625">
• We introduce PARALEX, an end-to-end open-
domain question answering system.
• We describe scalable learning algorithms that
induce general question templates and lexical
variants of entities and relations. These algo-
rithms require no manual annotation and can
be applied to large, noisy databases of rela-
tional triples.
• We evaluate PARALEX on the end-task of an-
swering questions from WikiAnswers using a
database of web extractions, and show that it
outperforms baseline systems.
• We release our learned lexicon and
question-paraphrase dataset to the
research community, available at
http://openie.cs.washington.edu.
</listItem>
<sectionHeader confidence="0.999364" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99911525">
Our work builds upon two major threads of re-
search in natural language processing: informa-
tion extraction (IE), and natural language inter-
faces to databases (NLIDB).
</bodyText>
<footnote confidence="0.940939">
1http://wiki.answers.com/
</footnote>
<bodyText confidence="0.999909697674419">
Research in IE has been moving towards the
goal of extracting facts from large text corpora,
across many domains, with minimal supervision
(Mintz et al., 2009; Hoffmann et al., 2010; Riedel
et al., 2010; Hoffmann et al., 2011; Banko et al.,
2007; Yao et al., 2012). While much progress
has been made in converting text into structured
knowledge, there has been little work on an-
swering natural language questions over these
databases. There has been some work on QA over
web text (Kwok et al., 2001; Brill et al., 2002), but
these systems do not operate over extracted rela-
tional data.
The NLIDB problem has been studied for
decades (Grosz et al., 1987; Katz, 1997). More
recently, researchers have created systems that
use machine learning techniques to automatically
construct question answering systems from data
(Zelle and Mooney, 1996; Popescu et al., 2004;
Zettlemoyer and Collins, 2005; Clarke et al., 2010;
Liang et al., 2011). These systems have the abil-
ity to handle questions with complex semantics
on small domain-specific databases like GeoQuery
(Tang and Mooney, 2001) or subsets of Freebase
(Cai and Yates, 2013), but have yet to scale to the
task of general, open-domain question answering.
In contrast, our system answers questions with
more limited semantics, but does so at a very large
scale in an open-domain manner. Some work has
been made towards more general databases like
DBpedia (Yahya et al., 2012; Unger et al., 2012),
but these systems rely on hand-written templates
for question interpretation.
The learning algorithms presented in this pa-
per are similar to algorithms used for paraphrase
extraction from sentence-aligned corpora (Barzi-
lay and McKeown, 2001; Barzilay and Lee, 2003;
Quirk et al., 2004; Bannard and Callison-Burch,
2005; Callison-Burch, 2008; Marton et al., 2009).
However, we use a paraphrase corpus for extract-
ing lexical items relating natural language patterns
to database concepts, as opposed to relationships
between pairs of natural language utterances.
</bodyText>
<sectionHeader confidence="0.919498" genericHeader="method">
3 Overview of the Approach
</sectionHeader>
<bodyText confidence="0.9997745">
In this section, we give a high-level overview of
the rest of the paper.
Problem Our goal is to learn a function that will
map a natural language question x to a query z
over a database D. The database D is a collection
of assertions in the form r(e1, e2) where r is a bi-
</bodyText>
<page confidence="0.992544">
1609
</page>
<bodyText confidence="0.999930075">
nary relation from a vocabulary R, and e1 and e2
are entities from a vocabulary E. We assume that
the elements of R and E are human-interpretable
strings like population or new-york. In our
experiments, R and E contain millions of en-
tries representing ambiguous and overlapping con-
cepts. The database is equipped with a simple in-
terface that accepts queries in the form r(?, e2) or
r(e1, ?). When executed, these queries return all
entities e that satisfy the given relationship. Thus,
our task is to find the query z that best captures the
semantics of the question x.
Model The question answering model includes a
lexicon and a linear ranking function. The lexicon
L associates natural language patterns to database
concepts, thereby defining the space of queries
that can be derived from the input question (see
Table 2). Lexical entries can pair strings with
database entities (nyc and new-york), strings with
database relations (big and population), or ques-
tion patterns with templated database queries (how
r is e? and r(?,e)). We describe this model in
more detail in Section 4.
Learning The learning algorithm induces a lex-
icon L and estimates the parameters 0 of the
linear ranking function. We learn L by boot-
strapping from an initial seed lexicon L0 over a
corpus of question paraphrases C = {(x, x&apos;) :
x&apos; is a paraphrase of x}, like the examples in Ta-
ble 1. We estimate 0 by using the initial lexicon to
automatically label queries in the paraphrase cor-
pus, as described in Section 5.2. The final result
is a scalable learning algorithm that requires no
manual annotation of questions.
Evaluation In Section 8, we evaluate our system
against various baselines on the end-task of ques-
tion answering against a large database of facts
extracted from the web. We use held-out known-
answerable questions from WikiAnswers as a test
set.
</bodyText>
<sectionHeader confidence="0.994791" genericHeader="method">
4 Question Answering Model
</sectionHeader>
<bodyText confidence="0.9998845">
To answer questions, we must find the best query
for a given natural language question.
</bodyText>
<subsectionHeader confidence="0.995526">
4.1 Lexicon and Derivations
</subsectionHeader>
<bodyText confidence="0.999894">
To define the space of possible queries, PARALEX
uses a lexicon L that encodes mappings from nat-
ural language to database concepts (entities, rela-
tions, and queries). Each entry in L is a pair (p, d)
</bodyText>
<table confidence="0.998526">
Entry Type NL Pattern DB Concept
Entity nyc new-york
Relation big population
Question (1-Arg.) how big is e population(?, e)
Question (2-Arg.) how r is e r(?, e)
</table>
<tableCaption confidence="0.998947">
Table 2: Example lexical entries.
</tableCaption>
<bodyText confidence="0.997462228571429">
where p is a pattern and d is an associated database
concept. Table 2 gives examples of the entry types
in L: entity, relation, and question patterns.
Entity patterns match a contiguous string of
words and are associated with some database en-
tity e E E.
Relation patterns match a contiguous string of
words and are associated with a relation r E R and
an argument ordering (e.g. the string child could
be modeled as either parent-of or child-of with
opposite argument ordering).
Question patterns match an entire question
string, with gaps that recursively match an en-
tity or relation patterns. Question patterns are as-
sociated with a templated database query, where
the values of the variables are determined by the
matched entity and relation patterns. A question
pattern may be 1-Argument, with a variable for
an entity pattern, or 2-Argument, with variables
for an entity pattern and a relation pattern. A 2-
argument question pattern may also invert the ar-
gument order of the matched relation pattern, e.g.
who r e? may have the opposite argument order
of who did e r?
The lexicon is used to generate a derivation y
from an input question x to a database query z.
For example, the entries in Table 2 can be used
to make the following derivation from the ques-
tion How big is nyc? to the query population(?,
new-york):
This derivation proceeds in two steps: first match-
ing a question form like How r is e? and then
mapping big to population and nyc to new-york.
Factoring the derivation this way allows the lexi-
cal entries for big and nyc to be reused in semanti-
</bodyText>
<page confidence="0.929528">
1610
</page>
<bodyText confidence="0.999742">
cally equivalent variants like nyc how big is it? or
approximately how big is nyc? This factorization
helps the system generalize to novel questions that
do not appear in the training set.
We model a derivation as a set of (pi, di) pairs,
where each pi matches a substring of x, the sub-
strings cover all words in x, and the database con-
cepts di compose to form z. Derivations are rooted
at either a 1-argument or 2-argument question en-
try and have entity or relation entries as leaves.
</bodyText>
<subsectionHeader confidence="0.997374">
4.2 Linear Ranking Function
</subsectionHeader>
<bodyText confidence="0.9999897">
In general, multiple queries may be derived from a
single input question x using a lexicon L. Many of
these derivations may be incorrect due to noise in
L. Given a question x, we consider all derivations
y and score them with B · O(x, y), where O(x, y) is
a n-dimensional feature representation and B is a
n-dimensional parameter vector. Let GEN(x; L)
be the set of all derivations y that can be generated
from x using L. The best derivation y*(x) accord-
ing to the model (B, L) is given by:
</bodyText>
<equation confidence="0.997938">
y*(x) = arg max B · O(x, y)
yEGEN(x;L)
</equation>
<bodyText confidence="0.9999138">
The best query z*(x) can be computed directly
from the derivation y*(x).
Computing the set GEN(x; L) involves finding
all 1-Argument and 2-Argument question patterns
that match x, and then enumerating all possible
database concepts that match entity and relation
strings. When the database and lexicon are large,
this becomes intractable. We prune GEN(x; L)
using the model parameters B by only considering
the N-best question patterns that match x, before
additionally enumerating any relations or entities.
For the end-to-end QA task, we return a ranked
list of answers from the k highest scoring queries.
We score an answer a with the highest score of all
derivations that generate a query with answer a.
</bodyText>
<sectionHeader confidence="0.982186" genericHeader="method">
5 Learning
</sectionHeader>
<bodyText confidence="0.999432833333333">
PARALEX uses a two-part learning algorithm; it
first induces an overly general lexicon (Section
5.1) and then learns to score derivations to increase
accuracy (Section 5.2). Both algorithms rely on an
initial seed lexicon, which we describe in Section
7.4.
</bodyText>
<subsectionHeader confidence="0.99488">
5.1 Lexical Learning
</subsectionHeader>
<bodyText confidence="0.999385717391304">
The lexical learning algorithm constructs a lexi-
con L from a corpus of question paraphrases C =
1(x, x&apos;) : x&apos; is a paraphrase of x}, where we as-
sume that all paraphrased questions (x, x&apos;) can be
answered with a single, initially unknown, query
(Table 1 shows example paraphrases). This as-
sumption allows the algorithm to generalize from
the initial seed lexicon L0, greatly increasing the
lexical coverage.
As an example, consider the paraphrase pair x
= What is the population of New York? and x&apos; =
How big is NYC? Suppose x can be mapped to a
query under L0 using the following derivation y:
what is the r of e = r(?, e)
population = population
new york = new-york
We can induce new lexical items by aligning the
patterns used in y to substrings in x&apos;. For example,
suppose we know that the words in (x, x&apos;) align in
the following way:
Using this information, we can hypothesize that
how r is e, big, and nyc should have the same in-
terpretations as what is the r of e, population, and
new york, respectively, and create the new entries:
how rise = r(?, e)
big = population
nyc = new-york
We call this procedure InduceLex(x, x&apos;, y, A),
which takes a paraphrase pair (x, x&apos;), a derivation
y of x, and a word alignment A, and returns a new
set of lexical entries. Before formally describing
InduceLex we need to introduce some definitions.
Let n and n&apos; be the number of words in x and
x&apos;. Let [k] denote the set of integers 11, ... , k}.
A word alignment A between x and x&apos; is a subset
of [n] x [n&apos;]. A phrase alignment is a pair of in-
dex sets (I, I&apos;) where I C_ [n] and I&apos; C_ [n&apos;]. A
phrase alignment (I, I&apos;) is consistent with a word
alignment A if for all (i, i&apos;) E A, i E I if and only
if i&apos; E I&apos;. In other words, a phrase alignment is
consistent with a word alignment if the words in
the phrases are aligned only with each other, and
not with any outside words.
We will now define InduceLex(x, x&apos;, y, A) for
the case where the derivation y consists of a 2-
argument question entry (pq, dq), a relation entry
</bodyText>
<page confidence="0.976755">
1611
</page>
<table confidence="0.959065571428572">
function LEARNLEXICON
Inputs:
- A corpus C of paraphrases (x, x&apos;). (Table 1)
- An initial lexicon L0 of (pattern, concept) pairs.
- A word alignment function WordAlign(x, x&apos;).
(Section 6)
- Initial parameters B0.
</table>
<tableCaption confidence="0.9725762">
- A function GEN(x; L) that derives queries from
a question x using lexicon L. (Section 4)
- A function InduceLex(x, x&apos;, y, A) that induces
new lexical items from the paraphrases (x, x&apos;) us-
ing their word alignment A and a derivation y of
</tableCaption>
<figure confidence="0.967562777777778">
x. (Section 5.1)
Output: A learned lexicon L.
L = {}
for all x, x&apos; G C do
if GEN(x; L0) is not empty then
A WordAlign(x, x&apos;)
y* arg maxyEGEN(x;LO) B0 · φ(x, y)
L L U InduceLex(x, x&apos;, y*, A)
return L
</figure>
<figureCaption confidence="0.999851">
Figure 1: Our lexicon learning algorithm.
</figureCaption>
<bodyText confidence="0.98753425">
(pr, dr), and an entity entry (pe, de), as shown in
the example above.2 InduceLex returns the set of
all triples (p0q, dq), (p0r, dr), (p0e, de) such that for
all p0q, p0r, p0e such that
</bodyText>
<listItem confidence="0.9830334">
1. p0q, p0r, p0e are a partition of the words in x0.
2. The phrase pairs (pq, p0q), (pr, p0r), (pe, p0e)
are consistent with the word alignment A.
3. The p0r and p0e are contiguous spans of words
in x0.
</listItem>
<bodyText confidence="0.996937">
Figure 1 shows the complete lexical learning al-
gorithm. In practice, for a given paraphrase pair
(x, x0) and alignment A, InduceLex will gener-
ate multiple sets of new lexical entries, resulting
in a lexicon with millions of entries. We use an
existing statistical word alignment algorithm for
WordAlign (see Section 6). In the next section,
we will introduce a scalable approach for learning
to score derivations to filter out lexical items that
generalize poorly.
</bodyText>
<subsectionHeader confidence="0.995971">
5.2 Parameter Learning
</subsectionHeader>
<bodyText confidence="0.981967447368421">
Parameter learning is necessary for filtering out
derivations that use incorrect lexical entries like
new mexico = mexico, which arise from noise in
the paraphrases and noise in the word alignment.
2InduceLex has similar behavior for the other type of
derivation, which consists of a 1-argument question entry
(p9, d9) and an entity (pe, de).
We use the hidden variable structured perceptron
algorithm to learn θ from a list of (question x,
query z) training examples. We adopt the itera-
tive parameter mixing variation of the perceptron
(McDonald et al., 2010) to scale to a large number
of training examples.
Figure 2 shows the parameter learning algo-
rithm. The parameter learning algorithm operates
in two stages. First, we use the initial lexicon
L0 to automatically generate (question x, query z)
training examples from the paraphrase corpus C.
Then we feed the training examples into the learn-
ing algorithm, which estimates parameters for the
learned lexicon L.
Because the number of training examples is
large, we adopt a parallel perceptron approach.
We first randomly partition the training data T
into K equally-sized subsets T1, ... , TK. We then
perform perceptron learning on each partition in
parallel. Finally, the learned weights from each
parallel run are aggregated by taking a uniformly
weighted average of each partition’s parameter
vector. This procedure is repeated for T iterations.
The training data consists of (question x, query
z) pairs, but our scoring model is over (question
x, derivation y) pairs, which are unobserved in
the training data. We use a hidden variable ver-
sion of the perceptron algorithm (Collins, 2002),
where the model parameters are updated using the
highest scoring derivation y∗ that will generate the
correct query z using the learned lexicon L.
</bodyText>
<sectionHeader confidence="0.998536" genericHeader="method">
6 Data
</sectionHeader>
<bodyText confidence="0.999903428571429">
For our database D, we use the publicly avail-
able set of 15 million REVERB extractions (Fader
et al., 2011).3 The database consists of a set
of triples r(e1, e2) over a vocabulary of ap-
proximately 600K relations and 2M entities, ex-
tracted from the ClueWeb09 corpus.4 The RE-
VERB database contains a large cross-section of
general world-knowledge, and thus is a good
testbed for developing an open-domain QA sys-
tem. However, the extractions are noisy, unnor-
malized (e.g., the strings obama, barack-obama,
and president-obama all appear as distinct en-
tities), and ambiguous (e.g., the relation born-in
contains facts about both dates and locations).
</bodyText>
<footnote confidence="0.9541598">
3We used version 1.1, downloaded from http://
reverb.cs.washington.edu/.
4The full set of REVERB extractions from ClueWeb09
contains over six billion triples. We used the smaller subset
of triples to simplify our experiments.
</footnote>
<page confidence="0.924518">
1612
</page>
<table confidence="0.890132111111111">
function LEARNPARAMETERS
Inputs:
- A corpus C of paraphrases (x, x&apos;). (Table 1)
- An initial lexicon L0 of (pattern, db concept)
pairs.
- A learned lexicon L of (pattern, db concept) pairs.
- Initial parameters B0.
- Number of perceptron epochs T.
- Number of training-data shards K.
</table>
<tableCaption confidence="0.7732325">
- A function GEN(x; L) that derives queries from
a question x using lexicon L. (Section 4)
</tableCaption>
<figure confidence="0.681157809523809">
- A function PerceptronEpoch(T, B, L) that runs
a single epoch of the hidden-variable structured
perceptron algorithm on training set T with initial
parameters B, returning a new parameter vector
B&apos;. (Section 5.2)
Output: A learned parameter vector B.
// Step 1: Generate Training Examples T
T = {}
for all x, x&apos; G C do
if GEN(x; L0) is not empty then
y* arg maxyEGEN(x;Lo) B0 · φ(x, y)
z* query of y*
Add (x&apos;, z*) to T
// Step 2: Learn Parameters from T
Randomly partition T into shards T1, ... , TK
fort = 1 ... T do
// Executed on k processors
Bk,t = PerceptronEpoch(Tk, Bt_1, L)
// Average the weights
Bt = K Ek Bk,t
return BT
</figure>
<figureCaption confidence="0.999576">
Figure 2: Our parameter learning algorithm.
</figureCaption>
<bodyText confidence="0.999854761904762">
Our paraphrase corpus C was constructed from
the collaboratively edited QA site WikiAnswers.
WikiAnswers users can tag pairs of questions as
alternate wordings of each other. We harvested
a set of 18M of these question-paraphrase pairs,
with 2.4M distinct questions in the corpus.
To estimate the precision of the paraphrase cor-
pus, we randomly sampled a set of 100 pairs and
manually tagged them as ‘paraphrase’ or ‘not-
paraphrase.’ We found that 55% of the sampled
pairs are valid paraphrased. Most of the incorrect
paraphrases were questions that were related, but
not paraphrased e.g. How big is the biggest mall?
and Most expensive mall in the world?
We word-aligned each paraphrase pair using
the MGIZA++ implementation of IBM Model 4
(Och and Ney, 2000; Gao and Vogel, 2008). The
word-alignment algorithm was run in each direc-
tion (x, x&apos;) and (x&apos;, x) and then combined using
the grow-diag-final-and heuristic (Koehn et al.,
2003).
</bodyText>
<sectionHeader confidence="0.996148" genericHeader="method">
7 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999278">
We compare the following systems:
</bodyText>
<listItem confidence="0.993353571428571">
• PARALEX: the full system, using the lexical
learning and parameter learning algorithms
from Section 5.
• NoParam: PARALEX without the learned
parameters.
• InitOnly: PARALEX using only the initial
seed lexicon.
</listItem>
<bodyText confidence="0.847732">
We evaluate the systems’ performance on the end-
task of QA on WikiAnswers questions.
</bodyText>
<subsectionHeader confidence="0.999039">
7.1 Test Set
</subsectionHeader>
<bodyText confidence="0.992684567567568">
A major challenge for evaluation is that the RE-
VERB database is incomplete. A system may cor-
rectly map a test question to a valid query, only
to return 0 results when executed against the in-
complete database. We factor out this source of
error by semi-automatically constructing a sample
of questions that are known to be answerable us-
ing the REVERB database, and thus allows for a
meaningful comparison on the task of question un-
derstanding.
To create the evaluation set, we identified ques-
tions x in a held out portion of the WikiAnswers
corpus such that (1) x can be mapped to some
query z using an initial lexicon (described in Sec-
tion 7.4), and (2) when z is executed against the
database, it returns at least one answer. We then
add x and all of its paraphrases as our evaluation
set. For example, the question What is the lan-
guage of Hong-Kong satisfies these requirements,
so we added these questions to the evaluation set:
What is the language of Hong-Kong?
What language do people in hong kong use?
How many languages are spoken in hong kong?
How many languages hong kong people use?
In Hong Kong what language is spoken?
Language of Hong-kong?
This methodology allows us to evaluate the sys-
tems’ ability to handle syntactic and lexical varia-
tions of questions that should have the same an-
swers. We created 37 question clusters, result-
ing in a total of 698 questions. We removed all
of these questions and their paraphrases from the
training set. We also manually filtered out any in-
correct paraphrases that appeared in the test clus-
ters.
We then created a gold-standard set of (x, a, l)
triples, where x is a question, a is an answer, and l
</bodyText>
<page confidence="0.899375">
1613
</page>
<table confidence="0.3821468">
Question Pattern Database Query
who r e r(?, e)
what r e r(?, e)
who does e r r(e, ?)
what does e r r(e, ?)
</table>
<bodyText confidence="0.998834666666667">
what is the r of e r(?, e)
who is the r of e r(?, e)
what is r by e r(e, ?)
who is e’s r r(?, e)
what is e’s r r(?, e)
who is r by e r(e, ?)
when did e r r-in(e, ?)
when did e r r-on(e, ?)
when was e r r-in(e, ?)
when was e r r-on(e, ?)
where was e r r-in(e, ?)
where did e r r-in(e, ?)
</bodyText>
<tableCaption confidence="0.943085">
Table 3: The question patterns used in the initial
lexicon L0.
</tableCaption>
<bodyText confidence="0.999763363636364">
is a label (correct or incorrect). To create the gold-
standard, we first ran each system on the evalua-
tion questions to generate (x, a) pairs. Then we
manually tagged each pair with a label l. This
resulted in a set of approximately 2, 000 human
judgments. If (x, a) was tagged with label l and x&apos;
is a paraphrase of x, we automatically added the
labeling (x&apos;, a, l), since questions in the same clus-
ter should have the same answer sets. This process
resulted in a gold standard set of approximately
48, 000 (x, a,l) triples.
</bodyText>
<subsectionHeader confidence="0.984085">
7.2 Metrics
</subsectionHeader>
<bodyText confidence="0.999977454545455">
We use two types of metrics to score the systems.
The first metric measures the precision and recall
of each system’s highest ranked answer. Precision
is the fraction of predicted answers that are cor-
rect and recall is the fraction of questions where a
correct answer was predicted. The second metric
measures the accuracy of the entire ranked answer
set returned for a question. We compute the mean
average precision (MAP) of each systems’ output,
which measures the average precision over all lev-
els of recall.
</bodyText>
<subsectionHeader confidence="0.961097">
7.3 Features and Settings
</subsectionHeader>
<bodyText confidence="0.999131">
The feature representation O(x, y) consists of in-
dicator functions for each lexical entry (p, d) ∈ L
used in the derivation y. For parameter learning,
we use an initial weight vector θ0 = 0, use T = 20
</bodyText>
<table confidence="0.99803675">
F1 Precision Recall MAP
PARALEX 0.54 0.77 0.42 0.22
NoParam 0.30 0.53 0.20 0.08
InitOnly 0.18 0.84 0.10 0.04
</table>
<tableCaption confidence="0.972675">
Table 4: Performance on WikiAnswers questions
known to be answerable using REVERB.
</tableCaption>
<table confidence="0.999925666666667">
F1 Precision Recall MAP
PARALEX 0.54 0.77 0.42 0.22
No 2-Arg. 0.40 0.86 0.26 0.12
No 1-Arg 0.35 0.81 0.22 0.11
No Relations 0.18 0.84 0.10 0.03
No Entity 0.36 0.55 0.27 0.15
</table>
<tableCaption confidence="0.999189">
Table 5: Ablation of the learned lexical items.
</tableCaption>
<figureCaption confidence="0.958876">
Figure 3: Precision-recall curves for PARALEX
with and without 2-argument question patterns.
</figureCaption>
<bodyText confidence="0.9987295">
iterations and shard the training data into K = 10
pieces. We limit each system to return the top 100
database queries for each test sentence. All input
words are lowercased and lemmatized.
</bodyText>
<subsectionHeader confidence="0.986484">
7.4 Initial Lexicon
</subsectionHeader>
<bodyText confidence="0.999988888888889">
Both the lexical learning and parameter learning
algorithms rely on an initial seed lexicon L0. The
initial lexicon allows the learning algorithms to
bootstrap from the paraphrase corpus.
We construct L0 from a set of 16 hand-written
2-argument question patterns and the output of the
identity transformation on the entity and relation
strings in the database. Table 3 shows the question
patterns that were used in L0.
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="evaluation">
8 Results
</sectionHeader>
<bodyText confidence="0.999897166666667">
Table 4 shows the performance of PARALEX on
the test questions. PARALEX outperforms the
baseline systems in terms of both F1 and MAP.
The lexicon-learning algorithm boosts the recall
by a factor of 4 over the initial lexicon, show-
ing the utility of the InduceLex algorithm. The
</bodyText>
<figure confidence="0.988936166666667">
0 0 0 1 0 2 0 0 0
Recall
Precision
0
0
0
0
0
1 0
PARALEX
No 2-Arg.
Initial Lexicon
</figure>
<page confidence="0.962702">
1614
</page>
<table confidence="0.945935">
String Learned Database Relations for String
get rid of treatment-for, cause, get-rid-of, cure-for, easiest-way-to-get-rid-of
word word-for, slang-term-for, definition-of, meaning-of, synonym-of
speak speak-language-in,language-speak-in,principal-language-of, dialect-of
useful main-use-of, purpose-of, importance-of, property-of, usefulness-of
String Learned Database Entities for String
smoking smoking, tobacco-smoking, cigarette, smoking-cigar, smoke, quit-smoking
radiation radiation, electromagnetic-radiation, nuclear-radiation
vancouver vancouver, vancouver-city, vancouver-island, vancouver-british-columbia
protein protein, protein-synthesis, plasma-protein, monomer, dna
</table>
<tableCaption confidence="0.999322">
Table 6: Examples of relation and entity synonyms learned from the WikiAnswers paraphrase corpus.
</tableCaption>
<bodyText confidence="0.999816482758621">
parameter-learning algorithm also results in a
large gain in both precision and recall: InduceLex
generates a noisy set of patterns, so selecting the
best query for a question is more challenging.
Table 5 shows an ablation of the different types
of lexical items learned by PARALEX. For each
row, we removed the learned lexical items from
each of the types described in Section 4, keeping
only the initial seed lexical items. The learned 2-
argument question templates significantly increase
the recall of the system. This increased recall
came at a cost, lowering precision from 0.86 to
0.77. Thresholding the query score allows us to
trade precision for recall, as shown in Figure 3.
Table 6 shows some examples of the learned en-
tity and relation synonyms.
The 2-argument question templates help PAR-
ALEX generalize over different variations of the
same question, like the test questions shown in
Table 7. For each question, PARALEX combines
a 2-argument question template (shown below the
questions) with the rules celebrate = holiday-of
and christians = christians to derive a full
query. Factoring the problem this way allows
PARALEX to reuse the same rules in different
syntactic configurations. Note that the imperfect
training data can lead to overly-specific templates
like what are the religious r of e, which can lower
accuracy.
</bodyText>
<sectionHeader confidence="0.992551" genericHeader="evaluation">
9 Error Analysis
</sectionHeader>
<bodyText confidence="0.999298625">
To understand how close we are to the goal of
open-domain QA, we ran PARALEX on an unre-
stricted sample of questions from WikiAnswers.
We used the same methodology as described in the
previous section, where PARALEX returns the top
answer for each question using REVERB.
We found that PARALEX performs significantly
worse on this dataset, with recall maxing out at ap-
</bodyText>
<subsectionHeader confidence="0.48156">
Celebrations for Christians?
r for e?
</subsectionHeader>
<bodyText confidence="0.8273726875">
Celebrations of Christians?
rofe?
What are some celebrations for Christians?
what are some r for e?
What are some celebrations of the Christians?
what are some r of e?
What are some of Christians celebrations?
what are some of e r?
What celebrations do Christians do?
what r do e do?
What did Christians celebrate?
what did e r?
What are the religious celebrations of Christians?
what are the religious r of e?
What celebration do Christians celebrate?
what r do e celebrate?
</bodyText>
<tableCaption confidence="0.651552333333333">
Table 7: Questions from the test set with 2-
argument question patterns that PARALEX used to
derive a correct query.
</tableCaption>
<bodyText confidence="0.999839294117647">
proximately 6% of the questions answered at pre-
cision 0.4. This is not surprising, since the test
questions are not restricted to topics covered by
the REVERB database, and may be too complex to
be answered by any database of relational triples.
We performed an error analysis on a sample
of 100 questions that were either incorrectly an-
swered or unanswered. We examined the can-
didate queries that PARALEX generated for each
question and tagged each query as correct (would
return a valid answer given a correct and com-
plete database) or incorrect. Because the input
questions are unrestricted, we also judged whether
the questions could be faithfully represented as a
r(?, e) or r(e, ?) query over the database vocabu-
lary. Table 8 shows the distribution of errors.
The largest source of error (36%) were on com-
</bodyText>
<page confidence="0.968249">
1615
</page>
<bodyText confidence="0.999196775510204">
plex questions that could not be represented as a
query for various reasons. We categorized these
questions into groups. The largest group (14%)
were questions that need n-ary or higher-order
database relations, for example How long does
it take to drive from Sacramento to Cancun? or
What do cats and dogs have in common? Approx-
imately 13% of the questions were how-to ques-
tions like How do you make axes in minecraft?
whose answers are a sequence of steps, instead
of a database entity. Lastly, 9% of the questions
require database operators like joins, for example
When were Bobby Orr’s children born?
The second largest source of error (32%) were
questions that could be represented as a query, but
where PARALEX was unable to derive any cor-
rect queries. For example, the question Things
grown on Nigerian farms? was not mapped to
any queries, even though the REVERB database
contains the relation grown-in and the entity
nigeria. We found that 13% of the incorrect
questions were cases where the entity was not rec-
ognized, 12% were cases where the relation was
not recognized, and 6% were cases where both the
entity and relation were not recognized.
We found that 28% of the errors were cases
where PARALEX derived a query that we judged to
be correct, but returned no answers when executed
against the database. For example, given the ques-
tion How much can a dietician earn? PARALEX
derived the query salary-of(?, dietician) but
this returned no answers in the REVERB database.
Finally, approximately 4% of the questions in-
cluded typos or were judged to be inscrutable, for
example Barovier hiriacy of evidence based for
pressure sore?
Discussion Our experiments show that the learn-
ing algorithms described in Section 5 allow PAR-
ALEX to generalize beyond an initial lexicon and
answer questions with significantly higher accu-
racy. Our error analysis on an unrestricted set of
WikiAnswers questions shows that PARALEX is
still far from the goal of truly high-recall, open-
domain QA. We found that many questions asked
on WikiAnswers are either too complex to be
mapped to a simple relational query, or are not
covered by the REVERB database. Further, ap-
proximately one third of the missing recall is due
to entity and relation recognition errors.
</bodyText>
<table confidence="0.997471333333333">
Incorrectly Answered/Unanswered Questions
36% Complex Questions
Need n-ary or higher-order relations (14%)
Answer is a set of instructions (13%)
Need database operators e.g. joins (9%)
32% Entity or Relation Recognition Errors
Entity recognition errors (13%)
Relation recognition errors (12%)
Entity &amp; relation recognition errors (7%)
28% Incomplete Database
Derived a correct query, but no answers
4% Typos/Inscrutable Questions
</table>
<tableCaption confidence="0.980785666666667">
Table 8: Error distribution of PARALEX on an un-
restricted sample of questions from the WikiAn-
swers dataset.
</tableCaption>
<sectionHeader confidence="0.989638" genericHeader="conclusions">
10 Conclusion
</sectionHeader>
<bodyText confidence="0.999995736842105">
We introduced a new learning approach that in-
duces a complete question-answering system from
a large corpus of noisy question-paraphrases. Us-
ing only a seed lexicon, the approach automat-
ically learns a lexicon and linear ranking func-
tion that demonstrated high accuracy on a held-out
evaluation set.
A number of open challenges remain. First,
precision could likely be improved by adding
new features to the ranking function. Second,
we would like to generalize the question under-
standing framework to produce more complex
queries, constructed within a compositional se-
mantic framework, but without sacrificing scala-
bility. Third, we would also like to extend the
system with other large databases like Freebase or
DBpedia. Lastly, we believe that it would be pos-
sible to leverage the user-provided answers from
WikiAnswers as a source of supervision.
</bodyText>
<sectionHeader confidence="0.997489" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999607875">
This research was supported in part by ONR grant
N00014-11-1-0294, DARPA contract FA8750-09-
C-0179, a gift from Google, a gift from Vulcan
Inc., and carried out at the University of Washing-
ton’s Turing Center. We would like to thank Yoav
Artzi, Tom Kwiatkowski, Yuval Marton, Mausam,
Dan Weld, and the anonymous reviewers for their
helpful comments.
</bodyText>
<page confidence="0.988999">
1616
</page>
<sectionHeader confidence="0.990046" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999718155963302">
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open Information Extraction from the Web. In Pro-
ceedings of the 20th international joint conference
on Artifical intelligence.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics.
Regina Barzilay and Lillian Lee. 2003. Learning
to Paraphrase: An Unsupervised Approach Using
Multiple-Sequence Alignment. In Proceedings of
the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics.
Regina Barzilay and Kathleen R. McKeown. 2001.
Extracting Paraphrases from a Parallel Corpus. In
Proceedings of the 39th Annual Meeting on Associ-
ation for Computational Linguistics.
Eric Brill, Susan Dumais, and Michele Banko. 2002.
An Analysis of the AskMSR Question-Answering
System. In Proceedings of Empirical Methods in
Natural Language Processing.
Qingqing Cai and Alexander Yates. 2013. Large-scale
Semantic Parsing via Schema Matching and Lexicon
Extension. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics.
Chris Callison-Burch. 2008. Syntactic Constraints
on Paraphrases Extracted from Parallel Corpora. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving Semantic Parsing from
the World’s Response. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Exper-
iments with Perceptron Algorithms. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying Relations for Open Information
Extraction. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Pro-
cessing.
Qin Gao and Stephan Vogel. 2008. Parallel Imple-
mentations of Word Alignment Tool. In Proc. of the
ACL 2008 Software Engineering, Testing, and Qual-
ity Assurance Workshop.
Barbara J. Grosz, Douglas E. Appelt, Paul A. Mar-
tin, and Fernando C. N. Pereira. 1987. TEAM:
An Experiment in the Design of Transportable
Natural-Language Interfaces. Artificial Intelligence,
32(2):173–243.
Raphael Hoffmann, Congle Zhang, and Daniel S.
Weld. 2010. Learning 5000 relational extractors.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-Based Weak Supervision for Informa-
tion Extraction of Overlapping Relations. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics.
Boris Katz. 1997. Annotating the World Wide Web
using Natural Language. In RIAO, pages 136–159.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics.
Cody Kwok, Oren Etzioni, and Daniel S. Weld. 2001.
Scaling Question Answering to the Web. ACM
Trans. Inf. Syst., 19(3):242–262.
Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning Dependency-Based Compositional Se-
mantics. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics.
Yuval Marton, Chris Callison-Burch, and Philip
Resnik. 2009. Improved Statistical Machine Trans-
lation Using Monolingually-Derived Paraphrases.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant Supervision for Relation Ex-
traction Without Labeled Data. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proceedings of the
38th Annual Meeting of the Association for Compu-
tational Linguistics.
Ana-Maria Popescu, Alex Armanasu, Oren Etzioni,
David Ko, and Alexander Yates. 2004. Modern
Natural Language Interfaces to Databases: Compos-
ing Statistical Parsing with Semantic Tractability. In
Proceedings of the Twentieth International Confer-
ence on Computational Linguistics.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual Machine Translation for Paraphrase
Generation. In Proceedings of the 2004 Conference
on Empirical Methods in Natural Language Pro-
cessing.
</reference>
<page confidence="0.827888">
1617
</page>
<reference confidence="0.999838882352941">
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling Relations and Their Mentions with-
out Labeled Text. In Proceedings of the 2010 Euro-
pean conference on Machine learning and Knowl-
edge Discovery in Databases.
Lappoon R. Tang and Raymond J. Mooney. 2001. Us-
ing Multiple Clause Constructors in Inductive Logic
Programming for Semantic Parsing.
Christina Unger, Lorenz B¨uhmann, Jens Lehmann,
Axel-Cyrille Ngonga Ngomo, Daniel Gerber, and
Philipp Cimiano. 2012. Template-Based Question
Answering over RDF Data. In Proceedings of the
21st World Wide Web Conference 2012.
Mohamed Yahya, Klaus Berberich, Shady Elbas-
suoni, Maya Ramanath, Volker Tresp, and Gerhard
Weikum. 2012. Natural Language Questions for
the Web of Data. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2012. Unsupervised Relation Discovery with Sense
Disambiguation. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to Parse Database Queries Using Inductive Logic
Programming. In Proceedings of the Thirteenth Na-
tional Conference on Artificial Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to Map Sentences to Logical Form: Struc-
tured Classification with Probabilistic Categorial
Grammars. In Proceedings of the 21st Conference
in Uncertainty in Artificial Intelligence.
</reference>
<page confidence="0.993061">
1618
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.968189">
<title confidence="0.999913">Paraphrase-Driven Learning for Open Question Answering</title>
<author confidence="0.999947">Anthony Fader Luke Zettlemoyer Oren</author>
<affiliation confidence="0.999833">Computer Science &amp; University of</affiliation>
<address confidence="0.997495">Seattle, WA</address>
<email confidence="0.994796">lsz,</email>
<abstract confidence="0.998567882352941">We study question answering as a machine learning problem, and induce a function that maps open-domain questions to queries over a database of web extractions. Given a large, community-authored, question-paraphrase corpus, we demonstrate that it is possible to learn a semantic lexicon and linear ranking function without manually annotating questions. Our approach automatically generalizes a seed lexicon and includes a scalable, parallelized perceptron parameter estimation scheme. Experiments show that our approach more than quadruples the recall of the seed lexicon, with only an 8% loss in precision.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderland</author>
<author>Matthew Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Open Information Extraction from the Web.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th international joint conference on Artifical intelligence.</booktitle>
<contexts>
<context position="1202" citStr="Banko et al., 2007" startWordPosition="175" endWordPosition="178">oach automatically generalizes a seed lexicon and includes a scalable, parallelized perceptron parameter estimation scheme. Experiments show that our approach more than quadruples the recall of the seed lexicon, with only an 8% loss in precision. 1 Introduction Open-domain question answering (QA) is a longstanding, unsolved problem. The central challenge is to automate every step of QA system construction, including gathering large databases and answering questions against these databases. While there has been significant work on large-scale information extraction (IE) from unstructured text (Banko et al., 2007; Hoffmann et al., 2010; Riedel et al., 2010), the problem of answering questions with the noisy knowledge bases that IE systems produce has received less attention. In this paper, we present an approach for learning to map questions to formal queries over a large, open-domain database of extracted facts (Fader et al., 2011). Our system learns from a large, noisy, questionparaphrase corpus, where question clusters have a common but unknown query, and can span a diverse set of topics. Table 1 shows example paraphrase clusters for a set of factual questions. Such data provides strong signal for </context>
<context position="6268" citStr="Banko et al., 2007" startWordPosition="980" endWordPosition="983">tperforms baseline systems. • We release our learned lexicon and question-paraphrase dataset to the research community, available at http://openie.cs.washington.edu. 2 Related Work Our work builds upon two major threads of research in natural language processing: information extraction (IE), and natural language interfaces to databases (NLIDB). 1http://wiki.answers.com/ Research in IE has been moving towards the goal of extracting facts from large text corpora, across many domains, with minimal supervision (Mintz et al., 2009; Hoffmann et al., 2010; Riedel et al., 2010; Hoffmann et al., 2011; Banko et al., 2007; Yao et al., 2012). While much progress has been made in converting text into structured knowledge, there has been little work on answering natural language questions over these databases. There has been some work on QA over web text (Kwok et al., 2001; Brill et al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to automatically construct question answering systems from data (Zelle and Mooney, 1996; Pope</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J. Cafarella, Stephen Soderland, Matthew Broadhead, and Oren Etzioni. 2007. Open Information Extraction from the Web. In Proceedings of the 20th international joint conference on Artifical intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with Bilingual Parallel Corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7789" citStr="Bannard and Callison-Burch, 2005" startWordPosition="1222" endWordPosition="1225">ave yet to scale to the task of general, open-domain question answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these systems rely on hand-written templates for question interpretation. The learning algorithms presented in this paper are similar to algorithms used for paraphrase extraction from sentence-aligned corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Marton et al., 2009). However, we use a paraphrase corpus for extracting lexical items relating natural language patterns to database concepts, as opposed to relationships between pairs of natural language utterances. 3 Overview of the Approach In this section, we give a high-level overview of the rest of the paper. Problem Our goal is to learn a function that will map a natural language question x to a query z over a database D. The database D is a collection of assertions in the form r(e1, e2) where r is a bi1609 nary relation from a vocabulary R, and e1 and e2 are en</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with Bilingual Parallel Corpora. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7735" citStr="Barzilay and Lee, 2003" startWordPosition="1214" endWordPosition="1217">ets of Freebase (Cai and Yates, 2013), but have yet to scale to the task of general, open-domain question answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these systems rely on hand-written templates for question interpretation. The learning algorithms presented in this paper are similar to algorithms used for paraphrase extraction from sentence-aligned corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Marton et al., 2009). However, we use a paraphrase corpus for extracting lexical items relating natural language patterns to database concepts, as opposed to relationships between pairs of natural language utterances. 3 Overview of the Approach In this section, we give a high-level overview of the rest of the paper. Problem Our goal is to learn a function that will map a natural language question x to a query z over a database D. The database D is a collection of assertions in the form r(e1, e2) where r is a bi1609 n</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Extracting Paraphrases from a Parallel Corpus.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7711" citStr="Barzilay and McKeown, 2001" startWordPosition="1209" endWordPosition="1213">ng and Mooney, 2001) or subsets of Freebase (Cai and Yates, 2013), but have yet to scale to the task of general, open-domain question answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these systems rely on hand-written templates for question interpretation. The learning algorithms presented in this paper are similar to algorithms used for paraphrase extraction from sentence-aligned corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Marton et al., 2009). However, we use a paraphrase corpus for extracting lexical items relating natural language patterns to database concepts, as opposed to relationships between pairs of natural language utterances. 3 Overview of the Approach In this section, we give a high-level overview of the rest of the paper. Problem Our goal is to learn a function that will map a natural language question x to a query z over a database D. The database D is a collection of assertions in the form r(e1, e</context>
</contexts>
<marker>Barzilay, McKeown, 2001</marker>
<rawString>Regina Barzilay and Kathleen R. McKeown. 2001. Extracting Paraphrases from a Parallel Corpus. In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Susan Dumais</author>
<author>Michele Banko</author>
</authors>
<title>An Analysis of the AskMSR Question-Answering System. In</title>
<date>2002</date>
<booktitle>Proceedings of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="6542" citStr="Brill et al., 2002" startWordPosition="1028" endWordPosition="1031">on extraction (IE), and natural language interfaces to databases (NLIDB). 1http://wiki.answers.com/ Research in IE has been moving towards the goal of extracting facts from large text corpora, across many domains, with minimal supervision (Mintz et al., 2009; Hoffmann et al., 2010; Riedel et al., 2010; Hoffmann et al., 2011; Banko et al., 2007; Yao et al., 2012). While much progress has been made in converting text into structured knowledge, there has been little work on answering natural language questions over these databases. There has been some work on QA over web text (Kwok et al., 2001; Brill et al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to automatically construct question answering systems from data (Zelle and Mooney, 1996; Popescu et al., 2004; Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2011). These systems have the ability to handle questions with complex semantics on small domain-specific databases like GeoQuery (Tang and Mooney, 2001) or subsets of Freebase (Cai and Yate</context>
</contexts>
<marker>Brill, Dumais, Banko, 2002</marker>
<rawString>Eric Brill, Susan Dumais, and Michele Banko. 2002. An Analysis of the AskMSR Question-Answering System. In Proceedings of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qingqing Cai</author>
<author>Alexander Yates</author>
</authors>
<title>Large-scale Semantic Parsing via Schema Matching and Lexicon Extension.</title>
<date>2013</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7150" citStr="Cai and Yates, 2013" startWordPosition="1122" endWordPosition="1125">t al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to automatically construct question answering systems from data (Zelle and Mooney, 1996; Popescu et al., 2004; Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2011). These systems have the ability to handle questions with complex semantics on small domain-specific databases like GeoQuery (Tang and Mooney, 2001) or subsets of Freebase (Cai and Yates, 2013), but have yet to scale to the task of general, open-domain question answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these systems rely on hand-written templates for question interpretation. The learning algorithms presented in this paper are similar to algorithms used for paraphrase extraction from sentence-aligned corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Quirk et al.,</context>
</contexts>
<marker>Cai, Yates, 2013</marker>
<rawString>Qingqing Cai and Alexander Yates. 2013. Large-scale Semantic Parsing via Schema Matching and Lexicon Extension. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Syntactic Constraints on Paraphrases Extracted from Parallel Corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="7811" citStr="Callison-Burch, 2008" startWordPosition="1226" endWordPosition="1227">neral, open-domain question answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these systems rely on hand-written templates for question interpretation. The learning algorithms presented in this paper are similar to algorithms used for paraphrase extraction from sentence-aligned corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Marton et al., 2009). However, we use a paraphrase corpus for extracting lexical items relating natural language patterns to database concepts, as opposed to relationships between pairs of natural language utterances. 3 Overview of the Approach In this section, we give a high-level overview of the rest of the paper. Problem Our goal is to learn a function that will map a natural language question x to a query z over a database D. The database D is a collection of assertions in the form r(e1, e2) where r is a bi1609 nary relation from a vocabulary R, and e1 and e2 are entities from a vocabula</context>
</contexts>
<marker>Callison-Burch, 2008</marker>
<rawString>Chris Callison-Burch. 2008. Syntactic Constraints on Paraphrases Extracted from Parallel Corpora. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Dan Goldwasser</author>
<author>Ming-Wei Chang</author>
<author>Dan Roth</author>
</authors>
<title>Driving Semantic Parsing from the World’s Response.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="6936" citStr="Clarke et al., 2010" startWordPosition="1088" endWordPosition="1091"> made in converting text into structured knowledge, there has been little work on answering natural language questions over these databases. There has been some work on QA over web text (Kwok et al., 2001; Brill et al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to automatically construct question answering systems from data (Zelle and Mooney, 1996; Popescu et al., 2004; Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2011). These systems have the ability to handle questions with complex semantics on small domain-specific databases like GeoQuery (Tang and Mooney, 2001) or subsets of Freebase (Cai and Yates, 2013), but have yet to scale to the task of general, open-domain question answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these systems rely on hand-written templates for question int</context>
</contexts>
<marker>Clarke, Goldwasser, Chang, Roth, 2010</marker>
<rawString>James Clarke, Dan Goldwasser, Ming-Wei Chang, and Dan Roth. 2010. Driving Semantic Parsing from the World’s Response. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="19560" citStr="Collins, 2002" startWordPosition="3300" endWordPosition="3301">lel perceptron approach. We first randomly partition the training data T into K equally-sized subsets T1, ... , TK. We then perform perceptron learning on each partition in parallel. Finally, the learned weights from each parallel run are aggregated by taking a uniformly weighted average of each partition’s parameter vector. This procedure is repeated for T iterations. The training data consists of (question x, query z) pairs, but our scoring model is over (question x, derivation y) pairs, which are unobserved in the training data. We use a hidden variable version of the perceptron algorithm (Collins, 2002), where the model parameters are updated using the highest scoring derivation y∗ that will generate the correct query z using the learned lexicon L. 6 Data For our database D, we use the publicly available set of 15 million REVERB extractions (Fader et al., 2011).3 The database consists of a set of triples r(e1, e2) over a vocabulary of approximately 600K relations and 2M entities, extracted from the ClueWeb09 corpus.4 The REVERB database contains a large cross-section of general world-knowledge, and thus is a good testbed for developing an open-domain QA system. However, the extractions are n</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Identifying Relations for Open Information Extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1528" citStr="Fader et al., 2011" startWordPosition="229" endWordPosition="232"> problem. The central challenge is to automate every step of QA system construction, including gathering large databases and answering questions against these databases. While there has been significant work on large-scale information extraction (IE) from unstructured text (Banko et al., 2007; Hoffmann et al., 2010; Riedel et al., 2010), the problem of answering questions with the noisy knowledge bases that IE systems produce has received less attention. In this paper, we present an approach for learning to map questions to formal queries over a large, open-domain database of extracted facts (Fader et al., 2011). Our system learns from a large, noisy, questionparaphrase corpus, where question clusters have a common but unknown query, and can span a diverse set of topics. Table 1 shows example paraphrase clusters for a set of factual questions. Such data provides strong signal for learning about lexical variation, but there are a number Who wrote the Winnie the Pooh books? Who is the author of winnie the pooh? What was the name of the authur of winnie the pooh? Who wrote the series of books for Winnie the poo? Who wrote the children’s storybook ‘Winnie the Pooh’? Who is poohs creator? What relieves a </context>
<context position="5007" citStr="Fader et al., 2011" startWordPosition="791" endWordPosition="794">ality queries for a small subset of the questions. The algorithm uses learned word alignments to aggressively generalize the seeds, producing a large set of possible lexical equivalences. We then learn a linear ranking model to filter the learned lexical equivalences, keeping only those that are likely to answer questions well in practice. Experimental results on 18 million paraphrase pairs gathered from WikiAnswers1 demonstrate the effectiveness of the overall approach. We performed an end-to-end evaluation against a database of 15 million facts automatically extracted from general web text (Fader et al., 2011). On known-answerable questions, the approach achieved 42% recall, with 77% precision, more than quadrupling the recall over a baseline system. In sum, we make the following contributions: • We introduce PARALEX, an end-to-end opendomain question answering system. • We describe scalable learning algorithms that induce general question templates and lexical variants of entities and relations. These algorithms require no manual annotation and can be applied to large, noisy databases of relational triples. • We evaluate PARALEX on the end-task of answering questions from WikiAnswers using a datab</context>
<context position="19823" citStr="Fader et al., 2011" startWordPosition="3344" endWordPosition="3347">g a uniformly weighted average of each partition’s parameter vector. This procedure is repeated for T iterations. The training data consists of (question x, query z) pairs, but our scoring model is over (question x, derivation y) pairs, which are unobserved in the training data. We use a hidden variable version of the perceptron algorithm (Collins, 2002), where the model parameters are updated using the highest scoring derivation y∗ that will generate the correct query z using the learned lexicon L. 6 Data For our database D, we use the publicly available set of 15 million REVERB extractions (Fader et al., 2011).3 The database consists of a set of triples r(e1, e2) over a vocabulary of approximately 600K relations and 2M entities, extracted from the ClueWeb09 corpus.4 The REVERB database contains a large cross-section of general world-knowledge, and thus is a good testbed for developing an open-domain QA system. However, the extractions are noisy, unnormalized (e.g., the strings obama, barack-obama, and president-obama all appear as distinct entities), and ambiguous (e.g., the relation born-in contains facts about both dates and locations). 3We used version 1.1, downloaded from http:// reverb.cs.wash</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying Relations for Open Information Extraction. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Gao</author>
<author>Stephan Vogel</author>
</authors>
<title>Parallel Implementations of Word Alignment Tool.</title>
<date>2008</date>
<booktitle>In Proc. of the ACL 2008 Software Engineering, Testing, and Quality Assurance Workshop.</booktitle>
<contexts>
<context position="22424" citStr="Gao and Vogel, 2008" startWordPosition="3783" endWordPosition="3786">of each other. We harvested a set of 18M of these question-paraphrase pairs, with 2.4M distinct questions in the corpus. To estimate the precision of the paraphrase corpus, we randomly sampled a set of 100 pairs and manually tagged them as ‘paraphrase’ or ‘notparaphrase.’ We found that 55% of the sampled pairs are valid paraphrased. Most of the incorrect paraphrases were questions that were related, but not paraphrased e.g. How big is the biggest mall? and Most expensive mall in the world? We word-aligned each paraphrase pair using the MGIZA++ implementation of IBM Model 4 (Och and Ney, 2000; Gao and Vogel, 2008). The word-alignment algorithm was run in each direction (x, x&apos;) and (x&apos;, x) and then combined using the grow-diag-final-and heuristic (Koehn et al., 2003). 7 Experimental Setup We compare the following systems: • PARALEX: the full system, using the lexical learning and parameter learning algorithms from Section 5. • NoParam: PARALEX without the learned parameters. • InitOnly: PARALEX using only the initial seed lexicon. We evaluate the systems’ performance on the endtask of QA on WikiAnswers questions. 7.1 Test Set A major challenge for evaluation is that the REVERB database is incomplete. A </context>
</contexts>
<marker>Gao, Vogel, 2008</marker>
<rawString>Qin Gao and Stephan Vogel. 2008. Parallel Implementations of Word Alignment Tool. In Proc. of the ACL 2008 Software Engineering, Testing, and Quality Assurance Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Douglas E Appelt</author>
<author>Paul A Martin</author>
<author>Fernando C N Pereira</author>
</authors>
<title>TEAM: An Experiment in the Design of Transportable Natural-Language Interfaces.</title>
<date>1987</date>
<journal>Artificial Intelligence,</journal>
<volume>32</volume>
<issue>2</issue>
<contexts>
<context position="6675" citStr="Grosz et al., 1987" startWordPosition="1051" endWordPosition="1054">ards the goal of extracting facts from large text corpora, across many domains, with minimal supervision (Mintz et al., 2009; Hoffmann et al., 2010; Riedel et al., 2010; Hoffmann et al., 2011; Banko et al., 2007; Yao et al., 2012). While much progress has been made in converting text into structured knowledge, there has been little work on answering natural language questions over these databases. There has been some work on QA over web text (Kwok et al., 2001; Brill et al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to automatically construct question answering systems from data (Zelle and Mooney, 1996; Popescu et al., 2004; Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2011). These systems have the ability to handle questions with complex semantics on small domain-specific databases like GeoQuery (Tang and Mooney, 2001) or subsets of Freebase (Cai and Yates, 2013), but have yet to scale to the task of general, open-domain question answering. In contrast, our system answers questions wit</context>
</contexts>
<marker>Grosz, Appelt, Martin, Pereira, 1987</marker>
<rawString>Barbara J. Grosz, Douglas E. Appelt, Paul A. Martin, and Fernando C. N. Pereira. 1987. TEAM: An Experiment in the Design of Transportable Natural-Language Interfaces. Artificial Intelligence, 32(2):173–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Daniel S Weld</author>
</authors>
<title>Learning 5000 relational extractors.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1225" citStr="Hoffmann et al., 2010" startWordPosition="179" endWordPosition="182">eneralizes a seed lexicon and includes a scalable, parallelized perceptron parameter estimation scheme. Experiments show that our approach more than quadruples the recall of the seed lexicon, with only an 8% loss in precision. 1 Introduction Open-domain question answering (QA) is a longstanding, unsolved problem. The central challenge is to automate every step of QA system construction, including gathering large databases and answering questions against these databases. While there has been significant work on large-scale information extraction (IE) from unstructured text (Banko et al., 2007; Hoffmann et al., 2010; Riedel et al., 2010), the problem of answering questions with the noisy knowledge bases that IE systems produce has received less attention. In this paper, we present an approach for learning to map questions to formal queries over a large, open-domain database of extracted facts (Fader et al., 2011). Our system learns from a large, noisy, questionparaphrase corpus, where question clusters have a common but unknown query, and can span a diverse set of topics. Table 1 shows example paraphrase clusters for a set of factual questions. Such data provides strong signal for learning about lexical </context>
<context position="6204" citStr="Hoffmann et al., 2010" startWordPosition="968" endWordPosition="971">ikiAnswers using a database of web extractions, and show that it outperforms baseline systems. • We release our learned lexicon and question-paraphrase dataset to the research community, available at http://openie.cs.washington.edu. 2 Related Work Our work builds upon two major threads of research in natural language processing: information extraction (IE), and natural language interfaces to databases (NLIDB). 1http://wiki.answers.com/ Research in IE has been moving towards the goal of extracting facts from large text corpora, across many domains, with minimal supervision (Mintz et al., 2009; Hoffmann et al., 2010; Riedel et al., 2010; Hoffmann et al., 2011; Banko et al., 2007; Yao et al., 2012). While much progress has been made in converting text into structured knowledge, there has been little work on answering natural language questions over these databases. There has been some work on QA over web text (Kwok et al., 2001; Brill et al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to automatically construct qu</context>
</contexts>
<marker>Hoffmann, Zhang, Weld, 2010</marker>
<rawString>Raphael Hoffmann, Congle Zhang, and Daniel S. Weld. 2010. Learning 5000 relational extractors. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Xiao Ling</author>
<author>Luke Zettlemoyer</author>
<author>Daniel S Weld</author>
</authors>
<title>Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6248" citStr="Hoffmann et al., 2011" startWordPosition="976" endWordPosition="979">ns, and show that it outperforms baseline systems. • We release our learned lexicon and question-paraphrase dataset to the research community, available at http://openie.cs.washington.edu. 2 Related Work Our work builds upon two major threads of research in natural language processing: information extraction (IE), and natural language interfaces to databases (NLIDB). 1http://wiki.answers.com/ Research in IE has been moving towards the goal of extracting facts from large text corpora, across many domains, with minimal supervision (Mintz et al., 2009; Hoffmann et al., 2010; Riedel et al., 2010; Hoffmann et al., 2011; Banko et al., 2007; Yao et al., 2012). While much progress has been made in converting text into structured knowledge, there has been little work on answering natural language questions over these databases. There has been some work on QA over web text (Kwok et al., 2001; Brill et al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to automatically construct question answering systems from data (Zelle an</context>
</contexts>
<marker>Hoffmann, Zhang, Ling, Zettlemoyer, Weld, 2011</marker>
<rawString>Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boris Katz</author>
</authors>
<title>Annotating the World Wide Web using Natural Language. In</title>
<date>1997</date>
<booktitle>RIAO,</booktitle>
<pages>136--159</pages>
<contexts>
<context position="6688" citStr="Katz, 1997" startWordPosition="1055" endWordPosition="1056">racting facts from large text corpora, across many domains, with minimal supervision (Mintz et al., 2009; Hoffmann et al., 2010; Riedel et al., 2010; Hoffmann et al., 2011; Banko et al., 2007; Yao et al., 2012). While much progress has been made in converting text into structured knowledge, there has been little work on answering natural language questions over these databases. There has been some work on QA over web text (Kwok et al., 2001; Brill et al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to automatically construct question answering systems from data (Zelle and Mooney, 1996; Popescu et al., 2004; Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2011). These systems have the ability to handle questions with complex semantics on small domain-specific databases like GeoQuery (Tang and Mooney, 2001) or subsets of Freebase (Cai and Yates, 2013), but have yet to scale to the task of general, open-domain question answering. In contrast, our system answers questions with more limite</context>
</contexts>
<marker>Katz, 1997</marker>
<rawString>Boris Katz. 1997. Annotating the World Wide Web using Natural Language. In RIAO, pages 136–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="22579" citStr="Koehn et al., 2003" startWordPosition="3808" endWordPosition="3811">raphrase corpus, we randomly sampled a set of 100 pairs and manually tagged them as ‘paraphrase’ or ‘notparaphrase.’ We found that 55% of the sampled pairs are valid paraphrased. Most of the incorrect paraphrases were questions that were related, but not paraphrased e.g. How big is the biggest mall? and Most expensive mall in the world? We word-aligned each paraphrase pair using the MGIZA++ implementation of IBM Model 4 (Och and Ney, 2000; Gao and Vogel, 2008). The word-alignment algorithm was run in each direction (x, x&apos;) and (x&apos;, x) and then combined using the grow-diag-final-and heuristic (Koehn et al., 2003). 7 Experimental Setup We compare the following systems: • PARALEX: the full system, using the lexical learning and parameter learning algorithms from Section 5. • NoParam: PARALEX without the learned parameters. • InitOnly: PARALEX using only the initial seed lexicon. We evaluate the systems’ performance on the endtask of QA on WikiAnswers questions. 7.1 Test Set A major challenge for evaluation is that the REVERB database is incomplete. A system may correctly map a test question to a valid query, only to return 0 results when executed against the incomplete database. We factor out this sourc</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cody Kwok</author>
<author>Oren Etzioni</author>
<author>Daniel S Weld</author>
</authors>
<title>Scaling Question Answering to the Web.</title>
<date>2001</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>19</volume>
<issue>3</issue>
<contexts>
<context position="6521" citStr="Kwok et al., 2001" startWordPosition="1024" endWordPosition="1027">ocessing: information extraction (IE), and natural language interfaces to databases (NLIDB). 1http://wiki.answers.com/ Research in IE has been moving towards the goal of extracting facts from large text corpora, across many domains, with minimal supervision (Mintz et al., 2009; Hoffmann et al., 2010; Riedel et al., 2010; Hoffmann et al., 2011; Banko et al., 2007; Yao et al., 2012). While much progress has been made in converting text into structured knowledge, there has been little work on answering natural language questions over these databases. There has been some work on QA over web text (Kwok et al., 2001; Brill et al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to automatically construct question answering systems from data (Zelle and Mooney, 1996; Popescu et al., 2004; Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2011). These systems have the ability to handle questions with complex semantics on small domain-specific databases like GeoQuery (Tang and Mooney, 2001) or subsets of F</context>
</contexts>
<marker>Kwok, Etzioni, Weld, 2001</marker>
<rawString>Cody Kwok, Oren Etzioni, and Daniel S. Weld. 2001. Scaling Question Answering to the Web. ACM Trans. Inf. Syst., 19(3):242–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning Dependency-Based Compositional Semantics.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6957" citStr="Liang et al., 2011" startWordPosition="1092" endWordPosition="1095">ext into structured knowledge, there has been little work on answering natural language questions over these databases. There has been some work on QA over web text (Kwok et al., 2001; Brill et al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to automatically construct question answering systems from data (Zelle and Mooney, 1996; Popescu et al., 2004; Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2011). These systems have the ability to handle questions with complex semantics on small domain-specific databases like GeoQuery (Tang and Mooney, 2001) or subsets of Freebase (Cai and Yates, 2013), but have yet to scale to the task of general, open-domain question answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these systems rely on hand-written templates for question interpretation. The lear</context>
</contexts>
<marker>Liang, Jordan, Klein, 2011</marker>
<rawString>Percy Liang, Michael Jordan, and Dan Klein. 2011. Learning Dependency-Based Compositional Semantics. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Chris Callison-Burch</author>
<author>Philip Resnik</author>
</authors>
<title>Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="7833" citStr="Marton et al., 2009" startWordPosition="1228" endWordPosition="1231">stion answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these systems rely on hand-written templates for question interpretation. The learning algorithms presented in this paper are similar to algorithms used for paraphrase extraction from sentence-aligned corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Marton et al., 2009). However, we use a paraphrase corpus for extracting lexical items relating natural language patterns to database concepts, as opposed to relationships between pairs of natural language utterances. 3 Overview of the Approach In this section, we give a high-level overview of the rest of the paper. Problem Our goal is to learn a function that will map a natural language question x to a query z over a database D. The database D is a collection of assertions in the form r(e1, e2) where r is a bi1609 nary relation from a vocabulary R, and e1 and e2 are entities from a vocabulary E. We assume that t</context>
</contexts>
<marker>Marton, Callison-Burch, Resnik, 2009</marker>
<rawString>Yuval Marton, Chris Callison-Burch, and Philip Resnik. 2009. Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Keith Hall</author>
<author>Gideon Mann</author>
</authors>
<title>Distributed training strategies for the structured perceptron.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="18473" citStr="McDonald et al., 2010" startWordPosition="3124" endWordPosition="3127"> lexical items that generalize poorly. 5.2 Parameter Learning Parameter learning is necessary for filtering out derivations that use incorrect lexical entries like new mexico = mexico, which arise from noise in the paraphrases and noise in the word alignment. 2InduceLex has similar behavior for the other type of derivation, which consists of a 1-argument question entry (p9, d9) and an entity (pe, de). We use the hidden variable structured perceptron algorithm to learn θ from a list of (question x, query z) training examples. We adopt the iterative parameter mixing variation of the perceptron (McDonald et al., 2010) to scale to a large number of training examples. Figure 2 shows the parameter learning algorithm. The parameter learning algorithm operates in two stages. First, we use the initial lexicon L0 to automatically generate (question x, query z) training examples from the paraphrase corpus C. Then we feed the training examples into the learning algorithm, which estimates parameters for the learned lexicon L. Because the number of training examples is large, we adopt a parallel perceptron approach. We first randomly partition the training data T into K equally-sized subsets T1, ... , TK. We then per</context>
</contexts>
<marker>McDonald, Hall, Mann, 2010</marker>
<rawString>Ryan McDonald, Keith Hall, and Gideon Mann. 2010. Distributed training strategies for the structured perceptron. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Distant Supervision for Relation Extraction Without Labeled Data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="6181" citStr="Mintz et al., 2009" startWordPosition="964" endWordPosition="967">ing questions from WikiAnswers using a database of web extractions, and show that it outperforms baseline systems. • We release our learned lexicon and question-paraphrase dataset to the research community, available at http://openie.cs.washington.edu. 2 Related Work Our work builds upon two major threads of research in natural language processing: information extraction (IE), and natural language interfaces to databases (NLIDB). 1http://wiki.answers.com/ Research in IE has been moving towards the goal of extracting facts from large text corpora, across many domains, with minimal supervision (Mintz et al., 2009; Hoffmann et al., 2010; Riedel et al., 2010; Hoffmann et al., 2011; Banko et al., 2007; Yao et al., 2012). While much progress has been made in converting text into structured knowledge, there has been little work on answering natural language questions over these databases. There has been some work on QA over web text (Kwok et al., 2001; Brill et al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to aut</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. 2009. Distant Supervision for Relation Extraction Without Labeled Data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Statistical Alignment Models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="22402" citStr="Och and Ney, 2000" startWordPosition="3779" endWordPosition="3782">alternate wordings of each other. We harvested a set of 18M of these question-paraphrase pairs, with 2.4M distinct questions in the corpus. To estimate the precision of the paraphrase corpus, we randomly sampled a set of 100 pairs and manually tagged them as ‘paraphrase’ or ‘notparaphrase.’ We found that 55% of the sampled pairs are valid paraphrased. Most of the incorrect paraphrases were questions that were related, but not paraphrased e.g. How big is the biggest mall? and Most expensive mall in the world? We word-aligned each paraphrase pair using the MGIZA++ implementation of IBM Model 4 (Och and Ney, 2000; Gao and Vogel, 2008). The word-alignment algorithm was run in each direction (x, x&apos;) and (x&apos;, x) and then combined using the grow-diag-final-and heuristic (Koehn et al., 2003). 7 Experimental Setup We compare the following systems: • PARALEX: the full system, using the lexical learning and parameter learning algorithms from Section 5. • NoParam: PARALEX without the learned parameters. • InitOnly: PARALEX using only the initial seed lexicon. We evaluate the systems’ performance on the endtask of QA on WikiAnswers questions. 7.1 Test Set A major challenge for evaluation is that the REVERB data</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved Statistical Alignment Models. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Alex Armanasu</author>
<author>Oren Etzioni</author>
<author>David Ko</author>
<author>Alexander Yates</author>
</authors>
<title>Modern Natural Language Interfaces to Databases: Composing Statistical Parsing with Semantic Tractability.</title>
<date>2004</date>
<booktitle>In Proceedings of the Twentieth International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="6884" citStr="Popescu et al., 2004" startWordPosition="1080" endWordPosition="1083">2007; Yao et al., 2012). While much progress has been made in converting text into structured knowledge, there has been little work on answering natural language questions over these databases. There has been some work on QA over web text (Kwok et al., 2001; Brill et al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to automatically construct question answering systems from data (Zelle and Mooney, 1996; Popescu et al., 2004; Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2011). These systems have the ability to handle questions with complex semantics on small domain-specific databases like GeoQuery (Tang and Mooney, 2001) or subsets of Freebase (Cai and Yates, 2013), but have yet to scale to the task of general, open-domain question answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these sys</context>
</contexts>
<marker>Popescu, Armanasu, Etzioni, Ko, Yates, 2004</marker>
<rawString>Ana-Maria Popescu, Alex Armanasu, Oren Etzioni, David Ko, and Alexander Yates. 2004. Modern Natural Language Interfaces to Databases: Composing Statistical Parsing with Semantic Tractability. In Proceedings of the Twentieth International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
<author>William Dolan</author>
</authors>
<title>Monolingual Machine Translation for Paraphrase Generation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="7755" citStr="Quirk et al., 2004" startWordPosition="1218" endWordPosition="1221"> Yates, 2013), but have yet to scale to the task of general, open-domain question answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these systems rely on hand-written templates for question interpretation. The learning algorithms presented in this paper are similar to algorithms used for paraphrase extraction from sentence-aligned corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Marton et al., 2009). However, we use a paraphrase corpus for extracting lexical items relating natural language patterns to database concepts, as opposed to relationships between pairs of natural language utterances. 3 Overview of the Approach In this section, we give a high-level overview of the rest of the paper. Problem Our goal is to learn a function that will map a natural language question x to a query z over a database D. The database D is a collection of assertions in the form r(e1, e2) where r is a bi1609 nary relation from a </context>
</contexts>
<marker>Quirk, Brockett, Dolan, 2004</marker>
<rawString>Chris Quirk, Chris Brockett, and William Dolan. 2004. Monolingual Machine Translation for Paraphrase Generation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
</authors>
<title>Modeling Relations and Their Mentions without Labeled Text.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 European conference on Machine learning and Knowledge Discovery in Databases.</booktitle>
<contexts>
<context position="1247" citStr="Riedel et al., 2010" startWordPosition="183" endWordPosition="186">on and includes a scalable, parallelized perceptron parameter estimation scheme. Experiments show that our approach more than quadruples the recall of the seed lexicon, with only an 8% loss in precision. 1 Introduction Open-domain question answering (QA) is a longstanding, unsolved problem. The central challenge is to automate every step of QA system construction, including gathering large databases and answering questions against these databases. While there has been significant work on large-scale information extraction (IE) from unstructured text (Banko et al., 2007; Hoffmann et al., 2010; Riedel et al., 2010), the problem of answering questions with the noisy knowledge bases that IE systems produce has received less attention. In this paper, we present an approach for learning to map questions to formal queries over a large, open-domain database of extracted facts (Fader et al., 2011). Our system learns from a large, noisy, questionparaphrase corpus, where question clusters have a common but unknown query, and can span a diverse set of topics. Table 1 shows example paraphrase clusters for a set of factual questions. Such data provides strong signal for learning about lexical variation, but there a</context>
<context position="6225" citStr="Riedel et al., 2010" startWordPosition="972" endWordPosition="975">base of web extractions, and show that it outperforms baseline systems. • We release our learned lexicon and question-paraphrase dataset to the research community, available at http://openie.cs.washington.edu. 2 Related Work Our work builds upon two major threads of research in natural language processing: information extraction (IE), and natural language interfaces to databases (NLIDB). 1http://wiki.answers.com/ Research in IE has been moving towards the goal of extracting facts from large text corpora, across many domains, with minimal supervision (Mintz et al., 2009; Hoffmann et al., 2010; Riedel et al., 2010; Hoffmann et al., 2011; Banko et al., 2007; Yao et al., 2012). While much progress has been made in converting text into structured knowledge, there has been little work on answering natural language questions over these databases. There has been some work on QA over web text (Kwok et al., 2001; Brill et al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to automatically construct question answering syst</context>
</contexts>
<marker>Riedel, Yao, McCallum, 2010</marker>
<rawString>Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling Relations and Their Mentions without Labeled Text. In Proceedings of the 2010 European conference on Machine learning and Knowledge Discovery in Databases.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lappoon R Tang</author>
<author>Raymond J Mooney</author>
</authors>
<title>Using Multiple Clause Constructors in Inductive Logic Programming for Semantic Parsing.</title>
<date>2001</date>
<contexts>
<context position="7105" citStr="Tang and Mooney, 2001" startWordPosition="1114" endWordPosition="1117">on QA over web text (Kwok et al., 2001; Brill et al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to automatically construct question answering systems from data (Zelle and Mooney, 1996; Popescu et al., 2004; Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2011). These systems have the ability to handle questions with complex semantics on small domain-specific databases like GeoQuery (Tang and Mooney, 2001) or subsets of Freebase (Cai and Yates, 2013), but have yet to scale to the task of general, open-domain question answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these systems rely on hand-written templates for question interpretation. The learning algorithms presented in this paper are similar to algorithms used for paraphrase extraction from sentence-aligned corpora (Barzilay and McKeown</context>
</contexts>
<marker>Tang, Mooney, 2001</marker>
<rawString>Lappoon R. Tang and Raymond J. Mooney. 2001. Using Multiple Clause Constructors in Inductive Logic Programming for Semantic Parsing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christina Unger</author>
<author>Lorenz B¨uhmann</author>
<author>Jens Lehmann</author>
<author>Axel-Cyrille Ngonga Ngomo</author>
<author>Daniel Gerber</author>
<author>Philipp Cimiano</author>
</authors>
<title>Template-Based Question Answering over RDF Data.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st World Wide Web Conference</booktitle>
<marker>Unger, B¨uhmann, Lehmann, Ngomo, Gerber, Cimiano, 2012</marker>
<rawString>Christina Unger, Lorenz B¨uhmann, Jens Lehmann, Axel-Cyrille Ngonga Ngomo, Daniel Gerber, and Philipp Cimiano. 2012. Template-Based Question Answering over RDF Data. In Proceedings of the 21st World Wide Web Conference 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Yahya</author>
<author>Klaus Berberich</author>
<author>Shady Elbassuoni</author>
<author>Maya Ramanath</author>
<author>Volker Tresp</author>
<author>Gerhard Weikum</author>
</authors>
<title>Natural Language Questions for the Web of Data.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</booktitle>
<contexts>
<context position="7448" citStr="Yahya et al., 2012" startWordPosition="1172" endWordPosition="1175">rom data (Zelle and Mooney, 1996; Popescu et al., 2004; Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2011). These systems have the ability to handle questions with complex semantics on small domain-specific databases like GeoQuery (Tang and Mooney, 2001) or subsets of Freebase (Cai and Yates, 2013), but have yet to scale to the task of general, open-domain question answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these systems rely on hand-written templates for question interpretation. The learning algorithms presented in this paper are similar to algorithms used for paraphrase extraction from sentence-aligned corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Marton et al., 2009). However, we use a paraphrase corpus for extracting lexical items relating natural language patterns to database concepts, as opposed to relationships between pairs of natural language utterances. 3 Overview of the</context>
</contexts>
<marker>Yahya, Berberich, Elbassuoni, Ramanath, Tresp, Weikum, 2012</marker>
<rawString>Mohamed Yahya, Klaus Berberich, Shady Elbassuoni, Maya Ramanath, Volker Tresp, and Gerhard Weikum. 2012. Natural Language Questions for the Web of Data. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Unsupervised Relation Discovery with Sense Disambiguation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6287" citStr="Yao et al., 2012" startWordPosition="984" endWordPosition="987">ystems. • We release our learned lexicon and question-paraphrase dataset to the research community, available at http://openie.cs.washington.edu. 2 Related Work Our work builds upon two major threads of research in natural language processing: information extraction (IE), and natural language interfaces to databases (NLIDB). 1http://wiki.answers.com/ Research in IE has been moving towards the goal of extracting facts from large text corpora, across many domains, with minimal supervision (Mintz et al., 2009; Hoffmann et al., 2010; Riedel et al., 2010; Hoffmann et al., 2011; Banko et al., 2007; Yao et al., 2012). While much progress has been made in converting text into structured knowledge, there has been little work on answering natural language questions over these databases. There has been some work on QA over web text (Kwok et al., 2001; Brill et al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to automatically construct question answering systems from data (Zelle and Mooney, 1996; Popescu et al., 2004; Z</context>
</contexts>
<marker>Yao, Riedel, McCallum, 2012</marker>
<rawString>Limin Yao, Sebastian Riedel, and Andrew McCallum. 2012. Unsupervised Relation Discovery with Sense Disambiguation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Zelle</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to Parse Database Queries Using Inductive Logic Programming.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="6862" citStr="Zelle and Mooney, 1996" startWordPosition="1076" endWordPosition="1079">l., 2011; Banko et al., 2007; Yao et al., 2012). While much progress has been made in converting text into structured knowledge, there has been little work on answering natural language questions over these databases. There has been some work on QA over web text (Kwok et al., 2001; Brill et al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to automatically construct question answering systems from data (Zelle and Mooney, 1996; Popescu et al., 2004; Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2011). These systems have the ability to handle questions with complex semantics on small domain-specific databases like GeoQuery (Tang and Mooney, 2001) or subsets of Freebase (Cai and Yates, 2013), but have yet to scale to the task of general, open-domain question answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al.</context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>John M. Zelle and Raymond J. Mooney. 1996. Learning to Parse Database Queries Using Inductive Logic Programming. In Proceedings of the Thirteenth National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the 21st Conference in Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="6915" citStr="Zettlemoyer and Collins, 2005" startWordPosition="1084" endWordPosition="1087">). While much progress has been made in converting text into structured knowledge, there has been little work on answering natural language questions over these databases. There has been some work on QA over web text (Kwok et al., 2001; Brill et al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to automatically construct question answering systems from data (Zelle and Mooney, 1996; Popescu et al., 2004; Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2011). These systems have the ability to handle questions with complex semantics on small domain-specific databases like GeoQuery (Tang and Mooney, 2001) or subsets of Freebase (Cai and Yates, 2013), but have yet to scale to the task of general, open-domain question answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these systems rely on hand-written templ</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars. In Proceedings of the 21st Conference in Uncertainty in Artificial Intelligence.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>