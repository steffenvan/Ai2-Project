<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007543">
<title confidence="0.902259">
Measuring semantic content in distributional vectors
</title>
<author confidence="0.566567">
Aur´elie Herbelot
</author>
<affiliation confidence="0.4836055">
EB Kognitionswissenschaft
Universit¨at Potsdam
</affiliation>
<address confidence="0.613779">
Golm, Germany
</address>
<email confidence="0.99337">
aurelie.herbelot@cantab.net
</email>
<author confidence="0.9717">
Mohan Ganesalingam
</author>
<affiliation confidence="0.9927255">
Trinity College
University of Cambridge
</affiliation>
<address confidence="0.896177">
Cambridge, UK
</address>
<email confidence="0.997572">
mohan0@gmail.com
</email>
<sectionHeader confidence="0.997369" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999980473684211">
Some words are more contentful than oth-
ers: for instance, make is intuitively more
general than produce and fifteen is more
‘precise’ than a group. In this paper,
we propose to measure the ‘semantic con-
tent’ of lexical items, as modelled by
distributional representations. We inves-
tigate the hypothesis that semantic con-
tent can be computed using the Kullback-
Leibler (KL) divergence, an information-
theoretic measure of the relative entropy
of two distributions. In a task focus-
ing on retrieving the correct ordering of
hyponym-hypernym pairs, the KL diver-
gence achieves close to 80% precision but
does not outperform a simpler (linguis-
tically unmotivated) frequency measure.
We suggest that this result illustrates the
rather ‘intensional’ aspect of distributions.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999975759259259">
Distributional semantics is a representation of lex-
ical meaning that relies on a statistical analysis
of the way words are used in corpora (Curran,
2003; Turney and Pantel, 2010; Erk, 2012). In
this framework, the semantics of a lexical item is
accounted for by modelling its co-occurrence with
other words (or any larger lexical context). The
representation of a target word is thus a vector in a
space where each dimension corresponds to a pos-
sible context. The weights of the vector compo-
nents can take various forms, ranging from sim-
ple co-occurrence frequencies to functions such as
Pointwise Mutual Information (for an overview,
see (Evert, 2004)).
This paper investigates the issue of comput-
ing the semantic content of distributional vectors.
That is, we look at the ways we can distribution-
ally express that make is a more general verb than
produce, which is itself more general than, for
instance, weave. Although the task is related to
the identification of hyponymy relations, it aims
to reflect a more encompassing phenomenon: we
wish to be able to compare the semantic content of
words within parts-of-speech where the standard
notion of hyponymy does not apply (e.g. preposi-
tions: see with vs. next to or of vs. concerning)
and across parts-of-speech (e.g. fifteen vs. group).
The hypothesis we will put forward is that se-
mantic content is related to notions of relative en-
tropy found in information theory. More specif-
ically, we hypothesise that the more specific a
word is, the more the distribution of the words
co-occurring with it will differ from the baseline
distribution of those words in the language as a
whole. (A more intuitive way to phrase this is that
the more specific a word is, the more information
it gives us about which other words are likely to
occur near it.) The specific measure of difference
that we will use is the Kullback-Leibler divergence
of the distribution of words co-ocurring with the
target word against the distribution of those words
in the language as a whole. We evaluate our hy-
pothesis against a subset of the WordNet hierar-
chy (given by (Baroni et al, 2012)), relying on the
intuition that in a hyponym-hypernym pair, the hy-
ponym should have higher semantic content than
its hypernym.
The paper is structured as follows. We first
define our notion of semantic content and moti-
vate the need for measuring semantic content in
distributional setups. We then describe the im-
plementation of the distributional system we use
in this paper, emphasising our choice of weight-
ing measure. We show that, using the compo-
</bodyText>
<page confidence="0.968905">
440
</page>
<bodyText confidence="0.845269666666667">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 440–445,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
nents of the described weighting measure, which
are both probability distributions, we can calculate
the relative entropy of a distribution by inserting
those probability distributions in the equation for
the Kullback-Leibler (KL) divergence. We finally
evaluate the KL measure against a basic notion of
frequency and conclude with some error analysis.
</bodyText>
<sectionHeader confidence="0.945929" genericHeader="introduction">
2 Semantic content
</sectionHeader>
<bodyText confidence="0.999972779661017">
As a first approximation, we will define seman-
tic content as informativeness with respect to de-
notation. Following Searle (1969), we will take
a ‘successful reference’ to be a speech act where
the choice of words used by the speaker appropri-
ately identifies a referent for the hearer. Glossing
over questions of pragmatics, we will assume that
a more informative word is more likely to lead to
a successful reference than a less informative one.
That is, if Kim owns a cat and a dog, the identify-
ing expression my cat is a better referent than my
pet and so cat can be said to have more semantic
content than pet.
While our definition relies on reference, it also
posits a correspondence between actual utterances
and denotation. Given two possible identifying ex-
pressions e1 and e2, e1 may be preferred in a par-
ticular context, and so, context will be an indicator
of the amount of semantic content in an expres-
sion. In Section 5, we will produce an explicit
hypothesis for how the amount of semantic con-
tent in a lexical item affects the contexts in which
it appears.
A case where semantic content has a direct cor-
respondence with a lexical relation is hyponymy.
Here, the correspondence relies entirely on a basic
notion of extension. For instance, it is clear that
hammer is more contentful than tool because the
extension of hammer is smaller than that of tool,
and therefore more discriminating in a given iden-
tifying expression (See Give me the hammer ver-
sus Give me the tool). But we can also talk about
semantic content in cases where the notion of ex-
tension does not necessarily apply. For example,
it is not usual to talk of the extension of a prepo-
sition. However, in context, the use of a preposi-
tion against another one might be more discrim-
inating in terms of reference. Compare a) Sandy
is with Kim and b) Sandy is next to Kim. Given a
set of possible situations involving, say, Kim and
Sandy at a party, we could show that b) is more
discriminating than a), because it excludes the sit-
uations where Sandy came to the party with Kim
but is currently talking to Kay at the other end of
the room. The fact that next to expresses physi-
cal proximity, as opposed to just being in the same
situation, confers it more semantic content accord-
ing to our definition. Further still, there may be a
need for comparing the informativeness of words
across parts of speech (compareA group of/Fifteen
people was/were waiting in front of the town hall).
Although we will not discuss this in detail, there
is a notion of semantic content above the word
level which should naturally derive from compo-
sition rules. For instance, we would expect the
composition of a given intersective adjective and
a given noun to result into a phrase with a seman-
tic content greater than that of its components (or
at least equal to it).
</bodyText>
<sectionHeader confidence="0.996691" genericHeader="method">
3 Motivation
</sectionHeader>
<bodyText confidence="0.999978">
The last few years have seen a growing interest in
distributional semantics as a representation of lex-
ical meaning. Owing to their mathematical inter-
pretation, distributions allow linguists to simulate
human similarity judgements (Lund, Burgess and
Atchley, 1995), and also reproduce some of the
features given by test subjects when asked to write
down the characteristics of a given concept (Ba-
roni and Lenci, 2008). In a distributional semantic
space, for instance, the word ‘cat’ may be close to
‘dog’ or to ‘tiger’, and its vector might have high
values along the dimensions ‘meow’, ‘mouse’ and
‘pet’. Distributional semantics has had great suc-
cesses in recent years, and for many computational
linguists, it is an essential tool for modelling phe-
nomena affected by lexical meaning.
If distributional semantics is to be seen as
a general-purpose representation, however, we
should evaluate it across all properties which we
deem relevant to a model of the lexicon. We con-
sider semantic content to be one such property. It
underlies the notion of hyponymy and naturally
models our intuitions about the ‘precision’ (as op-
posed to ‘vagueness’) of words.
Further, semantic content may be crucial in
solving some fundamental problems of distribu-
tional semantics. As pointed out by McNally
(2013), there is no easy way to define the notion
of a function word and this has consequences for
theories where function words are not assigned
a distributional representation. McNally suggests
that the most appropriate way to separate function
</bodyText>
<page confidence="0.993873">
441
</page>
<bodyText confidence="0.999816">
from content words might, in the end, involve tak-
ing into account how much ‘descriptive’ content
they have.
</bodyText>
<sectionHeader confidence="0.926374" genericHeader="method">
4 An implementation of a distributional
system
</sectionHeader>
<bodyText confidence="0.995437833333333">
The distributional system we implemented for this
paper is close to the system of Mitchell and La-
pata (2010) (subsequently M&amp;L). As background
data, we use the British National Corpus (BNC) in
lemmatised format. Each lemma is followed by a
part of speech according to the CLAWS tagset for-
mat (Leech, Garside, and Bryant, 1994). For our
experiments, we only keep the first letter of each
part-of-speech tag, thus obtaining broad categories
such as N or V. Furthermore, we only retain words
in the following categories: nouns, verbs, adjec-
tives and adverbs (punctuation is ignored). Each
article in the corpus is converted into a 11-word
window format, that is, we are assuming that con-
text in our system is defined by the five words pre-
ceding and the five words following the target.
To calculate co-occurrences, we use the follow-
ing equations:
</bodyText>
<equation confidence="0.966138333333333">
Efreqci = freqci,t (1)
t
Efreqt = freqci,t (2)
ci
Efreqtotal = freqci,t (3)
ci,t
</equation>
<bodyText confidence="0.998448363636363">
The quantities in these equations represent the
following:
freqci,t frequency of the context word ci
with the target word t
freqtotal total count of word tokens
freqt frequency of the target word t
freqci frequency of the context word ci
As in M&amp;L, we use the 2000 most frequent
words in our corpus as the semantic space dimen-
sions. M&amp;L calculate the weight of each context
term in the distribution as follows:
</bodyText>
<equation confidence="0.787405">
freqci,t x freqtotal (4)
freqt x freqci
</equation>
<bodyText confidence="0.985097333333333">
We will not directly use the measure vi(t) as it
is not a probability distribution and so is not suit-
able for entropic analysis; instead our analysis will
be phrased in terms of the probability distributions
p(ci|t) and p(ci) (the numerator and denominator
in vi(t)).
</bodyText>
<sectionHeader confidence="0.930569" genericHeader="method">
5 Semantic content as entropy: two
measures
</sectionHeader>
<bodyText confidence="0.999580485714286">
Resnik (1995) uses the notion of information con-
tent to improve on the standard edge counting
methods proposed to measure similarity in tax-
onomies such as WordNet. He proposes that the
information content of a term t is given by the self-
information measure −log p(t). The idea behind
this measure is that, as the frequency of the term
increases, its informativeness decreases. Although
a good first approximation, the measure cannot be
said to truly reflect our concept of semantic con-
tent. For instance, in the British National Corpus,
time and see are more frequent than thing or may
and man is more frequent than part. However, it
seems intuitively right to say that time, see and
man are more ‘precise’ concepts than thing, may
and part respectively. Or said otherwise, there is
no indication that more general concepts occur in
speech more than less general ones. We will there-
fore consider self-information as a baseline.
As we expect more specific words to be more
informative about which words co-occur with
them, it is natural to try to measure the specificity
of a word by using notions from information the-
ory to analyse the probability distribution p(ci|t)
associated with the word. The standard notion
of entropy is not appropriate for this purpose, be-
cause it does not take account of the fact that the
words serving as semantic space dimensions may
have different frequencies in language as a whole,
i.e. of the fact that p(ci) does not have a uniform
distribution. Instead we need to measure the de-
gree to which p(ci|t) differs from the context word
distribution p(ci). An appropriate measure for this
is the Kullback-Leibler (KL) divergence or rela-
tive entropy:
</bodyText>
<equation confidence="0.9989795">
ln(P (i)
Q(i))P(i) (5)
</equation>
<bodyText confidence="0.999412571428571">
By taking P(i) to be p(ci|t) and Q(i) to be p(ci)
(as given by Equation 4), we calculate the rela-
tive entropy of p(ci|t) and p(ci). The measure is
clearly informative: it reflects the way that t mod-
ifies the expectation of seeing ci in the corpus.
We hypothesise that when compared to the distri-
bution p(ci), more informative words will have a
</bodyText>
<equation confidence="0.99320525">
vi(t) = p(ci|t)
p(ci)
DKL(PIIQ) = E
i
</equation>
<page confidence="0.985197">
442
</page>
<bodyText confidence="0.9934225">
more ‘distorted’ distribution p(ci|t) and that the
KL divergence will reflect this.1
</bodyText>
<sectionHeader confidence="0.998451" genericHeader="method">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999988853658537">
In Section 2, we defined semantic content as a no-
tion encompassing various referential properties,
including a basic concept of extension in cases
where it is applicable. However, we do not know
of a dataset providing human judgements over the
general informativeness of lexical items. So in or-
der to evaluate our proposed measure, we inves-
tigate its ability to retrieve the right ordering of
hyponym pairs, which can be considered a subset
of the issue at hand.
Our assumption is that if X is a hypernym of
Y , then the information content in X will be lower
than in Y (because it has a more ‘general’ mean-
ing). So, given a pair of words {w1, w2} in a
known hyponymy relation, we should be able to
tell which of w1 or w2 is the hypernym by com-
puting the respective KL divergences.
We use the hypernym data provided by (Baroni
et al, 2012) as testbed for our experiment.2 This
set of hyponym-hypernym pairs contains 1385 in-
stances retrieved from the WordNet hierarchy. Be-
fore running our system on the data, we make
slight modifications to it. First, as our distributions
are created over the British National Corpus, some
spellings must be converted to British English: for
instance, color is replaced by colour. Second, five
of the nouns included in the test set are not in the
BNC. Those nouns are brethren, intranet, iPod,
webcam and IX. We remove the pairs containing
those words from the data. Third, numbers such as
eleven or sixty are present in the Baroni et al set as
nouns, but not in the BNC. Pairs containing seven
such numbers are therefore also removed from the
data. Finally, we encounter tagging issues with
three words, which we match to their BNC equiv-
alents: acoustics and annals are matched to acous-
tic and annal, and trouser to trousers. These mod-
ifications result in a test set of 1279 remaining
pairs.
We then calculate both the self-information
measure and the KL divergence of all terms in-
</bodyText>
<footnote confidence="0.946042625">
1Note that KL divergence is not symmetric:
DKL(p(ci|t)Ilp(ci))) is not necessarily equal to
DKL(p(ci)Ilp(ci|t)). The latter is inferior as a few
very small values of p(ci|t) can have an inappropriately large
effect on it.
2The data is available at http://clic.cimec.
unitn.it/Files/PublicData/eacl2012-data.
zip.
</footnote>
<bodyText confidence="0.999857">
cluded in our test set. In order to evaluate the sys-
tem, we record whether the calculated entropies
match the order of each hypernym-hyponym pair.
That is, we count a pair as correctly represented
by our system if w1 is a hypernym of w2 and
KL(w1) &lt; KL(w2) (or, in the case of the
baseline, 5I(w1) &lt; 5I(w2) where 5I is self-
information).
Self-information obtains 80.8% precision on the
task, with the KL divergence lagging a little be-
hind with 79.4% precision (the difference is not
significant). In other terms, both measures per-
form comparably. We analyse potential reasons
for this disappointing result in the next section.
</bodyText>
<sectionHeader confidence="0.9903" genericHeader="method">
7 Error analysis
</sectionHeader>
<bodyText confidence="0.999930633333333">
It is worth reminding ourselves of the assumption
we made with regard to semantic content. Our
hypothesis was that with a ‘more general’ target
word t, the p(ci|t) distribution would be fairly
similar to p(ci).
Manually checking some of the pairs which
were wrongly classified by the KL divergence re-
veals that our hypothesis might not hold. For ex-
ample, the pair beer – beverage is classified in-
correctly. When looking at the beverage distri-
bution, it is clear that it does not conform to our
expectations: it shows high vi(t) weights along
the food, wine, coffee and tea dimensions, for in-
stance, i.e. there is a large difference between
p(cfood) and p(cfood|t), etc. Although beverage
is an umbrella word for many various types of
drinks, speakers of English use it in very partic-
ular contexts. So, distributionally, it is not a ‘gen-
eral word’. Similar observations can be made for,
e.g. liquid (strongly associated with gas, presum-
ably via coordination), anniversary (linked to the
verb mark or the noun silver), or again projectile
(co-occurring with weapon, motion and speed).
The general point is that, as pointed out else-
where in the literature (Erk, 2013), distributions
are a good representation of (some aspects of) in-
tension, but they are less apt to model extension.3
So a term with a large extension like beverage
may have a more restricted (distributional) inten-
sion than a word with a smaller extension, such as
</bodyText>
<footnote confidence="0.991997">
3We qualify ‘intension’ here, because in the sense of a
mapping from possible worlds to extensions, intension can-
not be said to be provided by distributions: the distribution of
beverage, it seems, does not allow us to successfully pick out
all beverages in the real world.
</footnote>
<page confidence="0.998652">
443
</page>
<bodyText confidence="0.99846616">
beer.4
Contributing to this issue, fixed phrases, named
entities and generally strong collocations skew our
distributions. So for instance, in the jewelry distri-
bution, the most highly weighted context is mental
(with vi(t) = 395.3) because of the music album
Mental Jewelry. While named entities could eas-
ily be eliminated from the system’s results by pre-
processing the corpus with a named entity recog-
niser, the issue is not so simple when it comes to
fixed phrases of a more compositional nature (e.g.
army ant): excluding them might be detrimental
for the representation (it is, after all, part of the
meaning of ant that it can be used metaphorically
to refer to people) and identifying such phrases is
a non-trivial problem in itself.
Some of the errors we observe may also be
related to word senses. For instance, the word
medium, to be found in the pair magazine –
medium, can be synonymous with middle, clair-
voyant or again mode of communication. In the
sense of clairvoyant, it is clearly more specific
than in the sense intended in the test pair. As dis-
tributions do not distinguish between senses, this
will have an effect on our results.
</bodyText>
<sectionHeader confidence="0.998661" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.99998235">
In this paper, we attempted to define a mea-
sure of distributional semantic content in or-
der to model the fact that some words have a
more general meaning than others. We com-
pared the Kullback-Leibler divergence to a sim-
ple self-information measure. Our experiments,
which involved retrieving the correct ordering of
hyponym-hypernym pairs, had disappointing re-
sults: the KL divergence was unable to outperform
self-information, and both measures misclassified
around 20% of our testset.
Our error analysis showed that several factors
contributed to the misclassifications. First, distri-
butions are unable to model extensional properties
which, in many cases, account for the feeling that a
word is more general than another. Second, strong
collocation effects can influence the measurement
of information negatively: it is an open question
which phrases should be considered ‘words-with-
spaces’ when building distributions. Finally, dis-
</bodyText>
<footnote confidence="0.8964284">
4Although it is more difficult to talk of the extension of
e.g. adverbials (very) or some adjectives (skillful), the general
point is that text is biased towards a certain usage of words,
while the general meaning a competent speaker ascribes to
lexical items does not necessarily follow this bias.
</footnote>
<bodyText confidence="0.999820333333333">
tributional representations do not distinguish be-
tween word senses, which in many cases is a de-
sirable feature, but interferes with the task we sug-
gested in this work.
To conclude, we would like to stress that we do
not think another information-theoretic measure
would perform hugely better than the KL diver-
gence. The point is that the nature of distributional
vectors makes them sensitive to word usage and
that, despite the general assumption behind dis-
tributional semantics, word usage might not suf-
fice to model all aspects of lexical semantics. We
leave as an open problem the issue of whether a
modified form of our ‘basic’ distributional vectors
would encode the right information.
</bodyText>
<sectionHeader confidence="0.998379" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998312">
This work was funded by a postdoctoral fellow-
ship from the Alexander von Humboldt Founda-
tion to the first author, and a Title A Fellowship
from Trinity College, Cambridge, to the second
author.
</bodyText>
<sectionHeader confidence="0.999427" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999349035714286">
Baroni, Marco, and Lenci, Alessandro. 2008. Con-
cepts and properties in word spaces. In Alessan-
dro Lenci (ed.), From context to meaning: Distribu-
tional models of the lexicon in linguistics and cog-
nitive science (Special issue of the Italian Journal of
Linguistics 20(1)), pages 55–88.
Baroni, Marco, Raffaella Bernardi, Ngoc-Quynh Do
and Chung-chieh Shan. 2012. Entailment above the
word level in distributional semantics. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL2012), pages 23–32.
Baroni, Marco, Raffaella Bernardi, and Roberto Zam-
parelli. 2012. Frege in Space: a Program for Com-
positional Distributional Semantics. Under review.
Curran, James. 2003. From Distributional to Semantic
Similarity. Ph.D. thesis, University of Edinburgh,
Scotland, UK.
Erk, Katrin. 2012. Vector space models of word mean-
ing and phrase meaning: a survey. Language and
Linguistics Compass, 6:10:635–653.
Erk, Katrin. 2013. Towards a semantics for distribu-
tional representations. In Proceedings of the Tenth
International Conference on Computational Seman-
tics (IWCS2013).
Evert, Stefan. 2004. The statistics of word cooccur-
rences: word pairs and collocations. Ph.D. thesis,
University of Stuttgart.
</reference>
<page confidence="0.988088">
444
</page>
<reference confidence="0.9997103">
Leech, Geoffrey, Roger Garside, and Michael Bryant.
1994. Claws4: The tagging of the british national
corpus. In Proceedings of the 15th International
Conference on Computational Linguistics (COLING
94), pages 622–628, Kyoto, Japan.
Lund, Kevin, Curt Burgess, and Ruth Ann Atchley.
1995. Semantic and associative priming in high-
dimensional semantic space. In Proceedings of the
17th annual conference of the Cognitive Science So-
ciety, Vol. 17, pages 660–665.
McNally, Louise. 2013. Formal and distributional se-
mantics: From romance to relationship. In Proceed-
ings of the ‘Towards a Formal Distributional Seman-
tics’ workshop, 10th International Conference on
Computational Semantics (IWCS2013), Potsdam,
Germany. Invited talk.
Mitchell, Jeff and Mirella Lapata. 2010. Composition
in Distributional Models of Semantics. Cognitive
Science, 34(8):1388–1429, November.
Resnik, Philipp. 1995. Using information content
to evaluate semantic similarity in a taxonomy. In
Proceedings of the 14th International Joint Con-
ference on Artificial Intelligence (IJCAI-95), pages
448–453.
Searle, John R. 1969. Speech acts: An essay in the phi-
losophy of language. Cambridge University Press.
Turney, Peter D. and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141–188.
</reference>
<page confidence="0.999105">
445
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.153822">
<title confidence="0.7899225">Measuring semantic content in distributional vectors Aur´elie</title>
<author confidence="0.47711">EB</author>
<affiliation confidence="0.6719425">Universit¨at Golm,</affiliation>
<email confidence="0.925501">aurelie.herbelot@cantab.net</email>
<author confidence="0.985778">Mohan Ganesalingam</author>
<affiliation confidence="0.922579">Trinity University of Cambridge Cambridge,</affiliation>
<email confidence="0.9993">mohan0@gmail.com</email>
<abstract confidence="0.99965155">Some words are more contentful than othfor instance, intuitively more than more than In this paper, we propose to measure the ‘semantic content’ of lexical items, as modelled by distributional representations. We investigate the hypothesis that semantic content can be computed using the Kullback- Leibler (KL) divergence, an informationtheoretic measure of the relative entropy of two distributions. In a task focusing on retrieving the correct ordering of hyponym-hypernym pairs, the KL divergence achieves close to 80% precision but does not outperform a simpler (linguistically unmotivated) frequency measure. We suggest that this result illustrates the rather ‘intensional’ aspect of distributions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>Concepts and properties in word spaces.</title>
<date>2008</date>
<booktitle>From context to meaning: Distributional models of the lexicon in linguistics and cognitive science (Special issue of the Italian Journal of Linguistics</booktitle>
<volume>20</volume>
<issue>1</issue>
<pages>55--88</pages>
<editor>In Alessandro Lenci (ed.),</editor>
<contexts>
<context position="7414" citStr="Baroni and Lenci, 2008" startWordPosition="1221" endWordPosition="1225">we would expect the composition of a given intersective adjective and a given noun to result into a phrase with a semantic content greater than that of its components (or at least equal to it). 3 Motivation The last few years have seen a growing interest in distributional semantics as a representation of lexical meaning. Owing to their mathematical interpretation, distributions allow linguists to simulate human similarity judgements (Lund, Burgess and Atchley, 1995), and also reproduce some of the features given by test subjects when asked to write down the characteristics of a given concept (Baroni and Lenci, 2008). In a distributional semantic space, for instance, the word ‘cat’ may be close to ‘dog’ or to ‘tiger’, and its vector might have high values along the dimensions ‘meow’, ‘mouse’ and ‘pet’. Distributional semantics has had great successes in recent years, and for many computational linguists, it is an essential tool for modelling phenomena affected by lexical meaning. If distributional semantics is to be seen as a general-purpose representation, however, we should evaluate it across all properties which we deem relevant to a model of the lexicon. We consider semantic content to be one such pro</context>
</contexts>
<marker>Baroni, Lenci, 2008</marker>
<rawString>Baroni, Marco, and Lenci, Alessandro. 2008. Concepts and properties in word spaces. In Alessandro Lenci (ed.), From context to meaning: Distributional models of the lexicon in linguistics and cognitive science (Special issue of the Italian Journal of Linguistics 20(1)), pages 55–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Raffaella Bernardi</author>
<author>Ngoc-Quynh Do</author>
<author>Chung-chieh Shan</author>
</authors>
<title>Entailment above the word level in distributional semantics.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL2012),</booktitle>
<pages>23--32</pages>
<contexts>
<context position="3152" citStr="Baroni et al, 2012" startWordPosition="499" endWordPosition="502">more the distribution of the words co-occurring with it will differ from the baseline distribution of those words in the language as a whole. (A more intuitive way to phrase this is that the more specific a word is, the more information it gives us about which other words are likely to occur near it.) The specific measure of difference that we will use is the Kullback-Leibler divergence of the distribution of words co-ocurring with the target word against the distribution of those words in the language as a whole. We evaluate our hypothesis against a subset of the WordNet hierarchy (given by (Baroni et al, 2012)), relying on the intuition that in a hyponym-hypernym pair, the hyponym should have higher semantic content than its hypernym. The paper is structured as follows. We first define our notion of semantic content and motivate the need for measuring semantic content in distributional setups. We then describe the implementation of the distributional system we use in this paper, emphasising our choice of weighting measure. We show that, using the compo440 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 440–445, Sofia, Bulgaria, August 4-9 2013. c�2013 </context>
<context position="13405" citStr="Baroni et al, 2012" startWordPosition="2243" endWordPosition="2246">man judgements over the general informativeness of lexical items. So in order to evaluate our proposed measure, we investigate its ability to retrieve the right ordering of hyponym pairs, which can be considered a subset of the issue at hand. Our assumption is that if X is a hypernym of Y , then the information content in X will be lower than in Y (because it has a more ‘general’ meaning). So, given a pair of words {w1, w2} in a known hyponymy relation, we should be able to tell which of w1 or w2 is the hypernym by computing the respective KL divergences. We use the hypernym data provided by (Baroni et al, 2012) as testbed for our experiment.2 This set of hyponym-hypernym pairs contains 1385 instances retrieved from the WordNet hierarchy. Before running our system on the data, we make slight modifications to it. First, as our distributions are created over the British National Corpus, some spellings must be converted to British English: for instance, color is replaced by colour. Second, five of the nouns included in the test set are not in the BNC. Those nouns are brethren, intranet, iPod, webcam and IX. We remove the pairs containing those words from the data. Third, numbers such as eleven or sixty </context>
</contexts>
<marker>Baroni, Bernardi, Do, Shan, 2012</marker>
<rawString>Baroni, Marco, Raffaella Bernardi, Ngoc-Quynh Do and Chung-chieh Shan. 2012. Entailment above the word level in distributional semantics. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL2012), pages 23–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Raffaella Bernardi</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Frege in Space: a Program for Compositional Distributional Semantics. Under review.</title>
<date>2012</date>
<contexts>
<context position="3152" citStr="Baroni et al, 2012" startWordPosition="499" endWordPosition="502">more the distribution of the words co-occurring with it will differ from the baseline distribution of those words in the language as a whole. (A more intuitive way to phrase this is that the more specific a word is, the more information it gives us about which other words are likely to occur near it.) The specific measure of difference that we will use is the Kullback-Leibler divergence of the distribution of words co-ocurring with the target word against the distribution of those words in the language as a whole. We evaluate our hypothesis against a subset of the WordNet hierarchy (given by (Baroni et al, 2012)), relying on the intuition that in a hyponym-hypernym pair, the hyponym should have higher semantic content than its hypernym. The paper is structured as follows. We first define our notion of semantic content and motivate the need for measuring semantic content in distributional setups. We then describe the implementation of the distributional system we use in this paper, emphasising our choice of weighting measure. We show that, using the compo440 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 440–445, Sofia, Bulgaria, August 4-9 2013. c�2013 </context>
<context position="13405" citStr="Baroni et al, 2012" startWordPosition="2243" endWordPosition="2246">man judgements over the general informativeness of lexical items. So in order to evaluate our proposed measure, we investigate its ability to retrieve the right ordering of hyponym pairs, which can be considered a subset of the issue at hand. Our assumption is that if X is a hypernym of Y , then the information content in X will be lower than in Y (because it has a more ‘general’ meaning). So, given a pair of words {w1, w2} in a known hyponymy relation, we should be able to tell which of w1 or w2 is the hypernym by computing the respective KL divergences. We use the hypernym data provided by (Baroni et al, 2012) as testbed for our experiment.2 This set of hyponym-hypernym pairs contains 1385 instances retrieved from the WordNet hierarchy. Before running our system on the data, we make slight modifications to it. First, as our distributions are created over the British National Corpus, some spellings must be converted to British English: for instance, color is replaced by colour. Second, five of the nouns included in the test set are not in the BNC. Those nouns are brethren, intranet, iPod, webcam and IX. We remove the pairs containing those words from the data. Third, numbers such as eleven or sixty </context>
</contexts>
<marker>Baroni, Bernardi, Zamparelli, 2012</marker>
<rawString>Baroni, Marco, Raffaella Bernardi, and Roberto Zamparelli. 2012. Frege in Space: a Program for Compositional Distributional Semantics. Under review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Curran</author>
</authors>
<title>From Distributional to Semantic Similarity.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh,</institution>
<location>Scotland, UK.</location>
<contexts>
<context position="1191" citStr="Curran, 2003" startWordPosition="169" endWordPosition="170">tent can be computed using the KullbackLeibler (KL) divergence, an informationtheoretic measure of the relative entropy of two distributions. In a task focusing on retrieving the correct ordering of hyponym-hypernym pairs, the KL divergence achieves close to 80% precision but does not outperform a simpler (linguistically unmotivated) frequency measure. We suggest that this result illustrates the rather ‘intensional’ aspect of distributions. 1 Introduction Distributional semantics is a representation of lexical meaning that relies on a statistical analysis of the way words are used in corpora (Curran, 2003; Turney and Pantel, 2010; Erk, 2012). In this framework, the semantics of a lexical item is accounted for by modelling its co-occurrence with other words (or any larger lexical context). The representation of a target word is thus a vector in a space where each dimension corresponds to a possible context. The weights of the vector components can take various forms, ranging from simple co-occurrence frequencies to functions such as Pointwise Mutual Information (for an overview, see (Evert, 2004)). This paper investigates the issue of computing the semantic content of distributional vectors. Th</context>
</contexts>
<marker>Curran, 2003</marker>
<rawString>Curran, James. 2003. From Distributional to Semantic Similarity. Ph.D. thesis, University of Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>Vector space models of word meaning and phrase meaning: a survey. Language and Linguistics Compass,</title>
<date>2012</date>
<pages>6--10</pages>
<contexts>
<context position="1228" citStr="Erk, 2012" startWordPosition="175" endWordPosition="176">Leibler (KL) divergence, an informationtheoretic measure of the relative entropy of two distributions. In a task focusing on retrieving the correct ordering of hyponym-hypernym pairs, the KL divergence achieves close to 80% precision but does not outperform a simpler (linguistically unmotivated) frequency measure. We suggest that this result illustrates the rather ‘intensional’ aspect of distributions. 1 Introduction Distributional semantics is a representation of lexical meaning that relies on a statistical analysis of the way words are used in corpora (Curran, 2003; Turney and Pantel, 2010; Erk, 2012). In this framework, the semantics of a lexical item is accounted for by modelling its co-occurrence with other words (or any larger lexical context). The representation of a target word is thus a vector in a space where each dimension corresponds to a possible context. The weights of the vector components can take various forms, ranging from simple co-occurrence frequencies to functions such as Pointwise Mutual Information (for an overview, see (Evert, 2004)). This paper investigates the issue of computing the semantic content of distributional vectors. That is, we look at the ways we can dis</context>
</contexts>
<marker>Erk, 2012</marker>
<rawString>Erk, Katrin. 2012. Vector space models of word meaning and phrase meaning: a survey. Language and Linguistics Compass, 6:10:635–653.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>Towards a semantics for distributional representations.</title>
<date>2013</date>
<booktitle>In Proceedings of the Tenth International Conference on Computational Semantics (IWCS2013).</booktitle>
<contexts>
<context position="16601" citStr="Erk, 2013" startWordPosition="2776" endWordPosition="2777">ne, coffee and tea dimensions, for instance, i.e. there is a large difference between p(cfood) and p(cfood|t), etc. Although beverage is an umbrella word for many various types of drinks, speakers of English use it in very particular contexts. So, distributionally, it is not a ‘general word’. Similar observations can be made for, e.g. liquid (strongly associated with gas, presumably via coordination), anniversary (linked to the verb mark or the noun silver), or again projectile (co-occurring with weapon, motion and speed). The general point is that, as pointed out elsewhere in the literature (Erk, 2013), distributions are a good representation of (some aspects of) intension, but they are less apt to model extension.3 So a term with a large extension like beverage may have a more restricted (distributional) intension than a word with a smaller extension, such as 3We qualify ‘intension’ here, because in the sense of a mapping from possible worlds to extensions, intension cannot be said to be provided by distributions: the distribution of beverage, it seems, does not allow us to successfully pick out all beverages in the real world. 443 beer.4 Contributing to this issue, fixed phrases, named en</context>
</contexts>
<marker>Erk, 2013</marker>
<rawString>Erk, Katrin. 2013. Towards a semantics for distributional representations. In Proceedings of the Tenth International Conference on Computational Semantics (IWCS2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Evert</author>
</authors>
<title>The statistics of word cooccurrences: word pairs and collocations.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Stuttgart.</institution>
<contexts>
<context position="1691" citStr="Evert, 2004" startWordPosition="251" endWordPosition="252">tion of lexical meaning that relies on a statistical analysis of the way words are used in corpora (Curran, 2003; Turney and Pantel, 2010; Erk, 2012). In this framework, the semantics of a lexical item is accounted for by modelling its co-occurrence with other words (or any larger lexical context). The representation of a target word is thus a vector in a space where each dimension corresponds to a possible context. The weights of the vector components can take various forms, ranging from simple co-occurrence frequencies to functions such as Pointwise Mutual Information (for an overview, see (Evert, 2004)). This paper investigates the issue of computing the semantic content of distributional vectors. That is, we look at the ways we can distributionally express that make is a more general verb than produce, which is itself more general than, for instance, weave. Although the task is related to the identification of hyponymy relations, it aims to reflect a more encompassing phenomenon: we wish to be able to compare the semantic content of words within parts-of-speech where the standard notion of hyponymy does not apply (e.g. prepositions: see with vs. next to or of vs. concerning) and across par</context>
</contexts>
<marker>Evert, 2004</marker>
<rawString>Evert, Stefan. 2004. The statistics of word cooccurrences: word pairs and collocations. Ph.D. thesis, University of Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Leech</author>
<author>Roger Garside</author>
<author>Michael Bryant</author>
</authors>
<title>Claws4: The tagging of the british national corpus.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics (COLING 94),</booktitle>
<pages>622--628</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="9017" citStr="Leech, Garside, and Bryant, 1994" startWordPosition="1481" endWordPosition="1485">theories where function words are not assigned a distributional representation. McNally suggests that the most appropriate way to separate function 441 from content words might, in the end, involve taking into account how much ‘descriptive’ content they have. 4 An implementation of a distributional system The distributional system we implemented for this paper is close to the system of Mitchell and Lapata (2010) (subsequently M&amp;L). As background data, we use the British National Corpus (BNC) in lemmatised format. Each lemma is followed by a part of speech according to the CLAWS tagset format (Leech, Garside, and Bryant, 1994). For our experiments, we only keep the first letter of each part-of-speech tag, thus obtaining broad categories such as N or V. Furthermore, we only retain words in the following categories: nouns, verbs, adjectives and adverbs (punctuation is ignored). Each article in the corpus is converted into a 11-word window format, that is, we are assuming that context in our system is defined by the five words preceding and the five words following the target. To calculate co-occurrences, we use the following equations: Efreqci = freqci,t (1) t Efreqt = freqci,t (2) ci Efreqtotal = freqci,t (3) ci,t </context>
</contexts>
<marker>Leech, Garside, Bryant, 1994</marker>
<rawString>Leech, Geoffrey, Roger Garside, and Michael Bryant. 1994. Claws4: The tagging of the british national corpus. In Proceedings of the 15th International Conference on Computational Linguistics (COLING 94), pages 622–628, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lund</author>
<author>Curt Burgess</author>
<author>Ruth Ann Atchley</author>
</authors>
<title>Semantic and associative priming in highdimensional semantic space.</title>
<date>1995</date>
<booktitle>In Proceedings of the 17th annual conference of the Cognitive Science Society,</booktitle>
<volume>17</volume>
<pages>660--665</pages>
<marker>Lund, Burgess, Atchley, 1995</marker>
<rawString>Lund, Kevin, Curt Burgess, and Ruth Ann Atchley. 1995. Semantic and associative priming in highdimensional semantic space. In Proceedings of the 17th annual conference of the Cognitive Science Society, Vol. 17, pages 660–665.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Louise McNally</author>
</authors>
<title>Formal and distributional semantics: From romance to relationship.</title>
<date>2013</date>
<booktitle>In Proceedings of the ‘Towards a Formal Distributional Semantics’ workshop, 10th International Conference on Computational Semantics (IWCS2013),</booktitle>
<location>Potsdam, Germany.</location>
<note>Invited talk.</note>
<contexts>
<context position="8292" citStr="McNally (2013)" startWordPosition="1365" endWordPosition="1366">many computational linguists, it is an essential tool for modelling phenomena affected by lexical meaning. If distributional semantics is to be seen as a general-purpose representation, however, we should evaluate it across all properties which we deem relevant to a model of the lexicon. We consider semantic content to be one such property. It underlies the notion of hyponymy and naturally models our intuitions about the ‘precision’ (as opposed to ‘vagueness’) of words. Further, semantic content may be crucial in solving some fundamental problems of distributional semantics. As pointed out by McNally (2013), there is no easy way to define the notion of a function word and this has consequences for theories where function words are not assigned a distributional representation. McNally suggests that the most appropriate way to separate function 441 from content words might, in the end, involve taking into account how much ‘descriptive’ content they have. 4 An implementation of a distributional system The distributional system we implemented for this paper is close to the system of Mitchell and Lapata (2010) (subsequently M&amp;L). As background data, we use the British National Corpus (BNC) in lemmati</context>
</contexts>
<marker>McNally, 2013</marker>
<rawString>McNally, Louise. 2013. Formal and distributional semantics: From romance to relationship. In Proceedings of the ‘Towards a Formal Distributional Semantics’ workshop, 10th International Conference on Computational Semantics (IWCS2013), Potsdam, Germany. Invited talk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in Distributional Models of Semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="8800" citStr="Mitchell and Lapata (2010)" startWordPosition="1445" endWordPosition="1449">tent may be crucial in solving some fundamental problems of distributional semantics. As pointed out by McNally (2013), there is no easy way to define the notion of a function word and this has consequences for theories where function words are not assigned a distributional representation. McNally suggests that the most appropriate way to separate function 441 from content words might, in the end, involve taking into account how much ‘descriptive’ content they have. 4 An implementation of a distributional system The distributional system we implemented for this paper is close to the system of Mitchell and Lapata (2010) (subsequently M&amp;L). As background data, we use the British National Corpus (BNC) in lemmatised format. Each lemma is followed by a part of speech according to the CLAWS tagset format (Leech, Garside, and Bryant, 1994). For our experiments, we only keep the first letter of each part-of-speech tag, thus obtaining broad categories such as N or V. Furthermore, we only retain words in the following categories: nouns, verbs, adjectives and adverbs (punctuation is ignored). Each article in the corpus is converted into a 11-word window format, that is, we are assuming that context in our system is de</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Mitchell, Jeff and Mirella Lapata. 2010. Composition in Distributional Models of Semantics. Cognitive Science, 34(8):1388–1429, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI-95),</booktitle>
<pages>448--453</pages>
<contexts>
<context position="10393" citStr="Resnik (1995)" startWordPosition="1719" endWordPosition="1720">eqt frequency of the target word t freqci frequency of the context word ci As in M&amp;L, we use the 2000 most frequent words in our corpus as the semantic space dimensions. M&amp;L calculate the weight of each context term in the distribution as follows: freqci,t x freqtotal (4) freqt x freqci We will not directly use the measure vi(t) as it is not a probability distribution and so is not suitable for entropic analysis; instead our analysis will be phrased in terms of the probability distributions p(ci|t) and p(ci) (the numerator and denominator in vi(t)). 5 Semantic content as entropy: two measures Resnik (1995) uses the notion of information content to improve on the standard edge counting methods proposed to measure similarity in taxonomies such as WordNet. He proposes that the information content of a term t is given by the selfinformation measure −log p(t). The idea behind this measure is that, as the frequency of the term increases, its informativeness decreases. Although a good first approximation, the measure cannot be said to truly reflect our concept of semantic content. For instance, in the British National Corpus, time and see are more frequent than thing or may and man is more frequent th</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Resnik, Philipp. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI-95), pages 448–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Searle</author>
</authors>
<title>Speech acts: An essay in the philosophy of language.</title>
<date>1969</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="4294" citStr="Searle (1969)" startWordPosition="674" endWordPosition="675">l Linguistics, pages 440–445, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics nents of the described weighting measure, which are both probability distributions, we can calculate the relative entropy of a distribution by inserting those probability distributions in the equation for the Kullback-Leibler (KL) divergence. We finally evaluate the KL measure against a basic notion of frequency and conclude with some error analysis. 2 Semantic content As a first approximation, we will define semantic content as informativeness with respect to denotation. Following Searle (1969), we will take a ‘successful reference’ to be a speech act where the choice of words used by the speaker appropriately identifies a referent for the hearer. Glossing over questions of pragmatics, we will assume that a more informative word is more likely to lead to a successful reference than a less informative one. That is, if Kim owns a cat and a dog, the identifying expression my cat is a better referent than my pet and so cat can be said to have more semantic content than pet. While our definition relies on reference, it also posits a correspondence between actual utterances and denotation</context>
</contexts>
<marker>Searle, 1969</marker>
<rawString>Searle, John R. 1969. Speech acts: An essay in the philosophy of language. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="1216" citStr="Turney and Pantel, 2010" startWordPosition="171" endWordPosition="174">mputed using the KullbackLeibler (KL) divergence, an informationtheoretic measure of the relative entropy of two distributions. In a task focusing on retrieving the correct ordering of hyponym-hypernym pairs, the KL divergence achieves close to 80% precision but does not outperform a simpler (linguistically unmotivated) frequency measure. We suggest that this result illustrates the rather ‘intensional’ aspect of distributions. 1 Introduction Distributional semantics is a representation of lexical meaning that relies on a statistical analysis of the way words are used in corpora (Curran, 2003; Turney and Pantel, 2010; Erk, 2012). In this framework, the semantics of a lexical item is accounted for by modelling its co-occurrence with other words (or any larger lexical context). The representation of a target word is thus a vector in a space where each dimension corresponds to a possible context. The weights of the vector components can take various forms, ranging from simple co-occurrence frequencies to functions such as Pointwise Mutual Information (for an overview, see (Evert, 2004)). This paper investigates the issue of computing the semantic content of distributional vectors. That is, we look at the way</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Turney, Peter D. and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>