<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001847">
<title confidence="0.837493">
Coarse to Fine Grained Sense Disambiguation in Wikipedia
</title>
<author confidence="0.993825">
Hui Shen
</author>
<affiliation confidence="0.997015">
School of EECS
Ohio University
</affiliation>
<address confidence="0.891309">
Athens, OH 45701, USA
</address>
<email confidence="0.998716">
hui.shen.1@ohio.edu
</email>
<author confidence="0.985293">
Razvan Bunescu
</author>
<affiliation confidence="0.9954995">
School of EECS
Ohio University
</affiliation>
<address confidence="0.968519">
Athens, OH 45701, USA
</address>
<email confidence="0.999397">
bunescu@ohio.edu
</email>
<author confidence="0.993439">
Rada Mihalcea
</author>
<affiliation confidence="0.99873">
Department of CSE
University of North Texas
</affiliation>
<address confidence="0.741251">
Denton, TX 76203, USA
</address>
<email confidence="0.999332">
rada@cs.unt.edu
</email>
<sectionHeader confidence="0.995647" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99814825">
Wikipedia articles are annotated by volunteer
contributors with numerous links that connect
words and phrases to relevant titles. Links
to general senses of a word are used concur-
rently with links to more specific senses, with-
out being distinguished explicitly. We present
an approach to training coarse to fine grained
sense disambiguation systems in the presence
of such annotation inconsistencies. Experi-
mental results show that accounting for anno-
tation ambiguity in Wikipedia links leads to
significant improvements in disambiguation.
</bodyText>
<sectionHeader confidence="0.986971" genericHeader="categories and subject descriptors">
1 Introduction and Motivation
</sectionHeader>
<bodyText confidence="0.999951578947369">
The vast amount of world knowledge available in
Wikipedia has been shown to benefit many types
of text processing tasks, such as coreference res-
olution (Ponzetto and Strube, 2006; Haghighi and
Klein, 2009; Bryl et al., 2010; Rahman and Ng,
2011), information retrieval (Milne, 2007; Li et al.,
2007; Potthast et al., 2008; Cimiano et al., 2009),
or question answering (Ahn et al., 2004; Kaisser,
2008; Ferrucci et al., 2010). In particular, the user
contributed link structure of Wikipedia has been
shown to provide useful supervision for training
named entity disambiguation (Bunescu and Pasca,
2006; Cucerzan, 2007) and word sense disambigua-
tion (Mihalcea, 2007; Ponzetto and Navigli, 2010)
systems. Articles in Wikipedia often contain men-
tions of concepts or entities that already have a cor-
responding article. When contributing authors men-
tion an existing Wikipedia entity inside an article,
they are required to link at least its first mention to
</bodyText>
<page confidence="0.943673">
22
</page>
<bodyText confidence="0.999406588235294">
the corresponding article, by using links or piped
links. Consider, for example, the following Wiki
source annotations: The [[capital city|capital]] of
Georgia is [[Atlanta]]. The bracketed strings iden-
tify the title of the Wikipedia articles that describe
the corresponding named entities. If the editor wants
a different string displayed in the rendered text, then
the alternative string is included in a piped link, af-
ter the title string. Based on these Wiki processing
rules, the text that is rendered for the aforementioned
example is: The capital of Georgia is Atlanta.
Since many words and names mentioned in
Wikipedia articles are inherently ambiguous, their
corresponding links can be seen as a useful source
of supervision for training named entity and word
sense disambiguation systems. For example,
Wikipedia contains articles that describe possible
senses of the word “capital”, such as CAPITAL CITY,
CAPITAL (ECONOMICS), FINANCIAL CAPITAL, or
HUMAN CAPITAL, to name only a few. When dis-
ambiguating a word or a phrase in Wikipedia, a con-
tributor uses the context to determine the appropriate
Wikipedia title to include in the link. In the exam-
ple above, the editor of the article determined that
the word “capital” was mentioned with the political
center meaning, consequently it was mapped to the
article CAPITAL CITY through a piped link.
In order to use Wikipedia links for training a WSD
system for a given word, one needs first to define a
sense repository that specifies the possible meanings
for that word, and then use the Wikipedia links to
create training examples for each sense in the repos-
itory. This approach might be implemented using
the following sequence of steps:
</bodyText>
<note confidence="0.9639985">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 22–31, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics
In global climate models, the state and properties of the [[atmosphere]] are specified at a number of discrete locations
General = ATMOSPHERE; Specific = ATMOSPHERE OF EARTH; Label = A → A(S) → AE
The principal natural phenomena that contribute gases to the [[Atmosphere of Earth|atmosphere]] are emissions from volcanoes
General = ATMOSPHERE; Specific = ATMOSPHERE OF EARTH; Label = A → A(S) → AE
An aerogravity assist is a spacecraft maneuver designed to change velocity when arriving at a body with an [[atmosphere]]
General = ATMOSPHERE; Specific = ATMOSPHERE ⊲ generic; Label = A → A(G)
Assuming the planet’s [[atmosphere]] is close to equilibrium, it is predicted that 55 Cancri d is covered with water clouds
General = ATMOSPHERE; Specific = ATMOSPHERE OF CANCRI ⊲ missing; A → A(G)
</note>
<figureCaption confidence="0.941564">
Figure 1: Coarse and fine grained sense annotations in Wikipedia (bold). The proposed hierarchical Label (right).
A(S) = ATMOSPHERE (S), A(G) = ATMOSPHERE (G), A = ATMOSPHERE, AE = ATMOSPHERE OF EARTH.
</figureCaption>
<listItem confidence="0.992788363636364">
1. Collect all Wikipedia titles that are linked from
the ambiguous anchor word.
2. Create a repository of senses from all titles that
have sufficient support in Wikipedia i.e., titles
that are referenced at least a predefined min-
imum number of times using the ambiguous
word as anchor.
3. Use the links extracted for each sense in the
repository as labeled examples for that sense
and train a WSD model to distinguish between
alternative senses of the ambiguous word.
</listItem>
<bodyText confidence="0.996847851851852">
Taking the word “atmosphere” as an example, the
first step would result in a wide array of titles,
ranging from the general ATMOSPHERE and its in-
stantiations ATMOSPHERE OF EARTH or ATMO-
SPHERE OF MARS, to titles as diverse as ATMO-
SPHERE (UNIT), MOOD (PSYCHOLOGY), or AT-
MOSPHERE (MUSIC GROUP). In the second step,
the most frequent titles for the anchor word “at-
mosphere” would be assembled into a repository R
= {ATMOSPHERE, ATMOSPHERE OF EARTH, AT-
MOSPHERE OF MARS, ATMOSPHERE OF VENUS,
STELLAR ATMOSPHERE, ATMOSPHERE (UNIT),
ATMOSPHERE (MUSIC GROUP)}. The classifier
trained in the third step would use features ex-
tracted from the context to discriminate between
word senses.
This Wikipedia-based approach to creating train-
ing data for word sense disambiguation has a ma-
jor shortcoming. Many of the training examples ex-
tracted for the title ATMOSPHERE could very well
belong to more specific titles such as ATMOSPHERE
OF EARTH or ATMOSPHERE OF MARS. Whenever
the word “atmosphere” is used in a context with the
sense of “a layer of gases that may surround a ma-
terial body of sufficient mass, and that is held in
place by the gravity of the body,” the contributor
has the option of adding a link either to the title AT-
MOSPHERE that describes this general sense of the
word, or to the title of an article that describes the
atmosphere of the actual celestial body that is re-
ferred in that particular context, as shown in the first
2 examples in Figure 1. As shown in bold in Fig-
ure 1, different occurrences of the same word may
be tagged with either a general or a specific link, an
ambiguity that is pervasive in Wikipedia for words
like ”atmosphere” that have general senses that sub-
sume multiple, popular specific senses. There does
not seem to be a clear, general rule underlying the
decision to tag a word or a phrase with a general
or specific sense link in Wikipedia. We hypothesize
that, in some cases, editors may be unaware that an
article exists in Wikipedia for the actual reference
of a word or for a more specific sense of the word,
and therefore they end up using a link to an article
describing the general sense of the word. There is
also the possibility that more specific articles are in-
troduced only in newer versions of Wikipedia, and
thus earlier annotations were not aware of these re-
cent articles. Furthermore, since annotating words
with the most specific sense available in Wikipedia
may require substantial cognitive effort, editors may
often choose to link to a general sense of the word, a
choice that is still correct, yet less informative than
the more specific sense.
</bodyText>
<sectionHeader confidence="0.828681" genericHeader="method">
2 Annotation Inconsistencies in Wikipedia
</sectionHeader>
<bodyText confidence="0.999964333333333">
In order to get a sense of the potential magnitude
of the general vs. specific sense annotation ambi-
guity, we extracted all Wikipedia link annotations
</bodyText>
<page confidence="0.996628">
23
</page>
<bodyText confidence="0.996374916666667">
for the words “atmosphere”, “president”, “game”,
“dollar”, “diamond” and “Corinth”, and created
a special subset from those that were labeled by
Wikipedia editors with the general sense links AT-
MOSPHERE, PRESIDENT, GAME, DOLLAR, DIA-
MOND, and CORINTH, respectively. Then, for each
of the 7,079 links in this set, we used the context
to manually determine the corresponding more spe-
cific title, whenever such a title exists in Wikipedia.
The statistics in Tables 1 and 2 show a significant
overlap between the general and specific sense cate-
gories. For example, out of the 932 links from “at-
mosphere” to ATMOSPHERE that were extracted in
total, 518 were actually about the ATMOSPHERE OF
EARTH, but the user linked them to the more general
sense category ATMOSPHERE. On the other hand,
there are 345 links to ATMOSPHERE OF EARTH that
were explicitly made by the user. We manually as-
signed general links (G) whenever the word is used
with a generic sense, or when the reference is not
available in the repository of titles collected for that
word because either the more specific title does not
exist in Wikipedia or the specific title exists, but it
does not have sufficient support – at least 20 linked
anchors – in Wikipedia. We grouped the more spe-
cific links for any given sense into a special cate-
gory suffixed with (S), to distinguish them from the
general links (generic use, or missing reference) that
were grouped into the category suffixed with (G).
For many ambiguous words, the annotation in-
consistencies appear when the word has senses
that are in a subsumption relationship: the ATMO-
SPHERE OF EARTH is an instance of ATMOSPHERE,
whereas a STELLAR ATMOSPHERE is a particular
type of ATMOSPHERE. Subsumed senses can be
identified automatically using the category graph in
Wikipedia. The word “Corinth” is an interesting
case: the subsumption relationship between AN-
CIENT CORINTH and CORINTH appears because of
a temporal constraint. Furthermore, in the case of
the word “diamond”, the annotation inconsistencies
are not caused by a subsumption relation between
senses. Instead of linking to the DIAMOND (GEM-
STONE) sense, Wikipedia contributors often link to
the related DIAMOND sense indicating the mineral
used in the gemstone.
A supervised learning algorithm that uses the ex-
tracted links for training a WSD classification model
</bodyText>
<table confidence="0.99930475">
atmosphere Size
ATMOSPHERE 932
Atmosphere (S) 559
Atmosphere of Earth 518
Atmosphere of Mars 19
Atmosphere of Venus 9
Stellar Atmosphere 13
Atmosphere (G) 373
ATMOSPHERE OF EARTH 345
ATMOSPHERE OF MARS 37
ATMOSPHERE OF VENUS 26
STELLAR ATMOSPHERE 29
ATMOSPHERE (UNIT) 96
ATMOSPHERE (MUSIC GROUP) 104
president Size
PRESIDENT 3534
President (S) 989
Chancellor (education) 326
President of the United States 534
President of the Philippines 42
President of Pakistan 27
President of France 22
President of India 21
President of Russia 17
President (G) 2545
CHANCELLOR (EDUCATION) 210
PRESIDENT OF THE UNITED STATES 5941
PRESIDENT OF THE PHILIPPINES 549
PRESIDENT OF PAKISTAN 192
PRESIDENT OF FRANCE 151
PRESIDENT OF INDIA 86
PRESIDENT OF RUSSIA 101
</table>
<tableCaption confidence="0.999917">
Table 1: Wiki (CAPS) and manual (italics) annotations.
</tableCaption>
<bodyText confidence="0.999972277777778">
to distinguish between categories in the sense repos-
itory assumes implicitly that the categories, and
hence their training examples, are mutually disjoint.
This assumption is clearly violated for words like
“atmosphere,” consequently the learned model will
have a poor performance on distinguishing between
the overlapping categories. Alternatively, we can
say that sense categories like ATMOSPHERE are ill
defined, since their supporting dataset contains ex-
amples that could also belong to more specific sense
categories such as ATMOSPHERE OF EARTH.
We see two possible solutions to the problem of
inconsistent link annotations. In one solution, spe-
cific senses are grouped together with the subsuming
general sense, such that all categories in the result-
ing repository become disjoint. For “atmosphere”,
the general category ATMOSPHERE would be aug-
mented to contain all the links previously annotated
</bodyText>
<page confidence="0.996481">
24
</page>
<table confidence="0.999824263157895">
dollar Size
DOLLAR 379
Dollar (S) 231
United States dollar 228
Canadian dollar 3
Australian dollar 1
Dollar (G) 147
UNITED STATES DOLLAR 3516
CANADIAN DOLLAR 420
AUSTRALIAN DOLLAR 124
DOLLAR SIGN 290
DOLLAR (BAND) 30
DOLLAR, CLACKMANNANSHIRE 30
game Size
GAME 819
Game (S) 99
Video game 55
PC game 44
Game (G) 720
VIDEO GAME 312
PC GAME 24
GAME (FOOD) 232
GAME (RAPPER) 154
diamond Size
DIAMOND 716
Diamond (S) 221
Diamond (gemstone) 221
Diamond (G) 495
DIAMOND (GEMSTONE) 71
BASEBALL FIELD 36
MUSIC RECORDING SALES CERT. 36
Corinth Size
CORINTH 699
Corinth (S) 409
Ancient Corinth 409
Corinth (G) 290
ANCIENT CORINTH 92
CORINTH, MISSISSIPPI 72
</table>
<tableCaption confidence="0.99935">
Table 2: Wiki (CAPS) and manual (italics) annotations.
</tableCaption>
<bodyText confidence="0.975273388888889">
as ATMOSPHERE, ATMOSPHERE OF EARTH, AT-
MOSPHERE OF MARS, ATMOSPHERE OF VENUS,
or STELLAR ATMOSPHERE. This solution is
straightforward to implement, however it has the
disadvantage that the resulting WSD model will
never link words to more specific titles in Wikipedia
like ATMOSPHERE OF MARS.
Another solution is to reorganize the original
sense repository into a hierarchical classification
scheme such that sense categories at each classifi-
cation level become mutually disjoint. The resulting
WSD system has the advantage that it can make fine
grained sense distinctions for an ambiguous word,
despite the annotation inconsistencies present in the
training data. The rest of this paper describes a feasi-
ble implementation for this second solution that does
not require any manual annotation beyond the links
that are already provided by Wikipedia volunteers.
</bodyText>
<sectionHeader confidence="0.7874705" genericHeader="method">
3 Learning for Coarse to Fine Grained
Sense Disambiguation
</sectionHeader>
<bodyText confidence="0.999947717948718">
Figure 2 shows our proposed hierarchical classifica-
tion scheme for disambiguation, using “atmosphere”
as the ambiguous word. Shaded leaf nodes show
the final categories in the sense repository for each
word, whereas the doted elliptical frames on the
second level in the hierarchy denote artificial cate-
gories introduced to enable a finer grained classifi-
cation into more specific senses. Thick dotted ar-
rows illustrate the classification decisions that are
made in order to obtain a fine grained disambigua-
tion of the word. Thus, the word “atmosphere”
is first classified to have the general sense ATMO-
SPHERE, i.e. “a layer of gases that may surround a
material body of sufficient mass, and that is held in
place by the gravity of the body”. In the first so-
lution, the disambiguation process would stop here
and output the general sense ATMOSPHERE. In the
second solution, the disambiguation process contin-
ues and further classifies the word to be a reference
to ATMOSPHERE OF EARTH. To get to this final
classification, the process passes through an inter-
mediate binary classification level where it deter-
mines whether the word has a more specific sense
covered in Wikipedia, corresponding to the artificial
category ATMOSPHERE (S). If the answer is no, the
system stops the disambiguation process and out-
puts the general sense category ATMOSPHERE. This
basic sense hierarchy can be replicated depending
on the existence of even finer sense distinctions in
Wikipedia. For example, Wikipedia articles describ-
ing atmospheres of particular stars could be used to
further refine STELLAR ATMOSPHERE with two ad-
ditional levels of the type Level 2 and Level 3. Over-
all, the proposed disambiguation scheme could be
used to relabel the ATMOSPHERE links in Wikipedia
with more specific, and therefore more informative,
senses such as ATMOSPHERE OF EARTH. In gen-
eral, the Wikipedia category graph could be used
to automatically create hierarchical structures for re-
</bodyText>
<page confidence="0.996154">
25
</page>
<figureCaption confidence="0.998444">
Figure 2: Hierarchical disambiguation scheme, from coarse to fine grained senses.
</figureCaption>
<bodyText confidence="0.971060277777778">
lated senses of the same word.
Training word sense classifiers for Levels 1 and 3
is straightforward. For Level 1, Wikipedia links that
are annotated by users as ATMOSPHERE, ATMO-
SPHERE OF EARTH, ATMOSPHERE OF MARS, AT-
MOSPHERE OF VENUS, or STELLAR ATMOSPHERE
are collected as training examples for the general
sense category ATMOSPHERE. Similarly, links that
are annotated as ATMOSPHERE (UNIT) and ATMO-
SPHERE (MUSIC GROUP) will be used as training
examples for the two categories, respectively. A
multiclass classifier is then trained to distinguish be-
tween the three categories at this level. For Level 3,
a multiclass classifiers is trained on Wikipedia links
collected for each of the 4 specific senses.
For the binary classifier at Level 2, we could
use as training examples for the category ATMO-
SPHERE (G) all Wikipedia links that were anno-
tated as ATMOSPHERE, whereas for the category
ATMOSPHERE (S) we could use as training exam-
ples all Wikipedia links that were annotated specif-
ically as ATMOSPHERE OF EARTH, ATMOSPHERE
OF MARS, ATMOSPHERE OF VENUS, or STELLAR
ATMOSPHERE. A traditional binary classification
SVM could be trained on this dataset to distinguish
between the two categories. We call this approach
Naive SVM, since it does not account for the fact that
a significant number of the links that are annotated
by Wikipedia contributors as ATMOSPHERE should
actually belong to the ATMOSPHERE (S) category –
about 60% of them, according to Table 1. Instead,
we propose treating all ATMOSPHERE links as unla-
beled examples. If we consider the specific links in
ATMOSPHERE (S) to be positive examples, then the
problem becomes one of learning with positive and
unlabeled examples.
</bodyText>
<subsectionHeader confidence="0.9763515">
3.1 Learning with positive and unlabeled
examples
</subsectionHeader>
<bodyText confidence="0.999694304347826">
This general type of semi-supervised learning has
been studied before in the context of tasks such
as text classification and information retrieval (Lee
and Liu, 2003; Liu et al., 2003), or bioinformat-
ics (Elkan and Noto, 2008; Noto et al., 2008). In
this setting, the training data consists of positive ex-
amples x E P and unlabeled examples x E U.
Following the notation of Elkan and Noto (2008),
we define s(x) = 1 if the example is positive and
s(x) = −1 if the example is unlabeled. The true
label of an example is y(x) = 1 if the example
is positive and y(x) = −1 if the example is neg-
ative. Thus, x E P =�- s(x) = y(x) = 1 and
x E U =�- s(x) = −1 i.e., the true label y(x) of an
unlabeled example is unknown. For the experiments
reported in this paper, we use our implementation
of two state-of-the-art approaches to Learning with
Positive and Unlabeled (LPU) examples: the Biased
SVM formulation of Lee and Liu (2003) and the
Weighted Samples SVM formulation of Elkan and
Noto (2008). The original version of Biased SVM
was designed to maximize the product between pre-
cision and recall. In the next section we describe a
</bodyText>
<page confidence="0.978935">
26
</page>
<bodyText confidence="0.998778">
modification to the Biased SVM approach that can
be used to maximize accuracy, a measure that is of-
ten used to evaluate WSD performance.
</bodyText>
<subsectionHeader confidence="0.634586">
3.1.1 The Biased SVM
</subsectionHeader>
<bodyText confidence="0.966558666666667">
In the Biased SVM formulation (Lee and Liu,
2003; Liu et al., 2003), all unlabeled examples are
considered to be negative and the decision function
</bodyText>
<equation confidence="0.906918285714286">
f(x) = wT φ(x) + b is learned using the standard
soft-margin SVM formulation shown in Figure 3.
1 E
minimize: 2IIwII2 + CP
xEP
subject to: s(x) (wT φ(x) + b) &gt; 1 − ξx
ξx &gt; 0, dxEPUU
</equation>
<figureCaption confidence="0.997863">
Figure 3: Biased SVM optimization problem.
</figureCaption>
<bodyText confidence="0.998384821428571">
The capacity parameters CP and CU control how
much we penalize errors on positive examples vs. er-
rors on unlabeled examples. Since not all unlabeled
examples are negative, one would want to select ca-
pacity parameters satisfying CP &gt; CU, such that
false negative errors are penalized more than false
positive errors. In order to find the best capacity pa-
rameters to use during training, the Biased SVM ap-
proach runs a grid search on a separate development
dataset. This search is aimed at finding values for
the parameters CP and CU that maximize pr, the
product between precision p = p(y = 1|f = 1) and
recall r = p(f = 1|y = 1). Lee and Liu (2003)
show that maximizing the pr criterion is equivalent
with maximizing the objective r2/p(f = 1), where
both r = p(f = 1|y = 1) and p(f = 1) can be es-
timated using the trained decision function f(x) on
the development dataset.
Maximizing the pr criterion in the original Biased
SVM formulation was motivated by the need to opti-
mize the F measure in information retrieval settings,
where F = 2pr(p+ r). In the rest of this section we
show that classification accuracy can be maximized
using only positive and unlabeled examples, an im-
portant result for problems where classification ac-
curacy is the target performance measure.
The accuracy of a binary decision function f(x)
is, by definition, acc = p(f = 1|y = 1) + p(f =
</bodyText>
<equation confidence="0.990681863636364">
−1|y = −1). Since the recall is r = p(f = 1|y =
1), the accuracy can be re-written as:
acc = r + 1 − p(f = 1|y = −1) (1)
Using Bayes’ rule twice, the false positive term
p(f = 1|y = −1) can be re-written as:
p(f = 1|y = −1) = p(f = 1)p(y = −1|f = 1)
p(y = −1)
p(f = 1)
= x (1 − p(y = 1|f = 1))
p(y = −1)
p(f = 1) x p(y = 1)p(f = 1|y = 1)
p(y = −1) p(f = 1)
p(f = 1) − p(y = 1) x r
= (2)
p(y = −1)
Plugging identity 2 in Equation 1 leads to:
r x p(y = 1) − p(f = 1)
acc = 1 + r +
p(y = −1)
r − p(f = 1)
= 1 + (3)
p(y = −1)
</equation>
<bodyText confidence="0.999658866666667">
Since p(y = −1) can be assimilated with a con-
stant, Equation 3 implies that maximizing accu-
racy is equivalent with maximizing the criterion
r − p(f = 1), where both the recall r and p(f = 1)
can be estimated on the positive and unlabeled ex-
amples from a separate development dataset.
In conclusion, one can use the original Biased
SVM formulation to maximize r2/p(f = 1), which
has been shown by Lee and Liu (2003) to maximize
pr, a criterion that has a similar behavior with the
F-measure used in retrieval applications. Alterna-
tively, if the target performance measure is accuracy,
we can choose instead to maximize r − p(f = 1),
which we have shown above to correspond to accu-
racy maximization.
</bodyText>
<subsectionHeader confidence="0.921735">
3.1.2 The Weighted Samples SVM
</subsectionHeader>
<bodyText confidence="0.9999567">
Elkan and Noto (2008) introduced two ap-
proaches for learning with positive and unlabeled
data. Both approaches are based on the assumption
that labeled examples {x|s(x) = 11 are selected at
random from the positive examples {x|y(x) = 11
i.e., p(s = 1|x, y = 1) = p(s = 1|y = 1). Their
best performing approach uses the positive and unla-
beled examples to train two distinct classifiers. First,
the dataset P U U is split into a training set and a
validation set, and a classifier g(x) is trained on the
</bodyText>
<equation confidence="0.9991816">
ξx + CU E ξx
xEU
p(f = 1)
=
p(y = −1)
</equation>
<page confidence="0.816655">
27
</page>
<bodyText confidence="0.404010333333333">
labeling s to approximate the label distribution i.e.
g(x) = p(s = 1|x). The validation set is then used
to estimate p(s = 1|y = 1) as follows:
</bodyText>
<equation confidence="0.991309">
1 � g(x) (4)
p(s=1|y=1) = p(s=1|x,y=1) = |P |
XEP
</equation>
<bodyText confidence="0.9953299">
The second and final classifier f(x) is trained on a
dataset of weighted examples that are sampled from
the original training set as follows:
– Each positive example x E P is copied as a
positive example in the new training set with
weight p(y = 1|x, s = 1) = 1.
– Each unlabeled example x E U is duplicated
into two training examples in the new dataset:
a positive example with weight p(y = 1|x, s =
0) and a negative example with weight p(y =
</bodyText>
<equation confidence="0.793152">
−1|x,s = 0) = 1 − p(y = 1|x,s = 0).
</equation>
<bodyText confidence="0.873746">
Elkan and Noto (2008) show that the weights above
can be derived as:
</bodyText>
<equation confidence="0.99994">
p(y=1|x,s=0) = 1−p(s=1|y=1)X p(s=1|x) (5)
p(s=1|y=1) 1−p(s=1|x)
</equation>
<bodyText confidence="0.999964777777778">
The output of the first classifier g(x) is used to
approximate the probability p(s = 1|x), whereas
p(s = 1|y = 1) is estimated using Equation 4.
The two classifiers g and f are trained using
SVMs and a linear kernel. Platt scaling is used with
the first classifier to obtain the probability estimates
g(x) = p(s = 1|x), which are then converted into
weights following Equations 4 and 5, and used dur-
ing the training of the second classifier.
</bodyText>
<sectionHeader confidence="0.99408" genericHeader="method">
4 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999812814814815">
We ran disambiguation experiments on the 6 am-
biguous words atmosphere, president, dollar, game,
diamond and Corinth. The corresponding Wikipedia
sense repositories have been summarized in Tables 1
and 2. All WSD classifiers used the same set of stan-
dard WSD features (Ng and Lee, 1996; Stevenson
and Wilks, 2001), such as words and their part-of-
speech tags in a window of 3 words around the am-
biguous keyword, the unigram and bigram content
words that are within 2 sentences of the current sen-
tence, the syntactic governor of the keyword, and
its chains of syntactic dependencies of lengths up to
two. Furthermore, for each example, a Wikipedia
specific feature was computed as the cosine similar-
ity between the context of the ambiguous word and
the text of the article for the target sense or reference.
The Level, and Level3 classifiers were trained us-
ing the SVM`lti component of the SVMlight pack-
age.1 The WSD classifiers were evaluated in a 4-fold
cross validation scenario in which 50% of the data
was used for training, 25% for tuning the capacity
parameter C, and 25% for testing. The final accu-
racy numbers, shown in Table 3, were computed by
averaging the results over the 4 folds. Since the word
president has only one sense on Level,, no classifier
needed to be trained for this case. Similarly, words
diamond and Corinth have only one sense on Level3.
</bodyText>
<table confidence="0.994915333333333">
atmosphere president dollar
Levels 93.1% — 94.1%
Level3 85.6% 82.2% 90.8%
game diamond Corinth
Levels 82.9% 95.5% 92.7%
Level3 92.9% — —
</table>
<tableCaption confidence="0.999628">
Table 3: Disambiguation accuracy at Levels 1 &amp; 3.
</tableCaption>
<bodyText confidence="0.999885916666667">
The evaluation of the binary classifiers at the sec-
ond level follows the same 4-fold cross validation
scheme that was used for Level, and Level3. The
manual labels for specific senses and references in
the unlabeled datasets are always ignored during
training and tuning and used only during testing.
We compare the Naive SVM, Biased SVM, and
Weighted SVM in the two evaluation settings, using
for all of them the same train/development/test splits
of the data and the same features. We emphasize
that our manual labels are used only for testing pur-
poses – the manual labels are ignored during train-
ing and tuning, when the data is assumed to contain
only positive and unlabeled examples. We imple-
mented the Biased SVM approach on top of the bi-
nary SVMlight package. The CP and CU parameters
of the Biased SVM were tuned through the c and j
parameters of SVMlight (c = CU and j = CP/CU).
Eventually, all three methods use the development
data for tuning the c and j parameters of the SVM.
However, whereas the Naive SVM tunes these pa-
rameters to optimize the accuracy with respect to the
noisy label s(x), the Biased SVM tunes the same pa-
rameters to maximize an estimate of the accuracy or
</bodyText>
<footnote confidence="0.997901">
1http://svmlight.joachims.org
</footnote>
<page confidence="0.998577">
28
</page>
<bodyText confidence="0.999977586956522">
F-measure with respect to the true label y(x). The
Weighted SVM approach was implemented on top
of the LibSVM2 package. Even though the original
Weighted SVM method of Elkan and Noto (2008)
does not specify tuning any parameters, we noticed
it gave better results when the capacity c and weight
j parameters were tuned for the first classifier g(x).
Table 4 shows the accuracy results of the three
methods for Level2, whereas Table 5 shows the F-
measure results. The Biased SVM outperforms the
Naive SVM on all the words, in terms of both ac-
curacy and F-measure. The most dramatic increases
are seen for the words atmosphere, game, diamond,
and Corinth. For these words, the number of pos-
itive examples is significantly smaller compared to
the total number of positive and unlabeled examples.
Thus, the percentage of positive examples relative to
the total number of positive and unlabeled examples
is 31.9% for atmosphere, 29.1% for game, 9.0% for
diamond, and 11.6% for Corinth. The positive to to-
tal ratio is however significantly larger for the other
two words: 67.2% for president and 91.5% for dol-
lar. When the number of positive examples is large,
the false negative noise from the unlabeled dataset
in the Naive SVM approach will be relatively small,
hence the good performance of Naive SVM in these
cases. To check whether this is the case, we have
also run experiments where we used only half of
the available positive examples for the word presi-
dent and one tenth of the positive examples for the
word dollar, such that the positive datasets became
comparable in size with the unlabeled datasets. The
results for these experiments are shown in Tables 4
and 5 in the rows labeled presidents and dollars. As
expected, the difference between the performance of
Naive SVM and Biased SVM gets larger on these
smaller datasets, especially for the word dollar.
The Weighted SVM outperforms the Naive SVM
on five out of the six words, the exception being the
word president. Comparatively, the Biased SVM
has a more stable behavior and overall results in a
more substantial improvement over the Naive SVM.
Based on these initial results, we see the Biased
SVM as the method of choice for learning with pos-
itive and unlabeled examples in the task of coarse to
fine grained sense disambiguation in Wikipedia.
</bodyText>
<footnote confidence="0.954386">
2http://www.csie.ntu.edu.tw/˜cjlin/libsvm
</footnote>
<table confidence="0.999899333333333">
Word NaiveSVM BiasedSVM WeightedSVM
atmosphere 39.9% 79.6% 75.0%
president 91.9% 92.5% 89.5%
dollar 96.0% 97.0% 97.1%
game 83.8% 87.1% 84.6%
diamond 70.2% 74.5% 75.1%
Corinth 46.2% 75.1% 51.9%
presidents 88.1% 90.6% 87.4%
dollars 70.3% 84.9% 70.6%
</table>
<tableCaption confidence="0.924146">
Table 4: Disambiguation accuracy at Level2.
</tableCaption>
<table confidence="0.999961111111111">
Word NaiveSVM BiasedSVM WeightedSVM
atmosphere 30.5% 86.0% 83.2%
president 94.4% 95.0% 92.8%
dollar 97.9% 98.4% 98.5%
game 75.1% 81.8% 77.5%
diamond 8.6% 53.5% 46.3%
Corinth 15.3% 81.2% 68.0%
presidents 90.0% 92.4% 89.5%
dollars 77.9% 91.2% 78.2%
</table>
<tableCaption confidence="0.997035">
Table 5: Disambiguation F-measure at Level2.
</tableCaption>
<bodyText confidence="0.9999724375">
In a final set of experiments, we compared the
traditional flat classification approach and our pro-
posed hierarchical classifier in terms of their over-
all disambiguation accuracy. In these experiments,
the sense repository contains all the leaf nodes as
distinct sense categories. For example, the word
atmosphere would correspond to the sense repos-
itory R = {ATMOSPHERE (G), ATMOSPHERE OF
EARTH, ATMOSPHERE OF MARS, ATMOSPHERE
OF VENUS, STELLAR ATMOSPHERE, ATMO-
SPHERE (UNIT), ATMOSPHERE (MUSIC GROUP)}.
The overall accuracy results are shown in Table 6
and confirm the utility of using the LPU framework
in the hierarchical model, which outperforms the tra-
ditional flat model, especially on words with low ra-
tio of positive to unlabeled examples.
</bodyText>
<table confidence="0.9879565">
atmosphere president dollar
Flat 52.4% 89.4% 90.0%
Hierarchical 79.7% 91.0% 90.1%
game diamond Corinth
Flat 83.6% 65.7% 42.6%
Hierarchical 87.2% 76.8% 72.1%
</table>
<tableCaption confidence="0.999221">
Table 6: Flat vs. Hierarchical disambiguation accuracy.
</tableCaption>
<page confidence="0.998778">
29
</page>
<sectionHeader confidence="0.999375" genericHeader="method">
5 Future Work
</sectionHeader>
<bodyText confidence="0.999863307692308">
Annotation inconsistencies in Wikipedia were cir-
cumvented by adapting two existing approaches that
use only positive and unlabeled data to train binary
classifiers. This binary classification constraint led
to the introduction of the artificial specific (S) cat-
egory on Level2 in our disambiguation framework.
In future work, we plan to investigate a direct exten-
sion of learning with positive and unlabeled data to
the case of multiclass classification, which will re-
duce the number of classification levels from 3 to 2.
We also plan to investigate the use of unsupervised
techniques in order to incorporate less popular refer-
ences of a word in the hierarchical classification.
</bodyText>
<sectionHeader confidence="0.941397" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.999973523809524">
We presented an approach to training coarse to fine
grained sense disambiguation systems that treats
annotation inconsistencies in Wikipedia under the
framework of learning with positive and unlabeled
examples. Furthermore, we showed that the true ac-
curacy of a decision function can be optimized us-
ing only positive and unlabeled examples. For test-
ing purposes, we manually annotated 7,079 links be-
longing to six ambiguous words 3. Experimental
results demonstrate that accounting for annotation
ambiguity in Wikipedia links leads to consistent im-
provements in disambiguation accuracy. The man-
ual annotations were only used for testing and were
ignored during training and development. Conse-
quently, the proposed framework of learning with
positive and unlabeled examples for sense disam-
biguation could be applied on the entire Wikipedia
without any manual annotations. By augmenting
general sense links with links to more specific ar-
ticles, such an application could have a significant
impact on Wikipedia itself.
</bodyText>
<sectionHeader confidence="0.998315" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9858895">
This work was supported in part by the Na-
tional Science Foundation IIS awards #1018613 and
#1018590, and an allocation of computing time from
the Ohio Supercomputer Center.
</bodyText>
<footnote confidence="0.619818">
3Data and code will be made publicly available.
</footnote>
<sectionHeader confidence="0.924277" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999881980392157">
D. Ahn, V. Jijkoun, G. Mishne, K. Muller, M. de Ri-
jke, and S. Schlobach. 2004. Using Wikipedia at the
TREC QA track. In Proceedings of the 13th Text Re-
trieval Conference (TREC 2004).
Volha Bryl, Claudio Giuliano, Luciano Serafini, and
Kateryna Tymoshenko. 2010. Using background
knowledge to support coreference resolution. In Pro-
ceedings of the 2010 conference on ECAI 2010: 19th
European Conference on Artificial Intelligence, pages
759–764, Amsterdam, The Netherlands.
Razvan Bunescu and Marius Pasca. 2006. Using ency-
clopedic knowledge for named entity disambiguation.
In Proceesings of the 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL-06), pages 9–16, Trento, Italy.
Philipp Cimiano, Antje Schultz, Sergej Sizov, Philipp
Sorg, and Steffen Staab. 2009. Explicit versus la-
tent concept models for cross-language information re-
trieval. In International Joint Conference on Artificial
Intelligence (IJCAI-09, pages 1513–1518, Pasadena,
CA, july.
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on Wikipedia data. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing, pages 708–716.
Charles Elkan and Keith Noto. 2008. Learning clas-
sifiers from only positive and unlabeled data. In
Proceedings of the 14th ACM SIGKDD international
conference on Knowledge discovery and data mining,
KDD ’08, pages 213–220.
David A. Ferrucci, Eric W. Brown, Jennifer Chu-Carroll,
James Fan, David Gondek, Aditya Kalyanpur, Adam
Lally, J. William Murdock, Eric Nyberg, John M.
Prager, Nico Schlaefer, and Christopher A. Welty.
2010. Building watson: An overview of the deepqa
project. AI Magazine, 31(3):59–79.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1152–1161, Singapore, August.
M. Kaisser. 2008. The QuALiM question answering
demo: Supplementing answers with paragraphs drawn
from Wikipedia. In Proceedings of the ACL-08 Hu-
man Language Technology Demo Session, pages 32–
35, Columbus, Ohio.
Wee Sun Lee and Bing Liu. 2003. Learning with pos-
itive and unlabeled examples using weighted logistic
regression. In Proceedings of the Twentieth Interna-
tional Conference on Machine Learning (ICML, pages
448–455, Washington, DC, August.
</reference>
<page confidence="0.975346">
30
</page>
<reference confidence="0.999875929824562">
Y. Li, R. Luk, E. Ho, and K. Chung. 2007. Improv-
ing weak ad-hoc queries using Wikipedia as external
corpus. In Proceedings of the 30th Annual Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 797–798,
Amsterdam, Netherlands.
Bing Liu, Yang Dai, Xiaoli Li, Wee Sun Lee, and
Philip S. Yu. 2003. Building text classifiers using pos-
itive and unlabeled examples. In Proceedings of the
Third IEEE International Conference on Data Mining,
ICDM ’03, pages 179–186, Washington, DC, USA.
R. Mihalcea. 2007. Using Wikipedia for automatic word
sense disambiguation. In Human Language Technolo-
gies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 196–203, Rochester, New York, April.
D. Milne. 2007. Computing semantic relatedness using
Wikipedia link structure. In Proceedings of the New
Zealand Computer Science Research Student Confer-
ence, Hamilton, New Zealand.
Hwee Tou Ng and H. B. Lee. 1996. Integrating multiple
knowledge sources to disambiguate word sense: An
exemplar-based approach. In Proceedings of the 34th
Annual Meeting of the Association for Computational
Linguistics (ACL-96), pages 40–47, Santa Cruz, CA.
Keith Noto, Milton H. Saier, Jr., and Charles Elkan.
2008. Learning to find relevant biological articles
without negative training examples. In Proceedings of
the 21st Australasian Joint Conference on Artificial In-
telligence: Advances in Artificial Intelligence, AI ’08,
pages 202–213.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich word sense disambiguation rivaling
supervised systems. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1522–1531, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Simone Paolo Ponzetto and Michael Strube. 2006. Ex-
ploiting semantic role labeling, wordnet and wikipedia
for coreference resolution. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association of Computa-
tional Linguistics, pages 192–199.
M. Potthast, B. Stein, and M. A. Anderka. 2008.
Wikipedia-based multilingual retrieval model. In Pro-
ceedings of the 30th European Conference on IR Re-
search, Glasgow.
Altaf Rahman and Vincent Ng. 2011. Coreference res-
olution with world knowledge. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies -
Volume 1, pages 814–824, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Mark Stevenson and Yorick Wilks. 2001. The interaction
of knowledge sources in word sense disambiguation.
Computational Linguistics, 27(3):321–349, Septem-
ber.
</reference>
<page confidence="0.999913">
31
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.391278">
<title confidence="0.995447">Coarse to Fine Grained Sense Disambiguation in Wikipedia</title>
<author confidence="0.993131">Hui</author>
<affiliation confidence="0.974447">School of Ohio</affiliation>
<address confidence="0.964167">Athens, OH 45701,</address>
<email confidence="0.998766">hui.shen.1@ohio.edu</email>
<author confidence="0.855657">Razvan</author>
<affiliation confidence="0.998693">School of Ohio University</affiliation>
<address confidence="0.999861">Athens, OH 45701, USA</address>
<email confidence="0.999581">bunescu@ohio.edu</email>
<author confidence="0.980483">Rada</author>
<affiliation confidence="0.9998165">Department of University of North</affiliation>
<address confidence="0.519785">Denton, TX 76203,</address>
<email confidence="0.999776">rada@cs.unt.edu</email>
<abstract confidence="0.999428076923077">Wikipedia articles are annotated by volunteer contributors with numerous links that connect words and phrases to relevant titles. Links to general senses of a word are used concurrently with links to more specific senses, without being distinguished explicitly. We present an approach to training coarse to fine grained sense disambiguation systems in the presence of such annotation inconsistencies. Experimental results show that accounting for annotation ambiguity in Wikipedia links leads to significant improvements in disambiguation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Ahn</author>
<author>V Jijkoun</author>
<author>G Mishne</author>
<author>K Muller</author>
<author>M de Rijke</author>
<author>S Schlobach</author>
</authors>
<title>Using Wikipedia at the TREC QA track.</title>
<date>2004</date>
<booktitle>In Proceedings of the 13th Text Retrieval Conference (TREC</booktitle>
<marker>Ahn, Jijkoun, Mishne, Muller, de Rijke, Schlobach, 2004</marker>
<rawString>D. Ahn, V. Jijkoun, G. Mishne, K. Muller, M. de Rijke, and S. Schlobach. 2004. Using Wikipedia at the TREC QA track. In Proceedings of the 13th Text Retrieval Conference (TREC 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Volha Bryl</author>
<author>Claudio Giuliano</author>
<author>Luciano Serafini</author>
<author>Kateryna Tymoshenko</author>
</authors>
<title>Using background knowledge to support coreference resolution.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 conference on ECAI 2010: 19th European Conference on Artificial Intelligence,</booktitle>
<pages>759--764</pages>
<location>Amsterdam, The Netherlands.</location>
<contexts>
<context position="1122" citStr="Bryl et al., 2010" startWordPosition="163" endWordPosition="166">currently with links to more specific senses, without being distinguished explicitly. We present an approach to training coarse to fine grained sense disambiguation systems in the presence of such annotation inconsistencies. Experimental results show that accounting for annotation ambiguity in Wikipedia links leads to significant improvements in disambiguation. 1 Introduction and Motivation The vast amount of world knowledge available in Wikipedia has been shown to benefit many types of text processing tasks, such as coreference resolution (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Bryl et al., 2010; Rahman and Ng, 2011), information retrieval (Milne, 2007; Li et al., 2007; Potthast et al., 2008; Cimiano et al., 2009), or question answering (Ahn et al., 2004; Kaisser, 2008; Ferrucci et al., 2010). In particular, the user contributed link structure of Wikipedia has been shown to provide useful supervision for training named entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007) and word sense disambiguation (Mihalcea, 2007; Ponzetto and Navigli, 2010) systems. Articles in Wikipedia often contain mentions of concepts or entities that already have a corresponding article. When cont</context>
</contexts>
<marker>Bryl, Giuliano, Serafini, Tymoshenko, 2010</marker>
<rawString>Volha Bryl, Claudio Giuliano, Luciano Serafini, and Kateryna Tymoshenko. 2010. Using background knowledge to support coreference resolution. In Proceedings of the 2010 conference on ECAI 2010: 19th European Conference on Artificial Intelligence, pages 759–764, Amsterdam, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Marius Pasca</author>
</authors>
<title>Using encyclopedic knowledge for named entity disambiguation.</title>
<date>2006</date>
<booktitle>In Proceesings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-06),</booktitle>
<pages>9--16</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="1499" citStr="Bunescu and Pasca, 2006" startWordPosition="221" endWordPosition="224">duction and Motivation The vast amount of world knowledge available in Wikipedia has been shown to benefit many types of text processing tasks, such as coreference resolution (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Bryl et al., 2010; Rahman and Ng, 2011), information retrieval (Milne, 2007; Li et al., 2007; Potthast et al., 2008; Cimiano et al., 2009), or question answering (Ahn et al., 2004; Kaisser, 2008; Ferrucci et al., 2010). In particular, the user contributed link structure of Wikipedia has been shown to provide useful supervision for training named entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007) and word sense disambiguation (Mihalcea, 2007; Ponzetto and Navigli, 2010) systems. Articles in Wikipedia often contain mentions of concepts or entities that already have a corresponding article. When contributing authors mention an existing Wikipedia entity inside an article, they are required to link at least its first mention to 22 the corresponding article, by using links or piped links. Consider, for example, the following Wiki source annotations: The [[capital city|capital]] of Georgia is [[Atlanta]]. The bracketed strings identify the title of the Wikipedia articles th</context>
</contexts>
<marker>Bunescu, Pasca, 2006</marker>
<rawString>Razvan Bunescu and Marius Pasca. 2006. Using encyclopedic knowledge for named entity disambiguation. In Proceesings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-06), pages 9–16, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Cimiano</author>
<author>Antje Schultz</author>
<author>Sergej Sizov</author>
<author>Philipp Sorg</author>
<author>Steffen Staab</author>
</authors>
<title>Explicit versus latent concept models for cross-language information retrieval.</title>
<date>2009</date>
<booktitle>In International Joint Conference on Artificial Intelligence (IJCAI-09,</booktitle>
<pages>1513--1518</pages>
<location>Pasadena, CA,</location>
<contexts>
<context position="1243" citStr="Cimiano et al., 2009" startWordPosition="183" endWordPosition="186">ing coarse to fine grained sense disambiguation systems in the presence of such annotation inconsistencies. Experimental results show that accounting for annotation ambiguity in Wikipedia links leads to significant improvements in disambiguation. 1 Introduction and Motivation The vast amount of world knowledge available in Wikipedia has been shown to benefit many types of text processing tasks, such as coreference resolution (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Bryl et al., 2010; Rahman and Ng, 2011), information retrieval (Milne, 2007; Li et al., 2007; Potthast et al., 2008; Cimiano et al., 2009), or question answering (Ahn et al., 2004; Kaisser, 2008; Ferrucci et al., 2010). In particular, the user contributed link structure of Wikipedia has been shown to provide useful supervision for training named entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007) and word sense disambiguation (Mihalcea, 2007; Ponzetto and Navigli, 2010) systems. Articles in Wikipedia often contain mentions of concepts or entities that already have a corresponding article. When contributing authors mention an existing Wikipedia entity inside an article, they are required to link at least its first men</context>
</contexts>
<marker>Cimiano, Schultz, Sizov, Sorg, Staab, 2009</marker>
<rawString>Philipp Cimiano, Antje Schultz, Sergej Sizov, Philipp Sorg, and Steffen Staab. 2009. Explicit versus latent concept models for cross-language information retrieval. In International Joint Conference on Artificial Intelligence (IJCAI-09, pages 1513–1518, Pasadena, CA, july.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
</authors>
<title>Large-scale named entity disambiguation based on Wikipedia data.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>708--716</pages>
<contexts>
<context position="1516" citStr="Cucerzan, 2007" startWordPosition="225" endWordPosition="226">e vast amount of world knowledge available in Wikipedia has been shown to benefit many types of text processing tasks, such as coreference resolution (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Bryl et al., 2010; Rahman and Ng, 2011), information retrieval (Milne, 2007; Li et al., 2007; Potthast et al., 2008; Cimiano et al., 2009), or question answering (Ahn et al., 2004; Kaisser, 2008; Ferrucci et al., 2010). In particular, the user contributed link structure of Wikipedia has been shown to provide useful supervision for training named entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007) and word sense disambiguation (Mihalcea, 2007; Ponzetto and Navigli, 2010) systems. Articles in Wikipedia often contain mentions of concepts or entities that already have a corresponding article. When contributing authors mention an existing Wikipedia entity inside an article, they are required to link at least its first mention to 22 the corresponding article, by using links or piped links. Consider, for example, the following Wiki source annotations: The [[capital city|capital]] of Georgia is [[Atlanta]]. The bracketed strings identify the title of the Wikipedia articles that describe the c</context>
</contexts>
<marker>Cucerzan, 2007</marker>
<rawString>S. Cucerzan. 2007. Large-scale named entity disambiguation based on Wikipedia data. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 708–716.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Elkan</author>
<author>Keith Noto</author>
</authors>
<title>Learning classifiers from only positive and unlabeled data.</title>
<date>2008</date>
<booktitle>In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’08,</booktitle>
<pages>213--220</pages>
<contexts>
<context position="17715" citStr="Elkan and Noto, 2008" startWordPosition="2846" endWordPosition="2849">ors as ATMOSPHERE should actually belong to the ATMOSPHERE (S) category – about 60% of them, according to Table 1. Instead, we propose treating all ATMOSPHERE links as unlabeled examples. If we consider the specific links in ATMOSPHERE (S) to be positive examples, then the problem becomes one of learning with positive and unlabeled examples. 3.1 Learning with positive and unlabeled examples This general type of semi-supervised learning has been studied before in the context of tasks such as text classification and information retrieval (Lee and Liu, 2003; Liu et al., 2003), or bioinformatics (Elkan and Noto, 2008; Noto et al., 2008). In this setting, the training data consists of positive examples x E P and unlabeled examples x E U. Following the notation of Elkan and Noto (2008), we define s(x) = 1 if the example is positive and s(x) = −1 if the example is unlabeled. The true label of an example is y(x) = 1 if the example is positive and y(x) = −1 if the example is negative. Thus, x E P =�- s(x) = y(x) = 1 and x E U =�- s(x) = −1 i.e., the true label y(x) of an unlabeled example is unknown. For the experiments reported in this paper, we use our implementation of two state-of-the-art approaches to Lea</context>
<context position="21791" citStr="Elkan and Noto (2008)" startWordPosition="3648" endWordPosition="3651"> criterion r − p(f = 1), where both the recall r and p(f = 1) can be estimated on the positive and unlabeled examples from a separate development dataset. In conclusion, one can use the original Biased SVM formulation to maximize r2/p(f = 1), which has been shown by Lee and Liu (2003) to maximize pr, a criterion that has a similar behavior with the F-measure used in retrieval applications. Alternatively, if the target performance measure is accuracy, we can choose instead to maximize r − p(f = 1), which we have shown above to correspond to accuracy maximization. 3.1.2 The Weighted Samples SVM Elkan and Noto (2008) introduced two approaches for learning with positive and unlabeled data. Both approaches are based on the assumption that labeled examples {x|s(x) = 11 are selected at random from the positive examples {x|y(x) = 11 i.e., p(s = 1|x, y = 1) = p(s = 1|y = 1). Their best performing approach uses the positive and unlabeled examples to train two distinct classifiers. First, the dataset P U U is split into a training set and a validation set, and a classifier g(x) is trained on the ξx + CU E ξx xEU p(f = 1) = p(y = −1) 27 labeling s to approximate the label distribution i.e. g(x) = p(s = 1|x). The v</context>
<context position="26553" citStr="Elkan and Noto (2008)" startWordPosition="4501" endWordPosition="4504"> of the Biased SVM were tuned through the c and j parameters of SVMlight (c = CU and j = CP/CU). Eventually, all three methods use the development data for tuning the c and j parameters of the SVM. However, whereas the Naive SVM tunes these parameters to optimize the accuracy with respect to the noisy label s(x), the Biased SVM tunes the same parameters to maximize an estimate of the accuracy or 1http://svmlight.joachims.org 28 F-measure with respect to the true label y(x). The Weighted SVM approach was implemented on top of the LibSVM2 package. Even though the original Weighted SVM method of Elkan and Noto (2008) does not specify tuning any parameters, we noticed it gave better results when the capacity c and weight j parameters were tuned for the first classifier g(x). Table 4 shows the accuracy results of the three methods for Level2, whereas Table 5 shows the Fmeasure results. The Biased SVM outperforms the Naive SVM on all the words, in terms of both accuracy and F-measure. The most dramatic increases are seen for the words atmosphere, game, diamond, and Corinth. For these words, the number of positive examples is significantly smaller compared to the total number of positive and unlabeled example</context>
</contexts>
<marker>Elkan, Noto, 2008</marker>
<rawString>Charles Elkan and Keith Noto. 2008. Learning classifiers from only positive and unlabeled data. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’08, pages 213–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Ferrucci</author>
<author>Eric W Brown</author>
<author>Jennifer Chu-Carroll</author>
<author>James Fan</author>
<author>David Gondek</author>
<author>Aditya Kalyanpur</author>
<author>Adam Lally</author>
<author>J William Murdock</author>
<author>Eric Nyberg</author>
<author>John M Prager</author>
<author>Nico Schlaefer</author>
<author>Christopher A Welty</author>
</authors>
<title>Building watson: An overview of the deepqa project.</title>
<date>2010</date>
<journal>AI Magazine,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="1323" citStr="Ferrucci et al., 2010" startWordPosition="196" endWordPosition="199"> annotation inconsistencies. Experimental results show that accounting for annotation ambiguity in Wikipedia links leads to significant improvements in disambiguation. 1 Introduction and Motivation The vast amount of world knowledge available in Wikipedia has been shown to benefit many types of text processing tasks, such as coreference resolution (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Bryl et al., 2010; Rahman and Ng, 2011), information retrieval (Milne, 2007; Li et al., 2007; Potthast et al., 2008; Cimiano et al., 2009), or question answering (Ahn et al., 2004; Kaisser, 2008; Ferrucci et al., 2010). In particular, the user contributed link structure of Wikipedia has been shown to provide useful supervision for training named entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007) and word sense disambiguation (Mihalcea, 2007; Ponzetto and Navigli, 2010) systems. Articles in Wikipedia often contain mentions of concepts or entities that already have a corresponding article. When contributing authors mention an existing Wikipedia entity inside an article, they are required to link at least its first mention to 22 the corresponding article, by using links or piped links. Consider, f</context>
</contexts>
<marker>Ferrucci, Brown, Chu-Carroll, Fan, Gondek, Kalyanpur, Lally, Murdock, Nyberg, Prager, Schlaefer, Welty, 2010</marker>
<rawString>David A. Ferrucci, Eric W. Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya Kalyanpur, Adam Lally, J. William Murdock, Eric Nyberg, John M. Prager, Nico Schlaefer, and Christopher A. Welty. 2010. Building watson: An overview of the deepqa project. AI Magazine, 31(3):59–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Simple coreference resolution with rich syntactic and semantic features.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1152--1161</pages>
<location>Singapore,</location>
<contexts>
<context position="1103" citStr="Haghighi and Klein, 2009" startWordPosition="159" endWordPosition="162">ses of a word are used concurrently with links to more specific senses, without being distinguished explicitly. We present an approach to training coarse to fine grained sense disambiguation systems in the presence of such annotation inconsistencies. Experimental results show that accounting for annotation ambiguity in Wikipedia links leads to significant improvements in disambiguation. 1 Introduction and Motivation The vast amount of world knowledge available in Wikipedia has been shown to benefit many types of text processing tasks, such as coreference resolution (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Bryl et al., 2010; Rahman and Ng, 2011), information retrieval (Milne, 2007; Li et al., 2007; Potthast et al., 2008; Cimiano et al., 2009), or question answering (Ahn et al., 2004; Kaisser, 2008; Ferrucci et al., 2010). In particular, the user contributed link structure of Wikipedia has been shown to provide useful supervision for training named entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007) and word sense disambiguation (Mihalcea, 2007; Ponzetto and Navigli, 2010) systems. Articles in Wikipedia often contain mentions of concepts or entities that already have a corresponding</context>
</contexts>
<marker>Haghighi, Klein, 2009</marker>
<rawString>Aria Haghighi and Dan Klein. 2009. Simple coreference resolution with rich syntactic and semantic features. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1152–1161, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kaisser</author>
</authors>
<title>The QuALiM question answering demo: Supplementing answers with paragraphs drawn from Wikipedia.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL-08 Human Language Technology Demo Session,</booktitle>
<pages>32--35</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="1299" citStr="Kaisser, 2008" startWordPosition="194" endWordPosition="195">resence of such annotation inconsistencies. Experimental results show that accounting for annotation ambiguity in Wikipedia links leads to significant improvements in disambiguation. 1 Introduction and Motivation The vast amount of world knowledge available in Wikipedia has been shown to benefit many types of text processing tasks, such as coreference resolution (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Bryl et al., 2010; Rahman and Ng, 2011), information retrieval (Milne, 2007; Li et al., 2007; Potthast et al., 2008; Cimiano et al., 2009), or question answering (Ahn et al., 2004; Kaisser, 2008; Ferrucci et al., 2010). In particular, the user contributed link structure of Wikipedia has been shown to provide useful supervision for training named entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007) and word sense disambiguation (Mihalcea, 2007; Ponzetto and Navigli, 2010) systems. Articles in Wikipedia often contain mentions of concepts or entities that already have a corresponding article. When contributing authors mention an existing Wikipedia entity inside an article, they are required to link at least its first mention to 22 the corresponding article, by using links or </context>
</contexts>
<marker>Kaisser, 2008</marker>
<rawString>M. Kaisser. 2008. The QuALiM question answering demo: Supplementing answers with paragraphs drawn from Wikipedia. In Proceedings of the ACL-08 Human Language Technology Demo Session, pages 32– 35, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Sun Lee</author>
<author>Bing Liu</author>
</authors>
<title>Learning with positive and unlabeled examples using weighted logistic regression.</title>
<date>2003</date>
<booktitle>In Proceedings of the Twentieth International Conference on Machine Learning (ICML,</booktitle>
<pages>448--455</pages>
<location>Washington, DC,</location>
<contexts>
<context position="17655" citStr="Lee and Liu, 2003" startWordPosition="2835" endWordPosition="2838">er of the links that are annotated by Wikipedia contributors as ATMOSPHERE should actually belong to the ATMOSPHERE (S) category – about 60% of them, according to Table 1. Instead, we propose treating all ATMOSPHERE links as unlabeled examples. If we consider the specific links in ATMOSPHERE (S) to be positive examples, then the problem becomes one of learning with positive and unlabeled examples. 3.1 Learning with positive and unlabeled examples This general type of semi-supervised learning has been studied before in the context of tasks such as text classification and information retrieval (Lee and Liu, 2003; Liu et al., 2003), or bioinformatics (Elkan and Noto, 2008; Noto et al., 2008). In this setting, the training data consists of positive examples x E P and unlabeled examples x E U. Following the notation of Elkan and Noto (2008), we define s(x) = 1 if the example is positive and s(x) = −1 if the example is unlabeled. The true label of an example is y(x) = 1 if the example is positive and y(x) = −1 if the example is negative. Thus, x E P =�- s(x) = y(x) = 1 and x E U =�- s(x) = −1 i.e., the true label y(x) of an unlabeled example is unknown. For the experiments reported in this paper, we use </context>
<context position="19799" citStr="Lee and Liu (2003)" startWordPosition="3237" endWordPosition="3240"> CU control how much we penalize errors on positive examples vs. errors on unlabeled examples. Since not all unlabeled examples are negative, one would want to select capacity parameters satisfying CP &gt; CU, such that false negative errors are penalized more than false positive errors. In order to find the best capacity parameters to use during training, the Biased SVM approach runs a grid search on a separate development dataset. This search is aimed at finding values for the parameters CP and CU that maximize pr, the product between precision p = p(y = 1|f = 1) and recall r = p(f = 1|y = 1). Lee and Liu (2003) show that maximizing the pr criterion is equivalent with maximizing the objective r2/p(f = 1), where both r = p(f = 1|y = 1) and p(f = 1) can be estimated using the trained decision function f(x) on the development dataset. Maximizing the pr criterion in the original Biased SVM formulation was motivated by the need to optimize the F measure in information retrieval settings, where F = 2pr(p+ r). In the rest of this section we show that classification accuracy can be maximized using only positive and unlabeled examples, an important result for problems where classification accuracy is the targ</context>
<context position="21455" citStr="Lee and Liu (2003)" startWordPosition="3591" endWordPosition="3594"> = 1)p(f = 1|y = 1) p(y = −1) p(f = 1) p(f = 1) − p(y = 1) x r = (2) p(y = −1) Plugging identity 2 in Equation 1 leads to: r x p(y = 1) − p(f = 1) acc = 1 + r + p(y = −1) r − p(f = 1) = 1 + (3) p(y = −1) Since p(y = −1) can be assimilated with a constant, Equation 3 implies that maximizing accuracy is equivalent with maximizing the criterion r − p(f = 1), where both the recall r and p(f = 1) can be estimated on the positive and unlabeled examples from a separate development dataset. In conclusion, one can use the original Biased SVM formulation to maximize r2/p(f = 1), which has been shown by Lee and Liu (2003) to maximize pr, a criterion that has a similar behavior with the F-measure used in retrieval applications. Alternatively, if the target performance measure is accuracy, we can choose instead to maximize r − p(f = 1), which we have shown above to correspond to accuracy maximization. 3.1.2 The Weighted Samples SVM Elkan and Noto (2008) introduced two approaches for learning with positive and unlabeled data. Both approaches are based on the assumption that labeled examples {x|s(x) = 11 are selected at random from the positive examples {x|y(x) = 11 i.e., p(s = 1|x, y = 1) = p(s = 1|y = 1). Their </context>
</contexts>
<marker>Lee, Liu, 2003</marker>
<rawString>Wee Sun Lee and Bing Liu. 2003. Learning with positive and unlabeled examples using weighted logistic regression. In Proceedings of the Twentieth International Conference on Machine Learning (ICML, pages 448–455, Washington, DC, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Li</author>
<author>R Luk</author>
<author>E Ho</author>
<author>K Chung</author>
</authors>
<title>Improving weak ad-hoc queries using Wikipedia as external corpus.</title>
<date>2007</date>
<booktitle>In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>797--798</pages>
<location>Amsterdam, Netherlands.</location>
<contexts>
<context position="1197" citStr="Li et al., 2007" startWordPosition="175" endWordPosition="178">licitly. We present an approach to training coarse to fine grained sense disambiguation systems in the presence of such annotation inconsistencies. Experimental results show that accounting for annotation ambiguity in Wikipedia links leads to significant improvements in disambiguation. 1 Introduction and Motivation The vast amount of world knowledge available in Wikipedia has been shown to benefit many types of text processing tasks, such as coreference resolution (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Bryl et al., 2010; Rahman and Ng, 2011), information retrieval (Milne, 2007; Li et al., 2007; Potthast et al., 2008; Cimiano et al., 2009), or question answering (Ahn et al., 2004; Kaisser, 2008; Ferrucci et al., 2010). In particular, the user contributed link structure of Wikipedia has been shown to provide useful supervision for training named entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007) and word sense disambiguation (Mihalcea, 2007; Ponzetto and Navigli, 2010) systems. Articles in Wikipedia often contain mentions of concepts or entities that already have a corresponding article. When contributing authors mention an existing Wikipedia entity inside an article, th</context>
</contexts>
<marker>Li, Luk, Ho, Chung, 2007</marker>
<rawString>Y. Li, R. Luk, E. Ho, and K. Chung. 2007. Improving weak ad-hoc queries using Wikipedia as external corpus. In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 797–798, Amsterdam, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
<author>Yang Dai</author>
<author>Xiaoli Li</author>
<author>Wee Sun Lee</author>
<author>Philip S Yu</author>
</authors>
<title>Building text classifiers using positive and unlabeled examples.</title>
<date>2003</date>
<booktitle>In Proceedings of the Third IEEE International Conference on Data Mining, ICDM ’03,</booktitle>
<pages>179--186</pages>
<location>Washington, DC, USA.</location>
<contexts>
<context position="17674" citStr="Liu et al., 2003" startWordPosition="2839" endWordPosition="2842">t are annotated by Wikipedia contributors as ATMOSPHERE should actually belong to the ATMOSPHERE (S) category – about 60% of them, according to Table 1. Instead, we propose treating all ATMOSPHERE links as unlabeled examples. If we consider the specific links in ATMOSPHERE (S) to be positive examples, then the problem becomes one of learning with positive and unlabeled examples. 3.1 Learning with positive and unlabeled examples This general type of semi-supervised learning has been studied before in the context of tasks such as text classification and information retrieval (Lee and Liu, 2003; Liu et al., 2003), or bioinformatics (Elkan and Noto, 2008; Noto et al., 2008). In this setting, the training data consists of positive examples x E P and unlabeled examples x E U. Following the notation of Elkan and Noto (2008), we define s(x) = 1 if the example is positive and s(x) = −1 if the example is unlabeled. The true label of an example is y(x) = 1 if the example is positive and y(x) = −1 if the example is negative. Thus, x E P =�- s(x) = y(x) = 1 and x E U =�- s(x) = −1 i.e., the true label y(x) of an unlabeled example is unknown. For the experiments reported in this paper, we use our implementation </context>
</contexts>
<marker>Liu, Dai, Li, Lee, Yu, 2003</marker>
<rawString>Bing Liu, Yang Dai, Xiaoli Li, Wee Sun Lee, and Philip S. Yu. 2003. Building text classifiers using positive and unlabeled examples. In Proceedings of the Third IEEE International Conference on Data Mining, ICDM ’03, pages 179–186, Washington, DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
</authors>
<title>Using Wikipedia for automatic word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>196--203</pages>
<location>Rochester, New York,</location>
<contexts>
<context position="1562" citStr="Mihalcea, 2007" startWordPosition="232" endWordPosition="233">ikipedia has been shown to benefit many types of text processing tasks, such as coreference resolution (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Bryl et al., 2010; Rahman and Ng, 2011), information retrieval (Milne, 2007; Li et al., 2007; Potthast et al., 2008; Cimiano et al., 2009), or question answering (Ahn et al., 2004; Kaisser, 2008; Ferrucci et al., 2010). In particular, the user contributed link structure of Wikipedia has been shown to provide useful supervision for training named entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007) and word sense disambiguation (Mihalcea, 2007; Ponzetto and Navigli, 2010) systems. Articles in Wikipedia often contain mentions of concepts or entities that already have a corresponding article. When contributing authors mention an existing Wikipedia entity inside an article, they are required to link at least its first mention to 22 the corresponding article, by using links or piped links. Consider, for example, the following Wiki source annotations: The [[capital city|capital]] of Georgia is [[Atlanta]]. The bracketed strings identify the title of the Wikipedia articles that describe the corresponding named entities. If the editor wan</context>
</contexts>
<marker>Mihalcea, 2007</marker>
<rawString>R. Mihalcea. 2007. Using Wikipedia for automatic word sense disambiguation. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, pages 196–203, Rochester, New York, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Milne</author>
</authors>
<title>Computing semantic relatedness using Wikipedia link structure.</title>
<date>2007</date>
<booktitle>In Proceedings of the New Zealand Computer Science Research Student Conference,</booktitle>
<location>Hamilton, New Zealand.</location>
<contexts>
<context position="1180" citStr="Milne, 2007" startWordPosition="173" endWordPosition="174">inguished explicitly. We present an approach to training coarse to fine grained sense disambiguation systems in the presence of such annotation inconsistencies. Experimental results show that accounting for annotation ambiguity in Wikipedia links leads to significant improvements in disambiguation. 1 Introduction and Motivation The vast amount of world knowledge available in Wikipedia has been shown to benefit many types of text processing tasks, such as coreference resolution (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Bryl et al., 2010; Rahman and Ng, 2011), information retrieval (Milne, 2007; Li et al., 2007; Potthast et al., 2008; Cimiano et al., 2009), or question answering (Ahn et al., 2004; Kaisser, 2008; Ferrucci et al., 2010). In particular, the user contributed link structure of Wikipedia has been shown to provide useful supervision for training named entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007) and word sense disambiguation (Mihalcea, 2007; Ponzetto and Navigli, 2010) systems. Articles in Wikipedia often contain mentions of concepts or entities that already have a corresponding article. When contributing authors mention an existing Wikipedia entity insi</context>
</contexts>
<marker>Milne, 2007</marker>
<rawString>D. Milne. 2007. Computing semantic relatedness using Wikipedia link structure. In Proceedings of the New Zealand Computer Science Research Student Conference, Hamilton, New Zealand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>H B Lee</author>
</authors>
<title>Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL-96),</booktitle>
<pages>40--47</pages>
<location>Santa Cruz, CA.</location>
<contexts>
<context position="23873" citStr="Ng and Lee, 1996" startWordPosition="4037" endWordPosition="4040">ation 4. The two classifiers g and f are trained using SVMs and a linear kernel. Platt scaling is used with the first classifier to obtain the probability estimates g(x) = p(s = 1|x), which are then converted into weights following Equations 4 and 5, and used during the training of the second classifier. 4 Experimental Evaluation We ran disambiguation experiments on the 6 ambiguous words atmosphere, president, dollar, game, diamond and Corinth. The corresponding Wikipedia sense repositories have been summarized in Tables 1 and 2. All WSD classifiers used the same set of standard WSD features (Ng and Lee, 1996; Stevenson and Wilks, 2001), such as words and their part-ofspeech tags in a window of 3 words around the ambiguous keyword, the unigram and bigram content words that are within 2 sentences of the current sentence, the syntactic governor of the keyword, and its chains of syntactic dependencies of lengths up to two. Furthermore, for each example, a Wikipedia specific feature was computed as the cosine similarity between the context of the ambiguous word and the text of the article for the target sense or reference. The Level, and Level3 classifiers were trained using the SVM`lti component of t</context>
</contexts>
<marker>Ng, Lee, 1996</marker>
<rawString>Hwee Tou Ng and H. B. Lee. 1996. Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL-96), pages 40–47, Santa Cruz, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Noto</author>
<author>Milton H Saier</author>
<author>Charles Elkan</author>
</authors>
<title>Learning to find relevant biological articles without negative training examples.</title>
<date>2008</date>
<booktitle>In Proceedings of the 21st Australasian Joint Conference on Artificial Intelligence: Advances in Artificial Intelligence, AI ’08,</booktitle>
<pages>202--213</pages>
<contexts>
<context position="17735" citStr="Noto et al., 2008" startWordPosition="2850" endWordPosition="2853">ld actually belong to the ATMOSPHERE (S) category – about 60% of them, according to Table 1. Instead, we propose treating all ATMOSPHERE links as unlabeled examples. If we consider the specific links in ATMOSPHERE (S) to be positive examples, then the problem becomes one of learning with positive and unlabeled examples. 3.1 Learning with positive and unlabeled examples This general type of semi-supervised learning has been studied before in the context of tasks such as text classification and information retrieval (Lee and Liu, 2003; Liu et al., 2003), or bioinformatics (Elkan and Noto, 2008; Noto et al., 2008). In this setting, the training data consists of positive examples x E P and unlabeled examples x E U. Following the notation of Elkan and Noto (2008), we define s(x) = 1 if the example is positive and s(x) = −1 if the example is unlabeled. The true label of an example is y(x) = 1 if the example is positive and y(x) = −1 if the example is negative. Thus, x E P =�- s(x) = y(x) = 1 and x E U =�- s(x) = −1 i.e., the true label y(x) of an unlabeled example is unknown. For the experiments reported in this paper, we use our implementation of two state-of-the-art approaches to Learning with Positive </context>
</contexts>
<marker>Noto, Saier, Elkan, 2008</marker>
<rawString>Keith Noto, Milton H. Saier, Jr., and Charles Elkan. 2008. Learning to find relevant biological articles without negative training examples. In Proceedings of the 21st Australasian Joint Conference on Artificial Intelligence: Advances in Artificial Intelligence, AI ’08, pages 202–213.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Roberto Navigli</author>
</authors>
<title>Knowledge-rich word sense disambiguation rivaling supervised systems.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1522--1531</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1591" citStr="Ponzetto and Navigli, 2010" startWordPosition="234" endWordPosition="237">n shown to benefit many types of text processing tasks, such as coreference resolution (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Bryl et al., 2010; Rahman and Ng, 2011), information retrieval (Milne, 2007; Li et al., 2007; Potthast et al., 2008; Cimiano et al., 2009), or question answering (Ahn et al., 2004; Kaisser, 2008; Ferrucci et al., 2010). In particular, the user contributed link structure of Wikipedia has been shown to provide useful supervision for training named entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007) and word sense disambiguation (Mihalcea, 2007; Ponzetto and Navigli, 2010) systems. Articles in Wikipedia often contain mentions of concepts or entities that already have a corresponding article. When contributing authors mention an existing Wikipedia entity inside an article, they are required to link at least its first mention to 22 the corresponding article, by using links or piped links. Consider, for example, the following Wiki source annotations: The [[capital city|capital]] of Georgia is [[Atlanta]]. The bracketed strings identify the title of the Wikipedia articles that describe the corresponding named entities. If the editor wants a different string display</context>
</contexts>
<marker>Ponzetto, Navigli, 2010</marker>
<rawString>Simone Paolo Ponzetto and Roberto Navigli. 2010. Knowledge-rich word sense disambiguation rivaling supervised systems. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1522–1531, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Michael Strube</author>
</authors>
<title>Exploiting semantic role labeling, wordnet and wikipedia for coreference resolution.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>192--199</pages>
<contexts>
<context position="1077" citStr="Ponzetto and Strube, 2006" startWordPosition="155" endWordPosition="158">itles. Links to general senses of a word are used concurrently with links to more specific senses, without being distinguished explicitly. We present an approach to training coarse to fine grained sense disambiguation systems in the presence of such annotation inconsistencies. Experimental results show that accounting for annotation ambiguity in Wikipedia links leads to significant improvements in disambiguation. 1 Introduction and Motivation The vast amount of world knowledge available in Wikipedia has been shown to benefit many types of text processing tasks, such as coreference resolution (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Bryl et al., 2010; Rahman and Ng, 2011), information retrieval (Milne, 2007; Li et al., 2007; Potthast et al., 2008; Cimiano et al., 2009), or question answering (Ahn et al., 2004; Kaisser, 2008; Ferrucci et al., 2010). In particular, the user contributed link structure of Wikipedia has been shown to provide useful supervision for training named entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007) and word sense disambiguation (Mihalcea, 2007; Ponzetto and Navigli, 2010) systems. Articles in Wikipedia often contain mentions of concepts or entities that al</context>
</contexts>
<marker>Ponzetto, Strube, 2006</marker>
<rawString>Simone Paolo Ponzetto and Michael Strube. 2006. Exploiting semantic role labeling, wordnet and wikipedia for coreference resolution. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Potthast</author>
<author>B Stein</author>
<author>M A Anderka</author>
</authors>
<title>Wikipedia-based multilingual retrieval model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 30th European Conference on IR Research,</booktitle>
<location>Glasgow.</location>
<contexts>
<context position="1220" citStr="Potthast et al., 2008" startWordPosition="179" endWordPosition="182">nt an approach to training coarse to fine grained sense disambiguation systems in the presence of such annotation inconsistencies. Experimental results show that accounting for annotation ambiguity in Wikipedia links leads to significant improvements in disambiguation. 1 Introduction and Motivation The vast amount of world knowledge available in Wikipedia has been shown to benefit many types of text processing tasks, such as coreference resolution (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Bryl et al., 2010; Rahman and Ng, 2011), information retrieval (Milne, 2007; Li et al., 2007; Potthast et al., 2008; Cimiano et al., 2009), or question answering (Ahn et al., 2004; Kaisser, 2008; Ferrucci et al., 2010). In particular, the user contributed link structure of Wikipedia has been shown to provide useful supervision for training named entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007) and word sense disambiguation (Mihalcea, 2007; Ponzetto and Navigli, 2010) systems. Articles in Wikipedia often contain mentions of concepts or entities that already have a corresponding article. When contributing authors mention an existing Wikipedia entity inside an article, they are required to link</context>
</contexts>
<marker>Potthast, Stein, Anderka, 2008</marker>
<rawString>M. Potthast, B. Stein, and M. A. Anderka. 2008. Wikipedia-based multilingual retrieval model. In Proceedings of the 30th European Conference on IR Research, Glasgow.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Coreference resolution with world knowledge.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies -Volume</booktitle>
<volume>1</volume>
<pages>814--824</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1144" citStr="Rahman and Ng, 2011" startWordPosition="167" endWordPosition="170">s to more specific senses, without being distinguished explicitly. We present an approach to training coarse to fine grained sense disambiguation systems in the presence of such annotation inconsistencies. Experimental results show that accounting for annotation ambiguity in Wikipedia links leads to significant improvements in disambiguation. 1 Introduction and Motivation The vast amount of world knowledge available in Wikipedia has been shown to benefit many types of text processing tasks, such as coreference resolution (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Bryl et al., 2010; Rahman and Ng, 2011), information retrieval (Milne, 2007; Li et al., 2007; Potthast et al., 2008; Cimiano et al., 2009), or question answering (Ahn et al., 2004; Kaisser, 2008; Ferrucci et al., 2010). In particular, the user contributed link structure of Wikipedia has been shown to provide useful supervision for training named entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007) and word sense disambiguation (Mihalcea, 2007; Ponzetto and Navigli, 2010) systems. Articles in Wikipedia often contain mentions of concepts or entities that already have a corresponding article. When contributing authors menti</context>
</contexts>
<marker>Rahman, Ng, 2011</marker>
<rawString>Altaf Rahman and Vincent Ng. 2011. Coreference resolution with world knowledge. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies -Volume 1, pages 814–824, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Stevenson</author>
<author>Yorick Wilks</author>
</authors>
<title>The interaction of knowledge sources in word sense disambiguation.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>3</issue>
<contexts>
<context position="23901" citStr="Stevenson and Wilks, 2001" startWordPosition="4041" endWordPosition="4044">lassifiers g and f are trained using SVMs and a linear kernel. Platt scaling is used with the first classifier to obtain the probability estimates g(x) = p(s = 1|x), which are then converted into weights following Equations 4 and 5, and used during the training of the second classifier. 4 Experimental Evaluation We ran disambiguation experiments on the 6 ambiguous words atmosphere, president, dollar, game, diamond and Corinth. The corresponding Wikipedia sense repositories have been summarized in Tables 1 and 2. All WSD classifiers used the same set of standard WSD features (Ng and Lee, 1996; Stevenson and Wilks, 2001), such as words and their part-ofspeech tags in a window of 3 words around the ambiguous keyword, the unigram and bigram content words that are within 2 sentences of the current sentence, the syntactic governor of the keyword, and its chains of syntactic dependencies of lengths up to two. Furthermore, for each example, a Wikipedia specific feature was computed as the cosine similarity between the context of the ambiguous word and the text of the article for the target sense or reference. The Level, and Level3 classifiers were trained using the SVM`lti component of the SVMlight package.1 The WS</context>
</contexts>
<marker>Stevenson, Wilks, 2001</marker>
<rawString>Mark Stevenson and Yorick Wilks. 2001. The interaction of knowledge sources in word sense disambiguation. Computational Linguistics, 27(3):321–349, September.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>