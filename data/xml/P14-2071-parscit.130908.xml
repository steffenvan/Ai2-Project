<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013460">
<title confidence="0.9985415">
Improving Twitter Sentiment Analysis with Topic-Based Mixture
Modeling and Semi-Supervised Training
</title>
<author confidence="0.989256">
Bing Xiang *
</author>
<affiliation confidence="0.845044">
IBM Watson
</affiliation>
<address confidence="0.85385">
1101 Kitchawan Rd
Yorktown Heights, NY 10598, USA
</address>
<email confidence="0.998021">
bingxia@us.ibm.com
</email>
<sectionHeader confidence="0.99388" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997018">
In this paper, we present multiple ap-
proaches to improve sentiment analysis
on Twitter data. We first establish a
state-of-the-art baseline with a rich fea-
ture set. Then we build a topic-based sen-
timent mixture model with topic-specific
data in a semi-supervised training frame-
work. The topic information is generated
through topic modeling based on an ef-
ficient implementation of Latent Dirich-
let Allocation (LDA). The proposed sen-
timent model outperforms the top system
in the task of Sentiment Analysis in Twit-
ter in SemEval-2013 in terms of averaged
F scores.
</bodyText>
<sectionHeader confidence="0.998791" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998625190476191">
Social media, such as Twitter and Facebook, has
attracted significant attention in recent years. The
vast amount of data available online provides a
unique opportunity to the people working on nat-
ural language processing (NLP) and related fields.
Sentiment analysis is one of the areas that has
large potential in real-world applications. For ex-
ample, monitoring the trend of sentiment for a spe-
cific company or product mentioned in social me-
dia can be useful in stock prediction and product
marketing.
In this paper, we focus on sentiment analysis of
Twitter data (tweets). It is one of the challenging
tasks in NLP given the length limit on each tweet
(up to 140 characters) and also the informal con-
versation. Many approaches have been proposed
previously to improve sentiment analysis on Twit-
ter data. For example, Nakov et al. (2013) provide
an overview on the systems submitted to one of the
SemEval-2013 tasks, Sentiment Analysis in Twit-
ter. A variety of features have been utilized for
</bodyText>
<footnote confidence="0.9591605">
* This work was done when the author was with Thom-
son Reuters.
</footnote>
<note confidence="0.70698">
Liang Zhou
Thomson Reuters
3 Times Square
New York, NY 10036, USA
</note>
<email confidence="0.976773">
l.zhou@thomsonreuters.com
</email>
<bodyText confidence="0.998688555555555">
sentiment classification on tweets. They include
lexical features (e.g. word lexicon), syntactic fea-
tures (e.g. Part-of-Speech), Twitter-specific fea-
tures (e.g. emoticons), etc. However, all of these
features only capture local information in the data
and do not take into account of the global higher-
level information, such as topic information.
Two example tweets are given below, with the
word “offensive” appearing in both of them.
</bodyText>
<listItem confidence="0.9974634">
• Im gonna post something that might be offen-
sive to people in Singapore.
• #FSU offensive coordinator Randy Sanders
coached for Tennessee in 1st #BCS title
game.
</listItem>
<bodyText confidence="0.999154230769231">
Generally “offensive” is used as a negative word
(as in the first tweet), but it bears no sentiment in
the second tweet when people are talking about a
football game. Even though some local contextual
features could be helpful to distinguish the two
cases above, they still may not be enough to get the
sentiment on the whole message correct. Also, the
local features often suffer from the sparsity prob-
lem. This motivates us to explore topic informa-
tion explicitly in the task of sentiment analysis on
Twitter data.
There exists some work on applying topic in-
formation in sentiment analysis, such as (Mei et
al., 2007), (Branavan et al., 2008), (Jo and Oh,
2011) and (He et al., 2012). All these work are
significantly different from what we propose in
this work. Also they are conducted in a domain
other than Twitter. Most recently, Si et al. (2013)
propose a continuous Dirichlet Process Mixture
model for Twitter sentiment, for the purpose of
stock prediction. Unfortunately there is no eval-
uation on the accuracy of sentiment classification
alone in that work. Furthermore, no standard train-
ing or test corpus is used, which makes compari-
son with other approaches difficult.
Our work is organized in the following way:
</bodyText>
<page confidence="0.987895">
434
</page>
<bodyText confidence="0.9564909">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 434–439,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
• We first propose a universal sentiment model
that utilizes various features and resources.
The universal model outperforms the top
system submitted to the SemEval-2013 task
(Mohammad et al., 2013), which was trained
and tested on the same data. The universal
model serves as a strong baseline and also
provides an option for smoothing later.
</bodyText>
<listItem confidence="0.986644928571428">
• We introduce a topic-based mixture model
for Twitter sentiment. The model is inte-
grated in the framework of semi-supervised
training that takes advantage of large amount
of un-annotated Twitter data. Such a mixture
model results in further improvement on the
sentiment classification accuracy.
• We propose a smoothing technique through
interpolation between universal model and
topic-based mixture model.
• We also compare different approaches for
topic modeling, such as cross-domain topic
identification by utilizing data from newswire
domain.
</listItem>
<sectionHeader confidence="0.929293" genericHeader="method">
2 Universal Sentiment Classifier
</sectionHeader>
<bodyText confidence="0.9997775">
In this section we present a universal topic-
independent sentiment classifier to establish a
state-of-the-art baseline. The sentiment labels are
either positive, neutral or negative.
</bodyText>
<subsectionHeader confidence="0.993252">
2.1 SVM Classifier
</subsectionHeader>
<bodyText confidence="0.999945642857143">
Support Vector Machine (SVM) is an effec-
tive classifier that can achieve good performance
in high-dimensional feature space. An SVM
model represents the examples as points in space,
mapped so that the examples of the different cate-
gories are separated by a clear margin as wide as
possible. In this work an SVM classifier is trained
with LibSVM (Chang and Lin, 2011), a widely
used toolkit. The linear kernel is found to achieve
higher accuracy than other kernels in our initial ex-
periments. The option of probability estimation in
LibSVM is turned on so that it can produce the
probability of sentiment class c given tweet x at
the classification time, i.e. P(c|x).
</bodyText>
<subsectionHeader confidence="0.960055">
2.2 Features
</subsectionHeader>
<bodyText confidence="0.998458">
The training and testing data are run through
tweet-specific tokenization, similar to that used in
the CMU Twitter NLP tool (Gimpel et al., 2011).
It is shown in Section 5 that such customized tok-
enization is helpful. Here are the features that we
use for classification:
</bodyText>
<listItem confidence="0.972259818181818">
• Word N-grams: if certain N-gram (unigram,
bigram, trigram or 4-gram) appears in the
tweet, the corresponding feature is set to 1,
otherwise 0. These features are collected
from training data, with a count cutoff to
avoid overtraining.
• Manual lexicons: it has been shown in other
work (Nakov et al., 2013) that lexicons with
positive and negative words are important to
sentiment classification. In this work, we
adopt the lexicon from Bing Liu (Hu and
</listItem>
<bodyText confidence="0.7106876">
Liu, 2004) which includes about 2000 posi-
tive words and 4700 negative words. We also
experimented with the popular MPQA (Wil-
son et al., 2005) lexicon but found no extra
improvement on accuracies. A short list of
Twitter-specific positive/negative words are
also added to enhance the lexicons. We gen-
erate two features based on the lexicons: total
number of positive words or negative words
found in each tweet.
</bodyText>
<listItem confidence="0.890221333333333">
• Emoticons: it is known that people use emoti-
cons in social media data to express their
emotions. A set of popular emoticons are col-
lected from the Twitter data we have. Two
features are created to represent the presence
or absence of any positive/negative emoti-
cons.
• Last sentiment word: a “sentiment word” is
any word in the positive/negative lexicons
mentioned above. If the last sentiment word
found in the tweet is positive (or negative),
this feature is set to 1 (or -1). If none of the
words in the tweet is sentiment word, it is set
to 0 by default.
• PMI unigram lexicons: in (Mohammad et
</listItem>
<bodyText confidence="0.626426888888889">
al., 2013) two lexicons were automatically
generated based on pointwise mutual infor-
mation (PMI). One is NRC Hashtag Senti-
ment Lexicon with 54K unigrams, and the
other is Sentiment140 Lexicon with 62K un-
igrams. Each word in the lexicon has an as-
sociated sentiment score. We compute 7 fea-
tures based on each of the two lexicons: (1)
sum of sentiment score; (2) total number of
</bodyText>
<page confidence="0.996624">
435
</page>
<bodyText confidence="0.999974111111111">
positive words (with score s &gt; 1); (3) to-
tal number of negative words (s &lt; −1); (4)
maximal positive score; (5) minimal negative
score; (6) score of the last positive words; (7)
score of the last negative words. Note that for
the second and third features, we ignore those
with sentiment scores between -1 and 1, since
we found that inclusion of those weak subjec-
tive words results in unstable performance.
</bodyText>
<listItem confidence="0.993863653846154">
• PMI bigram lexicon: there are also 316K bi-
grams in the NRC Hashtag Sentiment Lexi-
con. For bigrams, we did not find the sen-
timent scores useful. Instead, we only com-
pute two features based on counts only: total
number of positive bigrams; total number of
negative bigrams.
• Punctuations: if there exists exclamation
mark or question mark in the tweet, the fea-
ture is set to 1, otherwise set to 0.
• Hashtag count: the number of hashtags in
each tweet.
• Negation: we collect a list of negation words,
including some informal words frequently
observed in online conversations, such as
“dunno” (“don’t know”), “nvr” (“never”),
etc. For any sentiment words within a win-
dow following a negation word and not af-
ter punctuations ‘.’, ‘,’, ‘;’, ‘?’, or ‘!’, we re-
verse its sentiment from positive to negative,
or vice versa, before computing the lexicon-
based features mentioned earlier. The win-
dow size was set to 4 in this work.
• Elongated words: the number of words in the
tweet that have letters repeated by at least 3
times in a row, e.g. the word “gooood”.
</listItem>
<sectionHeader confidence="0.982536" genericHeader="method">
3 Topic-Based Sentiment Mixture
</sectionHeader>
<subsectionHeader confidence="0.980894">
3.1 Topic Modeling
</subsectionHeader>
<bodyText confidence="0.999687619047619">
Latent Dirichlet Allocation (LDA) (Blei et al.,
2003) is one of the widely adopted generative
models for topic modeling. The fundamental idea
is that a document is a mixture of topics. For each
document there is a multinomial distribution over
topics, and a Dirichlet prior Dir(α) is introduced
on such distribution. For each topic, there is an-
other multinomial distribution over words. One of
the popular algorithms for LDA model parameter
estimation and inference is Gibbs sampling (Grif-
fiths and Steyvers, 2004), a form of Markov Chain
Monte Carlo. We adopt the efficient implementa-
tion of Gibbs sampling as proposed in (Yao et al.,
2009) in this work.
Each tweet is regarded as one document. We
conduct pre-processing by removing stop words
and some of the frequent words found in Twitter
data. Suppose that there are T topics in total in the
training data, i.e. t1, t2, ..., tT. The posterior prob-
ability of each topic given tweet xi is computed as
in Eq. 1:
</bodyText>
<equation confidence="0.996178">
Pt(tj|xi) = T (1)
� Cij + αj k=1 Cik + Tαj
</equation>
<bodyText confidence="0.999959166666667">
where Cij is the number of times that topic tj is
assigned to some word in tweet xi, usually aver-
aged over multiple iterations of Gibbs sampling.
αj is the j-th dimension of the hyperparameter of
Dirichlet distribution that can be optimized during
model estimation.
</bodyText>
<subsectionHeader confidence="0.998353">
3.2 Sentiment Mixture Model
</subsectionHeader>
<bodyText confidence="0.9999408">
Once we identify the topics for tweets in the train-
ing data, we can split the data into multiple sub-
sets based on topic distributions. For each subset,
a separate sentiment model can be trained. There
are many ways of splitting the data. For example,
K-means clustering can be conducted based on
the similarity between the topic distribution vec-
tors or their transformed versions. In this work,
we assign tweet xi to cluster j if Pt(tj|xi) &gt; τ
or Pt(tj|xi) = maxk Pt(tk|xi). Note that this is
a soft clustering, with some tweets possibily as-
signed to multiple topic-specific clusters. Similar
to the universal model, we train T topic-specific
sentiment models with LibSVM.
During classification on test tweets, we run
topic inference and sentiment classification with
multiple sentiment models. They jointly deter-
mine the final probability of sentiment class c
given tweet xi as the following in a sentiment mix-
ture model:
</bodyText>
<equation confidence="0.997693666666667">
T
P(c|xi) = E Pm(c|tj, xi)Pt(tj|xi) (2)
j=1
</equation>
<bodyText confidence="0.995859666666667">
where Pm(c|tj, xi) is the probability of sentiment
c from topic-specific sentiment model trained on
topic tj.
</bodyText>
<page confidence="0.998242">
436
</page>
<subsectionHeader confidence="0.995095">
3.3 Smoothing
</subsectionHeader>
<bodyText confidence="0.9997065">
Additionally, we also experiment with a smooth-
ing technique through linear interpolation between
the universal sentiment model and topic-based
sentiment mixture model.
</bodyText>
<equation confidence="0.9994445">
P(c|xi) = 0 X PU(c|xi) + (1 − 0)
T
X E Pm(c|tj, xi)Pt(tj|xi) (3)
j=1
</equation>
<bodyText confidence="0.999937">
where 0 is the interpolation parameter and
PU(c|xi) is the probability of sentiment c given
tweet xi from the universal sentiment model.
</bodyText>
<sectionHeader confidence="0.997139" genericHeader="method">
4 Semi-supervised Training
</sectionHeader>
<bodyText confidence="0.99862825">
In this section we propose an integrated frame-
work of semi-supervised training that contains
both topic modeling and sentiment classification.
The idea of semi-supervised training is to take
advantage of large amount low-cost un-annotated
data (tweets in this case) to further improve the ac-
curacy of sentiment classification. The algorithm
is as follows:
</bodyText>
<listItem confidence="0.987521826086957">
1. Set training corpus D for sentiment classifi-
cation to be the annotated training data Da;
2. Train a sentiment model with current training
corpus D;
3. Run sentiment classification on the un-
annotated data Du with the current sentiment
model and generate probabilities of sentiment
classes for each tweet, P(c|xi);
4. Perform data selection. For those tweets with
P(c|xi) &gt; p, add them to current training
corpus D. The rest is used to replace the un-
annotated corpus Du;
5. Train a topic model on D, and store the topic
inference model and topic distributions of
each tweet;
6. Cluster data in D based on the topic distribu-
tions from Step 5 and train a separate senti-
ment model for each cluster. Replace current
sentiment model with the new sentiment mix-
ture model;
7. Repeat from Step 3 until finishing a pre-
determined number of iterations or no more
data is added to D in Step 4.
</listItem>
<sectionHeader confidence="0.982727" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.977606">
5.1 Data and Evaluation
</subsectionHeader>
<bodyText confidence="0.999978">
We conduct experiments on the data from the task
B of Sentiment Analysis in Twitter in SemEval-
2013. The distribution of positive, neutral and
negative data is shown in Table 1. The develop-
ment set is used to tune parameters and features.
The test set is for the blind evaluation.
</bodyText>
<table confidence="0.99878425">
Set Pos Neu Neg Total
Training 3640 4586 1458 9684
Dev 575 739 340 1654
Test 1572 1640 601 3813
</table>
<tableCaption confidence="0.985052">
Table 1: Data from SemEval-2013. Pos: positive;
Neu: neutral; Neg: negative.
</tableCaption>
<bodyText confidence="0.9997125">
For semi-supervised training experiments, we
explored two sets of additional data. The first
one contains 2M tweets randomly sampled from
the collection in January and February 2014. The
other contains 74K news documents with 50M
words collected during the first half year of 2013
from online newswire.
For evaluation, we use macro averaged F score
as in (Nakov et al., 2013), i.e. average of the F
scores computed on positive and negative classes
only. Note that this does not make the task a binary
classification problem. Any errors related to neu-
tral class (false positives or false negatives) will
negatively impact the F scores.
</bodyText>
<subsectionHeader confidence="0.988743">
5.2 Universal Model
</subsectionHeader>
<bodyText confidence="0.999666294117647">
In Table 2, we show the incremental improvement
in adding various features described in Section 2,
measured on the test set. In addition to the fea-
tures, we also find SVM weighting on the training
samples is helpful. Due to the skewness in class
distribution in the training set, it is observed dur-
ing error analysis on the development set that sub-
jective (positive/negative) tweets are more likely
to be classified as neutral tweets. The weights for
positive, neutral and negative samples are set to
be (1, 0.4, 1) based on the results on the develop-
ment set. As shown in Table 2, weighting adds a
2% improvement. With all features combined, the
universal sentiment model achieves 69.7 on aver-
age F score. The F score from the best system in
SemEval-2013 (Mohammad et al., 2013) is also
listed in the last row of Table 2 for a comparison.
</bodyText>
<page confidence="0.995797">
437
</page>
<table confidence="0.999845384615385">
Model Avg. F score
Baseline with word N-grams 55.0
+ tweet tokenization 56.1
+ manual lexicon features 62.4
+ emoticons 62.8
+ last sentiment word 63.7
+ PMI unigram lexicons 64.5
+ hashtag counts 65.0
+ SVM weighting 67.0
+ PMI bigram lexicons 68.2
+ negations 69.0
+ elongated words 69.7
Mohammad et al., 2013 69.0
</table>
<tableCaption confidence="0.9919065">
Table 2: Results on the test set with universal sen-
timent model.
</tableCaption>
<subsectionHeader confidence="0.992454">
5.3 Topic-Based Mixture Model
</subsectionHeader>
<bodyText confidence="0.99996825">
For the topic-based mixture model and semi-
supervised training, based on the experiments on
the development set, we set the parameter τ used
in soft clustering to 0.4, the data selection pa-
rameter p to 0.96, and the interpolation parame-
ter for smoothing θ to 0.3. We found no more
noticeable benefits after two iterations of semi-
supervised training. The number of topics is set
to 100.
The results on the test set are shown Table 3,
with the topic information inferred from either
Twitter data (second column) or newswire data
(third column). The first row shows the per-
formance of the universal sentiment model as
a baseline. The second row shows the results
from re-training the universal model by simply
adding tweets selected from two iterations of
semi-supervised training (about 100K). It serves
as another baseline with more training data, for
a fair comparison with the topic-based mixture
modeling that uses the same amount of training
data.
We also conduct an experiment by only consid-
ering the most likely topic for each tweet when
computing the sentiment probabilities. The results
show that the topic-based mixture model outper-
forms both the baseline and the one that considers
the top topics only. Smoothing with the universal
model adds further improvement in addition to the
un-smoothed mixture model. With the topic in-
formation inferred from Twitter data, the F score
is 2 points higher than the baseline without semi-
</bodyText>
<table confidence="0.998714833333333">
Model Tweet-topic News-topic
Baseline 69.7 69.7
+ semi-supervised 70.3 70.2
top topic only 70.6 70.4
mixture 71.2 70.8
+ smoothing 71.7 71.1
</table>
<tableCaption confidence="0.994293">
Table 3: Results of topic-based sentiment mixture
model on SemEval test set.
</tableCaption>
<bodyText confidence="0.9777316875">
supervised training and 1.4 higher than the base-
line with semi-supervised data.
As shown in the third column in Table 3, sur-
prisingly, the model with topic information in-
ferred from the newswire data works well on the
Twitter domain. A 1.4 points of improvement can
be obtained compared to the baseline. This pro-
vides an opportunity for cross-domain topic iden-
tification when data from certain domain is more
difficult to obtain than others.
In Table 4, we provide some examples from the
topics identified in tweets as well as the newswire
data. The most frequent words in each topic are
listed in the table. We can clearly see that the top-
ics are about phones, sports, sales and politics, re-
spectively.
</bodyText>
<table confidence="0.99625725">
Tweet-1 Tweet-2 News-1 News-2
phone game sales party
call great stores government
answer play online election
question team retail minister
service win store political
text tonight retailer prime
texting super business state
</table>
<tableCaption confidence="0.924901">
Table 4: The most frequent words in example top-
ics from tweets and newswire data.
</tableCaption>
<sectionHeader confidence="0.99904" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999847">
In this paper, we presented multiple approaches
for advanced Twitter sentiment analysis. We es-
tablished a state-of-the-art baseline that utilizes a
variety of features, and built a topic-based sen-
timent mixture model with topic-specific Twitter
data, all integrated in a semi-supervised training
framework. The proposed model outperforms the
top system in SemEval-2013. Further research is
needed to continue to improve the accuracy in this
difficult domain.
</bodyText>
<page confidence="0.998617">
438
</page>
<sectionHeader confidence="0.990336" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999583626865672">
David Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. In Journal of Ma-
chine Learning Research. 3(2003), 993–1022.
S. R. K. Branavan, Harr Chen, Jacob Eisenstein, and
Regina Barzilay. 2008. Learning document-level
semantic properties from free-text annotations. In
Proceedings of the Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-2008).
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. In
ACM Transactions on Intelligent Systems and Tech-
nology.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: annotation, features, and experiments.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. In Proceedings of the National
Academy of Science. 101, 5228–5235.
Yulan He, Chenghua Lin, Wei Gao, and Kam-Fai
Wong. 2012. Tracking sentiment and topic dynam-
ics from social media. In Proceedings of the 6th In-
ternational AAAI Conference on Weblogs and Social
Media (ICWSM-2012).
Mingqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Proceedings of the
Tenth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining.
Yohan Jo and Alice Oh. 2011. Aspect and senti-
ment unification model for online review analysis.
In Proceedings of ACM Conference in Web Search
and Data Mining (WSDM-2011).
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: modeling facets and opinions in weblogs. In
Proceedings of International Conference on World
Wide Web (WWW-2007).
Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. NRC-Canada: Building the state-
of-the-art in sentiment analysis of tweets. In Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013). 312-320, At-
lanta, Georgia, June 14-15, 2013.
Preslav Nakov, Zornitsa Kozareva, Alan Ritter, Sara
Rosenthal, Veselin Stoyanov, and Theresa Wilson.
2013. SemEval-2013 Task 2: Sentiment Analysis in
Twitter. In Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013).
312-320, Atlanta, Georgia, June 14-15, 2013.
Jianfeng Si, Arjun Mukherjee, Bing Liu, Qing Li,
Huayi Li, and Xiaotie Deng. 2013. Exploiting topic
based Twitter sentiment for stock prediction. In Pro-
ceedings of the 51st Annual Meeting of the Associ-
ation for Computational Linguistics. 24-29, Sofia,
Bulgaria, August 4-9,2013.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT 05.
Limin Yao, David Mimno, and Andrew McCallum.
2009. Efficient methods for topic model inference
on streaming document collections. KDD’09.
</reference>
<page confidence="0.999264">
439
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.421176">
<title confidence="0.999528">Improving Twitter Sentiment Analysis with Topic-Based Modeling and Semi-Supervised Training</title>
<author confidence="0.999901">Bing Xiang</author>
<affiliation confidence="0.996093">IBM</affiliation>
<address confidence="0.8316885">1101 Kitchawan Yorktown Heights, NY 10598,</address>
<email confidence="0.999656">bingxia@us.ibm.com</email>
<abstract confidence="0.949698125">In this paper, we present multiple approaches to improve sentiment analysis on Twitter data. We first establish a state-of-the-art baseline with a rich feature set. Then we build a topic-based sentiment mixture model with topic-specific data in a semi-supervised training framework. The topic information is generated through topic modeling based on an efficient implementation of Latent Dirichlet Allocation (LDA). The proposed sentiment model outperforms the top system the task of Analysis in Twit- SemEval-2013 in terms of averaged F scores.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>In Journal of Machine Learning Research.</journal>
<volume>3</volume>
<issue>2003</issue>
<pages>993--1022</pages>
<contexts>
<context position="9465" citStr="Blei et al., 2003" startWordPosition="1557" endWordPosition="1560">observed in online conversations, such as “dunno” (“don’t know”), “nvr” (“never”), etc. For any sentiment words within a window following a negation word and not after punctuations ‘.’, ‘,’, ‘;’, ‘?’, or ‘!’, we reverse its sentiment from positive to negative, or vice versa, before computing the lexiconbased features mentioned earlier. The window size was set to 4 in this work. • Elongated words: the number of words in the tweet that have letters repeated by at least 3 times in a row, e.g. the word “gooood”. 3 Topic-Based Sentiment Mixture 3.1 Topic Modeling Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is one of the widely adopted generative models for topic modeling. The fundamental idea is that a document is a mixture of topics. For each document there is a multinomial distribution over topics, and a Dirichlet prior Dir(α) is introduced on such distribution. For each topic, there is another multinomial distribution over words. One of the popular algorithms for LDA model parameter estimation and inference is Gibbs sampling (Griffiths and Steyvers, 2004), a form of Markov Chain Monte Carlo. We adopt the efficient implementation of Gibbs sampling as proposed in (Yao et al., 2009) in this wor</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. In Journal of Machine Learning Research. 3(2003), 993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>Harr Chen</author>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Learning document-level semantic properties from free-text annotations.</title>
<date>2008</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL-2008).</booktitle>
<contexts>
<context position="3173" citStr="Branavan et al., 2008" startWordPosition="510" endWordPosition="513">is used as a negative word (as in the first tweet), but it bears no sentiment in the second tweet when people are talking about a football game. Even though some local contextual features could be helpful to distinguish the two cases above, they still may not be enough to get the sentiment on the whole message correct. Also, the local features often suffer from the sparsity problem. This motivates us to explore topic information explicitly in the task of sentiment analysis on Twitter data. There exists some work on applying topic information in sentiment analysis, such as (Mei et al., 2007), (Branavan et al., 2008), (Jo and Oh, 2011) and (He et al., 2012). All these work are significantly different from what we propose in this work. Also they are conducted in a domain other than Twitter. Most recently, Si et al. (2013) propose a continuous Dirichlet Process Mixture model for Twitter sentiment, for the purpose of stock prediction. Unfortunately there is no evaluation on the accuracy of sentiment classification alone in that work. Furthermore, no standard training or test corpus is used, which makes comparison with other approaches difficult. Our work is organized in the following way: 434 Proceedings of </context>
</contexts>
<marker>Branavan, Chen, Eisenstein, Barzilay, 2008</marker>
<rawString>S. R. K. Branavan, Harr Chen, Jacob Eisenstein, and Regina Barzilay. 2008. Learning document-level semantic properties from free-text annotations. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL-2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<booktitle>In ACM Transactions on Intelligent Systems and Technology.</booktitle>
<contexts>
<context position="5462" citStr="Chang and Lin, 2011" startWordPosition="862" endWordPosition="865">om newswire domain. 2 Universal Sentiment Classifier In this section we present a universal topicindependent sentiment classifier to establish a state-of-the-art baseline. The sentiment labels are either positive, neutral or negative. 2.1 SVM Classifier Support Vector Machine (SVM) is an effective classifier that can achieve good performance in high-dimensional feature space. An SVM model represents the examples as points in space, mapped so that the examples of the different categories are separated by a clear margin as wide as possible. In this work an SVM classifier is trained with LibSVM (Chang and Lin, 2011), a widely used toolkit. The linear kernel is found to achieve higher accuracy than other kernels in our initial experiments. The option of probability estimation in LibSVM is turned on so that it can produce the probability of sentiment class c given tweet x at the classification time, i.e. P(c|x). 2.2 Features The training and testing data are run through tweet-specific tokenization, similar to that used in the CMU Twitter NLP tool (Gimpel et al., 2011). It is shown in Section 5 that such customized tokenization is helpful. Here are the features that we use for classification: • Word N-grams</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. In ACM Transactions on Intelligent Systems and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for Twitter: annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech tagging for Twitter: annotation, features, and experiments. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>In Proceedings of the National Academy of Science.</booktitle>
<volume>101</volume>
<pages>5228--5235</pages>
<contexts>
<context position="9926" citStr="Griffiths and Steyvers, 2004" startWordPosition="1630" endWordPosition="1634"> repeated by at least 3 times in a row, e.g. the word “gooood”. 3 Topic-Based Sentiment Mixture 3.1 Topic Modeling Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is one of the widely adopted generative models for topic modeling. The fundamental idea is that a document is a mixture of topics. For each document there is a multinomial distribution over topics, and a Dirichlet prior Dir(α) is introduced on such distribution. For each topic, there is another multinomial distribution over words. One of the popular algorithms for LDA model parameter estimation and inference is Gibbs sampling (Griffiths and Steyvers, 2004), a form of Markov Chain Monte Carlo. We adopt the efficient implementation of Gibbs sampling as proposed in (Yao et al., 2009) in this work. Each tweet is regarded as one document. We conduct pre-processing by removing stop words and some of the frequent words found in Twitter data. Suppose that there are T topics in total in the training data, i.e. t1, t2, ..., tT. The posterior probability of each topic given tweet xi is computed as in Eq. 1: Pt(tj|xi) = T (1) � Cij + αj k=1 Cik + Tαj where Cij is the number of times that topic tj is assigned to some word in tweet xi, usually averaged over </context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. In Proceedings of the National Academy of Science. 101, 5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulan He</author>
<author>Chenghua Lin</author>
<author>Wei Gao</author>
<author>Kam-Fai Wong</author>
</authors>
<title>Tracking sentiment and topic dynamics from social media.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International AAAI Conference on Weblogs and Social Media (ICWSM-2012).</booktitle>
<contexts>
<context position="3214" citStr="He et al., 2012" startWordPosition="519" endWordPosition="522">et), but it bears no sentiment in the second tweet when people are talking about a football game. Even though some local contextual features could be helpful to distinguish the two cases above, they still may not be enough to get the sentiment on the whole message correct. Also, the local features often suffer from the sparsity problem. This motivates us to explore topic information explicitly in the task of sentiment analysis on Twitter data. There exists some work on applying topic information in sentiment analysis, such as (Mei et al., 2007), (Branavan et al., 2008), (Jo and Oh, 2011) and (He et al., 2012). All these work are significantly different from what we propose in this work. Also they are conducted in a domain other than Twitter. Most recently, Si et al. (2013) propose a continuous Dirichlet Process Mixture model for Twitter sentiment, for the purpose of stock prediction. Unfortunately there is no evaluation on the accuracy of sentiment classification alone in that work. Furthermore, no standard training or test corpus is used, which makes comparison with other approaches difficult. Our work is organized in the following way: 434 Proceedings of the 52nd Annual Meeting of the Associatio</context>
</contexts>
<marker>He, Lin, Gao, Wong, 2012</marker>
<rawString>Yulan He, Chenghua Lin, Wei Gao, and Kam-Fai Wong. 2012. Tracking sentiment and topic dynamics from social media. In Proceedings of the 6th International AAAI Conference on Weblogs and Social Media (ICWSM-2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mingqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="6514" citStr="Hu and Liu, 2004" startWordPosition="1039" endWordPosition="1042">ol (Gimpel et al., 2011). It is shown in Section 5 that such customized tokenization is helpful. Here are the features that we use for classification: • Word N-grams: if certain N-gram (unigram, bigram, trigram or 4-gram) appears in the tweet, the corresponding feature is set to 1, otherwise 0. These features are collected from training data, with a count cutoff to avoid overtraining. • Manual lexicons: it has been shown in other work (Nakov et al., 2013) that lexicons with positive and negative words are important to sentiment classification. In this work, we adopt the lexicon from Bing Liu (Hu and Liu, 2004) which includes about 2000 positive words and 4700 negative words. We also experimented with the popular MPQA (Wilson et al., 2005) lexicon but found no extra improvement on accuracies. A short list of Twitter-specific positive/negative words are also added to enhance the lexicons. We generate two features based on the lexicons: total number of positive words or negative words found in each tweet. • Emoticons: it is known that people use emoticons in social media data to express their emotions. A set of popular emoticons are collected from the Twitter data we have. Two features are created to </context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Mingqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yohan Jo</author>
<author>Alice Oh</author>
</authors>
<title>Aspect and sentiment unification model for online review analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of ACM Conference in Web Search and Data Mining (WSDM-2011).</booktitle>
<contexts>
<context position="3192" citStr="Jo and Oh, 2011" startWordPosition="514" endWordPosition="517">d (as in the first tweet), but it bears no sentiment in the second tweet when people are talking about a football game. Even though some local contextual features could be helpful to distinguish the two cases above, they still may not be enough to get the sentiment on the whole message correct. Also, the local features often suffer from the sparsity problem. This motivates us to explore topic information explicitly in the task of sentiment analysis on Twitter data. There exists some work on applying topic information in sentiment analysis, such as (Mei et al., 2007), (Branavan et al., 2008), (Jo and Oh, 2011) and (He et al., 2012). All these work are significantly different from what we propose in this work. Also they are conducted in a domain other than Twitter. Most recently, Si et al. (2013) propose a continuous Dirichlet Process Mixture model for Twitter sentiment, for the purpose of stock prediction. Unfortunately there is no evaluation on the accuracy of sentiment classification alone in that work. Furthermore, no standard training or test corpus is used, which makes comparison with other approaches difficult. Our work is organized in the following way: 434 Proceedings of the 52nd Annual Mee</context>
</contexts>
<marker>Jo, Oh, 2011</marker>
<rawString>Yohan Jo and Alice Oh. 2011. Aspect and sentiment unification model for online review analysis. In Proceedings of ACM Conference in Web Search and Data Mining (WSDM-2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>Xu Ling</author>
<author>Matthew Wondra</author>
<author>Hang Su</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Topic sentiment mixture: modeling facets and opinions in weblogs.</title>
<date>2007</date>
<booktitle>In Proceedings of International Conference on World Wide Web (WWW-2007).</booktitle>
<contexts>
<context position="3148" citStr="Mei et al., 2007" startWordPosition="506" endWordPosition="509">nerally “offensive” is used as a negative word (as in the first tweet), but it bears no sentiment in the second tweet when people are talking about a football game. Even though some local contextual features could be helpful to distinguish the two cases above, they still may not be enough to get the sentiment on the whole message correct. Also, the local features often suffer from the sparsity problem. This motivates us to explore topic information explicitly in the task of sentiment analysis on Twitter data. There exists some work on applying topic information in sentiment analysis, such as (Mei et al., 2007), (Branavan et al., 2008), (Jo and Oh, 2011) and (He et al., 2012). All these work are significantly different from what we propose in this work. Also they are conducted in a domain other than Twitter. Most recently, Si et al. (2013) propose a continuous Dirichlet Process Mixture model for Twitter sentiment, for the purpose of stock prediction. Unfortunately there is no evaluation on the accuracy of sentiment classification alone in that work. Furthermore, no standard training or test corpus is used, which makes comparison with other approaches difficult. Our work is organized in the following</context>
</contexts>
<marker>Mei, Ling, Wondra, Su, Zhai, 2007</marker>
<rawString>Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and ChengXiang Zhai. 2007. Topic sentiment mixture: modeling facets and opinions in weblogs. In Proceedings of International Conference on World Wide Web (WWW-2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>NRC-Canada: Building the stateof-the-art in sentiment analysis of tweets.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>312--320</pages>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="4167" citStr="Mohammad et al., 2013" startWordPosition="664" endWordPosition="667">cy of sentiment classification alone in that work. Furthermore, no standard training or test corpus is used, which makes comparison with other approaches difficult. Our work is organized in the following way: 434 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 434–439, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics • We first propose a universal sentiment model that utilizes various features and resources. The universal model outperforms the top system submitted to the SemEval-2013 task (Mohammad et al., 2013), which was trained and tested on the same data. The universal model serves as a strong baseline and also provides an option for smoothing later. • We introduce a topic-based mixture model for Twitter sentiment. The model is integrated in the framework of semi-supervised training that takes advantage of large amount of un-annotated Twitter data. Such a mixture model results in further improvement on the sentiment classification accuracy. • We propose a smoothing technique through interpolation between universal model and topic-based mixture model. • We also compare different approaches for top</context>
<context position="7526" citStr="Mohammad et al., 2013" startWordPosition="1216" endWordPosition="1219">ach tweet. • Emoticons: it is known that people use emoticons in social media data to express their emotions. A set of popular emoticons are collected from the Twitter data we have. Two features are created to represent the presence or absence of any positive/negative emoticons. • Last sentiment word: a “sentiment word” is any word in the positive/negative lexicons mentioned above. If the last sentiment word found in the tweet is positive (or negative), this feature is set to 1 (or -1). If none of the words in the tweet is sentiment word, it is set to 0 by default. • PMI unigram lexicons: in (Mohammad et al., 2013) two lexicons were automatically generated based on pointwise mutual information (PMI). One is NRC Hashtag Sentiment Lexicon with 54K unigrams, and the other is Sentiment140 Lexicon with 62K unigrams. Each word in the lexicon has an associated sentiment score. We compute 7 features based on each of the two lexicons: (1) sum of sentiment score; (2) total number of 435 positive words (with score s &gt; 1); (3) total number of negative words (s &lt; −1); (4) maximal positive score; (5) minimal negative score; (6) score of the last positive words; (7) score of the last negative words. Note that for the </context>
<context position="15382" citStr="Mohammad et al., 2013" startWordPosition="2560" endWordPosition="2563">also find SVM weighting on the training samples is helpful. Due to the skewness in class distribution in the training set, it is observed during error analysis on the development set that subjective (positive/negative) tweets are more likely to be classified as neutral tweets. The weights for positive, neutral and negative samples are set to be (1, 0.4, 1) based on the results on the development set. As shown in Table 2, weighting adds a 2% improvement. With all features combined, the universal sentiment model achieves 69.7 on average F score. The F score from the best system in SemEval-2013 (Mohammad et al., 2013) is also listed in the last row of Table 2 for a comparison. 437 Model Avg. F score Baseline with word N-grams 55.0 + tweet tokenization 56.1 + manual lexicon features 62.4 + emoticons 62.8 + last sentiment word 63.7 + PMI unigram lexicons 64.5 + hashtag counts 65.0 + SVM weighting 67.0 + PMI bigram lexicons 68.2 + negations 69.0 + elongated words 69.7 Mohammad et al., 2013 69.0 Table 2: Results on the test set with universal sentiment model. 5.3 Topic-Based Mixture Model For the topic-based mixture model and semisupervised training, based on the experiments on the development set, we set the </context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. NRC-Canada: Building the stateof-the-art in sentiment analysis of tweets. In Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013). 312-320, Atlanta, Georgia, June 14-15, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Zornitsa Kozareva</author>
<author>Alan Ritter</author>
<author>Sara Rosenthal</author>
<author>Veselin Stoyanov</author>
<author>Theresa Wilson</author>
</authors>
<title>SemEval-2013 Task 2: Sentiment Analysis in Twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>312--320</pages>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="1619" citStr="Nakov et al. (2013)" startWordPosition="254" endWordPosition="257">g (NLP) and related fields. Sentiment analysis is one of the areas that has large potential in real-world applications. For example, monitoring the trend of sentiment for a specific company or product mentioned in social media can be useful in stock prediction and product marketing. In this paper, we focus on sentiment analysis of Twitter data (tweets). It is one of the challenging tasks in NLP given the length limit on each tweet (up to 140 characters) and also the informal conversation. Many approaches have been proposed previously to improve sentiment analysis on Twitter data. For example, Nakov et al. (2013) provide an overview on the systems submitted to one of the SemEval-2013 tasks, Sentiment Analysis in Twitter. A variety of features have been utilized for * This work was done when the author was with Thomson Reuters. Liang Zhou Thomson Reuters 3 Times Square New York, NY 10036, USA l.zhou@thomsonreuters.com sentiment classification on tweets. They include lexical features (e.g. word lexicon), syntactic features (e.g. Part-of-Speech), Twitter-specific features (e.g. emoticons), etc. However, all of these features only capture local information in the data and do not take into account of the g</context>
<context position="6356" citStr="Nakov et al., 2013" startWordPosition="1013" endWordPosition="1016">cation time, i.e. P(c|x). 2.2 Features The training and testing data are run through tweet-specific tokenization, similar to that used in the CMU Twitter NLP tool (Gimpel et al., 2011). It is shown in Section 5 that such customized tokenization is helpful. Here are the features that we use for classification: • Word N-grams: if certain N-gram (unigram, bigram, trigram or 4-gram) appears in the tweet, the corresponding feature is set to 1, otherwise 0. These features are collected from training data, with a count cutoff to avoid overtraining. • Manual lexicons: it has been shown in other work (Nakov et al., 2013) that lexicons with positive and negative words are important to sentiment classification. In this work, we adopt the lexicon from Bing Liu (Hu and Liu, 2004) which includes about 2000 positive words and 4700 negative words. We also experimented with the popular MPQA (Wilson et al., 2005) lexicon but found no extra improvement on accuracies. A short list of Twitter-specific positive/negative words are also added to enhance the lexicons. We generate two features based on the lexicons: total number of positive words or negative words found in each tweet. • Emoticons: it is known that people use </context>
<context position="14323" citStr="Nakov et al., 2013" startWordPosition="2378" endWordPosition="2381">une parameters and features. The test set is for the blind evaluation. Set Pos Neu Neg Total Training 3640 4586 1458 9684 Dev 575 739 340 1654 Test 1572 1640 601 3813 Table 1: Data from SemEval-2013. Pos: positive; Neu: neutral; Neg: negative. For semi-supervised training experiments, we explored two sets of additional data. The first one contains 2M tweets randomly sampled from the collection in January and February 2014. The other contains 74K news documents with 50M words collected during the first half year of 2013 from online newswire. For evaluation, we use macro averaged F score as in (Nakov et al., 2013), i.e. average of the F scores computed on positive and negative classes only. Note that this does not make the task a binary classification problem. Any errors related to neutral class (false positives or false negatives) will negatively impact the F scores. 5.2 Universal Model In Table 2, we show the incremental improvement in adding various features described in Section 2, measured on the test set. In addition to the features, we also find SVM weighting on the training samples is helpful. Due to the skewness in class distribution in the training set, it is observed during error analysis on </context>
</contexts>
<marker>Nakov, Kozareva, Ritter, Rosenthal, Stoyanov, Wilson, 2013</marker>
<rawString>Preslav Nakov, Zornitsa Kozareva, Alan Ritter, Sara Rosenthal, Veselin Stoyanov, and Theresa Wilson. 2013. SemEval-2013 Task 2: Sentiment Analysis in Twitter. In Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013). 312-320, Atlanta, Georgia, June 14-15, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Si</author>
<author>Arjun Mukherjee</author>
<author>Bing Liu</author>
<author>Qing Li</author>
<author>Huayi Li</author>
<author>Xiaotie Deng</author>
</authors>
<title>Exploiting topic based Twitter sentiment for stock prediction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. 24-29,</booktitle>
<pages>4--9</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="3381" citStr="Si et al. (2013)" startWordPosition="548" endWordPosition="551">guish the two cases above, they still may not be enough to get the sentiment on the whole message correct. Also, the local features often suffer from the sparsity problem. This motivates us to explore topic information explicitly in the task of sentiment analysis on Twitter data. There exists some work on applying topic information in sentiment analysis, such as (Mei et al., 2007), (Branavan et al., 2008), (Jo and Oh, 2011) and (He et al., 2012). All these work are significantly different from what we propose in this work. Also they are conducted in a domain other than Twitter. Most recently, Si et al. (2013) propose a continuous Dirichlet Process Mixture model for Twitter sentiment, for the purpose of stock prediction. Unfortunately there is no evaluation on the accuracy of sentiment classification alone in that work. Furthermore, no standard training or test corpus is used, which makes comparison with other approaches difficult. Our work is organized in the following way: 434 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 434–439, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics • We first p</context>
</contexts>
<marker>Si, Mukherjee, Liu, Li, Li, Deng, 2013</marker>
<rawString>Jianfeng Si, Arjun Mukherjee, Bing Liu, Qing Li, Huayi Li, and Xiaotie Deng. 2013. Exploiting topic based Twitter sentiment for stock prediction. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. 24-29, Sofia, Bulgaria, August 4-9,2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT 05.</booktitle>
<contexts>
<context position="6645" citStr="Wilson et al., 2005" startWordPosition="1061" endWordPosition="1065">se for classification: • Word N-grams: if certain N-gram (unigram, bigram, trigram or 4-gram) appears in the tweet, the corresponding feature is set to 1, otherwise 0. These features are collected from training data, with a count cutoff to avoid overtraining. • Manual lexicons: it has been shown in other work (Nakov et al., 2013) that lexicons with positive and negative words are important to sentiment classification. In this work, we adopt the lexicon from Bing Liu (Hu and Liu, 2004) which includes about 2000 positive words and 4700 negative words. We also experimented with the popular MPQA (Wilson et al., 2005) lexicon but found no extra improvement on accuracies. A short list of Twitter-specific positive/negative words are also added to enhance the lexicons. We generate two features based on the lexicons: total number of positive words or negative words found in each tweet. • Emoticons: it is known that people use emoticons in social media data to express their emotions. A set of popular emoticons are collected from the Twitter data we have. Two features are created to represent the presence or absence of any positive/negative emoticons. • Last sentiment word: a “sentiment word” is any word in the </context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT 05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>David Mimno</author>
<author>Andrew McCallum</author>
</authors>
<title>Efficient methods for topic model inference on streaming document collections.</title>
<date>2009</date>
<pages>09</pages>
<contexts>
<context position="10053" citStr="Yao et al., 2009" startWordPosition="1654" endWordPosition="1657">ion (LDA) (Blei et al., 2003) is one of the widely adopted generative models for topic modeling. The fundamental idea is that a document is a mixture of topics. For each document there is a multinomial distribution over topics, and a Dirichlet prior Dir(α) is introduced on such distribution. For each topic, there is another multinomial distribution over words. One of the popular algorithms for LDA model parameter estimation and inference is Gibbs sampling (Griffiths and Steyvers, 2004), a form of Markov Chain Monte Carlo. We adopt the efficient implementation of Gibbs sampling as proposed in (Yao et al., 2009) in this work. Each tweet is regarded as one document. We conduct pre-processing by removing stop words and some of the frequent words found in Twitter data. Suppose that there are T topics in total in the training data, i.e. t1, t2, ..., tT. The posterior probability of each topic given tweet xi is computed as in Eq. 1: Pt(tj|xi) = T (1) � Cij + αj k=1 Cik + Tαj where Cij is the number of times that topic tj is assigned to some word in tweet xi, usually averaged over multiple iterations of Gibbs sampling. αj is the j-th dimension of the hyperparameter of Dirichlet distribution that can be opt</context>
</contexts>
<marker>Yao, Mimno, McCallum, 2009</marker>
<rawString>Limin Yao, David Mimno, and Andrew McCallum. 2009. Efficient methods for topic model inference on streaming document collections. KDD’09.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>