<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000154">
<affiliation confidence="0.6417125">
TOWARDS A THEORY OF COMPREHENSION OF DECLARATIVE CONTEXTS
Fernando Gomez
Department of Computer Science
University of Central Florida
</affiliation>
<address confidence="0.615848">
Orlando, Florida 32816
</address>
<email confidence="0.498756">
ABSTRACT
</email>
<bodyText confidence="0.999910166666667">
An outline of a theory of comprehension of
declarative contexts is presented. The main aspect
of the theory being developed is based on Kant&apos;s
distinction between concepts as rules (we have
called them conceptual specialists) and concepts
as an abstract representation (schemata, frames).
Comprehension is viewed as a process dependent on
the conceptual specialists (they contain the infe-
rential knowledge), the schemata or frames (they
contain the declarative knowledge), and a parser.
The function of the parser is to produce a segmen-
tation of the sentences in a case frame structure,
thus determininig the meaning of prepositions,
polysemous verbs, noun group etc. The function of
this parser is not to produce an output to be in-
terpreted by semantic routines or an interpreter,
but to start the parsing process and proceed until
a concept relevant to the theme of the text is
recognized. Then the concept takes control of the
comprehension process overriding the lower level
linguistic process. Hence comprehension is viewed
as a process in which high level sources of know-
ledge (concepts) override lower level linguistic
processes.
</bodyText>
<sectionHeader confidence="0.967839" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999927343283582">
This paper deals with a theory of computer
comprehension of descriptive contexts. By
&amp;quot;descriptive contexts&amp;quot; I refer to the language of
scientific books, text books, this text, etc.. In
the distinction performative vs. declarative,
descriptive texts clearly fall in the declarative
side. Recent work in natural language has dealt
with contexts in which the computer understanding
depends on the meaning of the action verbs and the
human actions (plans, intentions, goals) indicated
by them (Schank and Abelson 1977; Grosz 1977;
Wilensky 1978; Bruce and Newman 1978). Also a ,
considerable amount of work has been done in a
plan-based theory of task oriented dialogues (Cohen
and Perrault 1979; Perrault and Allen 1980; Hobbs
and Evans 1980). This work has had very little
bearing on a theory of ...velputer understanding of
descriptive contexts. One of the main tenets of
the proposed research is that descriptive (or
declarative as we prefer to call them) contexts
call for different theoretical ideas compared
to those proposed for the understanding of human
actions, although, naturally there are aspects that
are common.
An important characteristic of these contexts
is the predominance of descriptive predicates and
verbs (verbs such as &amp;quot;contain,&amp;quot; &amp;quot;refer,&amp;quot; &amp;quot;consist
of,&amp;quot; etc.) over action verbs. A direct result of
this is that the meaning of the sentence does not
depend as much on the main verb of the sentence as
on the concepts that make it up. Hence meaning
representations centered in the main verb of the
sentence are futile for these contexts. We have
approached the problem of comprehension in these
contexts by considering concepts both as active
agents that recognize themselves and as an abstract
representation of the properties of an object. This
aspect of the theory being developed is based on
Kant&apos;s distinction between concepts as rules (we
have called them conceptual specialists) and con-
cepts as an abstract representation (frames, sche-
mata). Comprehension is viewed as a process depen-
dent on the conceptual specialists (they contain
the inferential knowledge), the schemata (they con-
tain structural knowledge), and a parser. The
function of the parser is to produce a segmentation
of the sentences in a case frame structure, thus
determining the meaning of prepositions, polysemous
verbs, noun group, etc.. But the function of this
parser is not to produce an output to be interpre-
ted by semantic routines, but to start the parsing
process and to proceed until a concept relevant to
the theme of the text is recognized. Then the
concept (a cluster of production rules) takes con-
trol of the comprehension process overriding the
lower level linguistic processes. The concept
continues supervising and guiding the parsing until
the sentence has been understood, that is, the
meaning of the sentence has been mapped into the
final internal representation. Thus a text is
parsed directly into the final knowledge structures.
Hence comprehension is viewed as a process in which
high level sources of knowledge (concepts) override
lower level linguistic processes. We have used
these ideas to build a system, called LLULL, to
understand programming problems taken verbatim from
introductory books on programming.
</bodyText>
<listItem confidence="0.575595">
2. Concepts, Schemata and Inferences
</listItem>
<bodyText confidence="0.989235">
In Kant&apos;s Critique of Pure Reason one may find
two views of a concept. According to one view, a
concept is a system of rules governing the applica-
tion of a predicate to an object. The rule that
</bodyText>
<page confidence="0.996542">
36
</page>
<bodyText confidence="0.9999045">
tells us whether the predicate &amp;quot;large&amp;quot; applies to
the concept Canada is a such rule. The system of
rules that allows us to recognize any given
instance of the concept Canada constitutes our
concept of Canada. According to a second view,
Kant considers a concept as an abstract represen-
tation (vorstellung) of the properties of an
object. This second view of a concept is akin to
the notion of concept used in such knowledge
representation languages as FRL, KLONE and KRL.
Frames have played dual functions. They have
been used as a way to organize the inferences, and
also as a structural representation of what is re-
membered of a given situation. This has caused
confusion between two different cognitive aspects:
memory and comprehension (see Ortony, 1978). We
think that one of the reasons for this confusion
is due to the failure in distinguishing between
the two types of concepts (concepts as rules and
concepts as a structural representation). We have
based our analysis on Kant&apos;s distinction in order
to separate clearly between the organization of
the inferences and the memory aspect. For any
given text, a thematic frame contains structural
knowledge about what is remembered of a theme.
One of the slots in this frame contains a list of
the relevant concepts for that theme. Each of
these concepts in this list is separately organized
as a cluster of production rules. They contain
the inferential knowledge that allows the system
to interpret the information being presently
processed, to anticipate incoming information, and
to guide and supervise the parser (see below). In
some instances, the conceptual specialists access
the knowledge stored in the thematic frame to per-
form some of these actions.
</bodyText>
<listItem confidence="0.70043">
3. Linguistic Knowledge, Text Understanding
and Parsing
</listItem>
<bodyText confidence="0.999280192307693">
In text understanding, there are two distinct
issues. One has to do with the mapping of individ-
ual sentences into some internal representation
(syntactic markers, some type of case grammar,
Wilks&apos; preference semantics, Schank&apos;s conceptual
dependency etc.). In designing this mapping,
several approaches have been taken. In Winograd
(1972) and Marcus (1979), there is an interplay
between syntax, and semantic markers (in that
order), while in Wilks (1973) and Riesbeck (1975)
the parser rely almost exclusively on semantic
categories.
A separate issue has to do with the meaning
of the internal representation in relation to the
understanding of the text. For instance, consider
the following text (it belongs to the second
example):
&amp;quot;A bank would like to produce records
of the transactions during an account-
ing period in connection with their
checking accounts. For each account
the bank wants a list showing the
balance at the beginning of tie
period, the number of deposits and
withdrawals, and the final balance.&amp;quot;
Assume that we parse these sentences into our
favorite internal representation. Now what we do
with the internal representation? It is still far
distant from its textual meaning. In fact, the
first sentence is only introducing the topic of he
programming problem. The writer could have
achieved the same effect by saying: &amp;quot;The following
is a checking account problem&amp;quot;. The textual mean-
ing of the second sentence is the description of
the output for that problem. The writer could have
achieved the same effect by saying that the output
for the problem consists of the old-balance,
deposits, withdrawals, etc.. One way to produce
the textual meaning of the sentence is to interpret
the internal representation that has already been
built. Of course, that is equivalent to reparsing
the sentence. Another way is to map the sentence
directly into the final representation or the
textual meaning of the sentence. That is the
approach we have taken. DeJong (1979) and Schenk
et al. (1979) are two recent works that move in
that direction. DeJong&apos;s system, called FRUMP, is
a strong form of top down parser. It skims the
text looking for those concepts in which it is
interested. When it finds all of them, it ignores
the remainder of the text. In analogy to key-word
parsers, we may describe FRUMP as a key-concept
parser. In Schank et al. (1979), words are marked
in the dictionary as skippable or as having high
relevance for a given script. When a relevant word
is found, some questions are formulated as requests
tothe parser. These requests guide the parser in
the understanding of the story. In our opinion,
the criteria by which words are marked as skippable
or relevant are not clear.
There are significant differences between our
ideas and those in the aforementioned works. The
least significant of them is that the internal
representation selected by us has been a type of
case grammar, while in those works the sentences
are mapped into Schank&apos;s conceptual dependency
notation. Due to the declarative nature of the
texts we have studied, we have not seen a need for
a deeper representation of the action verbs. The
most important difference lies in the incorporation
in our model of Kant&apos;s distinction between concepts
as a system of rules and concepts as an abstract
representation (an epistemic notion that is absent
in Schank and his collobarators&apos; work). The in-
clusion of this distinction in our model makes the
role and the organization of the different compo-
nents that form part of comprehension differ
markedly from those in the aforementioned works.
</bodyText>
<listItem confidence="0.9570785">
4. Organization and Communication between
the System Components
</listItem>
<bodyText confidence="0.9974005">
The organization that we have proposed appears
in Fig. 1. Central to the organization are the
conceptual specialists. The other components are
subordinated to them.
</bodyText>
<page confidence="0.992089">
37
</page>
<figure confidence="0.982726666666667">
ACTIVE FRAMES
SPECIALISTS
PASSIVE FRAMES
</figure>
<figureCaption confidence="0.999841">
Figure 1 System Organization
</figureCaption>
<bodyText confidence="0.999989981481482">
The parser is essentially based on semantic markers
and parses a sentence in to a case frame structure.
The specialists contain contextual knowledge rele-
vant to each gpecific topic. This knowledge is Of
inferential type. What we have termed &amp;quot;passive
frames&amp;quot; contain what the system remembers of a
given topic. At the beginning of the parsing pro-
cess, the active frames contain nothing. At the
end of the process, the meaning of the text will
be recorded in them. Everything in these frames,
including the name of the slots, are built from
scratch by the conceptual specialists.
The communication between these elements is
as follows. When a text is Input to the system,
the parser begins to parse the first sentence. In
the parser there are mechanisms to recognize the
passive frame associated with the text. Once this
is done, mechanisms are set on to check if the most
recent parsed conceptual constituent of the sen-
tence is a relevant concept. This is done simply
by checking if the concept belongs to the list of
relevant concepts in the passive frame. If that is
the case the specialist (concept) override the
parser. What does this exactly mean? It does not
mean that the specialist will help the parser to
produce the segmentation of the sentence, in a way
similar to Winograd&apos;s and Marcus&apos; approaches in
which semantic selections help the syntax component
of the parser to produce the right segmentation of
the sentence. In fact when the specialists take
over the segmentation of the sentence stops. That
is what &amp;quot;overriding lower linguistic processes&amp;quot;
exactly means. The specialist has knowledge to
interpret whatever structure the parser has built
as well as to make sense directly of the remaining
constituents in the rest of the sentence. &amp;quot;To in-
terpret&amp;quot; and &amp;quot;make sense directly&amp;quot; means that the
constituents of the sentence will be mapped direct-
ly into the active frame that the conceptual
specialists are building. However this does not
mean that the parser will be turned off. The par-
ser continues functioning, not in order to continue
with the segmentation of the sentence but to return
the remaining of the conceptual constituents of the
sentence to the specialist in control when asked by
it. Thus what we have called &amp;quot;linguistic know-
ledge&amp;quot; has been separated from the high level
&amp;quot;inferential knowledge&amp;quot; that is dependent on the
subject matter of a given topic as well as from
the knowledge that is recalled from a given
situation. These three different cognitive aspects
correspond to what we have called &amp;quot;parser,&amp;quot; &amp;quot;con-
ceptual specialists,&amp;quot; and &amp;quot;passive frames&amp;quot;
respectively.
</bodyText>
<sectionHeader confidence="0.928011" genericHeader="method">
5. The Parser
</sectionHeader>
<bodyText confidence="0.999237918032787">
In this section we explain some of the compo-
nents of the parser so that the reader can follow
the discussion of the examples in the next section.
We refer the reader to Gomez (1981) for a detailed
description of these concepts. Noun Group: The
function that parses the noun group is called
DESCRIPTION. DESCR is a semantic marker used to
mark all words that may form part of a noun group.
An essential component of DESCRIPTION is a mecha-
nism to identify the concept underlying the complex
nominals (cf. Levi, 1978). See Finin (1980) for
a recent work on complex nominals that concen-
trates on concept modification. This is of most
importance because it is characteristic of declar-
ative contexts that the same concept may be
referred to by different complex nominals. For in-
stance, it is not rare to find the following com-
plex nominals in the same programming problem all
of them referring to the same concept: &amp;quot;the
previous balance,&amp;quot; &amp;quot;the starting balance,&amp;quot; &amp;quot;the
old balance&amp;quot; &amp;quot;the balance at the beginning of the
period.&amp;quot; DESCRIPTION will return with the same
token (old-bal) in all of these cases. The reader
may have realized that &amp;quot;the balance at the beginn-
ing of the period&amp;quot; is not a compound noun. They
are related to compound nouns. In fact many com-
pound nouns have been formed by deletion of prepo-
sitions. We have called them prepositional
phrases completing a description, and we have
treated them as complex nominals. Prepositions.:
For each preposition (also for each conjunction)
there is a procedure. The function of these pre-
positional experts (cf. Small, 1980) is to deter-
mine the meaning of the preposition. We refer to
them as FOR-SP, ON-SP, AS-SP, etc.. Descriptive
Verbs: (D-VERBS) are those used to describe. We
have categorized them in four classes. There are
those that describe the constituents of an object.
Among them are: consist of, show, include, be
given ba, contain, etc.. We refer to them as
CONSIST-OF D-VERBS. A second class are those
used to indicate that something is representing
something. Represent, indicate, mean, describe,
etc.. belong to this class. We refer to them as
REPRESENT D-VERBS. A third class are those that
fall under the notion of appear. To this class
belong appear, belong., be given on etc.. We refer
to them as APPEAR D-VERBS. The fourth class are
formed by those that express a spatial relation.
Some of these are: follow, precede, be followed
by any spatial verb. We refer to them as SPATIAL
D-VERBS. Action Verbs: We have used different
semantic features, which indicate different levels
of abstraction, to tag action verbs. Thus we have
used the marker SUPL to mark in the dictionary
&amp;quot;supply&amp;quot;, &amp;quot;provide&amp;quot;, &amp;quot;furnish&amp;quot;, but not &amp;quot;offer&amp;quot;.
From the highest level of abstraction all of them
are tagged with the marker ATRANS. The procedures
that parse the action verbs and the descriptive
verbs are called ACTION-VERB and DESCRIPTIVE-VERB
respectively.
</bodyText>
<sectionHeader confidence="0.591698" genericHeader="method">
6. Recognition of C,
</sectionHeader>
<bodyText confidence="0.881632">
The concepts relevant to a programming topic
are grouped in a passive frame. We distinguish
between those concepts which are relevant to a
PARSER
</bodyText>
<page confidence="0.996692">
38
</page>
<bodyText confidence="0.996843707692308">
specific programming task, like balance to check-
ing-account programs, and those relevant to any
kind of program, like output, input, end-of-data,
etc.. The former can be only recognized when the
programming topic has been identified. A concept
like output will not only be activated by the word
&amp;quot;output&amp;quot; or by a noun group containing that word.
The verb &amp;quot;print&amp;quot; will obviously activate that con-
cept. Any verb that has the feature REQUEST, a
semantic feature associated with such verbs as
&amp;quot;like,&amp;quot; &amp;quot;want,&amp;quot; &amp;quot;need,&amp;quot; etc., will activate also
the concept output. Similarly nominal concepts
like card and verbal concepts like record, a se-
mantic feature for verbs like &amp;quot;record,&amp;quot; &amp;quot;punch,&amp;quot;
etc. are just two examples of concepts that will
activate the input specialist.
The recognition of concepts is as follows:
Each time that a new sentence is going to be read,
a global variable RECOG is initialized to NIL.
Once a nominal or verbal concept in the sentence
has been parsed, the function RECOGNIZE-CONCEPT is
invoked (if the value of RECOG is NIL). This
function checks if the concept that has been parsed
is relevant to the programming task in general or
(if the topic has been identified) is relevant to
the topic of the programming example. If so,
RECOGNIZE-CONCEPT sets RECOG to T and passes con-
trol to the concept that takes control overriding
the parser. Once a concept has been recognized,
the specialist for that concept continues in con-
trol until the entire sentence has been processed.
The relevant concept may be the subject or any
other case of the sentence. However if the rele-
vant concept is in a prepositional phrase that
starts a sentence, the relevant concept will not
take control.
The following data structures are used during
parsing. A global variable, STRUCT, holds the re-
sult of the parsing. STRUCT can be considered as a
STM (short term memory) for the low level linguis-
tic processes. A BLACKBOARD (Erman and Lesser,
1975) is used for communication between the high
level conceptual specialists and the low level
linguistic experts. Because the information in the
blackboard does not go beyond the sentential level,
it may be considered as STM for the high level
sources of knowledge. A global variable WORD holds
the word being examined, and WORDSENSE holds the
semantic features of that word.
7. Example 1
An instructor records the name and five test
scores on a data card for each student. The regis-
trar also supplies data cards containing a student
name, identification number and number of courses
passed.
The parser is invoked by activating SENTENCE.
Because &amp;quot;an&amp;quot; has the marker DESCR, SENTENCE passes
control to DECLARATIVE which handles sentences
starting with a nominal phrase. (There are other
functions that respectively handle sentences start-
ing with a prepositional phrase, an adverbial
clause, a command, an -ing form, and sentences
introduced by &amp;quot;to be&amp;quot; (there be, will be, etc.)
with the meaning of existence.) DECLARATIVE in-
vokes DESCRIPTION. This parses &amp;quot;an instructor&amp;quot; ob-
taining the concept instructor. Before returning
control, DESCRIPTION activates the functions RECOG-
NIZE-TOPIC and RECOGNIZE-CONCEPT. The former
function checks in the dictionary if there is a
frame associated with the concept parsed by
DESCRIPTION. The frame EXAM-SCORES is associated
with instructor, then the variable TOPIC is instan-
tiated to that frame. The recognition of the frame,
which may be a very hard problem, is very simple
in the programming problems we have studied and
normally the first guess happens to be correct.
Next, RECOGNIZE-CONCEPT is invoked. Because
instructor does not belong to the relevant concepts
of the EXAM-SCORES frame, it returns control.
Finally DESCRIPTION returns control to DECLARATIVE,
along with a list containing the semantic features
of instructor. DECLARATIVE, after checking that
the feature TIME does not belong to those features,
inserts SUBJECT before &amp;quot;instructor&amp;quot; in STRUCT. Be-
fore storing the content of WORD, &amp;quot;records,&amp;quot; into
STRUCT, DECLARATIVE invokes RECOGNIZE-CONCEPT to
recognize the verbal concept. All verbs with the
feature record, as we said above, activate the in-
put specialist, called INPUT-SP. When INPUT-SP
is activated, STRUCT looks like (SUBJ (INSTUCTOR)).
As we said in the introduction, the INPUT special-
ist is a collection of production rules. One of
those rules says:
IF the marker RECORD belongs to WORDSENSE
then activate the function ACTION-
VERB and pass the following reco-
mmendations to it: 1)activate the
INPUT-SUPERVISOR each time you find
an object 2) if a RECIPIENT case is
found then if it has the feature HUMAN,
parse and ignore it. Otherwise awaken
the INPUT-SUPERVISOR 3) if a WHERE case
(the object where something is recorded)
is found, awaken the INPUT-SUPERVISOR.
The INPUT-SUPERVISOR is a function that is
controlling the input for each particular problem.
ACTION-VERB parses the first object and passes it
to the INPUT-SUPERVISOR. This checks if the seman-
tic feature IGENERIC (this is a semantic feature
associated with words that refer to generic infor-
mation like &amp;quot;data,&amp;quot; &amp;quot;information,&amp;quot; etc.) does not
belong to the object that has been parsed by
ACTION-VERB. If that is not the case, the INPUT-
SUPERVISOR, after checking in the PASSIVE-FRAME
that name is normally associated with the input
for EXAM-SCORES, inserts it in the CONSIST-OF slot
of input. The INPUT-SUPERVISOR returns control to
ACTION-VERB that parses the next object and the
process explained above is repeated.
When ACTION-VERB finds the preposition &amp;quot;on,&amp;quot;
the routine ON-SP is activated. This, after check-
ing that the main verb of the sentence has been
parsed and that it takes a WHERE case, checks the
BLACKBOARD to find out if there is a recommendation
for it. Because that is the case, ON-SP tells
DESCRIPTION to parse the nominal phrase &amp;quot;on data
cards&amp;quot;. This returns with the concept card. ON-
SP activates the INPUT-SUPERVISOR with card. This
routine, after checking that cards is a type of
input that the solver handles, inserts &amp;quot;card&amp;quot; in
</bodyText>
<page confidence="0.998672">
39
</page>
<bodyText confidence="0.999810269230769">
the INPUT-TYPE slot of input and returns control.
What if the sentence had said &amp;quot;... on a notebook&amp;quot;?
Because notebook is not a ferm of input, the INPUT-
SUPERVISOR would have not inserted &amp;quot;book&amp;quot; into the
INPUT-TYPE slot. Another alternative is to let the
INPUT-SUPERVISOR insert it in the INPUT-TYPE slot
and let the problem solver make sense out of it.
There is an interesting tradeoff between under-
standing and problem solving in these contexts.
The robuster the understander is, the weaker the
solver may be, and vice versa. The prepositional
phrase &amp;quot;for each student&amp;quot; is parsed similarly.
ACTION-VERB returns control to INPUT-SP that in-
serts &amp;quot;instructor&amp;quot; in the SOURCE slot of input.
Finally, it sets the variable QUIT to T to indi-
cate to DECLARATIVE that the sentence has been
parsed and returns control to it. DECLARATIVE
after checking that the variable QUIT has the
value T, returns control to SENTENCE. This resets
the variables RECOG, QUIT and STRUCT to NIL and
begins to examine the next sentence.
The calling sequence for the second sentence
is identical to that for the first sentence except
that the recognition of concepts is different. The
passive frame for EXAM-SCORES does not contain any-
thing about &amp;quot;registrar&amp;quot; nor about &amp;quot;supplies&amp;quot;.
DECLARATIVE has called ACTION-VERB to parse the
verbal phrase. This has invoked DESCRIPTION to
parse the object &amp;quot;data cards&amp;quot;. STRUCT looks like:
(SUBJ (REGISTRAR) ADV (ALSO) AV (SUPPLIES) OBJ ).
ACTION-VERB is waiting for DESCRIPTION to parse
&amp;quot;data cards&amp;quot; to fill the slot of OBJ. DESCRIPTION
comes with card from &amp;quot;data cards,&amp;quot; and invokes
RECOGNIZE-CONCEPT. The specialist INPUT-SP is
connected with card and it is again activated.
This time the production rule that fires says:
If what follows in the sentence is &lt;univer-
sal quatifier&gt; + &lt;D-VERB&gt; or simply
D-VERB then activate the function
DESCRIPTIVE-VERB and pass it the
recommendation of activating the
INPUT-SUPERVISOR each time a complement
is found.
The pattern &lt;universal quantifier&gt; + &lt;D-VERB&gt;
appears in the antecedent of the production rule
because we want the system also to understand:
&amp;quot;data cards each containing...&amp;quot;. The rest of the
sentence is parsed in a similar way to the first
sentence. The INPUT-SUPERVISOR returns control to
INPUT-SP that stacks &amp;quot;registrar&amp;quot; in the source slot
of input. Finally the concept input for this prob-
lem looks:
</bodyText>
<sectionHeader confidence="0.9740426" genericHeader="method">
INPUT CONSIST-OF (NAME (SCORES CARD (5)))
SOURCE (INSTRUCTOR)
(NAME ID-NUMBER P-COURSES)
SOURCE (REGISTRAR)
INPUT-TYPE (CARDS)
</sectionHeader>
<bodyText confidence="0.993549333333334">
If none of the concepts of a sentence are recog-
nized - that is the sentence has been parsed and
the variable RECOG is NIL - the system prints the
sentence followed by a question mark to indicate
that it could not make sense of it. That will
happen if we take a sentence from a problem about
checking-accounts and insert it in the middle bf a
problem about exam scores. The INPUT-SP and the
INPUT-SUPERVISOR are the same specialists. The
former overrides and guides the parser when a con-
cept is initially recognized, the latter plays the
same role after the concept has been recognized.
The following example illustrates how the INPUT-
SUPERVISOR may furthermore override and guide the
parser.
The registrar also provides cards.
Each card contains data including
an identification number ...
When processing the subject of the second sentence,
INPUT-SP is activated. This tells the function
DESCRIPTIVE-VERB to parse starting at &amp;quot;contains
...&amp;quot; and to awaken the INPUT-SUPERVISOR when an
object is parsed. The first object is &amp;quot;data&amp;quot; that
has the marker IGENERIC that tells the INPUT-SUPER-
VISOR that &amp;quot;data&amp;quot; can not be the value for the
input. The INPUT-SUPERVISOR will examine the next
concept looking for a D-VERB. Because that is the
case, it will ask the routine DESCRIPTIVE-VERB to
parse starting at &amp;quot;including an identification
number...&amp;quot;
8. Example 2
We will comment briefly on the first six
sentences of the example in Fig. 2. We will name
each sentence by quoting its beginning and its end.
There is a specialist that has grouped the know-
ledge about checking-accounts. This specialist,
whose name is ACCOUNT-SP, will be invoked when the
parser finds a concept that belongs to the slot of
relevant concepts in the passive frame. The first
sentence is: &amp;quot;A bank would like to produce...
checking accounts&amp;quot;. The OUTPUT-SP is activated by
&amp;quot;like&amp;quot;. When OUTPUT-SP is activated by a verb with
the feature of REQUEST, there are only two produc-
tion rules that follow. One that considers that
the next concept is an action verb, and another
that looks for the pattern &lt;REPORT + CONSIST
D-VERB&gt; (where &amp;quot;REPORT&amp;quot; is a semantic feature for
&amp;quot;report,&amp;quot; &amp;quot;list,&amp;quot; etc.). In this case, the first
rule is fired. Then ACTION-VERB is activated with
the recommendation of invoking the OUTPUT-SUPERVI-
SOR each time that an object is parsed. ACTION-
VERB awakens the OUTPUT-SUPERVISOR with (RECORDS
ABOUT (TRANSACTION)). Because &amp;quot;record&amp;quot; has the
feature IGENERIC the OUTPUT-SUPERVISOR tries to
redirect the parser by looking for a CONSIST
D-VERB. Because the next concept is not a D-VERB,
OUTPUT-SUPERVISOR sets RECOG to NIL and returns
control to ACTION-VERB. This parses the adverbial
phrase introduced by &amp;quot;during&amp;quot; and the prepositional
phrase introduced by &amp;quot;with&amp;quot;. ACTION-VERB parses
the entire sentence without recognizing any rele-
vant concept, except the identification of the
frame that was done while processing &amp;quot;a bank&amp;quot;.
The second sentence &amp;quot;For each account the bank
wants ... balance.&amp;quot; is parsed in the following
way. Although &amp;quot;account&amp;quot; belongs to slot of rele-
vant concepts for this problem, it is skipped be-
cause it is in a prepositional phrase that starts
a sentence. The OUTPUT-SP is activated by a
</bodyText>
<page confidence="0.9962">
40
</page>
<bodyText confidence="0.995961093750001">
REQUEST type verb, &amp;quot;want&amp;quot;. STRUCT looks like:
(RECIPIENT (ACCOUNT UQ (EACH)) SUBJECT (BANK)).
The production rule whose antecedent is &lt;RECORD +
CONSIST D-VERB&gt; is fired. The DESCRIPTIVE-VERB
function is asked to parse starting in &amp;quot;showing,&amp;quot;
and activate the OUTPUT-SUPERVISOR each time an
object is parsed. The OUTPUT-SUPERVISOR inserts
all objects in the CONSIST-OF slot of output, and
returns control to the OUTPUT-SP that inserts the
RECIPIENT, &amp;quot;account,&amp;quot; in the CONSIST-OF slot of
output and returns control.
The next sentence is &amp;quot;The accounts and trans-
actions ... as follows:&amp;quot; DECLARATIVE asks
DESCRIPTION to parse the subject. Because account
belongs to the relevant concepts of the passive
frame, the ACCOUNT-SP specialist is invoked. There
is nothing in STRUCT. When a topic specialist is
invoked and the next word is a boolean conjunction,
the specialist asks DESCRIPTION to get the next
concept for it. If the concept does not belong to
the list of relevant concepts, the specialist sets
RECOG to NIL and returns control. Otherwise it
continues examining the sentence. Because trans-
action belongs to the slot of relevant concepts of
the passive frame, ACCOUNT-SP continues in control.
ACCOUNT-SP finds &amp;quot;for&amp;quot; and asks DESCRIPTION to
parse the nominal phrase. ACCOUNT-SP ignores
anything that has the marker HUMAN or TIME.
Finally ACCOUNT-SP finds the verb, an APPEAR D-VERB
and invokes the DESCRIPTIVE-VERB routine with the
recommendation of invoking the ACCOUNT-SUPERVISOR
each time a complement is found. The ACCOUNT-
SUPERVISOR is awakened with card. This inserts
&amp;quot;card&amp;quot; in the INPUT-TYPE slot of account and
transaction and returns control to the DESCRIPTIVE-
VERB routine. AS-SP (the routine for &amp;quot;as&amp;quot;) is
invoked next. This, after finding &amp;quot;follows&amp;quot;
followed by &amp;quot;:,&amp;quot; indicate to DESCRIPTIVE-VERB that
the sentence has been parsed. ACCOUNT-SP returns
control to DECLARATIVE and this, after checking
that QUIT has the value T, returns control to
SENTENCE.
The next sentence is: &amp;quot;First will be a
sequence of cards ... accounts.&amp;quot; The INPUT-SP
specialist is invoked. STRUCT looks like: (ADV
(FIRST) EXIST ). &amp;quot;Sequence of cards&amp;quot; gives the
concept card activating the INPUT-SP specialist.
The next concept is a REPRESENT D-VERB. INPUT-SP
activates the DESCRIPTIVE-VERB routine and asks it
to activate the INPUT-SUPERVISOR each time an
object is found. The INPUT-SUPERVISOR checks if
the object belongs to the relevant concepts for
checking accounts. If not, the ACCOUNT-SUPERVISOR
will complain. That will be the case if the sen-
tence is: &amp;quot;First will be a sequence of cards
describing the students&amp;quot;. Assume that the above
sentence says: &amp;quot;First will be a sequence of cards
consisting of an account number and the old
balance.&amp;quot; In that case, the INPUT-SP will activate
also the INPUT-SUPERVISOR but because the verbal
concept is a CONSIST D-VERB, the INPUT-SUPERVISOR
will stack the complements in the slot for INPUT.
Thus, what the supervisor specialists do depend
on the verbal concept and what is coming after.
The next sentence is: &amp;quot;Each account is
described by ..., in dollars and cents.&amp;quot; Again,
the ACCOUNT-SP is activated. The next concept is
a CONSIST D-VERB. ACCOUNT-SP assumes that it is
the input for accounts and activates the
DESCRIPTIVE-VERB function, and passes to it the
recommendation of activating the INPUT-SUPERVISOR
each time an object is parsed. The INPUT-SUPERVI-
SOR is awakened with (NUMBERS CARDINAL (2)). Be-
cause number is not an individual concept (like,
say, 0 is) the INPUT-SUPERVISOR reexamines the sen-
tence and finds &amp;quot;:,&amp;quot; it then again asks to
DESCRIPTIVE-VERB to parse starting at &amp;quot;the account
number...&amp;quot;. The INPUT-SUPERVISOR stacks the com-
plements in the input slot of the concept that is
being described: account.
The next sentence is: &amp;quot;The last account is
followed by ... to indicate the end of the list.&amp;quot;
The ACCOUNT-SP is invoked again. The following
production rule is fired: If the ordinal &amp;quot;last&amp;quot;
is modifying &amp;quot;account&amp;quot; and the next concept is a
SPATIAL D-VERB then activate the END-OF-DATA
specialist. This assumes control and asks
DESCRIPTIVE-VERB to parse starting at &amp;quot;followed by&amp;quot;
with the usual recommendation of awakening the END-
OF-DATA supervisor when a complement is found, and
the recommendation of ignoring a PURPOSE clause if
the concept is end-of-list or end-of-account. The
END-OF-DATA is awakened with &amp;quot;dummy-account&amp;quot;.
Because &amp;quot;dummy-account&amp;quot; is not an individual con-
cept, the END-OF-DATA supervisor reexamines the
sentence expecting that the next concept is a
CONSIST D-VERB. It finds it, and redirects the
parser by asking the DESCRIPTIVE-VERB to parse
starting in &amp;quot;consisting of two zero values.&amp;quot; The
END-OF-DATA is awakened with &amp;quot;(ZERO CARD (2))&amp;quot;.
Because this time the object is an individual
concept, the END-OF-DATA supervisor inserts it in-
to the END-OF-DATA slot of the concept being des-
cribed: account.
9. Conclusion
LLULL was running in the Dec 20/20 under UCI
Lisp in the Department of Computer Science of the
Ohio State University. It has been able to under-
stand ten programming problems taken verbatim from
text books. A representative example can be found
in Fig. 2. After the necessary modifications, the
system is presently running in a VAX11/780 under
Franz Lisp. We are now in the planning stage of
extensively experimenting with the system. We
predict that the organization that we have proposed
will make relatively simple to add new problem
areas. Assume that we want LLULL to understand
programming problems about roman numerals, say.
We are going to find uses of verbs, prepositions,
etc. that our parser will not be able to handle.
We will integrate those uses in the parser. On
top of that we will build some conceptual special-
ists that will have inferential knowledge about
roman numerals, and a thematic frame that will hold
structural knowledge about roman numerals. We are
presently following this scheme in the extension of
LLULL. In the next few months we expect to fully
evaluate our ideas.
</bodyText>
<page confidence="0.892842">
10. A Computer Run
41
</page>
<bodyText confidence="0.999618470588235">
The example below has been taken verbatim
from Conway and Gries (1975). Some notes about
the output for this problem are in order.
1) &amp;quot;SPEC&amp;quot; is a semantic feature that stands for
specification. If it follows a concept,- it means
that the concept is being further specified or
described. The semantic feature &amp;quot;SPEC&amp;quot; is followed
by a descriptive verb or adjective, and finally it
comes the complement of the specification in paren-
theses. In the only instance in which the descrip-
tive predicate does not follow the word SPEC is in
expressions like &amp;quot;the old balance in dollars and
cents&amp;quot;. Those expressions have been treated as a
special construction. 2) All direct objects
connected by the conjunction &amp;quot;or&amp;quot; appear enclosed
in parentheses. 3) &amp;quot;REPRESENT&amp;quot; is a semantic
marker and stands for a REPRESENT D-VERB.
</bodyText>
<sectionHeader confidence="0.966475739130435" genericHeader="method">
4) Finally &amp;quot;(ZERO CARD (3))&amp;quot; means three zeros.
CA BANK WOULD LIKE TO PRODUCE RECORDS OF THE
TRANSACTIONS DURING AN ACCOUNTING PERIOD IN
CONNECTION WITH THEIR CHECKING ACCOUNTS. FOR EACH
ACCOUNT THE BANK WANTS A LIST SHOWING THE BALANCE
AT THE BEGINNING OF THE PERIOD, THE NUMBER OF
DEPOSITS AND WITHDRAWALS, AND THE FINAL BALANCE.
THE ACCOUNTS AND TRANSACTIONS FOR AN ACCOUNTING
PERIOD WILL BE GIVEN ON PUNCHED CARDS AS FOLLOWS:
FIRST WILL BE A SEQUENCE OF CARDS DESCRIBING THE
ACCOUNTS. EACH ACCOUNT IS DESCRIBED BY TWO NUM-
BERS: THE ACCOUNT NUMBER (GREATER THAN 0), AND
THE ACCOUNT BALANCE AT THE BEGINNING OF THE PERIOD,
IN DOLLARS AND CENTS. THE LAST ACCOUNT IS FOLLOWED
BY A DUMMY ACCOUNT CONSISTING OF TWO ZERO VALUES
TO INDICATE THE END OF THE LIST. THERE WILL BE AT
MOST 200 ACCOUNTS. FOLLOWING THE ACCOUNTS ARE THE
TRANSACTIONS. EACH TRANSACTION IS GIVEN BY THREE
NUMBERS: THE ACCOUNT NUMBER, A 1 OR -1 (INDICATING
A DEPOSIT OR WITHDRAWAL, RESPECTIVELY), AND THE
TRANSACTION AMOUNT, IN DOLLARS AND CENTS. THE LAST
REAL TRANSACTION IS FOLLOWED BY A DUMMY TRANSACTION
CONSISTING OF THREE ZERO VALUES.)
</sectionHeader>
<figureCaption confidence="0.983467">
Figure 2 A Programming Problem
</figureCaption>
<sectionHeader confidence="0.991022846153846" genericHeader="method">
OUTPUT CONSIST-OF (ACCOUNT OLD -BAL DEPOSITS
WITHDRAWALS FINAL -BAL)
ACCOUNT INPUT (ACCOUNT-NUMBER SPEC GREATER (0)
OLD -BAL SPEC (DOLLAR-CENT))
INPUT-TYPE (CARDS)
END-OF-DATA ((ZERO CARD (2)))
NUMBER-OF-ACCOUNTS (200)
TRANSACTION INPUT (ACCOUNT-NUMBER (1 OR -1)
REPRESENT
(DEPOSIT OR WITHDRAWAL)
TRANS-AMOUNT SPEC (DOLLAR-CENT))
INPUT-TYPE (CARDS)
END-OF-DATA ((ZERO CARD (3)))
</sectionHeader>
<figureCaption confidence="0.525221">
Figure 3 System Output for Problem in Figure 2
</figureCaption>
<reference confidence="0.9950256">
Ortony, .m.imembering, Understanding, and Repre-
sentation. Cognitive Science, v. 2, n. 1, 1978.
Perrault, R. and Allen F. A Plan-Based Analysis of
Indirect Speech Acts. American Journal of
Computational Linguistics, v. 6, n. 3, 1980.
</reference>
<sectionHeader confidence="0.898066" genericHeader="acknowledgments">
ACKNOWLEDGEMENTS
</sectionHeader>
<bodyText confidence="0.999622111111111">
This research was supported by the Air Force
Office of Scientific Research under contract
F49620-79-0152, and was done in part while the
author was a member of the Al group at the Ohio
State University.
I would like to thank Amar Mukhopadhyay for
reading and providing constructive comments on
drafts of this paper, and Mrs. Robin Cone for her
wonderful work in typing it.
</bodyText>
<sectionHeader confidence="0.959639" genericHeader="references">
REFERENCES
</sectionHeader>
<bodyText confidence="0.8130540625">
Bruce, B. and Newman D. Interacting Plans. Cogni-
tive Science. v. 2, 1978.
Cohen, P. and Perrault R. Elements of a Plan-Based
Theory of Speech Acts. Cognitive Science, v. 3,
n. 3, 1979.
Conway, R. and Gries, D. An Introduction to Pro-
gramming. Winthrop Publishers, Inc., Massachu-
setts, 1975.
DeJong, G. Prediction and Substantiation: A New
Approach to Natural Language Processing. Cogni-
tive Science, v. 3, n. 3, 1979.
Erman, D. and Lesser V. A Multi-Level Organization
for Problem-Solving Using Many Diverse Coopera-
ting Sources of Knowledge. IJCAI-75, University
Microfilms International, PO BOX 1467, Ann
Arbor, Michigan 48106, 1975.
</bodyText>
<figureCaption confidence="0.93549525">
Finin, T. The Semantic Interpretation of Compound
Nominals. Report T-96, Dept. of Computer
Science, University of Illinois, 1980.
Gomez, F. Understanding Programming Problems
Stated in Natural Language. OSU-CISR-TR-81,
Dept. of Computer Science, The Ohio State
University, 1981.
Grosz, B. The Representation and Use of Focus in
Dialogue Understanding. SRI Technical Note 151,
Menlo Park, Ca., 1977.
Hobbs, J. and Evans D. Conversation as Planned
Behavior. Cognitive Science. v.4, no. 4, 1980.
Levi, J. N. The Syntax and Semantics of Complex
Nominals. Academic Press, 1978.
Marcus, M. A Theory of Syntantic Recognition for
Natural Language. MIT Press, 1979.
</figureCaption>
<page confidence="0.996563">
42
</page>
<reference confidence="0.999808333333333">
Riesbeck, C. K. Conceptual Analysis. In R. Schank_
(Ed.), Conceptual Information Processing. N.
York, Elvesier-North Holland, 1975.
Schank, R. and Abelson, R. Scripts, Plans, Goals,
and Understanding. Laurence Erlbaum Associates,
Hillsdale N. J., 1977.
Schank, R. C., Lebowitz, M., and Lawrence, B.
Parsing Directly in Knowledge Structures.
in IJCAI-79, Computer Science Department,
Stanford University, Stanford, CA 94305.
Small, S. Word Expert Parsing: A Theory of Dis-
tributed Word-Based Natural Language Under-
standing. Tech. Report 954, Dept. of Computer
Science, University of Maryland, 1980.
Wilks, Y. An Artificial Intelligence Approach
to Machine Translation. In Schank and Colby
(eds.) Computer Models of Thought and
Language. San Francisco, W. H. Freeman and
Co., San Francisco, 1973.
Wilensky, R. Understanding Goal-Based Stories.
Dept. of Computer Science, Yale University.
Tech. Report 140, 1978.
Winograd, T. Understanding Natural Language. N.
York, Academic Press, 1972.
</reference>
<page confidence="0.999833">
43
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.050001">
<title confidence="0.999567">TOWARDS A THEORY OF COMPREHENSION OF DECLARATIVE CONTEXTS</title>
<author confidence="0.999995">Fernando Gomez</author>
<affiliation confidence="0.999587">Department of Computer Science University of Central Florida</affiliation>
<address confidence="0.997801">Orlando, Florida 32816</address>
<abstract confidence="0.985234337155963">An outline of a theory of comprehension of declarative contexts is presented. The main aspect of the theory being developed is based on Kant&apos;s distinction between concepts as rules (we have called them conceptual specialists) and concepts as an abstract representation (schemata, frames). Comprehension is viewed as a process dependent on the conceptual specialists (they contain the inferential knowledge), the schemata or frames (they contain the declarative knowledge), and a parser. The function of the parser is to produce a segmentation of the sentences in a case frame structure, thus determininig the meaning of prepositions, polysemous verbs, noun group etc. The function of this parser is not to produce an output to be interpreted by semantic routines or an interpreter, but to start the parsing process and proceed until a concept relevant to the theme of the text is recognized. Then the concept takes control of the process overridingthe lower level linguistic process. Hence comprehension is viewed as a process in which high level sources of know- (concepts) overridelower level linguistic processes. This paper deals with a theory of computer comprehension of descriptive contexts. By &amp;quot;descriptive contexts&amp;quot; I refer to the language of scientific books, text books, this text, etc.. In the distinction performative vs. declarative, descriptive texts clearly fall in the declarative side. Recent work in natural language has dealt with contexts in which the computer understanding depends on the meaning of the action verbs and the human actions (plans, intentions, goals) indicated by them (Schank and Abelson 1977; Grosz 1977; Wilensky 1978; Bruce and Newman 1978). Also a , considerable amount of work has been done in a plan-based theory of task oriented dialogues (Cohen and Perrault 1979; Perrault and Allen 1980; Hobbs and Evans 1980). This work has had very little bearing on a theory of ...velputer understanding of descriptive contexts. One of the main tenets of the proposed research is that descriptive (or declarative as we prefer to call them) contexts call for different theoretical ideas compared to those proposed for the understanding of human actions, although, naturally there are aspects that are common. An important characteristic of these contexts is the predominance of descriptive predicates and verbs (verbs such as &amp;quot;contain,&amp;quot; &amp;quot;refer,&amp;quot; &amp;quot;consist of,&amp;quot; etc.) over action verbs. A direct result of this is that the meaning of the sentence does not depend as much on the main verb of the sentence as on the concepts that make it up. Hence meaning representations centered in the main verb of the sentence are futile for these contexts. We have approached the problem of comprehension in these contexts by considering concepts both as active agents that recognize themselves and as an abstract representation of the properties of an object. This aspect of the theory being developed is based on Kant&apos;s distinction between concepts as rules (we have called them conceptual specialists) and concepts as an abstract representation (frames, schemata). Comprehension is viewed as a process dependent on the conceptual specialists (they contain the inferential knowledge), the schemata (they contain structural knowledge), and a parser. The function of the parser is to produce a segmentation of the sentences in a case frame structure, thus determining the meaning of prepositions, polysemous verbs, noun group, etc.. But the function of this parser is not to produce an output to be interpreted by semantic routines, but to start the parsing process and to proceed until a concept relevant to the theme of the text is recognized. Then the concept (a cluster of production rules) takes control of the comprehension process overriding the lower level linguistic processes. The concept continues supervising and guiding the parsing until the sentence has been understood, that is, the meaning of the sentence has been mapped into the final internal representation. Thus a text is parsed directly into the final knowledge structures. Hence comprehension is viewed as a process in which level sources of knowledge (concepts) lower level linguistic processes. We have used these ideas to build a system, called LLULL, to understand programming problems taken verbatim from introductory books on programming. Schemataand Kant&apos;s Critiqueof Pure Reasonone may find views of a concept.According to one view, a concept is a system of rules governing the application of a predicate to an object. The rule that 36 tells us whether the predicate &amp;quot;large&amp;quot; applies to concept Canadais a such rule. The system of rules that allows us to recognize any given of the concept Canadaconstitutes our concept of Canada. According to a second view, Kant considers a concept as an abstract represen- (vorstellung)of the properties of an object. This second view of a concept is akin to notion of conceptused in such knowledge representation languages as FRL, KLONE and KRL. Frames have played dual functions. They have been used as a way to organize the inferences, and also as a structural representation of what is remembered of a given situation. This has caused confusion between two different cognitive aspects: memory and comprehension (see Ortony, 1978). We think that one of the reasons for this confusion is due to the failure in distinguishing between the two types of concepts (concepts as rules and concepts as a structural representation). We have based our analysis on Kant&apos;s distinction in order to separate clearly between the organization of the inferences and the memory aspect. For any given text, a thematic frame contains structural knowledge about what is remembered of a theme. One of the slots in this frame contains a list of the relevant concepts for that theme. Each of these concepts in this list is separately organized as a cluster of production rules. They contain the inferential knowledge that allows the system to interpret the information being presently processed, to anticipate incoming information, and to guide and supervise the parser (see below). In some instances, the conceptual specialists access the knowledge stored in the thematic frame to perform some of these actions. Knowledge, Text Understanding In text understanding, there are two distinct issues. One has to do with the mapping of individual sentences into some internal representation (syntactic markers, some type of case grammar, Wilks&apos; preference semantics, Schank&apos;s conceptual dependency etc.). In designing this mapping, several approaches have been taken. In Winograd (1972) and Marcus (1979), there is an interplay between syntax, and semantic markers (in that order), while in Wilks (1973) and Riesbeck (1975) the parser rely almost exclusively on semantic categories. A separate issue has to do with the meaning of the internal representation in relation to the understanding of the text. For instance, consider following text to the second example): bank would like to produce of the transactions during an accounting period in connection with their checking accounts. For each account the bank wants a list showing the balance at the beginning of tie period, the number of deposits and withdrawals, and the final balance.&amp;quot; Assume that we parse these sentences into our favorite internal representation. Now what we do with the internal representation? It is still far distant from its textual meaning. In fact, the first sentence is only introducing the topic of he programming problem. The writer could have achieved the same effect by saying: &amp;quot;The following is a checking account problem&amp;quot;. The textual meaning of the second sentence is the description of the output for that problem. The writer could have achieved the same effect by saying that the output for the problem consists of the old-balance, deposits, withdrawals, etc.. One way to produce the textual meaning of the sentence is to interpret the internal representation that has already been built. Of course, that is equivalent to reparsing the sentence. Another way is to map the sentence directly into the final representation or the textual meaning of the sentence. That is the approach we have taken. DeJong (1979) and Schenk (1979) are two recent works that move in that direction. DeJong&apos;s system, called FRUMP, is a strong form of top down parser. It skims the looking for those concepts in which is When it finds all it ignores the remainder of the text. In analogy to key-word parsers, we may describe FRUMP as a key-concept parser. In Schank et al. (1979), words are marked in the dictionary as skippable or as having high relevance for a given script. When a relevant word is found, some questions are formulated as requests These requests guide the parser in the understanding of the story. In our opinion, the criteria by which words are marked as skippable or relevant are not clear. There are significant differences between our and those in the aforementioned works. least significant of them is that the internal representation selected by us has been a type of grammar, while in those works sentences are mapped into Schank&apos;s conceptual dependency notation. Due to the declarative nature of the have studied, we have not seen a need for deeper representation of action verbs. The most important difference lies in the incorporation in our model of Kant&apos;s distinction between concepts as a system of rules and concepts as an abstract representation (an epistemic notion that is absent Schank and his collobarators&apos; The inthis distinction in model makes the role and the organization of the different components that form part of comprehension differ markedly from those in the aforementioned works. Organizationand between Components The organization that we have proposed appears Fig. to the organization are the conceptual specialists. The other components are subordinated to them. 37 ACTIVE FRAMES SPECIALISTS PASSIVE FRAMES Figure 1 System Organization The parser is essentially based on semantic markers and parses a sentence in to a case frame structure. The specialists contain contextual knowledge relevant to each gpecific topic. This knowledge is Of inferential type. What we have termed &amp;quot;passive frames&amp;quot; contain what the system remembers of a given topic. At the beginning of the parsing process, the active frames contain nothing. At the end of the process, the meaning of the text will be recorded in them. Everything in these frames, including the name of the slots, are built from scratch by the conceptual specialists. The communication between these elements is as follows. When a text is Input to the system, the parser begins to parse the first sentence. In the parser there are mechanisms to recognize the passive frame associated with the text. Once this is done, mechanisms are set on to check if the most recent parsed conceptual constituent of the sentence is a relevant concept. This is done simply by checking if the concept belongs to the list of relevant concepts in the passive frame. If that is case the specialist (concept) overridethe parser. What does this exactly mean? It does not mean that the specialist will help the parser to produce the segmentation of the sentence, in a way similar to Winograd&apos;s and Marcus&apos; approaches in which semantic selections help the syntax component of the parser to produce the right segmentation of the sentence. In fact when the specialists take over the segmentation of the sentence stops. That is what &amp;quot;overriding lower linguistic processes&amp;quot; exactly means. The specialist has knowledge to interpret whatever structure the parser has built as well as to make sense directly of the remaining constituents in the rest of the sentence. &amp;quot;To interpret&amp;quot; and &amp;quot;make sense directly&amp;quot; means that the constituents of the sentence will be mapped directly into the active frame that the conceptual specialists are building. However this does not mean that the parser will be turned off. The parser continues functioning, not in order to continue with the segmentation of the sentence but to return the remaining of the conceptual constituents of the sentence to the specialist in control when asked by it. Thus what we have called &amp;quot;linguistic knowledge&amp;quot; has been separated from the high level &amp;quot;inferential knowledge&amp;quot; that is dependent on the subject matter of a given topic as well as from the knowledge that is recalled from a given situation. These three different cognitive aspects correspond to what we have called &amp;quot;parser,&amp;quot; &amp;quot;conceptual specialists,&amp;quot; and &amp;quot;passive frames&amp;quot; respectively. Parser In this section we explain some of the components of the parser so that the reader can follow the discussion of the examples in the next section. We refer the reader to Gomez (1981) for a detailed of these concepts. Group:The function that parses the noun group is called DESCRIPTION. DESCR is a semantic marker used to mark all words that may form part of a noun group. An essential component of DESCRIPTION is a mechanism to identify the concept underlying the complex nominals (cf. Levi, 1978). See Finin (1980) for a recent work on complex nominals that concentrates on concept modification. This is of most importance because it is characteristic of declarative contexts that the same concept may be referred to by different complex nominals. For instance, it is not rare to find the following complex nominals in the same programming problem all of them referring to the same concept: &amp;quot;the previous balance,&amp;quot; &amp;quot;the starting balance,&amp;quot; &amp;quot;the old balance&amp;quot; &amp;quot;the balance at the beginning of the period.&amp;quot; DESCRIPTION will return with the same token (old-bal) in all of these cases. The reader may have realized that &amp;quot;the balance at the beginning of the period&amp;quot; is not a compound noun. They are related to compound nouns. In fact many compound nouns have been formed by deletion of prepositions. We have called them prepositional phrases completing a description, and we have them as complex nominals. For each preposition (also for each conjunction) there is a procedure. The function of these prepositional experts (cf. Small, 1980) is to determine the meaning of the preposition. We refer to as FOR-SP, ON-SP, AS-SP, etc.. Verbs:(D-VERBS) are those used to describe. We have categorized them in four classes. There are those that describe the constituents of an object. them are: consistof, show, include,be ba, contain,etc.. We refer to them as CONSIST-OF D-VERBS. A second class are those used to indicate that something is representing indicate,mean, describe, etc.. belong to this class. We refer to them as REPRESENT D-VERBS. A third class are those that under the notion of appear.To this class be givenon etc.. We refer to them as APPEAR D-VERBS. The fourth class are formed by those that express a spatial relation. of these are: precede,be followed by any spatial verb. We refer to them as SPATIAL Verbs:We have used different semantic features, which indicate different levels of abstraction, to tag action verbs. Thus we have used the marker SUPL to mark in the dictionary &amp;quot;provide&amp;quot;, &amp;quot;furnish&amp;quot;, but not &amp;quot;offer&amp;quot;. From the highest level of abstraction all of them are tagged with the marker ATRANS. The procedures that parse the action verbs and the descriptive verbs are called ACTION-VERB and DESCRIPTIVE-VERB respectively. Recognitionof The concepts relevant to a programming topic are grouped in a passive frame. We distinguish between those concepts which are relevant to a PARSER 38 programming task, like balanceto checking-account programs, and those relevant to any of program, like input, end-of-data, etc.. The former can be only recognized when the programming topic has been identified. A concept outputwill not only be activated by the word &amp;quot;output&amp;quot; or by a noun group containing that word. The verb &amp;quot;print&amp;quot; will obviously activate that concept. Any verb that has the feature REQUEST, a semantic feature associated with such verbs as &amp;quot;like,&amp;quot; &amp;quot;want,&amp;quot; &amp;quot;need,&amp;quot; etc., will activate also concept output.Similarly nominal concepts card and verbal concepts like record,a semantic feature for verbs like &amp;quot;record,&amp;quot; &amp;quot;punch,&amp;quot; etc. are just two examples of concepts that will activate the input specialist. The recognition of concepts is as follows: Each time that a new sentence is going to be read, a global variable RECOG is initialized to NIL. Once a nominal or verbal concept in the sentence has been parsed, the function RECOGNIZE-CONCEPT is invoked (if the value of RECOG is NIL). This function checks if the concept that has been parsed is relevant to the programming task in general or (if the topic has been identified) is relevant to the topic of the programming example. If so, RECOGNIZE-CONCEPT sets RECOG to T and passes control to the concept that takes control overriding the parser. Once a concept has been recognized, the specialist for that concept continues in control until the entire sentence has been processed. The relevant concept may be the subject or any other case of the sentence. However if the relevant concept is in a prepositional phrase that starts a sentence, the relevant concept will not take control. The following data structures are used during parsing. A global variable, STRUCT, holds the result of the parsing. STRUCT can be considered as a STM (short term memory) for the low level linguistic processes. A BLACKBOARD (Erman and Lesser, 1975) is used for communication between the high level conceptual specialists and the low level linguistic experts. Because the information in the blackboard does not go beyond the sentential level, it may be considered as STM for the high level sources of knowledge. A global variable WORD holds the word being examined, and WORDSENSE holds the semantic features of that word. 1 An instructor records the name and five test scores on a data card for each student. The registrar also supplies data cards containing a student name, identification number and number of courses passed. The parser is invoked by activating SENTENCE. Because &amp;quot;an&amp;quot; has the marker DESCR, SENTENCE passes control to DECLARATIVE which handles sentences starting with a nominal phrase. (There are other functions that respectively handle sentences starting with a prepositional phrase, an adverbial clause, a command, an -ing form, and sentences introduced by &amp;quot;to be&amp;quot; (there be, will be, etc.) with the meaning of existence.) DECLARATIVE invokes DESCRIPTION. This parses &amp;quot;an instructor&amp;quot; obthe concept instructor.Before returning control, DESCRIPTION activates the functions RECOG- NIZE-TOPIC and RECOGNIZE-CONCEPT. The former function checks in the dictionary if there is a frame associated with the concept parsed by DESCRIPTION. The frame EXAM-SCORES is associated instructor,then the variable TOPIC is instantiated to that frame. The recognition of the frame, which may be a very hard problem, is very simple in the programming problems we have studied and normally the first guess happens to be correct. Next, RECOGNIZE-CONCEPT is invoked. Because instructordoes not belong to the relevant concepts of the EXAM-SCORES frame, it returns control. Finally DESCRIPTION returns control to DECLARATIVE, along with a list containing the semantic features instructor.DECLARATIVE, after checking that the feature TIME does not belong to those features, inserts SUBJECT before &amp;quot;instructor&amp;quot; in STRUCT. Before storing the content of WORD, &amp;quot;records,&amp;quot; into STRUCT, DECLARATIVE invokes RECOGNIZE-CONCEPT to recognize the verbal concept. All verbs with the record,as we said above, activate the input specialist, called INPUT-SP. When INPUT-SP is activated, STRUCT looks like (SUBJ (INSTUCTOR)).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>m imembering Ortony</author>
<author>Understanding</author>
<author>Representation</author>
</authors>
<date>1978</date>
<journal>Cognitive Science,</journal>
<volume>2</volume>
<marker>Ortony, Understanding, Representation, 1978</marker>
<rawString>Ortony, .m.imembering, Understanding, and Representation. Cognitive Science, v. 2, n. 1, 1978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Perrault</author>
<author>F Allen</author>
</authors>
<title>A Plan-Based Analysis of Indirect Speech Acts.</title>
<date>1980</date>
<journal>American Journal of Computational Linguistics,</journal>
<volume>6</volume>
<contexts>
<context position="2035" citStr="Perrault and Allen 1980" startWordPosition="311" endWordPosition="314">tive contexts&amp;quot; I refer to the language of scientific books, text books, this text, etc.. In the distinction performative vs. declarative, descriptive texts clearly fall in the declarative side. Recent work in natural language has dealt with contexts in which the computer understanding depends on the meaning of the action verbs and the human actions (plans, intentions, goals) indicated by them (Schank and Abelson 1977; Grosz 1977; Wilensky 1978; Bruce and Newman 1978). Also a , considerable amount of work has been done in a plan-based theory of task oriented dialogues (Cohen and Perrault 1979; Perrault and Allen 1980; Hobbs and Evans 1980). This work has had very little bearing on a theory of ...velputer understanding of descriptive contexts. One of the main tenets of the proposed research is that descriptive (or declarative as we prefer to call them) contexts call for different theoretical ideas compared to those proposed for the understanding of human actions, although, naturally there are aspects that are common. An important characteristic of these contexts is the predominance of descriptive predicates and verbs (verbs such as &amp;quot;contain,&amp;quot; &amp;quot;refer,&amp;quot; &amp;quot;consist of,&amp;quot; etc.) over action verbs. A direct result </context>
</contexts>
<marker>Perrault, Allen, 1980</marker>
<rawString>Perrault, R. and Allen F. A Plan-Based Analysis of Indirect Speech Acts. American Journal of Computational Linguistics, v. 6, n. 3, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C K Riesbeck</author>
</authors>
<title>Conceptual Analysis. In</title>
<date>1975</date>
<booktitle>Conceptual Information Processing. N.</booktitle>
<editor>R. Schank_ (Ed.),</editor>
<location>York, Elvesier-North Holland,</location>
<contexts>
<context position="7031" citStr="Riesbeck (1975)" startWordPosition="1116" endWordPosition="1117">access the knowledge stored in the thematic frame to perform some of these actions. 3. Linguistic Knowledge, Text Understanding and Parsing In text understanding, there are two distinct issues. One has to do with the mapping of individual sentences into some internal representation (syntactic markers, some type of case grammar, Wilks&apos; preference semantics, Schank&apos;s conceptual dependency etc.). In designing this mapping, several approaches have been taken. In Winograd (1972) and Marcus (1979), there is an interplay between syntax, and semantic markers (in that order), while in Wilks (1973) and Riesbeck (1975) the parser rely almost exclusively on semantic categories. A separate issue has to do with the meaning of the internal representation in relation to the understanding of the text. For instance, consider the following text (it belongs to the second example): &amp;quot;A bank would like to produce records of the transactions during an accounting period in connection with their checking accounts. For each account the bank wants a list showing the balance at the beginning of tie period, the number of deposits and withdrawals, and the final balance.&amp;quot; Assume that we parse these sentences into our favorite i</context>
</contexts>
<marker>Riesbeck, 1975</marker>
<rawString>Riesbeck, C. K. Conceptual Analysis. In R. Schank_ (Ed.), Conceptual Information Processing. N. York, Elvesier-North Holland, 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schank</author>
<author>R Scripts Abelson</author>
</authors>
<title>Plans, Goals, and Understanding. Laurence Erlbaum Associates,</title>
<date>1977</date>
<location>Hillsdale</location>
<contexts>
<context position="1832" citStr="Schank and Abelson 1977" startWordPosition="277" endWordPosition="280">n which high level sources of knowledge (concepts) override lower level linguistic processes. 1. Introduction This paper deals with a theory of computer comprehension of descriptive contexts. By &amp;quot;descriptive contexts&amp;quot; I refer to the language of scientific books, text books, this text, etc.. In the distinction performative vs. declarative, descriptive texts clearly fall in the declarative side. Recent work in natural language has dealt with contexts in which the computer understanding depends on the meaning of the action verbs and the human actions (plans, intentions, goals) indicated by them (Schank and Abelson 1977; Grosz 1977; Wilensky 1978; Bruce and Newman 1978). Also a , considerable amount of work has been done in a plan-based theory of task oriented dialogues (Cohen and Perrault 1979; Perrault and Allen 1980; Hobbs and Evans 1980). This work has had very little bearing on a theory of ...velputer understanding of descriptive contexts. One of the main tenets of the proposed research is that descriptive (or declarative as we prefer to call them) contexts call for different theoretical ideas compared to those proposed for the understanding of human actions, although, naturally there are aspects that a</context>
</contexts>
<marker>Schank, Abelson, 1977</marker>
<rawString>Schank, R. and Abelson, R. Scripts, Plans, Goals, and Understanding. Laurence Erlbaum Associates, Hillsdale N. J., 1977.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R C Schank</author>
<author>M Lebowitz</author>
<author>B Lawrence</author>
</authors>
<title>Parsing Directly in Knowledge Structures.</title>
<pages>94305</pages>
<institution>Computer Science Department, Stanford University,</institution>
<location>Stanford, CA</location>
<note>in IJCAI-79,</note>
<marker>Schank, Lebowitz, Lawrence, </marker>
<rawString>Schank, R. C., Lebowitz, M., and Lawrence, B. Parsing Directly in Knowledge Structures. in IJCAI-79, Computer Science Department, Stanford University, Stanford, CA 94305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Small</author>
</authors>
<title>Word Expert Parsing: A Theory of Distributed Word-Based Natural Language Understanding.</title>
<date>1980</date>
<tech>Tech. Report 954,</tech>
<institution>Dept. of Computer Science, University of Maryland,</institution>
<contexts>
<context position="14622" citStr="Small, 1980" startWordPosition="2381" endWordPosition="2382">e old balance&amp;quot; &amp;quot;the balance at the beginning of the period.&amp;quot; DESCRIPTION will return with the same token (old-bal) in all of these cases. The reader may have realized that &amp;quot;the balance at the beginning of the period&amp;quot; is not a compound noun. They are related to compound nouns. In fact many compound nouns have been formed by deletion of prepositions. We have called them prepositional phrases completing a description, and we have treated them as complex nominals. Prepositions.: For each preposition (also for each conjunction) there is a procedure. The function of these prepositional experts (cf. Small, 1980) is to determine the meaning of the preposition. We refer to them as FOR-SP, ON-SP, AS-SP, etc.. Descriptive Verbs: (D-VERBS) are those used to describe. We have categorized them in four classes. There are those that describe the constituents of an object. Among them are: consist of, show, include, be given ba, contain, etc.. We refer to them as CONSIST-OF D-VERBS. A second class are those used to indicate that something is representing something. Represent, indicate, mean, describe, etc.. belong to this class. We refer to them as REPRESENT D-VERBS. A third class are those that fall under the </context>
</contexts>
<marker>Small, 1980</marker>
<rawString>Small, S. Word Expert Parsing: A Theory of Distributed Word-Based Natural Language Understanding. Tech. Report 954, Dept. of Computer Science, University of Maryland, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wilks</author>
</authors>
<title>An Artificial Intelligence Approach to Machine Translation.</title>
<date>1973</date>
<booktitle>In Schank and Colby (eds.) Computer Models of Thought</booktitle>
<editor>and Language. San Francisco, W. H. Freeman and Co.,</editor>
<location>San Francisco,</location>
<contexts>
<context position="7011" citStr="Wilks (1973)" startWordPosition="1113" endWordPosition="1114">tual specialists access the knowledge stored in the thematic frame to perform some of these actions. 3. Linguistic Knowledge, Text Understanding and Parsing In text understanding, there are two distinct issues. One has to do with the mapping of individual sentences into some internal representation (syntactic markers, some type of case grammar, Wilks&apos; preference semantics, Schank&apos;s conceptual dependency etc.). In designing this mapping, several approaches have been taken. In Winograd (1972) and Marcus (1979), there is an interplay between syntax, and semantic markers (in that order), while in Wilks (1973) and Riesbeck (1975) the parser rely almost exclusively on semantic categories. A separate issue has to do with the meaning of the internal representation in relation to the understanding of the text. For instance, consider the following text (it belongs to the second example): &amp;quot;A bank would like to produce records of the transactions during an accounting period in connection with their checking accounts. For each account the bank wants a list showing the balance at the beginning of tie period, the number of deposits and withdrawals, and the final balance.&amp;quot; Assume that we parse these sentences</context>
</contexts>
<marker>Wilks, 1973</marker>
<rawString>Wilks, Y. An Artificial Intelligence Approach to Machine Translation. In Schank and Colby (eds.) Computer Models of Thought and Language. San Francisco, W. H. Freeman and Co., San Francisco, 1973.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Wilensky</author>
</authors>
<title>Understanding Goal-Based Stories.</title>
<date>1978</date>
<tech>Tech. Report 140,</tech>
<institution>Dept. of Computer Science, Yale University.</institution>
<contexts>
<context position="1859" citStr="Wilensky 1978" startWordPosition="283" endWordPosition="284">ge (concepts) override lower level linguistic processes. 1. Introduction This paper deals with a theory of computer comprehension of descriptive contexts. By &amp;quot;descriptive contexts&amp;quot; I refer to the language of scientific books, text books, this text, etc.. In the distinction performative vs. declarative, descriptive texts clearly fall in the declarative side. Recent work in natural language has dealt with contexts in which the computer understanding depends on the meaning of the action verbs and the human actions (plans, intentions, goals) indicated by them (Schank and Abelson 1977; Grosz 1977; Wilensky 1978; Bruce and Newman 1978). Also a , considerable amount of work has been done in a plan-based theory of task oriented dialogues (Cohen and Perrault 1979; Perrault and Allen 1980; Hobbs and Evans 1980). This work has had very little bearing on a theory of ...velputer understanding of descriptive contexts. One of the main tenets of the proposed research is that descriptive (or declarative as we prefer to call them) contexts call for different theoretical ideas compared to those proposed for the understanding of human actions, although, naturally there are aspects that are common. An important cha</context>
</contexts>
<marker>Wilensky, 1978</marker>
<rawString>Wilensky, R. Understanding Goal-Based Stories. Dept. of Computer Science, Yale University. Tech. Report 140, 1978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Winograd</author>
</authors>
<title>Understanding Natural Language.</title>
<date>1972</date>
<publisher>N. York, Academic Press,</publisher>
<contexts>
<context position="6894" citStr="Winograd (1972)" startWordPosition="1094" endWordPosition="1095">ed, to anticipate incoming information, and to guide and supervise the parser (see below). In some instances, the conceptual specialists access the knowledge stored in the thematic frame to perform some of these actions. 3. Linguistic Knowledge, Text Understanding and Parsing In text understanding, there are two distinct issues. One has to do with the mapping of individual sentences into some internal representation (syntactic markers, some type of case grammar, Wilks&apos; preference semantics, Schank&apos;s conceptual dependency etc.). In designing this mapping, several approaches have been taken. In Winograd (1972) and Marcus (1979), there is an interplay between syntax, and semantic markers (in that order), while in Wilks (1973) and Riesbeck (1975) the parser rely almost exclusively on semantic categories. A separate issue has to do with the meaning of the internal representation in relation to the understanding of the text. For instance, consider the following text (it belongs to the second example): &amp;quot;A bank would like to produce records of the transactions during an accounting period in connection with their checking accounts. For each account the bank wants a list showing the balance at the beginnin</context>
</contexts>
<marker>Winograd, 1972</marker>
<rawString>Winograd, T. Understanding Natural Language. N. York, Academic Press, 1972.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>