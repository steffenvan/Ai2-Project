<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000101">
<title confidence="0.596845">
Bootstrapping Semantic Role Labelers from Parallel Data
</title>
<author confidence="0.592366">
Mikhail Kozhevnikov Ivan Titov
</author>
<affiliation confidence="0.547821">
Saarland University, Postfach 15 11 50
</affiliation>
<address confidence="0.512008">
66041 Saarbr¨ucken, Germany
</address>
<email confidence="0.994587">
{mkozhevn|titov}@mmci.uni-saarland.de
</email>
<sectionHeader confidence="0.996589" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99965">
We present an approach which uses the sim-
ilarity in semantic structure of bilingual par-
allel sentences to bootstrap a pair of seman-
tic role labeling (SRL) models. The setting
is similar to co-training, except for the inter-
mediate model required to convert the SRL
structure between the two annotation schemes
used for different languages. Our approach
can facilitate the construction of SRL models
for resource-poor languages, while preserving
the annotation schemes designed for the tar-
get language and making use of the limited re-
sources available for it. We evaluate the model
on four language pairs, English vs German,
Spanish, Czech and Chinese. Consistent im-
provements are observed over the self-training
baseline.
</bodyText>
<sectionHeader confidence="0.998785" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999843375">
The success of statistical modeling methods in a va-
riety of natural language processing (NLP) tasks in
the last decade depended crucially on the availability
of annotated resources for their training. And while
sizable resources for most standard tasks are only
available for a few languages, the human effort re-
quired to achieve reasonable performance on such
tasks for other languages may be significantly re-
duced by leveraging existing resources and the sim-
ilarities between languages.
This idea has lead to the development of cross-
lingual annotation projection approaches, which
make use of parallel corpora (Pad´o and Lapata,
2009), as well as attempts to adapt models directly
to other languages (McDonald et al., 2011). In this
paper we consider correspondences between SRL
structures in translated sentences from a different
perspective. Most cross-lingual annotation projec-
tion approaches transfer the source language anno-
tation scheme to the target language without modifi-
cation, which makes it hard to combine their output
with existing target language resources, as annota-
tion schemes may vary significantly. We instead ad-
dress the problem of information transfer between
two existing annotation schemes (figure 1) for a pair
of languages using an intermediate model of role
correspondence (RCM). An RCM models the prob-
ability of a pair of corresponding arguments being
assigned a certain pair of roles. We then use it to
guide a pair of monolingual models toward compat-
ible predictions on parallel data in order to extend
the coverage and/or accuracy of one or both models.
</bodyText>
<figureCaption confidence="0.9948065">
Figure 1: Role correspondence in parallel sentences, an
example.
</figureCaption>
<bodyText confidence="0.999944571428571">
The notion of compatibility here is highly non-
trivial, even for sentences translated as close to the
original as possible. Zhuang and Zong (2010), for
example, observe that in the English-Chinese paral-
lel PropBank (Palmer et al., 2005b) corresponding
arguments often bear different labels, even though
the same inventory of semantic roles is used for both
</bodyText>
<figure confidence="0.77619175">
A1 AM-NEG AM-LOC
Romanian is not taught in their schools .
Ve školách se neuči rumunsky .
LOC PAT
</figure>
<page confidence="0.977423">
317
</page>
<note confidence="0.8993305">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 317–327, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999952769230769">
languages and the annotation guidelines are similar.
When different annotation schemes are considered,
the problem is further complicated by the difference
in the granularity of semantic roles used and varying
notions of what is an argument and what is not.
Manually annotated training data for such a model
is hard to come by. Instead, we propose an itera-
tive procedure similar to bootstrapping, where the
parameters of the RCM are initially estimated from
a parallel corpus automatically annotated with se-
mantic roles using the monolingual models indepen-
dently, and then the RCM is used to refine these an-
notations via a joint inference procedure, serving to
enforce consistency on the predictions of monolin-
gual models on parallel sentences. The obtained an-
notations on the parallel corpus are expected to be
of higher quality than the independent predictions of
the models, so they can be used to improve the SRL
models’ performance and/or coverage. We evalu-
ate this approach by augmenting the original train-
ing data with the annotations obtained on parallel
data and observing the change in the model’s perfor-
mance. This is especially useful if one of the lan-
guages is relatively poor in resources, in which case
the proposed procedure will help propagate infor-
mation from the stronger model to the weaker one.
Even if the two models are comparable in their pre-
dictive power, we may be able to benefit from the
fact that certain semantic roles are realized less am-
biguously in one language than in another. We will
henceforth refer to these two alternatives as the pro-
jection and symmetric setups.
The paper is structured as follows. In the next sec-
tion we present our approach and discuss the issues
of role correspondence modeling, then describe the
implementation and datasets used in evaluation in
section 3, present the evaluation and results in sec-
tion 4 and conclude with the discussion of related
work in section 5.
</bodyText>
<sectionHeader confidence="0.995498" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.999944">
We consider bootstrapping a pair of SRL models on
a parallel corpus, using the correspondence between
their predictions on parallel sentences to guide the
learning. The models are forced toward compatible
predictions, where the notion of compatibility is de-
fined by a (statistical) role correspondence model.
Let us consider a pair of languages, α and Q,
and their corresponding datasets Tα0 and Tβ0, anno-
tated with semantic roles (the upper indices here de-
note the iteration number). We will refer to these
as the initial training sets. We also assume that a
word-aligned parallel corpus is available for the pair
of languages, which we denote P, with the pred-
icates and their respective arguments identified on
both sides.
The procedure is then as follows: we train mono-
lingual models M0α and M0β on Tα0and Tβ0, respec-
tively, apply them to the two sides of the parallel
corpus, resulting in a labeling P0. We collect the se-
mantic role co-occurrence information and train the
role correspondence model C0 on it, then proceed to
the joint inference step involving M0α, M0β and C0,
resulting in a refined labeling P1 of the parallel cor-
pus. The two sides of the P1 are then used to aug-
ment the initial training sets, yielding Tα1 and Tβ1,
and new models M1β and M1β are trained on these.
The process can then be repeated using M1α and M1β
instead of the initial models.
We report the model’s performance on a held-out
test set, drawn from the same corpus as the corre-
sponding initial training set.
The procedure can be seen as a form of co-
training (Blum and Mitchell, 1998) of a pair of
monolingual SRL models. In our case, however, the
question of the models’ agreement is not as trivial as
in most applications of co-training, requiring a sta-
tistical model of its own (Ci).
In the low-resource (projection) setup our ap-
proach is also similar to self-training with weak su-
pervision coming from the stronger model.
Note that although the approach is iterative, we
have observed no significant improvements from re-
peating the procedure, possibly owing to the noise
introduced by the errors in preprocessing. In the
evaluation we run only one iteration. In the notation
introduced above, the self-training baseline model
(SELF) is trained on Pβ0, the joint model (JOINT) –
on Pβ1 and the combined model (COMB) – on Tβ1 .
</bodyText>
<subsectionHeader confidence="0.999007">
2.1 Modeling Role Correspondence
</subsectionHeader>
<bodyText confidence="0.998878">
It is necessary to distinguish between semantic
roles and their interpretation in a particular con-
text. The former can be defined in a variety of
</bodyText>
<page confidence="0.998158">
318
</page>
<bodyText confidence="0.999810111111111">
ways, depending on the formalism used. In case of
FrameNet (Baker et al., 1998), for example, the in-
terpretation of a semantic role (frame element) is ex-
plicitly provided for each separate frame, so a frame
and a frame element label together describe the se-
mantics of an argument. PropBank (Palmer et al.,
2005a) follows a mixed strategy – the labels for a
relatively small set of core roles are numbered and
their interpretations are provided separately for each
predicate (although those of the first two roles, A0
and A1, consistently denote what is known as Proto-
Agent and Proto-Patient), while modifiers (Merlo
and Leybold, 2001) bear labels that are interpreted
consistently across all predicates. Other resources,
such as Prague Dependency Treebank (Hajiˇc et al.,
2006), use a single set of semantic roles (functors),
which are interpretable across different predicates.
From the standpoint of defining the semantic sim-
ilarity of parallel sentences, the important implica-
tion is that we cannot assume that the corresponding
arguments should bear the same label, even if the
annotation schemes used are compatible (Zhuang
and Zong, 2010). Nor can we write down a single
mapping between the roles that will be valid across
different predicates (figure 2), which motivates the
need for a statistical model of semantic role corre-
spondence.
</bodyText>
<equation confidence="0.393273">
A0 A1
I do not have these concerns
Parliament adopted the resolution
El Parlamento aprueba la resolución
argG-agt arg1-pat
</equation>
<bodyText confidence="0.999971103448276">
ping between semantic roles for a given predicate
pair. As the mappings are not completely indepen-
dent – at least some roles have the same interpre-
tation across different predicate pairs, – we choose
to build a single model, which relies on features de-
rived from the pair of predicates in question, rather
than create a separate model for each predicate pair.
The model can then make decisions specific to par-
ticular predicates or predicate pairs, where sufficient
data has been observed or back off to a generic map-
ping where there is not enough data.
For the purpose of this study, we choose to sep-
arately model the probability of a target role, given
the source one and the necessary contextual infor-
mation and vice versa. These two components are
referred to as projection models and realized as a
pair of linear classifiers.
Training such a model in a conventional fash-
ion would require a rather specific kind of dataset,
namely a parallel corpus annotated with semantic
roles, and assuming the availability of such data
would severely limit the applicability of the ap-
proach proposed, as, to our knowledge, it is cur-
rently only available for two language pairs, namely
English-Chinese (Palmer et al., 2005b) and English-
Czech (Hajiˇc et al., 2012). We instead use the auto-
matically produced annotations on a parallel corpus,
effectively enforcing consistency on the role corre-
spondence in the monolingual models’ predictions.
</bodyText>
<subsectionHeader confidence="0.999256">
2.2 Joint Inference
</subsectionHeader>
<bodyText confidence="0.9994425">
The joint inference would have been simplest if the
arguments were classified independently. This as-
sumption is too restrictive, though, since the inter-
dependencies between the arguments can be used
to improve the accuracy of semantic role label-
ing (Roth and Yih, 2005).
</bodyText>
<figure confidence="0.345824333333333">
Yo no tengo tales preocupaciones
arg1-tem arg2-atr
A0 A1
We would like to know their names
Nos gustaria conocer sus nombres
arg2-ben arg1-tem
</figure>
<figureCaption confidence="0.997065666666667">
Figure 2: Predicate-specific role mapping. Note that A0
corresponds to art0-agt, art1-tem or art2-ben,
depending on the predicate.
</figureCaption>
<bodyText confidence="0.997909">
We assume the existence of a one-to-one map-
</bodyText>
<subsectionHeader confidence="0.775136">
2.2.1 Projection Setup
</subsectionHeader>
<bodyText confidence="0.999669888888889">
In the projection setup we assume that the model
for one of the languages, which we will henceforth
refer to as source, is much better informed than
the one for the other language, referred to as tar-
get, so we only have to propagate the information
one way. The scoring functions of these two mod-
els will be denoted fs and ft, respectively, and that
of the projection model from source to target – fst.
Source and target sentences are denoted 5s and 5t,
</bodyText>
<equation confidence="0.7189355">
A0
A1
</equation>
<page confidence="0.992945">
319
</page>
<bodyText confidence="0.99971892">
and aligned predicates in these sentences – ps and
pt. The task is then to identify the target language
role assignment rt that would maximize the objec-
tive L(rt) = λtft(rt, St, pt) + λstfst(rt, rs,ps,pt),
where rs = argmaxrfs(rs, Ss, ps) is the role as-
signment of the source-side arguments as predicted
by the monolingual model and λ are the weights as-
sociated with the models.
The exact maximization of this objective is com-
putationally expensive, so we resort to an approx-
imation. We chose to use the dual decomposition
method primarily because it fits the structure of the
objective particularly well (in that it is a sum of the
objectives of two independent models) and since it
allows a wide range of monolingual models to be
used in this setup. The only requirement here is that
the monolingual model must be able to incorporate
a bias toward or away from a certain prediction.
To apply this approximation, we decouple the
rt variables into rt and rst and get L1(rt, rst) =
λtft(rt,St,pt) + λstfst(rst,rs,ps,pt) under the
condition that rt = rst. Applying the Lagrangian
relaxation, we replace the hard equality constraint
on rt and rst with a soft one, using slack variables δ,
which results in the following objective:
</bodyText>
<equation confidence="0.94321125">
minδmaxrt,rstL&apos;(rt, rst, δ) =
λtft(rt,St,pt) + λstfst(rst,rs,ps,pt)+ (1)
� � δi,r �I(ri t = r) − I(rist = r)) ,
i rERt
</equation>
<bodyText confidence="0.999974">
where i indexes aligned argument pairs and I is an
indicator function. This is equivalent to
</bodyText>
<equation confidence="0.976906666666667">
minδmaxrt,rstL&apos;(rt, rst, δ) =
�minδ maxrtgt(rt, St, pt, δ)+ (2)
�maxrstgst(rst, rs, ps, pt, δ) ,
</equation>
<bodyText confidence="0.758843">
where
</bodyText>
<equation confidence="0.973123142857143">
gt(rt, St, pt, δ) =
�λtft(rt, St, pt) + � δi,rI(ri t = r)
i rERt
gstp(rst, rs, ps, pt, δ) = (3)
� �
λstfst(rst,rs,ps,pt) −
i rERt
</equation>
<bodyText confidence="0.9999493">
are the augmented objectives of the two component
models, incorporating bias factors on various possi-
ble predictions.
The minimization with respect to δ is per-
formed using a subgradient descent algorithm fol-
lowing Sontag et al. (2011). Whenever the method
converges, it converges to the global maximum of
the sum of the objectives. We found that in our case
it reaches a solution within the first 1000 iterations
over 99% of the time.
</bodyText>
<subsectionHeader confidence="0.497374">
2.2.2 Symmetric Setup
</subsectionHeader>
<bodyText confidence="0.999986315789474">
If the models have comparable accuracy, the
above inference procedure can be extended to per-
form projection both ways. Formulating this as a
dual decomposition problem would require using
three separate components, two for the monolingual
models and one for the RCM, which would have to
make its own predictions for the semantic roles on
both sides without conditioning on the predictions
of the monolingual models. This calls for a different
kind of model than the one we use – a model that
will rely on a (possibly simplified) feature represen-
tation of the source and target arguments to jointly
predict their labels. Instead, we perform the pro-
jection setup inference procedure in both directions
simultaneously, interleaving gradient descent steps
and allowing the projection models to access the up-
dated predictions of the monolingual models. This
results in a block gradient descent algorithm with the
following updates:
</bodyText>
<equation confidence="0.997505333333333">
rn+1
t = argmaxrtgt(rt, St, pt, δnt )
rn+1
s = argmaxrtgs(rs,Ss,ps,δn s )
rn+1
st = argmaxrstgst(rst, rn s , ps, pt, δn t )
rn+1
ts = argmaxrtsgts(rts, rnt , pt, ps, δns ) (4)
,nq Z&apos;r = ss&apos;Z&apos;r+
bibrERsδ
γs(n)(I(rn,i
ts = r) − I(rn,i
s = r))
n+1,i,r — n,i,r
∀i∀rERtδt — δt +
γt(n)(I(rn,i
st = r) − I(rn,i
t = r)),
</equation>
<bodyText confidence="0.986108428571429">
where γs(n) = γt(n) = γ�
n+1 is the update rate func-
tion for step n, and gs and gts are defined as in (3),
but with the direction reversed.
This procedure allows us to use the same RCM
implementation as in the projection setup. More-
over, the inference procedure for projection setup is
</bodyText>
<equation confidence="0.7718045">
δi,rI(ri= r)
st
</equation>
<page confidence="0.968278">
320
</page>
<bodyText confidence="0.99680175">
a special case of this one with -y,(n) set to 0. The
algorithm also demonstrates convergence similar to
that of the projection version, although it lacks the
optimality guarantees.
</bodyText>
<sectionHeader confidence="0.994554" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.99997275">
We evaluate our approach on four language pairs,
namely English vs German, Spanish, Czech and
Chinese, which we will denote en-de, en-es,
en-cz and en-zh respectively.
</bodyText>
<subsectionHeader confidence="0.999809">
3.1 Parallel Data
</subsectionHeader>
<bodyText confidence="0.99998528">
The parallel data for the first three language pairs
is drawn from Europarl v6 (Koehn, 2005) and
from MultiUN (Eisele and Chen, 2010) for English-
Chinese. We applied Stanford Tokenizer for En-
glish, tokenizer scripts (Koehn, 2005) provided
with the Europarl corpus to German, Spanish and
Czech, and Stanford Chinese Segmenter (Chang et
al., 2008) to Chinese, then performed POS-tagging,
morphology tagging (where applicable) and depen-
dency parsing using MATE-tools (Bohnet, 2010).
Word alignments were acquired using
GIZA++ (Och and Ney, 2003) with its stan-
dard settings. Predicate identification on the parallel
data was done using the supervised classifiers of
the monolingual SRL systems, except for German,
where a simple heuristic had to be used instead,
as only some of the predicates are marked in
the training data, which makes it hard to train a
supervised classifier. Following van der Plas et al.
(2011), we then retain only those sentences where
all identified predicates were aligned.
In the experiments we used 50 thousand predicate
pairs in each case, as increasing the amount further
did not yield noticeable benefits, while increasing
the running time.
</bodyText>
<subsectionHeader confidence="0.999383">
3.2 Annotated Data
</subsectionHeader>
<bodyText confidence="0.999972105263158">
The CoNLL’09 (Hajiˇc et al., 2009) datasets were
used as a source of annotated data for all languages.
Only verbal predicates were considered and pre-
dicted syntax was used in evaluation.
We consider subsets of the training data in order
to emulate the scenario with a resource-poor lan-
guage. Due to the different sources the datasets
were derived from, sentences contain different pro-
portions of annotated predicates depending on the
language. The German corpus, for example, con-
tains about 6 times fewer argument labels per sen-
tence than the English one. We will therefore in-
dicate the sizes of the datasets used in the number
of argument labels they contain, referred to as in-
stances, rather than the number of predicates or sen-
tences. The corpus for English, for example, con-
tains 6.2 such instances per sentence on average.
We use the 20 thousand instances of the available
data as the training corpus for each language and
split the rest equally between the development and
the test set. The secondary (“out-of-domain”) test
sets are preserved as they are.
In dependency-based SRL, only heads of syntac-
tic constituents are marked with semantic roles. The
heads of corresponding arguments may or may not
align, however, even if the arguments are lexically
very similar, because their syntactic structure may
differ. In general, one would have to identify the
whole phrase for each argument and take into ac-
count the links between constituents, rather than sin-
gle words (Pad´o and Lapata, 2005). As reconstruct-
ing the constituents from the dependency tree is non-
trivial (Hwang et al., 2010), we are using a heuristic
to address the most common version of this problem,
i.e. a preposition or an auxiliary verb being an argu-
ment head. In such a case we also take into account
any alignment links involving the head’s immediate
descendants.
</bodyText>
<subsectionHeader confidence="0.994067">
3.3 Implementation
</subsectionHeader>
<bodyText confidence="0.999980266666667">
Our system is based on that of Bj¨orkelund et al.
(2009). It is a pipeline system comprised of a set of
binary or multiclass linear classifiers. Both here and
in the projection model, the classifiers are trained
using Liblinear (Fan et al., 2008).
We employed a uniqueness constraint on role la-
bels (Chang et al., 2007), preventing some of them
from being assigned to more than one argument in
the same predicate, which appears to be more reli-
able in a low-resource setting we consider than the
reranker the original system employed. The con-
straint is enforced in the monolingual model infer-
ence using a beam-search approximation with the
beam size of 10. The label uniqueness information
was derived from the training sets.
</bodyText>
<page confidence="0.994384">
321
</page>
<subsectionHeader confidence="0.63419">
3.4 The Projection Model
</subsectionHeader>
<bodyText confidence="0.999966307692308">
Each projection model is realized by a single lin-
ear classifier applied to each argument pair indepen-
dently. It relies on features derived from the source
semantic role and source and target predicates, and
predicts the semantic role for the argument in the
target sentence.
The features include the source semantic role and
its conjunctions with (lowercased) forms and lem-
mata of the source and target predicates. For ex-
ample, assuming the source semantic role is A3 and
the source and target predicates are went and ging
(past tense of “gehen”, German), the features would
be as shown in figure 3.
</bodyText>
<figure confidence="0.972367714285714">
FORMPAIR=A3-went-ging
LEMMAPAIR=A3-go-gehen
FORMSRC=A3-went
FORMTGT=A3-ging
LEMMASRC=A3-go
LEMMATGT=A3-gehen
LABEL=A3
</figure>
<figureCaption confidence="0.999089">
Figure 3: Projection model features example.
</figureCaption>
<subsectionHeader confidence="0.876335">
3.5 Parameters
</subsectionHeader>
<bodyText confidence="0.972989">
In case of projection there are two parameters, Ast
and At, – the weights of the component models in the
objective. Only their relative values matter (except
in the choice of &apos;yo), so we set At to 1 and only tune
the weight of the role correspondence model.
In the symmetric setup, the objective takes
the form L(rt, rs) = Atft(rt, St,pt) +
Astfst(rt,rs,ps,pt) + Asfs(rs,Ss,ps) +
Atsfts(rs, rt, pt, ps). Since we assume that the
two monolingual models here have comparable
performance, we do not tune their relative weights,
setting both As and At to 1.
We also use the same weight for both projection
models, Ast = Ats, and this value plays an important
role – it basically indicates how strongly we insist
on the role correspondence models’ correctness. If
this weight is set to 0, the RCM will accept the ini-
tial predictions the monolingual models make, and if
it is set to a sufficiently large value, the predictions
of the monolingual models will be biased until they
match the mapping suggested by the RCM. The op-
timal weight will therefore depend on the language
pair, the sizes of the initial training sets and the RCM
used. We use the value of 0.7 in all projection ex-
periments and 0.5 in the symmetric setup, however,
as excessive tuning may be undesirable in the low-
resource setting.
</bodyText>
<subsectionHeader confidence="0.969535">
3.6 Domains
</subsectionHeader>
<bodyText confidence="0.999951341463415">
One important factor in the understanding of the
evaluation figures presented is the fact that sources
of annotated and parallel data belong to different do-
mains. The former usually contains some sort of
newswire text – Wall Street Journal in case of En-
glish, Xinhua newswire, Hong Kong news and Sino-
rama news magazine for Chinese, etc. Parallel data,
on the other hand, comes from the proceedings of
European Parliament and United Nations, which are
quite different. For example, the sentences in the
latter domain often start with someone being ad-
dressed, either by name or by title, which can hardly
be expected to occur as often in a newspaper or a
magazine article.
As is well-known, the performance of many sta-
tistical tools drops significantly outside the domain
they were trained on (Pradhan et al., 2008), and the
preprocessing and SRL models used here are no ex-
ception, which results in relatively low quality of
the initial predictions on the parallel text. The low
argument identification performance, in particular,
is presumably due to inaccurate dependency parses,
on which it heavily relies. Several approached have
been proposed to improve the accuracy of depen-
dency parsers and other tools on out-of-domain data,
but this is beyond the scope of this paper. In some
cases (though seldom), sources of parallel data be-
longing to the same domain as the annotated training
data can be obtained.
Another concern is that the performance of a
model trained on automatically labeled parallel data
as measured on a test set we use may not reflect the
quality of these annotations. To assess the resulting
model’s coverage, it would be interesting to evaluate
it on data outside the original domain, so we con-
sider the out-of-domain (OOD) test sets as provided
for the CoNLL Shared Task 2009 where available.
Perhaps the most interesting one of these is the
German OOD test set, which is drawn from Europarl
(as is the parallel data we use). It was originally
annotated with syntactic dependency trees and se-
</bodyText>
<page confidence="0.996611">
322
</page>
<bodyText confidence="0.999874875">
mantic structure in the SALSA format (Burchardt
et al., 2006) for Pad´o and Lapata (2005), and then
converted into a PropBank-like form for the CoNLL
Shared Task 2009 (Hajiˇc et al., 2009). The OOD
test set for English is drawn from the Brown cor-
pus (Francis and Kucera, 1967) and the one for
Czech – from a Czech translation of Wall Street
Journal articles (Hajiˇc et al., 2012).
</bodyText>
<sectionHeader confidence="0.99847" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9999613">
The first question we are interested in is how the
joint inference affects the quality of the automati-
cally obtained annotations on the parallel data. To
answer this, we will run the monolingual models in-
dependently and jointly, then train models on the
output of these two procedures and compare their
performance on a test set. Note that we do not add
the initial training data at this point, so the initial
model scores are provided for reference, rather than
as a baseline.
</bodyText>
<subsectionHeader confidence="0.998294">
4.1 Projection Setup
</subsectionHeader>
<bodyText confidence="0.99620025">
A small initial training set of 600 instances was used
here for the target language here and the full training
set (20000 instances) for the source one. Ast was set
to 0.7 in all experiments in this section.
</bodyText>
<table confidence="0.998750133333333">
INIT SELF JOINT DSELF
en-cz* 61.11 60.68 63.01 2.33
en-cz 62.45 62.15 63.11 0.96
en-de* 66.81 63.96 67.64 3.69
en-de 70.40 68.34 70.13 1.79
en-es 64.20 64.51 66.01 1.50
en-zh 75.80 73.52 74.87 1.35
cz-en* 66.82 63.95 64.97 1.02
cz-en 74.92 71.60 71.90 0.29
de-en* 66.82 63.58 63.21 -0.37
de-en 74.93 71.31 70.72 -0.59
es-en* 66.82 63.95 64.18 0.23
es-en 74.93 71.47 72.09 0.62
zh-en* 66.82 64.51 63.67 -0.83
zh-en 74.93 72.26 71.24 -1.01
</table>
<tableCaption confidence="0.9940865">
Table 1: Projection setup results: self-training baseline,
refined model and the difference in their performance.
Asterisk indicates out-of-domain test set, statistically sig-
nificant improvements are highlighted in bold.
</tableCaption>
<bodyText confidence="0.99957946875">
In table 1, we present the accuracy of the model
trained on the output of the joint inference (JOINT)
against that of the self-training baseline (SELF). The
DSELF column contains the difference between the
two. Note that the SELF model is trained on the
parallel data automatically annotated using mono-
lingual SRL models (not mixed with the initial train-
ing set), since we are interested in the effect of joint
inference on the quality of the annotation obtained.
Where the improvement is positive and statistically
significant with p &lt; 0.005 according to the permuta-
tion test (Good, 2000), they are highlighted in bold.
We can see that the refined model (JOINT) outper-
forms the self-training baseline in most cases by a
moderate, but statistically significant margin, which
indicates that the joint inference does improve the
quality of annotations on the parallel corpus.
The slightly higher improvement on the German
OOD test set supports our hypothesis that the proce-
dure enhances the performance of the model on par-
allel data, as the data for this test set is also drawn
from the Europarl corpus. The improvement over
the initial model (DINIT) in this case is statistically
significant with p &lt; 0.05. Higher p-value may be
attributed to the smaller test set size.
Figure 4 shows how the performance of the JOINT
model changes with the size of the initial training
set. The improvements are smaller for en-cz, en-
de and en-zh, but they are also statistically signifi-
cant for initial training sets of up to 2000 instances.
Projection to English from other languages performs
worse.
</bodyText>
<figureCaption confidence="0.971258">
Figure 4: Projection setup, English-Spanish, model per-
formance as a function of the size of the initial training
set.
</figureCaption>
<page confidence="0.997475">
323
</page>
<subsectionHeader confidence="0.964598">
4.2 Combining
</subsectionHeader>
<bodyText confidence="0.999108071428571">
In practice, automatically obtained annotations are
usually combined with the existing labeled data. For
this purpose, the initial training set is replicated so
as to constitute 0.3 (an empirically chosen value that
appears to work well in most experiments) of the
size of the automatically labeled dataset. We com-
pare the performance of the model trained on the re-
sulting dataset (COMB) with that of the JOINT model
and the initial models. The results are presented in
table 2. We omit projection from other languages to
English, since the JOINT model there fails to outper-
form the initial model and we do not expect to ben-
efit from adding the automatically annotated data to
the initial training set in this case.
</bodyText>
<table confidence="0.994754714285714">
INIT JOINT COMB OJOINT OINIT
en-cz* 61.11 63.01 62.98 -0.03 1.87
en-cz 62.45 63.11 63.30 0.19 0.85
en-de* 66.81 67.64 67.64 0.00 0.84
en-de 70.39 70.19 70.53 0.34 0.15
en-es 64.20 66.01 66.01 0.00 1.81
en-zh 75.80 74.87 75.03 0.16 -0.77
</table>
<tableCaption confidence="0.991888">
Table 2: The effect of adding automatically obtained an-
notation to the initial training set. Asterisk indicates out-
of-domain test set, statistically significant improvements
are highlighted in bold.
</tableCaption>
<subsectionHeader confidence="0.998873">
4.3 Symmetric Setup
</subsectionHeader>
<bodyText confidence="0.99997975">
In the symmetric setup evaluation, we use a slightly
larger initial training set of 1400 instances for both
source and target language. The projection model
weight is set to 0.5. Table 3 shows the accuracy of
the JOINT model and the SELF baseline.
Note that here, unlike section 4.1, the joint in-
ference is run once and then a model is trained for
each language and evaluated on the corresponding
test set(s).
The results support our intuition that joint infer-
ence helps improve the quality of the resulting an-
notations, at least in some cases.
</bodyText>
<subsectionHeader confidence="0.996913">
4.4 Oracle RCM
</subsectionHeader>
<bodyText confidence="0.99995575">
It would be useful to know to what extent the per-
formance of the role correspondence model affects
the quality of the output (and thus the performance
of the resulting model). The RCM we use is rather
</bodyText>
<table confidence="0.999679266666667">
INIT SELF JOINT OSELF
en-cz* 67.07 66.15 68.18 2.02
en-cz 67.56 66.42 66.72 0.30
en-de* 67.64 66.72 68.57 1.84
en-de 75.13 71.97 73.57 1.60
en-es 68.14 67.80 69.04 1.24
en-zh 76.28 72.96 75.22 2.26
cz-en* 69.37 66.45 66.22 -0.23
cz-en 77.32 74.72 75.02 0.31
de-en* 69.37 66.45 66.68 0.23
de-en 77.32 73.56 73.72 0.17
es-en* 69.37 66.64 66.40 -0.23
es-en 77.32 74.05 74.89 0.84
zh-en* 69.37 66.08 65.53 -0.56
zh-en 77.32 74.48 74.25 -0.24
</table>
<tableCaption confidence="0.9949345">
Table 3: Comparing JOINT model against the self-
training baseline in symmetric setup. Asterisk indicates
out-of-domain test set, statistically significant improve-
ments are highlighted in bold.
</tableCaption>
<bodyText confidence="0.999945166666667">
simplistic, and we believe it can be substantially im-
proved for any given language pair by incorporat-
ing prior knowledge and/or using external sources
of information. In order to estimate the potential
impact of such improvements, we simulate a better
informed projection model, giving it access to the
predictions of more accurate monolingual models on
the parallel data – those trained on the full training
set, rather than the initial training set used in this par-
ticular experiment. We refer to the resulting RCM as
oracle and assess the difference it makes, compared
to a regular one (table 4).
</bodyText>
<sectionHeader confidence="0.999823" genericHeader="conclusions">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999922285714286">
There is a number of approaches to semi-supervised
semantic role labeling, and most suggest that some
external supervision is required for such approaches
to work (He and Gildea, 2006), such as measures of
syntactic and semantic similarity (F¨urstenau and La-
pata, 2009) or external confidence measures (Gold-
wasser et al., 2011). The alternative we propose is
primarily motivated by the research on annotation
projection (Pad´o and Lapata, 2009; van der Plas
et al., 2011; Annesi and Basili, 2010; Naseem et
al., 2012) and direct transfer (Durrett et al., 2012;
Søgaard, 2011; Lopez et al., 2008; McDonald et al.,
2011). The key difference of the present approach
compared to annotation projection is that we assume
</bodyText>
<page confidence="0.996257">
324
</page>
<table confidence="0.999593933333333">
INIT SELF JOINT ASELF DINIT
en-cz* 61.11 60.68 72.49 11.81 11.38
en-cz 62.45 62.15 70.19 8.04 7.74
en-de* 66.81 63.96 76.78 12.82 9.97
en-de 70.39 68.34 79.22 10.88 8.84
en-es 64.20 64.51 75.43 10.92 11.23
en-zh 75.80 73.52 76.75 3.22 0.94
cz-en* 66.82 63.95 70.75 6.80 3.93
cz-en 74.93 71.60 79.70 8.10 4.76
de-en* 66.82 63.58 69.46 5.88 2.64
de-en 74.93 71.31 77.34 6.03 2.41
es-en* 66.82 63.95 69.92 5.97 3.10
es-en 74.93 71.47 79.55 8.08 4.62
zh-en* 66.82 64.51 67.19 2.68 0.37
zh-en 74.93 72.26 76.51 4.26 1.58
</table>
<tableCaption confidence="0.9957218">
Table 4: Oracle RCM performance, projection setup: ini-
tial model, self-training baseline, refined model and its
improvement over the other two. Asterisk indicates out-
of-domain test set, statistically significant improvements
are highlighted in bold.
</tableCaption>
<bodyText confidence="0.999085925925926">
the availability of some amount of training data for
the target language, possibly using a different inven-
tory of semantic roles.
As mentioned previously, from the training point
of view this approach can be seen as similar to co-
training (Blum and Mitchell, 1998), other applica-
tions of which to NLP are too numerous to list here.
Most closely related is the joint inference in
Zhuang and Zong (2010), the main difference being
that it relies on a manually annotated parallel corpus,
aligned on the argument level, and evaluates only the
inference procedure and only on in-domain data.
Other related approaches include Kim et al.
(2010), where a cross-lingual transfer of relations
is performed (which basically represent parts of
the predicate-argument structure considered by SRL
methods), and Frermann and Bond (2012), where
semantic structure matching is used to rank HPSG
parses for parallel sentences.
Unsupervised semantic role labeling meth-
ods (Lang and Lapata, 2010; Lang and Lapata,
2011; Titov and Klementiev, 2012a; Lorenzo and
Cerisara, 2012) present an alternative to the cross-
lingual information propagation approaches such as
ours, and at least one the methods in this area also
makes use of parallel data (Titov and Klementiev,
2012b).
</bodyText>
<subsectionHeader confidence="0.735966">
Conclusions
</subsectionHeader>
<bodyText confidence="0.9999835">
We have presented an approach to information trans-
fer between SRL systems for different language
pairs using parallel data. The task proves challeng-
ing due to non-trivial mapping between the role la-
bels used in different SRL annotation schemes and
the nature of parallel data – the difference in do-
mains and the limited accuracy of the preprocess-
ing tools. We observe consistent improvements over
self-training baseline from using joint inference and
the experiments suggest that improving the role cor-
respondence model, for example using language-
specific prior knowledge or external data sources,
may dramatically increase the performance of the re-
sulting system.
</bodyText>
<sectionHeader confidence="0.99847" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.994297">
The authors acknowledge the support of the MMCI
Cluster of Excellence and thank Alexandre Klemen-
tiev and Manfred Pinkal for valuable suggestions.
</bodyText>
<sectionHeader confidence="0.996991" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.983930038461538">
Paolo Annesi and Roberto Basili. 2010. Cross-lingual
alignment of framenet annotations through hidden
markov models. In Proceedings of the 11th interna-
tional conference on Computational Linguistics and
Intelligent Text Processing, CICLing’10, pages 12–25,
Berlin, Heidelberg. Springer-Verlag.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Pro-
ceedings of the Thirty-Sixth Annual Meeting of the
Association for Computational Linguistics and Sev-
enteenth International Conference on Computational
Linguistics (ACL-COLING’98), pages 86–90, Mon-
treal, Canada.
Anders Bj¨orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning (CoNLL 2009):
Shared Task, pages 43–48, Boulder, Colorado, June.
Association for Computational Linguistics.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the Workshop on Computational Learning The-
ory (COLT 98).
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of the
23rd International Conference on Computational Lin-
</reference>
<page confidence="0.995006">
325
</page>
<reference confidence="0.999292878504673">
guistics (Coling 2010), pages 89–97, Beijing, China,
August. Coling 2010 Organizing Committee.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of LREC 2006,
Genoa, Italy.
M.W. Chang, L. Ratinov, and D. Roth. 2007. Guiding
semi-supervision with constraint-driven learning. Ur-
bana, 51:61801.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of the Third Workshop on Statistical Machine
Translation, StatMT ’08, pages 224–232, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syntac-
tic transfer using a bilingual lexicon. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1–11, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
Andreas Eisele and Yu Chen. 2010. MultiUN: A multi-
lingual corpus from united nation documents. In Nico-
letta Calzolari (Conference Chair), Khalid Choukri,
Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios
Piperidis, Mike Rosner, and Daniel Tapias, editors,
Proceedings of the Seventh International Conference
on Language Resources and Evaluation (LREC’10),
Valletta, Malta, May. European Language Resources
Association (ELRA).
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871–1874.
S. Francis and H. Kucera. 1967. Computing Analysis
of Present-day American English. Brown University
Press, Providence, RI.
Lea Frermann and Francis Bond. 2012. Cross-lingual
parse disambiguation based on semantic correspon-
dence. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 125–129, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Hagen F¨urstenau and Mirella Lapata. 2009. Graph
alignment for semi-supervised semantic role labeling.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 11–
20, Singapore.
D. Goldwasser, R. Reichart, J. Clarke, and D. Roth.
2011. Confidence driven unsupervised semantic pars-
ing. In ACL.
P. Good. 2000. Permutation Tests: A Practical
Guide to Resampling Methods for Testing Hypotheses.
Springer.
J. Haji&amp;quot;c, J. Panevov´a, E. Haji&amp;quot;cov´a, P. Sgall, P. Pajas,
J. &amp;quot;St&amp;quot;ep´anek, J. Havelka, M. Mikulov´a, Z. &amp;quot;Zabokrtsk`y,
and M. &amp;quot;Sev&amp;quot;cfkov´a-Razfmov´a. 2006. Prague depen-
dency treebank 2.0. LDC.
Jan Haji&amp;quot;c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Martf, Llufs
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan &amp;quot;St&amp;quot;ep´anek, Pavel Stra&amp;quot;n´ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2009): Shared Task, pages
1–18, Boulder, Colorado.
Jan Haji&amp;quot;c, Eva Haji&amp;quot;cov´a, Jarmila Panevov´a, Petr Sgall,
Ond&amp;quot;rej Bojar, Silvie Cinkov´a, Eva Fu&amp;quot;c´ıkov´a, Marie
Mikulov´a, Petr Pajas, Jan Popelka, Ji&amp;quot;rfSemeck´y,
Jana &amp;quot;Sindlerov´a, Jan &amp;quot;St&amp;quot;ep´anek, Josef Toman, Zde&amp;quot;nka
Ure&amp;quot;sov´a, and Zden&amp;quot;ek &amp;quot;Zabokrtsk´y. 2012. Announc-
ing prague czech-english dependency treebank 2.0.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Thierry Declerck, Mehmet U&amp;quot;gur Do&amp;quot;gan,
Bente Maegaard, Joseph Mariani, Jan Odijk, and Ste-
lios Piperidis, editors, Proceedings of the Eight In-
ternational Conference on Language Resources and
Evaluation (LREC’12), Istanbul, Turkey, May. Euro-
pean Language Resources Association (ELRA).
Shan He and Daniel Gildea. 2006. Self-training and
co-training for semantic role labeling: Primary report.
Technical report, University of Rochester.
Jena D. Hwang, Rodney D. Nielsen, and Martha Palmer.
2010. Towards a domain independent semantics:
Enhancing semantic representation with construction
grammar. In Proceedings of the NAACL HLT Work-
shop on Extracting and Using Constructions in Com-
putational Linguistics, pages 1–8, Los Angeles, Cali-
fornia, June. Association for Computational Linguis-
tics.
Seokhwan Kim, Minwoo Jeong, Jonghoon Lee, and
Gary Geunbae Lee. 2010. A cross-lingual annota-
tion projection approach for relation detection. In Pro-
ceedings of the 23rd International Conference on Com-
putational Linguistics, COLING ’10, pages 564–571,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Summit,
pages 79–86, Phuket, Thailand. AAMT, AAMT.
Joel Lang and Mirella Lapata. 2010. Unsupervised in-
duction of semantic roles. In Human Language Tech-
</reference>
<page confidence="0.988336">
326
</page>
<reference confidence="0.999772138613862">
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 939–947, Los Angeles, Cal-
ifornia, June. Association for Computational Linguis-
tics.
Joel Lang and Mirella Lapata. 2011. Unsupervised se-
mantic role induction via split-merge clustering. In
Proc. of Annual Meeting of the Association for Com-
putational Linguistics (ACL).
Adam Lopez, Daniel Zeman, Michael Nossal, Philip
Resnik, and Rebecca Hwa. 2008. Cross-Language
Parser Adaptation between Related Languages. In
IJCNLP-08 Workshop on NLP for Less Privileged
Languages, pages 35–42, Hyderabad, India, January.
Alejandra Lorenzo and Christophe Cerisara. 2012. Un-
supervised frame based semantic role induction: ap-
plication to french and english. In Proceedings of the
ACL 2012 Joint Workshop on Statistical Parsing and
Semantic Processing of Morphologically Rich Lan-
guages, pages 30–35, Jeju, Republic of Korea, July 12.
Association for Computational Linguistics.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ’11, pages 62–72, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Paola Merlo and Matthias Leybold. 2001. Automatic
distinction of arguments and modifiers: the case of
prepositional phrases. In Proceedings of the Fifth
Computational Natural Language Learning Workshop
(CoNLL-2001), pages 121–128, Toulouse, France.
Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency
parsing. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 629–637, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1).
Sebastian Pad´o and Mirella Lapata. 2005. Cross-
linguistic projection of role-semantic information. In
Proceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Natu-
ral Language Processing, pages 859–866, Vancouver,
British Columbia, Canada.
Sebastian Pad´o and Mirella Lapata. 2009. Cross-lingual
annotation projection for semantic roles. Journal of
Artificial Intelligence Research, 36:307–340.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005a. The Proposition Bank: An annotated corpus
of semantic roles. Computational Linguistics, 31:71–
105.
Martha Palmer, Nianwen Xue, Olga Babko-Malaya, Jiny-
ing Chen, and Benjamin Snyder. 2005b. A parallel
Proposition Bank II for Chinese and English. In Pro-
ceedings of the Workshop on Frontiers in Corpus An-
notations II: Pie in the Sky, CorpusAnno ’05, pages
61–67, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Sameer S. Pradhan, Wayne Ward, and James H. Martin.
2008. Towards robust semantic role labeling. Compu-
tational Linguistics, 34(2):289–310.
Dan Roth and Wen-tau Yih. 2005. Integer linear pro-
gramming inference for conditional random fields. In
ICML, pages 736–743.
Anders Søgaard. 2011. Data point selection for cross-
language adaptation of dependency parsers. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies: short papers - Volume 2, HLT ’11,
pages 682–686, Stroudsburg, PA, USA. Association
for Computational Linguistics.
David Sontag, Amir Globerson, and Tommi Jaakkola.
2011. Introduction to dual decomposition for in-
ference. In Suvrit Sra, Sebastian Nowozin, and
Stephen J. Wright, editors, Optimization for Machine
Learning. MIT Press.
Ivan Titov and Alexandre Klementiev. 2012a. A
Bayesian approach to unsupervised semantic role in-
duction. In Proc. of European Chapter of the Associa-
tion for Computational Linguistics (EACL).
Ivan Titov and Alexandre Klementiev. 2012b. Crosslin-
gual induction of semantic roles. In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics, Jeju Island, South Korea, July.
Association for Computational Linguistics.
Lonneke van der Plas, Paola Merlo, and James Hender-
son. 2011. Scaling up automatic cross-lingual seman-
tic role annotation. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies: short papers -
Volume 2, HLT ’11, pages 299–304, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Tao Zhuang and Chengqing Zong. 2010. Joint inference
for bilingual semantic role labeling. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, EMNLP ’10, pages 304–314,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
</reference>
<page confidence="0.998401">
327
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.927024">
<title confidence="0.999548">Bootstrapping Semantic Role Labelers from Parallel Data</title>
<author confidence="0.998491">Mikhail Kozhevnikov Ivan Titov</author>
<affiliation confidence="0.985963">Saarland University, Postfach 15 11</affiliation>
<address confidence="0.994454">66041 Saarbr¨ucken, Germany</address>
<abstract confidence="0.996902055555556">We present an approach which uses the similarity in semantic structure of bilingual parallel sentences to bootstrap a pair of semantic role labeling (SRL) models. The setting is similar to co-training, except for the intermediate model required to convert the SRL structure between the two annotation schemes used for different languages. Our approach can facilitate the construction of SRL models for resource-poor languages, while preserving the annotation schemes designed for the target language and making use of the limited resources available for it. We evaluate the model on four language pairs, English vs German, Spanish, Czech and Chinese. Consistent improvements are observed over the self-training baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Paolo Annesi</author>
<author>Roberto Basili</author>
</authors>
<title>Cross-lingual alignment of framenet annotations through hidden markov models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th international conference on Computational Linguistics and Intelligent Text Processing, CICLing’10,</booktitle>
<pages>12--25</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="30839" citStr="Annesi and Basili, 2010" startWordPosition="5110" endWordPosition="5113">We refer to the resulting RCM as oracle and assess the difference it makes, compared to a regular one (table 4). 5 Related Work There is a number of approaches to semi-supervised semantic role labeling, and most suggest that some external supervision is required for such approaches to work (He and Gildea, 2006), such as measures of syntactic and semantic similarity (F¨urstenau and Lapata, 2009) or external confidence measures (Goldwasser et al., 2011). The alternative we propose is primarily motivated by the research on annotation projection (Pad´o and Lapata, 2009; van der Plas et al., 2011; Annesi and Basili, 2010; Naseem et al., 2012) and direct transfer (Durrett et al., 2012; Søgaard, 2011; Lopez et al., 2008; McDonald et al., 2011). The key difference of the present approach compared to annotation projection is that we assume 324 INIT SELF JOINT ASELF DINIT en-cz* 61.11 60.68 72.49 11.81 11.38 en-cz 62.45 62.15 70.19 8.04 7.74 en-de* 66.81 63.96 76.78 12.82 9.97 en-de 70.39 68.34 79.22 10.88 8.84 en-es 64.20 64.51 75.43 10.92 11.23 en-zh 75.80 73.52 76.75 3.22 0.94 cz-en* 66.82 63.95 70.75 6.80 3.93 cz-en 74.93 71.60 79.70 8.10 4.76 de-en* 66.82 63.58 69.46 5.88 2.64 de-en 74.93 71.31 77.34 6.03 2.4</context>
</contexts>
<marker>Annesi, Basili, 2010</marker>
<rawString>Paolo Annesi and Roberto Basili. 2010. Cross-lingual alignment of framenet annotations through hidden markov models. In Proceedings of the 11th international conference on Computational Linguistics and Intelligent Text Processing, CICLing’10, pages 12–25, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>In Proceedings of the Thirty-Sixth Annual Meeting of the Association for Computational Linguistics and Seventeenth International Conference on Computational Linguistics (ACL-COLING’98),</booktitle>
<pages>86--90</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="7804" citStr="Baker et al., 1998" startWordPosition="1265" endWordPosition="1268">e have observed no significant improvements from repeating the procedure, possibly owing to the noise introduced by the errors in preprocessing. In the evaluation we run only one iteration. In the notation introduced above, the self-training baseline model (SELF) is trained on Pβ0, the joint model (JOINT) – on Pβ1 and the combined model (COMB) – on Tβ1 . 2.1 Modeling Role Correspondence It is necessary to distinguish between semantic roles and their interpretation in a particular context. The former can be defined in a variety of 318 ways, depending on the formalism used. In case of FrameNet (Baker et al., 1998), for example, the interpretation of a semantic role (frame element) is explicitly provided for each separate frame, so a frame and a frame element label together describe the semantics of an argument. PropBank (Palmer et al., 2005a) follows a mixed strategy – the labels for a relatively small set of core roles are numbered and their interpretations are provided separately for each predicate (although those of the first two roles, A0 and A1, consistently denote what is known as ProtoAgent and Proto-Patient), while modifiers (Merlo and Leybold, 2001) bear labels that are interpreted consistentl</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet project. In Proceedings of the Thirty-Sixth Annual Meeting of the Association for Computational Linguistics and Seventeenth International Conference on Computational Linguistics (ACL-COLING’98), pages 86–90, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>Love Hafdell</author>
<author>Pierre Nugues</author>
</authors>
<title>Multilingual semantic role labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL 2009): Shared Task,</booktitle>
<pages>43--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<marker>Bj¨orkelund, Hafdell, Nugues, 2009</marker>
<rawString>Anders Bj¨orkelund, Love Hafdell, and Pierre Nugues. 2009. Multilingual semantic role labeling. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL 2009): Shared Task, pages 43–48, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of the Workshop on Computational Learning Theory (COLT 98).</booktitle>
<contexts>
<context position="6796" citStr="Blum and Mitchell, 1998" startWordPosition="1096" endWordPosition="1099">ce information and train the role correspondence model C0 on it, then proceed to the joint inference step involving M0α, M0β and C0, resulting in a refined labeling P1 of the parallel corpus. The two sides of the P1 are then used to augment the initial training sets, yielding Tα1 and Tβ1, and new models M1β and M1β are trained on these. The process can then be repeated using M1α and M1β instead of the initial models. We report the model’s performance on a held-out test set, drawn from the same corpus as the corresponding initial training set. The procedure can be seen as a form of cotraining (Blum and Mitchell, 1998) of a pair of monolingual SRL models. In our case, however, the question of the models’ agreement is not as trivial as in most applications of co-training, requiring a statistical model of its own (Ci). In the low-resource (projection) setup our approach is also similar to self-training with weak supervision coming from the stronger model. Note that although the approach is iterative, we have observed no significant improvements from repeating the procedure, possibly owing to the noise introduced by the errors in preprocessing. In the evaluation we run only one iteration. In the notation intro</context>
<context position="32092" citStr="Blum and Mitchell, 1998" startWordPosition="5314" endWordPosition="5317">97 3.10 es-en 74.93 71.47 79.55 8.08 4.62 zh-en* 66.82 64.51 67.19 2.68 0.37 zh-en 74.93 72.26 76.51 4.26 1.58 Table 4: Oracle RCM performance, projection setup: initial model, self-training baseline, refined model and its improvement over the other two. Asterisk indicates outof-domain test set, statistically significant improvements are highlighted in bold. the availability of some amount of training data for the target language, possibly using a different inventory of semantic roles. As mentioned previously, from the training point of view this approach can be seen as similar to cotraining (Blum and Mitchell, 1998), other applications of which to NLP are too numerous to list here. Most closely related is the joint inference in Zhuang and Zong (2010), the main difference being that it relies on a manually annotated parallel corpus, aligned on the argument level, and evaluates only the inference procedure and only on in-domain data. Other related approaches include Kim et al. (2010), where a cross-lingual transfer of relations is performed (which basically represent parts of the predicate-argument structure considered by SRL methods), and Frermann and Bond (2012), where semantic structure matching is used</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the Workshop on Computational Learning Theory (COLT 98).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
</authors>
<title>Top accuracy and fast dependency parsing is not a contradiction.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>89--97</pages>
<location>Beijing, China,</location>
<contexts>
<context position="16258" citStr="Bohnet, 2010" startWordPosition="2686" endWordPosition="2687">amely English vs German, Spanish, Czech and Chinese, which we will denote en-de, en-es, en-cz and en-zh respectively. 3.1 Parallel Data The parallel data for the first three language pairs is drawn from Europarl v6 (Koehn, 2005) and from MultiUN (Eisele and Chen, 2010) for EnglishChinese. We applied Stanford Tokenizer for English, tokenizer scripts (Koehn, 2005) provided with the Europarl corpus to German, Spanish and Czech, and Stanford Chinese Segmenter (Chang et al., 2008) to Chinese, then performed POS-tagging, morphology tagging (where applicable) and dependency parsing using MATE-tools (Bohnet, 2010). Word alignments were acquired using GIZA++ (Och and Ney, 2003) with its standard settings. Predicate identification on the parallel data was done using the supervised classifiers of the monolingual SRL systems, except for German, where a simple heuristic had to be used instead, as only some of the predicates are marked in the training data, which makes it hard to train a supervised classifier. Following van der Plas et al. (2011), we then retain only those sentences where all identified predicates were aligned. In the experiments we used 50 thousand predicate pairs in each case, as increasin</context>
</contexts>
<marker>Bohnet, 2010</marker>
<rawString>Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 89–97, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aljoscha Burchardt</author>
<author>Katrin Erk</author>
<author>Anette Frank</author>
<author>Andrea Kowalski</author>
<author>Sebastian Pado</author>
<author>Manfred Pinkal</author>
</authors>
<title>The SALSA corpus: a German corpus resource for lexical semantics.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC 2006,</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="23747" citStr="Burchardt et al., 2006" startWordPosition="3930" endWordPosition="3933">l trained on automatically labeled parallel data as measured on a test set we use may not reflect the quality of these annotations. To assess the resulting model’s coverage, it would be interesting to evaluate it on data outside the original domain, so we consider the out-of-domain (OOD) test sets as provided for the CoNLL Shared Task 2009 where available. Perhaps the most interesting one of these is the German OOD test set, which is drawn from Europarl (as is the parallel data we use). It was originally annotated with syntactic dependency trees and se322 mantic structure in the SALSA format (Burchardt et al., 2006) for Pad´o and Lapata (2005), and then converted into a PropBank-like form for the CoNLL Shared Task 2009 (Hajiˇc et al., 2009). The OOD test set for English is drawn from the Brown corpus (Francis and Kucera, 1967) and the one for Czech – from a Czech translation of Wall Street Journal articles (Hajiˇc et al., 2012). 4 Evaluation The first question we are interested in is how the joint inference affects the quality of the automatically obtained annotations on the parallel data. To answer this, we will run the monolingual models independently and jointly, then train models on the output of the</context>
</contexts>
<marker>Burchardt, Erk, Frank, Kowalski, Pado, Pinkal, 2006</marker>
<rawString>Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea Kowalski, Sebastian Pado, and Manfred Pinkal. 2006. The SALSA corpus: a German corpus resource for lexical semantics. In Proceedings of LREC 2006, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M W Chang</author>
<author>L Ratinov</author>
<author>D Roth</author>
</authors>
<title>Guiding semi-supervision with constraint-driven learning.</title>
<date>2007</date>
<pages>51--61801</pages>
<location>Urbana,</location>
<contexts>
<context position="19147" citStr="Chang et al., 2007" startWordPosition="3163" endWordPosition="3166">s nontrivial (Hwang et al., 2010), we are using a heuristic to address the most common version of this problem, i.e. a preposition or an auxiliary verb being an argument head. In such a case we also take into account any alignment links involving the head’s immediate descendants. 3.3 Implementation Our system is based on that of Bj¨orkelund et al. (2009). It is a pipeline system comprised of a set of binary or multiclass linear classifiers. Both here and in the projection model, the classifiers are trained using Liblinear (Fan et al., 2008). We employed a uniqueness constraint on role labels (Chang et al., 2007), preventing some of them from being assigned to more than one argument in the same predicate, which appears to be more reliable in a low-resource setting we consider than the reranker the original system employed. The constraint is enforced in the monolingual model inference using a beam-search approximation with the beam size of 10. The label uniqueness information was derived from the training sets. 321 3.4 The Projection Model Each projection model is realized by a single linear classifier applied to each argument pair independently. It relies on features derived from the source semantic r</context>
</contexts>
<marker>Chang, Ratinov, Roth, 2007</marker>
<rawString>M.W. Chang, L. Ratinov, and D. Roth. 2007. Guiding semi-supervision with constraint-driven learning. Urbana, 51:61801.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pi-Chuan Chang</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>Optimizing chinese word segmentation for machine translation performance.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation, StatMT ’08,</booktitle>
<pages>224--232</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="16125" citStr="Chang et al., 2008" startWordPosition="2667" endWordPosition="2670">he projection version, although it lacks the optimality guarantees. 3 Experimental Setup We evaluate our approach on four language pairs, namely English vs German, Spanish, Czech and Chinese, which we will denote en-de, en-es, en-cz and en-zh respectively. 3.1 Parallel Data The parallel data for the first three language pairs is drawn from Europarl v6 (Koehn, 2005) and from MultiUN (Eisele and Chen, 2010) for EnglishChinese. We applied Stanford Tokenizer for English, tokenizer scripts (Koehn, 2005) provided with the Europarl corpus to German, Spanish and Czech, and Stanford Chinese Segmenter (Chang et al., 2008) to Chinese, then performed POS-tagging, morphology tagging (where applicable) and dependency parsing using MATE-tools (Bohnet, 2010). Word alignments were acquired using GIZA++ (Och and Ney, 2003) with its standard settings. Predicate identification on the parallel data was done using the supervised classifiers of the monolingual SRL systems, except for German, where a simple heuristic had to be used instead, as only some of the predicates are marked in the training data, which makes it hard to train a supervised classifier. Following van der Plas et al. (2011), we then retain only those sent</context>
</contexts>
<marker>Chang, Galley, Manning, 2008</marker>
<rawString>Pi-Chuan Chang, Michel Galley, and Christopher D. Manning. 2008. Optimizing chinese word segmentation for machine translation performance. In Proceedings of the Third Workshop on Statistical Machine Translation, StatMT ’08, pages 224–232, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Syntactic transfer using a bilingual lexicon.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1--11</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="30903" citStr="Durrett et al., 2012" startWordPosition="5121" endWordPosition="5124">t makes, compared to a regular one (table 4). 5 Related Work There is a number of approaches to semi-supervised semantic role labeling, and most suggest that some external supervision is required for such approaches to work (He and Gildea, 2006), such as measures of syntactic and semantic similarity (F¨urstenau and Lapata, 2009) or external confidence measures (Goldwasser et al., 2011). The alternative we propose is primarily motivated by the research on annotation projection (Pad´o and Lapata, 2009; van der Plas et al., 2011; Annesi and Basili, 2010; Naseem et al., 2012) and direct transfer (Durrett et al., 2012; Søgaard, 2011; Lopez et al., 2008; McDonald et al., 2011). The key difference of the present approach compared to annotation projection is that we assume 324 INIT SELF JOINT ASELF DINIT en-cz* 61.11 60.68 72.49 11.81 11.38 en-cz 62.45 62.15 70.19 8.04 7.74 en-de* 66.81 63.96 76.78 12.82 9.97 en-de 70.39 68.34 79.22 10.88 8.84 en-es 64.20 64.51 75.43 10.92 11.23 en-zh 75.80 73.52 76.75 3.22 0.94 cz-en* 66.82 63.95 70.75 6.80 3.93 cz-en 74.93 71.60 79.70 8.10 4.76 de-en* 66.82 63.58 69.46 5.88 2.64 de-en 74.93 71.31 77.34 6.03 2.41 es-en* 66.82 63.95 69.92 5.97 3.10 es-en 74.93 71.47 79.55 8.0</context>
</contexts>
<marker>Durrett, Pauls, Klein, 2012</marker>
<rawString>Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syntactic transfer using a bilingual lexicon. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1–11, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Andreas Eisele</author>
<author>Yu Chen</author>
</authors>
<title>MultiUN: A multilingual corpus from united nation documents.</title>
<date>2010</date>
<booktitle>Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10),</booktitle>
<editor>In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors,</editor>
<location>Valletta, Malta,</location>
<contexts>
<context position="15914" citStr="Eisele and Chen, 2010" startWordPosition="2635" endWordPosition="2638"> the projection setup. Moreover, the inference procedure for projection setup is δi,rI(ri= r) st 320 a special case of this one with -y,(n) set to 0. The algorithm also demonstrates convergence similar to that of the projection version, although it lacks the optimality guarantees. 3 Experimental Setup We evaluate our approach on four language pairs, namely English vs German, Spanish, Czech and Chinese, which we will denote en-de, en-es, en-cz and en-zh respectively. 3.1 Parallel Data The parallel data for the first three language pairs is drawn from Europarl v6 (Koehn, 2005) and from MultiUN (Eisele and Chen, 2010) for EnglishChinese. We applied Stanford Tokenizer for English, tokenizer scripts (Koehn, 2005) provided with the Europarl corpus to German, Spanish and Czech, and Stanford Chinese Segmenter (Chang et al., 2008) to Chinese, then performed POS-tagging, morphology tagging (where applicable) and dependency parsing using MATE-tools (Bohnet, 2010). Word alignments were acquired using GIZA++ (Och and Ney, 2003) with its standard settings. Predicate identification on the parallel data was done using the supervised classifiers of the monolingual SRL systems, except for German, where a simple heuristic</context>
</contexts>
<marker>Eisele, Chen, 2010</marker>
<rawString>Andreas Eisele and Yu Chen. 2010. MultiUN: A multilingual corpus from united nation documents. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10), Valletta, Malta, May. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Xiang-Rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="19074" citStr="Fan et al., 2008" startWordPosition="3150" endWordPosition="3153">a, 2005). As reconstructing the constituents from the dependency tree is nontrivial (Hwang et al., 2010), we are using a heuristic to address the most common version of this problem, i.e. a preposition or an auxiliary verb being an argument head. In such a case we also take into account any alignment links involving the head’s immediate descendants. 3.3 Implementation Our system is based on that of Bj¨orkelund et al. (2009). It is a pipeline system comprised of a set of binary or multiclass linear classifiers. Both here and in the projection model, the classifiers are trained using Liblinear (Fan et al., 2008). We employed a uniqueness constraint on role labels (Chang et al., 2007), preventing some of them from being assigned to more than one argument in the same predicate, which appears to be more reliable in a low-resource setting we consider than the reranker the original system employed. The constraint is enforced in the monolingual model inference using a beam-search approximation with the beam size of 10. The label uniqueness information was derived from the training sets. 321 3.4 The Projection Model Each projection model is realized by a single linear classifier applied to each argument pai</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Francis</author>
<author>H Kucera</author>
</authors>
<title>Computing Analysis of Present-day American English.</title>
<date>1967</date>
<publisher>Brown University Press,</publisher>
<location>Providence, RI.</location>
<contexts>
<context position="23962" citStr="Francis and Kucera, 1967" startWordPosition="3969" endWordPosition="3972">on data outside the original domain, so we consider the out-of-domain (OOD) test sets as provided for the CoNLL Shared Task 2009 where available. Perhaps the most interesting one of these is the German OOD test set, which is drawn from Europarl (as is the parallel data we use). It was originally annotated with syntactic dependency trees and se322 mantic structure in the SALSA format (Burchardt et al., 2006) for Pad´o and Lapata (2005), and then converted into a PropBank-like form for the CoNLL Shared Task 2009 (Hajiˇc et al., 2009). The OOD test set for English is drawn from the Brown corpus (Francis and Kucera, 1967) and the one for Czech – from a Czech translation of Wall Street Journal articles (Hajiˇc et al., 2012). 4 Evaluation The first question we are interested in is how the joint inference affects the quality of the automatically obtained annotations on the parallel data. To answer this, we will run the monolingual models independently and jointly, then train models on the output of these two procedures and compare their performance on a test set. Note that we do not add the initial training data at this point, so the initial model scores are provided for reference, rather than as a baseline. 4.1 </context>
</contexts>
<marker>Francis, Kucera, 1967</marker>
<rawString>S. Francis and H. Kucera. 1967. Computing Analysis of Present-day American English. Brown University Press, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lea Frermann</author>
<author>Francis Bond</author>
</authors>
<title>Cross-lingual parse disambiguation based on semantic correspondence.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>125--129</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="32649" citStr="Frermann and Bond (2012)" startWordPosition="5401" endWordPosition="5404">oach can be seen as similar to cotraining (Blum and Mitchell, 1998), other applications of which to NLP are too numerous to list here. Most closely related is the joint inference in Zhuang and Zong (2010), the main difference being that it relies on a manually annotated parallel corpus, aligned on the argument level, and evaluates only the inference procedure and only on in-domain data. Other related approaches include Kim et al. (2010), where a cross-lingual transfer of relations is performed (which basically represent parts of the predicate-argument structure considered by SRL methods), and Frermann and Bond (2012), where semantic structure matching is used to rank HPSG parses for parallel sentences. Unsupervised semantic role labeling methods (Lang and Lapata, 2010; Lang and Lapata, 2011; Titov and Klementiev, 2012a; Lorenzo and Cerisara, 2012) present an alternative to the crosslingual information propagation approaches such as ours, and at least one the methods in this area also makes use of parallel data (Titov and Klementiev, 2012b). Conclusions We have presented an approach to information transfer between SRL systems for different language pairs using parallel data. The task proves challenging due</context>
</contexts>
<marker>Frermann, Bond, 2012</marker>
<rawString>Lea Frermann and Francis Bond. 2012. Cross-lingual parse disambiguation based on semantic correspondence. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 125–129, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hagen F¨urstenau</author>
<author>Mirella Lapata</author>
</authors>
<title>Graph alignment for semi-supervised semantic role labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>11--20</pages>
<marker>F¨urstenau, Lapata, 2009</marker>
<rawString>Hagen F¨urstenau and Mirella Lapata. 2009. Graph alignment for semi-supervised semantic role labeling. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 11– 20, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Goldwasser</author>
<author>R Reichart</author>
<author>J Clarke</author>
<author>D Roth</author>
</authors>
<title>Confidence driven unsupervised semantic parsing.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="30671" citStr="Goldwasser et al., 2011" startWordPosition="5082" endWordPosition="5086">more accurate monolingual models on the parallel data – those trained on the full training set, rather than the initial training set used in this particular experiment. We refer to the resulting RCM as oracle and assess the difference it makes, compared to a regular one (table 4). 5 Related Work There is a number of approaches to semi-supervised semantic role labeling, and most suggest that some external supervision is required for such approaches to work (He and Gildea, 2006), such as measures of syntactic and semantic similarity (F¨urstenau and Lapata, 2009) or external confidence measures (Goldwasser et al., 2011). The alternative we propose is primarily motivated by the research on annotation projection (Pad´o and Lapata, 2009; van der Plas et al., 2011; Annesi and Basili, 2010; Naseem et al., 2012) and direct transfer (Durrett et al., 2012; Søgaard, 2011; Lopez et al., 2008; McDonald et al., 2011). The key difference of the present approach compared to annotation projection is that we assume 324 INIT SELF JOINT ASELF DINIT en-cz* 61.11 60.68 72.49 11.81 11.38 en-cz 62.45 62.15 70.19 8.04 7.74 en-de* 66.81 63.96 76.78 12.82 9.97 en-de 70.39 68.34 79.22 10.88 8.84 en-es 64.20 64.51 75.43 10.92 11.23 en</context>
</contexts>
<marker>Goldwasser, Reichart, Clarke, Roth, 2011</marker>
<rawString>D. Goldwasser, R. Reichart, J. Clarke, and D. Roth. 2011. Confidence driven unsupervised semantic parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Good</author>
</authors>
<title>Permutation Tests: A Practical Guide to Resampling Methods for Testing Hypotheses.</title>
<date>2000</date>
<publisher>Springer.</publisher>
<contexts>
<context position="26035" citStr="Good, 2000" startWordPosition="4316" endWordPosition="4317"> highlighted in bold. In table 1, we present the accuracy of the model trained on the output of the joint inference (JOINT) against that of the self-training baseline (SELF). The DSELF column contains the difference between the two. Note that the SELF model is trained on the parallel data automatically annotated using monolingual SRL models (not mixed with the initial training set), since we are interested in the effect of joint inference on the quality of the annotation obtained. Where the improvement is positive and statistically significant with p &lt; 0.005 according to the permutation test (Good, 2000), they are highlighted in bold. We can see that the refined model (JOINT) outperforms the self-training baseline in most cases by a moderate, but statistically significant margin, which indicates that the joint inference does improve the quality of annotations on the parallel corpus. The slightly higher improvement on the German OOD test set supports our hypothesis that the procedure enhances the performance of the model on parallel data, as the data for this test set is also drawn from the Europarl corpus. The improvement over the initial model (DINIT) in this case is statistically significan</context>
</contexts>
<marker>Good, 2000</marker>
<rawString>P. Good. 2000. Permutation Tests: A Practical Guide to Resampling Methods for Testing Hypotheses. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hajic</author>
<author>J Panevov´a</author>
<author>E Hajicov´a</author>
<author>P Sgall</author>
<author>P Pajas</author>
<author>J Step´anek</author>
<author>J Havelka</author>
<author>M Mikulov´a</author>
<author>Z Zabokrtsk`y</author>
<author>M Sevcfkov´a-Razfmov´a</author>
</authors>
<title>Prague dependency treebank 2.0.</title>
<date>2006</date>
<publisher>LDC.</publisher>
<marker>Hajic, Panevov´a, Hajicov´a, Sgall, Pajas, Step´anek, Havelka, Mikulov´a, Zabokrtsk`y, Sevcfkov´a-Razfmov´a, 2006</marker>
<rawString>J. Haji&amp;quot;c, J. Panevov´a, E. Haji&amp;quot;cov´a, P. Sgall, P. Pajas, J. &amp;quot;St&amp;quot;ep´anek, J. Havelka, M. Mikulov´a, Z. &amp;quot;Zabokrtsk`y, and M. &amp;quot;Sev&amp;quot;cfkov´a-Razfmov´a. 2006. Prague dependency treebank 2.0. LDC.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Hajic</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria Ant`onia Martf</author>
<author>Llufs M`arquez</author>
<author>Adam Meyers</author>
<author>Joakim Nivre</author>
<author>Sebastian Pad´o</author>
<author>Jan Step´anek</author>
<author>Pavel Stran´ak</author>
<author>Mihai Surdeanu</author>
<author>Nianwen Xue</author>
<author>Yi Zhang</author>
</authors>
<title>The CoNLL2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL 2009): Shared Task,</booktitle>
<pages>1--18</pages>
<location>Boulder, Colorado.</location>
<marker>Hajic, Ciaramita, Johansson, Kawahara, Martf, M`arquez, Meyers, Nivre, Pad´o, Step´anek, Stran´ak, Surdeanu, Xue, Zhang, 2009</marker>
<rawString>Jan Haji&amp;quot;c, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Martf, Llufs M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan &amp;quot;St&amp;quot;ep´anek, Pavel Stra&amp;quot;n´ak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The CoNLL2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL 2009): Shared Task, pages 1–18, Boulder, Colorado.</rawString>
</citation>
<citation valid="false">
<title>Ure&amp;quot;sov´a, and Zden&amp;quot;ek &amp;quot;Zabokrtsk´y.</title>
<date>2012</date>
<booktitle>Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<editor>Jan Haji&amp;quot;c, Eva Haji&amp;quot;cov´a, Jarmila Panevov´a, Petr Sgall, Ond&amp;quot;rej Bojar, Silvie Cinkov´a, Eva Fu&amp;quot;c´ıkov´a, Marie Mikulov´a, Petr Pajas, Jan Popelka, Ji&amp;quot;rfSemeck´y, Jana &amp;quot;Sindlerov´a, Jan &amp;quot;St&amp;quot;ep´anek, Josef Toman, Zde&amp;quot;nka</editor>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="32649" citStr="(2012)" startWordPosition="5404" endWordPosition="5404">s similar to cotraining (Blum and Mitchell, 1998), other applications of which to NLP are too numerous to list here. Most closely related is the joint inference in Zhuang and Zong (2010), the main difference being that it relies on a manually annotated parallel corpus, aligned on the argument level, and evaluates only the inference procedure and only on in-domain data. Other related approaches include Kim et al. (2010), where a cross-lingual transfer of relations is performed (which basically represent parts of the predicate-argument structure considered by SRL methods), and Frermann and Bond (2012), where semantic structure matching is used to rank HPSG parses for parallel sentences. Unsupervised semantic role labeling methods (Lang and Lapata, 2010; Lang and Lapata, 2011; Titov and Klementiev, 2012a; Lorenzo and Cerisara, 2012) present an alternative to the crosslingual information propagation approaches such as ours, and at least one the methods in this area also makes use of parallel data (Titov and Klementiev, 2012b). Conclusions We have presented an approach to information transfer between SRL systems for different language pairs using parallel data. The task proves challenging due</context>
</contexts>
<marker>2012</marker>
<rawString>Jan Haji&amp;quot;c, Eva Haji&amp;quot;cov´a, Jarmila Panevov´a, Petr Sgall, Ond&amp;quot;rej Bojar, Silvie Cinkov´a, Eva Fu&amp;quot;c´ıkov´a, Marie Mikulov´a, Petr Pajas, Jan Popelka, Ji&amp;quot;rfSemeck´y, Jana &amp;quot;Sindlerov´a, Jan &amp;quot;St&amp;quot;ep´anek, Josef Toman, Zde&amp;quot;nka Ure&amp;quot;sov´a, and Zden&amp;quot;ek &amp;quot;Zabokrtsk´y. 2012. Announcing prague czech-english dependency treebank 2.0. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Mehmet U&amp;quot;gur Do&amp;quot;gan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), Istanbul, Turkey, May. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shan He</author>
<author>Daniel Gildea</author>
</authors>
<title>Self-training and co-training for semantic role labeling: Primary report.</title>
<date>2006</date>
<tech>Technical report,</tech>
<institution>University of Rochester.</institution>
<contexts>
<context position="30528" citStr="He and Gildea, 2006" startWordPosition="5061" endWordPosition="5064">estimate the potential impact of such improvements, we simulate a better informed projection model, giving it access to the predictions of more accurate monolingual models on the parallel data – those trained on the full training set, rather than the initial training set used in this particular experiment. We refer to the resulting RCM as oracle and assess the difference it makes, compared to a regular one (table 4). 5 Related Work There is a number of approaches to semi-supervised semantic role labeling, and most suggest that some external supervision is required for such approaches to work (He and Gildea, 2006), such as measures of syntactic and semantic similarity (F¨urstenau and Lapata, 2009) or external confidence measures (Goldwasser et al., 2011). The alternative we propose is primarily motivated by the research on annotation projection (Pad´o and Lapata, 2009; van der Plas et al., 2011; Annesi and Basili, 2010; Naseem et al., 2012) and direct transfer (Durrett et al., 2012; Søgaard, 2011; Lopez et al., 2008; McDonald et al., 2011). The key difference of the present approach compared to annotation projection is that we assume 324 INIT SELF JOINT ASELF DINIT en-cz* 61.11 60.68 72.49 11.81 11.38 </context>
</contexts>
<marker>He, Gildea, 2006</marker>
<rawString>Shan He and Daniel Gildea. 2006. Self-training and co-training for semantic role labeling: Primary report. Technical report, University of Rochester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jena D Hwang</author>
<author>Rodney D Nielsen</author>
<author>Martha Palmer</author>
</authors>
<title>Towards a domain independent semantics: Enhancing semantic representation with construction grammar.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT Workshop on Extracting and Using Constructions in Computational Linguistics,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="18561" citStr="Hwang et al., 2010" startWordPosition="3062" endWordPosition="3065"> and the test set. The secondary (“out-of-domain”) test sets are preserved as they are. In dependency-based SRL, only heads of syntactic constituents are marked with semantic roles. The heads of corresponding arguments may or may not align, however, even if the arguments are lexically very similar, because their syntactic structure may differ. In general, one would have to identify the whole phrase for each argument and take into account the links between constituents, rather than single words (Pad´o and Lapata, 2005). As reconstructing the constituents from the dependency tree is nontrivial (Hwang et al., 2010), we are using a heuristic to address the most common version of this problem, i.e. a preposition or an auxiliary verb being an argument head. In such a case we also take into account any alignment links involving the head’s immediate descendants. 3.3 Implementation Our system is based on that of Bj¨orkelund et al. (2009). It is a pipeline system comprised of a set of binary or multiclass linear classifiers. Both here and in the projection model, the classifiers are trained using Liblinear (Fan et al., 2008). We employed a uniqueness constraint on role labels (Chang et al., 2007), preventing s</context>
</contexts>
<marker>Hwang, Nielsen, Palmer, 2010</marker>
<rawString>Jena D. Hwang, Rodney D. Nielsen, and Martha Palmer. 2010. Towards a domain independent semantics: Enhancing semantic representation with construction grammar. In Proceedings of the NAACL HLT Workshop on Extracting and Using Constructions in Computational Linguistics, pages 1–8, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seokhwan Kim</author>
<author>Minwoo Jeong</author>
<author>Jonghoon Lee</author>
<author>Gary Geunbae Lee</author>
</authors>
<title>A cross-lingual annotation projection approach for relation detection.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10,</booktitle>
<pages>564--571</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="32465" citStr="Kim et al. (2010)" startWordPosition="5376" endWordPosition="5379"> some amount of training data for the target language, possibly using a different inventory of semantic roles. As mentioned previously, from the training point of view this approach can be seen as similar to cotraining (Blum and Mitchell, 1998), other applications of which to NLP are too numerous to list here. Most closely related is the joint inference in Zhuang and Zong (2010), the main difference being that it relies on a manually annotated parallel corpus, aligned on the argument level, and evaluates only the inference procedure and only on in-domain data. Other related approaches include Kim et al. (2010), where a cross-lingual transfer of relations is performed (which basically represent parts of the predicate-argument structure considered by SRL methods), and Frermann and Bond (2012), where semantic structure matching is used to rank HPSG parses for parallel sentences. Unsupervised semantic role labeling methods (Lang and Lapata, 2010; Lang and Lapata, 2011; Titov and Klementiev, 2012a; Lorenzo and Cerisara, 2012) present an alternative to the crosslingual information propagation approaches such as ours, and at least one the methods in this area also makes use of parallel data (Titov and Kle</context>
</contexts>
<marker>Kim, Jeong, Lee, Lee, 2010</marker>
<rawString>Seokhwan Kim, Minwoo Jeong, Jonghoon Lee, and Gary Geunbae Lee. 2010. A cross-lingual annotation projection approach for relation detection. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10, pages 564–571, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Conference Proceedings: the tenth Machine Translation Summit,</booktitle>
<pages>79--86</pages>
<location>Phuket, Thailand. AAMT, AAMT.</location>
<contexts>
<context position="15873" citStr="Koehn, 2005" startWordPosition="2630" endWordPosition="2631">e same RCM implementation as in the projection setup. Moreover, the inference procedure for projection setup is δi,rI(ri= r) st 320 a special case of this one with -y,(n) set to 0. The algorithm also demonstrates convergence similar to that of the projection version, although it lacks the optimality guarantees. 3 Experimental Setup We evaluate our approach on four language pairs, namely English vs German, Spanish, Czech and Chinese, which we will denote en-de, en-es, en-cz and en-zh respectively. 3.1 Parallel Data The parallel data for the first three language pairs is drawn from Europarl v6 (Koehn, 2005) and from MultiUN (Eisele and Chen, 2010) for EnglishChinese. We applied Stanford Tokenizer for English, tokenizer scripts (Koehn, 2005) provided with the Europarl corpus to German, Spanish and Czech, and Stanford Chinese Segmenter (Chang et al., 2008) to Chinese, then performed POS-tagging, morphology tagging (where applicable) and dependency parsing using MATE-tools (Bohnet, 2010). Word alignments were acquired using GIZA++ (Och and Ney, 2003) with its standard settings. Predicate identification on the parallel data was done using the supervised classifiers of the monolingual SRL systems, ex</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Conference Proceedings: the tenth Machine Translation Summit, pages 79–86, Phuket, Thailand. AAMT, AAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Lang</author>
<author>Mirella Lapata</author>
</authors>
<title>Unsupervised induction of semantic roles.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>939--947</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="32803" citStr="Lang and Lapata, 2010" startWordPosition="5424" endWordPosition="5427">s the joint inference in Zhuang and Zong (2010), the main difference being that it relies on a manually annotated parallel corpus, aligned on the argument level, and evaluates only the inference procedure and only on in-domain data. Other related approaches include Kim et al. (2010), where a cross-lingual transfer of relations is performed (which basically represent parts of the predicate-argument structure considered by SRL methods), and Frermann and Bond (2012), where semantic structure matching is used to rank HPSG parses for parallel sentences. Unsupervised semantic role labeling methods (Lang and Lapata, 2010; Lang and Lapata, 2011; Titov and Klementiev, 2012a; Lorenzo and Cerisara, 2012) present an alternative to the crosslingual information propagation approaches such as ours, and at least one the methods in this area also makes use of parallel data (Titov and Klementiev, 2012b). Conclusions We have presented an approach to information transfer between SRL systems for different language pairs using parallel data. The task proves challenging due to non-trivial mapping between the role labels used in different SRL annotation schemes and the nature of parallel data – the difference in domains and t</context>
</contexts>
<marker>Lang, Lapata, 2010</marker>
<rawString>Joel Lang and Mirella Lapata. 2010. Unsupervised induction of semantic roles. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 939–947, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Lang</author>
<author>Mirella Lapata</author>
</authors>
<title>Unsupervised semantic role induction via split-merge clustering.</title>
<date>2011</date>
<booktitle>In Proc. of Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="32826" citStr="Lang and Lapata, 2011" startWordPosition="5428" endWordPosition="5431">n Zhuang and Zong (2010), the main difference being that it relies on a manually annotated parallel corpus, aligned on the argument level, and evaluates only the inference procedure and only on in-domain data. Other related approaches include Kim et al. (2010), where a cross-lingual transfer of relations is performed (which basically represent parts of the predicate-argument structure considered by SRL methods), and Frermann and Bond (2012), where semantic structure matching is used to rank HPSG parses for parallel sentences. Unsupervised semantic role labeling methods (Lang and Lapata, 2010; Lang and Lapata, 2011; Titov and Klementiev, 2012a; Lorenzo and Cerisara, 2012) present an alternative to the crosslingual information propagation approaches such as ours, and at least one the methods in this area also makes use of parallel data (Titov and Klementiev, 2012b). Conclusions We have presented an approach to information transfer between SRL systems for different language pairs using parallel data. The task proves challenging due to non-trivial mapping between the role labels used in different SRL annotation schemes and the nature of parallel data – the difference in domains and the limited accuracy of </context>
</contexts>
<marker>Lang, Lapata, 2011</marker>
<rawString>Joel Lang and Mirella Lapata. 2011. Unsupervised semantic role induction via split-merge clustering. In Proc. of Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
<author>Daniel Zeman</author>
<author>Michael Nossal</author>
<author>Philip Resnik</author>
<author>Rebecca Hwa</author>
</authors>
<title>Cross-Language Parser Adaptation between Related Languages.</title>
<date>2008</date>
<booktitle>In IJCNLP-08 Workshop on NLP for Less Privileged Languages,</booktitle>
<pages>35--42</pages>
<location>Hyderabad, India,</location>
<contexts>
<context position="30938" citStr="Lopez et al., 2008" startWordPosition="5127" endWordPosition="5130">able 4). 5 Related Work There is a number of approaches to semi-supervised semantic role labeling, and most suggest that some external supervision is required for such approaches to work (He and Gildea, 2006), such as measures of syntactic and semantic similarity (F¨urstenau and Lapata, 2009) or external confidence measures (Goldwasser et al., 2011). The alternative we propose is primarily motivated by the research on annotation projection (Pad´o and Lapata, 2009; van der Plas et al., 2011; Annesi and Basili, 2010; Naseem et al., 2012) and direct transfer (Durrett et al., 2012; Søgaard, 2011; Lopez et al., 2008; McDonald et al., 2011). The key difference of the present approach compared to annotation projection is that we assume 324 INIT SELF JOINT ASELF DINIT en-cz* 61.11 60.68 72.49 11.81 11.38 en-cz 62.45 62.15 70.19 8.04 7.74 en-de* 66.81 63.96 76.78 12.82 9.97 en-de 70.39 68.34 79.22 10.88 8.84 en-es 64.20 64.51 75.43 10.92 11.23 en-zh 75.80 73.52 76.75 3.22 0.94 cz-en* 66.82 63.95 70.75 6.80 3.93 cz-en 74.93 71.60 79.70 8.10 4.76 de-en* 66.82 63.58 69.46 5.88 2.64 de-en 74.93 71.31 77.34 6.03 2.41 es-en* 66.82 63.95 69.92 5.97 3.10 es-en 74.93 71.47 79.55 8.08 4.62 zh-en* 66.82 64.51 67.19 2.6</context>
</contexts>
<marker>Lopez, Zeman, Nossal, Resnik, Hwa, 2008</marker>
<rawString>Adam Lopez, Daniel Zeman, Michael Nossal, Philip Resnik, and Rebecca Hwa. 2008. Cross-Language Parser Adaptation between Related Languages. In IJCNLP-08 Workshop on NLP for Less Privileged Languages, pages 35–42, Hyderabad, India, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alejandra Lorenzo</author>
<author>Christophe Cerisara</author>
</authors>
<title>Unsupervised frame based semantic role induction: application to french and english.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACL 2012 Joint Workshop on Statistical Parsing and Semantic Processing of Morphologically Rich Languages,</booktitle>
<pages>30--35</pages>
<location>Jeju, Republic of</location>
<contexts>
<context position="32884" citStr="Lorenzo and Cerisara, 2012" startWordPosition="5436" endWordPosition="5439">that it relies on a manually annotated parallel corpus, aligned on the argument level, and evaluates only the inference procedure and only on in-domain data. Other related approaches include Kim et al. (2010), where a cross-lingual transfer of relations is performed (which basically represent parts of the predicate-argument structure considered by SRL methods), and Frermann and Bond (2012), where semantic structure matching is used to rank HPSG parses for parallel sentences. Unsupervised semantic role labeling methods (Lang and Lapata, 2010; Lang and Lapata, 2011; Titov and Klementiev, 2012a; Lorenzo and Cerisara, 2012) present an alternative to the crosslingual information propagation approaches such as ours, and at least one the methods in this area also makes use of parallel data (Titov and Klementiev, 2012b). Conclusions We have presented an approach to information transfer between SRL systems for different language pairs using parallel data. The task proves challenging due to non-trivial mapping between the role labels used in different SRL annotation schemes and the nature of parallel data – the difference in domains and the limited accuracy of the preprocessing tools. We observe consistent improvement</context>
</contexts>
<marker>Lorenzo, Cerisara, 2012</marker>
<rawString>Alejandra Lorenzo and Christophe Cerisara. 2012. Unsupervised frame based semantic role induction: application to french and english. In Proceedings of the ACL 2012 Joint Workshop on Statistical Parsing and Semantic Processing of Morphologically Rich Languages, pages 30–35, Jeju, Republic of Korea, July 12. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Slav Petrov</author>
<author>Keith Hall</author>
</authors>
<title>Multi-source transfer of delexicalized dependency parsers.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>62--72</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1662" citStr="McDonald et al., 2011" startWordPosition="247" endWordPosition="250">the last decade depended crucially on the availability of annotated resources for their training. And while sizable resources for most standard tasks are only available for a few languages, the human effort required to achieve reasonable performance on such tasks for other languages may be significantly reduced by leveraging existing resources and the similarities between languages. This idea has lead to the development of crosslingual annotation projection approaches, which make use of parallel corpora (Pad´o and Lapata, 2009), as well as attempts to adapt models directly to other languages (McDonald et al., 2011). In this paper we consider correspondences between SRL structures in translated sentences from a different perspective. Most cross-lingual annotation projection approaches transfer the source language annotation scheme to the target language without modification, which makes it hard to combine their output with existing target language resources, as annotation schemes may vary significantly. We instead address the problem of information transfer between two existing annotation schemes (figure 1) for a pair of languages using an intermediate model of role correspondence (RCM). An RCM models th</context>
<context position="30962" citStr="McDonald et al., 2011" startWordPosition="5131" endWordPosition="5134">ork There is a number of approaches to semi-supervised semantic role labeling, and most suggest that some external supervision is required for such approaches to work (He and Gildea, 2006), such as measures of syntactic and semantic similarity (F¨urstenau and Lapata, 2009) or external confidence measures (Goldwasser et al., 2011). The alternative we propose is primarily motivated by the research on annotation projection (Pad´o and Lapata, 2009; van der Plas et al., 2011; Annesi and Basili, 2010; Naseem et al., 2012) and direct transfer (Durrett et al., 2012; Søgaard, 2011; Lopez et al., 2008; McDonald et al., 2011). The key difference of the present approach compared to annotation projection is that we assume 324 INIT SELF JOINT ASELF DINIT en-cz* 61.11 60.68 72.49 11.81 11.38 en-cz 62.45 62.15 70.19 8.04 7.74 en-de* 66.81 63.96 76.78 12.82 9.97 en-de 70.39 68.34 79.22 10.88 8.84 en-es 64.20 64.51 75.43 10.92 11.23 en-zh 75.80 73.52 76.75 3.22 0.94 cz-en* 66.82 63.95 70.75 6.80 3.93 cz-en 74.93 71.60 79.70 8.10 4.76 de-en* 66.82 63.58 69.46 5.88 2.64 de-en 74.93 71.31 77.34 6.03 2.41 es-en* 66.82 63.95 69.92 5.97 3.10 es-en 74.93 71.47 79.55 8.08 4.62 zh-en* 66.82 64.51 67.19 2.68 0.37 zh-en 74.93 72.26</context>
</contexts>
<marker>McDonald, Petrov, Hall, 2011</marker>
<rawString>Ryan McDonald, Slav Petrov, and Keith Hall. 2011. Multi-source transfer of delexicalized dependency parsers. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 62–72, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Merlo</author>
<author>Matthias Leybold</author>
</authors>
<title>Automatic distinction of arguments and modifiers: the case of prepositional phrases.</title>
<date>2001</date>
<booktitle>In Proceedings of the Fifth Computational Natural Language Learning Workshop (CoNLL-2001),</booktitle>
<pages>121--128</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="8359" citStr="Merlo and Leybold, 2001" startWordPosition="1357" endWordPosition="1360">nding on the formalism used. In case of FrameNet (Baker et al., 1998), for example, the interpretation of a semantic role (frame element) is explicitly provided for each separate frame, so a frame and a frame element label together describe the semantics of an argument. PropBank (Palmer et al., 2005a) follows a mixed strategy – the labels for a relatively small set of core roles are numbered and their interpretations are provided separately for each predicate (although those of the first two roles, A0 and A1, consistently denote what is known as ProtoAgent and Proto-Patient), while modifiers (Merlo and Leybold, 2001) bear labels that are interpreted consistently across all predicates. Other resources, such as Prague Dependency Treebank (Hajiˇc et al., 2006), use a single set of semantic roles (functors), which are interpretable across different predicates. From the standpoint of defining the semantic similarity of parallel sentences, the important implication is that we cannot assume that the corresponding arguments should bear the same label, even if the annotation schemes used are compatible (Zhuang and Zong, 2010). Nor can we write down a single mapping between the roles that will be valid across diffe</context>
</contexts>
<marker>Merlo, Leybold, 2001</marker>
<rawString>Paola Merlo and Matthias Leybold. 2001. Automatic distinction of arguments and modifiers: the case of prepositional phrases. In Proceedings of the Fifth Computational Natural Language Learning Workshop (CoNLL-2001), pages 121–128, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tahira Naseem</author>
<author>Regina Barzilay</author>
<author>Amir Globerson</author>
</authors>
<title>Selective sharing for multilingual dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>629--637</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="30861" citStr="Naseem et al., 2012" startWordPosition="5114" endWordPosition="5117"> RCM as oracle and assess the difference it makes, compared to a regular one (table 4). 5 Related Work There is a number of approaches to semi-supervised semantic role labeling, and most suggest that some external supervision is required for such approaches to work (He and Gildea, 2006), such as measures of syntactic and semantic similarity (F¨urstenau and Lapata, 2009) or external confidence measures (Goldwasser et al., 2011). The alternative we propose is primarily motivated by the research on annotation projection (Pad´o and Lapata, 2009; van der Plas et al., 2011; Annesi and Basili, 2010; Naseem et al., 2012) and direct transfer (Durrett et al., 2012; Søgaard, 2011; Lopez et al., 2008; McDonald et al., 2011). The key difference of the present approach compared to annotation projection is that we assume 324 INIT SELF JOINT ASELF DINIT en-cz* 61.11 60.68 72.49 11.81 11.38 en-cz 62.45 62.15 70.19 8.04 7.74 en-de* 66.81 63.96 76.78 12.82 9.97 en-de 70.39 68.34 79.22 10.88 8.84 en-es 64.20 64.51 75.43 10.92 11.23 en-zh 75.80 73.52 76.75 3.22 0.94 cz-en* 66.82 63.95 70.75 6.80 3.93 cz-en 74.93 71.60 79.70 8.10 4.76 de-en* 66.82 63.58 69.46 5.88 2.64 de-en 74.93 71.31 77.34 6.03 2.41 es-en* 66.82 63.95 6</context>
</contexts>
<marker>Naseem, Barzilay, Globerson, 2012</marker>
<rawString>Tahira Naseem, Regina Barzilay, and Amir Globerson. 2012. Selective sharing for multilingual dependency parsing. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 629–637, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="16322" citStr="Och and Ney, 2003" startWordPosition="2694" endWordPosition="2697">we will denote en-de, en-es, en-cz and en-zh respectively. 3.1 Parallel Data The parallel data for the first three language pairs is drawn from Europarl v6 (Koehn, 2005) and from MultiUN (Eisele and Chen, 2010) for EnglishChinese. We applied Stanford Tokenizer for English, tokenizer scripts (Koehn, 2005) provided with the Europarl corpus to German, Spanish and Czech, and Stanford Chinese Segmenter (Chang et al., 2008) to Chinese, then performed POS-tagging, morphology tagging (where applicable) and dependency parsing using MATE-tools (Bohnet, 2010). Word alignments were acquired using GIZA++ (Och and Ney, 2003) with its standard settings. Predicate identification on the parallel data was done using the supervised classifiers of the monolingual SRL systems, except for German, where a simple heuristic had to be used instead, as only some of the predicates are marked in the training data, which makes it hard to train a supervised classifier. Following van der Plas et al. (2011), we then retain only those sentences where all identified predicates were aligned. In the experiments we used 50 thousand predicate pairs in each case, as increasing the amount further did not yield noticeable benefits, while in</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Crosslinguistic projection of role-semantic information.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>859--866</pages>
<location>Vancouver, British Columbia, Canada.</location>
<marker>Pad´o, Lapata, 2005</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2005. Crosslinguistic projection of role-semantic information. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 859–866, Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Cross-lingual annotation projection for semantic roles.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>36--307</pages>
<marker>Pad´o, Lapata, 2009</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2009. Cross-lingual annotation projection for semantic roles. Journal of Artificial Intelligence Research, 36:307–340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<pages>105</pages>
<contexts>
<context position="2823" citStr="Palmer et al., 2005" startWordPosition="429" endWordPosition="432">diate model of role correspondence (RCM). An RCM models the probability of a pair of corresponding arguments being assigned a certain pair of roles. We then use it to guide a pair of monolingual models toward compatible predictions on parallel data in order to extend the coverage and/or accuracy of one or both models. Figure 1: Role correspondence in parallel sentences, an example. The notion of compatibility here is highly nontrivial, even for sentences translated as close to the original as possible. Zhuang and Zong (2010), for example, observe that in the English-Chinese parallel PropBank (Palmer et al., 2005b) corresponding arguments often bear different labels, even though the same inventory of semantic roles is used for both A1 AM-NEG AM-LOC Romanian is not taught in their schools . Ve školách se neuči rumunsky . LOC PAT 317 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task, pages 317–327, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics languages and the annotation guidelines are similar. When different annotation schemes are considered, the problem is further complicated b</context>
<context position="8035" citStr="Palmer et al., 2005" startWordPosition="1305" endWordPosition="1308">f-training baseline model (SELF) is trained on Pβ0, the joint model (JOINT) – on Pβ1 and the combined model (COMB) – on Tβ1 . 2.1 Modeling Role Correspondence It is necessary to distinguish between semantic roles and their interpretation in a particular context. The former can be defined in a variety of 318 ways, depending on the formalism used. In case of FrameNet (Baker et al., 1998), for example, the interpretation of a semantic role (frame element) is explicitly provided for each separate frame, so a frame and a frame element label together describe the semantics of an argument. PropBank (Palmer et al., 2005a) follows a mixed strategy – the labels for a relatively small set of core roles are numbered and their interpretations are provided separately for each predicate (although those of the first two roles, A0 and A1, consistently denote what is known as ProtoAgent and Proto-Patient), while modifiers (Merlo and Leybold, 2001) bear labels that are interpreted consistently across all predicates. Other resources, such as Prague Dependency Treebank (Hajiˇc et al., 2006), use a single set of semantic roles (functors), which are interpretable across different predicates. From the standpoint of defining</context>
<context position="10403" citStr="Palmer et al., 2005" startWordPosition="1691" endWordPosition="1694">e to separately model the probability of a target role, given the source one and the necessary contextual information and vice versa. These two components are referred to as projection models and realized as a pair of linear classifiers. Training such a model in a conventional fashion would require a rather specific kind of dataset, namely a parallel corpus annotated with semantic roles, and assuming the availability of such data would severely limit the applicability of the approach proposed, as, to our knowledge, it is currently only available for two language pairs, namely English-Chinese (Palmer et al., 2005b) and EnglishCzech (Hajiˇc et al., 2012). We instead use the automatically produced annotations on a parallel corpus, effectively enforcing consistency on the role correspondence in the monolingual models’ predictions. 2.2 Joint Inference The joint inference would have been simplest if the arguments were classified independently. This assumption is too restrictive, though, since the interdependencies between the arguments can be used to improve the accuracy of semantic role labeling (Roth and Yih, 2005). Yo no tengo tales preocupaciones arg1-tem arg2-atr A0 A1 We would like to know their name</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005a. The Proposition Bank: An annotated corpus of semantic roles. Computational Linguistics, 31:71– 105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Nianwen Xue</author>
<author>Olga Babko-Malaya</author>
<author>Jinying Chen</author>
<author>Benjamin Snyder</author>
</authors>
<title>A parallel Proposition Bank II for Chinese and English.</title>
<date>2005</date>
<booktitle>In Proceedings of the Workshop on Frontiers in Corpus Annotations II: Pie in the Sky, CorpusAnno ’05,</booktitle>
<pages>61--67</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2823" citStr="Palmer et al., 2005" startWordPosition="429" endWordPosition="432">diate model of role correspondence (RCM). An RCM models the probability of a pair of corresponding arguments being assigned a certain pair of roles. We then use it to guide a pair of monolingual models toward compatible predictions on parallel data in order to extend the coverage and/or accuracy of one or both models. Figure 1: Role correspondence in parallel sentences, an example. The notion of compatibility here is highly nontrivial, even for sentences translated as close to the original as possible. Zhuang and Zong (2010), for example, observe that in the English-Chinese parallel PropBank (Palmer et al., 2005b) corresponding arguments often bear different labels, even though the same inventory of semantic roles is used for both A1 AM-NEG AM-LOC Romanian is not taught in their schools . Ve školách se neuči rumunsky . LOC PAT 317 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task, pages 317–327, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics languages and the annotation guidelines are similar. When different annotation schemes are considered, the problem is further complicated b</context>
<context position="8035" citStr="Palmer et al., 2005" startWordPosition="1305" endWordPosition="1308">f-training baseline model (SELF) is trained on Pβ0, the joint model (JOINT) – on Pβ1 and the combined model (COMB) – on Tβ1 . 2.1 Modeling Role Correspondence It is necessary to distinguish between semantic roles and their interpretation in a particular context. The former can be defined in a variety of 318 ways, depending on the formalism used. In case of FrameNet (Baker et al., 1998), for example, the interpretation of a semantic role (frame element) is explicitly provided for each separate frame, so a frame and a frame element label together describe the semantics of an argument. PropBank (Palmer et al., 2005a) follows a mixed strategy – the labels for a relatively small set of core roles are numbered and their interpretations are provided separately for each predicate (although those of the first two roles, A0 and A1, consistently denote what is known as ProtoAgent and Proto-Patient), while modifiers (Merlo and Leybold, 2001) bear labels that are interpreted consistently across all predicates. Other resources, such as Prague Dependency Treebank (Hajiˇc et al., 2006), use a single set of semantic roles (functors), which are interpretable across different predicates. From the standpoint of defining</context>
<context position="10403" citStr="Palmer et al., 2005" startWordPosition="1691" endWordPosition="1694">e to separately model the probability of a target role, given the source one and the necessary contextual information and vice versa. These two components are referred to as projection models and realized as a pair of linear classifiers. Training such a model in a conventional fashion would require a rather specific kind of dataset, namely a parallel corpus annotated with semantic roles, and assuming the availability of such data would severely limit the applicability of the approach proposed, as, to our knowledge, it is currently only available for two language pairs, namely English-Chinese (Palmer et al., 2005b) and EnglishCzech (Hajiˇc et al., 2012). We instead use the automatically produced annotations on a parallel corpus, effectively enforcing consistency on the role correspondence in the monolingual models’ predictions. 2.2 Joint Inference The joint inference would have been simplest if the arguments were classified independently. This assumption is too restrictive, though, since the interdependencies between the arguments can be used to improve the accuracy of semantic role labeling (Roth and Yih, 2005). Yo no tengo tales preocupaciones arg1-tem arg2-atr A0 A1 We would like to know their name</context>
</contexts>
<marker>Palmer, Xue, Babko-Malaya, Chen, Snyder, 2005</marker>
<rawString>Martha Palmer, Nianwen Xue, Olga Babko-Malaya, Jinying Chen, and Benjamin Snyder. 2005b. A parallel Proposition Bank II for Chinese and English. In Proceedings of the Workshop on Frontiers in Corpus Annotations II: Pie in the Sky, CorpusAnno ’05, pages 61–67, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer S Pradhan</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
</authors>
<title>Towards robust semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="22480" citStr="Pradhan et al., 2008" startWordPosition="3719" endWordPosition="3722">ome sort of newswire text – Wall Street Journal in case of English, Xinhua newswire, Hong Kong news and Sinorama news magazine for Chinese, etc. Parallel data, on the other hand, comes from the proceedings of European Parliament and United Nations, which are quite different. For example, the sentences in the latter domain often start with someone being addressed, either by name or by title, which can hardly be expected to occur as often in a newspaper or a magazine article. As is well-known, the performance of many statistical tools drops significantly outside the domain they were trained on (Pradhan et al., 2008), and the preprocessing and SRL models used here are no exception, which results in relatively low quality of the initial predictions on the parallel text. The low argument identification performance, in particular, is presumably due to inaccurate dependency parses, on which it heavily relies. Several approached have been proposed to improve the accuracy of dependency parsers and other tools on out-of-domain data, but this is beyond the scope of this paper. In some cases (though seldom), sources of parallel data belonging to the same domain as the annotated training data can be obtained. Anoth</context>
</contexts>
<marker>Pradhan, Ward, Martin, 2008</marker>
<rawString>Sameer S. Pradhan, Wayne Ward, and James H. Martin. 2008. Towards robust semantic role labeling. Computational Linguistics, 34(2):289–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>Integer linear programming inference for conditional random fields.</title>
<date>2005</date>
<booktitle>In ICML,</booktitle>
<pages>736--743</pages>
<contexts>
<context position="10912" citStr="Roth and Yih, 2005" startWordPosition="1769" endWordPosition="1772"> knowledge, it is currently only available for two language pairs, namely English-Chinese (Palmer et al., 2005b) and EnglishCzech (Hajiˇc et al., 2012). We instead use the automatically produced annotations on a parallel corpus, effectively enforcing consistency on the role correspondence in the monolingual models’ predictions. 2.2 Joint Inference The joint inference would have been simplest if the arguments were classified independently. This assumption is too restrictive, though, since the interdependencies between the arguments can be used to improve the accuracy of semantic role labeling (Roth and Yih, 2005). Yo no tengo tales preocupaciones arg1-tem arg2-atr A0 A1 We would like to know their names Nos gustaria conocer sus nombres arg2-ben arg1-tem Figure 2: Predicate-specific role mapping. Note that A0 corresponds to art0-agt, art1-tem or art2-ben, depending on the predicate. We assume the existence of a one-to-one map2.2.1 Projection Setup In the projection setup we assume that the model for one of the languages, which we will henceforth refer to as source, is much better informed than the one for the other language, referred to as target, so we only have to propagate the information one way. T</context>
</contexts>
<marker>Roth, Yih, 2005</marker>
<rawString>Dan Roth and Wen-tau Yih. 2005. Integer linear programming inference for conditional random fields. In ICML, pages 736–743.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
</authors>
<title>Data point selection for crosslanguage adaptation of dependency parsers.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11,</booktitle>
<pages>682--686</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="30918" citStr="Søgaard, 2011" startWordPosition="5125" endWordPosition="5126"> regular one (table 4). 5 Related Work There is a number of approaches to semi-supervised semantic role labeling, and most suggest that some external supervision is required for such approaches to work (He and Gildea, 2006), such as measures of syntactic and semantic similarity (F¨urstenau and Lapata, 2009) or external confidence measures (Goldwasser et al., 2011). The alternative we propose is primarily motivated by the research on annotation projection (Pad´o and Lapata, 2009; van der Plas et al., 2011; Annesi and Basili, 2010; Naseem et al., 2012) and direct transfer (Durrett et al., 2012; Søgaard, 2011; Lopez et al., 2008; McDonald et al., 2011). The key difference of the present approach compared to annotation projection is that we assume 324 INIT SELF JOINT ASELF DINIT en-cz* 61.11 60.68 72.49 11.81 11.38 en-cz 62.45 62.15 70.19 8.04 7.74 en-de* 66.81 63.96 76.78 12.82 9.97 en-de 70.39 68.34 79.22 10.88 8.84 en-es 64.20 64.51 75.43 10.92 11.23 en-zh 75.80 73.52 76.75 3.22 0.94 cz-en* 66.82 63.95 70.75 6.80 3.93 cz-en 74.93 71.60 79.70 8.10 4.76 de-en* 66.82 63.58 69.46 5.88 2.64 de-en 74.93 71.31 77.34 6.03 2.41 es-en* 66.82 63.95 69.92 5.97 3.10 es-en 74.93 71.47 79.55 8.08 4.62 zh-en* 6</context>
</contexts>
<marker>Søgaard, 2011</marker>
<rawString>Anders Søgaard. 2011. Data point selection for crosslanguage adaptation of dependency parsers. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11, pages 682–686, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Sontag</author>
<author>Amir Globerson</author>
<author>Tommi Jaakkola</author>
</authors>
<title>Introduction to dual decomposition for inference.</title>
<date>2011</date>
<booktitle>Optimization for Machine Learning.</booktitle>
<editor>In Suvrit Sra, Sebastian Nowozin, and Stephen J. Wright, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="13624" citStr="Sontag et al. (2011)" startWordPosition="2240" endWordPosition="2243">s,ps,pt)+ (1) � � δi,r �I(ri t = r) − I(rist = r)) , i rERt where i indexes aligned argument pairs and I is an indicator function. This is equivalent to minδmaxrt,rstL&apos;(rt, rst, δ) = �minδ maxrtgt(rt, St, pt, δ)+ (2) �maxrstgst(rst, rs, ps, pt, δ) , where gt(rt, St, pt, δ) = �λtft(rt, St, pt) + � δi,rI(ri t = r) i rERt gstp(rst, rs, ps, pt, δ) = (3) � � λstfst(rst,rs,ps,pt) − i rERt are the augmented objectives of the two component models, incorporating bias factors on various possible predictions. The minimization with respect to δ is performed using a subgradient descent algorithm following Sontag et al. (2011). Whenever the method converges, it converges to the global maximum of the sum of the objectives. We found that in our case it reaches a solution within the first 1000 iterations over 99% of the time. 2.2.2 Symmetric Setup If the models have comparable accuracy, the above inference procedure can be extended to perform projection both ways. Formulating this as a dual decomposition problem would require using three separate components, two for the monolingual models and one for the RCM, which would have to make its own predictions for the semantic roles on both sides without conditioning on the </context>
</contexts>
<marker>Sontag, Globerson, Jaakkola, 2011</marker>
<rawString>David Sontag, Amir Globerson, and Tommi Jaakkola. 2011. Introduction to dual decomposition for inference. In Suvrit Sra, Sebastian Nowozin, and Stephen J. Wright, editors, Optimization for Machine Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Alexandre Klementiev</author>
</authors>
<title>A Bayesian approach to unsupervised semantic role induction.</title>
<date>2012</date>
<booktitle>In Proc. of European Chapter of the Association for Computational Linguistics (EACL).</booktitle>
<contexts>
<context position="32854" citStr="Titov and Klementiev, 2012" startWordPosition="5432" endWordPosition="5435">), the main difference being that it relies on a manually annotated parallel corpus, aligned on the argument level, and evaluates only the inference procedure and only on in-domain data. Other related approaches include Kim et al. (2010), where a cross-lingual transfer of relations is performed (which basically represent parts of the predicate-argument structure considered by SRL methods), and Frermann and Bond (2012), where semantic structure matching is used to rank HPSG parses for parallel sentences. Unsupervised semantic role labeling methods (Lang and Lapata, 2010; Lang and Lapata, 2011; Titov and Klementiev, 2012a; Lorenzo and Cerisara, 2012) present an alternative to the crosslingual information propagation approaches such as ours, and at least one the methods in this area also makes use of parallel data (Titov and Klementiev, 2012b). Conclusions We have presented an approach to information transfer between SRL systems for different language pairs using parallel data. The task proves challenging due to non-trivial mapping between the role labels used in different SRL annotation schemes and the nature of parallel data – the difference in domains and the limited accuracy of the preprocessing tools. We </context>
</contexts>
<marker>Titov, Klementiev, 2012</marker>
<rawString>Ivan Titov and Alexandre Klementiev. 2012a. A Bayesian approach to unsupervised semantic role induction. In Proc. of European Chapter of the Association for Computational Linguistics (EACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Alexandre Klementiev</author>
</authors>
<title>Crosslingual induction of semantic roles.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, Jeju Island, South Korea,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="32854" citStr="Titov and Klementiev, 2012" startWordPosition="5432" endWordPosition="5435">), the main difference being that it relies on a manually annotated parallel corpus, aligned on the argument level, and evaluates only the inference procedure and only on in-domain data. Other related approaches include Kim et al. (2010), where a cross-lingual transfer of relations is performed (which basically represent parts of the predicate-argument structure considered by SRL methods), and Frermann and Bond (2012), where semantic structure matching is used to rank HPSG parses for parallel sentences. Unsupervised semantic role labeling methods (Lang and Lapata, 2010; Lang and Lapata, 2011; Titov and Klementiev, 2012a; Lorenzo and Cerisara, 2012) present an alternative to the crosslingual information propagation approaches such as ours, and at least one the methods in this area also makes use of parallel data (Titov and Klementiev, 2012b). Conclusions We have presented an approach to information transfer between SRL systems for different language pairs using parallel data. The task proves challenging due to non-trivial mapping between the role labels used in different SRL annotation schemes and the nature of parallel data – the difference in domains and the limited accuracy of the preprocessing tools. We </context>
</contexts>
<marker>Titov, Klementiev, 2012</marker>
<rawString>Ivan Titov and Alexandre Klementiev. 2012b. Crosslingual induction of semantic roles. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, Jeju Island, South Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lonneke van der Plas</author>
<author>Paola Merlo</author>
<author>James Henderson</author>
</authors>
<title>Scaling up automatic cross-lingual semantic role annotation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers -Volume 2, HLT ’11,</booktitle>
<pages>299--304</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>van der Plas, Merlo, Henderson, 2011</marker>
<rawString>Lonneke van der Plas, Paola Merlo, and James Henderson. 2011. Scaling up automatic cross-lingual semantic role annotation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers -Volume 2, HLT ’11, pages 299–304, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Zhuang</author>
<author>Chengqing Zong</author>
</authors>
<title>Joint inference for bilingual semantic role labeling.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>304--314</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2734" citStr="Zhuang and Zong (2010)" startWordPosition="415" endWordPosition="418"> between two existing annotation schemes (figure 1) for a pair of languages using an intermediate model of role correspondence (RCM). An RCM models the probability of a pair of corresponding arguments being assigned a certain pair of roles. We then use it to guide a pair of monolingual models toward compatible predictions on parallel data in order to extend the coverage and/or accuracy of one or both models. Figure 1: Role correspondence in parallel sentences, an example. The notion of compatibility here is highly nontrivial, even for sentences translated as close to the original as possible. Zhuang and Zong (2010), for example, observe that in the English-Chinese parallel PropBank (Palmer et al., 2005b) corresponding arguments often bear different labels, even though the same inventory of semantic roles is used for both A1 AM-NEG AM-LOC Romanian is not taught in their schools . Ve školách se neuči rumunsky . LOC PAT 317 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task, pages 317–327, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics languages and the annotation guidelines are simila</context>
<context position="8869" citStr="Zhuang and Zong, 2010" startWordPosition="1433" endWordPosition="1436">1, consistently denote what is known as ProtoAgent and Proto-Patient), while modifiers (Merlo and Leybold, 2001) bear labels that are interpreted consistently across all predicates. Other resources, such as Prague Dependency Treebank (Hajiˇc et al., 2006), use a single set of semantic roles (functors), which are interpretable across different predicates. From the standpoint of defining the semantic similarity of parallel sentences, the important implication is that we cannot assume that the corresponding arguments should bear the same label, even if the annotation schemes used are compatible (Zhuang and Zong, 2010). Nor can we write down a single mapping between the roles that will be valid across different predicates (figure 2), which motivates the need for a statistical model of semantic role correspondence. A0 A1 I do not have these concerns Parliament adopted the resolution El Parlamento aprueba la resolución argG-agt arg1-pat ping between semantic roles for a given predicate pair. As the mappings are not completely independent – at least some roles have the same interpretation across different predicate pairs, – we choose to build a single model, which relies on features derived from the pair of pr</context>
<context position="32229" citStr="Zhuang and Zong (2010)" startWordPosition="5339" endWordPosition="5342">nce, projection setup: initial model, self-training baseline, refined model and its improvement over the other two. Asterisk indicates outof-domain test set, statistically significant improvements are highlighted in bold. the availability of some amount of training data for the target language, possibly using a different inventory of semantic roles. As mentioned previously, from the training point of view this approach can be seen as similar to cotraining (Blum and Mitchell, 1998), other applications of which to NLP are too numerous to list here. Most closely related is the joint inference in Zhuang and Zong (2010), the main difference being that it relies on a manually annotated parallel corpus, aligned on the argument level, and evaluates only the inference procedure and only on in-domain data. Other related approaches include Kim et al. (2010), where a cross-lingual transfer of relations is performed (which basically represent parts of the predicate-argument structure considered by SRL methods), and Frermann and Bond (2012), where semantic structure matching is used to rank HPSG parses for parallel sentences. Unsupervised semantic role labeling methods (Lang and Lapata, 2010; Lang and Lapata, 2011; T</context>
</contexts>
<marker>Zhuang, Zong, 2010</marker>
<rawString>Tao Zhuang and Chengqing Zong. 2010. Joint inference for bilingual semantic role labeling. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 304–314, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>