<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002048">
<title confidence="0.99047">
Semantic Representations for Domain Adaptation:
A Case Study on the Tree Kernel-based Method for Relation Extraction
</title>
<author confidence="0.938121">
Thien Huu Nguyen†, Barbara Plank§ and Ralph Grishman†† Computer Science Department, New York University, New York, NY 10003, USA
</author>
<affiliation confidence="0.600407">
§ Center for Language Technology, University of Copenhagen, Denmark
</affiliation>
<email confidence="0.979998">
thien@cs.nyu.edu,bplank@cst.dk,grishman@cs.nyu.edu
</email>
<sectionHeader confidence="0.993106" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999938842105263">
We study the application of word embed-
dings to generate semantic representations
for the domain adaptation problem of re-
lation extraction (RE) in the tree kernel-
based method. We systematically evaluate
various techniques to generate the seman-
tic representations and demonstrate that
they are effective to improve the general-
ization performance of a tree kernel-based
relation extractor across domains (up to
7% relative improvement). In addition,
we compare the tree kernel-based and the
feature-based method for RE in a compat-
ible way, on the same resources and set-
tings, to gain insights into which kind of
system is more robust to domain changes.
Our results and error analysis shows that
the tree kernel-based method outperforms
the feature-based approach.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999931169491526">
Relation Extraction (RE) is an important aspect of
information extraction that aims to discover the
semantic relationships between two entity men-
tions appearing in the same sentence. Previous
research on RE has followed either the kernel-
based approach (Zelenko et al., 2003; Bunescu
and Mooney, 2005; Zhao and Grishman, 2005;
Zhang et al., 2006; Bunescu, 2007; Qian et al.,
2008; Nguyen et al., 2009) or the feature-based ap-
proach (Kambhatla, 2004; Grishman et al., 2005;
Zhou et al., 2005; Jiang and Zhai, 2007a; Chan
and Roth, 2010; Sun et al., 2011). Usually, in
such supervised machine learning systems, it is as-
sumed that the training data and the data to which
the RE system is applied to are sampled inde-
pendently and identically from the same distribu-
tion. This assumption is often violated in reality
and exemplified in the fact that the performance
of the traditional RE techniques degrades signif-
icantly in such a domain mismatch case (Plank
and Moschitti, 2013). To alleviate this perfor-
mance loss, we need to resort to domain adaptation
(DA) techniques to adapt a system trained on some
source domain to perform well on new target do-
mains. We here focus on the unsupervised domain
adaptation (i.e., no labeled target data) and single-
system DA (Petrov and McDonald, 2012; Plank
and Moschitti, 2013), i.e., building a single sys-
tem that is able to cope with different, yet related
target domains.
While DA has been investigated extensively in
the last decade for various natural language pro-
cessing (NLP) tasks, the examination of DA for
RE is only very recent. To the best of our knowl-
edge, there have been only three studies on DA
for RE (Plank and Moschitti, 2013; Nguyen and
Grishman, 2014; Nguyen et al., 2014). Of these,
Nguyen et al. (2014) follow the supervised DA
paradigm and assume some labeled data in the
target domains. In contrast, Plank and Moschitti
(2013) and Nguyen and Grishman (2014) work
on the unsupervised DA. In our view, unsuper-
vised DA is more challenging, but more realistic
and practical for RE as we usually do not know
which target domains we need to work on in ad-
vance, thus cannot expect to possess labeled data
of the target domains. Our current work therefore
focuses on the single-system unsupervised DA.
Besides, note that this setting tries to construct a
single system that can work robustly with differ-
ent but related domains (multiple target domains),
thus being different from most previous studies on
DA (Blitzer et al., 2006; Blitzer et al., 2007) which
have attempted to design a specialized system for
every specific target domain.
Plank and Moschitti (2013) propose to embed
word clusters and latent semantic analysis (LSA)
of words into tree kernels for DA of RE, while
Nguyen and Grishman (2014) studies the appli-
</bodyText>
<page confidence="0.983176">
635
</page>
<note confidence="0.978399333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 635–644,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.998023123287671">
cation of word clusters and word embeddings for
DA of RE on the feature-based method. Although
word clusters (Brown et al., 1992) have been em-
ployed by both studies to improve the performance
of relation extractors across domains, the appli-
cation of word embeddings (Bengio et al., 2003;
Mnih and Hinton, 2008; Turian et al., 2010) for
DA of RE is only examined in the feature-based
method and never explored in the tree kernel-
based method so far, giving rise to the first ques-
tion we want to address in this paper:
(i) Can word embeddings help the tree kernel-
based methods on DA for RE and more impor-
tantly, in which way can we do it effectively?
This question is important as word embeddings
are real valued vectors, while the tree kernel-based
methods rely on the symbolic matches or mis-
matches of concrete labels in the parse trees to
compute the kernels. It is unclear at the first glance
how to encode word embeddings into the tree ker-
nels effectively so that word embeddings could
help to improve the generalization performance of
RE. One way is to use word embeddings to com-
pute similarities between words and embed these
similarity scores into the kernel functions, e.g.,
by resembling the method of Plank and Moschitti
(2013) that exploited LSA (in the semantic syntac-
tic tree kernel (SSTK), cf. §2.1). We explore vari-
ous methods to apply word embeddings to gener-
ate the semantic representations for DA of RE and
demonstrate that semantic representations are very
effective to significantly improve the portability of
the relation extractors based on the tree kernels,
bringing us to the second question:
(ii) Between the feature-based method in
Nguyen and Grishman (2014) and the SSTK
method in Plank and Moschitti (2013), which
method is better for DA of RE, given the recent
discovery of word embeddings for both methods?
It is worth noting that besides the approach dif-
ference, these two works employ rather different
resources and settings in their evaluation, mak-
ing it impossible to directly compare their perfor-
mance. In particular, while Plank and Moschitti
(2013) only use the path-enclosed trees induced
from the constituent parse trees as the represen-
tation for relation mentions, Nguyen and Grish-
man (2014) include a rich set of features extracted
from multiple resources such as constituent trees,
dependency trees, gazetteers, semantic resources
in the representation. Besides, Plank and Mos-
chitti (2013) consider the direction of relations in
their evaluation (i.e, distinguishing between rela-
tion classes and their inverses) but Nguyen and
Grishman (2014) disregard this relation direction.
Finally, we note that although both studies evalu-
ate their systems on the ACE 2005 dataset, they
actually have different dataset partitions. In order
to overcome this limitation, we conduct an eval-
uation in which the two methods are directed to
use the same resources and settings, and are thus
compared in a compatible manner to achieve an in-
sight on their effectiveness for DA of RE. In fact,
the problem of incompatible comparison is unfor-
tunately very common in the RE literature (Wang,
2008; Plank and Moschitti, 2013) and we believe
there is a need to tackle this increasing confusion
in this line of research. Therefore, this is actu-
ally the first attempt to compare the two methods
(tree kernel-based and feature-based) on the same
settings. To ease the comparison for future work
and circumvent the Zigglebottom pitfall (Pedersen,
2008), the entire setup and package is available.1
</bodyText>
<sectionHeader confidence="0.961533" genericHeader="method">
2 Relation Extraction Approaches
</sectionHeader>
<bodyText confidence="0.9995845">
In the following, we introduce the two relation ex-
traction systems further examined in this study.
</bodyText>
<subsectionHeader confidence="0.99735">
2.1 Tree kernel-based Method
</subsectionHeader>
<bodyText confidence="0.999971590909091">
In the tree kernel-based method (Moschitti, 2006;
Moschitti, 2008; Plank and Moschitti, 2013), a
relation mention (the two entity mentions and
the sentence containing them) is represented
by the path-enclosed tree (PET), the smallest
constituency-based subtree including the two tar-
get entity mentions (Zhang et al., 2006). The syn-
tactic tree kernel (STK) is then defined to compute
the similarity between two PET trees (where tar-
get entities are marked) by counting the common
sub-trees, without enumerating the whole frag-
ment space (Moschitti, 2006; Moschitti, 2008).
STK is then applied in the support vector ma-
chines (SVMs) for RE. The major limitation of
STK is its inability to match two trees that share
the same substructure, but involve different though
semantically related terminal nodes (words). This
is caused by the hard matches between words,
and consequently between sequences containing
them. For instance, in the following example taken
from Plank and Moschitti (2013), the two frag-
ments “governor from Texas” and “head of Mary-
</bodyText>
<footnote confidence="0.995813">
1https://bitbucket.org/nycphre/limo-re
</footnote>
<page confidence="0.998508">
636
</page>
<bodyText confidence="0.999972217391304">
land” would not match in STK although they have
very similar syntactic structures and basically con-
vey the same relationship.
Plank and Moschitti (2013) propose to resolve
this issue for STK using the semantic syntac-
tic tree kernel (SSTK) (Bloehdorn and Moschitti,
2007) and apply it to the domain adaptation prob-
lem of RE. The two following techniques are uti-
lized to activate the SSTK: (i) replace the part-of-
speech nodes in the PET trees by the new ones
labeled by the word clusters of the corresponding
terminals (words); (ii) replace the binary similar-
ity scores between words (i.e, either 1 or 0) by
the similarities induced from the latent semantic
analysis (LSA) of large corpus. The former gener-
alizes the part-of-speech similarity to the seman-
tic similarity on word clusters; the latter, on the
other hand, allows soft matches between words
that have the same latent semantic but differ in
symbolic representation. Both techniques empha-
size the invariants of word semantics in different
domains, thus being helpful to alleviate the vocab-
ulary difference across domains.
</bodyText>
<subsectionHeader confidence="0.983241">
2.2 Feature-based Method
</subsectionHeader>
<bodyText confidence="0.999941970588235">
In the feature-based method (Zhou et al., 2005;
Sun et al., 2011; Nguyen and Grishman, 2014), re-
lation mentions are first transformed into rich fea-
ture vectors that capture various characteristics of
the relation mentions (i.e, lexicon, syntax, seman-
tics etc). The resulting vectors are then fed into the
statistical classifiers such as Maximum Entropy
(MaxEnt) to perform classification for RE.
The main reason for the performance loss of
the feature-based systems on new domains is the
behavioral changes of the features when domains
shift. Some features might be very informative in
the source domain but become less relevant in the
target domains. For instance, some words, that
are very indicative in the source domain might
not appear in the target domains (lexical sparsity).
Consequently, the models putting high weights on
such words (features) in the source domain will
fail to perform well on the target domains. Nguyen
and Grishman (2014) address this problem for the
feature-based method in DA of RE by introduc-
ing word embeddings as additional features. The
rationale is based on the fact that word embed-
dings are low dimensional and real valued vec-
tors, capturing latent syntactic and semantic prop-
erties of words (Bengio et al., 2003; Mnih and
Hinton, 2008; Turian et al., 2010). The embed-
dings of symbolically different words are often
close to each other if they have similar semantic
and syntactic functions. This again helps to mit-
igate the lexical sparsity or the vocabulary differ-
ence between the domains and has proven helpful
for, amongst others, the feature-based method in
DA of RE.
</bodyText>
<subsectionHeader confidence="0.999645">
2.3 Tree Kernel-based vs Feature-based
</subsectionHeader>
<bodyText confidence="0.999978414634146">
The feature-based method explicitly encapsulates
the linguistic intuition and domain expertise for
RE into the features, while the tree kernel-based
method avoids the complicated feature engineer-
ing and implicitly encode the features into the
computation of the tree kernels. Which method
is better for DA of RE?
In order to ensure the two methods (Plank and
Moschitti, 2013; Nguyen and Grishman, 2014) are
compared compatibly on the same resources, we
make sure the two systems have access to the same
amount of information. Thus, we follow Plank
and Moschitti (2013) and use the PET trees (be-
side word clusters and word embeddings) as the
only resource the two methods can exploit.
For the feature-based method, we utilize all
the features extractable from the PET trees that
are standardly used in the state-of-the-art feature-
based systems for DA of RE (Nguyen and Gr-
ishman, 2014). Specifically, the feature set em-
ployed in this paper (denoted by FET) includes:
the lexical features, i.e., the context words, the
head words, the bigrams, the number of words,
the lexical path, the order of mention (Zhou et al.,
2005; Sun et al., 2011); and the syntactic features,
i.e., the path connecting the two mentions in PET
and the unigrams, bigrams, trigrams along this
path (Zhou et al., 2005; Jiang and Zhai, 2007a).
Hypothesis: Assuming identical settings and
resources, we hypothesize that the tree kernel-
based method is better than the feature-based
method for DA of RE. This is motivated because
of at least two reasons: (i) the tree kernel-based
method implicitly encodes a more comprehen-
sive feature set (involving all the sub-trees in the
PETs), thus potentially captures more domain-
independent features to be useful for DA of RE;
(ii) the tree kernel-based method avoids the in-
clusion of fine-tuned and domain-specific features
originated from the excessive feature engineer-
ing (i.e., hand-designing feature sets based on the
</bodyText>
<page confidence="0.988543">
637
</page>
<bodyText confidence="0.8112815">
linguistic intuition for specific domains) of the function in this case is then defined by:
feature-based method.
</bodyText>
<equation confidence="0.555558">
Knew(Ri, Rj) = (1 − α)SSTK(Ti, Tj) + αKvec(Vi, Vj)
</equation>
<sectionHeader confidence="0.888966" genericHeader="method">
3 Word Embeddings &amp; Tree Kernels
</sectionHeader>
<bodyText confidence="0.999957125">
In this section, we first give the intuition that
guides us in designing the proposed methods. In
particular, one limitation of the syntactic seman-
tic tree kernel presented in Plank and Moschitti
(2013) (§2.1) is that semantics is highly tied to
syntax (the PET trees) in the kernel computation,
limiting the generalization capacity of semantics
to the extent of syntactic matches. If two rela-
tion mentions have different syntactic structures,
the two relation mentions will not match, although
they share the same semantic representation and
express the same relation class. For instance, the
two fragments “Tom is the CEO of the company”
and “the company, headed by Tom” express the
same relationship between “Tom” and “company”
based on the semantics of their context words,
but cannot be matched in SSTK as their syntac-
tic structures are different. In such a case, it is
desirable to have a representation of relation men-
tions that is grounded on the semantics of the con-
text words and reflects the latent semantics of the
whole relation mentions. This representation is
expected to be general enough to be effective on
different domains. Once the semantic representa-
tion of relation mentions is established, we can use
it in conjunction with the traditional tree kernels
to extend their coverage. The benefit is mutual as
both semantics and syntax help to generalize rela-
tion mentions to improve the recall, but also con-
strain each other to support precision. This is the
basic idea of our approach, which we compare to
the previous methods.
</bodyText>
<subsectionHeader confidence="0.995634">
3.1 Methods
</subsectionHeader>
<bodyText confidence="0.999608016393443">
We propose to utilize word embeddings of the con-
text words as the principal components to obtain
semantic representations for relation mentions in
the tree kernel-based methods. Besides more tra-
ditional approaches to exploit word embeddings,
we investigate representations that go beyond the
word level and use compositionality embeddings
for domain adaptation for the first time.
In general, suppose we are able to acquire an
additional real-valued vector Vi from word embed-
dings to semantically represent a relation mention
Ri (along with the PET tree Ti), leading to the new
representation of Ri = (Ti, Vi). The new kernel
where Kvec(Vi, Vj) is some standard vector ker-
nel like the polynomial kernels. α is a trade-off
parameter and indicates whether the system at-
tributes more weight to the traditional SSTK or the
new semantic kernel Kvec.
In this work, we consider the following meth-
ods to obtain the semantic representation Vi from
the word embeddings of the context words of Ri
(assuming d is the dimensionality of the word em-
beddings):
HEAD: Vi = the concatenation of the word em-
beddings of the two entity mention heads of Ri.
This representation is inherited from Nguyen and
Grishman (2014) that only examine embeddings
at the word level separately for the feature-based
method without considering the compositionality
embeddings of relation mentions. The dimension-
ality of HEAD is 2d.
According to the principle of compositional-
ity (Werning et al., 2006; Baroni and Zamparelli,
2010; Paperno et al., 2014), the meaning of a com-
plex expression is determined by the meanings of
its components and the rules to combine them. We
study the following two compositionality embed-
dings for relation mentions that can be generated
from the embeddings of the context words:
PHRASE: Vi = the mean of the embeddings
of the words contained in the PET tree Ti of
Ri. Although this composition is simple, it is in
fact competitive to the more complicated methods
based on recursive neural networks (Socher et al.,
2012b; Blacoe and Lapata, 2012; Sterckx et al.,
2014) on representing phrase semantics.
TREE: This is motivated by the training of re-
cursive neural networks (Socher et al., 2012a) for
semantic compositionality and attempts to aggre-
gate the context words embeddings syntactically.
In particular, we compute an embedding for every
node in the PET tree in a bottom-up manner. The
embeddings of the leaves are the embeddings of
the words associated with them while the embed-
dings of the internal nodes are the means of the
embeddings of their children nodes. We use the
embeddings of the root of the PET tree to represent
the relation mention in this case. Both PHRASE
and TREE have d dimensions.
It is also interesting to examine combinations of
these three representations (cf., Table 1).
</bodyText>
<page confidence="0.994724">
638
</page>
<bodyText confidence="0.999810777777778">
SIM: Finally, for completeness, we experi-
ment with a more obvious way to introduce
word embeddings into tree kernels, resembling
more closely the approach of Plank and Moschitti
(2013). In particularly, the SIM method simply
replaces the similarity scores between word pairs
obtained from LSA by the cosine similarities be-
tween the word embeddings to be used in the
SSTK kernel.
</bodyText>
<sectionHeader confidence="0.999906" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999671">
4.1 Dataset, Resources and Parameters
</subsectionHeader>
<bodyText confidence="0.99993559375">
We use the word clusters trained by Plank and
Moschitti (2013) on the ukWaC corpus (Baroni
et al., 2009) with 2 billion words, and the C&amp;W
word embeddings from Turian el al. (2010)2 with
50 dimensions following Nguyen and Grishman
(2014). In order to make the comparisons com-
patible, we introduce word embeddings into the
tree kernel by extending the package provided by
Plank and Moschitti (2013), which uses the Char-
niak parser to obtain the constituent trees, the
SVM-light-TK for the SSTK kernel in SVM, the
directional relation classes, etc. We utilize the de-
fault vector kernel in the SVM-light-TK package
(d=3). For the feature-based method, we apply the
MaxEnt classifier in the MALLET3 package with
the L2 regularizer on the hierarchical architecture
for relation extraction as in Nguyen and Grishman
(2014).
Following prior work, we evaluate the sys-
tems on the ACE 2005 dataset which involves 6
domains: broadcast news (bn), newswire (nw),
broadcast conversation (bc), telephone conversa-
tion (cts), weblogs (wl) and usenet (un). The union
of bn and nw (news) is used as the source domain
while bc, cts and wl play the role of the target do-
mains. We take half of bc as the only target de-
velopment set, and use the remaining data and do-
mains for testing. The dataset partition is exactly
the same as in Plank and Moschitti (2013). As
described in their paper, the target domains quite
differ from the source domain in the relation dis-
tributions and vocabulary.
</bodyText>
<subsectionHeader confidence="0.992826">
4.2 Word Embeddings for Tree Kernel
</subsectionHeader>
<bodyText confidence="0.9991555">
We investigate the effectiveness of different se-
mantic representations (§3.1) in tree kernels by
</bodyText>
<footnote confidence="0.9998115">
2http://metaoptimize.com/projects/wordreprs/
3http://mallet.cs.umass.edu/
</footnote>
<bodyText confidence="0.524272333333333">
taking the PET tree as the baseline4, and evaluate
the performance of the representations when com-
bined with the baseline on the bc development set.
</bodyText>
<table confidence="0.9999444">
Method P R F1
PET (Plank and Moschitti, 2013) 52.2 41.7 46.4
PET+SIM 39.4 37.2 38.3
PET+HEAD 60.4 44.9 51.5
PET+PHRASE 58.4 40.7 48.0
PET+TREE 59.8 42.2 49.5
PET+HEAD+PHRASE 63.2 46.2 53.4
PET+HEAD+TREE 61.0 45.7 52.3
PET+PHRASE+TREE 59.2 42.4 49.4
PET+HEAD+PHRASE+TREE 60.8 45.2 51.9
</table>
<tableCaption confidence="0.989739">
Table 1: Performance on the bc dev set for PET. Best com-
bination (HEAD+PHRASE) is denoted WED in Table 2
</tableCaption>
<bodyText confidence="0.815460666666667">
Table 1 shows the results. The main conclusions
include:
(i) The substitution of LSA similarity scores
with the word embedding cosine similarities
(SIM) does not help to improve the performance
of the tree kernel method.
</bodyText>
<listItem confidence="0.909318875">
(ii) When employed independently, both the
word level embeddings (HEAD) and the compo-
sitionality embeddings (PHRASE, TREE) are ef-
fective for the tree kernel-based method on DA for
RE, showing a slight advantage for HEAD.
(iii) Thus, the compositionality embeddings
PHRASE and TREE seem to capture different
information with respect to the word level em-
beddings HEAD. We expect the combination of
HEAD with either PHRASE or TREE to further
improve performance. This is the case when
adding one of them at a time. PHRASE and TREE
seem to capture similar information, combining all
(last row in Table 1) is not the overall best sys-
tem. The best performance is achieved when the
HEAD and PHRASE embeddings are utilized at
</listItem>
<footnote confidence="0.46146">
4By using their system we obtained the same results.
</footnote>
<figure confidence="0.957143428571429">
52
F-measure
50
48
46
0 0.1 0.3 0.5 0.7 0.9 1
α
</figure>
<figureCaption confidence="0.999603">
Figure 1: α vs F-measure on PET+HEAD+PHRASE
</figureCaption>
<page confidence="0.990856">
639
</page>
<table confidence="0.999678">
nw+bn (in-dom.) P: bc F1: P: cts F1: P: wl F1:
# System: P: R: F1: R: R: R:
1 PET (Plank and Moschitti, 2013) 50.6 42.1 46.0 51.2 40.6 45.3 51.0 37.8 43.4 35.4 32.8 34.0
2 PET+WED 55.8 48.7 52.0 57.3 45.7 50.8 54.0 38.1 44.7 40.1 36.5 38.2
3 PET WC 55.4 44.6 49.4 54.3 41.4 47.0 55.9 37.1 44.6 40.0 32.7 36.0
4 PET WC+WED 56.3 48.2 51.9 57.0 44.3 49.8 56.1 38.1 45.4 40.7 36.1 38.2
5 PET LSA 52.3 44.1 47.9 51.4 41.7 46.0 49.7 36.5 42.1 38.1 36.5 37.3
6 PET LSA+WED 55.2 48.5 51.6 58.8 45.8 51.5 54.1 38.1 44.7 40.9 38.5 39.6
7 PET+PET WC 55.0 46.5 50.4 54.4 43.4 48.3 54.1 38.1 44.7 38.4 34.5 36.3
8 PET+PET WC+WED 56.3 50.3 53.1 57.5 46.6 51.5 55.6 39.8 46.4 41.5 37.9 39.6
9 PET+PET LSA 52.7 46.6 49.5 53.9 45.2 49.2 49.9 37.6 42.9 37.9 38.3 38.1
10 PET+PET LSA+WED 55.5 49.9 52.6 56.8 45.8 50.8 52.5 38.6 44.5 41.6 39.3 40.5
11 PET+PET WC+PET LSA 55.1 45.9 50.1 55.3 43.1 48.5 53.1 37.0 43.6 39.9 35.8 37.8
12 PET+PET WC+PET LSA+WED 55.0 48.8 51.7 58.5 47.3 52.3 52.6 38.8 44.7 42.3 38.9 40.5
</table>
<tableCaption confidence="0.9958845">
Table 2: In-domain (first column) and out-of-domain performance (columns two to four) on ACE 2005. Systems of the rows
not in gray come from Plank and Moschitti (2013) (the baselines). WED means HEAD+PHRASE.
</tableCaption>
<bodyText confidence="0.999921375">
the same time, reaching an F1 of 53.4% (compared
to 46.4% of the baseline) on the development set.
The results in Table 1 are obtained using the
trade-off parameter α = 0.7. Figure 1 addi-
tionally shows the variation of the performance
with changing α (for the best system on dev, i.e.,
for the representation PET+HEAD+PHRASE).
As we can see, the performance for α &gt; 0.5 is
in general better, suggesting a preference for the
semantic representation over the syntactic repre-
sentation in DA for RE. The performance reaches
its peak when the suitable amounts of semantics
and syntax are combined (i.e, α = 0.7).
In the following experiments, we use the
embedding combination (HEAD+PHRASE) with
α = 0.7 for the tree kernels, denoted WED.
</bodyText>
<subsectionHeader confidence="0.995897">
4.3 Domain Adaptation Experiments
</subsectionHeader>
<bodyText confidence="0.999866847826087">
In this section, we examine the semantic rep-
resentation for DA of RE in the tree kernel-
based method. In particular, we take the sys-
tems using the PET trees, word clusters and LSA
in Plank and Moschitti (2013) as the baselines
and augment them with the embeddings WED =
HEAD+PHRASE. We report the performance of
these augmented systems in Table 2 for the two
scenarios: (i) in-domain: both training and test-
ing are performed on the source domain via 5-fold
cross validation and (ii) out-of-domain: models
are trained on the source domain but evaluated on
the three target domains. To summarize, we find:
First, word embeddings seem to subsume word
clusters in the tree kernel-based method (compar-
ing rows 2 and 4, and except domain cts) while
word embeddings and LSA actually encode dif-
ferent information (comparing rows 2 and 6 for
the out-of-domain experiments) and their combi-
nation would be helpful for DA of RE.
Second, regarding composite kernels, given
word embeddings, the addition of the baseline ker-
nel (PET) is in general useful for the augmented
kernels PET WC and PET LSA (comparing rows
4 and 8, rows 6 and 10) although it is less pro-
nounced for PET LSA.
Third and most importantly, for all the systems
in Plank and Moschitti (2013) (the baselines) and
for all the target domains, whether word clusters
and LSA are utilized or not, we consistently wit-
ness the performance improvement of the base-
lines when combined with word embedding (com-
paring systems X and X+WED where X is some
baseline system). The best out-of-domain perfor-
mance is achieved when word embeddings are em-
ployed in conjunction with the composite kernels
(PET+PET WC+PET LSA for the target domains
bc and wl, and PET+PET WC for the target do-
main cts). To be more concrete, the best system
with word embeddings (row 12 in Table 2) signif-
icantly outperforms the best system in Plank and
Moschitti (2013) with p &lt; 0.05, an improvement
of 3.7%, 1.1% and 2.7% on the target domains bc,
cts and wl respectively, demonstrating the bene-
fit of word embeddings for DA of RE in the tree
kernel-based method.
</bodyText>
<subsectionHeader confidence="0.9749245">
4.4 Tree Kernel-based vs Feature-based DA
of RE
</subsectionHeader>
<bodyText confidence="0.9993124">
This section aims to compare the tree kernel-based
method in Plank and Moschitti (2013) and the
feature-based method in Nguyen and Grishman
(2014) for DA of RE on the same settings (i.e,
same dataset partition, the same pre-processing
</bodyText>
<page confidence="0.99543">
640
</page>
<table confidence="0.999669">
nw+bn (in-dom.) P: bc F1: P: cts F1: P: wl F1:
System: P: R: F1: R: R: R:
Tree kernel-based:
PET+PET WC+HEAD+PHRASE 56.3 50.3 53.1 57.5 46.6 51.5 55.6 39.8 46.4 41.5 37.9 39.6
Feature-based:
FET+WC+HEAD 44.5 51.0 47.5 46.5 49.3 47.8 44.5 40.0 42.1 35.4 39.5 37.3
FET+WC+TREE 44.4 50.2 47.1 46.4 48.7 47.6 43.7 40.3 41.9 32.7 36.7 34.6
FET+WC+HEAD+PHRASE 44.9 51.6 48.0 46.0 49.1 47.5 45.2 41.5 43.3 34.7 39.2 36.8
FET+WC+HEAD+TREE 45.1 51.0 47.8 46.9 48.4 47.6 43.8 39.5 41.5 34.7 38.8 36.6
</table>
<tableCaption confidence="0.9980595">
Table 3: Tree kernel-based in Plank and Moschitti (2013) vs feature-based in Nguyen and Grishman (2014). All the compar-
isons between the tree kernel-based method and the feature-based method in this table are significant with p &lt; 0.05.
</tableCaption>
<bodyText confidence="0.999454384615385">
procedure, the same model of directional relation
classes, the same PET trees for tree kernels and
feature extraction, the same word clusters and the
same word embeddings). We first evaluate the
feature-based system with different combinations
of embeddings (i.e, HEAD, PHRASE and TREE)
on the bc development set. Based on the evalua-
tion results, we then discuss the effect of the se-
mantic representations on the feature-based sys-
tem and the tree kernel-based system, and then
compare the performance of the two methods
when they are augmented with their best corre-
sponding embedding combinations.
</bodyText>
<table confidence="0.999906777777778">
System P R F1
B 51.2 49.4 50.3
B+HEAD 55.8 52.4 54.0
B+PHRASE 50.7 46.2 48.4
B+TREE 53.6 51.1 52.3
B+HEAD+PHRASE 53.2 50.1 51.6
B+HEAD+TREE 54.9 51.4 53.1
B+PHRASE+TREE 50.7 48.4 49.5
B+HEAD+PHRASE+TREE 52.7 49.4 51.0
</table>
<tableCaption confidence="0.999874">
Table 4: Performance of the feature-based method (dev).
</tableCaption>
<bodyText confidence="0.996792346153846">
Table 4 presents the evaluation results on the bc
development for the feature-based system where
B is the baseline feature set consisting of FET
and word clusters (WC) (Nguyen and Grishman,
2014).
The Role of Semantic Representations Con-
sidering Table 4 for the feature-based method and
Table 1 for the tree kernel-based method, we see
that when combined with the HEAD embeddings,
the compositionality embedding TREE is more ef-
fective for the feature-based method, in contrast to
the tree kernel-based method, where the PHRASE
embeddings are better. This can be partly ex-
plained by the fact that the tree kernel-based
method emphasizes the syntactic structure of the
relation mentions, while the feature-based method
exploits the sequential structure more. Conse-
quently, the syntactic semantics of TREE are more
helpful for the feature-based method, whereas the
sequential semantics of PHRASE are more useful
for the tree kernel-based method.
Performance Comparison The three best em-
bedding combinations for the feature-based sys-
tem in Table 4 are (listed by performance order):
(HEAD), (HEAD+TREE) and (TREE), where
(HEAD) is also the best word level method em-
ployed in Nguyen and Grishman (2014). In
order to enable a fairer and clearer evaluation,
when doing comparison, we use both the three
best embedding combinations in the feature-
based method and the best embedding combina-
tion (HEAD+PHRASE) in the tree kernel-based
method. In the tree kernel-based method, we do
not employ the LSA information as it comes in the
form of similarity scores between pairs of words,
and it is not clear how to encode this information
into the feature-based method effectively. Finally,
we utilize the composite kernel for its demon-
strated effectiveness in Section 4.3.
The most important observation from the ex-
perimental results (shown in Table 3) is that over
all the target domains, the tree kernel-based sys-
tem is significantly better than the feature-based
systems with p &lt; 0.05 (assuming the same re-
sources and settings mentioned above). In fact,
there are large margins between the tree kernel-
based and the feature-based methods in this case
(i.e, about 3.7% for bc, 3.1% for cts and 2.3% for
wl), clearly confirming the hypothesis about the
advantage of the tree kernel-based method over
the feature-based method on DA for RE in Section
2.3.
</bodyText>
<sectionHeader confidence="0.980646" genericHeader="evaluation">
5 Analysis
</sectionHeader>
<bodyText confidence="0.9986465">
This section analyzes the output of the systems to
gain more insights into their operation.
</bodyText>
<page confidence="0.997194">
641
</page>
<bodyText confidence="0.999732492307692">
Word Embeddings for the Tree-kernel based
Method We focus on the comparison of the best
model in Plank and Moschitti (2013) (row 11
in Table 2) (called P) with the same model but
augmented with the embedding WED (row 12 in
Tabel 2) (called P+WED). One of the most inter-
esting insights is that the embedding WED helps
to semantically generalize the phrases connecting
the two target entity mentions beyond the syntactic
constraints. For instance, model P fails to discover
the relation between “Chuck Hagel” and “Viet-
nam” in the sentence (of the target domain bc):
“Sergeant Chuck Hagel was seriously wounded
twice in Vietnam.” (i.e, it returns the NONE re-
lation as the prediction) as the substructure asso-
ciated with “seriously wounded twice” does not
appear with any relation in the source domain.
Model P+WED, on the other hand, correctly pre-
dicts the PHYS (Located) relation between the
two entities as the PHRASE embedding of “Chuck
Hagel was seriously wounded twice in Vietnam.”
(phrase X1) is very close to the embedding of the
source domain phrase: “Stewart faces up to 30
years in prison” (phrase X2) (annotated with the
PHYS relation between “Stewart” and “prison”).
In fact, X2 is only the 9th closest phrase in
the source domain of X1. The closest phrase of
X1 in the source domain is X3: the phrase be-
tween “Iraqi soldiers” and “herself” in the sen-
tence “The Washington Post is reporting she shot
several Iraqi soldiers before she was captured
and she was shot herself, too.”. However, as the
syntactical structure of X1 is more similar to X2’s,
and is remarkably different from X3 as well as the
other closest phrases (ranked from 2nd to 8th), the
new kernel function Knew would still prefer X2
due to its trade-off between syntax and semantics.
Tree Kernel-based vs Feature-based From the
analysis of the systems in Table 3, we find that,
among others, the tree kernel-based method im-
proves the precision significantly via the seman-
tic and syntactic refinement it maintains. Let us
consider the following phrase of the target domain
bc: “troops have dislodged stubborn Iraqi sol-
diers” (called Y1). The feature-based systems in
Table 3 incorrectly predict the ORG-AFF relation
(Employment or Membership) between “Iraqi sol-
diers” and “troops”. This is mainly due to the high
weights of the features linking the words “troop”
and “soldiers” with the relation type ORG-AFF in
the feature-based models, which is, in turn, orig-
inated from the high correlation of these words
and the relation type in the training data of the
source domain (domain bias). The tree kernel-
based model in Table 3 successfully recognizes the
NONE relation in this case. A closer examination
shows that the phrase with the closest embedding
to Y1 in the source domain is Y2: “Iraqi soldiers
abandoned their posts”,5 which is annotated with
the NONE relation between “Iraqi soldiers” and
“their posts”. As the syntactic structure of Y2 is
also very similar to Y1, it is not surprising that Y1
is closest to Y2 in the new kernel function, conse-
quently helping the tree kernel-based method work
correctly in this case.
</bodyText>
<sectionHeader confidence="0.999972" genericHeader="conclusions">
6 Related work
</sectionHeader>
<bodyText confidence="0.99963840625">
Word embeddings are only applied to RE recently.
Socher et al. (2012b) use word embeddings as in-
put for matrix-vector recursive neural networks in
relation classification while Zeng et al. (2014),
and Nguyen and Grishman (2015) employ word
embeddings in the framework of convolutional
neural networks for relation classification and ex-
traction, respectively. Sterckx et al. (2014) uti-
lize word embeddings to reduce noise of training
data in distant supervision. Kuksa et al. (2010)
present a string kernel for bio-relation extraction
with word embeddings, and Yu et al. (2014; 2015)
study the factor-based compositional embedding
models. However, none of this work examines
word embeddings for tree kernels as well as do-
main adaptation as we do.
Regarding DA, in the unsupervised DA setting,
Huang and Yates (2010) attempt to learn multi-
dimensional feature representations while Blitzer
et al. (2006) introduce structural correspondence
learning. Daum´e (2007) proposes an easy adapta-
tion framework (EA) while Xiao and Guo (2013)
present a log-bilinear language adaptation tech-
nique in the supervised DA setting. Unfortunately,
all of this work assumes some prior (in the form of
either labeled or unlabeled data) on the target do-
mains for the sequential labeling tasks, in contrast
to our single-system unsupervised DA setting for
relation extraction. An alternative method that is
also popular to DA is instance weighting (Jiang
and Zhai, 2007b). However, as shown by Plank
and Moschitti (2013), instance weighting is not
</bodyText>
<footnote confidence="0.975659333333333">
5The full sentence is: “After today’s air strikes, Iraqi sol-
diers abandoned their posts and surrendered to Kurdish fight-
ers.”.
</footnote>
<page confidence="0.97655">
642
</page>
<note confidence="0.717819">
very useful for DA of RE. Stephan Bloehdorn and Alessandro Moschitti. 2007.
Exploiting Structure and Semantics for Expressive
</note>
<page confidence="0.49507">
7 Conclusion Text Kernels. In CIKM.
</page>
<bodyText confidence="0.999985956521739">
In order to improve the generalization of rela-
tion extractors, we propose to augment the seman-
tic syntactic tree kernels with the semantic rep-
resentation of relation mentions, generated from
the word embeddings of the context words. The
method demonstrates strong promise for the DA
of RE, i.e, it significantly improves the best sys-
tem of Plank and Moschitti (2013) (up to 7% rela-
tive improvement). Moreover, we perform a com-
patible comparison between the tree kernel-based
method and the feature-based method on the same
settings and resources, which suggests that the tree
kernel-based method (Plank and Moschitti, 2013)
is better than the feature-based method (Nguyen
and Grishman, 2014) for DA of RE. An error anal-
ysis is conducted to get a deeper comprehension of
the systems. Our future plan is to investigate other
syntactic and semantic structures (such as depen-
dency trees, abstract meaning representation etc)
for DA of RE, as well as continue the comparison
between the kernel-based method and the feature-
based method when they are allowed to exploit
more resources.
</bodyText>
<sectionHeader confidence="0.998543" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998778347826087">
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
EMNLP.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky wide web:
a collection of very large linguistically processed
web-crawled corpora. In Language Resources and
Evaluation, pages 209–226.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. In Journal of Machine Learning Re-
search 3, pages 1137–1155.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for semantic
composition. In EMNLP.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In ACL.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer,
Vincent J. Della Pietra, and Jenifer C. Lai. 1992.
Class-based n-gram models of natural language. In
Computational Linguistics, pages 467–479.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In EMNLP.
Razvan C. Bunescu. 2007. Learning to extract rela-
tions from the web using minimal supervision. In
ACL.
Yee Seng Chan and Dan Roth. 2010. Exploiting back-
ground knowledge for relation extraction. In COL-
ING.
Hal Daume. 2007. Frustratingly easy domain adapta-
tion. In ACL.
Ralph Grishman, David Westbrook, and Adam Meyers.
2005. Nyu’s english ace 2005 system description.
In The ACE 2005 Evaluation Workshop.
Fei Huang and Alexander Yates. 2010. Exploring
representation-learning approaches to domain adap-
tation. In The ACL Workshop on Domain Adaptation
for Natural Language Processing (DANLP).
Jing Jiang and ChengXiang Zhai. 2007a. A systematic
exploration of the feature space for relation extrac-
tion. In NAACL-HLT.
Jing Jiang and ChengXiang Zhai. 2007b. Instance
weighting for domain adaptation in nlp. In ACL.
Nanda Kambhatla. 2004. Combining lexical, syntac-
tic, and semantic features with maximum entropy
models for information extraction. In ACL.
Pavel Kuksa, Yanjun Qi, Bing Bai, Ronan Col-
lobert, Jason Weston, Vladimir Pavlovic, and
Xia Ning. 2010. Semi-supervised abstraction-
augmented string kernel for multi-level bio-relation
extraction. In ECML PKDD.
Andriy Mnih and Geoffrey Hinton. 2008. A scalable
hierarchical distributed language model. In NIPS.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In ECML.
Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization. In
CIKM.
Thien Huu Nguyen and Ralph Grishman. 2014. Em-
ploying word representations and regularization for
domain adaptation of relation extraction. In ACL.
</reference>
<page confidence="0.98954">
643
</page>
<reference confidence="0.999719120481928">
Thien Huu Nguyen and Ralph Grishman. 2015. Rela-
tion extraction: Perspective from convolutional neu-
ral networks. In The NAACL Workshop on Vector
Space Modeling for NLP (VSM).
T. Truc-Vien Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution kernels on
constituent, dependency and sequential structures
for relation extraction. In EMNLP.
Luan Minh Nguyen, W. Ivor Tsang, A. Kian Ming
Chai, and Leong Hai Chieu. 2014. Robust domain
adaptation for relation extraction via clustering con-
sistency. In ACL.
Denis Paperno, The Nghia Pham, and Marco Baroni.
2014. A practical and linguistically-motivated ap-
proach to compositional distributional semantics. In
ACL.
Ted Pedersen. 2008. Empiricism is not a matter of
faith. In Computational Linguistics 3, pages 465–
470.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. In The First
Workshop on Syntactic Analysis of Non-Canonical
Language (SANCL).
Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding semantic similarity in tree kernels for do-
main adaptation of relation extraction. In ACL.
Longhua Qian, Guodong Zhou, Fang Kong, Qiaoming
Zhu, and Peide Qian. 2008. Exploiting constituent
dependencies for tree kernel-based semantic relation
extraction. In COLING.
Richard Socher, Alex Perelygin, Jean Y. Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2012a. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In EMNLP-CoNLL.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012b. Semantic com-
positionality through recursive matrix-vector spaces.
In EMNLP.
Lucas Sterckx, Thomas Demeester, Johannes Deleu,
and Chris Develder. 2014. Using active learning
and semantic clustering for noise reduction in dis-
tant supervision. In AKBC.
Ang Sun, Ralph Grishman, and Satoshi Sekine. 2011.
Semi-supervised relation extraction with large-scale
word clustering. In ACL.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In ACL.
Mengqiu Wang. 2008. A re-examination of depen-
dency path kernels for relation extraction. In IJC-
NLP.
Markus Werning, Edouard Machery, and Gerhard
Schurz. 2006. Compositionality of meaning and
content: Foundational issues (linguistics &amp; philoso-
phy). In Linguistics &amp; philosophy.
Min Xiao and Yuhong Guo. 2013. Domain adapta-
tion for sequence labeling tasks with a probabilistic
language adaptation model. In ICML.
Mo Yu, Matthew Gormley, and Mark Dredze. 2014.
Factor-based compositional embedding models. In
The NIPS workshop on Learning Semantics.
Mo Yu, Matthew Gormley, and Mark Dredze. 2015.
Combining word embeddings and feature embed-
dings for fine-grained relation extraction. In
NAACL.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. In Journal of Machine Learning Research
3, pages 1083–1106.
Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classification via con-
volutional deep neural network. In COLING.
Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A composite kernel to extract relations be-
tween entities with both flat and structured features.
In COLING-ACL.
Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In ACL.
GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In ACL.
</reference>
<page confidence="0.998664">
644
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.910804">
<title confidence="0.999482">Semantic Representations for Domain Adaptation: A Case Study on the Tree Kernel-based Method for Relation Extraction</title>
<author confidence="0.977392">Huu Barbara Ralph Science Department</author>
<author confidence="0.977392">New York University</author>
<author confidence="0.977392">New York</author>
<author confidence="0.977392">NY</author>
<affiliation confidence="0.99701">for Language Technology, University of Copenhagen,</affiliation>
<email confidence="0.999863">thien@cs.nyu.edu,bplank@cst.dk,grishman@cs.nyu.edu</email>
<abstract confidence="0.99658145">We study the application of word embeddings to generate semantic representations for the domain adaptation problem of relation extraction (RE) in the tree kernelbased method. We systematically evaluate various techniques to generate the semantic representations and demonstrate that they are effective to improve the generalization performance of a tree kernel-based relation extractor across domains (up to 7% relative improvement). In addition, we compare the tree kernel-based and the feature-based method for RE in a compatible way, on the same resources and settings, to gain insights into which kind of system is more robust to domain changes. Our results and error analysis shows that the tree kernel-based method outperforms the feature-based approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="16884" citStr="Baroni and Zamparelli, 2010" startWordPosition="2720" endWordPosition="2723">ider the following methods to obtain the semantic representation Vi from the word embeddings of the context words of Ri (assuming d is the dimensionality of the word embeddings): HEAD: Vi = the concatenation of the word embeddings of the two entity mention heads of Ri. This representation is inherited from Nguyen and Grishman (2014) that only examine embeddings at the word level separately for the feature-based method without considering the compositionality embeddings of relation mentions. The dimensionality of HEAD is 2d. According to the principle of compositionality (Werning et al., 2006; Baroni and Zamparelli, 2010; Paperno et al., 2014), the meaning of a complex expression is determined by the meanings of its components and the rules to combine them. We study the following two compositionality embeddings for relation mentions that can be generated from the embeddings of the context words: PHRASE: Vi = the mean of the embeddings of the words contained in the PET tree Ti of Ri. Although this composition is simple, it is in fact competitive to the more complicated methods based on recursive neural networks (Socher et al., 2012b; Blacoe and Lapata, 2012; Sterckx et al., 2014) on representing phrase semanti</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The WaCky wide web: a collection of very large linguistically processed web-crawled corpora.</title>
<date>2009</date>
<booktitle>In Language Resources and Evaluation,</booktitle>
<pages>209--226</pages>
<contexts>
<context position="18726" citStr="Baroni et al., 2009" startWordPosition="3028" endWordPosition="3031">teresting to examine combinations of these three representations (cf., Table 1). 638 SIM: Finally, for completeness, we experiment with a more obvious way to introduce word embeddings into tree kernels, resembling more closely the approach of Plank and Moschitti (2013). In particularly, the SIM method simply replaces the similarity scores between word pairs obtained from LSA by the cosine similarities between the word embeddings to be used in the SSTK kernel. 4 Experiments 4.1 Dataset, Resources and Parameters We use the word clusters trained by Plank and Moschitti (2013) on the ukWaC corpus (Baroni et al., 2009) with 2 billion words, and the C&amp;W word embeddings from Turian el al. (2010)2 with 50 dimensions following Nguyen and Grishman (2014). In order to make the comparisons compatible, we introduce word embeddings into the tree kernel by extending the package provided by Plank and Moschitti (2013), which uses the Charniak parser to obtain the constituent trees, the SVM-light-TK for the SSTK kernel in SVM, the directional relation classes, etc. We utilize the default vector kernel in the SVM-light-TK package (d=3). For the feature-based method, we apply the MaxEnt classifier in the MALLET3 package w</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The WaCky wide web: a collection of very large linguistically processed web-crawled corpora. In Language Resources and Evaluation, pages 209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>In Journal of Machine Learning Research</journal>
<volume>3</volume>
<pages>1137--1155</pages>
<contexts>
<context position="4474" citStr="Bengio et al., 2003" startWordPosition="712" endWordPosition="715">els for DA of RE, while Nguyen and Grishman (2014) studies the appli635 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 635–644, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics cation of word clusters and word embeddings for DA of RE on the feature-based method. Although word clusters (Brown et al., 1992) have been employed by both studies to improve the performance of relation extractors across domains, the application of word embeddings (Bengio et al., 2003; Mnih and Hinton, 2008; Turian et al., 2010) for DA of RE is only examined in the feature-based method and never explored in the tree kernelbased method so far, giving rise to the first question we want to address in this paper: (i) Can word embeddings help the tree kernelbased methods on DA for RE and more importantly, in which way can we do it effectively? This question is important as word embeddings are real valued vectors, while the tree kernel-based methods rely on the symbolic matches or mismatches of concrete labels in the parse trees to compute the kernels. It is unclear at the first</context>
<context position="11300" citStr="Bengio et al., 2003" startWordPosition="1811" endWordPosition="1814">n the target domains. For instance, some words, that are very indicative in the source domain might not appear in the target domains (lexical sparsity). Consequently, the models putting high weights on such words (features) in the source domain will fail to perform well on the target domains. Nguyen and Grishman (2014) address this problem for the feature-based method in DA of RE by introducing word embeddings as additional features. The rationale is based on the fact that word embeddings are low dimensional and real valued vectors, capturing latent syntactic and semantic properties of words (Bengio et al., 2003; Mnih and Hinton, 2008; Turian et al., 2010). The embeddings of symbolically different words are often close to each other if they have similar semantic and syntactic functions. This again helps to mitigate the lexical sparsity or the vocabulary difference between the domains and has proven helpful for, amongst others, the feature-based method in DA of RE. 2.3 Tree Kernel-based vs Feature-based The feature-based method explicitly encapsulates the linguistic intuition and domain expertise for RE into the features, while the tree kernel-based method avoids the complicated feature engineering an</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. In Journal of Machine Learning Research 3, pages 1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Blacoe</author>
<author>Mirella Lapata</author>
</authors>
<title>A comparison of vector-based representations for semantic composition.</title>
<date>2012</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="17430" citStr="Blacoe and Lapata, 2012" startWordPosition="2814" endWordPosition="2817">nciple of compositionality (Werning et al., 2006; Baroni and Zamparelli, 2010; Paperno et al., 2014), the meaning of a complex expression is determined by the meanings of its components and the rules to combine them. We study the following two compositionality embeddings for relation mentions that can be generated from the embeddings of the context words: PHRASE: Vi = the mean of the embeddings of the words contained in the PET tree Ti of Ri. Although this composition is simple, it is in fact competitive to the more complicated methods based on recursive neural networks (Socher et al., 2012b; Blacoe and Lapata, 2012; Sterckx et al., 2014) on representing phrase semantics. TREE: This is motivated by the training of recursive neural networks (Socher et al., 2012a) for semantic compositionality and attempts to aggregate the context words embeddings syntactically. In particular, we compute an embedding for every node in the PET tree in a bottom-up manner. The embeddings of the leaves are the embeddings of the words associated with them while the embeddings of the internal nodes are the means of the embeddings of their children nodes. We use the embeddings of the root of the PET tree to represent the relation</context>
</contexts>
<marker>Blacoe, Lapata, 2012</marker>
<rawString>William Blacoe and Mirella Lapata. 2012. A comparison of vector-based representations for semantic composition. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="3629" citStr="Blitzer et al., 2006" startWordPosition="581" endWordPosition="584">lank and Moschitti (2013) and Nguyen and Grishman (2014) work on the unsupervised DA. In our view, unsupervised DA is more challenging, but more realistic and practical for RE as we usually do not know which target domains we need to work on in advance, thus cannot expect to possess labeled data of the target domains. Our current work therefore focuses on the single-system unsupervised DA. Besides, note that this setting tries to construct a single system that can work robustly with different but related domains (multiple target domains), thus being different from most previous studies on DA (Blitzer et al., 2006; Blitzer et al., 2007) which have attempted to design a specialized system for every specific target domain. Plank and Moschitti (2013) propose to embed word clusters and latent semantic analysis (LSA) of words into tree kernels for DA of RE, while Nguyen and Grishman (2014) studies the appli635 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 635–644, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics cation of word clusters and word embeddin</context>
<context position="34323" citStr="Blitzer et al. (2006)" startWordPosition="5648" endWordPosition="5651">ional neural networks for relation classification and extraction, respectively. Sterckx et al. (2014) utilize word embeddings to reduce noise of training data in distant supervision. Kuksa et al. (2010) present a string kernel for bio-relation extraction with word embeddings, and Yu et al. (2014; 2015) study the factor-based compositional embedding models. However, none of this work examines word embeddings for tree kernels as well as domain adaptation as we do. Regarding DA, in the unsupervised DA setting, Huang and Yates (2010) attempt to learn multidimensional feature representations while Blitzer et al. (2006) introduce structural correspondence learning. Daum´e (2007) proposes an easy adaptation framework (EA) while Xiao and Guo (2013) present a log-bilinear language adaptation technique in the supervised DA setting. Unfortunately, all of this work assumes some prior (in the form of either labeled or unlabeled data) on the target domains for the sequential labeling tasks, in contrast to our single-system unsupervised DA setting for relation extraction. An alternative method that is also popular to DA is instance weighting (Jiang and Zhai, 2007b). However, as shown by Plank and Moschitti (2013), in</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="3652" citStr="Blitzer et al., 2007" startWordPosition="585" endWordPosition="588">13) and Nguyen and Grishman (2014) work on the unsupervised DA. In our view, unsupervised DA is more challenging, but more realistic and practical for RE as we usually do not know which target domains we need to work on in advance, thus cannot expect to possess labeled data of the target domains. Our current work therefore focuses on the single-system unsupervised DA. Besides, note that this setting tries to construct a single system that can work robustly with different but related domains (multiple target domains), thus being different from most previous studies on DA (Blitzer et al., 2006; Blitzer et al., 2007) which have attempted to design a specialized system for every specific target domain. Plank and Moschitti (2013) propose to embed word clusters and latent semantic analysis (LSA) of words into tree kernels for DA of RE, while Nguyen and Grishman (2014) studies the appli635 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 635–644, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics cation of word clusters and word embeddings for DA of RE on the </context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<booktitle>In Computational Linguistics,</booktitle>
<pages>467--479</pages>
<contexts>
<context position="4317" citStr="Brown et al., 1992" startWordPosition="686" endWordPosition="689">system for every specific target domain. Plank and Moschitti (2013) propose to embed word clusters and latent semantic analysis (LSA) of words into tree kernels for DA of RE, while Nguyen and Grishman (2014) studies the appli635 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 635–644, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics cation of word clusters and word embeddings for DA of RE on the feature-based method. Although word clusters (Brown et al., 1992) have been employed by both studies to improve the performance of relation extractors across domains, the application of word embeddings (Bengio et al., 2003; Mnih and Hinton, 2008; Turian et al., 2010) for DA of RE is only examined in the feature-based method and never explored in the tree kernelbased method so far, giving rise to the first question we want to address in this paper: (i) Can word embeddings help the tree kernelbased methods on DA for RE and more importantly, in which way can we do it effectively? This question is important as word embeddings are real valued vectors, while the </context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based n-gram models of natural language. In Computational Linguistics, pages 467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction.</title>
<date>2005</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1449" citStr="Bunescu and Mooney, 2005" startWordPosition="208" endWordPosition="211">ompare the tree kernel-based and the feature-based method for RE in a compatible way, on the same resources and settings, to gain insights into which kind of system is more robust to domain changes. Our results and error analysis shows that the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance of the traditional RE techniques degrade</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2005. A shortest path dependency kernel for relation extraction. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
</authors>
<title>Learning to extract relations from the web using minimal supervision.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1509" citStr="Bunescu, 2007" startWordPosition="220" endWordPosition="221">ompatible way, on the same resources and settings, to gain insights into which kind of system is more robust to domain changes. Our results and error analysis shows that the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance of the traditional RE techniques degrades significantly in such a domain mismatch case (Plank and Mo</context>
</contexts>
<marker>Bunescu, 2007</marker>
<rawString>Razvan C. Bunescu. 2007. Learning to extract relations from the web using minimal supervision. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Dan Roth</author>
</authors>
<title>Exploiting background knowledge for relation extraction.</title>
<date>2010</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="1683" citStr="Chan and Roth, 2010" startWordPosition="249" endWordPosition="252">t the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance of the traditional RE techniques degrades significantly in such a domain mismatch case (Plank and Moschitti, 2013). To alleviate this performance loss, we need to resort to domain adaptation (DA) techniques to adapt a system trained on some source domain to perform well on </context>
</contexts>
<marker>Chan, Roth, 2010</marker>
<rawString>Yee Seng Chan and Dan Roth. 2010. Exploiting background knowledge for relation extraction. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daume</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<marker>Daume, 2007</marker>
<rawString>Hal Daume. 2007. Frustratingly easy domain adaptation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>David Westbrook</author>
<author>Adam Meyers</author>
</authors>
<title>Nyu’s english ace 2005 system description.</title>
<date>2005</date>
<booktitle>In The ACE 2005 Evaluation Workshop.</booktitle>
<contexts>
<context position="1620" citStr="Grishman et al., 2005" startWordPosition="237" endWordPosition="240">obust to domain changes. Our results and error analysis shows that the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance of the traditional RE techniques degrades significantly in such a domain mismatch case (Plank and Moschitti, 2013). To alleviate this performance loss, we need to resort to domain adaptation (DA) techniques to a</context>
</contexts>
<marker>Grishman, Westbrook, Meyers, 2005</marker>
<rawString>Ralph Grishman, David Westbrook, and Adam Meyers. 2005. Nyu’s english ace 2005 system description. In The ACE 2005 Evaluation Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Alexander Yates</author>
</authors>
<title>Exploring representation-learning approaches to domain adaptation.</title>
<date>2010</date>
<booktitle>In The ACL Workshop on Domain Adaptation for Natural Language Processing (DANLP).</booktitle>
<contexts>
<context position="34237" citStr="Huang and Yates (2010)" startWordPosition="5636" endWordPosition="5639">14), and Nguyen and Grishman (2015) employ word embeddings in the framework of convolutional neural networks for relation classification and extraction, respectively. Sterckx et al. (2014) utilize word embeddings to reduce noise of training data in distant supervision. Kuksa et al. (2010) present a string kernel for bio-relation extraction with word embeddings, and Yu et al. (2014; 2015) study the factor-based compositional embedding models. However, none of this work examines word embeddings for tree kernels as well as domain adaptation as we do. Regarding DA, in the unsupervised DA setting, Huang and Yates (2010) attempt to learn multidimensional feature representations while Blitzer et al. (2006) introduce structural correspondence learning. Daum´e (2007) proposes an easy adaptation framework (EA) while Xiao and Guo (2013) present a log-bilinear language adaptation technique in the supervised DA setting. Unfortunately, all of this work assumes some prior (in the form of either labeled or unlabeled data) on the target domains for the sequential labeling tasks, in contrast to our single-system unsupervised DA setting for relation extraction. An alternative method that is also popular to DA is instance </context>
</contexts>
<marker>Huang, Yates, 2010</marker>
<rawString>Fei Huang and Alexander Yates. 2010. Exploring representation-learning approaches to domain adaptation. In The ACL Workshop on Domain Adaptation for Natural Language Processing (DANLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>A systematic exploration of the feature space for relation extraction.</title>
<date>2007</date>
<booktitle>In NAACL-HLT.</booktitle>
<contexts>
<context position="1661" citStr="Jiang and Zhai, 2007" startWordPosition="245" endWordPosition="248">rror analysis shows that the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance of the traditional RE techniques degrades significantly in such a domain mismatch case (Plank and Moschitti, 2013). To alleviate this performance loss, we need to resort to domain adaptation (DA) techniques to adapt a system trained on some source doma</context>
<context position="13009" citStr="Jiang and Zhai, 2007" startWordPosition="2093" endWordPosition="2096">e feature-based method, we utilize all the features extractable from the PET trees that are standardly used in the state-of-the-art featurebased systems for DA of RE (Nguyen and Grishman, 2014). Specifically, the feature set employed in this paper (denoted by FET) includes: the lexical features, i.e., the context words, the head words, the bigrams, the number of words, the lexical path, the order of mention (Zhou et al., 2005; Sun et al., 2011); and the syntactic features, i.e., the path connecting the two mentions in PET and the unigrams, bigrams, trigrams along this path (Zhou et al., 2005; Jiang and Zhai, 2007a). Hypothesis: Assuming identical settings and resources, we hypothesize that the tree kernelbased method is better than the feature-based method for DA of RE. This is motivated because of at least two reasons: (i) the tree kernel-based method implicitly encodes a more comprehensive feature set (involving all the sub-trees in the PETs), thus potentially captures more domainindependent features to be useful for DA of RE; (ii) the tree kernel-based method avoids the inclusion of fine-tuned and domain-specific features originated from the excessive feature engineering (i.e., hand-designing featu</context>
<context position="34868" citStr="Jiang and Zhai, 2007" startWordPosition="5732" endWordPosition="5735"> learn multidimensional feature representations while Blitzer et al. (2006) introduce structural correspondence learning. Daum´e (2007) proposes an easy adaptation framework (EA) while Xiao and Guo (2013) present a log-bilinear language adaptation technique in the supervised DA setting. Unfortunately, all of this work assumes some prior (in the form of either labeled or unlabeled data) on the target domains for the sequential labeling tasks, in contrast to our single-system unsupervised DA setting for relation extraction. An alternative method that is also popular to DA is instance weighting (Jiang and Zhai, 2007b). However, as shown by Plank and Moschitti (2013), instance weighting is not 5The full sentence is: “After today’s air strikes, Iraqi soldiers abandoned their posts and surrendered to Kurdish fighters.”. 642 very useful for DA of RE. Stephan Bloehdorn and Alessandro Moschitti. 2007. Exploiting Structure and Semantics for Expressive 7 Conclusion Text Kernels. In CIKM. In order to improve the generalization of relation extractors, we propose to augment the semantic syntactic tree kernels with the semantic representation of relation mentions, generated from the word embeddings of the context wo</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007a. A systematic exploration of the feature space for relation extraction. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Instance weighting for domain adaptation in nlp.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1661" citStr="Jiang and Zhai, 2007" startWordPosition="245" endWordPosition="248">rror analysis shows that the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance of the traditional RE techniques degrades significantly in such a domain mismatch case (Plank and Moschitti, 2013). To alleviate this performance loss, we need to resort to domain adaptation (DA) techniques to adapt a system trained on some source doma</context>
<context position="13009" citStr="Jiang and Zhai, 2007" startWordPosition="2093" endWordPosition="2096">e feature-based method, we utilize all the features extractable from the PET trees that are standardly used in the state-of-the-art featurebased systems for DA of RE (Nguyen and Grishman, 2014). Specifically, the feature set employed in this paper (denoted by FET) includes: the lexical features, i.e., the context words, the head words, the bigrams, the number of words, the lexical path, the order of mention (Zhou et al., 2005; Sun et al., 2011); and the syntactic features, i.e., the path connecting the two mentions in PET and the unigrams, bigrams, trigrams along this path (Zhou et al., 2005; Jiang and Zhai, 2007a). Hypothesis: Assuming identical settings and resources, we hypothesize that the tree kernelbased method is better than the feature-based method for DA of RE. This is motivated because of at least two reasons: (i) the tree kernel-based method implicitly encodes a more comprehensive feature set (involving all the sub-trees in the PETs), thus potentially captures more domainindependent features to be useful for DA of RE; (ii) the tree kernel-based method avoids the inclusion of fine-tuned and domain-specific features originated from the excessive feature engineering (i.e., hand-designing featu</context>
<context position="34868" citStr="Jiang and Zhai, 2007" startWordPosition="5732" endWordPosition="5735"> learn multidimensional feature representations while Blitzer et al. (2006) introduce structural correspondence learning. Daum´e (2007) proposes an easy adaptation framework (EA) while Xiao and Guo (2013) present a log-bilinear language adaptation technique in the supervised DA setting. Unfortunately, all of this work assumes some prior (in the form of either labeled or unlabeled data) on the target domains for the sequential labeling tasks, in contrast to our single-system unsupervised DA setting for relation extraction. An alternative method that is also popular to DA is instance weighting (Jiang and Zhai, 2007b). However, as shown by Plank and Moschitti (2013), instance weighting is not 5The full sentence is: “After today’s air strikes, Iraqi soldiers abandoned their posts and surrendered to Kurdish fighters.”. 642 very useful for DA of RE. Stephan Bloehdorn and Alessandro Moschitti. 2007. Exploiting Structure and Semantics for Expressive 7 Conclusion Text Kernels. In CIKM. In order to improve the generalization of relation extractors, we propose to augment the semantic syntactic tree kernels with the semantic representation of relation mentions, generated from the word embeddings of the context wo</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007b. Instance weighting for domain adaptation in nlp. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nanda Kambhatla</author>
</authors>
<title>Combining lexical, syntactic, and semantic features with maximum entropy models for information extraction.</title>
<date>2004</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1597" citStr="Kambhatla, 2004" startWordPosition="235" endWordPosition="236"> system is more robust to domain changes. Our results and error analysis shows that the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance of the traditional RE techniques degrades significantly in such a domain mismatch case (Plank and Moschitti, 2013). To alleviate this performance loss, we need to resort to domain adaptati</context>
</contexts>
<marker>Kambhatla, 2004</marker>
<rawString>Nanda Kambhatla. 2004. Combining lexical, syntactic, and semantic features with maximum entropy models for information extraction. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pavel Kuksa</author>
<author>Yanjun Qi</author>
<author>Bing Bai</author>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>Vladimir Pavlovic</author>
<author>Xia Ning</author>
</authors>
<title>Semi-supervised abstractionaugmented string kernel for multi-level bio-relation extraction.</title>
<date>2010</date>
<booktitle>In ECML PKDD.</booktitle>
<contexts>
<context position="33904" citStr="Kuksa et al. (2010)" startWordPosition="5583" endWordPosition="5586">s closest to Y2 in the new kernel function, consequently helping the tree kernel-based method work correctly in this case. 6 Related work Word embeddings are only applied to RE recently. Socher et al. (2012b) use word embeddings as input for matrix-vector recursive neural networks in relation classification while Zeng et al. (2014), and Nguyen and Grishman (2015) employ word embeddings in the framework of convolutional neural networks for relation classification and extraction, respectively. Sterckx et al. (2014) utilize word embeddings to reduce noise of training data in distant supervision. Kuksa et al. (2010) present a string kernel for bio-relation extraction with word embeddings, and Yu et al. (2014; 2015) study the factor-based compositional embedding models. However, none of this work examines word embeddings for tree kernels as well as domain adaptation as we do. Regarding DA, in the unsupervised DA setting, Huang and Yates (2010) attempt to learn multidimensional feature representations while Blitzer et al. (2006) introduce structural correspondence learning. Daum´e (2007) proposes an easy adaptation framework (EA) while Xiao and Guo (2013) present a log-bilinear language adaptation techniqu</context>
</contexts>
<marker>Kuksa, Qi, Bai, Collobert, Weston, Pavlovic, Ning, 2010</marker>
<rawString>Pavel Kuksa, Yanjun Qi, Bing Bai, Ronan Collobert, Jason Weston, Vladimir Pavlovic, and Xia Ning. 2010. Semi-supervised abstractionaugmented string kernel for multi-level bio-relation extraction. In ECML PKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>A scalable hierarchical distributed language model.</title>
<date>2008</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="4497" citStr="Mnih and Hinton, 2008" startWordPosition="716" endWordPosition="719">le Nguyen and Grishman (2014) studies the appli635 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 635–644, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics cation of word clusters and word embeddings for DA of RE on the feature-based method. Although word clusters (Brown et al., 1992) have been employed by both studies to improve the performance of relation extractors across domains, the application of word embeddings (Bengio et al., 2003; Mnih and Hinton, 2008; Turian et al., 2010) for DA of RE is only examined in the feature-based method and never explored in the tree kernelbased method so far, giving rise to the first question we want to address in this paper: (i) Can word embeddings help the tree kernelbased methods on DA for RE and more importantly, in which way can we do it effectively? This question is important as word embeddings are real valued vectors, while the tree kernel-based methods rely on the symbolic matches or mismatches of concrete labels in the parse trees to compute the kernels. It is unclear at the first glance how to encode w</context>
<context position="11323" citStr="Mnih and Hinton, 2008" startWordPosition="1815" endWordPosition="1818"> For instance, some words, that are very indicative in the source domain might not appear in the target domains (lexical sparsity). Consequently, the models putting high weights on such words (features) in the source domain will fail to perform well on the target domains. Nguyen and Grishman (2014) address this problem for the feature-based method in DA of RE by introducing word embeddings as additional features. The rationale is based on the fact that word embeddings are low dimensional and real valued vectors, capturing latent syntactic and semantic properties of words (Bengio et al., 2003; Mnih and Hinton, 2008; Turian et al., 2010). The embeddings of symbolically different words are often close to each other if they have similar semantic and syntactic functions. This again helps to mitigate the lexical sparsity or the vocabulary difference between the domains and has proven helpful for, amongst others, the feature-based method in DA of RE. 2.3 Tree Kernel-based vs Feature-based The feature-based method explicitly encapsulates the linguistic intuition and domain expertise for RE into the features, while the tree kernel-based method avoids the complicated feature engineering and implicitly encode the</context>
</contexts>
<marker>Mnih, Hinton, 2008</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. 2008. A scalable hierarchical distributed language model. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient convolution kernels for dependency and constituent syntactic trees.</title>
<date>2006</date>
<booktitle>In ECML.</booktitle>
<contexts>
<context position="7906" citStr="Moschitti, 2006" startWordPosition="1274" endWordPosition="1275">Wang, 2008; Plank and Moschitti, 2013) and we believe there is a need to tackle this increasing confusion in this line of research. Therefore, this is actually the first attempt to compare the two methods (tree kernel-based and feature-based) on the same settings. To ease the comparison for future work and circumvent the Zigglebottom pitfall (Pedersen, 2008), the entire setup and package is available.1 2 Relation Extraction Approaches In the following, we introduce the two relation extraction systems further examined in this study. 2.1 Tree kernel-based Method In the tree kernel-based method (Moschitti, 2006; Moschitti, 2008; Plank and Moschitti, 2013), a relation mention (the two entity mentions and the sentence containing them) is represented by the path-enclosed tree (PET), the smallest constituency-based subtree including the two target entity mentions (Zhang et al., 2006). The syntactic tree kernel (STK) is then defined to compute the similarity between two PET trees (where target entities are marked) by counting the common sub-trees, without enumerating the whole fragment space (Moschitti, 2006; Moschitti, 2008). STK is then applied in the support vector machines (SVMs) for RE. The major li</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In ECML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Kernel methods, syntax and semantics for relational text categorization.</title>
<date>2008</date>
<booktitle>In CIKM.</booktitle>
<contexts>
<context position="7923" citStr="Moschitti, 2008" startWordPosition="1276" endWordPosition="1277"> and Moschitti, 2013) and we believe there is a need to tackle this increasing confusion in this line of research. Therefore, this is actually the first attempt to compare the two methods (tree kernel-based and feature-based) on the same settings. To ease the comparison for future work and circumvent the Zigglebottom pitfall (Pedersen, 2008), the entire setup and package is available.1 2 Relation Extraction Approaches In the following, we introduce the two relation extraction systems further examined in this study. 2.1 Tree kernel-based Method In the tree kernel-based method (Moschitti, 2006; Moschitti, 2008; Plank and Moschitti, 2013), a relation mention (the two entity mentions and the sentence containing them) is represented by the path-enclosed tree (PET), the smallest constituency-based subtree including the two target entity mentions (Zhang et al., 2006). The syntactic tree kernel (STK) is then defined to compute the similarity between two PET trees (where target entities are marked) by counting the common sub-trees, without enumerating the whole fragment space (Moschitti, 2006; Moschitti, 2008). STK is then applied in the support vector machines (SVMs) for RE. The major limitation of STK i</context>
</contexts>
<marker>Moschitti, 2008</marker>
<rawString>Alessandro Moschitti. 2008. Kernel methods, syntax and semantics for relational text categorization. In CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thien Huu Nguyen</author>
<author>Ralph Grishman</author>
</authors>
<title>Employing word representations and regularization for domain adaptation of relation extraction.</title>
<date>2014</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2854" citStr="Nguyen and Grishman, 2014" startWordPosition="450" endWordPosition="453">stem trained on some source domain to perform well on new target domains. We here focus on the unsupervised domain adaptation (i.e., no labeled target data) and singlesystem DA (Petrov and McDonald, 2012; Plank and Moschitti, 2013), i.e., building a single system that is able to cope with different, yet related target domains. While DA has been investigated extensively in the last decade for various natural language processing (NLP) tasks, the examination of DA for RE is only very recent. To the best of our knowledge, there have been only three studies on DA for RE (Plank and Moschitti, 2013; Nguyen and Grishman, 2014; Nguyen et al., 2014). Of these, Nguyen et al. (2014) follow the supervised DA paradigm and assume some labeled data in the target domains. In contrast, Plank and Moschitti (2013) and Nguyen and Grishman (2014) work on the unsupervised DA. In our view, unsupervised DA is more challenging, but more realistic and practical for RE as we usually do not know which target domains we need to work on in advance, thus cannot expect to possess labeled data of the target domains. Our current work therefore focuses on the single-system unsupervised DA. Besides, note that this setting tries to construct a</context>
<context position="5870" citStr="Nguyen and Grishman (2014)" startWordPosition="950" endWordPosition="953">use word embeddings to compute similarities between words and embed these similarity scores into the kernel functions, e.g., by resembling the method of Plank and Moschitti (2013) that exploited LSA (in the semantic syntactic tree kernel (SSTK), cf. §2.1). We explore various methods to apply word embeddings to generate the semantic representations for DA of RE and demonstrate that semantic representations are very effective to significantly improve the portability of the relation extractors based on the tree kernels, bringing us to the second question: (ii) Between the feature-based method in Nguyen and Grishman (2014) and the SSTK method in Plank and Moschitti (2013), which method is better for DA of RE, given the recent discovery of word embeddings for both methods? It is worth noting that besides the approach difference, these two works employ rather different resources and settings in their evaluation, making it impossible to directly compare their performance. In particular, while Plank and Moschitti (2013) only use the path-enclosed trees induced from the constituent parse trees as the representation for relation mentions, Nguyen and Grishman (2014) include a rich set of features extracted from multip</context>
<context position="10143" citStr="Nguyen and Grishman, 2014" startWordPosition="1625" endWordPosition="1628">ween words (i.e, either 1 or 0) by the similarities induced from the latent semantic analysis (LSA) of large corpus. The former generalizes the part-of-speech similarity to the semantic similarity on word clusters; the latter, on the other hand, allows soft matches between words that have the same latent semantic but differ in symbolic representation. Both techniques emphasize the invariants of word semantics in different domains, thus being helpful to alleviate the vocabulary difference across domains. 2.2 Feature-based Method In the feature-based method (Zhou et al., 2005; Sun et al., 2011; Nguyen and Grishman, 2014), relation mentions are first transformed into rich feature vectors that capture various characteristics of the relation mentions (i.e, lexicon, syntax, semantics etc). The resulting vectors are then fed into the statistical classifiers such as Maximum Entropy (MaxEnt) to perform classification for RE. The main reason for the performance loss of the feature-based systems on new domains is the behavioral changes of the features when domains shift. Some features might be very informative in the source domain but become less relevant in the target domains. For instance, some words, that are very </context>
<context position="12101" citStr="Nguyen and Grishman, 2014" startWordPosition="1938" endWordPosition="1941">ns. This again helps to mitigate the lexical sparsity or the vocabulary difference between the domains and has proven helpful for, amongst others, the feature-based method in DA of RE. 2.3 Tree Kernel-based vs Feature-based The feature-based method explicitly encapsulates the linguistic intuition and domain expertise for RE into the features, while the tree kernel-based method avoids the complicated feature engineering and implicitly encode the features into the computation of the tree kernels. Which method is better for DA of RE? In order to ensure the two methods (Plank and Moschitti, 2013; Nguyen and Grishman, 2014) are compared compatibly on the same resources, we make sure the two systems have access to the same amount of information. Thus, we follow Plank and Moschitti (2013) and use the PET trees (beside word clusters and word embeddings) as the only resource the two methods can exploit. For the feature-based method, we utilize all the features extractable from the PET trees that are standardly used in the state-of-the-art featurebased systems for DA of RE (Nguyen and Grishman, 2014). Specifically, the feature set employed in this paper (denoted by FET) includes: the lexical features, i.e., the conte</context>
<context position="16591" citStr="Nguyen and Grishman (2014)" startWordPosition="2677" endWordPosition="2680">new representation of Ri = (Ti, Vi). The new kernel where Kvec(Vi, Vj) is some standard vector kernel like the polynomial kernels. α is a trade-off parameter and indicates whether the system attributes more weight to the traditional SSTK or the new semantic kernel Kvec. In this work, we consider the following methods to obtain the semantic representation Vi from the word embeddings of the context words of Ri (assuming d is the dimensionality of the word embeddings): HEAD: Vi = the concatenation of the word embeddings of the two entity mention heads of Ri. This representation is inherited from Nguyen and Grishman (2014) that only examine embeddings at the word level separately for the feature-based method without considering the compositionality embeddings of relation mentions. The dimensionality of HEAD is 2d. According to the principle of compositionality (Werning et al., 2006; Baroni and Zamparelli, 2010; Paperno et al., 2014), the meaning of a complex expression is determined by the meanings of its components and the rules to combine them. We study the following two compositionality embeddings for relation mentions that can be generated from the embeddings of the context words: PHRASE: Vi = the mean of t</context>
<context position="18859" citStr="Nguyen and Grishman (2014)" startWordPosition="3050" endWordPosition="3053">ent with a more obvious way to introduce word embeddings into tree kernels, resembling more closely the approach of Plank and Moschitti (2013). In particularly, the SIM method simply replaces the similarity scores between word pairs obtained from LSA by the cosine similarities between the word embeddings to be used in the SSTK kernel. 4 Experiments 4.1 Dataset, Resources and Parameters We use the word clusters trained by Plank and Moschitti (2013) on the ukWaC corpus (Baroni et al., 2009) with 2 billion words, and the C&amp;W word embeddings from Turian el al. (2010)2 with 50 dimensions following Nguyen and Grishman (2014). In order to make the comparisons compatible, we introduce word embeddings into the tree kernel by extending the package provided by Plank and Moschitti (2013), which uses the Charniak parser to obtain the constituent trees, the SVM-light-TK for the SSTK kernel in SVM, the directional relation classes, etc. We utilize the default vector kernel in the SVM-light-TK package (d=3). For the feature-based method, we apply the MaxEnt classifier in the MALLET3 package with the L2 regularizer on the hierarchical architecture for relation extraction as in Nguyen and Grishman (2014). Following prior wor</context>
<context position="26169" citStr="Nguyen and Grishman (2014)" startWordPosition="4311" endWordPosition="4314">C+PET LSA for the target domains bc and wl, and PET+PET WC for the target domain cts). To be more concrete, the best system with word embeddings (row 12 in Table 2) significantly outperforms the best system in Plank and Moschitti (2013) with p &lt; 0.05, an improvement of 3.7%, 1.1% and 2.7% on the target domains bc, cts and wl respectively, demonstrating the benefit of word embeddings for DA of RE in the tree kernel-based method. 4.4 Tree Kernel-based vs Feature-based DA of RE This section aims to compare the tree kernel-based method in Plank and Moschitti (2013) and the feature-based method in Nguyen and Grishman (2014) for DA of RE on the same settings (i.e, same dataset partition, the same pre-processing 640 nw+bn (in-dom.) P: bc F1: P: cts F1: P: wl F1: System: P: R: F1: R: R: R: Tree kernel-based: PET+PET WC+HEAD+PHRASE 56.3 50.3 53.1 57.5 46.6 51.5 55.6 39.8 46.4 41.5 37.9 39.6 Feature-based: FET+WC+HEAD 44.5 51.0 47.5 46.5 49.3 47.8 44.5 40.0 42.1 35.4 39.5 37.3 FET+WC+TREE 44.4 50.2 47.1 46.4 48.7 47.6 43.7 40.3 41.9 32.7 36.7 34.6 FET+WC+HEAD+PHRASE 44.9 51.6 48.0 46.0 49.1 47.5 45.2 41.5 43.3 34.7 39.2 36.8 FET+WC+HEAD+TREE 45.1 51.0 47.8 46.9 48.4 47.6 43.8 39.5 41.5 34.7 38.8 36.6 Table 3: Tree ke</context>
<context position="28056" citStr="Nguyen and Grishman, 2014" startWordPosition="4622" endWordPosition="4625"> tree kernel-based system, and then compare the performance of the two methods when they are augmented with their best corresponding embedding combinations. System P R F1 B 51.2 49.4 50.3 B+HEAD 55.8 52.4 54.0 B+PHRASE 50.7 46.2 48.4 B+TREE 53.6 51.1 52.3 B+HEAD+PHRASE 53.2 50.1 51.6 B+HEAD+TREE 54.9 51.4 53.1 B+PHRASE+TREE 50.7 48.4 49.5 B+HEAD+PHRASE+TREE 52.7 49.4 51.0 Table 4: Performance of the feature-based method (dev). Table 4 presents the evaluation results on the bc development for the feature-based system where B is the baseline feature set consisting of FET and word clusters (WC) (Nguyen and Grishman, 2014). The Role of Semantic Representations Considering Table 4 for the feature-based method and Table 1 for the tree kernel-based method, we see that when combined with the HEAD embeddings, the compositionality embedding TREE is more effective for the feature-based method, in contrast to the tree kernel-based method, where the PHRASE embeddings are better. This can be partly explained by the fact that the tree kernel-based method emphasizes the syntactic structure of the relation mentions, while the feature-based method exploits the sequential structure more. Consequently, the syntactic semantics </context>
</contexts>
<marker>Nguyen, Grishman, 2014</marker>
<rawString>Thien Huu Nguyen and Ralph Grishman. 2014. Employing word representations and regularization for domain adaptation of relation extraction. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thien Huu Nguyen</author>
<author>Ralph Grishman</author>
</authors>
<title>Relation extraction: Perspective from convolutional neural networks.</title>
<date>2015</date>
<booktitle>In The NAACL Workshop on Vector Space Modeling for NLP (VSM).</booktitle>
<contexts>
<context position="33650" citStr="Nguyen and Grishman (2015)" startWordPosition="5545" endWordPosition="5548">mbedding to Y1 in the source domain is Y2: “Iraqi soldiers abandoned their posts”,5 which is annotated with the NONE relation between “Iraqi soldiers” and “their posts”. As the syntactic structure of Y2 is also very similar to Y1, it is not surprising that Y1 is closest to Y2 in the new kernel function, consequently helping the tree kernel-based method work correctly in this case. 6 Related work Word embeddings are only applied to RE recently. Socher et al. (2012b) use word embeddings as input for matrix-vector recursive neural networks in relation classification while Zeng et al. (2014), and Nguyen and Grishman (2015) employ word embeddings in the framework of convolutional neural networks for relation classification and extraction, respectively. Sterckx et al. (2014) utilize word embeddings to reduce noise of training data in distant supervision. Kuksa et al. (2010) present a string kernel for bio-relation extraction with word embeddings, and Yu et al. (2014; 2015) study the factor-based compositional embedding models. However, none of this work examines word embeddings for tree kernels as well as domain adaptation as we do. Regarding DA, in the unsupervised DA setting, Huang and Yates (2010) attempt to l</context>
</contexts>
<marker>Nguyen, Grishman, 2015</marker>
<rawString>Thien Huu Nguyen and Ralph Grishman. 2015. Relation extraction: Perspective from convolutional neural networks. In The NAACL Workshop on Vector Space Modeling for NLP (VSM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Truc-Vien Nguyen</author>
<author>Alessandro Moschitti</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Convolution kernels on constituent, dependency and sequential structures for relation extraction.</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1550" citStr="Nguyen et al., 2009" startWordPosition="226" endWordPosition="229">es and settings, to gain insights into which kind of system is more robust to domain changes. Our results and error analysis shows that the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance of the traditional RE techniques degrades significantly in such a domain mismatch case (Plank and Moschitti, 2013). To alleviate this perform</context>
</contexts>
<marker>Nguyen, Moschitti, Riccardi, 2009</marker>
<rawString>T. Truc-Vien Nguyen, Alessandro Moschitti, and Giuseppe Riccardi. 2009. Convolution kernels on constituent, dependency and sequential structures for relation extraction. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luan Minh Nguyen</author>
<author>W Ivor Tsang</author>
<author>A Kian Ming Chai</author>
<author>Leong Hai Chieu</author>
</authors>
<title>Robust domain adaptation for relation extraction via clustering consistency.</title>
<date>2014</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2876" citStr="Nguyen et al., 2014" startWordPosition="454" endWordPosition="457"> domain to perform well on new target domains. We here focus on the unsupervised domain adaptation (i.e., no labeled target data) and singlesystem DA (Petrov and McDonald, 2012; Plank and Moschitti, 2013), i.e., building a single system that is able to cope with different, yet related target domains. While DA has been investigated extensively in the last decade for various natural language processing (NLP) tasks, the examination of DA for RE is only very recent. To the best of our knowledge, there have been only three studies on DA for RE (Plank and Moschitti, 2013; Nguyen and Grishman, 2014; Nguyen et al., 2014). Of these, Nguyen et al. (2014) follow the supervised DA paradigm and assume some labeled data in the target domains. In contrast, Plank and Moschitti (2013) and Nguyen and Grishman (2014) work on the unsupervised DA. In our view, unsupervised DA is more challenging, but more realistic and practical for RE as we usually do not know which target domains we need to work on in advance, thus cannot expect to possess labeled data of the target domains. Our current work therefore focuses on the single-system unsupervised DA. Besides, note that this setting tries to construct a single system that ca</context>
</contexts>
<marker>Nguyen, Tsang, Chai, Chieu, 2014</marker>
<rawString>Luan Minh Nguyen, W. Ivor Tsang, A. Kian Ming Chai, and Leong Hai Chieu. 2014. Robust domain adaptation for relation extraction via clustering consistency. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denis Paperno</author>
</authors>
<title>The Nghia Pham, and</title>
<date>2014</date>
<booktitle>In ACL.</booktitle>
<marker>Paperno, 2014</marker>
<rawString>Denis Paperno, The Nghia Pham, and Marco Baroni. 2014. A practical and linguistically-motivated approach to compositional distributional semantics. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>Empiricism is not a matter of faith.</title>
<date>2008</date>
<booktitle>In Computational Linguistics 3,</booktitle>
<pages>465--470</pages>
<contexts>
<context position="7651" citStr="Pedersen, 2008" startWordPosition="1236" endWordPosition="1237">re directed to use the same resources and settings, and are thus compared in a compatible manner to achieve an insight on their effectiveness for DA of RE. In fact, the problem of incompatible comparison is unfortunately very common in the RE literature (Wang, 2008; Plank and Moschitti, 2013) and we believe there is a need to tackle this increasing confusion in this line of research. Therefore, this is actually the first attempt to compare the two methods (tree kernel-based and feature-based) on the same settings. To ease the comparison for future work and circumvent the Zigglebottom pitfall (Pedersen, 2008), the entire setup and package is available.1 2 Relation Extraction Approaches In the following, we introduce the two relation extraction systems further examined in this study. 2.1 Tree kernel-based Method In the tree kernel-based method (Moschitti, 2006; Moschitti, 2008; Plank and Moschitti, 2013), a relation mention (the two entity mentions and the sentence containing them) is represented by the path-enclosed tree (PET), the smallest constituency-based subtree including the two target entity mentions (Zhang et al., 2006). The syntactic tree kernel (STK) is then defined to compute the simila</context>
</contexts>
<marker>Pedersen, 2008</marker>
<rawString>Ted Pedersen. 2008. Empiricism is not a matter of faith. In Computational Linguistics 3, pages 465– 470.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
</authors>
<title>Overview of the 2012 shared task on parsing the web.</title>
<date>2012</date>
<booktitle>In The First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL).</booktitle>
<contexts>
<context position="2432" citStr="Petrov and McDonald, 2012" startWordPosition="376" endWordPosition="379"> to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance of the traditional RE techniques degrades significantly in such a domain mismatch case (Plank and Moschitti, 2013). To alleviate this performance loss, we need to resort to domain adaptation (DA) techniques to adapt a system trained on some source domain to perform well on new target domains. We here focus on the unsupervised domain adaptation (i.e., no labeled target data) and singlesystem DA (Petrov and McDonald, 2012; Plank and Moschitti, 2013), i.e., building a single system that is able to cope with different, yet related target domains. While DA has been investigated extensively in the last decade for various natural language processing (NLP) tasks, the examination of DA for RE is only very recent. To the best of our knowledge, there have been only three studies on DA for RE (Plank and Moschitti, 2013; Nguyen and Grishman, 2014; Nguyen et al., 2014). Of these, Nguyen et al. (2014) follow the supervised DA paradigm and assume some labeled data in the target domains. In contrast, Plank and Moschitti (201</context>
</contexts>
<marker>Petrov, McDonald, 2012</marker>
<rawString>Slav Petrov and Ryan McDonald. 2012. Overview of the 2012 shared task on parsing the web. In The First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Embedding semantic similarity in tree kernels for domain adaptation of relation extraction.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2123" citStr="Plank and Moschitti, 2013" startWordPosition="323" endWordPosition="326">unescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance of the traditional RE techniques degrades significantly in such a domain mismatch case (Plank and Moschitti, 2013). To alleviate this performance loss, we need to resort to domain adaptation (DA) techniques to adapt a system trained on some source domain to perform well on new target domains. We here focus on the unsupervised domain adaptation (i.e., no labeled target data) and singlesystem DA (Petrov and McDonald, 2012; Plank and Moschitti, 2013), i.e., building a single system that is able to cope with different, yet related target domains. While DA has been investigated extensively in the last decade for various natural language processing (NLP) tasks, the examination of DA for RE is only very recent. </context>
<context position="3765" citStr="Plank and Moschitti (2013)" startWordPosition="602" endWordPosition="605">ging, but more realistic and practical for RE as we usually do not know which target domains we need to work on in advance, thus cannot expect to possess labeled data of the target domains. Our current work therefore focuses on the single-system unsupervised DA. Besides, note that this setting tries to construct a single system that can work robustly with different but related domains (multiple target domains), thus being different from most previous studies on DA (Blitzer et al., 2006; Blitzer et al., 2007) which have attempted to design a specialized system for every specific target domain. Plank and Moschitti (2013) propose to embed word clusters and latent semantic analysis (LSA) of words into tree kernels for DA of RE, while Nguyen and Grishman (2014) studies the appli635 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 635–644, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics cation of word clusters and word embeddings for DA of RE on the feature-based method. Although word clusters (Brown et al., 1992) have been employed by both studies to improve t</context>
<context position="5423" citStr="Plank and Moschitti (2013)" startWordPosition="880" endWordPosition="883">in which way can we do it effectively? This question is important as word embeddings are real valued vectors, while the tree kernel-based methods rely on the symbolic matches or mismatches of concrete labels in the parse trees to compute the kernels. It is unclear at the first glance how to encode word embeddings into the tree kernels effectively so that word embeddings could help to improve the generalization performance of RE. One way is to use word embeddings to compute similarities between words and embed these similarity scores into the kernel functions, e.g., by resembling the method of Plank and Moschitti (2013) that exploited LSA (in the semantic syntactic tree kernel (SSTK), cf. §2.1). We explore various methods to apply word embeddings to generate the semantic representations for DA of RE and demonstrate that semantic representations are very effective to significantly improve the portability of the relation extractors based on the tree kernels, bringing us to the second question: (ii) Between the feature-based method in Nguyen and Grishman (2014) and the SSTK method in Plank and Moschitti (2013), which method is better for DA of RE, given the recent discovery of word embeddings for both methods? </context>
<context position="7329" citStr="Plank and Moschitti, 2013" startWordPosition="1182" endWordPosition="1185">relation classes and their inverses) but Nguyen and Grishman (2014) disregard this relation direction. Finally, we note that although both studies evaluate their systems on the ACE 2005 dataset, they actually have different dataset partitions. In order to overcome this limitation, we conduct an evaluation in which the two methods are directed to use the same resources and settings, and are thus compared in a compatible manner to achieve an insight on their effectiveness for DA of RE. In fact, the problem of incompatible comparison is unfortunately very common in the RE literature (Wang, 2008; Plank and Moschitti, 2013) and we believe there is a need to tackle this increasing confusion in this line of research. Therefore, this is actually the first attempt to compare the two methods (tree kernel-based and feature-based) on the same settings. To ease the comparison for future work and circumvent the Zigglebottom pitfall (Pedersen, 2008), the entire setup and package is available.1 2 Relation Extraction Approaches In the following, we introduce the two relation extraction systems further examined in this study. 2.1 Tree kernel-based Method In the tree kernel-based method (Moschitti, 2006; Moschitti, 2008; Plan</context>
<context position="8844" citStr="Plank and Moschitti (2013)" startWordPosition="1417" endWordPosition="1420">hen defined to compute the similarity between two PET trees (where target entities are marked) by counting the common sub-trees, without enumerating the whole fragment space (Moschitti, 2006; Moschitti, 2008). STK is then applied in the support vector machines (SVMs) for RE. The major limitation of STK is its inability to match two trees that share the same substructure, but involve different though semantically related terminal nodes (words). This is caused by the hard matches between words, and consequently between sequences containing them. For instance, in the following example taken from Plank and Moschitti (2013), the two fragments “governor from Texas” and “head of Mary1https://bitbucket.org/nycphre/limo-re 636 land” would not match in STK although they have very similar syntactic structures and basically convey the same relationship. Plank and Moschitti (2013) propose to resolve this issue for STK using the semantic syntactic tree kernel (SSTK) (Bloehdorn and Moschitti, 2007) and apply it to the domain adaptation problem of RE. The two following techniques are utilized to activate the SSTK: (i) replace the part-ofspeech nodes in the PET trees by the new ones labeled by the word clusters of the corre</context>
<context position="12073" citStr="Plank and Moschitti, 2013" startWordPosition="1934" endWordPosition="1937">antic and syntactic functions. This again helps to mitigate the lexical sparsity or the vocabulary difference between the domains and has proven helpful for, amongst others, the feature-based method in DA of RE. 2.3 Tree Kernel-based vs Feature-based The feature-based method explicitly encapsulates the linguistic intuition and domain expertise for RE into the features, while the tree kernel-based method avoids the complicated feature engineering and implicitly encode the features into the computation of the tree kernels. Which method is better for DA of RE? In order to ensure the two methods (Plank and Moschitti, 2013; Nguyen and Grishman, 2014) are compared compatibly on the same resources, we make sure the two systems have access to the same amount of information. Thus, we follow Plank and Moschitti (2013) and use the PET trees (beside word clusters and word embeddings) as the only resource the two methods can exploit. For the feature-based method, we utilize all the features extractable from the PET trees that are standardly used in the state-of-the-art featurebased systems for DA of RE (Nguyen and Grishman, 2014). Specifically, the feature set employed in this paper (denoted by FET) includes: the lexic</context>
<context position="14034" citStr="Plank and Moschitti (2013)" startWordPosition="2255" endWordPosition="2258">o be useful for DA of RE; (ii) the tree kernel-based method avoids the inclusion of fine-tuned and domain-specific features originated from the excessive feature engineering (i.e., hand-designing feature sets based on the 637 linguistic intuition for specific domains) of the function in this case is then defined by: feature-based method. Knew(Ri, Rj) = (1 − α)SSTK(Ti, Tj) + αKvec(Vi, Vj) 3 Word Embeddings &amp; Tree Kernels In this section, we first give the intuition that guides us in designing the proposed methods. In particular, one limitation of the syntactic semantic tree kernel presented in Plank and Moschitti (2013) (§2.1) is that semantics is highly tied to syntax (the PET trees) in the kernel computation, limiting the generalization capacity of semantics to the extent of syntactic matches. If two relation mentions have different syntactic structures, the two relation mentions will not match, although they share the same semantic representation and express the same relation class. For instance, the two fragments “Tom is the CEO of the company” and “the company, headed by Tom” express the same relationship between “Tom” and “company” based on the semantics of their context words, but cannot be matched in</context>
<context position="18375" citStr="Plank and Moschitti (2013)" startWordPosition="2970" endWordPosition="2973"> in a bottom-up manner. The embeddings of the leaves are the embeddings of the words associated with them while the embeddings of the internal nodes are the means of the embeddings of their children nodes. We use the embeddings of the root of the PET tree to represent the relation mention in this case. Both PHRASE and TREE have d dimensions. It is also interesting to examine combinations of these three representations (cf., Table 1). 638 SIM: Finally, for completeness, we experiment with a more obvious way to introduce word embeddings into tree kernels, resembling more closely the approach of Plank and Moschitti (2013). In particularly, the SIM method simply replaces the similarity scores between word pairs obtained from LSA by the cosine similarities between the word embeddings to be used in the SSTK kernel. 4 Experiments 4.1 Dataset, Resources and Parameters We use the word clusters trained by Plank and Moschitti (2013) on the ukWaC corpus (Baroni et al., 2009) with 2 billion words, and the C&amp;W word embeddings from Turian el al. (2010)2 with 50 dimensions following Nguyen and Grishman (2014). In order to make the comparisons compatible, we introduce word embeddings into the tree kernel by extending the pa</context>
<context position="19959" citStr="Plank and Moschitti (2013)" startWordPosition="3237" endWordPosition="3240"> L2 regularizer on the hierarchical architecture for relation extraction as in Nguyen and Grishman (2014). Following prior work, we evaluate the systems on the ACE 2005 dataset which involves 6 domains: broadcast news (bn), newswire (nw), broadcast conversation (bc), telephone conversation (cts), weblogs (wl) and usenet (un). The union of bn and nw (news) is used as the source domain while bc, cts and wl play the role of the target domains. We take half of bc as the only target development set, and use the remaining data and domains for testing. The dataset partition is exactly the same as in Plank and Moschitti (2013). As described in their paper, the target domains quite differ from the source domain in the relation distributions and vocabulary. 4.2 Word Embeddings for Tree Kernel We investigate the effectiveness of different semantic representations (§3.1) in tree kernels by 2http://metaoptimize.com/projects/wordreprs/ 3http://mallet.cs.umass.edu/ taking the PET tree as the baseline4, and evaluate the performance of the representations when combined with the baseline on the bc development set. Method P R F1 PET (Plank and Moschitti, 2013) 52.2 41.7 46.4 PET+SIM 39.4 37.2 38.3 PET+HEAD 60.4 44.9 51.5 PET+</context>
<context position="22034" citStr="Plank and Moschitti, 2013" startWordPosition="3579" endWordPosition="3582">embeddings HEAD. We expect the combination of HEAD with either PHRASE or TREE to further improve performance. This is the case when adding one of them at a time. PHRASE and TREE seem to capture similar information, combining all (last row in Table 1) is not the overall best system. The best performance is achieved when the HEAD and PHRASE embeddings are utilized at 4By using their system we obtained the same results. 52 F-measure 50 48 46 0 0.1 0.3 0.5 0.7 0.9 1 α Figure 1: α vs F-measure on PET+HEAD+PHRASE 639 nw+bn (in-dom.) P: bc F1: P: cts F1: P: wl F1: # System: P: R: F1: R: R: R: 1 PET (Plank and Moschitti, 2013) 50.6 42.1 46.0 51.2 40.6 45.3 51.0 37.8 43.4 35.4 32.8 34.0 2 PET+WED 55.8 48.7 52.0 57.3 45.7 50.8 54.0 38.1 44.7 40.1 36.5 38.2 3 PET WC 55.4 44.6 49.4 54.3 41.4 47.0 55.9 37.1 44.6 40.0 32.7 36.0 4 PET WC+WED 56.3 48.2 51.9 57.0 44.3 49.8 56.1 38.1 45.4 40.7 36.1 38.2 5 PET LSA 52.3 44.1 47.9 51.4 41.7 46.0 49.7 36.5 42.1 38.1 36.5 37.3 6 PET LSA+WED 55.2 48.5 51.6 58.8 45.8 51.5 54.1 38.1 44.7 40.9 38.5 39.6 7 PET+PET WC 55.0 46.5 50.4 54.4 43.4 48.3 54.1 38.1 44.7 38.4 34.5 36.3 8 PET+PET WC+WED 56.3 50.3 53.1 57.5 46.6 51.5 55.6 39.8 46.4 41.5 37.9 39.6 9 PET+PET LSA 52.7 46.6 49.5 53.9</context>
</contexts>
<marker>Plank, Moschitti, 2013</marker>
<rawString>Barbara Plank and Alessandro Moschitti. 2013. Embedding semantic similarity in tree kernels for domain adaptation of relation extraction. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Longhua Qian</author>
<author>Guodong Zhou</author>
<author>Fang Kong</author>
<author>Qiaoming Zhu</author>
<author>Peide Qian</author>
</authors>
<title>Exploiting constituent dependencies for tree kernel-based semantic relation extraction.</title>
<date>2008</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="1528" citStr="Qian et al., 2008" startWordPosition="222" endWordPosition="225">on the same resources and settings, to gain insights into which kind of system is more robust to domain changes. Our results and error analysis shows that the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance of the traditional RE techniques degrades significantly in such a domain mismatch case (Plank and Moschitti, 2013). To </context>
</contexts>
<marker>Qian, Zhou, Kong, Zhu, Qian, 2008</marker>
<rawString>Longhua Qian, Guodong Zhou, Fang Kong, Qiaoming Zhu, and Peide Qian. 2008. Exploiting constituent dependencies for tree kernel-based semantic relation extraction. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2012</date>
<booktitle>In EMNLP-CoNLL.</booktitle>
<contexts>
<context position="17404" citStr="Socher et al., 2012" startWordPosition="2810" endWordPosition="2813">. According to the principle of compositionality (Werning et al., 2006; Baroni and Zamparelli, 2010; Paperno et al., 2014), the meaning of a complex expression is determined by the meanings of its components and the rules to combine them. We study the following two compositionality embeddings for relation mentions that can be generated from the embeddings of the context words: PHRASE: Vi = the mean of the embeddings of the words contained in the PET tree Ti of Ri. Although this composition is simple, it is in fact competitive to the more complicated methods based on recursive neural networks (Socher et al., 2012b; Blacoe and Lapata, 2012; Sterckx et al., 2014) on representing phrase semantics. TREE: This is motivated by the training of recursive neural networks (Socher et al., 2012a) for semantic compositionality and attempts to aggregate the context words embeddings syntactically. In particular, we compute an embedding for every node in the PET tree in a bottom-up manner. The embeddings of the leaves are the embeddings of the words associated with them while the embeddings of the internal nodes are the means of the embeddings of their children nodes. We use the embeddings of the root of the PET tree</context>
<context position="33491" citStr="Socher et al. (2012" startWordPosition="5521" endWordPosition="5524"> tree kernelbased model in Table 3 successfully recognizes the NONE relation in this case. A closer examination shows that the phrase with the closest embedding to Y1 in the source domain is Y2: “Iraqi soldiers abandoned their posts”,5 which is annotated with the NONE relation between “Iraqi soldiers” and “their posts”. As the syntactic structure of Y2 is also very similar to Y1, it is not surprising that Y1 is closest to Y2 in the new kernel function, consequently helping the tree kernel-based method work correctly in this case. 6 Related work Word embeddings are only applied to RE recently. Socher et al. (2012b) use word embeddings as input for matrix-vector recursive neural networks in relation classification while Zeng et al. (2014), and Nguyen and Grishman (2015) employ word embeddings in the framework of convolutional neural networks for relation classification and extraction, respectively. Sterckx et al. (2014) utilize word embeddings to reduce noise of training data in distant supervision. Kuksa et al. (2010) present a string kernel for bio-relation extraction with word embeddings, and Yu et al. (2014; 2015) study the factor-based compositional embedding models. However, none of this work exa</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2012</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2012a. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="17404" citStr="Socher et al., 2012" startWordPosition="2810" endWordPosition="2813">. According to the principle of compositionality (Werning et al., 2006; Baroni and Zamparelli, 2010; Paperno et al., 2014), the meaning of a complex expression is determined by the meanings of its components and the rules to combine them. We study the following two compositionality embeddings for relation mentions that can be generated from the embeddings of the context words: PHRASE: Vi = the mean of the embeddings of the words contained in the PET tree Ti of Ri. Although this composition is simple, it is in fact competitive to the more complicated methods based on recursive neural networks (Socher et al., 2012b; Blacoe and Lapata, 2012; Sterckx et al., 2014) on representing phrase semantics. TREE: This is motivated by the training of recursive neural networks (Socher et al., 2012a) for semantic compositionality and attempts to aggregate the context words embeddings syntactically. In particular, we compute an embedding for every node in the PET tree in a bottom-up manner. The embeddings of the leaves are the embeddings of the words associated with them while the embeddings of the internal nodes are the means of the embeddings of their children nodes. We use the embeddings of the root of the PET tree</context>
<context position="33491" citStr="Socher et al. (2012" startWordPosition="5521" endWordPosition="5524"> tree kernelbased model in Table 3 successfully recognizes the NONE relation in this case. A closer examination shows that the phrase with the closest embedding to Y1 in the source domain is Y2: “Iraqi soldiers abandoned their posts”,5 which is annotated with the NONE relation between “Iraqi soldiers” and “their posts”. As the syntactic structure of Y2 is also very similar to Y1, it is not surprising that Y1 is closest to Y2 in the new kernel function, consequently helping the tree kernel-based method work correctly in this case. 6 Related work Word embeddings are only applied to RE recently. Socher et al. (2012b) use word embeddings as input for matrix-vector recursive neural networks in relation classification while Zeng et al. (2014), and Nguyen and Grishman (2015) employ word embeddings in the framework of convolutional neural networks for relation classification and extraction, respectively. Sterckx et al. (2014) utilize word embeddings to reduce noise of training data in distant supervision. Kuksa et al. (2010) present a string kernel for bio-relation extraction with word embeddings, and Yu et al. (2014; 2015) study the factor-based compositional embedding models. However, none of this work exa</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012b. Semantic compositionality through recursive matrix-vector spaces. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucas Sterckx</author>
<author>Thomas Demeester</author>
<author>Johannes Deleu</author>
<author>Chris Develder</author>
</authors>
<title>Using active learning and semantic clustering for noise reduction in distant supervision.</title>
<date>2014</date>
<booktitle>In AKBC.</booktitle>
<contexts>
<context position="17453" citStr="Sterckx et al., 2014" startWordPosition="2818" endWordPosition="2821">y (Werning et al., 2006; Baroni and Zamparelli, 2010; Paperno et al., 2014), the meaning of a complex expression is determined by the meanings of its components and the rules to combine them. We study the following two compositionality embeddings for relation mentions that can be generated from the embeddings of the context words: PHRASE: Vi = the mean of the embeddings of the words contained in the PET tree Ti of Ri. Although this composition is simple, it is in fact competitive to the more complicated methods based on recursive neural networks (Socher et al., 2012b; Blacoe and Lapata, 2012; Sterckx et al., 2014) on representing phrase semantics. TREE: This is motivated by the training of recursive neural networks (Socher et al., 2012a) for semantic compositionality and attempts to aggregate the context words embeddings syntactically. In particular, we compute an embedding for every node in the PET tree in a bottom-up manner. The embeddings of the leaves are the embeddings of the words associated with them while the embeddings of the internal nodes are the means of the embeddings of their children nodes. We use the embeddings of the root of the PET tree to represent the relation mention in this case. </context>
<context position="33803" citStr="Sterckx et al. (2014)" startWordPosition="5566" endWordPosition="5569">eir posts”. As the syntactic structure of Y2 is also very similar to Y1, it is not surprising that Y1 is closest to Y2 in the new kernel function, consequently helping the tree kernel-based method work correctly in this case. 6 Related work Word embeddings are only applied to RE recently. Socher et al. (2012b) use word embeddings as input for matrix-vector recursive neural networks in relation classification while Zeng et al. (2014), and Nguyen and Grishman (2015) employ word embeddings in the framework of convolutional neural networks for relation classification and extraction, respectively. Sterckx et al. (2014) utilize word embeddings to reduce noise of training data in distant supervision. Kuksa et al. (2010) present a string kernel for bio-relation extraction with word embeddings, and Yu et al. (2014; 2015) study the factor-based compositional embedding models. However, none of this work examines word embeddings for tree kernels as well as domain adaptation as we do. Regarding DA, in the unsupervised DA setting, Huang and Yates (2010) attempt to learn multidimensional feature representations while Blitzer et al. (2006) introduce structural correspondence learning. Daum´e (2007) proposes an easy ad</context>
</contexts>
<marker>Sterckx, Demeester, Deleu, Develder, 2014</marker>
<rawString>Lucas Sterckx, Thomas Demeester, Johannes Deleu, and Chris Develder. 2014. Using active learning and semantic clustering for noise reduction in distant supervision. In AKBC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ang Sun</author>
<author>Ralph Grishman</author>
<author>Satoshi Sekine</author>
</authors>
<title>Semi-supervised relation extraction with large-scale word clustering.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1702" citStr="Sun et al., 2011" startWordPosition="253" endWordPosition="256">ed method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance of the traditional RE techniques degrades significantly in such a domain mismatch case (Plank and Moschitti, 2013). To alleviate this performance loss, we need to resort to domain adaptation (DA) techniques to adapt a system trained on some source domain to perform well on new target domains.</context>
<context position="10115" citStr="Sun et al., 2011" startWordPosition="1621" endWordPosition="1624">ilarity scores between words (i.e, either 1 or 0) by the similarities induced from the latent semantic analysis (LSA) of large corpus. The former generalizes the part-of-speech similarity to the semantic similarity on word clusters; the latter, on the other hand, allows soft matches between words that have the same latent semantic but differ in symbolic representation. Both techniques emphasize the invariants of word semantics in different domains, thus being helpful to alleviate the vocabulary difference across domains. 2.2 Feature-based Method In the feature-based method (Zhou et al., 2005; Sun et al., 2011; Nguyen and Grishman, 2014), relation mentions are first transformed into rich feature vectors that capture various characteristics of the relation mentions (i.e, lexicon, syntax, semantics etc). The resulting vectors are then fed into the statistical classifiers such as Maximum Entropy (MaxEnt) to perform classification for RE. The main reason for the performance loss of the feature-based systems on new domains is the behavioral changes of the features when domains shift. Some features might be very informative in the source domain but become less relevant in the target domains. For instance</context>
<context position="12837" citStr="Sun et al., 2011" startWordPosition="2064" endWordPosition="2067">ion. Thus, we follow Plank and Moschitti (2013) and use the PET trees (beside word clusters and word embeddings) as the only resource the two methods can exploit. For the feature-based method, we utilize all the features extractable from the PET trees that are standardly used in the state-of-the-art featurebased systems for DA of RE (Nguyen and Grishman, 2014). Specifically, the feature set employed in this paper (denoted by FET) includes: the lexical features, i.e., the context words, the head words, the bigrams, the number of words, the lexical path, the order of mention (Zhou et al., 2005; Sun et al., 2011); and the syntactic features, i.e., the path connecting the two mentions in PET and the unigrams, bigrams, trigrams along this path (Zhou et al., 2005; Jiang and Zhai, 2007a). Hypothesis: Assuming identical settings and resources, we hypothesize that the tree kernelbased method is better than the feature-based method for DA of RE. This is motivated because of at least two reasons: (i) the tree kernel-based method implicitly encodes a more comprehensive feature set (involving all the sub-trees in the PETs), thus potentially captures more domainindependent features to be useful for DA of RE; (ii</context>
</contexts>
<marker>Sun, Grishman, Sekine, 2011</marker>
<rawString>Ang Sun, Ralph Grishman, and Satoshi Sekine. 2011. Semi-supervised relation extraction with large-scale word clustering. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev-Arie Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="4519" citStr="Turian et al., 2010" startWordPosition="720" endWordPosition="723">(2014) studies the appli635 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 635–644, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics cation of word clusters and word embeddings for DA of RE on the feature-based method. Although word clusters (Brown et al., 1992) have been employed by both studies to improve the performance of relation extractors across domains, the application of word embeddings (Bengio et al., 2003; Mnih and Hinton, 2008; Turian et al., 2010) for DA of RE is only examined in the feature-based method and never explored in the tree kernelbased method so far, giving rise to the first question we want to address in this paper: (i) Can word embeddings help the tree kernelbased methods on DA for RE and more importantly, in which way can we do it effectively? This question is important as word embeddings are real valued vectors, while the tree kernel-based methods rely on the symbolic matches or mismatches of concrete labels in the parse trees to compute the kernels. It is unclear at the first glance how to encode word embeddings into th</context>
<context position="11345" citStr="Turian et al., 2010" startWordPosition="1819" endWordPosition="1822">ds, that are very indicative in the source domain might not appear in the target domains (lexical sparsity). Consequently, the models putting high weights on such words (features) in the source domain will fail to perform well on the target domains. Nguyen and Grishman (2014) address this problem for the feature-based method in DA of RE by introducing word embeddings as additional features. The rationale is based on the fact that word embeddings are low dimensional and real valued vectors, capturing latent syntactic and semantic properties of words (Bengio et al., 2003; Mnih and Hinton, 2008; Turian et al., 2010). The embeddings of symbolically different words are often close to each other if they have similar semantic and syntactic functions. This again helps to mitigate the lexical sparsity or the vocabulary difference between the domains and has proven helpful for, amongst others, the feature-based method in DA of RE. 2.3 Tree Kernel-based vs Feature-based The feature-based method explicitly encapsulates the linguistic intuition and domain expertise for RE into the features, while the tree kernel-based method avoids the complicated feature engineering and implicitly encode the features into the com</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
</authors>
<title>A re-examination of dependency path kernels for relation extraction.</title>
<date>2008</date>
<booktitle>In IJCNLP.</booktitle>
<contexts>
<context position="7301" citStr="Wang, 2008" startWordPosition="1180" endWordPosition="1181">ing between relation classes and their inverses) but Nguyen and Grishman (2014) disregard this relation direction. Finally, we note that although both studies evaluate their systems on the ACE 2005 dataset, they actually have different dataset partitions. In order to overcome this limitation, we conduct an evaluation in which the two methods are directed to use the same resources and settings, and are thus compared in a compatible manner to achieve an insight on their effectiveness for DA of RE. In fact, the problem of incompatible comparison is unfortunately very common in the RE literature (Wang, 2008; Plank and Moschitti, 2013) and we believe there is a need to tackle this increasing confusion in this line of research. Therefore, this is actually the first attempt to compare the two methods (tree kernel-based and feature-based) on the same settings. To ease the comparison for future work and circumvent the Zigglebottom pitfall (Pedersen, 2008), the entire setup and package is available.1 2 Relation Extraction Approaches In the following, we introduce the two relation extraction systems further examined in this study. 2.1 Tree kernel-based Method In the tree kernel-based method (Moschitti,</context>
</contexts>
<marker>Wang, 2008</marker>
<rawString>Mengqiu Wang. 2008. A re-examination of dependency path kernels for relation extraction. In IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Werning</author>
<author>Edouard Machery</author>
<author>Gerhard Schurz</author>
</authors>
<title>Compositionality of meaning and content: Foundational issues (linguistics &amp; philosophy).</title>
<date>2006</date>
<booktitle>In Linguistics &amp; philosophy.</booktitle>
<contexts>
<context position="16855" citStr="Werning et al., 2006" startWordPosition="2716" endWordPosition="2719"> In this work, we consider the following methods to obtain the semantic representation Vi from the word embeddings of the context words of Ri (assuming d is the dimensionality of the word embeddings): HEAD: Vi = the concatenation of the word embeddings of the two entity mention heads of Ri. This representation is inherited from Nguyen and Grishman (2014) that only examine embeddings at the word level separately for the feature-based method without considering the compositionality embeddings of relation mentions. The dimensionality of HEAD is 2d. According to the principle of compositionality (Werning et al., 2006; Baroni and Zamparelli, 2010; Paperno et al., 2014), the meaning of a complex expression is determined by the meanings of its components and the rules to combine them. We study the following two compositionality embeddings for relation mentions that can be generated from the embeddings of the context words: PHRASE: Vi = the mean of the embeddings of the words contained in the PET tree Ti of Ri. Although this composition is simple, it is in fact competitive to the more complicated methods based on recursive neural networks (Socher et al., 2012b; Blacoe and Lapata, 2012; Sterckx et al., 2014) o</context>
</contexts>
<marker>Werning, Machery, Schurz, 2006</marker>
<rawString>Markus Werning, Edouard Machery, and Gerhard Schurz. 2006. Compositionality of meaning and content: Foundational issues (linguistics &amp; philosophy). In Linguistics &amp; philosophy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Xiao</author>
<author>Yuhong Guo</author>
</authors>
<title>Domain adaptation for sequence labeling tasks with a probabilistic language adaptation model.</title>
<date>2013</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="34452" citStr="Xiao and Guo (2013)" startWordPosition="5666" endWordPosition="5669">duce noise of training data in distant supervision. Kuksa et al. (2010) present a string kernel for bio-relation extraction with word embeddings, and Yu et al. (2014; 2015) study the factor-based compositional embedding models. However, none of this work examines word embeddings for tree kernels as well as domain adaptation as we do. Regarding DA, in the unsupervised DA setting, Huang and Yates (2010) attempt to learn multidimensional feature representations while Blitzer et al. (2006) introduce structural correspondence learning. Daum´e (2007) proposes an easy adaptation framework (EA) while Xiao and Guo (2013) present a log-bilinear language adaptation technique in the supervised DA setting. Unfortunately, all of this work assumes some prior (in the form of either labeled or unlabeled data) on the target domains for the sequential labeling tasks, in contrast to our single-system unsupervised DA setting for relation extraction. An alternative method that is also popular to DA is instance weighting (Jiang and Zhai, 2007b). However, as shown by Plank and Moschitti (2013), instance weighting is not 5The full sentence is: “After today’s air strikes, Iraqi soldiers abandoned their posts and surrendered t</context>
</contexts>
<marker>Xiao, Guo, 2013</marker>
<rawString>Min Xiao and Yuhong Guo. 2013. Domain adaptation for sequence labeling tasks with a probabilistic language adaptation model. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mo Yu</author>
<author>Matthew Gormley</author>
<author>Mark Dredze</author>
</authors>
<title>Factor-based compositional embedding models.</title>
<date>2014</date>
<booktitle>In The NIPS workshop on Learning Semantics.</booktitle>
<contexts>
<context position="33998" citStr="Yu et al. (2014" startWordPosition="5598" endWordPosition="5601"> correctly in this case. 6 Related work Word embeddings are only applied to RE recently. Socher et al. (2012b) use word embeddings as input for matrix-vector recursive neural networks in relation classification while Zeng et al. (2014), and Nguyen and Grishman (2015) employ word embeddings in the framework of convolutional neural networks for relation classification and extraction, respectively. Sterckx et al. (2014) utilize word embeddings to reduce noise of training data in distant supervision. Kuksa et al. (2010) present a string kernel for bio-relation extraction with word embeddings, and Yu et al. (2014; 2015) study the factor-based compositional embedding models. However, none of this work examines word embeddings for tree kernels as well as domain adaptation as we do. Regarding DA, in the unsupervised DA setting, Huang and Yates (2010) attempt to learn multidimensional feature representations while Blitzer et al. (2006) introduce structural correspondence learning. Daum´e (2007) proposes an easy adaptation framework (EA) while Xiao and Guo (2013) present a log-bilinear language adaptation technique in the supervised DA setting. Unfortunately, all of this work assumes some prior (in the for</context>
</contexts>
<marker>Yu, Gormley, Dredze, 2014</marker>
<rawString>Mo Yu, Matthew Gormley, and Mark Dredze. 2014. Factor-based compositional embedding models. In The NIPS workshop on Learning Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mo Yu</author>
<author>Matthew Gormley</author>
<author>Mark Dredze</author>
</authors>
<title>Combining word embeddings and feature embeddings for fine-grained relation extraction.</title>
<date>2015</date>
<booktitle>In NAACL.</booktitle>
<marker>Yu, Gormley, Dredze, 2015</marker>
<rawString>Mo Yu, Matthew Gormley, and Mark Dredze. 2015. Combining word embeddings and feature embeddings for fine-grained relation extraction. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
<author>Chinatsu Aone</author>
<author>Anthony Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2003</date>
<journal>In Journal of Machine Learning Research</journal>
<volume>3</volume>
<pages>1083--1106</pages>
<contexts>
<context position="1423" citStr="Zelenko et al., 2003" startWordPosition="204" endWordPosition="207">nt). In addition, we compare the tree kernel-based and the feature-based method for RE in a compatible way, on the same resources and settings, to gain insights into which kind of system is more robust to domain changes. Our results and error analysis shows that the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance of the traditi</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel methods for relation extraction. In Journal of Machine Learning Research 3, pages 1083–1106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daojian Zeng</author>
<author>Kang Liu</author>
<author>Siwei Lai</author>
<author>Guangyou Zhou</author>
<author>Jun Zhao</author>
</authors>
<title>Relation classification via convolutional deep neural network.</title>
<date>2014</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="33618" citStr="Zeng et al. (2014)" startWordPosition="5540" endWordPosition="5543">hrase with the closest embedding to Y1 in the source domain is Y2: “Iraqi soldiers abandoned their posts”,5 which is annotated with the NONE relation between “Iraqi soldiers” and “their posts”. As the syntactic structure of Y2 is also very similar to Y1, it is not surprising that Y1 is closest to Y2 in the new kernel function, consequently helping the tree kernel-based method work correctly in this case. 6 Related work Word embeddings are only applied to RE recently. Socher et al. (2012b) use word embeddings as input for matrix-vector recursive neural networks in relation classification while Zeng et al. (2014), and Nguyen and Grishman (2015) employ word embeddings in the framework of convolutional neural networks for relation classification and extraction, respectively. Sterckx et al. (2014) utilize word embeddings to reduce noise of training data in distant supervision. Kuksa et al. (2010) present a string kernel for bio-relation extraction with word embeddings, and Yu et al. (2014; 2015) study the factor-based compositional embedding models. However, none of this work examines word embeddings for tree kernels as well as domain adaptation as we do. Regarding DA, in the unsupervised DA setting, Hua</context>
</contexts>
<marker>Zeng, Liu, Lai, Zhou, Zhao, 2014</marker>
<rawString>Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. 2014. Relation classification via convolutional deep neural network. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
<author>Guodong Zhou</author>
</authors>
<title>A composite kernel to extract relations between entities with both flat and structured features.</title>
<date>2006</date>
<booktitle>In COLING-ACL.</booktitle>
<contexts>
<context position="1494" citStr="Zhang et al., 2006" startWordPosition="216" endWordPosition="219">method for RE in a compatible way, on the same resources and settings, to gain insights into which kind of system is more robust to domain changes. Our results and error analysis shows that the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance of the traditional RE techniques degrades significantly in such a domain mismatch cas</context>
<context position="8180" citStr="Zhang et al., 2006" startWordPosition="1312" endWordPosition="1315">se the comparison for future work and circumvent the Zigglebottom pitfall (Pedersen, 2008), the entire setup and package is available.1 2 Relation Extraction Approaches In the following, we introduce the two relation extraction systems further examined in this study. 2.1 Tree kernel-based Method In the tree kernel-based method (Moschitti, 2006; Moschitti, 2008; Plank and Moschitti, 2013), a relation mention (the two entity mentions and the sentence containing them) is represented by the path-enclosed tree (PET), the smallest constituency-based subtree including the two target entity mentions (Zhang et al., 2006). The syntactic tree kernel (STK) is then defined to compute the similarity between two PET trees (where target entities are marked) by counting the common sub-trees, without enumerating the whole fragment space (Moschitti, 2006; Moschitti, 2008). STK is then applied in the support vector machines (SVMs) for RE. The major limitation of STK is its inability to match two trees that share the same substructure, but involve different though semantically related terminal nodes (words). This is caused by the hard matches between words, and consequently between sequences containing them. For instance</context>
</contexts>
<marker>Zhang, Zhang, Su, Zhou, 2006</marker>
<rawString>Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou. 2006. A composite kernel to extract relations between entities with both flat and structured features. In COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shubin Zhao</author>
<author>Ralph Grishman</author>
</authors>
<title>Extracting relations with integrated information using kernel methods.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1474" citStr="Zhao and Grishman, 2005" startWordPosition="212" endWordPosition="215">ed and the feature-based method for RE in a compatible way, on the same resources and settings, to gain insights into which kind of system is more robust to domain changes. Our results and error analysis shows that the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance of the traditional RE techniques degrades significantly in such a</context>
</contexts>
<marker>Zhao, Grishman, 2005</marker>
<rawString>Shubin Zhao and Ralph Grishman. 2005. Extracting relations with integrated information using kernel methods. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>GuoDong Zhou</author>
<author>Jian Su</author>
<author>Jie Zhang</author>
<author>Min Zhang</author>
</authors>
<title>Exploring various knowledge in relation extraction.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1639" citStr="Zhou et al., 2005" startWordPosition="241" endWordPosition="244">. Our results and error analysis shows that the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance of the traditional RE techniques degrades significantly in such a domain mismatch case (Plank and Moschitti, 2013). To alleviate this performance loss, we need to resort to domain adaptation (DA) techniques to adapt a system train</context>
<context position="10097" citStr="Zhou et al., 2005" startWordPosition="1617" endWordPosition="1620">lace the binary similarity scores between words (i.e, either 1 or 0) by the similarities induced from the latent semantic analysis (LSA) of large corpus. The former generalizes the part-of-speech similarity to the semantic similarity on word clusters; the latter, on the other hand, allows soft matches between words that have the same latent semantic but differ in symbolic representation. Both techniques emphasize the invariants of word semantics in different domains, thus being helpful to alleviate the vocabulary difference across domains. 2.2 Feature-based Method In the feature-based method (Zhou et al., 2005; Sun et al., 2011; Nguyen and Grishman, 2014), relation mentions are first transformed into rich feature vectors that capture various characteristics of the relation mentions (i.e, lexicon, syntax, semantics etc). The resulting vectors are then fed into the statistical classifiers such as Maximum Entropy (MaxEnt) to perform classification for RE. The main reason for the performance loss of the feature-based systems on new domains is the behavioral changes of the features when domains shift. Some features might be very informative in the source domain but become less relevant in the target dom</context>
<context position="12818" citStr="Zhou et al., 2005" startWordPosition="2060" endWordPosition="2063"> amount of information. Thus, we follow Plank and Moschitti (2013) and use the PET trees (beside word clusters and word embeddings) as the only resource the two methods can exploit. For the feature-based method, we utilize all the features extractable from the PET trees that are standardly used in the state-of-the-art featurebased systems for DA of RE (Nguyen and Grishman, 2014). Specifically, the feature set employed in this paper (denoted by FET) includes: the lexical features, i.e., the context words, the head words, the bigrams, the number of words, the lexical path, the order of mention (Zhou et al., 2005; Sun et al., 2011); and the syntactic features, i.e., the path connecting the two mentions in PET and the unigrams, bigrams, trigrams along this path (Zhou et al., 2005; Jiang and Zhai, 2007a). Hypothesis: Assuming identical settings and resources, we hypothesize that the tree kernelbased method is better than the feature-based method for DA of RE. This is motivated because of at least two reasons: (i) the tree kernel-based method implicitly encodes a more comprehensive feature set (involving all the sub-trees in the PETs), thus potentially captures more domainindependent features to be usefu</context>
</contexts>
<marker>Zhou, Su, Zhang, Zhang, 2005</marker>
<rawString>GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang. 2005. Exploring various knowledge in relation extraction. In ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>