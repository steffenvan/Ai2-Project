<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.020480">
<title confidence="0.9625075">
Dialog System for Mixed Initiative One-Turn Address Entry and Error
Recovery
</title>
<author confidence="0.4564945">
Rajesh Balchandran, Leonid Rachevsky, Larry Sansone, Roberto Sicconi
IBM T J Watson Research Center, Yorktown Heights, NY 10598, USA
</author>
<email confidence="0.993167">
rajeshb,lrachevs,lsansone,rsicconi@us.ibm.com
</email>
<sectionHeader confidence="0.993716" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999968894736842">
In this demonstration we present a mixed-
initiative dialog system for address recog-
nition that lets users to specify a complete
addresses in a single sentence with ad-
dress components spoken in their natural
sequence. Users can also specify fewer ad-
dress components in several ways, based
on their convenience. The system extracts
specified address components, prompts for
missing information, disambiguates items
independently or collectively all the while
guiding the user so as to obtain the de-
sired valid address. The language mod-
eling and dialog management techniques
developed for this purpose are also briefly
described. Finally, several use cases with
screen shots are presented. The combined
system yields very high task completion
accuracy on user tests.
</bodyText>
<sectionHeader confidence="0.997616" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999944666666667">
In recent years, speech recognition has been em-
ployed for address input by voice for GPS nav-
igation and similar applications. Users are typi-
cally directed to speak address components one at
a time - first a state name, then city, street and fi-
nally the house number - typically taking four or
more turns. In this demonstration we present a
mixed-initiative dialog system that makes address
input by voice more natural, so users can speak
the complete address (in normal order) (for e.g.
“Fifteen State Street Boston Massachusetts”), in a
single turn. They could also specify fewer address
components as per their convenience, and the sys-
tem would be expected to guide them to obtain a
complete and valid address.
</bodyText>
<sectionHeader confidence="0.972132" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.9995646">
Figure 1 shows the high-level architecture and
key components of the system. A programmable
framework consisting of a system bus that con-
nects various components (called plugins) forms
the core of the speech-dialog system. Key compo-
nents include plugins to connect to the ASR (Au-
tomatic Speech Recognition) and TTS (Text-To-
Speech ) engines, the GUI (Graphical User Inter-
face), the Natural Language Processor and the Di-
alog Manager.
</bodyText>
<subsectionHeader confidence="0.988875">
2.1 Speech Recognition and component
Extraction
</subsectionHeader>
<bodyText confidence="0.999915869565218">
Speech recognition is carried out using a statisti-
cal Language Model (LM) with Embedded Gram-
mars (Gillett and Ward, 1998) to represent Named
Entities such as city names, numbers etc. This pro-
vides flexibility for the user, while allowing for dy-
namic content to be updated when required, sim-
ply by swapping associated embedded grammars.
For e.g., the grammar of street names could be up-
dated based on the selected city. The IBM Embed-
ded Via Voice (EVV) (Sicconi et al., 2009) (Beran
et al., 2004) ASR engine provides this functional-
ity and is used in this system.
In this system, a two-pass speech recognition
technique (Balchandran et al., 2009) is employed,
based on multiple LMs where, the first pass is used
to accurately recognize some components, and the
values of these components are used to dynam-
ically update another LM which is used for the
second pass to recognize remaining components.
Specifically, the first LM is used to recognize the
city and state while the second is used to recognize
the street name and number. The street names and
optionally the house number embedded grammars
</bodyText>
<subsubsectionHeader confidence="0.621808">
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 152–155,
</subsubsectionHeader>
<affiliation confidence="0.828097">
Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics
</affiliation>
<page confidence="0.995551">
152
</page>
<figureCaption confidence="0.999944">
Figure 1: System Architecture
</figureCaption>
<bodyText confidence="0.99961775">
in the second LM are updated based on the city and
state recognized using the first LM. This is carried
out transparent to the user - so the user perceives
full address recognition in one step.
</bodyText>
<subsectionHeader confidence="0.996492">
2.2 Dialog management
</subsectionHeader>
<bodyText confidence="0.999995529411765">
A key part of this system is the dialog manage-
ment component that handles incompletely spec-
ified input, various types of ambiguities and er-
ror conditions, all the while having an intelligent
dialog with the user so as to correct these er-
rors and obtain a valid address at the end. A
goal oriented approach for managing the dialog
that does not require manual identification of all
possible scenarios was employed and is described
in (Balchandran et al., 2009). The algorithm iter-
atively tries to achieve the goal (getting valid val-
ues for all address components), validating avail-
able input components, and prompting for miss-
ing input components, as defined by a priority or-
der among components. This algorithm was im-
plemented on a state based, programmable dialog
manager as shown in Figure 1.
</bodyText>
<sectionHeader confidence="0.998149" genericHeader="method">
3 Scenarios
</sectionHeader>
<bodyText confidence="0.999882666666667">
The following scenarios illustrate different situa-
tions that need to be handled by the dialog system
when processing addresses.
</bodyText>
<subsectionHeader confidence="0.999486">
3.1 Successful one-turn address recognition
</subsectionHeader>
<bodyText confidence="0.854804">
Figure 2 shows the scenario where the user speaks
a complete address in one sentence and the system
recognizes it correctly.
</bodyText>
<subsectionHeader confidence="0.999058">
3.2 One-turn address with error correction
</subsectionHeader>
<bodyText confidence="0.9999445">
The user speaks a complete address, but the sys-
tem mis-recognizes the street name and number
(Figure 3 (b)). The user requests to “go back” and
the system re-prompts the user for the street name
and number. User repeats the number in a different
way (Figure 3 (c)) and the system gets it correctly.
</bodyText>
<subsectionHeader confidence="0.9973575">
3.3 Street and number around current
location
</subsectionHeader>
<bodyText confidence="0.999836125">
In addition to complete addresses, the language
models are built to include streets and numbers
around the “current location” of the car. This data
could be periodically updated based on changing
car positions. In this example, (Figure 4) the user
just specifies, “15 Lake View Drive” and the sys-
tem defaults to the current city – Shelter Island,
NY.
</bodyText>
<subsectionHeader confidence="0.996095">
3.4 Ambiguous city
</subsectionHeader>
<bodyText confidence="0.9999495">
In this example, the user specifies an ambiguous
city name (Figure 5 (a)). The system prompts the
user to disambiguate by selecting a state. Once
the user has done this, the system re-processes the
street name and number to obtain the full address
without needing the user to specify it again. The
same concept is applied to other address compo-
nents.
</bodyText>
<sectionHeader confidence="0.998464" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997936066666667">
Rajesh Balchandran, Leonid Rachevsky, and Larry
Sansone. 2009. Language modeling and dia-
log management for address recognition. In Inter-
speech.
Tom´as Beran, Vladim´ır Bergl, Radek Hampl, Pavel Kr-
bec, Jan Sediv´y, Borivoj Tydlit´at, and Josef Vopicka.
2004. Embedded viavoice. In TSD, pages 269–274.
John Gillett and Wayne Ward. 1998. A language
model combining trigrams and stochastic context-
free grammars. In in International Conference
on Spoken Language Processing, volume 6, pages
2319–2322.
Roberto Sicconi, Kenneth White, and Harvey Ruback.
2009. Honda next generation speech user interface.
In SAE World Congress, pages 2009–01–0518.
</reference>
<page confidence="0.999544">
153
154
</page>
<figure confidence="0.865137">
(a) User specifies address (b) System gets correct address
Figure 2: Successful one-turn address recognition
(c) User corrects erroneous components (d) System gets correct address
(a) User specifies address (b) System makes mistake
</figure>
<figureCaption confidence="0.958676666666667">
Figure 3: One-turn address recognition with error recovery
Figure 4: Street and number around current loca-
tion (Shelter Island)
</figureCaption>
<figure confidence="0.999413333333333">
(b) System locates street and number around current location
(a) User specifies street and number
(c) System gets correct address
(b) User selects state and system combines previously speci-
fied information to get complete address
(a) User specifies address with city which is ambiguous
</figure>
<figureCaption confidence="0.916617">
Figure 5: Ambiguous city example
</figureCaption>
<figure confidence="0.609451">
(c) System gets correct address
</figure>
<page confidence="0.982036">
155
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.951337">
<title confidence="0.9853915">Dialog System for Mixed Initiative One-Turn Address Entry and Error Recovery</title>
<author confidence="0.999238">Rajesh Balchandran</author>
<author confidence="0.999238">Leonid Rachevsky</author>
<author confidence="0.999238">Larry Sansone</author>
<author confidence="0.999238">Roberto</author>
<affiliation confidence="0.987187">IBM T J Watson Research Center, Yorktown Heights, NY 10598,</affiliation>
<email confidence="0.999391">rajeshb,lrachevs,lsansone,rsicconi@us.ibm.com</email>
<abstract confidence="0.99962395">In this demonstration we present a mixedinitiative dialog system for address recognition that lets users to specify a complete addresses in a single sentence with address components spoken in their natural sequence. Users can also specify fewer address components in several ways, based on their convenience. The system extracts specified address components, prompts for missing information, disambiguates items independently or collectively all the while guiding the user so as to obtain the desired valid address. The language modeling and dialog management techniques developed for this purpose are also briefly described. Finally, several use cases with screen shots are presented. The combined system yields very high task completion accuracy on user tests.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rajesh Balchandran</author>
<author>Leonid Rachevsky</author>
<author>Larry Sansone</author>
</authors>
<title>Language modeling and dialog management for address recognition.</title>
<date>2009</date>
<journal>In Interspeech.</journal>
<contexts>
<context position="2902" citStr="Balchandran et al., 2009" startWordPosition="456" endWordPosition="459">s carried out using a statistical Language Model (LM) with Embedded Grammars (Gillett and Ward, 1998) to represent Named Entities such as city names, numbers etc. This provides flexibility for the user, while allowing for dynamic content to be updated when required, simply by swapping associated embedded grammars. For e.g., the grammar of street names could be updated based on the selected city. The IBM Embedded Via Voice (EVV) (Sicconi et al., 2009) (Beran et al., 2004) ASR engine provides this functionality and is used in this system. In this system, a two-pass speech recognition technique (Balchandran et al., 2009) is employed, based on multiple LMs where, the first pass is used to accurately recognize some components, and the values of these components are used to dynamically update another LM which is used for the second pass to recognize remaining components. Specifically, the first LM is used to recognize the city and state while the second is used to recognize the street name and number. The street names and optionally the house number embedded grammars Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 152–155, Queen Mary University </context>
<context position="4287" citStr="Balchandran et al., 2009" startWordPosition="686" endWordPosition="689"> state recognized using the first LM. This is carried out transparent to the user - so the user perceives full address recognition in one step. 2.2 Dialog management A key part of this system is the dialog management component that handles incompletely specified input, various types of ambiguities and error conditions, all the while having an intelligent dialog with the user so as to correct these errors and obtain a valid address at the end. A goal oriented approach for managing the dialog that does not require manual identification of all possible scenarios was employed and is described in (Balchandran et al., 2009). The algorithm iteratively tries to achieve the goal (getting valid values for all address components), validating available input components, and prompting for missing input components, as defined by a priority order among components. This algorithm was implemented on a state based, programmable dialog manager as shown in Figure 1. 3 Scenarios The following scenarios illustrate different situations that need to be handled by the dialog system when processing addresses. 3.1 Successful one-turn address recognition Figure 2 shows the scenario where the user speaks a complete address in one sent</context>
</contexts>
<marker>Balchandran, Rachevsky, Sansone, 2009</marker>
<rawString>Rajesh Balchandran, Leonid Rachevsky, and Larry Sansone. 2009. Language modeling and dialog management for address recognition. In Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom´as Beran</author>
<author>Vladim´ır Bergl</author>
<author>Radek Hampl</author>
<author>Pavel Krbec</author>
<author>Jan Sediv´y</author>
<author>Borivoj Tydlit´at</author>
<author>Josef Vopicka</author>
</authors>
<title>Embedded viavoice.</title>
<date>2004</date>
<booktitle>In TSD,</booktitle>
<pages>269--274</pages>
<marker>Beran, Bergl, Hampl, Krbec, Sediv´y, Tydlit´at, Vopicka, 2004</marker>
<rawString>Tom´as Beran, Vladim´ır Bergl, Radek Hampl, Pavel Krbec, Jan Sediv´y, Borivoj Tydlit´at, and Josef Vopicka. 2004. Embedded viavoice. In TSD, pages 269–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Gillett</author>
<author>Wayne Ward</author>
</authors>
<title>A language model combining trigrams and stochastic contextfree grammars.</title>
<date>1998</date>
<booktitle>In in International Conference on Spoken Language Processing,</booktitle>
<volume>6</volume>
<pages>2319--2322</pages>
<contexts>
<context position="2378" citStr="Gillett and Ward, 1998" startWordPosition="366" endWordPosition="369"> 2 System Description Figure 1 shows the high-level architecture and key components of the system. A programmable framework consisting of a system bus that connects various components (called plugins) forms the core of the speech-dialog system. Key components include plugins to connect to the ASR (Automatic Speech Recognition) and TTS (Text-ToSpeech ) engines, the GUI (Graphical User Interface), the Natural Language Processor and the Dialog Manager. 2.1 Speech Recognition and component Extraction Speech recognition is carried out using a statistical Language Model (LM) with Embedded Grammars (Gillett and Ward, 1998) to represent Named Entities such as city names, numbers etc. This provides flexibility for the user, while allowing for dynamic content to be updated when required, simply by swapping associated embedded grammars. For e.g., the grammar of street names could be updated based on the selected city. The IBM Embedded Via Voice (EVV) (Sicconi et al., 2009) (Beran et al., 2004) ASR engine provides this functionality and is used in this system. In this system, a two-pass speech recognition technique (Balchandran et al., 2009) is employed, based on multiple LMs where, the first pass is used to accurat</context>
</contexts>
<marker>Gillett, Ward, 1998</marker>
<rawString>John Gillett and Wayne Ward. 1998. A language model combining trigrams and stochastic contextfree grammars. In in International Conference on Spoken Language Processing, volume 6, pages 2319–2322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Sicconi</author>
<author>Kenneth White</author>
<author>Harvey Ruback</author>
</authors>
<title>Honda next generation speech user interface.</title>
<date>2009</date>
<booktitle>In SAE World Congress,</booktitle>
<pages>2009--01</pages>
<contexts>
<context position="2731" citStr="Sicconi et al., 2009" startWordPosition="428" endWordPosition="431">ngines, the GUI (Graphical User Interface), the Natural Language Processor and the Dialog Manager. 2.1 Speech Recognition and component Extraction Speech recognition is carried out using a statistical Language Model (LM) with Embedded Grammars (Gillett and Ward, 1998) to represent Named Entities such as city names, numbers etc. This provides flexibility for the user, while allowing for dynamic content to be updated when required, simply by swapping associated embedded grammars. For e.g., the grammar of street names could be updated based on the selected city. The IBM Embedded Via Voice (EVV) (Sicconi et al., 2009) (Beran et al., 2004) ASR engine provides this functionality and is used in this system. In this system, a two-pass speech recognition technique (Balchandran et al., 2009) is employed, based on multiple LMs where, the first pass is used to accurately recognize some components, and the values of these components are used to dynamically update another LM which is used for the second pass to recognize remaining components. Specifically, the first LM is used to recognize the city and state while the second is used to recognize the street name and number. The street names and optionally the house n</context>
</contexts>
<marker>Sicconi, White, Ruback, 2009</marker>
<rawString>Roberto Sicconi, Kenneth White, and Harvey Ruback. 2009. Honda next generation speech user interface. In SAE World Congress, pages 2009–01–0518.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>