<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.982966">
Semantic Representation of Non-Sentential Utterances in Dialog
</title>
<author confidence="0.98857">
Silvie Cinková
</author>
<affiliation confidence="0.994711">
Charles University in Prague
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
</affiliation>
<address confidence="0.885794">
Malostranské náměstí 25
CZ-118 00 Praha 1
</address>
<email confidence="0.999094">
cinkova@ufal.mff.cuni.cz
</email>
<sectionHeader confidence="0.995637" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999531">
Being confronted with spontaneous
speech, our current annotation scheme
requires alterations that would reflect the
abundant use of non-sentential fragments
with clausal meaning tightly connected to
their context, which do not systematically
occur in written texts. The purpose of this
paper is to list the common patterns of
non-sentential fragments and their con-
texts and to find a smooth resolution of
their semantic annotation.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999783571428571">
Spontaneous speech, even assuming a perfect
ASR, is hard to parse because of the enormous
occurrence of disfluencies and syntactic devia-
tions. Some disfluencies can be regarded as
speaker’s errors, which are being corrected or
remain uncorrected during the speaker’s turn.
Such disfluencies are e.g.:
</bodyText>
<listItem confidence="0.966487818181818">
• stammering (We w-went there
to-together)
• restart with or without an interregnum
(John no sorry Jane was
there, too)
• repetitions (So you like you
like drinking)
• hesitation sounds, long silence, fillers,
filler phrases, etc. (EH so ... you
kinda like you know HMM
drinking)
</listItem>
<bodyText confidence="0.9996716875">
In NLP, such disfluencies can be removed be-
fore any syntactic or semantic processing since
they cause confusion without adding any seman-
tic information. In machine-learning tasks, dis-
fluency is sought to be automatically removed by
learning from disfluency-marked corpora or cor-
pora of text edits (Hajič et al., 2008; Fitzgerald
and Jelinek, 2008) to smooth the input text into
written-language standard before parsing.
On the other hand, there is another sort of dis-
fluencies, which do not disturb the course of the
dialog, namely contextual ellipsis: even though
most people remember being taught at school to
answer questions with a complete sentence, not
even educated speakers performing a sophisti-
cated dialog always do so, and yet they do not
sound incorrect. Clearly, an extensive use of el-
lipsis is an inherent feature of verbal interaction
between speakers, which is usually smoothly
perceived by the listener and thus all right in its
place.
Such “fragmentary utterances that do not have
the form of a full sentence according to most tra-
ditional grammars, but that nevertheless convey
a complete clausal meaning” are called non-
sentential utterances (NSUs)1. A consistent
reconstruction of their clausal meaning is inevi-
table for any semantic representation of dialogs.
The present paper describes a tentative semantic
representation of NSUs in the Functional Gen-
erative Description (FGD) framework (Sgall et
al., 1986).
</bodyText>
<footnote confidence="0.692142">
1 The term NSU as well as its definition comes from
Fernández et al., 2007.
</footnote>
<note confidence="0.9461365">
Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 26–33,
Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.992086">
26
</page>
<sectionHeader confidence="0.774478" genericHeader="method">
2 NSUs in PhotoPal Dialogs
</sectionHeader>
<subsectionHeader confidence="0.866029">
2.1 NSU taxonomy
</subsectionHeader>
<bodyText confidence="0.992814625">
Fernández et al. (2007) introduce a taxonomy of
NSUs based on the dialog transcripts from BNC
(Burnard, 2000). They stress that NSUs are not
limited to question-answer pairs but can appear
as responses to any preceding utterance. Our ob-
servations confirm this. NSUs are highly am-
biguous without context. Consider the following
example:
</bodyText>
<figure confidence="0.934332909090909">
A: I left it on the table.
B: On the table.
I confirm/I understand
what you say: you left it on
the table.
A: Where did you leave it?
B: On the table.
I answer your question: I
left it on the table.
A: I think I put it er...
B: On the table.
</figure>
<construct confidence="0.749483">
I know in advance what
you want to say or what you
would want to say if you
knew that.
</construct>
<listItem confidence="0.797463">
A: Should I put it back on
the shelf?
B: On the table.
</listItem>
<bodyText confidence="0.9854485">
No, don’t put it back on
the shelf, but put it on the
table instead.
If reconstructed into a complete sentence, the
NSU would get different shapes in the respective
contexts (see the paraphrases in italics).
The NSU taxonomy proposed by Fernández et
al. (2007) divides the NSUs into 15 classes:
</bodyText>
<listItem confidence="0.99745992">
• Clarification Ellipsis (Two people
[did you say were there]?)
• Check Question ([...]Okay?)
• Reprise Sluice (What[did you say
]?)
• Direct Sluice (What?/Who?/When?)
• Short Answer [to wh-question] (My
Aunty Peggy.)
• Plain Affirmative Answer / Rejection
(Yes. / No.)
• Repeated Affirmative Answer (Very
loud, yes.)
• Helpful Rejection (No, Billy.)
• Plain Acknowledgement (Mhm.)
• Repeated Acknowledgement (part of the
preceding segment repeated)
• Propositional and Factual Modifiers
(Probably not. / Oh,
great!)
• Bare Modifier Phrase (adjuncts modify-
ing a contextual utterance)
• Conjunct (fragments introduced by con-
junctions)
• Filler (fragments filling a gap left by a
previous unfinished utterance)
</listItem>
<subsectionHeader confidence="0.99609">
2.2 PhotoPal Dialog Corpora
</subsectionHeader>
<bodyText confidence="0.999984961538461">
Our goal is semantically annotated spoken con-
versations between two speakers over a family
album. One English corpus (NAP) and one
Czech corpus have been built within the Com-
panions project (www.companions-project.org)
as gold-standard data for a machine-learning
based dialog system (“PhotoPal”) that should be
able to handle a natural-like conversation with a
human user, helping to sort the user’s photo-
graphs and encouraging the user to reminisce.
The PhotoPal is supposed to keep track of the
mentioned entities as well as to make some in-
ferences.
The NAP corpus (Bradley et al., 2008) com-
prises about 200k tokens of literal manual tran-
scriptions of audio recordings, which are inter-
linked with a multiple disfluency annotation
(Cinková et al., 2008). The Czech PhotoPal cor-
pus is still growing (Hajič et al., 2009), compris-
ing about 200k tokens at the moment (including
double annotation).
To ease the understanding, all authentic cor-
pus examples will be taken from the English
NAP corpus. However, most examples in this
paper are taken from Fernández et al. (2007) and
modified when needed to illustrate a contrast.
</bodyText>
<sectionHeader confidence="0.9953065" genericHeader="method">
3 Semantic representation of NAP
NSUs
</sectionHeader>
<subsectionHeader confidence="0.999711">
3.1 Functional Generative Description
</subsectionHeader>
<bodyText confidence="0.999887333333333">
The Functional Generative Description (FGD) is
a stratified formal language description based on
the structuralist tradition, developed since the
</bodyText>
<page confidence="0.987553">
27
</page>
<bodyText confidence="0.9998035">
1960’s. The unique contribution of FGD is the
so-called tectogrammatical representation (TR).
It is being implemented in a family of semanti-
cally annotated treebanks.
</bodyText>
<subsectionHeader confidence="0.999649">
3.2 Tectogrammatical Representation
</subsectionHeader>
<bodyText confidence="0.999990230769231">
Being conceived as an underlying syntactic rep-
resentation, the TR captures the linguistic mean-
ing of the sentence, which is its basic description
unit. In the TR annotation, each sentence is rep-
resented as a projective dependency tree with
nodes and edges. The attribute values include
references to the analytical (surface-syntax)
layer. Only content words are represented by
nodes. Function words are represented as attrib-
ute values. Each node has a semantic label
(“functor”), which renders the semantic relation
of the given node to its parent node. The TR an-
notation captures the following aspects of text:
</bodyText>
<listItem confidence="0.999874428571428">
• syntactic and semantic dependencies
• argument structure (data interlinked with
a lexicon)
• information structure (topic-focus articu-
lation)
• grammatical and contextual coreference
• ellipsis restoration.
</listItem>
<bodyText confidence="0.773047666666667">
Fig. 1 shows a sentence with restored ellipsis.
The elided predicate in the second conjunct was
copied from the first conjunct predicate (copied
and generated nodes have square shape).
Fig.1 Mary prepared the lunch, and John [prepared] the
dinner.
</bodyText>
<subsectionHeader confidence="0.988934">
3.3 Ellipsis Restoration and Contextual
Coreference
</subsectionHeader>
<bodyText confidence="0.99995275">
Assumingly, any tectogrammatical representa-
tion of NSUs is about the most appropriate reso-
lution of contextual ellipsis and coreference.
TR distinguishes two types of ellipsis:
</bodyText>
<listItem confidence="0.957352">
• contextual ellipsis, i.e. ellipsis occurring
when the lexical content of the omitted
element is clear from the context and
easily recoverable. The speaker omitted
this element, since he considered its
repetition unnecessary.
• grammatical ellipsis, i.e. such ellipsis
that occurs when the elided element can-
not appear on the surface for grammati-
cal reasons but is cognitively present in
the meaning of the utterance (e.g. the
unexpressed subject of controlled infini-
tives).
</listItem>
<bodyText confidence="0.977796481481482">
Every occurrence of a given verb must corre-
spond to the appropriate lexicon frame. Any
obligatory arguments missing must be filled in as
node substitutes even if the node could be copied
from the context. The substitutes have special
lemmas according to their function.
Fig. 2 illustrates a contextual ellipsis of a de-
pendent node. The tree represents the answer: He
has [wrapped the book] to the ques-
tion: Has the shop assistant
wrapped the book? In fact, the tree ren-
ders the sentence He has. To complete the ar-
gument structure frame of the verb wrap, the
node book with the Patient semantic label is
inserted into the frame in form of a node with the
t-lemma substitute for personal pronoun
(#PersPron, square node) exactly in the
same way as the expressed he. The node-
constituting lexical verb wrap is copied from the
previous sentence as a square node while has
becomes its attribute value, since it is an auxil-
iary verb. The subject He is only converted into
the #PersPron substitute (with appropriate
values inside).
Fig. 2 He has [wrapped the book].
In the complete TR annotation, a contextual-
coreference arrow would lead from the
</bodyText>
<page confidence="0.993095">
28
</page>
<bodyText confidence="0.999670666666667">
#PersPron nodes to their antecedent nodes in
the previous sentence (to assistant and
book, respectively).
</bodyText>
<subsectionHeader confidence="0.5751645">
3.4 Basic Principles of NSU Representation
in TR
</subsectionHeader>
<bodyText confidence="0.999854666666667">
The effort to reconstruct the clausal meaning of
non-sentential utterances was motivated by the
following basic assumptions:
</bodyText>
<listItem confidence="0.999090434782609">
• The text contains utterance-response
pairs.
• NSU is the response to an utterance U2.
• The utterance U has a finite-verb predi-
cate UPred with or without modifiers
(arguments and adjuncts) UMods, which
can be assigned functors.
• Even UPred can be an elided predicate.
• All NSUs (except interjections but incl.
plain yes and no) contain an implicit
(elided) predicate NSUPred. NSUPred is
either identical with UPred, or it is an
unknown verb, but we can imagine how
it relates NSU and U.
• NSU can be attached to a finite clause.
• NSU inherits UPred along with all
UMods.
• When there is a semantic conflict,
NSUMods overrule the inherited implicit
UMods in NSU (repetition is also re-
garded as conflict).
• NSUMod overrules UMod in the highest
position possible in the subtree.
</listItem>
<subsectionHeader confidence="0.866072">
3.5 TR Representation Elements for NSU
</subsectionHeader>
<bodyText confidence="0.999899307692308">
This annotation introduced a new category into
the annotation scheme. We called the category
response_type and designed it in the same way as
the coreference annotation. It is visualized as
arrows of various colors pointing from NSUMod
to UMod. Each type is indicated by a different
color.
The utterance-response pair consists of two
parts: the antecedent utterance U and the re-
sponse NSU. The finite verb predicate UPred is
typically the effective root of U, which has the
functor PRED, but not necessarily. On the other
hand, the elided predicate of NSU, called NSU-
</bodyText>
<footnote confidence="0.680901">
2 NSU is regarded as a response even if U is a statement and
NSU a question.
</footnote>
<bodyText confidence="0.9512045">
Pred, is the effective root of NSU and has the
functor PRED. Fig. 3 describes U in more detail.
</bodyText>
<figureCaption confidence="0.740232">
Fig 3. Utterance-response pair.
</figureCaption>
<bodyText confidence="0.999978181818182">
Whenever the clausal meaning of NSU can be
reconstructed by using the copy of UPred as
predicate, the t-lemma substitute for NSUPred is
#VerbPron, which is normally also used for
the pro-form do (dummy-do). NSUPred is al-
ways linked to UPred by a contextual-
coreference arrow. When the clausal meaning of
NSU cannot be directly reconstructed by using
the copy of UPred as the predicate, NSUPred is
rendered as the coreference-less t-lemma substi-
tute #EmpVerb, which is normally used for
cases of grammatical ellipsis of the predicate.
#EmpVerb has no obligatory arguments and
inherits no modifiers from anywhere. An NSU-
Pred that has coreference inherits all modifiers
from UPred, but these are not explicitly copied to
NSUPred. NSUPred’s own arguments are re-
garded as added to the inherited modifiers.
Hence the NSU “Peggy.” does not have to be
explicitly reconstructed as “That is
Peggy.” (the left figure in Fig.4), but just with
the coreferential predicate (the right figure).
</bodyText>
<figureCaption confidence="0.842855">
Fig. 4 Response NSU: Full explanative reconstruction
(left) and the actual annotation resolution (right).
</figureCaption>
<page confidence="0.996198">
29
</page>
<bodyText confidence="0.999524285714286">
Obviously, NSUMods can be in a semantic
conflict with the inherited UMods. These cases
are marked by several types of arrows leading
from the given NSUMod to the conflicting
UMod in the antecedent utterance U. We distin-
guish four types of semantic conflict between
NSUMod and UMod:
</bodyText>
<listItem confidence="0.99992625">
• overruling
• rephrasing
• wh-path
• other
</listItem>
<subsectionHeader confidence="0.828321">
3.6 Overruling
</subsectionHeader>
<bodyText confidence="0.999928375">
Overruling is the most typical semantic conflict
where an NSUMod gives exactly the same type
of information, but relating to a different entity
in the real world. If NSU is to be expressed as a
clause that uses the predicate of U, the conflict-
ing UMod is erased (or prevented from inherit-
ing) by the explicitly present NSUMod. E.g. in
the following utterance-response pair:
</bodyText>
<listItem confidence="0.842446333333333">
U: I’m in a little place
called Hellenthorpe.
NSU: Ellenthorpe.
NSU-paraphrase: You are in a
little place called Hellen
thorpe Ellenthorpe.
</listItem>
<bodyText confidence="0.997861230769231">
Even the explicit repetition is regarded as over-
ruling:
U: There were just two peo-
ple in the class.
NSU: Two people?.
NSU-paraphrase: Were there
just two people two people
in the class?
In the tree representation, the crossed text would
be visible only in the tree of U, and an overrul-
ing-reference arrow would point at them from
the relevant NSUMod. This conception prevents
doubling the same modifier in NSU.
</bodyText>
<subsectionHeader confidence="0.974664">
3.7 Rephrasing
</subsectionHeader>
<bodyText confidence="0.959629518518518">
When an NSUMod is rephrasing an UMod, then
UMod and NSUMod refer to the same entity in
the real world, or one refers to the entire entity
whereas the other one refers only to its part, etc.,
using a different wording. The NSUMod-UMod
relation marked as rephrasing is meant to be-
come the starting material for bridging anaphora
research. Example:
U: There were just two peo-
ple in the class.
NSU: Just two students?
NSU-paraphrase: Were there
just two people two students
in the class?
It is also applied when the context is unambigu-
ous for the speakers but ambiguous for the anno-
tator, who lacks their background knowledge of
the given situation. In the following example the
annotator may not know whether this part
or just the end of this part should come up,
because he does not see the speakers pointing at
the crane, but it is rather evident that it is not a
completely different part of the crane but some-
thing at the end of it:
U1: You lift the crane, so
this part comes up.
NSU1/U2: The end?
</bodyText>
<footnote confidence="0.819596714285714">
NSU1/U2-paraphrase1: Do you
mean the end comes up?
NSU1/U2-paraphrase2: Do you
mean the end of this part
comes up?
NSU2/U3: Just this.
NSU3: Okay.
</footnote>
<bodyText confidence="0.997326">
The category “Other” (see below) is though
strongly preferred in ambiguous cases.
</bodyText>
<subsectionHeader confidence="0.997056">
3.8 Wh-path3
</subsectionHeader>
<bodyText confidence="0.999773133333333">
The wh-path relation is the relation between the
modifier that is focused by a wh-word in an U
that is a direct or indirect question and a NSU-
Mod that makes a good answer.
Overruling as well as rephrasing assume that
the conflicting modifiers have the same functor.
The wh-path category is different from the others
in that it allows setting in conflict a UMod with
an NSUMod with different semantic labels
(functors). Our tentative annotation suggests that
regular patterns will occur; e.g. with the question
about direction/location. When asking where,
speakers often get replies that would actually
match questions with whom (functor ACMP)
or with which intention (functor INTT,
</bodyText>
<footnote confidence="0.899026">
3 The term was found in Hajičová (1995) and reused
by placing it in context with other response types.
</footnote>
<page confidence="0.99787">
30
</page>
<bodyText confidence="0.940532875">
e.g., go shopping), and yet they are per-
ceived as good answers.
The relation between an utterance U which is
a statement and an NSU which is a sluice is not
wh-path but overruling. Cf.:
U: Where would you like to
go tomorrow?
NSU: Downtown with Mary, to
do some shopping. (wh-path)
U: I would like to go down-
town with Mary tomorrow.
NSU: Where? (overruling)
Sluices are not regarded as ambiguous in the
sense whether referring to the same entity as the
corresponding wh-word or not. They are not eli-
gible for the relation “other” (see next section).
</bodyText>
<subsectionHeader confidence="0.952697">
3.9 Other
</subsectionHeader>
<bodyText confidence="0.999836333333333">
“Other” is meant for inherently ambiguous cases
of conflicting UMod and NSUMod where it is
impossible to decide whether NSUMod is re-
phrasing or overruling UMod. Textual ambiguity
arises when NSU is a question that does not find
a proper answer in the context:
</bodyText>
<listItem confidence="0.830865">
U1: He’s got the best room.
NSU1/U2: Room 128?
NSU1/U2-paraphrase: Has he
got the best room Room 128?
U3: I don’t know which num-
ber.
</listItem>
<subsectionHeader confidence="0.371301">
3.10 TR-Conditioned Criteria for NSU types
</subsectionHeader>
<bodyText confidence="0.999087">
The original idea of the tectogrammatical repre-
sentation of NSU was to adopt the taxonomy
proposed by Fernández et al. (2007). However,
the rules of TR made some classes collapse as
they yielded identical tectogrammatical tree
structures. The main criteria for tectogrammati-
cal representation of NSU were the following:
Is the NSU a phrase or just an interjection? (Cf.
Fig. 5 and 6)
</bodyText>
<listItem confidence="0.9918621">
• If it is a content word or a phrase, it
should be reconstructed into a clause by
adding a predicate.
• If it is an interjection except yes and no
(and their colloquial variants), no predi-
cate is added.
• If it is yes/no (and variants), a predi-
cate should be added.
• If the interjection acts as a backchannel,
yes and no make no exception.
</listItem>
<figureCaption confidence="0.976409">
Fig. 5 Interjection
Fig. 6 Is this John? No, Billy [This is not John, this is Billy.]
</figureCaption>
<bodyText confidence="0.990179">
Can we copy UPred to make NSU a clause?
</bodyText>
<listItem confidence="0.6293832">
• If we can, NSUPred has the t-lemma
substitute #VerbPron and a corefer-
ential arrow points from NSUPred to
UPred.
• If we cannot, NSUPred has the t-lemma
</listItem>
<bodyText confidence="0.902088">
#EmpVerb with no coreferential arrow.
No response type arrows point from
NSUMods to UMods. In specific cases
the coreference to UPred leads from
elsewhere (Fig.7).
</bodyText>
<page confidence="0.999837">
31
</page>
<figureCaption confidence="0.9873">
Fig. 7 Check question/Evaluative response related to text:
U: I am allowed to record you.
</figureCaption>
<table confidence="0.565753">
NSU (same speaker): Okay?
NSU-paraphrase: Is it (that I’m allowed
to record you) okay?
or
U: I am allowed to record you.
NSU (turn switch): Okay.
</table>
<construct confidence="0.375088">
NSU-paraphrase: It &lt;is&gt; okay that you
are allowed to record me.
</construct>
<sectionHeader confidence="0.728918" genericHeader="method">
3.11 More Examples of U-NSU relation reso-
lution
</sectionHeader>
<bodyText confidence="0.9998316">
Fernández et al. (2007) distinguish two types of
sluice: the direct and the reprise sluice. In TR,
each has a different semantic representation. The
direct sluice has the coreferential predicate while
the reprise sluice, which can be paraphrased as
What did you mean by saying
this?, has the empty-verb predicate and the
wh-word gets the functor EFF, which is normally
assigned to what is being said in the argument
structure pattern of verbs of saying (Fig. 8).
</bodyText>
<figureCaption confidence="0.967792">
Fig. 8 Reprise sluice
Fig. 9 shows a sentence with wh-path linking
modifiers with different functors.
Fig. 9 Wh-path linking Mods with different functors
</figureCaption>
<construct confidence="0.75687275">
U: Where would you like to go tomorrow?
NSU: Shopping with Mary.
NSU-paraphrase: Tomorrow I would like to
go shopping with Mary.
</construct>
<bodyText confidence="0.995857857142857">
Choice questions (Fig.10) represent an interest-
ing example in which one NSUMod can enter
different relations to different UMods. The
NSUMod beer overrules the coordinated UMod
Coke or Pepsi, and at the same time it is
connected with the wh-question Which do
you like to drink? by wh-path.
</bodyText>
<figureCaption confidence="0.596687">
Fig. 10 Choice question.
</figureCaption>
<bodyText confidence="0.797624818181818">
U: Which do you like to drink: Coke or
Pepsi?
NSU: Beer.
NSU-paraphrase: I like to drink beer.
Seeing the many rephrasing cases in the data,
which are supposed to be subject to further
anaphora annotation (bridging etc.), we had to
ask the question whether the boundary between
response_type and coreference can be reliably
determined. We found good evidence in the
made-up but not unlikely example below (Fig.
</bodyText>
<page confidence="0.997411">
32
</page>
<bodyText confidence="0.995016">
11). In this context, him will be coreferential
with Paul and her will be coreferential with
Mary. On the other hand, him will overrule
Mary and her will overrule Paul (only the
relations of him are marked in the figure).
</bodyText>
<figureCaption confidence="0.880909">
Fig. 11 Coreference vs. response type
</figureCaption>
<subsectionHeader confidence="0.348234">
3.12 Current and Future Work
</subsectionHeader>
<bodyText confidence="0.9999374375">
The proposed enhancement of the annotation
scheme has been tested on a corpus of approx.
200 NSUs with context manually extracted from
the NAP transcripts as well as on example sen-
tences from Fernández et al. (2007) and many
sentences obtained by their modification per-
formed in order to get potentially difficult coun-
terexamples. As this is still a preparatory work,
neither the inter-annotator agreement nor any
other evaluation could be done so far.
In the next future, parts of the spoken corpora
should get tectogrammatical parsing. The manual
annotation is supposed to adopt this new feature
of the annotation scheme, and we will try to in-
corporate it into our statistically trained auto-
matic parsing tools.
</bodyText>
<sectionHeader confidence="0.937717" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.999920583333333">
The confrontation of our current annotation
scheme with spoken dialog data has raised issues
of ellipsis restoration and textual coreference in
non-sentential utterances. We have found com-
mon relations between non-sentential utterances
and their contexts, and we have integrated them
into our semantic annotation scheme without
violating its general principles. A tentative man-
ual annotation of these relations in a small corpus
suggests that such annotation is feasible. Further
investigation on larger data along with machine-
learning experiments is intended.
</bodyText>
<sectionHeader confidence="0.992142" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999940666666667">
This work was funded in part by the Companions
project (www.companions-project.org) spon-
sored by the European Commission as part of the
Information Society Technologies (IST) pro-
gramme under EC grant number IST-FP6-
034434, as well as by the Czech Science Founda-
tion (GA405/06/0589), and by the Czech Minis-
try of Education (MSM0021620838, MŠMT CR
LC536).
</bodyText>
<sectionHeader confidence="0.999184" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999727756097561">
Jay Bradley, Oli Mival, and D. Benyon. 2008. A
Novel Architecture for Designing by Wizard of Oz.
In: Proceeding of CREATE08, British computer
Society, Covent Garden, London, 24-25 June 2008.
Lou Burnard. 2000. Reference Guide for the British
National Corpus (World Edition). Oxford Univer-
sity Computing Services. Available from
ftp://sable.ox.ac.uk/pub/ota/BNC.
Silvie Cinková, Jan Hajic, Jan Ptácek. 2008. An An-
notation Scheme for Speech Reconstruction on a
Dialog Corpus. In Fourth International Workshop
on Human-Computer Conversation. Bellagio, Italy:
[http://www.companions-project.org/events/
200810_bellagio.cfm],2008:1-6.
Raquel Fernández, Jonathan Ginzburg, and Shalom
Lappin. 2007. Classifying Non-Sentential Utter-
ances in Dialogue: A Machine Learning Approach.
Computational Linguistics, Volume 33, Nr. 3. MIT
Press for the Association for Computational Lin-
guistics.
Erin Fitzgerald and Frederick Jelinek. 2008. Linguis-
tic Resources for Reconstructing Spontaneous
Speech Text. In: LREC 2008 Proceedings.
Jan Hajic, Silvie Cinková, Marie Mikulová, Petr Pa-
jas, Jan Ptácek, Josef Toman, Zdenka Urešová.
2008. PDTSL: An Annotated Resource For Speech
Reconstruction. In Proceedings of the 2008 IEEE
Workshop on Spoken Language Technology.
IEEE, 2008.
Jan Hajic, Marie Mikulová, Martina Otradovcová,
Petr Pajas, Nino Peterek, Pavel Ceška, Miroslav
Spousta. 2009. PDTSL - Prague Dependency Tree-
bank of Spoken Language - Czech, Institute of
Formal and Applied Linguistics, Charles Univer-
sity in Prague.
Eva Hajicová (ed.) 1995. Text And-Inference-Based
Approach to Question Answering. Prague.
Petr Sgall, Eva Hajicová, and Jarmila Panevová.
1986. The Meaning of the Sentence in Its Semantic
and Pragmatic Aspects. Dordrecht:Reidel Publish-
ing Company and Prague:Academia.
</reference>
<page confidence="0.999378">
33
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.389454">
<title confidence="0.999313">Semantic Representation of Non-Sentential Utterances in Dialog</title>
<author confidence="0.448389">Silvie</author>
<affiliation confidence="0.896811333333333">Charles University in Faculty of Mathematics and Institute of Formal and Applied</affiliation>
<address confidence="0.92999">CZ-118 00 Praha 1</address>
<email confidence="0.940948">cinkova@ufal.mff.cuni.cz</email>
<abstract confidence="0.99971125">Being confronted with spontaneous speech, our current annotation scheme requires alterations that would reflect the abundant use of non-sentential fragments with clausal meaning tightly connected to their context, which do not systematically occur in written texts. The purpose of this paper is to list the common patterns of non-sentential fragments and their contexts and to find a smooth resolution of their semantic annotation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jay Bradley</author>
<author>Oli Mival</author>
<author>D Benyon</author>
</authors>
<title>A Novel Architecture for Designing by Wizard of Oz. In:</title>
<date>2008</date>
<booktitle>Proceeding of CREATE08, British computer Society,</booktitle>
<location>Covent Garden, London,</location>
<contexts>
<context position="5372" citStr="Bradley et al., 2008" startWordPosition="856" endWordPosition="859">e) 2.2 PhotoPal Dialog Corpora Our goal is semantically annotated spoken conversations between two speakers over a family album. One English corpus (NAP) and one Czech corpus have been built within the Companions project (www.companions-project.org) as gold-standard data for a machine-learning based dialog system (“PhotoPal”) that should be able to handle a natural-like conversation with a human user, helping to sort the user’s photographs and encouraging the user to reminisce. The PhotoPal is supposed to keep track of the mentioned entities as well as to make some inferences. The NAP corpus (Bradley et al., 2008) comprises about 200k tokens of literal manual transcriptions of audio recordings, which are interlinked with a multiple disfluency annotation (Cinková et al., 2008). The Czech PhotoPal corpus is still growing (Hajič et al., 2009), comprising about 200k tokens at the moment (including double annotation). To ease the understanding, all authentic corpus examples will be taken from the English NAP corpus. However, most examples in this paper are taken from Fernández et al. (2007) and modified when needed to illustrate a contrast. 3 Semantic representation of NAP NSUs 3.1 Functional Generative Des</context>
</contexts>
<marker>Bradley, Mival, Benyon, 2008</marker>
<rawString>Jay Bradley, Oli Mival, and D. Benyon. 2008. A Novel Architecture for Designing by Wizard of Oz. In: Proceeding of CREATE08, British computer Society, Covent Garden, London, 24-25 June 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lou Burnard</author>
</authors>
<title>Reference Guide for the British National Corpus (World Edition). Oxford University Computing Services. Available from ftp://sable.ox.ac.uk/pub/ota/BNC.</title>
<date>2000</date>
<contexts>
<context position="3139" citStr="Burnard, 2000" startWordPosition="481" endWordPosition="482"> for any semantic representation of dialogs. The present paper describes a tentative semantic representation of NSUs in the Functional Generative Description (FGD) framework (Sgall et al., 1986). 1 The term NSU as well as its definition comes from Fernández et al., 2007. Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 26–33, Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics 26 2 NSUs in PhotoPal Dialogs 2.1 NSU taxonomy Fernández et al. (2007) introduce a taxonomy of NSUs based on the dialog transcripts from BNC (Burnard, 2000). They stress that NSUs are not limited to question-answer pairs but can appear as responses to any preceding utterance. Our observations confirm this. NSUs are highly ambiguous without context. Consider the following example: A: I left it on the table. B: On the table. I confirm/I understand what you say: you left it on the table. A: Where did you leave it? B: On the table. I answer your question: I left it on the table. A: I think I put it er... B: On the table. I know in advance what you want to say or what you would want to say if you knew that. A: Should I put it back on the shelf? B: On </context>
</contexts>
<marker>Burnard, 2000</marker>
<rawString>Lou Burnard. 2000. Reference Guide for the British National Corpus (World Edition). Oxford University Computing Services. Available from ftp://sable.ox.ac.uk/pub/ota/BNC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silvie Cinková</author>
<author>Jan Hajic</author>
<author>Jan Ptácek</author>
</authors>
<title>An Annotation Scheme for Speech Reconstruction on a Dialog Corpus.</title>
<date>2008</date>
<booktitle>In Fourth International Workshop on Human-Computer Conversation. Bellagio, Italy: [http://www.companions-project.org/events/</booktitle>
<pages>200810--2008</pages>
<contexts>
<context position="5537" citStr="Cinková et al., 2008" startWordPosition="882" endWordPosition="885">ch corpus have been built within the Companions project (www.companions-project.org) as gold-standard data for a machine-learning based dialog system (“PhotoPal”) that should be able to handle a natural-like conversation with a human user, helping to sort the user’s photographs and encouraging the user to reminisce. The PhotoPal is supposed to keep track of the mentioned entities as well as to make some inferences. The NAP corpus (Bradley et al., 2008) comprises about 200k tokens of literal manual transcriptions of audio recordings, which are interlinked with a multiple disfluency annotation (Cinková et al., 2008). The Czech PhotoPal corpus is still growing (Hajič et al., 2009), comprising about 200k tokens at the moment (including double annotation). To ease the understanding, all authentic corpus examples will be taken from the English NAP corpus. However, most examples in this paper are taken from Fernández et al. (2007) and modified when needed to illustrate a contrast. 3 Semantic representation of NAP NSUs 3.1 Functional Generative Description The Functional Generative Description (FGD) is a stratified formal language description based on the structuralist tradition, developed since the 27 1960’s.</context>
</contexts>
<marker>Cinková, Hajic, Ptácek, 2008</marker>
<rawString>Silvie Cinková, Jan Hajic, Jan Ptácek. 2008. An Annotation Scheme for Speech Reconstruction on a Dialog Corpus. In Fourth International Workshop on Human-Computer Conversation. Bellagio, Italy: [http://www.companions-project.org/events/ 200810_bellagio.cfm],2008:1-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raquel Fernández</author>
<author>Jonathan Ginzburg</author>
<author>Shalom Lappin</author>
</authors>
<title>Classifying Non-Sentential Utterances in Dialogue: A Machine Learning Approach.</title>
<date>2007</date>
<journal>Computational Linguistics, Volume</journal>
<volume>33</volume>
<publisher>MIT Press</publisher>
<contexts>
<context position="2795" citStr="Fernández et al., 2007" startWordPosition="426" endWordPosition="429">y smoothly perceived by the listener and thus all right in its place. Such “fragmentary utterances that do not have the form of a full sentence according to most traditional grammars, but that nevertheless convey a complete clausal meaning” are called nonsentential utterances (NSUs)1. A consistent reconstruction of their clausal meaning is inevitable for any semantic representation of dialogs. The present paper describes a tentative semantic representation of NSUs in the Functional Generative Description (FGD) framework (Sgall et al., 1986). 1 The term NSU as well as its definition comes from Fernández et al., 2007. Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 26–33, Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics 26 2 NSUs in PhotoPal Dialogs 2.1 NSU taxonomy Fernández et al. (2007) introduce a taxonomy of NSUs based on the dialog transcripts from BNC (Burnard, 2000). They stress that NSUs are not limited to question-answer pairs but can appear as responses to any preceding utterance. Our observations confirm this. NSUs are highly ambiguous without context. Consider the following example: A: I left it on the table. B:</context>
<context position="5853" citStr="Fernández et al. (2007)" startWordPosition="935" endWordPosition="938">ce. The PhotoPal is supposed to keep track of the mentioned entities as well as to make some inferences. The NAP corpus (Bradley et al., 2008) comprises about 200k tokens of literal manual transcriptions of audio recordings, which are interlinked with a multiple disfluency annotation (Cinková et al., 2008). The Czech PhotoPal corpus is still growing (Hajič et al., 2009), comprising about 200k tokens at the moment (including double annotation). To ease the understanding, all authentic corpus examples will be taken from the English NAP corpus. However, most examples in this paper are taken from Fernández et al. (2007) and modified when needed to illustrate a contrast. 3 Semantic representation of NAP NSUs 3.1 Functional Generative Description The Functional Generative Description (FGD) is a stratified formal language description based on the structuralist tradition, developed since the 27 1960’s. The unique contribution of FGD is the so-called tectogrammatical representation (TR). It is being implemented in a family of semantically annotated treebanks. 3.2 Tectogrammatical Representation Being conceived as an underlying syntactic representation, the TR captures the linguistic meaning of the sentence, which</context>
<context position="16640" citStr="Fernández et al. (2007)" startWordPosition="2717" endWordPosition="2720">igible for the relation “other” (see next section). 3.9 Other “Other” is meant for inherently ambiguous cases of conflicting UMod and NSUMod where it is impossible to decide whether NSUMod is rephrasing or overruling UMod. Textual ambiguity arises when NSU is a question that does not find a proper answer in the context: U1: He’s got the best room. NSU1/U2: Room 128? NSU1/U2-paraphrase: Has he got the best room Room 128? U3: I don’t know which number. 3.10 TR-Conditioned Criteria for NSU types The original idea of the tectogrammatical representation of NSU was to adopt the taxonomy proposed by Fernández et al. (2007). However, the rules of TR made some classes collapse as they yielded identical tectogrammatical tree structures. The main criteria for tectogrammatical representation of NSU were the following: Is the NSU a phrase or just an interjection? (Cf. Fig. 5 and 6) • If it is a content word or a phrase, it should be reconstructed into a clause by adding a predicate. • If it is an interjection except yes and no (and their colloquial variants), no predicate is added. • If it is yes/no (and variants), a predicate should be added. • If the interjection acts as a backchannel, yes and no make no exception.</context>
<context position="18059" citStr="Fernández et al. (2007)" startWordPosition="2970" endWordPosition="2973">erential arrow points from NSUPred to UPred. • If we cannot, NSUPred has the t-lemma #EmpVerb with no coreferential arrow. No response type arrows point from NSUMods to UMods. In specific cases the coreference to UPred leads from elsewhere (Fig.7). 31 Fig. 7 Check question/Evaluative response related to text: U: I am allowed to record you. NSU (same speaker): Okay? NSU-paraphrase: Is it (that I’m allowed to record you) okay? or U: I am allowed to record you. NSU (turn switch): Okay. NSU-paraphrase: It &lt;is&gt; okay that you are allowed to record me. 3.11 More Examples of U-NSU relation resolution Fernández et al. (2007) distinguish two types of sluice: the direct and the reprise sluice. In TR, each has a different semantic representation. The direct sluice has the coreferential predicate while the reprise sluice, which can be paraphrased as What did you mean by saying this?, has the empty-verb predicate and the wh-word gets the functor EFF, which is normally assigned to what is being said in the argument structure pattern of verbs of saying (Fig. 8). Fig. 8 Reprise sluice Fig. 9 shows a sentence with wh-path linking modifiers with different functors. Fig. 9 Wh-path linking Mods with different functors U: Whe</context>
<context position="20014" citStr="Fernández et al. (2007)" startWordPosition="3298" endWordPosition="3301"> coreference can be reliably determined. We found good evidence in the made-up but not unlikely example below (Fig. 32 11). In this context, him will be coreferential with Paul and her will be coreferential with Mary. On the other hand, him will overrule Mary and her will overrule Paul (only the relations of him are marked in the figure). Fig. 11 Coreference vs. response type 3.12 Current and Future Work The proposed enhancement of the annotation scheme has been tested on a corpus of approx. 200 NSUs with context manually extracted from the NAP transcripts as well as on example sentences from Fernández et al. (2007) and many sentences obtained by their modification performed in order to get potentially difficult counterexamples. As this is still a preparatory work, neither the inter-annotator agreement nor any other evaluation could be done so far. In the next future, parts of the spoken corpora should get tectogrammatical parsing. The manual annotation is supposed to adopt this new feature of the annotation scheme, and we will try to incorporate it into our statistically trained automatic parsing tools. Conclusion The confrontation of our current annotation scheme with spoken dialog data has raised issu</context>
</contexts>
<marker>Fernández, Ginzburg, Lappin, 2007</marker>
<rawString>Raquel Fernández, Jonathan Ginzburg, and Shalom Lappin. 2007. Classifying Non-Sentential Utterances in Dialogue: A Machine Learning Approach. Computational Linguistics, Volume 33, Nr. 3. MIT Press for the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erin Fitzgerald</author>
<author>Frederick Jelinek</author>
</authors>
<title>Linguistic Resources for Reconstructing Spontaneous Speech Text. In: LREC</title>
<date>2008</date>
<note>Proceedings.</note>
<contexts>
<context position="1637" citStr="Fitzgerald and Jelinek, 2008" startWordPosition="241" endWordPosition="244">: • stammering (We w-went there to-together) • restart with or without an interregnum (John no sorry Jane was there, too) • repetitions (So you like you like drinking) • hesitation sounds, long silence, fillers, filler phrases, etc. (EH so ... you kinda like you know HMM drinking) In NLP, such disfluencies can be removed before any syntactic or semantic processing since they cause confusion without adding any semantic information. In machine-learning tasks, disfluency is sought to be automatically removed by learning from disfluency-marked corpora or corpora of text edits (Hajič et al., 2008; Fitzgerald and Jelinek, 2008) to smooth the input text into written-language standard before parsing. On the other hand, there is another sort of disfluencies, which do not disturb the course of the dialog, namely contextual ellipsis: even though most people remember being taught at school to answer questions with a complete sentence, not even educated speakers performing a sophisticated dialog always do so, and yet they do not sound incorrect. Clearly, an extensive use of ellipsis is an inherent feature of verbal interaction between speakers, which is usually smoothly perceived by the listener and thus all right in its p</context>
</contexts>
<marker>Fitzgerald, Jelinek, 2008</marker>
<rawString>Erin Fitzgerald and Frederick Jelinek. 2008. Linguistic Resources for Reconstructing Spontaneous Speech Text. In: LREC 2008 Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajic</author>
<author>Silvie Cinková</author>
<author>Marie Mikulová</author>
<author>Petr Pajas</author>
<author>Jan Ptácek</author>
<author>Josef Toman</author>
<author>Zdenka Urešová</author>
</authors>
<title>PDTSL: An Annotated Resource For Speech Reconstruction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 IEEE Workshop on Spoken Language Technology. IEEE,</booktitle>
<marker>Hajic, Cinková, Mikulová, Pajas, Ptácek, Toman, Urešová, 2008</marker>
<rawString>Jan Hajic, Silvie Cinková, Marie Mikulová, Petr Pajas, Jan Ptácek, Josef Toman, Zdenka Urešová. 2008. PDTSL: An Annotated Resource For Speech Reconstruction. In Proceedings of the 2008 IEEE Workshop on Spoken Language Technology. IEEE, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajic</author>
<author>Marie Mikulová</author>
<author>Martina Otradovcová</author>
</authors>
<title>Petr Pajas, Nino Peterek, Pavel Ceška, Miroslav Spousta.</title>
<date>2009</date>
<marker>Hajic, Mikulová, Otradovcová, 2009</marker>
<rawString>Jan Hajic, Marie Mikulová, Martina Otradovcová, Petr Pajas, Nino Peterek, Pavel Ceška, Miroslav Spousta. 2009. PDTSL - Prague Dependency Treebank of Spoken Language - Czech, Institute of Formal and Applied Linguistics, Charles University in Prague.</rawString>
</citation>
<citation valid="true">
<title>Text And-Inference-Based Approach to Question Answering.</title>
<date>1995</date>
<editor>Eva Hajicová (ed.)</editor>
<location>Prague.</location>
<contexts>
<context position="15452" citStr="(1995)" startWordPosition="2511" endWordPosition="2511">indirect question and a NSUMod that makes a good answer. Overruling as well as rephrasing assume that the conflicting modifiers have the same functor. The wh-path category is different from the others in that it allows setting in conflict a UMod with an NSUMod with different semantic labels (functors). Our tentative annotation suggests that regular patterns will occur; e.g. with the question about direction/location. When asking where, speakers often get replies that would actually match questions with whom (functor ACMP) or with which intention (functor INTT, 3 The term was found in Hajičová (1995) and reused by placing it in context with other response types. 30 e.g., go shopping), and yet they are perceived as good answers. The relation between an utterance U which is a statement and an NSU which is a sluice is not wh-path but overruling. Cf.: U: Where would you like to go tomorrow? NSU: Downtown with Mary, to do some shopping. (wh-path) U: I would like to go downtown with Mary tomorrow. NSU: Where? (overruling) Sluices are not regarded as ambiguous in the sense whether referring to the same entity as the corresponding wh-word or not. They are not eligible for the relation “other” (se</context>
</contexts>
<marker>1995</marker>
<rawString>Eva Hajicová (ed.) 1995. Text And-Inference-Based Approach to Question Answering. Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Sgall</author>
<author>Eva Hajicová</author>
<author>Jarmila Panevová</author>
</authors>
<title>The Meaning of the Sentence in Its Semantic and Pragmatic Aspects. Dordrecht:Reidel Publishing Company and Prague:Academia.</title>
<date>1986</date>
<contexts>
<context position="2719" citStr="Sgall et al., 1986" startWordPosition="411" endWordPosition="414"> inherent feature of verbal interaction between speakers, which is usually smoothly perceived by the listener and thus all right in its place. Such “fragmentary utterances that do not have the form of a full sentence according to most traditional grammars, but that nevertheless convey a complete clausal meaning” are called nonsentential utterances (NSUs)1. A consistent reconstruction of their clausal meaning is inevitable for any semantic representation of dialogs. The present paper describes a tentative semantic representation of NSUs in the Functional Generative Description (FGD) framework (Sgall et al., 1986). 1 The term NSU as well as its definition comes from Fernández et al., 2007. Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 26–33, Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics 26 2 NSUs in PhotoPal Dialogs 2.1 NSU taxonomy Fernández et al. (2007) introduce a taxonomy of NSUs based on the dialog transcripts from BNC (Burnard, 2000). They stress that NSUs are not limited to question-answer pairs but can appear as responses to any preceding utterance. Our observations confirm this. NSUs are highly ambiguous wi</context>
</contexts>
<marker>Sgall, Hajicová, Panevová, 1986</marker>
<rawString>Petr Sgall, Eva Hajicová, and Jarmila Panevová. 1986. The Meaning of the Sentence in Its Semantic and Pragmatic Aspects. Dordrecht:Reidel Publishing Company and Prague:Academia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>