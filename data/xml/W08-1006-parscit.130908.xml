<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013485">
<title confidence="0.944846">
Parsing Three German Treebanks: Lexicalized and Unlexicalized Baselines
</title>
<author confidence="0.941474">
Anna N. Rafferty and Christopher D. Manning
</author>
<affiliation confidence="0.9822825">
Computer Science Department
Stanford University
</affiliation>
<address confidence="0.93054">
Stanford, CA 94305
</address>
<email confidence="0.999764">
{rafferty,manning}@stanford.edu
</email>
<sectionHeader confidence="0.998597" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999224947368421">
Previous work on German parsing has pro-
vided confusing and conflicting results con-
cerning the difficulty of the task and whether
techniques that are useful for English, such
as lexicalization, are effective for German.
This paper aims to provide some understand-
ing and solid baseline numbers for the task.
We examine the performance of three tech-
niques on three treebanks (Negra, Tiger, and
T¨uBa-D/Z): (i) Markovization, (ii) lexicaliza-
tion, and (iii) state splitting. We additionally
explore parsing with the inclusion of gram-
matical function information. Explicit gram-
matical functions are important to German
language understanding, but they are numer-
ous, and naively incorporating them into a
parser which assumes a small phrasal category
inventory causes large performance reductions
due to increasing sparsity.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999768461538461">
Recent papers provide mixed evidence as to whether
techniques that increase statistical parsing perfor-
mance for English also improve German parsing
performance (Dubey and Keller, 2003; K¨ubler et al.,
2006). We provide a systematic exploration of this
topic to shed light on what techniques might bene-
fit German parsing and show general trends in the
relative performance increases for each technique.
While these results vary across treebanks, due to
differences in annotation schemes as discussed by
K¨ubler (2005), we also find similarities and provide
explanations for the trend differences based on the
annotation schemes.
</bodyText>
<page confidence="0.975278">
40
</page>
<bodyText confidence="0.999976147058823">
We address three parsing techniques:
(i) Markovization, (ii) lexicalization, and (iii) state
splitting (i.e., subcategorization). These techniques
are not independent, and we thus examine how
lexicalization and Markovization interact, since
lexicalization for German has been the most
contentious area in the literature. Many of these
techniques have been investigated in other work
(Schiehlen, 2004; Dubey, 2004; Dubey, 2005),
but, we hope that by consolidating, replicating,
improving, and clarifying previous results we can
contribute to the re-evaluation of German proba-
bilistic parsing after a somewhat confusing start to
initial literature in this area.
One feature of German that differs markedly from
English is substantial free word order. This requires
the marking of grammatical functions on phrases to
indicate their syntactic function in sentences (sub-
ject, object, etc.), whereas for English these func-
tions can be derived from configurations (Chomsky,
1965; de Marneffe et al., 2006). While some simi-
lar functions are present in English treebanks, they
are used more frequently in German treebanks and
many more unique functions and category-function
pairings exist. Because of the relatively free word
ordering in German, the usefulness of parses is sub-
stantially increased by generating them with this in-
formation. We demonstrate the difficulties intro-
duced by naively concatenating these functions to
categories and how this treatment interacts with the
other parsing techniques. There are several avenues
for improving this situation in future work. The ver-
sions of the treebanks we use here do not include
case information in part-of-speech tags and we do
</bodyText>
<note confidence="0.407026">
Proceedings of the ACL-08: HLT Workshop on Parsing German (PaGe-08), pages 40–46,
</note>
<page confidence="0.396138">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<table confidence="0.9973315">
Treebank Train Dev &lt; 40 Test &lt; 40
Tiger 20894 2611 2535 2611 2525
T¨uBa-D/Z 20894 2611 2611 2611 2611
Negra v2 18602 1000 975 1000 968
</table>
<tableCaption confidence="0.999889">
Table 1: Size in sentences of treebanks used in this paper.
</tableCaption>
<bodyText confidence="0.900953230769231">
“Tiger” and “T¨uBa-D/Z” refer to the corpora prepared for
the ACL-08 workshop shared task; the full Tiger corpus
is much larger. Our Negra results are on the test set.
not use any morphological analyzer; this should be
rectified in future work. A new parsing model could
be written to treat separate grammatical functions
for nodes as first class objects, rather than just con-
catenating phrasal categories and functions. Finally,
assignment of grammatical functions could be left
to a separate post-processing phase, which could ex-
ploit not only case information inside noun phrases
but joint information across the subcategorization
frames of predicates.
</bodyText>
<sectionHeader confidence="0.995754" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.999992">
We use the Stanford Parser (Klein and Manning,
2003b) for all experiments. An advantage of this
parser for baseline experiments is that it provides
clean, simple implementations of component mod-
els, with many configuration options. We show re-
sults in most instances for evaluations both with and
without grammatical functions and with and without
gold tags. When training and parsing with the inclu-
sion of grammatical functions, we treat each pair-
ing of basic category and grammatical function as
one new category. Rules are learned for each such
category with a separate orthographic form, with
no attempt to learn general rules for nodes with the
same basic category but different functions. Clearly,
more sophisticated methods of handling grammat-
ical functions exist, but our focus is on providing
baseline results that are easily replicable by others.
We focus primarily on the T¨uBa-D/Z and Tiger
corpora, training on the training sets for the ACL
2008 Workshop on Parsing German shared task and
providing ablation results based on development set
performance. Additionally, we show a limited num-
ber of results on the Negra corpus, using the standard
training/development/test splits, defined in (Dubey
and Keller, 2003). The sizes of these data sets are
shown in table 1.
</bodyText>
<sectionHeader confidence="0.994348" genericHeader="method">
3 Markovization
</sectionHeader>
<bodyText confidence="0.999980065217391">
Previous work has shown that adding vertical
Markovization ((grand-)parent annotation) and us-
ing horizontal Markovization can greatly improve
English parsing performance (Klein and Manning,
2003a). Several papers have already reported par-
tially corresponding results on German: Schiehlen
(2004) and Dubey (2004) reported gains of several
percent for unlexicalized parsing on Negra; K¨ubler
et al. (2006) agreed with these results for Negra, but
suggests that they do not hold for T¨uBa-D/Z. We ex-
tend these results by examining a variety of com-
binations of Markovization parameters for all three
corpora (T¨uBa-D/Z, Tiger, and Negra) in table 2. No
results presented here do include grammatical func-
tions; we present results on the interaction between
these functions and Markovization in section 4.
For T¨uBa-D/Z, we see that adding vertical
Markovization provides a substantial performance
gain of about 2% (vertical Markovization = 2) for
all levels of horizontal Markovization; increasing
vertical Markovization improves performance only
slightly further. Decreasing horizontal Markoviza-
tion from the default of infinity for a standard
PCFG also provides marginal gains, and decreases
the number of rules learned by the parser, cre-
ating a more compact grammar. The results of
Markovization on the Tiger and Negra corpora il-
lustrate the problems of a large grammar. While a
modest improvement is found by using parent anno-
tation (vertical Markovization = 2) when horizontal
Markovization is small, increasing either horizontal
or vertical Markovization past this point decreases
performance due to sparsity. Thus, while the gen-
eral results concerning Markovization from English
hold, the size of performance increase is affected ap-
preciably by the annotation strategy.
In table 3, we show a subset of the results of var-
ious Markovization parameters when gold part-of-
speech tags are used, focusing on models that per-
formed well without gold tags and that produce rel-
atively compact grammars. Gold tags provide 2–3%
absolute improvement in F1 over tagging while pars-
ing; slightly greater improvements are seen when the
PCFG model is used individually (3–4% absolute
improvement), and absolute improvement does not
vary greatly between treebanks. These results are
</bodyText>
<page confidence="0.997865">
41
</page>
<table confidence="0.999451636363636">
T¨uBa-D/Z Tiger Negra
Horiz. Vertical Markov Order Vertical Markov Order Vertical Markov Order
Order 1 2 3 1 2 3 1 2 3
1 86.50 88.60 88.71 76.69 77.40 76.46 76.63 77.20 75.91
(+2.76) (+1.21) (+0.89) (+3.54) (+3.57) (+3.27) (+2.39) (+2.06) (+2.08)
2 86.55 88.61 88.84 75.91 75.30 74.20 76.39 75.39 73.77
(+2.63) (+1.22) (+0.90) (+3.22) (+3.09) (+3.10) (+3.40) (+2.20) (+2.16)
3 86.47 88.56 88.74 75.27 74.08 72.88 75.30 74.22 72.53
(+2.63) (+1.18) (+0.90) (+3.36) (+3.41) (+2.85) (+3.74) (+2.12) (+2.60)
00 86.04 88.41 88.67 74.44 73.26 71.96 74.48 73.50 71.84
(+2.17) (+1.07) (+0.91) (+3.10) (+3.02) (+2.51) (+3.31) (+1.97) (+3.02)
</table>
<tableCaption confidence="0.9954705">
Table 2: Factored parsing results for T¨uBa-D/Z, Tiger, and Negra when tagging is done by the parser. Numbers in
italics show difference between factored parser and PCFG, where improvements over the PCFG are positive.
</tableCaption>
<bodyText confidence="0.9993818">
comparable to Maier (2006), which found 3–6% im-
provement using an unlexicalized PCFG; these ab-
solute improvements hold despite the fact that the
Maier (2006) parser has results with 2–4% absolute
lower F1 than those in this paper.
</bodyText>
<sectionHeader confidence="0.981964" genericHeader="method">
4 Inclusion of Grammatical Functions
</sectionHeader>
<bodyText confidence="0.999990444444445">
In this section we examine how the addition of gram-
matical functions for training and evaluation affects
performance. As noted previously, we add gram-
matical functions simply by concatenating them to
the dependent phrasal categories and calling each
unique symbol a PCFG nonterminal; this is an ob-
vious way to adapt an existing PCFG parser, but not
a sophisticated model of grammatical functions. We
also present our shared task results (table 6).
</bodyText>
<subsectionHeader confidence="0.999135">
4.1 Effects on Evaluation
</subsectionHeader>
<bodyText confidence="0.999321785714286">
As shown in table 4, the inclusion of grammati-
cal functions decreases performance by 10–15% for
both treebanks. This is partially due to the increase
in grammar size, creating less supporting evidence
for each rule, and the fact that the parser must now
discriminate amongst more categories. The larger
grammar is particularly problematic for Tiger due to
its flat annotation style. Adding gold tags (table 5)
increases performace by 2–3%, a similar gain to that
for the parsers without grammatical functions. We
also see that lexicalization provides smaller gains
when grammatical functions are included; we dis-
cuss this further in section 5. Finally, especially for
the Tiger corpus, vertical Markovization diminishes
</bodyText>
<table confidence="0.999870833333333">
T¨uBa-D/Z Vertical Markov Order
Horizontal Order 1 2
1 89.66 91.69
(+1.82) (+0.54)
2 89.72 91.71
(+1.56) (+0.43)
00 89.34 91.43
(+1.39) (+0.29)
Tiger Vertical Markov Order
Horizontal Order 1 2
1 79.39 79.67
(+2.83) (+2.53)
2 78.60 77.40
(+2.74) (+2.22)
00 76.65 75.29
(+2.50) (+1.94)
Negra Vertical Markov Order
Horizontal Order 1 2
1 78.80 79.51
(+2.39) (+1.55)
2 77.92 77.43
(+2.15) (+1.81)
00 74.44 73.26
(+3.10) (+3.02)
</table>
<tableCaption confidence="0.9886136">
Table 3: Factored parsing results for T¨uBa-D/Z, Tiger,
and Negra when gold tags are provided as input to the
parser. Numbers in italics show difference between fac-
tored parser and PCFG, where improvements over the
PCFG are positive.
</tableCaption>
<page confidence="0.958333">
42
</page>
<table confidence="0.970953333333333">
TueBa-D/Z Tiger
Horiz. Vertical Vertical
Order 1 2 1 2
1 75.97 77.21 60.48 58.00
(+2.69) (+1.49) (+2.69) (+2.24)
2 76.96 53.68
(+1.44) (+2.22)
∞ 75.24 76.66 55.36 50.94
(+2.18) (+1.22) (+2.50) (+1.94)
</table>
<tableCaption confidence="0.9274654">
Table 4: Results for T¨uBa-D/Z and Tiger when gram-
matical functions are included and tagging is done by
the parser. Numbers in italics show difference between
factored parser and PCFG, where improvements over the
PCFG are positive.
</tableCaption>
<table confidence="0.988884666666667">
T¨uBa-D/Z Tiger
Horiz. Vertical Vertical
Order 1 2 1 2
1 78.91 80.64 67.72 64.93
(+1.60) (+0.81) (+1.16) (+0.77)
2 80.32 59.60
(+0.69) (+0.67)
∞ 78.38 80.01 60.36 56.77
(+1.33) (+0.59) (+0.89) (+0.18)
</table>
<tableCaption confidence="0.994193">
Table 5: Results for T¨uBa-D/Z and Tiger when gram-
matical functions are included and gold tags (including
grammatical functions) are given to the parser.
</tableCaption>
<table confidence="0.999675">
T¨uBa-D/Z Tiger
Petrov &amp; Klein 83.97 69.81
Rafferty &amp; Manning 79.24 59.44
Hall 75.37 65.18
Rafferty &amp; Manning -gf 73.36 49.03
</table>
<tableCaption confidence="0.97302025">
Table 6: Shared task results (F1) for T¨uBa-D/Z and Tiger
when grammatical functions are included and gold tags
are given to the parser. Gold tags include grammatical
functions except in the case of ”Rafferty &amp; Manning -gf”.
</tableCaption>
<bodyText confidence="0.999228666666667">
performance. Sparsity becomes too great of an is-
sue for increased vertical annotations to be effective:
the grammar grows from 11,170 rules with horizon-
tal Markovization = 1, vertical Markovization = 1
to 39,435 rules with horizontal Markovization = ∞,
vertical Markovization = 2.
</bodyText>
<equation confidence="0.972902928571428">
T¨uBa-D/Z Fact. Δ PCFG Δ
Configuration F1 F1
H = 1, V = 1 87.63 +1.63 85.32 +1.58
H = 1, V = 2 88.47 −0.13 87.31 −0.08
H = 2, V = 2 88.30 −0.31 87.13 −0.26
H = ∞, V = 1 87.23 +1.17 85.27 +1.40
H = ∞, V = 2 88.18 −0.23 87.09 −0.25
Tiger Fact. Δ PCFG Δ
Configuration F1 F1
H = 1, V = 1 72.09 −4.60 69.09 −4.06
H = 1, V = 2 69.25 −8.15 67.24 −6.59
H = 2, V = 2 66.08 −9.22 64.42 −7.79
H = ∞, V = 1 67.58 −9.07 64.85 −6.49
H = ∞, V = 2 63.54 −11.75 62.21 −8.03
</equation>
<tableCaption confidence="0.936918">
Table 7: Effect of adding grammatical functions infor-
mation to the training data only. The difference (Δ) is
from a parser with same Markovization parameters but
not trained with grammatical functions.
</tableCaption>
<subsectionHeader confidence="0.990602">
4.2 Effects on Training Only
</subsectionHeader>
<bodyText confidence="0.999986423076923">
While training and testing with grammatical func-
tions significantly reduces our performance, this
does not necessarily mean that we cannot benefit
from grammatical functions. We explored whether
training with grammatical functions could improve
the parser’s test time performance on syntactic cat-
egories (ignoring grammatical functions), hypothe-
sizing that the functions could provide additional in-
formation for disambiguating which rule should be
applied. This test also provides evidence of whether
decreased performance with grammatical functions
is due to sparseness caused by the large grammar
or simply that more categorization needs to be done
when grammatical functions are included.
We found, as shown in table 7, that grammatical
functions provide limited gains for basic categories
but have no extra utility once vertical Markoviza-
tion is added. These results suggest that adding
grammatical functions is not only problematic due to
increased categorization but because of sparseness
(this task has the same categorization demands as
parsing without grammatical functions considered
in section 3). The Stanford Parser was initially de-
signed under the assumption of a small phrasal cat-
egory set, and makes no attempts to smooth gram-
mar rule probabilities (smoothing only probabilities
</bodyText>
<page confidence="0.999508">
43
</page>
<bodyText confidence="0.999937">
of words having a certain tag and probabilities of de-
pendencies). While this approach is in general not
optimal when many category splits are used inside
the parser – smoothing helps, cf. Petrov et al. (2006)
– it becomes untenable as the category set grows
large, multi-faceted, and sparse. This is particularly
evident given the results in table 7 that show the pre-
cipitous decline in F1 on the Tiger corpus, where
the general problems are exacerbated by the flatter
annotation style of Tiger.
</bodyText>
<sectionHeader confidence="0.995886" genericHeader="method">
5 Lexicalization
</sectionHeader>
<bodyText confidence="0.9999434">
In the tables in section 3, we showed the utility
of lexicalization for German parsing when gram-
matical functions are not required. This contrasts
strongly with the results of (Dubey and Keller, 2003;
Dubey, 2004) where no performance increases (in-
deed, performance decreases) are reported from lex-
icalization. Lexicalization shows fairly consistent
2–3% gains on the Negra and Tiger treebanks. As
the number of tags increases, however, such as when
grammatical functions are included, gains from lex-
icalization are limited due to sparseness. While use-
ful category splits lessen the need for lexicaliza-
tion, we think the diminishing gain is primarily due
to problems resulting from the unsmoothed PCFG
model. As the grammar becomes sparser, there are
limited opportunities for the lexical dependencies
to correct the output of the PCFG grammar under
the factored parsing model of Klein and Manning
(2003b). Indeed, as shown in table 8, the grammar
becomes sufficiently sparse that for many sentences
there is no tree on which the PCFG and dependency
grammar can agree, and the parser falls back to sim-
ply returning the best PCFG parse. This falloff, in
addition to overall issues of sparsity, helps explain
the drop in performance with the addition of gram-
matical functions: our possible gain from lexicalized
parsing is decreased by the increasing rate of fail-
ure for the factored parser. Thus, for future German
work to gain from lexicalization, it may be necessary
to explore smoothing the grammar or working with
a diminished tagset without grammatical functions.
Lexicalized parsing focuses on identifying depen-
dencies. As recognized by Collins (2003), identi-
fying dependencies between words allows for bet-
ter evaluation of attachment accuracy, diminishing
</bodyText>
<table confidence="0.999535">
Total Parseable
Dataset Sent. w.o. GFs with GFs
T¨uBa-D/Z 2611 2610 2197
Tiger 2535 2534 1592
</table>
<tableCaption confidence="0.991185">
Table 8: Number of sentences parseable by the factored
lexicalized parser. If the factored model fails to return
a parse, the parser returns the best PCFG parse, so the
parser maintains 100% coverage.
</tableCaption>
<table confidence="0.9994774">
T¨uBa-D/Z Tiger
Gold Tags 91.00 90.21
Auto. Tags 86.90 83.39
Gold Tags -gf 89.89 88.97
Auto. Tags -gf 86.89 85.86
</table>
<tableCaption confidence="0.9895198">
Table 9: Performance (F1) on identifying dependencies
in T¨uBa-D/Z and Tiger. Tags were either provided (“Gold
Tags”) or generated during parsing (“Auto. Tags”); gram-
matical functions were used for the first two results and
omitted for the final two (“-gf”).
</tableCaption>
<bodyText confidence="0.999927740740741">
spurious effects on labeled bracketing F1 of differ-
ent annotation schemes. In particular, Rehbein and
van Genabith (2007) correctly emphasize how F1
scores are very dependent on the amount of branch-
ing structure in a treebank, and are hence not validly
comparable across annotation styles. We evaluate
performance on identifying unlabeled dependencies
between heads and modifiers, extracting dependen-
cies automatically from the parse trees. Most heads
in the T¨uBa-D/Z and Tiger treebanks are marked,
and we use marked heads when possible for train-
ing and evaluation. When heads were not marked,
we used heuristic rules to identify the likely head.
Broadly consistent with the results of Rehbein and
van Genabith (2007), Table 9 shows that the dis-
parity in performance between T¨uBa-D/Z and Tiger
is much smaller when measuring dependency accu-
racy rather than labeled bracketing F1, especially
when using gold tags. These results also reverse the
trend in our other results that adding grammatical
functions greatly reduces F1. While F1 decreases
or remains constant when grammatical functions are
used with automatic tags, probably reflecting a de-
crease in accuracy on tags when using grammatical
functions, they increase F1 given gold tags. These
results suggest both that useful information may be
gained from grammatical functions and that the dif-
</bodyText>
<page confidence="0.998034">
44
</page>
<bodyText confidence="0.999367">
ferences between the annotation schemes of T¨uBa-
D/Z and Tiger may not cause as large a fundamen-
tal difference in parser performance as suggested in
K¨ubler et al. (2006).
</bodyText>
<sectionHeader confidence="0.99664" genericHeader="method">
6 Feature Splits
</sectionHeader>
<bodyText confidence="0.999997076923077">
Another technique shown to improve accuracy in
English parsing is state splits (Klein and Manning,
2003a). We experimented with such splits in an
attempt to show similar utility for German. How-
ever, despite trying a number of splits that leveraged
observations of useful splits for English as well as
information from grammatical functions, we were
unable to find any splits that caused significant im-
provement for German parsing performance. Some-
what more positive results are reported by Schiehlen
(2004) – in particular, his relative clause marking
adds significantly to performance – although many
of the other features he explores also yield little.
</bodyText>
<sectionHeader confidence="0.982188" genericHeader="conclusions">
7 Errors by Category
</sectionHeader>
<bodyText confidence="0.999965">
In this section, we examine which categories have
the most parsing errors and possible reasons for
these biases. Two types of error patterns are con-
sidered: errors on particularly salient grammatical
functions and overall category errors.
</bodyText>
<subsectionHeader confidence="0.987195">
7.1 Grammatical Function Errors
</subsectionHeader>
<bodyText confidence="0.999980212121212">
A subset of grammatical functions was recognized
by K¨ubler et al. (2006) as particularly important for
using parsing results, so we investigated training
and testing with the inclusion of these grammatical
functions but without any others. These functions
were the subject, dative object, and accusative object
functions. We found that the three categories had
distinctively different patterns of errors, although we
unfortunately still do not achieve particularly high
F1 for any of the individual pairings of node label
and grammatical function. Note that this analysis
differs from that of K¨ubler et al. (2006) due to our
analysis of the accuracy of node labels and gram-
matical functions, rather than only performance on
identifying these three grammatical functions (with-
out regards to the correctness of the original node
label). Overall, dative objects occur much less fre-
quently than either of the other two types, and ac-
cusative objects occur less frequently than subjects.
Consistent with sparsity causing degradations in per-
formance, for both Tiger and T¨uBa-D/Z, we show
the best performance on subjects, followed by ac-
cusative objects and then dative objects. For all cat-
egories, we find that these functions occur most fre-
quently with noun phrases, and we achieve higher
performance when pairing tthem with a noun phrase
than with any other basic category. While K¨ubler
et al. (2006) suggests these functions are particu-
larly important for parsing, our low performance on
dative objects (F1 between 0.00 and 0.06) may not
matter a great deal given that dative objects consist
of only 0.42% of development set nodes in T¨uBa-
D/Z and 0.76% of such nodes in Tiger.
</bodyText>
<subsectionHeader confidence="0.992147">
7.2 Overall Errors
</subsectionHeader>
<bodyText confidence="0.99999596875">
One limiting factor for overall parsing accuracy is
roughly defined by the number of local (one-level)
trees in the test set that are present in the training set.
While changes such as Markovization may allow
rules to be learned that do not correspond directly to
such local trees, it is unlikely that many such rules
will be created. Thus, if a local tree in the test set
is not represented in the training set, it is unlikely
we will be able to correctly parse this sentence. The
number of such local trees and the amount of test set
coverage they provide varies widely between T¨uBa-
D/Z and Tiger. Without grammatical functions, the
training set for T¨uBa-D/Z contains 4,532 unique lo-
cal trees, whereas the training set for Tiger con-
tains 20,957; both have 20,894 complete trees. Lo-
cal trees from the training set represent 79.6% of
the unique local trees in the development set for
T¨uBa-D/Z, whereas they represent 61.8% of unique
local trees in Tiger’s development set. This trans-
lates to 99.3% of total local trees in the develop-
ment set represented in the training set for T¨uBa-
D/Z versus 92.3% for Tiger. With grammatical func-
tions, the number of unique local trees increases for
both T¨uBa-D/Z and Tiger (10,464 and 32,614 trees
in training, respectively), and total coverage in the
development sets drop to 98.6% (T¨uBa-D/Z) and
87.7% (Tiger). Part of the reason for this decrease
in coverage with the addition of grammatical func-
tions, and the disparity between corpora, is a large
increase in the number of possible categories for
each node: from 26 to 139 categories for T¨uBa-D/Z
and from 24 to 192 categories for Tiger.
</bodyText>
<page confidence="0.999055">
45
</page>
<sectionHeader confidence="0.998341" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99992174">
Noam Chomsky. 1965. Aspects of the Theory of Syntax.
MIT Press, Cambridge, MA.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. In Computational Lin-
guistics, pages 589–638.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In 5th
International Conference on Language Resources and
Evaluation (LREC 2006), pages 449–454.
Amit Dubey and Frank Keller. 2003. Probabilistic pars-
ing for German using sister-head dependencies. In
ACL 41, pages 96–103.
Amit Dubey. 2004. Statistical Parsing for German:
Modeling Syntactic Properties and Annotation Differ-
ences. Ph.D. thesis, Universitaet des Saarlandes.
Amit Dubey. 2005. What to do when lexicalization fails:
parsing German with suffix analysis and smoothing.
In ACL 43, pages 314–21.
Dan Klein and Christopher D. Manning. 2003a. Accu-
rate unlexicalized parsing. In ACL 41, pages 423–430.
Dan Klein and Christopher D. Manning. 2003b. Fast
exact inference with a factored model for natural lan-
guage parsing. Advances in Neural Information Pro-
cessing Systems, 15:3–10.
Sandra K¨ubler, Erward W. Hinrichs, and Wolfgang
Maier˙ 2006. Is it really that difficult to parse German?
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing.
Sandra K¨ubler. 2005. How do treebank annotation
schemes influence parsing results? Or how not to com-
pare apples and oranges. In Proceedings of RANLP
2005.
Wolfgang Maier. 2006. Annotation schemes and their in-
uence on parsing results. In Proceedings of the COL-
ING/ACL 2006 Student Research Workshop, pages 19–
24.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In ACL 44, pages 433–440.
Ines Rehbein and Josef van Genabith. 2007. Treebank
annotation schemes and parser evaluation for German.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 630–639.
Michael Schiehlen. 2004. Annotation strategies for
probabilistic parsing in German. In Proceedings of the
20th International Conference on Computational Lin-
guistics, pages 390–96.
</reference>
<page confidence="0.999612">
46
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.601153">
<title confidence="0.999899">Parsing Three German Treebanks: Lexicalized and Unlexicalized Baselines</title>
<author confidence="0.998443">Anna N Rafferty</author>
<author confidence="0.998443">D Christopher</author>
<affiliation confidence="0.82125">Computer Science Stanford</affiliation>
<address confidence="0.908855">Stanford, CA</address>
<abstract confidence="0.99923055">Previous work on German parsing has provided confusing and conflicting results concerning the difficulty of the task and whether techniques that are useful for English, such as lexicalization, are effective for German. This paper aims to provide some understanding and solid baseline numbers for the task. We examine the performance of three techniques on three treebanks (Negra, Tiger, and T¨uBa-D/Z): (i) Markovization, (ii) lexicalization, and (iii) state splitting. We additionally explore parsing with the inclusion of grammatical function information. Explicit grammatical functions are important to German language understanding, but they are numerous, and naively incorporating them into a parser which assumes a small phrasal category inventory causes large performance reductions due to increasing sparsity.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Aspects of the Theory of Syntax.</title>
<date>1965</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2659" citStr="Chomsky, 1965" startWordPosition="381" endWordPosition="382">vestigated in other work (Schiehlen, 2004; Dubey, 2004; Dubey, 2005), but, we hope that by consolidating, replicating, improving, and clarifying previous results we can contribute to the re-evaluation of German probabilistic parsing after a somewhat confusing start to initial literature in this area. One feature of German that differs markedly from English is substantial free word order. This requires the marking of grammatical functions on phrases to indicate their syntactic function in sentences (subject, object, etc.), whereas for English these functions can be derived from configurations (Chomsky, 1965; de Marneffe et al., 2006). While some similar functions are present in English treebanks, they are used more frequently in German treebanks and many more unique functions and category-function pairings exist. Because of the relatively free word ordering in German, the usefulness of parses is substantially increased by generating them with this information. We demonstrate the difficulties introduced by naively concatenating these functions to categories and how this treatment interacts with the other parsing techniques. There are several avenues for improving this situation in future work. Th</context>
</contexts>
<marker>Chomsky, 1965</marker>
<rawString>Noam Chomsky. 1965. Aspects of the Theory of Syntax. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<booktitle>In Computational Linguistics,</booktitle>
<pages>589--638</pages>
<contexts>
<context position="16502" citStr="Collins (2003)" startWordPosition="2591" endWordPosition="2592">endency grammar can agree, and the parser falls back to simply returning the best PCFG parse. This falloff, in addition to overall issues of sparsity, helps explain the drop in performance with the addition of grammatical functions: our possible gain from lexicalized parsing is decreased by the increasing rate of failure for the factored parser. Thus, for future German work to gain from lexicalization, it may be necessary to explore smoothing the grammar or working with a diminished tagset without grammatical functions. Lexicalized parsing focuses on identifying dependencies. As recognized by Collins (2003), identifying dependencies between words allows for better evaluation of attachment accuracy, diminishing Total Parseable Dataset Sent. w.o. GFs with GFs T¨uBa-D/Z 2611 2610 2197 Tiger 2535 2534 1592 Table 8: Number of sentences parseable by the factored lexicalized parser. If the factored model fails to return a parse, the parser returns the best PCFG parse, so the parser maintains 100% coverage. T¨uBa-D/Z Tiger Gold Tags 91.00 90.21 Auto. Tags 86.90 83.39 Gold Tags -gf 89.89 88.97 Auto. Tags -gf 86.89 85.86 Table 9: Performance (F1) on identifying dependencies in T¨uBa-D/Z and Tiger. Tags we</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-driven statistical models for natural language parsing. In Computational Linguistics, pages 589–638.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In 5th International Conference on Language Resources and Evaluation (LREC</booktitle>
<pages>449--454</pages>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In 5th International Conference on Language Resources and Evaluation (LREC 2006), pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Dubey</author>
<author>Frank Keller</author>
</authors>
<title>Probabilistic parsing for German using sister-head dependencies.</title>
<date>2003</date>
<booktitle>In ACL 41,</booktitle>
<pages>96--103</pages>
<contexts>
<context position="1240" citStr="Dubey and Keller, 2003" startWordPosition="171" endWordPosition="174">Ba-D/Z): (i) Markovization, (ii) lexicalization, and (iii) state splitting. We additionally explore parsing with the inclusion of grammatical function information. Explicit grammatical functions are important to German language understanding, but they are numerous, and naively incorporating them into a parser which assumes a small phrasal category inventory causes large performance reductions due to increasing sparsity. 1 Introduction Recent papers provide mixed evidence as to whether techniques that increase statistical parsing performance for English also improve German parsing performance (Dubey and Keller, 2003; K¨ubler et al., 2006). We provide a systematic exploration of this topic to shed light on what techniques might benefit German parsing and show general trends in the relative performance increases for each technique. While these results vary across treebanks, due to differences in annotation schemes as discussed by K¨ubler (2005), we also find similarities and provide explanations for the trend differences based on the annotation schemes. 40 We address three parsing techniques: (i) Markovization, (ii) lexicalization, and (iii) state splitting (i.e., subcategorization). These techniques are n</context>
<context position="5617" citStr="Dubey and Keller, 2003" startWordPosition="844" endWordPosition="847">empt to learn general rules for nodes with the same basic category but different functions. Clearly, more sophisticated methods of handling grammatical functions exist, but our focus is on providing baseline results that are easily replicable by others. We focus primarily on the T¨uBa-D/Z and Tiger corpora, training on the training sets for the ACL 2008 Workshop on Parsing German shared task and providing ablation results based on development set performance. Additionally, we show a limited number of results on the Negra corpus, using the standard training/development/test splits, defined in (Dubey and Keller, 2003). The sizes of these data sets are shown in table 1. 3 Markovization Previous work has shown that adding vertical Markovization ((grand-)parent annotation) and using horizontal Markovization can greatly improve English parsing performance (Klein and Manning, 2003a). Several papers have already reported partially corresponding results on German: Schiehlen (2004) and Dubey (2004) reported gains of several percent for unlexicalized parsing on Negra; K¨ubler et al. (2006) agreed with these results for Negra, but suggests that they do not hold for T¨uBa-D/Z. We extend these results by examining a v</context>
<context position="15045" citStr="Dubey and Keller, 2003" startWordPosition="2360" endWordPosition="2363">n general not optimal when many category splits are used inside the parser – smoothing helps, cf. Petrov et al. (2006) – it becomes untenable as the category set grows large, multi-faceted, and sparse. This is particularly evident given the results in table 7 that show the precipitous decline in F1 on the Tiger corpus, where the general problems are exacerbated by the flatter annotation style of Tiger. 5 Lexicalization In the tables in section 3, we showed the utility of lexicalization for German parsing when grammatical functions are not required. This contrasts strongly with the results of (Dubey and Keller, 2003; Dubey, 2004) where no performance increases (indeed, performance decreases) are reported from lexicalization. Lexicalization shows fairly consistent 2–3% gains on the Negra and Tiger treebanks. As the number of tags increases, however, such as when grammatical functions are included, gains from lexicalization are limited due to sparseness. While useful category splits lessen the need for lexicalization, we think the diminishing gain is primarily due to problems resulting from the unsmoothed PCFG model. As the grammar becomes sparser, there are limited opportunities for the lexical dependenci</context>
</contexts>
<marker>Dubey, Keller, 2003</marker>
<rawString>Amit Dubey and Frank Keller. 2003. Probabilistic parsing for German using sister-head dependencies. In ACL 41, pages 96–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Dubey</author>
</authors>
<title>Statistical Parsing for German: Modeling Syntactic Properties and Annotation Differences.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>Universitaet des Saarlandes.</institution>
<contexts>
<context position="2100" citStr="Dubey, 2004" startWordPosition="298" endWordPosition="299">s treebanks, due to differences in annotation schemes as discussed by K¨ubler (2005), we also find similarities and provide explanations for the trend differences based on the annotation schemes. 40 We address three parsing techniques: (i) Markovization, (ii) lexicalization, and (iii) state splitting (i.e., subcategorization). These techniques are not independent, and we thus examine how lexicalization and Markovization interact, since lexicalization for German has been the most contentious area in the literature. Many of these techniques have been investigated in other work (Schiehlen, 2004; Dubey, 2004; Dubey, 2005), but, we hope that by consolidating, replicating, improving, and clarifying previous results we can contribute to the re-evaluation of German probabilistic parsing after a somewhat confusing start to initial literature in this area. One feature of German that differs markedly from English is substantial free word order. This requires the marking of grammatical functions on phrases to indicate their syntactic function in sentences (subject, object, etc.), whereas for English these functions can be derived from configurations (Chomsky, 1965; de Marneffe et al., 2006). While some s</context>
<context position="5997" citStr="Dubey (2004)" startWordPosition="900" endWordPosition="901">ask and providing ablation results based on development set performance. Additionally, we show a limited number of results on the Negra corpus, using the standard training/development/test splits, defined in (Dubey and Keller, 2003). The sizes of these data sets are shown in table 1. 3 Markovization Previous work has shown that adding vertical Markovization ((grand-)parent annotation) and using horizontal Markovization can greatly improve English parsing performance (Klein and Manning, 2003a). Several papers have already reported partially corresponding results on German: Schiehlen (2004) and Dubey (2004) reported gains of several percent for unlexicalized parsing on Negra; K¨ubler et al. (2006) agreed with these results for Negra, but suggests that they do not hold for T¨uBa-D/Z. We extend these results by examining a variety of combinations of Markovization parameters for all three corpora (T¨uBa-D/Z, Tiger, and Negra) in table 2. No results presented here do include grammatical functions; we present results on the interaction between these functions and Markovization in section 4. For T¨uBa-D/Z, we see that adding vertical Markovization provides a substantial performance gain of about 2% (v</context>
<context position="15059" citStr="Dubey, 2004" startWordPosition="2364" endWordPosition="2365">en many category splits are used inside the parser – smoothing helps, cf. Petrov et al. (2006) – it becomes untenable as the category set grows large, multi-faceted, and sparse. This is particularly evident given the results in table 7 that show the precipitous decline in F1 on the Tiger corpus, where the general problems are exacerbated by the flatter annotation style of Tiger. 5 Lexicalization In the tables in section 3, we showed the utility of lexicalization for German parsing when grammatical functions are not required. This contrasts strongly with the results of (Dubey and Keller, 2003; Dubey, 2004) where no performance increases (indeed, performance decreases) are reported from lexicalization. Lexicalization shows fairly consistent 2–3% gains on the Negra and Tiger treebanks. As the number of tags increases, however, such as when grammatical functions are included, gains from lexicalization are limited due to sparseness. While useful category splits lessen the need for lexicalization, we think the diminishing gain is primarily due to problems resulting from the unsmoothed PCFG model. As the grammar becomes sparser, there are limited opportunities for the lexical dependencies to correct </context>
</contexts>
<marker>Dubey, 2004</marker>
<rawString>Amit Dubey. 2004. Statistical Parsing for German: Modeling Syntactic Properties and Annotation Differences. Ph.D. thesis, Universitaet des Saarlandes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Dubey</author>
</authors>
<title>What to do when lexicalization fails: parsing German with suffix analysis and smoothing.</title>
<date>2005</date>
<booktitle>In ACL 43,</booktitle>
<pages>314--21</pages>
<contexts>
<context position="2114" citStr="Dubey, 2005" startWordPosition="300" endWordPosition="301">due to differences in annotation schemes as discussed by K¨ubler (2005), we also find similarities and provide explanations for the trend differences based on the annotation schemes. 40 We address three parsing techniques: (i) Markovization, (ii) lexicalization, and (iii) state splitting (i.e., subcategorization). These techniques are not independent, and we thus examine how lexicalization and Markovization interact, since lexicalization for German has been the most contentious area in the literature. Many of these techniques have been investigated in other work (Schiehlen, 2004; Dubey, 2004; Dubey, 2005), but, we hope that by consolidating, replicating, improving, and clarifying previous results we can contribute to the re-evaluation of German probabilistic parsing after a somewhat confusing start to initial literature in this area. One feature of German that differs markedly from English is substantial free word order. This requires the marking of grammatical functions on phrases to indicate their syntactic function in sentences (subject, object, etc.), whereas for English these functions can be derived from configurations (Chomsky, 1965; de Marneffe et al., 2006). While some similar functio</context>
</contexts>
<marker>Dubey, 2005</marker>
<rawString>Amit Dubey. 2005. What to do when lexicalization fails: parsing German with suffix analysis and smoothing. In ACL 43, pages 314–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In ACL 41,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="4443" citStr="Klein and Manning, 2003" startWordPosition="661" endWordPosition="664">l Tiger corpus is much larger. Our Negra results are on the test set. not use any morphological analyzer; this should be rectified in future work. A new parsing model could be written to treat separate grammatical functions for nodes as first class objects, rather than just concatenating phrasal categories and functions. Finally, assignment of grammatical functions could be left to a separate post-processing phase, which could exploit not only case information inside noun phrases but joint information across the subcategorization frames of predicates. 2 Methodology We use the Stanford Parser (Klein and Manning, 2003b) for all experiments. An advantage of this parser for baseline experiments is that it provides clean, simple implementations of component models, with many configuration options. We show results in most instances for evaluations both with and without grammatical functions and with and without gold tags. When training and parsing with the inclusion of grammatical functions, we treat each pairing of basic category and grammatical function as one new category. Rules are learned for each such category with a separate orthographic form, with no attempt to learn general rules for nodes with the sa</context>
<context position="5880" citStr="Klein and Manning, 2003" startWordPosition="882" endWordPosition="885">primarily on the T¨uBa-D/Z and Tiger corpora, training on the training sets for the ACL 2008 Workshop on Parsing German shared task and providing ablation results based on development set performance. Additionally, we show a limited number of results on the Negra corpus, using the standard training/development/test splits, defined in (Dubey and Keller, 2003). The sizes of these data sets are shown in table 1. 3 Markovization Previous work has shown that adding vertical Markovization ((grand-)parent annotation) and using horizontal Markovization can greatly improve English parsing performance (Klein and Manning, 2003a). Several papers have already reported partially corresponding results on German: Schiehlen (2004) and Dubey (2004) reported gains of several percent for unlexicalized parsing on Negra; K¨ubler et al. (2006) agreed with these results for Negra, but suggests that they do not hold for T¨uBa-D/Z. We extend these results by examining a variety of combinations of Markovization parameters for all three corpora (T¨uBa-D/Z, Tiger, and Negra) in table 2. No results presented here do include grammatical functions; we present results on the interaction between these functions and Markovization in secti</context>
<context position="15749" citStr="Klein and Manning (2003" startWordPosition="2468" endWordPosition="2471"> reported from lexicalization. Lexicalization shows fairly consistent 2–3% gains on the Negra and Tiger treebanks. As the number of tags increases, however, such as when grammatical functions are included, gains from lexicalization are limited due to sparseness. While useful category splits lessen the need for lexicalization, we think the diminishing gain is primarily due to problems resulting from the unsmoothed PCFG model. As the grammar becomes sparser, there are limited opportunities for the lexical dependencies to correct the output of the PCFG grammar under the factored parsing model of Klein and Manning (2003b). Indeed, as shown in table 8, the grammar becomes sufficiently sparse that for many sentences there is no tree on which the PCFG and dependency grammar can agree, and the parser falls back to simply returning the best PCFG parse. This falloff, in addition to overall issues of sparsity, helps explain the drop in performance with the addition of grammatical functions: our possible gain from lexicalized parsing is decreased by the increasing rate of failure for the factored parser. Thus, for future German work to gain from lexicalization, it may be necessary to explore smoothing the grammar or</context>
<context position="18921" citStr="Klein and Manning, 2003" startWordPosition="2970" endWordPosition="2973">s F1. While F1 decreases or remains constant when grammatical functions are used with automatic tags, probably reflecting a decrease in accuracy on tags when using grammatical functions, they increase F1 given gold tags. These results suggest both that useful information may be gained from grammatical functions and that the dif44 ferences between the annotation schemes of T¨uBaD/Z and Tiger may not cause as large a fundamental difference in parser performance as suggested in K¨ubler et al. (2006). 6 Feature Splits Another technique shown to improve accuracy in English parsing is state splits (Klein and Manning, 2003a). We experimented with such splits in an attempt to show similar utility for German. However, despite trying a number of splits that leveraged observations of useful splits for English as well as information from grammatical functions, we were unable to find any splits that caused significant improvement for German parsing performance. Somewhat more positive results are reported by Schiehlen (2004) – in particular, his relative clause marking adds significantly to performance – although many of the other features he explores also yield little. 7 Errors by Category In this section, we examine</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003a. Accurate unlexicalized parsing. In ACL 41, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>15--3</pages>
<contexts>
<context position="4443" citStr="Klein and Manning, 2003" startWordPosition="661" endWordPosition="664">l Tiger corpus is much larger. Our Negra results are on the test set. not use any morphological analyzer; this should be rectified in future work. A new parsing model could be written to treat separate grammatical functions for nodes as first class objects, rather than just concatenating phrasal categories and functions. Finally, assignment of grammatical functions could be left to a separate post-processing phase, which could exploit not only case information inside noun phrases but joint information across the subcategorization frames of predicates. 2 Methodology We use the Stanford Parser (Klein and Manning, 2003b) for all experiments. An advantage of this parser for baseline experiments is that it provides clean, simple implementations of component models, with many configuration options. We show results in most instances for evaluations both with and without grammatical functions and with and without gold tags. When training and parsing with the inclusion of grammatical functions, we treat each pairing of basic category and grammatical function as one new category. Rules are learned for each such category with a separate orthographic form, with no attempt to learn general rules for nodes with the sa</context>
<context position="5880" citStr="Klein and Manning, 2003" startWordPosition="882" endWordPosition="885">primarily on the T¨uBa-D/Z and Tiger corpora, training on the training sets for the ACL 2008 Workshop on Parsing German shared task and providing ablation results based on development set performance. Additionally, we show a limited number of results on the Negra corpus, using the standard training/development/test splits, defined in (Dubey and Keller, 2003). The sizes of these data sets are shown in table 1. 3 Markovization Previous work has shown that adding vertical Markovization ((grand-)parent annotation) and using horizontal Markovization can greatly improve English parsing performance (Klein and Manning, 2003a). Several papers have already reported partially corresponding results on German: Schiehlen (2004) and Dubey (2004) reported gains of several percent for unlexicalized parsing on Negra; K¨ubler et al. (2006) agreed with these results for Negra, but suggests that they do not hold for T¨uBa-D/Z. We extend these results by examining a variety of combinations of Markovization parameters for all three corpora (T¨uBa-D/Z, Tiger, and Negra) in table 2. No results presented here do include grammatical functions; we present results on the interaction between these functions and Markovization in secti</context>
<context position="15749" citStr="Klein and Manning (2003" startWordPosition="2468" endWordPosition="2471"> reported from lexicalization. Lexicalization shows fairly consistent 2–3% gains on the Negra and Tiger treebanks. As the number of tags increases, however, such as when grammatical functions are included, gains from lexicalization are limited due to sparseness. While useful category splits lessen the need for lexicalization, we think the diminishing gain is primarily due to problems resulting from the unsmoothed PCFG model. As the grammar becomes sparser, there are limited opportunities for the lexical dependencies to correct the output of the PCFG grammar under the factored parsing model of Klein and Manning (2003b). Indeed, as shown in table 8, the grammar becomes sufficiently sparse that for many sentences there is no tree on which the PCFG and dependency grammar can agree, and the parser falls back to simply returning the best PCFG parse. This falloff, in addition to overall issues of sparsity, helps explain the drop in performance with the addition of grammatical functions: our possible gain from lexicalized parsing is decreased by the increasing rate of failure for the factored parser. Thus, for future German work to gain from lexicalization, it may be necessary to explore smoothing the grammar or</context>
<context position="18921" citStr="Klein and Manning, 2003" startWordPosition="2970" endWordPosition="2973">s F1. While F1 decreases or remains constant when grammatical functions are used with automatic tags, probably reflecting a decrease in accuracy on tags when using grammatical functions, they increase F1 given gold tags. These results suggest both that useful information may be gained from grammatical functions and that the dif44 ferences between the annotation schemes of T¨uBaD/Z and Tiger may not cause as large a fundamental difference in parser performance as suggested in K¨ubler et al. (2006). 6 Feature Splits Another technique shown to improve accuracy in English parsing is state splits (Klein and Manning, 2003a). We experimented with such splits in an attempt to show similar utility for German. However, despite trying a number of splits that leveraged observations of useful splits for English as well as information from grammatical functions, we were unable to find any splits that caused significant improvement for German parsing performance. Somewhat more positive results are reported by Schiehlen (2004) – in particular, his relative clause marking adds significantly to performance – although many of the other features he explores also yield little. 7 Errors by Category In this section, we examine</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003b. Fast exact inference with a factored model for natural language parsing. Advances in Neural Information Processing Systems, 15:3–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra K¨ubler</author>
<author>Erward W Hinrichs</author>
<author>Wolfgang Maier˙</author>
</authors>
<title>Is it really that difficult to parse German?</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<marker>K¨ubler, Hinrichs, Maier˙, 2006</marker>
<rawString>Sandra K¨ubler, Erward W. Hinrichs, and Wolfgang Maier˙ 2006. Is it really that difficult to parse German? In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra K¨ubler</author>
</authors>
<title>How do treebank annotation schemes influence parsing results? Or how not to compare apples and oranges.</title>
<date>2005</date>
<booktitle>In Proceedings of RANLP</booktitle>
<marker>K¨ubler, 2005</marker>
<rawString>Sandra K¨ubler. 2005. How do treebank annotation schemes influence parsing results? Or how not to compare apples and oranges. In Proceedings of RANLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Maier</author>
</authors>
<title>Annotation schemes and their inuence on parsing results.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Student Research Workshop,</booktitle>
<pages>19--24</pages>
<contexts>
<context position="8830" citStr="Maier (2006)" startWordPosition="1338" endWordPosition="1339">84 75.91 75.30 74.20 76.39 75.39 73.77 (+2.63) (+1.22) (+0.90) (+3.22) (+3.09) (+3.10) (+3.40) (+2.20) (+2.16) 3 86.47 88.56 88.74 75.27 74.08 72.88 75.30 74.22 72.53 (+2.63) (+1.18) (+0.90) (+3.36) (+3.41) (+2.85) (+3.74) (+2.12) (+2.60) 00 86.04 88.41 88.67 74.44 73.26 71.96 74.48 73.50 71.84 (+2.17) (+1.07) (+0.91) (+3.10) (+3.02) (+2.51) (+3.31) (+1.97) (+3.02) Table 2: Factored parsing results for T¨uBa-D/Z, Tiger, and Negra when tagging is done by the parser. Numbers in italics show difference between factored parser and PCFG, where improvements over the PCFG are positive. comparable to Maier (2006), which found 3–6% improvement using an unlexicalized PCFG; these absolute improvements hold despite the fact that the Maier (2006) parser has results with 2–4% absolute lower F1 than those in this paper. 4 Inclusion of Grammatical Functions In this section we examine how the addition of grammatical functions for training and evaluation affects performance. As noted previously, we add grammatical functions simply by concatenating them to the dependent phrasal categories and calling each unique symbol a PCFG nonterminal; this is an obvious way to adapt an existing PCFG parser, but not a sophist</context>
</contexts>
<marker>Maier, 2006</marker>
<rawString>Wolfgang Maier. 2006. Annotation schemes and their inuence on parsing results. In Proceedings of the COLING/ACL 2006 Student Research Workshop, pages 19– 24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In ACL 44,</booktitle>
<pages>433--440</pages>
<contexts>
<context position="14541" citStr="Petrov et al. (2006)" startWordPosition="2277" endWordPosition="2280">ding grammatical functions is not only problematic due to increased categorization but because of sparseness (this task has the same categorization demands as parsing without grammatical functions considered in section 3). The Stanford Parser was initially designed under the assumption of a small phrasal category set, and makes no attempts to smooth grammar rule probabilities (smoothing only probabilities 43 of words having a certain tag and probabilities of dependencies). While this approach is in general not optimal when many category splits are used inside the parser – smoothing helps, cf. Petrov et al. (2006) – it becomes untenable as the category set grows large, multi-faceted, and sparse. This is particularly evident given the results in table 7 that show the precipitous decline in F1 on the Tiger corpus, where the general problems are exacerbated by the flatter annotation style of Tiger. 5 Lexicalization In the tables in section 3, we showed the utility of lexicalization for German parsing when grammatical functions are not required. This contrasts strongly with the results of (Dubey and Keller, 2003; Dubey, 2004) where no performance increases (indeed, performance decreases) are reported from </context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In ACL 44, pages 433–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ines Rehbein</author>
<author>Josef van Genabith</author>
</authors>
<title>Treebank annotation schemes and parser evaluation for German.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>630--639</pages>
<marker>Rehbein, van Genabith, 2007</marker>
<rawString>Ines Rehbein and Josef van Genabith. 2007. Treebank annotation schemes and parser evaluation for German. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 630–639.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Schiehlen</author>
</authors>
<title>Annotation strategies for probabilistic parsing in German.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>390--96</pages>
<contexts>
<context position="2087" citStr="Schiehlen, 2004" startWordPosition="296" endWordPosition="297">esults vary across treebanks, due to differences in annotation schemes as discussed by K¨ubler (2005), we also find similarities and provide explanations for the trend differences based on the annotation schemes. 40 We address three parsing techniques: (i) Markovization, (ii) lexicalization, and (iii) state splitting (i.e., subcategorization). These techniques are not independent, and we thus examine how lexicalization and Markovization interact, since lexicalization for German has been the most contentious area in the literature. Many of these techniques have been investigated in other work (Schiehlen, 2004; Dubey, 2004; Dubey, 2005), but, we hope that by consolidating, replicating, improving, and clarifying previous results we can contribute to the re-evaluation of German probabilistic parsing after a somewhat confusing start to initial literature in this area. One feature of German that differs markedly from English is substantial free word order. This requires the marking of grammatical functions on phrases to indicate their syntactic function in sentences (subject, object, etc.), whereas for English these functions can be derived from configurations (Chomsky, 1965; de Marneffe et al., 2006).</context>
<context position="5980" citStr="Schiehlen (2004)" startWordPosition="897" endWordPosition="898">rsing German shared task and providing ablation results based on development set performance. Additionally, we show a limited number of results on the Negra corpus, using the standard training/development/test splits, defined in (Dubey and Keller, 2003). The sizes of these data sets are shown in table 1. 3 Markovization Previous work has shown that adding vertical Markovization ((grand-)parent annotation) and using horizontal Markovization can greatly improve English parsing performance (Klein and Manning, 2003a). Several papers have already reported partially corresponding results on German: Schiehlen (2004) and Dubey (2004) reported gains of several percent for unlexicalized parsing on Negra; K¨ubler et al. (2006) agreed with these results for Negra, but suggests that they do not hold for T¨uBa-D/Z. We extend these results by examining a variety of combinations of Markovization parameters for all three corpora (T¨uBa-D/Z, Tiger, and Negra) in table 2. No results presented here do include grammatical functions; we present results on the interaction between these functions and Markovization in section 4. For T¨uBa-D/Z, we see that adding vertical Markovization provides a substantial performance ga</context>
<context position="19324" citStr="Schiehlen (2004)" startWordPosition="3035" endWordPosition="3036"> large a fundamental difference in parser performance as suggested in K¨ubler et al. (2006). 6 Feature Splits Another technique shown to improve accuracy in English parsing is state splits (Klein and Manning, 2003a). We experimented with such splits in an attempt to show similar utility for German. However, despite trying a number of splits that leveraged observations of useful splits for English as well as information from grammatical functions, we were unable to find any splits that caused significant improvement for German parsing performance. Somewhat more positive results are reported by Schiehlen (2004) – in particular, his relative clause marking adds significantly to performance – although many of the other features he explores also yield little. 7 Errors by Category In this section, we examine which categories have the most parsing errors and possible reasons for these biases. Two types of error patterns are considered: errors on particularly salient grammatical functions and overall category errors. 7.1 Grammatical Function Errors A subset of grammatical functions was recognized by K¨ubler et al. (2006) as particularly important for using parsing results, so we investigated training and </context>
</contexts>
<marker>Schiehlen, 2004</marker>
<rawString>Michael Schiehlen. 2004. Annotation strategies for probabilistic parsing in German. In Proceedings of the 20th International Conference on Computational Linguistics, pages 390–96.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>