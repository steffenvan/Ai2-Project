<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.972901">
Keyword-based Document Clustering
</title>
<author confidence="0.966879">
Seung-Shik Kang
</author>
<affiliation confidence="0.985169">
School of Computer Science, Kookmin University &amp; AITrc
</affiliation>
<address confidence="0.734773">
Chungnung-dong, Songbuk-gu, Seoul 136-702, Korea
</address>
<email confidence="0.95233">
sskang@kookmin.ac.kr
</email>
<sectionHeader confidence="0.357808" genericHeader="abstract">
Abstract1
</sectionHeader>
<bodyText confidence="0.987701352941176">
Document clustering is an aggregation of
related documents to a cluster based on the
similarity evaluation task between documents and
the representatives of clusters. Terms and their
discriminating features of terms are the clue to
the clustering and the discriminating features are
based on the term and document frequencies.
Feature selection method on the basis of
frequency statistics has a limitation to the
enhancement of the clustering algorithm because
it does not consider the contents of the cluster
objects. In this paper, we adopt a content-based
analytic approach to refine the similarity
computation and propose a keyword-based
clustering algorithm. Experimental results show
that content-based keyword weighting
outperforms frequency-based weighting method.
</bodyText>
<keyword confidence="0.929747">
Keywords: Document Clustering, Weighting
Scheme, Feature Selection
</keyword>
<sectionHeader confidence="0.999609" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999905526315789">
Document clustering is an aggregation of
documents by discriminating the relevant documents
from the irrelevant documents. The relevance
determination criteria of any two documents is a
similarity measure and the representatives of the
documents [1,2,3,4]. There are some similarity
measures such as Dice coefficient, Jaccard’s
coefficient, and cosine measure. These similarity
measures require that the documents are represented
in document vectors and the similarity of two
documents is calculated from the operation of
document vectors.
In general, the representatives of a document or a
cluster are document vectors that consist of &lt;term,
weight&gt; pairs and the document similarities are
determined by the terms and their weighting values
that are extracted from the document [7,9]. In the
previous studies on the document clustering, we
focused on the clustering algorithm, but the document
</bodyText>
<footnote confidence="0.433887666666667">
This work was supported by the Korea Science and Engineering
Foundation(KOSEF) through the Advanced Information
Technology Research Center(AITrc).
</footnote>
<bodyText confidence="0.997077785714286">
representation methodology was not the important
issue. Document vectors are simply constructed from
the term frequency (TF) and the inverted document
frequency (IDF). This representation of term weighting
method starts from the precondition that terms or
keywords representing the document are calculated by
TF-IDF. Term weighting method by TF-IDF is
generally used to construct a document vector, but we
cannot say that it is the best way of representing a
document. So, we suppose that there is a limitation to
improve the accuracy of the clustering system only by
improving the clustering algorithm without changing
the document/cluster representation method.
Also, document clustering requires a large amount of
memory spaces to keep the representatives of
documents/clusters and the similarity measures [6, 8,
10]. Given N documents to be clustered, N × N
similarity matrix is needed to store document similarity
measures. Also, the recursive iteration of similarity
calculation and reconstructing the representative of the
clusters need a huge number of computations.
In this paper, we propose a new clustering method
that is based on the keyword weighting approach. The
clustering algorithm starts from the seed documents
and the cluster is expanded by the keyword relationship.
The evolution of the cluster stops when no more
documents are added to the cluster and irrelevant
documents are removed from the cluster candidates.
</bodyText>
<sectionHeader confidence="0.907478" genericHeader="method">
2 Keyword-based Weighting Scheme
</sectionHeader>
<bodyText confidence="0.999649208333333">
In general, the construction of a document vector
depends on the term frequency and document
frequency. If keywords are determined by frequency
information of the document, we are apt to generate an
error that nouns are often used regardless of substance
of the document and the words of a high frequency are
extracted. The clustering method, which is focused on
similarity calculation considers the whole words except
stopwords as the representative of the document, and
constitutes a document vector that is calculated by the
weight value from the term frequency and document
frequency.
It is common that terms and their weight values
represent a document and &lt;term, weight&gt; pairs are the
unique elements of the document vector. When we
construct a document vector, term frequency and
document frequency are the most important features to
calculate the weight of a term. As for the terms and
their weight values, the weight value of a term means
a ranking score just as an importance factor to the
document. So, the term weighting can be seen as an
evaluation of the term as a keyword or a stopword to
the document. The weighting function w(t) from a
term to its weight is described in expression (1).
</bodyText>
<equation confidence="0.9332965">
w: term Æ weight (1)
w(t) = 0, if t is a stopword
1, if t is a keyword
a, otherwise 0 !5 a !5 1
</equation>
<bodyText confidence="0.9802095">
For the weighting scheme of terms, there are two
points of views as the representation of a document:
</bodyText>
<listItem confidence="0.908118">
(1) a discriminative value that distinguishes or
characterizes the document from others;
(2) an importance measure as a keyword or a
stopword.
</listItem>
<bodyText confidence="0.999559881355932">
Frequency-based term weighting (FBW) is a
statistical measure of terms in an inter-document
relationship. This weighting scheme is a very efficient
method for distinguishing and characterizing a
document from others, and it performs well for the
applications of document classification or clustering
in the information retrieval system. The only
evaluation measure to characterize a document in
frequency-based weighting scheme is a frequency
statistics, but term frequencies are not the best
measures to characterize the document by terms.
Another weighting scheme is a keyword-based term
weighting (KBW) method that is based on the
keyword importance factors in a document. It is an
analytic approach that analyses the contents of a
document to get a keyword list from the document.
The weight value of a word is calculated by the
importance factors as a keyword in a document. The
weight value of a word is a combination value of
keyword-weighting factors and the terms are ordered
by the keyword ranking score. The ranking scores in
this weighting scheme are calculated from the analysis
results of the document. Keyword-based term
weighting will be a good solution to overcome the
limitation of the frequency-based weighting scheme.
Keywords in a text are the terms that represent a
document and the candidate keywords are extracted
from the analysis results of the document. Keyword
ranking method depends on several factors of a term
such as the type of a document, the location and the
role of words in a sentence or a paragraph [5].
Thematic words of a document are representative
terms for the document. Thematic words are extracted
from a text by analysing the contents of the text, but
keyword extraction depends on the type of text.
Keywords are easily found in the title or an abstract in
a research paper that consists of a title, abstract, body,
experiment, and conclusion. Also, newspaper article
contains a keyword in the title or the first part of the
text. There are some clues of determining a keyword
and we may classify them as word level, sentence level,
paragraph level, and text level features. Word-level
features are the type of part-of-speech and case-role
information. The part-of-speech of Korean noun is
divided into common noun, compound noun, proper
noun, and numeral.
Syntactic or sentence-level features are the type of a
phrase or a clause, sentence location, and sentence type.
From the rhetoric word in a sentence, the importance of
the sentence is computed and the terms in a sentence
are affected by the type of a sentence. Also, the
weighting scheme of a term in the subjective clause is
not the equal to the same term that appeared in an
auxiliary clause or in a modifying clause. Basic term
weight is assigned by the type of a term and
recomputed by the features that it accompanies in the
text. That is, the weight value of a term is also
determined by the characteristics of word, sentence,
phrase, and clause where the term is extracted.
</bodyText>
<sectionHeader confidence="0.995008" genericHeader="method">
3 Keyword-based Document Clustering
</sectionHeader>
<bodyText confidence="0.96078475">
Keyword-based document clustering creates a
cluster by the keywords of each document. Suppose
that C is a set of clusters that is finally created by the
clustering algorithm. If n is the number of clusters in C,
</bodyText>
<equation confidence="0.8790165">
then C is a set of clusters C1 , C2 , , Cn .
C = {C1 , C2, , Cn }
Each cluster is initialised by document d that is
Ci
</equation>
<bodyText confidence="0.837753833333333">
not assigned to the existing clusters, and d is a seed
document of . When a new cluster is created,
Ci
expansion and reduction steps are repeated until it
reaches a stable state from the start state. In each
evolution steps for cluster , is the j-th state of
</bodyText>
<equation confidence="0.30412275">
Ci Ci
Ci.
Ci : the j-th state of a clusterCi
j
</equation>
<bodyText confidence="0.9997255">
The characteristic vector of a cluster is a set of
&lt;keyword, weight value&gt; pairs that represents the
</bodyText>
<equation confidence="0.8896845">
cluster. If is a keyword set of a document
KD D
j
and KC is a keyword set of cluster , then
Ci KC
i i
</equation>
<bodyText confidence="0.727969166666667">
is the j-th state of cluster . Figure 1 shows a
Ci
keyword-based clustering algorithm for the cluster .
Ci
Given the keyword sets for each document, cluster Ci
is created by the self-expanding algorithm.
</bodyText>
<subsectionHeader confidence="0.995282">
3.1 Cluster Initialisation
</subsectionHeader>
<bodyText confidence="0.883837">
The first step of the clustering algorithm is a creation
and initialisation of a new cluster. A document is
D
selected that does not belong to any other cluster, and it
is assigned to a new cluster that is an initial state
</bodyText>
<page confidence="0.461934">
Ci0
</page>
<bodyText confidence="0.936040444444445">
of cluster Ci . Ci0 = D } 0
{ appear each keyword of KC (the keyword extracted
i
from the seed document) to the clusterC that is the
1
i
next state of clusterC expands the cluster.
i
At this time, a document that is the first document
D
in the new cluster is called a seed document (or an
initialisation document). The seed document is
randomly selected among the documents that do not
belong to the clusters ~
C1 Ci − 1 . Keyword set
KD of a document D is a set of keywords k1, k2, ,
kn that are extracted from document D. The initial0
state of keyword set is initialised by .
</bodyText>
<equation confidence="0.989802">
K KD
Ci
K 0 =K
C i
D
KD = { k  |k is a keyword that is extracted from D }
</equation>
<figureCaption confidence="0.997961">
Figure 1. Keyword-based clustering algorithm
</figureCaption>
<subsectionHeader confidence="0.999528">
3.2 Expanding the Cluster
</subsectionHeader>
<bodyText confidence="0.847347214285714">
In the initialisation step of the cluster, a new
Ci0
cluster , an initial state of cluster C , is
i
established as the seed document, and the keyword set
The keyword set i of the cluster is used to
j Cij
KC
calculate the characteristic vector of each cluster. The
characteristic vector is constituted the weight value
calculated by term frequency (TF) and inverted
document frequency (IDF) of the keywords and this is
used to calculate the similarity measure between a
document and the cluster.
</bodyText>
<subsectionHeader confidence="0.999798">
3.3 Cluster Reduction and Completion
</subsectionHeader>
<bodyText confidence="0.999987666666667">
This step is to produce a complete cluster by
removing the documents that are not related to the
cluster. For the cluster C , documents of a low
</bodyText>
<equation confidence="0.7184405">
j
i
</equation>
<bodyText confidence="0.7534936">
similarity to the cluster are removed, that are not
related to a cluster C through the similarity
j
i
computation with the cluster . The result of cluster
</bodyText>
<equation confidence="0.5602405">
Cj
i
</equation>
<bodyText confidence="0.852919">
reduction is a filtering of documents that are not related
to the cluster, and the cluster is generated as a
Cij+ 1
next step of the cluster C . Ultimately, the cluster
</bodyText>
<equation confidence="0.5527705">
j
i
</equation>
<bodyText confidence="0.910867">
Ci is completed that consists of the related documents
after filtering the non-related documents. If a cluster
Ci is completed, the next cluster C is created
</bodyText>
<equation confidence="0.576172">
i + 1
</equation>
<bodyText confidence="0.711025142857143">
The cluster expansion is performed by the iteration
of keyword expansion and cluster expansion. More
documents are added to a cluster by the similarity
evaluation between the keyword set and the document.
If a new document is added to a cluster, then the
keywords in the added document are also added to the
keyword set of the cluster. The first expansion is
performed by the keyword set extracted from the seed
document. The second expansion is performed by new
keywords that are added to a cluster as a result of the
first expansion. And the i-th expansion is performed by
the (i-1)-th state of the keyword set.
The number of iterations is decided through the
experiment. When a cluster is expanded from to
</bodyText>
<figure confidence="0.899565461538462">
Ci 0
, the keyword set K 0 is also expanded to a new
Ci
1
keyword set Kthat appears in the total documents
1Ci
j j
of the cluster . The keyword set
Ci Kof is a
Ci
Ci
union of the total keyword sets of .
Ci j
</figure>
<figureCaption confidence="0.676085">
through the same process. Clustering is terminated if
all the documents are clustered or no more clusters are
created.
</figureCaption>
<figure confidence="0.967218777777778">
Ci0 = {D}
0
KCi = KD
k ∈ KD
for ∀k such that k ∈ KCi}
0
x
j 1
do
{
j = ∪K x , where D ∈ C j
K
C i D x i
Cij
for x begin
j
all D ∈ Ci
s = sim(Dx,KCj )
if( s &lt; threshold )
Ci = { Dx  |document Dx, where
1
Ci j+1
+
Cj j
1 = C + − D
1 { }
i i x
end for
j j + 1
} while( isDeleteDocument
Ci = Ci
j
() )
0
Ci 1 = {Dx |k∈ Dx , k ∈ K
Ci
Cij
∈
}
KC i = i∪K , where D
D x
x
1 iC
KC
i
</figure>
<footnote confidence="0.471994571428571">
document. In the expanding step of the cluster, the
cluster is expanded by adding more related documents
to the cluster, that include the keywords of the seed
document as the related documents of the seed
document. That is, adding the total documents that
0
is initialised by the key word set of the seed
</footnote>
<figureCaption confidence="0.8654535">
Figure 2. Overall architecture of keyword-
based clustering
</figureCaption>
<sectionHeader confidence="0.97981" genericHeader="method">
4 Design and Implementation
</sectionHeader>
<bodyText confidence="0.999971714285714">
The structure of a keyword-based clustering system
is shown in Figure 2. At first, keywords are extracted
from each input document and the weight values of
them are computed. Keywords and their scores are
stored in an inverted-file structure. Inverted-file
structure is a good for the expansion of the cluster and
adding the documents that includes a keyword to the
initial cluster. Figure 3 shows an example of the
operation of the document clustering system:
initialization, expansion, reduction, and completion of
clusters.
A new cluster is created and it includes a seed
document D. An initial set of keywords for the initial
state of a cluster is a keyword set KD of document D.
</bodyText>
<equation confidence="0.982236">
KD = { T1,D, T2,D, ..., Ti,D ..., Tn,D }
</equation>
<bodyText confidence="0.999652714285714">
For the terms in KD, documents that contain the same
term are added as a candidate document in the cluster.
Let the candidate documents be D1a, D1b, ... , D2a, D2b,
..., Dna, Dnb, .... then Dxy, is a document that is
expanded by term Tx. Keyword set of the cluster is
reconstructed by new set of documents.
In each step of the cluster expansion, the number of
keywords that are used for the expansion, and the
threshold of the weight value are decided through
experiments considering the maximum number of
document candidates in a cluster. Also, &lt;keyword,
weight&gt; pairs as an intermediate representative of the
cluster are much important factor of the cluster
expansion.
</bodyText>
<figureCaption confidence="0.9333735">
Figure 3. Example of keyword-based
clustering
</figureCaption>
<bodyText confidence="0.999909">
Now, a new keyword set that is limited to the cluster
candidates is constructed to get cluster documents.
Through the similarity calculation between the
document and the candidate centroid of the cluster,
relevant documents are selected to be a member of the
cluster. Through the iterations on keyword selection
and the reconstruction of the related documents, a new
cluster is completed that reaches in a stable status with
a strong relationship between keyword set and
document set.
</bodyText>
<sectionHeader confidence="0.97325" genericHeader="method">
5 The Experiments
</sectionHeader>
<bodyText confidence="0.999792384615384">
We implemented our clustering algorithm and
applied it to the clustering of similar documents. The
test documents for the experiment are collected from
the three days of newspaper articles. The total number
of articles is 383 and average 132 terms are extracted
from the articles. We performed a document clustering
by applying the difference criteria for term selection: 1)
frequency-based term selection; 2) percentage-based
keyword selection; and 3) keyword selection by
absolute number of keywords. Figure 4 shows the
result of similarity clustering by frequency-based term
selection. In this experiment, three types of term
selection are performed.
</bodyText>
<figure confidence="0.997675375">
Input Document
Keyword Extraction
Create Inverted-File
Reduce/Complete Cluster
Create a Cluster
Expand Cluster
Clusters
create inverted-file
create cluster
Init. Cluster Keyword set
D
T1,D,
T2,D,
Tn,D,
expand cluster
D1a, D1b,
D2a, D2b,
,
,
Dna, Dnb,
T 1,Da,T2,Da ••• , T n , Da
,
T1,Db , T ••• , D b
2,Db, , Tn
T1,Dz, T2,Dz,•••,Tn , Dz
complete cluster
result
D1A, D1B,
D2A, D2B,
,
,
DnA, DnB,
</figure>
<bodyText confidence="0.997200235294118">
- all terms are used to the clustering
- terms with more than frequency 2
- terms with more than frequency 3
In each experiment, we varied the similarity decision
ratio by the percentage of term matches. Figure 4
shows that term selection by frequency 2 or 3 is not
good for the representation of a document.
similarity decision and auxiliary keywords are also
needed for the accuracy. Another point in this
experiment is that 30%~60% keyword selection
resulted better than the selection of all terms.
We compared the F1-measure for the selection of
maximum keywords. All the experiments in Figure 6
resulted better than the experiment of using all the
terms in the document. Also, 30~70 keywords with
60%~70% match ratio resulted a good performance for
the comparison of document similarity.
</bodyText>
<figure confidence="0.994184307692308">
4 0 % 5 0 % 6 0 % 7 0 % 8 0 % 9 0 %
term match ratio
0.9
20
30
40
50
60
70
80
90
100
Al terms
f-measure
0.8
0.7
0.6
40% 50% 60% 70% 80% 90%
term match ratio
0 .7
0 .6
0 .9
f-measure
0 .8
Fre q. 2 F re q. 3
A ll te rm s
</figure>
<figureCaption confidence="0.991833">
Figure 4. Frequency-based keyword selection Figure 6. Keyword selection by maximum
</figureCaption>
<figure confidence="0.993386642857143">
0.9
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0.6
40% 50% 60% 70% 80% 90%
term match ratio
</figure>
<figureCaption confidence="0.9203015">
Figure 5. Percentage-based keyword
selection
</figureCaption>
<bodyText confidence="0.999093333333333">
In the experiment of percentage-based keyword
selection, terms of high weight values are selected for
the similarity calculation of the document. All the
curves in Figure 5 are a similar shape, except for 10 %
selection. In case of 10% selection, we guess that less
than 10% of keywords are not sufficient for the
</bodyText>
<sectionHeader confidence="0.999575" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999983333333333">
It is common that clustering algorithm is based on
the similarity computation by frequency-based
statistics to aggregate the related documents. This
metric is an important factor for term weighting. We
proposed a term weighting method that is based on the
keyword features and we tried to complement the
drawback of frequency-based metric. Based on the
keyword weighting scheme, documents of the same
keywords are grouped into a cluster candidate and a
new cluster is created by removing irrelevant
documents. We performed an experiment for the
clustering of similar documents and the results showed
that keyword-based weighting scheme is better than the
frequency-based method.
Our keyword-based algorithm is using 30%~60% of
terms for a clustering and the similarity matrix is not a
necessity that it will be good for the clustering of a
huge number of documents. We also expect that this
algorithm will be good for the topic tracking of special
events. In the experiment, we randomly selected a seed
document and it is a bit sensitive for the seed document.
So, our next research will be focused on minimizing
the effect of the seed document by getting
representative keywords before starting the clustering.
</bodyText>
<figure confidence="0.716431">
f-measure
0.8
0.7
</figure>
<sectionHeader confidence="0.92429" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999934612903226">
[1] Anderberg, M. R., “Cluster Analysis for Applications”,
New York: Academic, 1973.
[2] Can, F., and E. A. Ozkarahan, “Dynamic Cluster
Maintenance”, Information Processing &amp; Management,
Vol. 25, pp.275-291, 1989.
[3] Dubes, R., and A. K. Jain, “Clustering Methodologies in
Exploratory Data Analysis”, Advances in Computers,
Vol. 19, pp.113-227, 1980.
[4] Frakes, W. B. and R. Baeza-Yates, Information Retrieval,
Prentice Hall, 1992.
[5] Kang, S. S., H. G. Lee, S. H. Son, G. C. Hong, and B. J.
Moon, “Term Weighting Method by Postposition and
Compound Noun Recognition”, Proceedings of 13th
Conference on Korean Language Computing, pp.196-
198, 2001.
[6] Murtagh, F., “Complexities of Hierarchic Clustering
Algorithms: State of the Art”, Computational Statistics
Quarterly, Vol. 1, pp.101-113, 1984.
[7] Perry, S. A., and P. Willett, “A Review of the Use of
Inverted Files for Best Match Searching in Information
Retrieval Systems”, Journal of Information Science, Vol.
6, pp.59-66, 1983.
[8] Sibson, R. “SLINK: an Optimally Efficient Algorithm
for the Single-Link Cluster Method”, Computer Journal,
Vol. 16, pp.328-342, 1973.
[9] Willett, P., “Document Clustering Using an Inverted File
Approach”, Journal of Information Science, Vol. 2,
pp.223-231, 1980.
[10] Willett, P., “Recent Trends in Hierarchic Document
Clustering: A Critical Review”, Information Processing
and Management, Vol. 24, No.5, pp.577- 597, 1988.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.527045">
<title confidence="0.997159">Keyword-based Document Clustering</title>
<author confidence="0.948718">Seung-Shik</author>
<affiliation confidence="0.999283">School of Computer Science, Kookmin University &amp;</affiliation>
<address confidence="0.915037">Chungnung-dong, Songbuk-gu, Seoul 136-702,</address>
<email confidence="0.895836">sskang@kookmin.ac.kr</email>
<abstract confidence="0.998970705882353">Document clustering is an aggregation of related documents to a cluster based on the similarity evaluation task between documents and the representatives of clusters. Terms and their discriminating features of terms are the clue to the clustering and the discriminating features are based on the term and document frequencies. Feature selection method on the basis of frequency statistics has a limitation to the enhancement of the clustering algorithm because it does not consider the contents of the cluster objects. In this paper, we adopt a content-based analytic approach to refine the similarity computation and propose a keyword-based clustering algorithm. Experimental results show that content-based keyword outperforms frequency-based weighting method.</abstract>
<keyword confidence="0.789918">Document Clustering, Weighting</keyword>
<intro confidence="0.845031">Scheme, Feature Selection</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M R Anderberg</author>
</authors>
<title>Cluster Analysis for Applications”,</title>
<date>1973</date>
<publisher>Academic,</publisher>
<location>New York:</location>
<contexts>
<context position="1295" citStr="[1,2,3,4]" startWordPosition="173" endWordPosition="173"> cluster objects. In this paper, we adopt a content-based analytic approach to refine the similarity computation and propose a keyword-based clustering algorithm. Experimental results show that content-based keyword weighting outperforms frequency-based weighting method. Keywords: Document Clustering, Weighting Scheme, Feature Selection 1 Introduction Document clustering is an aggregation of documents by discriminating the relevant documents from the irrelevant documents. The relevance determination criteria of any two documents is a similarity measure and the representatives of the documents [1,2,3,4]. There are some similarity measures such as Dice coefficient, Jaccard’s coefficient, and cosine measure. These similarity measures require that the documents are represented in document vectors and the similarity of two documents is calculated from the operation of document vectors. In general, the representatives of a document or a cluster are document vectors that consist of &lt;term, weight&gt; pairs and the document similarities are determined by the terms and their weighting values that are extracted from the document [7,9]. In the previous studies on the document clustering, we focused on the</context>
</contexts>
<marker>[1]</marker>
<rawString>Anderberg, M. R., “Cluster Analysis for Applications”, New York: Academic, 1973.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Can</author>
<author>E A Ozkarahan</author>
</authors>
<title>Dynamic Cluster Maintenance”,</title>
<date>1989</date>
<journal>Information Processing &amp; Management,</journal>
<volume>25</volume>
<pages>275--291</pages>
<contexts>
<context position="1295" citStr="[1,2,3,4]" startWordPosition="173" endWordPosition="173"> cluster objects. In this paper, we adopt a content-based analytic approach to refine the similarity computation and propose a keyword-based clustering algorithm. Experimental results show that content-based keyword weighting outperforms frequency-based weighting method. Keywords: Document Clustering, Weighting Scheme, Feature Selection 1 Introduction Document clustering is an aggregation of documents by discriminating the relevant documents from the irrelevant documents. The relevance determination criteria of any two documents is a similarity measure and the representatives of the documents [1,2,3,4]. There are some similarity measures such as Dice coefficient, Jaccard’s coefficient, and cosine measure. These similarity measures require that the documents are represented in document vectors and the similarity of two documents is calculated from the operation of document vectors. In general, the representatives of a document or a cluster are document vectors that consist of &lt;term, weight&gt; pairs and the document similarities are determined by the terms and their weighting values that are extracted from the document [7,9]. In the previous studies on the document clustering, we focused on the</context>
</contexts>
<marker>[2]</marker>
<rawString>Can, F., and E. A. Ozkarahan, “Dynamic Cluster Maintenance”, Information Processing &amp; Management, Vol. 25, pp.275-291, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dubes</author>
<author>A K Jain</author>
</authors>
<title>Clustering Methodologies in Exploratory Data Analysis”,</title>
<date>1980</date>
<journal>Advances in Computers,</journal>
<volume>19</volume>
<pages>113--227</pages>
<contexts>
<context position="1295" citStr="[1,2,3,4]" startWordPosition="173" endWordPosition="173"> cluster objects. In this paper, we adopt a content-based analytic approach to refine the similarity computation and propose a keyword-based clustering algorithm. Experimental results show that content-based keyword weighting outperforms frequency-based weighting method. Keywords: Document Clustering, Weighting Scheme, Feature Selection 1 Introduction Document clustering is an aggregation of documents by discriminating the relevant documents from the irrelevant documents. The relevance determination criteria of any two documents is a similarity measure and the representatives of the documents [1,2,3,4]. There are some similarity measures such as Dice coefficient, Jaccard’s coefficient, and cosine measure. These similarity measures require that the documents are represented in document vectors and the similarity of two documents is calculated from the operation of document vectors. In general, the representatives of a document or a cluster are document vectors that consist of &lt;term, weight&gt; pairs and the document similarities are determined by the terms and their weighting values that are extracted from the document [7,9]. In the previous studies on the document clustering, we focused on the</context>
</contexts>
<marker>[3]</marker>
<rawString>Dubes, R., and A. K. Jain, “Clustering Methodologies in Exploratory Data Analysis”, Advances in Computers, Vol. 19, pp.113-227, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W B Frakes</author>
<author>R Baeza-Yates</author>
</authors>
<title>Information Retrieval,</title>
<date>1992</date>
<publisher>Prentice Hall,</publisher>
<contexts>
<context position="1295" citStr="[1,2,3,4]" startWordPosition="173" endWordPosition="173"> cluster objects. In this paper, we adopt a content-based analytic approach to refine the similarity computation and propose a keyword-based clustering algorithm. Experimental results show that content-based keyword weighting outperforms frequency-based weighting method. Keywords: Document Clustering, Weighting Scheme, Feature Selection 1 Introduction Document clustering is an aggregation of documents by discriminating the relevant documents from the irrelevant documents. The relevance determination criteria of any two documents is a similarity measure and the representatives of the documents [1,2,3,4]. There are some similarity measures such as Dice coefficient, Jaccard’s coefficient, and cosine measure. These similarity measures require that the documents are represented in document vectors and the similarity of two documents is calculated from the operation of document vectors. In general, the representatives of a document or a cluster are document vectors that consist of &lt;term, weight&gt; pairs and the document similarities are determined by the terms and their weighting values that are extracted from the document [7,9]. In the previous studies on the document clustering, we focused on the</context>
</contexts>
<marker>[4]</marker>
<rawString>Frakes, W. B. and R. Baeza-Yates, Information Retrieval, Prentice Hall, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S S Kang</author>
<author>H G Lee</author>
<author>S H Son</author>
<author>G C Hong</author>
<author>B J Moon</author>
</authors>
<title>Term Weighting Method by Postposition and Compound Noun Recognition”,</title>
<date>2001</date>
<booktitle>Proceedings of 13th Conference on Korean Language Computing,</booktitle>
<pages>196--198</pages>
<contexts>
<context position="6636" citStr="[5]" startWordPosition="1025" endWordPosition="1025">ord-weighting factors and the terms are ordered by the keyword ranking score. The ranking scores in this weighting scheme are calculated from the analysis results of the document. Keyword-based term weighting will be a good solution to overcome the limitation of the frequency-based weighting scheme. Keywords in a text are the terms that represent a document and the candidate keywords are extracted from the analysis results of the document. Keyword ranking method depends on several factors of a term such as the type of a document, the location and the role of words in a sentence or a paragraph [5]. Thematic words of a document are representative terms for the document. Thematic words are extracted from a text by analysing the contents of the text, but keyword extraction depends on the type of text. Keywords are easily found in the title or an abstract in a research paper that consists of a title, abstract, body, experiment, and conclusion. Also, newspaper article contains a keyword in the title or the first part of the text. There are some clues of determining a keyword and we may classify them as word level, sentence level, paragraph level, and text level features. Word-level features</context>
</contexts>
<marker>[5]</marker>
<rawString>Kang, S. S., H. G. Lee, S. H. Son, G. C. Hong, and B. J. Moon, “Term Weighting Method by Postposition and Compound Noun Recognition”, Proceedings of 13th Conference on Korean Language Computing, pp.196-198, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Murtagh</author>
</authors>
<title>Complexities of Hierarchic Clustering Algorithms: State of the Art”,</title>
<date>1984</date>
<journal>Computational Statistics Quarterly,</journal>
<volume>1</volume>
<pages>101--113</pages>
<contexts>
<context position="2901" citStr="[6, 8, 10]" startWordPosition="407" endWordPosition="409">hod starts from the precondition that terms or keywords representing the document are calculated by TF-IDF. Term weighting method by TF-IDF is generally used to construct a document vector, but we cannot say that it is the best way of representing a document. So, we suppose that there is a limitation to improve the accuracy of the clustering system only by improving the clustering algorithm without changing the document/cluster representation method. Also, document clustering requires a large amount of memory spaces to keep the representatives of documents/clusters and the similarity measures [6, 8, 10]. Given N documents to be clustered, N × N similarity matrix is needed to store document similarity measures. Also, the recursive iteration of similarity calculation and reconstructing the representative of the clusters need a huge number of computations. In this paper, we propose a new clustering method that is based on the keyword weighting approach. The clustering algorithm starts from the seed documents and the cluster is expanded by the keyword relationship. The evolution of the cluster stops when no more documents are added to the cluster and irrelevant documents are removed from the clu</context>
</contexts>
<marker>[6]</marker>
<rawString>Murtagh, F., “Complexities of Hierarchic Clustering Algorithms: State of the Art”, Computational Statistics Quarterly, Vol. 1, pp.101-113, 1984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S A Perry</author>
<author>P Willett</author>
</authors>
<title>A Review of the Use of Inverted Files for Best Match Searching in Information Retrieval Systems”,</title>
<date>1983</date>
<journal>Journal of Information Science,</journal>
<volume>6</volume>
<pages>59--66</pages>
<contexts>
<context position="1824" citStr="[7,9]" startWordPosition="252" endWordPosition="252">s is a similarity measure and the representatives of the documents [1,2,3,4]. There are some similarity measures such as Dice coefficient, Jaccard’s coefficient, and cosine measure. These similarity measures require that the documents are represented in document vectors and the similarity of two documents is calculated from the operation of document vectors. In general, the representatives of a document or a cluster are document vectors that consist of &lt;term, weight&gt; pairs and the document similarities are determined by the terms and their weighting values that are extracted from the document [7,9]. In the previous studies on the document clustering, we focused on the clustering algorithm, but the document This work was supported by the Korea Science and Engineering Foundation(KOSEF) through the Advanced Information Technology Research Center(AITrc). representation methodology was not the important issue. Document vectors are simply constructed from the term frequency (TF) and the inverted document frequency (IDF). This representation of term weighting method starts from the precondition that terms or keywords representing the document are calculated by TF-IDF. Term weighting method by </context>
</contexts>
<marker>[7]</marker>
<rawString>Perry, S. A., and P. Willett, “A Review of the Use of Inverted Files for Best Match Searching in Information Retrieval Systems”, Journal of Information Science, Vol. 6, pp.59-66, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sibson</author>
</authors>
<title>SLINK: an Optimally Efficient Algorithm for the Single-Link Cluster Method”,</title>
<date>1973</date>
<journal>Computer Journal,</journal>
<volume>16</volume>
<pages>328--342</pages>
<contexts>
<context position="2901" citStr="[6, 8, 10]" startWordPosition="407" endWordPosition="409">hod starts from the precondition that terms or keywords representing the document are calculated by TF-IDF. Term weighting method by TF-IDF is generally used to construct a document vector, but we cannot say that it is the best way of representing a document. So, we suppose that there is a limitation to improve the accuracy of the clustering system only by improving the clustering algorithm without changing the document/cluster representation method. Also, document clustering requires a large amount of memory spaces to keep the representatives of documents/clusters and the similarity measures [6, 8, 10]. Given N documents to be clustered, N × N similarity matrix is needed to store document similarity measures. Also, the recursive iteration of similarity calculation and reconstructing the representative of the clusters need a huge number of computations. In this paper, we propose a new clustering method that is based on the keyword weighting approach. The clustering algorithm starts from the seed documents and the cluster is expanded by the keyword relationship. The evolution of the cluster stops when no more documents are added to the cluster and irrelevant documents are removed from the clu</context>
</contexts>
<marker>[8]</marker>
<rawString>Sibson, R. “SLINK: an Optimally Efficient Algorithm for the Single-Link Cluster Method”, Computer Journal, Vol. 16, pp.328-342, 1973.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Willett</author>
</authors>
<title>Document Clustering Using an Inverted File Approach”,</title>
<date>1980</date>
<journal>Journal of Information Science,</journal>
<volume>2</volume>
<pages>223--231</pages>
<contexts>
<context position="1824" citStr="[7,9]" startWordPosition="252" endWordPosition="252">s is a similarity measure and the representatives of the documents [1,2,3,4]. There are some similarity measures such as Dice coefficient, Jaccard’s coefficient, and cosine measure. These similarity measures require that the documents are represented in document vectors and the similarity of two documents is calculated from the operation of document vectors. In general, the representatives of a document or a cluster are document vectors that consist of &lt;term, weight&gt; pairs and the document similarities are determined by the terms and their weighting values that are extracted from the document [7,9]. In the previous studies on the document clustering, we focused on the clustering algorithm, but the document This work was supported by the Korea Science and Engineering Foundation(KOSEF) through the Advanced Information Technology Research Center(AITrc). representation methodology was not the important issue. Document vectors are simply constructed from the term frequency (TF) and the inverted document frequency (IDF). This representation of term weighting method starts from the precondition that terms or keywords representing the document are calculated by TF-IDF. Term weighting method by </context>
</contexts>
<marker>[9]</marker>
<rawString>Willett, P., “Document Clustering Using an Inverted File Approach”, Journal of Information Science, Vol. 2, pp.223-231, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Willett</author>
</authors>
<title>Recent Trends in Hierarchic Document Clustering: A Critical Review”,</title>
<date>1988</date>
<journal>Information Processing and Management,</journal>
<volume>24</volume>
<pages>577--597</pages>
<contexts>
<context position="2901" citStr="[6, 8, 10]" startWordPosition="407" endWordPosition="409">hod starts from the precondition that terms or keywords representing the document are calculated by TF-IDF. Term weighting method by TF-IDF is generally used to construct a document vector, but we cannot say that it is the best way of representing a document. So, we suppose that there is a limitation to improve the accuracy of the clustering system only by improving the clustering algorithm without changing the document/cluster representation method. Also, document clustering requires a large amount of memory spaces to keep the representatives of documents/clusters and the similarity measures [6, 8, 10]. Given N documents to be clustered, N × N similarity matrix is needed to store document similarity measures. Also, the recursive iteration of similarity calculation and reconstructing the representative of the clusters need a huge number of computations. In this paper, we propose a new clustering method that is based on the keyword weighting approach. The clustering algorithm starts from the seed documents and the cluster is expanded by the keyword relationship. The evolution of the cluster stops when no more documents are added to the cluster and irrelevant documents are removed from the clu</context>
</contexts>
<marker>[10]</marker>
<rawString>Willett, P., “Recent Trends in Hierarchic Document Clustering: A Critical Review”, Information Processing and Management, Vol. 24, No.5, pp.577- 597, 1988.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>