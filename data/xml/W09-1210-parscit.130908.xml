<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000175">
<title confidence="0.960772">
Efficient Parsing of Syntactic and Semantic Dependency Structures
</title>
<author confidence="0.975978">
Bernd Bohnet
</author>
<affiliation confidence="0.962861">
International Computer Science Institute
</affiliation>
<address confidence="0.701066">
1947 Center Street
Berkeley 94704, California
</address>
<email confidence="0.999295">
bohnet@icsi.Berkeley.edu
</email>
<sectionHeader confidence="0.995647" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999558125">
In this paper, we describe our system for the
2009 CoNLL shared task for joint parsing of
syntactic and semantic dependency structures
of multiple languages. Our system combines
and implements efficient parsing techniques to
get a high accuracy as well as very good pars-
ing and training time. For the applications
of syntactic and semantic parsing, the pars-
ing time and memory footprint are very im-
portant. We think that also the development of
systems can profit from this since one can per-
form more experiments in the given time. For
the subtask of syntactic dependency parsing,
we could reach the second place with an ac-
curacy in average of 85.68 which is only 0.09
points behind the first ranked system. For this
task, our system has the highest accuracy for
English with 89.88, German with 87.48 and
the out-of-domain data in average with 78.79.
The semantic role labeler works not as well as
our parser and we reached therefore the fourth
place (ranked by the macro F1 score) in the
joint task for syntactic and semantic depen-
dency parsing.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999848">
Depedendency parsing and semantic role labeling
improved in the last years significantly. One of the
reasons are CoNLL shared tasks for syntactic de-
pendency parsing in the years 2006, 2007 (Buch-
holz and Marsi, 2006; Nivre et al., 2007) and the
CoNLL shared task for joint parsing of syntactic and
semantic dependencies in the year 2008 and of cause
this shared task in 2009, cf. (Surdeanu et al., 2008;
Hajiˇc et al., 2009). The CoNLL Shared Task 2009
is to parse syntactic and semantic dependencies of
seven languages. Therefore, training and develop-
ment data in form of annotated corpora for Cata-
lan, Chinese, Czech, English, German, Japanese and
Spanish is provided, cf. (Taul´e et al., 2008; Palmer
and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al.,
2008; Burchardt et al., 2006; Kawahara et al., 2002).
There are two main approaches to dependency
parsing: Maximum Spanning Tree (MST) based de-
pendency parsing and Transition based dependency
parsing, cf. (Eisner, 1996; Nivre et al., 2004; Mc-
Donald and Pereira, 2006). Our system uses the first
approach since we saw better chance to improve the
parsing speed and additionally, the MST had so far
slightly better parsing results. For the task of seman-
tic role labeling, we adopted a pipeline architecture
where we used for each step the same learning tech-
nique (SVM) since we opted for the possibility to
build a synchronous combined parser with one score
function.
</bodyText>
<sectionHeader confidence="0.975322" genericHeader="introduction">
2 Parsing Algorithm
</sectionHeader>
<bodyText confidence="0.999879">
We adopted the second order MST parsing algo-
rithm as outlined by Eisner (1996). This algorithm
has a higher accuracy compared to the first order
parsing algorithm since it considers also siblings and
grandchildren of a node. Eisner‘s second order ap-
proach can compute a projective dependency tree
within cubic time (O(n3)).
Both algorithms are bottom up parsing algorithms
based on dynamic programming similar to the CKY
chart parsing algorithm. The score for a dependency
tree is the score of all edge scores. The following
</bodyText>
<page confidence="0.998406">
67
</page>
<note confidence="0.7743955">
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 67–72,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.966121368421053">
equation describes this formally.
score(S, t) = E∀(i,j)∈E score(i, j)
The score of the sentence S and a tree t over S
is defined as the sum of all edge scores where the
words of S are w0...w1. The tree consists of set of
nodes N and set of edges E = (N x N). The word
indices (0..n) are the elements of the node set N.
The expression (i, j) E E denotes an edge which is
going from the node i to the node j.
The edge score (score(i, j)) is computed as the
scalar product of a feature vector representation of
each edge fS(i, j) with a weight vector ��
�� w where
i, j are the indices of the words in a sentence. The
feature vector fS might take not only into account
the words with indices i and j but also additional
values such as the words before and after the words
wi and wj. The following equation shows the score
function.
</bodyText>
<equation confidence="0.781747">
score(i, j) = fS(i, j) � ��
�� w
</equation>
<bodyText confidence="0.999623857142857">
Many systems encode the features as strings and
map the strings to a number. The number becomes
the index of the feature in the feature vector and
weight vector. In order to compute the weight vec-
tor, we reimplemented the support vector machine
MIRA which implements online Margin Infused Re-
laxed Algorithm, cf. (Crammer et al., 2003).
</bodyText>
<sectionHeader confidence="0.97108" genericHeader="method">
3 Labeled Dependency Parsing
</sectionHeader>
<bodyText confidence="0.999949129032258">
The second order parsing algorithm builds an un-
labeled dependency tree. However, all dependency
tree banks of the shared task provide trees with edge
labels. The following two approaches are common
to solve this problem. An additional algorithm la-
bels the edges or the parsing algorithm itself is ex-
tended and the labeling algorithm is integrated into
the parsing algorithm. McDonald et al. (2006) use
an additional algorithm. Their two stage model has
a good computational complexity since the label-
ing algorithm contributes again only a cubic time
complexity to the algorithm and keeps therefore the
joint algorithm still cubic. The algorithm selects
the highest scored label due to the score function
score(wi, label) + score(wj, label) and inserts the
highest scored label into a matrix. The scores are
also used in the parsing algorithms and added to
the edge scores which improves the overall pars-
ing results as well. In the first order parsing sce-
nario, this procedure is sufficient since no combi-
nation of edges are considered by the parsing algo-
rithm. However, in the second order parsing sce-
nario where more than one edge are considered by
the parsing algorithm, combinations of two edges
might be more accurate.
Johansson and Nugues (2008) combines the edge
labeling with the second order parsing algorithm.
This adds an additional loop over the edge labels.
The complexity is therefore O(n4). However, they
could show that a system can gain accuracy of about
2-4% which is a lot.
</bodyText>
<sectionHeader confidence="0.989416" genericHeader="method">
4 Non-Projective Dependency Parsing
</sectionHeader>
<bodyText confidence="0.999993096774194">
The dependency parser developed in the last years
use two different techniques for non-projective de-
pendency parsing.
Nivre and Nilsson (2005) uses tree rewriting
which is the most common technique. With this
technique, the training input to the parser is first pro-
jectivized by applying a minimal number of lifting
operations to the non-projective edges and encoding
information about these lifts in edge labels. After
these operations, the trees are projective and there-
fore a projective dependency parser can be applied.
During the training, the parser learns also to built
trees with the lifted edges and so indirect to built
non-projective dependency trees by applying the in-
verse operations to the lifting on the projective tree.
McDonald and Pereira (2006) developed a tech-
nique to rearrange edges in the tree in a postpro-
cessing step after the projective parsing has taken
place. Their Approximate Dependency Parsing Al-
gorithm searches first the highest scoring projective
parse tree and then it rearranges edges in the tree
until the rearrangements does not increase the score
for the tree anymore. This technique is computa-
tionally expensive for trees with a large number of
non-projective edges since it considers to re-attach
all edges to any other node until no higher scoring
trees can be found. Their argument for the algo-
rithm is that most edges in a tree even in language
with lot of non-projective sentences, the portion of
non-projective edges are still small and therefore by
starting with the highest scoring projective tree, typ-
</bodyText>
<page confidence="0.996702">
68
</page>
<bodyText confidence="0.99971675">
ically the highest scoring non-projective tree is only
a small number of transformations away.
Our experiments showed that with the non-
projective Approximate Dependency Parsing Algo-
rithm and a threshold for the improvment of score
higher than about 0.7, the parsing accuracy improves
even for English slightly. With a threshold of 1.1, we
got the highest improvements.
</bodyText>
<sectionHeader confidence="0.977377" genericHeader="method">
5 Learning Framework
</sectionHeader>
<bodyText confidence="0.970605333333333">
As learning technique, we use Margin Infused Re-
laxed Algorithm (MIRA) as developed by Crammer
et al. (2003) and applied to dependency parsing by
McDonald et al. (2005). The online Algorithm in
Figure 1 processes one training instance on each it-
eration, and updates the parameters accordingly.
</bodyText>
<equation confidence="0.910015666666666">
Algorithm 1: MIRA
τ = {Sx, Tx}Xx=1 // The set of training data consists
// of sentences and the corresponding dependency trees
V(0) = 0, v = 0
for n = 1 to N
for x = 1 to X
wi+1 = update wi according to instance (Sx, Tx)
v = v + wi+1
i = i + 1
end for
end for
w=v/(N*X)
</equation>
<bodyText confidence="0.98687047826087">
The inner loop iterates over all sentences x of the
training set while the outer loop repeats the train i
times. The algorithm returns an averaged weight
vector and uses an auxiliary weight vector v that ac-
cumulates the values of w after each iteration. At
the end, the algorithm computes the average of all
weight vectors by dividing it by the number of train-
ing iterations and sentences. This helps to avoid
overfitting, cf. (Collins, 2002).
The update function computes the update to the
weight vector wz during the training so that wrong
classified edges of the training instances are possibly
correctly classified. This is computed by increasing
the weight for the correct features and decreasing the
weight for wrong features of the vectors for the tree
of the training set f�x�� * wz and the vector for the
��
predicted dependency treef� x ′ * wz.
The update function tries to keep the change to
the parameter vector wz as small as possible for cor-
rectly classifying the current instance with a differ-
ence at least as large as the loss of the incorrect clas-
sifications.
</bodyText>
<sectionHeader confidence="0.982914" genericHeader="method">
6 Selected Parsing Features
</sectionHeader>
<bodyText confidence="0.9908282">
Table 1, 4 and 2 give an overview of the selected
features for our system. Similar to Johansson and
Nugues (2008), we add the edge labels to each fea-
tures. In the feature selection, we follow a bit more
McDonald and Pereira (2006) since we have in addi-
tion the lemmas, morphologic features and the dis-
tance between the word forms.
For the parsing and training speed, most impor-
tant is a fast feature extraction beside of a fast pars-
ing algorithm.
</bodyText>
<table confidence="0.9992635">
Standard Features
h-f/l h-f/l, d-pos
h-pos h-pos, d-f/l
d-f/l h-f/l, d-f/l
d-pos h-pos, d-pos
h-f/l,h-pos h-f/l, d-f/l, h-pos
d-f/l,d-pos h-f/l, d-f/l, d-pos
h-pos, d-pos, h-f/l
h-pos, d-pos, d-f/l
h-pos, d-pos, h-f/l, d-f/l
</table>
<tableCaption confidence="0.917302">
Table 1: Selected standard parsing features. h is the ab-
brevation for head, d for dependent, g for grandchild, and
s for sibling. Each feature contains also the edge label
which is not listed in order to save some space. Addi-
</tableCaption>
<bodyText confidence="0.716560888888889">
tional features are build by adding the direction and the
distance plus the direction. The direction is left if the de-
pendent is left of the head otherwise right. The distance
is the number of words between the head and the depen-
dent, if &lt;5, 6 if &gt;5 and 11 if &gt;10. ® means that an
additional feature is built with the previous part plus the
following part. f/l represent features that are built once
with the form and once with the lemma.
Selected morphologic parsing features.
</bodyText>
<construct confidence="0.397778333333333">
`d h-morpheme E head-morphologic-feature-set do
`d d-morpheme E dependent-morphologic-feature-set do
build-feautre: h-pos, d-pos, h-morpheme, d-morpheme
</construct>
<sectionHeader confidence="0.989673" genericHeader="method">
7 Implementation Aspects
</sectionHeader>
<bodyText confidence="0.994402">
In this section, we provide implementation details
considering improvements of the parsing and train-
ing time. The training of our system (parser) has
</bodyText>
<page confidence="0.998221">
69
</page>
<table confidence="0.9992401">
Linear Features Grandchild Features Sibling Features
h-pos, d-pos, h-pos + 1 h-pos, d-pos, g-pos, dir(h,d), dir(d,g) d-f/l, s-f/l ® dir(d,s) ®dist(d,s)
h-pos, d-pos, h-pos - 1 h-f/l, g-f/l, dir(h,d), dir(d,g) d-pos, s-f/l ® dir(d,s) ®dist(d,s)
h-pos, d-pos, d-pos + 1 d-f/l, g-f/l, dir(h,d), dir(d,g) d-pos, s-f/l ® dir(d,s)+ ® dist(d,s)
h-pos, d-pos, d-pos - 1 h-pos, g-f/l, dir(h,d), dir(d,g) d-pos, s-pos ®dir(d,s)®dist(d,s)
h-pos, d-pos, h-pos - 1, d-pos - 1 d-pos, g-f/l, dir(h,d), dir(d,g) h-pos, d-pos, s-pos, dir(h,d), dir(d,s) ®dist(h,s)
h-f/l, g-pos, dir(h,d), dir(d,g) h-f/l, s-f/l, dir(h,d), dir(d,s)®dist(h,s) h-pos, s-f/l, dir(h,d), dir(d,s)®dist(h,s)
d-f/l, g-pos, dir(h,d), dir(d,g) d-f/l, s-f/l, dir(h,d), dir(d,s) ®dist(h,s) d-pos, s-f/l, dir(h,d), dir(d,s)®dist(h,s)
h-f/l, s-pos, dir(h,d), dir(d,s)®dist(h,s)
d-f/l, s-pos, dir(h,d), dir(d,s)®dist(h,s)
</table>
<tableCaption confidence="0.99949">
Table 2: Selected Features.
</tableCaption>
<bodyText confidence="0.995580956521739">
three passes. The goal of the first two passes is to
collect the set of possible features of the training
set. In order to determine the minimal description
length, the feature extractor collects in the first pass
all attributes that the features can contain. For each
attribute (labels, part-of-speech, etc.), the extractor
computes a mapping to a number which is continous
from 1 to the count of elements without duplicates.
We enumerate in the same way the feature pat-
terns (e.g. h-pos, d-pos) in order to distinguish the
patterns. In the second pass, the extractor builds the
features for all training examples which occur in the
train set. This means for all edges in the training
examples.
We create the features with a function that adds it-
eratively the attributes of a feature to a number rep-
resented with 64 bits and shifts it by the minimal
number of bits to encode the attribute and then enu-
merates and maps these numbers to 32 bit numbers
to save even more memory.
Beside this, the following list shows an overview
of the most important implementation details to im-
prove the speed:
</bodyText>
<listItem confidence="0.6338755">
1. We use as feature vector a custom array imple-
mentation with only a list of the features that
means without double floating point value.
2. We store the feature vectors for
</listItem>
<bodyText confidence="0.870745">
f(label, wi, wj), f(label, wi, wj, wg),
f(label, wi, wj, ws) etc. in a compressed
file since otherwise it becomes the bottleneck.
</bodyText>
<listItem confidence="0.999518">
3. After the training, we store only the parameters
of the support vector machine which are higher
than a threshold of 1×10−7
</listItem>
<tableCaption confidence="0.9969495">
Table 3 compares system regarding their perfor-
mance and memory usage. For the shared task, we
</tableCaption>
<table confidence="0.999585555555556">
System (1) (2) (3)
Type 2nd order 2nd order 2nd order
Labeling separate separate integrated
System baseline this this
Training 22 hours 3 hours 15 hours
7GB 1.5 GB 3 GB
Parsing 2000 ms 50 ms 610 ms
700 MB 1 GB
LAS 0.86 0.86 0.88
</table>
<tableCaption confidence="0.941298666666667">
Table 3: Performance Comparison. For the baseline sys-
tem (1), we used the system of McDonald and Pereira
(2006) on a MacPro 2.8 Ghz as well for our implementa-
tion (2). For system (3), we use a computer with Intel i7
3.2 Ghz which is faster than the MacPro. For all systems,
we use 10 training iterations for the SVM Mira.
</tableCaption>
<bodyText confidence="0.942155">
use the system (3) with integrated labeling.
</bodyText>
<sectionHeader confidence="0.828271" genericHeader="method">
8 Semantic Role Labeling
</sectionHeader>
<bodyText confidence="0.98974865">
The semantic role labeler is implemented as a
pipeline architecture. The components of the
pipeline are predicate selection (PS), argument iden-
tification (AI), argument classification (AC), and
word sense disambiguation (WSD).
In order to select the predicates, we look up the
lemmas in the Prob Bank, Nom Bank, etc. if avail-
able, cf. (Palmer et al., 2005; Meyers et al., 2004).
For all other components, we use the support vec-
tor machine MIRA to select and classify the seman-
tic role labels as well as to disambiguate the word
senese.
The AI component identifies the arguments of
each predicate. It iterates over the predicates and
over the words of a sentence. In the case that the
score function is large or equal to zero the argument
is added to the set of arguments of the predicate in
question. Table 5 lists for the attribute identification
and semantic role labeling.
The argument classification algorithm labels each
</bodyText>
<page confidence="0.990958">
70
</page>
<table confidence="0.9995217">
Language Catalan Chinese Czech English German Japanese Spanish Czech English German
Development Set
LAS 86.69 76.77 80.75 87.97 86.46 92.37 86.53
Semantic Unlabeled 93.92 85.09 94.05 91.06 91.61 93.90 93.87
Semantic Labeled 74.98 75.94 78.07 78.79 72.66 72.86 73.01
Macro (F1) 80.84 76.48 79.42 84.52 79.56 82.63 79.77
Test Set Out-of-domain data
LAS 86.35 76.51 80.11 @89.88 @87.48 92.21 87.19 76.40 @82.64 @77.34
Semantic Labeled 74.53 75.29 79.02 80.39 75.72 72.76 74.31 78.01 68.44 63.36
Macro (F1) 80.44 75.91 79.57 85.14 81.60 82.51 80.75 77.20 75.55 70.35
</table>
<tableCaption confidence="0.999264">
Table 4: Syntactic and Semantic Scores. @ indicate values that are the highest scores of all systems.
</tableCaption>
<figureCaption confidence="0.978322294117647">
Features with part-of-speech tags Features with lemmas Features with rels
arg, path-len arg, p-lemma ® dir ® path-len arg, a-rel ® path-len
arg, p-pos arg, a-lemma, path, dir arg, a-pos, p-pos, p-rel ® path-len
arg, sub-cat, a-pos, p-pos arg, p-lemma - 1, a-pos, path-len, dir arg, p-rel, a-pos, lms-lemma
arg, p-pos, a-pos, a-rmc-pos arg, p-lemma + 1, a-lemma, path, dir arg, a-pos, p-pos, a-rel
arg, p-pos, a-pos, a-lmc-pos arg, p-lemma - 1, a-lemma, path, dir arg, path-rel
arg, p-pos, a-pos, a-lemma-1 arg, p-lemma - 2, a-lemma, path, dir arg, p-lemma, a-pos, path-rel
arg, sub-cat, a-lemma, dir, path-len arg, p-lemma, a-lemma, pathPos, dir
arg, a-pos, a-lemma + 1 arg, p-lemma, p-lemma + 1
arg, a-pos, a-lemma + 2 arg, p-lemma, p-lemma - 1, a-pos ® dir ® path-len
arg, a-pos, a-lemma-lmc arg, p-lemma, a-lms-lemma, a-pos ® dir ® path-len
arg, p-sub-cat, p-pos ® dir arg, p-lemma, path-len ® dir
arg, p-pos, path, p-parent-lemma arg, p-lemma, path-len ® dir
arg, p-pos, path, p-parent-pos ® dir arg, a-pos, path
arg, p-pos, a-pos, familyship(p,a) arg, p-pos, p-lemma, familyship(p,a)
arg, path-pos arg, a-pos, p-lemma, familyship(p,a)
arg, p-pos, a-lemma, familyship(p,a)
</figureCaption>
<tableCaption confidence="0.648382">
Table 5: Argument identification and semantic role labeling Features. p is the abbrivation for predicate and a for
</tableCaption>
<bodyText confidence="0.974614818181818">
argument. For the AI component, the attribute arg is either the value yes and no and for the SRL component, ars is
the role label. path is the path in terms of up‘s and down‘s. pathPos is a path plus the part-of-speech on the path. dir
is left, if the argument is left of the predicate, equal if the predicate and argument are equal, otherwise right. rmc is
the abbrivation for right most child, lmc for left most child, and lms left most sibling. familiship(x,y) is a function that
computes the relation between two words: self, parent, child, ancestor, decendant and none.
identified argument with a semantic role label. The
argument classification algorithm selects with a
beam search algorithm the combination of argu-
ments with the highest score.
The last component of our pipeline is the word
sense disambiguation. We put this against the in-
tuition at the end of our pipeline since experiments
showed that other components could not profit from
disambiguated word senses but on the other hand
the word sense disambiguation could profit from the
argument identification and argument classification.
In order to disambiguate, we iterate over the words
in the corpus that have more than one sense and take
the sense with the highest score.
The average time to execute the SRL pipeline on
a sentence is less than 0.15 seconds and the training
time for all languages less than 2 hours.
</bodyText>
<sectionHeader confidence="0.996143" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999906777777778">
We provided a fast implementation with good pars-
ing time and memory footprint. Even if we traded
off a lot of the speed improvement by using a
more expensive decoder and more attributes to get
a higher accuracy.
For some languages, features are not provided or
the parser does not profit from using these features.
For instance, the English parser does not profit from
the lemmas and the Chinese as well as the Japanese
</bodyText>
<page confidence="0.99534">
71
</page>
<bodyText confidence="0.9981942">
corpus does not have lemmas different from the
word forms, etc. Therefore, a possible further ac-
curacy and parsing speed improvement would be to
select different features sets for different languages
or to leave out some features.
</bodyText>
<sectionHeader confidence="0.995505" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.990221333333333">
This work was supported by the German Academic
Exchange Service (DAAD). We gratefully acknowl-
edge this support.
</bodyText>
<sectionHeader confidence="0.998465" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999614967391304">
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing. In
In Proc. of CoNLL, pages 149–164.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Experi-
ments with Perceptron Algorithms. In EMNLP.
Koby Crammer, Ofer Dekel, Shai Shalev-Shwartz, and
Yoram Singer. 2003. Online Passive-Aggressive Al-
gorithms. In Sixteenth Annual Conference on Neural
Information Processing Systems (NIPS).
Jason Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proceed-
ings of the 16th International Conference on Com-
putational Linguistics (COLING-96), pages 340–345,
Copenhaen.
Jan Hajiˇc, Jarmila Panevov´a, Eva Hajiˇcov´a, Petr
Sgall, Petr Pajas, Jan ˇStˇep´anek, Jiˇr´ı Havelka, Marie
ˇZabokrtsk´y. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajiˇc, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Marti, Llu’is
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Miahi Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 Shared Task: Syntactic and Semantic Dependen-
cies in Multiple Languages. In Proceedings ofthe 13th
CoNLL-2009, June 4-5, Boulder, Colorado, USA.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic–semantic analysis
with PropBank and NomBank. In Proceedings of the
Shared Task Session of CoNLL-2008, Manchester,
UK.
Daisuke Kawahara, Sadao Kurohashi, and Kˆoiti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008–2013, Las Palmas, Canary
Islands.
Ryan McDonald and Fernando Pereira. 2006. Online
Learning of Approximate Dependency Parsing Algo-
rithms. In In Proc. of EACL, pages 81–88.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online Large-margin Training of Dependency
Parsers. In Proc. ACL, pages 91–98.
Ryan McDonald, Kevin Lerman, Koby Crammer, and
Fernando Pereira. 2006. Multilingual Dependency
Parsing with a Two-Stage Discriminative Parser. In
Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL-X), pages 91–98.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. The nombank project: An interim
report. In A. Meyers, editor, HLT-NAACL 2004 Work-
shop: Frontiers in Corpus Annotation, pages 24–31,
Boston, Massachusetts, USA, May 2 - May 7. Associ-
ation for Computational Linguistics.
Joakim. Nivre and Jens Nilsson. 2005. Pseudo-
Projective Dependency Parsing. In In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, pages 99–106.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-Based Dependency Parsing. In Proceed-
ings of the 8th CoNLL, pages 49–56, Boston, Mas-
sachusetts.
Joakim Nivre, Johan Hall, Sandra K¨ubler, Rayn McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The conll 2007 shared task on dependency pars-
ing. In Proc. of the CoNLL 2007 Shared Task. Joint
Conf. on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), June.
Martha Palmer and Nianwen Xue. 2009. Adding Seman-
tic Roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143–172.
Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005. The Proposition Bank: An Annotated Corpus
of Semantic Roles. volume 31, pages 71–106.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Lluis M`arquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and
semantic dependencies. In Proceedings of the 12th
CoNLL-2008.
Mariona Taul´e, Maria Ant`onia Marti, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the LREC-
2008, Marrakesh, Morroco.
</reference>
<figure confidence="0.4753575">
ˇ
Mikulov´a, and Zdenk
</figure>
<page confidence="0.976157">
72
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.726196">
<title confidence="0.99983">Efficient Parsing of Syntactic and Semantic Dependency Structures</title>
<author confidence="0.973168">Bernd</author>
<affiliation confidence="0.998904">International Computer Science</affiliation>
<address confidence="0.913422">1947 Center Berkeley 94704,</address>
<email confidence="0.999605">bohnet@icsi.Berkeley.edu</email>
<abstract confidence="0.99588064">In this paper, we describe our system for the 2009 CoNLL shared task for joint parsing of syntactic and semantic dependency structures of multiple languages. Our system combines and implements efficient parsing techniques to get a high accuracy as well as very good parsing and training time. For the applications of syntactic and semantic parsing, the parsing time and memory footprint are very important. We think that also the development of systems can profit from this since one can perform more experiments in the given time. For the subtask of syntactic dependency parsing, we could reach the second place with an accuracy in average of 85.68 which is only 0.09 points behind the first ranked system. For this task, our system has the highest accuracy for English with 89.88, German with 87.48 and the out-of-domain data in average with 78.79. The semantic role labeler works not as well as our parser and we reached therefore the fourth place (ranked by the macro F1 score) in the joint task for syntactic and semantic dependency parsing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X Shared Task on Multilingual Dependency Parsing. In</title>
<date>2006</date>
<booktitle>In Proc. of CoNLL,</booktitle>
<pages>149--164</pages>
<contexts>
<context position="1476" citStr="Buchholz and Marsi, 2006" startWordPosition="234" endWordPosition="238">ich is only 0.09 points behind the first ranked system. For this task, our system has the highest accuracy for English with 89.88, German with 87.48 and the out-of-domain data in average with 78.79. The semantic role labeler works not as well as our parser and we reached therefore the fourth place (ranked by the macro F1 score) in the joint task for syntactic and semantic dependency parsing. 1 Introduction Depedendency parsing and semantic role labeling improved in the last years significantly. One of the reasons are CoNLL shared tasks for syntactic dependency parsing in the years 2006, 2007 (Buchholz and Marsi, 2006; Nivre et al., 2007) and the CoNLL shared task for joint parsing of syntactic and semantic dependencies in the year 2008 and of cause this shared task in 2009, cf. (Surdeanu et al., 2008; Hajiˇc et al., 2009). The CoNLL Shared Task 2009 is to parse syntactic and semantic dependencies of seven languages. Therefore, training and development data in form of annotated corpora for Catalan, Chinese, Czech, English, German, Japanese and Spanish is provided, cf. (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006; Kawahara et al., 2002). Ther</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X Shared Task on Multilingual Dependency Parsing. In In Proc. of CoNLL, pages 149–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aljoscha Burchardt</author>
<author>Katrin Erk</author>
<author>Anette Frank</author>
<author>Andrea Kowalski</author>
<author>Sebastian Pado</author>
<author>Manfred Pinkal</author>
</authors>
<title>The SALSA corpus: a German corpus resource for lexical semantics.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC-2006),</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="2046" citStr="Burchardt et al., 2006" startWordPosition="332" endWordPosition="335">ng in the years 2006, 2007 (Buchholz and Marsi, 2006; Nivre et al., 2007) and the CoNLL shared task for joint parsing of syntactic and semantic dependencies in the year 2008 and of cause this shared task in 2009, cf. (Surdeanu et al., 2008; Hajiˇc et al., 2009). The CoNLL Shared Task 2009 is to parse syntactic and semantic dependencies of seven languages. Therefore, training and development data in form of annotated corpora for Catalan, Chinese, Czech, English, German, Japanese and Spanish is provided, cf. (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006; Kawahara et al., 2002). There are two main approaches to dependency parsing: Maximum Spanning Tree (MST) based dependency parsing and Transition based dependency parsing, cf. (Eisner, 1996; Nivre et al., 2004; McDonald and Pereira, 2006). Our system uses the first approach since we saw better chance to improve the parsing speed and additionally, the MST had so far slightly better parsing results. For the task of semantic role labeling, we adopted a pipeline architecture where we used for each step the same learning technique (SVM) since we opted for the possibility to build a synchronous com</context>
</contexts>
<marker>Burchardt, Erk, Frank, Kowalski, Pado, Pinkal, 2006</marker>
<rawString>Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea Kowalski, Sebastian Pado, and Manfred Pinkal. 2006. The SALSA corpus: a German corpus resource for lexical semantics. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC-2006), Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms.</title>
<date>2002</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="9133" citStr="Collins, 2002" startWordPosition="1538" endWordPosition="1539">rresponding dependency trees V(0) = 0, v = 0 for n = 1 to N for x = 1 to X wi+1 = update wi according to instance (Sx, Tx) v = v + wi+1 i = i + 1 end for end for w=v/(N*X) The inner loop iterates over all sentences x of the training set while the outer loop repeats the train i times. The algorithm returns an averaged weight vector and uses an auxiliary weight vector v that accumulates the values of w after each iteration. At the end, the algorithm computes the average of all weight vectors by dividing it by the number of training iterations and sentences. This helps to avoid overfitting, cf. (Collins, 2002). The update function computes the update to the weight vector wz during the training so that wrong classified edges of the training instances are possibly correctly classified. This is computed by increasing the weight for the correct features and decreasing the weight for wrong features of the vectors for the tree of the training set f�x�� * wz and the vector for the �� predicted dependency treef� x ′ * wz. The update function tries to keep the change to the parameter vector wz as small as possible for correctly classifying the current instance with a difference at least as large as the loss</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online Passive-Aggressive Algorithms.</title>
<date>2003</date>
<booktitle>In Sixteenth Annual Conference on Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="4626" citStr="Crammer et al., 2003" startWordPosition="783" endWordPosition="786">indices of the words in a sentence. The feature vector fS might take not only into account the words with indices i and j but also additional values such as the words before and after the words wi and wj. The following equation shows the score function. score(i, j) = fS(i, j) � �� �� w Many systems encode the features as strings and map the strings to a number. The number becomes the index of the feature in the feature vector and weight vector. In order to compute the weight vector, we reimplemented the support vector machine MIRA which implements online Margin Infused Relaxed Algorithm, cf. (Crammer et al., 2003). 3 Labeled Dependency Parsing The second order parsing algorithm builds an unlabeled dependency tree. However, all dependency tree banks of the shared task provide trees with edge labels. The following two approaches are common to solve this problem. An additional algorithm labels the edges or the parsing algorithm itself is extended and the labeling algorithm is integrated into the parsing algorithm. McDonald et al. (2006) use an additional algorithm. Their two stage model has a good computational complexity since the labeling algorithm contributes again only a cubic time complexity to the a</context>
<context position="8235" citStr="Crammer et al. (2003)" startWordPosition="1366" endWordPosition="1369">e portion of non-projective edges are still small and therefore by starting with the highest scoring projective tree, typ68 ically the highest scoring non-projective tree is only a small number of transformations away. Our experiments showed that with the nonprojective Approximate Dependency Parsing Algorithm and a threshold for the improvment of score higher than about 0.7, the parsing accuracy improves even for English slightly. With a threshold of 1.1, we got the highest improvements. 5 Learning Framework As learning technique, we use Margin Infused Relaxed Algorithm (MIRA) as developed by Crammer et al. (2003) and applied to dependency parsing by McDonald et al. (2005). The online Algorithm in Figure 1 processes one training instance on each iteration, and updates the parameters accordingly. Algorithm 1: MIRA τ = {Sx, Tx}Xx=1 // The set of training data consists // of sentences and the corresponding dependency trees V(0) = 0, v = 0 for n = 1 to N for x = 1 to X wi+1 = update wi according to instance (Sx, Tx) v = v + wi+1 i = i + 1 end for end for w=v/(N*X) The inner loop iterates over all sentences x of the training set while the outer loop repeats the train i times. The algorithm returns an averag</context>
</contexts>
<marker>Crammer, Dekel, Shalev-Shwartz, Singer, 2003</marker>
<rawString>Koby Crammer, Ofer Dekel, Shai Shalev-Shwartz, and Yoram Singer. 2003. Online Passive-Aggressive Algorithms. In Sixteenth Annual Conference on Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (COLING-96),</booktitle>
<pages>340--345</pages>
<contexts>
<context position="2236" citStr="Eisner, 1996" startWordPosition="362" endWordPosition="363">ask in 2009, cf. (Surdeanu et al., 2008; Hajiˇc et al., 2009). The CoNLL Shared Task 2009 is to parse syntactic and semantic dependencies of seven languages. Therefore, training and development data in form of annotated corpora for Catalan, Chinese, Czech, English, German, Japanese and Spanish is provided, cf. (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006; Kawahara et al., 2002). There are two main approaches to dependency parsing: Maximum Spanning Tree (MST) based dependency parsing and Transition based dependency parsing, cf. (Eisner, 1996; Nivre et al., 2004; McDonald and Pereira, 2006). Our system uses the first approach since we saw better chance to improve the parsing speed and additionally, the MST had so far slightly better parsing results. For the task of semantic role labeling, we adopted a pipeline architecture where we used for each step the same learning technique (SVM) since we opted for the possibility to build a synchronous combined parser with one score function. 2 Parsing Algorithm We adopted the second order MST parsing algorithm as outlined by Eisner (1996). This algorithm has a higher accuracy compared to the</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of the 16th International Conference on Computational Linguistics (COLING-96), pages 340–345, Copenhaen.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Hajiˇc</author>
<author>Jarmila Panevov´a</author>
</authors>
<location>Eva Hajiˇcov´a, Petr</location>
<marker>Hajiˇc, Panevov´a, </marker>
<rawString>Jan Hajiˇc, Jarmila Panevov´a, Eva Hajiˇcov´a, Petr</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Pajas Sgall</author>
<author>Jan ˇStˇep´anek</author>
<author>Jiˇr´ı Havelka</author>
<author>Marie ˇZabokrtsk´y</author>
</authors>
<date>2006</date>
<journal>Prague Dependency Treebank</journal>
<volume>2</volume>
<marker>Sgall, ˇStˇep´anek, Havelka, ˇZabokrtsk´y, 2006</marker>
<rawString>Sgall, Petr Pajas, Jan ˇStˇep´anek, Jiˇr´ı Havelka, Marie ˇZabokrtsk´y. 2006. Prague Dependency Treebank 2.0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria Ant`onia Marti</author>
<author>Llu’is M`arquez</author>
<author>Adam Meyers</author>
<author>Joakim Nivre</author>
<author>Sebastian Pad´o</author>
<author>Jan ˇStˇep´anek</author>
<author>Pavel Straˇn´ak</author>
<author>Miahi Surdeanu</author>
<author>Nianwen Xue</author>
<author>Yi Zhang</author>
</authors>
<date>2009</date>
<booktitle>The CoNLL2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Languages. In Proceedings ofthe 13th CoNLL-2009,</booktitle>
<location>Boulder, Colorado, USA.</location>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Marti, M`arquez, Meyers, Nivre, Pad´o, ˇStˇep´anek, Straˇn´ak, Surdeanu, Xue, Zhang, 2009</marker>
<rawString>Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Marti, Llu’is M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Miahi Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The CoNLL2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Languages. In Proceedings ofthe 13th CoNLL-2009, June 4-5, Boulder, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Dependency-based syntactic–semantic analysis with PropBank and NomBank.</title>
<date>2008</date>
<booktitle>In Proceedings of the Shared Task Session of CoNLL-2008,</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="5907" citStr="Johansson and Nugues (2008)" startWordPosition="992" endWordPosition="995"> cubic. The algorithm selects the highest scored label due to the score function score(wi, label) + score(wj, label) and inserts the highest scored label into a matrix. The scores are also used in the parsing algorithms and added to the edge scores which improves the overall parsing results as well. In the first order parsing scenario, this procedure is sufficient since no combination of edges are considered by the parsing algorithm. However, in the second order parsing scenario where more than one edge are considered by the parsing algorithm, combinations of two edges might be more accurate. Johansson and Nugues (2008) combines the edge labeling with the second order parsing algorithm. This adds an additional loop over the edge labels. The complexity is therefore O(n4). However, they could show that a system can gain accuracy of about 2-4% which is a lot. 4 Non-Projective Dependency Parsing The dependency parser developed in the last years use two different techniques for non-projective dependency parsing. Nivre and Nilsson (2005) uses tree rewriting which is the most common technique. With this technique, the training input to the parser is first projectivized by applying a minimal number of lifting operat</context>
<context position="9909" citStr="Johansson and Nugues (2008)" startWordPosition="1672" endWordPosition="1675">ssibly correctly classified. This is computed by increasing the weight for the correct features and decreasing the weight for wrong features of the vectors for the tree of the training set f�x�� * wz and the vector for the �� predicted dependency treef� x ′ * wz. The update function tries to keep the change to the parameter vector wz as small as possible for correctly classifying the current instance with a difference at least as large as the loss of the incorrect classifications. 6 Selected Parsing Features Table 1, 4 and 2 give an overview of the selected features for our system. Similar to Johansson and Nugues (2008), we add the edge labels to each features. In the feature selection, we follow a bit more McDonald and Pereira (2006) since we have in addition the lemmas, morphologic features and the distance between the word forms. For the parsing and training speed, most important is a fast feature extraction beside of a fast parsing algorithm. Standard Features h-f/l h-f/l, d-pos h-pos h-pos, d-f/l d-f/l h-f/l, d-f/l d-pos h-pos, d-pos h-f/l,h-pos h-f/l, d-f/l, h-pos d-f/l,d-pos h-f/l, d-f/l, d-pos h-pos, d-pos, h-f/l h-pos, d-pos, d-f/l h-pos, d-pos, h-f/l, d-f/l Table 1: Selected standard parsing featur</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>Richard Johansson and Pierre Nugues. 2008. Dependency-based syntactic–semantic analysis with PropBank and NomBank. In Proceedings of the Shared Task Session of CoNLL-2008, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
<author>Kˆoiti Hasida</author>
</authors>
<title>Construction of a Japanese relevance-tagged corpus.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC-2002),</booktitle>
<pages>2008--2013</pages>
<location>Las Palmas, Canary Islands.</location>
<contexts>
<context position="2070" citStr="Kawahara et al., 2002" startWordPosition="336" endWordPosition="339">07 (Buchholz and Marsi, 2006; Nivre et al., 2007) and the CoNLL shared task for joint parsing of syntactic and semantic dependencies in the year 2008 and of cause this shared task in 2009, cf. (Surdeanu et al., 2008; Hajiˇc et al., 2009). The CoNLL Shared Task 2009 is to parse syntactic and semantic dependencies of seven languages. Therefore, training and development data in form of annotated corpora for Catalan, Chinese, Czech, English, German, Japanese and Spanish is provided, cf. (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006; Kawahara et al., 2002). There are two main approaches to dependency parsing: Maximum Spanning Tree (MST) based dependency parsing and Transition based dependency parsing, cf. (Eisner, 1996; Nivre et al., 2004; McDonald and Pereira, 2006). Our system uses the first approach since we saw better chance to improve the parsing speed and additionally, the MST had so far slightly better parsing results. For the task of semantic role labeling, we adopted a pipeline architecture where we used for each step the same learning technique (SVM) since we opted for the possibility to build a synchronous combined parser with one sc</context>
</contexts>
<marker>Kawahara, Kurohashi, Hasida, 2002</marker>
<rawString>Daisuke Kawahara, Sadao Kurohashi, and Kˆoiti Hasida. 2002. Construction of a Japanese relevance-tagged corpus. In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC-2002), pages 2008–2013, Las Palmas, Canary Islands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online Learning of Approximate Dependency Parsing Algorithms. In</title>
<date>2006</date>
<booktitle>In Proc. of EACL,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="2285" citStr="McDonald and Pereira, 2006" startWordPosition="368" endWordPosition="372"> 2008; Hajiˇc et al., 2009). The CoNLL Shared Task 2009 is to parse syntactic and semantic dependencies of seven languages. Therefore, training and development data in form of annotated corpora for Catalan, Chinese, Czech, English, German, Japanese and Spanish is provided, cf. (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006; Kawahara et al., 2002). There are two main approaches to dependency parsing: Maximum Spanning Tree (MST) based dependency parsing and Transition based dependency parsing, cf. (Eisner, 1996; Nivre et al., 2004; McDonald and Pereira, 2006). Our system uses the first approach since we saw better chance to improve the parsing speed and additionally, the MST had so far slightly better parsing results. For the task of semantic role labeling, we adopted a pipeline architecture where we used for each step the same learning technique (SVM) since we opted for the possibility to build a synchronous combined parser with one score function. 2 Parsing Algorithm We adopted the second order MST parsing algorithm as outlined by Eisner (1996). This algorithm has a higher accuracy compared to the first order parsing algorithm since it considers</context>
<context position="6948" citStr="McDonald and Pereira (2006)" startWordPosition="1157" endWordPosition="1160">(2005) uses tree rewriting which is the most common technique. With this technique, the training input to the parser is first projectivized by applying a minimal number of lifting operations to the non-projective edges and encoding information about these lifts in edge labels. After these operations, the trees are projective and therefore a projective dependency parser can be applied. During the training, the parser learns also to built trees with the lifted edges and so indirect to built non-projective dependency trees by applying the inverse operations to the lifting on the projective tree. McDonald and Pereira (2006) developed a technique to rearrange edges in the tree in a postprocessing step after the projective parsing has taken place. Their Approximate Dependency Parsing Algorithm searches first the highest scoring projective parse tree and then it rearranges edges in the tree until the rearrangements does not increase the score for the tree anymore. This technique is computationally expensive for trees with a large number of non-projective edges since it considers to re-attach all edges to any other node until no higher scoring trees can be found. Their argument for the algorithm is that most edges i</context>
<context position="10026" citStr="McDonald and Pereira (2006)" startWordPosition="1694" endWordPosition="1697">ight for wrong features of the vectors for the tree of the training set f�x�� * wz and the vector for the �� predicted dependency treef� x ′ * wz. The update function tries to keep the change to the parameter vector wz as small as possible for correctly classifying the current instance with a difference at least as large as the loss of the incorrect classifications. 6 Selected Parsing Features Table 1, 4 and 2 give an overview of the selected features for our system. Similar to Johansson and Nugues (2008), we add the edge labels to each features. In the feature selection, we follow a bit more McDonald and Pereira (2006) since we have in addition the lemmas, morphologic features and the distance between the word forms. For the parsing and training speed, most important is a fast feature extraction beside of a fast parsing algorithm. Standard Features h-f/l h-f/l, d-pos h-pos h-pos, d-f/l d-f/l h-f/l, d-f/l d-pos h-pos, d-pos h-f/l,h-pos h-f/l, d-f/l, h-pos d-f/l,d-pos h-f/l, d-f/l, d-pos h-pos, d-pos, h-f/l h-pos, d-pos, d-f/l h-pos, d-pos, h-f/l, d-f/l Table 1: Selected standard parsing features. h is the abbrevation for head, d for dependent, g for grandchild, and s for sibling. Each feature contains also t</context>
<context position="14361" citStr="McDonald and Pereira (2006)" startWordPosition="2399" endWordPosition="2402">c. in a compressed file since otherwise it becomes the bottleneck. 3. After the training, we store only the parameters of the support vector machine which are higher than a threshold of 1×10−7 Table 3 compares system regarding their performance and memory usage. For the shared task, we System (1) (2) (3) Type 2nd order 2nd order 2nd order Labeling separate separate integrated System baseline this this Training 22 hours 3 hours 15 hours 7GB 1.5 GB 3 GB Parsing 2000 ms 50 ms 610 ms 700 MB 1 GB LAS 0.86 0.86 0.88 Table 3: Performance Comparison. For the baseline system (1), we used the system of McDonald and Pereira (2006) on a MacPro 2.8 Ghz as well for our implementation (2). For system (3), we use a computer with Intel i7 3.2 Ghz which is faster than the MacPro. For all systems, we use 10 training iterations for the SVM Mira. use the system (3) with integrated labeling. 8 Semantic Role Labeling The semantic role labeler is implemented as a pipeline architecture. The components of the pipeline are predicate selection (PS), argument identification (AI), argument classification (AC), and word sense disambiguation (WSD). In order to select the predicates, we look up the lemmas in the Prob Bank, Nom Bank, etc. if</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online Learning of Approximate Dependency Parsing Algorithms. In In Proc. of EACL, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online Large-margin Training of Dependency Parsers.</title>
<date>2005</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="8295" citStr="McDonald et al. (2005)" startWordPosition="1376" endWordPosition="1379">fore by starting with the highest scoring projective tree, typ68 ically the highest scoring non-projective tree is only a small number of transformations away. Our experiments showed that with the nonprojective Approximate Dependency Parsing Algorithm and a threshold for the improvment of score higher than about 0.7, the parsing accuracy improves even for English slightly. With a threshold of 1.1, we got the highest improvements. 5 Learning Framework As learning technique, we use Margin Infused Relaxed Algorithm (MIRA) as developed by Crammer et al. (2003) and applied to dependency parsing by McDonald et al. (2005). The online Algorithm in Figure 1 processes one training instance on each iteration, and updates the parameters accordingly. Algorithm 1: MIRA τ = {Sx, Tx}Xx=1 // The set of training data consists // of sentences and the corresponding dependency trees V(0) = 0, v = 0 for n = 1 to N for x = 1 to X wi+1 = update wi according to instance (Sx, Tx) v = v + wi+1 i = i + 1 end for end for w=v/(N*X) The inner loop iterates over all sentences x of the training set while the outer loop repeats the train i times. The algorithm returns an averaged weight vector and uses an auxiliary weight vector v that </context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online Large-margin Training of Dependency Parsers. In Proc. ACL, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Kevin Lerman</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Multilingual Dependency Parsing with a Two-Stage Discriminative Parser.</title>
<date>2006</date>
<booktitle>In Tenth Conference on Computational Natural Language Learning (CoNLL-X),</booktitle>
<pages>91--98</pages>
<contexts>
<context position="5054" citStr="McDonald et al. (2006)" startWordPosition="851" endWordPosition="854">or and weight vector. In order to compute the weight vector, we reimplemented the support vector machine MIRA which implements online Margin Infused Relaxed Algorithm, cf. (Crammer et al., 2003). 3 Labeled Dependency Parsing The second order parsing algorithm builds an unlabeled dependency tree. However, all dependency tree banks of the shared task provide trees with edge labels. The following two approaches are common to solve this problem. An additional algorithm labels the edges or the parsing algorithm itself is extended and the labeling algorithm is integrated into the parsing algorithm. McDonald et al. (2006) use an additional algorithm. Their two stage model has a good computational complexity since the labeling algorithm contributes again only a cubic time complexity to the algorithm and keeps therefore the joint algorithm still cubic. The algorithm selects the highest scored label due to the score function score(wi, label) + score(wj, label) and inserts the highest scored label into a matrix. The scores are also used in the parsing algorithms and added to the edge scores which improves the overall parsing results as well. In the first order parsing scenario, this procedure is sufficient since n</context>
</contexts>
<marker>McDonald, Lerman, Crammer, Pereira, 2006</marker>
<rawString>Ryan McDonald, Kevin Lerman, Koby Crammer, and Fernando Pereira. 2006. Multilingual Dependency Parsing with a Two-Stage Discriminative Parser. In Tenth Conference on Computational Natural Language Learning (CoNLL-X), pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grishman</author>
</authors>
<title>The nombank project: An interim report.</title>
<date>2004</date>
<booktitle>HLT-NAACL 2004 Workshop: Frontiers in Corpus Annotation,</booktitle>
<volume>2</volume>
<pages>24--31</pages>
<editor>Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel Szekely, Veronika Zielinska, Brian Young, and Ralph</editor>
<location>Boston, Massachusetts, USA,</location>
<marker>Grishman, 2004</marker>
<rawString>Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel Szekely, Veronika Zielinska, Brian Young, and Ralph Grishman. 2004. The nombank project: An interim report. In A. Meyers, editor, HLT-NAACL 2004 Workshop: Frontiers in Corpus Annotation, pages 24–31, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nivre</author>
<author>Jens Nilsson</author>
</authors>
<title>PseudoProjective Dependency Parsing. In</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>99--106</pages>
<contexts>
<context position="6327" citStr="Nivre and Nilsson (2005)" startWordPosition="1058" endWordPosition="1061">ing algorithm. However, in the second order parsing scenario where more than one edge are considered by the parsing algorithm, combinations of two edges might be more accurate. Johansson and Nugues (2008) combines the edge labeling with the second order parsing algorithm. This adds an additional loop over the edge labels. The complexity is therefore O(n4). However, they could show that a system can gain accuracy of about 2-4% which is a lot. 4 Non-Projective Dependency Parsing The dependency parser developed in the last years use two different techniques for non-projective dependency parsing. Nivre and Nilsson (2005) uses tree rewriting which is the most common technique. With this technique, the training input to the parser is first projectivized by applying a minimal number of lifting operations to the non-projective edges and encoding information about these lifts in edge labels. After these operations, the trees are projective and therefore a projective dependency parser can be applied. During the training, the parser learns also to built trees with the lifted edges and so indirect to built non-projective dependency trees by applying the inverse operations to the lifting on the projective tree. McDona</context>
</contexts>
<marker>Nivre, Nilsson, 2005</marker>
<rawString>Joakim. Nivre and Jens Nilsson. 2005. PseudoProjective Dependency Parsing. In In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 99–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Memory-Based Dependency Parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the 8th CoNLL,</booktitle>
<pages>49--56</pages>
<location>Boston, Massachusetts.</location>
<contexts>
<context position="2256" citStr="Nivre et al., 2004" startWordPosition="364" endWordPosition="367">f. (Surdeanu et al., 2008; Hajiˇc et al., 2009). The CoNLL Shared Task 2009 is to parse syntactic and semantic dependencies of seven languages. Therefore, training and development data in form of annotated corpora for Catalan, Chinese, Czech, English, German, Japanese and Spanish is provided, cf. (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006; Kawahara et al., 2002). There are two main approaches to dependency parsing: Maximum Spanning Tree (MST) based dependency parsing and Transition based dependency parsing, cf. (Eisner, 1996; Nivre et al., 2004; McDonald and Pereira, 2006). Our system uses the first approach since we saw better chance to improve the parsing speed and additionally, the MST had so far slightly better parsing results. For the task of semantic role labeling, we adopted a pipeline architecture where we used for each step the same learning technique (SVM) since we opted for the possibility to build a synchronous combined parser with one score function. 2 Parsing Algorithm We adopted the second order MST parsing algorithm as outlined by Eisner (1996). This algorithm has a higher accuracy compared to the first order parsing</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2004</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. Memory-Based Dependency Parsing. In Proceedings of the 8th CoNLL, pages 49–56, Boston, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra K¨ubler</author>
<author>Rayn McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>The conll</title>
<date>2007</date>
<booktitle>In Proc. of the CoNLL 2007 Shared Task. Joint Conf. on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Sandra K¨ubler, Rayn McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The conll 2007 shared task on dependency parsing. In Proc. of the CoNLL 2007 Shared Task. Joint Conf. on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Nianwen Xue</author>
</authors>
<title>Adding Semantic Roles to the Chinese Treebank.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<volume>15</volume>
<issue>1</issue>
<contexts>
<context position="1978" citStr="Palmer and Xue, 2009" startWordPosition="320" endWordPosition="323"> the reasons are CoNLL shared tasks for syntactic dependency parsing in the years 2006, 2007 (Buchholz and Marsi, 2006; Nivre et al., 2007) and the CoNLL shared task for joint parsing of syntactic and semantic dependencies in the year 2008 and of cause this shared task in 2009, cf. (Surdeanu et al., 2008; Hajiˇc et al., 2009). The CoNLL Shared Task 2009 is to parse syntactic and semantic dependencies of seven languages. Therefore, training and development data in form of annotated corpora for Catalan, Chinese, Czech, English, German, Japanese and Spanish is provided, cf. (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006; Kawahara et al., 2002). There are two main approaches to dependency parsing: Maximum Spanning Tree (MST) based dependency parsing and Transition based dependency parsing, cf. (Eisner, 1996; Nivre et al., 2004; McDonald and Pereira, 2006). Our system uses the first approach since we saw better chance to improve the parsing speed and additionally, the MST had so far slightly better parsing results. For the task of semantic role labeling, we adopted a pipeline architecture where we used for each step the same learning technique</context>
</contexts>
<marker>Palmer, Xue, 2009</marker>
<rawString>Martha Palmer and Nianwen Xue. 2009. Adding Semantic Roles to the Chinese Treebank. Natural Language Engineering, 15(1):143–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Paul Kingsbury</author>
<author>Daniel Gildea</author>
</authors>
<title>The Proposition Bank: An Annotated Corpus of Semantic Roles.</title>
<date>2005</date>
<volume>31</volume>
<pages>71--106</pages>
<contexts>
<context position="14997" citStr="Palmer et al., 2005" startWordPosition="2509" endWordPosition="2512">Ghz as well for our implementation (2). For system (3), we use a computer with Intel i7 3.2 Ghz which is faster than the MacPro. For all systems, we use 10 training iterations for the SVM Mira. use the system (3) with integrated labeling. 8 Semantic Role Labeling The semantic role labeler is implemented as a pipeline architecture. The components of the pipeline are predicate selection (PS), argument identification (AI), argument classification (AC), and word sense disambiguation (WSD). In order to select the predicates, we look up the lemmas in the Prob Bank, Nom Bank, etc. if available, cf. (Palmer et al., 2005; Meyers et al., 2004). For all other components, we use the support vector machine MIRA to select and classify the semantic role labels as well as to disambiguate the word senese. The AI component identifies the arguments of each predicate. It iterates over the predicates and over the words of a sentence. In the case that the score function is large or equal to zero the argument is added to the set of arguments of the predicate in question. Table 5 lists for the attribute identification and semantic role labeling. The argument classification algorithm labels each 70 Language Catalan Chinese C</context>
</contexts>
<marker>Palmer, Kingsbury, Gildea, 2005</marker>
<rawString>Martha Palmer, Paul Kingsbury, and Daniel Gildea. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. volume 31, pages 71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Lluis M`arquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The CoNLL2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of the 12th CoNLL-2008.</booktitle>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Lluis M`arquez, and Joakim Nivre. 2008. The CoNLL2008 shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of the 12th CoNLL-2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mariona Taul´e</author>
<author>Maria Ant`onia Marti</author>
<author>Marta Recasens</author>
</authors>
<title>AnCora: Multilevel Annotated Corpora for Catalan and Spanish.</title>
<date>2008</date>
<booktitle>In Proceedings of the LREC2008,</booktitle>
<location>Marrakesh, Morroco.</location>
<marker>Taul´e, Marti, Recasens, 2008</marker>
<rawString>Mariona Taul´e, Maria Ant`onia Marti, and Marta Recasens. 2008. AnCora: Multilevel Annotated Corpora for Catalan and Spanish. In Proceedings of the LREC2008, Marrakesh, Morroco.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>