<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<sectionHeader confidence="0.561444" genericHeader="method">
COMPOSE-REDUCE PARSING
</sectionHeader>
<title confidence="0.880645090909091">
Henry S. Thompson1
Mike Dixon2
John Lamping2
1: Human Communication Research Centre
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW
SCOTLAND
2: Xerox Palo Alto Research Center
3333 Coyote Hill Road
Palo Alto, CA 94304
</title>
<bodyText confidence="0.982493032786885">
achieve linear (indeed real-time) com-
plexity by performing a constant-time
step per word of the input. The other
took as its starting point tabular pars-
ing (Earley, C KY), and sought to
achieve linear complexity by perform-
ing a constant-time step for the identi-
fication/construction of constituents of
each length from 0 to n. The latter
route has been widely canvassed,
although to our knowledge has not yet
been implemented—see (Nijholt 1989,
90) for extensive references. The
former route, whereby real-time pars-
ing is achieved by processor forking at
non-deterministic choice points in an
extended shift-reduce parser, is to our
knowledge new. In this paper we pre-
sent outlines of two such parsers,
which we call compose-reduce
parsers.
L COMPOSE-REDUCE PARSING
Why couldn&apos;t a simple breadth-
first chart parser achieve linear per-
formance on an appropriate parallel
system? If you provided enough pro-
cessors to immediately process all
agenda entries as they were created,
would not this give the desired result?
No, because the processing of a single
word might require many serialised
ABSTRACT
Two new parsing algorithms for
context-free phrase structure gram-
mars are presented which perform a
bounded amount of processing per
word per analysis path, independently
of sentence length. They are thus ca-
pable of parsing in real-time in a par-
allel implementation which forks pro-
cessors in response to non-determinis-
tic choice points.
0. INTRODUCTION
The work reported here grew out of
our attempt to improve on the 0 (n2)
performance of the SIMD parallel
parser described in (Thompson 1991).
Rather than start with a commitment
to a specific SIMD architecture, as that
work had, we agreed that the best
place to start was with a more abstract
architecture-independent considera-
tion of the CF-PSG parsing problem—
given arbitrary resources, what algo-
rithms could one envisage which
could recognise and/or parse atomic
category phrase-structure grammars
in c) (n) ? In the end, two quite differ-
ent approaches emerged. One took as
its starting point non-deterministic
shift-reduce parsing, and sought to
</bodyText>
<page confidence="0.998085">
87
</page>
<bodyText confidence="0.991889189189189">
steps. Consider processing the word
&amp;quot;park&amp;quot; in the sentence &amp;quot;The people
who ran in the park got wet.&amp;quot; Given a
simple traditional sort of grammar,
that word completes an NP, which in
turn completes a P r, which in turn
completes a VP , which in turn com-
pletes an s, which in turn completes a
RE L, which in turn completes an NP.
The construction/recognition of these
constituents is necessarily serialised,
so regardless of the number of proces-
sors available a constant-time step is
impossible. (Note that this only pre-
cludes a real-time parse by this route,
but not necessarily a linear one.) In
the shift-reduce approach to parsing,
all this means is that for non-linear
grammars, a single shift step may be
followed by many reduce steps. This
in turn suggested the beginnings of a
way out, based on categorial gram-
mar, namely that multiple reduces
can be avoided if composition is al-
lowed. To return to our example
above, in a simple shift-reduce parser
we would have had all the words pre-
ceding the word &amp;quot;park&amp;quot; in the stack.
When it was shifted in, there would
follow six reduce steps. If alterna-
tively following a shift step one was al-
lowed (non-deterministically) a com-
pose step, this could be reduced (!) to a
single reduce step. Restricting our-
selves to a simpler example, consider
just &amp;quot;run in the park&amp;quot; as a VP, given
rules
</bodyText>
<construct confidence="0.988800666666667">
VP -4 V PP
NP -4 d n
PP -4 p NP.
</construct>
<bodyText confidence="0.911650642857143">
With a composition step allowed,
the parse would then proceed as fol-
lows:
Shift run as a v
Shift in as a p
Compose v and p to give
[vp v [PP p • NP]]
where I use a combination of brack-
eted strings and the &apos;dotted rule&apos; nota-
tion to indicate the result of composi-
tion. The categorial equivalent would
have been to notate v as VP / PP,P as
PP /NP, and the result of the composi-
tion as therefore VP /NP.
</bodyText>
<subsectionHeader confidence="0.944419333333333">
Shift the as d
Compose the dotted VP with a
to give
</subsectionHeader>
<bodyText confidence="0.990504">
[vp V [PP p [NE, d • nil]
</bodyText>
<subsectionHeader confidence="0.871369">
Shift park as n
</subsectionHeader>
<bodyText confidence="0.980886611111111">
Reduce the dotted VP with n to
give the complete result.
Although a number of details re-
mained to be worked out, this simple
move of allowing composition was the
enabling step to achieving 0 (n) pars-
ing. Parallelism would arise by fork-
ing processors at each non-determin-
istic choice point, following the gen-
eral model of Dixon&apos;s earlier work on
parallelising the ATMS (Dixon &amp; de
Kleer 1988).
Simply allowing composition is not
in itself sufficient to achieve o (n) per-
formance. Some means of guarantee-
ing that each step is constant time
must still be provided. Here we found
two different ways forward.
</bodyText>
<sectionHeader confidence="0.975966" genericHeader="method">
IL THE FIRST COMPOSE-REDUCE
</sectionHeader>
<subsectionHeader confidence="0.887927">
PARSER—CR-I
</subsectionHeader>
<bodyText confidence="0.9966905">
In this parser there is no stack.
We have simply a current structure,
which corresponds to the top node of
the stack in a normal shift-reduce
parser. This is achieved by extending
the appeal to composition to include a
form of left-embedded raising, which
will be discussed further below.
Special attention is also required to
handle left-recursive rules.
</bodyText>
<page confidence="0.999238">
88
</page>
<tableCaption confidence="0.950128916666667">
11.1 The Basic Parsing Algorithm
The constant-time parsing step is
given below (slightly simplified, in
that empty productions and some unit
productions are not handled). In this
algorithm schema, and in subsequent
discussion, the annotation &amp;quot;ND&amp;quot; will be
used in situations where a number of
alternatives are (or may be) described.
The meaning is that these alternatives
are to be pursued non-deterministi-
cally.
</tableCaption>
<figure confidence="0.6885305">
Algorithm CR-I
1 Shift the next word;
</figure>
<footnote confidence="0.662336666666667">
2 ND look it up in the lexicon;
3 ND close the resulting cate-
gory wrt the unit produc-
tions;
4a ND reduce the resulting
category with the current
structure
or
4b ND raise* the resulting cat-
</footnote>
<bodyText confidence="0.9579676875">
egory wrt the non-unary
rules in the grammar for
which it is a left corner, and
compose the result with the
current structure.
If reduction ever completes a
category which is marked as
the left corner of one or
more left-recursive rules or
rule sequences, ND raise* in
place wrt those rules
(sequences), and propagate
the marking.
Some of these ND steps may at var-
ious points produce complete struc-
tures. If the input is exhausted, then
those structures are parses, or not,
depending on whether or not they have
reached the distinguished symbol. If
the input is not exhausted, it is of
course the incomplete structures, the
results of composition or raising,
which are carried forward to the next
step.
The operation referred to above as
&amp;quot;raise*&amp;quot; is more than simple raising,
as was involved in the simple example
in section W. In order to allow for all
possible compositions to take place all
possible left-embedded raising must be
pursued. Consider the following
grammar fragment:
</bodyText>
<construct confidence="0.9505884">
S -4 NP VP
VP -4 v NP CMP
CMP -4 that S
NP -4 propn
NP -4 d n
</construct>
<bodyText confidence="0.9572826">
and the utterance &amp;quot;Kim told Robin that
the child likes Kim&amp;quot;.
If we ignore all the ND incorrect
paths, the current structure after the
&amp;quot;that&amp;quot; has been processed is
</bodyText>
<equation confidence="0.65831">
ES ENP Epropn Kim]]
[VP [v told]
ENP Epropn Robin]]
[CMP that • S ] ] 3
</equation>
<bodyText confidence="0.9803588125">
In order for the next word, &amp;quot;the&amp;quot;, to
be correctly processed, it must be
raised all the way to s, namely we
must have
ts INP Ed the] • n3 VP)]
to compose with the current structure.
What this means is that for every en-
try in the normal bottom-up reachabil-
ity table pairing a left corner with a top
category, we need a set of dotted struc-
tures, corresponding to all the ways
the grammar can get from that left
corner to that top category. It is these
structures which are ND made avail-
able in step 4b of the parsing step algo-
rithm CR-I above.
</bodyText>
<page confidence="0.998739">
89
</page>
<subsectionHeader confidence="0.855509">
11.2 Handling Left Recursion
</subsectionHeader>
<bodyText confidence="0.998875625">
Now this in itself is not sufficient to
handle left recursive structures, since
by definition there could be an arbi-
trary number of left-embeddings of a
left-recursive structure. The final
note in the description of algorithm
CR-I above is designed to handle this.
Glossing over some subtleties, left-re-
cursion is handled by marking some
of the structures introduced in step 3b,
and ND raising in place if the marked
structure is ever completed by reduc-
tion in the course of a parse. Consider
the sentence &amp;quot;Robin likes the child &apos;s
dog.&amp;quot; We add the following two rules
to the grammar:
</bodyText>
<equation confidence="0.9441015">
D -4 art
D -4 NP &apos;s
</equation>
<bodyText confidence="0.995650625">
thereby transforming D from a pre-
terminal to a non-terminal. When we
shift &amp;quot;the&amp;quot;, we will raise to inter alia
[NP [ID [art the]] • n]r
with the NP marked for potential re-
raising. This structure will be com-
posed with the then current structure
to produce
</bodyText>
<equation confidence="0.759623222222222">
ES ENP [propn Robin]]
EVP [v likes]
[NE) (as above) F]]
After reduction with &amp;quot;child&amp;quot;, we
will have
ES ENP [propn Robin]]
EVP [v likes]
ENP ED (art the]]
[n child]]9]
</equation>
<bodyText confidence="0.95091175">
The last reduction will have com-
pleted the marked NP introduced
above, so we ND left-recursively raise
in place, giving
</bodyText>
<equation confidence="0.4173158">
E s ENP propn Robin]]
[VP [v likes]
[NE) [D [NE) the child]
• &apos;s]
nix]]
</equation>
<bodyText confidence="0.988676583333334">
which will then take us through the
rest of the sentence.
One final detail needs to be cleared
up. Although directly left-recursive
rules, such as e.g. NP --&gt; NP PP, are
correctly dealt with by the above
mechanism, indirectly left-recursive
sets of rules, such as the one exempli-
fied above, require one additional sub-
tlety. Care must be taken not to intro-
duce the potential for spurious ambi-
guity. We will introduce the full de-
tails in the next section.
11.3 Nature of the required tables
Steps 3 and 4h of CR-I require tables
of partial structures: Closures of unit
productions up from pre-terminals,
for step 3; left-reachable raisings up
from (unit production closures of) pre-
terminals, for step 4b. In this section
we discuss the creation of the neces-
sary tables, in particular Raise*,
against the background of a simple
exemplary grammar, given below as
</bodyText>
<tableCaption confidence="0.756068">
Table 1.
We have grouped the rules accord-
ing to type—two kinds of unit produc-
tions (from pre-terminals or non-ter-
minals), two kinds of left recursive
rules (direct and indirect) and the re-
mainder.
</tableCaption>
<table confidence="0.9511788">
vanilla unitl unit2 lrd lri
S NP VP NP ---&gt; propn NP --&gt; CMP NP NP PP NP D n
VP v NP D art VP -4 VP PP D -4 NP &apos;s
CMP cmp S
PP prep NP
</table>
<tableCaption confidence="0.999914">
Table 1. Exemplary grammar in groups by rule type
</tableCaption>
<page confidence="0.898513">
90
</page>
<table confidence="0.9947252">
c1* [NP propn]1,2 [D art)4 [Np cmpj1,2
LRdir 1: [NP NP PP] 3: [vp VP PP]
LRindir2 2: INP ED NP &apos;s] n] 4: 1D [Np D n)1 &apos;s]
Rs* [cmp cmp S], [Np [Cmp cmp s]]1,2,
[D [NP [CMP crilP Sl 1,2 is],
[S [NP [CMP cn1P S]]1.2 VP]
[pp prep NP]
[vp v NI)] 3
[Np D ni1,2, [s [tip D n]1,2 VP]
ED Npl Isi4, Es Np1,2 VP]
</table>
<tableCaption confidence="0.971853">
Table 2. Partial structures for CR-I
</tableCaption>
<table confidence="0.973322545454546">
Ras* [NP propnl • ppi1,2, rNP rd1,2
[D [NP artl [D _LEE propnl • &apos;s]
[CMP cmp • S], • /IP- &apos;srl
[NP (alp cm • S]]&amp;quot;2,
[D [NP ECMp cmp • Sil 1&apos;2 &apos;sli
[s [NP [CMP cmp • S] ]lf 2 VP]
[pp prep • NP]
[vp v • NP]3
n] 1,2, rs n)1,2 VP]
[NP _LD artl [Np artl
[D INp propnll • &apos;s]4, [s 1Bpropn11,2 • VP]
</table>
<tableCaption confidence="0.999249">
Table 3. Projecting non-terminal left daughters
</tableCaption>
<bodyText confidence="0.997688277777778">
As a first step towards computing
the table which step 4b above would
use, we can pre-compute the partial
structures given above in Table 2.
Cl* contains all backbone frag-
ments constructable from the unit
productions, and is already essentially
what we require for step 3 of the algo-
rithm. LRdir contains all directly left-
recursive structures. LRindir2 con-
tains all indirectly left-recursive struc-
tures involving exactly two rules, and
there might be LPindir3, 4, ... as
well. Rs* contains all non-recursive
tree fragments constructable from left-
embedding of binary or greater rules
and non-terminal unit productions.
The superscripts denote loci where
left-recursion may be appropriate, and
identify the relevant structures.
In order to get the full Raise* table
needed for step 4b, first we need to pro-
ject the non-terminal left daughters of
rules such as [ s NP1,2 VP] down to
terminal left daughters. We achieve
this by substituting terminal entries
from ci* wherever we can in LRd
LRindir2 and Rs* to give us Table 3
from Table 2 (new embeddings are
underlined).
Left recursion has one remaining
problem for us. Algorithm CR-I only
checks for annotations and ND raises
in place after a reduction completes a
constituent. But in the last line of
Ras* above there are unit constituents
</bodyText>
<page confidence="0.991953">
91
</page>
<table confidence="0.947794222222222">
[NP [NP propn] • 1p]1,21 rNP [D &apos;NP propn] • &apos;s] n]1,2
[D [NE) [D art] , 150
[CMP crnP • S] , [tua [cmp cmp • sm 1,2,
[D [NP [cmp cmp • S]]1&apos;2 s]
[s [ NP [ CMP CMP • S]] 2 VP]
n ] 1, 2 , [s [tip (D art] • n]1,2 VP]
• &apos; s] 4, (D (Np (NE, propn] • PPP- vsi4
• VP], [ s &apos;NP [NP propn] • PP ] 1 r2 VP] ,
[s [NP [D [NP propn] • &apos;s] n]1,2 VP]
</table>
<tableCaption confidence="0.842431">
Table 4. Final form of the structure table Raise*
</tableCaption>
<table confidence="0.9847322">
[pp Prep • NP]
[VP v. • NP13
[NE) [D art] •
[D [NP propn]
[NP propn]
</table>
<bodyText confidence="0.997193661290323">
with annotations. Being already com-
plete, they will not ever be completed,
and consequently the annotations will
never be checked. So we pre-compute
the desired result, augmenting the
above list with expansions of those
units via the indicated left recursions.
This gives us the final version of
Raise*, now shown with dots in-
cluded, in Table 4.
This table is now suited to its role
in the algorithm. Every entry has a
lexical left daughter, all annotated
constituents are incomplete, and all
unit productions are factored in. It is
interesting to note that with these tree
fragments, taken together with the
terminal entries in cit, as the initial
trees and LRdir, LEtindir2, etc. as the
auxiliary trees we have a Tree
Adjoining Grammar (Joshi 1985)
which is strongly equivalent to the CF-
PSG we started with. We might call it
the left-lexical TAG for that CF-PSG,
after Schabes et al. (1988). Note fur-
ther that if a TAG parser respected the
annotations as restricting adjunction,
no spuriously ambiguous parses
would be produced.
Indeed it was via this relationship
with TAGs that the details were
worked out of how the annotations are
distributed, not presented here to con-
serve space.
11.4 Implementation and Efficiency
Only a serial pseudo-parallel im-
plementation has been written.
Because of the high degree of pre-
computation of structure, this version
even though serialised runs quite effi-
ciently. There is very little computa-
tion at each step, as it is straight-for-
ward to double index the Raise* table
so that only structures which will
compose with the current structure
are retrieved.
The price one pays for this effi-
ciency, whether in serial or parallel
versions, is that only left-common
structure is shared. Right-common
structure, as for instance in PP at-
tachment ambiguity, is not shared be-
tween analysis paths. This causes no
difficulties for the parallel approach in
one sense, in that it does not compro-
mise the real-time performance of the
parser. Indeed, it is precisely because
no recombination is attempted that the
basic parsing step is constant time.
But it does mean that if the CF-PSG be-
ing parsed is the first half of a two step
process, in which additional con-
</bodyText>
<page confidence="0.990806">
92
</page>
<bodyText confidence="0.999889">
straints are solved in the second pass,
then the duplication of structure will
give rise to duplication of effort. Any
parallel parser which adopts the
strategy of forking at non-determinis-
tic choice points will suffer from this
weakness, including CR-II below.
</bodyText>
<sectionHeader confidence="0.82651" genericHeader="method">
III. THE SECOND COMPOSE-REDUCE
PARSER-CR-II
</sectionHeader>
<bodyText confidence="0.998339272727273">
Our second approach to compose-
reduce parsing differs from the first in
retaining a stack, having a more com-
plex basic parsing step, while requir-
ing far less pre-processing of the
grammar. In particular, no special
treatment is required for left-recursive
rules. Nevertheless, the basic step is
still constant time, and despite the
stack there is no potential processing
&apos;balloon&apos; at the end of the input.
</bodyText>
<figure confidence="0.862211666666667">
111.1 The Basic Parsing Algorithm
Algorithm CR-II
1 Shift the next word;
</figure>
<bodyText confidence="0.917947777777778">
2 ND look it up in the lexicon;
3 ND close the resulting cate-
gory wrt the unit produc-
tions;
4 ND reduce the resulting cat-
egory with the top of the
stack—if results are com-
plete and there is input re-
maining, pop the stack;
</bodyText>
<footnote confidence="0.688130222222222">
5a ND raise the results of (2),
(3) and, where complete, (4)
and
5b ND either push the result
onto the stack
or
5c ND compose the result with
the top of the stack, replac-
ing it.
</footnote>
<bodyText confidence="0.984541129032258">
This is not an easy algorithm to
understand. In the next section we
present a number of different ways of
motivating it, together with an illus-
trative example.
111.2 CR-II Explained
Let us first consider how CR-II will
operate on purely left-branching and
purely right-branching structures. In
each case we will consider the se-
quence of algorithm steps along the
non-deterministically correct path,
ignoring the others. We will also re-
strict ourselves to considering binary
branching rules, as pre-terminal unit
productions are handled entirely by
step 3 of the algorithm, and non-ter-
minal unit productions must be fac-
tored into the grammar. On the other
hand, interior daughters of non-bi-
nary nodes are all handled by step 4
without changing the depth of the
stack.
111.2.1 Left-branching analysis
For a purely left-branching struc-
ture, the first word will be processed
by steps 1, 2, 5a and 5b, producing a
stack with one entry which we can
schematise as in Figure 1, where
filled circles are processed nodes and
unfilled ones are waiting.
</bodyText>
<figureCaption confidence="0.663158">
Figure 1.
</figureCaption>
<bodyText confidence="0.999931857142857">
All subsequent words except the
last will be processed by steps 4, 5a and
5b (here and subsequently we will not
mention steps 1 and 2, which occur for
all words), effectively replacing the
previous sole entry in the stack with
the one given in Figure 2.
</bodyText>
<page confidence="0.994777">
93
</page>
<figure confidence="0.982383">
A A
</figure>
<figureCaption confidence="0.999704">
Figure 2.
</figureCaption>
<bodyText confidence="0.9999116">
It should be evident that the cycle of
steps 4, 5a and 5b constructs a left-
branching structure of increasing
depth as the sole stack entry, with one
right daughter, of the top node, wait-
ing to be filled. The last input word of
course is simply processed by step 4
and, as there is no further input, left
on the stack as the final result. The
complete sequence of steps for any left-
branching analysis is thus raise—re-
duce&amp;raise*—reduce. An ordinary
shift-reduce or left-corner parser
would go through the same sequence
of steps.
</bodyText>
<subsectionHeader confidence="0.691058">
111.2.2 Right-branching analysis
</subsectionHeader>
<bodyText confidence="0.929521">
The first word of a purely right-
branching structure is analysed ex-
actly as for a left-branching one, that
is, with 5a and 5b, with results as in
</bodyText>
<figureCaption confidence="0.9106205">
Figure 1 (repeated here as Figure 3):
Figure 3.
</figureCaption>
<bodyText confidence="0.992130780487805">
Subsequent words, except the last,
are processed via steps 5a and 5c, with
the result remaining as the sole stack
entry, as in Figure 4.
Figure 4.
Again it should be evident that cy-
cling steps 5a and 5c will construct a
right-branching structure of increas-
ing depth as the sole stack entry, with
one right daughter, of the most em-
bedded node, waiting to be filled.
Again, the last input word will be pro-
cessed by step 4. The complete se-
quence of steps for any right-branch-
ing analysis is thus raise—
raise&amp;compose*—reduce. A catego-
rial grammar parser with a compose-
first strategy would go through an
isomorphic sequence of steps.
111.2.3 Mixed Left- and Right-branch-
ing Analysis
All the steps in algorithm CR-II
have now been illustrated, but we have
yet to see the stack grow beyond one
entry. This will occur in where an in-
dividual word, as opposed to a com-
pleted complex constituent, is pro-
cessed by steps 5a and 5b, that is,
where steps 5a and 5b apply other than
to the results of step 4.
Consider for instance the sentence
&amp;quot;the child believes that the dog likes
biscuits.&amp;quot; With a grammar which I
trust will be obvious, we would arrive
at the structure shown in Figure 5
after processing &amp;quot;the child believes
that&amp;quot;, having done raise—reduce&amp;
rai se—raise &amp;compo se—
raise&amp;compose, that is, a bit of left-
branching analysis, followed by a bit of
right-branching analysis.
</bodyText>
<page confidence="0.996735">
94
</page>
<figure confidence="0.71724">
the child believes that
</figure>
<figureCaption confidence="0.98972">
Figure 5.
</figureCaption>
<bodyText confidence="0.946456333333333">
Clearly there is nothing to be done
with &amp;quot;the&amp;quot; which will allow immediate
integration with this. The ND correct
path applies steps 5a and 5b1
raise&amp;push, giving a stack as shown
in Figure 6:
</bodyText>
<figure confidence="0.947331333333333">
NP
the
the child believes that
</figure>
<figureCaption confidence="0.974919">
Figure 6.
</figureCaption>
<bodyText confidence="0.999198333333333">
We can then apply steps 4, 5a and
5c, reduce&amp;raise&amp;compose, to &amp;quot;dog&amp;quot;,
with the result shown in Figure 7.
This puts uss back on the standard
right-branching path for the rest of the
sentence.
</bodyText>
<figure confidence="0.370606">
the dog
</figure>
<figureCaption confidence="0.960572">
Figure 7.
</figureCaption>
<bodyText confidence="0.980264172413793">
111.3 An Alternative View of CR-II
Returning to a question raised ear-
lier, we can now see how a chart
parser could be modified in order to
run in real-time given enough proces-
sors to empty the agenda as fast as it is
filled. We can reproduce the process-
ing of CR-II within the active chart
parsing framework by two modifica-
tions to the fundamental rule (see e.g.
Gazdar and Mellish 1989 or Thompson
and Ritchie 1984 for a tutorial intro-
duction to active chart parsing). First
we restrict its normal operation, in
which an active and an inactive edge
are combined, to apply only in the case
of pre-terminal inactive edges. This
corresponds to the fact that in CR-II
step 4, the reduction step, applies only
to pre-terminal categories (continuing
to ignore unit productions). Secondly
we allow the fundamental rule to
combine two active edges, provided the
category to be produced by one is what
is required by the other. This effects
composition. If we now run our chart
parser left-to-right, left-corner and
breadth-first, it will duplicate CR-II.
the child believes that
</bodyText>
<page confidence="0.995833">
95
</page>
<bodyText confidence="0.999761255319149">
The maximum number of edges along
a given analysis path which can be in-
troduced by the processing of a single
word is now at most four, correspond-
ing to steps 2, 4, 5a and 5c of CR-II—the
pre-terminal itself, a constituent com-
pleted by it, an active edge containing
that constituent as left daughter, cre-
ated by left-corner rule invocation, and
a further active edge combining that
one with one to its left. This in turn
means that there is a fixed limit to the
amount of processing required for
each word.
111.4 Implementation and Efficiency
Although clearly not benefiting
from as much pre-computation of
structure as CR-I, CR-II is also quite ef-
ficient. Two modifications can be
added to improve efficiency—a reach-
ability filter on step 5b, and a shaper
test (Kuno 1965), also on 5b. For the
latter, we need simply keep a count of
the number of open nodes on the stack
(equal to the number of stack entries if
all rules are binary), and ensure that
this number never exceeds the num-
ber of words remaining in the input,
as each entry will require a number of
words equal to the number of its open
nodes to pop it off the stack. This test
actually cuts down the number of non-
deterministic paths quite dramati-
cally, as the ND optionality of step 5b
means that quite deep stacks would
otherwise be pursued along some
search paths. Again this reduction in
search space is of limited significance
in a true parallel implementation, but
in the serial simulation it makes a big
difference.
Note also that no attention has been
paid to unit productions, which we
pre-compute as in CR-I. Furthermore,
neither CR-I nor CR-II address empty
productions, whose effect would also
need to be pre-computed.
</bodyText>
<sectionHeader confidence="0.623446" genericHeader="method">
N. CONCLUSIONS
</sectionHeader>
<bodyText confidence="0.999969571428572">
Aside from the intrinsic interest in
the abstract of real-time parsablility, is
there any practical significance to
these results. Two drawbacks, one al-
ready referred to, certainly restrict
their significance. One is that the re-
striction to atomic category CF-PSGs is
crucial—the fact that the comparison
between a rule element and a node la-
bel is atomic and constant time is fun-
damental. Any move to features or
other annotations would put an end to
real-time processing. This fact gives
added weight to the problem men-
tioned above in section 11.4, that only
left-common analysis results are
shared between alternatives. Thus if
one finesses the atomic category prob-
lem by using a parser such as those
described here only as the first pass of
a two pass system, one is only putting
off the payment of the complexity price
to the second pass, in the absence to
date of any linear time solution to the
constraint satisfaction problem. On
this basis, one would clearly prefer a
parallel CKY/Earley algorithm, which
does share all common substructure,
to the parsers presented here.
Nevertheless, there is one class of
applications where the left-to-right
real-time behaviour of these algo-
rithms may be of practical benefit,
namely in speech recognition.
Present day systems require on-line
availability of syntactic and domain-
semantic constraint to limit the
search space at lower levels of the sys-
tem. Hitherto this has meant these
constraints must be brought to bear
during recognition as some form of
regular grammar, either explicitly
</bodyText>
<page confidence="0.960726">
96
</page>
<bodyText confidence="0.999910454545455">
constructed as such or compiled into.
The parsers presented here offer the
alternative of parallel application of
genuinely context-free grammars di-
rectly, with the potential added benefit
that, with sufficient processor width,
quite high degrees of local ambiguity
can be tolerated, such as would arise if
(a finite subset of) a feature-based
grammar were expanded out into
atomic category form.
</bodyText>
<sectionHeader confidence="0.995546" genericHeader="acknowledgments">
ACKNOWLEDGEMENTS
</sectionHeader>
<bodyText confidence="0.9999766">
The work reported here was car-
ried out while the first author was a
visitor to the Embedded Computation
and Natural Language Theory and
Technology groups of the Systems
Science Laboratory at the Xerox Palo
Alto Research Center. These groups
provided both the intellectual and ma-
terial resources required to support
our work, for which our thanks.
</bodyText>
<sectionHeader confidence="0.998725" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999110441176471">
Dixon, Mike and de Kleer, Johan.
1988. &amp;quot;Massively Parallel
Assumption-based Truth
Maintenance&amp;quot;. In Proceedings of
the AAAI-88 National Conference
on Artificial Intelligence, also
reprinted in Proceedings of the
Second International Workshop on
Non-Monotonic Reasoning.
Gazdar, Gerald and Mellish, Chris.
1989. Natural Language
Processing in LISP. Addison-
Wesley, Wokingham, England
(sic).
Joshi, Aravind K. 1985. &amp;quot;How Much
Context-Sensitivity is Necessary for
Characterizing Structural
Descriptions—Tree Adjoining
Grammars&amp;quot;. In Dowty, D.,
Karttunen, L., and Zwicky, A. eds,
Natural Language Processing—
Theoretical Computational and
Psychological Perspectives.
Cambridge University Press, New
York.
Kuno, Susumo 1965. &amp;quot;The predictive
analyzer and a path elimination
technique&amp;quot;, Communications of the
ACM, 8, 687-698.
Nijholt, Anton. 1989. &amp;quot;Parallel
parsing strategies in natural
language processing&amp;quot;. In Tomita,
M. ed, Proceedings of the
International Workshop on
Parsing Technologies, 240-253,
Carnegie-Mellon University,
Pittsburgh.
Nijholt, Anton. 1990. The CYK-
Approach to Serial and Parallel
Parsing. Memoranda Informatica
90-13, faculteit der informatica,
Universiteit Twente, Netherlands.
Shabes, Yves, Abeille, Anne and
Joshi, Aravind K. 1988. &amp;quot;Parsing
Strategies with `Lexicalized&apos;
Grammars: Application to Tree
Adjoining Grammars&amp;quot;. In
Proceedings of the 12th
International Conference on
Computational Linguistics, 82-93.
Thompson, Henry S. 1991. &amp;quot;Parallel
Parsers for Context-Free
Grammars—Two Actual
Implementations Compared&amp;quot;. To
appear in Adriaens, G. and Hahn,
U. eds, Parallel Models of Natural
Language Computation, Ablex,
Norword NJ.
Thompson, Henry S. and Ritchie,
Graeme D. 1984. &amp;quot;Techniques for
Parsing Natural Language: Two
Examples&amp;quot;. In Eisenstadt, M., and
O&apos;Shea, T., editors, Artificial
Intelligence: Tools, Techniques,
and Applications. Harper and
Row, London. Also DAT Research
Paper 183, Dept. of Artificial
Intelligence, Univ. of Edinburgh.
</reference>
<page confidence="0.999694">
97
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.062435">
<note confidence="0.518">S.</note>
<affiliation confidence="0.835059">1: Human Communication Research Centre University of Edinburgh</affiliation>
<address confidence="0.6116815">2 Buccleuch Place 9LW</address>
<affiliation confidence="0.925964">SCOTLAND Alto Research Center</affiliation>
<address confidence="0.999206">3333 Coyote Hill Road Palo Alto, CA 94304</address>
<abstract confidence="0.997771166666667">achieve linear (indeed real-time) complexity by performing a constant-time step per word of the input. The other took as its starting point tabular pars- (Earley, and sought to achieve linear complexity by performing a constant-time step for the identification/construction of constituents of each length from 0 to n. The latter route has been widely canvassed, although to our knowledge has not yet been implemented—see (Nijholt 1989, 90) for extensive references. The former route, whereby real-time parsing is achieved by processor forking at non-deterministic choice points in an extended shift-reduce parser, is to our knowledge new. In this paper we present outlines of two such parsers, we call parsers. L COMPOSE-REDUCE PARSING Why couldn&apos;t a simple breadthfirst chart parser achieve linear performance on an appropriate parallel system? If you provided enough processors to immediately process all agenda entries as they were created, not this give the result? No, because the processing of a single word might require many serialised ABSTRACT Two new parsing algorithms for context-free phrase structure grammars are presented which perform a bounded amount of processing per word per analysis path, independently sentence length. are thus caof parsing in real-time a parwhich proresponse to non-deterministic choice points.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mike Dixon</author>
<author>Johan de Kleer</author>
</authors>
<title>Massively Parallel Assumption-based Truth Maintenance&amp;quot;.</title>
<date>1988</date>
<booktitle>In Proceedings of the AAAI-88 National Conference on Artificial Intelligence, also reprinted in Proceedings of the Second International Workshop on Non-Monotonic Reasoning.</booktitle>
<marker>Dixon, de Kleer, 1988</marker>
<rawString>Dixon, Mike and de Kleer, Johan. 1988. &amp;quot;Massively Parallel Assumption-based Truth Maintenance&amp;quot;. In Proceedings of the AAAI-88 National Conference on Artificial Intelligence, also reprinted in Proceedings of the Second International Workshop on Non-Monotonic Reasoning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
<author>Chris Mellish</author>
</authors>
<date>1989</date>
<booktitle>Natural Language Processing in LISP.</booktitle>
<publisher>AddisonWesley,</publisher>
<location>Wokingham, England (sic).</location>
<contexts>
<context position="20293" citStr="Gazdar and Mellish 1989" startWordPosition="3617" endWordPosition="3620">ild believes that Figure 6. We can then apply steps 4, 5a and 5c, reduce&amp;raise&amp;compose, to &amp;quot;dog&amp;quot;, with the result shown in Figure 7. This puts uss back on the standard right-branching path for the rest of the sentence. the dog Figure 7. 111.3 An Alternative View of CR-II Returning to a question raised earlier, we can now see how a chart parser could be modified in order to run in real-time given enough processors to empty the agenda as fast as it is filled. We can reproduce the processing of CR-II within the active chart parsing framework by two modifications to the fundamental rule (see e.g. Gazdar and Mellish 1989 or Thompson and Ritchie 1984 for a tutorial introduction to active chart parsing). First we restrict its normal operation, in which an active and an inactive edge are combined, to apply only in the case of pre-terminal inactive edges. This corresponds to the fact that in CR-II step 4, the reduction step, applies only to pre-terminal categories (continuing to ignore unit productions). Secondly we allow the fundamental rule to combine two active edges, provided the category to be produced by one is what is required by the other. This effects composition. If we now run our chart parser left-to-r</context>
</contexts>
<marker>Gazdar, Mellish, 1989</marker>
<rawString>Gazdar, Gerald and Mellish, Chris. 1989. Natural Language Processing in LISP. AddisonWesley, Wokingham, England (sic).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>How Much Context-Sensitivity is Necessary for Characterizing Structural Descriptions—Tree Adjoining Grammars&amp;quot;. In</title>
<date>1985</date>
<booktitle>Natural Language Processing— Theoretical Computational and Psychological Perspectives.</booktitle>
<contexts>
<context position="13352" citStr="Joshi 1985" startWordPosition="2407" endWordPosition="2408">e-compute the desired result, augmenting the above list with expansions of those units via the indicated left recursions. This gives us the final version of Raise*, now shown with dots included, in Table 4. This table is now suited to its role in the algorithm. Every entry has a lexical left daughter, all annotated constituents are incomplete, and all unit productions are factored in. It is interesting to note that with these tree fragments, taken together with the terminal entries in cit, as the initial trees and LRdir, LEtindir2, etc. as the auxiliary trees we have a Tree Adjoining Grammar (Joshi 1985) which is strongly equivalent to the CFPSG we started with. We might call it the left-lexical TAG for that CF-PSG, after Schabes et al. (1988). Note further that if a TAG parser respected the annotations as restricting adjunction, no spuriously ambiguous parses would be produced. Indeed it was via this relationship with TAGs that the details were worked out of how the annotations are distributed, not presented here to conserve space. 11.4 Implementation and Efficiency Only a serial pseudo-parallel implementation has been written. Because of the high degree of precomputation of structure, this </context>
</contexts>
<marker>Joshi, 1985</marker>
<rawString>Joshi, Aravind K. 1985. &amp;quot;How Much Context-Sensitivity is Necessary for Characterizing Structural Descriptions—Tree Adjoining Grammars&amp;quot;. In Dowty, D., Karttunen, L., and Zwicky, A. eds, Natural Language Processing— Theoretical Computational and Psychological Perspectives.</rawString>
</citation>
<citation valid="false">
<publisher>Cambridge University Press,</publisher>
<location>New York.</location>
<marker></marker>
<rawString>Cambridge University Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susumo Kuno</author>
</authors>
<title>The predictive analyzer and a path elimination technique&amp;quot;,</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<volume>8</volume>
<pages>687--698</pages>
<contexts>
<context position="21755" citStr="Kuno 1965" startWordPosition="3866" endWordPosition="3867">, 5a and 5c of CR-II—the pre-terminal itself, a constituent completed by it, an active edge containing that constituent as left daughter, created by left-corner rule invocation, and a further active edge combining that one with one to its left. This in turn means that there is a fixed limit to the amount of processing required for each word. 111.4 Implementation and Efficiency Although clearly not benefiting from as much pre-computation of structure as CR-I, CR-II is also quite efficient. Two modifications can be added to improve efficiency—a reachability filter on step 5b, and a shaper test (Kuno 1965), also on 5b. For the latter, we need simply keep a count of the number of open nodes on the stack (equal to the number of stack entries if all rules are binary), and ensure that this number never exceeds the number of words remaining in the input, as each entry will require a number of words equal to the number of its open nodes to pop it off the stack. This test actually cuts down the number of nondeterministic paths quite dramatically, as the ND optionality of step 5b means that quite deep stacks would otherwise be pursued along some search paths. Again this reduction in search space is of </context>
</contexts>
<marker>Kuno, 1965</marker>
<rawString>Kuno, Susumo 1965. &amp;quot;The predictive analyzer and a path elimination technique&amp;quot;, Communications of the ACM, 8, 687-698.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anton Nijholt</author>
</authors>
<title>Parallel parsing strategies in natural language processing&amp;quot;.</title>
<date>1989</date>
<booktitle>In Tomita, M. ed, Proceedings of the International Workshop on Parsing Technologies,</booktitle>
<pages>240--253</pages>
<institution>Carnegie-Mellon University,</institution>
<location>Pittsburgh.</location>
<contexts>
<context position="696" citStr="Nijholt 1989" startWordPosition="108" endWordPosition="109">cation Research Centre University of Edinburgh 2 Buccleuch Place Edinburgh EH8 9LW SCOTLAND 2: Xerox Palo Alto Research Center 3333 Coyote Hill Road Palo Alto, CA 94304 achieve linear (indeed real-time) complexity by performing a constant-time step per word of the input. The other took as its starting point tabular parsing (Earley, C KY), and sought to achieve linear complexity by performing a constant-time step for the identification/construction of constituents of each length from 0 to n. The latter route has been widely canvassed, although to our knowledge has not yet been implemented—see (Nijholt 1989, 90) for extensive references. The former route, whereby real-time parsing is achieved by processor forking at non-deterministic choice points in an extended shift-reduce parser, is to our knowledge new. In this paper we present outlines of two such parsers, which we call compose-reduce parsers. L COMPOSE-REDUCE PARSING Why couldn&apos;t a simple breadthfirst chart parser achieve linear performance on an appropriate parallel system? If you provided enough processors to immediately process all agenda entries as they were created, would not this give the desired result? No, because the processing of</context>
</contexts>
<marker>Nijholt, 1989</marker>
<rawString>Nijholt, Anton. 1989. &amp;quot;Parallel parsing strategies in natural language processing&amp;quot;. In Tomita, M. ed, Proceedings of the International Workshop on Parsing Technologies, 240-253, Carnegie-Mellon University, Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anton Nijholt</author>
</authors>
<title>The CYKApproach to Serial and Parallel Parsing. Memoranda Informatica 90-13, faculteit der informatica, Universiteit Twente,</title>
<date>1990</date>
<location>Netherlands.</location>
<marker>Nijholt, 1990</marker>
<rawString>Nijholt, Anton. 1990. The CYKApproach to Serial and Parallel Parsing. Memoranda Informatica 90-13, faculteit der informatica, Universiteit Twente, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Shabes</author>
<author>Anne Abeille</author>
<author>Aravind K Joshi</author>
</authors>
<title>Parsing Strategies with `Lexicalized&apos; Grammars: Application to Tree Adjoining Grammars&amp;quot;.</title>
<date>1988</date>
<booktitle>In Proceedings of the 12th International Conference on Computational Linguistics,</booktitle>
<pages>82--93</pages>
<marker>Shabes, Abeille, Joshi, 1988</marker>
<rawString>Shabes, Yves, Abeille, Anne and Joshi, Aravind K. 1988. &amp;quot;Parsing Strategies with `Lexicalized&apos; Grammars: Application to Tree Adjoining Grammars&amp;quot;. In Proceedings of the 12th International Conference on Computational Linguistics, 82-93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry S Thompson</author>
</authors>
<title>Parallel Parsers for Context-Free Grammars—Two Actual Implementations Compared&amp;quot;.</title>
<date>1991</date>
<journal>Parallel Models of Natural Language Computation, Ablex, Norword NJ.</journal>
<note>To appear in</note>
<contexts>
<context position="1843" citStr="Thompson 1991" startWordPosition="290" endWordPosition="291">uld not this give the desired result? No, because the processing of a single word might require many serialised ABSTRACT Two new parsing algorithms for context-free phrase structure grammars are presented which perform a bounded amount of processing per word per analysis path, independently of sentence length. They are thus capable of parsing in real-time in a parallel implementation which forks processors in response to non-deterministic choice points. 0. INTRODUCTION The work reported here grew out of our attempt to improve on the 0 (n2) performance of the SIMD parallel parser described in (Thompson 1991). Rather than start with a commitment to a specific SIMD architecture, as that work had, we agreed that the best place to start was with a more abstract architecture-independent consideration of the CF-PSG parsing problem— given arbitrary resources, what algorithms could one envisage which could recognise and/or parse atomic category phrase-structure grammars in c) (n) ? In the end, two quite different approaches emerged. One took as its starting point non-deterministic shift-reduce parsing, and sought to 87 steps. Consider processing the word &amp;quot;park&amp;quot; in the sentence &amp;quot;The people who ran in the </context>
</contexts>
<marker>Thompson, 1991</marker>
<rawString>Thompson, Henry S. 1991. &amp;quot;Parallel Parsers for Context-Free Grammars—Two Actual Implementations Compared&amp;quot;. To appear in Adriaens, G. and Hahn, U. eds, Parallel Models of Natural Language Computation, Ablex, Norword NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry S Thompson</author>
<author>Graeme D Ritchie</author>
</authors>
<title>Techniques for Parsing Natural Language: Two Examples&amp;quot;.</title>
<date>1984</date>
<booktitle>Artificial Intelligence: Tools, Techniques, and Applications. Harper and Row, London. Also DAT Research Paper 183, Dept. of Artificial Intelligence, Univ. of Edinburgh.</booktitle>
<editor>In Eisenstadt, M., and O&apos;Shea, T., editors,</editor>
<contexts>
<context position="20322" citStr="Thompson and Ritchie 1984" startWordPosition="3622" endWordPosition="3625"> We can then apply steps 4, 5a and 5c, reduce&amp;raise&amp;compose, to &amp;quot;dog&amp;quot;, with the result shown in Figure 7. This puts uss back on the standard right-branching path for the rest of the sentence. the dog Figure 7. 111.3 An Alternative View of CR-II Returning to a question raised earlier, we can now see how a chart parser could be modified in order to run in real-time given enough processors to empty the agenda as fast as it is filled. We can reproduce the processing of CR-II within the active chart parsing framework by two modifications to the fundamental rule (see e.g. Gazdar and Mellish 1989 or Thompson and Ritchie 1984 for a tutorial introduction to active chart parsing). First we restrict its normal operation, in which an active and an inactive edge are combined, to apply only in the case of pre-terminal inactive edges. This corresponds to the fact that in CR-II step 4, the reduction step, applies only to pre-terminal categories (continuing to ignore unit productions). Secondly we allow the fundamental rule to combine two active edges, provided the category to be produced by one is what is required by the other. This effects composition. If we now run our chart parser left-to-right, left-corner and breadth</context>
</contexts>
<marker>Thompson, Ritchie, 1984</marker>
<rawString>Thompson, Henry S. and Ritchie, Graeme D. 1984. &amp;quot;Techniques for Parsing Natural Language: Two Examples&amp;quot;. In Eisenstadt, M., and O&apos;Shea, T., editors, Artificial Intelligence: Tools, Techniques, and Applications. Harper and Row, London. Also DAT Research Paper 183, Dept. of Artificial Intelligence, Univ. of Edinburgh.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>