<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003029">
<sectionHeader confidence="0.655728" genericHeader="abstract">
LICENSING AND &apos;TREE ADJOINING GRAMMAR IN
GOVERNMENT BINDING PARSING
</sectionHeader>
<author confidence="0.877537">
Robert Frank*
</author>
<affiliation confidence="0.9576535">
Department of Computer and Information Sciences
University of Pennsylvania
</affiliation>
<address confidence="0.782553">
Philadelphia, PA 19104
</address>
<email confidence="0.999397">
email: frank@linc.cis.upenn.edu
</email>
<sectionHeader confidence="0.994759" genericHeader="introduction">
Abstract
</sectionHeader>
<bodyText confidence="0.999952333333333">
This paper presents an implemented, psychologically plau-
sible parsing model for Government Binding theory gram-
mars. I make use of two main ideas: (1) a generaliza-
tion of the licensing relations of [Abney, 1986] allows for
the direct encoding of certain principles of grammar (e.g.
Theta Criterion, Case Filter) which drive structure build-
ing; (2) the working space of the parser is constrained
to the domain determined by a Tree Adjoining Grammar
elementary tree. All dependencies and constraints are lo-
calized within this bounded structure. The resultant parser
operates in linear time and allows for incremental semantic
interpretation and determination of grammaticality.
</bodyText>
<sectionHeader confidence="0.960963" genericHeader="method">
I Introduction
</sectionHeader>
<bodyText confidence="0.999952714285714">
This paper aims to provide a psychologically plausible
mechanism for putting the knowledge which a speaker
has of the syntax of a language, the competence gram-
mar, to use. The representation of knowledge of language
I assume is that specified by Government Binding (GB)
Theory introduced in [Chomsky, 1981]. GB, as a com-
petence theory, emphatically does not specify the nature
of the language processing mechanism. In fact, &amp;quot;proofs&amp;quot;
that transformational grammar is inadequate as a linguis-
tic theory due to various performance measures are funda-
mentally flawed since they suppose a particular connection
between the grammar and parser [Berwick and Weinberg,
1984]. Nonetheless, it seems desirable to maintain a fairly
direct connection between the linguistic competence and
</bodyText>
<note confidence="0.9922895">
*I would like to thank the following for their valuable discussion and
suggestions: Naoki Fukui, Jamie Henderson, Aravind Joshi, Tony Kroch,
Mitch Marcus, Michael Niv, Yves Schabes, Mark Steedman, Enric Vall-
duvi. This work was partially supported by ARO Grants DAAL03-89-
COOS1 PRI and DAAG29-84-K-0061 and DARPA grant N00014-85-K-
0018. The author is supported by a Unisys doctoral fellowship.
</note>
<bodyText confidence="0.999866864864865">
its processing. Otherwise, claims of the psychological re-
ality of this particular conception of competence become
essentially vacuous since they cannot be falsified but for
the data on which they are founded, i.e. grammaticality
judgments. Thus, in building a model of language pro-
cessing, I would like to posit as direct a link as is possible
between linguistic competence and the operations of the
parser while still maintaining certain desirable computa-
tional properties.
What are the computational properties necessary for
psychological plausibility? Since human syntactic pro-
cessing is an effortless process, we should expect that it
take place efficiently, perhaps in linear time since sen-
tences do not become more difficult to process simply
as a function of their length. Determinism, as proposed
by Marcus [1980], seems desirable as well. In addition,
the mechanism should operate in an incremental fashion.
Incrementality is evidenced in the human language pro-
cessor in two ways. As we hear a sentence, we build
up semantic representations without waiting until the sen-
tence is complete. Thus, the semantic processor should
have access to syntactic representations prior to an utter-
ance&apos;s completion. Additionally, we are able to perceive
ungrammaticality in sentences almost immediately after
the ill formedness occurs. Thus, our processing mecha-
nism should mimic this early detection of ungrammatical
input.
Unfortunately, a parser with the most transparent rela-
tionship to the grammar, a &amp;quot;parsing as theorem proving&amp;quot;
approach as proposed by [Johnson, 1988] and [Stabler,
1990], does not fare well with respect to our computa-
tional desiderata. It suffers from the legacy of the com-
putational properties of first order theorem proving, most
notably undecidability, and is thus inadequate for our pur-
poses. The question, then, is how much we must retreat
from this direct instantiation so that we can maintain the
requisite properties. In this paper, I attempt to provide
</bodyText>
<page confidence="0.997876">
111
</page>
<bodyText confidence="0.999976466666667">
an answer. I propose a parsing model which represents
the principles of the grammar in a fairly direct manner,
yet preserves efficiency and incrementality. The model
depends upon two key ideas. First, I utilize the insight
of [Abney, 1986] in the use of licensing relations as the
foundation for GB parsing. By generalizing Abney&apos;s for-
mulation of licensing. I can directly encode and enforce
a particular class of the principles of GB theory and in
so doing efficiently build phrase structure. The principles
expressible through licensing are not all of those posited
by GB. Thus, the others must be enforced using a different
mechanism. Unfortunately, the unbounded size of the tree
created with licensing makes any such mechanism compu-
tationally abhorrent In order to remedy this, I make use
of the Tree Adjoining Grammar (TAG) framework [Joshi,
1985] to limit the working space of the parser. As the
parser proceeds, its working structure is bounded in size.
If this bound is exceeded, we reduce this structure by one
of the operations provided by the TAG formalism, either
substitution or adjunction. This results in two structures,
each of which form independent elementary trees. Inter-
estingly, the domain of locality imposed by a TAG ele-
mentary tree appears to be sufficient for the expression of
the remaining grammatical principles. Thus, we can check
for the satisfaction of the remaining grammatical princi-
ples in just the excised piece of structure and then send it
off for semantic interpretation. Since this domain of con-
straint checking is bounded in size, this process is done
efficiently. This mechanism also works in an incremental
fashion.
</bodyText>
<sectionHeader confidence="0.982032" genericHeader="method">
2 Abney&apos;s Licensing
</sectionHeader>
<bodyText confidence="0.997660277777778">
Since many grammatical constraints are concerned with
the licensing of elements, Abney [1986] proposes utiliz-
ing licensing structure as a more concrete representation
for parsing. This allows for more efficient processing yet
maintains the spirit of the abstract grammar.&amp;quot;
Abney&apos;s notion of licensing requires that every element
in a structure be licensed by performing some syntac-
tic function. Any structure with unlicensed elements is
ill-formed. Abney takes theta role assignment to be the
canonical case of licensing and assumes that the properties
of a general licensing relation should mirror those of theta
assignment, namely, that it he unique, local and lexical.
The uniqueness property for theta assignment requires that
an argument receives one and only one theta role. Corre-
spondingly, licensing is unique: an element is licensed via
exactly one licensing relation. Locality demands that theta
assignment, and correspondingly licensing, take place un-
der a strict definition of government sisterhood. Finally,
</bodyText>
<figureCaption confidence="0.879404666666667">
Figure 1: Abney&apos;s Licensing Relations in Clausal Struc-
ture (S = subjecthood, F = functional selection, M mod-
ification, T = theta)
</figureCaption>
<bodyText confidence="0.959145810810811">
theta assignment is lexical in that it is the properties of the
the theta assigner which determine what theta assignment
relations obtain. Licensing will have the same property; it
is the licenser that determines how many and what sort of
elements it licenses.
Each licensing relation is a 3-tuple (D, Cat, Type). D is
the direction in which licensing occurs. Cat is the syntac-
tic category of the element licensed by this relation. Type
specifies the linguistic function accomplished by this li-
censing relation. This can be either functional selection,
subjecthood, modification or theta-assignment. Functional
selection is the relation which obtains between a func-
tional head and the element for which it subcategorizes,
i.e. between C and IP, I and VP, D and NP. Subjecthood
is the relation between a head and its &amp;quot;subject&amp;quot;. Modifica-
tion holds between a head and adjunct. Theta assignment
occurs between a head and its subcategorized elements.
Figure 1 gives an example of the licensing relations which
might obtain in a simple clause. Parsing with these li-
censing relations simply consists of determining, for each
lexical item as it is read in a single left to right pass,
where it is licensed in the previously constructed structure
or whether it licenses the previous structure.
We can now re-examine Abney&apos;s claim that these licens-
ing relations allow him to retain &amp;quot;the spirit of the abstract
grammar.&amp;quot; Since licensing relations talk only of very lo-
cal relationships, that occurring between sisters, this sys-
tem cannot enforce the constraints of binding, control, and
ECP among others. Abney notes this and suggests that his
licensing should be seen as a single module in a parsing
system. One would hope, though, that principles which
have their roots in licensing, such as those of theta and
case theory, could receive natural treatments. Unfortu-
nately, this is not true. Consider the theta criterion. While
this licensing system is able to encode the portion of the
constraint that requires theta roles to be assigned uniquely,
it fails to guarantee that all NPs (arguments) receive a theta
</bodyText>
<figure confidence="0.998702428571429">
NI) will VP
Pt.-5
John help
I
Mary
AdvP
tomorrow
</figure>
<page confidence="0.980379">
112
</page>
<bodyText confidence="0.995787480769231">
role. This is crucially not the case since NPs are some- all nodes be uniquely licensed. Rather, we demand that
times licensed not by theta but by subject licensing. Thus, all gives and needs be uniquely &amp;quot;satisfied.&amp;quot; The unique-
the following pair will be indistinguishable: ness requirement in Abney&apos;s relations is now pushed down
i. It seems that the pigeon is dead to the level of individual gives and needs. Once a give
* Joe seems that the pigeon is dead or need is satisfied, it may not participate in any other
Both ft and Joe will be appropriately licensed by a subject licensing relationships.
licensing relation associated with seems. The case filter One further generalization which I make concerns the
also cannot be expressed since objects of ECM verbs are positioning of gives and needs. In Abney&apos;s system, licens-
&amp;quot;licensed&amp;quot; by the lower clause as subject, yet also require ing relations are associated with lexical heads and applied
case. Thus, the following distinction cannot accounted for: to maximal projections of other heads. Phrase structure is
i. Carol asked Ben to swat the fly thus entirely parasitic upon the reconstruction of licensing
* Carol tried Ben to swat the fly structure. I propose to have an independent process of
Here, in order to get the desired syntactic structure (with lexical projection. A lexical item projects to the correct
Ben in the lower clause in both cases), Ben will need to number of levels in its maximal projection, as determined
be licensed by the inflectional element to. Since such a by theta structure, f-features, and other lexical properties.2
licensing relation need be unique, the case assigning prop- Gives and needs are assigned to each of these nodes. As
erties of the matrix verbs will be irrelevant What seems with Abney&apos;s system, licensing takes place under a strict
to have happened is that we have lost the modularity of the notion of government (sisterhood). However, the projec-
the syntactic relations constrained by grammatical princi- tion process allows licensing relations determined by a
ples. Everything has been conflated onto one homoge- head to take place over a somewhat larger domain than
neous licensing structure. sisterhood to the head. A DP&apos;s theta need resulting from
3 Generalized Licensing the theta criterion, for example, is present only at the max-
In order to remedy these deficiencies, I propose a system imal projection level. This is the node which stands in the
of Generalized Licensing. In this system, every node is appropriate structural relation to a theta give. As a re-
assigned two sets of licensing relations: gives and needs. sult of this projection process, though, we must explicitly
Gives are similar to the Abney&apos;s licensing relations: they represent structural relations during parsing.
are satisfied locally and determined lexically. Needs spec- The reader may have noticed that multiple needs on a
ify the ways in which a node must be licensed.1 A need node might not all be satisfiable in one structural position.
of type theta, for example, requires a node to be licensed Consider the case of a DP subject which possesses both
by a theta relation. In the current formulation, needs differ theta and case needs. The S-structure subject of the sen-
from gives in that they are always directionally unspeci- tence receives its theta role from the verb, yet it receives its
fied. We can now represent the theta criterion by placing case from the tense/agreement morpheme heading IP. This
theta gives on a theta assigner for each argument and theta is impossible, though, since given the structural correlate
needs on all DPs. This encodes both that all theta roles of the licensing relation, the DP would then be directly
must be assigned and that all arguments must receive theta dominated both by IP and by VP. Yet, it cannot be in ei-
roles. ther one of these positions alone, since we will then have
In Generalized Licensing, we allow a greater vocabu- unsatisfied needs and hence an ill-formed structure. Thus,
lary of relation types; case, theta assignment, modification, our representation of grammatical principles and the con-
functional selection, predication, f-features, etc. We can straints on give and need satisfaction force us into adopt-
then explicitly represent many of the relations which are ing a general notion of chain and more specifically the VP
posited in the grammar and preserve the modularity of the internal subject hypothesis. A chain consist of a list of
theory. As a result, however, certain elements can and nodes (al,. _A) such that they share gives and needs and
must be multiply licensed. DPs, for instance, will have each ai c-commands each ai+1. The first element in the
needs for both theta and case as a result of the case filter chain, al, the head, is the only element which can have
and theta criterion. We therefore relax the requirement that phonological content. Others must be empty categories.
Now, since the elements of the chain can occupy differ-
ent structural positions, they may be governed and hence
licensed by distinct elements. In the simple sentence:
Johni tris/agr Iv) ti smile]]
1These bear some similarity to the anti-relations of Abney, but are 21 assume the relativized X-bar theory proposed in [Fukui and Speas,
used in a rather different fashion. 19861.
113
the trace node which is an argument of smile forms a chain
with the DP Jahn. In its V&apos; internal position, the theta
need is satisfied by the theta give associated with the V.
In subject position, the case need is satisfied by the case
give on the I&apos; projection of the inflectional morphology.
Now, how might we parse using these licensing rela-
tions? Abney&apos;s method is not sufficient since a single
instance of licensing no longer guarantees that all of a
node&apos;s licensing constraints are satisfied. I propose a sim-
ple mechanism, which generalizes Abney&apos;s approach: We
proceed left to right, project the current input token to its
maximal projection p and add the associated gives and
needs to each of the nodes. These are determined by ex-
amination of information in the lexical entries (such as
using the theta grid to determine theta gives), examination
of language specific parameters (using head directionality
in order to determined directionality of gives, for exam-
ple), and consultation of UG parameters (for instance as a
result of the case filter, every DP maximal projection will
have an associated case need). The parser then attempts to
combine this projection with previously built structure in
one of two ways. We may attach p as the sister of a node
n on the right frontier of the developing structure, when
p is licensed by n either by a give in it and/or a need in
the node p. Another possibility is that the previously built
structure is attached as sister to a node, m, dominated by
the maximal projection p. by satisfying a give in in and/or
a need on the root of the previously built structure. In the
case of multiple attachment possibilities, we order them
according to some metric such as the one proposed by
Abney, and choose the most highly ranked option.
As structure is built, nodes in the tree with unsatisfied
gives and needs may become closed off from the right
frontier of the working structure. In such positions, they
will never become satisfied. In the case of a need in an
internal node it which is unsatisfied, we posit the existence
of an empty category in. which will be attached later to
the structure such that (n, m) form a chain. We posit an
element to have been moved into a position exactly when it
is licensed at that position yet its needs are not completely
satisfied. After positing the empty category, we push it
onto the trace stack. When a node has an unsatisfied give
and no longer has access to the right frontier, we must posit
some element, not phonologically represented in the input,
which satisfies that give relation. If there is an element on
the top of the trace stack which can satisfy this give, we
pop it off the stack and attach it.3 Of course, if the trace
has any remaining needs, it is returned to the trace stack
since its new position is isolated from the right frontier.
If no such element appears on top of the trace stack, we
&apos;Note that the use of this stack to recover filler-gap structures forbids
non-nested dependencies as in [Fodor, 19783.
</bodyText>
<figure confidence="0.408832">
ins/agr
</figure>
<figureCaption confidence="0.997742">
Figure 2: Working Space after &amp;quot;Harry tris/agr&amp;quot;
</figureCaption>
<bodyText confidence="0.977211522727273">
posit a non-trace empty category of the appropriate type,
if one exists in the language.4
Let&apos;s try this mechanism on the sentence &amp;quot;Harry
laughs.&amp;quot; The first token received is Harry and is projected
to DP. No gives are associated with this node, but theta
and case needs are inserted into the need set as a result
of the theta criterion and the case filter. Next, tnslagr is
read and projected to I&amp;quot;, since it possesses f-features (cf.
[Fulcui and Speas, 1986]). Associated with the I° node
is a rightward functional selection give of value V. On
the node is a leftward nominative case give, from the
f-features, and a leftward subject give, as a result of the
Extended Projection Principle. The previously constructed
DP is attached as sister to the V node, thereby satisfying
the subject and case gives of the V as well as the case need
of the DP. We are thus left with the structure in figure 2.5
Next, we see that the theta need of the DP is inaccessible
from the right frontier, so we push an empty category DP
whose need set contains this unsatisfied theta need onto
the trace stack. The next input token is the verb laugh.
This is projected to a single bar level. Since laugh assigns
an external theta role, we insert a leftward theta give to
a DP into the V&apos; node. This verbal projection is attached
as sister to 10, satisfying the functional selection give of
I. However, the theta give in V&apos; remains unsatisfied and
since it is leftward, is inaccessible. We therefore need to
posit an empty category. Since the DP trace on top of the
trace stack will accept this give, the trace stack is popped
and the trace is attached via Chomsky-adjunction to the
4Such a simplistic approach to determining whether a trace or non-
trace empty category should be inserted is clearly not correct. For in-
stance, in &amp;quot;tough movement&amp;quot;
Alvin, is tough PRO to feed t,
the proposed mechanism will insert the trace of Alvin in subject posi-
tion rather than PRO. It remains for future work to determine the exact
mechanism by which such decisions are made.
5In the examples which follow, gives are shown as 4-mples
(D , Type,V al, SatBy) where I) is the direction, Type is the type
of licensing relation, Val is the licensing relation value and SatBy is
the node which satisfies the give (marked as 7, if the relation is as yet
unsatisfied). Needs are 3-tuples Type, Vat, SatEy) where these are
as in the gives. For purposes of readability, I remove previously satisfied
gives and needs from the figure. Of course, such information persists in
the parser&apos;s representation.
</bodyText>
<figure confidence="0.981474304347826">
&amp;vac ss
needs: &lt;theta,
&lt;caw norrOnatiee, ja
Harry
gives: cleft, came, 11.03/1211.tiVf6 b
&lt;left subject, OP, l&gt;
11 need= 0
I gives &lt;right, function-salary VP, ?a
merle 0
114
given 0
needy &lt;theta, agent, n&gt;
Harry
NN(‘&apos;
giver &lt;right function-salact, VP, rm. I
rusk 0 In•FaS
&amp;IV 0 ti
muds: &lt;thew agent, ns.
Elvis: &lt;right, theta, WM, b.
v
n needs 0
V
laugh
</figure>
<figureCaption confidence="0.999486">
Figure 4: Adjunction of auxiliary tree j9 into elementary
tree a to produce 7
Figure 3: Working space after &amp;quot;Harry tns/agr laugh&amp;quot;
</figureCaption>
<bodyText confidence="0.999944511111111">
V&apos; node yielding the structure in figure 3. Since this node
forms a chain with the subject DP, the theta need on the
subject DP is now satisfied. We have now reached the end
of our input. The resulting structure is easily seen to be
well-formed since all gives and needs are satisfied.
We have adopted a very particular view of traces: their
positions in the structure must be independently motivated
by some other licensing relation. Note, then, that we can-
not analyze long distance dependencies through successive
cyclic movement. There is no licensing relation which will
cause the intermediate traces to exist. Ordinarily these
traces exist only to allow a well-formed derivation, i.e.
not ruled out by subjacency or by a barrier to antecedent
government. Thus, we need to account for constraints on
long distance movement in another manner. We will return
to this in the next section.
The mechanism I have proposed allows a fairly direct
encoding for some of the principles of grammar such as
case theory, theta theory, and the extended projection prin-
ciple. However, many other constraints of GB, such as the
ECP, control theory, binding theory, and bounding the-
ory, cannot be expressed perspicuously through licensing.
Since we want our parser to maintain a fairly direct con-
nection with the grammar, we need some additional mech-
anism to ensure the satisfaction of these constraints.
Recall, again, the computational properties we wanted to
hold of our parsing model: efficiency and incrementality.
The structure building process I have described has worst
case complexity 0(n2) since the set of possible attach-
ments can grow linearly with the input. While not enor-
mously computationally intensive, this is greater that the
linear time bound we desire. Also, checking for satisfac-
tion of non-licensing constraints over unboundedly large
structures is likely to be quite inefficient. There is also
the question of when these other constraints are checked
To accord with incrementality, they must be checked as
soon as possible, and not function as post-processing &amp;quot;fil-
ters.&amp;quot; Unfortunately, it is not easily determinable when a
given constraint can apply such that further input will not
change the status of the satisfaction of a constraint. We
do not want to rule a structure ungrammatical simply be-
cause it is incomplete. Finally, it is unclear how we might
incorporate this mechanism which builds an ever larger
syntactic structure into a model which performs semantic
interpretation incrementally.
</bodyText>
<sectionHeader confidence="0.972282" genericHeader="method">
4 Limiting the Domain with TAG
</sectionHeader>
<bodyText confidence="0.9429909375">
These problems with our model are solved if we can place
a limit on the size of the structures we construct. The
number of licensing possibilities would be bounded yield-
ing linear time for structure construction. Also, constraint
checking could be done in a constant amount of process-
ing. Unfortunately, the productivity of language requires
us to handle sentences of unbounded length and thus lin-
guistic structures of unbounded size.
TAG provides us with a way to achieve this paradise.
TAG accomplishes linguistic description by factoring re-
cursion from local dependencies [JOshi, 1985]. It posits
a set of primitive structures, the elementary trees, which
may be combined through the operations of adjunction
and substitution. An elementary tree is a minimal non-
recursive syntactic tree, a predication structure containing
positions for all arguments. I propose that this is the pro-
jection of a lexical head together with any of the associated
functional projections of which it is a complement. For
instance, a single elementary tree may contain the projec-
tion of a V along with the I and C projections in which it
is embedded.6 Along the frontier of these trees may ap-
pear terminal symbols (i.e. lexical items) or non-terminals.
The substitution operation is the insertion of one elemen-
tary tree at a non-terminal of same type as the root on the
frontier of another elementary tree. Adjunction allows the
insertion of one elementary tree of a special kind, an aux-
iliary tree, at a node internal to another (cf. figure 4). In
6This definition of TAG elementary trees is consistent with the Lex-
icalized TAG framework [Schabes es at., 19881 in that the lexical head
may he seen as the anchor of the elementary trees. For further details
and consequences of this proposal on elementary tree well-fonnedness,
see [Frank, 1990].
</bodyText>
<page confidence="0.998029">
115
</page>
<bodyText confidence="0.999973192307693">
auxiliary trees, there is a single distinguished non-terminal
an the frontier of the tree, the foot node, which is iden-
tical in type to the root node, Only adjunctions, and not
substitutions, may occur at this node.
TAG has proven useful as a formalism in which one can
express linguistic generalizations since it seems to provide
a sufficient domain over which grammatical constraints
can be stated [Kroch and Josh&apos;, 1985] [Kroch and San-
torini, 1987]. Kroch, in two remarkable papers [1986] and
[1987], has shown that even constraints on long distance
dependencies, which intuitively demand a more &amp;quot;global&amp;quot;
perspective, can be expressed using an entirely local (i.e.
within a single elementary tree) formulation of the ECP
and allows for the collapsing of the CED with the ECP.
This analysis does not utilize intermediate traces, but in-
stead the link between filler and gap is &amp;quot;stretched&amp;quot; upon
the insertion of intervening structure during adjunctions.
Thus, we are relieved of the problem that intermediate
traces are not licensed, since we do not require their exis-
tence.
Let us suppose a formulation of GB in which all princi-
ples not enforced through generalized licensing are stated
over the local domain of a TAG elementary tree. Now,
we can use the model described above to create structures
corresponding to single elementary trees. However, we
restrict the working space of the parser to contain only a
single structure of this size. If we perform an attachment
which violates this &amp;quot;memory limitation,&amp;quot; we are forced to
reduce the structure in our working space. We will do
this in one of two ways, corresponding to the two mech-
anisms which TAG provides for combining structure. Ei-
ther we will undo a substitution or undo an adjunction.
However, all chains are required to be localized in indi-
vidual elementary tree. Once an elementary tree is fin-
ished, non-licensing constraints are checked and it is sent
off for semantic interpretation. This is the basis for my
proposed parsing model. For details of the algorithm, see
[Frank, 1990]. This mechanism operates in linear time and
deterministically, while maintaining coarse grained (i.e.
clausal) incrementality for grammaticality determination
and semantic interpretation.
Consider this model on the raising sentence &amp;quot;Harry
seemed to kiss Sally.&amp;quot; We begin as before with &amp;quot;Harry
tris/age yielding the structure in figure 2. Before we re-
ceive the next token of input, however, we see that the
working structure is larger than the domain of an elemen-
tary tree, since the subject DP constitutes an independent
predication from the one determined by the projection of
I. We therefore unsubstitute the subject DP and send it off
to constraint checking and semantic interpretation. At this
point, we push a copy of the subject DP node onto the
trace stack due to its unsatisfied theta need.
</bodyText>
<figureCaption confidence="0.991963">
Figure 5: Working space after &amp;quot;Harry tns/agr seem&amp;quot;
</figureCaption>
<figure confidence="0.976972625">
giv.r.
needs: rebate, ?. ?1&gt;:(
I
ersin: gives: deft subject DP, ?s.
giver. •erleit sheet IF, la V 5.
1 El Ores: estthi, fuschkereelsct, VP, Ts
needs10
needs: 0
</figure>
<figureCaption confidence="0.99988">
Figure 6: Working space after &amp;quot;Harry tns/agr seem to&amp;quot;
</figureCaption>
<bodyText confidence="0.999978944444444">
We continue with the verb seem which projects to V&apos;
and attaches as sister to I satisfying the functional selec-
tion give yielding the structure in figure 5. There remains
only one elementary tree in working space so we need
not perform any domain reduction. Next, to projects to 1&apos;
since it lacks f-features to assign to its specifier. This is
attached as object of seem as in figure 6. At this point,
we must again perform a domain reduction operation since
the upper and lower clauses form two separate elementary
trees. Since the subject DP remains on the trace stack, it
cannot yet be removed. All dependencies must be resolved
within a single elementary tree. Hence, we must unadjoin
the structure recursive on I&apos; shown in figure 7 leaving the
structure in figure 8 in the working space. This structure
is sent off for constraint checking and semantic interpreta-
tion. We continue with kiss, projecting and attaching it as
functionally selected sister of I and popping the DP from
the trace stack to serve as external argument. Finally, we
</bodyText>
<figureCaption confidence="0.999636">
Figure 7: Result of unadjunction
</figureCaption>
<figure confidence="0.64516925">
levee 0 DP
mule: &lt;thee.. 3, ?a
Oyes [right, funrkiar.el,e, VP, ks
needs 0
v geess cright the. IP. Pr
I maim 0
MGM
insfais
</figure>
<page confidence="0.970667">
116
</page>
<note confidence="0.8765434">
gives 4.3eft, subject, DP, i&gt;
nteds:0
gives: &lt;right, function-select, VP?›
needs: 0
to
</note>
<figureCaption confidence="0.9999895">
Figure 8: Working space after unadjunction
Figure 9: Working Structure after entire sentence
</figureCaption>
<bodyText confidence="0.9999354">
project and attach the DP Sally as sister of V. receiving
both theta role and case in this position. This DP is unsub-
stituted in the same manner as the subject and is sent off
for further processing. We are left finally with the struc-
ture in figure 9, all of whose gives and needs are satisfied,
and we are finished.
This model also handles control constructions, bare in-
finitives, ECM verbs and binding of anaphors, modifica-
tion, genitive DPs and others. Due to space constraints,
these are not discussed here, but see [Frank, 1990].
</bodyText>
<sectionHeader confidence="0.989528" genericHeader="evaluation">
5 Problems and Future Work
</sectionHeader>
<bodyText confidence="0.997716083333333">
The parsing model which I have presented here is still
rather preliminary. There are a number of areas which will
require further development before this can be considered
complete.
I have assumed that the process of projection is en-
tirely determined from lexical lookup. It is clear, though,
that lexical ambiguity abounds and that the assignment of
gives and needs to the projections of input tokens is not
determinate. An example of such indeterminacy has to do
with the assignment to argument maximal projections of
theta needs as a result of the theta criterion. DPs need not
always function as arguments, as I have been assuming.
This problem might be solved by allowing for the state-
ment of disjunctive constraints or a limited form of paral-
lelism. If the duration of such parallelism could be tightly
constrained, we might be able to retain the efficient nature
of the current model. Other strategies for resolving such
indeterminacies using statistical reasoning or hard coded
rules or templates might also be possible, but these con-
structs are not the sort of grammatical knowledge we have
been considering here and would entail further abstraction
from the competence grammar.
Another problem with the parser has to do with the
incompleteness of the algorithm. Sentences such as
Boris knew that Tom ate lunch
will not be parsed even though there exist well-formed
sets of elementary trees which can derive them. The prob-
lem results from the fact that the left to right processing
strategy we have adopted is a bit too strict. The comple-
mentizer that will be attached as object of know, but Tom
is not then licensed by any node on the right frontier. Ul-
timately, this DP is licensed by the Ins/agr morpheme in
the lower clause whose IP projection is licensed through
functional selection by C. Similarly, the parser would have
great difficulty handling head final languages_ Again, these
problems might be solved using extra-grammatical de-
vices, such as the attention shifting of [Marcus, 1980] or
some template matching mechanism, but this would entail
a process of &amp;quot;compiling out&amp;quot; of the grammar that we have
been trying to avoid.
Finally, phonologically empty heads and head move-
ment cause great difficulties for this mechanism. Heads
play a crucial role in this &amp;quot;project and attach&amp;quot; scheme.
Therefore, we must find a way of determining when and
where heads occur when they are either dislocated or not
present in the input string at all, perhaps in a similar man-
ner to the mechanism for movement of maximal projec-
tions I have proposed above.
</bodyText>
<sectionHeader confidence="0.999429" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.961715357142857">
In this paper, I have sketched a psychologically plausible
model for the use of GB grammars. The currently im-
plemented parser is a bit too simple to be truly robust,
but the general approach presented here seems promising.
Particularly interesting is that the computationally moti-
vated use of TAG to constrain processing locality pro-
vides us with insight on the nature of the meta-grammar
of possible grammatical constraints. Thus, if grammatical
principles are stated over such a bounded domain, we can
guarantee the existence of a perspicuous model for their
use, thereby lending credence to the cognitive reality of
this competence grammar.
gives: DPi
needs: &lt;theta, P. ?›
</bodyText>
<page confidence="0.978098">
117
</page>
<sectionHeader confidence="0.993451" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999836913793103">
[Abney, 1986] Steven Abney. Licensing and parsing. In
Proceedings of NELS 16, Amherst, MA.
[Berwick and Weinberg, 1984] Robert Berwick and Amy
Weinberg. The Grammatical Basis of Linguistic Per-
formance. MIT Press, Cambridge, MA.
[Chomsky, 1981] Noarn Chomsky. Lectures on Govern-
ment and Binding. FOriS, Dordrecht.
[Fodor, 1978] Janet D Fodor. Parsing strategies and con-
straints on transformations. Linguistic Inquiry, 9.
[Frank, 1990] Robert Frank. Computation and Linguistic
Theory: A Government Binding Theory Parser Using
Tree Adjoning Grammar. Master&apos;s thesis, University
of Pennsylvania.
[Fukui and Speas, 1986] Naoki
Fukui and Margaret Speas. Specifiers and projec-
tion. In Naoki Fukui, T. Rappaport, and E. Sagey,
editors, MIT Working Papers in Linguistics 8, MIT
Department of Linguistics.
[Johnson, 1988] Mark Johnson. Parsing as deduction: the
use of knowledge of language. In The MIT Parsing
Volume, 1987-88, MIT Center for Cognitive Science.
[Joshi, 1985] Aravind Joshi. How much context-
sensitivity is required to provide reasonable structural
descriptions: tree adjoining grammars. In D. Dowty,
L. Kartunnen, and A. Zwicky, editors, Natural Lan-
guage Processing: Psycholinguistic, Computational
and Theoretical Perspectives, Cambridge University
Press.
[&apos;Crock 1986] Anthony ICroch. Unbounded dependencies
and subjacency in a tree adjoining grammar. In A.
Manaster-Ramer, editor, The Mathematics of Lan-
guage, John Benjarnins.
[ICroch, 1987] Anthony ICroch. Assymetries in long
distance extraction in a tree adjoining grammar.
manuscript, University of Pennsylvania.
[ICroch and Joshi, 1985] Anthony ICroch and Aravind
Joshi. The Linguistic Relevance of Tree Adjoining
Grammar. Technical Report MS-CS-85-16, Univer-
sity of Pennsylvania Department of Computer and
Information Sciences. To appear in Linguistics and
Philosophy.
[Kroch and Santorini, 1987] Anthony ICroch and Beatrice
Santorini. The derived constituent structure of the
west germanic verb raising construction. In R. Frei-
din, editor, Proceedings of the Princeton Conference
on Comparative Grammar, MIT Press, Cambridge,
MA.
[Marcus, 1980] Mitchell Marcus. A Theory of Syntactic
Recognition for Natural Language. MIT Press, Cam-
bridge, MA.
[Schabes et al., 1988] Yves Schabes, Anne Abeilld, and
Aravind K. Joshi. Parsing strategies with &apos;lexical-
ized&apos; grammars: application to tree adjoining gram-
mars. In COLING Proceedings, Budapest.
[Stabler, 19901 Edward Stabler. Implementing govern-
ment binding theories. In Levine and Davis, ed-
itors, Formal Linguistics: Theory and Implementa-
tion. forthcoming.
</reference>
<page confidence="0.996184">
118
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.954960">
<title confidence="0.996049">LICENSING AND &apos;TREE ADJOINING GRAMMAR IN GOVERNMENT BINDING PARSING</title>
<author confidence="0.999993">Robert Frank</author>
<affiliation confidence="0.9997455">Department of Computer and Information Sciences University of Pennsylvania</affiliation>
<address confidence="0.999935">Philadelphia, PA 19104</address>
<email confidence="0.999935">frank@linc.cis.upenn.edu</email>
<abstract confidence="0.997120769230769">This paper presents an implemented, psychologically plausible parsing model for Government Binding theory grammars. I make use of two main ideas: (1) a generalization of the licensing relations of [Abney, 1986] allows for the direct encoding of certain principles of grammar (e.g. Theta Criterion, Case Filter) which drive structure building; (2) the working space of the parser is constrained to the domain determined by a Tree Adjoining Grammar elementary tree. All dependencies and constraints are localized within this bounded structure. The resultant parser operates in linear time and allows for incremental semantic interpretation and determination of grammaticality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Steven Abney</author>
</authors>
<title>Licensing and parsing.</title>
<booktitle>In Proceedings of NELS 16,</booktitle>
<location>Amherst, MA.</location>
<marker>[Abney, 1986]</marker>
<rawString>Steven Abney. Licensing and parsing. In Proceedings of NELS 16, Amherst, MA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Robert Berwick</author>
<author>Amy Weinberg</author>
</authors>
<title>The Grammatical Basis of Linguistic Performance.</title>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>[Berwick and Weinberg, 1984]</marker>
<rawString>Robert Berwick and Amy Weinberg. The Grammatical Basis of Linguistic Performance. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Noarn Chomsky</author>
</authors>
<booktitle>Lectures on Government and Binding. FOriS,</booktitle>
<location>Dordrecht.</location>
<marker>[Chomsky, 1981]</marker>
<rawString>Noarn Chomsky. Lectures on Government and Binding. FOriS, Dordrecht.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Janet D Fodor</author>
</authors>
<title>Parsing strategies and constraints on transformations.</title>
<journal>Linguistic Inquiry,</journal>
<volume>9</volume>
<marker>[Fodor, 1978]</marker>
<rawString>Janet D Fodor. Parsing strategies and constraints on transformations. Linguistic Inquiry, 9.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Robert Frank</author>
</authors>
<title>Computation and Linguistic Theory: A Government Binding Theory Parser Using Tree Adjoning Grammar. Master&apos;s thesis,</title>
<institution>University of Pennsylvania.</institution>
<marker>[Frank, 1990]</marker>
<rawString>Robert Frank. Computation and Linguistic Theory: A Government Binding Theory Parser Using Tree Adjoning Grammar. Master&apos;s thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Naoki Fukui</author>
<author>Margaret Speas</author>
</authors>
<title>Specifiers and projection.</title>
<booktitle>MIT Working Papers in Linguistics 8,</booktitle>
<editor>In Naoki Fukui, T. Rappaport, and E. Sagey, editors,</editor>
<institution>MIT Department of Linguistics.</institution>
<marker>[Fukui and Speas, 1986]</marker>
<rawString>Naoki Fukui and Margaret Speas. Specifiers and projection. In Naoki Fukui, T. Rappaport, and E. Sagey, editors, MIT Working Papers in Linguistics 8, MIT Department of Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Parsing as deduction: the use of knowledge of language.</title>
<date>1987</date>
<booktitle>In The MIT Parsing Volume,</booktitle>
<institution>MIT Center for Cognitive Science.</institution>
<marker>[Johnson, 1988]</marker>
<rawString>Mark Johnson. Parsing as deduction: the use of knowledge of language. In The MIT Parsing Volume, 1987-88, MIT Center for Cognitive Science.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Aravind Joshi</author>
</authors>
<title>How much contextsensitivity is required to provide reasonable structural descriptions: tree adjoining grammars. In</title>
<booktitle>Natural Language Processing: Psycholinguistic, Computational and Theoretical Perspectives,</booktitle>
<editor>D. Dowty, L. Kartunnen, and A. Zwicky, editors,</editor>
<publisher>Cambridge University Press.</publisher>
<marker>[Joshi, 1985]</marker>
<rawString>Aravind Joshi. How much contextsensitivity is required to provide reasonable structural descriptions: tree adjoining grammars. In D. Dowty, L. Kartunnen, and A. Zwicky, editors, Natural Language Processing: Psycholinguistic, Computational and Theoretical Perspectives, Cambridge University Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Anthony ICroch</author>
</authors>
<title>Unbounded dependencies and subjacency in a tree adjoining grammar.</title>
<booktitle>The Mathematics of Language,</booktitle>
<editor>In A. Manaster-Ramer, editor,</editor>
<location>John Benjarnins.</location>
<marker>[&apos;Crock 1986]</marker>
<rawString>Anthony ICroch. Unbounded dependencies and subjacency in a tree adjoining grammar. In A. Manaster-Ramer, editor, The Mathematics of Language, John Benjarnins.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Anthony ICroch</author>
</authors>
<title>Assymetries in long distance extraction in a tree adjoining grammar.</title>
<institution>University of Pennsylvania.</institution>
<note>manuscript,</note>
<marker>[ICroch, 1987]</marker>
<rawString>Anthony ICroch. Assymetries in long distance extraction in a tree adjoining grammar. manuscript, University of Pennsylvania.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Anthony ICroch</author>
<author>Aravind Joshi</author>
</authors>
<title>The Linguistic Relevance of Tree Adjoining Grammar.</title>
<tech>Technical Report MS-CS-85-16,</tech>
<institution>University of Pennsylvania Department of Computer and Information Sciences.</institution>
<note>To appear in Linguistics and Philosophy.</note>
<marker>[ICroch and Joshi, 1985]</marker>
<rawString>Anthony ICroch and Aravind Joshi. The Linguistic Relevance of Tree Adjoining Grammar. Technical Report MS-CS-85-16, University of Pennsylvania Department of Computer and Information Sciences. To appear in Linguistics and Philosophy.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Anthony ICroch</author>
<author>Beatrice Santorini</author>
</authors>
<title>The derived constituent structure of the west germanic verb raising construction. In</title>
<booktitle>Proceedings of the Princeton Conference on Comparative Grammar,</booktitle>
<editor>R. Freidin, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>[Kroch and Santorini, 1987]</marker>
<rawString>Anthony ICroch and Beatrice Santorini. The derived constituent structure of the west germanic verb raising construction. In R. Freidin, editor, Proceedings of the Princeton Conference on Comparative Grammar, MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Mitchell Marcus</author>
</authors>
<title>A Theory of Syntactic Recognition for Natural Language.</title>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>[Marcus, 1980]</marker>
<rawString>Mitchell Marcus. A Theory of Syntactic Recognition for Natural Language. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Anne Abeilld</author>
<author>Aravind K Joshi</author>
</authors>
<title>Parsing strategies with &apos;lexicalized&apos; grammars: application to tree adjoining grammars.</title>
<date>1990</date>
<booktitle>In COLING Proceedings,</booktitle>
<location>Budapest. [Stabler,</location>
<marker>[Schabes et al., 1988]</marker>
<rawString>Yves Schabes, Anne Abeilld, and Aravind K. Joshi. Parsing strategies with &apos;lexicalized&apos; grammars: application to tree adjoining grammars. In COLING Proceedings, Budapest. [Stabler, 19901 Edward Stabler. Implementing government binding theories. In Levine and Davis, editors, Formal Linguistics: Theory and Implementation. forthcoming.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>