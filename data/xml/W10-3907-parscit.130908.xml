<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996918">
A Look inside the Distributionally Similar Terms
</title>
<author confidence="0.78681">
Kow Kuroda Jun’ichi Kazama Kentaro Torisawa
</author>
<email confidence="0.829858">
kuroda@nict.go.jp kazama@nict.go.jp torisawa@nict.go.jp
</email>
<affiliation confidence="0.516666">
National Institute of Information and Communications Technology (NICT)
</affiliation>
<sectionHeader confidence="0.987841" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999987176470588">
We analyzed the details of a Web-derived
distributional data of Japanese nominal
terms with two aims. One aim is to
examine if distributionally similar terms
can be in fact equated with “semanti-
cally similar” terms, and if so to what
extent. The other is to investigate into
what kind of semantic relations con-
stitute (strongly) distributionally similar
terms. Our results show that over 85%
of the pairs of the terms derived from
the highly similar terms turned out to
be semantically similar in some way.
The ratio of “classmate,” synonymous,
hypernym-hyponym, and meronymic re-
lations are about 62%, 17%, 8% and 1%
of the classified data, respectively.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996620754717">
The explosion of online text allows us to enjoy
a broad variety of large-scale lexical resources
constructed from the texts in the Web in an un-
supervised fashion. This line of approach was
pioneered by researchers such as Hindle (1990),
Grefenstette (1993), Lee (1997) and Lin (1998).
At the heart of the approach is a crucial working
assumption called “distributional hypothesis,” as
with Harris (1954). We now see an impressive
number of applications in natural language pro-
cessing (NLP) that benefit from lexical resources
directly or indirectly derived from this assump-
tion. It seems that most researchers are reason-
ably satisfied with the results obtained thus far.
Does this mean, however, that the distribu-
tional hypothesis was proved to be valid? Not
necessarily: while we have a great deal of con-
firmative results reported in a variety of research
areas, but we would rather say that the hypothe-
sis has never been fully “validated” for two rea-
sons. First, it has yet to be tested under the pre-
cise definition of “semantic similarity.” Second,
it has yet to be tested against results obtained at
a truly large scale.
One of serious problems is that we have seen
no agreement on what “similar terms” mean and
should mean. This paper intends to cast light
on this unsolved problem through an investiga-
tion into the precise nature of lexical resources
constructed under the distributional hypothesis.
The crucial question to be asked is, Can distri-
butionally similar terms really be equated with
semantically similar terms or not? In our investi-
gation, we sought to recognize what types of se-
mantic relations can be found for pairs of terms
with high distributional similarity, and see where
the equation of distributional similarity with se-
mantic similarity fails. With this concern, this
paper tried to factor out as many components of
semantic similarity as possible. The effort of fac-
torization resulted in the 18 classes of semantic
(un)relatedness to be explained in §2.3.1. Such
factorization is a necessary step for a full valida-
tion of the hypothesis. To meet the criterion of
testing the hypothesis at a very large scale, we
analyzed 300,000 pairs of distributionally simi-
lar terms. Details of the data we used are given
in §2.2.
This paper is organized as follows. In §2, we
present our method and data we used. In §3, we
present the results and subsequent analysis. In
§4, we address a few remaining problems. In §5,
we state tentative conclusions.
</bodyText>
<page confidence="0.984123">
40
</page>
<note confidence="0.976767">
Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 40–49,
</note>
<keyword confidence="0.386923">
Beijing, August 2010
</keyword>
<sectionHeader confidence="0.816303" genericHeader="introduction">
2 Method and Data
</sectionHeader>
<subsectionHeader confidence="0.978572">
2.1 Method
</subsectionHeader>
<bodyText confidence="0.999988">
The question we need to address is how many
subtypes of semantic relation we can identify in
the highly similar terms. We examined the ques-
tion in the following procedure:
</bodyText>
<listItem confidence="0.881262066666667">
(1) a. Select a set of “base” terms B.
b. Use a similarity measure M to con-
struct a list of n terms T = [ti,1, ti,2,... ,
ti,j, ... , ti,n] where ti,j denotes the j-
th most similarity term in T against
bi ∈ B. P(k) are pairs of bi and ti,k, i.e.,
the k-th most similar term to bi.
c. Human raters classify a portion Q of
the pairs in P(k) with reference to
a classification guideline prepared for
the task.
Note that the selection of base set B can be
independent of the selection of T. Note also that
T is indexed by terms in B. To encode this, we
write: T[bi] = [ti,1, ti,2,... , ti,j, ... , ti,n].
</listItem>
<subsectionHeader confidence="0.99097">
2.2 Data
</subsectionHeader>
<bodyText confidence="0.99998535">
For T, we used Kazama’s nominal term cluster-
ing (Kazama and Torisawa, 2008; Kazama et al.,
2009). In this data, base set B for T is one mil-
lion terms defined by the type counts of depen-
dency relations, which is roughly equated with
the “frequencies” of the terms. Each base term
in B is associated with up to 500 of the most dis-
tributionally similar terms. This defines T.
For M, we used the Jensen-Shannon diver-
gence (JS-divergence) base on the probability
distributions derived by an EM-based soft clus-
tering (Kazama and Torisawa, 2008). For con-
venience, some relevant details of the data con-
struction are described in Appendix A, but in a
nutshell, we used dependency relations as dis-
tributional information. This makes our method
comparable to that used in Hindle (1990). The
statistics of the distributional data used were as
follows: roughly 920 million types of depen-
dency relations1) were automatically acquired
</bodyText>
<footnote confidence="0.7164895">
1)The 920 million types come in two kinds of context
triples: 590 million types of (t, p,v) and 320 million types
</footnote>
<bodyText confidence="0.9995612">
from a large-scale Japanese Web-corpus called
the Tsubaki corpus (Shinzato et al., 2008) which
consists of roughly 100 million Japanese pages
with six billion sentences. After excluding hapax
nouns, we had about 33 million types of nouns
(in terms of string) and 27 million types of verbs.
These nouns were ranked by type count of the
two context triples, i.e., (t, p,v) and (n*, p*,t). B
was determined by selecting the top one million
terms with the most variations of context triples.
</bodyText>
<subsectionHeader confidence="0.952727">
2.2.1 Sample of T[b]
</subsectionHeader>
<bodyText confidence="0.99339025">
For illustration, we present examples of the
Web-derived distributional similar terms. (2)
shows the 10 most distributionally similar terms
(i.e., [t1070,1, t1070,2, . . . , t1070,10] in T(b1070))
where b1070 = “ ” (piano) is the 1070-th
term in B. Likewise, (3) shows the 10 most dis-
tributionally similar terms [t38555,1, t38555,2, . . . ,
t38555,10] in T(b38555)) where b38555 = “
</bodyText>
<listItem confidence="0.978215095238095">
” (Tchaikovsky) is the 38555-th term in B.
(2) 10 most similar to “ ”
1. (Electone; electronic or-
gan) [-0.322]
2. (violin) [-0.357]
3. (violin) [-0.358]
4. (cello) [-0.358]
5. (trumpet) [-0.377]
6. (shamisen) [-0.383]
7. (saxophone) [-0.39]
8. (organ) [-0.392]
9. (clarinet) [-0.394]
10. (erh hu) (-0.396)
(3) 10 most similar to “ ”
1. (Brahms) [-0.152]
2. (Schumann) [-0.163]
3. (Mendelssohn) [-
0.166]
4. (Shostakovich) [-
0.178]
5. (Sibelius) [-0.18]
</listItem>
<bodyText confidence="0.882612666666667">
of (t, p*,n*), where t denotes the target nominal term, p a
postposition, v a verb, and n* a nominal term that follows t
and p*, i.e., “t-no” analogue to the English “of t.”
</bodyText>
<page confidence="0.996629">
41
</page>
<listItem confidence="0.992415">
6. (Haydn) [-0.181]
7. (H¨andel) [-0.181]
8. (Ravel) [-0.182]
9. (Schubert) [-0.187]
10. (Beethoven) [-0.19]
</listItem>
<bodyText confidence="0.988817375">
For construction of P(k), we had the follow-
ing settings: i) k = 1,2; and ii) for each k, we
selected the 150,000 most frequent terms (out of
one million terms) with some filtering specified
below. Thus, Q was 300,000 pairs whose base
terms are roughly the most frequent 150,000
terms in B with filtering and targets are terms
k = 1 or k = 2.
</bodyText>
<subsectionHeader confidence="0.979927">
2.2.2 Filtering of terms in B
</subsectionHeader>
<bodyText confidence="0.999901827586207">
For filtering, we excluded the terms of B with
one of the following properties: a) they are in an
invalid form that could have resulted from parse
errors; b) they have regular ending (e.g., -
, - [event], - [time or when], - [thing or
person], - [thing], - [person]). The reason
for the second is two-fold. First, it was desir-
able to reduce the ratio of the class of “class-
mates with common morpheme,” which is ex-
plained in §2.3.2, whose dominance turned out to
be evident in the preliminary analysis. Second,
the semantic property of the terms in this class
is relatively predictable from their morphology.
That notwithstanding, this filtering might have
had an undesirable impact on our results, at least
in terms of representativeness. Despite of this,
we decided to place priority on collecting more
varieties of classes.
The crucial question is, again, whether dis-
tributionally similar terms can really be equated
with semantically similar terms. Put differently,
what kinds of terms can we find in the sets con-
structed using distributionally similarity? We
can confirm the hypothesis if the most of the
term pairs are proved to be semantically simi-
lar for most sets of terms constructed based on
the distributional hypothesis. To do this, how-
ever, we need to clarify what constitutes seman-
tic similarity. We will deal with this prerequisite.
</bodyText>
<subsectionHeader confidence="0.9869675">
2.3 Classification
2.3.1 Factoring out “semantic similarity”
</subsectionHeader>
<bodyText confidence="0.9992216">
Building on lexicographic works like Fell-
baum (1998) and Murphy (2003), we assume
that the following are the four major classes
of semantic relation that contribute to semantic
similarity between two terms:
</bodyText>
<listItem confidence="0.966615653846154">
(4) a. “synonymic” relation (one can substi-
tute for another on an identity basis).
Examples are (Microsoft, MS).
b. “hypernym-hyponym” relation be-
tween two terms (one can substitute
for another on un underspecifica-
tion/abstraction basis). Examples are
(guys, players)
c. “meronymic” (part-whole) relation be-
tween two terms (one term can be a
substitute for another on metonymic
basis). Examples are (bodies, players)
[cf. All the players have strong bodies]
d. “classmate” relation between two
terms, t1 and t2, if and only if (i) they
are not synonymous and (ii) there is a
concrete enough class such that both t1
and t2 are instances (or subclasses).2)
For example, (China, South Korea)
[cf. (Both) China and South Korea
are countries in East Asia], (Ford, Toy-
ota) [cf. (Both) Ford and Toyota are
top-selling automotive companies] and
(tuna, cod) [cf. (Both) tuna and cod
are types offish that are eaten in the
Europe] are classmates.
</listItem>
<bodyText confidence="0.995458538461539">
For the substitution, the classmate class behaves
somewhat differently. In this case, one term can-
not substitute for another for a pair of terms. It
is hard to find out the context in which pairs like
(China, South Korea), (Ford, Toyota) and (tuna,
cod) can substitute one another. On the other
hand, substitution is more or less possible in the
other three types. For example, a synonymic pair
of (MS, Microsoft) can substitute for one another
in contexts like Many people regularly complain
2)The proper definition of classmates is extremely hard
to form. The authors are aware of the incompleteness of
their definition, but decided not to be overly meticulous.
</bodyText>
<page confidence="0.992625">
42
</page>
<figure confidence="0.9988778">
s:* synonymous
pair in the
broadest sense
k**: classmate
in the broadest
sense
o: pair in other,
unindentified
relation
p: meronymic
pair
h: hypernym-
hyponym pair
s: synonymous
pair of different
terms
v*: notational
variation of the
same term
k*: classmate
without obvious
contrastiveness
c*: contrastive
pairs
n: alias pair
t: pair of term
with inherent
temporal orde
k: classmate
without shared
morpheme
f: quasi-
erroneous pair
c: contrastive
pair without
antonymity
v: allographic
pair
w: classmate
with shared
morpheme
d: antonymic
pair
a: acronymic
pair
</figure>
<figureCaption confidence="0.999357">
Figure 1: Classification tree for semantic relations used
</figureCaption>
<figure confidence="0.989909846153846">
in no conceivable
semantic relation
pair of forms
pair of
meaningful
terms
x: pair with a
meaningless
form
y: undecidable
r: pair of terms in
a conceivable
semantic relation
</figure>
<bodyText confidence="0.932221857142857">
about products { i. MS; ii. Microsoft }. A
hypernym-hyponym pair of (guys, players) can
substitute in contexts like We have dozens of ex-
cellent { i. guys; ii. players } on our team. A
meronymic pair of (bodies, players) can substi-
tute for each other in contexts like They had a few
of excellent { i. bodies; ii. players } last year.
</bodyText>
<subsectionHeader confidence="0.684787">
2.3.2 Classification guidelines
</subsectionHeader>
<bodyText confidence="0.950081">
The classification guidelines were specified
based on a preliminary analysis of 5,000 ran-
domly selected examples. We asked four annota-
tors to perform the task. The guidelines were fi-
nalized after several revisions. This revision pro-
cess resulted in a hierarchy of binary semantic
relations as illustrated in Figure 1, which sub-
sumes 18 types as specified in (5). The essen-
tial division is made at the fourth level where
we have s* (pairs of synonyms in the broadest
sense) with two subtypes, p (pairs of terms in
the “part-whole” relation), h (pairs of terms in
the “hypernym-hyponym” relation), k** (pairs
of terms in the “classmate” relation), and o (pairs
of terms in any other relation). Note that this
system includes the four major types described
in (4). The following are some example pairs of
Japanese terms with or without English transla-
tions:
(5) s: synonymic pairs (subtype of s*) in
which the pair designates the same en-
tity, property, or relation. Examples
are: ( , ) [both mean root], (
, ) [(supporting mem-
ber, cooperating member)], (
, ) [(invoker of the pro-
cess, parent process)], (
, ) [(venture business, ven-
ture)], ( , ) [(op-
posing hurler, opposing pitcher)], (
, ) [(medical history, anamne-
ses)],
n: alias pairs (subtype of s*) in which
one term of the pair is the “alias” of
the other term. Examples are (Steve
Jobs, founder of Apple, Inc.), (Barak
Obama, US President), ( ,
), ( , )
</bodyText>
<page confidence="0.994695">
43
</page>
<listItem confidence="0.571300857142857">
a: acronymic pair (subtype of s*) in
which one term of the pair is the
acronym of of the other term. Ex-
amples are: (DEC, Digital Equip-
ment), (IBM, International Business
Machine) (Microsoft , MS ), (
, ), ( , ),
</listItem>
<bodyText confidence="0.917071823529412">
v: allographic pairs (subtype of s*) in
which the pair is the pair of two forms
of the same term. Examples are:
(convention centre, convention cen-
ter), (colour terms, color terms), (
of k*). Examples are: ( ,
) [(self-culture, training)], (
, )[(sub-organs, services)], (
, ) [(Dongba alphabets,
hieroglyphs)], (Tom, Jerry)
w: classmates not obviously contrastive
with common morpheme (subtype of
k*). Examples are: ( , )
[(gas facilities, electric facilities)], (
, ) [(products of other com-
pany, aforementioned products)], (
, ) [(affiliate station, local sta-
</bodyText>
<equation confidence="0.98237525">
, ), ( , ),
( ,
tion)], ( ,
Wakayama City)], (
) [(Niigata City,
,
), ( , ), ( , ) [(Sinai Peninsula, Malay Penin-
)
</equation>
<bodyText confidence="0.998562">
h: hypernym-hyponym pair in which one
term of the pair designates the “class”
of the other term. Examples (or-
der is irrelevant) are: (thesaurus, Ro-
</bodyText>
<equation confidence="0.965794307692308">
get’s), ( , ) [(search
tool, search software)], ( ,
) [(unemployment measures, em-
ployment measures)], ( ,
) [(business conditions, employment
conditions)], ( , )
[(festival, music festival)], ( ,
) [(test agent, pregnancy test)],
( , ) [(cymbidium, or-
chid)], ( , ) [(company
logo, logo)], ( , ) [(mys-
tical experiences, near-death experi-
ences)]
</equation>
<bodyText confidence="0.989483666666667">
p: meronymic pair in which one term of
the pair designates the “part” of the
other term. Examples (order is ir-
</bodyText>
<equation confidence="0.62680525">
relevant) are: ( , ) [(earth,
sea)], ( , ) [(affirmation, ad-
mission)], ( , ) [(findings,
research progress)], (
</equation>
<bodyText confidence="0.939918142857143">
, ) [(solar circuit system,
exterior thermal insulation method)],
( , ) [(Provence, South
France)],
k: classmates not obviously contrastive
without common morpheme (subtype
sula)],
</bodyText>
<listItem confidence="0.8721805">
c: contrastive pairs without antonymity
(subtype of c*). Examples are: (
</listItem>
<equation confidence="0.7578416">
, ) [(romanticism, natural-
ism)], ( ,
) [(mobile user, internet user)],
( , PS2 ), [(bootleg edition, PS2
edition)]
</equation>
<bodyText confidence="0.963798227272727">
d: antonymic pairs = contrastive pairs
with antonymity (subtype of c*). Ex-
amples are: ( , ) [(bond-
ing, disintegration)], ( , )
[(gravel road, pavement)], ( ,
) [(west walls, east walls)], ( ,
) [(daughter and son-in-law,
son and daughter-in-law)], ( ,
) [(tax-exclusive prices, tax-inclusive
prices)], ( ,
) [(front brake, rear brake)], (
, ) [(tag-team match,
solo match)], ( , ) [(wip-
ing with dry materials, wiping with
wet materials)], ( , )
[(sleeveless, long-sleeved)]
t: pairs with inherent temporal order
(subtype of c*). Examples are: (
, ) [(harvesting of rice, plant-
ing of rice)], ( , ) [(day
of departure, day of arrival)], (
, ) [(career decision, career
</bodyText>
<equation confidence="0.8240825">
selection)], ( , ) [(catnap,
stay up)], ( , ) [(poaching, con-
), ( , ), ( , ),
( , ), ( ,
</equation>
<page confidence="0.966176">
44
</page>
<bodyText confidence="0.992217714285714">
traband trade)], ( , ) [(surren-
der, dispatch of troops)], ( ,
) [(2nd-year student, 3rd-year stu-
dent)]
e: erroneous pairs are pairs in which
one term of the pair seems to suffer
from character-level input errors, i.e.
</bodyText>
<equation confidence="0.564007">
“mistypes.” Examples are: ( ,
), ( , ),
( , )
</equation>
<bodyText confidence="0.972392181818182">
f: quasi-erroneous pair is a pair of terms
with status somewhat between v and e.
Examples (order is irrelevant) are: (
, ) [(supoito, supoido)],
( , ) [(goru-
fubaggu, gorufugakku)], ( ,
) [(biggu ban, bikku ban)],
m: misuse pairs in which one term of the
pair seems to suffer from “mistake” or
“bad memory” of a word (e is caused
by mistypes but m is not). Examples
</bodyText>
<equation confidence="0.634603">
(order is irrelevant) are: ( ,
), ( , ), ( , ),
( , ), ( , )
</equation>
<bodyText confidence="0.879185578947368">
o: pairs in other unidentified relation in
which the pair is in some semantic re-
lation other than s*, k**, p, h, and
u. Examples are: ( , ) [(ul-
terior motives, possessive feeling)], (
, ) [(theoretical back-
ground, basic concepts)], (
, ) [(Alexandria, Sira-
cusa)],
u: unrelated pairs in which the pair is in
no promptly conceivable semantic re-
lation. Examples are: ( ,
) [(noncontact, high resolution)], (
, ) [(imitation, overinterpreta-
tion)],
x: nonsensical pairs in which either of the
pair is not a proper term of Japanese.
(but it can be a proper name with very
low familiarity). Examples are: (
</bodyText>
<equation confidence="0.738748">
, ), ( , ), ( ,
), ( , ), (ma, )
y: unclassifiable under the allowed time
limit.3) Examples are: ( ,
), (fj, ), ( , ) ,
</equation>
<bodyText confidence="0.999977888888889">
Note that some relation types are symmetric
and others are asymmetric: a, n, h, p, and t (and
e, f, and m, too) are asymmetric types. This
means that the order of the pair is relevant, but it
was not taken into account during classification.
Annotators were asked to ignore the direction of
pairs in the classification task. In the finaliza-
tion, we need to reclassify these to get them in
the right order.
</bodyText>
<subsectionHeader confidence="0.538109">
2.3.3 Notes on implicational relations
</subsectionHeader>
<bodyText confidence="0.9995135">
The overall implicational relation in the hier-
archy in Figure 1 is the following:
</bodyText>
<listItem confidence="0.8137345">
(6) a. s, k**, p, h, and o are supposed to be
mutually exclusive, but the distinction
is sometimes obscure.4)
b. k** has two subtypes: k* and c*.
c. k and w are two subtypes k*.
d. c, d and t three subtypes of c*.
</listItem>
<bodyText confidence="0.9754625">
To resolve the issue of ambiguity, priority was
set among the labels so that e, f &lt; v &lt; a &lt; n &lt;
p &lt; h &lt; s &lt; t &lt; d &lt; c &lt; w &lt; k &lt; m &lt; o &lt; u &lt;
x &lt; y, where the left label is more preferred over
the right. This guarantees preservation of the im-
plicational relationship among labels.
2.3.4 Notes on quality of classification
We would like to add a remark on the quality.
After a quick overview, we reclassified o and w,
because the first run of the final task ultimately
produced a resource of unsatisfactory quality.
Another note on inter-annotator agreement:
originally, the classification task was designed
and run as a part of a large-scale language re-
source development. Due to its overwhelming
size, we tried to make our development as effi-
cient as possible. In the final phase, we asked
3)We did not ask annotators to check for unknown terms.
4)To see this, consider pairs like (large bowel, bowel),
(small bowel, bowel). Are they instances of p or h? The
difficulty in the distinction between h and p becomes harder
in Japanese due to the lack of plurality marking: cases
like (Mars, heavenly body) (a case of h) and (Mars, heav-
enly bodies) (a p case) cannot be explicitly distinguished.
In fact, the Japanese term can mean both “heavenly
body” (singular) and “heavenly bodies” (plural).
</bodyText>
<page confidence="0.999766">
45
</page>
<tableCaption confidence="0.998903">
Table 1: Distribution of relation types
</tableCaption>
<table confidence="0.999606">
rank count ratio (%) cum. (%) class label
1 108,149 36.04 36.04 classmates without common morpheme k
2 67,089 22.35 58.39 classmates with common morpheme w
3 26,113 8.70 67.09 synonymic pairs s
4 24,599 8.20 75.29 hypernym-hyponym pairs h
5 20,766 6.92 82.21 allographic pairs v
6 18,950 6.31 88.52 pairs in other “unidentified” relation o
7 12,383 4.13 92.65 unrelated pairs u
8 8,092 2.70 95.34 contrastive pairs without antonymity c
9 3,793 1.26 96.61 pairs with inherent temporal order t
10 3,038 1.01 97.62 antonymic pairs d
11 2,995 1.00 98.62 meronymic pairs p
12 1,855 0.62 99.23 acronymic pairs a
13 725 0.24 99.48 alias pairs n
14 715 0.24 99.71 erroneous pairs e
15 397 0.13 99.85 misuse pairs m
16 250 0.08 99.93 nonsensical pairs x
17 180 0.06 99.99 quasi-erroneous pairs f
18 33 0.01 100.00 unclassified y
</table>
<bodyText confidence="0.998209428571428">
17 annotators to classify the data with no over-
lap. Ultimately we obtained results that deserve
a detailed report. This history, however, brought
us to an undesirable situation: no inter-annotator
agreement is calculable because there was no
overlap in the task. This is why no inter-rater
agreement data is now available.
</bodyText>
<sectionHeader confidence="0.999911" genericHeader="method">
3 Results
</sectionHeader>
<bodyText confidence="0.99964975">
Table 1 summarizes the distribution of relation
types with their respective ranks and proportions.
The statistics suggests that classes of e, f, m, x,
and y can be ignored without risk.
</bodyText>
<subsectionHeader confidence="0.998347">
3.1 Observations
</subsectionHeader>
<bodyText confidence="0.999972962962963">
We noticed the following. Firstly, the largest
class is the class of classmates, narrowly defined
or broadly defined. The narrow definition of the
classmates is the conjunction of k and w, which
makes 58.39%. The broader definition of class-
mates, k**, is the union of k, w, c, d and t, which
makes 62.10%. This confirms the distributional
hypothesis.
The second largest class is the narrowly de-
fined synonymous pairs s. This is 8.7% of the
total, but the general class of synonymic pairs,
s* as the union of s, a, n, v, e, f, and m, makes
16.91%. This comes next to h and w. Notice
also that the union of k** and s* makes 79.01%.
The third largest is the class of terms in
hypernym-hyponym relations. This is 8.20% of
the total. We are not sure if this is large or small.
These results look reasonable and can be
seen as validation of the distributional hypothe-
sis. But there is something uncomfortable about
the the fourth and fifth largest classes, pairs in
“other” relation and “unrelated” pairs, which
make 6.31% and 4.13% of the total, respectively.
Admittedly, 6.31% are 4.13% are not very large
numbers, but it does not guarantee that we can
ignore them safely. We need a closer examina-
tion of these classes and return to this in §4.
</bodyText>
<subsectionHeader confidence="0.998559">
3.2 Note on allography in Japanese
</subsectionHeader>
<bodyText confidence="0.96268">
There are some additional notes: the rate of al-
lographic pairs [v] (6.92%) is rather high.5) We
suspect that this ratio is considerably higher than
the similar results that are to be expected in other
5)Admittedly, 6.92% is not a large number in an absolute
value, but it is quite large for the rate of allographic pairs.
</bodyText>
<page confidence="0.998551">
46
</page>
<bodyText confidence="0.999949705882353">
languages. In fact, the range of notational varia-
tions in Japanese texts is notoriously large. Many
researchers in Japanese NLP became to be aware
of this, by experience, and claim that this is one
of the causes of Japanese NLP being less effi-
cient than NLP in other (typically “segmented”)
languages. Our result revealed only the allogra-
phy ratio in nominal terms. It is not clear to what
extent this result is applied to the notional varia-
tions on predicates, but it is unlikely that predi-
cates have a lesser degree of notational variation
than nominals. At the least, informal analysis
suggests that the ratio of allography is more fre-
quent and has more severe impacts in predicates
than in nominals. So, it is very unlikely that we
had a unreasonably high rate of allography in our
data.
</bodyText>
<subsectionHeader confidence="0.999453">
3.3 Summary of the results
</subsectionHeader>
<bodyText confidence="0.999934888888889">
Overall, we can say that the distributional hy-
pothesis was to a great extent positively con-
firmed to a large extent. Classes of classmates
and synonymous pairs are dominant. If the side
effects of filtering described in §2.2.2 are ig-
nored, nearly 88% (all but o, u, m, x, and y)
of the pairs in the data turned out to be “se-
mantically similar” in the sense they are clas-
sified into one of the regular semantic relations
defined in (5). While the status of the inclusion
of hypernym-hyponym pairs in classes of seman-
tically similar terms could be controversial, this
result cannot be seen as negative.
One aspect somewhat unclear in the results we
obtained, however, is that highly similar terms
in our data contain such a number of pairs in
unidentifiable relation. We will discuss this in
more detail in the following section.
</bodyText>
<sectionHeader confidence="0.999844" genericHeader="method">
4 Discussion
</sectionHeader>
<subsectionHeader confidence="0.999375">
4.1 Limits induced by parameters
</subsectionHeader>
<bodyText confidence="0.999934266666667">
Our results have certain limits. We specify those
here.
First, our results are based on the case of
k = 1, 2 for P(k). This may be too small and
it is rather likely that we did not acquire results
with enough representativeness. For more com-
plete results, we need to compare the present re-
sults under larger k, say k = 4, 8, 16,.... We did
not do this, but we have a comparable result in
one of the preliminary studies. In the prepara-
tion stage, we classified samples of pairs whose
base term is at frequency ranks 13–172, 798–
1,422 and 12,673–15,172 where k = 1, 2, 3, ... ,
9, 10.6) Table 2 shows the ratios of relation types
for this sample (k = 1, 2, 4, 8, 10).
</bodyText>
<tableCaption confidence="0.990475">
Table 2: Similarity rank = 1, 2, 4, 8, 10
</tableCaption>
<table confidence="0.998958352941177">
rank 1 2 4 8 10
v 18.13 10.48 3.92 2.51 1.04
o 17.08 21.24 26.93 28.24 29.56
w 13.65 13.33 14.30 12.19 12.75
s 11.74 9.14 7.05 4.64 4.06
u 11.07 16.48 17.63 20.79 20.87
h 10.50 10.29 11.17 12.96 10.20
k 7.82 8.38 7.84 7.74 8.22
d 2.58 2.00 1.57 1.16 0.85
p 2.00 1.14 1.08 1.35 1.79
c 1.43 1.05 1.27 1.35 1.89
a 1.05 1.33 0.88 0.39 0.57
x 1.05 1.14 1.27 1.64 2.08
t 0.29 0.19 0.20 0.39 0.47
f 0.10 0.10 0.00 0.10 0.09
m 0.00 0.10 0.20 0.00 0.19
#item 1,048 1,050 1,021 1,034 1,059
</table>
<bodyText confidence="0.945472117647059">
From Table 2, we notice that: as similarity
rank decreases, (i) the ratios of v, s, a, and d
decrease monotonically, and the ratios of v and s
decrease drastically; (ii) the ratios of o, u, and x
increases monotonically, and the ratio of o and u
increases considerably; and while (iii) the ratios
of h, k, p, w, m, and f seem to be constant. But
it is likely that the ratios of h, k, p, w, m, and f
change at larger k, say 128, 256.
Overall, however, this suggests that the differ-
ence in similarity rank has the greatest impact
on s* (recall that s and v are subtypes of s*),
o, and u, but not so much on others. Two ten-
dencies can be stated: first, terms at lower sim-
ilarity ranks become less synonymous. Second,
6)The frequency/rank in B was measured in terms of the
count of types of dependency relation.
</bodyText>
<page confidence="0.998461">
47
</page>
<bodyText confidence="0.999984129032258">
the relationships among terms at lower similar-
ity ranks become more obscure. Both are quite
understandable.
There are, however, two caveats concerning
the data in Table 2, however. First, the 15 la-
bels used in this preliminary task are a subset of
the 18 labels used in the final task. Second, the
definitions of some labels are not completely the
same even if the same labels are used (this is why
we have this great of a ratio of o in Table 2. We
must admit, therefore, that no direct comparison
is possible between the data in Tables 1 and 2.
Second, it is not clear if we made the best
choices for clustering algorithm and distribu-
tional data. For the issue of algorithm, there
are too many clustering algorithms and it is hard
to reasonably select candidates for comparison.
We do, however, plan to extend our evaluation
method to other clustering algorithms. Cur-
rently, one of such options is Bayesian cluster-
ing. We are planning to perform some compar-
isons.
For the issue of what kind of distributional in-
formation to use, many kinds of distributional
data other than dependency relation are avail-
able. For example, simple co-occurrences within
a “window” are a viable option. With a lack
of comparison, however, we cannot tell at the
present what will come about if another kind of
distributional data was used in the same cluster-
ing algorithm.
</bodyText>
<subsectionHeader confidence="0.993786">
4.2 Possible overestimation of hypernyms
</subsectionHeader>
<bodyText confidence="0.999987846153846">
A closer look suggests that the ratio of
hypernym-hyponym pairs was somewhat overes-
timated. This is due to the algorithm used in our
data construction. It was often the case that head
nouns were extracted as bare nouns from com-
plex, much longer noun phrases, sometimes due
to the extraction algorithms or parse errors. This
resulted in accidental removal of modifiers be-
ing attached to head nouns in their original uses.
We have not yet checked how often this was the
case. We are aware that this could have resulted
in the overestimation of the ratio of hypernymic
relations in our data.
</bodyText>
<subsectionHeader confidence="0.998178">
4.3 Remaining issues
</subsectionHeader>
<bodyText confidence="0.99315645">
As stated, the fourth largest class, roughly 6.31%
of the total, is that of the pairs in the “other”
unidentified relation [o]. In our setting, “other”
means that it is in none among the synonymous,
classmate, part-whole or hypernym-hyponym re-
lation. A closer look into some examples of
o suggest that they are pairs of terms with ex-
tremely vague association or contrast.
Admittedly, 6.31% is not a large number, but
its ratio is comparable with that of the allo-
graphic pairs [v], 6.92%. We have no explana-
tion why we have this much of an unindenfiable
kind of semantic relation distinguished from un-
related pairs [u]. All we can say now is that we
need further investigation into it.
u is not as large as o, but it has a status similar
to o. We need to know why this much amount of
this kind of pairs. A possible answer would be
that they are caused by parse errors, directly or
indirectly.
</bodyText>
<sectionHeader confidence="0.999348" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.998839416666667">
We analyzed the details of the Japanese nominal
terms automatically constructed under the “dis-
tributional hypothesis,” as in Harris (1954). We
had two aims. One aim was to examine to see
if what we acquire under the hypothesis is ex-
actly what we expect, i.e., if distributional sim-
ilarity can be equated with semantic similarity.
The other aim was to see what kind of seman-
tic relations comprise a class of distributionally
similar terms.
For the first aim, we obtained a positive result:
nearly 88% of the pairs in the data turned out to
be semantically similar under the 18 criteria de-
fined in (5), which include hypernym-hyponym,
meronymic, contrastive, and synonymic rela-
tions. Though some term pairs we evaluated
were among none of these relations, the ratio of
o and u in sum is about 14% and within the ac-
ceptable range.
For the second aim, our result revealed that
the ratio of the classmates, synonymous, rela-
tion, hypernym-hyponym, and meronymic rela-
tions are respectively about 62%, 17%, 8% and
1% of the classified data.
</bodyText>
<page confidence="0.996371">
48
</page>
<sectionHeader confidence="0.4066" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.764874">
Fellbaum, C., ed. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Overall, these results suggest that automatic
acquisition of terms under the distributional hy-
pothesis give us reasonable results.
</bodyText>
<subsectionHeader confidence="0.721016">
A Clustering of one million nominals
</subsectionHeader>
<bodyText confidence="0.964341666666667">
This appendix provides some details on how the
clustering of one million nominal terms was per-
formed.
To determine the similarity metric of a pair of
nominal terms (t1, t2), Kazama et al. (2009) used
the Jensen-Shannon divergence (JS-divergence)
DJS(p||q) = 12D(p||M) + 12D(q||M), where p
and q are probability distributions, and D =
q���
Ei p(i)log (Kullback-Leibler divergence, or
KL-divergence) of p and q, and M = 12(p + q).
We obtained p and q in the following way.
Instead of using raw distribution, Kazama et
al. (2009) applied smoothing using EM algo-
rithm (Rooth et al., 1999; Torisawa, 2001). In
Torisawa’s model (2001), the probability of the
occurrence of the dependency relation (v,r,n) is
defined as:
</bodyText>
<equation confidence="0.9172015">
P((v,r,t)) =def Y, P((v,r)|a)P(t|a)P(a),
aCA
</equation>
<bodyText confidence="0.9972622">
where a denotes a hidden class of (v,r) and term
t. In this equation, the probabilities P((v,r)|a),
P(t|a), and P(a) cannot be calculated directly
because class a is not observed in a given depen-
dency data. The EM-based clustering method
estimates these probabilities using a given cor-
pus. In the E-step, the probability P(a|(v,r))
is calculated. In the M-step, the probabilities
P((v,r)|a), P(t|a), and P(a) are updated until
the likelihood is improved using the results of
the E-step. From the results of this EM-based
clustering method, we can obtain the probabili-
ties P((v,r)|a), P(t|a), and P(a) for each (v,r), t,
and a. Then, P(a|t) is calculated by the follow-
ing equation:
</bodyText>
<equation confidence="0.997841">
P(t|a)P(a)
P(a|t) = EacAP(t|a)P(a)�
</equation>
<bodyText confidence="0.9904185">
The distributional similarity between t1 and t2
was calculated by the JS divergence between
</bodyText>
<reference confidence="0.986916041666667">
P(a|t1) and P(a|t2).
Grefenstette, G. 1993. Automatic thesaurus gener-
ation from raw text using knowledge-poor tech-
niques. In In Making Sense of Words: The 9th
Annual Conference of the UW Centre for the New
OED and Text Research.
Harris, Z. S. 1954. Distributional structure. Word,
10(2-3):146–162. Reprinted in Fodor, J. A and
Katz, J. J. (eds.), Readings in the Philosophy
of Language, pp. 33–49. Englewood Cliffs, NJ:
Prentice-Hall.
Hindle, D. 1990. Noun classification from predicate-
argument structures. In Proceedings of ACL-90,
pp. 268–275, Pittsburgh, PA.
Kazama, J. and K. Torisawa. 2008. Inducing
gazetteers for named entity recognition by large-
scale clustering of dependency relations. In Pro-
ceedings ofACL-2008: HLT, pp. 407–415.
Kazama, J., S. De Saeger, K. Torisawa, and M. Mu-
rata. 2009. Generating a large-scale analogy list
using a probabilistic clustering based on noun-
verb dependency profiles. In Proceedings of the
15th Annual Meeting of the Association for Natu-
ral Language Processing. [in Japanese].
Lee, L. 1997. Similarity-Based Approaches to Natu-
ral Language Processing. Unpublished Ph.D. the-
sis, Harvard University.
Lin, D. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING/ACL-
98, Montreal, Canda, pages 768–774.
Murphy, M. L. 2003. Semantic Relations and the
Lexicon. Cambridge University Press, Cambridge,
UK.
Rooth, M., S. Riezler, D. Presher, G. Carroll, and
F. Beil. 1999. Inducing a semantically annotated
lexicon via em-based clustering. In Proceedings
of the 37th Annual Meeting of the Association for
Computational Linguistics, pp. 104–111.
Shinzato, K., T. Shibata, D. Kawahara, C. Hashimoto,
and S. Kurohashi. 2008. TSUBAKI: An open
search engine infrastructure for developing new
information access. In Proceedings of IJCNLP
2008.
Torisawa, K. 2001. An unsupervised method for
canonicalization of Japanese postpositions. In
Proceedings of the 6th Natural Language Process-
ing Pacific Rim Symposium (NLPRS), pp. 211–
218.
</reference>
<page confidence="0.999544">
49
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.514619">
<title confidence="0.99985">A Look inside the Distributionally Similar Terms</title>
<author confidence="0.97724">Kow Kuroda Jun’ichi Kazama Kentaro Torisawa</author>
<email confidence="0.536387">kuroda@nict.go.jpkazama@nict.go.jp</email>
<affiliation confidence="0.970666">National Institute of Information and Communications Technology (NICT)</affiliation>
<abstract confidence="0.999086166666667">We analyzed the details of a Web-derived distributional data of Japanese nominal terms with two aims. One aim is to examine if distributionally similar terms can be in fact equated with “semantically similar” terms, and if so to what extent. The other is to investigate into what kind of semantic relations constitute (strongly) distributionally similar terms. Our results show that over 85% of the pairs of the terms derived from the highly similar terms turned out to be semantically similar in some way. The ratio of “classmate,” synonymous, hypernym-hyponym, and meronymic relations are about 62%, 17%, 8% and 1% of the classified data, respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<marker></marker>
<rawString>P(a|t1) and P(a|t2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Automatic thesaurus generation from raw text using knowledge-poor techniques.</title>
<date>1993</date>
<booktitle>In In Making Sense of Words: The 9th Annual Conference of the UW Centre for the New OED and Text Research.</booktitle>
<contexts>
<context position="1155" citStr="Grefenstette (1993)" startWordPosition="174" endWordPosition="175">constitute (strongly) distributionally similar terms. Our results show that over 85% of the pairs of the terms derived from the highly similar terms turned out to be semantically similar in some way. The ratio of “classmate,” synonymous, hypernym-hyponym, and meronymic relations are about 62%, 17%, 8% and 1% of the classified data, respectively. 1 Introduction The explosion of online text allows us to enjoy a broad variety of large-scale lexical resources constructed from the texts in the Web in an unsupervised fashion. This line of approach was pioneered by researchers such as Hindle (1990), Grefenstette (1993), Lee (1997) and Lin (1998). At the heart of the approach is a crucial working assumption called “distributional hypothesis,” as with Harris (1954). We now see an impressive number of applications in natural language processing (NLP) that benefit from lexical resources directly or indirectly derived from this assumption. It seems that most researchers are reasonably satisfied with the results obtained thus far. Does this mean, however, that the distributional hypothesis was proved to be valid? Not necessarily: while we have a great deal of confirmative results reported in a variety of research</context>
</contexts>
<marker>Grefenstette, 1993</marker>
<rawString>Grefenstette, G. 1993. Automatic thesaurus generation from raw text using knowledge-poor techniques. In In Making Sense of Words: The 9th Annual Conference of the UW Centre for the New OED and Text Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z S Harris</author>
</authors>
<date>1954</date>
<booktitle>Distributional structure. Word,</booktitle>
<pages>10--2</pages>
<editor>Fodor, J. A and Katz, J. J. (eds.),</editor>
<publisher>Prentice-Hall.</publisher>
<location>Englewood Cliffs, NJ:</location>
<note>Reprinted in</note>
<contexts>
<context position="1302" citStr="Harris (1954)" startWordPosition="197" endWordPosition="198">ed out to be semantically similar in some way. The ratio of “classmate,” synonymous, hypernym-hyponym, and meronymic relations are about 62%, 17%, 8% and 1% of the classified data, respectively. 1 Introduction The explosion of online text allows us to enjoy a broad variety of large-scale lexical resources constructed from the texts in the Web in an unsupervised fashion. This line of approach was pioneered by researchers such as Hindle (1990), Grefenstette (1993), Lee (1997) and Lin (1998). At the heart of the approach is a crucial working assumption called “distributional hypothesis,” as with Harris (1954). We now see an impressive number of applications in natural language processing (NLP) that benefit from lexical resources directly or indirectly derived from this assumption. It seems that most researchers are reasonably satisfied with the results obtained thus far. Does this mean, however, that the distributional hypothesis was proved to be valid? Not necessarily: while we have a great deal of confirmative results reported in a variety of research areas, but we would rather say that the hypothesis has never been fully “validated” for two reasons. First, it has yet to be tested under the prec</context>
<context position="29046" citStr="Harris (1954)" startWordPosition="5099" endWordPosition="5100">comparable with that of the allographic pairs [v], 6.92%. We have no explanation why we have this much of an unindenfiable kind of semantic relation distinguished from unrelated pairs [u]. All we can say now is that we need further investigation into it. u is not as large as o, but it has a status similar to o. We need to know why this much amount of this kind of pairs. A possible answer would be that they are caused by parse errors, directly or indirectly. 5 Conclusion We analyzed the details of the Japanese nominal terms automatically constructed under the “distributional hypothesis,” as in Harris (1954). We had two aims. One aim was to examine to see if what we acquire under the hypothesis is exactly what we expect, i.e., if distributional similarity can be equated with semantic similarity. The other aim was to see what kind of semantic relations comprise a class of distributionally similar terms. For the first aim, we obtained a positive result: nearly 88% of the pairs in the data turned out to be semantically similar under the 18 criteria defined in (5), which include hypernym-hyponym, meronymic, contrastive, and synonymic relations. Though some term pairs we evaluated were among none of t</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Harris, Z. S. 1954. Distributional structure. Word, 10(2-3):146–162. Reprinted in Fodor, J. A and Katz, J. J. (eds.), Readings in the Philosophy of Language, pp. 33–49. Englewood Cliffs, NJ: Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Noun classification from predicateargument structures.</title>
<date>1990</date>
<booktitle>In Proceedings of ACL-90,</booktitle>
<pages>268--275</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="1134" citStr="Hindle (1990)" startWordPosition="172" endWordPosition="173">ntic relations constitute (strongly) distributionally similar terms. Our results show that over 85% of the pairs of the terms derived from the highly similar terms turned out to be semantically similar in some way. The ratio of “classmate,” synonymous, hypernym-hyponym, and meronymic relations are about 62%, 17%, 8% and 1% of the classified data, respectively. 1 Introduction The explosion of online text allows us to enjoy a broad variety of large-scale lexical resources constructed from the texts in the Web in an unsupervised fashion. This line of approach was pioneered by researchers such as Hindle (1990), Grefenstette (1993), Lee (1997) and Lin (1998). At the heart of the approach is a crucial working assumption called “distributional hypothesis,” as with Harris (1954). We now see an impressive number of applications in natural language processing (NLP) that benefit from lexical resources directly or indirectly derived from this assumption. It seems that most researchers are reasonably satisfied with the results obtained thus far. Does this mean, however, that the distributional hypothesis was proved to be valid? Not necessarily: while we have a great deal of confirmative results reported in </context>
<context position="5086" citStr="Hindle (1990)" startWordPosition="861" endWordPosition="862">the type counts of dependency relations, which is roughly equated with the “frequencies” of the terms. Each base term in B is associated with up to 500 of the most distributionally similar terms. This defines T. For M, we used the Jensen-Shannon divergence (JS-divergence) base on the probability distributions derived by an EM-based soft clustering (Kazama and Torisawa, 2008). For convenience, some relevant details of the data construction are described in Appendix A, but in a nutshell, we used dependency relations as distributional information. This makes our method comparable to that used in Hindle (1990). The statistics of the distributional data used were as follows: roughly 920 million types of dependency relations1) were automatically acquired 1)The 920 million types come in two kinds of context triples: 590 million types of (t, p,v) and 320 million types from a large-scale Japanese Web-corpus called the Tsubaki corpus (Shinzato et al., 2008) which consists of roughly 100 million Japanese pages with six billion sentences. After excluding hapax nouns, we had about 33 million types of nouns (in terms of string) and 27 million types of verbs. These nouns were ranked by type count of the two c</context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>Hindle, D. 1990. Noun classification from predicateargument structures. In Proceedings of ACL-90, pp. 268–275, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kazama</author>
<author>K Torisawa</author>
</authors>
<title>Inducing gazetteers for named entity recognition by largescale clustering of dependency relations.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-2008: HLT,</booktitle>
<pages>407--415</pages>
<contexts>
<context position="4386" citStr="Kazama and Torisawa, 2008" startWordPosition="738" endWordPosition="741"> measure M to construct a list of n terms T = [ti,1, ti,2,... , ti,j, ... , ti,n] where ti,j denotes the jth most similarity term in T against bi ∈ B. P(k) are pairs of bi and ti,k, i.e., the k-th most similar term to bi. c. Human raters classify a portion Q of the pairs in P(k) with reference to a classification guideline prepared for the task. Note that the selection of base set B can be independent of the selection of T. Note also that T is indexed by terms in B. To encode this, we write: T[bi] = [ti,1, ti,2,... , ti,j, ... , ti,n]. 2.2 Data For T, we used Kazama’s nominal term clustering (Kazama and Torisawa, 2008; Kazama et al., 2009). In this data, base set B for T is one million terms defined by the type counts of dependency relations, which is roughly equated with the “frequencies” of the terms. Each base term in B is associated with up to 500 of the most distributionally similar terms. This defines T. For M, we used the Jensen-Shannon divergence (JS-divergence) base on the probability distributions derived by an EM-based soft clustering (Kazama and Torisawa, 2008). For convenience, some relevant details of the data construction are described in Appendix A, but in a nutshell, we used dependency rel</context>
</contexts>
<marker>Kazama, Torisawa, 2008</marker>
<rawString>Kazama, J. and K. Torisawa. 2008. Inducing gazetteers for named entity recognition by largescale clustering of dependency relations. In Proceedings ofACL-2008: HLT, pp. 407–415.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kazama</author>
<author>S De Saeger</author>
<author>K Torisawa</author>
<author>M Murata</author>
</authors>
<title>Generating a large-scale analogy list using a probabilistic clustering based on nounverb dependency profiles.</title>
<date>2009</date>
<booktitle>In Proceedings of the 15th Annual Meeting of</booktitle>
<note>[in Japanese].</note>
<marker>Kazama, De Saeger, Torisawa, Murata, 2009</marker>
<rawString>Kazama, J., S. De Saeger, K. Torisawa, and M. Murata. 2009. Generating a large-scale analogy list using a probabilistic clustering based on nounverb dependency profiles. In Proceedings of the 15th Annual Meeting of the Association for Natural Language Processing. [in Japanese].</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lee</author>
</authors>
<title>Similarity-Based Approaches to Natural Language Processing. Unpublished Ph.D. thesis,</title>
<date>1997</date>
<institution>Harvard University.</institution>
<contexts>
<context position="1167" citStr="Lee (1997)" startWordPosition="176" endWordPosition="177"> distributionally similar terms. Our results show that over 85% of the pairs of the terms derived from the highly similar terms turned out to be semantically similar in some way. The ratio of “classmate,” synonymous, hypernym-hyponym, and meronymic relations are about 62%, 17%, 8% and 1% of the classified data, respectively. 1 Introduction The explosion of online text allows us to enjoy a broad variety of large-scale lexical resources constructed from the texts in the Web in an unsupervised fashion. This line of approach was pioneered by researchers such as Hindle (1990), Grefenstette (1993), Lee (1997) and Lin (1998). At the heart of the approach is a crucial working assumption called “distributional hypothesis,” as with Harris (1954). We now see an impressive number of applications in natural language processing (NLP) that benefit from lexical resources directly or indirectly derived from this assumption. It seems that most researchers are reasonably satisfied with the results obtained thus far. Does this mean, however, that the distributional hypothesis was proved to be valid? Not necessarily: while we have a great deal of confirmative results reported in a variety of research areas, but </context>
</contexts>
<marker>Lee, 1997</marker>
<rawString>Lee, L. 1997. Similarity-Based Approaches to Natural Language Processing. Unpublished Ph.D. thesis, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL98,</booktitle>
<pages>768--774</pages>
<location>Montreal, Canda,</location>
<contexts>
<context position="1182" citStr="Lin (1998)" startWordPosition="179" endWordPosition="180">ly similar terms. Our results show that over 85% of the pairs of the terms derived from the highly similar terms turned out to be semantically similar in some way. The ratio of “classmate,” synonymous, hypernym-hyponym, and meronymic relations are about 62%, 17%, 8% and 1% of the classified data, respectively. 1 Introduction The explosion of online text allows us to enjoy a broad variety of large-scale lexical resources constructed from the texts in the Web in an unsupervised fashion. This line of approach was pioneered by researchers such as Hindle (1990), Grefenstette (1993), Lee (1997) and Lin (1998). At the heart of the approach is a crucial working assumption called “distributional hypothesis,” as with Harris (1954). We now see an impressive number of applications in natural language processing (NLP) that benefit from lexical resources directly or indirectly derived from this assumption. It seems that most researchers are reasonably satisfied with the results obtained thus far. Does this mean, however, that the distributional hypothesis was proved to be valid? Not necessarily: while we have a great deal of confirmative results reported in a variety of research areas, but we would rather</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, D. 1998. Automatic retrieval and clustering of similar words. In Proceedings of COLING/ACL98, Montreal, Canda, pages 768–774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M L Murphy</author>
</authors>
<title>Semantic Relations and the Lexicon.</title>
<date>2003</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="8829" citStr="Murphy (2003)" startWordPosition="1492" endWordPosition="1493">distributionally similar terms can really be equated with semantically similar terms. Put differently, what kinds of terms can we find in the sets constructed using distributionally similarity? We can confirm the hypothesis if the most of the term pairs are proved to be semantically similar for most sets of terms constructed based on the distributional hypothesis. To do this, however, we need to clarify what constitutes semantic similarity. We will deal with this prerequisite. 2.3 Classification 2.3.1 Factoring out “semantic similarity” Building on lexicographic works like Fellbaum (1998) and Murphy (2003), we assume that the following are the four major classes of semantic relation that contribute to semantic similarity between two terms: (4) a. “synonymic” relation (one can substitute for another on an identity basis). Examples are (Microsoft, MS). b. “hypernym-hyponym” relation between two terms (one can substitute for another on un underspecification/abstraction basis). Examples are (guys, players) c. “meronymic” (part-whole) relation between two terms (one term can be a substitute for another on metonymic basis). Examples are (bodies, players) [cf. All the players have strong bodies] d. “c</context>
</contexts>
<marker>Murphy, 2003</marker>
<rawString>Murphy, M. L. 2003. Semantic Relations and the Lexicon. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rooth</author>
<author>S Riezler</author>
<author>D Presher</author>
<author>G Carroll</author>
<author>F Beil</author>
</authors>
<title>Inducing a semantically annotated lexicon via em-based clustering.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="30782" citStr="Rooth et al., 1999" startWordPosition="5389" endWordPosition="5392">Clustering of one million nominals This appendix provides some details on how the clustering of one million nominal terms was performed. To determine the similarity metric of a pair of nominal terms (t1, t2), Kazama et al. (2009) used the Jensen-Shannon divergence (JS-divergence) DJS(p||q) = 12D(p||M) + 12D(q||M), where p and q are probability distributions, and D = q��� Ei p(i)log (Kullback-Leibler divergence, or KL-divergence) of p and q, and M = 12(p + q). We obtained p and q in the following way. Instead of using raw distribution, Kazama et al. (2009) applied smoothing using EM algorithm (Rooth et al., 1999; Torisawa, 2001). In Torisawa’s model (2001), the probability of the occurrence of the dependency relation (v,r,n) is defined as: P((v,r,t)) =def Y, P((v,r)|a)P(t|a)P(a), aCA where a denotes a hidden class of (v,r) and term t. In this equation, the probabilities P((v,r)|a), P(t|a), and P(a) cannot be calculated directly because class a is not observed in a given dependency data. The EM-based clustering method estimates these probabilities using a given corpus. In the E-step, the probability P(a|(v,r)) is calculated. In the M-step, the probabilities P((v,r)|a), P(t|a), and P(a) are updated unt</context>
</contexts>
<marker>Rooth, Riezler, Presher, Carroll, Beil, 1999</marker>
<rawString>Rooth, M., S. Riezler, D. Presher, G. Carroll, and F. Beil. 1999. Inducing a semantically annotated lexicon via em-based clustering. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pp. 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Shinzato</author>
<author>T Shibata</author>
<author>D Kawahara</author>
<author>C Hashimoto</author>
<author>S Kurohashi</author>
</authors>
<title>TSUBAKI: An open search engine infrastructure for developing new information access.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP</booktitle>
<contexts>
<context position="5434" citStr="Shinzato et al., 2008" startWordPosition="914" endWordPosition="917">clustering (Kazama and Torisawa, 2008). For convenience, some relevant details of the data construction are described in Appendix A, but in a nutshell, we used dependency relations as distributional information. This makes our method comparable to that used in Hindle (1990). The statistics of the distributional data used were as follows: roughly 920 million types of dependency relations1) were automatically acquired 1)The 920 million types come in two kinds of context triples: 590 million types of (t, p,v) and 320 million types from a large-scale Japanese Web-corpus called the Tsubaki corpus (Shinzato et al., 2008) which consists of roughly 100 million Japanese pages with six billion sentences. After excluding hapax nouns, we had about 33 million types of nouns (in terms of string) and 27 million types of verbs. These nouns were ranked by type count of the two context triples, i.e., (t, p,v) and (n*, p*,t). B was determined by selecting the top one million terms with the most variations of context triples. 2.2.1 Sample of T[b] For illustration, we present examples of the Web-derived distributional similar terms. (2) shows the 10 most distributionally similar terms (i.e., [t1070,1, t1070,2, . . . , t1070</context>
</contexts>
<marker>Shinzato, Shibata, Kawahara, Hashimoto, Kurohashi, 2008</marker>
<rawString>Shinzato, K., T. Shibata, D. Kawahara, C. Hashimoto, and S. Kurohashi. 2008. TSUBAKI: An open search engine infrastructure for developing new information access. In Proceedings of IJCNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Torisawa</author>
</authors>
<title>An unsupervised method for canonicalization of Japanese postpositions.</title>
<date>2001</date>
<booktitle>In Proceedings of the 6th Natural Language Processing Pacific Rim Symposium (NLPRS),</booktitle>
<pages>211--218</pages>
<contexts>
<context position="30799" citStr="Torisawa, 2001" startWordPosition="5393" endWordPosition="5394">llion nominals This appendix provides some details on how the clustering of one million nominal terms was performed. To determine the similarity metric of a pair of nominal terms (t1, t2), Kazama et al. (2009) used the Jensen-Shannon divergence (JS-divergence) DJS(p||q) = 12D(p||M) + 12D(q||M), where p and q are probability distributions, and D = q��� Ei p(i)log (Kullback-Leibler divergence, or KL-divergence) of p and q, and M = 12(p + q). We obtained p and q in the following way. Instead of using raw distribution, Kazama et al. (2009) applied smoothing using EM algorithm (Rooth et al., 1999; Torisawa, 2001). In Torisawa’s model (2001), the probability of the occurrence of the dependency relation (v,r,n) is defined as: P((v,r,t)) =def Y, P((v,r)|a)P(t|a)P(a), aCA where a denotes a hidden class of (v,r) and term t. In this equation, the probabilities P((v,r)|a), P(t|a), and P(a) cannot be calculated directly because class a is not observed in a given dependency data. The EM-based clustering method estimates these probabilities using a given corpus. In the E-step, the probability P(a|(v,r)) is calculated. In the M-step, the probabilities P((v,r)|a), P(t|a), and P(a) are updated until the likelihood</context>
</contexts>
<marker>Torisawa, 2001</marker>
<rawString>Torisawa, K. 2001. An unsupervised method for canonicalization of Japanese postpositions. In Proceedings of the 6th Natural Language Processing Pacific Rim Symposium (NLPRS), pp. 211– 218.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>