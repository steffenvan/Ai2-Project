<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.9974015">
SU-FMI: System Description for SemEval-2014 Task 9
on Sentiment Analysis in Twitter
</title>
<author confidence="0.782239">
Boris Velichkov∗, Borislav Kapukaranov†, Ivan Grozev‡, Jeni Karanesheva§, Todor Mihaylov¶,
Yasen Kiprovk, Georgi Georgiev∗∗
</author>
<title confidence="0.350666">
, Ivan Koychev††
, Preslav Nakov‡‡
</title>
<sectionHeader confidence="0.945099" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999714823529412">
We describe the submission of the team
of the Sofia University to SemEval-2014
Task 9 on Sentiment Analysis in Twit-
ter. We participated in subtask B, where
the participating systems had to predict
whether a Twitter message expresses pos-
itive, negative, or neutral sentiment. We
trained an SVM classifier with a linear
kernel using a variety of features. We
used publicly available resources only, and
thus our results should be easily replicable.
Overall, our system is ranked 20th out of
50 submissions (by 44 teams) based on the
average of the three 2014 evaluation data
scores, with an F1-score of 63.62 on gen-
eral tweets, 48.37 on sarcastic tweets, and
68.24 on LiveJournal messages.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997506">
We describe the submission of the team of the
Sofia University, Faculty of Mathematics and In-
formatics (SU-FMI) to SemEval-2014 Task 9 on
Sentiment Analysis in Twitter (Rosenthal et al.,
2014).
</bodyText>
<footnote confidence="0.9929424">
Sofia University, bobby.velichkov@gmail.com
Sofia University, b.kapukaranov@gmail.com
Sofia University, iigrozev@gmail.com
Sofia University, j.karanesheva@gmail.com
Sofia University, tbmihailov@gmail.com
</footnote>
<figure confidence="0.561796666666667">
�
Sofia University, yasen.kiprov@gmail.com
Ontotext, g.d.georgiev@gmail.com
��
Sofia University, koychev@fmi.uni-sofia.bg
��
Qatar
�� Computing Research Institute,
pnakov@qf.org.qa
</figure>
<footnote confidence="0.87292675">
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
</footnote>
<note confidence="0.236109">
This SemEval challenge had two subtasks:
</note>
<listItem confidence="0.941377111111111">
• subtaskA (term-level) asks to predict the sen-
timent of a phrase inside a tweet;
• subtask B (message-level) asks to predict the
overall sentiment of a tweet message.
In both subtasks, the sentiment can be positive,
negative, or neutral. Here are some examples:
• positive: Gas by my house hit $3.39!!!! I’m
going to Chapel Hill on Sat.:)
• neutral: New York Giants: Game-by-Game
Predictions for the 2nd Half of the Season
http://t.co/yK9VTjcs
• negative: Why the hell does Selma have
school tomorrow but Parlier clovis &amp; others
don’t?
• negative (sarcastic): @MetroNorth wall to
wall people on the platform at South Nor-
walk waiting for the 8:08. Thanks for the Sat.
Sched. Great sense
</listItem>
<bodyText confidence="0.9999748">
Below we first describe our preprocessing, fea-
tures and classifier in Section 2. Then, we discuss
our experiments, results and analysis in Section 3.
Finally, we conclude with possible directions for
future work in Section 4.
</bodyText>
<sectionHeader confidence="0.944796" genericHeader="introduction">
2 Method
</sectionHeader>
<bodyText confidence="0.999037375">
Our approach is inspired by the highest scoring
team in 2013, NRC Canada (Mohammad et al.,
2013). We reused many of their resources.1
Our system consists of two main submodules,
(i) feature extraction in the framework of GATE
(Cunningham et al., 2011), and (ii) machine learn-
ing using SVM with linear kernels as implemented
in LIBLINEAR2 (Fan et al., 2008).
</bodyText>
<footnote confidence="0.9992458">
1http://www.umiacs.umd.edu/
˜saif/WebPages/Abstracts/
NRC-SentimentAnalysis.htm
2http://www.csie.ntu.edu.tw/˜cjlin/
liblinear/
</footnote>
<page confidence="0.852439">
590
</page>
<note confidence="0.793412">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 590–595,
Dublin, Ireland, August 23-24, 2014.
</note>
<subsectionHeader confidence="0.990311">
2.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999972266666667">
We integrated a pipeline of various resources for
tweet analysis that are already available in GATE
(Bontcheva et al., 2013) such as a Twitter tok-
enizer, a sentence splitter, a hashtag tokenizer, a
Twitter POS tagger, a morphological analyzer, and
the Snowball3 stemmer.
We further implemented in GATE some shal-
low text processing components in order to handle
negation contexts, emoticons, elongated words,
all-caps words and punctuation. We also added
components to find words and phrases contained
in sentiment lexicons, as well as to annotate words
with word cluster IDs using the lexicon built at
CMU,4 which uses the Brown clusters (Brown et
al., 1992) as implemented5 by (Liang, 2005).
</bodyText>
<subsectionHeader confidence="0.911701">
2.2 Features
2.2.1 Sentiment lexicon features
</subsectionHeader>
<bodyText confidence="0.996343">
We used several preexisting lexicons, both manu-
ally designed and automatically generated:
</bodyText>
<listItem confidence="0.967932588235294">
• Minqing Hu and Bing Liu opinion lexicon
(Hu and Liu, 2004): 4,783 positive and 2,006
negative terms;
• MPQA Subjectivity Cues Lexicon (Wilson et
al., 2005): 8,222 terms;
• Macquarie Semantic Orientation Lexicon
(MSOL) (Mohammad et al., 2009): 30,458
positive and 45,942 negative terms;
• NRC Emotion Lexicon (Mohammad et al.,
2013): 14,181 terms with specified emotion.
For each lexicon, we find in the tweet the terms
that are listed in it, and then we calculate the fol-
lowing features:
• Negative terms count;
• Positive terms count;
• Positive negated terms count;
• Positive/negative terms count ratio;
• Sentiment of the last token;
• Overall sentiment terms count.
We further used the following lexicons:
• NRC Hashtag Sentiment Lexicon: list of
words and their associations with positive
and negative sentiment (Mohammad et al.,
2013): 54,129 unigrams, 316,531 bigrams,
480,010 pairs, and 78 high-quality positive
and negative hashtag terms;
• Sentiment140 Lexicon: list of words with as-
sociations to positive and negative sentiments
(Mohammad et al., 2013): 62,468 unigrams,
677,698 bigrams, 480,010 pairs;
• Stanford Sentiment Treebank: contains
239,231 evaluated words and phrases. If a
word or a phrase was found in the tweet, we
took the given sentiment label.
</listItem>
<bodyText confidence="0.693465333333333">
For the NRC Hashtag Sentiment Lexicon and
the Sentiment140 Lexicon, we calculated the fol-
lowing features for unigrams, bigrams and pairs:
</bodyText>
<listItem confidence="0.999645666666667">
• Sum of positive terms’ sentiment;
• Sum of negative terms’ sentiment;
• Sum of the sentiment for all terms in the
tweet;
• Sum of negated positive terms’ sentiment;
• Negative/positive terms ratio;
• Max positive sentiment;
• Min negative sentiment;
• Max sentiment of a term.
</listItem>
<bodyText confidence="0.999991230769231">
We used different features for the two lexicon
groups because their contents differ. The first four
lexicons provide a discrete sentiment value for
each word. In contrast, the following two lexicons
offer numeric sentiment scores, which allows for
different feature types such as sums and min/max
scores.
Finally, we manually built a new lexicon with
all emoticons we could find, where we assigned to
each emoticon a positive or a negative label. We
then calculated four features: number of positive
and negative emoticons in the tweet, and whether
the last token is a positive or a negative emoticon.
</bodyText>
<footnote confidence="0.9508844">
3http://snowball.tartarus.org/
4http://www.ark.cs.cmu.edu/TweetNLP/
cluster_viewer.html
5http://github.com/percyliang/
brown-cluster
</footnote>
<page confidence="0.99223">
591
</page>
<subsubsectionHeader confidence="0.584714">
2.2.2 Tweet-level features
</subsubsectionHeader>
<bodyText confidence="0.999504">
We use the following tweet-level features:
</bodyText>
<listItem confidence="0.999214">
• All caps: the number of words with all char-
acters in upper case;
• Hashtags: the number ot hashtags in the
tweet;
• Elongated words: the number of words with
character repetitions.
</listItem>
<subsubsectionHeader confidence="0.568367">
2.2.3 Term-level features
</subsubsectionHeader>
<bodyText confidence="0.998319">
We used the following term-level features:
</bodyText>
<listItem confidence="0.900859230769231">
• Word n-grams: presence or absence of 1-
grams, 2-grams, 3-grams, 4-grams, and 5-
grams. We add an NGRAM prefix to each n-
gram. Unfortunately, the n-grams increase
the feature space greatly and contribute to
higher sparseness. They also slow down
training dramatically. That is why our final
submission only includes 1-grams.
• Character n-grams: presence or absence of
one, two, three, four and five-character pre-
fixes and suffixes of all words. We add a PRE
or SUF prefix to each character n-gram.
• Negations: the number of negated contexts.
</listItem>
<bodyText confidence="0.793177625">
We define a negated context as a segment
of a tweet that starts with a negation word
(e.g., no, shouldnt) from our custom gazetteer
and ends with one of the punctuation marks:
,, ., :, ;, !, ?. A negated context affects the
n-gram and the lexicon features: we add a
NEG suffix to each word following the nega-
tion word, e.g., perfect becomes perfect NEG.
</bodyText>
<listItem confidence="0.995915870967742">
• Punctuation: the number of contiguous se-
quences of exclamation marks, of question
marks, of either exclamation or question
marks, and of both exclamation and question
marks. Also, whether the last token contains
an exclamation or a question mark (excluding
URLs).
• Stemmer: the stem of each word, excluding
URLs. We add a STEM prefix to each stem.
• Lemmatizer: the lemma of each word, ex-
cluding URLs. We add a LEMMA prefix to
each lemma. We use the built-in GATE Mor-
phological analyser as our lemmatizer.
• Word and word bigram clusters: word
clusters have been shown to improve the per-
formance of supervised NLP models (Turian
et al., 2010). We use the word clusters built
by CMU’s NLP toolkit, which were produced
over a collection of 56 million English tweets
(Owoputi et al., 2012) and built using the
Percy Liang’s HMM-based implementation6
of Brown clustering (Liang, 2005; Brown et
al., 1992), which group the words into 1,000
hierarchical clusters. We use two features
based on these clusters:
– presence/absence of a word in a word
cluster;
– presence/absence of a bigram in a bi-
gram cluster.
• POS tagging: Social media are generally
hard to process using standard NLP tools,
</listItem>
<bodyText confidence="0.936292647058824">
which are typically developed with newswire
text in mind. Such standard tools are not
a good fit for Twitter messages, which are
too brief, contain typos and special word-
forms. Thus, we used a specialized POS
tagger, TwitIE, which is available in GATE
(Bontcheva et al., 2013), and which we in-
tegrated in our pipeline. It provides (i) a
tokenizer specifically trained to handle smi-
lies, user names, URLs, etc., (ii) a normal-
izer to correct slang and misspellings, and
(iii) a POS tagger that uses the Penn Treebank
tagset, but is optimized for tweets. Using the
TwitIE toolkit, we performed POS tagging
and we extracted all POS tag types that we
can find in the tweet together with their fre-
quencies as features.
</bodyText>
<subsectionHeader confidence="0.98251">
2.3 Classifier
</subsectionHeader>
<bodyText confidence="0.9975975">
For classification, we used the above features and
a support vector machine (SVM) classifier as im-
plemented in LIBLINEAR. This is a very scal-
able implementation of SVM that does not support
kernels, and is suitable for classification on large
datasets with a large number of features. This is
particularly useful for text classification, where the
number of features is very large, which means that
the data is likely to be linearly separable, and thus
using kernels is not really necessary. We scaled the
SVM input and we used L2-regularization during
training.
</bodyText>
<footnote confidence="0.993765">
6https://github.com/percyliang/
brown-cluster
</footnote>
<page confidence="0.996593">
592
</page>
<sectionHeader confidence="0.982848" genericHeader="method">
3 Experiments, Results, Analysis
</sectionHeader>
<subsectionHeader confidence="0.989339">
3.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999961272727273">
At development time, we trained on train-2013,
tuned the C value of SVM on dev-2013, and eval-
uated on test-2013 (Nakov et al., 2013). For our
submission, we trained on train-2013+dev-2013,
and we evaluated on the 2014 test dataset pro-
vided by the organizers. This dataset contains two
parts and a total of five datasets: (a) progress test
(the Twitter and SMS test datasets for 2013), and
(b) new test datasets (from Twitter, from Twitter
with sarcasm, and from LiveJournal). We used
C=0.012, which was best on development.
</bodyText>
<subsectionHeader confidence="0.995771">
3.2 Official results
</subsectionHeader>
<bodyText confidence="0.999990555555556">
Due to our very late entering in the competition,
we have only managed to perform a small num-
ber of experiments, and we only participated in
subtask B. We were ranked 20th out of 50 sub-
missions; our official results are shown in Table 1.
The numbers after our score are the delta to the
best solution. We have also included a ranking
among 2014 participant systems on the 2013 data
sets, released by the organizers.
</bodyText>
<table confidence="0.997533571428571">
Data Category F1-score (best) Ranking
tweets2014 63.62 (6.23) 23
sarcasm2014 48.34 (9.82) 19
LiveJournal2014 68.23 (6.60) 21
tweets2013 60.96 (9.79) 29
SMS2013 61.67 (8.61) 16
2014 mean 60.07 (7.55) 20
</table>
<tableCaption confidence="0.999946">
Table 1: Our submitted system for subtask B.
</tableCaption>
<subsectionHeader confidence="0.997448">
3.3 Analysis
</subsectionHeader>
<bodyText confidence="0.999990558139535">
Tables 2 and 3 analyze the impact of the individual
features. They show the F1-scores and the loss
when a feature or a group of features is removed;
we show the impact on all test datasets, both from
2013 and from 2014. The exception here is the all
+ ngrams row, which contains our scores if we had
used the n-grams feature group.
The features are sorted by their impact on
the Twitter2014 test set. We can see that the
three most important feature groups are POS tags,
word/bigram clusters, and lexicons.
We can further see that although the overall lex-
icon feature group is beneficial, some of the lex-
icons actually hurt the 2014 score and we would
have been better off without them.
These are the Sentiment140 lexicon, the Stan-
ford Sentiment Treebank and the NRC Emotion
lexicon. The highest gain we get is from the lex-
icons of Minqing Hu and Bing Liu. It must be
noted that using lexicons with good results ap-
parently depends on the context, e.g., the Senti-
ment140 lexicon seems to be helping a lot with
the LiveJournal test dataset, but it hurts the Sar-
casm score by a sizeable margin.
Another interesting observation is that even
though including the n-gram feature group is per-
forming notably better on the Twitter2013 test
dataset, it actually worsens performance on all
2014 test sets. Had we included it in our results,
we would have scored lower.
The negation context feature brings little in re-
gards to regular tweets or LiveJournal text, but it
heavily improves our score on the Sarcasm tweets.
It is unclear why our results differ so much from
those of the NRC-Canada team in 2013 since our
features are quite similar. We attribute the differ-
ence to the fact that some of the lexicons we use
actually hurt our score as we mentioned above.
Another difference could be that last year’s NRC
system uses n-grams, which we have disabled as
they lowered our scores. Last but not least, there
could be bugs lurking in our feature representation
that additionally lower our results.
</bodyText>
<subsectionHeader confidence="0.967884">
3.4 Post-submission improvements
</subsectionHeader>
<bodyText confidence="0.995778125">
First, we did more extensive experiments to val-
idate our classifier’s C value. We found that the
best value for C is actually 0.08 instead of our
original proposal 0.012.
Then, we experimented further with our lexi-
con features and we removed the following ones,
which resulted in significant improvement over
our submitted version:
</bodyText>
<listItem confidence="0.999740090909091">
• Sentiment of the last token for NRC Emotion,
MSOL, MPQA, and Bing Liu lexicons;
• Max term positive, negative and sentiment
scores for unigrams of Sentiment140 and
NRC Sentiment lexicons;
• Max term positive, negative and sentiment
scores for bigrams of Sentiment140 and NRC
Sentiment lexicons;
• Max term positive, negative and sentiment
scores for hashtags of Sentiment140 and
NRC Sentiment lexicons.
</listItem>
<page confidence="0.995483">
593
</page>
<figure confidence="0.938220357142857">
Feature Diff
submitted features
no POS tags
no word clusters
all lex removed
no Hu-Liu lex
all + ngrams
no NRC #lex
no MSOL lex
no Stanford lex
no negation cntx
no encodings
no NRC emo lex
no Sent140 lex
</figure>
<table confidence="0.999295357142857">
SMS2013 SMS2013 delta Twitter2013 Twitter2013 delta
61.67 60.96
54.73 -6.94 52.32 -8.64
58.06 -3.61 55.44 -5.52
59.94 -1.73 58.35 -2.61
60.56 -1.11 60.10 -0.86
61.37 -0.30 62.22 1.26
61.35 -0.32 60.66 -0.30
61.88 0.21 61.35 0.39
61.84 0.17 61.02 0.06
61.94 0.27 60.88 -0.08
61.74 0.07 60.92 -0.04
61.67 0.00 60.96 0.00
61.61 -0.06 60.32 -0.64
</table>
<tableCaption confidence="0.999405">
Table 2: Ablation experiments on the 2013 test sets.
</tableCaption>
<figure confidence="0.947232571428572">
Feature Diff
submitted features
no POS tags
no word clusters
all lex removed
no Hu-Liu lex
all + ngrams
no NRC #lex
no MSOL lex
no Stanford lex
no negation cntx
no encodings
no NRC emo lex
no Sent140 lex
</figure>
<table confidence="0.999130571428572">
LiveJournal LJ delta Twitter Twitter delta Sarcasm Sarcasm delta
68.23 63.62 48.34
62.28 -5.95 59.00 -4.62 43.70 -4.64
65.08 -3.15 59.82 -3.80 43.96 -4.38
66.16 -2.07 60.73 -2.89 49.59 1.25
66.44 -1.79 62.15 -1.47 46.72 -1.62
67.79 -0.44 62.96 -0.66 47.82 -0.52
66.81 -1.42 63.25 -0.37 47.54 -0.80
68.50 0.27 63.54 -0.08 48.34 0.00
67.86 -0.37 63.70 0.08 48.34 0.00
68.09 -0.14 63.62 0.00 46.37 -1.97
68.23 0.00 63.64 0.02 47.54 -0.80
68.24 0.01 63.62 0.00 48.34 0.00
67.32 -0.91 63.94 0.32 49.47 1.13
</table>
<tableCaption confidence="0.999922">
Table 3: Ablation experiments on the 2014 test sets.
</tableCaption>
<bodyText confidence="0.959602666666667">
The improved scores are shown in Table 4, with
the submitted and the best system results.
Test Set New F1 Old F1 Best
</bodyText>
<table confidence="0.998884166666667">
tweets2014 66.23 63.62 69.85
sarcasm2014 50.00 48.34 58.16
LiveJournal2014 69.41 68.24 74.84
tweets2013 63.08 60.96 70.75
SMS2013 62.28 61.67 70.28
2014 mean 62.20 60.07 67.62
</table>
<tableCaption confidence="0.998486">
Table 4: Our post-submission results.
</tableCaption>
<sectionHeader confidence="0.99122" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999993363636364">
We have described the system built by the team of
SU-FMI for SemEval-2014 task 9. Due to our late
entering in the competition, we were only ranked
20th out of 50 submissions (from 44 teams).
We have made some interesting observations
about the impact of the different features. Among
the best-performing feature groups were POS-tag
counts, word cluster presence and bigrams, the
Hu-Liu lexicon and the NRC Hashtag Sentiment
lexicon. These had the most sustainable perfor-
mance over the 2013 and the 2014 test datasets.
Others we did not use, seemingly more context
dependent, seem to have been more suited for the
2013 test sets like the n-grams feature group.
Even though we made some improvements af-
ter submitting our initial version, we feel there is
more to gain and optimize. There seem to be sev-
eral low-hanging fruits based on our experiments
data, which could add few points to our F1-scores.
Going forward, our goal is to extend our experi-
ments with more feature sub- and super-sets and to
turn our classifier into a state-of-the-art performer.
</bodyText>
<page confidence="0.997664">
594
</page>
<sectionHeader confidence="0.998328" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999408333333333">
This work is partially supported by the FP7-
ICT Strategic Targeted Research Project PHEME
(No. 611233), and by the European Social Fund
through the Human Resource Development Oper-
ational Programme under contract BG051PO001-
3.3.06-0052 (2012/2014).
</bodyText>
<sectionHeader confidence="0.998497" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999886231884058">
Kalina Bontcheva, Leon Derczynski, Adam Funk,
Mark Greenwood, Diana Maynard, and Niraj
Aswani. 2013. TwitIE: An open-source infor-
mation extraction pipeline for microblog text. In
Proceedings of the International Conference on Re-
cent Advances in Natural Language Processing,
RANLP ’13, pages 83–90, Hissar, Bulgaria.
Peter Brown, Peter deSouza, Robert Mercer, Vin-
cent Della Pietra, and Jenifer Lai. 1992. Class-
based n-gram models of natural language. Compu-
tational Linguistics, 18:467–479.
Hamish Cunningham, Diana Maynard, and Kalina
Bontcheva. 2011. Text Processing with GATE.
Gateway Press CA.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. J. Mach.
Learn. Res., 9:1871–1874.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ’04, pages
168–177, New York, NY, USA.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language. Master’s thesis, Massachusetts Insti-
tute of Technology.
Saif Mohammad, Cody Dunne, and Bonnie Dorr.
2009. Generating high-coverage semantic orienta-
tion lexicons from overtly marked words and a the-
saurus. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing: Vol-
ume 2, EMNLP ’09, pages 599–608, Singapore.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the state-of-the-
art in sentiment analysis of tweets. In Proceedings
of the Seventh International Workshop on Semantic
Evaluation Exercises, SemEval ’13, Atlanta, Geor-
gia, USA.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 task 2: Sentiment analysis in
Twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation, SemEval ’13, pages 312–320,
Atlanta, Georgia, USA.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, and Nathan Schneider. 2012. Part-
of-speech tagging for Twitter: Word clusters and
other advances. Technical Report CMU-ML-12-
107, Carnegie Mellon University.
Sara Rosenthal, Alan Ritter, Veselin Stoyanov, and
Preslav Nakov. 2014. SemEval-2014 task 9: Sen-
timent analysis in Twitter. In Proceedings of the
Eighth International Workshop on Semantic Evalu-
ation, SemEval ’14, Dublin, Ireland.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, ACL ’10, pages 384–394, Upp-
sala, Sweden.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the
Conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, HLT ’05, pages 347–354, Vancouver, British
Columbia, Canada.
</reference>
<page confidence="0.998673">
595
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.257326">
<title confidence="0.9858565">SU-FMI: System Description for SemEval-2014 Task on Sentiment Analysis in Twitter</title>
<author confidence="0.766146333333333">Borislav Ivan Jeni Todor Georgi Ivan</author>
<email confidence="0.596573">Preslav</email>
<abstract confidence="0.982697777777778">We describe the submission of the team of the Sofia University to SemEval-2014 Task 9 on Sentiment Analysis in Twit- We participated in where the participating systems had to predict whether a Twitter message expresses positive, negative, or neutral sentiment. We trained an SVM classifier with a linear kernel using a variety of features. We used publicly available resources only, and thus our results should be easily replicable. Overall, our system is ranked 20th out of 50 submissions (by 44 teams) based on the average of the three 2014 evaluation data scores, with an F1-score of 63.62 on general tweets, 48.37 on sarcastic tweets, and 68.24 on LiveJournal messages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kalina Bontcheva</author>
<author>Leon Derczynski</author>
<author>Adam Funk</author>
<author>Mark Greenwood</author>
<author>Diana Maynard</author>
<author>Niraj Aswani</author>
</authors>
<title>TwitIE: An open-source information extraction pipeline for microblog text.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Conference on Recent Advances in Natural Language Processing, RANLP ’13,</booktitle>
<pages>83--90</pages>
<location>Hissar, Bulgaria.</location>
<contexts>
<context position="3477" citStr="Bontcheva et al., 2013" startWordPosition="501" endWordPosition="504">consists of two main submodules, (i) feature extraction in the framework of GATE (Cunningham et al., 2011), and (ii) machine learning using SVM with linear kernels as implemented in LIBLINEAR2 (Fan et al., 2008). 1http://www.umiacs.umd.edu/ ˜saif/WebPages/Abstracts/ NRC-SentimentAnalysis.htm 2http://www.csie.ntu.edu.tw/˜cjlin/ liblinear/ 590 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 590–595, Dublin, Ireland, August 23-24, 2014. 2.1 Preprocessing We integrated a pipeline of various resources for tweet analysis that are already available in GATE (Bontcheva et al., 2013) such as a Twitter tokenizer, a sentence splitter, a hashtag tokenizer, a Twitter POS tagger, a morphological analyzer, and the Snowball3 stemmer. We further implemented in GATE some shallow text processing components in order to handle negation contexts, emoticons, elongated words, all-caps words and punctuation. We also added components to find words and phrases contained in sentiment lexicons, as well as to annotate words with word cluster IDs using the lexicon built at CMU,4 which uses the Brown clusters (Brown et al., 1992) as implemented5 by (Liang, 2005). 2.2 Features 2.2.1 Sentiment le</context>
<context position="9300" citStr="Bontcheva et al., 2013" startWordPosition="1439" endWordPosition="1442">ation6 of Brown clustering (Liang, 2005; Brown et al., 1992), which group the words into 1,000 hierarchical clusters. We use two features based on these clusters: – presence/absence of a word in a word cluster; – presence/absence of a bigram in a bigram cluster. • POS tagging: Social media are generally hard to process using standard NLP tools, which are typically developed with newswire text in mind. Such standard tools are not a good fit for Twitter messages, which are too brief, contain typos and special wordforms. Thus, we used a specialized POS tagger, TwitIE, which is available in GATE (Bontcheva et al., 2013), and which we integrated in our pipeline. It provides (i) a tokenizer specifically trained to handle smilies, user names, URLs, etc., (ii) a normalizer to correct slang and misspellings, and (iii) a POS tagger that uses the Penn Treebank tagset, but is optimized for tweets. Using the TwitIE toolkit, we performed POS tagging and we extracted all POS tag types that we can find in the tweet together with their frequencies as features. 2.3 Classifier For classification, we used the above features and a support vector machine (SVM) classifier as implemented in LIBLINEAR. This is a very scalable im</context>
</contexts>
<marker>Bontcheva, Derczynski, Funk, Greenwood, Maynard, Aswani, 2013</marker>
<rawString>Kalina Bontcheva, Leon Derczynski, Adam Funk, Mark Greenwood, Diana Maynard, and Niraj Aswani. 2013. TwitIE: An open-source information extraction pipeline for microblog text. In Proceedings of the International Conference on Recent Advances in Natural Language Processing, RANLP ’13, pages 83–90, Hissar, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Brown</author>
<author>Peter deSouza</author>
<author>Robert Mercer</author>
<author>Vincent Della Pietra</author>
<author>Jenifer Lai</author>
</authors>
<title>Classbased n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--467</pages>
<contexts>
<context position="4011" citStr="Brown et al., 1992" startWordPosition="586" endWordPosition="589">ources for tweet analysis that are already available in GATE (Bontcheva et al., 2013) such as a Twitter tokenizer, a sentence splitter, a hashtag tokenizer, a Twitter POS tagger, a morphological analyzer, and the Snowball3 stemmer. We further implemented in GATE some shallow text processing components in order to handle negation contexts, emoticons, elongated words, all-caps words and punctuation. We also added components to find words and phrases contained in sentiment lexicons, as well as to annotate words with word cluster IDs using the lexicon built at CMU,4 which uses the Brown clusters (Brown et al., 1992) as implemented5 by (Liang, 2005). 2.2 Features 2.2.1 Sentiment lexicon features We used several preexisting lexicons, both manually designed and automatically generated: • Minqing Hu and Bing Liu opinion lexicon (Hu and Liu, 2004): 4,783 positive and 2,006 negative terms; • MPQA Subjectivity Cues Lexicon (Wilson et al., 2005): 8,222 terms; • Macquarie Semantic Orientation Lexicon (MSOL) (Mohammad et al., 2009): 30,458 positive and 45,942 negative terms; • NRC Emotion Lexicon (Mohammad et al., 2013): 14,181 terms with specified emotion. For each lexicon, we find in the tweet the terms that are</context>
<context position="8737" citStr="Brown et al., 1992" startWordPosition="1343" endWordPosition="1346">m of each word, excluding URLs. We add a STEM prefix to each stem. • Lemmatizer: the lemma of each word, excluding URLs. We add a LEMMA prefix to each lemma. We use the built-in GATE Morphological analyser as our lemmatizer. • Word and word bigram clusters: word clusters have been shown to improve the performance of supervised NLP models (Turian et al., 2010). We use the word clusters built by CMU’s NLP toolkit, which were produced over a collection of 56 million English tweets (Owoputi et al., 2012) and built using the Percy Liang’s HMM-based implementation6 of Brown clustering (Liang, 2005; Brown et al., 1992), which group the words into 1,000 hierarchical clusters. We use two features based on these clusters: – presence/absence of a word in a word cluster; – presence/absence of a bigram in a bigram cluster. • POS tagging: Social media are generally hard to process using standard NLP tools, which are typically developed with newswire text in mind. Such standard tools are not a good fit for Twitter messages, which are too brief, contain typos and special wordforms. Thus, we used a specialized POS tagger, TwitIE, which is available in GATE (Bontcheva et al., 2013), and which we integrated in our pipe</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter Brown, Peter deSouza, Robert Mercer, Vincent Della Pietra, and Jenifer Lai. 1992. Classbased n-gram models of natural language. Computational Linguistics, 18:467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hamish Cunningham</author>
<author>Diana Maynard</author>
<author>Kalina Bontcheva</author>
</authors>
<date>2011</date>
<booktitle>Text Processing with GATE. Gateway Press CA.</booktitle>
<contexts>
<context position="2960" citStr="Cunningham et al., 2011" startWordPosition="437" endWordPosition="440">sarcastic): @MetroNorth wall to wall people on the platform at South Norwalk waiting for the 8:08. Thanks for the Sat. Sched. Great sense Below we first describe our preprocessing, features and classifier in Section 2. Then, we discuss our experiments, results and analysis in Section 3. Finally, we conclude with possible directions for future work in Section 4. 2 Method Our approach is inspired by the highest scoring team in 2013, NRC Canada (Mohammad et al., 2013). We reused many of their resources.1 Our system consists of two main submodules, (i) feature extraction in the framework of GATE (Cunningham et al., 2011), and (ii) machine learning using SVM with linear kernels as implemented in LIBLINEAR2 (Fan et al., 2008). 1http://www.umiacs.umd.edu/ ˜saif/WebPages/Abstracts/ NRC-SentimentAnalysis.htm 2http://www.csie.ntu.edu.tw/˜cjlin/ liblinear/ 590 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 590–595, Dublin, Ireland, August 23-24, 2014. 2.1 Preprocessing We integrated a pipeline of various resources for tweet analysis that are already available in GATE (Bontcheva et al., 2013) such as a Twitter tokenizer, a sentence splitter, a hashtag tokenizer, a Twitter P</context>
</contexts>
<marker>Cunningham, Maynard, Bontcheva, 2011</marker>
<rawString>Hamish Cunningham, Diana Maynard, and Kalina Bontcheva. 2011. Text Processing with GATE. Gateway Press CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>9--1871</pages>
<contexts>
<context position="3065" citStr="Fan et al., 2008" startWordPosition="455" endWordPosition="458">e Sat. Sched. Great sense Below we first describe our preprocessing, features and classifier in Section 2. Then, we discuss our experiments, results and analysis in Section 3. Finally, we conclude with possible directions for future work in Section 4. 2 Method Our approach is inspired by the highest scoring team in 2013, NRC Canada (Mohammad et al., 2013). We reused many of their resources.1 Our system consists of two main submodules, (i) feature extraction in the framework of GATE (Cunningham et al., 2011), and (ii) machine learning using SVM with linear kernels as implemented in LIBLINEAR2 (Fan et al., 2008). 1http://www.umiacs.umd.edu/ ˜saif/WebPages/Abstracts/ NRC-SentimentAnalysis.htm 2http://www.csie.ntu.edu.tw/˜cjlin/ liblinear/ 590 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 590–595, Dublin, Ireland, August 23-24, 2014. 2.1 Preprocessing We integrated a pipeline of various resources for tweet analysis that are already available in GATE (Bontcheva et al., 2013) such as a Twitter tokenizer, a sentence splitter, a hashtag tokenizer, a Twitter POS tagger, a morphological analyzer, and the Snowball3 stemmer. We further implemented in GATE some shall</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. J. Mach. Learn. Res., 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’04,</booktitle>
<pages>168--177</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="4242" citStr="Hu and Liu, 2004" startWordPosition="621" endWordPosition="624">e further implemented in GATE some shallow text processing components in order to handle negation contexts, emoticons, elongated words, all-caps words and punctuation. We also added components to find words and phrases contained in sentiment lexicons, as well as to annotate words with word cluster IDs using the lexicon built at CMU,4 which uses the Brown clusters (Brown et al., 1992) as implemented5 by (Liang, 2005). 2.2 Features 2.2.1 Sentiment lexicon features We used several preexisting lexicons, both manually designed and automatically generated: • Minqing Hu and Bing Liu opinion lexicon (Hu and Liu, 2004): 4,783 positive and 2,006 negative terms; • MPQA Subjectivity Cues Lexicon (Wilson et al., 2005): 8,222 terms; • Macquarie Semantic Orientation Lexicon (MSOL) (Mohammad et al., 2009): 30,458 positive and 45,942 negative terms; • NRC Emotion Lexicon (Mohammad et al., 2013): 14,181 terms with specified emotion. For each lexicon, we find in the tweet the terms that are listed in it, and then we calculate the following features: • Negative terms count; • Positive terms count; • Positive negated terms count; • Positive/negative terms count ratio; • Sentiment of the last token; • Overall sentiment </context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’04, pages 168–177, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
</authors>
<title>Semi-supervised learning for natural language. Master’s thesis,</title>
<date>2005</date>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="4044" citStr="Liang, 2005" startWordPosition="593" endWordPosition="594">dy available in GATE (Bontcheva et al., 2013) such as a Twitter tokenizer, a sentence splitter, a hashtag tokenizer, a Twitter POS tagger, a morphological analyzer, and the Snowball3 stemmer. We further implemented in GATE some shallow text processing components in order to handle negation contexts, emoticons, elongated words, all-caps words and punctuation. We also added components to find words and phrases contained in sentiment lexicons, as well as to annotate words with word cluster IDs using the lexicon built at CMU,4 which uses the Brown clusters (Brown et al., 1992) as implemented5 by (Liang, 2005). 2.2 Features 2.2.1 Sentiment lexicon features We used several preexisting lexicons, both manually designed and automatically generated: • Minqing Hu and Bing Liu opinion lexicon (Hu and Liu, 2004): 4,783 positive and 2,006 negative terms; • MPQA Subjectivity Cues Lexicon (Wilson et al., 2005): 8,222 terms; • Macquarie Semantic Orientation Lexicon (MSOL) (Mohammad et al., 2009): 30,458 positive and 45,942 negative terms; • NRC Emotion Lexicon (Mohammad et al., 2013): 14,181 terms with specified emotion. For each lexicon, we find in the tweet the terms that are listed in it, and then we calcul</context>
<context position="8716" citStr="Liang, 2005" startWordPosition="1341" endWordPosition="1342">mmer: the stem of each word, excluding URLs. We add a STEM prefix to each stem. • Lemmatizer: the lemma of each word, excluding URLs. We add a LEMMA prefix to each lemma. We use the built-in GATE Morphological analyser as our lemmatizer. • Word and word bigram clusters: word clusters have been shown to improve the performance of supervised NLP models (Turian et al., 2010). We use the word clusters built by CMU’s NLP toolkit, which were produced over a collection of 56 million English tweets (Owoputi et al., 2012) and built using the Percy Liang’s HMM-based implementation6 of Brown clustering (Liang, 2005; Brown et al., 1992), which group the words into 1,000 hierarchical clusters. We use two features based on these clusters: – presence/absence of a word in a word cluster; – presence/absence of a bigram in a bigram cluster. • POS tagging: Social media are generally hard to process using standard NLP tools, which are typically developed with newswire text in mind. Such standard tools are not a good fit for Twitter messages, which are too brief, contain typos and special wordforms. Thus, we used a specialized POS tagger, TwitIE, which is available in GATE (Bontcheva et al., 2013), and which we i</context>
</contexts>
<marker>Liang, 2005</marker>
<rawString>Percy Liang. 2005. Semi-supervised learning for natural language. Master’s thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Cody Dunne</author>
<author>Bonnie Dorr</author>
</authors>
<title>Generating high-coverage semantic orientation lexicons from overtly marked words and a thesaurus.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing: Volume 2, EMNLP ’09,</booktitle>
<pages>599--608</pages>
<contexts>
<context position="4425" citStr="Mohammad et al., 2009" startWordPosition="648" endWordPosition="651">ded components to find words and phrases contained in sentiment lexicons, as well as to annotate words with word cluster IDs using the lexicon built at CMU,4 which uses the Brown clusters (Brown et al., 1992) as implemented5 by (Liang, 2005). 2.2 Features 2.2.1 Sentiment lexicon features We used several preexisting lexicons, both manually designed and automatically generated: • Minqing Hu and Bing Liu opinion lexicon (Hu and Liu, 2004): 4,783 positive and 2,006 negative terms; • MPQA Subjectivity Cues Lexicon (Wilson et al., 2005): 8,222 terms; • Macquarie Semantic Orientation Lexicon (MSOL) (Mohammad et al., 2009): 30,458 positive and 45,942 negative terms; • NRC Emotion Lexicon (Mohammad et al., 2013): 14,181 terms with specified emotion. For each lexicon, we find in the tweet the terms that are listed in it, and then we calculate the following features: • Negative terms count; • Positive terms count; • Positive negated terms count; • Positive/negative terms count ratio; • Sentiment of the last token; • Overall sentiment terms count. We further used the following lexicons: • NRC Hashtag Sentiment Lexicon: list of words and their associations with positive and negative sentiment (Mohammad et al., 2013)</context>
</contexts>
<marker>Mohammad, Dunne, Dorr, 2009</marker>
<rawString>Saif Mohammad, Cody Dunne, and Bonnie Dorr. 2009. Generating high-coverage semantic orientation lexicons from overtly marked words and a thesaurus. In Proceedings of the Conference on Empirical Methods in Natural Language Processing: Volume 2, EMNLP ’09, pages 599–608, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>NRC-Canada: Building the state-of-theart in sentiment analysis of tweets.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventh International Workshop on Semantic Evaluation Exercises, SemEval ’13,</booktitle>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="2805" citStr="Mohammad et al., 2013" startWordPosition="412" endWordPosition="415"> the 2nd Half of the Season http://t.co/yK9VTjcs • negative: Why the hell does Selma have school tomorrow but Parlier clovis &amp; others don’t? • negative (sarcastic): @MetroNorth wall to wall people on the platform at South Norwalk waiting for the 8:08. Thanks for the Sat. Sched. Great sense Below we first describe our preprocessing, features and classifier in Section 2. Then, we discuss our experiments, results and analysis in Section 3. Finally, we conclude with possible directions for future work in Section 4. 2 Method Our approach is inspired by the highest scoring team in 2013, NRC Canada (Mohammad et al., 2013). We reused many of their resources.1 Our system consists of two main submodules, (i) feature extraction in the framework of GATE (Cunningham et al., 2011), and (ii) machine learning using SVM with linear kernels as implemented in LIBLINEAR2 (Fan et al., 2008). 1http://www.umiacs.umd.edu/ ˜saif/WebPages/Abstracts/ NRC-SentimentAnalysis.htm 2http://www.csie.ntu.edu.tw/˜cjlin/ liblinear/ 590 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 590–595, Dublin, Ireland, August 23-24, 2014. 2.1 Preprocessing We integrated a pipeline of various resources for tw</context>
<context position="4515" citStr="Mohammad et al., 2013" startWordPosition="662" endWordPosition="665">notate words with word cluster IDs using the lexicon built at CMU,4 which uses the Brown clusters (Brown et al., 1992) as implemented5 by (Liang, 2005). 2.2 Features 2.2.1 Sentiment lexicon features We used several preexisting lexicons, both manually designed and automatically generated: • Minqing Hu and Bing Liu opinion lexicon (Hu and Liu, 2004): 4,783 positive and 2,006 negative terms; • MPQA Subjectivity Cues Lexicon (Wilson et al., 2005): 8,222 terms; • Macquarie Semantic Orientation Lexicon (MSOL) (Mohammad et al., 2009): 30,458 positive and 45,942 negative terms; • NRC Emotion Lexicon (Mohammad et al., 2013): 14,181 terms with specified emotion. For each lexicon, we find in the tweet the terms that are listed in it, and then we calculate the following features: • Negative terms count; • Positive terms count; • Positive negated terms count; • Positive/negative terms count ratio; • Sentiment of the last token; • Overall sentiment terms count. We further used the following lexicons: • NRC Hashtag Sentiment Lexicon: list of words and their associations with positive and negative sentiment (Mohammad et al., 2013): 54,129 unigrams, 316,531 bigrams, 480,010 pairs, and 78 high-quality positive and negati</context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. NRC-Canada: Building the state-of-theart in sentiment analysis of tweets. In Proceedings of the Seventh International Workshop on Semantic Evaluation Exercises, SemEval ’13, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Zornitsa Kozareva</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
<author>Theresa Wilson</author>
</authors>
<title>SemEval-2013 task 2: Sentiment analysis in Twitter.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation, SemEval ’13,</booktitle>
<pages>312--320</pages>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="10553" citStr="Nakov et al., 2013" startWordPosition="1644" endWordPosition="1647">support kernels, and is suitable for classification on large datasets with a large number of features. This is particularly useful for text classification, where the number of features is very large, which means that the data is likely to be linearly separable, and thus using kernels is not really necessary. We scaled the SVM input and we used L2-regularization during training. 6https://github.com/percyliang/ brown-cluster 592 3 Experiments, Results, Analysis 3.1 Experimental setup At development time, we trained on train-2013, tuned the C value of SVM on dev-2013, and evaluated on test-2013 (Nakov et al., 2013). For our submission, we trained on train-2013+dev-2013, and we evaluated on the 2014 test dataset provided by the organizers. This dataset contains two parts and a total of five datasets: (a) progress test (the Twitter and SMS test datasets for 2013), and (b) new test datasets (from Twitter, from Twitter with sarcasm, and from LiveJournal). We used C=0.012, which was best on development. 3.2 Official results Due to our very late entering in the competition, we have only managed to perform a small number of experiments, and we only participated in subtask B. We were ranked 20th out of 50 submi</context>
</contexts>
<marker>Nakov, Rosenthal, Kozareva, Stoyanov, Ritter, Wilson, 2013</marker>
<rawString>Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva, Veselin Stoyanov, Alan Ritter, and Theresa Wilson. 2013. SemEval-2013 task 2: Sentiment analysis in Twitter. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation, SemEval ’13, pages 312–320, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
</authors>
<title>Partof-speech tagging for Twitter: Word clusters and other advances.</title>
<date>2012</date>
<tech>Technical Report CMU-ML-12-107,</tech>
<institution>Carnegie Mellon University.</institution>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, 2012</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, and Nathan Schneider. 2012. Partof-speech tagging for Twitter: Word clusters and other advances. Technical Report CMU-ML-12-107, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Alan Ritter</author>
<author>Veselin Stoyanov</author>
<author>Preslav Nakov</author>
</authors>
<title>SemEval-2014 task 9: Sentiment analysis in Twitter.</title>
<date>2014</date>
<booktitle>In Proceedings of the Eighth International Workshop on Semantic Evaluation, SemEval ’14,</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="1147" citStr="Rosenthal et al., 2014" startWordPosition="176" endWordPosition="179">t. We trained an SVM classifier with a linear kernel using a variety of features. We used publicly available resources only, and thus our results should be easily replicable. Overall, our system is ranked 20th out of 50 submissions (by 44 teams) based on the average of the three 2014 evaluation data scores, with an F1-score of 63.62 on general tweets, 48.37 on sarcastic tweets, and 68.24 on LiveJournal messages. 1 Introduction We describe the submission of the team of the Sofia University, Faculty of Mathematics and Informatics (SU-FMI) to SemEval-2014 Task 9 on Sentiment Analysis in Twitter (Rosenthal et al., 2014). Sofia University, bobby.velichkov@gmail.com Sofia University, b.kapukaranov@gmail.com Sofia University, iigrozev@gmail.com Sofia University, j.karanesheva@gmail.com Sofia University, tbmihailov@gmail.com � Sofia University, yasen.kiprov@gmail.com Ontotext, g.d.georgiev@gmail.com �� Sofia University, koychev@fmi.uni-sofia.bg �� Qatar �� Computing Research Institute, pnakov@qf.org.qa This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ T</context>
</contexts>
<marker>Rosenthal, Ritter, Stoyanov, Nakov, 2014</marker>
<rawString>Sara Rosenthal, Alan Ritter, Veselin Stoyanov, and Preslav Nakov. 2014. SemEval-2014 task 9: Sentiment analysis in Twitter. In Proceedings of the Eighth International Workshop on Semantic Evaluation, SemEval ’14, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>384--394</pages>
<location>Uppsala,</location>
<contexts>
<context position="8479" citStr="Turian et al., 2010" startWordPosition="1301" endWordPosition="1304"> of contiguous sequences of exclamation marks, of question marks, of either exclamation or question marks, and of both exclamation and question marks. Also, whether the last token contains an exclamation or a question mark (excluding URLs). • Stemmer: the stem of each word, excluding URLs. We add a STEM prefix to each stem. • Lemmatizer: the lemma of each word, excluding URLs. We add a LEMMA prefix to each lemma. We use the built-in GATE Morphological analyser as our lemmatizer. • Word and word bigram clusters: word clusters have been shown to improve the performance of supervised NLP models (Turian et al., 2010). We use the word clusters built by CMU’s NLP toolkit, which were produced over a collection of 56 million English tweets (Owoputi et al., 2012) and built using the Percy Liang’s HMM-based implementation6 of Brown clustering (Liang, 2005; Brown et al., 1992), which group the words into 1,000 hierarchical clusters. We use two features based on these clusters: – presence/absence of a word in a word cluster; – presence/absence of a bigram in a bigram cluster. • POS tagging: Social media are generally hard to process using standard NLP tools, which are typically developed with newswire text in min</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 384–394, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>347--354</pages>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="4339" citStr="Wilson et al., 2005" startWordPosition="636" endWordPosition="639">ion contexts, emoticons, elongated words, all-caps words and punctuation. We also added components to find words and phrases contained in sentiment lexicons, as well as to annotate words with word cluster IDs using the lexicon built at CMU,4 which uses the Brown clusters (Brown et al., 1992) as implemented5 by (Liang, 2005). 2.2 Features 2.2.1 Sentiment lexicon features We used several preexisting lexicons, both manually designed and automatically generated: • Minqing Hu and Bing Liu opinion lexicon (Hu and Liu, 2004): 4,783 positive and 2,006 negative terms; • MPQA Subjectivity Cues Lexicon (Wilson et al., 2005): 8,222 terms; • Macquarie Semantic Orientation Lexicon (MSOL) (Mohammad et al., 2009): 30,458 positive and 45,942 negative terms; • NRC Emotion Lexicon (Mohammad et al., 2013): 14,181 terms with specified emotion. For each lexicon, we find in the tweet the terms that are listed in it, and then we calculate the following features: • Negative terms count; • Positive terms count; • Positive negated terms count; • Positive/negative terms count ratio; • Sentiment of the last token; • Overall sentiment terms count. We further used the following lexicons: • NRC Hashtag Sentiment Lexicon: list of wor</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 347–354, Vancouver, British Columbia, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>