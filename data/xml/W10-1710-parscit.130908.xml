<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000704">
<title confidence="0.9752495">
FBK at WMT 2010: Word Lattices for
Morphological Reduction and Chunk-based Reordering
</title>
<author confidence="0.929891">
Christian Hardmeier, Arianna Bisazza and Marcello Federico
</author>
<affiliation confidence="0.7876785">
Fondazione Bruno Kessler
Human Language Technologies
</affiliation>
<address confidence="0.641875">
Trento, Italy
</address>
<email confidence="0.998153">
{hardmeier,bisazza,federico}@fbk.eu
</email>
<sectionHeader confidence="0.993871" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997189375">
FBK participated in the WMT 2010
Machine Translation shared task with
phrase-based Statistical Machine Transla-
tion systems based on the Moses decoder
for English-German and German-English
translation. Our work concentrates on ex-
ploiting the available language modelling
resources by using linear mixtures of large
6-gram language models and on address-
ing linguistic differences between English
and German with methods based on word
lattices. In particular, we use lattices to in-
tegrate a morphological analyser for Ger-
man into our system, and we present some
initial work on rule-based word reorder-
ing.
</bodyText>
<sectionHeader confidence="0.915118" genericHeader="method">
1 System overview
</sectionHeader>
<bodyText confidence="0.9999215625">
The Human Language Technologies group at Fon-
dazione Bruno Kessler (FBK) participated in the
WMT 2010 Machine Translation (MT) evaluation
with systems for English-German and German-
English translation. While the English-German
system we submitted was relatively simple, we
put some more effort into the inverse translation
direction to make better use of the abundance
of language modelling data available for English
and to address the richness of German morphol-
ogy, which makes it hard for a Statistical Machine
Translation (SMT) system to achieve good vocab-
ulary coverage. In the remainder of this section,
an overview of the common features of our sys-
tems will be given. The next two sections provide
a more detailed description of our approaches to
language modelling, morphological preprocessing
and word reordering.
Both of our systems were based on the Moses
decoder (Koehn et al., 2007). They were simi-
lar to the WMT 2010 Moses baseline system. In-
stead of lowercasing the training data and adding
a recasing step, we retained the data in document
case throughout our system, except for the mor-
phologically normalised word forms described in
section 3. Our phrase tables were trained with the
standard Moses training script, then filtered based
on statistical significance according to the method
described by Johnson et al. (2007). Finally, we
used Minimum Bayes Risk decoding (Kumar and
Byrne, 2004) based on the BLEU score (Papineni
et al., 2002).
</bodyText>
<sectionHeader confidence="0.832247" genericHeader="method">
2 Language modelling
</sectionHeader>
<bodyText confidence="0.999903615384615">
At the 2009 NIST MT evaluation, our system ob-
tained good results using a mixture of linearly in-
terpolated language models (LMs) combining data
from different sources. As the training data pro-
vided for the present evaluation campaign again
included a large set of language modelling corpora
from different sources, especially for English as
a target language, we decided to adopt the same
strategy. The partial corpora for English and their
sizes can be found in table 1. Our base mod-
els of the English Gigaword texts were trained
on version 3 of the corpus (LDC2007T07). We
trained separate language models for the new data
from the years 2007 and 2008 included in ver-
sion 4 (LDC2009T13). Apart from the mono-
lingual English data, we also included language
models trained on the English part of the addi-
tional parallel datasets supplied for the French-
English and Czech-English tasks. All the mod-
els were estimated as 6-gram models with Kneser-
Ney smoothing using the IRSTLM language mod-
elling toolkit (Federico et al., 2008).
For technical reasons, we were unable to use all
the language models during decoding. We there-
fore selected a subset of the models with the fol-
lowing data selection procedure:
</bodyText>
<listItem confidence="0.5605595">
1. For a linear mixture of the complete set of
24 language models, we estimated a set of
</listItem>
<page confidence="0.9868">
88
</page>
<note confidence="0.693468">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 88–92,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<table confidence="0.9988984">
Corpus n-grams
Europarl v5 115,702,157
News 1,437,562,740
News commentary 10 10,381,511
Gigaword v3: 6 models 7,990,828,834
Gigaword 2007/08: 6 models 1,418,281,597
109 fr-en 1,190,593,051
UNDOC fr-en 333,120,732
CzEng: 7 models 153,355,518
Total: 24 models 12,649,826,140
</table>
<tableCaption confidence="0.998537">
Table 1: Language modelling corpora for English
</tableCaption>
<table confidence="0.807676875">
LMs Perplexity
DEV EVAL
2 188.57 181.38
5 163.68 158.99
10 156.43 151.73
15 154.71 144.98
20 154.39 144.91
24 154.42 144.92
</table>
<tableCaption confidence="0.991075">
Table 2: Perplexities of LM mixtures
</tableCaption>
<bodyText confidence="0.998444851851852">
optimal interpolation weights to minimise
the perplexity of the mixture model on the
news-test2008 development set.
2. By sorting the mixture coefficients in de-
scending order, we obtained an ordering of
the language models by their importance with
respect to the development set. We created
partial mixtures by selecting the top n mod-
els according to this order and retraining the
mixture weights with the same algorithm.
Computing the perplexities of these partial
mixtures on the news-test2008 (DEV) and
newstest2009 (EVAL) corpora shows that signif-
icant improvements can be obtained up to a mix-
tures size of about 15 elements. As this size still
turned out to be too large to be managed by our
systems, we used a 5-element mixture in our final
submission (see table 3 for details about the mix-
ture and table 4 for the evaluation results of the
submitted systems).
For the English-German system, the only cor-
pora available for the target language were Eu-
roparl v5, News commentary v10 and the mono-
lingual News corpus. Similar experiments showed
that the News corpus was by far the most impor-
tant for the text genre to be translated and that
including language models trained on the other
</bodyText>
<table confidence="0.6839975">
Weight Language model
0.368023 News
0.188156 109 fr-en
0.174802 Gigaword v3: NYT
0.144465 Gigaword v3: AFP
0.124553 Gigaword v3: APW
</table>
<tableCaption confidence="0.751844">
Table 3: 5-element LM mixture used for decoding
BLEU-cased BLEU
</tableCaption>
<figure confidence="0.54272125">
en-de
primary 15.5 15.8
secondary 15.3 15.6
primary: only News language model
secondary: linear mixture of 3 LMs
de-en
primary 20.9 21.9
secondary 20.3 21.3
</figure>
<figureCaption confidence="0.446644">
primary: morph. reduction, linear mixture of 5 LMs
secondary: reordering, only News LM
</figureCaption>
<tableCaption confidence="0.987283">
Table 4: Evaluation results of submitted systems
</tableCaption>
<bodyText confidence="0.999834666666667">
corpora could even degrade system performance.
We therefore decided not to use Europarl or News
commentary for language modelling in our pri-
mary submission. However, we submitted a sec-
ondary system using a mixture of language models
based on all three corpora.
</bodyText>
<sectionHeader confidence="0.61728" genericHeader="method">
3 Morphological reduction and
</sectionHeader>
<subsectionHeader confidence="0.724006">
decompounding of German
</subsectionHeader>
<bodyText confidence="0.99987845">
Compounding is a highly productive part of Ger-
man noun morphology. Unlike in English, Ger-
man compound nouns are usually spelt as sin-
gle words, which greatly increases the vocabulary.
For a Machine Translation system, this property
of the language causes a high number of out-of-
vocabulary (OOV) words. It is likely that many
compounds in an input text have not been seen in
the training corpus. We addressed this problem by
splitting compounds in the German source text.
Compound splitting was done using the Gert-
wol morphological analyser (Koskenniemi and
Haapalainen, 1996), a linguistically informed sys-
tem based on two-level finite state morphology.
Since Gertwol outputs all possible analyses of a
word form without taking into account the context,
the output has to be disambiguated. For this pur-
pose, we used part-of-speech (POS) tags obtained
from the TreeTagger (Schmid, 1994) along with
a set of POS-based heuristic disambiguation rules
</bodyText>
<page confidence="0.998998">
89
</page>
<bodyText confidence="0.999954074074074">
provided to us by the Institute of Computational
Linguistics of the University of Zurich.
As a side effect, Gertwol outputs the base forms
of all words that it processes: Nominative singu-
lar of nouns, infinitive of verbs etc. We decided to
combine the tokens analysed by Gertwol, whether
or not they had been decompounded and lower-
cased, in a further attempt to reduce data sparse-
ness, with their original form in a word lattice
(see fig. 1) and to let the decoder make the choice
between the two according to the translations the
phrase table can provide for each.
Our word lattices are similar to those used by
Dyer et al. (2008) for handling word segmentation
in Chinese and Arabic. For each word that was
segmented by Gertwol, we provide exactly one al-
ternative edge labelled with the component words
and base forms as identified by Gertwol, after re-
moving linking morphemes. The edge transition
probabilities are used to identify the source of an
edge: their values are e−1 = 0.36788 for edges de-
riving from Gertwol analysis and e0 = 1 for edges
carrying unprocessed words. Tokens whose de-
compounded base form according to Gertwol is
identical to the surface form in the input are rep-
resented by a single edge with transition proba-
bility e−0.5 = 0.606531. These transition proba-
bilities translate into a binary feature with values
−1, −0.5 and 0 after taking logarithms in the de-
coder. The feature weight is determined by Min-
imum Error-Rate Training (Och, 2003), together
with the weights of the other feature functions
used in the decoder. During system training, the
processed version of the training corpus was con-
catenated with the unprocessed text.
Experiments show that decompounding and
morphological analysis have a significant impact
on the performance of the MT system. After
these steps, the OOV rate of the newstest2009
test set decreases from 5.88 % to 3.21 %. Us-
ing only the News language model, the BLEU
score of our development system (measured on
the newstest2009 corpus) increases from 18.77
to 19.31. There is an interesting interaction with
the language models. While using a linear mixture
of 15 language models instead of just the News
LM does not improve the performance of the base-
line system (BLEU score 18.78 instead of 18.77),
the BLEU score of the 15-LM system increases to
20.08 when adding morphological reduction. In
the baseline system, the additional language mod-
els did not have a noticeable effect on translation
quality; however, their impact was realised in the
decompounding system.
</bodyText>
<sectionHeader confidence="0.953073" genericHeader="method">
4 Word reordering
</sectionHeader>
<bodyText confidence="0.999941282608696">
Current SMT systems are based on the assump-
tion that the word order of the source and the tar-
get languages are fundamentally similar. While
the models permit some local reordering, system-
atic differences in word order involving move-
ments of more than a few words pose major prob-
lems. In particular, Statistical Machine Transla-
tion between German and English is notoriously
impacted by the different fundamental word order
in subordinate clauses, where German Subject–
Object–Verb (SOV) order contrasts with English
Subject–Verb–Object (SVO) order.
In our English-German system, we made the
observation that the verb in an SVO subordi-
nate clause following a punctuation mark fre-
quently gets moved before the preceding punctu-
ation. This movement is triggered by the Ger-
man language model, which prefers verbs pre-
ceding punctuation as consistent with SOV or-
der, and it is facilitated by the fact that the dis-
tance from the verb to the end of the preceding
clause is often smaller than the distance to the end
of the current phrase, so moving the verb back-
wards results in a better score from the distance-
based reordering model. This tendency can be
counteracted effectively by enabling the Moses
decoder’s monotone-at-punctuation feature,
which makes sure that words are not reordered
across punctuation marks. The result is a mod-
est gain from 14.28 to 14.38 BLEU points
(newstest2009).
In the German-English system, we applied a
chunk-based technique to produce lattices repre-
senting multiple permutations of the test sentences
in order to enable long-range reorderings of verb
phrases. This approach is similar to the reorder-
ing technique based on part-of-speech tags pre-
sented by Niehues and Kolss (2009), which re-
sults in the addition of a large number of reorder-
ing paths to the lattices. By contrast, we assume
that verb reorderings only occur between shallow
syntax chunks, and not within them. This makes it
possible to limit the number of long-range reorder-
ing options in an effective way.
We used the TreeTagger to perform shallow
syntax chunking of the German text. By man-
</bodyText>
<page confidence="0.996276">
90
</page>
<figureCaption confidence="0.997303">
Figure 1: Word lattice for morphological reduction
</figureCaption>
<figure confidence="0.479661">
Sonst [drohe]VC , dass auch [weitere Länder]NC [vom Einbruch]PC [betroffen sein würden]VC .
</figure>
<figureCaption confidence="0.829508">
Figure 2: Chunk reordering lattice
</figureCaption>
<table confidence="0.999228">
BLEU
test-09 test-10
Baseline 18.77 20.1
+ chunk-based reordering 18.94 20.3
Morphological reduction 19.31 20.6
+ chunk-based reordering 19.79 21.1
</table>
<tableCaption confidence="0.863110333333333">
note: only News LM, case-sensitive evaluation
Table 5: Results with morphological reduction and
chunk reordering on newstest 2009/2010
</tableCaption>
<bodyText confidence="0.999807416666667">
ual inspection of a data sample, we then identi-
fied a few recurrent patterns of long reorderings
involving the verbs. In particular, we focused on
clause-final verbs in German SOV clauses, which
we move to the left in order to approximate the En-
glish SVO word order. For each sentence a chunk-
based lattice is created, which is then expanded
into a word lattice like the one shown in fig. 2. The
lattice representation provides the decoder with up
to three possible reorderings for a particular verb
chunk. It always retains the original word order as
an alternative input.
For technical reasons, we were unable to pre-
pare a system with reordering, morphological re-
duction and all language models in time for the
shared task. Our secondary submission with re-
ordering is therefore not comparable with our best
system, which includes more language models
and morphological reduction. In subsequent ex-
periments, we combined morphological reduction
with chunk-based reordering (table 5). When mor-
phological reduction is used, the reordering ap-
proach yields an improvement of about 0.5 BLEU
percentage points.
</bodyText>
<sectionHeader confidence="0.995959" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999991243243244">
There are three important features specific to the
FBK systems at WMT 2010: mixtures of large
language models, German morphological reduc-
tion and decompounding and word reordering.
Our approach to using large language models
proved successful at the 2009 NIST MT evalua-
tion. In the present evaluation, its effectiveness
was reduced by a number of technical problems,
which were mostly due to the limitations of disk
access throughput in our parallel computing en-
vironment. We are working on methods to re-
duce and distribute disk accesses to large lan-
guage models, which will be implemented in the
IRSTLM language modelling toolkit (Federico et
al., 2008). By doing so, we hope to overcome the
current limitations and exploit the power of lan-
guage model mixtures more fully.
The Gertwol-based morphological reduction
and decompounding component we used is a
working solution that results in a significant im-
provement in translation quality. It is an alterna-
tive to the popular statistical compound splitting
methods, such as the one by Koehn and Knight
(2003), incorporating a greater amount of linguis-
tic knowledge and offering morphological reduc-
tion even of simplex words to their base form in
addition. It would be interesting to compare the
relative performance of the two approaches sys-
tematically.
Word reordering between German and English
is a complex problem. Encouraged by the success
of chunk-based verb reordering lattices on Arabic-
English (Bisazza and Federico, 2010), we tried to
adapt the same approach to the German-English
language pair. It turned out that there is a larger
variety of long reordering patterns in this case.
Nevertheless, some experiments performed after
</bodyText>
<page confidence="0.996031">
91
</page>
<bodyText confidence="0.999924928571429">
the official evaluation showed promising results.
We plan to pursue this work in several directions:
Defining a lattice weighting scheme that distin-
guishes between original word order and reorder-
ing paths could help the decoder select the more
promising path through the lattice. Applying sim-
ilar reordering rules to the training corpus would
reduce the mismatch between the training data and
the reordered input sentences. Finally, it would be
useful to explore the impact of different distortion
limits on the decoding of reordering lattices in or-
der to find an optimal trade-off between decoder-
driven short-range and lattice-driven long-range
reordering.
</bodyText>
<sectionHeader confidence="0.996933" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999425">
This work was supported by the EuroMatrixPlus
project (IST-231720), which is funded by the Eu-
ropean Commission under the Seventh Framework
Programme for Research and Technological De-
velopment.
</bodyText>
<sectionHeader confidence="0.995334" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.991153222222223">
Arianna Bisazza and Marcello Federico. 2010. Chunk-
based verb reordering in VSO sentences for Arabic-
English statistical machine translation. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and Metrics MATR, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proceedings of ACL-08: HLT, pages 1012–
1020, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In Inter-
speech 2008, pages 1618–1621. ISCA.
Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation qual-
ity by discarding most of the phrasetable. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 967–975, Prague, Czech Republic,
June. Association for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In Proceedings of
EACL, pages 187–193.
Philipp Koehn, Hieu Hoang, Alexandra Birch, et al.
2007. Moses: open source toolkit for statistical ma-
chine translation. In Annual meeting of the Associa-
tion for Computational Linguistics: Demonstration
session, pages 177–180, Prague.
Kimmo Koskenniemi and Mariikka Haapalainen.
1996. GERTWOL – Lingsoft Oy. In Roland
Hausser, editor, Linguistische Verifikation. Doku-
mentation zur Ersten Morpholympics 1994, chap-
ter 11, pages 121–140. Niemeyer, Tübingen.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceed-
ings, pages 169–176, Boston, Massachusetts, USA,
May 2 - May 7. Association for Computational Lin-
guistics.
Jan Niehues and Muntsin Kolss. 2009. A POS-based
model for long-range reorderings in SMT. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 206–214, Athens, Greece,
March. Association for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st annual meeting of the Association for Computa-
tional Linguistics, pages 160–167, Sapporo (Japan).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia.
ACL.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44–49.
</reference>
<page confidence="0.996005">
92
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.99406">FBK at WMT 2010: Word Lattices Morphological Reduction and Chunk-based Reordering</title>
<author confidence="0.8653885">Christian Hardmeier</author>
<author confidence="0.8653885">Arianna Bisazza</author>
<author confidence="0.8653885">Marcello Fondazione Bruno</author>
<affiliation confidence="0.791985">Human Language</affiliation>
<address confidence="0.469502">Trento,</address>
<email confidence="0.99813">hardmeier@fbk.eu</email>
<email confidence="0.99813">bisazza@fbk.eu</email>
<email confidence="0.99813">federico@fbk.eu</email>
<abstract confidence="0.987612185185185">FBK participated in the WMT 2010 Machine Translation shared task with phrase-based Statistical Machine Translation systems based on the Moses decoder for English-German and German-English translation. Our work concentrates on exploiting the available language modelling resources by using linear mixtures of large 6-gram language models and on addressing linguistic differences between English and German with methods based on word lattices. In particular, we use lattices to integrate a morphological analyser for German into our system, and we present some initial work on rule-based word reordering. 1 System overview The Human Language Technologies group at Fondazione Bruno Kessler (FBK) participated in the WMT 2010 Machine Translation (MT) evaluation with systems for English-German and German- English translation. While the English-German system we submitted was relatively simple, we put some more effort into the inverse translation direction to make better use of the abundance of language modelling data available for English and to address the richness of German morphology, which makes it hard for a Statistical Machine Translation (SMT) system to achieve good vocabulary coverage. In the remainder of this section, an overview of the common features of our systems will be given. The next two sections provide a more detailed description of our approaches to language modelling, morphological preprocessing and word reordering. Both of our systems were based on the Moses decoder (Koehn et al., 2007). They were similar to the WMT 2010 Moses baseline system. Instead of lowercasing the training data and adding a recasing step, we retained the data in document case throughout our system, except for the morphologically normalised word forms described in section 3. Our phrase tables were trained with the standard Moses training script, then filtered based on statistical significance according to the method described by Johnson et al. (2007). Finally, we used Minimum Bayes Risk decoding (Kumar and Byrne, 2004) based on the BLEU score (Papineni et al., 2002). 2 Language modelling At the 2009 NIST MT evaluation, our system obtained good results using a mixture of linearly interpolated language models (LMs) combining data from different sources. As the training data provided for the present evaluation campaign again included a large set of language modelling corpora from different sources, especially for English as a target language, we decided to adopt the same strategy. The partial corpora for English and their sizes can be found in table 1. Our base models of the English Gigaword texts were trained on version 3 of the corpus (LDC2007T07). We trained separate language models for the new data from the years 2007 and 2008 included in version 4 (LDC2009T13). Apart from the monolingual English data, we also included language models trained on the English part of the additional parallel datasets supplied for the French- English and Czech-English tasks. All the models were estimated as 6-gram models with Kneser- Ney smoothing using the IRSTLM language modelling toolkit (Federico et al., 2008). For technical reasons, we were unable to use all the language models during decoding. We therefore selected a subset of the models with the following data selection procedure: 1. For a linear mixture of the complete set of 24 language models, we estimated a set of 88 of the Joint 5th Workshop on Statistical Machine Translation and pages</abstract>
<note confidence="0.953348083333334">Sweden, 15-16 July 2010. Association for Computational Linguistics Corpus n-grams Europarl v5 115,702,157 News 1,437,562,740 News commentary 10 10,381,511 Gigaword v3: 6 models 7,990,828,834 Gigaword 2007/08: 6 models 1,418,281,597 fr-en 1,190,593,051 UNDOC fr-en 333,120,732 CzEng: 7 models 153,355,518 Total: 24 models 12,649,826,140 Table 1: Language modelling corpora for English</note>
<affiliation confidence="0.27517">LMs Perplexity</affiliation>
<address confidence="0.613633">DEV EVAL 2 188.57 181.38 5 163.68 158.99 10 156.43 151.73 15 154.71 144.98 20 154.39 144.91 24 154.42 144.92</address>
<abstract confidence="0.997644558490566">Table 2: Perplexities of LM mixtures optimal interpolation weights to minimise the perplexity of the mixture model on the set. 2. By sorting the mixture coefficients in descending order, we obtained an ordering of the language models by their importance with respect to the development set. We created mixtures by selecting the top models according to this order and retraining the mixture weights with the same algorithm. Computing the perplexities of these partial on the and corpora shows that significant improvements can be obtained up to a mixtures size of about 15 elements. As this size still turned out to be too large to be managed by our systems, we used a 5-element mixture in our final submission (see table 3 for details about the mixture and table 4 for the evaluation results of the submitted systems). For the English-German system, the only corpora available for the target language were Europarl v5, News commentary v10 and the monolingual News corpus. Similar experiments showed that the News corpus was by far the most important for the text genre to be translated and that including language models trained on the other Weight Language model 0.368023 News fr-en 0.174802 Gigaword v3: NYT 0.144465 Gigaword v3: AFP 0.124553 Gigaword v3: APW Table 3: 5-element LM mixture used for decoding BLEU-cased BLEU en-de primary 15.5 15.8 secondary 15.3 15.6 News language model mixture of 3 LMs de-en primary 20.9 21.9 secondary 20.3 21.3 reduction, linear mixture of 5 LMs only News LM Table 4: Evaluation results of submitted systems corpora could even degrade system performance. We therefore decided not to use Europarl or News commentary for language modelling in our primary submission. However, we submitted a secondary system using a mixture of language models based on all three corpora. 3 Morphological reduction and decompounding of German Compounding is a highly productive part of German noun morphology. Unlike in English, German compound nouns are usually spelt as single words, which greatly increases the vocabulary. For a Machine Translation system, this property of the language causes a high number of out-ofvocabulary (OOV) words. It is likely that many compounds in an input text have not been seen in the training corpus. We addressed this problem by splitting compounds in the German source text. Compound splitting was done using the Gertwol morphological analyser (Koskenniemi and Haapalainen, 1996), a linguistically informed system based on two-level finite state morphology. Since Gertwol outputs all possible analyses of a word form without taking into account the context, the output has to be disambiguated. For this purpose, we used part-of-speech (POS) tags obtained from the TreeTagger (Schmid, 1994) along with a set of POS-based heuristic disambiguation rules 89 provided to us by the Institute of Computational Linguistics of the University of Zurich. As a side effect, Gertwol outputs the base forms of all words that it processes: Nominative singular of nouns, infinitive of verbs etc. We decided to combine the tokens analysed by Gertwol, whether or not they had been decompounded and lowercased, in a further attempt to reduce data sparseness, with their original form in a word lattice (see fig. 1) and to let the decoder make the choice between the two according to the translations the phrase table can provide for each. Our word lattices are similar to those used by Dyer et al. (2008) for handling word segmentation in Chinese and Arabic. For each word that was segmented by Gertwol, we provide exactly one alternative edge labelled with the component words and base forms as identified by Gertwol, after removing linking morphemes. The edge transition probabilities are used to identify the source of an their values are = for edges defrom Gertwol analysis and = for edges carrying unprocessed words. Tokens whose decompounded base form according to Gertwol is identical to the surface form in the input are represented by a single edge with transition proba- These transition probabilities translate into a binary feature with values and 0 after taking logarithms in the decoder. The feature weight is determined by Minimum Error-Rate Training (Och, 2003), together with the weights of the other feature functions used in the decoder. During system training, the processed version of the training corpus was concatenated with the unprocessed text. Experiments show that decompounding and morphological analysis have a significant impact on the performance of the MT system. After steps, the OOV rate of the test set decreases from 5.88 % to 3.21 %. Using only the News language model, the BLEU score of our development system (measured on increases from 18.77 to 19.31. There is an interesting interaction with the language models. While using a linear mixture of 15 language models instead of just the News LM does not improve the performance of the baseline system (BLEU score 18.78 instead of 18.77), the BLEU score of the 15-LM system increases to 20.08 when adding morphological reduction. In the baseline system, the additional language models did not have a noticeable effect on translation quality; however, their impact was realised in the decompounding system. 4 Word reordering Current SMT systems are based on the assumption that the word order of the source and the target languages are fundamentally similar. While the models permit some local reordering, systematic differences in word order involving movements of more than a few words pose major problems. In particular, Statistical Machine Translation between German and English is notoriously impacted by the different fundamental word order in subordinate clauses, where German Subject– Object–Verb (SOV) order contrasts with English Subject–Verb–Object (SVO) order. In our English-German system, we made the observation that the verb in an SVO subordinate clause following a punctuation mark frequently gets moved before the preceding punctuation. This movement is triggered by the German language model, which prefers verbs preceding punctuation as consistent with SOV order, and it is facilitated by the fact that the distance from the verb to the end of the preceding clause is often smaller than the distance to the end of the current phrase, so moving the verb backwards results in a better score from the distancebased reordering model. This tendency can be counteracted effectively by enabling the Moses which makes sure that words are not reordered across punctuation marks. The result is a modest gain from 14.28 to 14.38 BLEU points In the German-English system, we applied a chunk-based technique to produce lattices representing multiple permutations of the test sentences in order to enable long-range reorderings of verb phrases. This approach is similar to the reordering technique based on part-of-speech tags presented by Niehues and Kolss (2009), which results in the addition of a large number of reordering paths to the lattices. By contrast, we assume that verb reorderings only occur between shallow syntax chunks, and not within them. This makes it possible to limit the number of long-range reordering options in an effective way. We used the TreeTagger to perform shallow chunking of the German text. By man- 90 Figure 1: Word lattice for morphological reduction , dass auch [weitere [vom [betroffen sein . Figure 2: Chunk reordering lattice BLEU test-09 test-10 Baseline 18.77 20.1 + chunk-based reordering 18.94 20.3 Morphological reduction 19.31 20.6 + chunk-based reordering 19.79 21.1 News LM, case-sensitive evaluation Table 5: Results with morphological reduction and reordering on 2009/2010 ual inspection of a data sample, we then identified a few recurrent patterns of long reorderings involving the verbs. In particular, we focused on clause-final verbs in German SOV clauses, which we move to the left in order to approximate the English SVO word order. For each sentence a chunkbased lattice is created, which is then expanded into a word lattice like the one shown in fig. 2. The lattice representation provides the decoder with up to three possible reorderings for a particular verb chunk. It always retains the original word order as an alternative input. For technical reasons, we were unable to prepare a system with reordering, morphological reduction and all language models in time for the shared task. Our secondary submission with reordering is therefore not comparable with our best system, which includes more language models and morphological reduction. In subsequent experiments, we combined morphological reduction with chunk-based reordering (table 5). When morphological reduction is used, the reordering approach yields an improvement of about 0.5 BLEU percentage points. 5 Conclusions There are three important features specific to the FBK systems at WMT 2010: mixtures of large language models, German morphological reduction and decompounding and word reordering. Our approach to using large language models proved successful at the 2009 NIST MT evaluation. In the present evaluation, its effectiveness was reduced by a number of technical problems, which were mostly due to the limitations of disk access throughput in our parallel computing environment. We are working on methods to reduce and distribute disk accesses to large language models, which will be implemented in the IRSTLM language modelling toolkit (Federico et al., 2008). By doing so, we hope to overcome the current limitations and exploit the power of language model mixtures more fully. The Gertwol-based morphological reduction and decompounding component we used is a working solution that results in a significant improvement in translation quality. It is an alternative to the popular statistical compound splitting methods, such as the one by Koehn and Knight (2003), incorporating a greater amount of linguistic knowledge and offering morphological reduction even of simplex words to their base form in addition. It would be interesting to compare the relative performance of the two approaches systematically. Word reordering between German and English is a complex problem. Encouraged by the success of chunk-based verb reordering lattices on Arabic- English (Bisazza and Federico, 2010), we tried to adapt the same approach to the German-English language pair. It turned out that there is a larger variety of long reordering patterns in this case. Nevertheless, some experiments performed after 91 the official evaluation showed promising results. We plan to pursue this work in several directions: Defining a lattice weighting scheme that distinguishes between original word order and reordering paths could help the decoder select the more promising path through the lattice. Applying similar reordering rules to the training corpus would reduce the mismatch between the training data and the reordered input sentences. Finally, it would be useful to explore the impact of different distortion limits on the decoding of reordering lattices in order to find an optimal trade-off between decoderdriven short-range and lattice-driven long-range reordering.</abstract>
<note confidence="0.805247">Acknowledgements This work was supported by the EuroMatrixPlus project (IST-231720), which is funded by the Eu-</note>
<title confidence="0.56887975">ropean Commission under the Seventh Framework Programme for Research and Technological Development. References</title>
<author confidence="0.31174">Chunk-</author>
<affiliation confidence="0.3903015">based verb reordering in VSO sentences for Arabicstatistical machine translation. In Proceedings of the Joint Fifth Workshop on Statistical Translation and Metrics Uppsala,</affiliation>
<address confidence="0.820499">Sweden, July. Association for Computational Lin-</address>
<email confidence="0.843034">guistics.</email>
<note confidence="0.781098509803922">Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing word lattice transla- In of ACL-08: pages 1012– 1020, Columbus, Ohio, June. Association for Computational Linguistics. Marcello Federico, Nicola Bertoldi, and Mauro Cettolo. 2008. IRSTLM: an open source toolkit for large scale language models. In Interpages 1618–1621. ISCA. Howard Johnson, Joel Martin, George Foster, and Roland Kuhn. 2007. Improving translation qualby discarding most of the phrasetable. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPpages 967–975, Prague, Czech Republic, June. Association for Computational Linguistics. Philipp Koehn and Kevin Knight. 2003. Empirical for compound splitting. In of pages 187–193. Philipp Koehn, Hieu Hoang, Alexandra Birch, et al. 2007. Moses: open source toolkit for statistical matranslation. In meeting of the Association for Computational Linguistics: Demonstration pages 177–180, Prague. Kimmo Koskenniemi and Mariikka Haapalainen. 1996. GERTWOL – Lingsoft Oy. In Roland editor, Verifikation. Dokuzur Ersten Morpholympics chapter 11, pages 121–140. Niemeyer, Tübingen. Shankar Kumar and William Byrne. 2004. Minimum bayes-risk decoding for statistical machine translation. In Daniel Marcu Susan Dumais and Salim editors, 2004: Main Proceedpages 169–176, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics. Jan Niehues and Muntsin Kolss. 2009. A POS-based for long-range reorderings in SMT. In Proceedings of the Fourth Workshop on Statistical Mapages 206–214, Athens, Greece, March. Association for Computational Linguistics. Franz Josef Och. 2003. Minimum error rate training in machine translation. In of the 41st annual meeting of the Association for Computapages 160–167, Sapporo (Japan). Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. BLEU: a method for automatic of machine translation. In of the 40th annual meeting of the Association for Compages 311–318, Philadelphia.</note>
<abstract confidence="0.6694312">ACL. Helmut Schmid. 1994. Probabilistic part-of-speech using decision trees. In of the International Conference on New Methods in Lanpages 44–49.</abstract>
<intro confidence="0.777088">92</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Arianna Bisazza</author>
<author>Marcello Federico</author>
</authors>
<title>Chunkbased verb reordering in VSO sentences for ArabicEnglish statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="14946" citStr="Bisazza and Federico, 2010" startWordPosition="2391" endWordPosition="2394">s a working solution that results in a significant improvement in translation quality. It is an alternative to the popular statistical compound splitting methods, such as the one by Koehn and Knight (2003), incorporating a greater amount of linguistic knowledge and offering morphological reduction even of simplex words to their base form in addition. It would be interesting to compare the relative performance of the two approaches systematically. Word reordering between German and English is a complex problem. Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish (Bisazza and Federico, 2010), we tried to adapt the same approach to the German-English language pair. It turned out that there is a larger variety of long reordering patterns in this case. Nevertheless, some experiments performed after 91 the official evaluation showed promising results. We plan to pursue this work in several directions: Defining a lattice weighting scheme that distinguishes between original word order and reordering paths could help the decoder select the more promising path through the lattice. Applying similar reordering rules to the training corpus would reduce the mismatch between the training data</context>
</contexts>
<marker>Bisazza, Federico, 2010</marker>
<rawString>Arianna Bisazza and Marcello Federico. 2010. Chunkbased verb reordering in VSO sentences for ArabicEnglish statistical machine translation. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Dyer</author>
<author>Smaranda Muresan</author>
<author>Philip Resnik</author>
</authors>
<title>Generalizing word lattice translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>1012--1020</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="7891" citStr="Dyer et al. (2008)" startWordPosition="1257" endWordPosition="1260">s by the Institute of Computational Linguistics of the University of Zurich. As a side effect, Gertwol outputs the base forms of all words that it processes: Nominative singular of nouns, infinitive of verbs etc. We decided to combine the tokens analysed by Gertwol, whether or not they had been decompounded and lowercased, in a further attempt to reduce data sparseness, with their original form in a word lattice (see fig. 1) and to let the decoder make the choice between the two according to the translations the phrase table can provide for each. Our word lattices are similar to those used by Dyer et al. (2008) for handling word segmentation in Chinese and Arabic. For each word that was segmented by Gertwol, we provide exactly one alternative edge labelled with the component words and base forms as identified by Gertwol, after removing linking morphemes. The edge transition probabilities are used to identify the source of an edge: their values are e−1 = 0.36788 for edges deriving from Gertwol analysis and e0 = 1 for edges carrying unprocessed words. Tokens whose decompounded base form according to Gertwol is identical to the surface form in the input are represented by a single edge with transition </context>
</contexts>
<marker>Dyer, Muresan, Resnik, 2008</marker>
<rawString>Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing word lattice translation. In Proceedings of ACL-08: HLT, pages 1012– 1020, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Mauro Cettolo</author>
</authors>
<title>IRSTLM: an open source toolkit for handling large scale language models. In Interspeech</title>
<date>2008</date>
<pages>1618--1621</pages>
<publisher>ISCA.</publisher>
<contexts>
<context position="3378" citStr="Federico et al., 2008" startWordPosition="526" endWordPosition="529">e partial corpora for English and their sizes can be found in table 1. Our base models of the English Gigaword texts were trained on version 3 of the corpus (LDC2007T07). We trained separate language models for the new data from the years 2007 and 2008 included in version 4 (LDC2009T13). Apart from the monolingual English data, we also included language models trained on the English part of the additional parallel datasets supplied for the FrenchEnglish and Czech-English tasks. All the models were estimated as 6-gram models with KneserNey smoothing using the IRSTLM language modelling toolkit (Federico et al., 2008). For technical reasons, we were unable to use all the language models during decoding. We therefore selected a subset of the models with the following data selection procedure: 1. For a linear mixture of the complete set of 24 language models, we estimated a set of 88 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 88–92, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics Corpus n-grams Europarl v5 115,702,157 News 1,437,562,740 News commentary 10 10,381,511 Gigaword v3: 6 models 7,990,828,834 Gigaword 2007/08: 6 </context>
<context position="14120" citStr="Federico et al., 2008" startWordPosition="2262" endWordPosition="2265">es specific to the FBK systems at WMT 2010: mixtures of large language models, German morphological reduction and decompounding and word reordering. Our approach to using large language models proved successful at the 2009 NIST MT evaluation. In the present evaluation, its effectiveness was reduced by a number of technical problems, which were mostly due to the limitations of disk access throughput in our parallel computing environment. We are working on methods to reduce and distribute disk accesses to large language models, which will be implemented in the IRSTLM language modelling toolkit (Federico et al., 2008). By doing so, we hope to overcome the current limitations and exploit the power of language model mixtures more fully. The Gertwol-based morphological reduction and decompounding component we used is a working solution that results in a significant improvement in translation quality. It is an alternative to the popular statistical compound splitting methods, such as the one by Koehn and Knight (2003), incorporating a greater amount of linguistic knowledge and offering morphological reduction even of simplex words to their base form in addition. It would be interesting to compare the relative </context>
</contexts>
<marker>Federico, Bertoldi, Cettolo, 2008</marker>
<rawString>Marcello Federico, Nicola Bertoldi, and Mauro Cettolo. 2008. IRSTLM: an open source toolkit for handling large scale language models. In Interspeech 2008, pages 1618–1621. ISCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Howard Johnson</author>
<author>Joel Martin</author>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Improving translation quality by discarding most of the phrasetable.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>967--975</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2215" citStr="Johnson et al. (2007)" startWordPosition="331" endWordPosition="334">ed description of our approaches to language modelling, morphological preprocessing and word reordering. Both of our systems were based on the Moses decoder (Koehn et al., 2007). They were similar to the WMT 2010 Moses baseline system. Instead of lowercasing the training data and adding a recasing step, we retained the data in document case throughout our system, except for the morphologically normalised word forms described in section 3. Our phrase tables were trained with the standard Moses training script, then filtered based on statistical significance according to the method described by Johnson et al. (2007). Finally, we used Minimum Bayes Risk decoding (Kumar and Byrne, 2004) based on the BLEU score (Papineni et al., 2002). 2 Language modelling At the 2009 NIST MT evaluation, our system obtained good results using a mixture of linearly interpolated language models (LMs) combining data from different sources. As the training data provided for the present evaluation campaign again included a large set of language modelling corpora from different sources, especially for English as a target language, we decided to adopt the same strategy. The partial corpora for English and their sizes can be found </context>
</contexts>
<marker>Johnson, Martin, Foster, Kuhn, 2007</marker>
<rawString>Howard Johnson, Joel Martin, George Foster, and Roland Kuhn. 2007. Improving translation quality by discarding most of the phrasetable. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 967–975, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Empirical methods for compound splitting.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>187--193</pages>
<contexts>
<context position="14524" citStr="Koehn and Knight (2003)" startWordPosition="2327" endWordPosition="2330">n our parallel computing environment. We are working on methods to reduce and distribute disk accesses to large language models, which will be implemented in the IRSTLM language modelling toolkit (Federico et al., 2008). By doing so, we hope to overcome the current limitations and exploit the power of language model mixtures more fully. The Gertwol-based morphological reduction and decompounding component we used is a working solution that results in a significant improvement in translation quality. It is an alternative to the popular statistical compound splitting methods, such as the one by Koehn and Knight (2003), incorporating a greater amount of linguistic knowledge and offering morphological reduction even of simplex words to their base form in addition. It would be interesting to compare the relative performance of the two approaches systematically. Word reordering between German and English is a complex problem. Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish (Bisazza and Federico, 2010), we tried to adapt the same approach to the German-English language pair. It turned out that there is a larger variety of long reordering patterns in this case. Nevertheless, so</context>
</contexts>
<marker>Koehn, Knight, 2003</marker>
<rawString>Philipp Koehn and Kevin Knight. 2003. Empirical methods for compound splitting. In Proceedings of EACL, pages 187–193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Annual meeting of the Association for Computational Linguistics: Demonstration session,</booktitle>
<pages>177--180</pages>
<location>Prague.</location>
<contexts>
<context position="1771" citStr="Koehn et al., 2007" startWordPosition="259" endWordPosition="262"> some more effort into the inverse translation direction to make better use of the abundance of language modelling data available for English and to address the richness of German morphology, which makes it hard for a Statistical Machine Translation (SMT) system to achieve good vocabulary coverage. In the remainder of this section, an overview of the common features of our systems will be given. The next two sections provide a more detailed description of our approaches to language modelling, morphological preprocessing and word reordering. Both of our systems were based on the Moses decoder (Koehn et al., 2007). They were similar to the WMT 2010 Moses baseline system. Instead of lowercasing the training data and adding a recasing step, we retained the data in document case throughout our system, except for the morphologically normalised word forms described in section 3. Our phrase tables were trained with the standard Moses training script, then filtered based on statistical significance according to the method described by Johnson et al. (2007). Finally, we used Minimum Bayes Risk decoding (Kumar and Byrne, 2004) based on the BLEU score (Papineni et al., 2002). 2 Language modelling At the 2009 NIS</context>
</contexts>
<marker>Koehn, Hoang, Birch, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, et al. 2007. Moses: open source toolkit for statistical machine translation. In Annual meeting of the Association for Computational Linguistics: Demonstration session, pages 177–180, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Koskenniemi</author>
<author>Mariikka Haapalainen</author>
</authors>
<date>1996</date>
<booktitle>Linguistische Verifikation. Dokumentation zur Ersten Morpholympics 1994, chapter 11,</booktitle>
<pages>121--140</pages>
<editor>GERTWOL – Lingsoft Oy. In Roland Hausser, editor,</editor>
<location>Niemeyer, Tübingen.</location>
<contexts>
<context position="6885" citStr="Koskenniemi and Haapalainen, 1996" startWordPosition="1087" endWordPosition="1090">pora. 3 Morphological reduction and decompounding of German Compounding is a highly productive part of German noun morphology. Unlike in English, German compound nouns are usually spelt as single words, which greatly increases the vocabulary. For a Machine Translation system, this property of the language causes a high number of out-ofvocabulary (OOV) words. It is likely that many compounds in an input text have not been seen in the training corpus. We addressed this problem by splitting compounds in the German source text. Compound splitting was done using the Gertwol morphological analyser (Koskenniemi and Haapalainen, 1996), a linguistically informed system based on two-level finite state morphology. Since Gertwol outputs all possible analyses of a word form without taking into account the context, the output has to be disambiguated. For this purpose, we used part-of-speech (POS) tags obtained from the TreeTagger (Schmid, 1994) along with a set of POS-based heuristic disambiguation rules 89 provided to us by the Institute of Computational Linguistics of the University of Zurich. As a side effect, Gertwol outputs the base forms of all words that it processes: Nominative singular of nouns, infinitive of verbs etc.</context>
</contexts>
<marker>Koskenniemi, Haapalainen, 1996</marker>
<rawString>Kimmo Koskenniemi and Mariikka Haapalainen. 1996. GERTWOL – Lingsoft Oy. In Roland Hausser, editor, Linguistische Verifikation. Dokumentation zur Ersten Morpholympics 1994, chapter 11, pages 121–140. Niemeyer, Tübingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Minimum bayes-risk decoding for statistical machine translation.</title>
<date>2004</date>
<booktitle>HLT-NAACL 2004: Main Proceedings,</booktitle>
<volume>2</volume>
<pages>169--176</pages>
<editor>In Daniel Marcu Susan Dumais and Salim Roukos, editors,</editor>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="2285" citStr="Kumar and Byrne, 2004" startWordPosition="342" endWordPosition="345"> preprocessing and word reordering. Both of our systems were based on the Moses decoder (Koehn et al., 2007). They were similar to the WMT 2010 Moses baseline system. Instead of lowercasing the training data and adding a recasing step, we retained the data in document case throughout our system, except for the morphologically normalised word forms described in section 3. Our phrase tables were trained with the standard Moses training script, then filtered based on statistical significance according to the method described by Johnson et al. (2007). Finally, we used Minimum Bayes Risk decoding (Kumar and Byrne, 2004) based on the BLEU score (Papineni et al., 2002). 2 Language modelling At the 2009 NIST MT evaluation, our system obtained good results using a mixture of linearly interpolated language models (LMs) combining data from different sources. As the training data provided for the present evaluation campaign again included a large set of language modelling corpora from different sources, especially for English as a target language, we decided to adopt the same strategy. The partial corpora for English and their sizes can be found in table 1. Our base models of the English Gigaword texts were trained</context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>Shankar Kumar and William Byrne. 2004. Minimum bayes-risk decoding for statistical machine translation. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings, pages 169–176, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Niehues</author>
<author>Muntsin Kolss</author>
</authors>
<title>A POS-based model for long-range reorderings in SMT.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>206--214</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="11508" citStr="Niehues and Kolss (2009)" startWordPosition="1846" endWordPosition="1849">ore from the distancebased reordering model. This tendency can be counteracted effectively by enabling the Moses decoder’s monotone-at-punctuation feature, which makes sure that words are not reordered across punctuation marks. The result is a modest gain from 14.28 to 14.38 BLEU points (newstest2009). In the German-English system, we applied a chunk-based technique to produce lattices representing multiple permutations of the test sentences in order to enable long-range reorderings of verb phrases. This approach is similar to the reordering technique based on part-of-speech tags presented by Niehues and Kolss (2009), which results in the addition of a large number of reordering paths to the lattices. By contrast, we assume that verb reorderings only occur between shallow syntax chunks, and not within them. This makes it possible to limit the number of long-range reordering options in an effective way. We used the TreeTagger to perform shallow syntax chunking of the German text. By man90 Figure 1: Word lattice for morphological reduction Sonst [drohe]VC , dass auch [weitere Länder]NC [vom Einbruch]PC [betroffen sein würden]VC . Figure 2: Chunk reordering lattice BLEU test-09 test-10 Baseline 18.77 20.1 + </context>
</contexts>
<marker>Niehues, Kolss, 2009</marker>
<rawString>Jan Niehues and Muntsin Kolss. 2009. A POS-based model for long-range reorderings in SMT. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 206–214, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<location>Sapporo</location>
<contexts>
<context position="8726" citStr="Och, 2003" startWordPosition="1400" endWordPosition="1401">ving linking morphemes. The edge transition probabilities are used to identify the source of an edge: their values are e−1 = 0.36788 for edges deriving from Gertwol analysis and e0 = 1 for edges carrying unprocessed words. Tokens whose decompounded base form according to Gertwol is identical to the surface form in the input are represented by a single edge with transition probability e−0.5 = 0.606531. These transition probabilities translate into a binary feature with values −1, −0.5 and 0 after taking logarithms in the decoder. The feature weight is determined by Minimum Error-Rate Training (Och, 2003), together with the weights of the other feature functions used in the decoder. During system training, the processed version of the training corpus was concatenated with the unprocessed text. Experiments show that decompounding and morphological analysis have a significant impact on the performance of the MT system. After these steps, the OOV rate of the newstest2009 test set decreases from 5.88 % to 3.21 %. Using only the News language model, the BLEU score of our development system (measured on the newstest2009 corpus) increases from 18.77 to 19.31. There is an interesting interaction with </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st annual meeting of the Association for Computational Linguistics, pages 160–167, Sapporo (Japan).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>ACL.</publisher>
<location>Philadelphia.</location>
<contexts>
<context position="2333" citStr="Papineni et al., 2002" startWordPosition="351" endWordPosition="354">systems were based on the Moses decoder (Koehn et al., 2007). They were similar to the WMT 2010 Moses baseline system. Instead of lowercasing the training data and adding a recasing step, we retained the data in document case throughout our system, except for the morphologically normalised word forms described in section 3. Our phrase tables were trained with the standard Moses training script, then filtered based on statistical significance according to the method described by Johnson et al. (2007). Finally, we used Minimum Bayes Risk decoding (Kumar and Byrne, 2004) based on the BLEU score (Papineni et al., 2002). 2 Language modelling At the 2009 NIST MT evaluation, our system obtained good results using a mixture of linearly interpolated language models (LMs) combining data from different sources. As the training data provided for the present evaluation campaign again included a large set of language modelling corpora from different sources, especially for English as a target language, we decided to adopt the same strategy. The partial corpora for English and their sizes can be found in table 1. Our base models of the English Gigaword texts were trained on version 3 of the corpus (LDC2007T07). We tra</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on New Methods in Language Processing,</booktitle>
<pages>44--49</pages>
<contexts>
<context position="7195" citStr="Schmid, 1994" startWordPosition="1137" endWordPosition="1138">out-ofvocabulary (OOV) words. It is likely that many compounds in an input text have not been seen in the training corpus. We addressed this problem by splitting compounds in the German source text. Compound splitting was done using the Gertwol morphological analyser (Koskenniemi and Haapalainen, 1996), a linguistically informed system based on two-level finite state morphology. Since Gertwol outputs all possible analyses of a word form without taking into account the context, the output has to be disambiguated. For this purpose, we used part-of-speech (POS) tags obtained from the TreeTagger (Schmid, 1994) along with a set of POS-based heuristic disambiguation rules 89 provided to us by the Institute of Computational Linguistics of the University of Zurich. As a side effect, Gertwol outputs the base forms of all words that it processes: Nominative singular of nouns, infinitive of verbs etc. We decided to combine the tokens analysed by Gertwol, whether or not they had been decompounded and lowercased, in a further attempt to reduce data sparseness, with their original form in a word lattice (see fig. 1) and to let the decoder make the choice between the two according to the translations the phra</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the International Conference on New Methods in Language Processing, pages 44–49.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>