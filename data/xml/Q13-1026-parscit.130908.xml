<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.997997">
Parsing entire discourses as very long strings: Capturing topic continuity in
grounded language learning
</title>
<author confidence="0.999249">
Minh-Thang Luong Michael C. Frank Mark Johnson
</author>
<affiliation confidence="0.9995035">
Department of Computer Science Department of Psychology Department of Computing
Stanford University Stanford University Macquarie University
</affiliation>
<address confidence="0.791736">
Stanford, California Stanford, California Sydney, Australia
</address>
<email confidence="0.999413">
lmthang@stanford.edu mcfrank@stanford.edu Mark.Johnson@MQ.edu.au
</email>
<sectionHeader confidence="0.997395" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998295">
Grounded language learning, the task of map-
ping from natural language to a representation
of meaning, has attracted more and more in-
terest in recent years. In most work on this
topic, however, utterances in a conversation
are treated independently and discourse struc-
ture information is largely ignored. In the
context of language acquisition, this indepen-
dence assumption discards cues that are im-
portant to the learner, e.g., the fact that con-
secutive utterances are likely to share the same
referent (Frank et al., 2013). The current pa-
per describes an approach to the problem of
simultaneously modeling grounded language
at the sentence and discourse levels. We com-
bine ideas from parsing and grammar induc-
tion to produce a parser that can handle long
input strings with thousands of tokens, creat-
ing parse trees that represent full discourses.
By casting grounded language learning as a
grammatical inference task, we use our parser
to extend the work of Johnson et al. (2012),
investigating the importance of discourse con-
tinuity in children’s language acquisition and
its interaction with social cues. Our model
boosts performance in a language acquisition
task and yields good discourse segmentations
compared with human annotators.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999903511627907">
Learning mappings between natural language (NL)
and meaning representations (MR) is an important
goal for both computational linguistics and cognitive
science. Accurately learning novel mappings is cru-
cial in grounded language understanding tasks and
such systems can suggest insights into the nature of
children language learning.
Two influential examples of grounded language
learning tasks are the sportscasting task, RoboCup,
where the NL is the set of running commentary and
the MR is the set of logical forms representing ac-
tions like kicking or passing (Chen and Mooney,
2008), and the cross-situational word-learning task,
where the NL is the caregiver’s utterances and the
MR is the set of objects present in the context
(Siskind, 1996; Yu and Ballard, 2007). Work
in these domains suggests that, based on the co-
occurrence between words and their referents in
context, it is possible to learn mappings between NL
and MR even under substantial ambiguity.
Nevertheless, contexts like RoboCup—where ev-
ery single utterance is grounded—are extremely
rare. Much more common are cases where a sin-
gle topic is introduced and then discussed at length
throughout a discourse. In a television news show,
for example, a topic might be introduced by present-
ing a relevant picture or video clip. Once the topic
is introduced, the anchors can discuss it by name
or even using a pronoun without showing a picture.
The discourse is grounded without having to ground
every utterance.
Moreover, although previous work has largely
treated utterance order as independent, the order of
utterances is critical in grounded discourse contexts:
if the order is scrambled, it can become impossible
to recover the topic. Supporting this idea, Frank et
al. (2013) found that topic continuity—the tendency
to talk about the same topic in multiple utterances
that are contiguous in time—is both prevalent and
informative for word learning. This paper examines
the importance of topic continuity through a gram-
matical inference problem. We build on Johnson et
al. (2012)’s work that used grammatical inference to
</bodyText>
<page confidence="0.993678">
315
</page>
<note confidence="0.372773">
Transactions of the Association for Computational Linguistics, 1 (2013) 315–326. Action Editor: Mark Steedman.
Submitted 2/2013; Revised 6/2013; Published 7/2013. c�2013 Association for Computational Linguistics.
</note>
<figure confidence="0.993294681818182">
Sentence
Topic.pig Words.pig
.dog # .pig child.eyes mom.hands # ## wheres the piggie
Word.None Words.pig
T.None
Topic.pig
NotTopical.child.eyes
NotTopical.child.hands
NotTopical.mom.eyes
NotTopical.mom.hands
NotTopical.mom.point
T.pig
Topical.child.eyes
Topical.child.hands
Topical.mom.eyes
Topical.mom.hands
Topical.mom.point
Words.pig
Word.pig
Topic.None
Word.None
dog object prefix pig object prefix utterance
</figure>
<figureCaption confidence="0.995982333333333">
Figure 1: Unigram Social Cue PCFGs (Johnson et al., 2012) – shown is a parse tree of the input utterance “wheres
the piggie” accompanied with social cue prefixes, indicating that the caregiver is holding a pig toy while the child is
looking at it; at the same time, a dog toy is present in the screen.
</figureCaption>
<bodyText confidence="0.999958464285714">
learn word-object mappings and to investigate the
role of social information (cues like eye-gaze and
pointing) in a child language acquisition task.
Our main contribution lies in the novel integra-
tion of existing techniques and algorithms in parsing
and grammar induction to offer a complete solution
for simultaneously modeling grounded language at
the sentence and discourse levels. Specifically, we:
(1) use the Earley algorithm to exploit the special
structure of our grammars, which are deterministic
or have at most bounded ambiguity, to achieve ap-
proximately linear parsing time; (2) suggest a rescal-
ing approach that enables us to build a PCFG parser
capable of handling very long strings with thou-
sands of tokens; and (3) employ Variational Bayes
for grammatical inference to obtain better grammars
than those given by the EM algorithm.
By parsing entire discourses at once, we shed light
on a scientifically interesting question about why the
child’s own gaze is a positive cue for word learn-
ing (Johnson et al., 2012). Our data provide support
for the hypothesis (from previous work) that care-
givers “follow in”: they name objects that the child
is already looking at (Tomasello and Farrar, 1986).
In addition, our discourse model produces a perfor-
mance improvement in a language acquisition task
and yields good discourse segmentations compared
with human annotators.
</bodyText>
<sectionHeader confidence="0.999963" genericHeader="introduction">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.745073">
Supervised semantic parsers. Previous work has
</subsectionHeader>
<bodyText confidence="0.997152032258064">
developed supervised semantic parsers to map sen-
tences to meaning representations of various forms,
including meaning hierarchies (Lu et al., 2008) and,
most dominantly, A-calculus expressions (Zettle-
moyer and Collins, 2005; Zettlemoyer, 2007; Wong
and Mooney, 2007; Kwiatkowski et al., 2010).
These approaches rely on training data of annotated
sentence-meaning pairs, however. Such data are
costly to obtain and are quite different from the ex-
perience of language learners.
Grounded Language Learning. In contrast to se-
mantic parsers, grounded language learning systems
aim to learn the meanings of words and sentences
given an observed world state (Yu and Ballard, 2004;
Gorniak and Roy, 2007). A growing body of work
in this field employs distinct techniques from a wide
variety of perspectives from text-to-record align-
ment using structured classification (Barzilay and
Lapata, 2005; Snyder and Barzilay, 2007), iterative
retraining (Chen et al., 2010), and generative models
of segmentation and alignment (Liang et al., 2009)
to text-to-interaction mapping using reinforcement
learning (Branavan et al., 2009; Vogel and Juraf-
sky, 2010), graphical model semantics representa-
tion (Tellex et al., 2011a; Tellex et al., 2011b), and
Combinatory Categorial Grammar (Artzi and Zettle-
moyer, 2013). A number of systems have also used
alternative forms of supervision, including sentences
paired with responses (Clarke et al., 2010; Gold-
wasser and Roth, 2011; Liang et al., 2011) and
no supervision (Poon and Domingos, 2009; Gold-
</bodyText>
<page confidence="0.998535">
316
</page>
<bodyText confidence="0.98409995">
wasser et al., 2011).
Recent work has also introduced an alternative
approach to grounded learning by reducing it to a
grammatical inference problem. B¨orschinger et al.
(2011) casted the problem of learning a semantic
parser as a PCFG induction task, achieving state-of
the art performance in the RoboCup domain. Kim
and Mooney (2012) extended the technique to make
it tractable for more complex problems. Later, Kim
and Mooney (2013) adapted discriminative rerank-
ing to the grounded learning problem using a form
of weak supervision. We employ this general gram-
matical inference approach in the current work.
Children Language Acquisition. In the context of
language acquisition, Frank et al. (2008) proposed
a system that learned words and jointly inferred
speakers’ intended referent (utterance topic) using
graphical models. Johnson et al. (2012) used gram-
matical inference to demonstrate the importance of
social cues in children’s early word learning. We ex-
tend this body of work by capturing discourse-based
dependencies among utterances rather than treating
each utterance independently.
Discourse Parsing. A substantial literature has ex-
amined formal representations of discourse across
a wide variety of theoretical perspectives (Mann
and Thompson, 1988; Scha and Polanyi, 1988;
Hobbs, 1990; Lascarides and Asher, 1993; Knott
and Sanders, 1997). Although much of this work
was highly influential, Marcu (1997)’s work on dis-
course parsing brought this task to special promi-
nence. Since then, more and more sophisticated
models of discourse analysis have been developed:,
e.g., (Marcu, 1999; Soricut and Marcu, 2003; Forbes
et al., 2003; Polanyi et al., 2004; Baldridge and Las-
carides, 2005; Subba and Di Eugenio, 2009; Her-
nault et al., 2010; Lin et al., 2012; Feng and Hirst,
2012). Our contribution to work on this task is
to examine latent discourse structure specifically in
grounded language learning.
</bodyText>
<sectionHeader confidence="0.973824" genericHeader="method">
3 A Grounded Learning Task
</sectionHeader>
<bodyText confidence="0.99996535">
Our focus in this paper is to develop computational
models that help us better understand children’s lan-
guage acquisition. The goal is to learn both the
long term lexicon of mappings between words and
objects (language learning) as well as the intended
topic of individual utterances (language comprehen-
sion). We consider a corpus of child-directed speech
annotated with social cues, described in (Frank et
al., 2013). There are a total of 4,763 utterances
in the corpus, each of which is orthographically-
transcribed from videos of caregivers playing with
pre-linguistic children of various ages (6, 12, and
18 months) during home visits.&apos; Each utterance
was hand-annotated with objects present in the
(non-linguistic) context, e.g. dog and pig (Fig-
ure 1), together with sets of social cues, one set
per object. The social cues describe objects the
care-giver is looking at (mom.eyes), holding onto
(mom.hands), or pointing to (mom.point); sim-
ilarly, for (child.eyes) and (child.hands).
</bodyText>
<subsectionHeader confidence="0.982492">
3.1 Sentence-level Models
</subsectionHeader>
<bodyText confidence="0.996138233333334">
Motivated by the importance of social information
in children’s early language acquisition (Carpenter
et al., 1998), Johnson et al. (2012) proposed a joint
model of non-linguistic information including the
physical context and social cues, and the linguis-
tic content of individual utterances. They framed
the joint inference problem of inferring word-object
mappings and inferring sentence topics as a gram-
mar induction task where input strings are utterances
prefixed with non-linguistic information. Objects
present in the non-linguistic context of an utterance
are considered its potential topics. There is also a
special null topic, None, to indicate non-topical ut-
terances. The goal of the model is then to select the
most probable topic for each utterance.
Top-level rules, Sentence -+ Topict Wordst
(unigram PCFG) or Sentence -+ Topict
Collocst (collocation Adaptor Grammar), are tai-
lored to link the two modalities (t ranges over T′,
the set of all available topics (T) and None). These
rules enforce sharing of topics between prefixes
(Topict) and words (Wordst or Collocst). Each
word in the utterance is drawn from either a topic-
specific distribution Wordt or a general “null” dis-
tribution WordNone.
As illustrated in Figure 1, the selected topic, pig,
is propagated down to the input string through two
paths: (a) through topical nodes until an object is
&apos;Caregivers were given pairs of toys to play with, e.g. a
stuffed dog and pig, or a wooden car and truck.
</bodyText>
<page confidence="0.997448">
317
</page>
<bodyText confidence="0.9999584">
reached, in this case the .pig object, and (b) through
lexical nodes to topical word tokens, e.g. piggie. So-
cial cues are then generated by a series of binary
decisions as detailed in Johnson et al. (2012). The
key feature of these grammars is that parameter in-
ference corresponds both to learning word-topic re-
lations and learning the salience of social cues in
grounded learning.
In the current work, we restrict our attention to
only the unigram PCFG model to focus on investi-
gating the role of topic continuity. Unlike the ap-
proach of Johnson et al. (2012), which uses Markov
Chain Monte Carlo techniques to perform gram-
matical inference, we experiment with Variational
Bayes methods, detailed in Section 6.
</bodyText>
<subsectionHeader confidence="0.996806">
3.2 A Discourse-level Model
</subsectionHeader>
<bodyText confidence="0.999881260869565">
Topic continuity—the tendency to group utterances
into coherent discourses about a single topic—may
be an important source of information for children
learning the meanings of words (Frank et al., 2013).
To address this issue, we consider a new discourse-
level model of grounded language that captures de-
pendencies between utterances. By linking multiple
utterances in a single parse, our proposed grammati-
cal formalism is a bigram Markov process that mod-
els transitions among utterance topics.
Our grammar starts with a root symbol
Discourse, which then selects a starting
topic through a set of discourse initial rules,
Discourse -+ Discourset for t E V. Each
of the Discourset nodes generates an utter-
ance of the same topic, and advances into other
topics through transition rules, Discourset -+
Sentencet Discourset′ for t′ E V. Dis-
courses terminate by ending rules, Discourset
-+ Sentencet. Other rules in the unigram PCFG
model by Johnson are reused except for the top-
level rules in which we replace the non-terminal
Sentence by topic-specific ones Sentencet.
</bodyText>
<subsectionHeader confidence="0.99996">
3.3 Parsing Discourses and Challenges
</subsectionHeader>
<bodyText confidence="0.999875833333333">
Using a discourse-level grammar, we must parse
a concatenation of all the utterances (with annota-
tions) in each conversation. This concatenation re-
sults in an extremely long string: in the social-cue
corpus (Frank et al., 2013), the average length of
these per-recording concatenations is 2152 tokens
(u=972). Parsing such strings poses many chal-
lenges for existing algorithms.
For familiar algorithms such as CYK, runtime
quickly becomes enormous: the time complexity of
CYK is O(n3) for an input of length n. Fortunately,
we can take advantage of a special structural prop-
erty of our grammars. The shape of the parse tree is
completely determined by the input string; the only
variation is in the topic annotations in the nonter-
minal labels. So even though the number of possi-
ble parses grows exponentially with input length n,
the number of possible constituents grows only lin-
early with input length, and the possible constituents
can be identified from the left context.2 These con-
straints ensure that the Earley algorithm3 (Earley,
1970) will parse an input of length n with this gram-
mar in time O(n).
A second challenge in parsing very long strings is
that the probability of a parse is the product of the
probabilities of the rules involved in its derivation.
As the length of a derivation grows linearly with the
length of the input, the parse probabilities decrease
exponentially as a function of sentence length, caus-
ing floating-point underflow on inputs of even mod-
erate length. The standard method for handling this
is to compute log probabilities (which decrease lin-
early as a function of input length, rather than ex-
ponentially), but as we explain later (Section 5), we
can use the ability of the Earley algorithm to com-
pute prefix probabilities (Stolcke, 1995) to rescale
the probability of the parse incrementally and avoid
floating-point underflows.
In the next section, we provide background in-
formation on the Earley algorithm for PCFGs, the
prefix probability scheme we use, and the inside-
outside algorithm in the Earley context.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="method">
4 Background
</sectionHeader>
<subsectionHeader confidence="0.999155">
4.1 Earley Algorithm for PCFGs
</subsectionHeader>
<bodyText confidence="0.998306">
The Earley algorithm was developed by Earley
(1970) and known to be efficient for certain kinds
of CFGs (Aho and Ullman, 1972). An Earley parser
</bodyText>
<footnote confidence="0.439410333333333">
2The prefix markers # and ## and the topic markers such
as “.dog” enable a left-to- right parser to unambiguously iden-
tify its location in the input string.
3In order to achieve linear time the parsing chart must have
suitable indexing; see Aho and Ullman (1972), Leo (1991) and
Aycock and Horspool (2002) for details.
</footnote>
<page confidence="0.996545">
318
</page>
<bodyText confidence="0.999100416666667">
constructs left-most derivations of strings, using dot-
ted productions to keep track of partial derivations.
Specifically, each state in an Earley parser is rep-
resented as [l, r]: X-+α . 0 to indicate that input
symbols xl, ... , xr−1 have been processed and the
parser is expecting to expand 0. States are gen-
erated on the fly using three transition operations:
predict (add states to charts), scan (shift dots across
terminals), and complete (merge two states). Fig-
ure 2 shows an example of a completion step which
also illustrates the implicit binarization automati-
cally done in Earley algorithm.
</bodyText>
<figureCaption confidence="0.689431">
Figure 2: Completion step – merging two states [l, m]:
X-+α . Y o and [m, r]:Y-+v . to produce a new state
[l, r]: X-+αY . o.
</figureCaption>
<bodyText confidence="0.999939176470588">
In order to handle PCFGs, Stolcke (1995) extends
the Earley parsing algorithm to introduce the no-
tion of an Earley path being a sequence of states
linked by Earley operations. By establishing a one-
to-one mapping between partial derivations and Ear-
ley paths, Stolcke could then assign each path a
derivation probability, that is the product of the all
rule probabilities used in the predicted states of that
path. Here, each production X-+v corresponds to a
predicted state [l, l] : X-+. V.
Besides parsing, being able to compute string and
prefix probabilities by summing derivation probabil-
ities is also of great importance. To compute these
sums efficiently, each Earley state is attached with a
forward and an inner probability which are updated
incrementally as new states are spawned by the three
transition operations.
</bodyText>
<subsectionHeader confidence="0.995654">
4.2 Forward and Prefix Probabilities
</subsectionHeader>
<bodyText confidence="0.9998337">
Intuitively, the forward probability of a state [l, r]:
X-+α . 0 is the probability of an Earley path
through that state, generating input up to position
r-1. This probability generalizes a similar concept
in HMM and lends itself to the computation of pre-
fix probabilities, sums of forward probabilities over
scanned states yielding a prefix x.
Computing prefix probabilities is important be-
cause it enables probabilistic prediction of pos-
sible follow-words xi+1 as P(xi+1|x0 ... xi) _
</bodyText>
<equation confidence="0.911312">
P(x°...xixi+1) (Jelinek and Lafferty, 1991). These
P(xp ... xi)
</equation>
<bodyText confidence="0.999989818181818">
conditional probabilities allow estimation of the in-
cremental costs of a stack decoder (Bahl et al.,
1983). In (Huang and Sagae, 2010), a conceptu-
ally similar prefix cost is defined to order states in
a beam search decoder. Moreover, the negative log-
arithm of such conditional probabilities are termed
as surprisal values in the psycholinguistics literature
(e.g., Hale, 2001; Levy, 2008), to describe how dif-
ficult a word is in a given context. Interestingly, we
show that prefix probabilities lead us to construct a
parser that could parse extremely long strings next.
</bodyText>
<subsectionHeader confidence="0.996165">
4.3 Inside Outside Algorithm
</subsectionHeader>
<bodyText confidence="0.98174775">
To extend the Inside Outside (IO) algorithm (Baker,
1979) to the Earley context, Stolcke introduced in-
ner and outer probabilities which generalize the in-
side and outside probabilities in the IO algorithm.
Specifically, the inner probability of a state [l, r]:
X-+α . 0 is the probability of generating an input
substring xl, ... , xr−1 from a non-terminal X using
a production X-+α 0.4
</bodyText>
<equation confidence="0.529910666666667">
X-+aY.Q
X - +a . Y Q Y -+ V.
I m r
</equation>
<figureCaption confidence="0.600910166666667">
Figure 3: Inner and outer probabilities. The outer
probability of X-+α . Y o is a sum of all products of
its parent outer probability (X-+αY . o) and its sibling
inner probability (Y-+v .). Similarly, the outer proba-
bility of Y -+v . is derived from the outer probability of
X-+αY . o and the inner probability of X-+α . Y o.
</figureCaption>
<bodyText confidence="0.999932666666667">
Once all inner probabilities have been populated
in a forward pass, outer probabilities are derived
backward, starting from the outer probability of the
goal state [0, n] :-+ S . being 1. Here, each Earley
state is associated with an outer probability which
complements the inner probability by referring pre-
cisely to those parts (not covered by the correspond-
ing inner probability) of the complete paths generat-
ing the input string x. The implicit binarization in
</bodyText>
<footnote confidence="0.580039">
4Summing up inner probabilities of all states Y-+v . exactly
yields Baker’s inside probability for Y .
</footnote>
<equation confidence="0.989282666666667">
X -+ a Y . /3
X - +a .Y /3 Y - +v.
l m r
</equation>
<page confidence="0.991927">
319
</page>
<bodyText confidence="0.999827166666667">
Earley parsing allows outer probabilities to be accu-
mulated in a similar way as its counterpart in the IO
algorithm (see Figure 3).
These quantities allow for efficient grammatical
inference in which the expected count of each rule
X-+A given a string x is computed as:
</bodyText>
<equation confidence="0.90043175">
c(X-+A|x) = Es-[l,r]X-+.λ outer(s) · inner(s)
.
P(S =&gt;* x)
(1)
</equation>
<sectionHeader confidence="0.980343" genericHeader="method">
5 A Rescaling Approach for Parsing
</sectionHeader>
<bodyText confidence="0.999521571428571">
Our parser originated from the prefix probability
parser by Levy (2008), but has diverged markedly
since then. The parser, called Earleyx5, is ca-
pable of producing Viterbi parses and performing
grammatical induction based on the expectation-
maximization and variational Bayes algorithms.
To tackle the underflow problem posed when
parsing discourses (§3.3), we borrow the rescal-
ing concept from HMMs (Rabiner, 1990) to extend
the probabilistic Earley algorithm. Specifically, the
probability of each Earley path is scaled by a con-
stant ci each time it passes through a scanned state
generating the input symbol xi. In fact, each path
passes through each scanned state exactly once, so
we consistently accumulate scaling factors for the
forward and inner probabilities of a state [l, r] :
X-+α . 0 as c0 ... cr−1 and cl ... cr−1 respectively.
Arguably, the most intuitive choice of the scal-
ing factors are the prefix probabilities, which essen-
tially resets the probability of any Earley path start-
ing from any position i to 1. Concretely, we set
</bodyText>
<equation confidence="0.988287">
c0 =1 and c = P(x0 ... xi−1) for i=1, ... , n-
P(x0) P(x0 ... xi)
</equation>
<bodyText confidence="0.970327428571429">
1 where n is the input length. As noted in section
§4.2, the logarithm of ci gives us the surprisal value
for the input symbol xi.
Rescaling factors are only introduced in the for-
ward pass, during which the outer probability of a
state [l, r]: X-+α . 0 has already been scaled by fac-
tors c0 ... cl−1cr ... cn−1.6 More importantly, when
</bodyText>
<footnote confidence="0.981295714285714">
5Parser code is available at http://nlp.stanford.
edu/˜lmthang/earleyx.
6The outer probability of a state is essentially the product of
inner probabilities covering all input symbols outside the span
of that state. For grammars containing cyclic unit productions,
we also need to multiply with terms from the unit-production
relation matrix (Stolcke, 1995).
</footnote>
<bodyText confidence="0.999112">
computing expected counts, scaling factors in the
outer and inner terms cancel out with those in the
string probability in Eq. (1), implying that rule prob-
ability estimation is unaffected by rescaling.
</bodyText>
<subsectionHeader confidence="0.997441">
5.1 Parsing Time on Dense Grammars
</subsectionHeader>
<bodyText confidence="0.99954075">
We compare in Table 1 the parsing time (on a
2.4GHz Xeon CPU) of our parser (Earleyx) and
Levy’s. The task is to compute surprisal values for
a 22-word sentence over a dense grammar.7 Given
that our parser is now capable of performing scaling
to avoid underflow, we avoid converting probabili-
ties to logarithmic form, which yields a speedup of
about 4 times compared to Levy’s parser.
</bodyText>
<table confidence="0.427136666666667">
Parser Time (s)
(Levy, 2008) 640
Earleyx + scaling 145
</table>
<tableCaption confidence="0.984334666666667">
Table 1: Parsing time (dense grammars) – to compute
surprisal values for a 22-word sentence using Levy’s
parser and ours (Earleyx).
</tableCaption>
<figure confidence="0.451433">
# Words
</figure>
<figureCaption confidence="0.9257165">
Figure 4: Parsing time (sparse grammars) – to compute
Viterbi parses for sentences of increasing lengths.
</figureCaption>
<bodyText confidence="0.994068">
Figure 4 shows the time taken (as a function of
the input length) for Earleyx to compute a Viterbi
parses over our sparse grammars (§3.2). The plot
confirmed our analysis in that the special structure
of our grammars yields approximately linear parsing
time in the input length (see §3.3).
</bodyText>
<footnote confidence="0.893229">
7MLE estimated from the English Penn Treebank.
</footnote>
<figure confidence="0.9545978">
5.2 Parsing Time on Sparse Grammars
7
6
5
4
3
2
seconds
1
200 400 600 800 1000 1200 1400 1600 1800 2000
</figure>
<page confidence="0.969801">
320
</page>
<sectionHeader confidence="0.988538" genericHeader="method">
6 Grammar Induction
</sectionHeader>
<bodyText confidence="0.998358590909091">
We employ a Variational Bayes (VB) approach to
perform grammatical inference instead of the stan-
dard Inside Outside (IO) algorithm, or equivalently
the Expectation Maximization (EM) algorithm, for
several reasons: (1) it has been shown to be less
likely to cause over-fitting for PCFGs than EM
(Kurihara and Sato, 2004) and (2) implementation-
wise, VB is a straightforward extension from EM
as they both share the same process of computing
the expected counts (the IO part) and only differ
at how rule probabilities are reestimated. At the
same time, VB has also been demonstrated to do
well on large datasets and is competitive with Gibbs
samplers while having the fastest convergence time
among these estimators (Gao and Johnson, 2008).
The rule reestimation in VB is carried as fol-
lows. Let αr be the prior hyperparameter of a
rule r in the rule set R and cr be its expected
count accumulated over the entire corpus after an
IO iteration. The posterior hyperparameter for r is
αr = αr + cr. Let ψ be the digamma function,
the rule parameter update formula is: θr:X-+λ =
</bodyText>
<equation confidence="0.9704535">
exp (αr) − ψ (�r′:X-+λ′ α� )].
r′
</equation>
<bodyText confidence="0.999244625">
Whereas IO minimizes the negative log-
likelihood of the observed data (sentences),
-log p(x), VB minimizes a quantity called free
energy, which we will use later to monitor con-
vergence. Here x denotes the observed data and
0 represents the model parameters (PCFG rule
probabilities). Following (Kurihara and Sato, 2006),
we compute the free energy as:
</bodyText>
<equation confidence="0.5385306">
� lo r (Er:X�λ αr)
F(x, 0) = −log p(x) + g Γ (�r:X�λ αr)
XEN
−1: (log T �arj + cr log θr
rER r
</equation>
<bodyText confidence="0.998848">
where Γ denotes the gamma function.
</bodyText>
<subsectionHeader confidence="0.990824">
6.1 Sparse Dirichlet Priors
</subsectionHeader>
<bodyText confidence="0.999690571428571">
In our application, since each topic should only be
associated with a few words rather than the entire
vocabulary, we impose sparse Dirichlet priors over
the Wordt distributions by setting a symmetric prior
α&lt;1 for all rules Wordt-+w (bt E T, w E W),
where W is the set of all words in the corpus. This
biases the model to select only a few rules per non-
terminal Wordt.8 For all other rules, a uniform hy-
perparameter value of 1 is used. We initialized rule
probabilities with uniform distributions plus random
noise. It is worthwhile to mention that sparse Dirich-
let priors were proposed in Johnson (2010)’s work
that learns Latent Dirchlet Allocation topic models
using Bayesian inference for PCFGs.
</bodyText>
<sectionHeader confidence="0.999397" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<bodyText confidence="0.99998480952381">
Our experiments apply sentence- and discourse-
level models to the annotated corpus of child-
directed speech described in Section 3. Each model
is evaluated on (a) topic accuracy—how many utter-
ances are labeled with correct topics (including the
null), (b) topic metrics (f-scores/precision/recall)—
how well the model predicts non-null topical utter-
ances, (c) word metrics—how well the model pre-
dicts topical words,9 and (d) lexicon metrics—how
well word types are assigned to the topic that they
attach to most frequently. For example, in Figure 1,
the model assigns topic pig to the entire utterance.
At the word level, it labels piggie with topic pig and
assigns null topic to wheres and the. See (Johnson et
al., 2012) for more details of these metrics.
In Section 7.1, we examine baseline models that
do not make use of social cues (mother and child’s
eye-gaze and hand position) to discover the topic;
these baselines are contrasted with a range of social
cues (§7.2 and §7.3). In Section 7.4, we evaluate the
discourse structures discovered by our models.
</bodyText>
<subsectionHeader confidence="0.979282">
7.1 Baseline Models (No Social Cues)
</subsectionHeader>
<bodyText confidence="0.999797428571429">
To create baselines for later experiments, we eval-
uate our models without social information. We
compare sentence-level models using three different
inference procedures—Markov Chain Monte Carlo
(MCMC) (Johnson et al., 2012), Expectation Max-
imization (EM), and Variational Bayes (VB)10—as
well as the discourse-level model described above.
</bodyText>
<footnote confidence="0.543242875">
8It is important to not sparsify the WordNone distribution
since WordNone could expand into many non-topical words.
9Topics assigned by the model are compared with those
given by the gold dictionary provided by (Johnson et al., 2012).
10To determine the best sparsity hyperparameter α for
lexical rules (�6.1), we performed a line search over
{1,0.1, 0.01, 0.001, 0.00011. As α decreases, performance im-
proves, peaking at 0.001, the value used for all reported results
</footnote>
<page confidence="0.995178">
321
</page>
<table confidence="0.998513833333333">
Acc. Topic Model F1 Word R F1 Lexicon R Energy
F1 P R P P
MCMC 49.07 60.64 48.67 80.43 29.50 17.63 90.31 14.83 8.10 88.10
VB 53.14 60.89 50.53 76.59 25.62 14.94 89.91 16.71 9.25 85.71 156719
discourse 51.02 59.40 48.60 76.35 23.86 13.82 87.33 15.05 8.27 83.33 150023
discourse+init 55.78 60.91 52.15 73.22 29.75 17.91 87.65 21.11 11.95 90.48 149458
</table>
<tableCaption confidence="0.9721995">
Table 2: Social-cue models. Comparison of sentence- and discourse-level models (init: initialized from the VB
sentence-level model) over full metrics. Free energies are shown to compare VB-based models.
</tableCaption>
<table confidence="0.9994738">
Model Acc. Topic F1 Word F1 Lexicon F1
MCMC 33.95 40.44 20.07 10.37
EM 32.08 39.76 13.31 6.09
VB 39.64 39.22 17.40 12.27
discourse 40.63 42.01 19.31 12.72
</table>
<tableCaption confidence="0.811629">
Table 3: Baseline (non-social) models. Comparison of
sentence-level models (MCMC (Johnson et al., 2012),
EM, VB) and the discourse-level model.
</tableCaption>
<bodyText confidence="0.999344125">
Results in Table 3 suggest that incorporating
topic continuity through the discourse model boosts
performance compared to sentence-level models.
Within sentence-level models, EM is inferior to both
MCMC and VB (in accordance with the consensus
that EM is likely to overfit for PCFGs). Comparing
VB and MCMC, VB is significantly better at topic
accuracy but is worse at topic F1. This result sug-
gests that VB predicts that more utterances are non-
topical compared with MCMC, perhaps explaining
why MCMC has the highest word F1. Nevertheless,
unlike VB, the discourse model outperforms MCMC
in all topic metrics, indicating that topic continuity
helps in predicting both null and topical utterances.
The discourse model is also capable of captur-
ing topical transitions. Examining one instance of
a learned grammar reveals that the distribution un-
der Discourset is often dominated by a few major
transitions. For example, car tends to have transi-
tions into car (0.72) and truck (0.19); while pig
prefers to transit into pig (0.69) and dog (0.24).
These learned transitions nicely recover the struc-
ture of the task that caregivers were given: to play
with toy pairs like car/truck and pig/dog.
</bodyText>
<subsectionHeader confidence="0.938883">
7.2 Social-cue Models
</subsectionHeader>
<bodyText confidence="0.996898766666667">
We next explore how topic continuity interacts with
social information via a set of simulations mirroring
those in the previous section. Results are shown in
Table 2. For the sentence-level models using social
cues, VB now outperforms MCMC in topic accuracy
and F1, as well as lexicon evaluations, suggesting
that VB is overall quite competitive with MCMC.11
Turning to the discourse models, social informa-
tion and topic continuity both independently boost
learning performance (as evidenced in Johnson et al.
(2012) and in Section 7.1). Nevertheless, joint infer-
ence using both information sources (discourse row)
resulted in a performance decrement. Rather than
reflecting issues in the model itself, perhaps the in-
creased complexity of the inference problem might
have led to this performance decrement.
To test this explanation, we initialized our
discourse-level model with the VB sentence-level
model. Results are shown in the discourse+init
row. With a sentence-level initialization, perfor-
mance improved substantially, yielding the best re-
sults over most metrics. In addition, the discourse
model with sentence-level initialization achieved
lower free energy than the standard initialization dis-
course model. Both of these results support the hy-
pothesis that initialization facilitated inference in the
more complex discourse model. From a cognitive
science perspective, this sort of result may point to
the utility of beginning the task of discourse segmen-
tation with some initial sentence-level expectations.
</bodyText>
<subsectionHeader confidence="0.999622">
7.3 Effects of Individual Social Cues
</subsectionHeader>
<bodyText confidence="0.978341833333333">
The importance of particular social cues and their
relationship to discourse continuity is an additional
topic of interest from the cognitive science per-
spective (Frank et al., 2013). Returning to one of
the questions that motivated this work, we can use
&amp;quot;Detailed breakdown of word f-scores reveals that MCMC
is much better at precision, indicating that VB predicts more
words as topical than MCMC. An explanation for such effect
is that we use the same α for all lexical rules, which results in
suboptimal sparsity levels for Wordt distributions. For MCMC,
Johnson et al. (2012) used the adaptor grammar software to
learn the hyperparameters automatically from data.
</bodyText>
<page confidence="0.99352">
322
</page>
<table confidence="0.9989505">
all no.child.eyes no.child.hands no.mom.eyes no.mom.hands no.mom.point
MCMC 49.1/60.6/29.5/14.8 38.4/46.6/21.5/11.1 49.1/60.6/29.6/15.3 48.0/59.7/29.0/15.5 48.7/60.0/29.3/15.6 48.8/60.3/29.3/15.6
VB 53.1/60.9/25.62/16.71 49.3/56.0/22.6/15.1 52.9/60.4/26.2/16.2 51.5/59.1/24.6/16.3 51.9/59.2/25.3/16.3 52.9/60.6/25.5/16.6
discourse+init 55.8/60.9/29.8/21.1 53.7/59.2/27.8/19.7∗+ 55.2/60.7/29.0/21.4+ 54.7/60.0/29.0/21.6 55.2/60.1/29.1/21.4 55.6/60.8/29.5/21.7
</table>
<tableCaption confidence="0.98191975">
Table 4: Social cue influence. Ablation test results across models without discourse (MCMC, VB) and with discourse
(discourse+init). We start with the full set of social cues and drop one at a time. Each cell contains results for metrics:
topic accuracy/topic F1/word F1/lexicon F1. For row discourse+init, we compare models with/without a social cue
using chi-square tests and denote statistically significant results (p &lt; .05) at the utterance (*) and word (+) levels.
</tableCaption>
<table confidence="0.9995">
none child.eyes child.hands mom.eyes mom.hands mom.point
MCMC 34.0/40.4/20.1/10.4 45.7/57.3/28.9/13.6 34.0/40.1/20.1/9.7 33.8/40.2/19.9/9.7 35.6/42.8/19.8/10.0 30.6/35.5/18.1/9.2
VB 39.6/39.2/17.4/12.27 47.2/53.0/21.9/13.9 43.0/45.8/15.4/12.9 42.9/46.5/14.6/12.4 41.1/43.8/17.1/12.4 39.7/39.7/17.5/13.4
discourse 40.7/41.8/19.2/12.1 47.8/55.4/22.8/14.2∗+ 44.6/50.8/20.3/13.1∗+ 44.7/50.1/21.7/14.3∗+ 42.7/46.4/19.0/11.6+ 38.7/40.2/16.6/11.9∗+
</table>
<tableCaption confidence="0.92957275">
Table 5: Social cue influence. Add-one test results across models without discourse (MCMC, VB) and with discourse
(discourse). We start with no social information and add one cue at a time. Each cell contains results for metrics:
topic accuracy/topic F1/word F1/lexicon F1. For row discourse, we compare models with/without a social cue using
chi-square tests and denote statistically significant results (p &lt; .05) at the utterance (*) and word (+) levels.
</tableCaption>
<bodyText confidence="0.963390648648649">
our discourse model to answer the question about
the role that the child.eyes cue plays in child-
directed discourses. Johnson et al. (2012) raised
two hypotheses that could explain the importance of
child.eyes as a social cue: (1) caregivers “fol-
low in” on the child’s gaze: they tend to talk about
what the child is looking at (Baldwin, 1993), or (2)
the child.eyes cue encodes the topic of the pre-
vious sentence, inadvertently giving a non-discourse
model access to rudimentary discourse information.
To address this question, we conduct two tests:
(1) ablation – eliminating each social cue in turn
(e.g. child.eyes), and (2) add-one, using a sin-
gle social cue per turn. Table 4 and 5 show corre-
sponding results for models without discourse (the
MCMC and VB sentence-level models) and with
discourse (discourse+init for the ablation test and
discourse for the add-one test). We observe simi-
lar trends to Johnson et al. (2012): the childs gaze is
the most important cue. Removing it from the full
model with all social cues or adding it to the base
model with no cues both result in the largest perfor-
mance change; in both cases this change is statisti-
cally reliable.12 The large performance differences
for child.eyes are consistent with the hypothe-
sis that caregivers are following in, or discussing the
object that children are interested in – even control-
12It is somewhat surprising when child.eye has much less
influence on VB than on MCMC in the ablation test. Though re-
sults in the add-one test reveal that VB generalizes much better
than MCMC when presented with a single social cue, it remains
interesting to find out internally what causes the difference.
ling for the continuity of discourse, a confound in
previous analyses. In other words, the importance
of child.eyes in the discourse model suggests
that this cue encodes useful information in addition
to the intersentential discourse topic.
</bodyText>
<subsectionHeader confidence="0.998664">
7.4 Discourse Structure Evaluation
</subsectionHeader>
<bodyText confidence="0.99997925">
While the discourse model performs well using met-
rics from previous work, these metrics do not fully
reflect an important strength of the model: its abil-
ity to capture inter-utterance structure. For exam-
</bodyText>
<subsectionHeader confidence="0.975827">
Raw Discourse Utterance
</subsectionHeader>
<bodyText confidence="0.833262">
car car come here lets find the car
car there
car car is that a car
car car the car goes vroom vroom vroom
</bodyText>
<tableCaption confidence="0.941174">
Table 6: Topic annotation examples. raw (previous
metrics) and discourse (new metrics).
</tableCaption>
<bodyText confidence="0.998879384615385">
ple, consider the sequence of utterances in Table 6.
Our previous evaluation is based on the raw annota-
tion, which labels as topical only utterances contain-
ing topical words or pronouns referring to an object.
As a result, classifying “there” as car is incorrect.
From the perspective of a human listener, however,
“there” is part of a broader discourse about the car,
and labeling it with the same topic captures the fact
that it encodes useful information for learners. To
differentiate these cases, Frank and Rohde (under re-
view) added a new set of annotations (to the dataset
used in Section 7) based on the discourse structure
perceived by human, similar to column discourse, .
</bodyText>
<page confidence="0.998282">
323
</page>
<bodyText confidence="0.999340166666667">
We utilize these new annotations to judge topics
predicted by our discourse model and adopt previ-
ous metrics for discourse segmentation evaluation:
a=b, a simple proportion equivalence of discourse
assignments; pk, a window method (Beeferman et
al., 1999) to measure the probability of two random
utterances correctly classified as being in the same
discourse; and WindowDiff (Pevzner and Hearst,
2002), an improved version of pk which gives “par-
tial credit” to boundaries close to the correct ones.
Results in Table 7 demonstrate that our model is
in better agreement with human annotation (model-
human) than the raw annotation (raw-human) across
all metrics. As is visible from the limited change
in the a=b metric, relatively few topic assignments
are altered; yet these alterations create much more
coherent discourses that allow for far better segmen-
tation performance under pk and WindowDiff.
</bodyText>
<table confidence="0.98858125">
raw-human model-human
a=b 63.6 69.3
pk 57.0 83.6
WindowDiff 36.2 61.2
</table>
<tableCaption confidence="0.990047666666667">
Table 7: Discourse evaluation. Single annotator sample,
comparison between topics assigned by the raw annota-
tion, our discourse model, and a human coder.
</tableCaption>
<bodyText confidence="0.998308714285714">
To put an upper bound on possible discourse seg-
mentation results, we further evaluated performance
on a subset of 634 utterances for which multiple an-
notations were collected. Results in Table 8 demon-
strate that our model predicts discourse topics (m-h1,
m-h1) at a level quite close to the level of agreement
between human annotators (column h1-h2).
</bodyText>
<table confidence="0.9979485">
r-h1 r-h2 m-h1 m-h2 h1-h2
a=b 60.1 65.6 70.4 72.4 81.7
pk 50.7 51.8 85.1 84.9 89.7
WindowDiff 29.0 30.1 60.1 66.9 72.7
</table>
<tableCaption confidence="0.888472">
Table 8: Discourse evaluation. Multiple annotator sam-
ple, comparison between raw annotations (r), our model
(m), and two independent human coders (h1, h2).
</tableCaption>
<sectionHeader confidence="0.993652" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99998119047619">
In this paper, we proposed a novel integration of
existing techniques in parsing and grammar induc-
tion to offer a complete solution for simultaneously
modeling grounded language at the sentence and
discourse levels. Specifically, we used the Ear-
ley algorithm to exploit the special structure of our
grammars to achieve approximately linear parsing
time, introduced a rescaling approach to handle very
long input strings, and utilized Variational Bayes for
grammar induction to obtain better solutions than
the Expectation Maximization algorithm.
By transforming a grounded language learning
problem into a grammatical inference task, we used
our parser to study how discourse structure could
facilitate children’s language acquisition. In ad-
dition, we investigate the interaction between dis-
course structure and social cues, both important
and complementary sources of information in lan-
guage learning (Baldwin, 1993; Frank et al., 2013).
We also examined why individual children’s gaze
was an important predictor of reference in previ-
ous work (Johnson et al., 2012). Using ablation
tests, we showed that information provided by the
child’s gaze is still valuable even in the presence of
discourse continuity, supporting the hypothesis that
parents “follow in” on the particular focus of chil-
dren’s attention (Tomasello and Farrar, 1986).
Lastly, we showed that our models can produce
accurate discourse segmentations. Our system’s out-
put is considerably better than the raw topic anno-
tations provided in the previous social cue corpus
(Frank et al., 2013) and is in good agreement with
discourse topics assigned by human annotators in
Frank and Rohde (under review).
In conclusion, although previous work on
grounded language learning has treated individual
utterances as independent entities, we have shown
that the ability to incorporate discourse information
can be quite useful for such problems. Discourse
continuity is an important source of information in
children language acquisition and may be a valuable
part of future grounded language learning systems.
</bodyText>
<sectionHeader confidence="0.99775" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.997653571428571">
We thank the TACL action editor, Mark Steed-
man, and the anonymous reviewers for their valu-
able feedback, as well as Chris Manning for helpful
discussions. This research was supported under the
Australian Research Council’s Discovery Projects
funding scheme (project numbers DP110102506
and DP110102593).
</bodyText>
<page confidence="0.998163">
324
</page>
<sectionHeader confidence="0.994828" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999298981132076">
Alfred V. Aho and Jeffery D. Ullman. 1972. The The-
ory of Parsing, Translation and Compiling; Volume 1:
Parsing. Prentice-Hall, Englewood Cliffs, New Jersey.
Yoav Artzi and Luke S. Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping in-
structions to actions. Transactions of the Association
for Computational Linguistics, 1:49–62.
John Aycock and R. Nigel Horspool. 2002. Practical ear-
ley parsing. The Computer Journal, 45(6):620–630.
Lalit R. Bahl, Frederick Jelinek, and Robert L. Mercer.
1983. A maximum likelihood approach to continu-
ous speech recognition. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 5(2):179 –190.
James K. Baker. 1979. Trainable grammars for speech
recognition. The Journal of the Acoustical Society of
America, 65(S1):S132.
Jason Baldridge and Alex Lascarides. 2005. Proba-
bilistic head-driven parsing for discourse structure. In
CONLL.
Dare A. Baldwin. 1993. Infants’ ability to consult the
speaker for clues to word reference. Journal of Child
Language, 20:395–418.
Regina Barzilay and Mirella Lapata. 2005. Collective
content selection for concept-to-text generation. In
HLT-EMNLP.
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical models for text segmentation. Ma-
chine Learning, 34(1-3):177–210.
Benjamin B¨orschinger, Bevan K. Jones, and Mark John-
son. 2011. Reducing grounded learning tasks to gram-
matical inference. In EMNLP.
S. R. K. Branavan, Harr Chen, Luke S. Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement learning for
mapping instructions to actions. In ACL-IJCNLP.
Malinda Carpenter, Katherine Nagell, Michael
Tomasello, George Butterworth, and Chris Moore.
1998. Social cognition, joint attention, and com-
municative competence from 9 to 15 months of age.
Monographs of the society for research in child
development, 63(4).
David L. Chen and Raymond J. Mooney. 2008. Learning
to sportscast: A test of grounded language acquisition.
In ICML.
David L. Chen, Joohyun Kim, and Raymond J. Mooney.
2010. Training a multilingual sportscaster: Using per-
ceptual context to learn language. Journal ofArtificial
Intelligence Research, 37:397–435.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from the
world’s response. In CoNLL.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 13(2):94–102.
Vanessa Wei Feng and Graeme Hirst. 2012. Text-level
discourse parsing with rich linguistic features. In ACL.
Katherine Forbes, Eleni Miltsakaki, Rashmi Prasad,
Anoop Sarkar, Joshi Aravind, and Bonnie Webber.
2003. D-ltag system: Discourse parsing with a lexical-
ized tree adjoining grammar. Journal of Logic, Lan-
guage and Information, 12:261–279.
Michael C. Frank and Hannah Rohde. under review.
Markers of topical discourse in child-directed speech.
Michael C. Frank, Noah D. Goodman, and Josh B. Tenen-
baum. 2008. A Bayesian framework for cross-
situational word-learning. Advances in Neural Infor-
mation Processing Systems 20.
Michael C. Frank, Joshua B. Tenenbaum, and Anne
Fernald. 2013. Social and discourse contributions
to the determination of reference in cross-situational
word learning. Language Learning and Development,
9(1):1–24.
Jianfeng Gao and Mark Johnson. 2008. A comparison of
Bayesian estimators for unsupervised Hidden Markov
Model POS taggers. In EMNLP.
Dan Goldwasser and Dan Roth. 2011. Learning from
natural instructions. In IJCAI.
Dan Goldwasser, Roi Reichart, James Clarke, and Dan
Roth. 2011. Confidence driven unsupervised semantic
parsing. In ACL.
Peter Gorniak and Deb Roy. 2007. Situated language
understanding as filtering perceived affordances. Cog-
nitive Science, 31(2):197–231.
Hugo Hernault, Helmut Prendinger, David A. duVerle,
and Mitsuru Ishizuk. 2010. HILDA: A discourse
parser using support vector machine classification. Di-
alogue and Discourse, 1(3):1–33.
Jerry R. Hobbs. 1990. Literature and Cognition. CSLI
Lecture Notes 21.
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In ACL.
Frederick Jelinek and John D. Lafferty. 1991. Compu-
tation of the probability of initial substring generation
by stochastic context-free grammars. Computational
Linguistics, 17(3):315–323.
Mark Johnson, Katherine Demuth, and Michael Frank.
2012. Exploiting social information in grounded lan-
guage learning via grammatical reduction. In ACL.
Mark Johnson. 2010. Pcfgs, topic models, adaptor gram-
mars and learning topical collocations and the struc-
ture of proper names. In ACL.
Joohyun Kim and Raymond J. Mooney. 2012. Unsu-
pervised pcfg induction for grounded language learn-
ing with highly ambiguous supervision. In EMNLP-
CoNLL.
Joohyun Kim and Raymond J. Mooney. 2013. Adapting
discriminative reranking to grounded language learn-
ing. In ACL.
</reference>
<page confidence="0.987147">
325
</page>
<reference confidence="0.999855077669903">
Alistair Knott and Ted Sanders. 1997. The classification
of coherence relations and their linguistic markers: An
exploration of two languages. Journal of Pragmatics,
30(2):135–175.
Kenichi Kurihara and Taisuke Sato. 2004. An appli-
cation of the variational bayesian approach to proba-
bilistic contextfree grammars. In IJCNLP Workshop
Beyond Shallow Analyses.
Kenichi Kurihara and Taisuke Sato. 2006. Variational
bayesian grammar induction for natural language. In
ICGI.
Tom Kwiatkowski, Luke S. Zettlemoyer, Sharon Gold-
water, and Mark Steedman. 2010. Inducing proba-
bilistic ccg grammars from logical form with higher-
order unification. In EMNLP.
Alex Lascarides and Nicholas Asher. 1993. Temporal
interpretation, discourse relations, and common sense
entailment. Linguistics and Philosophy, 16(5):437–
493.
Joop M. I. M. Leo. 1991. A general context-free parsing
algorithm running in linear time on every lr(k) gram-
mar without using lookahead. Theoretical Computer
Science, 82(1):165–176.
Roger Levy. 2008. Expectation-based syntactic compre-
hension. Cognition, 106(3):1126–1177.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less supervi-
sion. In ACL-IJCNLP, pages 91–99.
Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional semantics.
In Association for Computational Linguistics (ACL).
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2012. A
pdtb-styled end-to-end discourse parser. Natural Lan-
guage Engineering, FirstView:1–34.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettle-
moyer. 2008. A generative model for parsing natural
language to meaning representations. In EMNLP.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243–281.
Daniel Marcu. 1997. The rhetorical parsing of natural
language texts. In ACL.
Daniel Marcu. 1999. A decision-based approach to
rhetorical parsing. In ACL.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28(1):19–36.
Livia Polanyi, Chris Culy, Martin Van Den Berg,
Gian Lorenzo Thione, and David Ahn. 2004. A rule
based approach to discourse parsing. In SigDIAL.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In EMNLP.
Lawrence R. Rabiner. 1990. A tutorial on hidden
Markov models and selected applications in speech
recognition.
Remko Scha and Livia Polanyi. 1988. An augmented
context free grammar for discourse. In COLING.
Jeffrey M. Siskind. 1996. A computational study
of cross-situational techniques for learning word-to-
meaning mappings. Cognition, 61(1-2):39–91.
Benjamin Snyder and Regina Barzilay. 2007. Database-
text alignment via structured multilabel classification.
In IJCAI.
Radu Soricut and Daniel Marcu. 2003. Sentence level
discourse parsing using syntactic and lexical informa-
tion. In NAACL.
Andreas Stolcke. 1995. An efficient probabilistic
context-free parsing algorithm that computes prefix
probabilities. Computational Linguistics, 21(2):165–
201.
Rajen Subba and Barbara Di Eugenio. 2009. An effec-
tive discourse parser that uses rich linguistic informa-
tion. In NAACL.
Stefanie Tellex, Thomas Kolla, Steven Dickerson,
Matthew R. Walter, Ashis G. Banerjee, Seth Teller,
and Nicholas. Roy. 2011a. Approaching the symbol
grounding problem with probabilistic graphical mod-
els. AI Magazine, 32(4):64–76.
Stefanie Tellex, Thomas Kolla, Steven Dickerson,
Matthew R. Walter, Ashis G. Banerjee, Seth Teller, and
Nicholas Roy. 2011b. Understanding Natural Lan-
guage Commands for Robotic Navigation and Mobile
Manipulation. In AAAI.
Michael Tomasello and Michael Jeffrey Farrar. 1986.
Joint attention and early language. Child development,
pages 1454–1463.
Adam Vogel and Daniel Jurafsky. 2010. Learning to fol-
low navigational directions. In ACL.
Yuk Wah Wong and Raymond J. Mooney. 2007. Learn-
ing synchronous grammars for semantic parsing with
lambda calculus. In ACL.
Chen Yu and Dana H. Ballard. 2004. On the integration
of grounding language and learning objects. In AAAI.
Chen Yu and Dana H Ballard. 2007. A unified model of
early word learning: Integrating statistical and social
cues. Neurocomputing, 70(13):2149–2165.
Luke S. Zettlemoyer and Michael Collins. 2005. Learn-
ing to Map Sentences to Logical Form: Structured
Classification with Probabilistic Categorial Grammars.
In UAI.
Michael Zettlemoyer, Luke S.and Collins. 2007. Online
learning of relaxed CCG grammars for parsing to log-
ical form. In EMNLP-CoNLL.
</reference>
<page confidence="0.999117">
326
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.849291">
<title confidence="0.9887475">Parsing entire discourses as very long strings: Capturing topic continuity in grounded language learning</title>
<author confidence="0.999991">Minh-Thang Luong Michael C Frank Mark Johnson</author>
<affiliation confidence="0.9999305">Department of Computer Science Department of Psychology Department of Computing Stanford University Stanford University Macquarie University</affiliation>
<address confidence="0.901519">Stanford, California Stanford, California Sydney,</address>
<email confidence="0.999281">lmthang@stanford.edumcfrank@stanford.eduMark.Johnson@MQ.edu.au</email>
<abstract confidence="0.998659551724138">Grounded language learning, the task of mapping from natural language to a representation of meaning, has attracted more and more interest in recent years. In most work on this topic, however, utterances in a conversation are treated independently and discourse structure information is largely ignored. In the context of language acquisition, this independence assumption discards cues that are important to the learner, e.g., the fact that consecutive utterances are likely to share the same referent (Frank et al., 2013). The current paper describes an approach to the problem of simultaneously modeling grounded language at the sentence and discourse levels. We combine ideas from parsing and grammar induction to produce a parser that can handle long input strings with thousands of tokens, creating parse trees that represent full discourses. By casting grounded language learning as a grammatical inference task, we use our parser to extend the work of Johnson et al. (2012), investigating the importance of discourse continuity in children’s language acquisition and its interaction with social cues. Our model boosts performance in a language acquisition task and yields good discourse segmentations compared with human annotators.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffery D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation and Compiling; Volume 1: Parsing. Prentice-Hall,</booktitle>
<location>Englewood Cliffs, New Jersey.</location>
<contexts>
<context position="16174" citStr="Aho and Ullman, 1972" startWordPosition="2496" endWordPosition="2499">of input length, rather than exponentially), but as we explain later (Section 5), we can use the ability of the Earley algorithm to compute prefix probabilities (Stolcke, 1995) to rescale the probability of the parse incrementally and avoid floating-point underflows. In the next section, we provide background information on the Earley algorithm for PCFGs, the prefix probability scheme we use, and the insideoutside algorithm in the Earley context. 4 Background 4.1 Earley Algorithm for PCFGs The Earley algorithm was developed by Earley (1970) and known to be efficient for certain kinds of CFGs (Aho and Ullman, 1972). An Earley parser 2The prefix markers # and ## and the topic markers such as “.dog” enable a left-to- right parser to unambiguously identify its location in the input string. 3In order to achieve linear time the parsing chart must have suitable indexing; see Aho and Ullman (1972), Leo (1991) and Aycock and Horspool (2002) for details. 318 constructs left-most derivations of strings, using dotted productions to keep track of partial derivations. Specifically, each state in an Earley parser is represented as [l, r]: X-+α . 0 to indicate that input symbols xl, ... , xr−1 have been processed and </context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Alfred V. Aho and Jeffery D. Ullman. 1972. The Theory of Parsing, Translation and Compiling; Volume 1: Parsing. Prentice-Hall, Englewood Cliffs, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Artzi</author>
<author>Luke S Zettlemoyer</author>
</authors>
<title>Weakly supervised learning of semantic parsers for mapping instructions to actions.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>1--49</pages>
<contexts>
<context position="7441" citStr="Artzi and Zettlemoyer, 2013" startWordPosition="1105" endWordPosition="1109">, 2004; Gorniak and Roy, 2007). A growing body of work in this field employs distinct techniques from a wide variety of perspectives from text-to-record alignment using structured classification (Barzilay and Lapata, 2005; Snyder and Barzilay, 2007), iterative retraining (Chen et al., 2010), and generative models of segmentation and alignment (Liang et al., 2009) to text-to-interaction mapping using reinforcement learning (Branavan et al., 2009; Vogel and Jurafsky, 2010), graphical model semantics representation (Tellex et al., 2011a; Tellex et al., 2011b), and Combinatory Categorial Grammar (Artzi and Zettlemoyer, 2013). A number of systems have also used alternative forms of supervision, including sentences paired with responses (Clarke et al., 2010; Goldwasser and Roth, 2011; Liang et al., 2011) and no supervision (Poon and Domingos, 2009; Gold316 wasser et al., 2011). Recent work has also introduced an alternative approach to grounded learning by reducing it to a grammatical inference problem. B¨orschinger et al. (2011) casted the problem of learning a semantic parser as a PCFG induction task, achieving state-of the art performance in the RoboCup domain. Kim and Mooney (2012) extended the technique to mak</context>
</contexts>
<marker>Artzi, Zettlemoyer, 2013</marker>
<rawString>Yoav Artzi and Luke S. Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics, 1:49–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Aycock</author>
<author>R Nigel Horspool</author>
</authors>
<title>Practical earley parsing.</title>
<date>2002</date>
<journal>The Computer Journal,</journal>
<volume>45</volume>
<issue>6</issue>
<contexts>
<context position="16498" citStr="Aycock and Horspool (2002)" startWordPosition="2552" endWordPosition="2555">ion on the Earley algorithm for PCFGs, the prefix probability scheme we use, and the insideoutside algorithm in the Earley context. 4 Background 4.1 Earley Algorithm for PCFGs The Earley algorithm was developed by Earley (1970) and known to be efficient for certain kinds of CFGs (Aho and Ullman, 1972). An Earley parser 2The prefix markers # and ## and the topic markers such as “.dog” enable a left-to- right parser to unambiguously identify its location in the input string. 3In order to achieve linear time the parsing chart must have suitable indexing; see Aho and Ullman (1972), Leo (1991) and Aycock and Horspool (2002) for details. 318 constructs left-most derivations of strings, using dotted productions to keep track of partial derivations. Specifically, each state in an Earley parser is represented as [l, r]: X-+α . 0 to indicate that input symbols xl, ... , xr−1 have been processed and the parser is expecting to expand 0. States are generated on the fly using three transition operations: predict (add states to charts), scan (shift dots across terminals), and complete (merge two states). Figure 2 shows an example of a completion step which also illustrates the implicit binarization automatically done in E</context>
</contexts>
<marker>Aycock, Horspool, 2002</marker>
<rawString>John Aycock and R. Nigel Horspool. 2002. Practical earley parsing. The Computer Journal, 45(6):620–630.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lalit R Bahl</author>
<author>Frederick Jelinek</author>
<author>Robert L Mercer</author>
</authors>
<title>A maximum likelihood approach to continuous speech recognition.</title>
<date>1983</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>5</volume>
<issue>2</issue>
<pages>190</pages>
<contexts>
<context position="18759" citStr="Bahl et al., 1983" startWordPosition="2925" endWordPosition="2928">f a state [l, r]: X-+α . 0 is the probability of an Earley path through that state, generating input up to position r-1. This probability generalizes a similar concept in HMM and lends itself to the computation of prefix probabilities, sums of forward probabilities over scanned states yielding a prefix x. Computing prefix probabilities is important because it enables probabilistic prediction of possible follow-words xi+1 as P(xi+1|x0 ... xi) _ P(x°...xixi+1) (Jelinek and Lafferty, 1991). These P(xp ... xi) conditional probabilities allow estimation of the incremental costs of a stack decoder (Bahl et al., 1983). In (Huang and Sagae, 2010), a conceptually similar prefix cost is defined to order states in a beam search decoder. Moreover, the negative logarithm of such conditional probabilities are termed as surprisal values in the psycholinguistics literature (e.g., Hale, 2001; Levy, 2008), to describe how difficult a word is in a given context. Interestingly, we show that prefix probabilities lead us to construct a parser that could parse extremely long strings next. 4.3 Inside Outside Algorithm To extend the Inside Outside (IO) algorithm (Baker, 1979) to the Earley context, Stolcke introduced inner </context>
</contexts>
<marker>Bahl, Jelinek, Mercer, 1983</marker>
<rawString>Lalit R. Bahl, Frederick Jelinek, and Robert L. Mercer. 1983. A maximum likelihood approach to continuous speech recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 5(2):179 –190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James K Baker</author>
</authors>
<title>Trainable grammars for speech recognition.</title>
<date>1979</date>
<journal>The Journal of the Acoustical Society of America,</journal>
<pages>65--1</pages>
<contexts>
<context position="19310" citStr="Baker, 1979" startWordPosition="3015" endWordPosition="3016"> the incremental costs of a stack decoder (Bahl et al., 1983). In (Huang and Sagae, 2010), a conceptually similar prefix cost is defined to order states in a beam search decoder. Moreover, the negative logarithm of such conditional probabilities are termed as surprisal values in the psycholinguistics literature (e.g., Hale, 2001; Levy, 2008), to describe how difficult a word is in a given context. Interestingly, we show that prefix probabilities lead us to construct a parser that could parse extremely long strings next. 4.3 Inside Outside Algorithm To extend the Inside Outside (IO) algorithm (Baker, 1979) to the Earley context, Stolcke introduced inner and outer probabilities which generalize the inside and outside probabilities in the IO algorithm. Specifically, the inner probability of a state [l, r]: X-+α . 0 is the probability of generating an input substring xl, ... , xr−1 from a non-terminal X using a production X-+α 0.4 X-+aY.Q X - +a . Y Q Y -+ V. I m r Figure 3: Inner and outer probabilities. The outer probability of X-+α . Y o is a sum of all products of its parent outer probability (X-+αY . o) and its sibling inner probability (Y-+v .). Similarly, the outer probability of Y -+v . is</context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>James K. Baker. 1979. Trainable grammars for speech recognition. The Journal of the Acoustical Society of America, 65(S1):S132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
<author>Alex Lascarides</author>
</authors>
<title>Probabilistic head-driven parsing for discourse structure.</title>
<date>2005</date>
<booktitle>In CONLL.</booktitle>
<contexts>
<context position="9375" citStr="Baldridge and Lascarides, 2005" startWordPosition="1400" endWordPosition="1404">ch utterance independently. Discourse Parsing. A substantial literature has examined formal representations of discourse across a wide variety of theoretical perspectives (Mann and Thompson, 1988; Scha and Polanyi, 1988; Hobbs, 1990; Lascarides and Asher, 1993; Knott and Sanders, 1997). Although much of this work was highly influential, Marcu (1997)’s work on discourse parsing brought this task to special prominence. Since then, more and more sophisticated models of discourse analysis have been developed:, e.g., (Marcu, 1999; Soricut and Marcu, 2003; Forbes et al., 2003; Polanyi et al., 2004; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Lin et al., 2012; Feng and Hirst, 2012). Our contribution to work on this task is to examine latent discourse structure specifically in grounded language learning. 3 A Grounded Learning Task Our focus in this paper is to develop computational models that help us better understand children’s language acquisition. The goal is to learn both the long term lexicon of mappings between words and objects (language learning) as well as the intended topic of individual utterances (language comprehension). We consider a corpus of child-directed speech </context>
</contexts>
<marker>Baldridge, Lascarides, 2005</marker>
<rawString>Jason Baldridge and Alex Lascarides. 2005. Probabilistic head-driven parsing for discourse structure. In CONLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dare A Baldwin</author>
</authors>
<title>Infants’ ability to consult the speaker for clues to word reference.</title>
<date>1993</date>
<journal>Journal of Child Language,</journal>
<pages>20--395</pages>
<contexts>
<context position="34876" citStr="Baldwin, 1993" startWordPosition="5494" endWordPosition="5495">ime. Each cell contains results for metrics: topic accuracy/topic F1/word F1/lexicon F1. For row discourse, we compare models with/without a social cue using chi-square tests and denote statistically significant results (p &lt; .05) at the utterance (*) and word (+) levels. our discourse model to answer the question about the role that the child.eyes cue plays in childdirected discourses. Johnson et al. (2012) raised two hypotheses that could explain the importance of child.eyes as a social cue: (1) caregivers “follow in” on the child’s gaze: they tend to talk about what the child is looking at (Baldwin, 1993), or (2) the child.eyes cue encodes the topic of the previous sentence, inadvertently giving a non-discourse model access to rudimentary discourse information. To address this question, we conduct two tests: (1) ablation – eliminating each social cue in turn (e.g. child.eyes), and (2) add-one, using a single social cue per turn. Table 4 and 5 show corresponding results for models without discourse (the MCMC and VB sentence-level models) and with discourse (discourse+init for the ablation test and discourse for the add-one test). We observe similar trends to Johnson et al. (2012): the childs ga</context>
<context position="40279" citStr="Baldwin, 1993" startWordPosition="6357" endWordPosition="6358">mars to achieve approximately linear parsing time, introduced a rescaling approach to handle very long input strings, and utilized Variational Bayes for grammar induction to obtain better solutions than the Expectation Maximization algorithm. By transforming a grounded language learning problem into a grammatical inference task, we used our parser to study how discourse structure could facilitate children’s language acquisition. In addition, we investigate the interaction between discourse structure and social cues, both important and complementary sources of information in language learning (Baldwin, 1993; Frank et al., 2013). We also examined why individual children’s gaze was an important predictor of reference in previous work (Johnson et al., 2012). Using ablation tests, we showed that information provided by the child’s gaze is still valuable even in the presence of discourse continuity, supporting the hypothesis that parents “follow in” on the particular focus of children’s attention (Tomasello and Farrar, 1986). Lastly, we showed that our models can produce accurate discourse segmentations. Our system’s output is considerably better than the raw topic annotations provided in the previou</context>
</contexts>
<marker>Baldwin, 1993</marker>
<rawString>Dare A. Baldwin. 1993. Infants’ ability to consult the speaker for clues to word reference. Journal of Child Language, 20:395–418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Collective content selection for concept-to-text generation.</title>
<date>2005</date>
<booktitle>In HLT-EMNLP.</booktitle>
<contexts>
<context position="7034" citStr="Barzilay and Lapata, 2005" startWordPosition="1048" endWordPosition="1051">007; Kwiatkowski et al., 2010). These approaches rely on training data of annotated sentence-meaning pairs, however. Such data are costly to obtain and are quite different from the experience of language learners. Grounded Language Learning. In contrast to semantic parsers, grounded language learning systems aim to learn the meanings of words and sentences given an observed world state (Yu and Ballard, 2004; Gorniak and Roy, 2007). A growing body of work in this field employs distinct techniques from a wide variety of perspectives from text-to-record alignment using structured classification (Barzilay and Lapata, 2005; Snyder and Barzilay, 2007), iterative retraining (Chen et al., 2010), and generative models of segmentation and alignment (Liang et al., 2009) to text-to-interaction mapping using reinforcement learning (Branavan et al., 2009; Vogel and Jurafsky, 2010), graphical model semantics representation (Tellex et al., 2011a; Tellex et al., 2011b), and Combinatory Categorial Grammar (Artzi and Zettlemoyer, 2013). A number of systems have also used alternative forms of supervision, including sentences paired with responses (Clarke et al., 2010; Goldwasser and Roth, 2011; Liang et al., 2011) and no supe</context>
</contexts>
<marker>Barzilay, Lapata, 2005</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2005. Collective content selection for concept-to-text generation. In HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Beeferman</author>
<author>Adam Berger</author>
<author>John Lafferty</author>
</authors>
<title>Statistical models for text segmentation.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="37841" citStr="Beeferman et al., 1999" startWordPosition="5979" endWordPosition="5982">re” is part of a broader discourse about the car, and labeling it with the same topic captures the fact that it encodes useful information for learners. To differentiate these cases, Frank and Rohde (under review) added a new set of annotations (to the dataset used in Section 7) based on the discourse structure perceived by human, similar to column discourse, . 323 We utilize these new annotations to judge topics predicted by our discourse model and adopt previous metrics for discourse segmentation evaluation: a=b, a simple proportion equivalence of discourse assignments; pk, a window method (Beeferman et al., 1999) to measure the probability of two random utterances correctly classified as being in the same discourse; and WindowDiff (Pevzner and Hearst, 2002), an improved version of pk which gives “partial credit” to boundaries close to the correct ones. Results in Table 7 demonstrate that our model is in better agreement with human annotation (modelhuman) than the raw annotation (raw-human) across all metrics. As is visible from the limited change in the a=b metric, relatively few topic assignments are altered; yet these alterations create much more coherent discourses that allow for far better segment</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1999</marker>
<rawString>Doug Beeferman, Adam Berger, and John Lafferty. 1999. Statistical models for text segmentation. Machine Learning, 34(1-3):177–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin B¨orschinger</author>
<author>Bevan K Jones</author>
<author>Mark Johnson</author>
</authors>
<title>Reducing grounded learning tasks to grammatical inference.</title>
<date>2011</date>
<booktitle>In EMNLP.</booktitle>
<marker>B¨orschinger, Jones, Johnson, 2011</marker>
<rawString>Benjamin B¨orschinger, Bevan K. Jones, and Mark Johnson. 2011. Reducing grounded learning tasks to grammatical inference. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>Harr Chen</author>
<author>Luke S Zettlemoyer</author>
<author>Regina Barzilay</author>
</authors>
<title>Reinforcement learning for mapping instructions to actions.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP.</booktitle>
<contexts>
<context position="7261" citStr="Branavan et al., 2009" startWordPosition="1079" endWordPosition="1082"> Learning. In contrast to semantic parsers, grounded language learning systems aim to learn the meanings of words and sentences given an observed world state (Yu and Ballard, 2004; Gorniak and Roy, 2007). A growing body of work in this field employs distinct techniques from a wide variety of perspectives from text-to-record alignment using structured classification (Barzilay and Lapata, 2005; Snyder and Barzilay, 2007), iterative retraining (Chen et al., 2010), and generative models of segmentation and alignment (Liang et al., 2009) to text-to-interaction mapping using reinforcement learning (Branavan et al., 2009; Vogel and Jurafsky, 2010), graphical model semantics representation (Tellex et al., 2011a; Tellex et al., 2011b), and Combinatory Categorial Grammar (Artzi and Zettlemoyer, 2013). A number of systems have also used alternative forms of supervision, including sentences paired with responses (Clarke et al., 2010; Goldwasser and Roth, 2011; Liang et al., 2011) and no supervision (Poon and Domingos, 2009; Gold316 wasser et al., 2011). Recent work has also introduced an alternative approach to grounded learning by reducing it to a grammatical inference problem. B¨orschinger et al. (2011) casted t</context>
</contexts>
<marker>Branavan, Chen, Zettlemoyer, Barzilay, 2009</marker>
<rawString>S. R. K. Branavan, Harr Chen, Luke S. Zettlemoyer, and Regina Barzilay. 2009. Reinforcement learning for mapping instructions to actions. In ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malinda Carpenter</author>
<author>Katherine Nagell</author>
<author>Michael Tomasello</author>
<author>George Butterworth</author>
<author>Chris Moore</author>
</authors>
<title>Social cognition, joint attention, and communicative competence from 9 to 15 months of age. Monographs of the society for research in child development,</title>
<date>1998</date>
<pages>63--4</pages>
<contexts>
<context position="10747" citStr="Carpenter et al., 1998" startWordPosition="1614" endWordPosition="1617">transcribed from videos of caregivers playing with pre-linguistic children of various ages (6, 12, and 18 months) during home visits.&apos; Each utterance was hand-annotated with objects present in the (non-linguistic) context, e.g. dog and pig (Figure 1), together with sets of social cues, one set per object. The social cues describe objects the care-giver is looking at (mom.eyes), holding onto (mom.hands), or pointing to (mom.point); similarly, for (child.eyes) and (child.hands). 3.1 Sentence-level Models Motivated by the importance of social information in children’s early language acquisition (Carpenter et al., 1998), Johnson et al. (2012) proposed a joint model of non-linguistic information including the physical context and social cues, and the linguistic content of individual utterances. They framed the joint inference problem of inferring word-object mappings and inferring sentence topics as a grammar induction task where input strings are utterances prefixed with non-linguistic information. Objects present in the non-linguistic context of an utterance are considered its potential topics. There is also a special null topic, None, to indicate non-topical utterances. The goal of the model is then to sel</context>
</contexts>
<marker>Carpenter, Nagell, Tomasello, Butterworth, Moore, 1998</marker>
<rawString>Malinda Carpenter, Katherine Nagell, Michael Tomasello, George Butterworth, and Chris Moore. 1998. Social cognition, joint attention, and communicative competence from 9 to 15 months of age. Monographs of the society for research in child development, 63(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to sportscast: A test of grounded language acquisition.</title>
<date>2008</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="2266" citStr="Chen and Mooney, 2008" startWordPosition="329" endWordPosition="332"> with human annotators. 1 Introduction Learning mappings between natural language (NL) and meaning representations (MR) is an important goal for both computational linguistics and cognitive science. Accurately learning novel mappings is crucial in grounded language understanding tasks and such systems can suggest insights into the nature of children language learning. Two influential examples of grounded language learning tasks are the sportscasting task, RoboCup, where the NL is the set of running commentary and the MR is the set of logical forms representing actions like kicking or passing (Chen and Mooney, 2008), and the cross-situational word-learning task, where the NL is the caregiver’s utterances and the MR is the set of objects present in the context (Siskind, 1996; Yu and Ballard, 2007). Work in these domains suggests that, based on the cooccurrence between words and their referents in context, it is possible to learn mappings between NL and MR even under substantial ambiguity. Nevertheless, contexts like RoboCup—where every single utterance is grounded—are extremely rare. Much more common are cases where a single topic is introduced and then discussed at length throughout a discourse. In a tel</context>
</contexts>
<marker>Chen, Mooney, 2008</marker>
<rawString>David L. Chen and Raymond J. Mooney. 2008. Learning to sportscast: A test of grounded language acquisition. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>Joohyun Kim</author>
<author>Raymond J Mooney</author>
</authors>
<title>Training a multilingual sportscaster: Using perceptual context to learn language.</title>
<date>2010</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<pages>37--397</pages>
<contexts>
<context position="7104" citStr="Chen et al., 2010" startWordPosition="1058" endWordPosition="1061">otated sentence-meaning pairs, however. Such data are costly to obtain and are quite different from the experience of language learners. Grounded Language Learning. In contrast to semantic parsers, grounded language learning systems aim to learn the meanings of words and sentences given an observed world state (Yu and Ballard, 2004; Gorniak and Roy, 2007). A growing body of work in this field employs distinct techniques from a wide variety of perspectives from text-to-record alignment using structured classification (Barzilay and Lapata, 2005; Snyder and Barzilay, 2007), iterative retraining (Chen et al., 2010), and generative models of segmentation and alignment (Liang et al., 2009) to text-to-interaction mapping using reinforcement learning (Branavan et al., 2009; Vogel and Jurafsky, 2010), graphical model semantics representation (Tellex et al., 2011a; Tellex et al., 2011b), and Combinatory Categorial Grammar (Artzi and Zettlemoyer, 2013). A number of systems have also used alternative forms of supervision, including sentences paired with responses (Clarke et al., 2010; Goldwasser and Roth, 2011; Liang et al., 2011) and no supervision (Poon and Domingos, 2009; Gold316 wasser et al., 2011). Recent</context>
</contexts>
<marker>Chen, Kim, Mooney, 2010</marker>
<rawString>David L. Chen, Joohyun Kim, and Raymond J. Mooney. 2010. Training a multilingual sportscaster: Using perceptual context to learn language. Journal ofArtificial Intelligence Research, 37:397–435.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Dan Goldwasser</author>
<author>Ming-Wei Chang</author>
<author>Dan Roth</author>
</authors>
<title>Driving semantic parsing from the world’s response.</title>
<date>2010</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="7574" citStr="Clarke et al., 2010" startWordPosition="1126" endWordPosition="1129">-to-record alignment using structured classification (Barzilay and Lapata, 2005; Snyder and Barzilay, 2007), iterative retraining (Chen et al., 2010), and generative models of segmentation and alignment (Liang et al., 2009) to text-to-interaction mapping using reinforcement learning (Branavan et al., 2009; Vogel and Jurafsky, 2010), graphical model semantics representation (Tellex et al., 2011a; Tellex et al., 2011b), and Combinatory Categorial Grammar (Artzi and Zettlemoyer, 2013). A number of systems have also used alternative forms of supervision, including sentences paired with responses (Clarke et al., 2010; Goldwasser and Roth, 2011; Liang et al., 2011) and no supervision (Poon and Domingos, 2009; Gold316 wasser et al., 2011). Recent work has also introduced an alternative approach to grounded learning by reducing it to a grammatical inference problem. B¨orschinger et al. (2011) casted the problem of learning a semantic parser as a PCFG induction task, achieving state-of the art performance in the RoboCup domain. Kim and Mooney (2012) extended the technique to make it tractable for more complex problems. Later, Kim and Mooney (2013) adapted discriminative reranking to the grounded learning prob</context>
</contexts>
<marker>Clarke, Goldwasser, Chang, Roth, 2010</marker>
<rawString>James Clarke, Dan Goldwasser, Ming-Wei Chang, and Dan Roth. 2010. Driving semantic parsing from the world’s response. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="14995" citStr="Earley, 1970" startWordPosition="2301" endWordPosition="2302"> becomes enormous: the time complexity of CYK is O(n3) for an input of length n. Fortunately, we can take advantage of a special structural property of our grammars. The shape of the parse tree is completely determined by the input string; the only variation is in the topic annotations in the nonterminal labels. So even though the number of possible parses grows exponentially with input length n, the number of possible constituents grows only linearly with input length, and the possible constituents can be identified from the left context.2 These constraints ensure that the Earley algorithm3 (Earley, 1970) will parse an input of length n with this grammar in time O(n). A second challenge in parsing very long strings is that the probability of a parse is the product of the probabilities of the rules involved in its derivation. As the length of a derivation grows linearly with the length of the input, the parse probabilities decrease exponentially as a function of sentence length, causing floating-point underflow on inputs of even moderate length. The standard method for handling this is to compute log probabilities (which decrease linearly as a function of input length, rather than exponentially</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Jay Earley. 1970. An efficient context-free parsing algorithm. Communications of the ACM, 13(2):94–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Wei Feng</author>
<author>Graeme Hirst</author>
</authors>
<title>Text-level discourse parsing with rich linguistic features.</title>
<date>2012</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="9467" citStr="Feng and Hirst, 2012" startWordPosition="1419" endWordPosition="1422">ations of discourse across a wide variety of theoretical perspectives (Mann and Thompson, 1988; Scha and Polanyi, 1988; Hobbs, 1990; Lascarides and Asher, 1993; Knott and Sanders, 1997). Although much of this work was highly influential, Marcu (1997)’s work on discourse parsing brought this task to special prominence. Since then, more and more sophisticated models of discourse analysis have been developed:, e.g., (Marcu, 1999; Soricut and Marcu, 2003; Forbes et al., 2003; Polanyi et al., 2004; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Lin et al., 2012; Feng and Hirst, 2012). Our contribution to work on this task is to examine latent discourse structure specifically in grounded language learning. 3 A Grounded Learning Task Our focus in this paper is to develop computational models that help us better understand children’s language acquisition. The goal is to learn both the long term lexicon of mappings between words and objects (language learning) as well as the intended topic of individual utterances (language comprehension). We consider a corpus of child-directed speech annotated with social cues, described in (Frank et al., 2013). There are a total of 4,763 ut</context>
</contexts>
<marker>Feng, Hirst, 2012</marker>
<rawString>Vanessa Wei Feng and Graeme Hirst. 2012. Text-level discourse parsing with rich linguistic features. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katherine Forbes</author>
<author>Eleni Miltsakaki</author>
<author>Rashmi Prasad</author>
<author>Anoop Sarkar</author>
<author>Joshi Aravind</author>
<author>Bonnie Webber</author>
</authors>
<title>D-ltag system: Discourse parsing with a lexicalized tree adjoining grammar.</title>
<date>2003</date>
<journal>Journal of Logic, Language and Information,</journal>
<pages>12--261</pages>
<contexts>
<context position="9321" citStr="Forbes et al., 2003" startWordPosition="1392" endWordPosition="1395">es among utterances rather than treating each utterance independently. Discourse Parsing. A substantial literature has examined formal representations of discourse across a wide variety of theoretical perspectives (Mann and Thompson, 1988; Scha and Polanyi, 1988; Hobbs, 1990; Lascarides and Asher, 1993; Knott and Sanders, 1997). Although much of this work was highly influential, Marcu (1997)’s work on discourse parsing brought this task to special prominence. Since then, more and more sophisticated models of discourse analysis have been developed:, e.g., (Marcu, 1999; Soricut and Marcu, 2003; Forbes et al., 2003; Polanyi et al., 2004; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Lin et al., 2012; Feng and Hirst, 2012). Our contribution to work on this task is to examine latent discourse structure specifically in grounded language learning. 3 A Grounded Learning Task Our focus in this paper is to develop computational models that help us better understand children’s language acquisition. The goal is to learn both the long term lexicon of mappings between words and objects (language learning) as well as the intended topic of individual utterances (language comprehe</context>
</contexts>
<marker>Forbes, Miltsakaki, Prasad, Sarkar, Aravind, Webber, 2003</marker>
<rawString>Katherine Forbes, Eleni Miltsakaki, Rashmi Prasad, Anoop Sarkar, Joshi Aravind, and Bonnie Webber. 2003. D-ltag system: Discourse parsing with a lexicalized tree adjoining grammar. Journal of Logic, Language and Information, 12:261–279.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Michael C Frank</author>
<author>Hannah Rohde</author>
</authors>
<title>under review. Markers of topical discourse in child-directed speech.</title>
<marker>Frank, Rohde, </marker>
<rawString>Michael C. Frank and Hannah Rohde. under review. Markers of topical discourse in child-directed speech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael C Frank</author>
<author>Noah D Goodman</author>
<author>Josh B Tenenbaum</author>
</authors>
<title>A Bayesian framework for crosssituational word-learning.</title>
<date>2008</date>
<booktitle>Advances in Neural Information Processing Systems 20.</booktitle>
<contexts>
<context position="8377" citStr="Frank et al. (2008)" startWordPosition="1253" endWordPosition="1256">rounded learning by reducing it to a grammatical inference problem. B¨orschinger et al. (2011) casted the problem of learning a semantic parser as a PCFG induction task, achieving state-of the art performance in the RoboCup domain. Kim and Mooney (2012) extended the technique to make it tractable for more complex problems. Later, Kim and Mooney (2013) adapted discriminative reranking to the grounded learning problem using a form of weak supervision. We employ this general grammatical inference approach in the current work. Children Language Acquisition. In the context of language acquisition, Frank et al. (2008) proposed a system that learned words and jointly inferred speakers’ intended referent (utterance topic) using graphical models. Johnson et al. (2012) used grammatical inference to demonstrate the importance of social cues in children’s early word learning. We extend this body of work by capturing discourse-based dependencies among utterances rather than treating each utterance independently. Discourse Parsing. A substantial literature has examined formal representations of discourse across a wide variety of theoretical perspectives (Mann and Thompson, 1988; Scha and Polanyi, 1988; Hobbs, 1990</context>
</contexts>
<marker>Frank, Goodman, Tenenbaum, 2008</marker>
<rawString>Michael C. Frank, Noah D. Goodman, and Josh B. Tenenbaum. 2008. A Bayesian framework for crosssituational word-learning. Advances in Neural Information Processing Systems 20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael C Frank</author>
<author>Joshua B Tenenbaum</author>
<author>Anne Fernald</author>
</authors>
<title>Social and discourse contributions to the determination of reference in cross-situational word learning.</title>
<date>2013</date>
<journal>Language Learning and Development,</journal>
<volume>9</volume>
<issue>1</issue>
<contexts>
<context position="950" citStr="Frank et al., 2013" startWordPosition="128" endWordPosition="131">nia Sydney, Australia lmthang@stanford.edu mcfrank@stanford.edu Mark.Johnson@MQ.edu.au Abstract Grounded language learning, the task of mapping from natural language to a representation of meaning, has attracted more and more interest in recent years. In most work on this topic, however, utterances in a conversation are treated independently and discourse structure information is largely ignored. In the context of language acquisition, this independence assumption discards cues that are important to the learner, e.g., the fact that consecutive utterances are likely to share the same referent (Frank et al., 2013). The current paper describes an approach to the problem of simultaneously modeling grounded language at the sentence and discourse levels. We combine ideas from parsing and grammar induction to produce a parser that can handle long input strings with thousands of tokens, creating parse trees that represent full discourses. By casting grounded language learning as a grammatical inference task, we use our parser to extend the work of Johnson et al. (2012), investigating the importance of discourse continuity in children’s language acquisition and its interaction with social cues. Our model boos</context>
<context position="3426" citStr="Frank et al. (2013)" startWordPosition="516" endWordPosition="519">d then discussed at length throughout a discourse. In a television news show, for example, a topic might be introduced by presenting a relevant picture or video clip. Once the topic is introduced, the anchors can discuss it by name or even using a pronoun without showing a picture. The discourse is grounded without having to ground every utterance. Moreover, although previous work has largely treated utterance order as independent, the order of utterances is critical in grounded discourse contexts: if the order is scrambled, it can become impossible to recover the topic. Supporting this idea, Frank et al. (2013) found that topic continuity—the tendency to talk about the same topic in multiple utterances that are contiguous in time—is both prevalent and informative for word learning. This paper examines the importance of topic continuity through a grammatical inference problem. We build on Johnson et al. (2012)’s work that used grammatical inference to 315 Transactions of the Association for Computational Linguistics, 1 (2013) 315–326. Action Editor: Mark Steedman. Submitted 2/2013; Revised 6/2013; Published 7/2013. c�2013 Association for Computational Linguistics. Sentence Topic.pig Words.pig .dog # </context>
<context position="10036" citStr="Frank et al., 2013" startWordPosition="1508" endWordPosition="1511">al., 2010; Lin et al., 2012; Feng and Hirst, 2012). Our contribution to work on this task is to examine latent discourse structure specifically in grounded language learning. 3 A Grounded Learning Task Our focus in this paper is to develop computational models that help us better understand children’s language acquisition. The goal is to learn both the long term lexicon of mappings between words and objects (language learning) as well as the intended topic of individual utterances (language comprehension). We consider a corpus of child-directed speech annotated with social cues, described in (Frank et al., 2013). There are a total of 4,763 utterances in the corpus, each of which is orthographicallytranscribed from videos of caregivers playing with pre-linguistic children of various ages (6, 12, and 18 months) during home visits.&apos; Each utterance was hand-annotated with objects present in the (non-linguistic) context, e.g. dog and pig (Figure 1), together with sets of social cues, one set per object. The social cues describe objects the care-giver is looking at (mom.eyes), holding onto (mom.hands), or pointing to (mom.point); similarly, for (child.eyes) and (child.hands). 3.1 Sentence-level Models Moti</context>
<context position="13051" citStr="Frank et al., 2013" startWordPosition="1987" endWordPosition="1990">learning the salience of social cues in grounded learning. In the current work, we restrict our attention to only the unigram PCFG model to focus on investigating the role of topic continuity. Unlike the approach of Johnson et al. (2012), which uses Markov Chain Monte Carlo techniques to perform grammatical inference, we experiment with Variational Bayes methods, detailed in Section 6. 3.2 A Discourse-level Model Topic continuity—the tendency to group utterances into coherent discourses about a single topic—may be an important source of information for children learning the meanings of words (Frank et al., 2013). To address this issue, we consider a new discourselevel model of grounded language that captures dependencies between utterances. By linking multiple utterances in a single parse, our proposed grammatical formalism is a bigram Markov process that models transitions among utterance topics. Our grammar starts with a root symbol Discourse, which then selects a starting topic through a set of discourse initial rules, Discourse -+ Discourset for t E V. Each of the Discourset nodes generates an utterance of the same topic, and advances into other topics through transition rules, Discourset -+ Sent</context>
<context position="32214" citStr="Frank et al., 2013" startWordPosition="5155" endWordPosition="5158">evel initialization achieved lower free energy than the standard initialization discourse model. Both of these results support the hypothesis that initialization facilitated inference in the more complex discourse model. From a cognitive science perspective, this sort of result may point to the utility of beginning the task of discourse segmentation with some initial sentence-level expectations. 7.3 Effects of Individual Social Cues The importance of particular social cues and their relationship to discourse continuity is an additional topic of interest from the cognitive science perspective (Frank et al., 2013). Returning to one of the questions that motivated this work, we can use &amp;quot;Detailed breakdown of word f-scores reveals that MCMC is much better at precision, indicating that VB predicts more words as topical than MCMC. An explanation for such effect is that we use the same α for all lexical rules, which results in suboptimal sparsity levels for Wordt distributions. For MCMC, Johnson et al. (2012) used the adaptor grammar software to learn the hyperparameters automatically from data. 322 all no.child.eyes no.child.hands no.mom.eyes no.mom.hands no.mom.point MCMC 49.1/60.6/29.5/14.8 38.4/46.6/21.</context>
<context position="40300" citStr="Frank et al., 2013" startWordPosition="6359" endWordPosition="6362"> approximately linear parsing time, introduced a rescaling approach to handle very long input strings, and utilized Variational Bayes for grammar induction to obtain better solutions than the Expectation Maximization algorithm. By transforming a grounded language learning problem into a grammatical inference task, we used our parser to study how discourse structure could facilitate children’s language acquisition. In addition, we investigate the interaction between discourse structure and social cues, both important and complementary sources of information in language learning (Baldwin, 1993; Frank et al., 2013). We also examined why individual children’s gaze was an important predictor of reference in previous work (Johnson et al., 2012). Using ablation tests, we showed that information provided by the child’s gaze is still valuable even in the presence of discourse continuity, supporting the hypothesis that parents “follow in” on the particular focus of children’s attention (Tomasello and Farrar, 1986). Lastly, we showed that our models can produce accurate discourse segmentations. Our system’s output is considerably better than the raw topic annotations provided in the previous social cue corpus (</context>
</contexts>
<marker>Frank, Tenenbaum, Fernald, 2013</marker>
<rawString>Michael C. Frank, Joshua B. Tenenbaum, and Anne Fernald. 2013. Social and discourse contributions to the determination of reference in cross-situational word learning. Language Learning and Development, 9(1):1–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Mark Johnson</author>
</authors>
<title>A comparison of Bayesian estimators for unsupervised Hidden Markov Model POS taggers.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="24907" citStr="Gao and Johnson, 2008" startWordPosition="3977" endWordPosition="3980">e (IO) algorithm, or equivalently the Expectation Maximization (EM) algorithm, for several reasons: (1) it has been shown to be less likely to cause over-fitting for PCFGs than EM (Kurihara and Sato, 2004) and (2) implementationwise, VB is a straightforward extension from EM as they both share the same process of computing the expected counts (the IO part) and only differ at how rule probabilities are reestimated. At the same time, VB has also been demonstrated to do well on large datasets and is competitive with Gibbs samplers while having the fastest convergence time among these estimators (Gao and Johnson, 2008). The rule reestimation in VB is carried as follows. Let αr be the prior hyperparameter of a rule r in the rule set R and cr be its expected count accumulated over the entire corpus after an IO iteration. The posterior hyperparameter for r is αr = αr + cr. Let ψ be the digamma function, the rule parameter update formula is: θr:X-+λ = exp (αr) − ψ (�r′:X-+λ′ α� )]. r′ Whereas IO minimizes the negative loglikelihood of the observed data (sentences), -log p(x), VB minimizes a quantity called free energy, which we will use later to monitor convergence. Here x denotes the observed data and 0 repres</context>
</contexts>
<marker>Gao, Johnson, 2008</marker>
<rawString>Jianfeng Gao and Mark Johnson. 2008. A comparison of Bayesian estimators for unsupervised Hidden Markov Model POS taggers. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Goldwasser</author>
<author>Dan Roth</author>
</authors>
<title>Learning from natural instructions.</title>
<date>2011</date>
<booktitle>In IJCAI.</booktitle>
<contexts>
<context position="7601" citStr="Goldwasser and Roth, 2011" startWordPosition="1130" endWordPosition="1134">using structured classification (Barzilay and Lapata, 2005; Snyder and Barzilay, 2007), iterative retraining (Chen et al., 2010), and generative models of segmentation and alignment (Liang et al., 2009) to text-to-interaction mapping using reinforcement learning (Branavan et al., 2009; Vogel and Jurafsky, 2010), graphical model semantics representation (Tellex et al., 2011a; Tellex et al., 2011b), and Combinatory Categorial Grammar (Artzi and Zettlemoyer, 2013). A number of systems have also used alternative forms of supervision, including sentences paired with responses (Clarke et al., 2010; Goldwasser and Roth, 2011; Liang et al., 2011) and no supervision (Poon and Domingos, 2009; Gold316 wasser et al., 2011). Recent work has also introduced an alternative approach to grounded learning by reducing it to a grammatical inference problem. B¨orschinger et al. (2011) casted the problem of learning a semantic parser as a PCFG induction task, achieving state-of the art performance in the RoboCup domain. Kim and Mooney (2012) extended the technique to make it tractable for more complex problems. Later, Kim and Mooney (2013) adapted discriminative reranking to the grounded learning problem using a form of weak su</context>
</contexts>
<marker>Goldwasser, Roth, 2011</marker>
<rawString>Dan Goldwasser and Dan Roth. 2011. Learning from natural instructions. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Goldwasser</author>
<author>Roi Reichart</author>
<author>James Clarke</author>
<author>Dan Roth</author>
</authors>
<title>Confidence driven unsupervised semantic parsing.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<marker>Goldwasser, Reichart, Clarke, Roth, 2011</marker>
<rawString>Dan Goldwasser, Roi Reichart, James Clarke, and Dan Roth. 2011. Confidence driven unsupervised semantic parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Gorniak</author>
<author>Deb Roy</author>
</authors>
<title>Situated language understanding as filtering perceived affordances.</title>
<date>2007</date>
<journal>Cognitive Science,</journal>
<volume>31</volume>
<issue>2</issue>
<contexts>
<context position="6843" citStr="Gorniak and Roy, 2007" startWordPosition="1020" endWordPosition="1023">tations of various forms, including meaning hierarchies (Lu et al., 2008) and, most dominantly, A-calculus expressions (Zettlemoyer and Collins, 2005; Zettlemoyer, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010). These approaches rely on training data of annotated sentence-meaning pairs, however. Such data are costly to obtain and are quite different from the experience of language learners. Grounded Language Learning. In contrast to semantic parsers, grounded language learning systems aim to learn the meanings of words and sentences given an observed world state (Yu and Ballard, 2004; Gorniak and Roy, 2007). A growing body of work in this field employs distinct techniques from a wide variety of perspectives from text-to-record alignment using structured classification (Barzilay and Lapata, 2005; Snyder and Barzilay, 2007), iterative retraining (Chen et al., 2010), and generative models of segmentation and alignment (Liang et al., 2009) to text-to-interaction mapping using reinforcement learning (Branavan et al., 2009; Vogel and Jurafsky, 2010), graphical model semantics representation (Tellex et al., 2011a; Tellex et al., 2011b), and Combinatory Categorial Grammar (Artzi and Zettlemoyer, 2013). </context>
</contexts>
<marker>Gorniak, Roy, 2007</marker>
<rawString>Peter Gorniak and Deb Roy. 2007. Situated language understanding as filtering perceived affordances. Cognitive Science, 31(2):197–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Hernault</author>
<author>Helmut Prendinger</author>
<author>David A duVerle</author>
<author>Mitsuru Ishizuk</author>
</authors>
<title>HILDA: A discourse parser using support vector machine classification. Dialogue and Discourse,</title>
<date>2010</date>
<contexts>
<context position="9426" citStr="Hernault et al., 2010" startWordPosition="1410" endWordPosition="1414"> literature has examined formal representations of discourse across a wide variety of theoretical perspectives (Mann and Thompson, 1988; Scha and Polanyi, 1988; Hobbs, 1990; Lascarides and Asher, 1993; Knott and Sanders, 1997). Although much of this work was highly influential, Marcu (1997)’s work on discourse parsing brought this task to special prominence. Since then, more and more sophisticated models of discourse analysis have been developed:, e.g., (Marcu, 1999; Soricut and Marcu, 2003; Forbes et al., 2003; Polanyi et al., 2004; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Lin et al., 2012; Feng and Hirst, 2012). Our contribution to work on this task is to examine latent discourse structure specifically in grounded language learning. 3 A Grounded Learning Task Our focus in this paper is to develop computational models that help us better understand children’s language acquisition. The goal is to learn both the long term lexicon of mappings between words and objects (language learning) as well as the intended topic of individual utterances (language comprehension). We consider a corpus of child-directed speech annotated with social cues, described in (Frank et </context>
</contexts>
<marker>Hernault, Prendinger, duVerle, Ishizuk, 2010</marker>
<rawString>Hugo Hernault, Helmut Prendinger, David A. duVerle, and Mitsuru Ishizuk. 2010. HILDA: A discourse parser using support vector machine classification. Dialogue and Discourse, 1(3):1–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<title>Literature and Cognition.</title>
<date>1990</date>
<journal>CSLI Lecture Notes</journal>
<volume>21</volume>
<contexts>
<context position="8977" citStr="Hobbs, 1990" startWordPosition="1340" endWordPosition="1341"> al. (2008) proposed a system that learned words and jointly inferred speakers’ intended referent (utterance topic) using graphical models. Johnson et al. (2012) used grammatical inference to demonstrate the importance of social cues in children’s early word learning. We extend this body of work by capturing discourse-based dependencies among utterances rather than treating each utterance independently. Discourse Parsing. A substantial literature has examined formal representations of discourse across a wide variety of theoretical perspectives (Mann and Thompson, 1988; Scha and Polanyi, 1988; Hobbs, 1990; Lascarides and Asher, 1993; Knott and Sanders, 1997). Although much of this work was highly influential, Marcu (1997)’s work on discourse parsing brought this task to special prominence. Since then, more and more sophisticated models of discourse analysis have been developed:, e.g., (Marcu, 1999; Soricut and Marcu, 2003; Forbes et al., 2003; Polanyi et al., 2004; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Lin et al., 2012; Feng and Hirst, 2012). Our contribution to work on this task is to examine latent discourse structure specifically in grounded lang</context>
</contexts>
<marker>Hobbs, 1990</marker>
<rawString>Jerry R. Hobbs. 1990. Literature and Cognition. CSLI Lecture Notes 21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kenji Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="18787" citStr="Huang and Sagae, 2010" startWordPosition="2930" endWordPosition="2933"> 0 is the probability of an Earley path through that state, generating input up to position r-1. This probability generalizes a similar concept in HMM and lends itself to the computation of prefix probabilities, sums of forward probabilities over scanned states yielding a prefix x. Computing prefix probabilities is important because it enables probabilistic prediction of possible follow-words xi+1 as P(xi+1|x0 ... xi) _ P(x°...xixi+1) (Jelinek and Lafferty, 1991). These P(xp ... xi) conditional probabilities allow estimation of the incremental costs of a stack decoder (Bahl et al., 1983). In (Huang and Sagae, 2010), a conceptually similar prefix cost is defined to order states in a beam search decoder. Moreover, the negative logarithm of such conditional probabilities are termed as surprisal values in the psycholinguistics literature (e.g., Hale, 2001; Levy, 2008), to describe how difficult a word is in a given context. Interestingly, we show that prefix probabilities lead us to construct a parser that could parse extremely long strings next. 4.3 Inside Outside Algorithm To extend the Inside Outside (IO) algorithm (Baker, 1979) to the Earley context, Stolcke introduced inner and outer probabilities whic</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>John D Lafferty</author>
</authors>
<title>Computation of the probability of initial substring generation by stochastic context-free grammars.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>3</issue>
<contexts>
<context position="18632" citStr="Jelinek and Lafferty, 1991" startWordPosition="2904" endWordPosition="2907">s new states are spawned by the three transition operations. 4.2 Forward and Prefix Probabilities Intuitively, the forward probability of a state [l, r]: X-+α . 0 is the probability of an Earley path through that state, generating input up to position r-1. This probability generalizes a similar concept in HMM and lends itself to the computation of prefix probabilities, sums of forward probabilities over scanned states yielding a prefix x. Computing prefix probabilities is important because it enables probabilistic prediction of possible follow-words xi+1 as P(xi+1|x0 ... xi) _ P(x°...xixi+1) (Jelinek and Lafferty, 1991). These P(xp ... xi) conditional probabilities allow estimation of the incremental costs of a stack decoder (Bahl et al., 1983). In (Huang and Sagae, 2010), a conceptually similar prefix cost is defined to order states in a beam search decoder. Moreover, the negative logarithm of such conditional probabilities are termed as surprisal values in the psycholinguistics literature (e.g., Hale, 2001; Levy, 2008), to describe how difficult a word is in a given context. Interestingly, we show that prefix probabilities lead us to construct a parser that could parse extremely long strings next. 4.3 Insi</context>
</contexts>
<marker>Jelinek, Lafferty, 1991</marker>
<rawString>Frederick Jelinek and John D. Lafferty. 1991. Computation of the probability of initial substring generation by stochastic context-free grammars. Computational Linguistics, 17(3):315–323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Katherine Demuth</author>
<author>Michael Frank</author>
</authors>
<title>Exploiting social information in grounded language learning via grammatical reduction.</title>
<date>2012</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1408" citStr="Johnson et al. (2012)" startWordPosition="204" endWordPosition="207">e assumption discards cues that are important to the learner, e.g., the fact that consecutive utterances are likely to share the same referent (Frank et al., 2013). The current paper describes an approach to the problem of simultaneously modeling grounded language at the sentence and discourse levels. We combine ideas from parsing and grammar induction to produce a parser that can handle long input strings with thousands of tokens, creating parse trees that represent full discourses. By casting grounded language learning as a grammatical inference task, we use our parser to extend the work of Johnson et al. (2012), investigating the importance of discourse continuity in children’s language acquisition and its interaction with social cues. Our model boosts performance in a language acquisition task and yields good discourse segmentations compared with human annotators. 1 Introduction Learning mappings between natural language (NL) and meaning representations (MR) is an important goal for both computational linguistics and cognitive science. Accurately learning novel mappings is crucial in grounded language understanding tasks and such systems can suggest insights into the nature of children language lea</context>
<context position="3730" citStr="Johnson et al. (2012)" startWordPosition="563" endWordPosition="566">ounded without having to ground every utterance. Moreover, although previous work has largely treated utterance order as independent, the order of utterances is critical in grounded discourse contexts: if the order is scrambled, it can become impossible to recover the topic. Supporting this idea, Frank et al. (2013) found that topic continuity—the tendency to talk about the same topic in multiple utterances that are contiguous in time—is both prevalent and informative for word learning. This paper examines the importance of topic continuity through a grammatical inference problem. We build on Johnson et al. (2012)’s work that used grammatical inference to 315 Transactions of the Association for Computational Linguistics, 1 (2013) 315–326. Action Editor: Mark Steedman. Submitted 2/2013; Revised 6/2013; Published 7/2013. c�2013 Association for Computational Linguistics. Sentence Topic.pig Words.pig .dog # .pig child.eyes mom.hands # ## wheres the piggie Word.None Words.pig T.None Topic.pig NotTopical.child.eyes NotTopical.child.hands NotTopical.mom.eyes NotTopical.mom.hands NotTopical.mom.point T.pig Topical.child.eyes Topical.child.hands Topical.mom.eyes Topical.mom.hands Topical.mom.point Words.pig Wor</context>
<context position="5733" citStr="Johnson et al., 2012" startWordPosition="854" endWordPosition="857">use the Earley algorithm to exploit the special structure of our grammars, which are deterministic or have at most bounded ambiguity, to achieve approximately linear parsing time; (2) suggest a rescaling approach that enables us to build a PCFG parser capable of handling very long strings with thousands of tokens; and (3) employ Variational Bayes for grammatical inference to obtain better grammars than those given by the EM algorithm. By parsing entire discourses at once, we shed light on a scientifically interesting question about why the child’s own gaze is a positive cue for word learning (Johnson et al., 2012). Our data provide support for the hypothesis (from previous work) that caregivers “follow in”: they name objects that the child is already looking at (Tomasello and Farrar, 1986). In addition, our discourse model produces a performance improvement in a language acquisition task and yields good discourse segmentations compared with human annotators. 2 Related Work Supervised semantic parsers. Previous work has developed supervised semantic parsers to map sentences to meaning representations of various forms, including meaning hierarchies (Lu et al., 2008) and, most dominantly, A-calculus expre</context>
<context position="8527" citStr="Johnson et al. (2012)" startWordPosition="1274" endWordPosition="1277"> PCFG induction task, achieving state-of the art performance in the RoboCup domain. Kim and Mooney (2012) extended the technique to make it tractable for more complex problems. Later, Kim and Mooney (2013) adapted discriminative reranking to the grounded learning problem using a form of weak supervision. We employ this general grammatical inference approach in the current work. Children Language Acquisition. In the context of language acquisition, Frank et al. (2008) proposed a system that learned words and jointly inferred speakers’ intended referent (utterance topic) using graphical models. Johnson et al. (2012) used grammatical inference to demonstrate the importance of social cues in children’s early word learning. We extend this body of work by capturing discourse-based dependencies among utterances rather than treating each utterance independently. Discourse Parsing. A substantial literature has examined formal representations of discourse across a wide variety of theoretical perspectives (Mann and Thompson, 1988; Scha and Polanyi, 1988; Hobbs, 1990; Lascarides and Asher, 1993; Knott and Sanders, 1997). Although much of this work was highly influential, Marcu (1997)’s work on discourse parsing br</context>
<context position="10770" citStr="Johnson et al. (2012)" startWordPosition="1618" endWordPosition="1621">f caregivers playing with pre-linguistic children of various ages (6, 12, and 18 months) during home visits.&apos; Each utterance was hand-annotated with objects present in the (non-linguistic) context, e.g. dog and pig (Figure 1), together with sets of social cues, one set per object. The social cues describe objects the care-giver is looking at (mom.eyes), holding onto (mom.hands), or pointing to (mom.point); similarly, for (child.eyes) and (child.hands). 3.1 Sentence-level Models Motivated by the importance of social information in children’s early language acquisition (Carpenter et al., 1998), Johnson et al. (2012) proposed a joint model of non-linguistic information including the physical context and social cues, and the linguistic content of individual utterances. They framed the joint inference problem of inferring word-object mappings and inferring sentence topics as a grammar induction task where input strings are utterances prefixed with non-linguistic information. Objects present in the non-linguistic context of an utterance are considered its potential topics. There is also a special null topic, None, to indicate non-topical utterances. The goal of the model is then to select the most probable t</context>
<context position="12314" citStr="Johnson et al. (2012)" startWordPosition="1870" endWordPosition="1873">s (Wordst or Collocst). Each word in the utterance is drawn from either a topicspecific distribution Wordt or a general “null” distribution WordNone. As illustrated in Figure 1, the selected topic, pig, is propagated down to the input string through two paths: (a) through topical nodes until an object is &apos;Caregivers were given pairs of toys to play with, e.g. a stuffed dog and pig, or a wooden car and truck. 317 reached, in this case the .pig object, and (b) through lexical nodes to topical word tokens, e.g. piggie. Social cues are then generated by a series of binary decisions as detailed in Johnson et al. (2012). The key feature of these grammars is that parameter inference corresponds both to learning word-topic relations and learning the salience of social cues in grounded learning. In the current work, we restrict our attention to only the unigram PCFG model to focus on investigating the role of topic continuity. Unlike the approach of Johnson et al. (2012), which uses Markov Chain Monte Carlo techniques to perform grammatical inference, we experiment with Variational Bayes methods, detailed in Section 6. 3.2 A Discourse-level Model Topic continuity—the tendency to group utterances into coherent d</context>
<context position="27222" citStr="Johnson et al., 2012" startWordPosition="4381" endWordPosition="4384">speech described in Section 3. Each model is evaluated on (a) topic accuracy—how many utterances are labeled with correct topics (including the null), (b) topic metrics (f-scores/precision/recall)— how well the model predicts non-null topical utterances, (c) word metrics—how well the model predicts topical words,9 and (d) lexicon metrics—how well word types are assigned to the topic that they attach to most frequently. For example, in Figure 1, the model assigns topic pig to the entire utterance. At the word level, it labels piggie with topic pig and assigns null topic to wheres and the. See (Johnson et al., 2012) for more details of these metrics. In Section 7.1, we examine baseline models that do not make use of social cues (mother and child’s eye-gaze and hand position) to discover the topic; these baselines are contrasted with a range of social cues (§7.2 and §7.3). In Section 7.4, we evaluate the discourse structures discovered by our models. 7.1 Baseline Models (No Social Cues) To create baselines for later experiments, we evaluate our models without social information. We compare sentence-level models using three different inference procedures—Markov Chain Monte Carlo (MCMC) (Johnson et al., 201</context>
<context position="29223" citStr="Johnson et al., 2012" startWordPosition="4697" endWordPosition="4700">9 discourse 51.02 59.40 48.60 76.35 23.86 13.82 87.33 15.05 8.27 83.33 150023 discourse+init 55.78 60.91 52.15 73.22 29.75 17.91 87.65 21.11 11.95 90.48 149458 Table 2: Social-cue models. Comparison of sentence- and discourse-level models (init: initialized from the VB sentence-level model) over full metrics. Free energies are shown to compare VB-based models. Model Acc. Topic F1 Word F1 Lexicon F1 MCMC 33.95 40.44 20.07 10.37 EM 32.08 39.76 13.31 6.09 VB 39.64 39.22 17.40 12.27 discourse 40.63 42.01 19.31 12.72 Table 3: Baseline (non-social) models. Comparison of sentence-level models (MCMC (Johnson et al., 2012), EM, VB) and the discourse-level model. Results in Table 3 suggest that incorporating topic continuity through the discourse model boosts performance compared to sentence-level models. Within sentence-level models, EM is inferior to both MCMC and VB (in accordance with the consensus that EM is likely to overfit for PCFGs). Comparing VB and MCMC, VB is significantly better at topic accuracy but is worse at topic F1. This result suggests that VB predicts that more utterances are nontopical compared with MCMC, perhaps explaining why MCMC has the highest word F1. Nevertheless, unlike VB, the disc</context>
<context position="30991" citStr="Johnson et al. (2012)" startWordPosition="4977" endWordPosition="4980">ask that caregivers were given: to play with toy pairs like car/truck and pig/dog. 7.2 Social-cue Models We next explore how topic continuity interacts with social information via a set of simulations mirroring those in the previous section. Results are shown in Table 2. For the sentence-level models using social cues, VB now outperforms MCMC in topic accuracy and F1, as well as lexicon evaluations, suggesting that VB is overall quite competitive with MCMC.11 Turning to the discourse models, social information and topic continuity both independently boost learning performance (as evidenced in Johnson et al. (2012) and in Section 7.1). Nevertheless, joint inference using both information sources (discourse row) resulted in a performance decrement. Rather than reflecting issues in the model itself, perhaps the increased complexity of the inference problem might have led to this performance decrement. To test this explanation, we initialized our discourse-level model with the VB sentence-level model. Results are shown in the discourse+init row. With a sentence-level initialization, performance improved substantially, yielding the best results over most metrics. In addition, the discourse model with senten</context>
<context position="32612" citStr="Johnson et al. (2012)" startWordPosition="5222" endWordPosition="5225">s. 7.3 Effects of Individual Social Cues The importance of particular social cues and their relationship to discourse continuity is an additional topic of interest from the cognitive science perspective (Frank et al., 2013). Returning to one of the questions that motivated this work, we can use &amp;quot;Detailed breakdown of word f-scores reveals that MCMC is much better at precision, indicating that VB predicts more words as topical than MCMC. An explanation for such effect is that we use the same α for all lexical rules, which results in suboptimal sparsity levels for Wordt distributions. For MCMC, Johnson et al. (2012) used the adaptor grammar software to learn the hyperparameters automatically from data. 322 all no.child.eyes no.child.hands no.mom.eyes no.mom.hands no.mom.point MCMC 49.1/60.6/29.5/14.8 38.4/46.6/21.5/11.1 49.1/60.6/29.6/15.3 48.0/59.7/29.0/15.5 48.7/60.0/29.3/15.6 48.8/60.3/29.3/15.6 VB 53.1/60.9/25.62/16.71 49.3/56.0/22.6/15.1 52.9/60.4/26.2/16.2 51.5/59.1/24.6/16.3 51.9/59.2/25.3/16.3 52.9/60.6/25.5/16.6 discourse+init 55.8/60.9/29.8/21.1 53.7/59.2/27.8/19.7∗+ 55.2/60.7/29.0/21.4+ 54.7/60.0/29.0/21.6 55.2/60.1/29.1/21.4 55.6/60.8/29.5/21.7 Table 4: Social cue influence. Ablation test res</context>
</contexts>
<marker>Johnson, Demuth, Frank, 2012</marker>
<rawString>Mark Johnson, Katherine Demuth, and Michael Frank. 2012. Exploiting social information in grounded language learning via grammatical reduction. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Pcfgs, topic models, adaptor grammars and learning topical collocations and the structure of proper names.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<marker>Johnson, 2010</marker>
<rawString>Mark Johnson. 2010. Pcfgs, topic models, adaptor grammars and learning topical collocations and the structure of proper names. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joohyun Kim</author>
<author>Raymond J Mooney</author>
</authors>
<title>Unsupervised pcfg induction for grounded language learning with highly ambiguous supervision.</title>
<date>2012</date>
<booktitle>In EMNLPCoNLL.</booktitle>
<contexts>
<context position="8011" citStr="Kim and Mooney (2012)" startWordPosition="1197" endWordPosition="1200">ry Categorial Grammar (Artzi and Zettlemoyer, 2013). A number of systems have also used alternative forms of supervision, including sentences paired with responses (Clarke et al., 2010; Goldwasser and Roth, 2011; Liang et al., 2011) and no supervision (Poon and Domingos, 2009; Gold316 wasser et al., 2011). Recent work has also introduced an alternative approach to grounded learning by reducing it to a grammatical inference problem. B¨orschinger et al. (2011) casted the problem of learning a semantic parser as a PCFG induction task, achieving state-of the art performance in the RoboCup domain. Kim and Mooney (2012) extended the technique to make it tractable for more complex problems. Later, Kim and Mooney (2013) adapted discriminative reranking to the grounded learning problem using a form of weak supervision. We employ this general grammatical inference approach in the current work. Children Language Acquisition. In the context of language acquisition, Frank et al. (2008) proposed a system that learned words and jointly inferred speakers’ intended referent (utterance topic) using graphical models. Johnson et al. (2012) used grammatical inference to demonstrate the importance of social cues in children</context>
</contexts>
<marker>Kim, Mooney, 2012</marker>
<rawString>Joohyun Kim and Raymond J. Mooney. 2012. Unsupervised pcfg induction for grounded language learning with highly ambiguous supervision. In EMNLPCoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joohyun Kim</author>
<author>Raymond J Mooney</author>
</authors>
<title>Adapting discriminative reranking to grounded language learning.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="8111" citStr="Kim and Mooney (2013)" startWordPosition="1213" endWordPosition="1216">forms of supervision, including sentences paired with responses (Clarke et al., 2010; Goldwasser and Roth, 2011; Liang et al., 2011) and no supervision (Poon and Domingos, 2009; Gold316 wasser et al., 2011). Recent work has also introduced an alternative approach to grounded learning by reducing it to a grammatical inference problem. B¨orschinger et al. (2011) casted the problem of learning a semantic parser as a PCFG induction task, achieving state-of the art performance in the RoboCup domain. Kim and Mooney (2012) extended the technique to make it tractable for more complex problems. Later, Kim and Mooney (2013) adapted discriminative reranking to the grounded learning problem using a form of weak supervision. We employ this general grammatical inference approach in the current work. Children Language Acquisition. In the context of language acquisition, Frank et al. (2008) proposed a system that learned words and jointly inferred speakers’ intended referent (utterance topic) using graphical models. Johnson et al. (2012) used grammatical inference to demonstrate the importance of social cues in children’s early word learning. We extend this body of work by capturing discourse-based dependencies among </context>
</contexts>
<marker>Kim, Mooney, 2013</marker>
<rawString>Joohyun Kim and Raymond J. Mooney. 2013. Adapting discriminative reranking to grounded language learning. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Knott</author>
<author>Ted Sanders</author>
</authors>
<title>The classification of coherence relations and their linguistic markers: An exploration of two languages.</title>
<date>1997</date>
<journal>Journal of Pragmatics,</journal>
<volume>30</volume>
<issue>2</issue>
<contexts>
<context position="9031" citStr="Knott and Sanders, 1997" startWordPosition="1346" endWordPosition="1349">d words and jointly inferred speakers’ intended referent (utterance topic) using graphical models. Johnson et al. (2012) used grammatical inference to demonstrate the importance of social cues in children’s early word learning. We extend this body of work by capturing discourse-based dependencies among utterances rather than treating each utterance independently. Discourse Parsing. A substantial literature has examined formal representations of discourse across a wide variety of theoretical perspectives (Mann and Thompson, 1988; Scha and Polanyi, 1988; Hobbs, 1990; Lascarides and Asher, 1993; Knott and Sanders, 1997). Although much of this work was highly influential, Marcu (1997)’s work on discourse parsing brought this task to special prominence. Since then, more and more sophisticated models of discourse analysis have been developed:, e.g., (Marcu, 1999; Soricut and Marcu, 2003; Forbes et al., 2003; Polanyi et al., 2004; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Lin et al., 2012; Feng and Hirst, 2012). Our contribution to work on this task is to examine latent discourse structure specifically in grounded language learning. 3 A Grounded Learning Task Our focus in</context>
</contexts>
<marker>Knott, Sanders, 1997</marker>
<rawString>Alistair Knott and Ted Sanders. 1997. The classification of coherence relations and their linguistic markers: An exploration of two languages. Journal of Pragmatics, 30(2):135–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenichi Kurihara</author>
<author>Taisuke Sato</author>
</authors>
<title>An application of the variational bayesian approach to probabilistic contextfree grammars.</title>
<date>2004</date>
<booktitle>In IJCNLP Workshop Beyond Shallow Analyses.</booktitle>
<contexts>
<context position="24490" citStr="Kurihara and Sato, 2004" startWordPosition="3908" endWordPosition="3911">r analysis in that the special structure of our grammars yields approximately linear parsing time in the input length (see §3.3). 7MLE estimated from the English Penn Treebank. 5.2 Parsing Time on Sparse Grammars 7 6 5 4 3 2 seconds 1 200 400 600 800 1000 1200 1400 1600 1800 2000 320 6 Grammar Induction We employ a Variational Bayes (VB) approach to perform grammatical inference instead of the standard Inside Outside (IO) algorithm, or equivalently the Expectation Maximization (EM) algorithm, for several reasons: (1) it has been shown to be less likely to cause over-fitting for PCFGs than EM (Kurihara and Sato, 2004) and (2) implementationwise, VB is a straightforward extension from EM as they both share the same process of computing the expected counts (the IO part) and only differ at how rule probabilities are reestimated. At the same time, VB has also been demonstrated to do well on large datasets and is competitive with Gibbs samplers while having the fastest convergence time among these estimators (Gao and Johnson, 2008). The rule reestimation in VB is carried as follows. Let αr be the prior hyperparameter of a rule r in the rule set R and cr be its expected count accumulated over the entire corpus a</context>
</contexts>
<marker>Kurihara, Sato, 2004</marker>
<rawString>Kenichi Kurihara and Taisuke Sato. 2004. An application of the variational bayesian approach to probabilistic contextfree grammars. In IJCNLP Workshop Beyond Shallow Analyses.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenichi Kurihara</author>
<author>Taisuke Sato</author>
</authors>
<title>Variational bayesian grammar induction for natural language. In ICGI.</title>
<date>2006</date>
<contexts>
<context position="25595" citStr="Kurihara and Sato, 2006" startWordPosition="4100" endWordPosition="4103">he prior hyperparameter of a rule r in the rule set R and cr be its expected count accumulated over the entire corpus after an IO iteration. The posterior hyperparameter for r is αr = αr + cr. Let ψ be the digamma function, the rule parameter update formula is: θr:X-+λ = exp (αr) − ψ (�r′:X-+λ′ α� )]. r′ Whereas IO minimizes the negative loglikelihood of the observed data (sentences), -log p(x), VB minimizes a quantity called free energy, which we will use later to monitor convergence. Here x denotes the observed data and 0 represents the model parameters (PCFG rule probabilities). Following (Kurihara and Sato, 2006), we compute the free energy as: � lo r (Er:X�λ αr) F(x, 0) = −log p(x) + g Γ (�r:X�λ αr) XEN −1: (log T �arj + cr log θr rER r where Γ denotes the gamma function. 6.1 Sparse Dirichlet Priors In our application, since each topic should only be associated with a few words rather than the entire vocabulary, we impose sparse Dirichlet priors over the Wordt distributions by setting a symmetric prior α&lt;1 for all rules Wordt-+w (bt E T, w E W), where W is the set of all words in the corpus. This biases the model to select only a few rules per nonterminal Wordt.8 For all other rules, a uniform hyperp</context>
</contexts>
<marker>Kurihara, Sato, 2006</marker>
<rawString>Kenichi Kurihara and Taisuke Sato. 2006. Variational bayesian grammar induction for natural language. In ICGI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Luke S Zettlemoyer</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Inducing probabilistic ccg grammars from logical form with higherorder unification.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="6439" citStr="Kwiatkowski et al., 2010" startWordPosition="957" endWordPosition="960">s “follow in”: they name objects that the child is already looking at (Tomasello and Farrar, 1986). In addition, our discourse model produces a performance improvement in a language acquisition task and yields good discourse segmentations compared with human annotators. 2 Related Work Supervised semantic parsers. Previous work has developed supervised semantic parsers to map sentences to meaning representations of various forms, including meaning hierarchies (Lu et al., 2008) and, most dominantly, A-calculus expressions (Zettlemoyer and Collins, 2005; Zettlemoyer, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010). These approaches rely on training data of annotated sentence-meaning pairs, however. Such data are costly to obtain and are quite different from the experience of language learners. Grounded Language Learning. In contrast to semantic parsers, grounded language learning systems aim to learn the meanings of words and sentences given an observed world state (Yu and Ballard, 2004; Gorniak and Roy, 2007). A growing body of work in this field employs distinct techniques from a wide variety of perspectives from text-to-record alignment using structured classification (Barzilay and Lapata, 2005; Sny</context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2010</marker>
<rawString>Tom Kwiatkowski, Luke S. Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2010. Inducing probabilistic ccg grammars from logical form with higherorder unification. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Lascarides</author>
<author>Nicholas Asher</author>
</authors>
<title>Temporal interpretation, discourse relations, and common sense entailment.</title>
<date>1993</date>
<journal>Linguistics and Philosophy,</journal>
<volume>16</volume>
<issue>5</issue>
<pages>493</pages>
<contexts>
<context position="9005" citStr="Lascarides and Asher, 1993" startWordPosition="1342" endWordPosition="1345">roposed a system that learned words and jointly inferred speakers’ intended referent (utterance topic) using graphical models. Johnson et al. (2012) used grammatical inference to demonstrate the importance of social cues in children’s early word learning. We extend this body of work by capturing discourse-based dependencies among utterances rather than treating each utterance independently. Discourse Parsing. A substantial literature has examined formal representations of discourse across a wide variety of theoretical perspectives (Mann and Thompson, 1988; Scha and Polanyi, 1988; Hobbs, 1990; Lascarides and Asher, 1993; Knott and Sanders, 1997). Although much of this work was highly influential, Marcu (1997)’s work on discourse parsing brought this task to special prominence. Since then, more and more sophisticated models of discourse analysis have been developed:, e.g., (Marcu, 1999; Soricut and Marcu, 2003; Forbes et al., 2003; Polanyi et al., 2004; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Lin et al., 2012; Feng and Hirst, 2012). Our contribution to work on this task is to examine latent discourse structure specifically in grounded language learning. 3 A Grounded </context>
</contexts>
<marker>Lascarides, Asher, 1993</marker>
<rawString>Alex Lascarides and Nicholas Asher. 1993. Temporal interpretation, discourse relations, and common sense entailment. Linguistics and Philosophy, 16(5):437– 493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joop M I M Leo</author>
</authors>
<title>A general context-free parsing algorithm running in linear time on every lr(k) grammar without using lookahead.</title>
<date>1991</date>
<journal>Theoretical Computer Science,</journal>
<volume>82</volume>
<issue>1</issue>
<contexts>
<context position="16467" citStr="Leo (1991)" startWordPosition="2549" endWordPosition="2550">ground information on the Earley algorithm for PCFGs, the prefix probability scheme we use, and the insideoutside algorithm in the Earley context. 4 Background 4.1 Earley Algorithm for PCFGs The Earley algorithm was developed by Earley (1970) and known to be efficient for certain kinds of CFGs (Aho and Ullman, 1972). An Earley parser 2The prefix markers # and ## and the topic markers such as “.dog” enable a left-to- right parser to unambiguously identify its location in the input string. 3In order to achieve linear time the parsing chart must have suitable indexing; see Aho and Ullman (1972), Leo (1991) and Aycock and Horspool (2002) for details. 318 constructs left-most derivations of strings, using dotted productions to keep track of partial derivations. Specifically, each state in an Earley parser is represented as [l, r]: X-+α . 0 to indicate that input symbols xl, ... , xr−1 have been processed and the parser is expecting to expand 0. States are generated on the fly using three transition operations: predict (add states to charts), scan (shift dots across terminals), and complete (merge two states). Figure 2 shows an example of a completion step which also illustrates the implicit binar</context>
</contexts>
<marker>Leo, 1991</marker>
<rawString>Joop M. I. M. Leo. 1991. A general context-free parsing algorithm running in linear time on every lr(k) grammar without using lookahead. Theoretical Computer Science, 82(1):165–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
</authors>
<title>Expectation-based syntactic comprehension.</title>
<date>2008</date>
<journal>Cognition,</journal>
<volume>106</volume>
<issue>3</issue>
<contexts>
<context position="19041" citStr="Levy, 2008" startWordPosition="2971" endWordPosition="2972">lding a prefix x. Computing prefix probabilities is important because it enables probabilistic prediction of possible follow-words xi+1 as P(xi+1|x0 ... xi) _ P(x°...xixi+1) (Jelinek and Lafferty, 1991). These P(xp ... xi) conditional probabilities allow estimation of the incremental costs of a stack decoder (Bahl et al., 1983). In (Huang and Sagae, 2010), a conceptually similar prefix cost is defined to order states in a beam search decoder. Moreover, the negative logarithm of such conditional probabilities are termed as surprisal values in the psycholinguistics literature (e.g., Hale, 2001; Levy, 2008), to describe how difficult a word is in a given context. Interestingly, we show that prefix probabilities lead us to construct a parser that could parse extremely long strings next. 4.3 Inside Outside Algorithm To extend the Inside Outside (IO) algorithm (Baker, 1979) to the Earley context, Stolcke introduced inner and outer probabilities which generalize the inside and outside probabilities in the IO algorithm. Specifically, the inner probability of a state [l, r]: X-+α . 0 is the probability of generating an input substring xl, ... , xr−1 from a non-terminal X using a production X-+α 0.4 X-</context>
<context position="21052" citStr="Levy (2008)" startWordPosition="3332" endWordPosition="3333">mplicit binarization in 4Summing up inner probabilities of all states Y-+v . exactly yields Baker’s inside probability for Y . X -+ a Y . /3 X - +a .Y /3 Y - +v. l m r 319 Earley parsing allows outer probabilities to be accumulated in a similar way as its counterpart in the IO algorithm (see Figure 3). These quantities allow for efficient grammatical inference in which the expected count of each rule X-+A given a string x is computed as: c(X-+A|x) = Es-[l,r]X-+.λ outer(s) · inner(s) . P(S =&gt;* x) (1) 5 A Rescaling Approach for Parsing Our parser originated from the prefix probability parser by Levy (2008), but has diverged markedly since then. The parser, called Earleyx5, is capable of producing Viterbi parses and performing grammatical induction based on the expectationmaximization and variational Bayes algorithms. To tackle the underflow problem posed when parsing discourses (§3.3), we borrow the rescaling concept from HMMs (Rabiner, 1990) to extend the probabilistic Earley algorithm. Specifically, the probability of each Earley path is scaled by a constant ci each time it passes through a scanned state generating the input symbol xi. In fact, each path passes through each scanned state exac</context>
<context position="23433" citStr="Levy, 2008" startWordPosition="3732" endWordPosition="3733">ors in the outer and inner terms cancel out with those in the string probability in Eq. (1), implying that rule probability estimation is unaffected by rescaling. 5.1 Parsing Time on Dense Grammars We compare in Table 1 the parsing time (on a 2.4GHz Xeon CPU) of our parser (Earleyx) and Levy’s. The task is to compute surprisal values for a 22-word sentence over a dense grammar.7 Given that our parser is now capable of performing scaling to avoid underflow, we avoid converting probabilities to logarithmic form, which yields a speedup of about 4 times compared to Levy’s parser. Parser Time (s) (Levy, 2008) 640 Earleyx + scaling 145 Table 1: Parsing time (dense grammars) – to compute surprisal values for a 22-word sentence using Levy’s parser and ours (Earleyx). # Words Figure 4: Parsing time (sparse grammars) – to compute Viterbi parses for sentences of increasing lengths. Figure 4 shows the time taken (as a function of the input length) for Earleyx to compute a Viterbi parses over our sparse grammars (§3.2). The plot confirmed our analysis in that the special structure of our grammars yields approximately linear parsing time in the input length (see §3.3). 7MLE estimated from the English Penn </context>
</contexts>
<marker>Levy, 2008</marker>
<rawString>Roger Levy. 2008. Expectation-based syntactic comprehension. Cognition, 106(3):1126–1177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning semantic correspondences with less supervision.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP,</booktitle>
<pages>91--99</pages>
<contexts>
<context position="7178" citStr="Liang et al., 2009" startWordPosition="1069" endWordPosition="1072">d are quite different from the experience of language learners. Grounded Language Learning. In contrast to semantic parsers, grounded language learning systems aim to learn the meanings of words and sentences given an observed world state (Yu and Ballard, 2004; Gorniak and Roy, 2007). A growing body of work in this field employs distinct techniques from a wide variety of perspectives from text-to-record alignment using structured classification (Barzilay and Lapata, 2005; Snyder and Barzilay, 2007), iterative retraining (Chen et al., 2010), and generative models of segmentation and alignment (Liang et al., 2009) to text-to-interaction mapping using reinforcement learning (Branavan et al., 2009; Vogel and Jurafsky, 2010), graphical model semantics representation (Tellex et al., 2011a; Tellex et al., 2011b), and Combinatory Categorial Grammar (Artzi and Zettlemoyer, 2013). A number of systems have also used alternative forms of supervision, including sentences paired with responses (Clarke et al., 2010; Goldwasser and Roth, 2011; Liang et al., 2011) and no supervision (Poon and Domingos, 2009; Gold316 wasser et al., 2011). Recent work has also introduced an alternative approach to grounded learning by </context>
</contexts>
<marker>Liang, Jordan, Klein, 2009</marker>
<rawString>Percy Liang, Michael I. Jordan, and Dan Klein. 2009. Learning semantic correspondences with less supervision. In ACL-IJCNLP, pages 91–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2011</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="7622" citStr="Liang et al., 2011" startWordPosition="1135" endWordPosition="1138">tion (Barzilay and Lapata, 2005; Snyder and Barzilay, 2007), iterative retraining (Chen et al., 2010), and generative models of segmentation and alignment (Liang et al., 2009) to text-to-interaction mapping using reinforcement learning (Branavan et al., 2009; Vogel and Jurafsky, 2010), graphical model semantics representation (Tellex et al., 2011a; Tellex et al., 2011b), and Combinatory Categorial Grammar (Artzi and Zettlemoyer, 2013). A number of systems have also used alternative forms of supervision, including sentences paired with responses (Clarke et al., 2010; Goldwasser and Roth, 2011; Liang et al., 2011) and no supervision (Poon and Domingos, 2009; Gold316 wasser et al., 2011). Recent work has also introduced an alternative approach to grounded learning by reducing it to a grammatical inference problem. B¨orschinger et al. (2011) casted the problem of learning a semantic parser as a PCFG induction task, achieving state-of the art performance in the RoboCup domain. Kim and Mooney (2012) extended the technique to make it tractable for more complex problems. Later, Kim and Mooney (2013) adapted discriminative reranking to the grounded learning problem using a form of weak supervision. We employ </context>
</contexts>
<marker>Liang, Jordan, Klein, 2011</marker>
<rawString>Percy Liang, Michael I. Jordan, and Dan Klein. 2011. Learning dependency-based compositional semantics. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Hwee Tou Ng</author>
<author>Min-Yen Kan</author>
</authors>
<title>A pdtb-styled end-to-end discourse parser. Natural Language Engineering,</title>
<date>2012</date>
<location>FirstView:1–34.</location>
<contexts>
<context position="9444" citStr="Lin et al., 2012" startWordPosition="1415" endWordPosition="1418">d formal representations of discourse across a wide variety of theoretical perspectives (Mann and Thompson, 1988; Scha and Polanyi, 1988; Hobbs, 1990; Lascarides and Asher, 1993; Knott and Sanders, 1997). Although much of this work was highly influential, Marcu (1997)’s work on discourse parsing brought this task to special prominence. Since then, more and more sophisticated models of discourse analysis have been developed:, e.g., (Marcu, 1999; Soricut and Marcu, 2003; Forbes et al., 2003; Polanyi et al., 2004; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Lin et al., 2012; Feng and Hirst, 2012). Our contribution to work on this task is to examine latent discourse structure specifically in grounded language learning. 3 A Grounded Learning Task Our focus in this paper is to develop computational models that help us better understand children’s language acquisition. The goal is to learn both the long term lexicon of mappings between words and objects (language learning) as well as the intended topic of individual utterances (language comprehension). We consider a corpus of child-directed speech annotated with social cues, described in (Frank et al., 2013). There </context>
</contexts>
<marker>Lin, Ng, Kan, 2012</marker>
<rawString>Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2012. A pdtb-styled end-to-end discourse parser. Natural Language Engineering, FirstView:1–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Lu</author>
<author>Hwee Tou Ng</author>
<author>Wee Sun Lee</author>
<author>Luke S Zettlemoyer</author>
</authors>
<title>A generative model for parsing natural language to meaning representations.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="6294" citStr="Lu et al., 2008" startWordPosition="937" endWordPosition="940">a positive cue for word learning (Johnson et al., 2012). Our data provide support for the hypothesis (from previous work) that caregivers “follow in”: they name objects that the child is already looking at (Tomasello and Farrar, 1986). In addition, our discourse model produces a performance improvement in a language acquisition task and yields good discourse segmentations compared with human annotators. 2 Related Work Supervised semantic parsers. Previous work has developed supervised semantic parsers to map sentences to meaning representations of various forms, including meaning hierarchies (Lu et al., 2008) and, most dominantly, A-calculus expressions (Zettlemoyer and Collins, 2005; Zettlemoyer, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010). These approaches rely on training data of annotated sentence-meaning pairs, however. Such data are costly to obtain and are quite different from the experience of language learners. Grounded Language Learning. In contrast to semantic parsers, grounded language learning systems aim to learn the meanings of words and sentences given an observed world state (Yu and Ballard, 2004; Gorniak and Roy, 2007). A growing body of work in this field employs dist</context>
</contexts>
<marker>Lu, Ng, Lee, Zettlemoyer, 2008</marker>
<rawString>Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettlemoyer. 2008. A generative model for parsing natural language to meaning representations. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="8940" citStr="Mann and Thompson, 1988" startWordPosition="1332" endWordPosition="1335"> In the context of language acquisition, Frank et al. (2008) proposed a system that learned words and jointly inferred speakers’ intended referent (utterance topic) using graphical models. Johnson et al. (2012) used grammatical inference to demonstrate the importance of social cues in children’s early word learning. We extend this body of work by capturing discourse-based dependencies among utterances rather than treating each utterance independently. Discourse Parsing. A substantial literature has examined formal representations of discourse across a wide variety of theoretical perspectives (Mann and Thompson, 1988; Scha and Polanyi, 1988; Hobbs, 1990; Lascarides and Asher, 1993; Knott and Sanders, 1997). Although much of this work was highly influential, Marcu (1997)’s work on discourse parsing brought this task to special prominence. Since then, more and more sophisticated models of discourse analysis have been developed:, e.g., (Marcu, 1999; Soricut and Marcu, 2003; Forbes et al., 2003; Polanyi et al., 2004; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Lin et al., 2012; Feng and Hirst, 2012). Our contribution to work on this task is to examine latent discourse st</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William C. Mann and Sandra A. Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The rhetorical parsing of natural language texts.</title>
<date>1997</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="9096" citStr="Marcu (1997)" startWordPosition="1358" endWordPosition="1359">ing graphical models. Johnson et al. (2012) used grammatical inference to demonstrate the importance of social cues in children’s early word learning. We extend this body of work by capturing discourse-based dependencies among utterances rather than treating each utterance independently. Discourse Parsing. A substantial literature has examined formal representations of discourse across a wide variety of theoretical perspectives (Mann and Thompson, 1988; Scha and Polanyi, 1988; Hobbs, 1990; Lascarides and Asher, 1993; Knott and Sanders, 1997). Although much of this work was highly influential, Marcu (1997)’s work on discourse parsing brought this task to special prominence. Since then, more and more sophisticated models of discourse analysis have been developed:, e.g., (Marcu, 1999; Soricut and Marcu, 2003; Forbes et al., 2003; Polanyi et al., 2004; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Lin et al., 2012; Feng and Hirst, 2012). Our contribution to work on this task is to examine latent discourse structure specifically in grounded language learning. 3 A Grounded Learning Task Our focus in this paper is to develop computational models that help us bette</context>
</contexts>
<marker>Marcu, 1997</marker>
<rawString>Daniel Marcu. 1997. The rhetorical parsing of natural language texts. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>A decision-based approach to rhetorical parsing.</title>
<date>1999</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="9275" citStr="Marcu, 1999" startWordPosition="1386" endWordPosition="1387">y capturing discourse-based dependencies among utterances rather than treating each utterance independently. Discourse Parsing. A substantial literature has examined formal representations of discourse across a wide variety of theoretical perspectives (Mann and Thompson, 1988; Scha and Polanyi, 1988; Hobbs, 1990; Lascarides and Asher, 1993; Knott and Sanders, 1997). Although much of this work was highly influential, Marcu (1997)’s work on discourse parsing brought this task to special prominence. Since then, more and more sophisticated models of discourse analysis have been developed:, e.g., (Marcu, 1999; Soricut and Marcu, 2003; Forbes et al., 2003; Polanyi et al., 2004; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Lin et al., 2012; Feng and Hirst, 2012). Our contribution to work on this task is to examine latent discourse structure specifically in grounded language learning. 3 A Grounded Learning Task Our focus in this paper is to develop computational models that help us better understand children’s language acquisition. The goal is to learn both the long term lexicon of mappings between words and objects (language learning) as well as the intended top</context>
</contexts>
<marker>Marcu, 1999</marker>
<rawString>Daniel Marcu. 1999. A decision-based approach to rhetorical parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Pevzner</author>
<author>Marti A Hearst</author>
</authors>
<title>A critique and improvement of an evaluation metric for text segmentation.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="37988" citStr="Pevzner and Hearst, 2002" startWordPosition="6001" endWordPosition="6004">arners. To differentiate these cases, Frank and Rohde (under review) added a new set of annotations (to the dataset used in Section 7) based on the discourse structure perceived by human, similar to column discourse, . 323 We utilize these new annotations to judge topics predicted by our discourse model and adopt previous metrics for discourse segmentation evaluation: a=b, a simple proportion equivalence of discourse assignments; pk, a window method (Beeferman et al., 1999) to measure the probability of two random utterances correctly classified as being in the same discourse; and WindowDiff (Pevzner and Hearst, 2002), an improved version of pk which gives “partial credit” to boundaries close to the correct ones. Results in Table 7 demonstrate that our model is in better agreement with human annotation (modelhuman) than the raw annotation (raw-human) across all metrics. As is visible from the limited change in the a=b metric, relatively few topic assignments are altered; yet these alterations create much more coherent discourses that allow for far better segmentation performance under pk and WindowDiff. raw-human model-human a=b 63.6 69.3 pk 57.0 83.6 WindowDiff 36.2 61.2 Table 7: Discourse evaluation. Sin</context>
</contexts>
<marker>Pevzner, Hearst, 2002</marker>
<rawString>Lev Pevzner and Marti A. Hearst. 2002. A critique and improvement of an evaluation metric for text segmentation. Computational Linguistics, 28(1):19–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Livia Polanyi</author>
<author>Chris Culy</author>
<author>Martin Van Den Berg</author>
<author>Gian Lorenzo Thione</author>
<author>David Ahn</author>
</authors>
<title>A rule based approach to discourse parsing.</title>
<date>2004</date>
<booktitle>In SigDIAL.</booktitle>
<marker>Polanyi, Culy, Van Den Berg, Thione, Ahn, 2004</marker>
<rawString>Livia Polanyi, Chris Culy, Martin Van Den Berg, Gian Lorenzo Thione, and David Ahn. 2004. A rule based approach to discourse parsing. In SigDIAL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Unsupervised semantic parsing.</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="7666" citStr="Poon and Domingos, 2009" startWordPosition="1142" endWordPosition="1145">and Barzilay, 2007), iterative retraining (Chen et al., 2010), and generative models of segmentation and alignment (Liang et al., 2009) to text-to-interaction mapping using reinforcement learning (Branavan et al., 2009; Vogel and Jurafsky, 2010), graphical model semantics representation (Tellex et al., 2011a; Tellex et al., 2011b), and Combinatory Categorial Grammar (Artzi and Zettlemoyer, 2013). A number of systems have also used alternative forms of supervision, including sentences paired with responses (Clarke et al., 2010; Goldwasser and Roth, 2011; Liang et al., 2011) and no supervision (Poon and Domingos, 2009; Gold316 wasser et al., 2011). Recent work has also introduced an alternative approach to grounded learning by reducing it to a grammatical inference problem. B¨orschinger et al. (2011) casted the problem of learning a semantic parser as a PCFG induction task, achieving state-of the art performance in the RoboCup domain. Kim and Mooney (2012) extended the technique to make it tractable for more complex problems. Later, Kim and Mooney (2013) adapted discriminative reranking to the grounded learning problem using a form of weak supervision. We employ this general grammatical inference approach </context>
</contexts>
<marker>Poon, Domingos, 2009</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2009. Unsupervised semantic parsing. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence R Rabiner</author>
</authors>
<title>A tutorial on hidden Markov models and selected applications in speech recognition.</title>
<date>1990</date>
<contexts>
<context position="21395" citStr="Rabiner, 1990" startWordPosition="3382" endWordPosition="3383"> grammatical inference in which the expected count of each rule X-+A given a string x is computed as: c(X-+A|x) = Es-[l,r]X-+.λ outer(s) · inner(s) . P(S =&gt;* x) (1) 5 A Rescaling Approach for Parsing Our parser originated from the prefix probability parser by Levy (2008), but has diverged markedly since then. The parser, called Earleyx5, is capable of producing Viterbi parses and performing grammatical induction based on the expectationmaximization and variational Bayes algorithms. To tackle the underflow problem posed when parsing discourses (§3.3), we borrow the rescaling concept from HMMs (Rabiner, 1990) to extend the probabilistic Earley algorithm. Specifically, the probability of each Earley path is scaled by a constant ci each time it passes through a scanned state generating the input symbol xi. In fact, each path passes through each scanned state exactly once, so we consistently accumulate scaling factors for the forward and inner probabilities of a state [l, r] : X-+α . 0 as c0 ... cr−1 and cl ... cr−1 respectively. Arguably, the most intuitive choice of the scaling factors are the prefix probabilities, which essentially resets the probability of any Earley path starting from any positi</context>
</contexts>
<marker>Rabiner, 1990</marker>
<rawString>Lawrence R. Rabiner. 1990. A tutorial on hidden Markov models and selected applications in speech recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Remko Scha</author>
<author>Livia Polanyi</author>
</authors>
<title>An augmented context free grammar for discourse.</title>
<date>1988</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="8964" citStr="Scha and Polanyi, 1988" startWordPosition="1336" endWordPosition="1339">ge acquisition, Frank et al. (2008) proposed a system that learned words and jointly inferred speakers’ intended referent (utterance topic) using graphical models. Johnson et al. (2012) used grammatical inference to demonstrate the importance of social cues in children’s early word learning. We extend this body of work by capturing discourse-based dependencies among utterances rather than treating each utterance independently. Discourse Parsing. A substantial literature has examined formal representations of discourse across a wide variety of theoretical perspectives (Mann and Thompson, 1988; Scha and Polanyi, 1988; Hobbs, 1990; Lascarides and Asher, 1993; Knott and Sanders, 1997). Although much of this work was highly influential, Marcu (1997)’s work on discourse parsing brought this task to special prominence. Since then, more and more sophisticated models of discourse analysis have been developed:, e.g., (Marcu, 1999; Soricut and Marcu, 2003; Forbes et al., 2003; Polanyi et al., 2004; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Lin et al., 2012; Feng and Hirst, 2012). Our contribution to work on this task is to examine latent discourse structure specifically in </context>
</contexts>
<marker>Scha, Polanyi, 1988</marker>
<rawString>Remko Scha and Livia Polanyi. 1988. An augmented context free grammar for discourse. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey M Siskind</author>
</authors>
<title>A computational study of cross-situational techniques for learning word-tomeaning mappings.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>61--1</pages>
<contexts>
<context position="2427" citStr="Siskind, 1996" startWordPosition="357" endWordPosition="358">stics and cognitive science. Accurately learning novel mappings is crucial in grounded language understanding tasks and such systems can suggest insights into the nature of children language learning. Two influential examples of grounded language learning tasks are the sportscasting task, RoboCup, where the NL is the set of running commentary and the MR is the set of logical forms representing actions like kicking or passing (Chen and Mooney, 2008), and the cross-situational word-learning task, where the NL is the caregiver’s utterances and the MR is the set of objects present in the context (Siskind, 1996; Yu and Ballard, 2007). Work in these domains suggests that, based on the cooccurrence between words and their referents in context, it is possible to learn mappings between NL and MR even under substantial ambiguity. Nevertheless, contexts like RoboCup—where every single utterance is grounded—are extremely rare. Much more common are cases where a single topic is introduced and then discussed at length throughout a discourse. In a television news show, for example, a topic might be introduced by presenting a relevant picture or video clip. Once the topic is introduced, the anchors can discuss</context>
</contexts>
<marker>Siskind, 1996</marker>
<rawString>Jeffrey M. Siskind. 1996. A computational study of cross-situational techniques for learning word-tomeaning mappings. Cognition, 61(1-2):39–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Databasetext alignment via structured multilabel classification.</title>
<date>2007</date>
<booktitle>In IJCAI.</booktitle>
<contexts>
<context position="7062" citStr="Snyder and Barzilay, 2007" startWordPosition="1052" endWordPosition="1055">10). These approaches rely on training data of annotated sentence-meaning pairs, however. Such data are costly to obtain and are quite different from the experience of language learners. Grounded Language Learning. In contrast to semantic parsers, grounded language learning systems aim to learn the meanings of words and sentences given an observed world state (Yu and Ballard, 2004; Gorniak and Roy, 2007). A growing body of work in this field employs distinct techniques from a wide variety of perspectives from text-to-record alignment using structured classification (Barzilay and Lapata, 2005; Snyder and Barzilay, 2007), iterative retraining (Chen et al., 2010), and generative models of segmentation and alignment (Liang et al., 2009) to text-to-interaction mapping using reinforcement learning (Branavan et al., 2009; Vogel and Jurafsky, 2010), graphical model semantics representation (Tellex et al., 2011a; Tellex et al., 2011b), and Combinatory Categorial Grammar (Artzi and Zettlemoyer, 2013). A number of systems have also used alternative forms of supervision, including sentences paired with responses (Clarke et al., 2010; Goldwasser and Roth, 2011; Liang et al., 2011) and no supervision (Poon and Domingos, </context>
</contexts>
<marker>Snyder, Barzilay, 2007</marker>
<rawString>Benjamin Snyder and Regina Barzilay. 2007. Databasetext alignment via structured multilabel classification. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Daniel Marcu</author>
</authors>
<title>Sentence level discourse parsing using syntactic and lexical information.</title>
<date>2003</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="9300" citStr="Soricut and Marcu, 2003" startWordPosition="1388" endWordPosition="1391">iscourse-based dependencies among utterances rather than treating each utterance independently. Discourse Parsing. A substantial literature has examined formal representations of discourse across a wide variety of theoretical perspectives (Mann and Thompson, 1988; Scha and Polanyi, 1988; Hobbs, 1990; Lascarides and Asher, 1993; Knott and Sanders, 1997). Although much of this work was highly influential, Marcu (1997)’s work on discourse parsing brought this task to special prominence. Since then, more and more sophisticated models of discourse analysis have been developed:, e.g., (Marcu, 1999; Soricut and Marcu, 2003; Forbes et al., 2003; Polanyi et al., 2004; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Lin et al., 2012; Feng and Hirst, 2012). Our contribution to work on this task is to examine latent discourse structure specifically in grounded language learning. 3 A Grounded Learning Task Our focus in this paper is to develop computational models that help us better understand children’s language acquisition. The goal is to learn both the long term lexicon of mappings between words and objects (language learning) as well as the intended topic of individual utteranc</context>
</contexts>
<marker>Soricut, Marcu, 2003</marker>
<rawString>Radu Soricut and Daniel Marcu. 2003. Sentence level discourse parsing using syntactic and lexical information. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>An efficient probabilistic context-free parsing algorithm that computes prefix probabilities.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<pages>201</pages>
<contexts>
<context position="15729" citStr="Stolcke, 1995" startWordPosition="2427" endWordPosition="2428"> the probability of a parse is the product of the probabilities of the rules involved in its derivation. As the length of a derivation grows linearly with the length of the input, the parse probabilities decrease exponentially as a function of sentence length, causing floating-point underflow on inputs of even moderate length. The standard method for handling this is to compute log probabilities (which decrease linearly as a function of input length, rather than exponentially), but as we explain later (Section 5), we can use the ability of the Earley algorithm to compute prefix probabilities (Stolcke, 1995) to rescale the probability of the parse incrementally and avoid floating-point underflows. In the next section, we provide background information on the Earley algorithm for PCFGs, the prefix probability scheme we use, and the insideoutside algorithm in the Earley context. 4 Background 4.1 Earley Algorithm for PCFGs The Earley algorithm was developed by Earley (1970) and known to be efficient for certain kinds of CFGs (Aho and Ullman, 1972). An Earley parser 2The prefix markers # and ## and the topic markers such as “.dog” enable a left-to- right parser to unambiguously identify its location </context>
<context position="17281" citStr="Stolcke (1995)" startWordPosition="2691" endWordPosition="2692">r is represented as [l, r]: X-+α . 0 to indicate that input symbols xl, ... , xr−1 have been processed and the parser is expecting to expand 0. States are generated on the fly using three transition operations: predict (add states to charts), scan (shift dots across terminals), and complete (merge two states). Figure 2 shows an example of a completion step which also illustrates the implicit binarization automatically done in Earley algorithm. Figure 2: Completion step – merging two states [l, m]: X-+α . Y o and [m, r]:Y-+v . to produce a new state [l, r]: X-+αY . o. In order to handle PCFGs, Stolcke (1995) extends the Earley parsing algorithm to introduce the notion of an Earley path being a sequence of states linked by Earley operations. By establishing a oneto-one mapping between partial derivations and Earley paths, Stolcke could then assign each path a derivation probability, that is the product of the all rule probabilities used in the predicted states of that path. Here, each production X-+v corresponds to a predicted state [l, l] : X-+. V. Besides parsing, being able to compute string and prefix probabilities by summing derivation probabilities is also of great importance. To compute the</context>
<context position="22781" citStr="Stolcke, 1995" startWordPosition="3622" endWordPosition="3623">ives us the surprisal value for the input symbol xi. Rescaling factors are only introduced in the forward pass, during which the outer probability of a state [l, r]: X-+α . 0 has already been scaled by factors c0 ... cl−1cr ... cn−1.6 More importantly, when 5Parser code is available at http://nlp.stanford. edu/˜lmthang/earleyx. 6The outer probability of a state is essentially the product of inner probabilities covering all input symbols outside the span of that state. For grammars containing cyclic unit productions, we also need to multiply with terms from the unit-production relation matrix (Stolcke, 1995). computing expected counts, scaling factors in the outer and inner terms cancel out with those in the string probability in Eq. (1), implying that rule probability estimation is unaffected by rescaling. 5.1 Parsing Time on Dense Grammars We compare in Table 1 the parsing time (on a 2.4GHz Xeon CPU) of our parser (Earleyx) and Levy’s. The task is to compute surprisal values for a 22-word sentence over a dense grammar.7 Given that our parser is now capable of performing scaling to avoid underflow, we avoid converting probabilities to logarithmic form, which yields a speedup of about 4 times com</context>
</contexts>
<marker>Stolcke, 1995</marker>
<rawString>Andreas Stolcke. 1995. An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. Computational Linguistics, 21(2):165– 201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajen Subba</author>
<author>Barbara Di Eugenio</author>
</authors>
<title>An effective discourse parser that uses rich linguistic information.</title>
<date>2009</date>
<booktitle>In NAACL.</booktitle>
<marker>Subba, Di Eugenio, 2009</marker>
<rawString>Rajen Subba and Barbara Di Eugenio. 2009. An effective discourse parser that uses rich linguistic information. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy</author>
</authors>
<title>Approaching the symbol grounding problem with probabilistic graphical models.</title>
<date>2011</date>
<journal>AI Magazine,</journal>
<volume>32</volume>
<issue>4</issue>
<marker>Roy, 2011</marker>
<rawString>Stefanie Tellex, Thomas Kolla, Steven Dickerson, Matthew R. Walter, Ashis G. Banerjee, Seth Teller, and Nicholas. Roy. 2011a. Approaching the symbol grounding problem with probabilistic graphical models. AI Magazine, 32(4):64–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefanie Tellex</author>
<author>Thomas Kolla</author>
<author>Steven Dickerson</author>
<author>Matthew R Walter</author>
<author>Ashis G Banerjee</author>
<author>Seth Teller</author>
<author>Nicholas Roy</author>
</authors>
<title>Understanding Natural Language Commands for Robotic Navigation and Mobile Manipulation. In</title>
<date>2011</date>
<publisher>AAAI.</publisher>
<contexts>
<context position="7351" citStr="Tellex et al., 2011" startWordPosition="1093" endWordPosition="1096">the meanings of words and sentences given an observed world state (Yu and Ballard, 2004; Gorniak and Roy, 2007). A growing body of work in this field employs distinct techniques from a wide variety of perspectives from text-to-record alignment using structured classification (Barzilay and Lapata, 2005; Snyder and Barzilay, 2007), iterative retraining (Chen et al., 2010), and generative models of segmentation and alignment (Liang et al., 2009) to text-to-interaction mapping using reinforcement learning (Branavan et al., 2009; Vogel and Jurafsky, 2010), graphical model semantics representation (Tellex et al., 2011a; Tellex et al., 2011b), and Combinatory Categorial Grammar (Artzi and Zettlemoyer, 2013). A number of systems have also used alternative forms of supervision, including sentences paired with responses (Clarke et al., 2010; Goldwasser and Roth, 2011; Liang et al., 2011) and no supervision (Poon and Domingos, 2009; Gold316 wasser et al., 2011). Recent work has also introduced an alternative approach to grounded learning by reducing it to a grammatical inference problem. B¨orschinger et al. (2011) casted the problem of learning a semantic parser as a PCFG induction task, achieving state-of the </context>
</contexts>
<marker>Tellex, Kolla, Dickerson, Walter, Banerjee, Teller, Roy, 2011</marker>
<rawString>Stefanie Tellex, Thomas Kolla, Steven Dickerson, Matthew R. Walter, Ashis G. Banerjee, Seth Teller, and Nicholas Roy. 2011b. Understanding Natural Language Commands for Robotic Navigation and Mobile Manipulation. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Tomasello</author>
<author>Michael Jeffrey Farrar</author>
</authors>
<title>Joint attention and early language. Child development,</title>
<date>1986</date>
<pages>1454--1463</pages>
<contexts>
<context position="5912" citStr="Tomasello and Farrar, 1986" startWordPosition="883" endWordPosition="886"> time; (2) suggest a rescaling approach that enables us to build a PCFG parser capable of handling very long strings with thousands of tokens; and (3) employ Variational Bayes for grammatical inference to obtain better grammars than those given by the EM algorithm. By parsing entire discourses at once, we shed light on a scientifically interesting question about why the child’s own gaze is a positive cue for word learning (Johnson et al., 2012). Our data provide support for the hypothesis (from previous work) that caregivers “follow in”: they name objects that the child is already looking at (Tomasello and Farrar, 1986). In addition, our discourse model produces a performance improvement in a language acquisition task and yields good discourse segmentations compared with human annotators. 2 Related Work Supervised semantic parsers. Previous work has developed supervised semantic parsers to map sentences to meaning representations of various forms, including meaning hierarchies (Lu et al., 2008) and, most dominantly, A-calculus expressions (Zettlemoyer and Collins, 2005; Zettlemoyer, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010). These approaches rely on training data of annotated sentence-meaning pa</context>
<context position="40700" citStr="Tomasello and Farrar, 1986" startWordPosition="6421" endWordPosition="6424">n’s language acquisition. In addition, we investigate the interaction between discourse structure and social cues, both important and complementary sources of information in language learning (Baldwin, 1993; Frank et al., 2013). We also examined why individual children’s gaze was an important predictor of reference in previous work (Johnson et al., 2012). Using ablation tests, we showed that information provided by the child’s gaze is still valuable even in the presence of discourse continuity, supporting the hypothesis that parents “follow in” on the particular focus of children’s attention (Tomasello and Farrar, 1986). Lastly, we showed that our models can produce accurate discourse segmentations. Our system’s output is considerably better than the raw topic annotations provided in the previous social cue corpus (Frank et al., 2013) and is in good agreement with discourse topics assigned by human annotators in Frank and Rohde (under review). In conclusion, although previous work on grounded language learning has treated individual utterances as independent entities, we have shown that the ability to incorporate discourse information can be quite useful for such problems. Discourse continuity is an importan</context>
</contexts>
<marker>Tomasello, Farrar, 1986</marker>
<rawString>Michael Tomasello and Michael Jeffrey Farrar. 1986. Joint attention and early language. Child development, pages 1454–1463.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Vogel</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Learning to follow navigational directions.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="7288" citStr="Vogel and Jurafsky, 2010" startWordPosition="1083" endWordPosition="1087">to semantic parsers, grounded language learning systems aim to learn the meanings of words and sentences given an observed world state (Yu and Ballard, 2004; Gorniak and Roy, 2007). A growing body of work in this field employs distinct techniques from a wide variety of perspectives from text-to-record alignment using structured classification (Barzilay and Lapata, 2005; Snyder and Barzilay, 2007), iterative retraining (Chen et al., 2010), and generative models of segmentation and alignment (Liang et al., 2009) to text-to-interaction mapping using reinforcement learning (Branavan et al., 2009; Vogel and Jurafsky, 2010), graphical model semantics representation (Tellex et al., 2011a; Tellex et al., 2011b), and Combinatory Categorial Grammar (Artzi and Zettlemoyer, 2013). A number of systems have also used alternative forms of supervision, including sentences paired with responses (Clarke et al., 2010; Goldwasser and Roth, 2011; Liang et al., 2011) and no supervision (Poon and Domingos, 2009; Gold316 wasser et al., 2011). Recent work has also introduced an alternative approach to grounded learning by reducing it to a grammatical inference problem. B¨orschinger et al. (2011) casted the problem of learning a se</context>
</contexts>
<marker>Vogel, Jurafsky, 2010</marker>
<rawString>Adam Vogel and Daniel Jurafsky. 2010. Learning to follow navigational directions. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning synchronous grammars for semantic parsing with lambda calculus.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="6412" citStr="Wong and Mooney, 2007" startWordPosition="953" endWordPosition="956">us work) that caregivers “follow in”: they name objects that the child is already looking at (Tomasello and Farrar, 1986). In addition, our discourse model produces a performance improvement in a language acquisition task and yields good discourse segmentations compared with human annotators. 2 Related Work Supervised semantic parsers. Previous work has developed supervised semantic parsers to map sentences to meaning representations of various forms, including meaning hierarchies (Lu et al., 2008) and, most dominantly, A-calculus expressions (Zettlemoyer and Collins, 2005; Zettlemoyer, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010). These approaches rely on training data of annotated sentence-meaning pairs, however. Such data are costly to obtain and are quite different from the experience of language learners. Grounded Language Learning. In contrast to semantic parsers, grounded language learning systems aim to learn the meanings of words and sentences given an observed world state (Yu and Ballard, 2004; Gorniak and Roy, 2007). A growing body of work in this field employs distinct techniques from a wide variety of perspectives from text-to-record alignment using structured classification (Bar</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Yuk Wah Wong and Raymond J. Mooney. 2007. Learning synchronous grammars for semantic parsing with lambda calculus. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Yu</author>
<author>Dana H Ballard</author>
</authors>
<title>On the integration of grounding language and learning objects.</title>
<date>2004</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="6819" citStr="Yu and Ballard, 2004" startWordPosition="1016" endWordPosition="1019">es to meaning representations of various forms, including meaning hierarchies (Lu et al., 2008) and, most dominantly, A-calculus expressions (Zettlemoyer and Collins, 2005; Zettlemoyer, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010). These approaches rely on training data of annotated sentence-meaning pairs, however. Such data are costly to obtain and are quite different from the experience of language learners. Grounded Language Learning. In contrast to semantic parsers, grounded language learning systems aim to learn the meanings of words and sentences given an observed world state (Yu and Ballard, 2004; Gorniak and Roy, 2007). A growing body of work in this field employs distinct techniques from a wide variety of perspectives from text-to-record alignment using structured classification (Barzilay and Lapata, 2005; Snyder and Barzilay, 2007), iterative retraining (Chen et al., 2010), and generative models of segmentation and alignment (Liang et al., 2009) to text-to-interaction mapping using reinforcement learning (Branavan et al., 2009; Vogel and Jurafsky, 2010), graphical model semantics representation (Tellex et al., 2011a; Tellex et al., 2011b), and Combinatory Categorial Grammar (Artzi </context>
</contexts>
<marker>Yu, Ballard, 2004</marker>
<rawString>Chen Yu and Dana H. Ballard. 2004. On the integration of grounding language and learning objects. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Yu</author>
<author>Dana H Ballard</author>
</authors>
<title>A unified model of early word learning: Integrating statistical and social cues.</title>
<date>2007</date>
<journal>Neurocomputing,</journal>
<volume>70</volume>
<issue>13</issue>
<contexts>
<context position="2450" citStr="Yu and Ballard, 2007" startWordPosition="359" endWordPosition="362">tive science. Accurately learning novel mappings is crucial in grounded language understanding tasks and such systems can suggest insights into the nature of children language learning. Two influential examples of grounded language learning tasks are the sportscasting task, RoboCup, where the NL is the set of running commentary and the MR is the set of logical forms representing actions like kicking or passing (Chen and Mooney, 2008), and the cross-situational word-learning task, where the NL is the caregiver’s utterances and the MR is the set of objects present in the context (Siskind, 1996; Yu and Ballard, 2007). Work in these domains suggests that, based on the cooccurrence between words and their referents in context, it is possible to learn mappings between NL and MR even under substantial ambiguity. Nevertheless, contexts like RoboCup—where every single utterance is grounded—are extremely rare. Much more common are cases where a single topic is introduced and then discussed at length throughout a discourse. In a television news show, for example, a topic might be introduced by presenting a relevant picture or video clip. Once the topic is introduced, the anchors can discuss it by name or even usi</context>
</contexts>
<marker>Yu, Ballard, 2007</marker>
<rawString>Chen Yu and Dana H Ballard. 2007. A unified model of early word learning: Integrating statistical and social cues. Neurocomputing, 70(13):2149–2165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars. In UAI.</title>
<date>2005</date>
<contexts>
<context position="6370" citStr="Zettlemoyer and Collins, 2005" startWordPosition="946" endWordPosition="950">ta provide support for the hypothesis (from previous work) that caregivers “follow in”: they name objects that the child is already looking at (Tomasello and Farrar, 1986). In addition, our discourse model produces a performance improvement in a language acquisition task and yields good discourse segmentations compared with human annotators. 2 Related Work Supervised semantic parsers. Previous work has developed supervised semantic parsers to map sentences to meaning representations of various forms, including meaning hierarchies (Lu et al., 2008) and, most dominantly, A-calculus expressions (Zettlemoyer and Collins, 2005; Zettlemoyer, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010). These approaches rely on training data of annotated sentence-meaning pairs, however. Such data are costly to obtain and are quite different from the experience of language learners. Grounded Language Learning. In contrast to semantic parsers, grounded language learning systems aim to learn the meanings of words and sentences given an observed world state (Yu and Ballard, 2004; Gorniak and Roy, 2007). A growing body of work in this field employs distinct techniques from a wide variety of perspectives from text-to-record alig</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars. In UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Zettlemoyer</author>
<author>Luke S and Collins</author>
</authors>
<title>Online learning of relaxed CCG grammars for parsing to logical form.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL.</booktitle>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>Michael Zettlemoyer, Luke S.and Collins. 2007. Online learning of relaxed CCG grammars for parsing to logical form. In EMNLP-CoNLL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>