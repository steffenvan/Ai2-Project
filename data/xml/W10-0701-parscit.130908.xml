<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005521">
<title confidence="0.985614">
Creating Speech and Language Data With Amazon’s Mechanical Turk
</title>
<author confidence="0.990944">
Chris Callison-Burch and Mark Dredze
</author>
<affiliation confidence="0.952954333333333">
Human Language Technology Center of Excellence
&amp; Center for Language and Speech Processing
Johns Hopkins University
</affiliation>
<email confidence="0.998397">
ccb,mdredze@cs.jhu.edu
</email>
<sectionHeader confidence="0.995857" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99002675">
In this paper we give an introduction to us-
ing Amazon’s Mechanical Turk crowdsourc-
ing platform for the purpose of collecting
data for human language technologies. We
survey the papers published in the NAACL-
2010 Workshop. 24 researchers participated
in the workshop’s shared task to create data for
speech and language applications with $100.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999974590909091">
This paper gives an overview of the NAACL-2010
Workshop on Creating Speech and Language Data
With Amazon’s Mechanical Turk. A number of re-
cent papers have evaluated the effectiveness of us-
ing Mechanical Turk to create annotated data for
natural language processing applications. The low
cost, scalable workforce available through Mechan-
ical Turk (MTurk) and other crowdsourcing sites
opens new possibilities for annotating speech and
text, and has the potential to dramatically change
how we create data for human language technolo-
gies. Open questions include: What kind of research
is possible when the cost of creating annotated train-
ing data is dramatically reduced? What new tasks
should we try to solve if we do not limit ourselves to
reusing existing training and test sets? Can complex
annotation be done by untrained annotators? How
can we ensure high quality annotations from crowd-
sourced contributors?
To begin addressing these questions, we orga-
nized an open-ended $100 shared task. Researchers
were given $100 of credit on Amazon Mechanical
</bodyText>
<page confidence="0.84572">
1
</page>
<bodyText confidence="0.999840153846154">
Turk to spend on an annotation task of their choos-
ing. They were required to write a short paper de-
scribing their experience, and to distribute the data
that they created. They were encouraged to ad-
dress the following questions: How did you convey
the task in terms that were simple enough for non-
experts to understand? Were non-experts as good as
experts? What did you do to ensure quality? How
quickly did the data get annotated? What is the cost
per label? Researchers submitted a 1 page proposal
to the workshop organizers that described their in-
tended experiments and expected outcomes. The
organizers selected proposals based on merit, and
awarded $100 credits that were generously provided
by Amazon Mechanical Turk. In total, 35 credits
were awarded to researchers.
Shared task participants were given 10 days to run
experiments between the distribution of the credit
and the initial submission deadline. 30 papers were
submitted to the shared task track, of which 24 were
accepted. 14 papers were submitted to the general
track of which 10 were accepted, giving a 77% ac-
ceptance rate and a total of 34 papers. Shared task
participants were required to provide the data col-
lected as part of their experiments. All of the shared
task data is available on the workshop website.
</bodyText>
<sectionHeader confidence="0.996647" genericHeader="method">
2 Mechanical Turk
</sectionHeader>
<bodyText confidence="0.99440375">
Amazon’s Mechanical Turk1 is an online market-
place for work. Amazon’s tag line for Mechani-
cal Turk is artificial artificial intelligence, and the
name refers to a historical hoax from the 18th cen-
</bodyText>
<footnote confidence="0.989812">
1http://www.mturk.com/
</footnote>
<note confidence="0.819382333333333">
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 1–12,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
Hours spent on Mechanical Turk per week Number of HITs completed per week Weekly income from Mechanical Turk
</note>
<figure confidence="0.9967975">
30%
25%
20%
15%
10%
5%
0%
20%
30%
20%
10%
0%
40%
15%
10%
5%
0%
&lt; 1 1-2 2-4 4-88-20 40+ &lt; 1 HIT 5-10 20-50 100-200 500-1k 5k+ HITs &lt; $1 $5-10 20-50 100-200
</figure>
<figureCaption confidence="0.999983">
Figure 1: Time spent, HITs completed, and amount earned from a survey of 1,000 Turkers by Ipeirotis (2010).
</figureCaption>
<bodyText confidence="0.990383083333333">
tury where a chess-playing automaton appeared to
be able to beat human opponents using a mecha-
nism, but was, in fact, controlled by a person hiding
inside the machine. These hint at the the primary fo-
cus of the web service, which is to get people to per-
form tasks that are simple for humans but difficult
for computers. The basic unit of work on MTurk is
even called a Human Intelligence Task (HIT).
Amazon’s web service provides an easy way to
pay people small amounts of money to perform
HITs. Anyone with an Amazon account can either
submit HITs or work on HITs that were submitted
by others. Workers are referred to as “Turkers” and
people designing the HITs are called “Requesters.”
Requesters set the amount that they will pay for each
item that is completed. Payments are frequently as
low as $0.01. Turkers are free to select whichever
HITs interest them.], and to disregard HITs that they
find uninteresting or which they deem pay too little.
Because of its focus on tasks requiring human in-
telligence, Mechanical Turk is obviously applicable
to the field of natural language processing. Snow
et al. (2008) used Mechanical Turk to inexpensively
collect labels for several NLP tasks including word
sense disambiguation, word similarity, textual en-
tailment, and temporal ordering of events. Snow et
al. had two exciting findings. First, they showed that
a strong correlation between non-expert and expert
annotators can be achieved by combining the judg-
ments of multiple non-experts, for instance by vot-
ing on each label using 10 different Turkers. Cor-
relation and accuracy of labeling could be further
improved by weighting each Turker’s vote by cal-
ibrating them on a small amount of gold standard
data created by expert annotators. Second, they col-
lected a staggering number of labels for a very small
amount of money. They collected 21,000 labels for
just over $25. Turkers put in over 140+ hours worth
Why do you complete tasks in MTurk? US India
To spend free time fruitfully and get 70% 60%
cash (e.g., instead of watching TV)
For “primary” income purposes (e.g., 15% 27%
gas, bills, groceries, credit cards)
For “secondary” income purposes, 60% 37%
pocket change (for hobbies, gadgets)
To kill time 33% 5%
The tasks are fun 40% 20%
Currently unemployed or part time work 30% 27%
</bodyText>
<tableCaption confidence="0.9848055">
Table 1: Motivations for participating on Mechanical
Turk from a survey of 1,000 Turkers by Ipeirotis (2010).
</tableCaption>
<bodyText confidence="0.998864333333333">
of human effort to generate the labels. The amount
of participation is surprisingly high, given the small
payment.
</bodyText>
<subsectionHeader confidence="0.880675">
Turker demographics
</subsectionHeader>
<bodyText confidence="0.99967225">
Given the amount of work that can get done for so
little, it is natural to ask: who would contribute so
much work for so little pay, and why? The answers
to these questions are often mysterious because
Amazon does not provide any personal informa-
tion about Turkers (each Turker is identifiable only
through a serial number like A23KO2TP7I4KK2).
Ipeirotis (2010) elucidates some of the reasons by
presenting a demographic analysis of Turkers. He
built a profile of 1000 Turkers by posting a survey to
MTurk and paying $0.10 for people to answer ques-
tions about their reasons for participating on Me-
chanical Turk, the amount that they earn each week,
and how much time they spend, as well as demo-
graphic information like country of origin, gender,
age, education level, and household income.
One suspicion that people often have when they
first hear about MTurk is that it is some sort of dig-
ital sweatshop that exploits workers in third world
countries. However, Ipeirotis reports that nearly half
</bodyText>
<page confidence="0.987741">
2
</page>
<bodyText confidence="0.993005">
(47%) of the Turkers who answered his survey were
from the United States, with the next largest group
(34%) coming from India, and the remaining 19%
spread between 66 other countries.
Table 1 gives the survey results for questions
relating to why people participate on Mechanical
Turk. It shows that most US-based workers use Me-
chanical Turk for secondary income purposes (to
have spending money for hobbies or going out),
but that the overwhelming majority of them use
it to spend their time more fruitfully (i.e., instead
of watching TV). The economic downturn may
have increased participation, with 30% of the US-
based Turkers reporting that they are unemployed
or underemployed. The public radio show Mar-
ketplace recently interviewed unemployed Turkers
(Rose, 2010). It reports that they earn a little in-
come, but that they do not earn enough to make a
living. Figure 1 confirms this, giving a break down
of how much time people spend on Mechanical Turk
each week, how many HITs they complete, and how
much money they earn. Most Turkers spend less
than 8 hours per week on Mechanical Turk, and earn
less than $10 per week through the site.
</bodyText>
<sectionHeader confidence="0.995232" genericHeader="method">
3 Quality Control
</sectionHeader>
<bodyText confidence="0.9979573">
Ipeirotis (2010) reports that just over half of Turkers
have a college education. Despite being reasonably
well educated, it is important to keep in mind that
Turkers do not have training in specialized subjects
like NLP. Because the Turkers are non-experts, and
because the payments are generally so low, quality
control is an important consideration when creating
data with MTurk.
Amazon provides three mechanisms to help en-
sure quality:
</bodyText>
<listItem confidence="0.81691975">
• Requesters have the option of rejecting the
work of individual Turkers, in which case they
are not paid.2 Turkers can also be blocked from
doing future work for a requester.
</listItem>
<footnote confidence="0.960321375">
2Since the results are downloadable even if they are rejected,
this could allow unscrupulous Requesters to abuse Turkers by
rejecting all of their work, even if it was done well. Turkers have
message boards at http://www.turkernation.com/,
where they discuss Requesters. They even have a Firefox plu-
gin called Turkopticon that lets them see ratings of how good
the Requesters are in terms of communicating with Turkers, be-
ing generous and fair, and paying promptly.
</footnote>
<listItem confidence="0.925688">
• Requesters can specify that each HIT should
be redundantly completed by several different
Turkers. This allows higher quality labels to
be selected, for instance, by taking the majority
label.
• Requesters can require that all workers meet
a particular set of qualifications, such as suffi-
cient accuracy on a small test set or a minimum
percentage of previously accepted submissions.
</listItem>
<bodyText confidence="0.999376552631579">
Amazon provides two qualifications that a Re-
quester can use by default. These are past HIT Ap-
proval Rate and Location. The location qualifica-
tion allows the Requester to have HITs done only by
residents of a certain country (or to exclude Turk-
ers from certain regions). Additionally, Requesters
can design custom Qualification Tests that Turkers
must complete before working on a particular HIT.
These can be created through the MTurk API, and
can either be graded manually or automatically. An
important qualification that isn’t among Amazon’s
default qualifications is language skills. One might
design a qualification test to determine a Turker’s
ability to speak Arabic or Farsi before allowing them
to do part of speech tagging in those languages, for
instance.
There are several reasons that poor quality data
might be generated. The task may be too complex or
the instructions might not be clear enough for Turk-
ers to follow. The financial incentives may be too
low for Turkers to act conscientiously, and certain
HIT designs may allow them to simply randomly
click instead of thinking about the task. Mason and
Watts (2009) present a study of financial incentives
on Mechanical Turk and find, counterintuitively, that
increasing the amount of compensation for a partic-
ular task does not tend to improve the quality of the
results. Anecdotally, we have observed that some-
times there is an inverse relationship between the
amount of payment and the quality of work, because
it is more tempting to cheat on high-paying HITs if
you don’t have the skills to complete them. For ex-
ample, a number of Turkers tried to cheat on an Urdu
to English translation HIT by cutting-and-pasting
the Urdu text into an online machine translation sys-
tem (expressly forbidden in the instructions) because
we were paying the comparatively high amount of
$1.
</bodyText>
<page confidence="0.990664">
3
</page>
<subsectionHeader confidence="0.998865">
3.1 Designing HITs for quality control
</subsectionHeader>
<bodyText confidence="0.99998015">
We suggest designing your HITs in a way that will
deter cheating or that will make cheating obvious.
HIT design is part of the art of using MTurk. It
can’t be easily quantified, but it has a large impact on
the outcome. For instance, we reduced cheating on
our translation HIT by changing the design so that
we displayed images of the Urdu sentences instead
of text, which made it impossible to copy-and-paste
into an MT system for anyone who could not type in
Arabic script.
Another suggestion is to include information
within the data that you upload to MTurk that will
not be displayed to the Turkers, but will be useful
to you when reviewing the HITs. For example, we
include machine translation output along with the
source sentences. Although this is not displayed to
Turkers, when we review the Turkers’ translations
we compare them to the MT output. This allows us
to reject translations that are identical to the MT, or
which are just random sentences that are unrelated to
the original Urdu. We also use a javascript3 to gather
the IP addresses of the Turkers and do geolocation
to look up their location. Turkers in Pakistan require
less careful scrutiny since they are more likely to be
bilingual Urdu speakers than those in Romania, for
instance.
CrowdFlower4 provides an interface for design-
ing HITs that includes a phase for the Requester to
input gold standard data with known labels. Insert-
ing items with known labels alongside items which
need labels allows a Requester to see which Turkers
are correctly replicating the gold standard labels and
which are not. This is an excellent idea. If it is possi-
ble to include positive and negative controls in your
HITs, then do so. Turkers who fail the controls can
be blocked and their labels can be excluded from the
final data set. CrowdFlower-generated HITs even
display a score to the Turkers to give them feedback
on how well they are doing. This provides training
for Turkers, and discourages cheating.
</bodyText>
<footnote confidence="0.999802">
3http://wiki.github.com/callison-burch/
mechanical_turk_workshop/geolocation
4http://crowdflower.com/
</footnote>
<subsectionHeader confidence="0.976242">
3.2 Iterative improvements on MTurk
</subsectionHeader>
<bodyText confidence="0.9999288125">
Another class of quality control on Mechanical Turk
is through iterative HITs that build on the output of
previous HITs. This could be used to have Turkers
judge whether the results from a previous HIT con-
formed to the instructions, and whether it is of high
quality. Alternately, the second set of Turkers could
be used to improve the quality of what the first Turk-
ers created. For instance, in a translation task, a sec-
ond set of US-based Turkers could edit the English
produced by non-native speakers.
CastingWords,5 a transcription company that uses
Turker labor, employs this strategy by having a first-
pass transcription graded and iteratively improved
in subsequent passes. Little et al. (2009) even de-
signed an API specifically for running iterative tasks
on MTurk.6
</bodyText>
<sectionHeader confidence="0.996255" genericHeader="method">
4 Recommended Practices
</sectionHeader>
<bodyText confidence="0.999965730769231">
Although it is hard to define a set of “best practices”
that applies to all HITs, or even to all NLP HITs, we
recommend the following guidelines to Requesters.
First and foremost, it is critical to convey instruc-
tions appropriately for non-experts. The instructions
should be clear and concise. To calibrate whether
the HIT is doable, you should first try the task your-
self, and then have a friend from outside the field try
it. This will help to ensure that the instructions are
clear, and to calibrate how long each HIT will take
(which ought to allow you to price the HITs fairly).
If possible, you should insert positive and nega-
tive controls so that you can quickly screen out bad
Turkers. This is especially important for HITs that
only require clicking buttons to complete. If pos-
sible, you should include a small amount of gold
standard data in each HIT. This will allow you to
determine which Turkers are good, but will also al-
low you weight the Turkers if you are combining
the judgments of multiple Turkers. If you are hav-
ing Turkers evaluate the output of systems, then ran-
domize the order that the systems are shown in.
When publishing papers that use Mechanical Turk
as a source of training data or to evaluate the output
of an NLP system, report how you ensured the qual-
ity of your data. You can do this by measuring the
</bodyText>
<footnote confidence="0.9994">
5http://castingwords.com/
6http://groups.csail.mit.edu/uid/turkit/
</footnote>
<page confidence="0.996174">
4
</page>
<bodyText confidence="0.99995875">
inter-annotator agreement of the Turkers against ex-
perts on small amounts of gold standard data, or by
stating what controls you used and what criteria you
used to block bad Turkers. Finally, whenever possi-
ble you should publish the data that you generate on
Mechanical Turk (and your analysis scripts and HIT
templates) alongside your paper so that other people
can verify it.
</bodyText>
<sectionHeader confidence="0.999841" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.999984785714286">
In the past two years, several papers have published
about applying Mechanical Turk to a diverse set of
natural language processing tasks, including: cre-
ating question-answer sentence pairs (Kaisser and
Lowe, 2008), evaluating machine translation qual-
ity and crowdsouring translations (Callison-Burch,
2009), paraphrasing noun-noun compouds for Se-
mEval (Butnariu et al., 2009), human evaluation of
topic models (Chang et al., 2009), and speech tran-
scription (McGraw et al., 2010; Marge et al., 2010a;
Novotney and Callison-Burch, 2010a). Others have
used MTurk for novel research directions like non-
simulated active learning for NLP tasks such as sen-
timent classification (Hsueh et al., 2009) or doing
quixotic things like doing human-in-the-loop min-
imum error rate training for machine translation
(Zaidan and Callison-Burch, 2009).
Some projects have demonstrated the super-
scalability of crowdsourced efforts. Deng et al.
(2009) used MTurk to construct ImageNet, an anno-
tated image database containing 3.2 million that are
hierarchically categorized using the WordNet ontol-
ogy (Fellbaum, 1998). Because Mechanical Turk
allows researchers to experiment with crowdsourc-
ing by providing small incentives to Turkers, other
successful crowdsourcing efforts like Wikipedia or
Games with a Purpose (von Ahn and Dabbish, 2008)
also share something in common with MTurk.
</bodyText>
<sectionHeader confidence="0.988271" genericHeader="method">
6 Shared Task
</sectionHeader>
<bodyText confidence="0.99994355">
The workshop included a shared task in which par-
ticipants were provided with $100 to spend on Me-
chanical Turk experiments. Participants submitted a
1 page proposal in advance describing their intended
use of the funds. Selected proposals were provided
$100 seed money, to which many participants added
their own funds. As part of their participation, each
team submitted a workshop paper describing their
experiments as well as the data collected and de-
scribed in the paper. Data for the shared papers is
available at the workshop website.7
This section describes the variety of data types ex-
plored and collected in the shared task. Of the 24
participating teams, most did not exceed the $100
that they were awarded by a significant amount.
Therefore, the variety and extent of data described in
this section is the result of a minimal $2,400 invest-
ment. This achievement demonstrates the potential
for MTurk’s impact on the creation and curation of
speech and language corpora.
</bodyText>
<subsectionHeader confidence="0.924178">
6.1 Traditional NLP Tasks
</subsectionHeader>
<bodyText confidence="0.999940166666667">
An established core set of computational linguistic
tasks have received considerable attention in the nat-
ural language processing community. These include
knowledge extraction, textual entailment and word
sense disambiguation. Each of these tasks requires a
large and carefully curated annotated corpus to train
and evaluate statistical models. Many of the shared
task teams attempted to create new corpora for these
tasks at substantially reduced costs using MTurk.
Parent and Eskenazi (2010) produce new corpora
for the task of word sense disambiguation. The
study used MTurk to create unique word definitions
for 50 words, which Turkers then also mapped onto
existing definitions. Sentences containing these 50
words were then assigned to unique definitions ac-
cording to word sense.
Madnani and Boyd-Graber (2010) measured the
concept of transitivity of verbs in the style of Hop-
per and Thompson (1980), a theory that goes beyond
simple grammatical transitivity – whether verbs take
objects (transitive) or not – to capture the amount of
action indicated by a sentence. Videos that portrayed
verbs were shown to Turkers who described the ac-
tions shown in the video. Additionally, sentences
containing the verbs were rated for aspect, affirma-
tion, benefit, harm, kinesis, punctuality, and volition.
The authors investigated several approaches for elic-
iting descriptions of transitivity from Turkers.
Two teams explored textual entailment tasks.
Wang and Callison-Burch (2010) created data for
</bodyText>
<footnote confidence="0.9978915">
7http://sites.google.com/site/
amtworkshop2010/
</footnote>
<page confidence="0.991134">
5
</page>
<bodyText confidence="0.999972379310345">
recognizing textual entailment (RTE). They submit-
ted 600 text segments and asked Turkers to identify
facts and counter-facts (unsupported facts and con-
tradictions) given the provided text. The resulting
collection includes 790 facts and 203 counter-facts.
Negri and Mehdad (2010) created a bi-lingual en-
tailment corpus using English and Spanish entail-
ment pairs, where the hypothesis and text come from
different languages. The authors took a publicly
available English RTE data set (the PASCAL-RTE3
dataset1) and created an English-Spanish equivalent
by having Turkers translating the hypotheses into
Spanish. The authors include a timeline of their
progress, complete with total cost over the 10 days
that they ran the experiments.
In the area of natural language generation, Heil-
man and Smith (2010) explored the potential of
MTurk for ranking of computer generated questions
about provided texts. These questions can be used to
test reading comprehension and understanding. 60
Wikipedia articles were selected, for each of which
20 questions were generated. Turkers provided 5 rat-
ings for each of the 1,200 questions, creating a sig-
nificant corpus of scored questions.
Finally, Gordon et al. (2010) relied on MTurk to
evaluate the quality and accuracy of automatically
extracted common sense knowledge (factoids) from
news and Wikipedia articles. Factoids were pro-
vided by the KNEXT knowledge extraction system.
</bodyText>
<subsectionHeader confidence="0.998975">
6.2 Speech and Vision
</subsectionHeader>
<bodyText confidence="0.999941945945946">
While MTurk naturally lends itself to text tasks,
several teams explored annotation and collection of
speech and image data. We note that one of the pa-
pers in the main track described tools for collecting
such data (Lane et al., 2010).
Two teams used MTurk to collect text annotations
on speech data. Marge et al. (2010b) identified easy
and hard sections of meeting speech to transcribe
and focused data collection on difficult segments.
Transcripts were collected on 48 audio clips from
4 different speakers, as well as other types of an-
notations. Kunath and Weinberger (2010) collected
ratings of accented English speech, in which non-
native speakers were rated as either Arabic, Man-
darin or Russian native speakers. The authors ob-
tained multiple annotations for each speech sample,
and tracked the native language of each annotator,
allowing for an analysis of rating accuracy between
native English and non-native English annotators.
Novotney and Callison-Burch (2010b) used
MTurk to elicit new speech samples. As part of an
effort to increase the accessibility of public knowl-
edge, such as Wikipedia, the team prompted Turkers
to narrate Wikipedia articles. This required Turkers
to record audio files and upload them. An additional
HIT was used to evaluate the quality of the narra-
tions.
A particularly creative data collection approach
asked Turkers to create handwriting samples and
then to submit images of their writing (Tong et al.,
2010). Turkers were asked to submit handwritten
shopping lists (large vocabulary) or weather descrip-
tions (small vocabulary) in either Arabic or Spanish.
Subsequent Turkers provided a transcription and a
translation. The team collected 18 images per lan-
guage, 2 transcripts per image and 1 translation per
transcript.
</bodyText>
<subsectionHeader confidence="0.999355">
6.3 Sentiment, Polarity and Bias
</subsectionHeader>
<bodyText confidence="0.99999775">
Two papers investigated the topics of sentiment, po-
larity and bias. Mellebeek et al. (2010) used several
methods to obtain polarity scores for Spanish sen-
tences expressing opinions about automative topics.
They evaluated three HITs for collecting such data
and compared results for quality and expressiveness.
Yano et al. (2010) evaluated the political bias of blog
posts. Annotators labeled 1000 sentences to deter-
mine biased phrases in political blogs from the 2008
election season. Knowledge of the annotators own
biases allowed the authors to study how bias differs
on the different ends of the political spectrum.
</bodyText>
<subsectionHeader confidence="0.96912">
6.4 Information Retrieval
</subsectionHeader>
<bodyText confidence="0.9995736">
Large scale evaluations requiring significant human
labor for evaluation have a long history in the in-
formation retrieval community (TREC). Grady and
Lease (2010) study four factors that influence Turker
performance on a document relevance search task.
The authors present some negative results on how
these factors influence data collection. For further
work on MTurk and information retrieval, readers
are encouraged to see the SIGIR 2010 Workshop on
Crowdsourcing for Search Evaluation.8
</bodyText>
<footnote confidence="0.984313">
8http://www.ischool.utexas.edu/˜cse2010/
call.htm
</footnote>
<page confidence="0.997559">
6
</page>
<subsectionHeader confidence="0.853052">
6.5 Information Extraction
</subsectionHeader>
<bodyText confidence="0.9999412">
Information extraction (IE) seeks to identify specific
types of information in natural languages. The IE
papers in the shared tasks focused on new domains
and genres as well as new relation types.
The goal of relation extraction is to identify rela-
tions between entities or terms in a sentence, such as
born in or religion. Gormley et al. (2010) automat-
ically generate potential relation pairs in sentences
by finding relation pairs appearing in news articles
as given by a knowledge base. They ask Turkers if
a sentence supports a relation, does not support a re-
lation, or whether the relation makes sense. They
collected close to 2500 annotations for 17 different
person relation types.
The other IE papers explored new genres and do-
mains. Finin et al. (2010) obtained named entity an-
notations (person, organization, geopolitical entity)
for several hundred Twitter messages. They con-
ducted experiments using both MTurk and Crowd-
Flower. Yetisgen-Yildiz et al. (2010) explored
medical named entity recognition. They selected
100 clinical trial announcements from ClinicalTri-
als.gov. 4 annotators for each of the 100 announce-
ments identified 3 types of medical entities: medical
conditions, medications, and laboratory test.
</bodyText>
<subsectionHeader confidence="0.972892">
6.6 Machine Translation
</subsectionHeader>
<bodyText confidence="0.999985866666667">
The most popular shared task topic was Machine
Translation (MT). MT is a data hungry task that re-
lies on huge corpora of parallel texts between two
languages. Performance of MT systems depends
on the size of training corpora, so there is a con-
stant search for new and larger data sets. Such data
sets are traditionally expensive to produce, requiring
skilled translators. One of the advantages to MTurk
is the diversity of the Turker population, making it
an especially attractive source of MT data. Shared
task papers in MT explored the full range of MT
tasks, including alignments, parallel corpus creation,
paraphrases and bilingual lexicons.
Gao and Vogel (2010) create alignments in a 300
sentence Chinese-English corpus (Chinese aligned
to English). Both Ambati and Vogel (2010) and
Bloodgood and Callison-Burch (2010) explore the
potential of MTurk in the creation of MT paral-
lel corpora for evaluation and training. Bloodgood
and Callison-Burch replicate the NIST 2009 Urdu-
English test set of 1792 sentences, paying only $0.10
a sentence, a substantially reduced price than the
typical annotator cost. The result is a data set that is
still effective for comparing MT systems in an eval-
uation. Ambati and Vogel create corpora with 100
sentences and 3 translations per sentence for all the
language pairs between English, Spanish, Urdu and
Telugu. This demonstrates the feasibility of creating
cheap corpora for high and low resource languages.
Two papers focused on the creation and evalua-
tion of paraphrases. Denkowski et al. (2010) gen-
erated and evaluated 728 paraphrases for Arabic-
English translation. MTurk was used to identify
correct and fix incorrect paraphrases. Over 1200
high quality paraphrases were created. Buzek et
al. (2010) evaluated error driven paraphrases for
MT. In this setting, paraphrases are used to sim-
plify potentially difficult to translate segments of
text. Turkers identified 1780 error regions in 1006
English/Chinese sentences. Turkers provided 4821
paraphrases for these regions.
External resources can be an important part of an
MT system. Irvine and Klementiev (2010) created
lexicons for low resource languages. They evaluated
translation candidates for 100 English words in 32
languages and solicited translations for 10 additional
languages. Higgins et al. (2010) expanded name
lists in Arabic by soliciting common Arabic nick-
names. The 332 collected nicknames were primar-
ily provided by Turkers in Arab speaking countries
(35%), India (46%), and the United States (13%).
Finally, Zaidan and Ganitkevitch (2010) explored
how MTurk could be used to directly improve an MT
grammar. Each rule in an Urdu to English transla-
tion system was characterized by 12 features. Turk-
ers were provided examples for which their feed-
back was used to rescore grammar productions di-
rectly. This approach shows the potential of fine
tuning an MT system with targeted feedback from
annotators.
</bodyText>
<sectionHeader confidence="0.99688" genericHeader="discussions">
7 Future Directions
</sectionHeader>
<bodyText confidence="0.99961075">
Looking ahead, we can’t help but wonder what im-
pact MTurk and crowdsourcing will have on the
speech and language research community. Keep-
ing in mind Niels Bohr’s famous exhortation “Pre-
</bodyText>
<page confidence="0.998772">
7
</page>
<bodyText confidence="0.9999775">
diction is very difficult, especially if it’s about the
future,” we attempt to draw some conclusions and
predict future directions and impact on the field.
Some have predicted that access to low cost,
highly scalable methods for creating language and
speech annotations means the end of work on un-
supervised learning. Many a researcher has advo-
cated his or her unsupervised learning approach be-
cause of annotation costs. However, if 100 exam-
ples for any task are obtainable for less than $100,
why spend the time and effort developing often infe-
rior unsupervised methods? Such a radical change is
highly debatable, in fact, one of this paper’s authors
is a strong advocate of such a position while the
other disagrees, perhaps because he himself works
on unsupervised methods. Certainly, we can agree
that the potential exists for a change in focus in a
number of ways.
In natural language processing, data drives re-
search. The introduction of new large and widely
accessible data sets creates whole new areas of re-
search. There are many examples of such impact,
the most famous of which is the Penn Treebank
(Marcus. et al., 1994), which has 2910 citations in
Google scholar and is the single most cited paper
on the ACL anthology network (Radev et al., 2009).
Other examples include the CoNLL named entity
corpus (Sang and Meulder (2003) with 348 citations
on Google Scholar), the IMDB movie reviews senti-
ment data (Pang et al. (2002) with 894 citations) and
the Amazon sentiment multi-domain data (Blitzer et
al. (2007) with 109 citations) . MTurk means that
creating similar data sets is now much cheaper and
easier than ever before. It is highly likely that new
MTurk produced data sets will achieve prominence
and have significant impact. Additionally, the cre-
ation of shared data means more comparison and
evaluation against previous work. Progress is made
when it can be demonstrated against previous ap-
proaches on the same data. The reduction of data
cost and the rise of independent corpus producers
likely means more accessible data.
More than a new source for cheap data, MTurk is
a source for new types of data. Several of the pa-
pers in this workshop collected information about
the annotators in addition to their annotations. This
creates potential for studying how different user de-
mographics understand language and allow for tar-
geting specific demographics in data creation. Be-
yond efficiencies in cost, MTurk provides access to
a global user population far more diverse than those
provided by more professional annotation settings.
This will have a significant impact on low resource
languages as corpora can be cheaply built for a much
wider array of languages. As one example, Irvine
and Klementiev (2010) collected data for 42 lan-
guages without worrying about how to find speak-
ers of such a wide variety of languages. Addition-
ally, the collection of Arabic nicknames requires a
diverse and numerous Arabic speaking population
(Higgins et al., 2010). In addition to extending into
new languages, MTurk also allows for the creation
of evaluation sets in new genres and domains, which
was the focus of two papers in this workshop (Finin
et al., 2010; Yetisgen-Yildiz et al., 2010). We ex-
pect to see new research emphasis on low resource
languages and new domains and genres.
Another factor is the change of data type and its
impact on machine learning algorithms. With pro-
fessional annotators, great time and care are paid to
annotation guidelines and annotator training. These
are difficult tasks with MTurk, which favors simple
intuitive annotations and little training. Many papers
applied creative methods of using simpler annota-
tion tasks to create more complex data sets. This
process can impact machine learning in a number
of ways. Rather than a single gold standard, anno-
tations are now available for many users. Learn-
ing across multiple annotations may improve sys-
tems (Dredze et al., 2009). Additionally, even with
efforts to clean up MTurk annotations, we can ex-
pect an increase in noisy examples in data. This will
push for new more robust learning algorithms that
are less sensitive to noise. If we increase the size
of the data ten-fold but also increase the noise, can
learning still be successful? Another learning area
of great interest is active learning, which has long
relied on simulated user experiments. New work
evaluated active learning methods with real users us-
ing MTurk (Baker et al., 2009; Ambati et al., 2010;
Hsueh et al., 2009; ?). Finally, the composition of
complex data set annotations from simple user in-
puts can transform the method by which we learn
complex outputs. Current approaches expect exam-
ples of labels that exactly match the expectation of
the system. Can we instead provide lower level sim-
</bodyText>
<page confidence="0.995266">
8
</page>
<bodyText confidence="0.999874666666666">
pler user annotations and teach systems how to learn
from these to construct complex output? This would
open more complex annotation tasks to MTurk.
A general trend in research is that good ideas
come from unexpected places. Major transforma-
tions in the field have come from creative new ap-
proaches. Consider the Penn Treebank, an ambitious
and difficult project of unknown potential. Such
large changes can be uncommon since they are often
associated with high cost, as was the Penn Treebank.
However, MTurk greatly reduces these costs, en-
couraging researchers to try creative new tasks. For
example, in this workshop Tong et al. (2010) col-
lected handwriting samples in multiple languages.
Their creative data collection may or may not have
a significant impact, but it is unlikely that it would
have been tried had the cost been very high.
Finally, while obtaining new data annotations
from MTurk is cheap, it is not trivial. Workshop par-
ticipants struggled with how to attract Turkers, how
to price HITs, HIT design, instructions, cheating de-
tection, etc. No doubt that as work progresses, so
will a communal knowledge and experience of how
to use MTurk. There can be great benefit in new
toolkits for collecting language data using MTurk,
and indeed some of these have already started to
emerge (Lane et al., 2010)9.
</bodyText>
<sectionHeader confidence="0.974584" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998569">
Thanks to Sharon Chiarella of Amazon’s Mechan-
ical Turk for providing $100 credits for the shared
task, and to CrowdFlower for allowing free use of
their tool to workshop participants.
Research funding was provided by the NSF un-
der grant IIS-0713448, by the European Commis-
sion through the EuroMatrixPlus project, and by
the DARPA GALE program under Contract No.
HR0011-06-2-0001. The views and findings are the
authors’ alone.
</bodyText>
<sectionHeader confidence="0.917331" genericHeader="references">
References
</sectionHeader>
<footnote confidence="0.748531">
Cem Akkaya, Alexander Conrad, Janyce Wiebe, and
Rada Mihalcea. 2010. Amazon Mechanical Turk for
subjectivity word sense disambiguation. In NAACL
9http://wiki.github.com/callison-burch/
mechanical_turk_workshop/
</footnote>
<note confidence="0.575182166666667">
Workshop on Creating Speech and Language Data
With Amazon’s Mechanical Turk.
Vamshi Ambati and Stephan Vogel. 2010. Can crowds
build parallel corpora for machine translation systems?
In NAACL Workshop on Creating Speech and Lan-
guage Data With Amazon’s Mechanical Turk.
</note>
<reference confidence="0.983876553191489">
Vamshi Ambati, Stephan Vogel, and Jamie Carbonell.
2010. Active learning and crowd-sourcing for ma-
chine translation. Language Resources and Evalua-
tion (LREC).
Kathy Baker, Steven Bethard, Michael Bloodgood, Ralf
Brown, Chris Callison-Burch, Glen Coppersmith,
Bonnie Dorr, Wes Filardo, Kendall Giles, Ann Irvine,
Mike Kayser, Lori Levin, Justin Martineau, Jim May-
field, Scott Miller, Aaron Phillips, Andrew Philpot,
Christine Piatko, Lane Schwartz, and David Zajic.
2009. Semantically-informed machine translation.
Technical Report 002, Johns Hopkins Human Lan-
guage Technology Center of Excellence, Summer
Camp for Applied Language Exploration, Johns Hop-
kins University, Baltimore, MD.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In As-
sociation for Computational Linguistics (ACL).
Michael Bloodgood and Chris Callison-Burch. 2010a.
Bucking the trend: Large-scale cost-focused active
learning for statistical machine translation. In 48th
Annual Meeting of the Association for Computational
Linguistics, Uppsala, Sweden.
Michael Bloodgood and Chris Callison-Burch. 2010b.
Using Mechanical Turk to build machine translation
evaluation sets. In NAACL Workshop on Creating
Speech and Language Data With Amazon’s Mechan-
ical Turk.
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Diar-
muid O´ S´eaghdha, Stan Szpakowicz, and Tony Veale.
2009. Semeval-2010 Task 9: The interpretation of
noun compounds using paraphrasing verbs and prepo-
sitions. In Workshop on Semantic Evaluations.
Olivia Buzek, Philip Resnik, and Ben Bederson. 2010.
Error driven paraphrase annotation using Mechanical
Turk. In NAACL Workshop on Creating Speech and
Language Data With Amazon’s Mechanical Turk.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using amazon’s mechan-
ical turk. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2009), Singapore.
Jonathan Chang, Jordan Boyd-Graber, Chong Wang,
Sean Gerrish, and David M. Blei. 2009. Reading tea
leaves: How humans interpret topic models. In Neural
Information Processing Systems.
</reference>
<page confidence="0.978262">
9
</page>
<reference confidence="0.998817212962963">
Jonathan Chang. 2010. Not-so-Latent Dirichlet Allo-
cation: Collapsed Gibbs sampling using human judg-
ments. In NAACL Workshop on Creating Speech and
Language Data With Amazon’s Mechanical Turk.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. Imagenet: A large-scale hier-
archical image database. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recogni-
tion (CVPR, Miami Beach, Floriday.
Michael Denkowski and Alon Lavie. 2010. Explor-
ing normalization techniques for human judgments of
machine translation adequacy collected using Amazon
Mechanical Turk. In NAACL Workshop on Creating
Speech and Language Data With Amazon’s Mechani-
cal Turk.
Michael Denkowski, Hassan Al-Haj, and Alon Lavie.
2010. Turker-assisted paraphrasing for English-
Arabic machine translation. In NAACL Workshop on
Creating Speech and Language Data With Amazon’s
Mechanical Turk.
Mark Dredze, Partha Pratim Talukdar, and Koby Cram-
mer. 2009. Sequence learning from data with multiple
labels. In ECML/PKDD Workshop on Learning from
Multi-Label Data (MLD).
Keelan Evanini, Derrick Higgins, and Klaus Zechner.
2010. Using Amazon Mechanical Turk for transcrip-
tion of non-native speech. In NAACL Workshop on
Creating Speech and Language Data With Amazon’s
Mechanical Turk.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Tim Finin, William Murnane, Anand Karandikar,
Nicholas Keller, Justin Martineau, and Mark Dredze.
2010. Touring PER, ORG and LOC on $100 a day. In
NAACL Workshop on Creating Speech and Language
Data With Amazon’s Mechanical Turk.
Qin Gao and Stephan Vogel. 2010. Semi-supervised
word alignment with Mechanical Turk. In NAACL
Workshop on Creating Speech and Language Data
With Amazon’s Mechanical Turk.
Dan Gillick and Yang Liu. 2010. Non-expert evaluation
of summarization systems is risky. In NAACL Work-
shop on Creating Speech and Language Data With
Amazon’s Mechanical Turk.
Jonathan Gordon, Benjamin Van Durme, and Lenhart
Schubert. 2010. Evaluation of commonsense knowl-
edge with Mechanical Turk. In NAACL Workshop on
Creating Speech and Language Data With Amazon’s
Mechanical Turk.
Matthew R. Gormley, Adam Gerber, Mary Harper, and
Mark Dredze. 2010. Non-expert correction of auto-
matically generated relation annotations. In NAACL
Workshop on Creating Speech and Language Data
With Amazon’s Mechanical Turk.
Catherine Grady and Matthew Lease. 2010. Crowd-
sourcing document relevance assessment with Me-
chanical Turk. In NAACL Workshop on Creating
Speech and Language Data With Amazon’s Mechan-
ical Turk.
Michael Heilman and Noah A. Smith. 2010. Rating
computer-generated questions with Mechanical Turk.
In NAACL Workshop on Creating Speech and Lan-
guage Data With Amazon’s Mechanical Turk.
Chiara Higgins, Elizabeth McGrath, and Laila Moretto.
2010. AMT crowdsourcing: A viable method for rapid
discovery of Arabic nicknames? In NAACL Workshop
on Creating Speech and Language Data With Ama-
zon’s Mechanical Turk.
Paul J. Hopper and Sandra A. Thompson. 1980. Transi-
tivity in grammar and discourse. Language, 56:251–
299.
Pei-Yun Hsueh, Prem Melville, and Vikas Sindhwani.
2009. Data quality from crowdsourcing: A study of
annotation selection criteria. In Proceedings of the
NAACL HLT 2009 Workshop on Active Learning for
Natural Language Processing, pages 27–35, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
Panos Ipeirotis. 2010. New demographics of Mechanical
Turk. http://behind-the-enemy-lines.
blogspot.com/2010/03/
new-demographics-of-mechanical-turk.
html.
Ann Irvine and Alexandre Klementiev. 2010. Using Me-
chanical Turk to annotate lexicons for less commonly
used languages. In NAACL Workshop on Creating
Speech and Language Data With Amazon’s Mechan-
ical Turk.
Mukund Jha, Jacob Andreas, Kapil Thadani, Sara Rosen-
thal, and Kathleen McKeown. 2010. Corpus creation
for new genres: A crowdsourced approach to PP at-
tachment. In NAACL Workshop on Creating Speech
and Language Data With Amazon’s Mechanical Turk.
Michael Kaisser and John Lowe. 2008. Creating a re-
search collection of question answer sentence pairs
with Amazons Mechanical Turk. In Proceedings of
the Sixth International Language Resources and Eval-
uation (LREC’08), Marrakech, Morocco.
Stephen Kunath and Steven Weinberger. 2010. The wis-
dom of the crowd’s ear: Speech accent rating and an-
notation with Amazon Mechanical Turk. In NAACL
Workshop on Creating Speech and Language Data
With Amazon’s Mechanical Turk.
Ian Lane, Matthias Eck, Kay Rottmann, and Alex
Waibel. 2010. Tools for collecting speech corpora via
Mechanical-Turk. In NAACL Workshop on Creating
Speech and Language Data With Amazon’s Mechani-
cal Turk.
</reference>
<page confidence="0.949803">
10
</page>
<reference confidence="0.999439254716981">
Nolan Lawson, Kevin Eustice, Mike Perkowitz, and
Meliha Yetisgen Yildiz. 2010. Annotating large email
datasets for named entity recognition with Mechanical
Turk. In NAACL Workshop on Creating Speech and
Language Data With Amazon’s Mechanical Turk.
Greg Little, Lydia B. Chilton, Rob Miller, and Max Gold-
man. 2009. Turkit: Tools for iterative tasks on me-
chanical turk. In Proceedings of the Workshop on
Human Computation at the International Conference
on Knowledge Discovery and Data Mining (KDD-
HCOMP ’09), Paris.
Nitin Madnani and Jordan Boyd-Graber. 2010. Measur-
ing transitivity using untrained annotators. In NAACL
Workshop on Creating Speech and Language Data
With Amazon’s Mechanical Turk.
Mitch Marcus., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of english: The penn treebank. Computational lin-
guistics, 19(2):313–330.
Matthew Marge, Satanjeev Banerjee, and Alexander
Rudnicky. 2010a. Using the Amazon Mechanical
Turk for transcription of spoken language. ICASSP,
March.
Matthew Marge, Satanjeev Banerjee, and Alexander
Rudnicky. 2010b. Using the Amazon Mechanical
Turk to transcribe and annotate meeting speech for ex-
tractive summarization. In NAACL Workshop on Cre-
ating Speech and Language Data With Amazon’s Me-
chanical Turk.
Winter Mason and Duncan J. Watts. 2009. Financial
incentives and the “performance of crowds”. In Pro-
ceedings of the Workshop on Human Computation at
the International Conference on Knowledge Discovery
and Data Mining (KDD-HCOMP ’09), Paris.
Ian McGraw, Chia ying Lee, Lee Hetherington, and Jim
Glass. 2010. Collecting voices from the crowd.
LREC, May.
Bart Mellebeek, Francesc Benavent, Jens Grivolla, Joan
Codina, Marta R. Costa-Juss`a, and Rafael Banchs.
2010. Opinion mining of spanish customer comments
with non-expert annotations on Mechanical Turk. In
NAACL Workshop on Creating Speech and Language
Data With Amazon’s Mechanical Turk.
Robert Munro, Steven Bethard, Victor Kuperman,
Vicky Tzuyin Lai, Robin Melnick, Christopher Potts,
Tyler Schnoebelen, and Harry Tily. 2010. Crowd-
sourcing and language studies: the new generation
of linguistic data. In NAACL Workshop on Creating
Speech and Language Data With Amazon’s Mechani-
cal Turk.
Matteo Negri and Yashar Mehdad. 2010. Creating a
bi-lingual entailment corpus through translations with
Mechanical Turk: $100 for a 10 days rush. In NAACL
Workshop on Creating Speech and Language Data
With Amazon’s Mechanical Turk.
Scott Novotney and Chris Callison-Burch. 2010a.
Cheap, fast and good enough: Automatic speech
recognition with non-expert transcription. NAACL,
June.
Scott Novotney and Chris Callison-Burch. 2010b.
Crowdsourced accessibility: Elicitation of Wikipedia
articles. In NAACL Workshop on Creating Speech and
Language Data With Amazon’s Mechanical Turk.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Empirical Methods in
Natural Language Processing (EMNLP).
Gabriel Parent and Maxine Eskenazi. 2010. Cluster-
ing dictionary definitions using Amazon Mechanical
Turk. In NAACL Workshop on Creating Speech and
Language Data With Amazon’s Mechanical Turk.
Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed
Qazvinian. 2009. The acl anthology network. In Pro-
ceedings of the 2009 Workshop on Text and Citation
Analysis for Scholarly Digital Libraries, pages 54–61,
Suntec City, Singapore, August. Association for Com-
putational Linguistics.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and Julia
Hockenmaier. 2010. Collecting image annotations us-
ing Amazon’s Mechanical Turk. In NAACL Workshop
on Creating Speech and Language Data With Ama-
zon’s Mechanical Turk.
Joel Rose. 2010. Some turn to ‘Mechanical’ job search.
http://marketplace.publicradio.org/
display/web/2009/06/30/pm_turking/.
Marketplace public radio. Air date: June 30, 2009.
Erik Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the conll-2003 shared task: Language-
independent named entity recognition. In CoNLL-
2003.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast - but is it
good? Evaluating non-expert annotations for natural
language tasks. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP-2008), Honolulu, Hawaii.
Audrey Tong, Jerome Ajot, Mark Przybocki, and
Stephanie Strassel. 2010. Document image collection
using Amazon’s Mechanical Turk. In NAACL Work-
shop on Creating Speech and Language Data With
Amazon’s Mechanical Turk.
Luis von Ahn and Laura Dabbish. 2008. General tech-
niques for designing games with a purpose. Commu-
nications of the ACM.
Rui Wang and Chris Callison-Burch. 2010. Cheap facts
and counter-facts. In NAACL Workshop on Creating
</reference>
<page confidence="0.990375">
11
</page>
<reference confidence="0.997222947368421">
Speech and Language Data With Amazon’s Mechani-
cal Turk.
Tae Yano, Philip Resnik, and Noah A Smith. 2010.
Shedding (a thousand points of) light on biased lan-
guage. In NAACL Workshop on Creating Speech and
Language Data With Amazon’s Mechanical Turk.
Meliha Yetisgen-Yildiz, Imre Solti, Scott Halgrim, and
Fei Xia. 2010. Preliminary experiments with Ama-
zon’s Mechanical Turk for annotating medical named
entities. In NAACL Workshop on Creating Speech and
Language Data With Amazon’s Mechanical Turk.
Omar F. Zaidan and Chris Callison-Burch. 2009. Feasi-
bility of human-in-the-loop minimum error rate train-
ing. In Proceedings of EMNLP 2009, pages 52–61,
August.
Omar Zaidan and Juri Ganitkevitch. 2010. An enriched
MT grammar for under $100. In NAACL Workshop on
Creating Speech and Language Data With Amazon’s
Mechanical Turk.
</reference>
<page confidence="0.998436">
12
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.504104">
<title confidence="0.99630225">Creating Speech and Language Data With Amazon’s Mechanical Turk Callison-Burch Human Language Technology Center of &amp; Center for Language and Speech</title>
<author confidence="0.991147">Johns Hopkins</author>
<email confidence="0.999629">ccb,mdredze@cs.jhu.edu</email>
<abstract confidence="0.514426"></abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Vamshi Ambati</author>
<author>Stephan Vogel</author>
<author>Jamie Carbonell</author>
</authors>
<title>Active learning and crowd-sourcing for machine translation. Language Resources and Evaluation (LREC).</title>
<date>2010</date>
<contexts>
<context position="33724" citStr="Ambati et al., 2010" startWordPosition="5431" endWordPosition="5434">. Learning across multiple annotations may improve systems (Dredze et al., 2009). Additionally, even with efforts to clean up MTurk annotations, we can expect an increase in noisy examples in data. This will push for new more robust learning algorithms that are less sensitive to noise. If we increase the size of the data ten-fold but also increase the noise, can learning still be successful? Another learning area of great interest is active learning, which has long relied on simulated user experiments. New work evaluated active learning methods with real users using MTurk (Baker et al., 2009; Ambati et al., 2010; Hsueh et al., 2009; ?). Finally, the composition of complex data set annotations from simple user inputs can transform the method by which we learn complex outputs. Current approaches expect examples of labels that exactly match the expectation of the system. Can we instead provide lower level sim8 pler user annotations and teach systems how to learn from these to construct complex output? This would open more complex annotation tasks to MTurk. A general trend in research is that good ideas come from unexpected places. Major transformations in the field have come from creative new approaches</context>
</contexts>
<marker>Ambati, Vogel, Carbonell, 2010</marker>
<rawString>Vamshi Ambati, Stephan Vogel, and Jamie Carbonell. 2010. Active learning and crowd-sourcing for machine translation. Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kathy Baker</author>
<author>Steven Bethard</author>
<author>Michael Bloodgood</author>
<author>Ralf Brown</author>
<author>Chris Callison-Burch</author>
<author>Glen Coppersmith</author>
<author>Bonnie Dorr</author>
<author>Wes Filardo</author>
<author>Kendall Giles</author>
</authors>
<title>Semantically-informed machine translation.</title>
<date>2009</date>
<tech>Technical Report 002,</tech>
<institution>Johns Hopkins Human Language Technology Center of Excellence, Summer Camp for Applied Language Exploration, Johns Hopkins University,</institution>
<location>Ann Irvine, Mike Kayser, Lori Levin, Justin Martineau, Jim Mayfield, Scott Miller, Aaron Phillips, Andrew Philpot, Christine Piatko, Lane</location>
<contexts>
<context position="33703" citStr="Baker et al., 2009" startWordPosition="5427" endWordPosition="5430">lable for many users. Learning across multiple annotations may improve systems (Dredze et al., 2009). Additionally, even with efforts to clean up MTurk annotations, we can expect an increase in noisy examples in data. This will push for new more robust learning algorithms that are less sensitive to noise. If we increase the size of the data ten-fold but also increase the noise, can learning still be successful? Another learning area of great interest is active learning, which has long relied on simulated user experiments. New work evaluated active learning methods with real users using MTurk (Baker et al., 2009; Ambati et al., 2010; Hsueh et al., 2009; ?). Finally, the composition of complex data set annotations from simple user inputs can transform the method by which we learn complex outputs. Current approaches expect examples of labels that exactly match the expectation of the system. Can we instead provide lower level sim8 pler user annotations and teach systems how to learn from these to construct complex output? This would open more complex annotation tasks to MTurk. A general trend in research is that good ideas come from unexpected places. Major transformations in the field have come from cr</context>
</contexts>
<marker>Baker, Bethard, Bloodgood, Brown, Callison-Burch, Coppersmith, Dorr, Filardo, Giles, 2009</marker>
<rawString>Kathy Baker, Steven Bethard, Michael Bloodgood, Ralf Brown, Chris Callison-Burch, Glen Coppersmith, Bonnie Dorr, Wes Filardo, Kendall Giles, Ann Irvine, Mike Kayser, Lori Levin, Justin Martineau, Jim Mayfield, Scott Miller, Aaron Phillips, Andrew Philpot, Christine Piatko, Lane Schwartz, and David Zajic. 2009. Semantically-informed machine translation. Technical Report 002, Johns Hopkins Human Language Technology Center of Excellence, Summer Camp for Applied Language Exploration, Johns Hopkins University, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="30788" citStr="Blitzer et al. (2007)" startWordPosition="4948" endWordPosition="4951"> data drives research. The introduction of new large and widely accessible data sets creates whole new areas of research. There are many examples of such impact, the most famous of which is the Penn Treebank (Marcus. et al., 1994), which has 2910 citations in Google scholar and is the single most cited paper on the ACL anthology network (Radev et al., 2009). Other examples include the CoNLL named entity corpus (Sang and Meulder (2003) with 348 citations on Google Scholar), the IMDB movie reviews sentiment data (Pang et al. (2002) with 894 citations) and the Amazon sentiment multi-domain data (Blitzer et al. (2007) with 109 citations) . MTurk means that creating similar data sets is now much cheaper and easier than ever before. It is highly likely that new MTurk produced data sets will achieve prominence and have significant impact. Additionally, the creation of shared data means more comparison and evaluation against previous work. Progress is made when it can be demonstrated against previous approaches on the same data. The reduction of data cost and the rise of independent corpus producers likely means more accessible data. More than a new source for cheap data, MTurk is a source for new types of dat</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Bloodgood</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Bucking the trend: Large-scale cost-focused active learning for statistical machine translation.</title>
<date>2010</date>
<booktitle>In 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="26965" citStr="Bloodgood and Callison-Burch (2010)" startWordPosition="4330" endWordPosition="4333">nds on the size of training corpora, so there is a constant search for new and larger data sets. Such data sets are traditionally expensive to produce, requiring skilled translators. One of the advantages to MTurk is the diversity of the Turker population, making it an especially attractive source of MT data. Shared task papers in MT explored the full range of MT tasks, including alignments, parallel corpus creation, paraphrases and bilingual lexicons. Gao and Vogel (2010) create alignments in a 300 sentence Chinese-English corpus (Chinese aligned to English). Both Ambati and Vogel (2010) and Bloodgood and Callison-Burch (2010) explore the potential of MTurk in the creation of MT parallel corpora for evaluation and training. Bloodgood and Callison-Burch replicate the NIST 2009 UrduEnglish test set of 1792 sentences, paying only $0.10 a sentence, a substantially reduced price than the typical annotator cost. The result is a data set that is still effective for comparing MT systems in an evaluation. Ambati and Vogel create corpora with 100 sentences and 3 translations per sentence for all the language pairs between English, Spanish, Urdu and Telugu. This demonstrates the feasibility of creating cheap corpora for high </context>
</contexts>
<marker>Bloodgood, Callison-Burch, 2010</marker>
<rawString>Michael Bloodgood and Chris Callison-Burch. 2010a. Bucking the trend: Large-scale cost-focused active learning for statistical machine translation. In 48th Annual Meeting of the Association for Computational Linguistics, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Bloodgood</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Using Mechanical Turk to build machine translation evaluation sets.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="26965" citStr="Bloodgood and Callison-Burch (2010)" startWordPosition="4330" endWordPosition="4333">nds on the size of training corpora, so there is a constant search for new and larger data sets. Such data sets are traditionally expensive to produce, requiring skilled translators. One of the advantages to MTurk is the diversity of the Turker population, making it an especially attractive source of MT data. Shared task papers in MT explored the full range of MT tasks, including alignments, parallel corpus creation, paraphrases and bilingual lexicons. Gao and Vogel (2010) create alignments in a 300 sentence Chinese-English corpus (Chinese aligned to English). Both Ambati and Vogel (2010) and Bloodgood and Callison-Burch (2010) explore the potential of MTurk in the creation of MT parallel corpora for evaluation and training. Bloodgood and Callison-Burch replicate the NIST 2009 UrduEnglish test set of 1792 sentences, paying only $0.10 a sentence, a substantially reduced price than the typical annotator cost. The result is a data set that is still effective for comparing MT systems in an evaluation. Ambati and Vogel create corpora with 100 sentences and 3 translations per sentence for all the language pairs between English, Spanish, Urdu and Telugu. This demonstrates the feasibility of creating cheap corpora for high </context>
</contexts>
<marker>Bloodgood, Callison-Burch, 2010</marker>
<rawString>Michael Bloodgood and Chris Callison-Burch. 2010b. Using Mechanical Turk to build machine translation evaluation sets. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristina Butnariu</author>
<author>Su Nam Kim</author>
<author>Preslav Nakov</author>
<author>Diarmuid O´ S´eaghdha</author>
<author>Stan Szpakowicz</author>
<author>Tony Veale</author>
</authors>
<title>Semeval-2010 Task 9: The interpretation of noun compounds using paraphrasing verbs and prepositions.</title>
<date>2009</date>
<booktitle>In Workshop on Semantic Evaluations.</booktitle>
<marker>Butnariu, Kim, Nakov, S´eaghdha, Szpakowicz, Veale, 2009</marker>
<rawString>Cristina Butnariu, Su Nam Kim, Preslav Nakov, Diarmuid O´ S´eaghdha, Stan Szpakowicz, and Tony Veale. 2009. Semeval-2010 Task 9: The interpretation of noun compounds using paraphrasing verbs and prepositions. In Workshop on Semantic Evaluations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivia Buzek</author>
<author>Philip Resnik</author>
<author>Ben Bederson</author>
</authors>
<title>Error driven paraphrase annotation using Mechanical Turk.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="27888" citStr="Buzek et al. (2010)" startWordPosition="4477" endWordPosition="4480">ta set that is still effective for comparing MT systems in an evaluation. Ambati and Vogel create corpora with 100 sentences and 3 translations per sentence for all the language pairs between English, Spanish, Urdu and Telugu. This demonstrates the feasibility of creating cheap corpora for high and low resource languages. Two papers focused on the creation and evaluation of paraphrases. Denkowski et al. (2010) generated and evaluated 728 paraphrases for ArabicEnglish translation. MTurk was used to identify correct and fix incorrect paraphrases. Over 1200 high quality paraphrases were created. Buzek et al. (2010) evaluated error driven paraphrases for MT. In this setting, paraphrases are used to simplify potentially difficult to translate segments of text. Turkers identified 1780 error regions in 1006 English/Chinese sentences. Turkers provided 4821 paraphrases for these regions. External resources can be an important part of an MT system. Irvine and Klementiev (2010) created lexicons for low resource languages. They evaluated translation candidates for 100 English words in 32 languages and solicited translations for 10 additional languages. Higgins et al. (2010) expanded name lists in Arabic by solic</context>
</contexts>
<marker>Buzek, Resnik, Bederson, 2010</marker>
<rawString>Olivia Buzek, Philip Resnik, and Ben Bederson. 2010. Error driven paraphrase annotation using Mechanical Turk. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Fast, cheap, and creative: Evaluating translation quality using amazon’s mechanical turk.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP-2009),</booktitle>
<contexts>
<context position="16823" citStr="Callison-Burch, 2009" startWordPosition="2777" endWordPosition="2778">ld standard data, or by stating what controls you used and what criteria you used to block bad Turkers. Finally, whenever possible you should publish the data that you generate on Mechanical Turk (and your analysis scripts and HIT templates) alongside your paper so that other people can verify it. 5 Related work In the past two years, several papers have published about applying Mechanical Turk to a diverse set of natural language processing tasks, including: creating question-answer sentence pairs (Kaisser and Lowe, 2008), evaluating machine translation quality and crowdsouring translations (Callison-Burch, 2009), paraphrasing noun-noun compouds for SemEval (Butnariu et al., 2009), human evaluation of topic models (Chang et al., 2009), and speech transcription (McGraw et al., 2010; Marge et al., 2010a; Novotney and Callison-Burch, 2010a). Others have used MTurk for novel research directions like nonsimulated active learning for NLP tasks such as sentiment classification (Hsueh et al., 2009) or doing quixotic things like doing human-in-the-loop minimum error rate training for machine translation (Zaidan and Callison-Burch, 2009). Some projects have demonstrated the superscalability of crowdsourced effo</context>
</contexts>
<marker>Callison-Burch, 2009</marker>
<rawString>Chris Callison-Burch. 2009. Fast, cheap, and creative: Evaluating translation quality using amazon’s mechanical turk. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP-2009), Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Chang</author>
<author>Jordan Boyd-Graber</author>
<author>Chong Wang</author>
<author>Sean Gerrish</author>
<author>David M Blei</author>
</authors>
<title>Reading tea leaves: How humans interpret topic models.</title>
<date>2009</date>
<booktitle>In Neural Information Processing Systems.</booktitle>
<contexts>
<context position="16947" citStr="Chang et al., 2009" startWordPosition="2794" endWordPosition="2797">ible you should publish the data that you generate on Mechanical Turk (and your analysis scripts and HIT templates) alongside your paper so that other people can verify it. 5 Related work In the past two years, several papers have published about applying Mechanical Turk to a diverse set of natural language processing tasks, including: creating question-answer sentence pairs (Kaisser and Lowe, 2008), evaluating machine translation quality and crowdsouring translations (Callison-Burch, 2009), paraphrasing noun-noun compouds for SemEval (Butnariu et al., 2009), human evaluation of topic models (Chang et al., 2009), and speech transcription (McGraw et al., 2010; Marge et al., 2010a; Novotney and Callison-Burch, 2010a). Others have used MTurk for novel research directions like nonsimulated active learning for NLP tasks such as sentiment classification (Hsueh et al., 2009) or doing quixotic things like doing human-in-the-loop minimum error rate training for machine translation (Zaidan and Callison-Burch, 2009). Some projects have demonstrated the superscalability of crowdsourced efforts. Deng et al. (2009) used MTurk to construct ImageNet, an annotated image database containing 3.2 million that are hierar</context>
</contexts>
<marker>Chang, Boyd-Graber, Wang, Gerrish, Blei, 2009</marker>
<rawString>Jonathan Chang, Jordan Boyd-Graber, Chong Wang, Sean Gerrish, and David M. Blei. 2009. Reading tea leaves: How humans interpret topic models. In Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Chang</author>
</authors>
<title>Not-so-Latent Dirichlet Allocation: Collapsed Gibbs sampling using human judgments.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<marker>Chang, 2010</marker>
<rawString>Jonathan Chang. 2010. Not-so-Latent Dirichlet Allocation: Collapsed Gibbs sampling using human judgments. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Deng</author>
<author>Wei Dong</author>
<author>Richard Socher</author>
<author>Li-Jia Li</author>
<author>Kai Li</author>
<author>Li Fei-Fei</author>
</authors>
<title>Imagenet: A large-scale hierarchical image database.</title>
<date>2009</date>
<booktitle>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR,</booktitle>
<location>Miami Beach, Floriday.</location>
<contexts>
<context position="17446" citStr="Deng et al. (2009)" startWordPosition="2868" endWordPosition="2871">aphrasing noun-noun compouds for SemEval (Butnariu et al., 2009), human evaluation of topic models (Chang et al., 2009), and speech transcription (McGraw et al., 2010; Marge et al., 2010a; Novotney and Callison-Burch, 2010a). Others have used MTurk for novel research directions like nonsimulated active learning for NLP tasks such as sentiment classification (Hsueh et al., 2009) or doing quixotic things like doing human-in-the-loop minimum error rate training for machine translation (Zaidan and Callison-Burch, 2009). Some projects have demonstrated the superscalability of crowdsourced efforts. Deng et al. (2009) used MTurk to construct ImageNet, an annotated image database containing 3.2 million that are hierarchically categorized using the WordNet ontology (Fellbaum, 1998). Because Mechanical Turk allows researchers to experiment with crowdsourcing by providing small incentives to Turkers, other successful crowdsourcing efforts like Wikipedia or Games with a Purpose (von Ahn and Dabbish, 2008) also share something in common with MTurk. 6 Shared Task The workshop included a shared task in which participants were provided with $100 to spend on Mechanical Turk experiments. Participants submitted a 1 pa</context>
</contexts>
<marker>Deng, Dong, Socher, Li, Li, Fei-Fei, 2009</marker>
<rawString>Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR, Miami Beach, Floriday.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Exploring normalization techniques for human judgments of machine translation adequacy collected using Amazon Mechanical Turk.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<marker>Denkowski, Lavie, 2010</marker>
<rawString>Michael Denkowski and Alon Lavie. 2010. Exploring normalization techniques for human judgments of machine translation adequacy collected using Amazon Mechanical Turk. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Hassan Al-Haj</author>
<author>Alon Lavie</author>
</authors>
<title>Turker-assisted paraphrasing for EnglishArabic machine translation.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="27682" citStr="Denkowski et al. (2010)" startWordPosition="4446" endWordPosition="4449">ning. Bloodgood and Callison-Burch replicate the NIST 2009 UrduEnglish test set of 1792 sentences, paying only $0.10 a sentence, a substantially reduced price than the typical annotator cost. The result is a data set that is still effective for comparing MT systems in an evaluation. Ambati and Vogel create corpora with 100 sentences and 3 translations per sentence for all the language pairs between English, Spanish, Urdu and Telugu. This demonstrates the feasibility of creating cheap corpora for high and low resource languages. Two papers focused on the creation and evaluation of paraphrases. Denkowski et al. (2010) generated and evaluated 728 paraphrases for ArabicEnglish translation. MTurk was used to identify correct and fix incorrect paraphrases. Over 1200 high quality paraphrases were created. Buzek et al. (2010) evaluated error driven paraphrases for MT. In this setting, paraphrases are used to simplify potentially difficult to translate segments of text. Turkers identified 1780 error regions in 1006 English/Chinese sentences. Turkers provided 4821 paraphrases for these regions. External resources can be an important part of an MT system. Irvine and Klementiev (2010) created lexicons for low resour</context>
</contexts>
<marker>Denkowski, Al-Haj, Lavie, 2010</marker>
<rawString>Michael Denkowski, Hassan Al-Haj, and Alon Lavie. 2010. Turker-assisted paraphrasing for EnglishArabic machine translation. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
</authors>
<title>Partha Pratim Talukdar, and Koby Crammer.</title>
<date>2009</date>
<booktitle>In ECML/PKDD Workshop on Learning from Multi-Label Data (MLD).</booktitle>
<marker>Dredze, 2009</marker>
<rawString>Mark Dredze, Partha Pratim Talukdar, and Koby Crammer. 2009. Sequence learning from data with multiple labels. In ECML/PKDD Workshop on Learning from Multi-Label Data (MLD).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keelan Evanini</author>
<author>Derrick Higgins</author>
<author>Klaus Zechner</author>
</authors>
<title>Using Amazon Mechanical Turk for transcription of non-native speech.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<marker>Evanini, Higgins, Zechner, 2010</marker>
<rawString>Keelan Evanini, Derrick Higgins, and Klaus Zechner. 2010. Using Amazon Mechanical Turk for transcription of non-native speech. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>Bradford Books.</publisher>
<contexts>
<context position="17611" citStr="Fellbaum, 1998" startWordPosition="2894" endWordPosition="2895">Marge et al., 2010a; Novotney and Callison-Burch, 2010a). Others have used MTurk for novel research directions like nonsimulated active learning for NLP tasks such as sentiment classification (Hsueh et al., 2009) or doing quixotic things like doing human-in-the-loop minimum error rate training for machine translation (Zaidan and Callison-Burch, 2009). Some projects have demonstrated the superscalability of crowdsourced efforts. Deng et al. (2009) used MTurk to construct ImageNet, an annotated image database containing 3.2 million that are hierarchically categorized using the WordNet ontology (Fellbaum, 1998). Because Mechanical Turk allows researchers to experiment with crowdsourcing by providing small incentives to Turkers, other successful crowdsourcing efforts like Wikipedia or Games with a Purpose (von Ahn and Dabbish, 2008) also share something in common with MTurk. 6 Shared Task The workshop included a shared task in which participants were provided with $100 to spend on Mechanical Turk experiments. Participants submitted a 1 page proposal in advance describing their intended use of the funds. Selected proposals were provided $100 seed money, to which many participants added their own funds</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. Bradford Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Finin</author>
<author>William Murnane</author>
<author>Anand Karandikar</author>
<author>Nicholas Keller</author>
<author>Justin Martineau</author>
<author>Mark Dredze</author>
</authors>
<date>2010</date>
<booktitle>Touring PER, ORG and LOC on $100 a day. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="25653" citStr="Finin et al. (2010)" startWordPosition="4132" endWordPosition="4135"> new domains and genres as well as new relation types. The goal of relation extraction is to identify relations between entities or terms in a sentence, such as born in or religion. Gormley et al. (2010) automatically generate potential relation pairs in sentences by finding relation pairs appearing in news articles as given by a knowledge base. They ask Turkers if a sentence supports a relation, does not support a relation, or whether the relation makes sense. They collected close to 2500 annotations for 17 different person relation types. The other IE papers explored new genres and domains. Finin et al. (2010) obtained named entity annotations (person, organization, geopolitical entity) for several hundred Twitter messages. They conducted experiments using both MTurk and CrowdFlower. Yetisgen-Yildiz et al. (2010) explored medical named entity recognition. They selected 100 clinical trial announcements from ClinicalTrials.gov. 4 annotators for each of the 100 announcements identified 3 types of medical entities: medical conditions, medications, and laboratory test. 6.6 Machine Translation The most popular shared task topic was Machine Translation (MT). MT is a data hungry task that relies on huge co</context>
<context position="32434" citStr="Finin et al., 2010" startWordPosition="5219" endWordPosition="5222"> This will have a significant impact on low resource languages as corpora can be cheaply built for a much wider array of languages. As one example, Irvine and Klementiev (2010) collected data for 42 languages without worrying about how to find speakers of such a wide variety of languages. Additionally, the collection of Arabic nicknames requires a diverse and numerous Arabic speaking population (Higgins et al., 2010). In addition to extending into new languages, MTurk also allows for the creation of evaluation sets in new genres and domains, which was the focus of two papers in this workshop (Finin et al., 2010; Yetisgen-Yildiz et al., 2010). We expect to see new research emphasis on low resource languages and new domains and genres. Another factor is the change of data type and its impact on machine learning algorithms. With professional annotators, great time and care are paid to annotation guidelines and annotator training. These are difficult tasks with MTurk, which favors simple intuitive annotations and little training. Many papers applied creative methods of using simpler annotation tasks to create more complex data sets. This process can impact machine learning in a number of ways. Rather th</context>
</contexts>
<marker>Finin, Murnane, Karandikar, Keller, Martineau, Dredze, 2010</marker>
<rawString>Tim Finin, William Murnane, Anand Karandikar, Nicholas Keller, Justin Martineau, and Mark Dredze. 2010. Touring PER, ORG and LOC on $100 a day. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Gao</author>
<author>Stephan Vogel</author>
</authors>
<title>Semi-supervised word alignment with Mechanical Turk.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="26807" citStr="Gao and Vogel (2010)" startWordPosition="4308" endWordPosition="4311"> Translation (MT). MT is a data hungry task that relies on huge corpora of parallel texts between two languages. Performance of MT systems depends on the size of training corpora, so there is a constant search for new and larger data sets. Such data sets are traditionally expensive to produce, requiring skilled translators. One of the advantages to MTurk is the diversity of the Turker population, making it an especially attractive source of MT data. Shared task papers in MT explored the full range of MT tasks, including alignments, parallel corpus creation, paraphrases and bilingual lexicons. Gao and Vogel (2010) create alignments in a 300 sentence Chinese-English corpus (Chinese aligned to English). Both Ambati and Vogel (2010) and Bloodgood and Callison-Burch (2010) explore the potential of MTurk in the creation of MT parallel corpora for evaluation and training. Bloodgood and Callison-Burch replicate the NIST 2009 UrduEnglish test set of 1792 sentences, paying only $0.10 a sentence, a substantially reduced price than the typical annotator cost. The result is a data set that is still effective for comparing MT systems in an evaluation. Ambati and Vogel create corpora with 100 sentences and 3 transla</context>
</contexts>
<marker>Gao, Vogel, 2010</marker>
<rawString>Qin Gao and Stephan Vogel. 2010. Semi-supervised word alignment with Mechanical Turk. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
<author>Yang Liu</author>
</authors>
<title>Non-expert evaluation of summarization systems is risky.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<marker>Gillick, Liu, 2010</marker>
<rawString>Dan Gillick and Yang Liu. 2010. Non-expert evaluation of summarization systems is risky. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Gordon</author>
<author>Benjamin Van Durme</author>
<author>Lenhart Schubert</author>
</authors>
<title>Evaluation of commonsense knowledge with Mechanical Turk.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<marker>Gordon, Van Durme, Schubert, 2010</marker>
<rawString>Jonathan Gordon, Benjamin Van Durme, and Lenhart Schubert. 2010. Evaluation of commonsense knowledge with Mechanical Turk. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew R Gormley</author>
<author>Adam Gerber</author>
<author>Mary Harper</author>
<author>Mark Dredze</author>
</authors>
<title>Non-expert correction of automatically generated relation annotations.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="25237" citStr="Gormley et al. (2010)" startWordPosition="4063" endWordPosition="4066"> on how these factors influence data collection. For further work on MTurk and information retrieval, readers are encouraged to see the SIGIR 2010 Workshop on Crowdsourcing for Search Evaluation.8 8http://www.ischool.utexas.edu/˜cse2010/ call.htm 6 6.5 Information Extraction Information extraction (IE) seeks to identify specific types of information in natural languages. The IE papers in the shared tasks focused on new domains and genres as well as new relation types. The goal of relation extraction is to identify relations between entities or terms in a sentence, such as born in or religion. Gormley et al. (2010) automatically generate potential relation pairs in sentences by finding relation pairs appearing in news articles as given by a knowledge base. They ask Turkers if a sentence supports a relation, does not support a relation, or whether the relation makes sense. They collected close to 2500 annotations for 17 different person relation types. The other IE papers explored new genres and domains. Finin et al. (2010) obtained named entity annotations (person, organization, geopolitical entity) for several hundred Twitter messages. They conducted experiments using both MTurk and CrowdFlower. Yetisg</context>
</contexts>
<marker>Gormley, Gerber, Harper, Dredze, 2010</marker>
<rawString>Matthew R. Gormley, Adam Gerber, Mary Harper, and Mark Dredze. 2010. Non-expert correction of automatically generated relation annotations. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catherine Grady</author>
<author>Matthew Lease</author>
</authors>
<title>Crowdsourcing document relevance assessment with Mechanical Turk.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="24484" citStr="Grady and Lease (2010)" startWordPosition="3950" endWordPosition="3953">t automative topics. They evaluated three HITs for collecting such data and compared results for quality and expressiveness. Yano et al. (2010) evaluated the political bias of blog posts. Annotators labeled 1000 sentences to determine biased phrases in political blogs from the 2008 election season. Knowledge of the annotators own biases allowed the authors to study how bias differs on the different ends of the political spectrum. 6.4 Information Retrieval Large scale evaluations requiring significant human labor for evaluation have a long history in the information retrieval community (TREC). Grady and Lease (2010) study four factors that influence Turker performance on a document relevance search task. The authors present some negative results on how these factors influence data collection. For further work on MTurk and information retrieval, readers are encouraged to see the SIGIR 2010 Workshop on Crowdsourcing for Search Evaluation.8 8http://www.ischool.utexas.edu/˜cse2010/ call.htm 6 6.5 Information Extraction Information extraction (IE) seeks to identify specific types of information in natural languages. The IE papers in the shared tasks focused on new domains and genres as well as new relation ty</context>
</contexts>
<marker>Grady, Lease, 2010</marker>
<rawString>Catherine Grady and Matthew Lease. 2010. Crowdsourcing document relevance assessment with Mechanical Turk. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Noah A Smith</author>
</authors>
<title>Rating computer-generated questions with Mechanical Turk.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="21246" citStr="Heilman and Smith (2010)" startWordPosition="3446" endWordPosition="3450">ided text. The resulting collection includes 790 facts and 203 counter-facts. Negri and Mehdad (2010) created a bi-lingual entailment corpus using English and Spanish entailment pairs, where the hypothesis and text come from different languages. The authors took a publicly available English RTE data set (the PASCAL-RTE3 dataset1) and created an English-Spanish equivalent by having Turkers translating the hypotheses into Spanish. The authors include a timeline of their progress, complete with total cost over the 10 days that they ran the experiments. In the area of natural language generation, Heilman and Smith (2010) explored the potential of MTurk for ranking of computer generated questions about provided texts. These questions can be used to test reading comprehension and understanding. 60 Wikipedia articles were selected, for each of which 20 questions were generated. Turkers provided 5 ratings for each of the 1,200 questions, creating a significant corpus of scored questions. Finally, Gordon et al. (2010) relied on MTurk to evaluate the quality and accuracy of automatically extracted common sense knowledge (factoids) from news and Wikipedia articles. Factoids were provided by the KNEXT knowledge extra</context>
</contexts>
<marker>Heilman, Smith, 2010</marker>
<rawString>Michael Heilman and Noah A. Smith. 2010. Rating computer-generated questions with Mechanical Turk. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chiara Higgins</author>
<author>Elizabeth McGrath</author>
<author>Laila Moretto</author>
</authors>
<title>AMT crowdsourcing: A viable method for rapid discovery of Arabic nicknames?</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="28449" citStr="Higgins et al. (2010)" startWordPosition="4558" endWordPosition="4561">0 high quality paraphrases were created. Buzek et al. (2010) evaluated error driven paraphrases for MT. In this setting, paraphrases are used to simplify potentially difficult to translate segments of text. Turkers identified 1780 error regions in 1006 English/Chinese sentences. Turkers provided 4821 paraphrases for these regions. External resources can be an important part of an MT system. Irvine and Klementiev (2010) created lexicons for low resource languages. They evaluated translation candidates for 100 English words in 32 languages and solicited translations for 10 additional languages. Higgins et al. (2010) expanded name lists in Arabic by soliciting common Arabic nicknames. The 332 collected nicknames were primarily provided by Turkers in Arab speaking countries (35%), India (46%), and the United States (13%). Finally, Zaidan and Ganitkevitch (2010) explored how MTurk could be used to directly improve an MT grammar. Each rule in an Urdu to English translation system was characterized by 12 features. Turkers were provided examples for which their feedback was used to rescore grammar productions directly. This approach shows the potential of fine tuning an MT system with targeted feedback from an</context>
<context position="32236" citStr="Higgins et al., 2010" startWordPosition="5184" endWordPosition="5187">ing specific demographics in data creation. Beyond efficiencies in cost, MTurk provides access to a global user population far more diverse than those provided by more professional annotation settings. This will have a significant impact on low resource languages as corpora can be cheaply built for a much wider array of languages. As one example, Irvine and Klementiev (2010) collected data for 42 languages without worrying about how to find speakers of such a wide variety of languages. Additionally, the collection of Arabic nicknames requires a diverse and numerous Arabic speaking population (Higgins et al., 2010). In addition to extending into new languages, MTurk also allows for the creation of evaluation sets in new genres and domains, which was the focus of two papers in this workshop (Finin et al., 2010; Yetisgen-Yildiz et al., 2010). We expect to see new research emphasis on low resource languages and new domains and genres. Another factor is the change of data type and its impact on machine learning algorithms. With professional annotators, great time and care are paid to annotation guidelines and annotator training. These are difficult tasks with MTurk, which favors simple intuitive annotations</context>
</contexts>
<marker>Higgins, McGrath, Moretto, 2010</marker>
<rawString>Chiara Higgins, Elizabeth McGrath, and Laila Moretto. 2010. AMT crowdsourcing: A viable method for rapid discovery of Arabic nicknames? In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul J Hopper</author>
<author>Sandra A Thompson</author>
</authors>
<title>Transitivity in grammar and discourse.</title>
<date>1980</date>
<journal>Language,</journal>
<volume>56</volume>
<pages>299</pages>
<contexts>
<context position="19804" citStr="Hopper and Thompson (1980)" startWordPosition="3235" endWordPosition="3239">urated annotated corpus to train and evaluate statistical models. Many of the shared task teams attempted to create new corpora for these tasks at substantially reduced costs using MTurk. Parent and Eskenazi (2010) produce new corpora for the task of word sense disambiguation. The study used MTurk to create unique word definitions for 50 words, which Turkers then also mapped onto existing definitions. Sentences containing these 50 words were then assigned to unique definitions according to word sense. Madnani and Boyd-Graber (2010) measured the concept of transitivity of verbs in the style of Hopper and Thompson (1980), a theory that goes beyond simple grammatical transitivity – whether verbs take objects (transitive) or not – to capture the amount of action indicated by a sentence. Videos that portrayed verbs were shown to Turkers who described the actions shown in the video. Additionally, sentences containing the verbs were rated for aspect, affirmation, benefit, harm, kinesis, punctuality, and volition. The authors investigated several approaches for eliciting descriptions of transitivity from Turkers. Two teams explored textual entailment tasks. Wang and Callison-Burch (2010) created data for 7http://si</context>
</contexts>
<marker>Hopper, Thompson, 1980</marker>
<rawString>Paul J. Hopper and Sandra A. Thompson. 1980. Transitivity in grammar and discourse. Language, 56:251– 299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pei-Yun Hsueh</author>
<author>Prem Melville</author>
<author>Vikas Sindhwani</author>
</authors>
<title>Data quality from crowdsourcing: A study of annotation selection criteria.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL HLT 2009 Workshop on Active Learning for Natural Language Processing,</booktitle>
<pages>27--35</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="17208" citStr="Hsueh et al., 2009" startWordPosition="2835" endWordPosition="2838">nical Turk to a diverse set of natural language processing tasks, including: creating question-answer sentence pairs (Kaisser and Lowe, 2008), evaluating machine translation quality and crowdsouring translations (Callison-Burch, 2009), paraphrasing noun-noun compouds for SemEval (Butnariu et al., 2009), human evaluation of topic models (Chang et al., 2009), and speech transcription (McGraw et al., 2010; Marge et al., 2010a; Novotney and Callison-Burch, 2010a). Others have used MTurk for novel research directions like nonsimulated active learning for NLP tasks such as sentiment classification (Hsueh et al., 2009) or doing quixotic things like doing human-in-the-loop minimum error rate training for machine translation (Zaidan and Callison-Burch, 2009). Some projects have demonstrated the superscalability of crowdsourced efforts. Deng et al. (2009) used MTurk to construct ImageNet, an annotated image database containing 3.2 million that are hierarchically categorized using the WordNet ontology (Fellbaum, 1998). Because Mechanical Turk allows researchers to experiment with crowdsourcing by providing small incentives to Turkers, other successful crowdsourcing efforts like Wikipedia or Games with a Purpose</context>
<context position="33744" citStr="Hsueh et al., 2009" startWordPosition="5435" endWordPosition="5438">tiple annotations may improve systems (Dredze et al., 2009). Additionally, even with efforts to clean up MTurk annotations, we can expect an increase in noisy examples in data. This will push for new more robust learning algorithms that are less sensitive to noise. If we increase the size of the data ten-fold but also increase the noise, can learning still be successful? Another learning area of great interest is active learning, which has long relied on simulated user experiments. New work evaluated active learning methods with real users using MTurk (Baker et al., 2009; Ambati et al., 2010; Hsueh et al., 2009; ?). Finally, the composition of complex data set annotations from simple user inputs can transform the method by which we learn complex outputs. Current approaches expect examples of labels that exactly match the expectation of the system. Can we instead provide lower level sim8 pler user annotations and teach systems how to learn from these to construct complex output? This would open more complex annotation tasks to MTurk. A general trend in research is that good ideas come from unexpected places. Major transformations in the field have come from creative new approaches. Consider the Penn </context>
</contexts>
<marker>Hsueh, Melville, Sindhwani, 2009</marker>
<rawString>Pei-Yun Hsueh, Prem Melville, and Vikas Sindhwani. 2009. Data quality from crowdsourcing: A study of annotation selection criteria. In Proceedings of the NAACL HLT 2009 Workshop on Active Learning for Natural Language Processing, pages 27–35, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Panos Ipeirotis</author>
</authors>
<title>New demographics of Mechanical Turk.</title>
<date>2010</date>
<note>http://behind-the-enemy-lines. blogspot.com/2010/03/</note>
<contexts>
<context position="3760" citStr="Ipeirotis (2010)" startWordPosition="608" endWordPosition="609">om the 18th cen1http://www.mturk.com/ Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 1–12, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics Hours spent on Mechanical Turk per week Number of HITs completed per week Weekly income from Mechanical Turk 30% 25% 20% 15% 10% 5% 0% 20% 30% 20% 10% 0% 40% 15% 10% 5% 0% &lt; 1 1-2 2-4 4-88-20 40+ &lt; 1 HIT 5-10 20-50 100-200 500-1k 5k+ HITs &lt; $1 $5-10 20-50 100-200 Figure 1: Time spent, HITs completed, and amount earned from a survey of 1,000 Turkers by Ipeirotis (2010). tury where a chess-playing automaton appeared to be able to beat human opponents using a mechanism, but was, in fact, controlled by a person hiding inside the machine. These hint at the the primary focus of the web service, which is to get people to perform tasks that are simple for humans but difficult for computers. The basic unit of work on MTurk is even called a Human Intelligence Task (HIT). Amazon’s web service provides an easy way to pay people small amounts of money to perform HITs. Anyone with an Amazon account can either submit HITs or work on HITs that were submitted by others. Wo</context>
<context position="6163" citStr="Ipeirotis (2010)" startWordPosition="1014" endWordPosition="1015">f labels for a very small amount of money. They collected 21,000 labels for just over $25. Turkers put in over 140+ hours worth Why do you complete tasks in MTurk? US India To spend free time fruitfully and get 70% 60% cash (e.g., instead of watching TV) For “primary” income purposes (e.g., 15% 27% gas, bills, groceries, credit cards) For “secondary” income purposes, 60% 37% pocket change (for hobbies, gadgets) To kill time 33% 5% The tasks are fun 40% 20% Currently unemployed or part time work 30% 27% Table 1: Motivations for participating on Mechanical Turk from a survey of 1,000 Turkers by Ipeirotis (2010). of human effort to generate the labels. The amount of participation is surprisingly high, given the small payment. Turker demographics Given the amount of work that can get done for so little, it is natural to ask: who would contribute so much work for so little pay, and why? The answers to these questions are often mysterious because Amazon does not provide any personal information about Turkers (each Turker is identifiable only through a serial number like A23KO2TP7I4KK2). Ipeirotis (2010) elucidates some of the reasons by presenting a demographic analysis of Turkers. He built a profile of</context>
<context position="8476" citStr="Ipeirotis (2010)" startWordPosition="1404" endWordPosition="1405">n may have increased participation, with 30% of the USbased Turkers reporting that they are unemployed or underemployed. The public radio show Marketplace recently interviewed unemployed Turkers (Rose, 2010). It reports that they earn a little income, but that they do not earn enough to make a living. Figure 1 confirms this, giving a break down of how much time people spend on Mechanical Turk each week, how many HITs they complete, and how much money they earn. Most Turkers spend less than 8 hours per week on Mechanical Turk, and earn less than $10 per week through the site. 3 Quality Control Ipeirotis (2010) reports that just over half of Turkers have a college education. Despite being reasonably well educated, it is important to keep in mind that Turkers do not have training in specialized subjects like NLP. Because the Turkers are non-experts, and because the payments are generally so low, quality control is an important consideration when creating data with MTurk. Amazon provides three mechanisms to help ensure quality: • Requesters have the option of rejecting the work of individual Turkers, in which case they are not paid.2 Turkers can also be blocked from doing future work for a requester. </context>
</contexts>
<marker>Ipeirotis, 2010</marker>
<rawString>Panos Ipeirotis. 2010. New demographics of Mechanical Turk. http://behind-the-enemy-lines. blogspot.com/2010/03/</rawString>
</citation>
<citation valid="false">
<note>new-demographics-of-mechanical-turk. html.</note>
<marker></marker>
<rawString>new-demographics-of-mechanical-turk. html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Irvine</author>
<author>Alexandre Klementiev</author>
</authors>
<title>Using Mechanical Turk to annotate lexicons for less commonly used languages.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="28250" citStr="Irvine and Klementiev (2010)" startWordPosition="4530" endWordPosition="4533">ation and evaluation of paraphrases. Denkowski et al. (2010) generated and evaluated 728 paraphrases for ArabicEnglish translation. MTurk was used to identify correct and fix incorrect paraphrases. Over 1200 high quality paraphrases were created. Buzek et al. (2010) evaluated error driven paraphrases for MT. In this setting, paraphrases are used to simplify potentially difficult to translate segments of text. Turkers identified 1780 error regions in 1006 English/Chinese sentences. Turkers provided 4821 paraphrases for these regions. External resources can be an important part of an MT system. Irvine and Klementiev (2010) created lexicons for low resource languages. They evaluated translation candidates for 100 English words in 32 languages and solicited translations for 10 additional languages. Higgins et al. (2010) expanded name lists in Arabic by soliciting common Arabic nicknames. The 332 collected nicknames were primarily provided by Turkers in Arab speaking countries (35%), India (46%), and the United States (13%). Finally, Zaidan and Ganitkevitch (2010) explored how MTurk could be used to directly improve an MT grammar. Each rule in an Urdu to English translation system was characterized by 12 features.</context>
<context position="31992" citStr="Irvine and Klementiev (2010)" startWordPosition="5144" endWordPosition="5147">rce for new types of data. Several of the papers in this workshop collected information about the annotators in addition to their annotations. This creates potential for studying how different user demographics understand language and allow for targeting specific demographics in data creation. Beyond efficiencies in cost, MTurk provides access to a global user population far more diverse than those provided by more professional annotation settings. This will have a significant impact on low resource languages as corpora can be cheaply built for a much wider array of languages. As one example, Irvine and Klementiev (2010) collected data for 42 languages without worrying about how to find speakers of such a wide variety of languages. Additionally, the collection of Arabic nicknames requires a diverse and numerous Arabic speaking population (Higgins et al., 2010). In addition to extending into new languages, MTurk also allows for the creation of evaluation sets in new genres and domains, which was the focus of two papers in this workshop (Finin et al., 2010; Yetisgen-Yildiz et al., 2010). We expect to see new research emphasis on low resource languages and new domains and genres. Another factor is the change of </context>
</contexts>
<marker>Irvine, Klementiev, 2010</marker>
<rawString>Ann Irvine and Alexandre Klementiev. 2010. Using Mechanical Turk to annotate lexicons for less commonly used languages. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mukund Jha</author>
<author>Jacob Andreas</author>
<author>Kapil Thadani</author>
<author>Sara Rosenthal</author>
<author>Kathleen McKeown</author>
</authors>
<title>Corpus creation for new genres: A crowdsourced approach to PP attachment.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<marker>Jha, Andreas, Thadani, Rosenthal, McKeown, 2010</marker>
<rawString>Mukund Jha, Jacob Andreas, Kapil Thadani, Sara Rosenthal, and Kathleen McKeown. 2010. Corpus creation for new genres: A crowdsourced approach to PP attachment. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Kaisser</author>
<author>John Lowe</author>
</authors>
<title>Creating a research collection of question answer sentence pairs with Amazons Mechanical Turk.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08),</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="16730" citStr="Kaisser and Lowe, 2008" startWordPosition="2765" endWordPosition="2768">u/uid/turkit/ 4 inter-annotator agreement of the Turkers against experts on small amounts of gold standard data, or by stating what controls you used and what criteria you used to block bad Turkers. Finally, whenever possible you should publish the data that you generate on Mechanical Turk (and your analysis scripts and HIT templates) alongside your paper so that other people can verify it. 5 Related work In the past two years, several papers have published about applying Mechanical Turk to a diverse set of natural language processing tasks, including: creating question-answer sentence pairs (Kaisser and Lowe, 2008), evaluating machine translation quality and crowdsouring translations (Callison-Burch, 2009), paraphrasing noun-noun compouds for SemEval (Butnariu et al., 2009), human evaluation of topic models (Chang et al., 2009), and speech transcription (McGraw et al., 2010; Marge et al., 2010a; Novotney and Callison-Burch, 2010a). Others have used MTurk for novel research directions like nonsimulated active learning for NLP tasks such as sentiment classification (Hsueh et al., 2009) or doing quixotic things like doing human-in-the-loop minimum error rate training for machine translation (Zaidan and Cal</context>
</contexts>
<marker>Kaisser, Lowe, 2008</marker>
<rawString>Michael Kaisser and John Lowe. 2008. Creating a research collection of question answer sentence pairs with Amazons Mechanical Turk. In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08), Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Kunath</author>
<author>Steven Weinberger</author>
</authors>
<title>The wisdom of the crowd’s ear: Speech accent rating and annotation with Amazon Mechanical Turk.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="22460" citStr="Kunath and Weinberger (2010)" startWordPosition="3640" endWordPosition="3643">nowledge extraction system. 6.2 Speech and Vision While MTurk naturally lends itself to text tasks, several teams explored annotation and collection of speech and image data. We note that one of the papers in the main track described tools for collecting such data (Lane et al., 2010). Two teams used MTurk to collect text annotations on speech data. Marge et al. (2010b) identified easy and hard sections of meeting speech to transcribe and focused data collection on difficult segments. Transcripts were collected on 48 audio clips from 4 different speakers, as well as other types of annotations. Kunath and Weinberger (2010) collected ratings of accented English speech, in which nonnative speakers were rated as either Arabic, Mandarin or Russian native speakers. The authors obtained multiple annotations for each speech sample, and tracked the native language of each annotator, allowing for an analysis of rating accuracy between native English and non-native English annotators. Novotney and Callison-Burch (2010b) used MTurk to elicit new speech samples. As part of an effort to increase the accessibility of public knowledge, such as Wikipedia, the team prompted Turkers to narrate Wikipedia articles. This required T</context>
</contexts>
<marker>Kunath, Weinberger, 2010</marker>
<rawString>Stephen Kunath and Steven Weinberger. 2010. The wisdom of the crowd’s ear: Speech accent rating and annotation with Amazon Mechanical Turk. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Lane</author>
<author>Matthias Eck</author>
<author>Kay Rottmann</author>
<author>Alex Waibel</author>
</authors>
<title>Tools for collecting speech corpora via Mechanical-Turk. In</title>
<date>2010</date>
<booktitle>NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="22116" citStr="Lane et al., 2010" startWordPosition="3585" endWordPosition="3588">erated. Turkers provided 5 ratings for each of the 1,200 questions, creating a significant corpus of scored questions. Finally, Gordon et al. (2010) relied on MTurk to evaluate the quality and accuracy of automatically extracted common sense knowledge (factoids) from news and Wikipedia articles. Factoids were provided by the KNEXT knowledge extraction system. 6.2 Speech and Vision While MTurk naturally lends itself to text tasks, several teams explored annotation and collection of speech and image data. We note that one of the papers in the main track described tools for collecting such data (Lane et al., 2010). Two teams used MTurk to collect text annotations on speech data. Marge et al. (2010b) identified easy and hard sections of meeting speech to transcribe and focused data collection on difficult segments. Transcripts were collected on 48 audio clips from 4 different speakers, as well as other types of annotations. Kunath and Weinberger (2010) collected ratings of accented English speech, in which nonnative speakers were rated as either Arabic, Mandarin or Russian native speakers. The authors obtained multiple annotations for each speech sample, and tracked the native language of each annotator</context>
<context position="35344" citStr="Lane et al., 2010" startWordPosition="5701" endWordPosition="5704">ative data collection may or may not have a significant impact, but it is unlikely that it would have been tried had the cost been very high. Finally, while obtaining new data annotations from MTurk is cheap, it is not trivial. Workshop participants struggled with how to attract Turkers, how to price HITs, HIT design, instructions, cheating detection, etc. No doubt that as work progresses, so will a communal knowledge and experience of how to use MTurk. There can be great benefit in new toolkits for collecting language data using MTurk, and indeed some of these have already started to emerge (Lane et al., 2010)9. Acknowledgements Thanks to Sharon Chiarella of Amazon’s Mechanical Turk for providing $100 credits for the shared task, and to CrowdFlower for allowing free use of their tool to workshop participants. Research funding was provided by the NSF under grant IIS-0713448, by the European Commission through the EuroMatrixPlus project, and by the DARPA GALE program under Contract No. HR0011-06-2-0001. The views and findings are the authors’ alone. References Cem Akkaya, Alexander Conrad, Janyce Wiebe, and Rada Mihalcea. 2010. Amazon Mechanical Turk for subjectivity word sense disambiguation. In NAA</context>
</contexts>
<marker>Lane, Eck, Rottmann, Waibel, 2010</marker>
<rawString>Ian Lane, Matthias Eck, Kay Rottmann, and Alex Waibel. 2010. Tools for collecting speech corpora via Mechanical-Turk. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nolan Lawson</author>
<author>Kevin Eustice</author>
<author>Mike Perkowitz</author>
<author>Meliha Yetisgen Yildiz</author>
</authors>
<title>Annotating large email datasets for named entity recognition with Mechanical Turk.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<marker>Lawson, Eustice, Perkowitz, Yildiz, 2010</marker>
<rawString>Nolan Lawson, Kevin Eustice, Mike Perkowitz, and Meliha Yetisgen Yildiz. 2010. Annotating large email datasets for named entity recognition with Mechanical Turk. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Little</author>
<author>Lydia B Chilton</author>
<author>Rob Miller</author>
<author>Max Goldman</author>
</authors>
<title>Turkit: Tools for iterative tasks on mechanical turk.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Human Computation at the International Conference on Knowledge Discovery and Data Mining (KDDHCOMP ’09),</booktitle>
<location>Paris.</location>
<contexts>
<context position="14620" citStr="Little et al. (2009)" startWordPosition="2408" endWordPosition="2411">at build on the output of previous HITs. This could be used to have Turkers judge whether the results from a previous HIT conformed to the instructions, and whether it is of high quality. Alternately, the second set of Turkers could be used to improve the quality of what the first Turkers created. For instance, in a translation task, a second set of US-based Turkers could edit the English produced by non-native speakers. CastingWords,5 a transcription company that uses Turker labor, employs this strategy by having a firstpass transcription graded and iteratively improved in subsequent passes. Little et al. (2009) even designed an API specifically for running iterative tasks on MTurk.6 4 Recommended Practices Although it is hard to define a set of “best practices” that applies to all HITs, or even to all NLP HITs, we recommend the following guidelines to Requesters. First and foremost, it is critical to convey instructions appropriately for non-experts. The instructions should be clear and concise. To calibrate whether the HIT is doable, you should first try the task yourself, and then have a friend from outside the field try it. This will help to ensure that the instructions are clear, and to calibrat</context>
</contexts>
<marker>Little, Chilton, Miller, Goldman, 2009</marker>
<rawString>Greg Little, Lydia B. Chilton, Rob Miller, and Max Goldman. 2009. Turkit: Tools for iterative tasks on mechanical turk. In Proceedings of the Workshop on Human Computation at the International Conference on Knowledge Discovery and Data Mining (KDDHCOMP ’09), Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Jordan Boyd-Graber</author>
</authors>
<title>Measuring transitivity using untrained annotators.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="19715" citStr="Madnani and Boyd-Graber (2010)" startWordPosition="3220" endWordPosition="3223">ntailment and word sense disambiguation. Each of these tasks requires a large and carefully curated annotated corpus to train and evaluate statistical models. Many of the shared task teams attempted to create new corpora for these tasks at substantially reduced costs using MTurk. Parent and Eskenazi (2010) produce new corpora for the task of word sense disambiguation. The study used MTurk to create unique word definitions for 50 words, which Turkers then also mapped onto existing definitions. Sentences containing these 50 words were then assigned to unique definitions according to word sense. Madnani and Boyd-Graber (2010) measured the concept of transitivity of verbs in the style of Hopper and Thompson (1980), a theory that goes beyond simple grammatical transitivity – whether verbs take objects (transitive) or not – to capture the amount of action indicated by a sentence. Videos that portrayed verbs were shown to Turkers who described the actions shown in the video. Additionally, sentences containing the verbs were rated for aspect, affirmation, benefit, harm, kinesis, punctuality, and volition. The authors investigated several approaches for eliciting descriptions of transitivity from Turkers. Two teams expl</context>
</contexts>
<marker>Madnani, Boyd-Graber, 2010</marker>
<rawString>Nitin Madnani and Jordan Boyd-Graber. 2010. Measuring transitivity using untrained annotators. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitch Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1994</date>
<booktitle>Computational linguistics,</booktitle>
<pages>19--2</pages>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>Mitch Marcus., Beatrice Santorini, and Mary Ann Marcinkiewicz. 1994. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Marge</author>
<author>Satanjeev Banerjee</author>
<author>Alexander Rudnicky</author>
</authors>
<title>Using the Amazon Mechanical Turk for transcription of spoken language.</title>
<date>2010</date>
<publisher>ICASSP,</publisher>
<contexts>
<context position="17014" citStr="Marge et al., 2010" startWordPosition="2806" endWordPosition="2809">k (and your analysis scripts and HIT templates) alongside your paper so that other people can verify it. 5 Related work In the past two years, several papers have published about applying Mechanical Turk to a diverse set of natural language processing tasks, including: creating question-answer sentence pairs (Kaisser and Lowe, 2008), evaluating machine translation quality and crowdsouring translations (Callison-Burch, 2009), paraphrasing noun-noun compouds for SemEval (Butnariu et al., 2009), human evaluation of topic models (Chang et al., 2009), and speech transcription (McGraw et al., 2010; Marge et al., 2010a; Novotney and Callison-Burch, 2010a). Others have used MTurk for novel research directions like nonsimulated active learning for NLP tasks such as sentiment classification (Hsueh et al., 2009) or doing quixotic things like doing human-in-the-loop minimum error rate training for machine translation (Zaidan and Callison-Burch, 2009). Some projects have demonstrated the superscalability of crowdsourced efforts. Deng et al. (2009) used MTurk to construct ImageNet, an annotated image database containing 3.2 million that are hierarchically categorized using the WordNet ontology (Fellbaum, 1998). B</context>
<context position="22201" citStr="Marge et al. (2010" startWordPosition="3600" endWordPosition="3603">icant corpus of scored questions. Finally, Gordon et al. (2010) relied on MTurk to evaluate the quality and accuracy of automatically extracted common sense knowledge (factoids) from news and Wikipedia articles. Factoids were provided by the KNEXT knowledge extraction system. 6.2 Speech and Vision While MTurk naturally lends itself to text tasks, several teams explored annotation and collection of speech and image data. We note that one of the papers in the main track described tools for collecting such data (Lane et al., 2010). Two teams used MTurk to collect text annotations on speech data. Marge et al. (2010b) identified easy and hard sections of meeting speech to transcribe and focused data collection on difficult segments. Transcripts were collected on 48 audio clips from 4 different speakers, as well as other types of annotations. Kunath and Weinberger (2010) collected ratings of accented English speech, in which nonnative speakers were rated as either Arabic, Mandarin or Russian native speakers. The authors obtained multiple annotations for each speech sample, and tracked the native language of each annotator, allowing for an analysis of rating accuracy between native English and non-native E</context>
</contexts>
<marker>Marge, Banerjee, Rudnicky, 2010</marker>
<rawString>Matthew Marge, Satanjeev Banerjee, and Alexander Rudnicky. 2010a. Using the Amazon Mechanical Turk for transcription of spoken language. ICASSP, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Marge</author>
<author>Satanjeev Banerjee</author>
<author>Alexander Rudnicky</author>
</authors>
<title>Using the Amazon Mechanical Turk to transcribe and annotate meeting speech for extractive summarization.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="17014" citStr="Marge et al., 2010" startWordPosition="2806" endWordPosition="2809">k (and your analysis scripts and HIT templates) alongside your paper so that other people can verify it. 5 Related work In the past two years, several papers have published about applying Mechanical Turk to a diverse set of natural language processing tasks, including: creating question-answer sentence pairs (Kaisser and Lowe, 2008), evaluating machine translation quality and crowdsouring translations (Callison-Burch, 2009), paraphrasing noun-noun compouds for SemEval (Butnariu et al., 2009), human evaluation of topic models (Chang et al., 2009), and speech transcription (McGraw et al., 2010; Marge et al., 2010a; Novotney and Callison-Burch, 2010a). Others have used MTurk for novel research directions like nonsimulated active learning for NLP tasks such as sentiment classification (Hsueh et al., 2009) or doing quixotic things like doing human-in-the-loop minimum error rate training for machine translation (Zaidan and Callison-Burch, 2009). Some projects have demonstrated the superscalability of crowdsourced efforts. Deng et al. (2009) used MTurk to construct ImageNet, an annotated image database containing 3.2 million that are hierarchically categorized using the WordNet ontology (Fellbaum, 1998). B</context>
<context position="22201" citStr="Marge et al. (2010" startWordPosition="3600" endWordPosition="3603">icant corpus of scored questions. Finally, Gordon et al. (2010) relied on MTurk to evaluate the quality and accuracy of automatically extracted common sense knowledge (factoids) from news and Wikipedia articles. Factoids were provided by the KNEXT knowledge extraction system. 6.2 Speech and Vision While MTurk naturally lends itself to text tasks, several teams explored annotation and collection of speech and image data. We note that one of the papers in the main track described tools for collecting such data (Lane et al., 2010). Two teams used MTurk to collect text annotations on speech data. Marge et al. (2010b) identified easy and hard sections of meeting speech to transcribe and focused data collection on difficult segments. Transcripts were collected on 48 audio clips from 4 different speakers, as well as other types of annotations. Kunath and Weinberger (2010) collected ratings of accented English speech, in which nonnative speakers were rated as either Arabic, Mandarin or Russian native speakers. The authors obtained multiple annotations for each speech sample, and tracked the native language of each annotator, allowing for an analysis of rating accuracy between native English and non-native E</context>
</contexts>
<marker>Marge, Banerjee, Rudnicky, 2010</marker>
<rawString>Matthew Marge, Satanjeev Banerjee, and Alexander Rudnicky. 2010b. Using the Amazon Mechanical Turk to transcribe and annotate meeting speech for extractive summarization. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Winter Mason</author>
<author>Duncan J Watts</author>
</authors>
<title>Financial incentives and the “performance of crowds”.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Human Computation at the International Conference on Knowledge Discovery and Data Mining (KDD-HCOMP ’09),</booktitle>
<location>Paris.</location>
<contexts>
<context position="11059" citStr="Mason and Watts (2009)" startWordPosition="1820" endWordPosition="1823">ification that isn’t among Amazon’s default qualifications is language skills. One might design a qualification test to determine a Turker’s ability to speak Arabic or Farsi before allowing them to do part of speech tagging in those languages, for instance. There are several reasons that poor quality data might be generated. The task may be too complex or the instructions might not be clear enough for Turkers to follow. The financial incentives may be too low for Turkers to act conscientiously, and certain HIT designs may allow them to simply randomly click instead of thinking about the task. Mason and Watts (2009) present a study of financial incentives on Mechanical Turk and find, counterintuitively, that increasing the amount of compensation for a particular task does not tend to improve the quality of the results. Anecdotally, we have observed that sometimes there is an inverse relationship between the amount of payment and the quality of work, because it is more tempting to cheat on high-paying HITs if you don’t have the skills to complete them. For example, a number of Turkers tried to cheat on an Urdu to English translation HIT by cutting-and-pasting the Urdu text into an online machine translati</context>
</contexts>
<marker>Mason, Watts, 2009</marker>
<rawString>Winter Mason and Duncan J. Watts. 2009. Financial incentives and the “performance of crowds”. In Proceedings of the Workshop on Human Computation at the International Conference on Knowledge Discovery and Data Mining (KDD-HCOMP ’09), Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian McGraw</author>
<author>Chia ying Lee</author>
<author>Lee Hetherington</author>
<author>Jim Glass</author>
</authors>
<title>Collecting voices from the crowd.</title>
<date>2010</date>
<location>LREC,</location>
<contexts>
<context position="16994" citStr="McGraw et al., 2010" startWordPosition="2802" endWordPosition="2805">ate on Mechanical Turk (and your analysis scripts and HIT templates) alongside your paper so that other people can verify it. 5 Related work In the past two years, several papers have published about applying Mechanical Turk to a diverse set of natural language processing tasks, including: creating question-answer sentence pairs (Kaisser and Lowe, 2008), evaluating machine translation quality and crowdsouring translations (Callison-Burch, 2009), paraphrasing noun-noun compouds for SemEval (Butnariu et al., 2009), human evaluation of topic models (Chang et al., 2009), and speech transcription (McGraw et al., 2010; Marge et al., 2010a; Novotney and Callison-Burch, 2010a). Others have used MTurk for novel research directions like nonsimulated active learning for NLP tasks such as sentiment classification (Hsueh et al., 2009) or doing quixotic things like doing human-in-the-loop minimum error rate training for machine translation (Zaidan and Callison-Burch, 2009). Some projects have demonstrated the superscalability of crowdsourced efforts. Deng et al. (2009) used MTurk to construct ImageNet, an annotated image database containing 3.2 million that are hierarchically categorized using the WordNet ontology</context>
</contexts>
<marker>McGraw, Lee, Hetherington, Glass, 2010</marker>
<rawString>Ian McGraw, Chia ying Lee, Lee Hetherington, and Jim Glass. 2010. Collecting voices from the crowd. LREC, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bart Mellebeek</author>
<author>Francesc Benavent</author>
<author>Jens Grivolla</author>
<author>Joan Codina</author>
<author>Marta R Costa-Juss`a</author>
<author>Rafael Banchs</author>
</authors>
<title>Opinion mining of spanish customer comments with non-expert annotations on Mechanical Turk.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<marker>Mellebeek, Benavent, Grivolla, Codina, Costa-Juss`a, Banchs, 2010</marker>
<rawString>Bart Mellebeek, Francesc Benavent, Jens Grivolla, Joan Codina, Marta R. Costa-Juss`a, and Rafael Banchs. 2010. Opinion mining of spanish customer comments with non-expert annotations on Mechanical Turk. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Munro</author>
<author>Steven Bethard</author>
<author>Victor Kuperman</author>
<author>Vicky Tzuyin Lai</author>
<author>Robin Melnick</author>
<author>Christopher Potts</author>
<author>Tyler Schnoebelen</author>
<author>Harry Tily</author>
</authors>
<title>Crowdsourcing and language studies: the new generation of linguistic data.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<marker>Munro, Bethard, Kuperman, Lai, Melnick, Potts, Schnoebelen, Tily, 2010</marker>
<rawString>Robert Munro, Steven Bethard, Victor Kuperman, Vicky Tzuyin Lai, Robin Melnick, Christopher Potts, Tyler Schnoebelen, and Harry Tily. 2010. Crowdsourcing and language studies: the new generation of linguistic data. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matteo Negri</author>
<author>Yashar Mehdad</author>
</authors>
<title>Creating a bi-lingual entailment corpus through translations with Mechanical Turk: $100 for a 10 days rush.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="20723" citStr="Negri and Mehdad (2010)" startWordPosition="3366" endWordPosition="3369">ng the verbs were rated for aspect, affirmation, benefit, harm, kinesis, punctuality, and volition. The authors investigated several approaches for eliciting descriptions of transitivity from Turkers. Two teams explored textual entailment tasks. Wang and Callison-Burch (2010) created data for 7http://sites.google.com/site/ amtworkshop2010/ 5 recognizing textual entailment (RTE). They submitted 600 text segments and asked Turkers to identify facts and counter-facts (unsupported facts and contradictions) given the provided text. The resulting collection includes 790 facts and 203 counter-facts. Negri and Mehdad (2010) created a bi-lingual entailment corpus using English and Spanish entailment pairs, where the hypothesis and text come from different languages. The authors took a publicly available English RTE data set (the PASCAL-RTE3 dataset1) and created an English-Spanish equivalent by having Turkers translating the hypotheses into Spanish. The authors include a timeline of their progress, complete with total cost over the 10 days that they ran the experiments. In the area of natural language generation, Heilman and Smith (2010) explored the potential of MTurk for ranking of computer generated questions </context>
</contexts>
<marker>Negri, Mehdad, 2010</marker>
<rawString>Matteo Negri and Yashar Mehdad. 2010. Creating a bi-lingual entailment corpus through translations with Mechanical Turk: $100 for a 10 days rush. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Novotney</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Cheap, fast and good enough: Automatic speech recognition with non-expert transcription.</title>
<date>2010</date>
<publisher>NAACL,</publisher>
<contexts>
<context position="17050" citStr="Novotney and Callison-Burch, 2010" startWordPosition="2810" endWordPosition="2813">scripts and HIT templates) alongside your paper so that other people can verify it. 5 Related work In the past two years, several papers have published about applying Mechanical Turk to a diverse set of natural language processing tasks, including: creating question-answer sentence pairs (Kaisser and Lowe, 2008), evaluating machine translation quality and crowdsouring translations (Callison-Burch, 2009), paraphrasing noun-noun compouds for SemEval (Butnariu et al., 2009), human evaluation of topic models (Chang et al., 2009), and speech transcription (McGraw et al., 2010; Marge et al., 2010a; Novotney and Callison-Burch, 2010a). Others have used MTurk for novel research directions like nonsimulated active learning for NLP tasks such as sentiment classification (Hsueh et al., 2009) or doing quixotic things like doing human-in-the-loop minimum error rate training for machine translation (Zaidan and Callison-Burch, 2009). Some projects have demonstrated the superscalability of crowdsourced efforts. Deng et al. (2009) used MTurk to construct ImageNet, an annotated image database containing 3.2 million that are hierarchically categorized using the WordNet ontology (Fellbaum, 1998). Because Mechanical Turk allows resear</context>
<context position="22853" citStr="Novotney and Callison-Burch (2010" startWordPosition="3698" endWordPosition="3701">and hard sections of meeting speech to transcribe and focused data collection on difficult segments. Transcripts were collected on 48 audio clips from 4 different speakers, as well as other types of annotations. Kunath and Weinberger (2010) collected ratings of accented English speech, in which nonnative speakers were rated as either Arabic, Mandarin or Russian native speakers. The authors obtained multiple annotations for each speech sample, and tracked the native language of each annotator, allowing for an analysis of rating accuracy between native English and non-native English annotators. Novotney and Callison-Burch (2010b) used MTurk to elicit new speech samples. As part of an effort to increase the accessibility of public knowledge, such as Wikipedia, the team prompted Turkers to narrate Wikipedia articles. This required Turkers to record audio files and upload them. An additional HIT was used to evaluate the quality of the narrations. A particularly creative data collection approach asked Turkers to create handwriting samples and then to submit images of their writing (Tong et al., 2010). Turkers were asked to submit handwritten shopping lists (large vocabulary) or weather descriptions (small vocabulary) in</context>
</contexts>
<marker>Novotney, Callison-Burch, 2010</marker>
<rawString>Scott Novotney and Chris Callison-Burch. 2010a. Cheap, fast and good enough: Automatic speech recognition with non-expert transcription. NAACL, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Novotney</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Crowdsourced accessibility: Elicitation of Wikipedia articles.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="17050" citStr="Novotney and Callison-Burch, 2010" startWordPosition="2810" endWordPosition="2813">scripts and HIT templates) alongside your paper so that other people can verify it. 5 Related work In the past two years, several papers have published about applying Mechanical Turk to a diverse set of natural language processing tasks, including: creating question-answer sentence pairs (Kaisser and Lowe, 2008), evaluating machine translation quality and crowdsouring translations (Callison-Burch, 2009), paraphrasing noun-noun compouds for SemEval (Butnariu et al., 2009), human evaluation of topic models (Chang et al., 2009), and speech transcription (McGraw et al., 2010; Marge et al., 2010a; Novotney and Callison-Burch, 2010a). Others have used MTurk for novel research directions like nonsimulated active learning for NLP tasks such as sentiment classification (Hsueh et al., 2009) or doing quixotic things like doing human-in-the-loop minimum error rate training for machine translation (Zaidan and Callison-Burch, 2009). Some projects have demonstrated the superscalability of crowdsourced efforts. Deng et al. (2009) used MTurk to construct ImageNet, an annotated image database containing 3.2 million that are hierarchically categorized using the WordNet ontology (Fellbaum, 1998). Because Mechanical Turk allows resear</context>
<context position="22853" citStr="Novotney and Callison-Burch (2010" startWordPosition="3698" endWordPosition="3701">and hard sections of meeting speech to transcribe and focused data collection on difficult segments. Transcripts were collected on 48 audio clips from 4 different speakers, as well as other types of annotations. Kunath and Weinberger (2010) collected ratings of accented English speech, in which nonnative speakers were rated as either Arabic, Mandarin or Russian native speakers. The authors obtained multiple annotations for each speech sample, and tracked the native language of each annotator, allowing for an analysis of rating accuracy between native English and non-native English annotators. Novotney and Callison-Burch (2010b) used MTurk to elicit new speech samples. As part of an effort to increase the accessibility of public knowledge, such as Wikipedia, the team prompted Turkers to narrate Wikipedia articles. This required Turkers to record audio files and upload them. An additional HIT was used to evaluate the quality of the narrations. A particularly creative data collection approach asked Turkers to create handwriting samples and then to submit images of their writing (Tong et al., 2010). Turkers were asked to submit handwritten shopping lists (large vocabulary) or weather descriptions (small vocabulary) in</context>
</contexts>
<marker>Novotney, Callison-Burch, 2010</marker>
<rawString>Scott Novotney and Chris Callison-Burch. 2010b. Crowdsourced accessibility: Elicitation of Wikipedia articles. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="30702" citStr="Pang et al. (2002)" startWordPosition="4935" endWordPosition="4938">l exists for a change in focus in a number of ways. In natural language processing, data drives research. The introduction of new large and widely accessible data sets creates whole new areas of research. There are many examples of such impact, the most famous of which is the Penn Treebank (Marcus. et al., 1994), which has 2910 citations in Google scholar and is the single most cited paper on the ACL anthology network (Radev et al., 2009). Other examples include the CoNLL named entity corpus (Sang and Meulder (2003) with 348 citations on Google Scholar), the IMDB movie reviews sentiment data (Pang et al. (2002) with 894 citations) and the Amazon sentiment multi-domain data (Blitzer et al. (2007) with 109 citations) . MTurk means that creating similar data sets is now much cheaper and easier than ever before. It is highly likely that new MTurk produced data sets will achieve prominence and have significant impact. Additionally, the creation of shared data means more comparison and evaluation against previous work. Progress is made when it can be demonstrated against previous approaches on the same data. The reduction of data cost and the rise of independent corpus producers likely means more accessib</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Parent</author>
<author>Maxine Eskenazi</author>
</authors>
<title>Clustering dictionary definitions using Amazon Mechanical Turk.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="19392" citStr="Parent and Eskenazi (2010)" startWordPosition="3170" endWordPosition="3173">demonstrates the potential for MTurk’s impact on the creation and curation of speech and language corpora. 6.1 Traditional NLP Tasks An established core set of computational linguistic tasks have received considerable attention in the natural language processing community. These include knowledge extraction, textual entailment and word sense disambiguation. Each of these tasks requires a large and carefully curated annotated corpus to train and evaluate statistical models. Many of the shared task teams attempted to create new corpora for these tasks at substantially reduced costs using MTurk. Parent and Eskenazi (2010) produce new corpora for the task of word sense disambiguation. The study used MTurk to create unique word definitions for 50 words, which Turkers then also mapped onto existing definitions. Sentences containing these 50 words were then assigned to unique definitions according to word sense. Madnani and Boyd-Graber (2010) measured the concept of transitivity of verbs in the style of Hopper and Thompson (1980), a theory that goes beyond simple grammatical transitivity – whether verbs take objects (transitive) or not – to capture the amount of action indicated by a sentence. Videos that portraye</context>
</contexts>
<marker>Parent, Eskenazi, 2010</marker>
<rawString>Gabriel Parent and Maxine Eskenazi. 2010. Clustering dictionary definitions using Amazon Mechanical Turk. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Pradeep Muthukrishnan</author>
<author>Vahed Qazvinian</author>
</authors>
<title>The acl anthology network.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries,</booktitle>
<pages>54--61</pages>
<institution>Suntec City, Singapore, August. Association for Computational Linguistics.</institution>
<contexts>
<context position="30526" citStr="Radev et al., 2009" startWordPosition="4906" endWordPosition="4909">’s authors is a strong advocate of such a position while the other disagrees, perhaps because he himself works on unsupervised methods. Certainly, we can agree that the potential exists for a change in focus in a number of ways. In natural language processing, data drives research. The introduction of new large and widely accessible data sets creates whole new areas of research. There are many examples of such impact, the most famous of which is the Penn Treebank (Marcus. et al., 1994), which has 2910 citations in Google scholar and is the single most cited paper on the ACL anthology network (Radev et al., 2009). Other examples include the CoNLL named entity corpus (Sang and Meulder (2003) with 348 citations on Google Scholar), the IMDB movie reviews sentiment data (Pang et al. (2002) with 894 citations) and the Amazon sentiment multi-domain data (Blitzer et al. (2007) with 109 citations) . MTurk means that creating similar data sets is now much cheaper and easier than ever before. It is highly likely that new MTurk produced data sets will achieve prominence and have significant impact. Additionally, the creation of shared data means more comparison and evaluation against previous work. Progress is m</context>
</contexts>
<marker>Radev, Muthukrishnan, Qazvinian, 2009</marker>
<rawString>Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed Qazvinian. 2009. The acl anthology network. In Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, pages 54–61, Suntec City, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyrus Rashtchian</author>
<author>Peter Young</author>
<author>Micah Hodosh</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Collecting image annotations using Amazon’s Mechanical Turk.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<marker>Rashtchian, Young, Hodosh, Hockenmaier, 2010</marker>
<rawString>Cyrus Rashtchian, Peter Young, Micah Hodosh, and Julia Hockenmaier. 2010. Collecting image annotations using Amazon’s Mechanical Turk. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Rose</author>
</authors>
<title>Some turn to ‘Mechanical’ job search. http://marketplace.publicradio.org/ display/web/2009/06/30/pm_turking/. Marketplace public radio. Air date:</title>
<date>2010</date>
<contexts>
<context position="8067" citStr="Rose, 2010" startWordPosition="1327" endWordPosition="1328">etween 66 other countries. Table 1 gives the survey results for questions relating to why people participate on Mechanical Turk. It shows that most US-based workers use Mechanical Turk for secondary income purposes (to have spending money for hobbies or going out), but that the overwhelming majority of them use it to spend their time more fruitfully (i.e., instead of watching TV). The economic downturn may have increased participation, with 30% of the USbased Turkers reporting that they are unemployed or underemployed. The public radio show Marketplace recently interviewed unemployed Turkers (Rose, 2010). It reports that they earn a little income, but that they do not earn enough to make a living. Figure 1 confirms this, giving a break down of how much time people spend on Mechanical Turk each week, how many HITs they complete, and how much money they earn. Most Turkers spend less than 8 hours per week on Mechanical Turk, and earn less than $10 per week through the site. 3 Quality Control Ipeirotis (2010) reports that just over half of Turkers have a college education. Despite being reasonably well educated, it is important to keep in mind that Turkers do not have training in specialized subj</context>
</contexts>
<marker>Rose, 2010</marker>
<rawString>Joel Rose. 2010. Some turn to ‘Mechanical’ job search. http://marketplace.publicradio.org/ display/web/2009/06/30/pm_turking/. Marketplace public radio. Air date: June 30, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Tjong Kim Sang</author>
<author>Fien De Meulder</author>
</authors>
<title>Introduction to the conll-2003 shared task: Languageindependent named entity recognition.</title>
<date>2003</date>
<booktitle>In CoNLL2003.</booktitle>
<marker>Sang, De Meulder, 2003</marker>
<rawString>Erik Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Languageindependent named entity recognition. In CoNLL2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast - but is it good? Evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP-2008),</booktitle>
<location>Honolulu, Hawaii.</location>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast - but is it good? Evaluating non-expert annotations for natural language tasks. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP-2008), Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Audrey Tong</author>
<author>Jerome Ajot</author>
<author>Mark Przybocki</author>
<author>Stephanie Strassel</author>
</authors>
<title>Document image collection using Amazon’s Mechanical Turk.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="23331" citStr="Tong et al., 2010" startWordPosition="3776" endWordPosition="3779">tor, allowing for an analysis of rating accuracy between native English and non-native English annotators. Novotney and Callison-Burch (2010b) used MTurk to elicit new speech samples. As part of an effort to increase the accessibility of public knowledge, such as Wikipedia, the team prompted Turkers to narrate Wikipedia articles. This required Turkers to record audio files and upload them. An additional HIT was used to evaluate the quality of the narrations. A particularly creative data collection approach asked Turkers to create handwriting samples and then to submit images of their writing (Tong et al., 2010). Turkers were asked to submit handwritten shopping lists (large vocabulary) or weather descriptions (small vocabulary) in either Arabic or Spanish. Subsequent Turkers provided a transcription and a translation. The team collected 18 images per language, 2 transcripts per image and 1 translation per transcript. 6.3 Sentiment, Polarity and Bias Two papers investigated the topics of sentiment, polarity and bias. Mellebeek et al. (2010) used several methods to obtain polarity scores for Spanish sentences expressing opinions about automative topics. They evaluated three HITs for collecting such da</context>
<context position="34663" citStr="Tong et al. (2010)" startWordPosition="5586" endWordPosition="5589">ns and teach systems how to learn from these to construct complex output? This would open more complex annotation tasks to MTurk. A general trend in research is that good ideas come from unexpected places. Major transformations in the field have come from creative new approaches. Consider the Penn Treebank, an ambitious and difficult project of unknown potential. Such large changes can be uncommon since they are often associated with high cost, as was the Penn Treebank. However, MTurk greatly reduces these costs, encouraging researchers to try creative new tasks. For example, in this workshop Tong et al. (2010) collected handwriting samples in multiple languages. Their creative data collection may or may not have a significant impact, but it is unlikely that it would have been tried had the cost been very high. Finally, while obtaining new data annotations from MTurk is cheap, it is not trivial. Workshop participants struggled with how to attract Turkers, how to price HITs, HIT design, instructions, cheating detection, etc. No doubt that as work progresses, so will a communal knowledge and experience of how to use MTurk. There can be great benefit in new toolkits for collecting language data using M</context>
</contexts>
<marker>Tong, Ajot, Przybocki, Strassel, 2010</marker>
<rawString>Audrey Tong, Jerome Ajot, Mark Przybocki, and Stephanie Strassel. 2010. Document image collection using Amazon’s Mechanical Turk. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis von Ahn</author>
<author>Laura Dabbish</author>
</authors>
<title>General techniques for designing games with a purpose.</title>
<date>2008</date>
<journal>Communications of the ACM.</journal>
<marker>von Ahn, Dabbish, 2008</marker>
<rawString>Luis von Ahn and Laura Dabbish. 2008. General techniques for designing games with a purpose. Communications of the ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Wang</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Cheap facts and counter-facts.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="20376" citStr="Wang and Callison-Burch (2010)" startWordPosition="3320" endWordPosition="3323">tivity of verbs in the style of Hopper and Thompson (1980), a theory that goes beyond simple grammatical transitivity – whether verbs take objects (transitive) or not – to capture the amount of action indicated by a sentence. Videos that portrayed verbs were shown to Turkers who described the actions shown in the video. Additionally, sentences containing the verbs were rated for aspect, affirmation, benefit, harm, kinesis, punctuality, and volition. The authors investigated several approaches for eliciting descriptions of transitivity from Turkers. Two teams explored textual entailment tasks. Wang and Callison-Burch (2010) created data for 7http://sites.google.com/site/ amtworkshop2010/ 5 recognizing textual entailment (RTE). They submitted 600 text segments and asked Turkers to identify facts and counter-facts (unsupported facts and contradictions) given the provided text. The resulting collection includes 790 facts and 203 counter-facts. Negri and Mehdad (2010) created a bi-lingual entailment corpus using English and Spanish entailment pairs, where the hypothesis and text come from different languages. The authors took a publicly available English RTE data set (the PASCAL-RTE3 dataset1) and created an English</context>
</contexts>
<marker>Wang, Callison-Burch, 2010</marker>
<rawString>Rui Wang and Chris Callison-Burch. 2010. Cheap facts and counter-facts. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tae Yano</author>
<author>Philip Resnik</author>
<author>Noah A Smith</author>
</authors>
<title>Shedding (a thousand points of) light on biased language.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="24005" citStr="Yano et al. (2010)" startWordPosition="3877" endWordPosition="3880">s (large vocabulary) or weather descriptions (small vocabulary) in either Arabic or Spanish. Subsequent Turkers provided a transcription and a translation. The team collected 18 images per language, 2 transcripts per image and 1 translation per transcript. 6.3 Sentiment, Polarity and Bias Two papers investigated the topics of sentiment, polarity and bias. Mellebeek et al. (2010) used several methods to obtain polarity scores for Spanish sentences expressing opinions about automative topics. They evaluated three HITs for collecting such data and compared results for quality and expressiveness. Yano et al. (2010) evaluated the political bias of blog posts. Annotators labeled 1000 sentences to determine biased phrases in political blogs from the 2008 election season. Knowledge of the annotators own biases allowed the authors to study how bias differs on the different ends of the political spectrum. 6.4 Information Retrieval Large scale evaluations requiring significant human labor for evaluation have a long history in the information retrieval community (TREC). Grady and Lease (2010) study four factors that influence Turker performance on a document relevance search task. The authors present some negat</context>
</contexts>
<marker>Yano, Resnik, Smith, 2010</marker>
<rawString>Tae Yano, Philip Resnik, and Noah A Smith. 2010. Shedding (a thousand points of) light on biased language. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meliha Yetisgen-Yildiz</author>
<author>Imre Solti</author>
<author>Scott Halgrim</author>
<author>Fei Xia</author>
</authors>
<title>Preliminary experiments with Amazon’s Mechanical Turk for annotating medical named entities.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="25860" citStr="Yetisgen-Yildiz et al. (2010)" startWordPosition="4160" endWordPosition="4163">(2010) automatically generate potential relation pairs in sentences by finding relation pairs appearing in news articles as given by a knowledge base. They ask Turkers if a sentence supports a relation, does not support a relation, or whether the relation makes sense. They collected close to 2500 annotations for 17 different person relation types. The other IE papers explored new genres and domains. Finin et al. (2010) obtained named entity annotations (person, organization, geopolitical entity) for several hundred Twitter messages. They conducted experiments using both MTurk and CrowdFlower. Yetisgen-Yildiz et al. (2010) explored medical named entity recognition. They selected 100 clinical trial announcements from ClinicalTrials.gov. 4 annotators for each of the 100 announcements identified 3 types of medical entities: medical conditions, medications, and laboratory test. 6.6 Machine Translation The most popular shared task topic was Machine Translation (MT). MT is a data hungry task that relies on huge corpora of parallel texts between two languages. Performance of MT systems depends on the size of training corpora, so there is a constant search for new and larger data sets. Such data sets are traditionally </context>
<context position="32465" citStr="Yetisgen-Yildiz et al., 2010" startWordPosition="5223" endWordPosition="5226">gnificant impact on low resource languages as corpora can be cheaply built for a much wider array of languages. As one example, Irvine and Klementiev (2010) collected data for 42 languages without worrying about how to find speakers of such a wide variety of languages. Additionally, the collection of Arabic nicknames requires a diverse and numerous Arabic speaking population (Higgins et al., 2010). In addition to extending into new languages, MTurk also allows for the creation of evaluation sets in new genres and domains, which was the focus of two papers in this workshop (Finin et al., 2010; Yetisgen-Yildiz et al., 2010). We expect to see new research emphasis on low resource languages and new domains and genres. Another factor is the change of data type and its impact on machine learning algorithms. With professional annotators, great time and care are paid to annotation guidelines and annotator training. These are difficult tasks with MTurk, which favors simple intuitive annotations and little training. Many papers applied creative methods of using simpler annotation tasks to create more complex data sets. This process can impact machine learning in a number of ways. Rather than a single gold standard, anno</context>
</contexts>
<marker>Yetisgen-Yildiz, Solti, Halgrim, Xia, 2010</marker>
<rawString>Meliha Yetisgen-Yildiz, Imre Solti, Scott Halgrim, and Fei Xia. 2010. Preliminary experiments with Amazon’s Mechanical Turk for annotating medical named entities. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Feasibility of human-in-the-loop minimum error rate training.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP 2009,</booktitle>
<pages>52--61</pages>
<contexts>
<context position="17348" citStr="Zaidan and Callison-Burch, 2009" startWordPosition="2854" endWordPosition="2857">nd Lowe, 2008), evaluating machine translation quality and crowdsouring translations (Callison-Burch, 2009), paraphrasing noun-noun compouds for SemEval (Butnariu et al., 2009), human evaluation of topic models (Chang et al., 2009), and speech transcription (McGraw et al., 2010; Marge et al., 2010a; Novotney and Callison-Burch, 2010a). Others have used MTurk for novel research directions like nonsimulated active learning for NLP tasks such as sentiment classification (Hsueh et al., 2009) or doing quixotic things like doing human-in-the-loop minimum error rate training for machine translation (Zaidan and Callison-Burch, 2009). Some projects have demonstrated the superscalability of crowdsourced efforts. Deng et al. (2009) used MTurk to construct ImageNet, an annotated image database containing 3.2 million that are hierarchically categorized using the WordNet ontology (Fellbaum, 1998). Because Mechanical Turk allows researchers to experiment with crowdsourcing by providing small incentives to Turkers, other successful crowdsourcing efforts like Wikipedia or Games with a Purpose (von Ahn and Dabbish, 2008) also share something in common with MTurk. 6 Shared Task The workshop included a shared task in which participa</context>
</contexts>
<marker>Zaidan, Callison-Burch, 2009</marker>
<rawString>Omar F. Zaidan and Chris Callison-Burch. 2009. Feasibility of human-in-the-loop minimum error rate training. In Proceedings of EMNLP 2009, pages 52–61, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar Zaidan</author>
<author>Juri Ganitkevitch</author>
</authors>
<title>An enriched MT grammar for under $100.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="28697" citStr="Zaidan and Ganitkevitch (2010)" startWordPosition="4596" endWordPosition="4599">gions in 1006 English/Chinese sentences. Turkers provided 4821 paraphrases for these regions. External resources can be an important part of an MT system. Irvine and Klementiev (2010) created lexicons for low resource languages. They evaluated translation candidates for 100 English words in 32 languages and solicited translations for 10 additional languages. Higgins et al. (2010) expanded name lists in Arabic by soliciting common Arabic nicknames. The 332 collected nicknames were primarily provided by Turkers in Arab speaking countries (35%), India (46%), and the United States (13%). Finally, Zaidan and Ganitkevitch (2010) explored how MTurk could be used to directly improve an MT grammar. Each rule in an Urdu to English translation system was characterized by 12 features. Turkers were provided examples for which their feedback was used to rescore grammar productions directly. This approach shows the potential of fine tuning an MT system with targeted feedback from annotators. 7 Future Directions Looking ahead, we can’t help but wonder what impact MTurk and crowdsourcing will have on the speech and language research community. Keeping in mind Niels Bohr’s famous exhortation “Pre7 diction is very difficult, espe</context>
</contexts>
<marker>Zaidan, Ganitkevitch, 2010</marker>
<rawString>Omar Zaidan and Juri Ganitkevitch. 2010. An enriched MT grammar for under $100. In NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>