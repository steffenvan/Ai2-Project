<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.9963845">
Everyday Language is Highly
Intensional
</title>
<author confidence="0.997667">
Allan Ramsay
</author>
<affiliation confidence="0.995084">
University of Manchester (UK)
</affiliation>
<email confidence="0.994476">
email: allan.ramsay@manchester.ac.uk
</email>
<author confidence="0.992581">
Debora Field
</author>
<affiliation confidence="0.996625">
University of Sheffield (UK)
</affiliation>
<email confidence="0.995374">
email: D.Field@sheffield.ac.uk
</email>
<sectionHeader confidence="0.989317" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999967466666667">
There has recently been a great deal of work aimed at trying to extract
information from substantial texts for tasks such as question answering.
Much of this work has dealt with texts which are reasonably large, but
which are known to contain reliable relevant information, e.g. FAQ lists,
on-line encyclopaedias, rather than looking at huge unorganised resources
such as the web. We believe, however, that even this work underestimates
the complexity and subtlety of language, and hence will inevitably be
restricted in what it can cope with. In particular, everyday use of lan-
guage involves considerable amounts of reasoning over intensional ob-
jects (properties and propositions). In order to respond appropriately to
simple-seeming questions such as ‘Is going for a walk good for me?’, for
instance, you have to be able to talk about event-types, which are intrinsi-
cally intensional. We discuss the issues involved in handling such items,
and shows the kind of background knowledge that is required for drawing
the appropriate conclusions about them.
</bodyText>
<page confidence="0.998353">
193
</page>
<note confidence="0.983703">
194 Ramsay and Field
</note>
<sectionHeader confidence="0.993803" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997313">
The work reported here aims to allow users to interact with a health information sys-
tem via natural language. In this context, allowing a user to make simple statements
about their condition and then ask questions about what they can or should do, as in
(1), seems to be a minimal requirement.
</bodyText>
<listItem confidence="0.4092">
(1) My doctor says I am allergic to eggs. Is it safe for me to eat cake?
</listItem>
<bodyText confidence="0.999711888888889">
Understanding such utterances requires the use of a highly intensional representa-
tion language, and responding to them requires a surprising amount of background
knowledge. We will consider below the problems that such everyday utterances bring
for formal paraphrases of natural language, and we will look at the kind of back-
ground knowledge that is required for producing the right kinds of response. In order
to produce a system that carries out the required inference we need access to an in-
ference engine for carrying out proofs in a representation language with the required
expressive power. The details of the engine we use are beyond the scope of this paper.
(Ramsay, 2001; Ramsay and Field, 2008). For the purposes of the current paper we
will simply show the results that can be obtained by using it.
The work reported here is complementary to work on corpus-based approaches
such as textual entailment: approaches that ignore the intensionality of everyday lan-
guage will inevitably fail to capture important inference patterns, but on the other hand
the work reported here cannot deal with large amounts of information provided as free
text. Ideally, the two approaches will be combined. The aim of the current paper is
to provide a reminder of the prevalence of intensionality in everyday language, and
to demonstrate that modern theorem proving techniques can cope with this kind of
knowledge without introducing undue processing delays.
</bodyText>
<sectionHeader confidence="0.995481" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.9998445">
The general idea behind the work reported here is that users will input statements
about their health, either spontaneously or in response to prompts from the system,
and will ask questions about what they can and should do, and the system will provide
them with appropriate guidance. The overall architecture is completely classical:
</bodyText>
<listItem confidence="0.938628909090909">
1. The user’s input is translated into a meaning representation (logical form, LF)
in some suitable representation language.
2. This LF contains a specification of the illocutionary force of the input (is it a
statement, or a question, or a command, or ... ?).
3. If the utterance is classified as a statement, its propositional content is added to
the system’s view of the user’s beliefs, and if it is classified as a question, the
system will attempt to use its background knowledge of the domain to answer
it. We are not currently attempting to make the system do anything in response
to a command from the user, since users do not generally issue commands in
our chosen domain, but clearly if this did happen then we would want to make
the system construct a plan to carry out the required action.
</listItem>
<bodyText confidence="0.9594929">
Everyday Language is Highly Intensional 195
This part of the system’s activity requires it be able to access and exploit relevant
background knowledge. This is obvious in the case of questions, but in the given
domain it is also important to be able to spot situations where the user’s beliefs
are incomplete or are in conflict with the system’s beliefs, since most people’s
understanding about medical topics is flawed. The ability to reason about what
has been said, then, is crucial to the construction of appropriate responses.
This architecture is entirely orthodox. What is unusual about the current work is
the emphasis on intensionality, so the first thing to do is examine why we believe that
this is such a significant problem.
</bodyText>
<listItem confidence="0.97217">
1. Doctors and patients make extensive use of generic NPs and bare plurals: ‘If
you follow this diet you should manage to control them without drugs’, ‘Do you
normally have snacks?’, ‘When I started chemotherapy, on the 2nd of August,
glycaemia was still rather high’ ...
</listItem>
<tableCaption confidence="0.828858176470588">
Such NPs are not, in fact, all that much more prevalent in this domain than in
general language. Across the BNC, for instance, it turns out that 27% of NPs
have ‘the’ as their determiner, 19% are bare plurals, 29% are bare singulars,
11% have ‘a’ or ‘an’ as their determiner, and the remainder have a variety of
other determiners1.
Thus bare plural and generic singular NPs occur about as frequently as ‘the’ and
‘a’, and substantially more freqently than ‘some’, ‘all’ and ‘every’ (less than
1% each). They have, however, been much less widely discussed by formal
semanticists, and there are a number of serious problems with the analyses that
have been proposed (Carlson, 1989; Ramsay, 1992; Cohen, 1994).
2. Everyday language is littered with words that can be used either as nouns or
verbs, and many of the apparently verbal uses of such words occur in essentially
nominal contexts. Table 1 shows the pattern of usage for three common words2,
but it should be noted that about 25% of the instances that are classified as
verbs are present participle forms, many of which are actually nominal or verbal
gerunds and hence should be regarded as nouns.
Table 1: Uses of common words in the BNC
</tableCaption>
<table confidence="0.947724">
Verb Noun Other
walk 75% 22% 3%
run 70% 24% 6%
kick 63% 35% 2%
</table>
<subsectionHeader confidence="0.4488975">
Axiomatisation of the semantics of such words requires considerable care, since
we need to ensure that all the examples in (2) have very similar consequences.
</subsectionHeader>
<bodyText confidence="0.36767">
(2) a. Swimming is good for you.
</bodyText>
<footnote confidence="0.965516333333333">
1The count of bare singulars is in fact a slight overestimate, since it includes some uses of singular nouns
as modifiers.
2The classification is taken directly from the BNC tags.
</footnote>
<note confidence="0.737772">
196 Ramsay and Field
</note>
<bodyText confidence="0.9330437">
b. Going for a swim is good for you.
c. It is good for you to go swimming.
3. The goal of the project is to produce appropriate responses to simple statements
and queries about a patient’s health. To do this, we need to be able to specify
a body of background knowledge in this area. We believe that for applications
such as medical information provision it is important that the information pro-
vided be as accurate as possible, and hence that it may be necessary to provide
the required background knowledge from scratch. This is, of course, a very
time-consuming and challenging activity, and it would be nice to be able to
side-step it by extracting the required information from existing texts. Unfortu-
nately, it seems likely that any such existing text will contain gaps which will
lead to the generation of partial, or wrong, answers. As noted above, ideally we
would want to link special purpose knowledge of the kind outlined here with
information extracted from existing texts, but for the current paper we are just
looking at what is involved in providing the required knowledge from scratch.
It turns out, as will be seen below, that much of this knowledge involves quantifica-
tion over situation types (of roughly the kind discussed by (Barwise and Perry, 1983)),
and in particular it involves statements about whether one situation type is a subset of
another, or is incompatible with it. This kind of knowledge is intrinsically intensional,
but it is hard to see how it can be avoided in this domain.
</bodyText>
<sectionHeader confidence="0.994301" genericHeader="method">
3 Logical forms
</sectionHeader>
<bodyText confidence="0.974319">
The logical forms that we use are fairly orthodox.
</bodyText>
<listItem confidence="0.994928875">
• We assume that events are first-class objects, as suggested by Davidson David-
son (1967, 1980).
• We allow other entities to play named roles with respect to these events, where
we denote that some item X is, for instance, the agent of some event E by writing
θ(E,agent,X): using this notation, rather than writing agent(E,X), allows us
to quantify over thematic roles, which in turn allows us to state generalisations
that would otherwise be awkward.
• We treat tense as a relation between speech time and ‘reference time’, and aspect
as a relation between reference time and event time, as suggested by Reichen-
bach Reichenbach (1947, 1956).
• We use ‘reference terms’ to denote referring expressions, so that ref (λXman(X))
is used to denote ‘the man’. Reference terms are similar to ‘anchors’ from (Bar-
wise and Perry, 1983), though the treatment is essentially proof-theoretic (sim-
ilar to the discussion of presupposition in (Gazdar, 1979; van der Sandt, 1992))
rather than model theoretic.
• Given that we are particularly concerned with the intensional nature of natural
</listItem>
<bodyText confidence="0.980966">
language, we need to use a formal language that supports intensionaly. The
Everyday Language is Highly Intensional 197
language we choose is a constructive version of property theory (Turner, 1987;
Ramsay, 2001). We have extended the theorem prover described in (Ramsay,
2001) to cope with reasoning about knowledge and belief, and we have shown
how this can be used to carry out interesting inferences in cooperative and non-
cooperative situations (Ramsay and Field, 2008).
We also include the surface illocutionary force in the LF, since this is part of the
meaning of the utterance and hence it seems sensible to include it in the LF. In partic-
ular, there are interactions between surface illocutionary force and other aspects of the
meaning which are hard to capture if you treat them independently. This is slightly
less standard than the other aspects of our LFs, but it does have the advantage that
these LFs keep all the information that we can obtain by inspecting the form of the
utterance in one place.
A typical example of an LF for a simple sentence is given in Figure 13.
</bodyText>
<listItem confidence="0.614079">
(3) The man loves a woman.
</listItem>
<equation confidence="0.917286833333333">
claim(EB : {woman(B)}
EC : {past(now,C)}
ED : {aspect(C,simplePast,D)}
9(D, agent, ref(λE(man(E))))
&amp;9(D,object,B)
&amp;event(D, love))
</equation>
<figureCaption confidence="0.998946">
Figure 1: Logical form for (3)
</figureCaption>
<bodyText confidence="0.999965785714286">
If you want to reason about utterances in natural language, e.g. in order to answer
questions on the basis of things you have been told, then there seems to be no alterna-
tive to constructing LFs of the kind in Figure 1, axiomatising the relevant background
knowledge, and then invoking your favourite theorem prover. Shallow semantic anal-
ysis simply does not provide the necessary detail, and it is very hard to link textual
entailment algorithms (Dagan et al., 2005) to complex domain knowledge. The crit-
ical issue in connecting NLP systems to rich axiomatisations of domain knowledge
seems likely to be that existing frameworks for constructing meaning representations
are not rich enough, not that they are too rich. In the remainder of this paper we will
explore three specific issues that have arisen in our attempt to use natural language
as a means for accessing medical knowledge. We have beoome sensitised to these
issues because of their importance for our application, but we believe that they are ac-
tually widespread, and they will need to be solved for any system which links natural
language to complex domain knowledge.
</bodyText>
<footnote confidence="0.802414">
4 Bare NPs
Consider (4):
3All the formal paraphrases in this paper are obtained from the target sentences by parsing the text and
using the standard techniques of compositional semantics.
</footnote>
<note confidence="0.669338">
198 Ramsay and Field
</note>
<listItem confidence="0.432876333333333">
(4) a. I am eating eggs.
b. I eat eggs.
c. I am allergic to eggs.
</listItem>
<bodyText confidence="0.991048034482759">
What is the status of ‘eggs’ in these sentences?
It is clear that in (4a) there are some eggs that I am eating, so that (4a) means some-
thing quite like ‘I am eating some eggs.’. (4b), on the other hand, means something
fairly different from ‘There are some eggs that I eat’, since it does not seem to commit
the speaker to the existence of any specific set of eggs. The use of the simple aspect
with a non-stative verb gives (4b) a habitual/repeated interpretation, saying that there
are numerous eating events, each of which involves at least one egg.
It seems, then, that it is possible to treat ‘eggs’ in (4a) and (4b) as a narrow scope
existential, with the simple aspect introducing a set of eating events of the required
kind.
You would not, however, want to paraphrase (4c) by saying that there are some eggs
to which I am allergic. (4b) says that there is a relationship between me and situations
where there is an egg present, namely that if I eat something which has been made
out of some part of an egg then I am likely to have an allergic reaction. The bare
plural ‘eggs’ in (4c) seems to have some of the force of a universal quantifier. This is
problematic: does the bare plural ‘eggs’ induce an existential or a universal reading,
or something entirely different?
Note that the word ‘eggs’ can appear as a free-standing NP (as in (4a)) or as the head
noun of an NP with an explicit determiner (as in ‘He was cooking some eggs.’). In the
latter context, the meaning of ‘eggs’ is normally taken be the property XX(egg(X)),
to be combined with the determiner ‘some’ to produce an existentially quantified ex-
pression which can be used as part of the interpretation of the entire sentence.
It is clear that there are constructions that involve allowing prepositions to take
nouns rather than NPs as their complements, in examples like ‘For example, cockerels
generally have more decorative plumage than hens’, where ‘example’ is evidently a
noun rather than an NP. If we allow the adjective ‘allergic’ to select for a PP with a
noun complement rather than an NP complement, we can obtain an interpretation of
(4c) which says that my allergy is a relation between me and the property of being an
egg (= the set of eggs) (Figure 2).
</bodyText>
<figure confidence="0.7405918">
utt(claim,
∃Bstate(B,allergic(to,
XC(egg(C))),
ref (XD(speaker(D)))!0)
&amp;aspect(now,simple,B))
</figure>
<figureCaption confidence="0.997308">
Figure 2: Logical form for (4c)
</figureCaption>
<bodyText confidence="0.978220571428571">
Thus we can distinguish between cases where ‘eggs’ is being used as an NP, where
it introduces a narrow scope existential quantifier, and ones where it is being used as
an NN, where it denotes, as usual, the property X(X,egg(X)). We still have to work
Everyday Language is Highly Intensional 199
out saying that the relationship ‘allergic’ holds between me and the property of being
an egg, but at least we have escaped the trap of saying that it holds between me and
some eggs (or indeed all eggs). We will return ton this in §6
</bodyText>
<sectionHeader confidence="0.979904" genericHeader="method">
5 Nominalisations and paraphrases
</sectionHeader>
<bodyText confidence="0.99892325">
As noted above, there are often numerous ways of saying very much the same thing,
and these often involve using combinations of nominal and verbal forms of the same
root. To cope with these, we have to do two things: we have to construct appropriate
logical forms, and we have to spot cases where we believe that there is no significant
difference between the various natural language forms and introduce appropriate rules
for treating one as canonical.
Gerunds and gerundives occur in very much the same places as bare NPs, and have
very much the same feeling of being about types of entity.
</bodyText>
<listItem confidence="0.9984635">
(5) a. Exercise is good for you.
b. Swimming is good for you.
(6) a. I like watching old movies.
b. I like old movies.
</listItem>
<bodyText confidence="0.9899005">
It therefore seems natural to treat them in much the same way, as descriptions of
event types, as in Figure 3
</bodyText>
<figure confidence="0.806742833333333">
utt(claim,
∃Bstate(B,
λC(∃Devent(D,swim) &amp; θ(D,agent,C)),
λE(good(E)))
&amp;for(B,ref(λF(hearer(F)))!4)
&amp;aspect(now,simple,B))
</figure>
<figureCaption confidence="0.998887">
Figure 3: Logical form for (5b)
</figureCaption>
<bodyText confidence="0.92692">
The logical form in Figure 3 says that there is a state of affairs relating events where
someone does some swimming and the property of being good, and that this state of
affairs concerns the speaker. This does at least have the benefit of exposing the key
concepts mentioned in (5b), and of doing so in such a way that it is possible to write
rules that support appropriate chains of inference.
The kind of inference we are interested in concerns patterns like the ones in Figure 4
Exercise is good for you if you are overweight
Swimming is a form of exercise
I am obese
Should I go swimming?
</bodyText>
<figureCaption confidence="0.996981">
Figure 4: A simple(!) pattern of natural reasoning
</figureCaption>
<bodyText confidence="0.979145083333333">
200 Ramsay and Field
We will discuss the rules and inference engine that are required in order to support
this kind of reasoning in §6 and §7. For now we are concerned with the fact that the
last line in Figure 4 could have been replaced by a number of alternative forms such
as ‘Is swimming good for me?’ or ‘Is it good for me to go swimming’ without any
substantial change of meaning.
In general, we believe that determining the relationships between sentences re-
quires inference based on background rules which describe the relationships between
terms. However, when we have forms which are essentially paraphrases of one an-
other, these rules will tend to be bi-equivalences–rules of the form P ↔ Q. Such rules
are awkward for any theorem prover, since they potentially introduce infinite loops:
in order to prove P you can try proving Q, where one of the possible ways of proving
Q is by proving P, ... It is possible to catch such loops, and our inference engine does
monitor for various straightforward loops of this kind, but they do introduce an extra
overhead. Equivalences of this kind are, in any case, not really facts about the world so
much as facts about the way natural language describes the world. It seems therefore
more sensible to capture them at the point when we construct our logical forms, when
they can be dealt with by straightforward pattern matching and substitution on logical
forms, rather than by embodying them as bi-directional rules to be used as required by
the inference engine. We use rules of the kind given in Figure 5 to canonical versions
of logical forms for sentences which we regard as mutual paraphrases. These rules
are matched against elements of the logical form, and the required substitutions are
made. This process is applied iteratively, so that multiple rules can be applied when
necessary.
</bodyText>
<equation confidence="0.998679555555556">
∃B : {allergy(B,C)}
∃Devent(D,have) &amp; θ(D,object,B)
&amp; θ(D,agent,E) &amp; aspect(X,Y,D)
↔ ∃Fstate(F,E,λG(allergic(G)),to(C))
&amp; aspect(X,Y,F)
event(B,go)
&amp;θ(B,event,λC(event(C,D)))
&amp;θ(B,agent,E)
↔ event(B,D) &amp;θ(B,agent,E)
</equation>
<figureCaption confidence="0.990098">
Figure 5: Canonical form rules
</figureCaption>
<bodyText confidence="0.966142">
The first of the rules in Figure 5 captures the equivalences between ‘I have an al-
lergy to eggs’ and ‘I am allergic to eggs’, ‘having an allergy to milk is bad news’ and
‘being allergic to milk is bad news’, and so on, and the second captures the equiv-
alences between ‘I like walking’ and ‘I like going walking’, ‘Swimming is good for
you’ and ‘Going for a swim is good for you’, and so on. These equivalences have to
be captured somewhere, and we believe that canonical forms of this kind arte a good
way to do it. We will return to where the rules in Figure 5 come from in §8.
Everyday Language is Highly Intensional 201
</bodyText>
<sectionHeader confidence="0.996917" genericHeader="method">
6 Intensional predicates
</sectionHeader>
<bodyText confidence="0.996641714285714">
The material we are interested in, like all natural language, makes extensive use of
intensional predicates. The adjective ‘good’ in ‘Going swimming is good for you’ ex-
presses a relationship between an event type (‘going swimming’) and an individual;
the verb ‘make’ in ‘Eating raw meat will make you feel sick’ expresses a relation-
ship between an event type (‘eating raw meat’) and a state of affairs (‘you are ill’).
Constructions like these are widespread, and are inherently intensional. To draw con-
clusions about sentences involving them, you have to be able to reason about whether
one event type or one parameterised state of affairs is a subset of another, which is the
essence of intensionality.
Once you recognise that examples like these involve event types and propositions,
it is fairly straightforward to construct appropriate logical forms. We simply use the
notation of the λ-calculus to depict abstractions (e.g. event types), and we allow propo-
sitions to appear in argument positions, and standard techniques from comppsitional
semantics do the rest.
</bodyText>
<equation confidence="0.996539846153846">
3C : {future(now,C)}
3Bevent(B,make)
&amp;θ(B,
scomp,
λD(event(D,feel)
&amp; θ(D,object,λE(sick(E)))
&amp; θ(D,agent,ref(λF(hearer(F)))!5)))
&amp;θ(B,
cause,
λG 3Hevent(H,eat)
&amp;3I : {raw(I) &amp; meat(I)}θ(H,object,I)
&amp;θ(H,agent,G))
&amp;aspect(C,simple,B)
</equation>
<figureCaption confidence="0.995292">
Figure 6: Eating raw meat will make you feel sick
</figureCaption>
<bodyText confidence="0.999751933333333">
Figure 6 describes a relationship between situations where you eat raw meat and
ones where you feel sick. This is entirely correct: what else could this sentence de-
note?
Constructing formal paraphrases for sentences involving intensional predicates is
thus both straightforward (so long as you can parse them) and essential. Formal lan-
guages that support such paraphrases are, however, potentially problematic. The key
problem is that such languages tend to permit paradoxical constructions such as the
Liar Paradox and Ruessll’s set which introduce sentences which are true if and only if
they are false. It is difficult to provide semantics for languages which allow paradoxes
to be stated, but there are a number of ways out of this dilemma, either by putting
syntactic restrictions on what can be said (Whitehead and Russell, 1925; Jech, 1971)
or by devising appropriate interpretations (Turner, 1987; Aczel, 1988). We choose to
employ a constructive variant of property theory, because it allows us a comparatively
straightforward and implemetable proof theory, but it does not really matter what you
choose. What does matter is that if you choose a language with less expressive power
</bodyText>
<note confidence="0.868688">
202 Ramsay and Field
</note>
<bodyText confidence="0.996092666666667">
than natural language, such as description logic, your paraphrases must fail to support
some of the distinctions that are expressible in natural language, and as a consequence
you will inevitably draw incorrect conclusions from the texts you are processing.
</bodyText>
<sectionHeader confidence="0.992634" genericHeader="method">
7 Inference
</sectionHeader>
<reference confidence="0.3995405">
Consider (7):
(7) a. Eating eggs will make you ill if you are allergic to eggs.
b. I am allergic to eggs.
c. Will eating fried-egg sandwiches make me ill?
</reference>
<bodyText confidence="0.999964518518519">
It is pretty obvious that the answer to (7c), given (7a) and (7b), must be ‘Yes’. The
reasoning that is required to arrive at this answer turns out to be suprisingly complex.
The problem is, as noted above, that we need to reason about relationships between
event types. We need to be able to spot that events where someone eats a fried-egg
sandwich involve situations where they eat an egg. It is clearly quite easy, if tedious,
to write rules that say that if someone eats something which contains an egg then they
must eat an egg, and that fried-egg sandwiches contain eggs. The trouble is that we
have to be able invoke this rule in order to determine whether the arguments of ‘make’
are of the right kind. Because we are (correctly) allowing event types as arguments
in intensional predicates, we have to be able to invoke arbitrary and unpredictable
amounts of inference even to determine whether the arguments of a predicate are ad-
missible. Roughly speaking, we have to be prepared to carry out arbitrary amounts of
inference at the point where first-order theorem provers invoke unification.
There is nothing to stop us doing this. Sorted logics, for instance, use an extended
notion of unification to try to ensure that items that are being considered as arguments
have specific properties (Cohn, 1987). We can, indeed, do any computation we like in
order to verify the suitability of arguments. The more complex the computations we
perform, of course, the longer it may take to come to a decision. The key is thus to
try to bound the potential costs without compromising what we can do too much. We
exploit a notion of ‘guarded’ axioms, where we allow arbitrary amounts of reasoning
to be performed to verify that some item fits a fully specified description, but we do
not allow such reasoning to be used for generating candidates. We do, of course, have
to put a bound on the amount of work that will be done at any point, as indeed any
inference engine for a language as expressive as first-order logic must do. In general,
however, using guarded intensionality in this way allows us to cover a wide range
of cases which are simply inexpressible using first-order logic (or any fragment of
first-order logic, such as description logic) comparatively inexpensively.
</bodyText>
<sectionHeader confidence="0.999378" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.994923217391304">
We have argued that in order to cope properly with even quite straightforward uses
of language, you need large amounts of background knowledge, much of which has
to be couched in some highly intensional framework, and you need inference engines
which can manipulate this knowledge. In the body of the paper we have shown a
number of examples which we believe illustrate this argument, and have looked at the
representations and rules that we employ for dealing with these cases. The natural
Everyday Language is Highly Intensional 203
question that arises at this point is: that’s all very well, but can the approach outlined
here be extended to cover a more substantial set of cases?
There are two key issues here. How difficult is it to capture a reasonably substantial
body of knowledge within the framework we have outlined, and what will happen to
the inference engine when we do?
Writing rules in property theory is very hard work. Writing rules in property theory
which will mesh nicely with logical forms obtained from natural language sentences
is extremely hard work. If we had to hand-code the rules we want directly in property
theory (or indeed in any formal language) then the approach discussed here would,
clearly, be impossible to extend to cover more than a handful of cases. Fortunately,
however, we have a much easier way of constructing rules. We have, after all, a
mechanism for converting natural language sentences into logical forms. So if we
state the rules we want in natural language we will obtain logical forms of those rules,
and furthermore those paraphrases will automatically be couched in terms which mesh
nicely with logical forms obtained from other natural language sentences. Thus (8)
produces the rule in Figure 7
</bodyText>
<figure confidence="0.996958153846154">
(8) Eating Y will make X ill if X is allergic to Y.
bCbD]Estate(E,C,λF(allergic(F))) &amp; to(E,D)
&amp; aspect(now,simple,E)
→ ]G : {future(now,G)}
]Bevent(B,make)
&amp;θ(B,object,C)
&amp;θ(B,object1,λH(ill(H)))
&amp;θ(B,
agent,
λI ]Jevent(J,eat)
&amp; θ(J,object,D)
&amp; θ(J,agent,I))
&amp;aspect(G,simple,B)
</figure>
<figureCaption confidence="0.999902">
Figure 7: Logical form for (8)
</figureCaption>
<bodyText confidence="0.999969307692308">
Writing rules like (8) is clearly easier than producing formulae like Figure 7 by
hand. Writing down all the knowledge you need in order to cope with a non-trivial
domain is still a very substantial task, but doing it in English is at least feasible in a
way that doing it directly in a formal language is not.
How will the inference engine cope when confronted with thousands of rules? Very
large parts of everyday knowledge can, in fact, be expressed pretty much as Horn
clauses. Our inference engine converts Horn clauses into (almost) pure Prolog, and
there is certainly no problem in using very large sets of Horn clauses converted to
this form (a modern Prolog system will cope comfortably with sets of several hun-
dred thousand Horn clauses, and will carry out substantial inference chains involving
such sets in small fractions of a second). The only concern here relates to non-Horn
clauses (which do not tend to occur all that frequently in rules explaining the rela-
tionships between natural language terms) and intensional rules. The fact that most
</bodyText>
<note confidence="0.495446">
204 Ramsay and Field
</note>
<bodyText confidence="0.99965975">
intensional rules are guarded has certainly meant that so far we have not encountered
any problems when using them, and we are hopeful that this will remain the case.
In any case, there is an alternative question to be answered: what will happen if
you don’t take the approach outlined here? All the phenomena we have discussed are
widespread–bare plurals, mutual paraphrases, intensional attitudes all occur all over
the place. It is extremely hard to see that systems that rely on surface patterns (either
directly, as in textual entailment, or indirectly through shallow parsing/information
extraction) can support the kind of reasoning required for getting from ‘I have an
allergy to eggs.’ to ‘It is dangerous for me to eat pancakes’, so at some point inference
based on background knowledge will have to be invoked. There seems little alternative
to constructing formal paraphrases that capture the subtleties of natural language in all
its glory. If you don’t, then you will by definition lose some of the information that
was expressed in the text, and that will inevitably mean that you get things wrong.
There is no way round it: either you bite the bullet, construct formal paraphrases that
capture the content of the input and use them to carry out inference, or you will get
some things wrong.
</bodyText>
<sectionHeader confidence="0.999426" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99911672972973">
Aczel, P. (1988). Non-Well-Founded-Sets. Stanford: CSLI Publications.
Barwise, J. and J. Perry (1983). Situations and Attitudes. Cambridge, MA: Bradford
Books.
Carlson, G. (1989). On the semantic composition of English generic sentences. In
G. Chierchia, B. H. Partee, and R. Turner (Eds.), Properties, Types and Meaning
II: Semantic Issues, Dordrecht, pp. 167–192. Kluwer Academic Press.
Cohen, A. (1994). Reasoning with generics. In H. C. Bunt (Ed.), 1st International
Workshop on Computational Semantics, University of Tilburg, pp. 263–270.
Cohn, A. G. (1987). A more expressive formulation of many sorted logic. Journal of
Automated Reasoning 3, 113–200.
Dagan, I., B. Magnini, and O. Glickman (2005). The PASCAL recognising textual en-
tailment challenge. In Proceedings of Pascal Challenge Workshop on Recognizing
Textual Entailment.
Davidson, D. (1967). The logical form of action sentences. In N. Rescher (Ed.), The
Logic of Decision and Action, Pittsburgh. University of Pittsburgh Press.
Davidson, D. (1980). Essays on actions and events. Oxford: Clarendon Press.
Gazdar, G. (1979). Pragmatics: Implicature, Presupposition and Logical Form. New
York: Academic Press.
Jech, T. J. (1971). Lectures in Set Theory, with Particular Emphasis on the Method of
Forcing. Berlin: Springer Verlag (Lecture Notes in Mathematics 217).
Everyday Language is Highly Intensional 205
Ramsay, A. M. (1992). Bare plural NPs and habitual VPs. In Proceedings of the 14th
International Conference on Computational Linguistics (COLING-92), Nantes, pp.
226–231.
Ramsay, A. M. (2001). Theorem proving for untyped constructive λ-calculus: imple-
mentation and application. Logic Journal of the Interest Group in Pure and Applied
Logics 9(1), 89–106.
Ramsay, A. M. and D. G. Field (2008). Speech acts, epistemic planning and Grice’s
maxims. Journal of Logic and Computation 18(3), 431–457.
Reichenbach, H. (1947). Elements of Symbolic Logic. New York: The Free Press.
Reichenbach, H. (1956). The Direction of Time. Berkeley: University of California
Press.
Turner, R. (1987). A theory of properties. Journal of Symbolic Logic 52(2), 455–472.
van der Sandt, R. (1992). Presupposition projection as anaphora resolution. Journal
of Semantics 9, 333–377.
Whitehead, A. N. and B. Russell (1925). Principia Mathematica. Cambridge: Cam-
bridge University Press.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.194506">
<title confidence="0.9912615">Everyday Language is Highly Intensional</title>
<author confidence="0.999967">Allan Ramsay</author>
<affiliation confidence="0.999765">University of Manchester (UK)</affiliation>
<author confidence="0.867221">Debora Field</author>
<affiliation confidence="0.991537">University of Sheffield (UK)</affiliation>
<abstract confidence="0.999330875">There has recently been a great deal of work aimed at trying to extract information from substantial texts for tasks such as question answering. Much of this work has dealt with texts which are reasonably large, but which are known to contain reliable relevant information, e.g. FAQ lists, on-line encyclopaedias, rather than looking at huge unorganised resources such as the web. We believe, however, that even this work underestimates the complexity and subtlety of language, and hence will inevitably be restricted in what it can cope with. In particular, everyday use of language involves considerable amounts of reasoning over intensional objects (properties and propositions). In order to respond appropriately to questions such as going for a walk good for for instance, you have to be able to talk about event-types, which are intrinsically intensional. We discuss the issues involved in handling such items, and shows the kind of background knowledge that is required for drawing the appropriate conclusions about them.</abstract>
<date confidence="0.309822">193</date>
<address confidence="0.670556">194 Ramsay and Field</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>Consider (7): (7) a. Eating eggs will make you ill if you are allergic to eggs. b. I am allergic to eggs.</title>
<marker></marker>
<rawString>Consider (7): (7) a. Eating eggs will make you ill if you are allergic to eggs. b. I am allergic to eggs.</rawString>
</citation>
<citation valid="true">
<authors>
<author>c Will</author>
</authors>
<title>eating fried-egg sandwiches make me ill? Aczel,</title>
<date>1988</date>
<publisher>CSLI Publications.</publisher>
<location>P.</location>
<marker>Will, 1988</marker>
<rawString>c. Will eating fried-egg sandwiches make me ill? Aczel, P. (1988). Non-Well-Founded-Sets. Stanford: CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Barwise</author>
<author>J Perry</author>
</authors>
<title>Situations and Attitudes.</title>
<date>1983</date>
<publisher>Bradford Books.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="8196" citStr="Barwise and Perry, 1983" startWordPosition="1365" endWordPosition="1368">ting the required information from existing texts. Unfortunately, it seems likely that any such existing text will contain gaps which will lead to the generation of partial, or wrong, answers. As noted above, ideally we would want to link special purpose knowledge of the kind outlined here with information extracted from existing texts, but for the current paper we are just looking at what is involved in providing the required knowledge from scratch. It turns out, as will be seen below, that much of this knowledge involves quantification over situation types (of roughly the kind discussed by (Barwise and Perry, 1983)), and in particular it involves statements about whether one situation type is a subset of another, or is incompatible with it. This kind of knowledge is intrinsically intensional, but it is hard to see how it can be avoided in this domain. 3 Logical forms The logical forms that we use are fairly orthodox. • We assume that events are first-class objects, as suggested by Davidson Davidson (1967, 1980). • We allow other entities to play named roles with respect to these events, where we denote that some item X is, for instance, the agent of some event E by writing θ(E,agent,X): using this notat</context>
</contexts>
<marker>Barwise, Perry, 1983</marker>
<rawString>Barwise, J. and J. Perry (1983). Situations and Attitudes. Cambridge, MA: Bradford Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Carlson</author>
</authors>
<title>On the semantic composition of English generic sentences. In</title>
<date>1989</date>
<pages>167--192</pages>
<publisher>Kluwer Academic Press.</publisher>
<location>Dordrecht,</location>
<contexts>
<context position="5952" citStr="Carlson, 1989" startWordPosition="976" endWordPosition="977">omain than in general language. Across the BNC, for instance, it turns out that 27% of NPs have ‘the’ as their determiner, 19% are bare plurals, 29% are bare singulars, 11% have ‘a’ or ‘an’ as their determiner, and the remainder have a variety of other determiners1. Thus bare plural and generic singular NPs occur about as frequently as ‘the’ and ‘a’, and substantially more freqently than ‘some’, ‘all’ and ‘every’ (less than 1% each). They have, however, been much less widely discussed by formal semanticists, and there are a number of serious problems with the analyses that have been proposed (Carlson, 1989; Ramsay, 1992; Cohen, 1994). 2. Everyday language is littered with words that can be used either as nouns or verbs, and many of the apparently verbal uses of such words occur in essentially nominal contexts. Table 1 shows the pattern of usage for three common words2, but it should be noted that about 25% of the instances that are classified as verbs are present participle forms, many of which are actually nominal or verbal gerunds and hence should be regarded as nouns. Table 1: Uses of common words in the BNC Verb Noun Other walk 75% 22% 3% run 70% 24% 6% kick 63% 35% 2% Axiomatisation of the</context>
</contexts>
<marker>Carlson, 1989</marker>
<rawString>Carlson, G. (1989). On the semantic composition of English generic sentences. In G. Chierchia, B. H. Partee, and R. Turner (Eds.), Properties, Types and Meaning II: Semantic Issues, Dordrecht, pp. 167–192. Kluwer Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Cohen</author>
</authors>
<title>Reasoning with generics. In</title>
<date>1994</date>
<booktitle>1st International Workshop on Computational Semantics, University of Tilburg,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="5980" citStr="Cohen, 1994" startWordPosition="980" endWordPosition="981">e. Across the BNC, for instance, it turns out that 27% of NPs have ‘the’ as their determiner, 19% are bare plurals, 29% are bare singulars, 11% have ‘a’ or ‘an’ as their determiner, and the remainder have a variety of other determiners1. Thus bare plural and generic singular NPs occur about as frequently as ‘the’ and ‘a’, and substantially more freqently than ‘some’, ‘all’ and ‘every’ (less than 1% each). They have, however, been much less widely discussed by formal semanticists, and there are a number of serious problems with the analyses that have been proposed (Carlson, 1989; Ramsay, 1992; Cohen, 1994). 2. Everyday language is littered with words that can be used either as nouns or verbs, and many of the apparently verbal uses of such words occur in essentially nominal contexts. Table 1 shows the pattern of usage for three common words2, but it should be noted that about 25% of the instances that are classified as verbs are present participle forms, many of which are actually nominal or verbal gerunds and hence should be regarded as nouns. Table 1: Uses of common words in the BNC Verb Noun Other walk 75% 22% 3% run 70% 24% 6% kick 63% 35% 2% Axiomatisation of the semantics of such words req</context>
</contexts>
<marker>Cohen, 1994</marker>
<rawString>Cohen, A. (1994). Reasoning with generics. In H. C. Bunt (Ed.), 1st International Workshop on Computational Semantics, University of Tilburg, pp. 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A G Cohn</author>
</authors>
<title>A more expressive formulation of many sorted logic.</title>
<date>1987</date>
<journal>Journal of Automated Reasoning</journal>
<volume>3</volume>
<pages>113--200</pages>
<marker>Cohn, 1987</marker>
<rawString>Cohn, A. G. (1987). A more expressive formulation of many sorted logic. Journal of Automated Reasoning 3, 113–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>B Magnini</author>
<author>O Glickman</author>
</authors>
<title>The PASCAL recognising textual entailment challenge.</title>
<date>2005</date>
<booktitle>In Proceedings of Pascal Challenge Workshop on Recognizing Textual Entailment.</booktitle>
<contexts>
<context position="11324" citStr="Dagan et al., 2005" startWordPosition="1886" endWordPosition="1889">{woman(B)} EC : {past(now,C)} ED : {aspect(C,simplePast,D)} 9(D, agent, ref(λE(man(E)))) &amp;9(D,object,B) &amp;event(D, love)) Figure 1: Logical form for (3) If you want to reason about utterances in natural language, e.g. in order to answer questions on the basis of things you have been told, then there seems to be no alternative to constructing LFs of the kind in Figure 1, axiomatising the relevant background knowledge, and then invoking your favourite theorem prover. Shallow semantic analysis simply does not provide the necessary detail, and it is very hard to link textual entailment algorithms (Dagan et al., 2005) to complex domain knowledge. The critical issue in connecting NLP systems to rich axiomatisations of domain knowledge seems likely to be that existing frameworks for constructing meaning representations are not rich enough, not that they are too rich. In the remainder of this paper we will explore three specific issues that have arisen in our attempt to use natural language as a means for accessing medical knowledge. We have beoome sensitised to these issues because of their importance for our application, but we believe that they are actually widespread, and they will need to be solved for a</context>
</contexts>
<marker>Dagan, Magnini, Glickman, 2005</marker>
<rawString>Dagan, I., B. Magnini, and O. Glickman (2005). The PASCAL recognising textual entailment challenge. In Proceedings of Pascal Challenge Workshop on Recognizing Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Davidson</author>
</authors>
<title>The logical form of action sentences. In</title>
<date>1967</date>
<booktitle>The Logic of Decision</booktitle>
<editor>N. Rescher (Ed.),</editor>
<publisher>Press.</publisher>
<institution>and Action, Pittsburgh. University of Pittsburgh Press.</institution>
<location>Davidson, D.</location>
<contexts>
<context position="8593" citStr="Davidson (1967" startWordPosition="1435" endWordPosition="1437"> providing the required knowledge from scratch. It turns out, as will be seen below, that much of this knowledge involves quantification over situation types (of roughly the kind discussed by (Barwise and Perry, 1983)), and in particular it involves statements about whether one situation type is a subset of another, or is incompatible with it. This kind of knowledge is intrinsically intensional, but it is hard to see how it can be avoided in this domain. 3 Logical forms The logical forms that we use are fairly orthodox. • We assume that events are first-class objects, as suggested by Davidson Davidson (1967, 1980). • We allow other entities to play named roles with respect to these events, where we denote that some item X is, for instance, the agent of some event E by writing θ(E,agent,X): using this notation, rather than writing agent(E,X), allows us to quantify over thematic roles, which in turn allows us to state generalisations that would otherwise be awkward. • We treat tense as a relation between speech time and ‘reference time’, and aspect as a relation between reference time and event time, as suggested by Reichenbach Reichenbach (1947, 1956). • We use ‘reference terms’ to denote referri</context>
</contexts>
<marker>Davidson, 1967</marker>
<rawString>Davidson, D. (1967). The logical form of action sentences. In N. Rescher (Ed.), The Logic of Decision and Action, Pittsburgh. University of Pittsburgh Press. Davidson, D. (1980). Essays on actions and events. Oxford: Clarendon Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
</authors>
<title>Pragmatics: Implicature, Presupposition and Logical Form.</title>
<date>1979</date>
<publisher>Academic Press.</publisher>
<location>New York:</location>
<contexts>
<context position="9447" citStr="Gazdar, 1979" startWordPosition="1576" endWordPosition="1577">lows us to quantify over thematic roles, which in turn allows us to state generalisations that would otherwise be awkward. • We treat tense as a relation between speech time and ‘reference time’, and aspect as a relation between reference time and event time, as suggested by Reichenbach Reichenbach (1947, 1956). • We use ‘reference terms’ to denote referring expressions, so that ref (λXman(X)) is used to denote ‘the man’. Reference terms are similar to ‘anchors’ from (Barwise and Perry, 1983), though the treatment is essentially proof-theoretic (similar to the discussion of presupposition in (Gazdar, 1979; van der Sandt, 1992)) rather than model theoretic. • Given that we are particularly concerned with the intensional nature of natural language, we need to use a formal language that supports intensionaly. The Everyday Language is Highly Intensional 197 language we choose is a constructive version of property theory (Turner, 1987; Ramsay, 2001). We have extended the theorem prover described in (Ramsay, 2001) to cope with reasoning about knowledge and belief, and we have shown how this can be used to carry out interesting inferences in cooperative and noncooperative situations (Ramsay and Field</context>
</contexts>
<marker>Gazdar, 1979</marker>
<rawString>Gazdar, G. (1979). Pragmatics: Implicature, Presupposition and Logical Form. New York: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T J Jech</author>
</authors>
<title>Lectures in Set Theory, with Particular Emphasis on the Method of Forcing.</title>
<date>1971</date>
<booktitle>Verlag (Lecture Notes in Mathematics 217). Everyday Language is Highly Intensional 205</booktitle>
<publisher>Springer</publisher>
<location>Berlin:</location>
<contexts>
<context position="21759" citStr="Jech, 1971" startWordPosition="3644" endWordPosition="3645">l predicates is thus both straightforward (so long as you can parse them) and essential. Formal languages that support such paraphrases are, however, potentially problematic. The key problem is that such languages tend to permit paradoxical constructions such as the Liar Paradox and Ruessll’s set which introduce sentences which are true if and only if they are false. It is difficult to provide semantics for languages which allow paradoxes to be stated, but there are a number of ways out of this dilemma, either by putting syntactic restrictions on what can be said (Whitehead and Russell, 1925; Jech, 1971) or by devising appropriate interpretations (Turner, 1987; Aczel, 1988). We choose to employ a constructive variant of property theory, because it allows us a comparatively straightforward and implemetable proof theory, but it does not really matter what you choose. What does matter is that if you choose a language with less expressive power 202 Ramsay and Field than natural language, such as description logic, your paraphrases must fail to support some of the distinctions that are expressible in natural language, and as a consequence you will inevitably draw incorrect conclusions from the tex</context>
</contexts>
<marker>Jech, 1971</marker>
<rawString>Jech, T. J. (1971). Lectures in Set Theory, with Particular Emphasis on the Method of Forcing. Berlin: Springer Verlag (Lecture Notes in Mathematics 217). Everyday Language is Highly Intensional 205</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Ramsay</author>
</authors>
<title>Bare plural NPs and habitual VPs.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics (COLING-92), Nantes,</booktitle>
<pages>226--231</pages>
<contexts>
<context position="5966" citStr="Ramsay, 1992" startWordPosition="978" endWordPosition="979">eneral language. Across the BNC, for instance, it turns out that 27% of NPs have ‘the’ as their determiner, 19% are bare plurals, 29% are bare singulars, 11% have ‘a’ or ‘an’ as their determiner, and the remainder have a variety of other determiners1. Thus bare plural and generic singular NPs occur about as frequently as ‘the’ and ‘a’, and substantially more freqently than ‘some’, ‘all’ and ‘every’ (less than 1% each). They have, however, been much less widely discussed by formal semanticists, and there are a number of serious problems with the analyses that have been proposed (Carlson, 1989; Ramsay, 1992; Cohen, 1994). 2. Everyday language is littered with words that can be used either as nouns or verbs, and many of the apparently verbal uses of such words occur in essentially nominal contexts. Table 1 shows the pattern of usage for three common words2, but it should be noted that about 25% of the instances that are classified as verbs are present participle forms, many of which are actually nominal or verbal gerunds and hence should be regarded as nouns. Table 1: Uses of common words in the BNC Verb Noun Other walk 75% 22% 3% run 70% 24% 6% kick 63% 35% 2% Axiomatisation of the semantics of </context>
</contexts>
<marker>Ramsay, 1992</marker>
<rawString>Ramsay, A. M. (1992). Bare plural NPs and habitual VPs. In Proceedings of the 14th International Conference on Computational Linguistics (COLING-92), Nantes, pp. 226–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Ramsay</author>
</authors>
<title>Theorem proving for untyped constructive λ-calculus: implementation and application.</title>
<date>2001</date>
<journal>Logic Journal of the Interest Group in Pure and Applied Logics</journal>
<volume>9</volume>
<issue>1</issue>
<pages>89--106</pages>
<contexts>
<context position="2343" citStr="Ramsay, 2001" startWordPosition="373" endWordPosition="374">al representation language, and responding to them requires a surprising amount of background knowledge. We will consider below the problems that such everyday utterances bring for formal paraphrases of natural language, and we will look at the kind of background knowledge that is required for producing the right kinds of response. In order to produce a system that carries out the required inference we need access to an inference engine for carrying out proofs in a representation language with the required expressive power. The details of the engine we use are beyond the scope of this paper. (Ramsay, 2001; Ramsay and Field, 2008). For the purposes of the current paper we will simply show the results that can be obtained by using it. The work reported here is complementary to work on corpus-based approaches such as textual entailment: approaches that ignore the intensionality of everyday language will inevitably fail to capture important inference patterns, but on the other hand the work reported here cannot deal with large amounts of information provided as free text. Ideally, the two approaches will be combined. The aim of the current paper is to provide a reminder of the prevalence of intens</context>
<context position="9793" citStr="Ramsay, 2001" startWordPosition="1629" endWordPosition="1630">enote referring expressions, so that ref (λXman(X)) is used to denote ‘the man’. Reference terms are similar to ‘anchors’ from (Barwise and Perry, 1983), though the treatment is essentially proof-theoretic (similar to the discussion of presupposition in (Gazdar, 1979; van der Sandt, 1992)) rather than model theoretic. • Given that we are particularly concerned with the intensional nature of natural language, we need to use a formal language that supports intensionaly. The Everyday Language is Highly Intensional 197 language we choose is a constructive version of property theory (Turner, 1987; Ramsay, 2001). We have extended the theorem prover described in (Ramsay, 2001) to cope with reasoning about knowledge and belief, and we have shown how this can be used to carry out interesting inferences in cooperative and noncooperative situations (Ramsay and Field, 2008). We also include the surface illocutionary force in the LF, since this is part of the meaning of the utterance and hence it seems sensible to include it in the LF. In particular, there are interactions between surface illocutionary force and other aspects of the meaning which are hard to capture if you treat them independently. This is </context>
</contexts>
<marker>Ramsay, 2001</marker>
<rawString>Ramsay, A. M. (2001). Theorem proving for untyped constructive λ-calculus: implementation and application. Logic Journal of the Interest Group in Pure and Applied Logics 9(1), 89–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Ramsay</author>
<author>D G Field</author>
</authors>
<title>Speech acts, epistemic planning and Grice’s maxims.</title>
<date>2008</date>
<journal>Journal of Logic and Computation</journal>
<volume>18</volume>
<issue>3</issue>
<pages>431--457</pages>
<contexts>
<context position="2368" citStr="Ramsay and Field, 2008" startWordPosition="375" endWordPosition="378">ion language, and responding to them requires a surprising amount of background knowledge. We will consider below the problems that such everyday utterances bring for formal paraphrases of natural language, and we will look at the kind of background knowledge that is required for producing the right kinds of response. In order to produce a system that carries out the required inference we need access to an inference engine for carrying out proofs in a representation language with the required expressive power. The details of the engine we use are beyond the scope of this paper. (Ramsay, 2001; Ramsay and Field, 2008). For the purposes of the current paper we will simply show the results that can be obtained by using it. The work reported here is complementary to work on corpus-based approaches such as textual entailment: approaches that ignore the intensionality of everyday language will inevitably fail to capture important inference patterns, but on the other hand the work reported here cannot deal with large amounts of information provided as free text. Ideally, the two approaches will be combined. The aim of the current paper is to provide a reminder of the prevalence of intensionality in everyday lang</context>
<context position="10054" citStr="Ramsay and Field, 2008" startWordPosition="1669" endWordPosition="1672">in (Gazdar, 1979; van der Sandt, 1992)) rather than model theoretic. • Given that we are particularly concerned with the intensional nature of natural language, we need to use a formal language that supports intensionaly. The Everyday Language is Highly Intensional 197 language we choose is a constructive version of property theory (Turner, 1987; Ramsay, 2001). We have extended the theorem prover described in (Ramsay, 2001) to cope with reasoning about knowledge and belief, and we have shown how this can be used to carry out interesting inferences in cooperative and noncooperative situations (Ramsay and Field, 2008). We also include the surface illocutionary force in the LF, since this is part of the meaning of the utterance and hence it seems sensible to include it in the LF. In particular, there are interactions between surface illocutionary force and other aspects of the meaning which are hard to capture if you treat them independently. This is slightly less standard than the other aspects of our LFs, but it does have the advantage that these LFs keep all the information that we can obtain by inspecting the form of the utterance in one place. A typical example of an LF for a simple sentence is given i</context>
</contexts>
<marker>Ramsay, Field, 2008</marker>
<rawString>Ramsay, A. M. and D. G. Field (2008). Speech acts, epistemic planning and Grice’s maxims. Journal of Logic and Computation 18(3), 431–457.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Reichenbach</author>
</authors>
<title>Elements of Symbolic Logic.</title>
<date>1947</date>
<publisher>The Free Press.</publisher>
<location>New York:</location>
<contexts>
<context position="9140" citStr="Reichenbach (1947" startWordPosition="1528" endWordPosition="1529">vents are first-class objects, as suggested by Davidson Davidson (1967, 1980). • We allow other entities to play named roles with respect to these events, where we denote that some item X is, for instance, the agent of some event E by writing θ(E,agent,X): using this notation, rather than writing agent(E,X), allows us to quantify over thematic roles, which in turn allows us to state generalisations that would otherwise be awkward. • We treat tense as a relation between speech time and ‘reference time’, and aspect as a relation between reference time and event time, as suggested by Reichenbach Reichenbach (1947, 1956). • We use ‘reference terms’ to denote referring expressions, so that ref (λXman(X)) is used to denote ‘the man’. Reference terms are similar to ‘anchors’ from (Barwise and Perry, 1983), though the treatment is essentially proof-theoretic (similar to the discussion of presupposition in (Gazdar, 1979; van der Sandt, 1992)) rather than model theoretic. • Given that we are particularly concerned with the intensional nature of natural language, we need to use a formal language that supports intensionaly. The Everyday Language is Highly Intensional 197 language we choose is a constructive ve</context>
</contexts>
<marker>Reichenbach, 1947</marker>
<rawString>Reichenbach, H. (1947). Elements of Symbolic Logic. New York: The Free Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Reichenbach</author>
</authors>
<title>The Direction of Time.</title>
<date>1956</date>
<publisher>University of California Press.</publisher>
<location>Berkeley:</location>
<marker>Reichenbach, 1956</marker>
<rawString>Reichenbach, H. (1956). The Direction of Time. Berkeley: University of California Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Turner</author>
</authors>
<title>A theory of properties.</title>
<date>1987</date>
<journal>Journal of Symbolic Logic</journal>
<volume>52</volume>
<issue>2</issue>
<pages>455--472</pages>
<contexts>
<context position="9778" citStr="Turner, 1987" startWordPosition="1627" endWordPosition="1628">ce terms’ to denote referring expressions, so that ref (λXman(X)) is used to denote ‘the man’. Reference terms are similar to ‘anchors’ from (Barwise and Perry, 1983), though the treatment is essentially proof-theoretic (similar to the discussion of presupposition in (Gazdar, 1979; van der Sandt, 1992)) rather than model theoretic. • Given that we are particularly concerned with the intensional nature of natural language, we need to use a formal language that supports intensionaly. The Everyday Language is Highly Intensional 197 language we choose is a constructive version of property theory (Turner, 1987; Ramsay, 2001). We have extended the theorem prover described in (Ramsay, 2001) to cope with reasoning about knowledge and belief, and we have shown how this can be used to carry out interesting inferences in cooperative and noncooperative situations (Ramsay and Field, 2008). We also include the surface illocutionary force in the LF, since this is part of the meaning of the utterance and hence it seems sensible to include it in the LF. In particular, there are interactions between surface illocutionary force and other aspects of the meaning which are hard to capture if you treat them independ</context>
</contexts>
<marker>Turner, 1987</marker>
<rawString>Turner, R. (1987). A theory of properties. Journal of Symbolic Logic 52(2), 455–472.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R van der Sandt</author>
</authors>
<title>Presupposition projection as anaphora resolution.</title>
<date>1992</date>
<journal>Journal of Semantics</journal>
<volume>9</volume>
<pages>333--377</pages>
<marker>van der Sandt, 1992</marker>
<rawString>van der Sandt, R. (1992). Presupposition projection as anaphora resolution. Journal of Semantics 9, 333–377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A N Whitehead</author>
<author>B Russell</author>
</authors>
<title>Principia Mathematica. Cambridge:</title>
<date>1925</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="21746" citStr="Whitehead and Russell, 1925" startWordPosition="3640" endWordPosition="3643">entences involving intensional predicates is thus both straightforward (so long as you can parse them) and essential. Formal languages that support such paraphrases are, however, potentially problematic. The key problem is that such languages tend to permit paradoxical constructions such as the Liar Paradox and Ruessll’s set which introduce sentences which are true if and only if they are false. It is difficult to provide semantics for languages which allow paradoxes to be stated, but there are a number of ways out of this dilemma, either by putting syntactic restrictions on what can be said (Whitehead and Russell, 1925; Jech, 1971) or by devising appropriate interpretations (Turner, 1987; Aczel, 1988). We choose to employ a constructive variant of property theory, because it allows us a comparatively straightforward and implemetable proof theory, but it does not really matter what you choose. What does matter is that if you choose a language with less expressive power 202 Ramsay and Field than natural language, such as description logic, your paraphrases must fail to support some of the distinctions that are expressible in natural language, and as a consequence you will inevitably draw incorrect conclusions</context>
</contexts>
<marker>Whitehead, Russell, 1925</marker>
<rawString>Whitehead, A. N. and B. Russell (1925). Principia Mathematica. Cambridge: Cambridge University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>