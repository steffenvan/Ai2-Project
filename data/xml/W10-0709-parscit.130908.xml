<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.021583">
<title confidence="0.9994895">
Exploring Normalization Techniques for Human Judgments of Machine
Translation Adequacy Collected Using Amazon Mechanical Turk
</title>
<author confidence="0.985279">
Michael Denkowski and Alon Lavie
</author>
<affiliation confidence="0.89922475">
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15232, USA
</affiliation>
<email confidence="0.999164">
{mdenkows,alavie}@cs.cmu.edu
</email>
<sectionHeader confidence="0.995646" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999497545454545">
This paper discusses a machine translation
evaluation task conducted using Amazon Me-
chanical Turk. We present a translation ade-
quacy assessment task for untrained Arabic-
speaking annotators and discuss several tech-
niques for normalizing the resulting data. We
present a novel 2-stage normalization tech-
nique shown to have the best performance on
this task and further discuss the results of all
techniques and the usability of the resulting
adequacy scores.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998474">
Human judgments of translation quality play a vital
role in the development of effective machine trans-
lation (MT) systems. Such judgments can be used
to measure system quality in evaluations (Callison-
Burch et al., 2009) and to tune automatic metrics
such as METEOR (Banerjee and Lavie, 2005) which
act as stand-ins for human evaluators. However, col-
lecting reliable human judgments often requires sig-
nificant time commitments from expert annotators,
leading to a general scarcity of judgments and a sig-
nificant time lag when seeking judgments for new
tasks or languages.
Amazon’s Mechanical Turk (MTurk) service fa-
cilitates inexpensive collection of large amounts of
data from users around the world. However, Turk-
ers are not trained to provide reliable annotations for
natural language processing (NLP) tasks, and some
Turkers attempt to game the system by submitting
random answers. For these reasons, NLP tasks must
be designed to be accessible to untrained users and
data normalization techniques must be employed to
ensure that the data collected is usable.
This paper describes a MT evaluation task for
translations of English into Arabic conducted us-
ing MTurk and compares several data normaliza-
tion techniques. A novel 2-stage normalization tech-
nique is demonstrated to produce the highest agree-
ment between Turkers and experts while retaining
enough judgments to provide a robust tuning set for
automatic evaluation metrics.
</bodyText>
<sectionHeader confidence="0.945101" genericHeader="method">
2 Data Set
</sectionHeader>
<bodyText confidence="0.999844727272727">
Our data set consists of human adequacy judgments
for automatic translations of 1314 English sentences
into Arabic. The English source sentences and Ara-
bic reference translations are taken from the Arabic-
English sections of the NIST Open Machine Trans-
lation Evaluation (Garofolo, 2001) data sets for 2002
through 2005. Selected sentences are between 10
and 20 words in length on the Arabic side. Arabic
machine translation (MT) hypotheses are obtained
by passing the English sentences through Google’s
free online translation service.
</bodyText>
<subsectionHeader confidence="0.990712">
2.1 Data Collection
</subsectionHeader>
<bodyText confidence="0.967358">
Human judgments of translation adequacy are col-
lected for each of the 1314 Arabic MT output hy-
potheses. Given a translation hypothesis and the
corresponding reference translation, annotators are
asked to assign an adequacy score according to the
following scale:
4 – Hypothesis is completely meaning equivalent
with the reference translation.
</bodyText>
<page confidence="0.989719">
57
</page>
<note confidence="0.508401">
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 57–61,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<listItem confidence="0.86415525">
• Δ: For judgments (J = j1...jn) and gold stan-
dard (G = g1...gn), we define average distance:
3 – Hypothesis captures more than half of meaning
of the reference translation.
2 – Hypothesis captures less than half of meaning
of the reference translation.
1 – Hypothesis captures no meaning of the refer-
ence translation.
</listItem>
<bodyText confidence="0.99992571875">
Adequacy judgments are collected from untrained
Arabic-speaking annotators using Amazon’s Me-
chanical Turk (MTurk) service. We create a human
intelligence task (HIT) type that presents Turkers
with a MT hypothesis/reference pair and asks for
an adequacy judgment. To make this task accessi-
ble to non-experts, the traditional definitions of ad-
equacy scores are replaced with the following: (4)
excellent, (3) good, (2) bad, (1) very bad. Each rat-
ing is accompanied by an example from the data set
which fits the corresponding criteria from the tradi-
tional scale. To make this task accessible to the Ara-
bic speakers we would like to complete the HITs,
the instructions are provided in Arabic as well as En-
glish.
To allow experimentation with various data nor-
malization techniques, we collect judgments from
10 unique Turkers for each of the translations. We
also ask an expert to provide “gold standard” judg-
ments for 101 translations drawn uniformly from the
data. These 101 translations are recombined with the
data and repeated such that every 6th translation has
a gold standard judgment, resulting in a total of 1455
HITs. We pay Turkers $0.01 per HIT and Ama-
zon fees of $0.005 per HIT, leading to a total cost
of $218.25 for data collection and an effective cost
of $0.015 per judgment. Despite requiring Arabic
speakers, our HITs are completed at a rate of 1000-
3000 per day. It should be noted that the vast ma-
jority of Turkers working on our HITs are located in
India, with fewer in Arabic-speaking countries such
as Egypt and Syria.
</bodyText>
<sectionHeader confidence="0.987828" genericHeader="method">
3 Normalization Techniques
</sectionHeader>
<bodyText confidence="0.999993">
We apply multiple normalization techniques to the
data set and evaluate their relative performance.
Several techniques use the following measures:
</bodyText>
<equation confidence="0.986813">
�n i=1 |gi − ji|
Δ(J, G) =
n
</equation>
<listItem confidence="0.914867">
• K: For two annotators, Cohen’s kappa coeffi-
cient (Smeeton, 1985) is defined:
</listItem>
<equation confidence="0.9940495">
P(A) − P(E)
1 − P(E)
</equation>
<bodyText confidence="0.999917666666667">
where P(A) is the proportion of times that an-
notators agree and P(E) is the proportion of
times that agreement is expected by chance.
</bodyText>
<subsectionHeader confidence="0.997586">
3.1 Straight Average
</subsectionHeader>
<bodyText confidence="0.999857333333333">
The baseline approach consists of keeping all judg-
ments and taking the straight average on a per-
translation basis without additional normalization.
</bodyText>
<subsectionHeader confidence="0.999804">
3.2 Removing Low-Agreement Judges
</subsectionHeader>
<bodyText confidence="0.999941714285714">
Following Callison-Burch et al. (2009), we calcu-
late pairwise inter-annotator agreement (P(A)) of
each annotator with all others and remove judgments
from annotators with P(A) below some threshold.
We set this threshold such that the highest overall
agreement can be achieved while retaining at least
one judgment for each translation.
</bodyText>
<subsectionHeader confidence="0.999165">
3.3 Removing Outlying Judgments
</subsectionHeader>
<bodyText confidence="0.999991333333333">
For a given translation and human judgments
(j1...jn), we calculate the distance (S) of each judg-
ment from the mean ( j):
</bodyText>
<equation confidence="0.981443">
S(ji) = |ji − j|
</equation>
<bodyText confidence="0.99994075">
We then remove outlying judgments with S(ji) ex-
ceeding some threshold. This threshold is also set
such that the highest agreement is achieved while
retaining at least one judgment per translation.
</bodyText>
<subsectionHeader confidence="0.947753">
3.4 Weighted Voting
</subsectionHeader>
<bodyText confidence="0.995761333333333">
Following Callison-Burch (2009), we treat evalua-
tion as a weighted voting problem where each anno-
tator’s contribution is weighted by agreement with
either a gold standard or with other annotators. For
this evaluation, we weigh contribution by P(A) with
the 101 gold standard judgments.
</bodyText>
<equation confidence="0.473599">
K=
</equation>
<page confidence="0.988844">
58
</page>
<subsectionHeader confidence="0.954652">
3.5 Scaling Judgments
</subsectionHeader>
<bodyText confidence="0.998404142857143">
To account for the notion that some annotators judge
translations more harshly than others, we apply per-
annotator scaling to the adequacy judgments based
on annotators’ signed distance from gold standard
judgments. For judgments (J = j1...jn) and gold
standard (G = g1...gn), an additive scaling factor is
calculated:
</bodyText>
<equation confidence="0.988438333333333">
�n i=1 gi − ji
λ+(J, G) =
n
</equation>
<bodyText confidence="0.999178">
Adding this scaling factor to each judgment has the
effect of shifting the judgments’ center of mass to
match that of the gold standard.
</bodyText>
<subsectionHeader confidence="0.97006">
3.6 2-Stage Technique
</subsectionHeader>
<bodyText confidence="0.999640272727273">
We combine judgment scaling with weighted vot-
ing to produce a 2-stage normalization technique
addressing two types of divergence in Turker judg-
ments from the gold standard. Divergence can be
either consistent, where Turkers regularly assign
higher or lower scores than experts, or random,
where Turkers guess blindly or do not understand
the task.
Stage 1: Given a gold standard (G = g1...gn),
consistent divergences are corrected by calculat-
ing λ+(J, G) for each annotator’s judgments (J =
ji...jn) and applying λ+(J, G) to each ji to produce
adjusted judgment set J&apos;. If Δ(J&apos;, G) &lt; Δ(J, G),
where Δ(J, G) is defined in Section 3, the annotator
is considered consistently divergent and J&apos; is used
in place of J. Inconsistently divergent annotators’
judgments are unaffected by this stage.
Stage 2: All annotators are considered in a
weighted voting scenario. In this case, annotator
contribution is determined by a distance measure
similar to the kappa coefficient. For judgments (J =
j1...jn) and gold standard (G = g1...gn), we define:
</bodyText>
<equation confidence="0.967702">
KΔ(J, G) = (max Δ − Δ(J, G)) − E(Δ)
max Δ − E(Δ)
</equation>
<bodyText confidence="0.998812375">
where max Δ is the average maximum distance be-
tween judgments and E(Δ) is the expected distance
between judgments. Perfect agreement with the gold
standard produces KΔ = 1 while chance agreement
produces KΔ = 0. Annotators with KΔ &lt; 0 are re-
moved from the voting pool and final scores are cal-
culated as the weighted averages of judgments from
all remaining annotators.
</bodyText>
<table confidence="0.999280666666667">
Type Δ KΔ
Uniform-a 1.02 0.184
Uniform-b 1.317 -0.053
Gaussian-2 1.069 0.145
Gaussian-2.5 0.96 0.232
Gaussian-3 1.228 0.018
</table>
<tableCaption confidence="0.999267">
Table 2: Weights assigned to random data
</tableCaption>
<sectionHeader confidence="0.999202" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999930185185185">
Table 1 outlines the performance of all normaliza-
tion techniques. To calculate P(A) and K with the
gold standard, final adequacy scores are rounded to
the nearest whole number. As shown in the table, re-
moving low-agreement annotators or outlying judg-
ments greatly improves Turker agreement and, in
the case of removing judgments, decreases distance
from the gold standard. However, these approaches
remove a large portion of the judgments, leaving a
skewed data set. When removing judgments, 1172
of the 1314 translations receive a score of 3, making
tasks such as tuning automatic metrics infeasible.
Weighing votes by agreement with the gold stan-
dard retains most judgments, though neither Turker
agreement nor agreement with the gold standard im-
proves. The scaling approach retains all judgments
and slightly improves correlation and Δ, though K
decreases. As scaled judgments are not whole num-
bers, Turker P(A) and K are not applicable.
The 2-stage approach outperforms all other tech-
niques when compared against the gold standard,
being the only technique to significantly raise cor-
relation. Over 90% of the judgments are used, as
shown in Figure 1. Further, the distribution of fi-
nal adequacy scores (shown in Figure 2) resembles
a normal distribution, allowing this data to be used
for tuning automatic evaluation metrics.
</bodyText>
<subsectionHeader confidence="0.997554">
4.1 Resistance to Randomness
</subsectionHeader>
<bodyText confidence="0.998459625">
To verify that our 2-stage technique handles prob-
lematic data properly, we simulate user data from
5 unreliable Turkers. Turkers “Uniform-a” and
“Uniform-b” draw answers randomly from a uni-
form distribution. “Gaussian” Turkers draw answers
randomly from Gaussian distributions with σ = 1
and µ according to name. Each “Turker” contributes
one judgment for each translation. As shown in Ta-
</bodyText>
<page confidence="0.995565">
59
</page>
<table confidence="0.999161875">
Technique Retained Gold Standard K Turker
Correlation A P(A) P(A) K
Straight Average 14550 0.078 0.988 0.356 0.142 0.484 0.312
Remove Judges 6627 -0.152 1.002 0.347 0.129 0.664 0.552
Remove Judgments 9250 0 0.891 0.356 0.142 0.944 0.925
Weighted Voting 14021 0.152 0.968 0.356 0.142 0.484 0.312
Scale Judgments 14550 0.24 0.89 0.317 0.089 N/A N/A
2-Stage Technique 13621 0.487 0.836 0.366 0.155 N/A N/A
</table>
<tableCaption confidence="0.999888">
Table 1: Performance of normalization techniques
</tableCaption>
<figure confidence="0.755146">
Vote Weight
</figure>
<figureCaption confidence="0.999984">
Figure 1: Distribution of weights for judgments
</figureCaption>
<bodyText confidence="0.9992872">
ble 2, only Gaussian-2.5 receives substantial weight
while the others receive low or zero weight. This fol-
lows from the fact that the actual data follows a sim-
ilar distribution, and thus the random Turkers have
negligible impact on the final distribution of scores.
</bodyText>
<sectionHeader confidence="0.998267" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999957666666667">
We have presented an Arabic MT evaluation task
conducted using Amazon MTurk and discussed
several possibilities for normalizing the collected
data. Our 2-stage normalization technique has been
shown to provide the highest agreement between
Turkers and experts while retaining enough judg-
ments to avoid problems of data sparsity and appro-
priately down-weighting random data. As we cur-
rently have a single set of expert judgments, our fu-
ture work involves collecting additional judgments
from multiple experts against which to further test
our techniques. We then plan to use normalized
</bodyText>
<figure confidence="0.696151">
Adequacy Score
</figure>
<figureCaption confidence="0.9926355">
Figure 2: Distribution of adequacy scores after 2-stage
normalization
</figureCaption>
<bodyText confidence="0.9996">
Turker adequacy judgments to tune an Arabic ver-
sion of the METEOR (Banerjee and Lavie, 2005) MT
evaluation metric.
</bodyText>
<sectionHeader confidence="0.999271" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996092142857143">
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Proc.
ACL WIEEMMTS.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of WMT09. In
Proc. WMT09.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon’s Me-
chanical Turk. In Proc. EMNLP09.
John Garofolo. 2001. NIST Open Machine Translation
Evaluation. http://www.itl.nist.gov/iad/mig/tests/mt/.
N. C. Smeeton. 1985. Early History of the Kappa Statis-
tic. In Biometrics, volume 41.
</reference>
<figure confidence="0.999739952380952">
0 0.25 0.5 0.75 1
Number of Judgments
4500
4000
2500
2000
5000
3500
3000
1500
1000
500
0
1 2 3 4
Number of Segments
250
200
150
100
50
0
</figure>
<page confidence="0.943534">
60
61
</page>
<figureCaption confidence="0.999402">
Figure 3: Example HIT as seen by Turkers
</figureCaption>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.595552">
<title confidence="0.82384325">Exploring Normalization Techniques for Human Judgments of Translation Adequacy Collected Using Amazon Mechanical Turk Denkowski Language Technologies</title>
<affiliation confidence="0.9967285">School of Computer Carnegie Mellon</affiliation>
<address confidence="0.95506">Pittsburgh, PA 15232,</address>
<email confidence="0.998992">mdenkows@cs.cmu.edu</email>
<email confidence="0.998992">alavie@cs.cmu.edu</email>
<abstract confidence="0.999707333333333">This paper discusses a machine translation evaluation task conducted using Amazon Mechanical Turk. We present a translation adequacy assessment task for untrained Arabicspeaking annotators and discuss several techniques for normalizing the resulting data. We present a novel 2-stage normalization technique shown to have the best performance on this task and further discuss the results of all techniques and the usability of the resulting adequacy scores.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.</title>
<date>2005</date>
<booktitle>In Proc. ACL WIEEMMTS.</booktitle>
<contexts>
<context position="1072" citStr="Banerjee and Lavie, 2005" startWordPosition="151" endWordPosition="154">sment task for untrained Arabicspeaking annotators and discuss several techniques for normalizing the resulting data. We present a novel 2-stage normalization technique shown to have the best performance on this task and further discuss the results of all techniques and the usability of the resulting adequacy scores. 1 Introduction Human judgments of translation quality play a vital role in the development of effective machine translation (MT) systems. Such judgments can be used to measure system quality in evaluations (CallisonBurch et al., 2009) and to tune automatic metrics such as METEOR (Banerjee and Lavie, 2005) which act as stand-ins for human evaluators. However, collecting reliable human judgments often requires significant time commitments from expert annotators, leading to a general scarcity of judgments and a significant time lag when seeking judgments for new tasks or languages. Amazon’s Mechanical Turk (MTurk) service facilitates inexpensive collection of large amounts of data from users around the world. However, Turkers are not trained to provide reliable annotations for natural language processing (NLP) tasks, and some Turkers attempt to game the system by submitting random answers. For th</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Proc. ACL WIEEMMTS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>Findings of WMT09. In</title>
<date>2009</date>
<booktitle>Proc. WMT09.</booktitle>
<contexts>
<context position="5874" citStr="Callison-Burch et al. (2009)" startWordPosition="918" endWordPosition="921">lization techniques to the data set and evaluate their relative performance. Several techniques use the following measures: �n i=1 |gi − ji| Δ(J, G) = n • K: For two annotators, Cohen’s kappa coefficient (Smeeton, 1985) is defined: P(A) − P(E) 1 − P(E) where P(A) is the proportion of times that annotators agree and P(E) is the proportion of times that agreement is expected by chance. 3.1 Straight Average The baseline approach consists of keeping all judgments and taking the straight average on a pertranslation basis without additional normalization. 3.2 Removing Low-Agreement Judges Following Callison-Burch et al. (2009), we calculate pairwise inter-annotator agreement (P(A)) of each annotator with all others and remove judgments from annotators with P(A) below some threshold. We set this threshold such that the highest overall agreement can be achieved while retaining at least one judgment for each translation. 3.3 Removing Outlying Judgments For a given translation and human judgments (j1...jn), we calculate the distance (S) of each judgment from the mean ( j): S(ji) = |ji − j| We then remove outlying judgments with S(ji) exceeding some threshold. This threshold is also set such that the highest agreement i</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Schroeder, 2009</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Josh Schroeder. 2009. Findings of WMT09. In Proc. WMT09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Fast, cheap, and creative: Evaluating translation quality using Amazon’s Mechanical Turk. In</title>
<date>2009</date>
<booktitle>Proc. EMNLP09.</booktitle>
<contexts>
<context position="6591" citStr="Callison-Burch (2009)" startWordPosition="1033" endWordPosition="1034">emove judgments from annotators with P(A) below some threshold. We set this threshold such that the highest overall agreement can be achieved while retaining at least one judgment for each translation. 3.3 Removing Outlying Judgments For a given translation and human judgments (j1...jn), we calculate the distance (S) of each judgment from the mean ( j): S(ji) = |ji − j| We then remove outlying judgments with S(ji) exceeding some threshold. This threshold is also set such that the highest agreement is achieved while retaining at least one judgment per translation. 3.4 Weighted Voting Following Callison-Burch (2009), we treat evaluation as a weighted voting problem where each annotator’s contribution is weighted by agreement with either a gold standard or with other annotators. For this evaluation, we weigh contribution by P(A) with the 101 gold standard judgments. K= 58 3.5 Scaling Judgments To account for the notion that some annotators judge translations more harshly than others, we apply perannotator scaling to the adequacy judgments based on annotators’ signed distance from gold standard judgments. For judgments (J = j1...jn) and gold standard (G = g1...gn), an additive scaling factor is calculated:</context>
</contexts>
<marker>Callison-Burch, 2009</marker>
<rawString>Chris Callison-Burch. 2009. Fast, cheap, and creative: Evaluating translation quality using Amazon’s Mechanical Turk. In Proc. EMNLP09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Garofolo</author>
</authors>
<date>2001</date>
<journal>NIST Open Machine Translation Evaluation. http://www.itl.nist.gov/iad/mig/tests/mt/.</journal>
<contexts>
<context position="2511" citStr="Garofolo, 2001" startWordPosition="374" endWordPosition="375">ations of English into Arabic conducted using MTurk and compares several data normalization techniques. A novel 2-stage normalization technique is demonstrated to produce the highest agreement between Turkers and experts while retaining enough judgments to provide a robust tuning set for automatic evaluation metrics. 2 Data Set Our data set consists of human adequacy judgments for automatic translations of 1314 English sentences into Arabic. The English source sentences and Arabic reference translations are taken from the ArabicEnglish sections of the NIST Open Machine Translation Evaluation (Garofolo, 2001) data sets for 2002 through 2005. Selected sentences are between 10 and 20 words in length on the Arabic side. Arabic machine translation (MT) hypotheses are obtained by passing the English sentences through Google’s free online translation service. 2.1 Data Collection Human judgments of translation adequacy are collected for each of the 1314 Arabic MT output hypotheses. Given a translation hypothesis and the corresponding reference translation, annotators are asked to assign an adequacy score according to the following scale: 4 – Hypothesis is completely meaning equivalent with the reference </context>
</contexts>
<marker>Garofolo, 2001</marker>
<rawString>John Garofolo. 2001. NIST Open Machine Translation Evaluation. http://www.itl.nist.gov/iad/mig/tests/mt/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N C Smeeton</author>
</authors>
<title>Early History of the Kappa Statistic.</title>
<date>1985</date>
<journal>In Biometrics,</journal>
<volume>41</volume>
<contexts>
<context position="5465" citStr="Smeeton, 1985" startWordPosition="854" endWordPosition="855"> to a total cost of $218.25 for data collection and an effective cost of $0.015 per judgment. Despite requiring Arabic speakers, our HITs are completed at a rate of 1000- 3000 per day. It should be noted that the vast majority of Turkers working on our HITs are located in India, with fewer in Arabic-speaking countries such as Egypt and Syria. 3 Normalization Techniques We apply multiple normalization techniques to the data set and evaluate their relative performance. Several techniques use the following measures: �n i=1 |gi − ji| Δ(J, G) = n • K: For two annotators, Cohen’s kappa coefficient (Smeeton, 1985) is defined: P(A) − P(E) 1 − P(E) where P(A) is the proportion of times that annotators agree and P(E) is the proportion of times that agreement is expected by chance. 3.1 Straight Average The baseline approach consists of keeping all judgments and taking the straight average on a pertranslation basis without additional normalization. 3.2 Removing Low-Agreement Judges Following Callison-Burch et al. (2009), we calculate pairwise inter-annotator agreement (P(A)) of each annotator with all others and remove judgments from annotators with P(A) below some threshold. We set this threshold such that</context>
</contexts>
<marker>Smeeton, 1985</marker>
<rawString>N. C. Smeeton. 1985. Early History of the Kappa Statistic. In Biometrics, volume 41.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>