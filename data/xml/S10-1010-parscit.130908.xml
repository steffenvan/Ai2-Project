<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000462">
<title confidence="0.75015">
SemEval-2010 Task 13: TempEval-2
</title>
<author confidence="0.739198">
Marc Verhagen†, Roser Sauri‡, Tommaso Caselli∗ and James Pustejovsky†
</author>
<affiliation confidence="0.57131">
† Computer Science Department, Brandeis University, Massachusetts, USA
‡Barcelona Media, Barcelona, Spain ∗ ILC-CNR, Pisa, Italy
</affiliation>
<email confidence="0.99454">
marc@cs.brandeis.edu roser.sauri@barcelonamedia.org
tommaso.caselli@ilc.cnr.it jamesp@cs.brandeis.edu
</email>
<sectionHeader confidence="0.993763" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.991208">
Tempeval-2 comprises evaluation tasks for
time expressions, events and temporal re-
lations, the latter of which was split up in
four sub tasks, motivated by the notion that
smaller subtasks would make both data
preparation and temporal relation extrac-
tion easier. Manually annotated data were
provided for six languages: Chinese, En-
glish, French, Italian, Korean and Spanish.
</bodyText>
<sectionHeader confidence="0.998791" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954518518519">
The ultimate aim of temporal processing is the au-
tomatic identification of all temporal referring ex-
pressions, events and temporal relations within a
text. However, addressing this aim is beyond the
scope of an evaluation challenge and a more mod-
est approach is appropriate.
The 2007 SemEval task, TempEval-1 (Verhagen
et al., 2007; Verhagen et al., 2009), was an initial
evaluation exercise based on three limited tempo-
ral ordering and anchoring tasks that were consid-
ered realistic both from the perspective of assem-
bling resources for development and testing and
from the perspective of developing systems capa-
ble of addressing the tasks.1
TempEval-2 is based on TempEval-1, but is
more elaborate in two respects: (i) it is a multilin-
gual task, and (ii) it consists of six subtasks rather
than three.
In the rest of this paper, we first introduce the
data that we are dealing with. Which gets us in
a position to present the list of task introduced by
TempEval-2, including some motivation as to why
we feel that it is a good idea to split up temporal
relation classification into sub tasks. We proceed
by shortly describing the data resources and their
creation, followed by the performance of the sys-
tems that participated in the tasks.
</bodyText>
<footnote confidence="0.935396">
1The Semeval-2007 task was actually known simply as
TempEval, but here we use Tempeval-1 to avoid confusion.
</footnote>
<sectionHeader confidence="0.973087" genericHeader="method">
2 TempEval Annotation
</sectionHeader>
<bodyText confidence="0.999297125">
The TempEval annotation language is a simplified
version of TimeML.2 using three TimeML tags:
TIMEX3, EVENT and TLINK.
TIMEX3 tags the time expressions in the text and
is identical to the TIMEX3 tag in TimeML. Times
can be expressed syntactically by adverbial or
prepositional phrases, as shown in the following
example.
</bodyText>
<listItem confidence="0.9925906">
(1) a. on Thursday
b. November 15, 2004
c. Thursday evening
d. in the late 80’s
e. later this afternoon
</listItem>
<bodyText confidence="0.9451115">
The two main attributes of the TIMEX3 tag are
TYPE and VAL, both shown in the example (2).
</bodyText>
<note confidence="0.527592">
(2) November 22, 2004
</note>
<bodyText confidence="0.972868842105263">
type=&amp;quot;DATE&amp;quot; val=&amp;quot;2004-11-22&amp;quot;
For TempEval-2, we distinguish four temporal
types: TIME (at 2:45 p.m.), DATE (January 27,
1920, yesterday), DURATION (two weeks) and SET
(every Monday morning). The VAL attribute as-
sumes values according to an extension of the ISO
8601 standard, as enhanced by TIMEX2.
Each document has one special TIMEX3 tag,
the Document Creation Time (DCT), which is in-
terpreted as an interval that spans a whole day.
The EVENT tag is used to annotate those ele-
ments in a text that describe what is conventionally
referred to as an eventuality. Syntactically, events
are typically expressed as inflected verbs, although
event nominals, such as ”crash” in killed by the
crash, should also be annotated as EVENTS. The
most salient event attributes encode tense, aspect,
modality and polarity information. Examples of
some of these features are shown below:
</bodyText>
<footnote confidence="0.992391">
2See http://www.timeml.org for language speci-
fications and annotation guidelines
</footnote>
<page confidence="0.985941">
57
</page>
<bodyText confidence="0.490339">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 57–62,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</bodyText>
<equation confidence="0.604017714285714">
(3) should have bought A. Determine the extent of the time expressions
tense=&amp;quot;PAST&amp;quot; aspect=&amp;quot;PERFECTIVE&amp;quot; in a text as defined by the TimeML TIMEX3
modality=&amp;quot;SHOULD&amp;quot; polarity=&amp;quot;POS&amp;quot; tag. In addition, determine value of the fea-
tures TYPE and VAL.
(4) did not teach
tense=&amp;quot;PAST&amp;quot; aspect=&amp;quot;NONE&amp;quot;
modality=&amp;quot;NONE&amp;quot; polarity=&amp;quot;NEG&amp;quot;
</equation>
<bodyText confidence="0.9972735625">
The relation types for the TimeML TLINK tag
form a fine-grained set based on James Allen’s
interval logic (Allen, 1983). For TempEval, the
set of labels was simplified to aid data preparation
and to reduce the complexity of the task. We use
only six relation types including the three core re-
lations BEFORE, AFTER, and OVERLAP, the two
less specific relations BEFORE-OR-OVERLAP and
OVERLAP-OR-AFTER for ambiguous cases, and fi-
nally the relation VAGUE for those cases where no
particular relation can be established.
Temporal relations come in two broad flavours:
anchorings of events to time expressions and or-
derings of events. Events can be anchored to an
adjacent time expression as in examples 5 and 6 or
to the document creation time as in 7.
</bodyText>
<listItem confidence="0.993318833333333">
(5) Mary taughte, on Tuesday morningt,
OVERLAP(e1,t1)
(6) They cancelled the eveningt2 classe2
OVERLAP(e2,t2)
(7) Most troops will leavee, Iraq by August of
2010. AFTER(e1,dct)
</listItem>
<bodyText confidence="0.863439875">
The country defaultede2 on debts for that en-
tire year. BEFORE(e2,dct)
In addition, events can be ordered relative to
other events, as in the examples below.
(8) The President spokee, to the nation on
Tuesday on the financial crisis. He had
conferrede2 with his cabinet regarding pol-
icy the day before. AFTER(e1,e2)
</bodyText>
<listItem confidence="0.998682">
(9) The students hearde, afire alarme2.
OVERLAP(e1,e2)
(10) He saide, they had postponede2 the meeting.
AFTER(e1,e2)
</listItem>
<sectionHeader confidence="0.98725" genericHeader="method">
3 TempEval-2 Tasks
</sectionHeader>
<bodyText confidence="0.878338666666667">
We can now define the six TempEval tasks:
B. Determine the extent of the events in a text
as defined by the TimeML EVENT tag. In
addition, determine the value of the features
CLASS, TENSE, ASPECT, POLARITY, and
MODALITY.
</bodyText>
<listItem confidence="0.994838909090909">
C. Determine the temporal relation between an
event and a time expression in the same
sentence. This task is further restricted by
requiring that either the event syntactically
dominates the time expression or the event
and time expression occur in the same noun
phrase.
D. Determine the temporal relation between an
event and the document creation time.
E. Determine the temporal relation between two
main events in consecutive sentences.
</listItem>
<bodyText confidence="0.957770714285714">
F. Determine the temporal relation between two
events where one event syntactically domi-
nates the other event.
Of these tasks, C, D and E were also defined for
TempEval-1. However, the syntactic locality re-
striction in task C was not present in TempEval-1.
Task participants could choose to either do all
tasks, focus on the time expression task, focus on
the event task, or focus on the four temporal rela-
tion tasks. In addition, participants could choose
one or more of the six languages for which we pro-
vided data: Chinese, English, French, Italian, Ko-
rean, and Spanish.
We feel that well-defined tasks allow us to struc-
ture the workflow, allowing us to create task-
specific guidelines and using task-specific anno-
tation tools to speed up annotation. More im-
portantly, each task can be evaluated in a fairly
straightforward way, contrary to for example the
problems that pop up when evaluating two com-
plex temporal graphs for the same document. In
addition, tasks can be ranked, allowing systems to
feed the results of one (more precise) task as a fea-
ture into another task.
Splitting the task into substask reduces the error
rate in the manual annotation, and that merging
the different sub-task into a unique layer as a post-
processing operation (see figure 1) provides better
</bodyText>
<page confidence="0.998048">
58
</page>
<figureCaption confidence="0.999836">
Figure 1: Merging Relations
</figureCaption>
<bodyText confidence="0.9961445">
and more reliable results (annotated data) than do-
ing a complex task all at once.
</bodyText>
<sectionHeader confidence="0.98611" genericHeader="method">
4 Data Preparation
</sectionHeader>
<bodyText confidence="0.9998944">
The data for the five languages were prepared in-
dependently of each other and do not comprise a
parallel corpus. However, annotation specifica-
tions and guidelines for the five languages were
developed in conjunction with one other, in many
cases based on version 1.2.1 of the TimeML an-
notation guidelines for English3. Not all corpora
contained data for all six tasks. Table 1 gives the
size of the training set and the relation tasks that
were included.
</bodyText>
<table confidence="0.974157">
language tokens C D E F X
Chinese 23,000 ✓ ✓ ✓ ✓
English 63,000 ✓ ✓ ✓ ✓
Italian 27,000 ✓ ✓ ✓
French 19,000 ✓
Korean 14,000
Spanish 68,000 ✓ ✓
</table>
<tableCaption confidence="0.999846">
Table 1: Corpus size and relation tasks
</tableCaption>
<bodyText confidence="0.99064475">
All corpora include event and timex annota-
tion. The French corpus contained a subcorpus
with temporal relations but these relations were
not split into the four tasks C through F.
Annotation proceeded in two phases: a dual
annotation phase where two annotators annotate
each document and an adjudication phase where
a judge resolves disagreements between the an-
notators. Most languages used BAT, the Brandeis
Annotation Tool (Verhagen, 2010), a generic web-
based annotation tool that is centered around the
notion of annotation tasks. With the task decom-
position allowed by BAT, it is possible to structure
the complex task of temporal annotation by split-
ting it up in as many sub tasks as seems useful. As
3Seehttp://www.timeml.org.
such, BAT was well-suited for TempEval-2 anno-
tation.
We now give a few more details on the English
and Spanish data, skipping the other languages for
reasons that will become obvious at the beginning
of section 6.
The English data sets were based on TimeBank
(Pustejovsky et al., 2003; Boguraev et al., 2007),
a hand-built gold standard of annotated texts us-
ing the TimeML markup scheme.4 However, all
event annotation was reviewed to make sure that
the annotation complied with the latest guidelines
and all temporal relations were added according to
the Tempeval-2 relation tasks, using the specified
relation types.
The data released for the TempEval-2 Spanish
edition is a fragment of the Spanish TimeBank,
currently under development. Its documents are
originally from the Spanish part of the AnCora
corpus (Taul´e et al., 2008). Data preparation fol-
lowed the annotation guidelines created to deal
with the specificities of event and timex expres-
sions in Spanish (Saur´ı et al., 2009a; Sauriet al.,
2009b).
</bodyText>
<sectionHeader confidence="0.996828" genericHeader="method">
5 Evaluation Metrics
</sectionHeader>
<bodyText confidence="0.9744685">
For the extents of events and time expres-
sions (tasks A and B), precision, recall and the
f1-measure are used as evaluation metrics, using
the following formulas:
</bodyText>
<equation confidence="0.996926666666667">
precision = tp/(tp + fp)
recall = tp/(tp + fn)
f-measure = 2 * (P * R)/(P + R)
</equation>
<bodyText confidence="0.998796363636364">
Where tp is the number of tokens that are part
of an extent in both key and response, fp is the
number of tokens that are part of an extent in the
response but not in the key, and fn is the number
of tokens that are part of an extent in the key but
not in the response.
For attributes of events and time expressions
(the second part of tasks A and B) and for relation
types (tasks C through F) we use an even simpler
metric: the number of correct answers divided by
the number of answers.
</bodyText>
<footnote confidence="0.9991735">
4See www.timeml.org for details on TimeML, Time-
Bank is distributed free of charge by the Linguistic
Data Consortium (www.ldc.upenn.edu), catalog num-
ber LDC2006T08.
</footnote>
<page confidence="0.999418">
59
</page>
<sectionHeader confidence="0.989716" genericHeader="method">
6 System Results
</sectionHeader>
<bodyText confidence="0.9987369">
Eight teams participated in TempEval-2, submit-
ting a grand total of eighteen systems. Some of
these systems only participated in one or two tasks
while others participated in all tasks. The distribu-
tion over the six languages was very uneven: six-
teen systems for English, two for Spanish and one
for English and Spanish.
The results for task A, recognition and normal-
ization of time expressions, are given in tables 2
and 3.
</bodyText>
<table confidence="0.99412">
team p r f type val
UC3M 0.90 0.87 0.88 0.91 0.83
TIPSem 0.95 0.87 0.91 0.91 0.78
TIPSem-B 0.97 0.81 0.88 0.99 0.75
</table>
<tableCaption confidence="0.975889">
Table 2: Task A results for Spanish
</tableCaption>
<table confidence="0.9997515625">
team p r f type val
Edinburgh 0.85 0.82 0.84 0.84 0.63
HeidelTime1 0.90 0.82 0.86 0.96 0.85
HeidelTime2 0.82 0.91 0.86 0.92 0.77
JU CSE 0.55 0.17 0.26 0.00 0.00
KUL 0.78 0.82 0.80 0.91 0.55
KUL Run 2 0.73 0.88 0.80 0.91 0.55
KUL Run 3 0.85 0.84 0.84 0.91 0.55
KUL Run 4 0.76 0.83 0.80 0.91 0.51
KUL Run 5 0.75 0.85 0.80 0.91 0.51
TERSEO 0.76 0.66 0.71 0.98 0.65
TIPSem 0.92 0.80 0.85 0.92 0.65
TIPSem-B 0.88 0.60 0.71 0.88 0.59
TRIOS 0.85 0.85 0.85 0.94 0.76
TRIPS 0.85 0.85 0.85 0.94 0.76
USFD2 0.84 0.79 0.82 0.90 0.17
</table>
<tableCaption confidence="0.999708">
Table 3: Task A results for English
</tableCaption>
<bodyText confidence="0.99989">
The results for Spanish are more uniform and
generally higher than the results for English.
For Spanish, the f-measure for TIMEX3 extents
ranges from 0.88 through 0.91 with an average of
0.89; for English the f-measure ranges from 0.26
through 0.86, for an average of 0.78. However,
due to the small sample size it is hard to make
any generalizations. In both languages, type de-
tection clearly was a simpler task than determining
the value.
The results for task B, event recognition, are given
in tables 4 and 5. Both tables contain results for
both Spanish and English, the first part of each ta-
ble contains the results for Spanish and the next
part the results for English.
</bodyText>
<table confidence="0.9994399">
team p r f
TIPSem 0.90 0.86 0.88
TIPSem-B 0.92 0.85 0.88
team p r f
Edinburgh 0.75 0.85 0.80
JU CSE 0.48 0.56 0.52
TIPSem 0.81 0.86 0.83
TIPSem-B 0.83 0.81 0.82
TRIOS 0.80 0.74 0.77
TRIPS 0.55 0.88 0.68
</table>
<tableCaption confidence="0.999833">
Table 4: Event extent results
</tableCaption>
<bodyText confidence="0.9975942">
The column headers in table 5 are abbrevia-
tions for polarity (pol), mood (moo), modality
(mod), tense (tns), aspect (asp) and class (cl). Note
that the English team chose to include modality
whereas the Spanish team used mood.
</bodyText>
<table confidence="0.9983936">
team pol moo tns asp cl
TIPSem 0.92 0.80 0.96 0.89 0.66
TIPSem-B 0.92 0.79 0.96 0.89 0.66
team pol mod tns asp cl
Edinburgh 0.99 0.99 0.92 0.98 0.76
JU CSE 0.98 0.98 0.30 0.95 0.53
TIPSem 0.98 0.97 0.86 0.97 0.79
TIPSem-B 0.98 0.98 0.85 0.97 0.79
TRIOS 0.99 0.95 0.91 0.98 0.77
TRIPS 0.99 0.96 0.67 0.97 0.67
</table>
<tableCaption confidence="0.999523">
Table 5: Event attribute results
</tableCaption>
<bodyText confidence="0.999972647058824">
As with the time expressions results, the sample
size for Spanish is small, but note again the higher
f-measure for event extents in Spanish.
Table 6 shows the results for all relation tasks, with
the Spanish systems in the first two rows and the
English systems in the last six rows. Recall that for
Spanish the training and test sets only contained
data for tasks C and D.
Interestingly, the version of the TIPSem sys-
tems that were applied to the Spanish data did
much better on task C compared to its English
cousins, but much worse on task D, which is rather
puzzling.
Such a difference in performance of the systems
could be due to differences in annotation accurate-
ness, or it could be due to some particularities of
how the two languages express certain temporal
</bodyText>
<page confidence="0.994288">
60
</page>
<table confidence="0.999756818181818">
team C D E F
TIPSem 0.81 0.59 - -
TIPSem-B 0.81 0.59 - -
JU CSE 0.63 0.80 0.56 0.56
NCSU-indi 0.63 0.68 0.48 0.66
NCSU-joint 0.62 0.21 0.51 0.25
TIPSem 0.55 0.82 0.55 0.59
TIPSem-B 0.54 0.81 0.55 0.60
TRIOS 0.65 0.79 0.56 0.60
TRIPS 0.63 0.76 0.58 0.59
USFD2 0.63 - 0.45 -
</table>
<tableCaption confidence="0.995602">
Table 6: Results for relation tasks
</tableCaption>
<bodyText confidence="0.9995688">
aspects, or perhaps the one corpus is more ho-
mogeneous than the other. Again, there are not
enough data points, but the issue deserves further
attention.
For each task, the test data provided the event
pairs or event-timex pairs with the relation type
set to NONE and participating systems would re-
place that value with one of the six allowed rela-
tion types. However, participating systems were
allowed to not replace NONE and not be penalized
for it. Those cases would not be counted when
compiling the scores in table 6. Table 7 lists those
systems that did not classify all relation and the
percentage of relations for each task that those sys-
tems did not classify.
</bodyText>
<table confidence="0.960704333333333">
team C D E F
TRIOS 25% 19% 36% 31%
TRIPS 20% 10% 17% 10%
</table>
<tableCaption confidence="0.998953">
Table 7: Percentage not classified
</tableCaption>
<bodyText confidence="0.992057571428571">
A comparison with the Tempeval-1 results from
Semeval-2007 may be of interest. Six systems
participated in the TempEval-1 tasks, compared
to seven or eight systems for TempEval-2. Table
8 lists the average scores and the standard devi-
ations for all the tasks (on the English data) that
Tempeval-1 and Tempeval-2 have in common.
</bodyText>
<table confidence="0.9894234">
C D E
tempeval-1 average 0.59 0.76 0.51
stddev 0.03 0.03 0.05
tempeval-2 average 0.61 0.70 0.53
stddev 0.04 0.22 0.05
</table>
<tableCaption confidence="0.999557">
Table 8: Comparing Tempevals
</tableCaption>
<bodyText confidence="0.996306111111111">
The results are very similar except for task D,
but if we take a away the one outlier (the NCSU-
joint score of 0.21) then the average becomes 0.78
with a standard deviation of 0.05. However, we
had expected that for TempEval-2 the systems
would score better on task C since we added the
restriction that the event and time expression had
to be syntactically adjacent. It is not clear why the
results on task C have not improved.
</bodyText>
<sectionHeader confidence="0.998372" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99996775">
In this paper, we described the TempEval-2 task
within the SemEval 2010 competition. This task
involves identifying the temporal relations be-
tween events and temporal expressions in text. Us-
ing a subset of TimeML temporal relations, we
show how temporal relations and anchorings can
be annotated and identified in six different lan-
guages. The markup language adopted presents
a descriptive framework with which to examine
the temporal aspects of natural language informa-
tion, demonstrating in particular, how tense and
temporal information is encoded in specific sen-
tences, and how temporal relations are encoded
between events and temporal expressions. This
work paves the way towards establishing a broad
and open standard metadata markup language for
natural language texts, examining events, tempo-
ral expressions, and their orderings.
One thing that would need to be addressed in
a follow-up task is what the optimal number of
tasks is. Tempeval-2 had six tasks, spread out over
six languages. This brought about some logisti-
cal challenges that delayed data delivery and may
have given rise to a situation where there was sim-
ply not enough time for many systems to properly
prepare. And clearly, the shared task was not suc-
cessful in attracting systems to four of the six lan-
guages.
</bodyText>
<sectionHeader confidence="0.998614" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<reference confidence="0.851482818181818">
Many people were involved in TempEval-2. We
want to express our gratitude to the following key
contributors: Nianwen Xue, Estela Saquete, Lo-
tus Goldberg, Seohyun Im, Andr´e Bittar, Nicoletta
Calzolari, Jessica Moszkowicz and Hyopil Shin.
Additional thanks to Joan Banach, Judith
Domingo, Pau Gim´enez, Jimena del Solar, Teresa
Su˜nol, Allyson Ettinger, Sharon Spivak, Nahed
Abul-Hassan, Ari Abelman, John Polson, Alexan-
dra Nunez, Virginia Partridge, , Amber Stubbs,
Alex Plotnick, Yuping Zhou, Philippe Muller and
</reference>
<page confidence="0.999512">
61
</page>
<bodyText confidence="0.997426076923077">
Irina Prodanof.
The work on the Spanish corpus was supported
by a EU Marie Curie International Reintegration
Grant (PIRG04-GA-2008-239414). Work on the
English corpus was supported under the NSF-CRI
grant 0551615, ”Towards a Comprehensive Lin-
guistic Annotation of Language” and the NSF-
INT-0753069 project ”Sustainable Interoperabil-
ity for Language Technology (SILT)”, funded by
the National Science Foundation.
Finally, thanks to all the participants, for stick-
ing with a task that was not always as flawless and
timely as it could have been in a perfect world.
</bodyText>
<sectionHeader confidence="0.999441" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999363097560975">
James Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832–843.
Bran Boguraev, James Pustejovsky, Rie Ando, and
Marc Verhagen. 2007. Timebank evolution as a
community resource for timeml parsing. Language
Resource and Evaluation, 41(1):91–115.
James Pustejovsky, David Day, Lisa Ferro, Robert
Gaizauskas, Patrick Hanks, Marcia Lazo, Roser
Sauri, Andrew See, Andrea Setzer, and Beth Sund-
heim. 2003. The TimeBank Corpus. Corpus Lin-
guistics, March.
Roser Saur´ı, Olga Batiukova, and James Pustejovsky.
2009a. Annotating events in spanish. timeml an-
notation guidelines. Technical Report Version
TempEval-2010., Barcelona Media - Innovation
Center.
Roser Sauri, Estela Saquete, and James Pustejovsky.
2009b. Annotating time expressions in spanish.
timeml annotation guidelines. Technical Report
Version TempEval-2010, Barcelona Media - Inno-
vation Center.
Mariona Taul´e, Toni Marti, and Marta Recasens. 2008.
Ancora: Multilevel annotated corpora for catalan
and spanish. In Proceedings of the LREC 2008,
Marrakesh, Morocco.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval tempo-
ral relation identification. In Proc. of the Fourth
Int. Workshop on Semantic Evaluations (SemEval-
2007), pages 75–80, Prague, Czech Republic, June.
Association for Computational Linguistics.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Jessica Moszkowicz, and James
Pustejovsky. 2009. The tempeval challenge: iden-
tifying temporal relations in text. Language Re-
sources and Evaluation.
Marc Verhagen. 2010. The Brandeis Annotation Tool.
In Language Resources and Evaluation Conference,
LREC 2010, Malta.
</reference>
<page confidence="0.999178">
62
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.291872">
<title confidence="0.632065">SemEval-2010 Task 13: TempEval-2</title>
<author confidence="0.733117">Roser Tommaso</author>
<author confidence="0.733117">James</author>
<affiliation confidence="0.918361">Science Department, Brandeis University, Massachusetts, USA</affiliation>
<address confidence="0.956496">Media, Barcelona, Spain Pisa, Italy</address>
<email confidence="0.9770065">marc@cs.brandeis.eduroser.sauri@barcelonamedia.orgtommaso.caselli@ilc.cnr.itjamesp@cs.brandeis.edu</email>
<abstract confidence="0.995718625">Tempeval-2 comprises evaluation tasks for time expressions, events and temporal relations, the latter of which was split up in four sub tasks, motivated by the notion that smaller subtasks would make both data preparation and temporal relation extraction easier. Manually annotated data were</abstract>
<note confidence="0.677468">provided for six languages: Chinese, English, French, Italian, Korean and Spanish.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Amber Stubbs</author>
</authors>
<title>Many people were involved in TempEval-2. We want to express our gratitude to the following key contributors: Nianwen Xue, Estela Saquete, Lotus Goldberg, Seohyun Im, Andr´e Bittar, Nicoletta Calzolari, Jessica Moszkowicz and Hyopil Shin. Additional thanks to</title>
<institution>Joan Banach, Judith Domingo, Pau Gim´enez, Jimena del Solar, Teresa Su˜nol, Allyson Ettinger, Sharon Spivak, Nahed Abul-Hassan,</institution>
<location>Ari Abelman, John Polson, Alexandra Nunez, Virginia</location>
<marker>Stubbs, </marker>
<rawString>Many people were involved in TempEval-2. We want to express our gratitude to the following key contributors: Nianwen Xue, Estela Saquete, Lotus Goldberg, Seohyun Im, Andr´e Bittar, Nicoletta Calzolari, Jessica Moszkowicz and Hyopil Shin. Additional thanks to Joan Banach, Judith Domingo, Pau Gim´enez, Jimena del Solar, Teresa Su˜nol, Allyson Ettinger, Sharon Spivak, Nahed Abul-Hassan, Ari Abelman, John Polson, Alexandra Nunez, Virginia Partridge, , Amber Stubbs, Alex Plotnick, Yuping Zhou, Philippe Muller and</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Allen</author>
</authors>
<title>Maintaining knowledge about temporal intervals.</title>
<date>1983</date>
<journal>Communications of the ACM,</journal>
<volume>26</volume>
<issue>11</issue>
<contexts>
<context position="4214" citStr="Allen, 1983" startWordPosition="649" endWordPosition="650">oceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 57–62, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics (3) should have bought A. Determine the extent of the time expressions tense=&amp;quot;PAST&amp;quot; aspect=&amp;quot;PERFECTIVE&amp;quot; in a text as defined by the TimeML TIMEX3 modality=&amp;quot;SHOULD&amp;quot; polarity=&amp;quot;POS&amp;quot; tag. In addition, determine value of the features TYPE and VAL. (4) did not teach tense=&amp;quot;PAST&amp;quot; aspect=&amp;quot;NONE&amp;quot; modality=&amp;quot;NONE&amp;quot; polarity=&amp;quot;NEG&amp;quot; The relation types for the TimeML TLINK tag form a fine-grained set based on James Allen’s interval logic (Allen, 1983). For TempEval, the set of labels was simplified to aid data preparation and to reduce the complexity of the task. We use only six relation types including the three core relations BEFORE, AFTER, and OVERLAP, the two less specific relations BEFORE-OR-OVERLAP and OVERLAP-OR-AFTER for ambiguous cases, and finally the relation VAGUE for those cases where no particular relation can be established. Temporal relations come in two broad flavours: anchorings of events to time expressions and orderings of events. Events can be anchored to an adjacent time expression as in examples 5 and 6 or to the doc</context>
</contexts>
<marker>Allen, 1983</marker>
<rawString>James Allen. 1983. Maintaining knowledge about temporal intervals. Communications of the ACM, 26(11):832–843.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bran Boguraev</author>
<author>James Pustejovsky</author>
<author>Rie Ando</author>
<author>Marc Verhagen</author>
</authors>
<title>Timebank evolution as a community resource for timeml parsing. Language Resource and Evaluation,</title>
<date>2007</date>
<pages>41--1</pages>
<contexts>
<context position="9222" citStr="Boguraev et al., 2007" startWordPosition="1483" endWordPosition="1486"> Tool (Verhagen, 2010), a generic webbased annotation tool that is centered around the notion of annotation tasks. With the task decomposition allowed by BAT, it is possible to structure the complex task of temporal annotation by splitting it up in as many sub tasks as seems useful. As 3Seehttp://www.timeml.org. such, BAT was well-suited for TempEval-2 annotation. We now give a few more details on the English and Spanish data, skipping the other languages for reasons that will become obvious at the beginning of section 6. The English data sets were based on TimeBank (Pustejovsky et al., 2003; Boguraev et al., 2007), a hand-built gold standard of annotated texts using the TimeML markup scheme.4 However, all event annotation was reviewed to make sure that the annotation complied with the latest guidelines and all temporal relations were added according to the Tempeval-2 relation tasks, using the specified relation types. The data released for the TempEval-2 Spanish edition is a fragment of the Spanish TimeBank, currently under development. Its documents are originally from the Spanish part of the AnCora corpus (Taul´e et al., 2008). Data preparation followed the annotation guidelines created to deal with </context>
</contexts>
<marker>Boguraev, Pustejovsky, Ando, Verhagen, 2007</marker>
<rawString>Bran Boguraev, James Pustejovsky, Rie Ando, and Marc Verhagen. 2007. Timebank evolution as a community resource for timeml parsing. Language Resource and Evaluation, 41(1):91–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>David Day</author>
<author>Lisa Ferro</author>
<author>Robert Gaizauskas</author>
<author>Patrick Hanks</author>
</authors>
<title>The TimeBank Corpus. Corpus Linguistics,</title>
<date>2003</date>
<location>Marcia Lazo, Roser Sauri, Andrew</location>
<contexts>
<context position="9198" citStr="Pustejovsky et al., 2003" startWordPosition="1479" endWordPosition="1482">T, the Brandeis Annotation Tool (Verhagen, 2010), a generic webbased annotation tool that is centered around the notion of annotation tasks. With the task decomposition allowed by BAT, it is possible to structure the complex task of temporal annotation by splitting it up in as many sub tasks as seems useful. As 3Seehttp://www.timeml.org. such, BAT was well-suited for TempEval-2 annotation. We now give a few more details on the English and Spanish data, skipping the other languages for reasons that will become obvious at the beginning of section 6. The English data sets were based on TimeBank (Pustejovsky et al., 2003; Boguraev et al., 2007), a hand-built gold standard of annotated texts using the TimeML markup scheme.4 However, all event annotation was reviewed to make sure that the annotation complied with the latest guidelines and all temporal relations were added according to the Tempeval-2 relation tasks, using the specified relation types. The data released for the TempEval-2 Spanish edition is a fragment of the Spanish TimeBank, currently under development. Its documents are originally from the Spanish part of the AnCora corpus (Taul´e et al., 2008). Data preparation followed the annotation guidelin</context>
</contexts>
<marker>Pustejovsky, Day, Ferro, Gaizauskas, Hanks, 2003</marker>
<rawString>James Pustejovsky, David Day, Lisa Ferro, Robert Gaizauskas, Patrick Hanks, Marcia Lazo, Roser Sauri, Andrew See, Andrea Setzer, and Beth Sundheim. 2003. The TimeBank Corpus. Corpus Linguistics, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Saur´ı</author>
<author>Olga Batiukova</author>
<author>James Pustejovsky</author>
</authors>
<title>Annotating events in spanish. timeml annotation guidelines.</title>
<date>2009</date>
<tech>Technical Report Version TempEval-2010.,</tech>
<institution>Innovation Center.</institution>
<location>Barcelona Media</location>
<marker>Saur´ı, Batiukova, Pustejovsky, 2009</marker>
<rawString>Roser Saur´ı, Olga Batiukova, and James Pustejovsky. 2009a. Annotating events in spanish. timeml annotation guidelines. Technical Report Version TempEval-2010., Barcelona Media - Innovation Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Sauri</author>
<author>Estela Saquete</author>
<author>James Pustejovsky</author>
</authors>
<title>Annotating time expressions in spanish. timeml annotation guidelines.</title>
<date>2009</date>
<tech>Technical Report Version TempEval-2010,</tech>
<institution>Innovation Center.</institution>
<location>Barcelona Media</location>
<marker>Sauri, Saquete, Pustejovsky, 2009</marker>
<rawString>Roser Sauri, Estela Saquete, and James Pustejovsky. 2009b. Annotating time expressions in spanish. timeml annotation guidelines. Technical Report Version TempEval-2010, Barcelona Media - Innovation Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mariona Taul´e</author>
<author>Toni Marti</author>
<author>Marta Recasens</author>
</authors>
<title>Ancora: Multilevel annotated corpora for catalan and spanish.</title>
<date>2008</date>
<booktitle>In Proceedings of the LREC</booktitle>
<location>Marrakesh, Morocco.</location>
<marker>Taul´e, Marti, Recasens, 2008</marker>
<rawString>Mariona Taul´e, Toni Marti, and Marta Recasens. 2008. Ancora: Multilevel annotated corpora for catalan and spanish. In Proceedings of the LREC 2008, Marrakesh, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Verhagen</author>
<author>Robert Gaizauskas</author>
<author>Frank Schilder</author>
<author>Mark Hepple</author>
<author>Graham Katz</author>
<author>James Pustejovsky</author>
</authors>
<title>Semeval-2007 task 15: Tempeval temporal relation identification.</title>
<date>2007</date>
<booktitle>In Proc. of the Fourth Int. Workshop on Semantic Evaluations (SemEval2007),</booktitle>
<pages>75--80</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1064" citStr="Verhagen et al., 2007" startWordPosition="142" endWordPosition="145">r of which was split up in four sub tasks, motivated by the notion that smaller subtasks would make both data preparation and temporal relation extraction easier. Manually annotated data were provided for six languages: Chinese, English, French, Italian, Korean and Spanish. 1 Introduction The ultimate aim of temporal processing is the automatic identification of all temporal referring expressions, events and temporal relations within a text. However, addressing this aim is beyond the scope of an evaluation challenge and a more modest approach is appropriate. The 2007 SemEval task, TempEval-1 (Verhagen et al., 2007; Verhagen et al., 2009), was an initial evaluation exercise based on three limited temporal ordering and anchoring tasks that were considered realistic both from the perspective of assembling resources for development and testing and from the perspective of developing systems capable of addressing the tasks.1 TempEval-2 is based on TempEval-1, but is more elaborate in two respects: (i) it is a multilingual task, and (ii) it consists of six subtasks rather than three. In the rest of this paper, we first introduce the data that we are dealing with. Which gets us in a position to present the lis</context>
</contexts>
<marker>Verhagen, Gaizauskas, Schilder, Hepple, Katz, Pustejovsky, 2007</marker>
<rawString>Marc Verhagen, Robert Gaizauskas, Frank Schilder, Mark Hepple, Graham Katz, and James Pustejovsky. 2007. Semeval-2007 task 15: Tempeval temporal relation identification. In Proc. of the Fourth Int. Workshop on Semantic Evaluations (SemEval2007), pages 75–80, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Verhagen</author>
<author>Robert Gaizauskas</author>
<author>Frank Schilder</author>
<author>Mark Hepple</author>
<author>Jessica Moszkowicz</author>
<author>James Pustejovsky</author>
</authors>
<title>The tempeval challenge: identifying temporal relations in text. Language Resources and Evaluation.</title>
<date>2009</date>
<contexts>
<context position="1088" citStr="Verhagen et al., 2009" startWordPosition="146" endWordPosition="149"> in four sub tasks, motivated by the notion that smaller subtasks would make both data preparation and temporal relation extraction easier. Manually annotated data were provided for six languages: Chinese, English, French, Italian, Korean and Spanish. 1 Introduction The ultimate aim of temporal processing is the automatic identification of all temporal referring expressions, events and temporal relations within a text. However, addressing this aim is beyond the scope of an evaluation challenge and a more modest approach is appropriate. The 2007 SemEval task, TempEval-1 (Verhagen et al., 2007; Verhagen et al., 2009), was an initial evaluation exercise based on three limited temporal ordering and anchoring tasks that were considered realistic both from the perspective of assembling resources for development and testing and from the perspective of developing systems capable of addressing the tasks.1 TempEval-2 is based on TempEval-1, but is more elaborate in two respects: (i) it is a multilingual task, and (ii) it consists of six subtasks rather than three. In the rest of this paper, we first introduce the data that we are dealing with. Which gets us in a position to present the list of task introduced by </context>
</contexts>
<marker>Verhagen, Gaizauskas, Schilder, Hepple, Moszkowicz, Pustejovsky, 2009</marker>
<rawString>Marc Verhagen, Robert Gaizauskas, Frank Schilder, Mark Hepple, Jessica Moszkowicz, and James Pustejovsky. 2009. The tempeval challenge: identifying temporal relations in text. Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Verhagen</author>
</authors>
<title>The Brandeis Annotation Tool.</title>
<date>2010</date>
<booktitle>In Language Resources and Evaluation Conference, LREC</booktitle>
<contexts>
<context position="8622" citStr="Verhagen, 2010" startWordPosition="1383" endWordPosition="1384"> tokens C D E F X Chinese 23,000 ✓ ✓ ✓ ✓ English 63,000 ✓ ✓ ✓ ✓ Italian 27,000 ✓ ✓ ✓ French 19,000 ✓ Korean 14,000 Spanish 68,000 ✓ ✓ Table 1: Corpus size and relation tasks All corpora include event and timex annotation. The French corpus contained a subcorpus with temporal relations but these relations were not split into the four tasks C through F. Annotation proceeded in two phases: a dual annotation phase where two annotators annotate each document and an adjudication phase where a judge resolves disagreements between the annotators. Most languages used BAT, the Brandeis Annotation Tool (Verhagen, 2010), a generic webbased annotation tool that is centered around the notion of annotation tasks. With the task decomposition allowed by BAT, it is possible to structure the complex task of temporal annotation by splitting it up in as many sub tasks as seems useful. As 3Seehttp://www.timeml.org. such, BAT was well-suited for TempEval-2 annotation. We now give a few more details on the English and Spanish data, skipping the other languages for reasons that will become obvious at the beginning of section 6. The English data sets were based on TimeBank (Pustejovsky et al., 2003; Boguraev et al., 2007)</context>
</contexts>
<marker>Verhagen, 2010</marker>
<rawString>Marc Verhagen. 2010. The Brandeis Annotation Tool. In Language Resources and Evaluation Conference, LREC 2010, Malta.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>