<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003030">
<title confidence="0.970317">
OE: WSD Using Optimal Ensembling (OE) Method
</title>
<author confidence="0.998464">
Harri M. T. Saarikoski
</author>
<affiliation confidence="0.961016">
Helsinki University
Language Technology PhD Programme
</affiliation>
<address confidence="0.845744">
F-00014 Helsinki, Finland
</address>
<email confidence="0.999172">
harri.saarikoski@helsinki.fi
</email>
<sectionHeader confidence="0.993905" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999248">
Optimal ensembling (OE) is a word sense
disambiguation (WSD) method using
word-specific training factors (average pos-
itive vs negative training per sense, posex
and negex) to predict best system (classifier
algorithm / applicable feature set) for given
target word. Our official entry (OE1) in
Senseval-4 Task 17 (coarse-grained En-
glish lexical sample task) contained many
design flaws and thus failed to show the
whole potential of the method, finishing
-4.9% behind top system (+0.5 gain over
best base system). A fixed system (OE2)
finished only -3.4% behind (+2.0% net
gain). All our systems were &apos;closed&apos;, i.e.
used the official training data only (average
56 training examples per each sense). We
also show that the official evaluation mea-
sure tends to favor systems that do well
with high-trained words.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999098125">
Optimal ensembling is a novel method for
combining WSD systems and obtaining higher
classification accuracy (presented more fully in
Saarikoski et al. 2007). The essential difference
from other ensembling methods (such as various
types of voting ensembles and cross-validation
based best machine selection) is that best machine
is predicted using factors calculated from words
(e.g. number of senses) and their training data (e.g.
number of training examples per sense). The
method is loosely based on findings of system
performance differences in both WSD (different
machines by Yarowsky et al., 2002 and different
feature sets by Mihalcea, 2002) and other
classification tasks such as text categorization
(Forman et al., 2004, Bay et al., 2002).
</bodyText>
<sectionHeader confidence="0.987154" genericHeader="introduction">
2 Method
</sectionHeader>
<bodyText confidence="0.9993545">
We first describe in detail the two selection
routines in OE as deployed in this experiment.
</bodyText>
<subsectionHeader confidence="0.95987">
2.1 Machine (Mach) Selection
</subsectionHeader>
<bodyText confidence="0.999259833333333">
We selected support vector machine (SVM)
(Vapnik, 1995) and Naive Bayes (NB) (John et al.
1995) as classifiers for our base systems to be
optimally ensembled. This was mainly because of
their attested strength at earlier Senseval
evaluations (Edmonds et al. 2002, Mihalcea et al.
2004) and mutual complementarity discovered by
us (Saarikoski et al., 2007). Original batch of
candidate machines that we tested for OE using
Senseval-2 dataset included the following
classifiers: Decision Stump, Decision Tree with
various values of confidence (c) parameter 0.05,
0.15, 0.25 and instance-based classifier with k
values ranging from 1..15 at intervals of two 1.
After cross-validation runs against current dataset
(see below), however, SVM and NB proved again
to be overall strongest regardless of training input,
so we built OE around those two classifiers.
</bodyText>
<subsectionHeader confidence="0.998685">
2.2 Feature Set (Fset) Selection
</subsectionHeader>
<bodyText confidence="0.998395285714286">
We extracted three contextual feature sets from
training data for all words to train the machines: 1-
grams (1g) and sequential 2-grams both from
whole instance (2g) as well as part-of-speech tags
from local 1-word window around and including
target word (pos3). We also used three &apos;multifsets&apos;
(1g-2g, 1g-pos3, 2g-pos3).
</bodyText>
<footnote confidence="0.941725">
1We used Weka implementations (J48, Ibk, SMO, Decision
Stump, NaiveBayes) of these algorithms (Witten, 2005).
</footnote>
<page confidence="0.961853">
257
</page>
<bodyText confidence="0.803989">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 257–260,
Prague, June 2007. c�2007 Association for Computational Linguistics
</bodyText>
<subsectionHeader confidence="0.998687">
2.3 Best-System Prediction Factors
</subsectionHeader>
<bodyText confidence="0.998811">
In Figure 1, we quote prediction factors used for
predicting best system for some test words.
</bodyText>
<figureCaption confidence="0.584197333333333">
Figure 1. Prediction factors and OE1 accuracy for
some test words in Senseval-4 Task 17 (sorted by
OE1 accuracy at the word).
</figureCaption>
<sectionHeader confidence="0.991749" genericHeader="method">
3 System Descriptions
</sectionHeader>
<bodyText confidence="0.998936911764706">
We designed and ran two systems:
OE1 (official): For OE1, we used two machines
in three configurations (SVMc=0.1, SVMc=1.0,
NB) trained on 3 feature sets, totalling at 3*3 = 9
base systems (number of machines * number of
fsets for each). Selection of c(omplexity) parameter
for SVM was based on previous knowledge of
performance differences of c=0.1 and c=1.0 based
systems as reported in Saarikoski et al. (2007).
This is based on accounts by e.g. Vapnik (1995)
that lower c value makes the classifier generate a
more complex training model which is more
suitable for tougher words (lower posex, higher
negex).
We learned the best-system predictor model
using performance data from Senseval-4 10CV
runs only. For 70 words where two fsets performed
within +/-5% of each other, we added the next best
fset into a &apos;multifset&apos;.
OE2 (unofficial): This system incorporated the
following fixes to OE1 (see Discussion below for
motivations for these fixes): First, we significantly
reduced the base system grain. We only used two
machines strongest in 10CV runs (SVMc=0.1 and
NB) and these machines were trained with fsets
found best for those machines in 10CV runs: pos3
for both machines, SVMc=0.1 was additionally
trained with 1g and NB with 2g respectively. This
resulted in a 2 * 2 = 4-system ensemble. Best fset
was still selected on the basis of 10CV runs.
As training data for the best-machine predictor,
we used the performance profiles of about 50
systems (both our own and Senseval systems) run
mainly against Senseval-2 English lexical sample
dataset. We decided to use only two prediction
factors (posex and negex, see Figure 1) to predict
best machine for each word. This was because
previously we had found these two machines
(SVM and NB) particularly differing with regard
to the combination or cross-section of these two
factors. (For illustration of the predictor model
with posex and negex as the two axes and
discussion of other possible factors, see Saarikoski
et. al, 2007. As to reasons for such a performance
difference between any two classification
machines, see also Yarowsky et al., 2002).
Difference in the best-system predictions of
these two systems (OE1 vs OE2) was substantial:
33 words fully changed machine (from SVM to
NB or vice versa), 40 words partially changed the
system (change of SVM configuration or change of
fset from multifset to single fset). Only 27 words
kept the same machine in same configuration and
fset. We can therefore call OE2 a substantial
revision of OE1 (in effect a rather total departure
from CV-based selection toward actual word factor
based optimal ensembling).
In both OEs, the mach-fset combination
predicted to be the best for a word was run against
the test instances of that word 2. In case of
&apos;multifsets&apos;, each single fset had equal probability-
based vote in disambiguating the test instances of
2 SyntaLex code (Mohammad and Pedersen, 2002,
http://www.d.umn.edu/~tpederse/syntalex.html) was used for
extracting n-grams and carrying out disambiguation. Brill
Tagger (Brill, 1995) was used for extracting PoS tags. Weka
library of classifiers (Witten, 2005) was used to run cross-vali-
dations and best-system predictors.
</bodyText>
<page confidence="0.986206">
258
</page>
<bodyText confidence="0.993159">
that word. As usual, the sense with highest
probability was chosen as answer for each
instance.
</bodyText>
<sectionHeader confidence="0.819231" genericHeader="method">
4 Test Results
</sectionHeader>
<bodyText confidence="0.618688">
Here are the results:
</bodyText>
<table confidence="0.994595">
system name gross gain net gain accuracy3
OE1 +3.0 (+7.8) +0.5 (+4.4) 83.8
OE2 +2.3 (+7.0) +2.0 (+5.8) 85.3
</table>
<tableCaption confidence="0.9079715">
Table 1. Results of OE systems. In columns 2-3,
macro (micro) averaged per-word gross and net
</tableCaption>
<bodyText confidence="0.711735111111111">
gains calculated from actual test runs (not 10CV
runs) are reported. Column 4 reports the official
macro-averaged accuracy for all words of our
systems. (Differences of the respective benefits of
these evaluation measures are outlined in
Discussion below and more generally in Sebastiani
(2002). Terms &apos;gross (or potential) gain&apos; and &apos;net
(realized) gain&apos; are defined in Saarikoski et al.
(2007).).
</bodyText>
<sectionHeader confidence="0.998664" genericHeader="evaluation">
5 Discussion
</sectionHeader>
<bodyText confidence="0.997124375">
We now turn to analyze these results. We can first
note that results are largely in line with our
previous findings with OEs and other types of
ensembles (see Saarikoski et al., 2007). In what
follows we attempt to account for the results: why
OE1 finished as much behind top system and also
why OE2 performed that much better than OE1.
This first &apos;known issue&apos; concerns both OEs:
(1) Base system accuracy was low because we
did not use strong fsets: Our official entry
finished at 7th place in the evaluation, -4.9% behind
top system while the inofficial entry would have
finished in 5th place (-3.4% behind). We attribute
this mainly to the absence of more advanced
feature sets. For example, we did not employ
syntactic parse features (such as predicate-object
pairs) from which Yarowsky et al. (2002) showed
+2% gain. We would also naturally lose to any
systems using extra training or lexical knowledge
(e.g. 2nd place finisher UBC-ALM, at 86.9
accuracy, used both semantic domains and SemCor
corpus). But without knowing how much extra
knowledge such &apos;open&apos; systems used, we cannot
say by how much.
</bodyText>
<page confidence="0.451485">
3 Best base system in both OEs was NB-pos3 (83.3).
</page>
<bodyText confidence="0.9239841">
Specifically in OE1 entry, there were two basic
design flaws which we address next.
(2) Base system grain was too high to produce
enough net gain: The base system grain (18 base
systems) we attempted to predict in OE1 was far
too great since prediction accuracy rapidly
decreases when adding new systems. The grain
was also unnecessarily great, since the 4-grain we
used for OE2 could harvest most of the gross gain
(cf. gross gains of the two systems in Table 1).
</bodyText>
<listItem confidence="0.862631294117647">
(3) Using 10CV runs uncritically for best fset
selection: This was ill-advised because of many
reasons. First, selecting best fset for WSD based on
CV runs is known to be a difficult task (Mihalcea,
2002). Prediction accuracy for the three fsets we
used for OE1 was 0.74, i.e. for 26 words out of 100
best fset was mispredicted. About half of these
were cases where machine was mispredicted as
well and average loss tended to be even greater.
Second, multifsets could not be 10CV-tested with
the Weka machine-learning toolkit we used
(Witten, 2005). Our custom resolution to this
multifset selection task was to select best and next
best fset. This turned out to produce many false
predictions, some of which were quite substantial
(&gt; 10% loss to best fset). For instance, at system.n
we lost &gt; 30% from selecting NB-2g instead of
</listItem>
<bodyText confidence="0.799482222222222">
actual best system (NB-pos3). Third, only after
submitting the entry, we also realized two strongest
fsets are not necessarily complementary (i.e. that
each would contain relevant clues for different test
instances) and that learning machines might be
confused (i.e. could not effectively carry out
feature selection and weighting) by the profusion
and heterogeneity of features in multifsets. In fact,
we found that omitting multifsets from OE1 (i.e.
having 3 single fsets with the same 3 machines =
6-system OE) would have worked slightly better
than OE1 (3*3=9): the accuracy rose from 83.8 to
84.1. Fourth, it was found previously (Saarikoski et
al., 2007) that CV-based best system prediction
scheme tends to produce less gain than OE (cf.
accuracy of OE1 &lt; OE2 in Table 1).
The remaining argument discusses Senseval
evaluation measure (applies to all OE systems):
</bodyText>
<listItem confidence="0.98644275">
(4) Official evaluation measure is particularly
unfavorable to OE systems: Senseval scoring
scheme4 is calculated as the number of instances
disambiguated correctly divided by number of all
</listItem>
<footnote confidence="0.992917">
4 Documentation for scoring scheme can be found at:
http://www.cse.unt.edu/~rada/senseval/senseval3/scoring/
</footnote>
<page confidence="0.997577">
259
</page>
<bodyText confidence="0.999862369565218">
instances in test dataset. This measure (termed
&apos;macro-averaged accuracy&apos; in Sebastiani, 2002) is
known to upweigh classification cases (words) that
have more test instances. While we recognize the
usefulness of this measure, we calculated in Table
1 the alternative measure (termed &apos;micro-averaged
accuracy&apos; in Sebastiani, 2002). It differs from the
former (defined by e.g. Sebastiani, 2002) in that
all words are treated equally (i.e. &apos;normalized&apos;)
regardless of number of test instances. In addition,
it has been Senseval practice (Edmonds et al. 2002,
Mihalcea et al. 2004) that words with great number
of test instances tend to have an equally great
number of training instances. At such &apos;easier&apos;
words, system performance differences (sysdiff)
occur much less and since OE is based on locating
and making use of sysdiff, it cannot perform well.
Therefore, it is liable to lose to single-machine
systems with inherently stronger fsets (see point 1
above). For these reasons, the measures are very
different with the latter revealing the OE potential
more appropriately.
In fact, we estimate that only 40 out of the 100
test words in this dataset show any kind of sysdiff
between most participating systems (&gt; 5% macro-
averaged sysdiff per word). Furthermore, only 20
of them only are likely to produce substantial
sysdiff (&gt; 10%). For example, in our 10CV runs,
we got 0.99 accuracies by all base systems for the
very highly trained word say.v with posex &gt; 500. If
there was a participating system that achieved 1.00
in such a single high-train word (say.v), the huge
number of test instances of that word raised its
macro-averaged accuracy, winning considerably
over systems performing well with low-train words
(e.g. propose.v with posex=11 and negex=24 and
grain=3 where both OE1 and OE2 performed at
0.93 accuracy owing to correct best system
choice). In other words, the official measure does
not account for the finding (Yarowsky et al., 2002
and Saarikoski et al., 2007) that systems
considerably differ precisely in terms of their
ability to disambiguate high/low-train words
(measured by posex/negex factors). Therefore, it
can be said that the official measure fails to treat
all systems equally.
</bodyText>
<sectionHeader confidence="0.995245" genericHeader="conclusions">
6 Conclusion and Further Work
</sectionHeader>
<bodyText confidence="0.999872133333333">
Since OE is a generic method that can be applied
to any base systems, we believe it has a place in
WSD methodology. With remaining open
questions resolved (optimizing system grain to
feasible prediction accuracy, discovering more
predictive factors for both machines and fsets,
understanding how the evaluation measures
complete each other), it is probable that OE can
improve current state of the art WSD systems
(especially if provided with stronger while still
complementary base systems). Though OE systems
run the risk that OE may in fact be inferior to its
best base system, we would like to note that thus
far no OE of ours (around 10-15 different tests) has
failed to produce net gain.
</bodyText>
<sectionHeader confidence="0.999442" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998779595238095">
Bay, S. D., and Pazzani, M. J. Characterizing model errors and
differences. In 17th International Conference on Machine
Learning (2000)
Brill, E. Transformation-Based Error-Driven Learning and
Natural Language Processing: A Case Study in Part of
Speech Tagging Computational Linguistics (1995)
Edmonds, P., and Kilgarriff, A. Introduction to the Special
Issue on evaluating word sense disambiguation programs.
Journal of Natural Language Engineering 8(4) (2002)
Forman, G., and Cohen, I. Learning from Little: Comparison
of Classifiers Given Little Training. In ECML, 15th
European Conference on Machine Learning and the 8th
European Conference on Principles and Practice of
Knowledge Discovery in Databases (2004)
John, G. and Langley, P. Estimating Continuous Distributions
in Bayesian Classifiers. Proceedings of the Eleventh
Conference on Uncertainty in Artificial Intelligence.
Morgan Kaufmann, San Mateo (1995)
Mihalcea, R. Word sense disambiguation with pattern learning
and automatic feature selection. Journal of Natural
Language Engineering, 8(4) (2002)
Mihalcea, R., Kilgarriff, A. and Chklovski, T. The
SENSEVAL-3 English lexical sample task. Proceedings of
SENSEVAL-3 Workshop at ACL (2004)
Mohammad, S. and Pedersen, T (2004). Complementarity of
Lexical and Simple Syntactic Features: The Syntalex
Approach to Senseval-3. Proceedings of Senseval-3
Saarikoski, H., Legrand, S., Gelbukh, A. (2007) Case-
Sensitivity of Classifiers for WSD: Complex Systems
Disambiguate Tough Words Better. In CICLING 2007 and
Lecture Notes in Computer Science, Springer
Sebastiani, F. Machine learning in automated text
categorization, ACM Computing Surveys (CSUR), Vol.
34 Issue 1 (2002) ACM Press, New York, NY, USA.
Vapnik, V. N. The Nature of Statistical Learning Theory.
Springer (1995)
Witten, I., Frank, E. Data Mining: Practical Machine Learning
Tools and Techniques (Second Edition). Morgan
Kaufmann (2005).
Yarowsky, D. and Florian, R. Evaluating sense
disambiguation across diverse parameter spaces. Journal
of Natural Language Engineering, 8(4) (2002) 293-311.
</reference>
<page confidence="0.996817">
260
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.896629">
<title confidence="0.99812">OE: WSD Using Optimal Ensembling (OE) Method</title>
<author confidence="0.999964">Harri M T Saarikoski</author>
<affiliation confidence="0.9569065">Helsinki University Language Technology PhD Programme</affiliation>
<address confidence="0.999942">F-00014 Helsinki, Finland</address>
<email confidence="0.993219">harri.saarikoski@helsinki.fi</email>
<abstract confidence="0.999495095238095">Optimal ensembling (OE) is a word sense disambiguation (WSD) method using word-specific training factors (average posvs negative training per sense, to predict best system (classifier algorithm / applicable feature set) for given target word. Our official entry (OE1) in Senseval-4 Task 17 (coarse-grained English lexical sample task) contained many design flaws and thus failed to show the whole potential of the method, finishing -4.9% behind top system (+0.5 gain over best base system). A fixed system (OE2) finished only -3.4% behind (+2.0% net gain). All our systems were &apos;closed&apos;, i.e. used the official training data only (average 56 training examples per each sense). We also show that the official evaluation measure tends to favor systems that do well with high-trained words.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S D Bay</author>
<author>M J Pazzani</author>
</authors>
<title>Characterizing model errors and differences.</title>
<date>2000</date>
<booktitle>In 17th International Conference on Machine Learning</booktitle>
<marker>Bay, Pazzani, 2000</marker>
<rawString>Bay, S. D., and Pazzani, M. J. Characterizing model errors and differences. In 17th International Conference on Machine Learning (2000)</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part of Speech Tagging Computational Linguistics</title>
<date>1995</date>
<contexts>
<context position="6705" citStr="Brill, 1995" startWordPosition="1044" endWordPosition="1045">iguration and fset. We can therefore call OE2 a substantial revision of OE1 (in effect a rather total departure from CV-based selection toward actual word factor based optimal ensembling). In both OEs, the mach-fset combination predicted to be the best for a word was run against the test instances of that word 2. In case of &apos;multifsets&apos;, each single fset had equal probabilitybased vote in disambiguating the test instances of 2 SyntaLex code (Mohammad and Pedersen, 2002, http://www.d.umn.edu/~tpederse/syntalex.html) was used for extracting n-grams and carrying out disambiguation. Brill Tagger (Brill, 1995) was used for extracting PoS tags. Weka library of classifiers (Witten, 2005) was used to run cross-validations and best-system predictors. 258 that word. As usual, the sense with highest probability was chosen as answer for each instance. 4 Test Results Here are the results: system name gross gain net gain accuracy3 OE1 +3.0 (+7.8) +0.5 (+4.4) 83.8 OE2 +2.3 (+7.0) +2.0 (+5.8) 85.3 Table 1. Results of OE systems. In columns 2-3, macro (micro) averaged per-word gross and net gains calculated from actual test runs (not 10CV runs) are reported. Column 4 reports the official macro-averaged accurac</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Brill, E. Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part of Speech Tagging Computational Linguistics (1995)</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Edmonds</author>
<author>A Kilgarriff</author>
</authors>
<title>Introduction to the Special Issue on evaluating word sense disambiguation programs.</title>
<date>2002</date>
<journal>Journal of Natural Language Engineering</journal>
<booktitle>In ECML, 15th European Conference on Machine Learning and the 8th European Conference on Principles and Practice of Knowledge Discovery in Databases</booktitle>
<volume>8</volume>
<issue>4</issue>
<marker>Edmonds, Kilgarriff, 2002</marker>
<rawString>Edmonds, P., and Kilgarriff, A. Introduction to the Special Issue on evaluating word sense disambiguation programs. Journal of Natural Language Engineering 8(4) (2002) Forman, G., and Cohen, I. Learning from Little: Comparison of Classifiers Given Little Training. In ECML, 15th European Conference on Machine Learning and the 8th European Conference on Principles and Practice of Knowledge Discovery in Databases (2004)</rawString>
</citation>
<citation valid="true">
<authors>
<author>G John</author>
<author>P Langley</author>
</authors>
<title>Estimating Continuous Distributions in Bayesian Classifiers.</title>
<date>1995</date>
<booktitle>Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence.</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo</location>
<marker>John, Langley, 1995</marker>
<rawString>John, G. and Langley, P. Estimating Continuous Distributions in Bayesian Classifiers. Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence. Morgan Kaufmann, San Mateo (1995)</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
</authors>
<title>Word sense disambiguation with pattern learning and automatic feature selection.</title>
<date>2002</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="1659" citStr="Mihalcea, 2002" startWordPosition="247" endWordPosition="248">thod for combining WSD systems and obtaining higher classification accuracy (presented more fully in Saarikoski et al. 2007). The essential difference from other ensembling methods (such as various types of voting ensembles and cross-validation based best machine selection) is that best machine is predicted using factors calculated from words (e.g. number of senses) and their training data (e.g. number of training examples per sense). The method is loosely based on findings of system performance differences in both WSD (different machines by Yarowsky et al., 2002 and different feature sets by Mihalcea, 2002) and other classification tasks such as text categorization (Forman et al., 2004, Bay et al., 2002). 2 Method We first describe in detail the two selection routines in OE as deployed in this experiment. 2.1 Machine (Mach) Selection We selected support vector machine (SVM) (Vapnik, 1995) and Naive Bayes (NB) (John et al. 1995) as classifiers for our base systems to be optimally ensembled. This was mainly because of their attested strength at earlier Senseval evaluations (Edmonds et al. 2002, Mihalcea et al. 2004) and mutual complementarity discovered by us (Saarikoski et al., 2007). Original ba</context>
<context position="9421" citStr="Mihalcea, 2002" startWordPosition="1500" endWordPosition="1501"> flaws which we address next. (2) Base system grain was too high to produce enough net gain: The base system grain (18 base systems) we attempted to predict in OE1 was far too great since prediction accuracy rapidly decreases when adding new systems. The grain was also unnecessarily great, since the 4-grain we used for OE2 could harvest most of the gross gain (cf. gross gains of the two systems in Table 1). (3) Using 10CV runs uncritically for best fset selection: This was ill-advised because of many reasons. First, selecting best fset for WSD based on CV runs is known to be a difficult task (Mihalcea, 2002). Prediction accuracy for the three fsets we used for OE1 was 0.74, i.e. for 26 words out of 100 best fset was mispredicted. About half of these were cases where machine was mispredicted as well and average loss tended to be even greater. Second, multifsets could not be 10CV-tested with the Weka machine-learning toolkit we used (Witten, 2005). Our custom resolution to this multifset selection task was to select best and next best fset. This turned out to produce many false predictions, some of which were quite substantial (&gt; 10% loss to best fset). For instance, at system.n we lost &gt; 30% from </context>
</contexts>
<marker>Mihalcea, 2002</marker>
<rawString>Mihalcea, R. Word sense disambiguation with pattern learning and automatic feature selection. Journal of Natural Language Engineering, 8(4) (2002)</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>A Kilgarriff</author>
<author>T Chklovski</author>
</authors>
<title>The SENSEVAL-3 English lexical sample task.</title>
<date>2004</date>
<booktitle>Proceedings of SENSEVAL-3 Workshop at ACL</booktitle>
<contexts>
<context position="2176" citStr="Mihalcea et al. 2004" startWordPosition="329" endWordPosition="332">s in both WSD (different machines by Yarowsky et al., 2002 and different feature sets by Mihalcea, 2002) and other classification tasks such as text categorization (Forman et al., 2004, Bay et al., 2002). 2 Method We first describe in detail the two selection routines in OE as deployed in this experiment. 2.1 Machine (Mach) Selection We selected support vector machine (SVM) (Vapnik, 1995) and Naive Bayes (NB) (John et al. 1995) as classifiers for our base systems to be optimally ensembled. This was mainly because of their attested strength at earlier Senseval evaluations (Edmonds et al. 2002, Mihalcea et al. 2004) and mutual complementarity discovered by us (Saarikoski et al., 2007). Original batch of candidate machines that we tested for OE using Senseval-2 dataset included the following classifiers: Decision Stump, Decision Tree with various values of confidence (c) parameter 0.05, 0.15, 0.25 and instance-based classifier with k values ranging from 1..15 at intervals of two 1. After cross-validation runs against current dataset (see below), however, SVM and NB proved again to be overall strongest regardless of training input, so we built OE around those two classifiers. 2.2 Feature Set (Fset) Selecti</context>
<context position="11798" citStr="Mihalcea et al. 2004" startWordPosition="1866" endWordPosition="1869">t.edu/~rada/senseval/senseval3/scoring/ 259 instances in test dataset. This measure (termed &apos;macro-averaged accuracy&apos; in Sebastiani, 2002) is known to upweigh classification cases (words) that have more test instances. While we recognize the usefulness of this measure, we calculated in Table 1 the alternative measure (termed &apos;micro-averaged accuracy&apos; in Sebastiani, 2002). It differs from the former (defined by e.g. Sebastiani, 2002) in that all words are treated equally (i.e. &apos;normalized&apos;) regardless of number of test instances. In addition, it has been Senseval practice (Edmonds et al. 2002, Mihalcea et al. 2004) that words with great number of test instances tend to have an equally great number of training instances. At such &apos;easier&apos; words, system performance differences (sysdiff) occur much less and since OE is based on locating and making use of sysdiff, it cannot perform well. Therefore, it is liable to lose to single-machine systems with inherently stronger fsets (see point 1 above). For these reasons, the measures are very different with the latter revealing the OE potential more appropriately. In fact, we estimate that only 40 out of the 100 test words in this dataset show any kind of sysdiff b</context>
</contexts>
<marker>Mihalcea, Kilgarriff, Chklovski, 2004</marker>
<rawString>Mihalcea, R., Kilgarriff, A. and Chklovski, T. The SENSEVAL-3 English lexical sample task. Proceedings of SENSEVAL-3 Workshop at ACL (2004)</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Mohammad</author>
<author>T Pedersen</author>
</authors>
<title>Complementarity of Lexical and Simple Syntactic Features: The Syntalex Approach to Senseval-3.</title>
<date>2004</date>
<booktitle>Proceedings of Senseval-3</booktitle>
<marker>Mohammad, Pedersen, 2004</marker>
<rawString>Mohammad, S. and Pedersen, T (2004). Complementarity of Lexical and Simple Syntactic Features: The Syntalex Approach to Senseval-3. Proceedings of Senseval-3</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Saarikoski</author>
<author>S Legrand</author>
<author>A Gelbukh</author>
</authors>
<title>CaseSensitivity of Classifiers for WSD: Complex Systems Disambiguate Tough Words Better.</title>
<date>2007</date>
<booktitle>In CICLING 2007 and Lecture Notes in Computer Science,</booktitle>
<publisher>Springer</publisher>
<contexts>
<context position="1168" citStr="Saarikoski et al. 2007" startWordPosition="171" endWordPosition="174">ontained many design flaws and thus failed to show the whole potential of the method, finishing -4.9% behind top system (+0.5 gain over best base system). A fixed system (OE2) finished only -3.4% behind (+2.0% net gain). All our systems were &apos;closed&apos;, i.e. used the official training data only (average 56 training examples per each sense). We also show that the official evaluation measure tends to favor systems that do well with high-trained words. 1 Introduction Optimal ensembling is a novel method for combining WSD systems and obtaining higher classification accuracy (presented more fully in Saarikoski et al. 2007). The essential difference from other ensembling methods (such as various types of voting ensembles and cross-validation based best machine selection) is that best machine is predicted using factors calculated from words (e.g. number of senses) and their training data (e.g. number of training examples per sense). The method is loosely based on findings of system performance differences in both WSD (different machines by Yarowsky et al., 2002 and different feature sets by Mihalcea, 2002) and other classification tasks such as text categorization (Forman et al., 2004, Bay et al., 2002). 2 Method</context>
<context position="4074" citStr="Saarikoski et al. (2007)" startWordPosition="618" endWordPosition="621">n factors used for predicting best system for some test words. Figure 1. Prediction factors and OE1 accuracy for some test words in Senseval-4 Task 17 (sorted by OE1 accuracy at the word). 3 System Descriptions We designed and ran two systems: OE1 (official): For OE1, we used two machines in three configurations (SVMc=0.1, SVMc=1.0, NB) trained on 3 feature sets, totalling at 3*3 = 9 base systems (number of machines * number of fsets for each). Selection of c(omplexity) parameter for SVM was based on previous knowledge of performance differences of c=0.1 and c=1.0 based systems as reported in Saarikoski et al. (2007). This is based on accounts by e.g. Vapnik (1995) that lower c value makes the classifier generate a more complex training model which is more suitable for tougher words (lower posex, higher negex). We learned the best-system predictor model using performance data from Senseval-4 10CV runs only. For 70 words where two fsets performed within +/-5% of each other, we added the next best fset into a &apos;multifset&apos;. OE2 (unofficial): This system incorporated the following fixes to OE1 (see Discussion below for motivations for these fixes): First, we significantly reduced the base system grain. We only</context>
<context position="7579" citStr="Saarikoski et al. (2007)" startWordPosition="1182" endWordPosition="1185">ults Here are the results: system name gross gain net gain accuracy3 OE1 +3.0 (+7.8) +0.5 (+4.4) 83.8 OE2 +2.3 (+7.0) +2.0 (+5.8) 85.3 Table 1. Results of OE systems. In columns 2-3, macro (micro) averaged per-word gross and net gains calculated from actual test runs (not 10CV runs) are reported. Column 4 reports the official macro-averaged accuracy for all words of our systems. (Differences of the respective benefits of these evaluation measures are outlined in Discussion below and more generally in Sebastiani (2002). Terms &apos;gross (or potential) gain&apos; and &apos;net (realized) gain&apos; are defined in Saarikoski et al. (2007).). 5 Discussion We now turn to analyze these results. We can first note that results are largely in line with our previous findings with OEs and other types of ensembles (see Saarikoski et al., 2007). In what follows we attempt to account for the results: why OE1 finished as much behind top system and also why OE2 performed that much better than OE1. This first &apos;known issue&apos; concerns both OEs: (1) Base system accuracy was low because we did not use strong fsets: Our official entry finished at 7th place in the evaluation, -4.9% behind top system while the inofficial entry would have finished i</context>
<context position="10708" citStr="Saarikoski et al., 2007" startWordPosition="1708" endWordPosition="1711"> only after submitting the entry, we also realized two strongest fsets are not necessarily complementary (i.e. that each would contain relevant clues for different test instances) and that learning machines might be confused (i.e. could not effectively carry out feature selection and weighting) by the profusion and heterogeneity of features in multifsets. In fact, we found that omitting multifsets from OE1 (i.e. having 3 single fsets with the same 3 machines = 6-system OE) would have worked slightly better than OE1 (3*3=9): the accuracy rose from 83.8 to 84.1. Fourth, it was found previously (Saarikoski et al., 2007) that CV-based best system prediction scheme tends to produce less gain than OE (cf. accuracy of OE1 &lt; OE2 in Table 1). The remaining argument discusses Senseval evaluation measure (applies to all OE systems): (4) Official evaluation measure is particularly unfavorable to OE systems: Senseval scoring scheme4 is calculated as the number of instances disambiguated correctly divided by number of all 4 Documentation for scoring scheme can be found at: http://www.cse.unt.edu/~rada/senseval/senseval3/scoring/ 259 instances in test dataset. This measure (termed &apos;macro-averaged accuracy&apos; in Sebastiani</context>
<context position="13202" citStr="Saarikoski et al., 2007" startWordPosition="2098" endWordPosition="2101">CV runs, we got 0.99 accuracies by all base systems for the very highly trained word say.v with posex &gt; 500. If there was a participating system that achieved 1.00 in such a single high-train word (say.v), the huge number of test instances of that word raised its macro-averaged accuracy, winning considerably over systems performing well with low-train words (e.g. propose.v with posex=11 and negex=24 and grain=3 where both OE1 and OE2 performed at 0.93 accuracy owing to correct best system choice). In other words, the official measure does not account for the finding (Yarowsky et al., 2002 and Saarikoski et al., 2007) that systems considerably differ precisely in terms of their ability to disambiguate high/low-train words (measured by posex/negex factors). Therefore, it can be said that the official measure fails to treat all systems equally. 6 Conclusion and Further Work Since OE is a generic method that can be applied to any base systems, we believe it has a place in WSD methodology. With remaining open questions resolved (optimizing system grain to feasible prediction accuracy, discovering more predictive factors for both machines and fsets, understanding how the evaluation measures complete each other)</context>
</contexts>
<marker>Saarikoski, Legrand, Gelbukh, 2007</marker>
<rawString>Saarikoski, H., Legrand, S., Gelbukh, A. (2007) CaseSensitivity of Classifiers for WSD: Complex Systems Disambiguate Tough Words Better. In CICLING 2007 and Lecture Notes in Computer Science, Springer</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sebastiani</author>
</authors>
<title>Machine learning in automated text categorization,</title>
<date>2002</date>
<journal>ACM Computing Surveys (CSUR),</journal>
<volume>34</volume>
<publisher>ACM Press,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7478" citStr="Sebastiani (2002)" startWordPosition="1168" endWordPosition="1169">s usual, the sense with highest probability was chosen as answer for each instance. 4 Test Results Here are the results: system name gross gain net gain accuracy3 OE1 +3.0 (+7.8) +0.5 (+4.4) 83.8 OE2 +2.3 (+7.0) +2.0 (+5.8) 85.3 Table 1. Results of OE systems. In columns 2-3, macro (micro) averaged per-word gross and net gains calculated from actual test runs (not 10CV runs) are reported. Column 4 reports the official macro-averaged accuracy for all words of our systems. (Differences of the respective benefits of these evaluation measures are outlined in Discussion below and more generally in Sebastiani (2002). Terms &apos;gross (or potential) gain&apos; and &apos;net (realized) gain&apos; are defined in Saarikoski et al. (2007).). 5 Discussion We now turn to analyze these results. We can first note that results are largely in line with our previous findings with OEs and other types of ensembles (see Saarikoski et al., 2007). In what follows we attempt to account for the results: why OE1 finished as much behind top system and also why OE2 performed that much better than OE1. This first &apos;known issue&apos; concerns both OEs: (1) Base system accuracy was low because we did not use strong fsets: Our official entry finished at </context>
<context position="11315" citStr="Sebastiani, 2002" startWordPosition="1794" endWordPosition="1795">al., 2007) that CV-based best system prediction scheme tends to produce less gain than OE (cf. accuracy of OE1 &lt; OE2 in Table 1). The remaining argument discusses Senseval evaluation measure (applies to all OE systems): (4) Official evaluation measure is particularly unfavorable to OE systems: Senseval scoring scheme4 is calculated as the number of instances disambiguated correctly divided by number of all 4 Documentation for scoring scheme can be found at: http://www.cse.unt.edu/~rada/senseval/senseval3/scoring/ 259 instances in test dataset. This measure (termed &apos;macro-averaged accuracy&apos; in Sebastiani, 2002) is known to upweigh classification cases (words) that have more test instances. While we recognize the usefulness of this measure, we calculated in Table 1 the alternative measure (termed &apos;micro-averaged accuracy&apos; in Sebastiani, 2002). It differs from the former (defined by e.g. Sebastiani, 2002) in that all words are treated equally (i.e. &apos;normalized&apos;) regardless of number of test instances. In addition, it has been Senseval practice (Edmonds et al. 2002, Mihalcea et al. 2004) that words with great number of test instances tend to have an equally great number of training instances. At such &apos;</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>Sebastiani, F. Machine learning in automated text categorization, ACM Computing Surveys (CSUR), Vol. 34 Issue 1 (2002) ACM Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer</publisher>
<contexts>
<context position="1946" citStr="Vapnik, 1995" startWordPosition="293" endWordPosition="294">achine is predicted using factors calculated from words (e.g. number of senses) and their training data (e.g. number of training examples per sense). The method is loosely based on findings of system performance differences in both WSD (different machines by Yarowsky et al., 2002 and different feature sets by Mihalcea, 2002) and other classification tasks such as text categorization (Forman et al., 2004, Bay et al., 2002). 2 Method We first describe in detail the two selection routines in OE as deployed in this experiment. 2.1 Machine (Mach) Selection We selected support vector machine (SVM) (Vapnik, 1995) and Naive Bayes (NB) (John et al. 1995) as classifiers for our base systems to be optimally ensembled. This was mainly because of their attested strength at earlier Senseval evaluations (Edmonds et al. 2002, Mihalcea et al. 2004) and mutual complementarity discovered by us (Saarikoski et al., 2007). Original batch of candidate machines that we tested for OE using Senseval-2 dataset included the following classifiers: Decision Stump, Decision Tree with various values of confidence (c) parameter 0.05, 0.15, 0.25 and instance-based classifier with k values ranging from 1..15 at intervals of two </context>
<context position="4123" citStr="Vapnik (1995)" startWordPosition="629" endWordPosition="630">s. Figure 1. Prediction factors and OE1 accuracy for some test words in Senseval-4 Task 17 (sorted by OE1 accuracy at the word). 3 System Descriptions We designed and ran two systems: OE1 (official): For OE1, we used two machines in three configurations (SVMc=0.1, SVMc=1.0, NB) trained on 3 feature sets, totalling at 3*3 = 9 base systems (number of machines * number of fsets for each). Selection of c(omplexity) parameter for SVM was based on previous knowledge of performance differences of c=0.1 and c=1.0 based systems as reported in Saarikoski et al. (2007). This is based on accounts by e.g. Vapnik (1995) that lower c value makes the classifier generate a more complex training model which is more suitable for tougher words (lower posex, higher negex). We learned the best-system predictor model using performance data from Senseval-4 10CV runs only. For 70 words where two fsets performed within +/-5% of each other, we added the next best fset into a &apos;multifset&apos;. OE2 (unofficial): This system incorporated the following fixes to OE1 (see Discussion below for motivations for these fixes): First, we significantly reduced the base system grain. We only used two machines strongest in 10CV runs (SVMc=0</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vapnik, V. N. The Nature of Statistical Learning Theory. Springer (1995)</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Witten</author>
<author>E Frank</author>
</authors>
<title>Data Mining:</title>
<date>2005</date>
<booktitle>Practical Machine Learning Tools and Techniques (Second Edition).</booktitle>
<publisher>Morgan Kaufmann</publisher>
<marker>Witten, Frank, 2005</marker>
<rawString>Witten, I., Frank, E. Data Mining: Practical Machine Learning Tools and Techniques (Second Edition). Morgan Kaufmann (2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
<author>R Florian</author>
</authors>
<title>Evaluating sense disambiguation across diverse parameter spaces.</title>
<date>2002</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>8</volume>
<issue>4</issue>
<pages>293--311</pages>
<marker>Yarowsky, Florian, 2002</marker>
<rawString>Yarowsky, D. and Florian, R. Evaluating sense disambiguation across diverse parameter spaces. Journal of Natural Language Engineering, 8(4) (2002) 293-311.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>