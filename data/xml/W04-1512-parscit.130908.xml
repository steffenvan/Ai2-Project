<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.897748">
Dependencies as Categories
</title>
<author confidence="0.84966">
Alexander Dikovsky
</author>
<bodyText confidence="0.894097">
LINA, Universite de Nantes
2 rue de la Houssiniere
BP 92208 F 44322 Nantes Cedex 3,
France
Alexandre.DikovskyAlina.univ-nantesir
</bodyText>
<sectionHeader confidence="0.879774" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997455833333333">
Categorial Dependency Grammars (CDG) in-
troduced in this paper make clear-cut distinc-
tion between local and distant word driven de-
pendencies. The former are treated in classi-
cal categorial grammar terms. The latter are
defined in terms of polarized valencies, whose
neutralization is controlled by the simple prin-
ciple &amp;quot;choose the first available&amp;quot; ( FA). Being
very expressive, CDGs are analyzed in polyno-
mial time. Besides this, CDGs represent a con-
venient frame for relating dependency grammar
with linguistic semantics.
</bodyText>
<sectionHeader confidence="0.99549" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999846466666667">
Dependency grammars (DGs) are formal gram-
mars assigning dependency trees (DTs) to well-
formed sentences. A DT of a sentence is a la-
belled arrows tree whose nodes are the words
of the sentence. A rather formal descrip-
tion of DGs and DG syntax was given by L.
Tesniere (Tesniere, 1959). The first exact defi-
nitions are due to D. Hays (Hays, 1960) and H.
Gaifman (Gaifman, 1961). The basic syntac-
tic principle behind the DGs is quite different
from that of syntagmatic grammars. They are
designed for and more adapted to definitions of
binary relations between wordforms (syntactic
dependencies), than to definitions of sentence
constituents. Meanwhile, historically the first
DGs define in fact both. The Hays-Gaifman&apos;s
DGs are lexicalized. They assign grammatical
categories to words and position the subordi-
nates with respect to their governors. In this
manner, they define not only the binary rela-
tions &amp;quot;governor —&gt; subordinate&amp;quot;, whose union
forms a tree, but also (due to the order given)
the projections of words on the sentence, which
form a system of constituents with the pro-
jected words serving as the constituents&apos; heads.
From the 70ies, it is known ((Gladkij, 1966;
Robinson, 1970)) that this link between the two
structures is reversible: a selection of one im-
mediate head per constituent induces a unique
DT by the following induction: C C C&apos;
</bodyText>
<equation confidence="0.473933">
root(ImmHead(C1)) —&gt;* root(ImmHead(C)).
</equation>
<bodyText confidence="0.99995684057971">
This structural &amp;quot;equivalence&amp;quot; produced an illu-
sion that DTs are byproduct of head selection
in constituent structures. So all syntagmatic
grammars with head selection (e.g., LFG (Ka-
plan and Bresnan, 1982) and HPSG (Pollard
and Sag, 1994)) may in a way be considered as
DGs. Formally, such &amp;quot;extension&amp;quot; to DG con-
cerns TAGs (Joshi et al., 1975) and catego-
rial grammars (CGs) (Bar-Hillel, 1953; Lambek,
1958)) as well. However, this analogy is very
superficial. It was soon realized that depen-
dency and precedence are rather independent.
For instance, the head driven DT above are al-
ways projective: the projections of all words fill
continuous segments. Meanwhile, discontinu-
ous non-projective dependencies are inevitable
in languages. They often mark communica-
tive structure (e.g. topicalization) and special
constructions encoding complex semantic rela-
tions (e.g. clefting, subject or object extrac-
tion in pied-piping, etc.). This distinction led to
many propositions, both in terms of order con-
straints (e.g., (Maruyama, 1990; Broker, 1998;
Duchier and Debusmann, 2001)) and in struc-
ture sharing terms like lifting (e.g., (Lombardo
and Lesmo, 1998; Kahane et al., 1998)). In log-
ical type based grammars, like CGs, adequate
extension to DG presupposes one more distinc-
tion: that of semantic functionality and of syn-
tactic subordinacy, which are opposite, for in-
stance, for verb and noun modifiers. Multi-
modal extensions of CGs take proper account
of these distinctions (cf. (Moortgat and Morrill,
1991; Morrill, 1994; Kruijff, 2001)).
The overwhelming majority of DGs are head
driven in the above sense. At the same
time, some linguistic theories, e.g. &amp;quot;Meaning-
Text Theory&amp;quot; (Mereuk, 1997), &amp;quot;Word Gram-
mar&amp;quot; (Hudson, 1984) use a very different per-
spective of syntactic dependency, to which we
will refer as word driven. Word-driven DG is
closer to the original idea of Tesniere. Basi-
cally, it describes dependencies as binary rela-
tions between words in the language without ad-
dressing head scopes. This leads to far-reaching
consequences for grammatical classification of
words and dependency typology (cf. (Jackend-
off, 1977; Keenan and Comrie, 1977) and a
word driven DG of English in (Mel&apos;euk and
Pertsov, 1987)). Formal definitions of word
driven DGs are few in number. Historically
the first was (Gladkij and Mel&apos;euk, 1971). It
proposed a concept of tree generating DG and
started a mathematical research into subfami-
lies of this very general class of grammars (see a
review in (Dikovsky and Modina, 2000)). More
recent propositions are Link Grammars (Sleator
and Temperly, 1993) expressing only projective
DTs and Polarized DGs (Dikovsky, 2001). The
existing word driven DG definitions are opera-
tional and make no link with logic. In particu-
lar, there is no definition based on word driven
dependency types. In this paper, we propose
such a definition in terms of classical CCs. The
Categorial Dependency Grammars (CDCs) we
propose have two particularities: a specific type
system inspired by (Mel&apos;euk and Pertsov, 1987)
and a simple dependency calculus for projective
and discontinuous dependencies.
</bodyText>
<sectionHeader confidence="0.745082" genericHeader="method">
2 Syntactic types
</sectionHeader>
<bodyText confidence="0.999680916666667">
A word driven dependency G(overnor)
S(ubordinate) encodes an irreflexive antisym-
metric antitransitive binary relation on words
with intuitive meaning &amp;quot;G licenses S&amp;quot; defined
by constraints on lexical and grammatical
features, precedence constraints, pronominal-
ization constraints, etc. concerning the words
G and S and sometimes also close context
words (see (Iordanskaja and Mel&apos;euk, 2000)
for a detailed presentation). Let us see a frag-
ment of the definition in (Mel&apos;euk and Pertsov,
1987) of the modificative dependency in English:
</bodyText>
<equation confidence="0.824375090909091">
77-Jodi f* &lt;#. Yi3 . . . WHERE
if Y Z Xa then
(if 17,3 fi&gt; Z then Z = &apos;ENOUGH&apos; or
(D = coordin and Z = ;INV)))
(detlquant)
otherwise if Xa Z then
exists Y,3 U Z: (U = (`SUCHII`W HAT&apos;)
and Z = (`All&apos;AN1)) and
if Xa = (Num) then
(forall) T: not T quant Xa
etc.)...)
</equation>
<bodyText confidence="0.464696">
and a = (N, not pron),(Num),`ONE21
and )3 = (A, not det, not pred!, not postpos!),
</bodyText>
<equation confidence="0.619823">
(V)ppr„, (V, not postpos!)pass
</equation>
<bodyText confidence="0.9994296">
This complex formula is encoded by the de-
pendency name modif. The corresponding de-
pendency is the set of pairs of wordforms satis-
fying the formula in their occurrences.
Suppose that we have to describe syntactic
type of the word theory in terms of such
dependencies. Then we are to look at this word
from two different points of view.
For theory as dependent, we must find all
possible incoming dependencies:
</bodyText>
<construct confidence="0.385651">
subj dir — obj prepos
theory theory theory
</construct>
<bodyText confidence="0.943157333333333">
For theory as governor, we must find all possible
dependencies outgoing from it:
In so doing, we must of course take into
account the precedence of beginnings (ends) of
the arrows with respect to the word. This we
can do using the classical CG type constructors.
The fundamental difference compared with CG
is that now a primitive type D corresponds to
the incoming dependency D and in complex
types, e.g. D\a assigned to a word w, D
corresponds to the beginning of dependency D.
Finally, we must find all mutually compatible
combinations of outgoing arrows and of an
incoming arrow compatible with the features
of the word theory. Each such combination
describes a dependency type:
dir — obj
det attr — rel
</bodyText>
<figure confidence="0.968280909090909">
modi f *
•
theory
[modi f *\det\d — obj /attr — rel]
el
det
attr — posess
theory
•
theory
appos —compos
</figure>
<bodyText confidence="0.986009953846153">
In this example, being assigned to theory, the
type [modi f*\det\dir — obj I attr — rel] admits
several (possibly no) left dependents of theory
through dependency modi f (modi f * denotes it-
eration), requires a left dependent through det,
a right dependent through dir — obj and the in-
coming dependency attr — rel. The position of
the end of attr—rel must be determined through
derivation. As we will see, such types are suf-
ficient do describe projective dependencies. In
order to specify the long distance discontinuous
dependencies, we use, as we do in (Dikovsky,
2001), polarized dependency types. A positive
polarized type specifies the name and the direc-
tion of an outgoing dependency. For instance,
in the sentence It was yesterday that we had the
meeting, the positive subcategory (\it—cleft)
of the category assigned to that marks the begin-
ning of the distant dependency named it—cleft
outgoing from the clause root (see Fig. 3).
A negative polarized type specifies the name
and the direction of an incoming dependency.
For instance, the end of the discontinuous de-
pendency it — cleft incoming to the expletive
pronoun in the syntactic role of subject of the
main sentence is defined by the negative cat-
egory (/ it — cle t) assigned to It. The cate-
gories (/it—cleft) and (\it—cleft) are dual.
Together, they describe the discontinuous de-
it— clef t
pendency that It.
Now, let us define the dependency types for-
mally.
For simplicity, in this paper we consider
the primitive dependency types as elemen-
tary categories without parameters. C de-
notes a finite set of such elementary categories.
C*=df {A* I A E Cl denotes the set of all iter-
ated categories.
Defining polarized categories, we distinguish
between four dependency polarities: left and
right positive \, and left and right negative
\, . For each polarity v E {\, \, jt, /}
there is the unique dual polarity 1): \
, =\, /=\, \=/.C , \ C, \ C and
C denote the corresponding sets of polarized
distant dependency categories. E.g., C =
C) I C E Cl is the set of right positive cat-
egories. V+ (C) = C U \C is the set of pos-
itive distant dependency categories, V(C) =
\C U C is the set of those negative.
If we limit ourself to these categories we can-
not express adjacency of distant subordinates to
a given word. E.g., in French, the negative de-
pendency category / clit — dobj of a cliticized
direct object must be anchored to the auxil-
iary verb or to the verb in a non-analytic form.
For that, we will use specially marked anchored
negative categories: Anc(C)=df {#(a) a E
V(C)} - our name for negative categories
whose position is determined relative to some
other category - whereas the negative categories
in V(C) will be called loose.
Definition 1 The set Cat(C) of categories is
the least set verifying the conditions:
</bodyText>
<equation confidence="0.8294572">
I. C U V(C) U Anc(C) c Cat(C).
2. For C E Cat(C), A1 E (C U C*
Anc(C) U \ C) and A2 E (C U C* U
Anc(C) U C), the categories [Al\C] and
[C /A2] also belong to Cat(C).
</equation>
<bodyText confidence="0.9494926">
By the nature of word driven dependencies, the
type constructors \, / are associative. So every
complex category can be presented in the form:
[Lk\ . Li\C/R1
E.g., [#(/ clit_dobj)\subj\S I aux] is one of
possible categories of an auxiliary verb, which
defines it as the host word for a cliticized direct
object, requires the local subject dependency on
its left, and on its right, the local outgoing de-
pendency aux.
</bodyText>
<sectionHeader confidence="0.991697" genericHeader="method">
3 Grammar definition
</sectionHeader>
<construct confidence="0.977375714285714">
Definition 2 A categorial dependency grammar
(CDG) is a system G = (W, C, S, 6), where W
is a finite set of words, C is a finite set of ele-
mentary categories containing the selected root
category S, and - called lexicon - is an assign-
ment of finite sets of categories in Cat(C) to
words in W (i.e. 6(a) c Cat(C) for a E W).
</construct>
<bodyText confidence="0.999856181818182">
The language and DT language generated by
a DCG are defined by a calculus of local and
polarized dependencies. In this calculus, most
specific are the rules for polarized dependen-
cies. These rules establish a distant dependency
between two words with dual polarized cate-
gories, if the corresponding negative category
is loose. The anchored negative dependency va-
lencies serve only to anchor a distant subordi-
nate to a host word. As soon as the correct
position of the subordinate is identified by the
anchored dependency rule, its category becomes
loose and so available to the governor. So the
anchored dependency marker serves as an expo-
nential in this resource sensitive calculus.
In the definition below, indexed F denote
strings of categories. An occurrence of an el-
ementary category C in a derived string of cat-
egories corresponds to a DT D of category C.
r(D) denotes the root of D. For space reasons,
we present only the rules for left constructors.
The rules for right constructors are similar.
</bodyText>
<figure confidence="0.65537475">
Definition 3 Provability relation I-
Local dependency rule:
L. FiC[C\a]F2 1 FlaF2.
If C is the category of Di_ and [C\a] is that of
D2, then a becomes the category of the new DT
C ,,
▪ U D2 U {r(D1) rl&amp;quot;211•
Iterated dependency rules:
I. F,C[C&apos;\a]F I Fi[C*\a]F2.
If C is the category of D1 and [C*\a] is that
of D2, then [C*\a] in the consequence becomes
the category of the new DT
Anchored dependency rule:
A. Fi#(a)[#(a)VF2 H Flai@r2, #(a)
Anc(C).
Distant dependency rule:
</figure>
<construct confidence="0.8481765">
D. Fi(/C)F2[(\ C)\a]F3 FiF2aF3.
The rule applies if there are no occurrences of
subcategories /C, #(/C) and \C in F2.
If C is the category of D1 and [(\C)\a] is
that of D2, then a becomes the category of the
new DT
D4 U D2 U {r(D1) r(D2)}.
H* is the reflexive-transitive closure of H .
</construct>
<bodyText confidence="0.810652333333333">
Comments: 1. Li, If are standard elimi-
nation rules present in all kinds of categorial
grammars. The rigid distribution constraints
encoded by elementary word driven depen-
dencies make impossible type raising and
symmetrical introduction rules.
</bodyText>
<listItem confidence="0.993906777777778">
2. The iteration rules II, I&apos;, IT, Or induce
different realizations of the same iterated
modifier type. In fact, the iteration marker
also serves as an exponential in this calculus.
3. DI,Dr are rules establishing distant
valencies between the &amp;quot;closest&amp;quot; dual loose de-
pendency valencies. This is another important
particularity of this calculus implied by the
nature of word driven dependencies and the
</listItem>
<footnote confidence="0.850976">
1-The DTs rest unchanged when no instruction.
</footnote>
<note confidence="0.475493">
underlying concept of polarity.
</note>
<construct confidence="0.74523375">
Definition 4 A DT D is assigned by a CDG
G = (147,C,S,6) to a sentence w (denoted
G(D,w)) if D is defined as DT of category S
in a proof F 1-* S for some F E 6(w).
The DT-language generated by G is the set of
DTs A(G) = {D I 3w EW+ G(D,w)}.
The language generated by G is the set of sen-
tences L(G) = {w E W+ I 3D G(D, w)}.
</construct>
<bodyText confidence="0.92274008">
This definition is correct in the following sense:
Proposition 1 For each CDG G, G(D,w) im-
plies that D is a DT on w.
Proposition 2 L(G0)= {di and2Pd3en &gt; 0}.
In Figures 1-3, two meeting continuous slanting
lines correspond to one application of rules L or
I, two meeting dashed slanting lines correspond
to one application of rule A, and right-angled
dashed lines connect categories to which rule
D is applied. Anchored categories must first
be made loose by applying the rule A. Only
then a distant dependency can be introduced by
application of the rule D. E.g., in Fig. 2, first
A is applied to C3 and C4, and then D applies
to C3 and C7.
Using CDGs - and this is their important ad-
vantage - one can describe discontinuous syntac-
tic dependencies caused by topicalization, ex-
traction, and other movement operations in a
uniform and concise manner. E.g., let us see
how can be described PP-movement (Fig. 2)
and it-clefting (Fig. 3).
In the proof in Fig. 2, we use the categories
= [det\subj I attr — rel] E 6(per son), C2 =
[#(/ prepos) \attr — rel I wh — rel] E (whom),
</bodyText>
<equation confidence="0.974375333333333">
C3 = [subj\wh — rellinf — obj] E 6(mast),
C4 = [(\preposAinf —obj] E (refer),6 C5 =
[subj\SIn—copul] E 6(is).
</equation>
<figure confidence="0.759813166666667">
▪ U D2 U {r(D1) r(D2)}.
IL FAC*\a]F2 H FlaF2
4 Expressivity
DCGs are more expressive than CFGs. Here is
an example of a CDG generating a non-context-
free language.
Example 1 Let Go = (Wo, Co, S, so), with (5o
a [/3\a], [a\a], b [al\D/A],
d2 [a\AAS D], d3 D, c [D\A],
E where a = #(/B), al = (\B), = #(/
and )31 = (\C). For instance, Fig. I presents
a derivation of dia3d2b3d3c3.
</figure>
<figureCaption confidence="0.720283">
Fig. 1. A proof of Go(D,dia3d2b3d3c3).
</figureCaption>
<figure confidence="0.817830444444444">
a a a d2
d3 c c
,3 [,3\a][a\a] [a\a] [cy\,31\S D][cyi\D I A] [cyi\D IA]
[cti\D I A] D [D\A] [D\A][D\A]
In this proof, the rule L applied to det and C1
det
gives the dependency the &lt;— person of cat-
egory [subj I attr — rel]. L applied to subj and
subj
</figure>
<figureCaption confidence="0.157014">
C3 gives the dependency you must of cat-
</figureCaption>
<bodyText confidence="0.902637515151515">
egory [wh — rel I in f — obj]. A applied to #(/
prepos) and C2 gives (/ prepos)[attr—rellwh—
rel]. Now we can eliminate ( prepos) applying
D to it and C4. This reduces C4 to in f —obj and
gives the distant dependency to wh—relrefer.
Finally, L is applied four times.
The proof in Fig. 3 uses categories: C =
[#(/ it— cleft)\SIsubj — cleft/circ] E 6(was),
= [(\ it—cleft) subj — cleftlth—rel] E
5(that), C = [subj\th — rel I obj] E 6(had),
= [det\obj] E 6(meeting).
The weak expressive power of CDGs is
not completely explored. Meanwhile, some
observations can already be made.
Let G(m), m &gt; 1, be the CGD defined by:
do [S/Do] and dm,+1 Dm,
ao [Do/Do/(/ Am)/ • • • /(/
for 0 &lt; i &lt; m, [Di_1/#(\ Ai)],
and ai [#(\fAi)/#(\fAi)],[#(\fAi)/Di].
Then the following proposition holds:
Proposition 3 For each m &gt; 0, L(G(,)) =
{doaljdia7... dThanmdTh+iln &gt; 0}.
This means that not all languages in G(CDG)
are generated by TAGs. On the other
hand, we suppose that the copy language
{ww I w E {a, b}+} is not generated by CDGs.
The CDGs are sensitive to the following
structural dependency measure.
Definition 5 Let D be a DT of a sentence
w = al ... an. For a space i between the words
ai and a+1, 1 &lt; i &lt; n, we define the distant
dependencies thickness in i (denoted dth(D,i))
as the number of distant dependencies (ak
</bodyText>
<construct confidence="0.7278264">
a1), (ak —&gt; al) in D covering i (i.e.
such that k &lt; i &lt; 1 for some k,1 and d).
dth(D)=4 fmax{dth(D,i)Il &lt; i &lt; IDI} and
dth(G)=df max{0,minfdth(D)IG(D,w)} I w E
L(G)}.
</construct>
<bodyText confidence="0.81565175">
E.g., dth(Go) = oo (same for all G(m), m &gt; 1).
For natural languages, this measure is seemingly
bounded by a small constant (2 or 3). In both
subj
</bodyText>
<figureCaption confidence="0.998346">
Fig. 2. Example of PP-movement.
</figureCaption>
<figure confidence="0.9958047">
[#(/pre-TO-obj) \ attr-rel /
inf-obj
refer
C4
inif-obj]
det
the person
det
[subj/attr-rel]
n-copul
is Smith
C5 n-copul
[subj \ S]
C2
subj
#(/pre-TO-obj)
rep-wh
to whom
prep-wh
[attr-rel
</figure>
<bodyText confidence="0.889996125">
examples above, dth(D) = 1. Meanwhile, the
following theorem is a consequence of Theorem
2 in (Dikovsky, 2001).
Theorem 1 If for a CDG G, the measure
dth(G) is bounded by a constant, then L(G) is
context-free.
This result confirms once more that relevant is
the strong and not the weak expressive power.
</bodyText>
<sectionHeader confidence="0.975448" genericHeader="method">
5 Complexity
</sectionHeader>
<bodyText confidence="0.99460528">
It turns out that correctness of sentences with
respect to a CDG can be expressed in terms of
two independent tests: the first in terms of only
local (projective) dependencies, and the second
in terms of neutralizability of distant dual po-
larized dependencies. This fact ensures efficient
parsing algorithms.
In this short paper we don&apos;t describe the no-
tions and technical details underlying efficient
analysis of CDGs (see (Dekhtyar and Dikovsky,
2004)) and only announce several facts.
Theorem 2 (i) There is an algorithm pars
which parses CDGs in time 0(n5).
(ii) If a CDG G has bounded distant dependen-
cies thickness dth(G) &lt; const, then the algo-
rithm pars parses G in time 0(n3).
The analysis algorithm is Earley-type with
items including current state of counters con-
trolling well-pairing of loose polarized depen-
dencies. When no polarized dependencies
are used in the grammar or if their thick-
ness is bounded, the items are counter-less (or
bounded-counter).
Corollary 1 There is an algorithm parsP
which parses projective CDGs in time 0(n3).
</bodyText>
<sectionHeader confidence="0.945329" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.944816">
The Categorial Dependency Grammars intro-
duced in this paper can be used in practice as
</bodyText>
<equation confidence="0.66623475">
it—cleft
sub j
[th — r el I obj]
th— r el
</equation>
<figureCaption confidence="0.990454">
Fig. 3. Example of it-clefting.
</figureCaption>
<figure confidence="0.993845055555556">
it was
th — r el
yesterday
we
that
had
subj — cleft
# (/ it— cle f t) Cf circ
N
N/
[S bsubj — cleft/circ
[S subj — cle f t]
_I
obj
det
the meeting
det
obj
</figure>
<bodyText confidence="0.999831470588235">
low level DGs into which should be compiled
for parsing high level word driven DGs. They
can be easily extended to grammars using
bounded depth feature structures as primitive
categories and feature unification and propa-
gation through dependencies. Using anchored
categories, it is possible to express a variety of
linear order constraints. At the same time, the
CDGs have efficient parsing algorithms, com-
parable with or more fast than those for other
dependency grammars expressing unlimited dis-
tant dependencies (cf. (Lombardo and Lesmo,
1998; Neuhaus and Broker, 1997; Kahane et al.,
1998)). A very important advantage of CDGs is
their type-driven style definition which fits well
the standard methods of constructing formal se-
mantics.
</bodyText>
<sectionHeader confidence="0.998358" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999822">
The author is grateful to Denis Bechet, Annie
Foret and Michael Dekhtyar for helpful com-
ments and fruitful discussions of this work.
</bodyText>
<sectionHeader confidence="0.995972" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9808776796875">
Yehoshua Bar-Hillel. 1953. A quasi-
arithmetical notation for syntactic de-
scription. 29(1):47-58.
Norbert Broker. 1998. Separating surface order
and syntactic relations in a dependency gram-
mar. In Proc. COLING-ACL, pages 174-180,
Montreal.
Michael Dekhtyar and Alexander Dikovsky.
2004. Categorial dependency grammars. In
Proc. of Intern. Conf. Cat egorial Grammars
(CG2004), pages 76-91, Montpellier, France.
Alexander Dikovsky and Larissa Modina. 2000.
Dependencies on the other side of the curtain.
Traitement Automatigue des Langues (TAL),
41(1):79-111.
Alexander Dikovsky. 2001. Grammars for local
and long dependencies. In Proc. of the 39th
Intern. Conf. ACL&apos;2001, pages 156-163. ACL
Sz Morgan Kaufman.
Denis Duchier and Ralf Debusmann. 2001.
Topological dependency trees: A constraint-
based account of linear precedence. In Proc.
of the 39th Intern. Conf. ACL&apos;2001, pages
180-187. ACL Sz Morgan Kaufman.
Haim Gaifman. 1961. Dependency systems
and phrase structure systems. Report p-2315,
RAND Corp. Santa Monica (CA). Published
in: Information and Control, 1965, v. 8, n 3,
pp. 304-337.
Alexej V. Gladkij and Igor A. Mel&apos;euk.
1971. Grammatiki derev&apos;ev. I. Opyt formal-
izacii preobrazovanij sintaksieeskix struktur
estestvennogo jazyka [Tree grammars. I. A
formalism for syntactic transformations in
natural languages]. In Informacionnye vo-
prosy semiotiki, lingvistiki i avtomatioeskogo
perevoda [Informational Problems of Semi-
otics, Linguistics and Automatic Transla-
tion], volume 1, pages 16-41. Published In
&amp;quot;Linguistics. An International Review&amp;quot;, n.
150. April 15, 1975, The Hague : Mouton,
pp. 47-82.
Alexej V. Gladkij. 1966. Lekcii po
Matematieeskoj Lingvistike dlja Studentov
NGU [Course of Mathematical Linguis-
tics. Novosibirsk State University (Rms.)].
(French transl. Lecons de linguistique
mathematique. facs. I, 1970, Dunod).
Novosibirsk State University.
David Hays. 1960. Grouping and dependency
theories. Research memorandum RM-2646,
The RAND Corporation. Published in: Proc.
of the National Symp. on Machine Translaion,
Englewood Cliffs (N.Y.), 1961, pp. 258-266.
Richard Hudson. 1984. Word Grammar. Basil
Blackwell, Oxford-New York.
Lydija Iordanskaja and Igor Mel&apos;euk. 2000.
The notion of surface-syntactic relation revis-
ited. valence-controlled surface-syntactic rela-
tions in french. In L. Iomdin and L. Krysin,
editors, S/ovo v tekste i v slovare [Word in
Text and in Dictionary], pages 391-433. Lan-
guages of Russian Culture, Moscow.
Ray Jackendoff. 1977. X-bar Syntax. A Study
of Phrase Structure. MIT Press, Cambridge.
A.K. Joshi, L.S. Levy, and M. Takahashi. 1975.
Tree adjunct grammars. Journ. of Comput.
and Syst. Sci., 10(4136-163.
Sylvain Kahane, Alexis Nasr, and Owen Ram-
bow. 1998. Pseudo-projectivity : A poly-
nomially parsable non-projective dependency
grammar. In Proc. COLING-ACL, pages
646-652, Montreal.
Ronald Kaplan and Joan Bresnan. 1982. Lex-
ical functional grammar : A formal system
for grammatical representation. In Bresnan
J.W., editor, The Mental Representation of
Grammatical Relations, pages 173-281. MIT
Press, Cambridge, Massachusetts.
Edward Keenan and Bernard Comrie. 1977.
Noun phrase accessibility and universal gram-
mar. Linguistic Inquiry, 8(463-99.
Geert-Jan M. Kruijff. 2001. A Cat egorial-
ModalLogical Architecture of Informativity.
Dependency Grammar Logic &amp; Information
Structure. Ph.D. thesis, Charles University,
Prague.
Joachim Lambek. 1958. The mathematics of
sentence structure. American Mathematical
Monthly, pages 154-170.
Vincenzo Lombardo and Leonardo Lesmo.
1998. Formal aspects and parsing issues of
dependency theory. In Proc. COLING-ACL,
pages 787-793, Montreal.
Hiroshi Maruyama. 1990. Structural disam-
biguation with constraint propagation. In
Proc. of 28th ACL Annual Meeting, pages 31-
38.
I. Mel&apos;euk and N.V. Pertsov. 1987. Sur-
face Syntax of English. A Formal Model
Within the Meaning-Text Framework. John
Benjamins Publishing Company, Amster-
dam/Philadelphia.
Igor Mel&apos;euk. 1997. Vers une linguistique
&lt;Sens-Texte&gt;&gt; . Lecon inagurale au College
de France.
Michael Moortgat and Glin V. Morrill. 1991.
Heads and phrases. Type calculus for depen-
dency and constituent structure. Ms OTS,
Utrecht.
Glin V. Morrill. 1994. Type Logical Grammar.
Categorial Logic of Signs. Kluwer Academic
Publishers, Dordrecht.
Peter Neuhaus and Norbert Broker. 1997. The
Complexity of Recognition of Linguistically
Adequate Dependency Grammars. In Proc.
of 35th ACL Annual Meeting and 8th Conf.
of the ECACL, pages 337-343.
Carl Pollard and Ivan Sag. 1994. Head-driven
Phrase Structure Grammar. CSLI, Stanford,
California.
Jane J. Robinson. 1970. Dependency structures
and transformational rules. 46(2):259-285.
Daniel Sleator and Davy Temperly. 1993. Pars-
ing English with a Link Grammar. In Proc.
IWPT&apos;93, pages 277-291.
Lucien Tesniere. 1959. Elements de syntaxe
structurale. Librairie C. Klincksiek, Paris.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.460194">
<title confidence="0.9985">Dependencies as Categories</title>
<author confidence="0.994065">Alexander Dikovsky</author>
<affiliation confidence="0.842375">LINA, Universite de</affiliation>
<note confidence="0.701087666666667">2 rue de la BP 92208 F 44322 Nantes Cedex Alexandre.DikovskyAlina.univ-nantesir</note>
<abstract confidence="0.997555307692308">Categorial Dependency Grammars (CDG) introduced in this paper make clear-cut distinction between local and distant word driven dependencies. The former are treated in classical categorial grammar terms. The latter are defined in terms of polarized valencies, whose neutralization is controlled by the simple prin- &amp;quot;choose the first available&amp;quot; ( very expressive, CDGs are analyzed in polynomial time. Besides this, CDGs represent a convenient frame for relating dependency grammar with linguistic semantics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yehoshua Bar-Hillel</author>
</authors>
<title>A quasiarithmetical notation for syntactic description.</title>
<date>1953</date>
<pages>29--1</pages>
<contexts>
<context position="2503" citStr="Bar-Hillel, 1953" startWordPosition="395" endWordPosition="396">, 1966; Robinson, 1970)) that this link between the two structures is reversible: a selection of one immediate head per constituent induces a unique DT by the following induction: C C C&apos; root(ImmHead(C1)) —&gt;* root(ImmHead(C)). This structural &amp;quot;equivalence&amp;quot; produced an illusion that DTs are byproduct of head selection in constituent structures. So all syntagmatic grammars with head selection (e.g., LFG (Kaplan and Bresnan, 1982) and HPSG (Pollard and Sag, 1994)) may in a way be considered as DGs. Formally, such &amp;quot;extension&amp;quot; to DG concerns TAGs (Joshi et al., 1975) and categorial grammars (CGs) (Bar-Hillel, 1953; Lambek, 1958)) as well. However, this analogy is very superficial. It was soon realized that dependency and precedence are rather independent. For instance, the head driven DT above are always projective: the projections of all words fill continuous segments. Meanwhile, discontinuous non-projective dependencies are inevitable in languages. They often mark communicative structure (e.g. topicalization) and special constructions encoding complex semantic relations (e.g. clefting, subject or object extraction in pied-piping, etc.). This distinction led to many propositions, both in terms of orde</context>
</contexts>
<marker>Bar-Hillel, 1953</marker>
<rawString>Yehoshua Bar-Hillel. 1953. A quasiarithmetical notation for syntactic description. 29(1):47-58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norbert Broker</author>
</authors>
<title>Separating surface order and syntactic relations in a dependency grammar.</title>
<date>1998</date>
<booktitle>In Proc. COLING-ACL,</booktitle>
<pages>174--180</pages>
<location>Montreal.</location>
<contexts>
<context position="3153" citStr="Broker, 1998" startWordPosition="489" endWordPosition="490">s analogy is very superficial. It was soon realized that dependency and precedence are rather independent. For instance, the head driven DT above are always projective: the projections of all words fill continuous segments. Meanwhile, discontinuous non-projective dependencies are inevitable in languages. They often mark communicative structure (e.g. topicalization) and special constructions encoding complex semantic relations (e.g. clefting, subject or object extraction in pied-piping, etc.). This distinction led to many propositions, both in terms of order constraints (e.g., (Maruyama, 1990; Broker, 1998; Duchier and Debusmann, 2001)) and in structure sharing terms like lifting (e.g., (Lombardo and Lesmo, 1998; Kahane et al., 1998)). In logical type based grammars, like CGs, adequate extension to DG presupposes one more distinction: that of semantic functionality and of syntactic subordinacy, which are opposite, for instance, for verb and noun modifiers. Multimodal extensions of CGs take proper account of these distinctions (cf. (Moortgat and Morrill, 1991; Morrill, 1994; Kruijff, 2001)). The overwhelming majority of DGs are head driven in the above sense. At the same time, some linguistic th</context>
</contexts>
<marker>Broker, 1998</marker>
<rawString>Norbert Broker. 1998. Separating surface order and syntactic relations in a dependency grammar. In Proc. COLING-ACL, pages 174-180, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Dekhtyar</author>
<author>Alexander Dikovsky</author>
</authors>
<title>Categorial dependency grammars.</title>
<date>2004</date>
<booktitle>In Proc. of Intern. Conf. Cat egorial Grammars (CG2004),</booktitle>
<pages>76--91</pages>
<location>Montpellier, France.</location>
<contexts>
<context position="18428" citStr="Dekhtyar and Dikovsky, 2004" startWordPosition="3198" endWordPosition="3201">measure dth(G) is bounded by a constant, then L(G) is context-free. This result confirms once more that relevant is the strong and not the weak expressive power. 5 Complexity It turns out that correctness of sentences with respect to a CDG can be expressed in terms of two independent tests: the first in terms of only local (projective) dependencies, and the second in terms of neutralizability of distant dual polarized dependencies. This fact ensures efficient parsing algorithms. In this short paper we don&apos;t describe the notions and technical details underlying efficient analysis of CDGs (see (Dekhtyar and Dikovsky, 2004)) and only announce several facts. Theorem 2 (i) There is an algorithm pars which parses CDGs in time 0(n5). (ii) If a CDG G has bounded distant dependencies thickness dth(G) &lt; const, then the algorithm pars parses G in time 0(n3). The analysis algorithm is Earley-type with items including current state of counters controlling well-pairing of loose polarized dependencies. When no polarized dependencies are used in the grammar or if their thickness is bounded, the items are counter-less (or bounded-counter). Corollary 1 There is an algorithm parsP which parses projective CDGs in time 0(n3). 6 C</context>
</contexts>
<marker>Dekhtyar, Dikovsky, 2004</marker>
<rawString>Michael Dekhtyar and Alexander Dikovsky. 2004. Categorial dependency grammars. In Proc. of Intern. Conf. Cat egorial Grammars (CG2004), pages 76-91, Montpellier, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Dikovsky</author>
<author>Larissa Modina</author>
</authors>
<date>2000</date>
<booktitle>Dependencies on the other side of the curtain. Traitement Automatigue des Langues (TAL),</booktitle>
<pages>41--1</pages>
<contexts>
<context position="4619" citStr="Dikovsky and Modina, 2000" startWordPosition="723" endWordPosition="726">cally, it describes dependencies as binary relations between words in the language without addressing head scopes. This leads to far-reaching consequences for grammatical classification of words and dependency typology (cf. (Jackendoff, 1977; Keenan and Comrie, 1977) and a word driven DG of English in (Mel&apos;euk and Pertsov, 1987)). Formal definitions of word driven DGs are few in number. Historically the first was (Gladkij and Mel&apos;euk, 1971). It proposed a concept of tree generating DG and started a mathematical research into subfamilies of this very general class of grammars (see a review in (Dikovsky and Modina, 2000)). More recent propositions are Link Grammars (Sleator and Temperly, 1993) expressing only projective DTs and Polarized DGs (Dikovsky, 2001). The existing word driven DG definitions are operational and make no link with logic. In particular, there is no definition based on word driven dependency types. In this paper, we propose such a definition in terms of classical CCs. The Categorial Dependency Grammars (CDCs) we propose have two particularities: a specific type system inspired by (Mel&apos;euk and Pertsov, 1987) and a simple dependency calculus for projective and discontinuous dependencies. 2 S</context>
</contexts>
<marker>Dikovsky, Modina, 2000</marker>
<rawString>Alexander Dikovsky and Larissa Modina. 2000. Dependencies on the other side of the curtain. Traitement Automatigue des Langues (TAL), 41(1):79-111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Dikovsky</author>
</authors>
<title>Grammars for local and long dependencies.</title>
<date>2001</date>
<booktitle>In Proc. of the 39th Intern. Conf. ACL&apos;2001,</booktitle>
<pages>156--163</pages>
<publisher>ACL Sz Morgan Kaufman.</publisher>
<contexts>
<context position="4759" citStr="Dikovsky, 2001" startWordPosition="744" endWordPosition="745">ces for grammatical classification of words and dependency typology (cf. (Jackendoff, 1977; Keenan and Comrie, 1977) and a word driven DG of English in (Mel&apos;euk and Pertsov, 1987)). Formal definitions of word driven DGs are few in number. Historically the first was (Gladkij and Mel&apos;euk, 1971). It proposed a concept of tree generating DG and started a mathematical research into subfamilies of this very general class of grammars (see a review in (Dikovsky and Modina, 2000)). More recent propositions are Link Grammars (Sleator and Temperly, 1993) expressing only projective DTs and Polarized DGs (Dikovsky, 2001). The existing word driven DG definitions are operational and make no link with logic. In particular, there is no definition based on word driven dependency types. In this paper, we propose such a definition in terms of classical CCs. The Categorial Dependency Grammars (CDCs) we propose have two particularities: a specific type system inspired by (Mel&apos;euk and Pertsov, 1987) and a simple dependency calculus for projective and discontinuous dependencies. 2 Syntactic types A word driven dependency G(overnor) S(ubordinate) encodes an irreflexive antisymmetric antitransitive binary relation on word</context>
<context position="7981" citStr="Dikovsky, 2001" startWordPosition="1297" endWordPosition="1298">attr — posess theory • theory appos —compos In this example, being assigned to theory, the type [modi f*\det\dir — obj I attr — rel] admits several (possibly no) left dependents of theory through dependency modi f (modi f * denotes iteration), requires a left dependent through det, a right dependent through dir — obj and the incoming dependency attr — rel. The position of the end of attr—rel must be determined through derivation. As we will see, such types are sufficient do describe projective dependencies. In order to specify the long distance discontinuous dependencies, we use, as we do in (Dikovsky, 2001), polarized dependency types. A positive polarized type specifies the name and the direction of an outgoing dependency. For instance, in the sentence It was yesterday that we had the meeting, the positive subcategory (\it—cleft) of the category assigned to that marks the beginning of the distant dependency named it—cleft outgoing from the clause root (see Fig. 3). A negative polarized type specifies the name and the direction of an incoming dependency. For instance, the end of the discontinuous dependency it — cleft incoming to the expletive pronoun in the syntactic role of subject of the main</context>
<context position="17768" citStr="Dikovsky, 2001" startWordPosition="3091" endWordPosition="3092">ng i (i.e. such that k &lt; i &lt; 1 for some k,1 and d). dth(D)=4 fmax{dth(D,i)Il &lt; i &lt; IDI} and dth(G)=df max{0,minfdth(D)IG(D,w)} I w E L(G)}. E.g., dth(Go) = oo (same for all G(m), m &gt; 1). For natural languages, this measure is seemingly bounded by a small constant (2 or 3). In both subj Fig. 2. Example of PP-movement. [#(/pre-TO-obj) \ attr-rel / inf-obj refer C4 inif-obj] det the person det [subj/attr-rel] n-copul is Smith C5 n-copul [subj \ S] C2 subj #(/pre-TO-obj) rep-wh to whom prep-wh [attr-rel examples above, dth(D) = 1. Meanwhile, the following theorem is a consequence of Theorem 2 in (Dikovsky, 2001). Theorem 1 If for a CDG G, the measure dth(G) is bounded by a constant, then L(G) is context-free. This result confirms once more that relevant is the strong and not the weak expressive power. 5 Complexity It turns out that correctness of sentences with respect to a CDG can be expressed in terms of two independent tests: the first in terms of only local (projective) dependencies, and the second in terms of neutralizability of distant dual polarized dependencies. This fact ensures efficient parsing algorithms. In this short paper we don&apos;t describe the notions and technical details underlying e</context>
</contexts>
<marker>Dikovsky, 2001</marker>
<rawString>Alexander Dikovsky. 2001. Grammars for local and long dependencies. In Proc. of the 39th Intern. Conf. ACL&apos;2001, pages 156-163. ACL Sz Morgan Kaufman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denis Duchier</author>
<author>Ralf Debusmann</author>
</authors>
<title>Topological dependency trees: A constraintbased account of linear precedence.</title>
<date>2001</date>
<booktitle>In Proc. of the 39th Intern. Conf. ACL&apos;2001,</booktitle>
<pages>180--187</pages>
<publisher>ACL Sz Morgan Kaufman.</publisher>
<contexts>
<context position="3183" citStr="Duchier and Debusmann, 2001" startWordPosition="491" endWordPosition="494">ery superficial. It was soon realized that dependency and precedence are rather independent. For instance, the head driven DT above are always projective: the projections of all words fill continuous segments. Meanwhile, discontinuous non-projective dependencies are inevitable in languages. They often mark communicative structure (e.g. topicalization) and special constructions encoding complex semantic relations (e.g. clefting, subject or object extraction in pied-piping, etc.). This distinction led to many propositions, both in terms of order constraints (e.g., (Maruyama, 1990; Broker, 1998; Duchier and Debusmann, 2001)) and in structure sharing terms like lifting (e.g., (Lombardo and Lesmo, 1998; Kahane et al., 1998)). In logical type based grammars, like CGs, adequate extension to DG presupposes one more distinction: that of semantic functionality and of syntactic subordinacy, which are opposite, for instance, for verb and noun modifiers. Multimodal extensions of CGs take proper account of these distinctions (cf. (Moortgat and Morrill, 1991; Morrill, 1994; Kruijff, 2001)). The overwhelming majority of DGs are head driven in the above sense. At the same time, some linguistic theories, e.g. &amp;quot;MeaningText Theo</context>
</contexts>
<marker>Duchier, Debusmann, 2001</marker>
<rawString>Denis Duchier and Ralf Debusmann. 2001. Topological dependency trees: A constraintbased account of linear precedence. In Proc. of the 39th Intern. Conf. ACL&apos;2001, pages 180-187. ACL Sz Morgan Kaufman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haim Gaifman</author>
</authors>
<title>Dependency systems and phrase structure systems. Report p-2315, RAND Corp. Santa Monica (CA). Published in: Information and Control,</title>
<date>1961</date>
<volume>8</volume>
<pages>304--337</pages>
<contexts>
<context position="1095" citStr="Gaifman, 1961" startWordPosition="172" endWordPosition="173">the simple principle &amp;quot;choose the first available&amp;quot; ( FA). Being very expressive, CDGs are analyzed in polynomial time. Besides this, CDGs represent a convenient frame for relating dependency grammar with linguistic semantics. 1 Introduction Dependency grammars (DGs) are formal grammars assigning dependency trees (DTs) to wellformed sentences. A DT of a sentence is a labelled arrows tree whose nodes are the words of the sentence. A rather formal description of DGs and DG syntax was given by L. Tesniere (Tesniere, 1959). The first exact definitions are due to D. Hays (Hays, 1960) and H. Gaifman (Gaifman, 1961). The basic syntactic principle behind the DGs is quite different from that of syntagmatic grammars. They are designed for and more adapted to definitions of binary relations between wordforms (syntactic dependencies), than to definitions of sentence constituents. Meanwhile, historically the first DGs define in fact both. The Hays-Gaifman&apos;s DGs are lexicalized. They assign grammatical categories to words and position the subordinates with respect to their governors. In this manner, they define not only the binary relations &amp;quot;governor —&gt; subordinate&amp;quot;, whose union forms a tree, but also (due to t</context>
</contexts>
<marker>Gaifman, 1961</marker>
<rawString>Haim Gaifman. 1961. Dependency systems and phrase structure systems. Report p-2315, RAND Corp. Santa Monica (CA). Published in: Information and Control, 1965, v. 8, n 3, pp. 304-337.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Alexej V Gladkij</author>
<author>Igor A Mel&apos;euk</author>
</authors>
<title>Grammatiki derev&apos;ev. I. Opyt formalizacii preobrazovanij sintaksieeskix struktur estestvennogo jazyka [Tree grammars. I. A formalism for syntactic transformations in natural languages].</title>
<date>1971</date>
<booktitle>In Informacionnye voprosy semiotiki, lingvistiki i avtomatioeskogo perevoda [Informational Problems of Semiotics, Linguistics and Automatic Translation],</booktitle>
<volume>1</volume>
<pages>16--41</pages>
<contexts>
<context position="4437" citStr="Gladkij and Mel&apos;euk, 1971" startWordPosition="692" endWordPosition="695">ammar&amp;quot; (Hudson, 1984) use a very different perspective of syntactic dependency, to which we will refer as word driven. Word-driven DG is closer to the original idea of Tesniere. Basically, it describes dependencies as binary relations between words in the language without addressing head scopes. This leads to far-reaching consequences for grammatical classification of words and dependency typology (cf. (Jackendoff, 1977; Keenan and Comrie, 1977) and a word driven DG of English in (Mel&apos;euk and Pertsov, 1987)). Formal definitions of word driven DGs are few in number. Historically the first was (Gladkij and Mel&apos;euk, 1971). It proposed a concept of tree generating DG and started a mathematical research into subfamilies of this very general class of grammars (see a review in (Dikovsky and Modina, 2000)). More recent propositions are Link Grammars (Sleator and Temperly, 1993) expressing only projective DTs and Polarized DGs (Dikovsky, 2001). The existing word driven DG definitions are operational and make no link with logic. In particular, there is no definition based on word driven dependency types. In this paper, we propose such a definition in terms of classical CCs. The Categorial Dependency Grammars (CDCs) w</context>
</contexts>
<marker>Gladkij, Mel&apos;euk, 1971</marker>
<rawString>Alexej V. Gladkij and Igor A. Mel&apos;euk. 1971. Grammatiki derev&apos;ev. I. Opyt formalizacii preobrazovanij sintaksieeskix struktur estestvennogo jazyka [Tree grammars. I. A formalism for syntactic transformations in natural languages]. In Informacionnye voprosy semiotiki, lingvistiki i avtomatioeskogo perevoda [Informational Problems of Semiotics, Linguistics and Automatic Translation], volume 1, pages 16-41. Published In &amp;quot;Linguistics. An International Review&amp;quot;, n. 150. April 15, 1975, The Hague : Mouton, pp. 47-82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexej V Gladkij</author>
</authors>
<title>Lekcii po Matematieeskoj Lingvistike dlja Studentov NGU [Course of Mathematical Linguistics. Novosibirsk State University (Rms.)]. (French transl. Lecons de linguistique mathematique. facs. I,</title>
<date>1966</date>
<institution>Dunod). Novosibirsk State University.</institution>
<contexts>
<context position="1893" citStr="Gladkij, 1966" startWordPosition="297" endWordPosition="298">rdforms (syntactic dependencies), than to definitions of sentence constituents. Meanwhile, historically the first DGs define in fact both. The Hays-Gaifman&apos;s DGs are lexicalized. They assign grammatical categories to words and position the subordinates with respect to their governors. In this manner, they define not only the binary relations &amp;quot;governor —&gt; subordinate&amp;quot;, whose union forms a tree, but also (due to the order given) the projections of words on the sentence, which form a system of constituents with the projected words serving as the constituents&apos; heads. From the 70ies, it is known ((Gladkij, 1966; Robinson, 1970)) that this link between the two structures is reversible: a selection of one immediate head per constituent induces a unique DT by the following induction: C C C&apos; root(ImmHead(C1)) —&gt;* root(ImmHead(C)). This structural &amp;quot;equivalence&amp;quot; produced an illusion that DTs are byproduct of head selection in constituent structures. So all syntagmatic grammars with head selection (e.g., LFG (Kaplan and Bresnan, 1982) and HPSG (Pollard and Sag, 1994)) may in a way be considered as DGs. Formally, such &amp;quot;extension&amp;quot; to DG concerns TAGs (Joshi et al., 1975) and categorial grammars (CGs) (Bar-Hi</context>
</contexts>
<marker>Gladkij, 1966</marker>
<rawString>Alexej V. Gladkij. 1966. Lekcii po Matematieeskoj Lingvistike dlja Studentov NGU [Course of Mathematical Linguistics. Novosibirsk State University (Rms.)]. (French transl. Lecons de linguistique mathematique. facs. I, 1970, Dunod). Novosibirsk State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Hays</author>
</authors>
<title>Grouping and dependency theories. Research memorandum RM-2646, The RAND Corporation. Published in:</title>
<date>1960</date>
<booktitle>Proc. of the National Symp. on Machine Translaion,</booktitle>
<pages>258--266</pages>
<location>Englewood Cliffs (N.Y.),</location>
<contexts>
<context position="1064" citStr="Hays, 1960" startWordPosition="167" endWordPosition="168">ralization is controlled by the simple principle &amp;quot;choose the first available&amp;quot; ( FA). Being very expressive, CDGs are analyzed in polynomial time. Besides this, CDGs represent a convenient frame for relating dependency grammar with linguistic semantics. 1 Introduction Dependency grammars (DGs) are formal grammars assigning dependency trees (DTs) to wellformed sentences. A DT of a sentence is a labelled arrows tree whose nodes are the words of the sentence. A rather formal description of DGs and DG syntax was given by L. Tesniere (Tesniere, 1959). The first exact definitions are due to D. Hays (Hays, 1960) and H. Gaifman (Gaifman, 1961). The basic syntactic principle behind the DGs is quite different from that of syntagmatic grammars. They are designed for and more adapted to definitions of binary relations between wordforms (syntactic dependencies), than to definitions of sentence constituents. Meanwhile, historically the first DGs define in fact both. The Hays-Gaifman&apos;s DGs are lexicalized. They assign grammatical categories to words and position the subordinates with respect to their governors. In this manner, they define not only the binary relations &amp;quot;governor —&gt; subordinate&amp;quot;, whose union f</context>
</contexts>
<marker>Hays, 1960</marker>
<rawString>David Hays. 1960. Grouping and dependency theories. Research memorandum RM-2646, The RAND Corporation. Published in: Proc. of the National Symp. on Machine Translaion, Englewood Cliffs (N.Y.), 1961, pp. 258-266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Hudson</author>
</authors>
<title>Word Grammar. Basil Blackwell,</title>
<date>1984</date>
<location>Oxford-New York.</location>
<contexts>
<context position="3832" citStr="Hudson, 1984" startWordPosition="597" endWordPosition="598">e lifting (e.g., (Lombardo and Lesmo, 1998; Kahane et al., 1998)). In logical type based grammars, like CGs, adequate extension to DG presupposes one more distinction: that of semantic functionality and of syntactic subordinacy, which are opposite, for instance, for verb and noun modifiers. Multimodal extensions of CGs take proper account of these distinctions (cf. (Moortgat and Morrill, 1991; Morrill, 1994; Kruijff, 2001)). The overwhelming majority of DGs are head driven in the above sense. At the same time, some linguistic theories, e.g. &amp;quot;MeaningText Theory&amp;quot; (Mereuk, 1997), &amp;quot;Word Grammar&amp;quot; (Hudson, 1984) use a very different perspective of syntactic dependency, to which we will refer as word driven. Word-driven DG is closer to the original idea of Tesniere. Basically, it describes dependencies as binary relations between words in the language without addressing head scopes. This leads to far-reaching consequences for grammatical classification of words and dependency typology (cf. (Jackendoff, 1977; Keenan and Comrie, 1977) and a word driven DG of English in (Mel&apos;euk and Pertsov, 1987)). Formal definitions of word driven DGs are few in number. Historically the first was (Gladkij and Mel&apos;euk, </context>
</contexts>
<marker>Hudson, 1984</marker>
<rawString>Richard Hudson. 1984. Word Grammar. Basil Blackwell, Oxford-New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lydija Iordanskaja</author>
<author>Igor Mel&apos;euk</author>
</authors>
<title>The notion of surface-syntactic relation revisited. valence-controlled surface-syntactic relations in french.</title>
<date>2000</date>
<booktitle>S/ovo v tekste i v slovare [Word in Text and in Dictionary],</booktitle>
<pages>391--433</pages>
<editor>In L. Iomdin and L. Krysin, editors,</editor>
<location>Moscow.</location>
<contexts>
<context position="5623" citStr="Iordanskaja and Mel&apos;euk, 2000" startWordPosition="869" endWordPosition="872"> The Categorial Dependency Grammars (CDCs) we propose have two particularities: a specific type system inspired by (Mel&apos;euk and Pertsov, 1987) and a simple dependency calculus for projective and discontinuous dependencies. 2 Syntactic types A word driven dependency G(overnor) S(ubordinate) encodes an irreflexive antisymmetric antitransitive binary relation on words with intuitive meaning &amp;quot;G licenses S&amp;quot; defined by constraints on lexical and grammatical features, precedence constraints, pronominalization constraints, etc. concerning the words G and S and sometimes also close context words (see (Iordanskaja and Mel&apos;euk, 2000) for a detailed presentation). Let us see a fragment of the definition in (Mel&apos;euk and Pertsov, 1987) of the modificative dependency in English: 77-Jodi f* &lt;#. Yi3 . . . WHERE if Y Z Xa then (if 17,3 fi&gt; Z then Z = &apos;ENOUGH&apos; or (D = coordin and Z = ;INV))) (detlquant) otherwise if Xa Z then exists Y,3 U Z: (U = (`SUCHII`W HAT&apos;) and Z = (`All&apos;AN1)) and if Xa = (Num) then (forall) T: not T quant Xa etc.)...) and a = (N, not pron),(Num),`ONE21 and )3 = (A, not det, not pred!, not postpos!), (V)ppr„, (V, not postpos!)pass This complex formula is encoded by the dependency name modif. The correspondi</context>
</contexts>
<marker>Iordanskaja, Mel&apos;euk, 2000</marker>
<rawString>Lydija Iordanskaja and Igor Mel&apos;euk. 2000. The notion of surface-syntactic relation revisited. valence-controlled surface-syntactic relations in french. In L. Iomdin and L. Krysin, editors, S/ovo v tekste i v slovare [Word in Text and in Dictionary], pages 391-433. Languages of Russian Culture, Moscow.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>X-bar Syntax. A Study of Phrase Structure.</title>
<date>1977</date>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="4234" citStr="Jackendoff, 1977" startWordPosition="659" endWordPosition="661">rrill, 1994; Kruijff, 2001)). The overwhelming majority of DGs are head driven in the above sense. At the same time, some linguistic theories, e.g. &amp;quot;MeaningText Theory&amp;quot; (Mereuk, 1997), &amp;quot;Word Grammar&amp;quot; (Hudson, 1984) use a very different perspective of syntactic dependency, to which we will refer as word driven. Word-driven DG is closer to the original idea of Tesniere. Basically, it describes dependencies as binary relations between words in the language without addressing head scopes. This leads to far-reaching consequences for grammatical classification of words and dependency typology (cf. (Jackendoff, 1977; Keenan and Comrie, 1977) and a word driven DG of English in (Mel&apos;euk and Pertsov, 1987)). Formal definitions of word driven DGs are few in number. Historically the first was (Gladkij and Mel&apos;euk, 1971). It proposed a concept of tree generating DG and started a mathematical research into subfamilies of this very general class of grammars (see a review in (Dikovsky and Modina, 2000)). More recent propositions are Link Grammars (Sleator and Temperly, 1993) expressing only projective DTs and Polarized DGs (Dikovsky, 2001). The existing word driven DG definitions are operational and make no link </context>
</contexts>
<marker>Jackendoff, 1977</marker>
<rawString>Ray Jackendoff. 1977. X-bar Syntax. A Study of Phrase Structure. MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>L S Levy</author>
<author>M Takahashi</author>
</authors>
<title>Tree adjunct grammars.</title>
<date>1975</date>
<booktitle>Journ. of Comput. and Syst. Sci.,</booktitle>
<pages>10--4136</pages>
<contexts>
<context position="2455" citStr="Joshi et al., 1975" startWordPosition="386" endWordPosition="389">uents&apos; heads. From the 70ies, it is known ((Gladkij, 1966; Robinson, 1970)) that this link between the two structures is reversible: a selection of one immediate head per constituent induces a unique DT by the following induction: C C C&apos; root(ImmHead(C1)) —&gt;* root(ImmHead(C)). This structural &amp;quot;equivalence&amp;quot; produced an illusion that DTs are byproduct of head selection in constituent structures. So all syntagmatic grammars with head selection (e.g., LFG (Kaplan and Bresnan, 1982) and HPSG (Pollard and Sag, 1994)) may in a way be considered as DGs. Formally, such &amp;quot;extension&amp;quot; to DG concerns TAGs (Joshi et al., 1975) and categorial grammars (CGs) (Bar-Hillel, 1953; Lambek, 1958)) as well. However, this analogy is very superficial. It was soon realized that dependency and precedence are rather independent. For instance, the head driven DT above are always projective: the projections of all words fill continuous segments. Meanwhile, discontinuous non-projective dependencies are inevitable in languages. They often mark communicative structure (e.g. topicalization) and special constructions encoding complex semantic relations (e.g. clefting, subject or object extraction in pied-piping, etc.). This distinction</context>
</contexts>
<marker>Joshi, Levy, Takahashi, 1975</marker>
<rawString>A.K. Joshi, L.S. Levy, and M. Takahashi. 1975. Tree adjunct grammars. Journ. of Comput. and Syst. Sci., 10(4136-163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvain Kahane</author>
<author>Alexis Nasr</author>
<author>Owen Rambow</author>
</authors>
<title>Pseudo-projectivity : A polynomially parsable non-projective dependency grammar.</title>
<date>1998</date>
<booktitle>In Proc. COLING-ACL,</booktitle>
<pages>646--652</pages>
<location>Montreal.</location>
<contexts>
<context position="3283" citStr="Kahane et al., 1998" startWordPosition="508" endWordPosition="511">he head driven DT above are always projective: the projections of all words fill continuous segments. Meanwhile, discontinuous non-projective dependencies are inevitable in languages. They often mark communicative structure (e.g. topicalization) and special constructions encoding complex semantic relations (e.g. clefting, subject or object extraction in pied-piping, etc.). This distinction led to many propositions, both in terms of order constraints (e.g., (Maruyama, 1990; Broker, 1998; Duchier and Debusmann, 2001)) and in structure sharing terms like lifting (e.g., (Lombardo and Lesmo, 1998; Kahane et al., 1998)). In logical type based grammars, like CGs, adequate extension to DG presupposes one more distinction: that of semantic functionality and of syntactic subordinacy, which are opposite, for instance, for verb and noun modifiers. Multimodal extensions of CGs take proper account of these distinctions (cf. (Moortgat and Morrill, 1991; Morrill, 1994; Kruijff, 2001)). The overwhelming majority of DGs are head driven in the above sense. At the same time, some linguistic theories, e.g. &amp;quot;MeaningText Theory&amp;quot; (Mereuk, 1997), &amp;quot;Word Grammar&amp;quot; (Hudson, 1984) use a very different perspective of syntactic depe</context>
</contexts>
<marker>Kahane, Nasr, Rambow, 1998</marker>
<rawString>Sylvain Kahane, Alexis Nasr, and Owen Rambow. 1998. Pseudo-projectivity : A polynomially parsable non-projective dependency grammar. In Proc. COLING-ACL, pages 646-652, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Kaplan</author>
<author>Joan Bresnan</author>
</authors>
<title>Lexical functional grammar : A formal system for grammatical representation.</title>
<date>1982</date>
<booktitle>The Mental Representation of Grammatical Relations,</booktitle>
<pages>173--281</pages>
<editor>In Bresnan J.W., editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="2318" citStr="Kaplan and Bresnan, 1982" startWordPosition="359" endWordPosition="363"> the order given) the projections of words on the sentence, which form a system of constituents with the projected words serving as the constituents&apos; heads. From the 70ies, it is known ((Gladkij, 1966; Robinson, 1970)) that this link between the two structures is reversible: a selection of one immediate head per constituent induces a unique DT by the following induction: C C C&apos; root(ImmHead(C1)) —&gt;* root(ImmHead(C)). This structural &amp;quot;equivalence&amp;quot; produced an illusion that DTs are byproduct of head selection in constituent structures. So all syntagmatic grammars with head selection (e.g., LFG (Kaplan and Bresnan, 1982) and HPSG (Pollard and Sag, 1994)) may in a way be considered as DGs. Formally, such &amp;quot;extension&amp;quot; to DG concerns TAGs (Joshi et al., 1975) and categorial grammars (CGs) (Bar-Hillel, 1953; Lambek, 1958)) as well. However, this analogy is very superficial. It was soon realized that dependency and precedence are rather independent. For instance, the head driven DT above are always projective: the projections of all words fill continuous segments. Meanwhile, discontinuous non-projective dependencies are inevitable in languages. They often mark communicative structure (e.g. topicalization) and speci</context>
</contexts>
<marker>Kaplan, Bresnan, 1982</marker>
<rawString>Ronald Kaplan and Joan Bresnan. 1982. Lexical functional grammar : A formal system for grammatical representation. In Bresnan J.W., editor, The Mental Representation of Grammatical Relations, pages 173-281. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Keenan</author>
<author>Bernard Comrie</author>
</authors>
<title>Noun phrase accessibility and universal grammar. Linguistic Inquiry,</title>
<date>1977</date>
<pages>8--463</pages>
<contexts>
<context position="4260" citStr="Keenan and Comrie, 1977" startWordPosition="662" endWordPosition="665">ff, 2001)). The overwhelming majority of DGs are head driven in the above sense. At the same time, some linguistic theories, e.g. &amp;quot;MeaningText Theory&amp;quot; (Mereuk, 1997), &amp;quot;Word Grammar&amp;quot; (Hudson, 1984) use a very different perspective of syntactic dependency, to which we will refer as word driven. Word-driven DG is closer to the original idea of Tesniere. Basically, it describes dependencies as binary relations between words in the language without addressing head scopes. This leads to far-reaching consequences for grammatical classification of words and dependency typology (cf. (Jackendoff, 1977; Keenan and Comrie, 1977) and a word driven DG of English in (Mel&apos;euk and Pertsov, 1987)). Formal definitions of word driven DGs are few in number. Historically the first was (Gladkij and Mel&apos;euk, 1971). It proposed a concept of tree generating DG and started a mathematical research into subfamilies of this very general class of grammars (see a review in (Dikovsky and Modina, 2000)). More recent propositions are Link Grammars (Sleator and Temperly, 1993) expressing only projective DTs and Polarized DGs (Dikovsky, 2001). The existing word driven DG definitions are operational and make no link with logic. In particular,</context>
</contexts>
<marker>Keenan, Comrie, 1977</marker>
<rawString>Edward Keenan and Bernard Comrie. 1977. Noun phrase accessibility and universal grammar. Linguistic Inquiry, 8(463-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geert-Jan M Kruijff</author>
</authors>
<title>A Cat egorialModalLogical Architecture of Informativity.</title>
<date>2001</date>
<booktitle>Dependency Grammar Logic &amp; Information Structure. Ph.D. thesis,</booktitle>
<institution>Charles University,</institution>
<location>Prague.</location>
<contexts>
<context position="3645" citStr="Kruijff, 2001" startWordPosition="567" endWordPosition="568"> etc.). This distinction led to many propositions, both in terms of order constraints (e.g., (Maruyama, 1990; Broker, 1998; Duchier and Debusmann, 2001)) and in structure sharing terms like lifting (e.g., (Lombardo and Lesmo, 1998; Kahane et al., 1998)). In logical type based grammars, like CGs, adequate extension to DG presupposes one more distinction: that of semantic functionality and of syntactic subordinacy, which are opposite, for instance, for verb and noun modifiers. Multimodal extensions of CGs take proper account of these distinctions (cf. (Moortgat and Morrill, 1991; Morrill, 1994; Kruijff, 2001)). The overwhelming majority of DGs are head driven in the above sense. At the same time, some linguistic theories, e.g. &amp;quot;MeaningText Theory&amp;quot; (Mereuk, 1997), &amp;quot;Word Grammar&amp;quot; (Hudson, 1984) use a very different perspective of syntactic dependency, to which we will refer as word driven. Word-driven DG is closer to the original idea of Tesniere. Basically, it describes dependencies as binary relations between words in the language without addressing head scopes. This leads to far-reaching consequences for grammatical classification of words and dependency typology (cf. (Jackendoff, 1977; Keenan an</context>
</contexts>
<marker>Kruijff, 2001</marker>
<rawString>Geert-Jan M. Kruijff. 2001. A Cat egorialModalLogical Architecture of Informativity. Dependency Grammar Logic &amp; Information Structure. Ph.D. thesis, Charles University, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Lambek</author>
</authors>
<title>The mathematics of sentence structure.</title>
<date>1958</date>
<journal>American Mathematical Monthly,</journal>
<pages>154--170</pages>
<contexts>
<context position="2518" citStr="Lambek, 1958" startWordPosition="397" endWordPosition="398">1970)) that this link between the two structures is reversible: a selection of one immediate head per constituent induces a unique DT by the following induction: C C C&apos; root(ImmHead(C1)) —&gt;* root(ImmHead(C)). This structural &amp;quot;equivalence&amp;quot; produced an illusion that DTs are byproduct of head selection in constituent structures. So all syntagmatic grammars with head selection (e.g., LFG (Kaplan and Bresnan, 1982) and HPSG (Pollard and Sag, 1994)) may in a way be considered as DGs. Formally, such &amp;quot;extension&amp;quot; to DG concerns TAGs (Joshi et al., 1975) and categorial grammars (CGs) (Bar-Hillel, 1953; Lambek, 1958)) as well. However, this analogy is very superficial. It was soon realized that dependency and precedence are rather independent. For instance, the head driven DT above are always projective: the projections of all words fill continuous segments. Meanwhile, discontinuous non-projective dependencies are inevitable in languages. They often mark communicative structure (e.g. topicalization) and special constructions encoding complex semantic relations (e.g. clefting, subject or object extraction in pied-piping, etc.). This distinction led to many propositions, both in terms of order constraints (</context>
</contexts>
<marker>Lambek, 1958</marker>
<rawString>Joachim Lambek. 1958. The mathematics of sentence structure. American Mathematical Monthly, pages 154-170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincenzo Lombardo</author>
<author>Leonardo Lesmo</author>
</authors>
<title>Formal aspects and parsing issues of dependency theory.</title>
<date>1998</date>
<booktitle>In Proc. COLING-ACL,</booktitle>
<pages>787--793</pages>
<location>Montreal.</location>
<contexts>
<context position="3261" citStr="Lombardo and Lesmo, 1998" startWordPosition="504" endWordPosition="507">dependent. For instance, the head driven DT above are always projective: the projections of all words fill continuous segments. Meanwhile, discontinuous non-projective dependencies are inevitable in languages. They often mark communicative structure (e.g. topicalization) and special constructions encoding complex semantic relations (e.g. clefting, subject or object extraction in pied-piping, etc.). This distinction led to many propositions, both in terms of order constraints (e.g., (Maruyama, 1990; Broker, 1998; Duchier and Debusmann, 2001)) and in structure sharing terms like lifting (e.g., (Lombardo and Lesmo, 1998; Kahane et al., 1998)). In logical type based grammars, like CGs, adequate extension to DG presupposes one more distinction: that of semantic functionality and of syntactic subordinacy, which are opposite, for instance, for verb and noun modifiers. Multimodal extensions of CGs take proper account of these distinctions (cf. (Moortgat and Morrill, 1991; Morrill, 1994; Kruijff, 2001)). The overwhelming majority of DGs are head driven in the above sense. At the same time, some linguistic theories, e.g. &amp;quot;MeaningText Theory&amp;quot; (Mereuk, 1997), &amp;quot;Word Grammar&amp;quot; (Hudson, 1984) use a very different perspec</context>
</contexts>
<marker>Lombardo, Lesmo, 1998</marker>
<rawString>Vincenzo Lombardo and Leonardo Lesmo. 1998. Formal aspects and parsing issues of dependency theory. In Proc. COLING-ACL, pages 787-793, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Maruyama</author>
</authors>
<title>Structural disambiguation with constraint propagation.</title>
<date>1990</date>
<booktitle>In Proc. of 28th ACL Annual Meeting,</booktitle>
<pages>31--38</pages>
<contexts>
<context position="3139" citStr="Maruyama, 1990" startWordPosition="487" endWordPosition="488">ll. However, this analogy is very superficial. It was soon realized that dependency and precedence are rather independent. For instance, the head driven DT above are always projective: the projections of all words fill continuous segments. Meanwhile, discontinuous non-projective dependencies are inevitable in languages. They often mark communicative structure (e.g. topicalization) and special constructions encoding complex semantic relations (e.g. clefting, subject or object extraction in pied-piping, etc.). This distinction led to many propositions, both in terms of order constraints (e.g., (Maruyama, 1990; Broker, 1998; Duchier and Debusmann, 2001)) and in structure sharing terms like lifting (e.g., (Lombardo and Lesmo, 1998; Kahane et al., 1998)). In logical type based grammars, like CGs, adequate extension to DG presupposes one more distinction: that of semantic functionality and of syntactic subordinacy, which are opposite, for instance, for verb and noun modifiers. Multimodal extensions of CGs take proper account of these distinctions (cf. (Moortgat and Morrill, 1991; Morrill, 1994; Kruijff, 2001)). The overwhelming majority of DGs are head driven in the above sense. At the same time, some</context>
</contexts>
<marker>Maruyama, 1990</marker>
<rawString>Hiroshi Maruyama. 1990. Structural disambiguation with constraint propagation. In Proc. of 28th ACL Annual Meeting, pages 31-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mel&apos;euk</author>
<author>N V Pertsov</author>
</authors>
<title>Surface Syntax of English. A Formal Model Within the Meaning-Text Framework.</title>
<date>1987</date>
<publisher>John Benjamins Publishing Company, Amsterdam/Philadelphia.</publisher>
<contexts>
<context position="4323" citStr="Mel&apos;euk and Pertsov, 1987" startWordPosition="674" endWordPosition="677">in the above sense. At the same time, some linguistic theories, e.g. &amp;quot;MeaningText Theory&amp;quot; (Mereuk, 1997), &amp;quot;Word Grammar&amp;quot; (Hudson, 1984) use a very different perspective of syntactic dependency, to which we will refer as word driven. Word-driven DG is closer to the original idea of Tesniere. Basically, it describes dependencies as binary relations between words in the language without addressing head scopes. This leads to far-reaching consequences for grammatical classification of words and dependency typology (cf. (Jackendoff, 1977; Keenan and Comrie, 1977) and a word driven DG of English in (Mel&apos;euk and Pertsov, 1987)). Formal definitions of word driven DGs are few in number. Historically the first was (Gladkij and Mel&apos;euk, 1971). It proposed a concept of tree generating DG and started a mathematical research into subfamilies of this very general class of grammars (see a review in (Dikovsky and Modina, 2000)). More recent propositions are Link Grammars (Sleator and Temperly, 1993) expressing only projective DTs and Polarized DGs (Dikovsky, 2001). The existing word driven DG definitions are operational and make no link with logic. In particular, there is no definition based on word driven dependency types. </context>
<context position="5724" citStr="Mel&apos;euk and Pertsov, 1987" startWordPosition="887" endWordPosition="890">spired by (Mel&apos;euk and Pertsov, 1987) and a simple dependency calculus for projective and discontinuous dependencies. 2 Syntactic types A word driven dependency G(overnor) S(ubordinate) encodes an irreflexive antisymmetric antitransitive binary relation on words with intuitive meaning &amp;quot;G licenses S&amp;quot; defined by constraints on lexical and grammatical features, precedence constraints, pronominalization constraints, etc. concerning the words G and S and sometimes also close context words (see (Iordanskaja and Mel&apos;euk, 2000) for a detailed presentation). Let us see a fragment of the definition in (Mel&apos;euk and Pertsov, 1987) of the modificative dependency in English: 77-Jodi f* &lt;#. Yi3 . . . WHERE if Y Z Xa then (if 17,3 fi&gt; Z then Z = &apos;ENOUGH&apos; or (D = coordin and Z = ;INV))) (detlquant) otherwise if Xa Z then exists Y,3 U Z: (U = (`SUCHII`W HAT&apos;) and Z = (`All&apos;AN1)) and if Xa = (Num) then (forall) T: not T quant Xa etc.)...) and a = (N, not pron),(Num),`ONE21 and )3 = (A, not det, not pred!, not postpos!), (V)ppr„, (V, not postpos!)pass This complex formula is encoded by the dependency name modif. The corresponding dependency is the set of pairs of wordforms satisfying the formula in their occurrences. Suppose t</context>
</contexts>
<marker>Mel&apos;euk, Pertsov, 1987</marker>
<rawString>I. Mel&apos;euk and N.V. Pertsov. 1987. Surface Syntax of English. A Formal Model Within the Meaning-Text Framework. John Benjamins Publishing Company, Amsterdam/Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Mel&apos;euk</author>
</authors>
<title>Vers une linguistique . Lecon inagurale au College de France.</title>
<date>1997</date>
<marker>Mel&apos;euk, 1997</marker>
<rawString>Igor Mel&apos;euk. 1997. Vers une linguistique &lt;Sens-Texte&gt;&gt; . Lecon inagurale au College de France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Moortgat</author>
<author>Glin V Morrill</author>
</authors>
<title>Heads and phrases. Type calculus for dependency and constituent structure. Ms OTS,</title>
<date>1991</date>
<location>Utrecht.</location>
<contexts>
<context position="3614" citStr="Moortgat and Morrill, 1991" startWordPosition="561" endWordPosition="564">ubject or object extraction in pied-piping, etc.). This distinction led to many propositions, both in terms of order constraints (e.g., (Maruyama, 1990; Broker, 1998; Duchier and Debusmann, 2001)) and in structure sharing terms like lifting (e.g., (Lombardo and Lesmo, 1998; Kahane et al., 1998)). In logical type based grammars, like CGs, adequate extension to DG presupposes one more distinction: that of semantic functionality and of syntactic subordinacy, which are opposite, for instance, for verb and noun modifiers. Multimodal extensions of CGs take proper account of these distinctions (cf. (Moortgat and Morrill, 1991; Morrill, 1994; Kruijff, 2001)). The overwhelming majority of DGs are head driven in the above sense. At the same time, some linguistic theories, e.g. &amp;quot;MeaningText Theory&amp;quot; (Mereuk, 1997), &amp;quot;Word Grammar&amp;quot; (Hudson, 1984) use a very different perspective of syntactic dependency, to which we will refer as word driven. Word-driven DG is closer to the original idea of Tesniere. Basically, it describes dependencies as binary relations between words in the language without addressing head scopes. This leads to far-reaching consequences for grammatical classification of words and dependency typology (c</context>
</contexts>
<marker>Moortgat, Morrill, 1991</marker>
<rawString>Michael Moortgat and Glin V. Morrill. 1991. Heads and phrases. Type calculus for dependency and constituent structure. Ms OTS, Utrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glin V Morrill</author>
</authors>
<title>Type Logical Grammar. Categorial Logic of Signs.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="3629" citStr="Morrill, 1994" startWordPosition="565" endWordPosition="566">in pied-piping, etc.). This distinction led to many propositions, both in terms of order constraints (e.g., (Maruyama, 1990; Broker, 1998; Duchier and Debusmann, 2001)) and in structure sharing terms like lifting (e.g., (Lombardo and Lesmo, 1998; Kahane et al., 1998)). In logical type based grammars, like CGs, adequate extension to DG presupposes one more distinction: that of semantic functionality and of syntactic subordinacy, which are opposite, for instance, for verb and noun modifiers. Multimodal extensions of CGs take proper account of these distinctions (cf. (Moortgat and Morrill, 1991; Morrill, 1994; Kruijff, 2001)). The overwhelming majority of DGs are head driven in the above sense. At the same time, some linguistic theories, e.g. &amp;quot;MeaningText Theory&amp;quot; (Mereuk, 1997), &amp;quot;Word Grammar&amp;quot; (Hudson, 1984) use a very different perspective of syntactic dependency, to which we will refer as word driven. Word-driven DG is closer to the original idea of Tesniere. Basically, it describes dependencies as binary relations between words in the language without addressing head scopes. This leads to far-reaching consequences for grammatical classification of words and dependency typology (cf. (Jackendoff,</context>
</contexts>
<marker>Morrill, 1994</marker>
<rawString>Glin V. Morrill. 1994. Type Logical Grammar. Categorial Logic of Signs. Kluwer Academic Publishers, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Neuhaus</author>
<author>Norbert Broker</author>
</authors>
<title>The Complexity of Recognition of Linguistically Adequate Dependency Grammars.</title>
<date>1997</date>
<booktitle>In Proc. of 35th ACL Annual Meeting and 8th Conf. of the ECACL,</booktitle>
<pages>337--343</pages>
<marker>Neuhaus, Broker, 1997</marker>
<rawString>Peter Neuhaus and Norbert Broker. 1997. The Complexity of Recognition of Linguistically Adequate Dependency Grammars. In Proc. of 35th ACL Annual Meeting and 8th Conf. of the ECACL, pages 337-343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan Sag</author>
</authors>
<date>1994</date>
<booktitle>Head-driven Phrase Structure Grammar. CSLI,</booktitle>
<location>Stanford, California.</location>
<contexts>
<context position="2351" citStr="Pollard and Sag, 1994" startWordPosition="366" endWordPosition="369"> words on the sentence, which form a system of constituents with the projected words serving as the constituents&apos; heads. From the 70ies, it is known ((Gladkij, 1966; Robinson, 1970)) that this link between the two structures is reversible: a selection of one immediate head per constituent induces a unique DT by the following induction: C C C&apos; root(ImmHead(C1)) —&gt;* root(ImmHead(C)). This structural &amp;quot;equivalence&amp;quot; produced an illusion that DTs are byproduct of head selection in constituent structures. So all syntagmatic grammars with head selection (e.g., LFG (Kaplan and Bresnan, 1982) and HPSG (Pollard and Sag, 1994)) may in a way be considered as DGs. Formally, such &amp;quot;extension&amp;quot; to DG concerns TAGs (Joshi et al., 1975) and categorial grammars (CGs) (Bar-Hillel, 1953; Lambek, 1958)) as well. However, this analogy is very superficial. It was soon realized that dependency and precedence are rather independent. For instance, the head driven DT above are always projective: the projections of all words fill continuous segments. Meanwhile, discontinuous non-projective dependencies are inevitable in languages. They often mark communicative structure (e.g. topicalization) and special constructions encoding complex</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan Sag. 1994. Head-driven Phrase Structure Grammar. CSLI, Stanford, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane J Robinson</author>
</authors>
<title>Dependency structures and transformational rules.</title>
<date>1970</date>
<pages>46--2</pages>
<contexts>
<context position="1910" citStr="Robinson, 1970" startWordPosition="299" endWordPosition="300">tic dependencies), than to definitions of sentence constituents. Meanwhile, historically the first DGs define in fact both. The Hays-Gaifman&apos;s DGs are lexicalized. They assign grammatical categories to words and position the subordinates with respect to their governors. In this manner, they define not only the binary relations &amp;quot;governor —&gt; subordinate&amp;quot;, whose union forms a tree, but also (due to the order given) the projections of words on the sentence, which form a system of constituents with the projected words serving as the constituents&apos; heads. From the 70ies, it is known ((Gladkij, 1966; Robinson, 1970)) that this link between the two structures is reversible: a selection of one immediate head per constituent induces a unique DT by the following induction: C C C&apos; root(ImmHead(C1)) —&gt;* root(ImmHead(C)). This structural &amp;quot;equivalence&amp;quot; produced an illusion that DTs are byproduct of head selection in constituent structures. So all syntagmatic grammars with head selection (e.g., LFG (Kaplan and Bresnan, 1982) and HPSG (Pollard and Sag, 1994)) may in a way be considered as DGs. Formally, such &amp;quot;extension&amp;quot; to DG concerns TAGs (Joshi et al., 1975) and categorial grammars (CGs) (Bar-Hillel, 1953; Lambe</context>
</contexts>
<marker>Robinson, 1970</marker>
<rawString>Jane J. Robinson. 1970. Dependency structures and transformational rules. 46(2):259-285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Sleator</author>
<author>Davy Temperly</author>
</authors>
<title>Parsing English with a Link Grammar. In</title>
<date>1993</date>
<booktitle>Proc. IWPT&apos;93,</booktitle>
<pages>277--291</pages>
<contexts>
<context position="4693" citStr="Sleator and Temperly, 1993" startWordPosition="733" endWordPosition="736"> language without addressing head scopes. This leads to far-reaching consequences for grammatical classification of words and dependency typology (cf. (Jackendoff, 1977; Keenan and Comrie, 1977) and a word driven DG of English in (Mel&apos;euk and Pertsov, 1987)). Formal definitions of word driven DGs are few in number. Historically the first was (Gladkij and Mel&apos;euk, 1971). It proposed a concept of tree generating DG and started a mathematical research into subfamilies of this very general class of grammars (see a review in (Dikovsky and Modina, 2000)). More recent propositions are Link Grammars (Sleator and Temperly, 1993) expressing only projective DTs and Polarized DGs (Dikovsky, 2001). The existing word driven DG definitions are operational and make no link with logic. In particular, there is no definition based on word driven dependency types. In this paper, we propose such a definition in terms of classical CCs. The Categorial Dependency Grammars (CDCs) we propose have two particularities: a specific type system inspired by (Mel&apos;euk and Pertsov, 1987) and a simple dependency calculus for projective and discontinuous dependencies. 2 Syntactic types A word driven dependency G(overnor) S(ubordinate) encodes a</context>
</contexts>
<marker>Sleator, Temperly, 1993</marker>
<rawString>Daniel Sleator and Davy Temperly. 1993. Parsing English with a Link Grammar. In Proc. IWPT&apos;93, pages 277-291.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucien Tesniere</author>
</authors>
<title>Elements de syntaxe structurale. Librairie C. Klincksiek,</title>
<date>1959</date>
<location>Paris.</location>
<contexts>
<context position="1003" citStr="Tesniere, 1959" startWordPosition="155" endWordPosition="156">he latter are defined in terms of polarized valencies, whose neutralization is controlled by the simple principle &amp;quot;choose the first available&amp;quot; ( FA). Being very expressive, CDGs are analyzed in polynomial time. Besides this, CDGs represent a convenient frame for relating dependency grammar with linguistic semantics. 1 Introduction Dependency grammars (DGs) are formal grammars assigning dependency trees (DTs) to wellformed sentences. A DT of a sentence is a labelled arrows tree whose nodes are the words of the sentence. A rather formal description of DGs and DG syntax was given by L. Tesniere (Tesniere, 1959). The first exact definitions are due to D. Hays (Hays, 1960) and H. Gaifman (Gaifman, 1961). The basic syntactic principle behind the DGs is quite different from that of syntagmatic grammars. They are designed for and more adapted to definitions of binary relations between wordforms (syntactic dependencies), than to definitions of sentence constituents. Meanwhile, historically the first DGs define in fact both. The Hays-Gaifman&apos;s DGs are lexicalized. They assign grammatical categories to words and position the subordinates with respect to their governors. In this manner, they define not only </context>
</contexts>
<marker>Tesniere, 1959</marker>
<rawString>Lucien Tesniere. 1959. Elements de syntaxe structurale. Librairie C. Klincksiek, Paris.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>