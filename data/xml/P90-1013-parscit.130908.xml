<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<sectionHeader confidence="0.510911" genericHeader="method">
THE COMPUTATIONAL COMPLEXITY OF
AVOIDING CONVERSATIONAL IMPLICATURES
Ehud Reitert
Aiken Computation Lab
Harvard University
Cambridge, Mass 02138
ABSTRACT&apos;
</sectionHeader>
<bodyText confidence="0.998498444444445">
Referring expressions and other object descriptions
should be maximal under the Local Brevity, No
Unnecessary Components, and Lexical Preference
preference rules; otherwise, they may lead hearers to
infer unwanted conversational implicatures. These
preference rules can be incorporated into a polyno-
mial time generation algorithm, while some alterna-
tive formalizations of conversational implicature
make the generation task NP-Hard.
</bodyText>
<sectionHeader confidence="0.995155" genericHeader="method">
1. Introduction
</sectionHeader>
<bodyText confidence="0.956165481481482">
Natural language generation (NLG) systems
should produce referring expressions and other object
descriptions that are free of false implicatures, i.e.,
that do not cause the user of the system to infer
incorrect and unwanted conversational implicaatres
(Grice 1975). The following utterances illustrate
referring expressions that are and are not free of false
implicatures:
la) &amp;quot;Sit by the table&amp;quot;
Ib) &amp;quot;Sit by the brown wooden table&amp;quot;
In a context where only one table was visible, and
this table was brown and made of wood, utterances
(la) and (lb) would both fulfill the referring goal: a
hearer who heard either utterance would have no
trouble picking out the object being referred to.
However, a hearer who heard utterance (lb) would
probably assume that it was somehow important that
the table was brown and made of wood, i.e., that the
speaker was trying to do more than just identify the
table. If the speaker did not have this intention, and
only wished to tell the hearer where to sit, then this
would be an incorrect conversational implicature, and
could lead to problems later in the discourse.
Accordingly, a speaker who only wished to identify
the table should use utterance (la) in this situation,
t Currently at the Department of Artificial Intelligence,
University of Edinburgh, 80 South Bridge, Edinburgh EH1
</bodyText>
<note confidence="0.782959">
1HN, Scotland. 97
</note>
<bodyText confidence="0.970969714285714">
and avoid utterance (lb).
Incorrect conversational implicatures may also
arise from inappropriate attributive (informational)
descriptions.&apos; This is illustrated by the following
utterances, which might be used by a salesman who
wished to inform a customer of the color, material,
and sleeve-length of a shirt;
</bodyText>
<listItem confidence="0.461138">
2a) &amp;quot;I have a red T-shirt&amp;quot;
2b) &amp;quot;I have a lightweight red cotton shirt with
short sleeves&amp;quot;
</listItem>
<bodyText confidence="0.962880805555555">
Utterances (2a) and (2b) both successfully inform the
hearer of the relevant properties of the shirt, assum-
ing the hearer has some domain knowledge about 1-
shirts. However, if the hearer has this domain
knowledge, the use of utterance (2b) might
incorrectly implicate that the object being described
was not a T-shirt — because if it was, the hearer
would reason, then the speaker would have used
utterance (2a).
Therefore, in the above situations the speaker,
whether a human or a computer NLG system, should
use utterances (la) and (2a), and should avoid utter-
ances (lb) and (2b); utterances (la) and (2a) are free
of false implicatures, while the utterances (lb) and
(2b) are not. This paper proposes a computational
model for determining when an object description is
free of false implicatures. Briefly, a description is
considered free of false implicatures if it is maximal
under the Local Brevity, No Unnecessary Com-
ponents, and Lexical Preference preference rules.
These preference rules were chosen on complexity-
theoretic as well as linguistic criteria; descriptions
that are maximal under these preference rules can be
found in polynomial time, while some alternative for-
malizations of the free-of-false-implicatures con-
straint make the generation task NP-Hard.
1 The referrineattributive distinction follows Donnellan
(1966): a referring expression is intended to identify an object
in the current context, while an attributive description is in-
tended to communicate information about an object.
This paper only addresses the problem of gen-
erating free-of-false-implicatures referring expres-
sions, such as utterance (1a). Reiter (1990a,b) uses
the same preference rules to formalize the task of
generating free-of-false-implicatures attributive
descriptions, such as utterance (2a).
</bodyText>
<sectionHeader confidence="0.995902" genericHeader="method">
2. Referring Expression Model
</sectionHeader>
<bodyText confidence="0.998217522727272">
The referring-expression model used in this
paper is a variant of Dale&apos;s (1989) model for full
definite noun phrase referring expressions. Dale&apos;s
model is applicable in situations in which the speaker
intends to refer to an object that the speaker and
hearer are mutually aware of, and the speaker has no
other communicative goal besides identifying the
referred-to object.2 The model assumes that objects
belong to a taxonomy class (e.g., Chair) and possess
values for various attributes (e.g., Color:Brown).3
Referring expressions are represented as a
classification and a set of attribute-value pairs: the
classification is syntactically realized as the head
noun, while the attribute-value pairs are syntactically
realized as NP modifiers. Successful referring
expressions are required to be distinguishing descrip-
tions, i.e., descriptions that contain a classification
and a set of attributes that are true of the object being
referred to, but not of any other object in the current
discourse context.4
More formally, and using a somewhat different
terminology from Dale, let a component be either a
classification or an attribute-value pair. A
classification component will be written class:Class;
an attribute-value pair component will be written
Attribute: Value. Then, given a target object, denoted
Target, and a set of contrasting objects in the current
discourse context, denoted Excluded, a set of com-
ponents will represent a successful referring expres-
sion (a distinguishing description, in Dale&apos;s terminal-
ogy) if the set, denoted RE, satisfies the following
constraints:
I) Every component in RE applies to Target: that
is, every component in RE is either a
classification that subsumes Target, or an
attribute-value pair that Target possesses.
2) For every member E of Excluded, there is at
least one component in RE that does not apply
to E.
Example: the current discourse context con-
tains objects A, B, and C (and no other objects), and
these objects have the following classifications and
attributes (of which both the speaker and the hearer
are aware):
</bodyText>
<subsectionHeader confidence="0.537339333333333">
A) Table with Material:Wood and Color:Brown.
B) Chair with Material:Wood and Color:Brown
C) Chair with Material:Wood and Color :Black
</subsectionHeader>
<bodyText confidence="0.998910230769231">
In this context, the referring expressions
{class:Table) (&amp;quot;the table&amp;quot;) and {class:Table,
Material:Wood, Color:Brown) (&amp;quot;the brown wooden
table&amp;quot;) both successfully refer to object A, because
they match object A but no other object. Similarly,
the referring expressions {class:Chair,
Color:Brown) (&amp;quot;the brown chair&amp;quot;) and (class:Chair,
Material:Wood, Color:Brown) (&amp;quot;the brown wooden
chair&amp;quot;) both successfully refer to object B, because
they match object B, but no other object. The refer-
ring expression (class:Chair) (&amp;quot;the chair&amp;quot;), how-
ever, does not successfully refer to object B, because
it also matches object C.
</bodyText>
<sectionHeader confidence="0.97459" genericHeader="method">
3. Conversational InipIicature
</sectionHeader>
<subsectionHeader confidence="0.997089">
3.1. Grice&apos;s Maxims and Their Interpretation
</subsectionHeader>
<bodyText confidence="0.999755272727273">
Once (1975) proposed four maxims of conver-
sation that speakers needed to obey: Quality, Quan-
tity, Relevance, and Manner. For the task of generat-
ing referring expressions as formalized in Section 2,
these maxims can be interpreted as follows:
Quality: The Quality maxim requires utter-
ances to be truthful. In this context, it requires refer-
ring expressions to be factual descriptions of the
referred-to object. This condition is already part of
the definition of a successful referring expression,
and does not need to be restated as a conversational
</bodyText>
<subsectionHeader confidence="0.54373">
98 implicature constraint.
</subsectionHeader>
<bodyText confidence="0.9935137">
2 Appelt (1985) presented a more complex referring-
expression model that covered situations where the hearer
was not already aware of the referred-to object, and that al-
lowed the speaker to have more complex communicative
goals. A similar analysis to the one presented in this per
could in principle be done for Appeles model, but it would
be substantially more difficuh, in part because the model is
more complex, and in part because Appelt did not separate
his &apos;content determination&apos; subsystem from his planner and
his surface-form generator.
</bodyText>
<footnote confidence="0.9332846">
3 All attributes are assumed to be predicative (Kamp
1975).
4 Dale also suggested that NI..G systems should choose
distinguishing descriptions of minimal cardinally; this is dis-
cussed in footnote 7.
</footnote>
<bodyText confidence="0.9986635">
Quantity: The Quality maxim requires utter-
ances to contain enough information to fulfill the
speaker&apos;s communicative goal, but not more informa-
tion. In this context, it requires referring expressions
to contain enough information to enable the hearer to
identify the referred-to object, but not more informa-
tion. Therefore, referring expressions should be suc-
cessful (as defined in Section 2), but should not con-
tain additional elements that are unnecessary for
fulfilling the referring goal.
Relevance: The Relevance maxim requires
utterances to be relevant to the discourse. In this
context, where the speaker is assumed just to have
the communicative goal of identifying an object to
the hearer, the maxim prohibits referring expressions
from containing elements that do not help distinguish
the target object from other objects in the discourse
context. Irrelevant elements are also unnecessary
elements, so the Relevance maxim may be con-
sidered to be a special case of the Quantity maxim, at
least for the referring-expression generation task as
formalized in Section 2.
Manner: The Brevity submaxim of the Manner
maxim requires a speaker to use short utterances if
possible. In this context it requires the speaker to use
a short referring expression if such a referring
expression exists. The analysis of the other Manner
submaxims is left for future work.
An additional source of conversational impli-
cature was proposed by Cruse (1977) and Hirschberg
(1985), who hypothesized that- implicatures might
arise from the failure to use basic-level classes
(Rosch 1978) in an utterance. In this paper, such
implicatures are generalized by assuming that there is
a lexical-preference hierarchy among the lexical
classes (classes that can be realized with single lexi-
cal units) known to the hearer, and that the use of a
lexical class in an utterance implicates that no pre-
ferred lexical class could have been used in its place.
In summary, conversational implicauire con-
siderations require referring expressions to be brief,
to not contain unnecessary elements, and to use
lexically-preferred classes whenever possible. The
following requests illustrate how violations of these
principles in referring expressions may lead to
unwanted conversational implicatures:
</bodyText>
<listItem confidence="0.987797625">
3a) &amp;quot;Wait for me by the pine.&amp;quot;
U class:? me))
3b) &amp;quot;Wait for me by the tree that has pinecones.&amp;quot;
((class:Tree, Seed-type:Pinecone))
3c) &amp;quot;Wait for me by the 50-foot-high pine.&amp;quot;
((class:? me. Height.-50-feetD
3d) &amp;quot;Wait for me by the sugar pine.&amp;quot;
((class:Sugar-pine))
</listItem>
<bodyText confidence="0.999745961538462">
If there were only two trees in the hearer&apos;s immediate
surroundings, a pine and an oak, then all of the above
utterances would be successful referring expressions
that enabled the hearer to pick out the object being
referred to (assuming the hearer could recognize
pines and oaks). In such a situation, however, utter-
ance (3b) would violate the brevity principle, and
thus would implicate that the tree could not be
described as a &amp;quot;pine&amp;quot; (which might lead the hearer to
infer that the tree was not a real pine, but some other
tree that happened to have pinecones). Utterance
(3c) would violate the no-unnecessary-elements prin-
ciple, and thus would implicate that it was important
that the tree was 50 feet tall (which might lead the
hearer to infer that there was another pine tree in the
area that had a different height). Utterance (3d)
would violate the lexical-preference principle, and
thus would implicate that the speaker wished to
emphasize that the tree was a sugar pine and not
some other kind of pine (which might lead the hearer
to infer that the speaker was trying to impress her
with his botanical knowledge). A speaker who only
wished to tell the hearer where to wait, and did not
want the hearer to make any of these implicatures,
would need to use utterance (3a), and to avoid utter-
ances (3b), (3c), and (3d).
</bodyText>
<subsectionHeader confidence="0.7733185">
3.2. Formalizing Conversational Implkature
Through Preference Rules
</subsectionHeader>
<bodyText confidence="0.990347575757576">
The brevity, no-unnecessary-elements, and
lexical-preference principles may be formalized by
requiring a description to be a maximal element
under a preference function of the set of successful
referring expressions. More formally, let D be the set
of successful referring expressions, and let » be a
preference function that prefers descriptions that are
short, that do not contain unnecessary elements, and
that use lexically preferred classes. Then, a referring
expression is considered free of false implicatures if
it is a maximal element of D with respect to ». In
other words, a description B in D is free of false
implicatures if there is no description A in D, such
that A » B. This formalization is similar to the par-
tially ordered sets that Hirschberg (1985) used to for-
99 malize scalar implicatures: D and &gt;&gt; together form a
partially ordered set, and the assumption is that the
use of an element in D carries the conversational
implicature that no higher-ranked element in D could
have been used.
The overall preference function &gt;&gt; will be
decomposed into separate preference rules that cover
each type of implicature: &gt;r.:-B for brevity, »u for
unnecessary elements, and ›,r, for lexical prefer-
ence. &gt;&gt; is then defined as the disjunction of these
preference rules, i.e., A&gt;&gt; B if A &gt;&gt;B B, A »u B,
or A &gt;N. B. The assumption will be made in this
paper that there are no conflicts between preference
rules, i.e., that it is never the case that A is preferred
over B by one preference rule, but B is preferred over
A by another preference rule.5 Therefore, » will be
a partial order if&gt;-,B, »u, and »1, are partial ord-
ers.
</bodyText>
<subsectionHeader confidence="0.979898">
3.3. Computational Tractability
</subsectionHeader>
<bodyText confidence="0.999800571428572">
Computational complexity considerations are
used in this paper to determine exactly how the no-
unnecessary-elements, brevity, and lexical-
preference principles should be formalized as prefer-
ence rules. Sections 4, 5, and 6 examine various
preference rules that might plausibly be used to for-
malize these implicatures, and reject preference rules
that make the generation task NP-Hard. This is
justified on the grounds that computer NLG systems
should not be asked to solve NP-Hard probierns.6
Human speakers and hearers are also probably not
very proficient at solving NP-Hard problems, which
suggests that it is unlikely that NP-Hard preference
rules have been incorporated into language.
</bodyText>
<sectionHeader confidence="0.987095" genericHeader="method">
4. Brevity
</sectionHeader>
<bodyText confidence="0.997225833333333">
Grice&apos;s submaxim of brevity states that utter-
ances should be kept brief. Many NLG researchers
(e.g., Dale 1989: Appelt 1985: pages 117-118) have
suggested that this means generation systems need to
produce the shortest possible utterance. This will be
called the Full Brevity preference rule. Unfor-
tunately, it is NP-Hard to find the shortest successful
referring expression (Section 4.1). Local Brevity
(Section 4.2) is a weaker version of the brevity sub-
maxim that can be incorporated into a polynomial-
time algorithm for generating successful referring
expressions.
</bodyText>
<sectionHeader confidence="0.717310666666667" genericHeader="method">
5 Section 7.2 discusses this assumption.
6 Section 7.1 discusses the computational impact of NP-
Hard preference rules.
</sectionHeader>
<subsectionHeader confidence="0.965877">
4.1. Full Brevity
</subsectionHeader>
<bodyText confidence="0.9997048">
The Full Brevity preference rule requires the
generation system to generate the shortest successful
referring expression. Formally, A »F8 B if
length(A) &lt; length(B). The task of finding a maximal
element of »FB, i.e., of finding the shortest success-
ful referring expression, is NP-Hard. This result
holds for all definitions of length the author has
examined (number of open-class words, number of
words, number of characters, number of com-
ponents).
To prove this, let Target-Components denote
those components (classifications and attribute-value
pairs) of Target that are mutually known by the
speaker and the hearer. For each Ti in Target-
Components, let Rules-Out(T) be the members of
Excluded that do not possess Ti (so, the presence of
Ti in a referring expression &apos;rules out&apos; these
members). Then, consider a potential referring
expression, RE = (C1 ..... C„ ). RE will be a suc-
cessful referring expression if and only if
</bodyText>
<listItem confidence="0.498392">
a) Every Ci is in Target-Components
b) The union of Rules-Our(C;), for all Ci in RE, is
equal to Excluded.
</listItem>
<bodyText confidence="0.9998475">
For example, if the task was referring to object
B in the example context of Section 2, then Target-
Components would be [ class:Chair, Material:Wood,
Color:Brown), Excluded would be (A, C), and
</bodyText>
<equation confidence="0.977494">
Rutes-Out(class:Chair)= (A)
Rules-Out(Material:Wood)= empty set
Rules-Out(Color:Brown)= (C)
</equation>
<bodyText confidence="0.997867388888889">
Therefore, (class:Chair, Color:Brown) (i.e., &amp;quot;the
brown chair&amp;quot;) would be a successful referring
expression for object B in this context.
If description length is measured by number of
components,7 finding the minimal length referring
expression is equivalent to solving a minimum set
cover problem, where Excluded is the set being
covered, and the Rules-Out(Ti) are the covering sets.
Unfortunately, finding a minimal set cover is an NP-
Dale&apos;s (1989) minimal distinguishing descriptions are,
in the terminology of this paper, successful referring expres-
sions that are maximal under Full 13revity when number of
components is used as the measure of description length.
Therefore, finding a minimal distinguishing description is an
NP-Hard problem. The algorithm Dale used was essentially
equivalent to the greedy heuristic for minimal set cover
(Johnson 1974); as such it ran quickly, but did not always
find a true minimal distinguishing description.
</bodyText>
<page confidence="0.964389">
100
</page>
<bodyText confidence="0.9996203">
Hard problem (Garey and Johnson 1979), and thus
solving it is in general computationally intractable
(assuming that P *NP).
Similar proofs will work for the other
definitions of length mentioned above. On an intui-
tive level, the basic problem is that finding the shor-
test description requires searching for the global
minimum of the length function, and this global
minimum (like many global minima) may be very
expensive to locate.
</bodyText>
<subsectionHeader confidence="0.987445">
4.2. Local Brevity
</subsectionHeader>
<bodyText confidence="0.999850347826087">
The Local Brevity preference rule is a weaker
interpretation of Griee&apos;s brevity submaxim. It states
that it should not be possible to generate a shorter
successful referring expression by replacing a set of
components by a single new component Formally,
&gt;&gt;Lx is the transitive closure of &gt;&gt;/jr, where A »LB,
B if size(components(A)-components(B)) = 1,8 and
length(A) &lt; length(B). The best definition of
length(A) is probably the number of open-class
words in the surface realization of A.
Local brevity can be checked by selecting a
potential new component, finding all minimal sets of
old components whose combined length is greater
than the length of the new component, performing
the substitution, and checking if the result is a suc-
cessful referring expression. This can be done in
polynomial time if the number of minimal sets is
polynomial in the length of the description, which
will happen if (non-zero) upper and lower bounds are
placed on the length of any individual component
(e.g., the surface realization of every component
must use at least one open-class word, but no more
than some fixed number of open-class words).
</bodyText>
<subsectionHeader confidence="0.878627">
S. No Unnecessary Elements
</subsectionHeader>
<bodyText confidence="0.834840214285714">
The Grimm maxims of Quantity and
Relevance prohibit utterances from containing ele-
ments that are unnecessary for fulfilling the speaker&apos;s
communicative goals. The undesirability of unneces-
sary elements is further supported by the observation
that humans find pleonasms (Cruse 1986) such as &amp;quot;a
female mother&amp;quot; and &amp;quot;an unmarried bachelor&amp;quot; to be
anomalous. The computational tractability of the
no-unnecessary-elements principle depends on how
8 This is a set formula, where &amp;quot;-• means set-difference
and &amp;quot;size&amp;quot; means number of members. The formula requires
A to have exactly ate component that is not present in B
can have an arbitrary number of components that are not
present in A.
</bodyText>
<page confidence="0.98643">
101
</page>
<bodyText confidence="0.99942675">
element is defined: detecting unnecessary words in
referring expressions is NP-Hard (Section 5.1), but
unnecessary components can always be found in
polynomial time (Section 5.2).
</bodyText>
<subsectionHeader confidence="0.93213">
5.1. No Unnecessary Words
</subsectionHeader>
<bodyText confidence="0.99972856">
The No Unnecessary Words preference rule
forbids referring expressions from containing
unnecessary words: Formally, A &gt;D. uw B if A&apos;s sur-
face form uses a subset of the words used by B&apos;s sur-
face form. There are several variants, such as only
considering open-class words, or requiring the words
in B to be in the same order as the corresponding
words in A. All of these variants make the genera-
tion problem NP-Hard.
The formal proofs are in Reiter (1990b). Intui-
tively, the basic problem is that any preference that is
stated solely in terms of surface forms must deal with
the possibility that new parses and semantic interpre-
tations may arise when the surface form is modified.
This means that the only way a generation system
can guarantee that an utterance satisfies the No
Unnecessary Words rule is to generate all possible
subsets of the surface form, and then run each subset
through a parser and semantic interpreter to check if
it happens to be a successful referring expression.
The number of subsets of the surface form is
exponential in the size of the surface form, so this
process will take exponential time.
To illustrate the &apos;new parse&apos; problem, consider
two possible referring expressions:
</bodyText>
<listItem confidence="0.8483945">
4a) &amp;quot;the child holding a pumpkin&amp;quot;
4b) &amp;quot;the child holding a slice of pumpkin pie&amp;quot;
</listItem>
<bodyText confidence="0.999079823529411">
If utterances (4a) and (4b) were both successful
referring expressions (i.e., the child had a pumpkin in
one hand, and a slice of pumpkin pie in the other),
then (4a) xr.vw (4b) under any of the variants men-
tioned above. However, because utterance (4a) has a
different syntactic structure than utterance (4b), the
only way the generation system could discover that
(4a) »uw (4b) would be by constructing utterance
(4b)&apos;s surface form, removing the words &amp;quot;slice,&amp;quot;
&amp;quot;of,&amp;quot; and &amp;quot;pie&amp;quot; from it, and analyzing the reduced
surface form.
This problem, of new parses and semantic
interpretations being uncovered by modifications to
the surface form, causes difficulties whenever a
preference rule is stated solely in terms of the surface
form. Accordingly, such preference rules should be
avoided.
</bodyText>
<subsectionHeader confidence="0.96482">
5.2. No Unnecessary Components
</subsectionHeader>
<bodyText confidence="0.9999404">
The No Unnecessary Components preference
rule forbids referring expressions from containing
unnecessary components. Formally, A x.uc. B if A
uses a a subset of the components used by B.
Unnecessary components can be found in poly-
nomial time by using a simple incremental algorithm
that just removes each component in turn, and checks
if what is left constitutes a successful referring
expression.
The key algorithmic difference between No
Unnecessary Components and No Unnecessary
Words is that this simple incremental algorithm will
not work for the No Unnecessary Words preference
rule. This is because there are cases where removing
any single word from an utterance&apos;s surface form
will leave an unsuccessful (or incoherent) referring
expression (e.g., imagine removing just &amp;quot;slice&amp;quot; from
utterance (4b)), but removing several words will
uncover a new parse that corresponds to a successful
referring expression. In contrast, if B is a successful
referring expression, and there exists another suc-
cessful referring expression A that satisfies
components(A) c components(B) (and hence A is
preferred over B under the No Unnecessary Com-
ponents preference rule), then it will be the case that
any referring expression C that satisfies
components(A) c components(C) components(B)
will also be successful. This means that the simple
algorithm can always produce A from B by incre-
mental steps that remove a single component at a
time, because the intermediate descriptions formed in
this process will always be successful referring
expressions. Therefore, the simple incremental algo-
rithm will always find unnecessary components, but
may not always find unnecessary words.
</bodyText>
<sectionHeader confidence="0.946349" genericHeader="method">
6. Lexical Preference
</sectionHeader>
<bodyText confidence="0.999541105263158">
If the attribute values and classifications used
in the description are members of a taxonomy, then
they can be realized at different levels of specificity.
For example, the object in the parking lot outside the
author&apos;s window might be called &amp;quot;a vehicle,&amp;quot; &amp;quot;a
MOW vehicle,&amp;quot; &amp;quot;a car,&amp;quot; &amp;quot;a sports car,&amp;quot; or &amp;quot;a
Porsche.&amp;quot;
The Lexical Preference nide assumes there is a
lexical-preference hierarchy among the taxonomy&apos;s
lexical classes (classes that can be realized with sin-
gle lexical units). The rule states that utterances
should use preferred lexical classes whenever possi-
ble. Formally, A &gt;&gt;1, B if for every component in A,
that is a component in B that has the same structure,
and the lexical class used by the A component is
equal to or lexically preferred over the lexical class
used by the B componeni
The lexical-preference hierarchy should, at
minimum, incorporate the following preferences:
</bodyText>
<sectionHeader confidence="0.504324" genericHeader="method">
1) Lexical class A is preferred over lexical class
</sectionHeader>
<bodyText confidence="0.962279578947368">
B if A&apos;s realization uses a subset of the open-
class words used in B&apos;s realization. For exam-
ple, the class with realization &amp;quot;vehicle&amp;quot; is pre-
ferred over the class with realization &amp;quot;motor
vehicle.&amp;quot;
ii) Lexical class A is preferred over lexical class
B if A is a basic-level class, and B is not. For
example, if car was a basic-level class, then &amp;quot;a
car&amp;quot; would be preferred over &amp;quot;a vehicle&amp;quot; or &amp;quot;a
Porsche. &amp;quot;9
In some cases these two preferences may conflict:
this is discussed in Section 7.2.
Utterances that violate either preference (i) or
preference (ii) may implicate unwanted implicatura.
Preference rule (ii) has been discussed by Cruse
(1977) and Hirschberg (1985). Preference rule (it)
may be considered to be another application of the
Gricean maxim of quantity, and is illustrated by the
following utterances:
</bodyText>
<listItem confidence="0.8945315">
5a) &amp;quot;Wait for me by my car&amp;quot;
5b) &amp;quot;Wait for me by my sports car&amp;quot;
</listItem>
<bodyText confidence="0.999777272727273">
If utterances (5a) and (5b) were both successful
referring expressions (e.g., if the speaker possessed
only one car), then the use of utterance (5b) would
implicate that the speaker wished to emphasize that
his vehicle was a sports car, and not some other kind
of car.
From an algorithmic point of view, referring
expressions that are maximal under the lexical-
preference criteria can be found in polynomial time if
the following restriction is imposed on the lexical-
preference hierarchy:
</bodyText>
<sectionHeader confidence="0.385443" genericHeader="method">
Restriction:
</sectionHeader>
<bodyText confidence="0.9955415">
If lexical class A is preferred over lexical class
B, then A must either subsume B or be sub-
sumed by B in the class taxonomy.
For example, it is acceptable for car to be preferred
over vehicle or Porsche, but it is not acceptable for
car to be preferred over gift (because car neither sub-
</bodyText>
<page confidence="0.997356">
102
</page>
<bodyText confidence="0.9888017">
sumes nor is subsumed by at).
If the above restriction holds, a variant of the
simple incremental algorithm of Section 5.2 may be
used to implement lexical preference: the algorithm
simply attempts each replacement that lexical prefer-
ence suggests, and checks if this results in a success-
ful referring expression. If the restriction does not
hold, then the simple incremental algorithm may fail,
and obeying the Lexical Preference rule is in fact
NP-Hard (the formal proof is in Reiter (1990b)).
</bodyText>
<sectionHeader confidence="0.968033" genericHeader="evaluation">
7. Issues
</sectionHeader>
<subsectionHeader confidence="0.938916">
7.1. The Impact of NP-Hard Preference Rules
</subsectionHeader>
<bodyText confidence="0.980247076923077">
It is difficult to precisely determine the compu-
tational expense of generating referring expressions
that are maximal under the Full Brevity or No
Unnecessary Words preference rules. The most
straightforward algorithm that obeys Full Brevity (a
similar analysis can be done for No Unnecessary
Words) simply does an exhaustive search: it first
checks if any one-component referring expression is
successful, then checks if any two-component refer-
ring expression is successful, and so forth. Let L be
the number of components in the shortest referring
expression, and let N be the number of components
that are potentially useful in a description. Le., the
number of members of Target-Components that rule
out at least one member of Excluded. The straight-
forward full-brevity algorithm will then need to
examine the following number of descriptions before
it finds a successful referring expression:
N!
07-0!
For the problem of generating a retailing expression
that identifies object B in the example context
presented in Section 2, N is 3 and L is 2, so the
straightforward brevity algorithm will take only 6
steps to find the shortest description. This problem is
artificially simple, however, because 14, the number
of potential description components, is so small. In a
more realistic problem, one would expect Target-
Components to include size, shape, orientation, posi-
tion, and probably many other attribute-value pairs as
well, which would mean that N would probably be at
least 10 or 20. L, the number of attributes in the
shortest possible referring expression, is probably
fairly small in most realistic situations, but there are
cases where it might be at least 3 or 4 (e.g., consider
&amp;quot;the upside-down blue cup on the second shelf&amp;quot;).
For some example values of L and N in this range,
the straightforward brevity algorithm will need to
examine the following number of descriptions:
</bodyText>
<equation confidence="0.646012666666667">
L = 3, N = 10; 175 descriptions
L =4, N 20; over 6000 descriptions
L =5, N = 50; over 2,000,000 descriptions
</equation>
<bodyText confidence="0.999967071428571">
The straightforward full-brevity algorithm,
then, seems prohibitively expensive in at least some
circumstances. Because finding the shortest descrip-
tion is NP-Hard, it seems likely (existing
complexity-theoretic techniques are too weak to
prove such statements) that all algorithms for finding
the shortest description will have similarly bad per-
formance in the worst case. It is possible, however,
that there exist algorithms that have acceptable per-
formance in almost all &apos;realistic&apos; cases. Any such
proposed algorithm, however, should be carefully
analyzed to determine in what circumstances it will
fail to find the shortest description or will take
exponential time to run.
</bodyText>
<subsectionHeader confidence="0.981591">
7.2. Conflicts Between Preference Rules
</subsectionHeader>
<bodyText confidence="0.999949636363636">
The assumption has been made in this paper
that the preference rules do not conflict, i.e., that it is
never the case that description A is preferred over
description B by one preference rule, while descrip-
tion B is preferred over description A by another
preference rule. This means, in particular, that if lex-
ical class LC1 is preferred over lexical class LC2,
then LC i&apos;s realization must not contain more open-
class words than LC2&apos;s realization; otherwise, the
Lexical Preference and Local Brevity preference
rules may contlict.10 This can be supported by
psychological and linguistic findings that basic-level
classes are almost always realized with single words
(Rosch 1978; Berlin, Breedlove, and Raven 1973).
However, there are a few exceptions to this rule, i.e.,
there do exist a small number of basic-level
categories that have realizations that require more
than one open-class word. For example, Washing-
Machine is a basic-level class for some people, and it
has a realization that uses two open-class words.
This leads to a conflict of the type mentioned above:
basic-level Washing-Machine is preferred over non-
</bodyText>
<footnote confidence="0.244586375">
10 This assumes that the Local Brevity preference rule
USN number of open-class words as its measure of descrip-
tion length. If number of components or number of lexical
units is used as the measure of description length, then Local
Brevity will never conflict with Lexical Preference.
No other conflicts can occur between the No Unneces-
sary Components, Local Brevity, and Lexical Preference
preference rules.
</footnote>
<page confidence="0.99273">
103
</page>
<bodyText confidence="0.998988888888889">
basic-level Appliance, but Washing-Machine&apos;s reali-
zation contains more open-class words than
Appliance&apos;s.
The presence of a basic-level class with a
multi-word realization can also cause a conflict to
occur between the two lexical-preference principles
given in Section 6 (such conflicts are otherwise
impossible). For example, Washing-Machine&apos;s reali-
zation contains a superset of the open-class words
used in the realization of Machine, so the basic-level
preference of Section 6 indicates that Washing-
Machine should be lexically preferred over Machine,
while the realization-subset preference indicates that
Machine should be lexically preferred over
Washing-Machine. The basic-level preference
should take priority in such cases, so Washing-
Machine is the true lexically-preferred class in this
example.
</bodyText>
<sectionHeader confidence="0.479331" genericHeader="conclusions">
73. Generalizability of Results
</sectionHeader>
<bodyText confidence="0.987886153846154">
For the task of generating attributive descrip-
tions as formalized in Reiter (1990a, 1990b), the
Local Brevity, No Unnecessary Components, and
Lexical Preference rules are effective at prohibiting
utterances that carry unwanted conversational impli-
catures, and also can be incorporated into a
polynomial-time generation algorithm, provided that
some restrictions are imposed on the underlying
knowledge base. The effectiveness and tractability
of these preference rules for other generation tasks is
an open problem that requires further investigation.
The Full Brevity and No Unnecessary Words
preference rules are computationally intractable for
the attributive description generation task (Reiter
1990b), and it seems likely that they will be intract-
able for most other generation tasks as well. Because
global maxima are usually expensive to locate,
finding the shortest acceptable utterance will prob-
ably be computationally expensive for most genera-
tion tasks. Because the &apos;new parse&apos; problem arises
whenever the preference function is stated solely in
terms of the surface form, detecting unnecessary
words will also probably be quite expensive in most
situations.
8. Conclusion
Referring expressions and other object descrip-
tions need to be brief, to avoid unnecessary elements,
and to use lexically preferred classes; otherwise, they
may carry unwanted and incorrect conversational
implicatures. These principles can be formalized by
requiring referring expressions to be maximal under
the Local Brevity, No Unnecessary Components, and 104
Lexical Preference preference rules. These prefer-
ence rules can be incorporated into a polynomial-
time algorithm for generating free-of-false-
implicatures referring expressions, while some alter-
native preference rules (Full Brevity and No
Unnecessary Words) make this generation task NP-
Hard.
</bodyText>
<sectionHeader confidence="0.997611" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<reference confidence="0.917756111111111">
Many thanks to Robert Dale, Joyce Friedman, Barbara
Grosz, Joe Marks, Warren Rath, Candy Sidner, Jeff Sislcind, Bill
Woods, and the anonymous reviewers for their help and sugges-
tions. This work was partially supported by a National Science
Foundation Graduate Fellowship, an IBM Graduate Fellowship,
and a contract from U S WEST Advanced Technologies. Any
opinions, findings, conclusions, or recommendations are those of
the author and do not necessarily reflect the views of the National
Science Foundation, IBM, or U S WEST Advanced Technologies.
</reference>
<sectionHeader confidence="0.682318" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999862384615385">
Appelt, D. 1985 Planning English Referring Expressions. Cam-
bridge University Press: New York.
Berlin, B.; Breedlove, D,; and Raven, P. 1973 General Principles
of Classification and Nomenclature in Folk Biology. Amer-
ican Anthropologist 75:214-242.
Cruse, D. 1977 The pragmatics of lexical specificity. Journal of
Linguistics 13:153-164,
Cruse, D. 1986 Lexical Semantics. Cambridge University Press:
New York.
Dale, R. 1989 Cocking up Referring Expressions. In Proceedings
of the 27111 Annual Meeting of the Association for Compu-
tational Linguistics.
Dconellan, K 1966 Reference and Definite Descriptions. Philo-
sophical Review 75:281-304. •
Garey, M. and Johnson, D. 1979 Computers and Intractability: a
Guide to the Theory of NP-Completeness. W. H. Freeman:
San Francisco.
Grice, H. 1975 Logic and conversation. In P. Cole and J. Morgan
(Eds.), Syntax and Semantics: Vol 3, Speech Acts, pg 43-
58. Academic Press: New York.
Hirschberg, J. 1985 A Theory of Scalar Impikature. Report MS-
CIS-85-56, LINC LAB 21. Department of Computer and
Infomiation Science, University of Pennsylvania.
Johnson, D. 1974 Approximation algorithms for combinatorial
problems. Journal of Computer and Systems Sciences
9:256-178.
Kamp, H. (1975) Two Theories about Adjectives. In E. Keenan
(Ed.) Formal Semantics of Natural Language, pg 123-155.
Cambridge University Press: New York.
Reiter, E. 1990a Generating Descriptions that Exploit a User&apos;s
Domain Knowledge. To appear in R. Dale, C. Mellish, and
M. Zack (Eds.), Current Research in Natural Language
Generation. Academic Press: New York.
Reiter, E. 1990b Generating Appropriate Natural Language
Object Descriptions. PhD thesis. Aiken Computation Lab,
Harvard University: Cambridge, Mass.
Rosch, E. 1978 Principles of Categorization. In E. Rosch and B.
Lloyd (Eds.), Cognition and Categorization. Lawrence Erl-
baurn: Hillsdale, NJ.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.865853">
<title confidence="0.9945335">THE COMPUTATIONAL COMPLEXITY OF AVOIDING CONVERSATIONAL IMPLICATURES</title>
<author confidence="0.892642">Ehud Reitert</author>
<affiliation confidence="0.990568">Aiken Computation Lab Harvard University</affiliation>
<address confidence="0.999964">Cambridge, Mass 02138</address>
<abstract confidence="0.9949744">Referring expressions and other object descriptions should be maximal under the Local Brevity, No Unnecessary Components, and Lexical Preference preference rules; otherwise, they may lead hearers to infer unwanted conversational implicatures. These be incorporated into a polynomial time generation algorithm, while some alternative formalizations of conversational implicature make the generation task NP-Hard.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Many thanks to Robert Dale</author>
<author>Joyce Friedman</author>
<author>Barbara Grosz</author>
<author>Joe Marks</author>
<author>Warren Rath</author>
<author>Candy Sidner</author>
<author>Jeff Sislcind</author>
<author>Bill Woods</author>
</authors>
<title>and the anonymous reviewers for their help and suggestions. This work was partially supported by a National Science Foundation Graduate Fellowship, an IBM Graduate Fellowship, and a contract from U S WEST Advanced Technologies. Any opinions, findings, conclusions, or recommendations are those of the author and do not necessarily reflect the views of the National Science Foundation,</title>
<journal>IBM, or U S WEST Advanced Technologies.</journal>
<marker>Dale, Friedman, Grosz, Marks, Rath, Sidner, Sislcind, Woods, </marker>
<rawString>Many thanks to Robert Dale, Joyce Friedman, Barbara Grosz, Joe Marks, Warren Rath, Candy Sidner, Jeff Sislcind, Bill Woods, and the anonymous reviewers for their help and suggestions. This work was partially supported by a National Science Foundation Graduate Fellowship, an IBM Graduate Fellowship, and a contract from U S WEST Advanced Technologies. Any opinions, findings, conclusions, or recommendations are those of the author and do not necessarily reflect the views of the National Science Foundation, IBM, or U S WEST Advanced Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Appelt</author>
</authors>
<title>Planning English Referring Expressions.</title>
<date>1985</date>
<publisher>Cambridge University Press:</publisher>
<location>New York.</location>
<contexts>
<context position="7659" citStr="Appelt (1985)" startWordPosition="1158" endWordPosition="1159">Their Interpretation Once (1975) proposed four maxims of conversation that speakers needed to obey: Quality, Quantity, Relevance, and Manner. For the task of generating referring expressions as formalized in Section 2, these maxims can be interpreted as follows: Quality: The Quality maxim requires utterances to be truthful. In this context, it requires referring expressions to be factual descriptions of the referred-to object. This condition is already part of the definition of a successful referring expression, and does not need to be restated as a conversational 98 implicature constraint. 2 Appelt (1985) presented a more complex referringexpression model that covered situations where the hearer was not already aware of the referred-to object, and that allowed the speaker to have more complex communicative goals. A similar analysis to the one presented in this per could in principle be done for Appeles model, but it would be substantially more difficuh, in part because the model is more complex, and in part because Appelt did not separate his &apos;content determination&apos; subsystem from his planner and his surface-form generator. 3 All attributes are assumed to be predicative (Kamp 1975). 4 Dale als</context>
<context position="14800" citStr="Appelt 1985" startWordPosition="2310" endWordPosition="2311"> 5, and 6 examine various preference rules that might plausibly be used to formalize these implicatures, and reject preference rules that make the generation task NP-Hard. This is justified on the grounds that computer NLG systems should not be asked to solve NP-Hard probierns.6 Human speakers and hearers are also probably not very proficient at solving NP-Hard problems, which suggests that it is unlikely that NP-Hard preference rules have been incorporated into language. 4. Brevity Grice&apos;s submaxim of brevity states that utterances should be kept brief. Many NLG researchers (e.g., Dale 1989: Appelt 1985: pages 117-118) have suggested that this means generation systems need to produce the shortest possible utterance. This will be called the Full Brevity preference rule. Unfortunately, it is NP-Hard to find the shortest successful referring expression (Section 4.1). Local Brevity (Section 4.2) is a weaker version of the brevity submaxim that can be incorporated into a polynomialtime algorithm for generating successful referring expressions. 5 Section 7.2 discusses this assumption. 6 Section 7.1 discusses the computational impact of NPHard preference rules. 4.1. Full Brevity The Full Brevity pr</context>
</contexts>
<marker>Appelt, 1985</marker>
<rawString>Appelt, D. 1985 Planning English Referring Expressions. Cambridge University Press: New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Berlin</author>
<author>D Breedlove</author>
</authors>
<date>1973</date>
<booktitle>General Principles of Classification and Nomenclature in Folk Biology. American Anthropologist</booktitle>
<pages>75--214</pages>
<marker>Berlin, Breedlove, 1973</marker>
<rawString>Berlin, B.; Breedlove, D,; and Raven, P. 1973 General Principles of Classification and Nomenclature in Folk Biology. American Anthropologist 75:214-242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cruse</author>
</authors>
<title>The pragmatics of lexical specificity.</title>
<date>1977</date>
<journal>Journal of Linguistics</journal>
<pages>13--153</pages>
<contexts>
<context position="9831" citStr="Cruse (1977)" startWordPosition="1499" endWordPosition="1500"> in the discourse context. Irrelevant elements are also unnecessary elements, so the Relevance maxim may be considered to be a special case of the Quantity maxim, at least for the referring-expression generation task as formalized in Section 2. Manner: The Brevity submaxim of the Manner maxim requires a speaker to use short utterances if possible. In this context it requires the speaker to use a short referring expression if such a referring expression exists. The analysis of the other Manner submaxims is left for future work. An additional source of conversational implicature was proposed by Cruse (1977) and Hirschberg (1985), who hypothesized that- implicatures might arise from the failure to use basic-level classes (Rosch 1978) in an utterance. In this paper, such implicatures are generalized by assuming that there is a lexical-preference hierarchy among the lexical classes (classes that can be realized with single lexical units) known to the hearer, and that the use of a lexical class in an utterance implicates that no preferred lexical class could have been used in its place. In summary, conversational implicauire considerations require referring expressions to be brief, to not contain un</context>
<context position="25545" citStr="Cruse (1977)" startWordPosition="4016" endWordPosition="4017">et of the openclass words used in B&apos;s realization. For example, the class with realization &amp;quot;vehicle&amp;quot; is preferred over the class with realization &amp;quot;motor vehicle.&amp;quot; ii) Lexical class A is preferred over lexical class B if A is a basic-level class, and B is not. For example, if car was a basic-level class, then &amp;quot;a car&amp;quot; would be preferred over &amp;quot;a vehicle&amp;quot; or &amp;quot;a Porsche. &amp;quot;9 In some cases these two preferences may conflict: this is discussed in Section 7.2. Utterances that violate either preference (i) or preference (ii) may implicate unwanted implicatura. Preference rule (ii) has been discussed by Cruse (1977) and Hirschberg (1985). Preference rule (it) may be considered to be another application of the Gricean maxim of quantity, and is illustrated by the following utterances: 5a) &amp;quot;Wait for me by my car&amp;quot; 5b) &amp;quot;Wait for me by my sports car&amp;quot; If utterances (5a) and (5b) were both successful referring expressions (e.g., if the speaker possessed only one car), then the use of utterance (5b) would implicate that the speaker wished to emphasize that his vehicle was a sports car, and not some other kind of car. From an algorithmic point of view, referring expressions that are maximal under the lexicalprefer</context>
</contexts>
<marker>Cruse, 1977</marker>
<rawString>Cruse, D. 1977 The pragmatics of lexical specificity. Journal of Linguistics 13:153-164,</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cruse</author>
</authors>
<title>Lexical Semantics.</title>
<date>1986</date>
<publisher>Cambridge University Press:</publisher>
<location>New York.</location>
<contexts>
<context position="19549" citStr="Cruse 1986" startWordPosition="3049" endWordPosition="3050"> is polynomial in the length of the description, which will happen if (non-zero) upper and lower bounds are placed on the length of any individual component (e.g., the surface realization of every component must use at least one open-class word, but no more than some fixed number of open-class words). S. No Unnecessary Elements The Grimm maxims of Quantity and Relevance prohibit utterances from containing elements that are unnecessary for fulfilling the speaker&apos;s communicative goals. The undesirability of unnecessary elements is further supported by the observation that humans find pleonasms (Cruse 1986) such as &amp;quot;a female mother&amp;quot; and &amp;quot;an unmarried bachelor&amp;quot; to be anomalous. The computational tractability of the no-unnecessary-elements principle depends on how 8 This is a set formula, where &amp;quot;-• means set-difference and &amp;quot;size&amp;quot; means number of members. The formula requires A to have exactly ate component that is not present in B can have an arbitrary number of components that are not present in A. 101 element is defined: detecting unnecessary words in referring expressions is NP-Hard (Section 5.1), but unnecessary components can always be found in polynomial time (Section 5.2). 5.1. No Unnecessa</context>
</contexts>
<marker>Cruse, 1986</marker>
<rawString>Cruse, D. 1986 Lexical Semantics. Cambridge University Press: New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
</authors>
<title>Cocking up Referring Expressions.</title>
<date>1989</date>
<booktitle>In Proceedings of the 27111 Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="14787" citStr="Dale 1989" startWordPosition="2308" endWordPosition="2309">Sections 4, 5, and 6 examine various preference rules that might plausibly be used to formalize these implicatures, and reject preference rules that make the generation task NP-Hard. This is justified on the grounds that computer NLG systems should not be asked to solve NP-Hard probierns.6 Human speakers and hearers are also probably not very proficient at solving NP-Hard problems, which suggests that it is unlikely that NP-Hard preference rules have been incorporated into language. 4. Brevity Grice&apos;s submaxim of brevity states that utterances should be kept brief. Many NLG researchers (e.g., Dale 1989: Appelt 1985: pages 117-118) have suggested that this means generation systems need to produce the shortest possible utterance. This will be called the Full Brevity preference rule. Unfortunately, it is NP-Hard to find the shortest successful referring expression (Section 4.1). Local Brevity (Section 4.2) is a weaker version of the brevity submaxim that can be incorporated into a polynomialtime algorithm for generating successful referring expressions. 5 Section 7.2 discusses this assumption. 6 Section 7.1 discusses the computational impact of NPHard preference rules. 4.1. Full Brevity The Fu</context>
</contexts>
<marker>Dale, 1989</marker>
<rawString>Dale, R. 1989 Cocking up Referring Expressions. In Proceedings of the 27111 Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Dconellan</author>
</authors>
<title>Reference and Definite Descriptions. Philosophical Review 75:281-304.</title>
<date>1966</date>
<publisher></publisher>
<marker>Dconellan, 1966</marker>
<rawString>Dconellan, K 1966 Reference and Definite Descriptions. Philosophical Review 75:281-304. •</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Garey</author>
<author>D Johnson</author>
</authors>
<title>Computers and Intractability: a Guide to the Theory of NP-Completeness.</title>
<date>1979</date>
<contexts>
<context position="17692" citStr="Garey and Johnson 1979" startWordPosition="2752" endWordPosition="2755">e covering sets. Unfortunately, finding a minimal set cover is an NPDale&apos;s (1989) minimal distinguishing descriptions are, in the terminology of this paper, successful referring expressions that are maximal under Full 13revity when number of components is used as the measure of description length. Therefore, finding a minimal distinguishing description is an NP-Hard problem. The algorithm Dale used was essentially equivalent to the greedy heuristic for minimal set cover (Johnson 1974); as such it ran quickly, but did not always find a true minimal distinguishing description. 100 Hard problem (Garey and Johnson 1979), and thus solving it is in general computationally intractable (assuming that P *NP). Similar proofs will work for the other definitions of length mentioned above. On an intuitive level, the basic problem is that finding the shortest description requires searching for the global minimum of the length function, and this global minimum (like many global minima) may be very expensive to locate. 4.2. Local Brevity The Local Brevity preference rule is a weaker interpretation of Griee&apos;s brevity submaxim. It states that it should not be possible to generate a shorter successful referring expression </context>
</contexts>
<marker>Garey, Johnson, 1979</marker>
<rawString>Garey, M. and Johnson, D. 1979 Computers and Intractability: a Guide to the Theory of NP-Completeness. W. H. Freeman: San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Grice</author>
</authors>
<title>Logic and conversation.</title>
<date>1975</date>
<publisher>Academic Press:</publisher>
<location>New York.</location>
<contexts>
<context position="869" citStr="Grice 1975" startWordPosition="113" endWordPosition="114">essary Components, and Lexical Preference preference rules; otherwise, they may lead hearers to infer unwanted conversational implicatures. These preference rules can be incorporated into a polynomial time generation algorithm, while some alternative formalizations of conversational implicature make the generation task NP-Hard. 1. Introduction Natural language generation (NLG) systems should produce referring expressions and other object descriptions that are free of false implicatures, i.e., that do not cause the user of the system to infer incorrect and unwanted conversational implicaatres (Grice 1975). The following utterances illustrate referring expressions that are and are not free of false implicatures: la) &amp;quot;Sit by the table&amp;quot; Ib) &amp;quot;Sit by the brown wooden table&amp;quot; In a context where only one table was visible, and this table was brown and made of wood, utterances (la) and (lb) would both fulfill the referring goal: a hearer who heard either utterance would have no trouble picking out the object being referred to. However, a hearer who heard utterance (lb) would probably assume that it was somehow important that the table was brown and made of wood, i.e., that the speaker was trying to do </context>
</contexts>
<marker>Grice, 1975</marker>
<rawString>Grice, H. 1975 Logic and conversation. In P. Cole and J. Morgan (Eds.), Syntax and Semantics: Vol 3, Speech Acts, pg 43-58. Academic Press: New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hirschberg</author>
</authors>
<title>A Theory of Scalar Impikature.</title>
<date>1985</date>
<tech>Report MSCIS-85-56, LINC LAB 21.</tech>
<institution>Department of Computer and Infomiation Science, University of Pennsylvania.</institution>
<contexts>
<context position="9853" citStr="Hirschberg (1985)" startWordPosition="1502" endWordPosition="1503"> context. Irrelevant elements are also unnecessary elements, so the Relevance maxim may be considered to be a special case of the Quantity maxim, at least for the referring-expression generation task as formalized in Section 2. Manner: The Brevity submaxim of the Manner maxim requires a speaker to use short utterances if possible. In this context it requires the speaker to use a short referring expression if such a referring expression exists. The analysis of the other Manner submaxims is left for future work. An additional source of conversational implicature was proposed by Cruse (1977) and Hirschberg (1985), who hypothesized that- implicatures might arise from the failure to use basic-level classes (Rosch 1978) in an utterance. In this paper, such implicatures are generalized by assuming that there is a lexical-preference hierarchy among the lexical classes (classes that can be realized with single lexical units) known to the hearer, and that the use of a lexical class in an utterance implicates that no preferred lexical class could have been used in its place. In summary, conversational implicauire considerations require referring expressions to be brief, to not contain unnecessary elements, an</context>
<context position="13081" citStr="Hirschberg (1985)" startWordPosition="2026" endWordPosition="2027">ference function of the set of successful referring expressions. More formally, let D be the set of successful referring expressions, and let » be a preference function that prefers descriptions that are short, that do not contain unnecessary elements, and that use lexically preferred classes. Then, a referring expression is considered free of false implicatures if it is a maximal element of D with respect to ». In other words, a description B in D is free of false implicatures if there is no description A in D, such that A » B. This formalization is similar to the partially ordered sets that Hirschberg (1985) used to for99 malize scalar implicatures: D and &gt;&gt; together form a partially ordered set, and the assumption is that the use of an element in D carries the conversational implicature that no higher-ranked element in D could have been used. The overall preference function &gt;&gt; will be decomposed into separate preference rules that cover each type of implicature: &gt;r.:-B for brevity, »u for unnecessary elements, and ›,r, for lexical preference. &gt;&gt; is then defined as the disjunction of these preference rules, i.e., A&gt;&gt; B if A &gt;&gt;B B, A »u B, or A &gt;N. B. The assumption will be made in this paper that</context>
<context position="25567" citStr="Hirschberg (1985)" startWordPosition="4019" endWordPosition="4020">ss words used in B&apos;s realization. For example, the class with realization &amp;quot;vehicle&amp;quot; is preferred over the class with realization &amp;quot;motor vehicle.&amp;quot; ii) Lexical class A is preferred over lexical class B if A is a basic-level class, and B is not. For example, if car was a basic-level class, then &amp;quot;a car&amp;quot; would be preferred over &amp;quot;a vehicle&amp;quot; or &amp;quot;a Porsche. &amp;quot;9 In some cases these two preferences may conflict: this is discussed in Section 7.2. Utterances that violate either preference (i) or preference (ii) may implicate unwanted implicatura. Preference rule (ii) has been discussed by Cruse (1977) and Hirschberg (1985). Preference rule (it) may be considered to be another application of the Gricean maxim of quantity, and is illustrated by the following utterances: 5a) &amp;quot;Wait for me by my car&amp;quot; 5b) &amp;quot;Wait for me by my sports car&amp;quot; If utterances (5a) and (5b) were both successful referring expressions (e.g., if the speaker possessed only one car), then the use of utterance (5b) would implicate that the speaker wished to emphasize that his vehicle was a sports car, and not some other kind of car. From an algorithmic point of view, referring expressions that are maximal under the lexicalpreference criteria can be f</context>
</contexts>
<marker>Hirschberg, 1985</marker>
<rawString>Hirschberg, J. 1985 A Theory of Scalar Impikature. Report MSCIS-85-56, LINC LAB 21. Department of Computer and Infomiation Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Johnson</author>
</authors>
<title>Approximation algorithms for combinatorial problems.</title>
<date>1974</date>
<journal>Journal of Computer and Systems Sciences</journal>
<pages>9--256</pages>
<contexts>
<context position="17558" citStr="Johnson 1974" startWordPosition="2732" endWordPosition="2733"> is equivalent to solving a minimum set cover problem, where Excluded is the set being covered, and the Rules-Out(Ti) are the covering sets. Unfortunately, finding a minimal set cover is an NPDale&apos;s (1989) minimal distinguishing descriptions are, in the terminology of this paper, successful referring expressions that are maximal under Full 13revity when number of components is used as the measure of description length. Therefore, finding a minimal distinguishing description is an NP-Hard problem. The algorithm Dale used was essentially equivalent to the greedy heuristic for minimal set cover (Johnson 1974); as such it ran quickly, but did not always find a true minimal distinguishing description. 100 Hard problem (Garey and Johnson 1979), and thus solving it is in general computationally intractable (assuming that P *NP). Similar proofs will work for the other definitions of length mentioned above. On an intuitive level, the basic problem is that finding the shortest description requires searching for the global minimum of the length function, and this global minimum (like many global minima) may be very expensive to locate. 4.2. Local Brevity The Local Brevity preference rule is a weaker inter</context>
</contexts>
<marker>Johnson, 1974</marker>
<rawString>Johnson, D. 1974 Approximation algorithms for combinatorial problems. Journal of Computer and Systems Sciences 9:256-178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kamp</author>
</authors>
<title>Two Theories about Adjectives. In E. Keenan (Ed.) Formal Semantics of Natural Language, pg 123-155.</title>
<date>1975</date>
<publisher>Cambridge University Press:</publisher>
<location>New York.</location>
<contexts>
<context position="8247" citStr="Kamp 1975" startWordPosition="1253" endWordPosition="1254">raint. 2 Appelt (1985) presented a more complex referringexpression model that covered situations where the hearer was not already aware of the referred-to object, and that allowed the speaker to have more complex communicative goals. A similar analysis to the one presented in this per could in principle be done for Appeles model, but it would be substantially more difficuh, in part because the model is more complex, and in part because Appelt did not separate his &apos;content determination&apos; subsystem from his planner and his surface-form generator. 3 All attributes are assumed to be predicative (Kamp 1975). 4 Dale also suggested that NI..G systems should choose distinguishing descriptions of minimal cardinally; this is discussed in footnote 7. Quantity: The Quality maxim requires utterances to contain enough information to fulfill the speaker&apos;s communicative goal, but not more information. In this context, it requires referring expressions to contain enough information to enable the hearer to identify the referred-to object, but not more information. Therefore, referring expressions should be successful (as defined in Section 2), but should not contain additional elements that are unnecessary f</context>
</contexts>
<marker>Kamp, 1975</marker>
<rawString>Kamp, H. (1975) Two Theories about Adjectives. In E. Keenan (Ed.) Formal Semantics of Natural Language, pg 123-155. Cambridge University Press: New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
</authors>
<title>Generating Descriptions that Exploit a User&apos;s Domain Knowledge.</title>
<date>1990</date>
<publisher>Academic Press:</publisher>
<location>New York.</location>
<note>To appear in</note>
<contexts>
<context position="3993" citStr="Reiter (1990" startWordPosition="607" endWordPosition="608"> well as linguistic criteria; descriptions that are maximal under these preference rules can be found in polynomial time, while some alternative formalizations of the free-of-false-implicatures constraint make the generation task NP-Hard. 1 The referrineattributive distinction follows Donnellan (1966): a referring expression is intended to identify an object in the current context, while an attributive description is intended to communicate information about an object. This paper only addresses the problem of generating free-of-false-implicatures referring expressions, such as utterance (1a). Reiter (1990a,b) uses the same preference rules to formalize the task of generating free-of-false-implicatures attributive descriptions, such as utterance (2a). 2. Referring Expression Model The referring-expression model used in this paper is a variant of Dale&apos;s (1989) model for full definite noun phrase referring expressions. Dale&apos;s model is applicable in situations in which the speaker intends to refer to an object that the speaker and hearer are mutually aware of, and the speaker has no other communicative goal besides identifying the referred-to object.2 The model assumes that objects belong to a tax</context>
<context position="20610" citStr="Reiter (1990" startWordPosition="3225" endWordPosition="3226">s in referring expressions is NP-Hard (Section 5.1), but unnecessary components can always be found in polynomial time (Section 5.2). 5.1. No Unnecessary Words The No Unnecessary Words preference rule forbids referring expressions from containing unnecessary words: Formally, A &gt;D. uw B if A&apos;s surface form uses a subset of the words used by B&apos;s surface form. There are several variants, such as only considering open-class words, or requiring the words in B to be in the same order as the corresponding words in A. All of these variants make the generation problem NP-Hard. The formal proofs are in Reiter (1990b). Intuitively, the basic problem is that any preference that is stated solely in terms of surface forms must deal with the possibility that new parses and semantic interpretations may arise when the surface form is modified. This means that the only way a generation system can guarantee that an utterance satisfies the No Unnecessary Words rule is to generate all possible subsets of the surface form, and then run each subset through a parser and semantic interpreter to check if it happens to be a successful referring expression. The number of subsets of the surface form is exponential in the </context>
<context position="27063" citStr="Reiter (1990" startWordPosition="4274" endWordPosition="4275">referred over vehicle or Porsche, but it is not acceptable for car to be preferred over gift (because car neither sub102 sumes nor is subsumed by at). If the above restriction holds, a variant of the simple incremental algorithm of Section 5.2 may be used to implement lexical preference: the algorithm simply attempts each replacement that lexical preference suggests, and checks if this results in a successful referring expression. If the restriction does not hold, then the simple incremental algorithm may fail, and obeying the Lexical Preference rule is in fact NP-Hard (the formal proof is in Reiter (1990b)). 7. Issues 7.1. The Impact of NP-Hard Preference Rules It is difficult to precisely determine the computational expense of generating referring expressions that are maximal under the Full Brevity or No Unnecessary Words preference rules. The most straightforward algorithm that obeys Full Brevity (a similar analysis can be done for No Unnecessary Words) simply does an exhaustive search: it first checks if any one-component referring expression is successful, then checks if any two-component referring expression is successful, and so forth. Let L be the number of components in the shortest r</context>
<context position="32295" citStr="Reiter (1990" startWordPosition="5087" endWordPosition="5088">sible). For example, Washing-Machine&apos;s realization contains a superset of the open-class words used in the realization of Machine, so the basic-level preference of Section 6 indicates that WashingMachine should be lexically preferred over Machine, while the realization-subset preference indicates that Machine should be lexically preferred over Washing-Machine. The basic-level preference should take priority in such cases, so WashingMachine is the true lexically-preferred class in this example. 73. Generalizability of Results For the task of generating attributive descriptions as formalized in Reiter (1990a, 1990b), the Local Brevity, No Unnecessary Components, and Lexical Preference rules are effective at prohibiting utterances that carry unwanted conversational implicatures, and also can be incorporated into a polynomial-time generation algorithm, provided that some restrictions are imposed on the underlying knowledge base. The effectiveness and tractability of these preference rules for other generation tasks is an open problem that requires further investigation. The Full Brevity and No Unnecessary Words preference rules are computationally intractable for the attributive description genera</context>
</contexts>
<marker>Reiter, 1990</marker>
<rawString>Reiter, E. 1990a Generating Descriptions that Exploit a User&apos;s Domain Knowledge. To appear in R. Dale, C. Mellish, and M. Zack (Eds.), Current Research in Natural Language Generation. Academic Press: New York.</rawString>
</citation>
<citation valid="false">
<authors>
<author>E Reiter</author>
</authors>
<title>1990b Generating Appropriate Natural Language Object Descriptions. PhD thesis. Aiken Computation Lab,</title>
<location>Harvard University: Cambridge, Mass.</location>
<marker>Reiter, </marker>
<rawString>Reiter, E. 1990b Generating Appropriate Natural Language Object Descriptions. PhD thesis. Aiken Computation Lab, Harvard University: Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Rosch</author>
</authors>
<title>Principles of Categorization. In</title>
<date>1978</date>
<location>Hillsdale, NJ.</location>
<contexts>
<context position="9959" citStr="Rosch 1978" startWordPosition="1517" endWordPosition="1518">ecial case of the Quantity maxim, at least for the referring-expression generation task as formalized in Section 2. Manner: The Brevity submaxim of the Manner maxim requires a speaker to use short utterances if possible. In this context it requires the speaker to use a short referring expression if such a referring expression exists. The analysis of the other Manner submaxims is left for future work. An additional source of conversational implicature was proposed by Cruse (1977) and Hirschberg (1985), who hypothesized that- implicatures might arise from the failure to use basic-level classes (Rosch 1978) in an utterance. In this paper, such implicatures are generalized by assuming that there is a lexical-preference hierarchy among the lexical classes (classes that can be realized with single lexical units) known to the hearer, and that the use of a lexical class in an utterance implicates that no preferred lexical class could have been used in its place. In summary, conversational implicauire considerations require referring expressions to be brief, to not contain unnecessary elements, and to use lexically-preferred classes whenever possible. The following requests illustrate how violations o</context>
<context position="30518" citStr="Rosch 1978" startWordPosition="4822" endWordPosition="4823"> do not conflict, i.e., that it is never the case that description A is preferred over description B by one preference rule, while description B is preferred over description A by another preference rule. This means, in particular, that if lexical class LC1 is preferred over lexical class LC2, then LC i&apos;s realization must not contain more openclass words than LC2&apos;s realization; otherwise, the Lexical Preference and Local Brevity preference rules may contlict.10 This can be supported by psychological and linguistic findings that basic-level classes are almost always realized with single words (Rosch 1978; Berlin, Breedlove, and Raven 1973). However, there are a few exceptions to this rule, i.e., there do exist a small number of basic-level categories that have realizations that require more than one open-class word. For example, WashingMachine is a basic-level class for some people, and it has a realization that uses two open-class words. This leads to a conflict of the type mentioned above: basic-level Washing-Machine is preferred over non10 This assumes that the Local Brevity preference rule USN number of open-class words as its measure of description length. If number of components or numb</context>
</contexts>
<marker>Rosch, 1978</marker>
<rawString>Rosch, E. 1978 Principles of Categorization. In E. Rosch and B. Lloyd (Eds.), Cognition and Categorization. Lawrence Erlbaurn: Hillsdale, NJ.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>