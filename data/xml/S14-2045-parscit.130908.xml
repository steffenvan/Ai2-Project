<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001027">
<title confidence="0.998322">
ezDI: A Hybrid CRF and SVM based Model for Detecting and Encoding
Disorder Mentions in Clinical Notes
</title>
<author confidence="0.944574">
Parth Pathak, Pinal Patel, Vishal Panchal, Narayan Choudhary, Amrish Patel, Gautam Joshi
</author>
<affiliation confidence="0.816951">
ezDI, LLC.
</affiliation>
<email confidence="0.986011">
{parth.p,pinal.p,vishal.p,narayan.c,amrish.p,gautam.j}@ezdi.us
</email>
<sectionHeader confidence="0.997217" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99968385">
This paper describes the system used
in Task-7 (Analysis of Clinical Text) of
SemEval-2014 for detecting disorder men-
tions and associating them with their re-
lated CUI of UMLS1. For Task-A, a CRF
based sequencing algorithm was used to
find different medical entities and a binary
SVM classifier was used to find relation-
ship between entities. For Task-B, a dic-
tionary look-up algorithm on a customized
UMLS-2012 dictionary was used to find
relative CUI for a given disorder mention.
The system achieved F-score of 0.714 for
Task A &amp; accuracy of 0.599 for Task B
when trained only on training data set, and
it achieved F-score of 0.755 for Task A &amp;
accuracy of 0.646 for Task B when trained
on both training as well as development
data set. Our system was placed 3rd for
both task A and B.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997875">
A clinical document contains plethora of informa-
tion regarding patient’s medical condition in un-
structured format. So a sophisticated NLP sys-
tem built specifically for clinical domain can be
very useful in many different clinical applications.
In recent years, clinical NLP has gained a lot
of significance in research community because it
contains challenging tasks such as medical entity
recognition, abbreviation disambiguation, inter-
conceptual relationship detection, anaphora res-
olution, and text summarization. Clinical NLP
has also gained a significant attraction among the
</bodyText>
<footnote confidence="0.9975078">
1http://www.nlm.nih.gov/research/umls/
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
</footnote>
<bodyText confidence="0.99908243902439">
health care industry because it promises to de-
liver applications like computer assisted coding,
automated data abstraction, core/quality measure
monitoring, fraud detection, revenue loss preven-
tion system, clinical document improvement sys-
tem and so on.
Task-7 of SemEval-2014 was in continuation
of the 2013 ShaRe/CLEF Task-1 (Sameer Prad-
han, et al., 2013). This task was about finding
disorder mentions from the clinical text and as-
sociating them with their related CUIs (concept
unique identifiers) as given in the UMLS (Unified
Medical Language System). UMLS is the largest
available medical knowledge resource. It contains
2,885,877 different CUIs having 6,497,937 differ-
ent medical terms from over 100 different medi-
cal vocabularies. Finding accurate CUIs from free
clinical text can be very helpful in many healthcare
applications. Our aim for participating in this task
was to explore new techniques of finding CUIs
from clinical document.
Over the last few years many different Clin-
ical NLP systems like cTAKES (Savova, Guer-
gana K., et al., 2010), MetaMap (A. Aronson,
2001), MedLEE (C. Friedman et al., 1994) have
been developed to extract medical concepts from
a clinical document. Most of these systems focus
on rule based, medical knowledge driven dictio-
nary look-up approaches. In very recent past, a
few attempts have been made to use supervised or
semi-supervised learning models. In 2009, Yefang
Wang (Wang et al., 2009) used cascading clas-
sifiers on manually annotated data which fetched
F-score of 0.832. In 2010, i2b2 shared task chal-
lenge focused on finding test, treatment and prob-
lem mentions from clinical document.
In 2013, ShARe/CLEF task focused on finding
disorder mentions from clinical document and as-
signing relevant CUI code to it. In both i2b2 task
and ShaRe/CLEF task most of the systems used
either supervised or semi-supervised learning ap-
</bodyText>
<page confidence="0.955005">
278
</page>
<note confidence="0.73087">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 278–283,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.997391428571428">
proaches.
In this paper we have proposed a hybrid super-
vised learning approach based on CRF and SVM
to find out disorder mentions from clinical doc-
uments and a dictionary look-up approach on a
customized UMLS meta-thesaurus to find corre-
sponding CUI.
</bodyText>
<sectionHeader confidence="0.991384" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.9994985">
The SemEval-2014 corpus comprises of de-
identified plain text from MIMIC2 version 2.5
database. A disorder mention was defined as any
span of text which can be mapped to a concept
in UMLS and which belongs to the Disorder se-
mantic group. There were 431 notes extracted
from intensive care unit having various clinical
report types (like radiology, discharge summary,
echocardiogram and ECG), out of which 99 notes
were used in development data set, 199 notes were
used in training data set and 133 notes were used
in testing data set.
Preliminary analysis on this data showed that
number of sentences in training documents were
comparatively smaller than the development or
test data set (Table 1). Number of disorder men-
tions were also significantly lower in training data
set than in development data set (Table 1).
</bodyText>
<table confidence="0.996484888888889">
Type Dev Train Test
Docuemnts 99 199 133
Sentence 9860 10485 17368
Token 102k 113k 177k
Avg token/sen 10.42 10.79 10.24
Cont. entity 4912 5,165 7,186
Disjoint Entity 439 651 4588
Avg Ent/Doc 54.05 29.22 57.47
Distinct CUI 1007 938 NA
</table>
<tableCaption confidence="0.999771">
Table 1: Numerical analysis on data.
</tableCaption>
<sectionHeader confidence="0.987019" genericHeader="method">
3 System Design
</sectionHeader>
<bodyText confidence="0.999026428571429">
Analysis of Task-A showed that disorder men-
tions also contain other UMLS semantic types like
findings, anatomical sites and modifiers (Table 2).
So we divided the task of finding disorder men-
tion in to two subtasks. First a CRF based se-
quencing model was used to find different disorder
mentions, modifiers, anatomical sites and findings.
</bodyText>
<footnote confidence="0.917956">
2http://mimic.physionet.org/database/
releases/70-version-25.html
</footnote>
<bodyText confidence="0.996456">
Then a binary SVM classifier was used to check
if relationship exists between a disorder and other
types of entities or not.
Example Disorder Findings Anatomy Modifier
There is persistent ✓ ✓ X X
left lower lobe opacity
presumably atelectasis.
He had substernal chest ✓ ✓ X X
pain, sharp but without
radiation.
Patientt also developed ✓ X ✓ X
some erythema around
the stoma site on
hospital day two.
The tricuspid valve X ✓ ✓ X
leaflets are mildly thick-
ened.
Please call,if you find ✓ ✓ X X
swelling in the wound.
She also notes new sharp ✓ X ✓ X
pain in left shoulder
blade/back area.
An echocardiogram ✓ X X ✓
demonstrated mild
left and right atrial
dilatation
</bodyText>
<tableCaption confidence="0.916707">
Table 2: Entity Types co-relation and examples
</tableCaption>
<bodyText confidence="0.999956214285714">
For Task-B, we have used a simple dictionary
look up algorithm on a customized UMLS dictio-
nary. A preliminary analysis of UMLS entities in
general show that a single disorder mention may
consist of various types of linguistic phrases. It is
not necesarry that the system to detect these enti-
ties as a single phrase. The entities and their re-
lations may also occur in disjoint phrases as well.
Our analysis of the disorder entities inside UMLS
reveals that out of a total 278,859 disorders (based
on SNOMED-CT library), 96,069 are such that
can be broken down into more than one phrase,
which is roughly 1/3 of total number of disorders
in the UMLS.
</bodyText>
<subsectionHeader confidence="0.995054">
3.1 System Workflow
</subsectionHeader>
<bodyText confidence="0.976564">
The Work-flow of the system is as follow:
</bodyText>
<page confidence="0.996195">
279
</page>
<figureCaption confidence="0.999842">
Figure 1: System Workflow
</figureCaption>
<subsectionHeader confidence="0.483811">
3.1.2 openNLP
</subsectionHeader>
<bodyText confidence="0.999822666666667">
We have used openNLP3 to perform basic NLP
tasks like sentence detection, tokenizing, PoS tag-
ging, chunking, parsing and stemming.
</bodyText>
<subsectionHeader confidence="0.740069">
3.1.3 Dictionary Lookup
</subsectionHeader>
<bodyText confidence="0.999336153846154">
UMLS 2012AA dictionary with Lexical Variant
Generator (LVG)4 was used to perform dictionary
lookup task. Even though the task was only about
finding disorder mentions, we also identified en-
tities like procedures, finding, lab data, medicine,
anatomical site and medical devices to be used as
features in our CRF model. This was helpful in
decreasing the number of false positive. UMLS
TUI (Type Unique Identifier) used for different
entity type is described in Table 3. A rule-based
approach on the output of the OpenNLP syntac-
tic parser was used to detect possible modifiers for
disorder mentions.
</bodyText>
<table confidence="0.999678">
Type Tui list
Disorder T046,T047,T048,T049,T050,T191,
T037,T019,T184
Anatomical T017,T021,T023,T024,T025,T026,
Sites T029,T030
Procedures T059,T060,T061
Medicines T200,T120,T110
Lab Data T196,T119
Modifiers Customized Dictionary
Findings T033,T034,T041,T084,T032,T201,
T053,T054
</table>
<tableCaption confidence="0.965044">
Table 3: Entity Types and their related TUI list
from UMLS
</tableCaption>
<subsectionHeader confidence="0.935666">
3.1.1 Pre-processing
</subsectionHeader>
<bodyText confidence="0.999986733333333">
All the clinical documents used in this task
were de-identified. So information related to
hospital name, patient demographics, physician
names/signatures, dates, places, and certain lab-
data were converted into predefined patterns.
These patterns were hindering the flow of natu-
ral language. As a result of it, we were unable to
get accurate results for PoS tagging and chunking
of the sentences. So we replaced all of these de-
identified patterns with some text that appear more
as natural language. There were also some head-
ers and footers associated with all the documents,
which were actually irrelevant to this task. There-
fore all headers and footers were also removed at
the pre-processing level.
</bodyText>
<subsectionHeader confidence="0.895566">
3.1.4 CRF Feature Generation
</subsectionHeader>
<bodyText confidence="0.996196">
The feature sets were divided into three categories.
</bodyText>
<sectionHeader confidence="0.937193" genericHeader="method">
1) Clinical Features
</sectionHeader>
<bodyText confidence="0.999726636363636">
i) Section Headers: A clinical note is often di-
vided into relevant segments called Section Head-
ers. These section headers provide very useful
information at the discourse level. Same section
header can have multiple variants. For example
History of Present Illness can also be written as
HPI, HPIS, Brief History etc. We have created a
dictionary of more than 550 different section head-
ers and classified them into more than 40 hierar-
chical categories. But using only section header
dictionary for classification can fetch many false
</bodyText>
<footnote confidence="0.999895">
3https://opennlp.apache.org/
4http://lexsrv2.nlm.nih.gov/
</footnote>
<page confidence="0.996993">
280
</page>
<bodyText confidence="0.999828333333333">
positives. Section header always appears in a pre-
defined similar sequences. So to remove these
false positives, we have used a Hidden Markov
Model(HMM) (Parth Pathak, et al, 2013). For this
task, we have used unigram section header id as a
feature for all the tokens in CRF.
</bodyText>
<listItem confidence="0.555486">
ii) Dictionary Lookup: A binary feature was
used for all the different entity types detected from
UMLS dictionary from last pipeline.
iii) Abbreviations: Abbreviations Disambigua-
</listItem>
<bodyText confidence="0.99430275">
tion is one of the most challenging tasks in clinical
NLP. The primary reason for the same is a lack of
dictionary which contains most of the valid list of
abbreviations. For this task, we have used LRABR
as base dictionary to find out all the possible ab-
breviations and on top of that, a binary SVM clas-
sifier was used to check if the given abbreviation
has medical sense or not.
</bodyText>
<sectionHeader confidence="0.932608" genericHeader="method">
2) Textual Feature:
</sectionHeader>
<bodyText confidence="0.999935714285714">
Snowball stemmer5 was used to find out stem
value of all the word tokens. Prefix and suffix of
length 2 to 5 were also used as features. Different
orthographic features like whole word capital, first
char capital, numeric values, dates, words contain-
ing hyphen or slash, medical units (mg/gram/ltr
etc.) were used as features.
</bodyText>
<sectionHeader confidence="0.858885" genericHeader="method">
3) Syntactic Features:
</sectionHeader>
<bodyText confidence="0.999724">
Different linguistic features like PoS tags and
chunks for each token were used. We have also
used head of the noun phrase as one of the feature
which can be very helpful in detecting the type of
an entity.
</bodyText>
<subsectionHeader confidence="0.871248">
3.1.5 CRF toolkit
</subsectionHeader>
<bodyText confidence="0.999822">
All the annotated data was converted into BIO
sequencing format. CRF++6 toolkit was used to
train and predict the model.
</bodyText>
<subsectionHeader confidence="0.821952">
3.1.6 SVM
</subsectionHeader>
<bodyText confidence="0.999643666666667">
SVM was used to check whether a relationship ex-
ists between two entities or not. For this purpose
all the tokens between these two entities, their part
of speech tags and chunks were used as features.
Rules based on output of a syntactic parser were
also used as a binary feature. Some orthographic
features like all letter capital, contains colon (:),
contains semi colon (;), were also used as features.
LibSVM7 was used to train as well as predict the
</bodyText>
<footnote confidence="0.995667">
5http://snowball.tartarus.org/
6http://crfpp.googlecode.com/
7http://www.csie.ntu.edu.tw/\˜cjlin/
libsvm/
</footnote>
<bodyText confidence="0.660768">
model.
</bodyText>
<subsectionHeader confidence="0.801281">
3.1.7 Dictionary Look-up for CUI detection
</subsectionHeader>
<bodyText confidence="0.999675055555556">
For a better mapping of the entities detected by
NLP inside the given input text, we found it to
be a better approach to divide the UMLS enti-
ties into various phrases. This was done semi-
automatically by splitting the strings based on
function words such as prepositions, particles and
non-nominal word classes such as verbs, adjec-
tives and adverbs. While most of the disorder enti-
ties in UMLS can be contained into a single noun
phrase (NP) there are also quite a few that contain
multiple NPs related with prepositional phrases
(PPs), verb phrases (VPs) and adjectival phrases
(ADJPs).
This task gave us a modified version of the
UMLS disorder entities along with their CUIs.
The following table (Table 4) gives a snapshot
of what this customized UMLS dictionary looked
like.
</bodyText>
<table confidence="0.999790785714286">
CUI Text P1 P2 P3
C001 Dribbling Dribbling from mouth
3132 from
mouth
C001 Bleeding Bleeding from nose
4591 from nose
C002 Hemorr- Hemo- from mouth
9163 hage from rrhage
mouth
C039 Chest pain Chest pain at rest
2685 at rest
C026 Fatigue Fatigue during pregn
9678 during ancy
pregnancy
</table>
<tableCaption confidence="0.995006">
Table 4: An example of the modified UMLS disor-
</tableCaption>
<bodyText confidence="0.899752">
der entities split as per their linguistic phrase types
Our dictionary look-up algorithm used this cus-
tomized UMLS dictionary as resource to find the
entities and assign the right CUIs.
</bodyText>
<sectionHeader confidence="0.999941" genericHeader="evaluation">
4 Results &amp; Error Analysis
</sectionHeader>
<subsectionHeader confidence="0.902415">
4.1 Evaluation Calculations
</subsectionHeader>
<bodyText confidence="0.996592">
The evaluation measures for Task A are Precision,
Recall and F-Meas, defined as:
</bodyText>
<equation confidence="0.966774">
Precision = TP
FP+TP
TP
Recall =
TP+FN
</equation>
<page confidence="0.987895">
281
</page>
<table confidence="0.9364387">
F-measure — 2∗Precision∗Recall Strict Accuracy
— Precision+Recall (T) 0.599
where Relaxed 0.878
TP = Disorder mention span matches with gold (T) 0.643
standard Strict 0.868
FP = Disorder mention span detected by the (T+D)
system was not present in the gold standard; Relaxed
FN = Disorder mention span was present in the (T+D)
gold standard but system was not able detect it
Table 6: Task-B Results
</table>
<bodyText confidence="0.9990915">
In Task B, the Accuracy was defined as the
number of pre-annotated spans with correctly
generated code divided by the total number of
pre-annotated spans.
</bodyText>
<figure confidence="0.5956415">
Total correct CUIs
Strict Accuracy = Total annotation in gold standard
Total correct CUIs
Relaxed Accuracy = Total span detected by system
</figure>
<subsectionHeader confidence="0.974182">
4.2 System Accuracy
</subsectionHeader>
<bodyText confidence="0.999775714285714">
The system results were calculated on two dif-
ferent runs. For the first evaluation, only training
data was used for the training purpose while for
the second evaluation, both the training as well as
the development data sets were used for training
purpose. The results for Task A and B are as
follows:
</bodyText>
<table confidence="0.999819">
Precision Recall F-Meas
Strict 0.750 0.682 0.714
(T)
Relaxed 0.915 0.827 0.869
(T)
Strict 0.770 0.740 0.755
(T+D)
Relaxed 0.911 0.887 0.899
(T+D)
</table>
<tableCaption confidence="0.993368">
Table 5: Task-A Results
</tableCaption>
<bodyText confidence="0.8869075">
where T= Training Data set
D= Development Data set
</bodyText>
<subsectionHeader confidence="0.958878">
4.3 Error Analysis
</subsectionHeader>
<bodyText confidence="0.9997599">
Error Analysis on training data revealed that for
Task-A our system got poor results in detecting
non-contiguous disjoint entities. Our system also
performed very poorly in identifying abbrevia-
tions and misspelled entities. We also observed
that the accuracy of the part of speech tagger and
the chunker also contributes a lot towards the final
outcome. For Task-B, we got many false positives.
Many CUIs which we identified from the UMLS
were not actually annotated.
</bodyText>
<sectionHeader confidence="0.999775" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9996518">
In this paper we have proposed a CRF and SVM
based hybrid approach to find Disorder mentions
from a given clinical text and a novel dictio-
nary look-up approach for discovering CUIs from
UMLS meta-thesaurus. Our system did produce
competitive results and was third best among the
participants of this task. In future, we would like
to explore semi-supervised learning approaches to
take advantage of large amount of available un-
annotated free clinical text.
</bodyText>
<sectionHeader confidence="0.999551" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999856636363636">
Savova, Guergana K., James J. Masanz, Philip V.
Ogren, Jiaping Zheng, Sunghwan Sohn, Karin C.
Kipper-Schuler, and Christopher G. Chute. 2010.
Mayo clinical Text Analysis and Knowledge Ex-
traction System (cTAKES): architecture, component
evaluation and applications. Journal of the Amer-
ican Medical Informatics Association 17, no. 5
(2010): 507-513.
Friedman C, Alderson PO, Austin JH, Cimino JJ, John-
son SB. 1994. A general natural-language text pro-
cessor for clinical radiology. J Am Med Inform As-
soc 1994 Mar-Apr;1(2):16174. [PubMed:7719797]
Aronson, Alan R. 2001. Effective mapping of biomed-
ical text to the UMLS Metathesaurus: the MetaMap
program. In Proceedings of the AMIA Symposium,
p. 17. American Medical Informatics Association,
2001.
Wang, Yefeng, and Jon Patrick. 2009. Cascading clas-
sifiers for named entity recognition in clinical notes.
In Proceedings of the workshop on biomedical infor-
mation extraction, pp. 42-49. Association for Com-
putational Linguistics, 2009.
</reference>
<page confidence="0.960528">
282
</page>
<reference confidence="0.979743909090909">
Suominen, Hanna, Sanna Salanter, Sumithra Velupil-
lai, Wendy W. Chapman, Guergana Savova, Noemie
Elhadad, Sameer Pradhan 2013 Overview of the
ShARe/CLEF eHealth evaluation lab 2013. In In-
formation Access Evaluation. Multilinguality, Mul-
timodality, and Visualization, pp. 212-231. Springer
Berlin Heidelberg, 2013.
. ””
Parth Pathak, Raxit Goswami, Gautam Joshi, Pinal Pa-
tel, and Amrish Patel. 2013 CRF-based Clinical
Named Entity Recognition using clinical Features
</reference>
<page confidence="0.998576">
283
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.532251">
<title confidence="0.9994965">ezDI: A Hybrid CRF and SVM based Model for Detecting and Disorder Mentions in Clinical Notes</title>
<author confidence="0.99839">Parth Pathak</author>
<author confidence="0.99839">Pinal Patel</author>
<author confidence="0.99839">Vishal Panchal</author>
<author confidence="0.99839">Narayan Choudhary</author>
<author confidence="0.99839">Amrish Patel</author>
<author confidence="0.99839">Gautam Joshi</author>
<affiliation confidence="0.557542">ezDI, LLC.</affiliation>
<abstract confidence="0.997644857142857">This paper describes the system used in Task-7 (Analysis of Clinical Text) of SemEval-2014 for detecting disorder mentions and associating them with their re- CUI of For Task-A, a CRF based sequencing algorithm was used to find different medical entities and a binary SVM classifier was used to find relationship between entities. For Task-B, a dictionary look-up algorithm on a customized UMLS-2012 dictionary was used to find relative CUI for a given disorder mention. The system achieved F-score of 0.714 for Task A &amp; accuracy of 0.599 for Task B when trained only on training data set, and it achieved F-score of 0.755 for Task A &amp; accuracy of 0.646 for Task B when trained on both training as well as development data set. Our system was placed 3rd for both task A and B.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Guergana K Savova</author>
<author>James J Masanz</author>
<author>Philip V Ogren</author>
<author>Jiaping Zheng</author>
<author>Sunghwan Sohn</author>
<author>Karin C Kipper-Schuler</author>
<author>Christopher G Chute</author>
</authors>
<title>Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and applications.</title>
<date>2010</date>
<journal>Journal of the American Medical Informatics Association</journal>
<volume>17</volume>
<pages>507--513</pages>
<marker>Savova, Masanz, Ogren, Zheng, Sohn, Kipper-Schuler, Chute, 2010</marker>
<rawString>Savova, Guergana K., James J. Masanz, Philip V. Ogren, Jiaping Zheng, Sunghwan Sohn, Karin C. Kipper-Schuler, and Christopher G. Chute. 2010. Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and applications. Journal of the American Medical Informatics Association 17, no. 5 (2010): 507-513.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Friedman</author>
<author>Alderson PO</author>
<author>Austin JH</author>
<author>Cimino JJ</author>
<author>Johnson SB</author>
</authors>
<title>A general natural-language text processor for clinical radiology.</title>
<date>1994</date>
<journal>J Am Med Inform Assoc</journal>
<note>Mar-Apr;1(2):16174. [PubMed:7719797]</note>
<contexts>
<context position="3028" citStr="Friedman et al., 1994" startWordPosition="452" endWordPosition="455">iers) as given in the UMLS (Unified Medical Language System). UMLS is the largest available medical knowledge resource. It contains 2,885,877 different CUIs having 6,497,937 different medical terms from over 100 different medical vocabularies. Finding accurate CUIs from free clinical text can be very helpful in many healthcare applications. Our aim for participating in this task was to explore new techniques of finding CUIs from clinical document. Over the last few years many different Clinical NLP systems like cTAKES (Savova, Guergana K., et al., 2010), MetaMap (A. Aronson, 2001), MedLEE (C. Friedman et al., 1994) have been developed to extract medical concepts from a clinical document. Most of these systems focus on rule based, medical knowledge driven dictionary look-up approaches. In very recent past, a few attempts have been made to use supervised or semi-supervised learning models. In 2009, Yefang Wang (Wang et al., 2009) used cascading classifiers on manually annotated data which fetched F-score of 0.832. In 2010, i2b2 shared task challenge focused on finding test, treatment and problem mentions from clinical document. In 2013, ShARe/CLEF task focused on finding disorder mentions from clinical do</context>
</contexts>
<marker>Friedman, PO, JH, JJ, SB, 1994</marker>
<rawString>Friedman C, Alderson PO, Austin JH, Cimino JJ, Johnson SB. 1994. A general natural-language text processor for clinical radiology. J Am Med Inform Assoc 1994 Mar-Apr;1(2):16174. [PubMed:7719797]</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan R Aronson</author>
</authors>
<title>Effective mapping of biomedical text to the UMLS Metathesaurus: the MetaMap program.</title>
<date>2001</date>
<booktitle>In Proceedings of the AMIA Symposium,</booktitle>
<pages>17</pages>
<publisher>American Medical Informatics Association,</publisher>
<contexts>
<context position="2993" citStr="Aronson, 2001" startWordPosition="448" endWordPosition="449">UIs (concept unique identifiers) as given in the UMLS (Unified Medical Language System). UMLS is the largest available medical knowledge resource. It contains 2,885,877 different CUIs having 6,497,937 different medical terms from over 100 different medical vocabularies. Finding accurate CUIs from free clinical text can be very helpful in many healthcare applications. Our aim for participating in this task was to explore new techniques of finding CUIs from clinical document. Over the last few years many different Clinical NLP systems like cTAKES (Savova, Guergana K., et al., 2010), MetaMap (A. Aronson, 2001), MedLEE (C. Friedman et al., 1994) have been developed to extract medical concepts from a clinical document. Most of these systems focus on rule based, medical knowledge driven dictionary look-up approaches. In very recent past, a few attempts have been made to use supervised or semi-supervised learning models. In 2009, Yefang Wang (Wang et al., 2009) used cascading classifiers on manually annotated data which fetched F-score of 0.832. In 2010, i2b2 shared task challenge focused on finding test, treatment and problem mentions from clinical document. In 2013, ShARe/CLEF task focused on finding</context>
</contexts>
<marker>Aronson, 2001</marker>
<rawString>Aronson, Alan R. 2001. Effective mapping of biomedical text to the UMLS Metathesaurus: the MetaMap program. In Proceedings of the AMIA Symposium, p. 17. American Medical Informatics Association, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yefeng Wang</author>
<author>Jon Patrick</author>
</authors>
<title>Cascading classifiers for named entity recognition in clinical notes.</title>
<date>2009</date>
<booktitle>In Proceedings of the workshop on biomedical information extraction,</booktitle>
<pages>42--49</pages>
<marker>Wang, Patrick, 2009</marker>
<rawString>Wang, Yefeng, and Jon Patrick. 2009. Cascading classifiers for named entity recognition in clinical notes. In Proceedings of the workshop on biomedical information extraction, pp. 42-49. Association for Computational Linguistics, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna Suominen</author>
<author>Sanna Salanter</author>
<author>Sumithra Velupillai</author>
<author>Wendy W Chapman</author>
</authors>
<title>Guergana Savova, Noemie Elhadad, Sameer Pradhan</title>
<date>2013</date>
<booktitle>In Information Access Evaluation. Multilinguality, Multimodality, and Visualization,</booktitle>
<pages>212--231</pages>
<publisher>Springer</publisher>
<location>Berlin Heidelberg,</location>
<marker>Suominen, Salanter, Velupillai, Chapman, 2013</marker>
<rawString>Suominen, Hanna, Sanna Salanter, Sumithra Velupillai, Wendy W. Chapman, Guergana Savova, Noemie Elhadad, Sameer Pradhan 2013 Overview of the ShARe/CLEF eHealth evaluation lab 2013. In Information Access Evaluation. Multilinguality, Multimodality, and Visualization, pp. 212-231. Springer Berlin Heidelberg, 2013.</rawString>
</citation>
<citation valid="false">
<publisher></publisher>
<marker></marker>
<rawString>. ””</rawString>
</citation>
<citation valid="true">
<authors>
<author>Parth Pathak</author>
<author>Raxit Goswami</author>
<author>Gautam Joshi</author>
<author>Pinal Patel</author>
<author>Amrish Patel</author>
</authors>
<title>CRF-based Clinical Named Entity Recognition using clinical Features</title>
<date>2013</date>
<contexts>
<context position="9851" citStr="Pathak, et al, 2013" startWordPosition="1540" endWordPosition="1543">t the discourse level. Same section header can have multiple variants. For example History of Present Illness can also be written as HPI, HPIS, Brief History etc. We have created a dictionary of more than 550 different section headers and classified them into more than 40 hierarchical categories. But using only section header dictionary for classification can fetch many false 3https://opennlp.apache.org/ 4http://lexsrv2.nlm.nih.gov/ 280 positives. Section header always appears in a predefined similar sequences. So to remove these false positives, we have used a Hidden Markov Model(HMM) (Parth Pathak, et al, 2013). For this task, we have used unigram section header id as a feature for all the tokens in CRF. ii) Dictionary Lookup: A binary feature was used for all the different entity types detected from UMLS dictionary from last pipeline. iii) Abbreviations: Abbreviations Disambiguation is one of the most challenging tasks in clinical NLP. The primary reason for the same is a lack of dictionary which contains most of the valid list of abbreviations. For this task, we have used LRABR as base dictionary to find out all the possible abbreviations and on top of that, a binary SVM classifier was used to che</context>
</contexts>
<marker>Pathak, Goswami, Joshi, Patel, Patel, 2013</marker>
<rawString>Parth Pathak, Raxit Goswami, Gautam Joshi, Pinal Patel, and Amrish Patel. 2013 CRF-based Clinical Named Entity Recognition using clinical Features</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>