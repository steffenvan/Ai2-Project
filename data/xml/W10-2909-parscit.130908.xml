<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000365">
<title confidence="0.9990285">
Improved Unsupervised POS Induction Using Intrinsic Clustering Quality
and a Zipfian Constraint
</title>
<author confidence="0.979038">
Roi Reichart Raanan Fattal Ari Rappoport
</author>
<affiliation confidence="0.890694">
ICNC Institute of computer science Institute of computer science
The Hebrew University The Hebrew University The Hebrew University
</affiliation>
<email confidence="0.988288">
roiri@cs.huji.ac.il raananf@cs.huji.ac.il arir@cs.huji.ac.il
</email>
<sectionHeader confidence="0.993633" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999953642857143">
Modern unsupervised POS taggers usually
apply an optimization procedure to a non-
convex function, and tend to converge to
local maxima that are sensitive to start-
ing conditions. The quality of the tag-
ging induced by such algorithms is thus
highly variable, and researchers report av-
erage results over several random initial-
izations. Consequently, applications are
not guaranteed to use an induced tagging
of the quality reported for the algorithm.
In this paper we address this issue using
an unsupervised test for intrinsic cluster-
ing quality. We run a base tagger with
different random initializations, and select
the best tagging using the quality test. As
a base tagger, we modify a leading un-
supervised POS tagger (Clark, 2003) to
constrain the distributions of word types
across clusters to be Zipfian, allowing us
to utilize a perplexity-based quality test.
We show that the correlation between our
quality test and gold standard-based tag-
ging quality measures is high. Our re-
sults are better in most evaluation mea-
sures than all results reported in the liter-
ature for this task, and are always better
than the Clark average results.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995938">
Unsupervised part-of-speech (POS) induction is
of major theoretical and practical importance. It
counters the arbitrary nature of manually designed
tag sets, and avoids manual corpus annotation
costs. The task enjoys considerable current inter-
est in the research community (see Section 3).
Most unsupervised POS tagging algorithms ap-
ply an optimization procedure to a non-convex
function, and tend to converge to local maxima
that strongly depend on the algorithm’s (usually
random) initialization. The quality of the tag-
gings produced by different initializations varies
substantially. Figure 1 demonstrates this phe-
nomenon for a leading POS induction algorithm
(Clark, 2003). The absolute variability of the in-
duced tagging quality is 10-15%, which is around
20% of the mean. Strong variability has also been
reported by other authors (Section 3).
The common practice in the literature is to re-
port mean results over several random initializa-
tions of the algorithm (e.g. (Clark, 2003; Smith
and Eisner, 2005; Goldwater and Griffiths, 2007;
Johnson, 2007)). This means that applications us-
ing the induced tagging are not guaranteed to use
a tagging of the reported quality.
In this paper we address this issue using an
unsupervised test for intrinsic clustering quality.
We present a quality-based algorithmic family Q.
Each of its concrete member algorithms Q(B) runs
a base tagger B with different random initializa-
tions, and selects the best tagging according the
quality test. If the test is highly positively corre-
lated with external tagging quality measures (e.g.,
those based on gold standard tagging), Q(B) will
produce better results than B with high probability.
We experiment with two base taggers, Clark’s
original tagger (CT) and Zipf Constrained Clark
(ZCC). ZCC is a novel algorithm of interest in its
own right, which is especially suitable as a base
tagger in the family Q. ZCC is a modification of
Clark’s algorithm in which the distribution of the
number of word types in a cluster (cluster type
size) is constrained to be Zipfian. This property
holds for natural languages, hence we can expect
a higher correlation between ZCC and an accepted
unsupervised quality measure, perplexity.
We show that for both base taggers, the corre-
lation between our unsupervised quality test and
gold standard based tagging quality measures is
high. For the English WSJ corpus, the Q(ZCC)
</bodyText>
<page confidence="0.995838">
57
</page>
<note confidence="0.839965333333333">
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 57–66,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
Many to 1
</note>
<figureCaption confidence="0.99862">
Figure 1: Distribution of the quality of the tag-
</figureCaption>
<bodyText confidence="0.99053815">
gings produced in 100 runs of the Clark POS in-
duction algorithm (with different random initial-
izations) for sections 2-21 of the WSJ corpus. All
graphs are 10-bin histograms presenting the num-
ber of runs (y-axis) with the corresponding qual-
ity (x-axis). Quality is evaluated with 4 clustering
evaluation measures: V, NVI, greedy m-1 map-
ping and greedy 1-1 mapping. The quality of the
induced tagging varies considerably.
algorithm gives better results than CT with proba-
bility 82-100% (depending on the external quality
measure used). Q(CT) is shown to be better than
the original CT algorithm as well. Our results are
better in most evaluation measures than all previ-
ous results reported in the literature for this task,
and are always better than Clark’s average results.
Section 2 describes the ZCC algorithm and our
quality measure. Section 3 discusses previous
work. Section 4 presents the experimental setup
and Section 5 reports our results.
</bodyText>
<sectionHeader confidence="0.924206" genericHeader="method">
2 The Q(ZCC) Algorithm
</sectionHeader>
<bodyText confidence="0.999681428571428">
Given an N word corpus M consisting of plain
text, with word types W = {w1, ... , wm}, the
unsupervised POS induction task is to find a class
membership function g from W into a set of class
labels {c1, ... , cn}. In the version tackled in this
paper, the number of classes n is an input of the al-
gorithm. The membership function g can be used
to tag a corpus if it is deterministic (as the func-
tion learned in this work) or if a rule for selecting
a single tag for every word is provided.
Most modern unsupervised POS taggers pro-
duce taggings of variable quality that strongly de-
pend on their initialization. Our approach towards
generating a single high quality tagging is to use a
family of algorithms Q. Each member Q(B) of Q
utilizes a base tagger B, which is run using several
random initializations. The final output is selected
according to an unsupervised quality test. We fo-
cus here on Clark’s tagger (Clark, 2003) (CT),
probably the leading POS induction algorithm (see
Table 3).
We start with a description of the original CT.
We then detail ZCC, a modification of CT that
constrains the clustering space by adding a Zipf-
based constraint. Our perplexity-based unsuper-
vised tagging quality test is discussed next. Fi-
nally, we provide an unsupervised technique for
selecting the parameter of the Zipfian constraint.
</bodyText>
<subsectionHeader confidence="0.998024">
2.1 The Original Clark Tagger (CT)
</subsectionHeader>
<bodyText confidence="0.998804615384615">
The tagger’s statistical model combines dis-
tributional and morphological information with
the likelihood function of the Brown algorithm
(Brown et al., 1992; Ney et al., 1994; Martin et
al., 1998). In the Brown algorithm a class assign-
ment function g is selected such that the class bi-
gram likelihood of the corpus, p(M|g), is max-
imized. Morphological and distributional infor-
mation is introduced to the Clark model through
a prior p(g). The prior prefers morphologically
uniform clusters and skewed cluster sizes.
The probability function the algorithm tries to
maximize is:
</bodyText>
<equation confidence="0.993071">
(1) p(M, g) = p(M|g) · p(g)
(2) p(M|g) = l li=N
i=� p(g(wi)|g(wi−1))
(3) p(g) = l lnl l
j=1 �j g(w)=j qj(w)
</equation>
<bodyText confidence="0.997707428571428">
Where qj(wi) is the probability of assigning
wi ∈ W by cluster cj according to the morpho-
logical model and aj is the coefficient of cluster
j, which equals to the number of word types as-
signed to that cluster divided by the total number
of word types in the vocabulary W. The objective
of the algorithm is formally specified by:
</bodyText>
<equation confidence="0.99571">
g* = argmaxgp(M, g)
</equation>
<bodyText confidence="0.999931307692308">
To find the cluster assignment g* an iterative
algorithm is applied. As initialization, the words
in W are randomly assigned to clusters (clusters
are thus of similar sizes). Then, for each word
(words are ordered by their frequency in the cor-
pus) the algorithm computes the effect that mov-
ing it from its current cluster to each of the other
clusters would have on the probability function.
The word is moved to the cluster having the high-
est positive effect (if there is no such cluster, the
word is not moved). The last step is performed it-
eratively until no improvement to the probability
function is possible through a single operation.
</bodyText>
<figure confidence="0.98861595">
40
20
40
20
0
0.45 0.5 0.55
0.5 0.6
0
0.4
V
0.4 0.5
1 to 1
20
10
40
20
0
0
0.7 0.8 0.9
NVI
</figure>
<page confidence="0.99849">
58
</page>
<bodyText confidence="0.999944666666667">
The probability function has many local max-
ima and the one to which the algorithm conver-
gences strongly depends on the initial assignment
of words to clusters. The quality of the clusters in-
duced in different runs of the algorithm is highly
variable (Figure 1).
</bodyText>
<subsectionHeader confidence="0.99823">
2.2 The Cluster Type Size Zipf Constraint
</subsectionHeader>
<bodyText confidence="0.999877727272727">
The motivation behind using a Zipfian constraint is
the following observation: when a certain statistic
is known to affect the quality of the induced clus-
tering and it is not explicitly manipulated by the al-
gorithm, strong fluctuations in its values are likely
to imply that there are uncontrolled fluctuations in
the quality of the induced clusterings. Thus, in-
troducing a constraint that we believe holds in real
data increases the correlation between clustering
quality and a well accepted unsupervised quality
measure (perplexity).
Our ZCC algorithm searches for a class assign-
ment function g that maximizes the probability
function (1) under a constraint on the clustering
space, namely constraining the cluster type size
distribution induced by g to be Zipfian. This con-
straint holds in many languages (Mitzenmacher,
2004) and is demonstrated in Figure 3 for the En-
glish corpus with which we experiment in this pa-
per.
Zipf’s law predicts that the fraction of elements
in class k is given by:
</bodyText>
<equation confidence="0.994634">
f(k; s; n) = �ni=1(1/is)
</equation>
<bodyText confidence="0.9998346">
where s is a parameter of the distribution and n the
number of clusters.
Denote the cluster type size distribution derived
from the algorithm’s cluster assignment function g
by T(g). The objective of the algorithm is
</bodyText>
<equation confidence="0.991144">
g** = argmaxgp(M, g) s.t. T (g) — Zipf(s)
</equation>
<bodyText confidence="0.999981392857143">
To impose the Zipfian distribution on the in-
duced clusters size, we make two modifications to
the original CT algorithm. First, at initialization,
words are randomly assigned to clusters in a way
that cluster sizes are distributed according to the
Zipfian distribution (with a parameter s). Specifi-
cally, we randomly select words to be assigned to
the first cluster until the fraction of word types in
the cluster equals to the prediction given by Zipf’s
law. We then randomly assign words to the second
cluster and so on.
Second, we change the basic operation of the al-
gorithm from moving a word to a cluster to swap-
ping two words between two different clusters.
For each word wi (again, words are ordered by
their frequency in the corpus as in CT), the algo-
rithm computes the effect on the probability func-
tion of moving it from its current cluster ccurr to
each of the other clusters. We denote the cluster
showing the best effect by cbest. Then, we search
the words of cbest for the word wj whose transition
to ccurr has the best effect on the probability func-
tion. If the sum of the effects of moving wi from
ccurr to cbest and moving wj from cbest to ccurr
is positive, the swapping is performed. If swap-
ping is not performed, we repeat the process for
wi, this time searching for cbest among all other
clusters except of former cbest candidates1.
</bodyText>
<subsectionHeader confidence="0.9942855">
2.3 Unsupervised Identification of High
Quality Runs
</subsectionHeader>
<bodyText confidence="0.999816833333333">
Perplexity is a standard measure for language
model evaluation. A language model defines the
transition probabilities for every word wi given the
words that precede it. The perplexity of a language
model for a given corpus having N words is de-
fined to be
</bodyText>
<equation confidence="0.514888">
� � �N 1
N �i=1
p(wi|w1 ... wi−1)
</equation>
<bodyText confidence="0.963159956521739">
An important property of perplexity that makes
it attractive as a measure for language model per-
formance is that in some sense the best model for
any corpus has the lowest perplexity for that cor-
pus (Goodman, 2001). Thus, the lower the per-
plexity of the language model, the better it is.
Clark (2003) proposed a perplexity based test
for the quality of his POS induction algorithm. In
that test, a bigram class-based language model is
trained on a training corpus (using the tagging of
the unsupervised tagger) and applied to another
test corpus. In such a model the transition prob-
ability from a word wj to a word wi is given
by p(C(wi)|C(wj)) where C(wk) is the class as-
signed by the POS induction algorithm to wk. In
the training phase the bigram transition probabili-
ties are computed using the training corpus, and in
1To make the algorithm more time efficient, for each word
wi we perform only three iterations of the searching for cbest,
and for each cbest candidate we compute for at most 500
words the effect on the probability function of the removal
to ccurr.
1/ks
</bodyText>
<page confidence="0.98092">
59
</page>
<figureCaption confidence="0.5158345">
Figure 2: Left: average perplexity vs. the param-
eter K (tightness of the entropy outliers filter; see
</figureCaption>
<bodyText confidence="0.999802208333333">
text for a full explanation). Right: Spearman’s
rank correlation between perplexity and an exter-
nal (many-to-one) quality of the clustering as a
function of K. The three curves are for ZCC,
using different exponents (triangles: 0.9, circles:
1.3, solid: 1.1). A model whose quality improves
(decreased perplexity) with K (left) demonstrates
better correlation between perplexity and external
quality (right). In all three graphs the x axis is in
units of 5K (e.g., a graph x value of 2 means that
10 clusterings were removed from the top of the
list and 10 from its bottom).
the test phase the perplexity of the learned model
is evaluated on the test corpus. Better POS induc-
tion algorithms yield lower perplexity language
models. However, Clark did not study the correla-
tion between the perplexity measure and the gold
standard tagging.
In this paper, we use Clark’s perplexity based
test as the unsupervised quality test used by the
family Q. To provide a high quality prediction, this
test should highly correlate with external cluster-
ing quality. To the best of our knowledge, such a
correlation has not been explored so far.
</bodyText>
<subsectionHeader confidence="0.99905">
2.4 Unsupervised Parameter Selection
</subsectionHeader>
<bodyText confidence="0.999809873015873">
The base ZCC algorithm has one input parame-
ter, the exponent s of the Zipfian distribution. Vir-
tually all unsupervised algorithms utilize param-
eters whose values affect their results. While it
is methodologically valid to simply determine a
value based on reasonable considerations or a de-
velopment set, to keep the fully unsupervised na-
ture of our work we now present a method for
identifying the best parameter assignment. The
method also casts some additional interesting light
on the nature of the problem.
Like cluster type size, the distribution of cluster
instance size in natural languages is also Zipfian
(see Figure 3). A naive application of this con-
straint into the ZCC algorithm would be to allow
swapping words between clusters only if they an-
notate the same number of word instances in the
corpus. However, this constraint, either by itself
or in combination with the cluster type size con-
straint, is too restrictive.
We utilize it for parameter selection as follows.
Recall that our family of algorithms Q(B) runs a
base tagger B several times. Each specific run
yields a clustering CZ. The final result is selected
from the set of clusterings C = {CZ}. We do
not explicitly address the number of instances con-
tained in a cluster, but we can prune from C those
clusterings for which this distribution is very dif-
ferent. Again, imposing a constraint that is known
to hold reduces quality fluctuations between dif-
ferent runs.
To measure the similarity between the cluster
instance size distribution of two clusterings in-
duced by two runs of the algorithm, we treat the
clusters induced by a given run as samples from
a random variable. The events of this variable are
the induced clusters and the probability assigned
to each event is equal to the number of word in-
stances contained in the corresponding cluster, di-
vided by the total number of word instances in the
tagged corpus. The entropy of this random vari-
able is used as a statistic for the word instance
distribution. Clusterings having similar cluster in-
stance size distributions also have similar values
of this statistic.
We apply an entropy outliers filter to the set of
clusterings C. In this filter, we sort the members
of C (these are clusterings obtained in different
runs of the base tagger) according to their clus-
ter instance size entropy, and prune K runs from
the beginning and K runs from the end of the list.
The perplexity-based quality test described above
is applied only to members of C that were not
pruned in this step.
Figure 2 (left) shows the average perplexity of
a set of clusterings as a function of the parame-
ter K of the entropy-based filter. Results are pre-
sented for 100 runs of ZCC2 with three different
exponent values (0.9, 1.1, 1.3). These assignments
yield considerably different Zipfian distributions.
While all three models have similar average per-
plexity over all 100 runs, only the solid line (cor-
responding to an exponent value of 1.1) consis-
</bodyText>
<figure confidence="0.977965052631579">
2See Section 4 for the experimental setup.
890
0.9
K
K
Average Perplexity
888
Rank Correlation
0.8
0.7
0.6
0.5
2 4 6 8 10
2 4 6 8 10
880
0.4
886
884
882
</figure>
<page confidence="0.968247">
60
</page>
<bodyText confidence="0.999925">
tently decreases (improves) with K. The circled
line (corresponding to an exponent value of 1.3)
monotonically decreases with K until a certain K
value, while the line with triangles (correspond-
ing to an exponent value of 0.9) remains relatively
constant.
Figure 2 (right) shows that models for which
the entropy-based filter improves perplexity more
drastically, exhibit better correlation between per-
plexity and external clustering quality3.
Our unsupervised parameter selection method is
thus based on finding a value which exhibits a con-
sistent decrease in perplexity as a function of K,
the number of clusterings pruned from the begin-
ning and end of the entropy-sorted list. In the rest
of this paper we show results where the exponent
value is 1.1.
</bodyText>
<sectionHeader confidence="0.99922" genericHeader="method">
3 Previous Work
</sectionHeader>
<bodyText confidence="0.99837875">
Unsupervised POS induction/tagging is a fruitful
area of research. A major direction is Hidden
Markov Models (HMM) (Merialdo, 1994; Banko
and Moore, 2004; Wang and Schuurmans, 2005).
Several recent works have tried to improve this
model using Bayesian estimation (Goldwater and
Griffiths, 2007; Johnson, 2007; Gao and Johnson,
2008), sophisticated initialization (Goldberg et al.,
2008), induction of an initial clustering used to
train an HMM (Freitag, 2004; Biemann, 2006),
infinite HMM models (Van Gael et al., 2009), in-
tegration of integer linear programming into the
parameter estimation process (Ravi and Knight,
2009), and biasing the model such that the num-
ber of possible tags that each word can get is small
(Grac¸a et al., 2009).
The Bayesian works integrated into the model
information about the distribution of words to POS
tags. For example, Johnson (2007) integrated to
the EM-HMM model a prior that prefers cluster-
ings where the distributions of hidden states to
words is skewed.
Other approaches include transformation based
learning (Brill, 1995), contrastive estimation for
conditional random fields (Smith and Eisner,
2005), Markov random fields (Haghighi and
Klein, 2006), a multilingual approach (Snyder et
al., 2008; Snyder et al., 2008) and expanding a
</bodyText>
<footnote confidence="0.995182">
3The figure is for greedy many-to-one mapping and
Spearman’s rank correlation coefficient, explained in further
Sections. Other external measures and rank correlation scores
demonstrate the same pattern.
</footnote>
<bodyText confidence="0.999845117647059">
partial dictionary and use it to learn disambigua-
tion rules (Zhao and Marcus, 2009).
These works, except (Haghighi and Klein,
2006; Johnson, 2007; Gao and Johnson, 2008)
and one experiment in (Goldwater and Griffiths,
2007), used a dictionary listing the allowable tags
for each word in the text. This dictionary is usu-
ally extracted from the manual tagging of the text,
contradicting the unsupervised nature of the task.
Clearly, the availability of such a dictionary is not
always a reasonable assumption (see e.g. (Gold-
water and Griffiths, 2007)).
In a different algorithmic direction, (Schuetze,
1995) applied latent semantic analysis with SVD
based dimensionality reduction, and (Schuetze,
1995; Clark, 2003; Dasgupta and NG, 2007) used
distributional and morphological statistics to find
meaningful word types clusters. Clark (2003) is
the only such work to have evaluated its algorithm
as a POS tagger for large corpora, like we do in
this paper.
A Zipfian constraint was utilized in (Goldwater
and et al., 2006) for language modeling and mor-
phological disambiguation.
The problem of convergence to local maxima
has been discussed in (Smith and Eisner, 2005;
Haghighi and Klein, 2006; Goldwater and Grif-
fiths, 2007; Johnson, 2007; Gao and Johnson,
2008) with a detailed demonstration in (Johnson,
2007). All these authors (except Smith and Eisner
(2005), see below), however, reported average re-
sults over several runs and did not try to identify
the runs that produce high quality tagging.
Smith and Eisner (2005) initialized with all
weights equal to zero (uninformed, deterministic
initialization) and performed unsupervised model
selection across smoothing parameters by evaluat-
ing the training criterion on unseen, unlabeled de-
velopment data. In this paper we show that for the
tagger of (Clark, 2003) such a method provides
mediocre results (Table 2) even when the train-
ing criterion (likelihood or data probability for this
tagger) is evaluated on the test set. Moreover, we
show that our algorithm outperforms existing POS
taggers for most evaluation measures (Table 3).
Identifying good solutions among many runs of
a randomly-initialized algorithm is a well known
problem. We discuss here the work of (Smith and
Eisner, 2004) that addressed the problem in the un-
supervised POS tagging context. In this work, de-
terministic annealing (Rose et al., 1990) was ap-
</bodyText>
<page confidence="0.998431">
61
</page>
<bodyText confidence="0.999873125">
plied to an HMM model for unsupervised POS
tagging with a dictionary. This method is not sen-
sitive to its initialization, and while it is not the-
oretically guaranteed to converge to a better so-
lution than the traditional EM-HMM, it was ex-
perimentally shown to achieve better results. The
problem has, of course, been addressed in other
contexts as well (see, e.g., (Wang et al., 2002)).
</bodyText>
<sectionHeader confidence="0.985497" genericHeader="method">
4 Experimental Setup and Evaluation
</sectionHeader>
<bodyText confidence="0.99982654054054">
Setup. We used the English WSJ PennTreebank
corpus in our experiments. We induced POS tags
for sections 2-21 (43K word types, 950K word in-
stances of which 832K (87.6%) are not punctua-
tion marks), using Q(ZCC), Q(CT), and CT. For
the unsupervised quality test, we trained the bi-
gram class-based language model on sections 2-21
with the induced clusters, and computed its per-
plexity on section 23.
In Q(ZCC) and Q(CT), the base taggers were
run a 100 times each, using different random ini-
tializations. In each run we induce 13 clusters,
since this is the number of unique POS tags re-
quired to cover 98% of the word types in WSJ
(Figure 3)4. Some previous work (e.g., (Smith and
Eisner, 2005)) also induced 13 non-punctuation
tags.
We compare the results of our algorithm to
those of the original Clark algorithm5. The in-
duced clusters are evaluated against two POS tag
sets: one is the full set of WSJ POS tags, and the
other consists of the non-punctuation tags of the
first set.
Punctuation marks constitute a sizeable volume
of corpus tokens and are easy to cluster correctly.
Hence, evaluting against the full tag set that in-
cludes punctuation artificially increases the qual-
ity of the reported results, which is why we report
results for the non-punctuation tag set. However,
to be able to directly compare with previous work,
we also report results for the full WSJ POS tag
set. We do so by assigning a singleton cluster to
each punctuation mark (in addition to the 13 clus-
ters). This simple heuristic yields very high per-
formance on punctuation, scoring (when all other
terminals are assumed perfect tagging) 99.6% in
1-to-1 accuracy.
</bodyText>
<footnote confidence="0.9863116">
4Some words can get more than one POS tag. In the fig-
ure, for these words we increased the counters of all their
possible tags.
5Downloaded from www.cs.rhul.ac.uk/home/alexc/
RHUL/Downloads.html.
</footnote>
<bodyText confidence="0.999862145833334">
In addition to comparing the different algo-
rithms, we compare the correlation between our
tagging quality test and external clustering quality
for both the original CT algorithm and our ZCC
algorithm.
Clustering Quality Evaluation. The induced
POS tags have arbitrary names. To evaluate them
against a manually annotated corpus, a proper
correspondence with the gold standard POS tags
should be established. Many evaluation measures
for unsupervised clustering against gold standard
exist. Here we use measures from two well ac-
cepted families: mapping based and information
theoretic (IT) based. For a recent discussion on
this subject see (Reichart and Rappoport, 2009).
The mapping based measures are accuracy with
greedy many-to-1 (M-1) and with greedy 1-to-1
(1-1) mappings of the induced to the gold labels.
In the former mapping, two induced clusters can
be mapped to the same gold standard cluster, while
in the latter mapping each and every induced clus-
ter is assigned a unique gold cluster.
After each induced label is mapped to a gold
label, tagging accuracy is computed. Accuracy is
defined to be the number of correctly tagged words
in the corpus divided by the total number of words
in the corpus.
The IT based measures we use are V (Rosen-
berg and Hirschberg, 2007) and NVI (Reichart and
Rappoport, 2009). The latter is a normalization of
the VI measure (Meila, 2007). VI and NVI induce
the same order over clusterings but NVI values for
good clusterings lie in [0, 1]. For V, the higher
the score, the better the clustering. For NVI lower
scores imply improved clustering quality. We use
e as the base of the logarithm.
Evaluation of the Quality Test. To mea-
sure the correlation between the score produced
by the tagging quality test and the external qual-
ity of a tagging, we use two well accepted mea-
sures: Spearman’s rank correlation coefficient
and Kendall Tau (Kendall and Dickinson, 1990).
These measure the correlation between two sorted
lists. For the computation of these measures, we
rank the clusterings once according to the identifi-
cation criterion and once according to the external
quality measure.
The measures are given by the equations:
</bodyText>
<listItem confidence="0.766212">
(6) kndall − tau = 2(nc−n�)
r(r−1)
(7) 5pearsman = 1 − 6 E2 1 d�z
r(r —1)
</listItem>
<page confidence="0.997896">
62
</page>
<figureCaption confidence="0.7776544">
Figure 3: The fraction of word types (solid curve)
and word instances (dashed curve) labeled with
the k (X axis) most frequent POS tags (in types
and tokens respectively) in sections 2-21 of the
WSJ corpus.
</figureCaption>
<bodyText confidence="0.992234615384615">
where r is the number of runs (100 in our case),
n, and nd are the numbers of concordant and dis-
cordant pairs respectively6 and di is the absolute
value of the difference between the ranks of item
i.
The two measures have the properties that a
perfect agreement between rankings results in a
score of 1, a perfect disagreement results in a score
of −1, completely independent rankings have the
value of 0 on the average, the range of values is
between −1 and 1, and increasing values imply
increasing agreement between the rankings. For a
discussion see (Lapata, 2006).
</bodyText>
<sectionHeader confidence="0.999836" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999837294117647">
Table 1 presents the results of the Q(ZCC) and
Q(CT) algorithms, which are both better than
those of the original Clark tagger CT. The Q al-
gorithms provide a tagging that is better than that
produced by CT in 82-100% (Q(ZCC)) and 75-
100% (Q(CT)) of the cases.
The Q(ZCC) algorithm is superior when eval-
uated with the mapping based measures. The
Q(CT) algorithm is superior when evaluated with
the IT measures.
Table 3 presents reported results for all recent
algorithms we are aware of that tackled the task
of unsupervised POS induction from plain text 7.
The settings of the various experiments vary in
terms of the exact gold annotation scheme used
for evaluation (the full WSJ set was used by all
authors except Goldwater and Griffiths (2007) and
</bodyText>
<footnote confidence="0.704134333333333">
6A pair r, t in two lists X and Y is concordant if
sign(Xt − X,.) = sign(Yt − Y,.), where X, is the index
of r in the list X.
7VG and GG used 2 as the base of the logarithm in IT
measures, which affects VI. We converted the VI numbers
reported in their papers to base e.
</footnote>
<bodyText confidence="0.9998935">
the GGTP-17 model which used the set of 17
coarse grained tags proposed by (Smith and Eis-
ner, 2005)) and the size of the test set. The num-
bers reported for the algorithms of other works are
the average performance over multiple runs, since
no method for identification of high quality tag-
gings was used.
The results of our algorithms are superior, ex-
cept for the M-1 performance of some of the mod-
els of (Johnson, 2007) and of the GGTP-17 and
GGTP-45 models of (Grac¸a et al., 2009). Note
that the models of (Johnson, 2007) and the GGTP-
45 model induce 40-50 clusters compared to our
34 (13 non-punctuation plus the additional 21 sin-
gleton punctuation tags). Increasing the number
of clusters is known to improve the M-1 mea-
sure (Reichart and Rappoport, 2009). GGTP-17
gives the best M-1 results, but its 1-1 results are
much worse than those of Q(ZCC), Q(CT), and
CT, and the information theoretic measures V and
NVI were not reported for it.
Recall that the Q algorithms tag punctuation
marks according to the scheme which assigns each
of them a unique cluster (Section 4), while previ-
ous work does not distinguish punctuation marks
from other tokens. To quantify the effect vari-
ous punctuation schemes have on the results re-
ported in Table 3, we evaluated the ‘iHMM: PY-
fixed’ model (Van Gael et al., 2009) and the Q al-
gorithms when punctuation is excluded and when
both PY-fixed and Q algorithms use the punctua-
tion scheme described in Section 4.
For the PY-fixed, which induces 91 clusters,
results are (punctuation is excluded, heuristic is
used): V(0.530, 0.608), NVI (0.999, 0.823), 1-1
(0.484, 0.543), M-1 (0.591, 0.639). The results
for the Q algorithms are given in Table 1 (top
line: excluding punctuation, bottom line: using
the heuristic). The Q algorithms are better for the
V, NVI and 1-1 measures. For M-1 evaluation,
PY-fixed, which induces substantially more clus-
ters (91 compared to our 34) is better.
In what follows, we provide an analysis of the
components of our algorithms. To explore the
quality of our tagging component, ZCC, table 4
compares the mean, mode and standard deviation
of a 100 runs of ZCC with 100 runs of the original
CT algorithm8. The performance of the tagging
</bodyText>
<footnote confidence="0.97893375">
8In mode calculation we treat the 100 runs as samples of
a continuous random variable. We divide the results range
to 10 bins of the same size. The mode is the center of the
bin having the largest number of runs. If there is more than
</footnote>
<figure confidence="0.989028333333333">
1
0.8
0.6
0.4
0.2
0
0 10 20 30 40
Number of POS Tags
Fraction of Items
</figure>
<page confidence="0.994837">
63
</page>
<table confidence="0.931765142857143">
Alg. V NVI 1-1 M-1
Q(ZCC)
no punct. 0.538 (85, 2.6) 0.849 (82, 3.2) 0.521 (100, 4.3) 0.533 (84, 1.7)
with punct. 0.637 (85, 1.8) 0.678 (82, 2.6) 0.58 (100, 3) 0.591 (84, 1.18)
Q(CT) 0.545 (92, 3.3) 0.837 (88, 4.4) 0.492 (99,1.4) 0.526 (75, 1)
no punct. 0.644 (92, 2.5) 0.662 (88, 4.2) 0.555 (99, 0.5) 0.585 (75, 0.58)
with punct.
</table>
<tableCaption confidence="0.966993">
Table 1: Quality of the tagging produced by Q(ZCC) and Q(CT). The top (bottom) line for each algorithm
</tableCaption>
<bodyText confidence="0.9961844">
presents the results when punctuation is not included (is included) in the evaluation (Section 4). The left
number in the parentheses is the fraction of Clark’s (CT) results that scored worse than our models (%
from 100 runs). The right number in the parentheses is 100 times the difference between the score of our
model and the mean score of 100 runs of Clark’s (CT). Q(ZCC) is better than Q(CT) in the mappings
measures, while Q(CT) is better in the IT measures. Both are better than the original Clark tagger CT.
</bodyText>
<table confidence="0.9993968">
Data Probability Likelihood Perplexity
Alg. V m-to-1 V m-to-1 V m-to-1
SRC KT SRC KT SRC KT SRC KT SRC KT SRC KT
CT 0.2 0.143 0.071 0.045 0.338 0.23 0.22 0.148 0.568 0.397 0.476 0.33
ZCC 0.134 0.094 0.118 0.078 0.517 0.352 0.453 0.321 0.82 0.62 0.659 0.484
</table>
<tableCaption confidence="0.995763">
Table 2: Correlation of unsupervised quality measures (columns) with clustering quality of two base
</tableCaption>
<bodyText confidence="0.98479109375">
taggers (CT and ZCC, rows). Correlation is measured by Spearman (SRC) and Kendall Tau (KT) rank
correlation coefficients. The quality measures are data probability (left part), likelihood (middle side)
and perplexity (right part), and correlation is between these and two of the external evaluation measures,
m-to-1 mapping and V (results for the other two clustering evaluation measures, 1-1 mapping and NVI,
are very similar). Results for the perplexity quality test used by family Q are superior; data probability
and likelihood provide only a mediocre indication for the quality of induced clustering. Note that the
correlation values are much higher for ZCC than for CT.
components are quite similar, with a small advan-
tage to CT in mean and to ZCC in mode.
Our quality test is based on the perplexity of a
class bigram language model trained with the in-
duced tagging. To emphasize its strength we com-
pare it to two natural quality tests: the likelihood
and value of the probability function to which the
tagging algorithm converges (equations (2) and (1)
in Section 2.1). The results are shown in Table
2 First, we see that our perplexity quality test is
much better correlated with the quality of the tag-
ging induced by both ZCC and CT. Second, the
correlation is indeed much higher for ZCC than
for CT.
The power of Q(ZCC) lies in the combination
between the perplexity-based quality test and the
tagging component ZCC. The performance of the
tagging component ZCC does not provide a def-
inite improvement over the original Clark tagger.
ZCC compromises mean tagging results for an im-
proved correlation between Q’s quality measure
one such bin, we average their centers. We use this technique
since it is rare to see two different runs of either algorithm
with the exact same quality.
and gold standard-based tagging evaluation.
</bodyText>
<sectionHeader confidence="0.99857" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999421190476191">
In this paper we addressed unsupervised POS tag-
ging as a task where the quality of a single tag-
ging is to be reported, rather than the average per-
formance of a tagging algorithm over many runs.
We introduced a family of algorithms Q(B) based
on an unsupervised test for tagging quality that is
used to select a high quality tagging from the out-
put of multiple runs of a POS tagger B.
We introduced the ZCC tagger which modifies
the original Clark tagger by constraining the clus-
tering space using a cluster type size Zipfian con-
straint, conforming with a known property of nat-
ural languages.
We showed that the tagging produced by our
Q(ZCC) algorithm is better than that of the Clark
algorithm with a probability of 82-100%, depend-
ing on the measure used. Moreover, our tagging
outperforms in most evaluation measures the re-
sults reported in all recent works that addressed
the task.
In future work, we intend to try to improve
</bodyText>
<page confidence="0.998365">
64
</page>
<table confidence="0.999764">
Alg. V VI M-1 1-1
Q(ZCC) 0.637 2.06 0.591 0.58
Q(CT) 0.644 2.01 0.585 0.555
CT 0.619 2.14 0.576 0.543
HK – – – 0.413
J – 4.23 - 0.43 - 0.37 –
5.74 0.62 0.47
GG – 2.8 – –
G-J – 4.03 – – 0.4 –
4.47 0.499
VG 0.54 - 2.49 – – –
0.59 2.91
GGTP-45 — — 0.654 0.445
GGTP-17 — — 0.702 0.495
</table>
<tableCaption confidence="0.999391">
Table 3: Comparison of our algorithms with the
</tableCaption>
<bodyText confidence="0.9889828">
recent fully unsupervised POS taggers for which
results are reported. HK: (Haghighi and Klein,
2006), trained and evaluated with a corpus of
193K tokens and 45 induced tags. GG: (Goldwa-
ter and Griffiths, 2007), trained and evaluated with
a corpus of 24K tokens and 17 induced tags. J :
(Johnson, 2007) inducing 25-50 tags (the results
that are higher than Q in the M-1 measure are for
40-50 tags). GJ: (Gao and Johnson, 2008), induc-
ing 50 tags. VG: (Van Gael et al., 2009), inducing
47-192 tags. GGTP-45: (Grac¸a et al., 2009), in-
ducing 45 tags. GGTP-17: (Grac¸a et al., 2009),
inducing 17 tags. All five were trained and evalu-
ated with the full WSJ PTB (1.17M words). Lower
VI values indicates better clustering.
</bodyText>
<table confidence="0.99882">
Statistic V NVI M-1 1-1
CT
Mean 0.512 0.881 0.516 0.478
Mode 0.502 0.886 0.514 0.465
Std 0.022 0.035 0.018 0.028
ZCC
Mean 0.503 0.908 0.512 0.478
Mode 0.509 0.907 0.518 0.47
Std 0.021 0.036 0.018 0.0295
</table>
<tableCaption confidence="0.99694">
Table 4: Average performance of ZCC compared
</tableCaption>
<bodyText confidence="0.959980916666667">
with CT (results presented without punctuation).
Presented are mean, mode (see text for its calcu-
lation), and standard deviation (std). CT mean re-
sults are slightly better, and both algorithms have
about the same standard deviation. ZCC sacrifices
a small amount of mean quality for a good corre-
lation with our quality test, which allows Q(ZCC)
to be much better than the mean of CT and most
of its runs.
our quality measure, experiment with additional
languages, and apply the ‘family of algorithms’
paradigm to additional relevant NLP tasks.
</bodyText>
<sectionHeader confidence="0.996463" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999818488888889">
Michele Banko and Robert C. Moore, 2003. Part of
Speech Tagging in Context. COLING ’04.
Chris Biemann, 2006. Unsupervised Part-of-
Speech Tagging Employing Efficient Graph Cluster-
ing. COLING-ACL ’06 Student Research Work-
shop.
Thorsten Brants, 1997. The NEGRA Export Format.
CLAUS Report, Saarland University.
Eric Brill, 1995. Unsupervised Learning if Disam-
biguation Rules for Part of Speech Tagging. 3rd
Workshop on Very Large Corpora.
Peter F. Brown, Vincent J. Della Pietra, Peter V. de
Souze, Jenifer C. Lai and Robert Mercer, 1992.
Class-Based N-Gram Models of Natural Language.
Computational Linguistics, 18:467-479.
Alexander Clark, 2003. Combining Distributional and
Morphological Information for Part of Speech In-
duction. EACL ’03.
Sajib Dasgupta and Vincent Ng, 2007. Unsu-
pervised Part-of-Speech Acquisition for Resource-
Scarce Languages. EMNLP ’07.
Steven Finch, Nick Chater and Martin Redington,
1995. Acquiring syntactic information from distri-
butional statistics. Connectionist models of memory
and language. UCL Press, London.
Dayne Freitag, 2004. Toward Unsupervised Whole-
Corpus Tagging. COLING ’04.
Jianfeng Gao and Mark Johnson, 2008. A compari-
son of Bayesian estimators for unsupervised Hidden
Markov Model POS taggers. EMNLP ’08.
Yoav Goldberg, Meni Adler and Michael Elhadad,
2008. EM Can Find Pretty Good HMM POS-
Taggers (When Given a Good Start). ACL ’08
Sharon Goldwater, Tom Griffiths, and Mark Johnson,
2006. Interpolating between types and tokens by es-
timating power-law generators. NIPS ’06.
Sharon Goldwater and Tom Griffiths, 2007. A fully
Bayesian approach to unsupervised part-of-speech
tagging. ACL ’07.
Joshua Goodman, 2001. A Bit of Progress in Lan-
guage Modeling, Extended Version. Microsoft Re-
search Technical Report MSR-TR-2001-72.
Jo˜ao Grac¸a, Kuzman Ganchev, Ben Taskar and Fre-
nando Pereira, 2009. Posterior vs. Parameter Spar-
sity in Latent Variable Models. NIPS ’09.
</reference>
<page confidence="0.995234">
65
</page>
<reference confidence="0.960202416666667">
Maurice Kendall and Jean Dickinson, 1990. Rank
Correlation methods. Oxford University Press, New
York.
Noah A. Smith and Jason Eisner, 2005. Contrastive
Estimation: Training Log-Linear Models on Unla-
beled Data. ACL ’05.
Aria Haghighi and Dan Klein, 2006. Prototype-driven
Learning for Sequence Labeling. HLT-NAACL ’06.
Mark Johnson, 2007. Why Doesnt EM Find Good
HMM POS-Taggers? EMNLP-CoNLL ’07.
Harold W. Kuhn, 1955. The Hungarian method for
the assignment problem. Naval Research Logistics
Quarterly, 2:83-97.
Mirella Lapata, 2006. Automatic Evaluation of In-
formation Ordering: Kendall’s Tau. Computational
Linguistics, 4:471-484.
Sven Martin, Jorg Liermann, and Hermann Ney, 1998.
Algorithms for bigram and trigram word clustering.
Speech Communication, 24:19-37.
Marina Meila, 2007. Comparing Clustering - an In-
formation Based Distance. Journal of Multivariate
Analysis, 98:873-895.
Bernard Merialdo, 1994. Tagging English Text with
a Probabilistic Model. Computational Linguistics,
20(2):155-172.
Michael Mitzenmacher , 2004. A Brief History of
Generative Models for Power Law and Lognormal
Distributions. Internet Mathematics, 1(2):226-251.
James Munkres, 1957. Algorithms for the Assignment
and Transportation Problems. Journal of the SIAM,
5(1):32-38.
Hermann Ney, Ute Essen, and Reinhard Kneser,
1994. On structuring probabilistic dependencies in
stochastic language modelling. Computer Speech
and Language, 8:1-38.
Sujith Ravi and Kevin Knight, 2009. Minimized Mod-
els for Unsupervised Part-of-Speech Tagging. ACL
’09.
Roi Reichart and Ari Rappoport, 2009. The NVI Clus-
tering Evaluation Measure. CoNLL ’09.
Kenneth Rose, Eitan Gurewitz, and Geoffrey C. Fox,
1990. Statistical Mechanics and Phase Transitions
in Clustering. Physical Review Letters, 65(8):945-
948.
Andrew Rosenberg and Julia Hirschberg, 2007. V-
Measure: A Conditional Entropy-Based External
Cluster Evaluation Measure. EMNLP ’07.
Hinrich Schuetze, 1995. Distributional part-of-speech
tagging. EACL ’95.
Noah A. Smith and Jason Eisner, 2004. Annealing
Techniques for Unsupervised Statistical Language
Learning. ACL ’04.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein,
and Regina Barzilay, 2009. Adding More Lan-
guages Improves Unsupervised Multilingual Part-
of-Speech Tagging: A Bayesian Non-Parametric
Approach. NAACL ’09.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein,
and Regina Barzilay, 2008. Unsupervised Multi-
lingual Learning for POS Tagging. EMNLP ’08.
Jurgen Van Gael, Andreas Vlachos and Zoubin Ghahra-
mani, 2009. The Infinite HMM for Unsupervised
POS Tagging. EMNLP ’09.
Qin Iris Wang and Dale Schuurmans, 2005. Im-
proved Estimation for Unsupervised Part-of-Speech
Tagging. IEEE NLP-KE ’05.
Shaojun Wang, Dale Schuurmans and Yunxin Zhao,
2002. The Latent Maximum Entropy Principle.
ISIT ’02.
Qiuye Zhao and Mitch Marcus, 2009. A Simple Un-
supervised Learner for POS Disambiguation Rules
Given Only a Minimal Lexicon. EMNLP ’09.
</reference>
<page confidence="0.985448">
66
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.692973">
<title confidence="0.995675">Improved Unsupervised POS Induction Using Intrinsic Clustering Quality and a Zipfian Constraint</title>
<author confidence="0.999607">Roi Reichart Raanan Fattal Ari Rappoport</author>
<affiliation confidence="0.990477">ICNC Institute of computer science Institute of computer science</affiliation>
<author confidence="0.735096">The Hebrew University The Hebrew University The Hebrew</author>
<email confidence="0.890822">roiri@cs.huji.ac.ilraananf@cs.huji.ac.ilarir@cs.huji.ac.il</email>
<abstract confidence="0.999913793103448">Modern unsupervised POS taggers usually apply an optimization procedure to a nonconvex function, and tend to converge to local maxima that are sensitive to starting conditions. The quality of the tagging induced by such algorithms is thus highly variable, and researchers report average results over several random initializations. Consequently, applications are not guaranteed to use an induced tagging of the quality reported for the algorithm. In this paper we address this issue using an unsupervised test for intrinsic clustering quality. We run a base tagger with different random initializations, and select the best tagging using the quality test. As a base tagger, we modify a leading unsupervised POS tagger (Clark, 2003) to constrain the distributions of word types across clusters to be Zipfian, allowing us to utilize a perplexity-based quality test. We show that the correlation between our quality test and gold standard-based tagging quality measures is high. Our results are better in most evaluation measures than all results reported in the literature for this task, and are always better than the Clark average results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Robert C Moore</author>
</authors>
<title>Part of Speech Tagging in Context.</title>
<date>2003</date>
<journal>COLING</journal>
<volume>04</volume>
<marker>Banko, Moore, 2003</marker>
<rawString>Michele Banko and Robert C. Moore, 2003. Part of Speech Tagging in Context. COLING ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
</authors>
<title>Unsupervised Part-ofSpeech Tagging Employing Efficient Graph Clustering.</title>
<date>2006</date>
<journal>COLING-ACL ’06 Student Research Workshop.</journal>
<contexts>
<context position="18215" citStr="Biemann, 2006" startWordPosition="3050" endWordPosition="3051">he beginning and end of the entropy-sorted list. In the rest of this paper we show results where the exponent value is 1.1. 3 Previous Work Unsupervised POS induction/tagging is a fruitful area of research. A major direction is Hidden Markov Models (HMM) (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005). Several recent works have tried to improve this model using Bayesian estimation (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008), sophisticated initialization (Goldberg et al., 2008), induction of an initial clustering used to train an HMM (Freitag, 2004; Biemann, 2006), infinite HMM models (Van Gael et al., 2009), integration of integer linear programming into the parameter estimation process (Ravi and Knight, 2009), and biasing the model such that the number of possible tags that each word can get is small (Grac¸a et al., 2009). The Bayesian works integrated into the model information about the distribution of words to POS tags. For example, Johnson (2007) integrated to the EM-HMM model a prior that prefers clusterings where the distributions of hidden states to words is skewed. Other approaches include transformation based learning (Brill, 1995), contrast</context>
</contexts>
<marker>Biemann, 2006</marker>
<rawString>Chris Biemann, 2006. Unsupervised Part-ofSpeech Tagging Employing Efficient Graph Clustering. COLING-ACL ’06 Student Research Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>The NEGRA Export Format.</title>
<date>1997</date>
<tech>CLAUS Report,</tech>
<institution>Saarland University.</institution>
<marker>Brants, 1997</marker>
<rawString>Thorsten Brants, 1997. The NEGRA Export Format. CLAUS Report, Saarland University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Unsupervised Learning if Disambiguation Rules for Part of Speech Tagging.</title>
<date>1995</date>
<booktitle>3rd Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="18805" citStr="Brill, 1995" startWordPosition="3145" endWordPosition="3146">, 2004; Biemann, 2006), infinite HMM models (Van Gael et al., 2009), integration of integer linear programming into the parameter estimation process (Ravi and Knight, 2009), and biasing the model such that the number of possible tags that each word can get is small (Grac¸a et al., 2009). The Bayesian works integrated into the model information about the distribution of words to POS tags. For example, Johnson (2007) integrated to the EM-HMM model a prior that prefers clusterings where the distributions of hidden states to words is skewed. Other approaches include transformation based learning (Brill, 1995), contrastive estimation for conditional random fields (Smith and Eisner, 2005), Markov random fields (Haghighi and Klein, 2006), a multilingual approach (Snyder et al., 2008; Snyder et al., 2008) and expanding a 3The figure is for greedy many-to-one mapping and Spearman’s rank correlation coefficient, explained in further Sections. Other external measures and rank correlation scores demonstrate the same pattern. partial dictionary and use it to learn disambiguation rules (Zhao and Marcus, 2009). These works, except (Haghighi and Klein, 2006; Johnson, 2007; Gao and Johnson, 2008) and one exper</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill, 1995. Unsupervised Learning if Disambiguation Rules for Part of Speech Tagging. 3rd Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V de Souze</author>
<author>Jenifer C Lai</author>
<author>Robert Mercer</author>
</authors>
<title>Class-Based N-Gram Models of Natural Language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--467</pages>
<marker>Brown, Pietra, de Souze, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Peter V. de Souze, Jenifer C. Lai and Robert Mercer, 1992. Class-Based N-Gram Models of Natural Language. Computational Linguistics, 18:467-479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Combining Distributional and Morphological Information for Part of Speech Induction.</title>
<date>2003</date>
<journal>EACL</journal>
<volume>03</volume>
<contexts>
<context position="1069" citStr="Clark, 2003" startWordPosition="156" endWordPosition="157">o local maxima that are sensitive to starting conditions. The quality of the tagging induced by such algorithms is thus highly variable, and researchers report average results over several random initializations. Consequently, applications are not guaranteed to use an induced tagging of the quality reported for the algorithm. In this paper we address this issue using an unsupervised test for intrinsic clustering quality. We run a base tagger with different random initializations, and select the best tagging using the quality test. As a base tagger, we modify a leading unsupervised POS tagger (Clark, 2003) to constrain the distributions of word types across clusters to be Zipfian, allowing us to utilize a perplexity-based quality test. We show that the correlation between our quality test and gold standard-based tagging quality measures is high. Our results are better in most evaluation measures than all results reported in the literature for this task, and are always better than the Clark average results. 1 Introduction Unsupervised part-of-speech (POS) induction is of major theoretical and practical importance. It counters the arbitrary nature of manually designed tag sets, and avoids manual </context>
<context position="2478" citStr="Clark, 2003" startWordPosition="375" endWordPosition="376">nvex function, and tend to converge to local maxima that strongly depend on the algorithm’s (usually random) initialization. The quality of the taggings produced by different initializations varies substantially. Figure 1 demonstrates this phenomenon for a leading POS induction algorithm (Clark, 2003). The absolute variability of the induced tagging quality is 10-15%, which is around 20% of the mean. Strong variability has also been reported by other authors (Section 3). The common practice in the literature is to report mean results over several random initializations of the algorithm (e.g. (Clark, 2003; Smith and Eisner, 2005; Goldwater and Griffiths, 2007; Johnson, 2007)). This means that applications using the induced tagging are not guaranteed to use a tagging of the reported quality. In this paper we address this issue using an unsupervised test for intrinsic clustering quality. We present a quality-based algorithmic family Q. Each of its concrete member algorithms Q(B) runs a base tagger B with different random initializations, and selects the best tagging according the quality test. If the test is highly positively correlated with external tagging quality measures (e.g., those based o</context>
<context position="6024" citStr="Clark, 2003" startWordPosition="966" endWordPosition="967">mbership function g can be used to tag a corpus if it is deterministic (as the function learned in this work) or if a rule for selecting a single tag for every word is provided. Most modern unsupervised POS taggers produce taggings of variable quality that strongly depend on their initialization. Our approach towards generating a single high quality tagging is to use a family of algorithms Q. Each member Q(B) of Q utilizes a base tagger B, which is run using several random initializations. The final output is selected according to an unsupervised quality test. We focus here on Clark’s tagger (Clark, 2003) (CT), probably the leading POS induction algorithm (see Table 3). We start with a description of the original CT. We then detail ZCC, a modification of CT that constrains the clustering space by adding a Zipfbased constraint. Our perplexity-based unsupervised tagging quality test is discussed next. Finally, we provide an unsupervised technique for selecting the parameter of the Zipfian constraint. 2.1 The Original Clark Tagger (CT) The tagger’s statistical model combines distributional and morphological information with the likelihood function of the Brown algorithm (Brown et al., 1992; Ney e</context>
<context position="11812" citStr="Clark (2003)" startWordPosition="1974" endWordPosition="1975">n of High Quality Runs Perplexity is a standard measure for language model evaluation. A language model defines the transition probabilities for every word wi given the words that precede it. The perplexity of a language model for a given corpus having N words is defined to be � � �N 1 N �i=1 p(wi|w1 ... wi−1) An important property of perplexity that makes it attractive as a measure for language model performance is that in some sense the best model for any corpus has the lowest perplexity for that corpus (Goodman, 2001). Thus, the lower the perplexity of the language model, the better it is. Clark (2003) proposed a perplexity based test for the quality of his POS induction algorithm. In that test, a bigram class-based language model is trained on a training corpus (using the tagging of the unsupervised tagger) and applied to another test corpus. In such a model the transition probability from a word wj to a word wi is given by p(C(wi)|C(wj)) where C(wk) is the class assigned by the POS induction algorithm to wk. In the training phase the bigram transition probabilities are computed using the training corpus, and in 1To make the algorithm more time efficient, for each word wi we perform only t</context>
<context position="19934" citStr="Clark, 2003" startWordPosition="3312" endWordPosition="3313">ept (Haghighi and Klein, 2006; Johnson, 2007; Gao and Johnson, 2008) and one experiment in (Goldwater and Griffiths, 2007), used a dictionary listing the allowable tags for each word in the text. This dictionary is usually extracted from the manual tagging of the text, contradicting the unsupervised nature of the task. Clearly, the availability of such a dictionary is not always a reasonable assumption (see e.g. (Goldwater and Griffiths, 2007)). In a different algorithmic direction, (Schuetze, 1995) applied latent semantic analysis with SVD based dimensionality reduction, and (Schuetze, 1995; Clark, 2003; Dasgupta and NG, 2007) used distributional and morphological statistics to find meaningful word types clusters. Clark (2003) is the only such work to have evaluated its algorithm as a POS tagger for large corpora, like we do in this paper. A Zipfian constraint was utilized in (Goldwater and et al., 2006) for language modeling and morphological disambiguation. The problem of convergence to local maxima has been discussed in (Smith and Eisner, 2005; Haghighi and Klein, 2006; Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008) with a detailed demonstration in (Johnson, 2007). A</context>
</contexts>
<marker>Clark, 2003</marker>
<rawString>Alexander Clark, 2003. Combining Distributional and Morphological Information for Part of Speech Induction. EACL ’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sajib Dasgupta</author>
<author>Vincent Ng</author>
</authors>
<title>Unsupervised Part-of-Speech Acquisition for ResourceScarce Languages.</title>
<date>2007</date>
<journal>EMNLP</journal>
<volume>07</volume>
<marker>Dasgupta, Ng, 2007</marker>
<rawString>Sajib Dasgupta and Vincent Ng, 2007. Unsupervised Part-of-Speech Acquisition for ResourceScarce Languages. EMNLP ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Finch</author>
<author>Nick Chater</author>
<author>Martin Redington</author>
</authors>
<title>Acquiring syntactic information from distributional statistics. Connectionist models of memory and language.</title>
<date>1995</date>
<publisher>UCL Press,</publisher>
<location>London.</location>
<marker>Finch, Chater, Redington, 1995</marker>
<rawString>Steven Finch, Nick Chater and Martin Redington, 1995. Acquiring syntactic information from distributional statistics. Connectionist models of memory and language. UCL Press, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dayne Freitag</author>
</authors>
<title>Toward Unsupervised WholeCorpus Tagging.</title>
<date>2004</date>
<journal>COLING</journal>
<volume>04</volume>
<contexts>
<context position="18199" citStr="Freitag, 2004" startWordPosition="3048" endWordPosition="3049">s pruned from the beginning and end of the entropy-sorted list. In the rest of this paper we show results where the exponent value is 1.1. 3 Previous Work Unsupervised POS induction/tagging is a fruitful area of research. A major direction is Hidden Markov Models (HMM) (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005). Several recent works have tried to improve this model using Bayesian estimation (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008), sophisticated initialization (Goldberg et al., 2008), induction of an initial clustering used to train an HMM (Freitag, 2004; Biemann, 2006), infinite HMM models (Van Gael et al., 2009), integration of integer linear programming into the parameter estimation process (Ravi and Knight, 2009), and biasing the model such that the number of possible tags that each word can get is small (Grac¸a et al., 2009). The Bayesian works integrated into the model information about the distribution of words to POS tags. For example, Johnson (2007) integrated to the EM-HMM model a prior that prefers clusterings where the distributions of hidden states to words is skewed. Other approaches include transformation based learning (Brill,</context>
</contexts>
<marker>Freitag, 2004</marker>
<rawString>Dayne Freitag, 2004. Toward Unsupervised WholeCorpus Tagging. COLING ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Mark Johnson</author>
</authors>
<title>A comparison of Bayesian estimators for unsupervised Hidden Markov Model POS taggers.</title>
<date>2008</date>
<journal>EMNLP</journal>
<volume>08</volume>
<contexts>
<context position="18073" citStr="Gao and Johnson, 2008" startWordPosition="3028" endWordPosition="3031">method is thus based on finding a value which exhibits a consistent decrease in perplexity as a function of K, the number of clusterings pruned from the beginning and end of the entropy-sorted list. In the rest of this paper we show results where the exponent value is 1.1. 3 Previous Work Unsupervised POS induction/tagging is a fruitful area of research. A major direction is Hidden Markov Models (HMM) (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005). Several recent works have tried to improve this model using Bayesian estimation (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008), sophisticated initialization (Goldberg et al., 2008), induction of an initial clustering used to train an HMM (Freitag, 2004; Biemann, 2006), infinite HMM models (Van Gael et al., 2009), integration of integer linear programming into the parameter estimation process (Ravi and Knight, 2009), and biasing the model such that the number of possible tags that each word can get is small (Grac¸a et al., 2009). The Bayesian works integrated into the model information about the distribution of words to POS tags. For example, Johnson (2007) integrated to the EM-HMM model a prior that prefers clusterin</context>
<context position="19391" citStr="Gao and Johnson, 2008" startWordPosition="3228" endWordPosition="3231">rmation based learning (Brill, 1995), contrastive estimation for conditional random fields (Smith and Eisner, 2005), Markov random fields (Haghighi and Klein, 2006), a multilingual approach (Snyder et al., 2008; Snyder et al., 2008) and expanding a 3The figure is for greedy many-to-one mapping and Spearman’s rank correlation coefficient, explained in further Sections. Other external measures and rank correlation scores demonstrate the same pattern. partial dictionary and use it to learn disambiguation rules (Zhao and Marcus, 2009). These works, except (Haghighi and Klein, 2006; Johnson, 2007; Gao and Johnson, 2008) and one experiment in (Goldwater and Griffiths, 2007), used a dictionary listing the allowable tags for each word in the text. This dictionary is usually extracted from the manual tagging of the text, contradicting the unsupervised nature of the task. Clearly, the availability of such a dictionary is not always a reasonable assumption (see e.g. (Goldwater and Griffiths, 2007)). In a different algorithmic direction, (Schuetze, 1995) applied latent semantic analysis with SVD based dimensionality reduction, and (Schuetze, 1995; Clark, 2003; Dasgupta and NG, 2007) used distributional and morpholo</context>
<context position="35268" citStr="Gao and Johnson, 2008" startWordPosition="5963" endWordPosition="5966">3 - 0.37 – 5.74 0.62 0.47 GG – 2.8 – – G-J – 4.03 – – 0.4 – 4.47 0.499 VG 0.54 - 2.49 – – – 0.59 2.91 GGTP-45 — — 0.654 0.445 GGTP-17 — — 0.702 0.495 Table 3: Comparison of our algorithms with the recent fully unsupervised POS taggers for which results are reported. HK: (Haghighi and Klein, 2006), trained and evaluated with a corpus of 193K tokens and 45 induced tags. GG: (Goldwater and Griffiths, 2007), trained and evaluated with a corpus of 24K tokens and 17 induced tags. J : (Johnson, 2007) inducing 25-50 tags (the results that are higher than Q in the M-1 measure are for 40-50 tags). GJ: (Gao and Johnson, 2008), inducing 50 tags. VG: (Van Gael et al., 2009), inducing 47-192 tags. GGTP-45: (Grac¸a et al., 2009), inducing 45 tags. GGTP-17: (Grac¸a et al., 2009), inducing 17 tags. All five were trained and evaluated with the full WSJ PTB (1.17M words). Lower VI values indicates better clustering. Statistic V NVI M-1 1-1 CT Mean 0.512 0.881 0.516 0.478 Mode 0.502 0.886 0.514 0.465 Std 0.022 0.035 0.018 0.028 ZCC Mean 0.503 0.908 0.512 0.478 Mode 0.509 0.907 0.518 0.47 Std 0.021 0.036 0.018 0.0295 Table 4: Average performance of ZCC compared with CT (results presented without punctuation). Presented are </context>
</contexts>
<marker>Gao, Johnson, 2008</marker>
<rawString>Jianfeng Gao and Mark Johnson, 2008. A comparison of Bayesian estimators for unsupervised Hidden Markov Model POS taggers. EMNLP ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Meni Adler</author>
<author>Michael Elhadad</author>
</authors>
<date>2008</date>
<journal>EM Can Find Pretty Good HMM POSTaggers (When Given a Good Start). ACL</journal>
<volume>08</volume>
<contexts>
<context position="18127" citStr="Goldberg et al., 2008" startWordPosition="3034" endWordPosition="3037"> a consistent decrease in perplexity as a function of K, the number of clusterings pruned from the beginning and end of the entropy-sorted list. In the rest of this paper we show results where the exponent value is 1.1. 3 Previous Work Unsupervised POS induction/tagging is a fruitful area of research. A major direction is Hidden Markov Models (HMM) (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005). Several recent works have tried to improve this model using Bayesian estimation (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008), sophisticated initialization (Goldberg et al., 2008), induction of an initial clustering used to train an HMM (Freitag, 2004; Biemann, 2006), infinite HMM models (Van Gael et al., 2009), integration of integer linear programming into the parameter estimation process (Ravi and Knight, 2009), and biasing the model such that the number of possible tags that each word can get is small (Grac¸a et al., 2009). The Bayesian works integrated into the model information about the distribution of words to POS tags. For example, Johnson (2007) integrated to the EM-HMM model a prior that prefers clusterings where the distributions of hidden states to words i</context>
</contexts>
<marker>Goldberg, Adler, Elhadad, 2008</marker>
<rawString>Yoav Goldberg, Meni Adler and Michael Elhadad, 2008. EM Can Find Pretty Good HMM POSTaggers (When Given a Good Start). ACL ’08</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Interpolating between types and tokens by estimating power-law generators.</title>
<date>2006</date>
<journal>NIPS</journal>
<volume>06</volume>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Tom Griffiths, and Mark Johnson, 2006. Interpolating between types and tokens by estimating power-law generators. NIPS ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
</authors>
<title>A fully Bayesian approach to unsupervised part-of-speech tagging.</title>
<date>2007</date>
<journal>ACL</journal>
<volume>07</volume>
<contexts>
<context position="2533" citStr="Goldwater and Griffiths, 2007" startWordPosition="381" endWordPosition="384">o local maxima that strongly depend on the algorithm’s (usually random) initialization. The quality of the taggings produced by different initializations varies substantially. Figure 1 demonstrates this phenomenon for a leading POS induction algorithm (Clark, 2003). The absolute variability of the induced tagging quality is 10-15%, which is around 20% of the mean. Strong variability has also been reported by other authors (Section 3). The common practice in the literature is to report mean results over several random initializations of the algorithm (e.g. (Clark, 2003; Smith and Eisner, 2005; Goldwater and Griffiths, 2007; Johnson, 2007)). This means that applications using the induced tagging are not guaranteed to use a tagging of the reported quality. In this paper we address this issue using an unsupervised test for intrinsic clustering quality. We present a quality-based algorithmic family Q. Each of its concrete member algorithms Q(B) runs a base tagger B with different random initializations, and selects the best tagging according the quality test. If the test is highly positively correlated with external tagging quality measures (e.g., those based on gold standard tagging), Q(B) will produce better resu</context>
<context position="18034" citStr="Goldwater and Griffiths, 2007" startWordPosition="3022" endWordPosition="3025">uality3. Our unsupervised parameter selection method is thus based on finding a value which exhibits a consistent decrease in perplexity as a function of K, the number of clusterings pruned from the beginning and end of the entropy-sorted list. In the rest of this paper we show results where the exponent value is 1.1. 3 Previous Work Unsupervised POS induction/tagging is a fruitful area of research. A major direction is Hidden Markov Models (HMM) (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005). Several recent works have tried to improve this model using Bayesian estimation (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008), sophisticated initialization (Goldberg et al., 2008), induction of an initial clustering used to train an HMM (Freitag, 2004; Biemann, 2006), infinite HMM models (Van Gael et al., 2009), integration of integer linear programming into the parameter estimation process (Ravi and Knight, 2009), and biasing the model such that the number of possible tags that each word can get is small (Grac¸a et al., 2009). The Bayesian works integrated into the model information about the distribution of words to POS tags. For example, Johnson (2007) integrated to the EM-H</context>
<context position="19445" citStr="Goldwater and Griffiths, 2007" startWordPosition="3236" endWordPosition="3239">ive estimation for conditional random fields (Smith and Eisner, 2005), Markov random fields (Haghighi and Klein, 2006), a multilingual approach (Snyder et al., 2008; Snyder et al., 2008) and expanding a 3The figure is for greedy many-to-one mapping and Spearman’s rank correlation coefficient, explained in further Sections. Other external measures and rank correlation scores demonstrate the same pattern. partial dictionary and use it to learn disambiguation rules (Zhao and Marcus, 2009). These works, except (Haghighi and Klein, 2006; Johnson, 2007; Gao and Johnson, 2008) and one experiment in (Goldwater and Griffiths, 2007), used a dictionary listing the allowable tags for each word in the text. This dictionary is usually extracted from the manual tagging of the text, contradicting the unsupervised nature of the task. Clearly, the availability of such a dictionary is not always a reasonable assumption (see e.g. (Goldwater and Griffiths, 2007)). In a different algorithmic direction, (Schuetze, 1995) applied latent semantic analysis with SVD based dimensionality reduction, and (Schuetze, 1995; Clark, 2003; Dasgupta and NG, 2007) used distributional and morphological statistics to find meaningful word types cluster</context>
<context position="27639" citStr="Goldwater and Griffiths (2007)" startWordPosition="4599" endWordPosition="4602">rk tagger CT. The Q algorithms provide a tagging that is better than that produced by CT in 82-100% (Q(ZCC)) and 75- 100% (Q(CT)) of the cases. The Q(ZCC) algorithm is superior when evaluated with the mapping based measures. The Q(CT) algorithm is superior when evaluated with the IT measures. Table 3 presents reported results for all recent algorithms we are aware of that tackled the task of unsupervised POS induction from plain text 7. The settings of the various experiments vary in terms of the exact gold annotation scheme used for evaluation (the full WSJ set was used by all authors except Goldwater and Griffiths (2007) and 6A pair r, t in two lists X and Y is concordant if sign(Xt − X,.) = sign(Yt − Y,.), where X, is the index of r in the list X. 7VG and GG used 2 as the base of the logarithm in IT measures, which affects VI. We converted the VI numbers reported in their papers to base e. the GGTP-17 model which used the set of 17 coarse grained tags proposed by (Smith and Eisner, 2005)) and the size of the test set. The numbers reported for the algorithms of other works are the average performance over multiple runs, since no method for identification of high quality taggings was used. The results of our a</context>
<context position="35052" citStr="Goldwater and Griffiths, 2007" startWordPosition="5922" endWordPosition="5926"> in all recent works that addressed the task. In future work, we intend to try to improve 64 Alg. V VI M-1 1-1 Q(ZCC) 0.637 2.06 0.591 0.58 Q(CT) 0.644 2.01 0.585 0.555 CT 0.619 2.14 0.576 0.543 HK – – – 0.413 J – 4.23 - 0.43 - 0.37 – 5.74 0.62 0.47 GG – 2.8 – – G-J – 4.03 – – 0.4 – 4.47 0.499 VG 0.54 - 2.49 – – – 0.59 2.91 GGTP-45 — — 0.654 0.445 GGTP-17 — — 0.702 0.495 Table 3: Comparison of our algorithms with the recent fully unsupervised POS taggers for which results are reported. HK: (Haghighi and Klein, 2006), trained and evaluated with a corpus of 193K tokens and 45 induced tags. GG: (Goldwater and Griffiths, 2007), trained and evaluated with a corpus of 24K tokens and 17 induced tags. J : (Johnson, 2007) inducing 25-50 tags (the results that are higher than Q in the M-1 measure are for 40-50 tags). GJ: (Gao and Johnson, 2008), inducing 50 tags. VG: (Van Gael et al., 2009), inducing 47-192 tags. GGTP-45: (Grac¸a et al., 2009), inducing 45 tags. GGTP-17: (Grac¸a et al., 2009), inducing 17 tags. All five were trained and evaluated with the full WSJ PTB (1.17M words). Lower VI values indicates better clustering. Statistic V NVI M-1 1-1 CT Mean 0.512 0.881 0.516 0.478 Mode 0.502 0.886 0.514 0.465 Std 0.022 </context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Sharon Goldwater and Tom Griffiths, 2007. A fully Bayesian approach to unsupervised part-of-speech tagging. ACL ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>A Bit of Progress in Language Modeling, Extended Version. Microsoft Research</title>
<date>2001</date>
<tech>Technical Report MSR-TR-2001-72.</tech>
<contexts>
<context position="11726" citStr="Goodman, 2001" startWordPosition="1958" endWordPosition="1959">ng all other clusters except of former cbest candidates1. 2.3 Unsupervised Identification of High Quality Runs Perplexity is a standard measure for language model evaluation. A language model defines the transition probabilities for every word wi given the words that precede it. The perplexity of a language model for a given corpus having N words is defined to be � � �N 1 N �i=1 p(wi|w1 ... wi−1) An important property of perplexity that makes it attractive as a measure for language model performance is that in some sense the best model for any corpus has the lowest perplexity for that corpus (Goodman, 2001). Thus, the lower the perplexity of the language model, the better it is. Clark (2003) proposed a perplexity based test for the quality of his POS induction algorithm. In that test, a bigram class-based language model is trained on a training corpus (using the tagging of the unsupervised tagger) and applied to another test corpus. In such a model the transition probability from a word wj to a word wi is given by p(C(wi)|C(wj)) where C(wk) is the class assigned by the POS induction algorithm to wk. In the training phase the bigram transition probabilities are computed using the training corpus,</context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>Joshua Goodman, 2001. A Bit of Progress in Language Modeling, Extended Version. Microsoft Research Technical Report MSR-TR-2001-72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jo˜ao Grac¸a</author>
</authors>
<title>Kuzman Ganchev, Ben Taskar and Frenando Pereira,</title>
<date>2009</date>
<journal>NIPS</journal>
<volume>09</volume>
<marker>Grac¸a, 2009</marker>
<rawString>Jo˜ao Grac¸a, Kuzman Ganchev, Ben Taskar and Frenando Pereira, 2009. Posterior vs. Parameter Sparsity in Latent Variable Models. NIPS ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maurice Kendall</author>
<author>Jean Dickinson</author>
</authors>
<title>Rank Correlation methods.</title>
<date>1990</date>
<publisher>Oxford University Press,</publisher>
<location>New York.</location>
<contexts>
<context position="25767" citStr="Kendall and Dickinson, 1990" startWordPosition="4272" endWordPosition="4275">and NVI (Reichart and Rappoport, 2009). The latter is a normalization of the VI measure (Meila, 2007). VI and NVI induce the same order over clusterings but NVI values for good clusterings lie in [0, 1]. For V, the higher the score, the better the clustering. For NVI lower scores imply improved clustering quality. We use e as the base of the logarithm. Evaluation of the Quality Test. To measure the correlation between the score produced by the tagging quality test and the external quality of a tagging, we use two well accepted measures: Spearman’s rank correlation coefficient and Kendall Tau (Kendall and Dickinson, 1990). These measure the correlation between two sorted lists. For the computation of these measures, we rank the clusterings once according to the identification criterion and once according to the external quality measure. The measures are given by the equations: (6) kndall − tau = 2(nc−n�) r(r−1) (7) 5pearsman = 1 − 6 E2 1 d�z r(r —1) 62 Figure 3: The fraction of word types (solid curve) and word instances (dashed curve) labeled with the k (X axis) most frequent POS tags (in types and tokens respectively) in sections 2-21 of the WSJ corpus. where r is the number of runs (100 in our case), n, and</context>
</contexts>
<marker>Kendall, Dickinson, 1990</marker>
<rawString>Maurice Kendall and Jean Dickinson, 1990. Rank Correlation methods. Oxford University Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Contrastive Estimation: Training Log-Linear Models on Unlabeled Data.</title>
<date>2005</date>
<journal>ACL</journal>
<volume>05</volume>
<contexts>
<context position="2502" citStr="Smith and Eisner, 2005" startWordPosition="377" endWordPosition="380">, and tend to converge to local maxima that strongly depend on the algorithm’s (usually random) initialization. The quality of the taggings produced by different initializations varies substantially. Figure 1 demonstrates this phenomenon for a leading POS induction algorithm (Clark, 2003). The absolute variability of the induced tagging quality is 10-15%, which is around 20% of the mean. Strong variability has also been reported by other authors (Section 3). The common practice in the literature is to report mean results over several random initializations of the algorithm (e.g. (Clark, 2003; Smith and Eisner, 2005; Goldwater and Griffiths, 2007; Johnson, 2007)). This means that applications using the induced tagging are not guaranteed to use a tagging of the reported quality. In this paper we address this issue using an unsupervised test for intrinsic clustering quality. We present a quality-based algorithmic family Q. Each of its concrete member algorithms Q(B) runs a base tagger B with different random initializations, and selects the best tagging according the quality test. If the test is highly positively correlated with external tagging quality measures (e.g., those based on gold standard tagging)</context>
<context position="18884" citStr="Smith and Eisner, 2005" startWordPosition="3153" endWordPosition="3156"> integration of integer linear programming into the parameter estimation process (Ravi and Knight, 2009), and biasing the model such that the number of possible tags that each word can get is small (Grac¸a et al., 2009). The Bayesian works integrated into the model information about the distribution of words to POS tags. For example, Johnson (2007) integrated to the EM-HMM model a prior that prefers clusterings where the distributions of hidden states to words is skewed. Other approaches include transformation based learning (Brill, 1995), contrastive estimation for conditional random fields (Smith and Eisner, 2005), Markov random fields (Haghighi and Klein, 2006), a multilingual approach (Snyder et al., 2008; Snyder et al., 2008) and expanding a 3The figure is for greedy many-to-one mapping and Spearman’s rank correlation coefficient, explained in further Sections. Other external measures and rank correlation scores demonstrate the same pattern. partial dictionary and use it to learn disambiguation rules (Zhao and Marcus, 2009). These works, except (Haghighi and Klein, 2006; Johnson, 2007; Gao and Johnson, 2008) and one experiment in (Goldwater and Griffiths, 2007), used a dictionary listing the allowab</context>
<context position="20386" citStr="Smith and Eisner, 2005" startWordPosition="3383" endWordPosition="3386"> 2007)). In a different algorithmic direction, (Schuetze, 1995) applied latent semantic analysis with SVD based dimensionality reduction, and (Schuetze, 1995; Clark, 2003; Dasgupta and NG, 2007) used distributional and morphological statistics to find meaningful word types clusters. Clark (2003) is the only such work to have evaluated its algorithm as a POS tagger for large corpora, like we do in this paper. A Zipfian constraint was utilized in (Goldwater and et al., 2006) for language modeling and morphological disambiguation. The problem of convergence to local maxima has been discussed in (Smith and Eisner, 2005; Haghighi and Klein, 2006; Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008) with a detailed demonstration in (Johnson, 2007). All these authors (except Smith and Eisner (2005), see below), however, reported average results over several runs and did not try to identify the runs that produce high quality tagging. Smith and Eisner (2005) initialized with all weights equal to zero (uninformed, deterministic initialization) and performed unsupervised model selection across smoothing parameters by evaluating the training criterion on unseen, unlabeled development data. In this p</context>
<context position="22713" citStr="Smith and Eisner, 2005" startWordPosition="3768" endWordPosition="3771">ts. We induced POS tags for sections 2-21 (43K word types, 950K word instances of which 832K (87.6%) are not punctuation marks), using Q(ZCC), Q(CT), and CT. For the unsupervised quality test, we trained the bigram class-based language model on sections 2-21 with the induced clusters, and computed its perplexity on section 23. In Q(ZCC) and Q(CT), the base taggers were run a 100 times each, using different random initializations. In each run we induce 13 clusters, since this is the number of unique POS tags required to cover 98% of the word types in WSJ (Figure 3)4. Some previous work (e.g., (Smith and Eisner, 2005)) also induced 13 non-punctuation tags. We compare the results of our algorithm to those of the original Clark algorithm5. The induced clusters are evaluated against two POS tag sets: one is the full set of WSJ POS tags, and the other consists of the non-punctuation tags of the first set. Punctuation marks constitute a sizeable volume of corpus tokens and are easy to cluster correctly. Hence, evaluting against the full tag set that includes punctuation artificially increases the quality of the reported results, which is why we report results for the non-punctuation tag set. However, to be able</context>
<context position="28014" citStr="Smith and Eisner, 2005" startWordPosition="4678" endWordPosition="4682"> the task of unsupervised POS induction from plain text 7. The settings of the various experiments vary in terms of the exact gold annotation scheme used for evaluation (the full WSJ set was used by all authors except Goldwater and Griffiths (2007) and 6A pair r, t in two lists X and Y is concordant if sign(Xt − X,.) = sign(Yt − Y,.), where X, is the index of r in the list X. 7VG and GG used 2 as the base of the logarithm in IT measures, which affects VI. We converted the VI numbers reported in their papers to base e. the GGTP-17 model which used the set of 17 coarse grained tags proposed by (Smith and Eisner, 2005)) and the size of the test set. The numbers reported for the algorithms of other works are the average performance over multiple runs, since no method for identification of high quality taggings was used. The results of our algorithms are superior, except for the M-1 performance of some of the models of (Johnson, 2007) and of the GGTP-17 and GGTP-45 models of (Grac¸a et al., 2009). Note that the models of (Johnson, 2007) and the GGTP45 model induce 40-50 clusters compared to our 34 (13 non-punctuation plus the additional 21 singleton punctuation tags). Increasing the number of clusters is know</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A. Smith and Jason Eisner, 2005. Contrastive Estimation: Training Log-Linear Models on Unlabeled Data. ACL ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Prototype-driven Learning for Sequence Labeling.</title>
<date>2006</date>
<journal>HLT-NAACL</journal>
<volume>06</volume>
<contexts>
<context position="18933" citStr="Haghighi and Klein, 2006" startWordPosition="3160" endWordPosition="3163"> the parameter estimation process (Ravi and Knight, 2009), and biasing the model such that the number of possible tags that each word can get is small (Grac¸a et al., 2009). The Bayesian works integrated into the model information about the distribution of words to POS tags. For example, Johnson (2007) integrated to the EM-HMM model a prior that prefers clusterings where the distributions of hidden states to words is skewed. Other approaches include transformation based learning (Brill, 1995), contrastive estimation for conditional random fields (Smith and Eisner, 2005), Markov random fields (Haghighi and Klein, 2006), a multilingual approach (Snyder et al., 2008; Snyder et al., 2008) and expanding a 3The figure is for greedy many-to-one mapping and Spearman’s rank correlation coefficient, explained in further Sections. Other external measures and rank correlation scores demonstrate the same pattern. partial dictionary and use it to learn disambiguation rules (Zhao and Marcus, 2009). These works, except (Haghighi and Klein, 2006; Johnson, 2007; Gao and Johnson, 2008) and one experiment in (Goldwater and Griffiths, 2007), used a dictionary listing the allowable tags for each word in the text. This dictionar</context>
<context position="20412" citStr="Haghighi and Klein, 2006" startWordPosition="3387" endWordPosition="3390">algorithmic direction, (Schuetze, 1995) applied latent semantic analysis with SVD based dimensionality reduction, and (Schuetze, 1995; Clark, 2003; Dasgupta and NG, 2007) used distributional and morphological statistics to find meaningful word types clusters. Clark (2003) is the only such work to have evaluated its algorithm as a POS tagger for large corpora, like we do in this paper. A Zipfian constraint was utilized in (Goldwater and et al., 2006) for language modeling and morphological disambiguation. The problem of convergence to local maxima has been discussed in (Smith and Eisner, 2005; Haghighi and Klein, 2006; Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008) with a detailed demonstration in (Johnson, 2007). All these authors (except Smith and Eisner (2005), see below), however, reported average results over several runs and did not try to identify the runs that produce high quality tagging. Smith and Eisner (2005) initialized with all weights equal to zero (uninformed, deterministic initialization) and performed unsupervised model selection across smoothing parameters by evaluating the training criterion on unseen, unlabeled development data. In this paper we show that for the </context>
<context position="34943" citStr="Haghighi and Klein, 2006" startWordPosition="5904" endWordPosition="5907"> on the measure used. Moreover, our tagging outperforms in most evaluation measures the results reported in all recent works that addressed the task. In future work, we intend to try to improve 64 Alg. V VI M-1 1-1 Q(ZCC) 0.637 2.06 0.591 0.58 Q(CT) 0.644 2.01 0.585 0.555 CT 0.619 2.14 0.576 0.543 HK – – – 0.413 J – 4.23 - 0.43 - 0.37 – 5.74 0.62 0.47 GG – 2.8 – – G-J – 4.03 – – 0.4 – 4.47 0.499 VG 0.54 - 2.49 – – – 0.59 2.91 GGTP-45 — — 0.654 0.445 GGTP-17 — — 0.702 0.495 Table 3: Comparison of our algorithms with the recent fully unsupervised POS taggers for which results are reported. HK: (Haghighi and Klein, 2006), trained and evaluated with a corpus of 193K tokens and 45 induced tags. GG: (Goldwater and Griffiths, 2007), trained and evaluated with a corpus of 24K tokens and 17 induced tags. J : (Johnson, 2007) inducing 25-50 tags (the results that are higher than Q in the M-1 measure are for 40-50 tags). GJ: (Gao and Johnson, 2008), inducing 50 tags. VG: (Van Gael et al., 2009), inducing 47-192 tags. GGTP-45: (Grac¸a et al., 2009), inducing 45 tags. GGTP-17: (Grac¸a et al., 2009), inducing 17 tags. All five were trained and evaluated with the full WSJ PTB (1.17M words). Lower VI values indicates bette</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>Aria Haghighi and Dan Klein, 2006. Prototype-driven Learning for Sequence Labeling. HLT-NAACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Why Doesnt EM Find Good HMM POS-Taggers? EMNLP-CoNLL ’07.</title>
<date>2007</date>
<contexts>
<context position="2549" citStr="Johnson, 2007" startWordPosition="385" endWordPosition="386">pend on the algorithm’s (usually random) initialization. The quality of the taggings produced by different initializations varies substantially. Figure 1 demonstrates this phenomenon for a leading POS induction algorithm (Clark, 2003). The absolute variability of the induced tagging quality is 10-15%, which is around 20% of the mean. Strong variability has also been reported by other authors (Section 3). The common practice in the literature is to report mean results over several random initializations of the algorithm (e.g. (Clark, 2003; Smith and Eisner, 2005; Goldwater and Griffiths, 2007; Johnson, 2007)). This means that applications using the induced tagging are not guaranteed to use a tagging of the reported quality. In this paper we address this issue using an unsupervised test for intrinsic clustering quality. We present a quality-based algorithmic family Q. Each of its concrete member algorithms Q(B) runs a base tagger B with different random initializations, and selects the best tagging according the quality test. If the test is highly positively correlated with external tagging quality measures (e.g., those based on gold standard tagging), Q(B) will produce better results than B with </context>
<context position="18049" citStr="Johnson, 2007" startWordPosition="3026" endWordPosition="3027">eter selection method is thus based on finding a value which exhibits a consistent decrease in perplexity as a function of K, the number of clusterings pruned from the beginning and end of the entropy-sorted list. In the rest of this paper we show results where the exponent value is 1.1. 3 Previous Work Unsupervised POS induction/tagging is a fruitful area of research. A major direction is Hidden Markov Models (HMM) (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005). Several recent works have tried to improve this model using Bayesian estimation (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008), sophisticated initialization (Goldberg et al., 2008), induction of an initial clustering used to train an HMM (Freitag, 2004; Biemann, 2006), infinite HMM models (Van Gael et al., 2009), integration of integer linear programming into the parameter estimation process (Ravi and Knight, 2009), and biasing the model such that the number of possible tags that each word can get is small (Grac¸a et al., 2009). The Bayesian works integrated into the model information about the distribution of words to POS tags. For example, Johnson (2007) integrated to the EM-HMM model a prio</context>
<context position="19367" citStr="Johnson, 2007" startWordPosition="3226" endWordPosition="3227">include transformation based learning (Brill, 1995), contrastive estimation for conditional random fields (Smith and Eisner, 2005), Markov random fields (Haghighi and Klein, 2006), a multilingual approach (Snyder et al., 2008; Snyder et al., 2008) and expanding a 3The figure is for greedy many-to-one mapping and Spearman’s rank correlation coefficient, explained in further Sections. Other external measures and rank correlation scores demonstrate the same pattern. partial dictionary and use it to learn disambiguation rules (Zhao and Marcus, 2009). These works, except (Haghighi and Klein, 2006; Johnson, 2007; Gao and Johnson, 2008) and one experiment in (Goldwater and Griffiths, 2007), used a dictionary listing the allowable tags for each word in the text. This dictionary is usually extracted from the manual tagging of the text, contradicting the unsupervised nature of the task. Clearly, the availability of such a dictionary is not always a reasonable assumption (see e.g. (Goldwater and Griffiths, 2007)). In a different algorithmic direction, (Schuetze, 1995) applied latent semantic analysis with SVD based dimensionality reduction, and (Schuetze, 1995; Clark, 2003; Dasgupta and NG, 2007) used dis</context>
<context position="28334" citStr="Johnson, 2007" startWordPosition="4739" endWordPosition="4740">− Y,.), where X, is the index of r in the list X. 7VG and GG used 2 as the base of the logarithm in IT measures, which affects VI. We converted the VI numbers reported in their papers to base e. the GGTP-17 model which used the set of 17 coarse grained tags proposed by (Smith and Eisner, 2005)) and the size of the test set. The numbers reported for the algorithms of other works are the average performance over multiple runs, since no method for identification of high quality taggings was used. The results of our algorithms are superior, except for the M-1 performance of some of the models of (Johnson, 2007) and of the GGTP-17 and GGTP-45 models of (Grac¸a et al., 2009). Note that the models of (Johnson, 2007) and the GGTP45 model induce 40-50 clusters compared to our 34 (13 non-punctuation plus the additional 21 singleton punctuation tags). Increasing the number of clusters is known to improve the M-1 measure (Reichart and Rappoport, 2009). GGTP-17 gives the best M-1 results, but its 1-1 results are much worse than those of Q(ZCC), Q(CT), and CT, and the information theoretic measures V and NVI were not reported for it. Recall that the Q algorithms tag punctuation marks according to the scheme w</context>
<context position="35144" citStr="Johnson, 2007" startWordPosition="5942" endWordPosition="5943">-1 Q(ZCC) 0.637 2.06 0.591 0.58 Q(CT) 0.644 2.01 0.585 0.555 CT 0.619 2.14 0.576 0.543 HK – – – 0.413 J – 4.23 - 0.43 - 0.37 – 5.74 0.62 0.47 GG – 2.8 – – G-J – 4.03 – – 0.4 – 4.47 0.499 VG 0.54 - 2.49 – – – 0.59 2.91 GGTP-45 — — 0.654 0.445 GGTP-17 — — 0.702 0.495 Table 3: Comparison of our algorithms with the recent fully unsupervised POS taggers for which results are reported. HK: (Haghighi and Klein, 2006), trained and evaluated with a corpus of 193K tokens and 45 induced tags. GG: (Goldwater and Griffiths, 2007), trained and evaluated with a corpus of 24K tokens and 17 induced tags. J : (Johnson, 2007) inducing 25-50 tags (the results that are higher than Q in the M-1 measure are for 40-50 tags). GJ: (Gao and Johnson, 2008), inducing 50 tags. VG: (Van Gael et al., 2009), inducing 47-192 tags. GGTP-45: (Grac¸a et al., 2009), inducing 45 tags. GGTP-17: (Grac¸a et al., 2009), inducing 17 tags. All five were trained and evaluated with the full WSJ PTB (1.17M words). Lower VI values indicates better clustering. Statistic V NVI M-1 1-1 CT Mean 0.512 0.881 0.516 0.478 Mode 0.502 0.886 0.514 0.465 Std 0.022 0.035 0.018 0.028 ZCC Mean 0.503 0.908 0.512 0.478 Mode 0.509 0.907 0.518 0.47 Std 0.021 0.0</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>Mark Johnson, 2007. Why Doesnt EM Find Good HMM POS-Taggers? EMNLP-CoNLL ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold W Kuhn</author>
</authors>
<title>The Hungarian method for the assignment problem.</title>
<date>1955</date>
<journal>Naval Research Logistics Quarterly,</journal>
<pages>2--83</pages>
<marker>Kuhn, 1955</marker>
<rawString>Harold W. Kuhn, 1955. The Hungarian method for the assignment problem. Naval Research Logistics Quarterly, 2:83-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
</authors>
<title>Automatic Evaluation of Information Ordering: Kendall’s Tau.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<pages>4--471</pages>
<contexts>
<context position="26880" citStr="Lapata, 2006" startWordPosition="4470" endWordPosition="4471">ctively) in sections 2-21 of the WSJ corpus. where r is the number of runs (100 in our case), n, and nd are the numbers of concordant and discordant pairs respectively6 and di is the absolute value of the difference between the ranks of item i. The two measures have the properties that a perfect agreement between rankings results in a score of 1, a perfect disagreement results in a score of −1, completely independent rankings have the value of 0 on the average, the range of values is between −1 and 1, and increasing values imply increasing agreement between the rankings. For a discussion see (Lapata, 2006). 5 Results Table 1 presents the results of the Q(ZCC) and Q(CT) algorithms, which are both better than those of the original Clark tagger CT. The Q algorithms provide a tagging that is better than that produced by CT in 82-100% (Q(ZCC)) and 75- 100% (Q(CT)) of the cases. The Q(ZCC) algorithm is superior when evaluated with the mapping based measures. The Q(CT) algorithm is superior when evaluated with the IT measures. Table 3 presents reported results for all recent algorithms we are aware of that tackled the task of unsupervised POS induction from plain text 7. The settings of the various ex</context>
</contexts>
<marker>Lapata, 2006</marker>
<rawString>Mirella Lapata, 2006. Automatic Evaluation of Information Ordering: Kendall’s Tau. Computational Linguistics, 4:471-484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sven Martin</author>
<author>Jorg Liermann</author>
<author>Hermann Ney</author>
</authors>
<title>Algorithms for bigram and trigram word clustering.</title>
<date>1998</date>
<journal>Speech Communication,</journal>
<pages>24--19</pages>
<contexts>
<context position="6657" citStr="Martin et al., 1998" startWordPosition="1063" endWordPosition="1066">ly the leading POS induction algorithm (see Table 3). We start with a description of the original CT. We then detail ZCC, a modification of CT that constrains the clustering space by adding a Zipfbased constraint. Our perplexity-based unsupervised tagging quality test is discussed next. Finally, we provide an unsupervised technique for selecting the parameter of the Zipfian constraint. 2.1 The Original Clark Tagger (CT) The tagger’s statistical model combines distributional and morphological information with the likelihood function of the Brown algorithm (Brown et al., 1992; Ney et al., 1994; Martin et al., 1998). In the Brown algorithm a class assignment function g is selected such that the class bigram likelihood of the corpus, p(M|g), is maximized. Morphological and distributional information is introduced to the Clark model through a prior p(g). The prior prefers morphologically uniform clusters and skewed cluster sizes. The probability function the algorithm tries to maximize is: (1) p(M, g) = p(M|g) · p(g) (2) p(M|g) = l li=N i=� p(g(wi)|g(wi−1)) (3) p(g) = l lnl l j=1 �j g(w)=j qj(w) Where qj(wi) is the probability of assigning wi ∈ W by cluster cj according to the morphological model and aj is</context>
</contexts>
<marker>Martin, Liermann, Ney, 1998</marker>
<rawString>Sven Martin, Jorg Liermann, and Hermann Ney, 1998. Algorithms for bigram and trigram word clustering. Speech Communication, 24:19-37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marina Meila</author>
</authors>
<title>Comparing Clustering - an Information Based Distance.</title>
<date>2007</date>
<journal>Journal of Multivariate Analysis,</journal>
<pages>98--873</pages>
<contexts>
<context position="25240" citStr="Meila, 2007" startWordPosition="4183" endWordPosition="4184"> mappings of the induced to the gold labels. In the former mapping, two induced clusters can be mapped to the same gold standard cluster, while in the latter mapping each and every induced cluster is assigned a unique gold cluster. After each induced label is mapped to a gold label, tagging accuracy is computed. Accuracy is defined to be the number of correctly tagged words in the corpus divided by the total number of words in the corpus. The IT based measures we use are V (Rosenberg and Hirschberg, 2007) and NVI (Reichart and Rappoport, 2009). The latter is a normalization of the VI measure (Meila, 2007). VI and NVI induce the same order over clusterings but NVI values for good clusterings lie in [0, 1]. For V, the higher the score, the better the clustering. For NVI lower scores imply improved clustering quality. We use e as the base of the logarithm. Evaluation of the Quality Test. To measure the correlation between the score produced by the tagging quality test and the external quality of a tagging, we use two well accepted measures: Spearman’s rank correlation coefficient and Kendall Tau (Kendall and Dickinson, 1990). These measure the correlation between two sorted lists. For the computa</context>
</contexts>
<marker>Meila, 2007</marker>
<rawString>Marina Meila, 2007. Comparing Clustering - an Information Based Distance. Journal of Multivariate Analysis, 98:873-895.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging English Text with a Probabilistic Model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--2</pages>
<contexts>
<context position="17871" citStr="Merialdo, 1994" startWordPosition="3000" endWordPosition="3001">els for which the entropy-based filter improves perplexity more drastically, exhibit better correlation between perplexity and external clustering quality3. Our unsupervised parameter selection method is thus based on finding a value which exhibits a consistent decrease in perplexity as a function of K, the number of clusterings pruned from the beginning and end of the entropy-sorted list. In the rest of this paper we show results where the exponent value is 1.1. 3 Previous Work Unsupervised POS induction/tagging is a fruitful area of research. A major direction is Hidden Markov Models (HMM) (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005). Several recent works have tried to improve this model using Bayesian estimation (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008), sophisticated initialization (Goldberg et al., 2008), induction of an initial clustering used to train an HMM (Freitag, 2004; Biemann, 2006), infinite HMM models (Van Gael et al., 2009), integration of integer linear programming into the parameter estimation process (Ravi and Knight, 2009), and biasing the model such that the number of possible tags that each word can get is small (Grac¸a et a</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Bernard Merialdo, 1994. Tagging English Text with a Probabilistic Model. Computational Linguistics, 20(2):155-172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Mitzenmacher</author>
</authors>
<title>A Brief History of Generative Models for Power Law and Lognormal Distributions.</title>
<date>2004</date>
<journal>Internet Mathematics,</journal>
<pages>1--2</pages>
<contexts>
<context position="9362" citStr="Mitzenmacher, 2004" startWordPosition="1535" endWordPosition="1536">rithm, strong fluctuations in its values are likely to imply that there are uncontrolled fluctuations in the quality of the induced clusterings. Thus, introducing a constraint that we believe holds in real data increases the correlation between clustering quality and a well accepted unsupervised quality measure (perplexity). Our ZCC algorithm searches for a class assignment function g that maximizes the probability function (1) under a constraint on the clustering space, namely constraining the cluster type size distribution induced by g to be Zipfian. This constraint holds in many languages (Mitzenmacher, 2004) and is demonstrated in Figure 3 for the English corpus with which we experiment in this paper. Zipf’s law predicts that the fraction of elements in class k is given by: f(k; s; n) = �ni=1(1/is) where s is a parameter of the distribution and n the number of clusters. Denote the cluster type size distribution derived from the algorithm’s cluster assignment function g by T(g). The objective of the algorithm is g** = argmaxgp(M, g) s.t. T (g) — Zipf(s) To impose the Zipfian distribution on the induced clusters size, we make two modifications to the original CT algorithm. First, at initialization,</context>
</contexts>
<marker>Mitzenmacher, 2004</marker>
<rawString>Michael Mitzenmacher , 2004. A Brief History of Generative Models for Power Law and Lognormal Distributions. Internet Mathematics, 1(2):226-251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Munkres</author>
</authors>
<title>Algorithms for the Assignment and Transportation Problems.</title>
<date>1957</date>
<journal>Journal of the SIAM,</journal>
<pages>5--1</pages>
<marker>Munkres, 1957</marker>
<rawString>James Munkres, 1957. Algorithms for the Assignment and Transportation Problems. Journal of the SIAM, 5(1):32-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hermann Ney</author>
<author>Ute Essen</author>
<author>Reinhard Kneser</author>
</authors>
<title>On structuring probabilistic dependencies in stochastic language modelling.</title>
<date>1994</date>
<journal>Computer Speech and Language,</journal>
<pages>8--1</pages>
<contexts>
<context position="6635" citStr="Ney et al., 1994" startWordPosition="1059" endWordPosition="1062">2003) (CT), probably the leading POS induction algorithm (see Table 3). We start with a description of the original CT. We then detail ZCC, a modification of CT that constrains the clustering space by adding a Zipfbased constraint. Our perplexity-based unsupervised tagging quality test is discussed next. Finally, we provide an unsupervised technique for selecting the parameter of the Zipfian constraint. 2.1 The Original Clark Tagger (CT) The tagger’s statistical model combines distributional and morphological information with the likelihood function of the Brown algorithm (Brown et al., 1992; Ney et al., 1994; Martin et al., 1998). In the Brown algorithm a class assignment function g is selected such that the class bigram likelihood of the corpus, p(M|g), is maximized. Morphological and distributional information is introduced to the Clark model through a prior p(g). The prior prefers morphologically uniform clusters and skewed cluster sizes. The probability function the algorithm tries to maximize is: (1) p(M, g) = p(M|g) · p(g) (2) p(M|g) = l li=N i=� p(g(wi)|g(wi−1)) (3) p(g) = l lnl l j=1 �j g(w)=j qj(w) Where qj(wi) is the probability of assigning wi ∈ W by cluster cj according to the morphol</context>
</contexts>
<marker>Ney, Essen, Kneser, 1994</marker>
<rawString>Hermann Ney, Ute Essen, and Reinhard Kneser, 1994. On structuring probabilistic dependencies in stochastic language modelling. Computer Speech and Language, 8:1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Minimized Models for Unsupervised Part-of-Speech Tagging.</title>
<date>2009</date>
<journal>ACL</journal>
<volume>09</volume>
<contexts>
<context position="18365" citStr="Ravi and Knight, 2009" startWordPosition="3071" endWordPosition="3074">supervised POS induction/tagging is a fruitful area of research. A major direction is Hidden Markov Models (HMM) (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005). Several recent works have tried to improve this model using Bayesian estimation (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008), sophisticated initialization (Goldberg et al., 2008), induction of an initial clustering used to train an HMM (Freitag, 2004; Biemann, 2006), infinite HMM models (Van Gael et al., 2009), integration of integer linear programming into the parameter estimation process (Ravi and Knight, 2009), and biasing the model such that the number of possible tags that each word can get is small (Grac¸a et al., 2009). The Bayesian works integrated into the model information about the distribution of words to POS tags. For example, Johnson (2007) integrated to the EM-HMM model a prior that prefers clusterings where the distributions of hidden states to words is skewed. Other approaches include transformation based learning (Brill, 1995), contrastive estimation for conditional random fields (Smith and Eisner, 2005), Markov random fields (Haghighi and Klein, 2006), a multilingual approach (Snyde</context>
</contexts>
<marker>Ravi, Knight, 2009</marker>
<rawString>Sujith Ravi and Kevin Knight, 2009. Minimized Models for Unsupervised Part-of-Speech Tagging. ACL ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<title>The NVI Clustering Evaluation Measure.</title>
<date>2009</date>
<journal>CoNLL</journal>
<volume>09</volume>
<contexts>
<context position="24530" citStr="Reichart and Rappoport, 2009" startWordPosition="4057" endWordPosition="4060">gorithms, we compare the correlation between our tagging quality test and external clustering quality for both the original CT algorithm and our ZCC algorithm. Clustering Quality Evaluation. The induced POS tags have arbitrary names. To evaluate them against a manually annotated corpus, a proper correspondence with the gold standard POS tags should be established. Many evaluation measures for unsupervised clustering against gold standard exist. Here we use measures from two well accepted families: mapping based and information theoretic (IT) based. For a recent discussion on this subject see (Reichart and Rappoport, 2009). The mapping based measures are accuracy with greedy many-to-1 (M-1) and with greedy 1-to-1 (1-1) mappings of the induced to the gold labels. In the former mapping, two induced clusters can be mapped to the same gold standard cluster, while in the latter mapping each and every induced cluster is assigned a unique gold cluster. After each induced label is mapped to a gold label, tagging accuracy is computed. Accuracy is defined to be the number of correctly tagged words in the corpus divided by the total number of words in the corpus. The IT based measures we use are V (Rosenberg and Hirschber</context>
<context position="28673" citStr="Reichart and Rappoport, 2009" startWordPosition="4795" endWordPosition="4798">. The numbers reported for the algorithms of other works are the average performance over multiple runs, since no method for identification of high quality taggings was used. The results of our algorithms are superior, except for the M-1 performance of some of the models of (Johnson, 2007) and of the GGTP-17 and GGTP-45 models of (Grac¸a et al., 2009). Note that the models of (Johnson, 2007) and the GGTP45 model induce 40-50 clusters compared to our 34 (13 non-punctuation plus the additional 21 singleton punctuation tags). Increasing the number of clusters is known to improve the M-1 measure (Reichart and Rappoport, 2009). GGTP-17 gives the best M-1 results, but its 1-1 results are much worse than those of Q(ZCC), Q(CT), and CT, and the information theoretic measures V and NVI were not reported for it. Recall that the Q algorithms tag punctuation marks according to the scheme which assigns each of them a unique cluster (Section 4), while previous work does not distinguish punctuation marks from other tokens. To quantify the effect various punctuation schemes have on the results reported in Table 3, we evaluated the ‘iHMM: PYfixed’ model (Van Gael et al., 2009) and the Q algorithms when punctuation is excluded </context>
</contexts>
<marker>Reichart, Rappoport, 2009</marker>
<rawString>Roi Reichart and Ari Rappoport, 2009. The NVI Clustering Evaluation Measure. CoNLL ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Rose</author>
<author>Eitan Gurewitz</author>
<author>Geoffrey C Fox</author>
</authors>
<title>Statistical Mechanics and Phase Transitions in Clustering. Physical Review Letters,</title>
<date>1990</date>
<pages>65--8</pages>
<contexts>
<context position="21590" citStr="Rose et al., 1990" startWordPosition="3571" endWordPosition="3574">ata. In this paper we show that for the tagger of (Clark, 2003) such a method provides mediocre results (Table 2) even when the training criterion (likelihood or data probability for this tagger) is evaluated on the test set. Moreover, we show that our algorithm outperforms existing POS taggers for most evaluation measures (Table 3). Identifying good solutions among many runs of a randomly-initialized algorithm is a well known problem. We discuss here the work of (Smith and Eisner, 2004) that addressed the problem in the unsupervised POS tagging context. In this work, deterministic annealing (Rose et al., 1990) was ap61 plied to an HMM model for unsupervised POS tagging with a dictionary. This method is not sensitive to its initialization, and while it is not theoretically guaranteed to converge to a better solution than the traditional EM-HMM, it was experimentally shown to achieve better results. The problem has, of course, been addressed in other contexts as well (see, e.g., (Wang et al., 2002)). 4 Experimental Setup and Evaluation Setup. We used the English WSJ PennTreebank corpus in our experiments. We induced POS tags for sections 2-21 (43K word types, 950K word instances of which 832K (87.6%)</context>
</contexts>
<marker>Rose, Gurewitz, Fox, 1990</marker>
<rawString>Kenneth Rose, Eitan Gurewitz, and Geoffrey C. Fox, 1990. Statistical Mechanics and Phase Transitions in Clustering. Physical Review Letters, 65(8):945-948.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Rosenberg</author>
<author>Julia Hirschberg</author>
</authors>
<title>VMeasure: A Conditional Entropy-Based External Cluster Evaluation Measure.</title>
<date>2007</date>
<journal>EMNLP</journal>
<volume>07</volume>
<contexts>
<context position="25138" citStr="Rosenberg and Hirschberg, 2007" startWordPosition="4163" endWordPosition="4167">rt and Rappoport, 2009). The mapping based measures are accuracy with greedy many-to-1 (M-1) and with greedy 1-to-1 (1-1) mappings of the induced to the gold labels. In the former mapping, two induced clusters can be mapped to the same gold standard cluster, while in the latter mapping each and every induced cluster is assigned a unique gold cluster. After each induced label is mapped to a gold label, tagging accuracy is computed. Accuracy is defined to be the number of correctly tagged words in the corpus divided by the total number of words in the corpus. The IT based measures we use are V (Rosenberg and Hirschberg, 2007) and NVI (Reichart and Rappoport, 2009). The latter is a normalization of the VI measure (Meila, 2007). VI and NVI induce the same order over clusterings but NVI values for good clusterings lie in [0, 1]. For V, the higher the score, the better the clustering. For NVI lower scores imply improved clustering quality. We use e as the base of the logarithm. Evaluation of the Quality Test. To measure the correlation between the score produced by the tagging quality test and the external quality of a tagging, we use two well accepted measures: Spearman’s rank correlation coefficient and Kendall Tau </context>
</contexts>
<marker>Rosenberg, Hirschberg, 2007</marker>
<rawString>Andrew Rosenberg and Julia Hirschberg, 2007. VMeasure: A Conditional Entropy-Based External Cluster Evaluation Measure. EMNLP ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schuetze</author>
</authors>
<title>Distributional part-of-speech tagging.</title>
<date>1995</date>
<journal>EACL</journal>
<volume>95</volume>
<contexts>
<context position="19827" citStr="Schuetze, 1995" startWordPosition="3298" endWordPosition="3299"> pattern. partial dictionary and use it to learn disambiguation rules (Zhao and Marcus, 2009). These works, except (Haghighi and Klein, 2006; Johnson, 2007; Gao and Johnson, 2008) and one experiment in (Goldwater and Griffiths, 2007), used a dictionary listing the allowable tags for each word in the text. This dictionary is usually extracted from the manual tagging of the text, contradicting the unsupervised nature of the task. Clearly, the availability of such a dictionary is not always a reasonable assumption (see e.g. (Goldwater and Griffiths, 2007)). In a different algorithmic direction, (Schuetze, 1995) applied latent semantic analysis with SVD based dimensionality reduction, and (Schuetze, 1995; Clark, 2003; Dasgupta and NG, 2007) used distributional and morphological statistics to find meaningful word types clusters. Clark (2003) is the only such work to have evaluated its algorithm as a POS tagger for large corpora, like we do in this paper. A Zipfian constraint was utilized in (Goldwater and et al., 2006) for language modeling and morphological disambiguation. The problem of convergence to local maxima has been discussed in (Smith and Eisner, 2005; Haghighi and Klein, 2006; Goldwater and</context>
</contexts>
<marker>Schuetze, 1995</marker>
<rawString>Hinrich Schuetze, 1995. Distributional part-of-speech tagging. EACL ’95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Annealing Techniques for Unsupervised Statistical Language Learning.</title>
<date>2004</date>
<journal>ACL</journal>
<volume>04</volume>
<contexts>
<context position="21464" citStr="Smith and Eisner, 2004" startWordPosition="3550" endWordPosition="3553">ed unsupervised model selection across smoothing parameters by evaluating the training criterion on unseen, unlabeled development data. In this paper we show that for the tagger of (Clark, 2003) such a method provides mediocre results (Table 2) even when the training criterion (likelihood or data probability for this tagger) is evaluated on the test set. Moreover, we show that our algorithm outperforms existing POS taggers for most evaluation measures (Table 3). Identifying good solutions among many runs of a randomly-initialized algorithm is a well known problem. We discuss here the work of (Smith and Eisner, 2004) that addressed the problem in the unsupervised POS tagging context. In this work, deterministic annealing (Rose et al., 1990) was ap61 plied to an HMM model for unsupervised POS tagging with a dictionary. This method is not sensitive to its initialization, and while it is not theoretically guaranteed to converge to a better solution than the traditional EM-HMM, it was experimentally shown to achieve better results. The problem has, of course, been addressed in other contexts as well (see, e.g., (Wang et al., 2002)). 4 Experimental Setup and Evaluation Setup. We used the English WSJ PennTreeba</context>
</contexts>
<marker>Smith, Eisner, 2004</marker>
<rawString>Noah A. Smith and Jason Eisner, 2004. Annealing Techniques for Unsupervised Statistical Language Learning. ACL ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Tahira Naseem</author>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Adding More Languages Improves Unsupervised Multilingual Partof-Speech Tagging: A Bayesian Non-Parametric Approach.</title>
<date>2009</date>
<journal>NAACL</journal>
<volume>09</volume>
<marker>Snyder, Naseem, Eisenstein, Barzilay, 2009</marker>
<rawString>Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, and Regina Barzilay, 2009. Adding More Languages Improves Unsupervised Multilingual Partof-Speech Tagging: A Bayesian Non-Parametric Approach. NAACL ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Tahira Naseem</author>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Unsupervised Multilingual Learning for POS Tagging.</title>
<date>2008</date>
<journal>EMNLP</journal>
<volume>08</volume>
<contexts>
<context position="18979" citStr="Snyder et al., 2008" startWordPosition="3167" endWordPosition="3170">2009), and biasing the model such that the number of possible tags that each word can get is small (Grac¸a et al., 2009). The Bayesian works integrated into the model information about the distribution of words to POS tags. For example, Johnson (2007) integrated to the EM-HMM model a prior that prefers clusterings where the distributions of hidden states to words is skewed. Other approaches include transformation based learning (Brill, 1995), contrastive estimation for conditional random fields (Smith and Eisner, 2005), Markov random fields (Haghighi and Klein, 2006), a multilingual approach (Snyder et al., 2008; Snyder et al., 2008) and expanding a 3The figure is for greedy many-to-one mapping and Spearman’s rank correlation coefficient, explained in further Sections. Other external measures and rank correlation scores demonstrate the same pattern. partial dictionary and use it to learn disambiguation rules (Zhao and Marcus, 2009). These works, except (Haghighi and Klein, 2006; Johnson, 2007; Gao and Johnson, 2008) and one experiment in (Goldwater and Griffiths, 2007), used a dictionary listing the allowable tags for each word in the text. This dictionary is usually extracted from the manual tagging</context>
</contexts>
<marker>Snyder, Naseem, Eisenstein, Barzilay, 2008</marker>
<rawString>Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, and Regina Barzilay, 2008. Unsupervised Multilingual Learning for POS Tagging. EMNLP ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jurgen Van Gael</author>
<author>Andreas Vlachos</author>
<author>Zoubin Ghahramani</author>
</authors>
<date>2009</date>
<booktitle>The Infinite HMM for Unsupervised POS Tagging. EMNLP ’09.</booktitle>
<marker>Van Gael, Vlachos, Ghahramani, 2009</marker>
<rawString>Jurgen Van Gael, Andreas Vlachos and Zoubin Ghahramani, 2009. The Infinite HMM for Unsupervised POS Tagging. EMNLP ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Iris Wang</author>
<author>Dale Schuurmans</author>
</authors>
<title>Improved Estimation for Unsupervised Part-of-Speech Tagging.</title>
<date>2005</date>
<journal>IEEE NLP-KE</journal>
<volume>05</volume>
<contexts>
<context position="17922" citStr="Wang and Schuurmans, 2005" startWordPosition="3006" endWordPosition="3009">improves perplexity more drastically, exhibit better correlation between perplexity and external clustering quality3. Our unsupervised parameter selection method is thus based on finding a value which exhibits a consistent decrease in perplexity as a function of K, the number of clusterings pruned from the beginning and end of the entropy-sorted list. In the rest of this paper we show results where the exponent value is 1.1. 3 Previous Work Unsupervised POS induction/tagging is a fruitful area of research. A major direction is Hidden Markov Models (HMM) (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005). Several recent works have tried to improve this model using Bayesian estimation (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008), sophisticated initialization (Goldberg et al., 2008), induction of an initial clustering used to train an HMM (Freitag, 2004; Biemann, 2006), infinite HMM models (Van Gael et al., 2009), integration of integer linear programming into the parameter estimation process (Ravi and Knight, 2009), and biasing the model such that the number of possible tags that each word can get is small (Grac¸a et al., 2009). The Bayesian works integrated into the m</context>
</contexts>
<marker>Wang, Schuurmans, 2005</marker>
<rawString>Qin Iris Wang and Dale Schuurmans, 2005. Improved Estimation for Unsupervised Part-of-Speech Tagging. IEEE NLP-KE ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shaojun Wang</author>
<author>Dale Schuurmans</author>
<author>Yunxin Zhao</author>
</authors>
<title>The Latent Maximum Entropy Principle.</title>
<date>2002</date>
<journal>ISIT</journal>
<volume>02</volume>
<contexts>
<context position="21984" citStr="Wang et al., 2002" startWordPosition="3641" endWordPosition="3644">nitialized algorithm is a well known problem. We discuss here the work of (Smith and Eisner, 2004) that addressed the problem in the unsupervised POS tagging context. In this work, deterministic annealing (Rose et al., 1990) was ap61 plied to an HMM model for unsupervised POS tagging with a dictionary. This method is not sensitive to its initialization, and while it is not theoretically guaranteed to converge to a better solution than the traditional EM-HMM, it was experimentally shown to achieve better results. The problem has, of course, been addressed in other contexts as well (see, e.g., (Wang et al., 2002)). 4 Experimental Setup and Evaluation Setup. We used the English WSJ PennTreebank corpus in our experiments. We induced POS tags for sections 2-21 (43K word types, 950K word instances of which 832K (87.6%) are not punctuation marks), using Q(ZCC), Q(CT), and CT. For the unsupervised quality test, we trained the bigram class-based language model on sections 2-21 with the induced clusters, and computed its perplexity on section 23. In Q(ZCC) and Q(CT), the base taggers were run a 100 times each, using different random initializations. In each run we induce 13 clusters, since this is the number </context>
</contexts>
<marker>Wang, Schuurmans, Zhao, 2002</marker>
<rawString>Shaojun Wang, Dale Schuurmans and Yunxin Zhao, 2002. The Latent Maximum Entropy Principle. ISIT ’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiuye Zhao</author>
<author>Mitch Marcus</author>
</authors>
<title>A Simple Unsupervised Learner for POS Disambiguation Rules Given Only a Minimal Lexicon.</title>
<date>2009</date>
<journal>EMNLP</journal>
<volume>09</volume>
<contexts>
<context position="19305" citStr="Zhao and Marcus, 2009" startWordPosition="3215" endWordPosition="3218">he distributions of hidden states to words is skewed. Other approaches include transformation based learning (Brill, 1995), contrastive estimation for conditional random fields (Smith and Eisner, 2005), Markov random fields (Haghighi and Klein, 2006), a multilingual approach (Snyder et al., 2008; Snyder et al., 2008) and expanding a 3The figure is for greedy many-to-one mapping and Spearman’s rank correlation coefficient, explained in further Sections. Other external measures and rank correlation scores demonstrate the same pattern. partial dictionary and use it to learn disambiguation rules (Zhao and Marcus, 2009). These works, except (Haghighi and Klein, 2006; Johnson, 2007; Gao and Johnson, 2008) and one experiment in (Goldwater and Griffiths, 2007), used a dictionary listing the allowable tags for each word in the text. This dictionary is usually extracted from the manual tagging of the text, contradicting the unsupervised nature of the task. Clearly, the availability of such a dictionary is not always a reasonable assumption (see e.g. (Goldwater and Griffiths, 2007)). In a different algorithmic direction, (Schuetze, 1995) applied latent semantic analysis with SVD based dimensionality reduction, and</context>
</contexts>
<marker>Zhao, Marcus, 2009</marker>
<rawString>Qiuye Zhao and Mitch Marcus, 2009. A Simple Unsupervised Learner for POS Disambiguation Rules Given Only a Minimal Lexicon. EMNLP ’09.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>