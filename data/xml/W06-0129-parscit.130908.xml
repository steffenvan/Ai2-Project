<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009388">
<title confidence="0.9982415">
Character Language Models for Chinese Word Segmentation and Named
Entity Recognition
</title>
<author confidence="0.948275">
Bob Carpenter
</author>
<affiliation confidence="0.817428">
Alias-i, Inc.
</affiliation>
<email confidence="0.725954">
carp@alias-i.com
</email>
<sectionHeader confidence="0.994832" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99646225">
We describe the application of the Ling-
Pipe toolkit to Chinese word segmentation
and named entity recognition for the 3rd
SIGHAN bakeoff.
</bodyText>
<sectionHeader confidence="0.975882" genericHeader="method">
1 Word Segmentation
</sectionHeader>
<bodyText confidence="0.999778454545455">
Chinese is written without spaces between words.
For the word segmentation task, four training cor-
pora were provided with one sentence per line and
a single space character between words. Test data
consisted of Chinese text, one sentence per line,
without spaces between words. The task is to in-
sert single space characters between the words.
For this task and named entity recognition, we
used the UTF8-encoded Unicode versions of the
corpora converted from their native formats by the
bakeoff organizers.
</bodyText>
<sectionHeader confidence="0.991461" genericHeader="method">
2 Named Entity Recognition
</sectionHeader>
<bodyText confidence="0.999766944444444">
Named entities consist of proper noun mentions
of persons (PER), locations (LOC), and organiza-
tions (ORG). Two training corpora were provided.
Each line consists of a single character, a single
space character, and then a tag. The tags were in
the standard BIO (begin/in/out) encoding. B-PER
tags the first character in a person entity, I-PER
tags subsequent characters in a person, and 0 char-
acters not part of entities. We segmented the
data into sentences by taking Unicode character
0x3002, which is rendered as a baseline-aligned
small circle, as marking end of sentence (EOS). As
judged by our own sentence numbers (see Figures
1 and 2), this missed around 20% of the sentence
boundaries in the City U NE corpus and 5% of
the boundaries in the Microsoft NE corpus. Test
data is in the same format as the word segmenta-
tion task.
</bodyText>
<sectionHeader confidence="0.999543" genericHeader="method">
3 LingPipe
</sectionHeader>
<bodyText confidence="0.999926833333333">
LingPipe is a Java-based natural language process-
ing toolkit distributed with source code by Alias-i
(2006). For this bakeoff, we used two LingPipe
packages, com.aliasi.spell for Chinese
word segmentation and com.aliasi.chunk
for named-entity extraction. Both of these de-
pend on the character language modeling pack-
age com.aliasi.lm, and the chunker also
depends on the hidden Markov model package
com.alias.hmm. The experiments reported in
this paper were carried out in May 2006 using (a
prerelease version of) LingPipe 2.3.0.
</bodyText>
<subsectionHeader confidence="0.999658">
3.1 LingPipe’s Character Language Models
</subsectionHeader>
<bodyText confidence="0.996059210526316">
LingPipe provides n-gram based character lan-
guage models with a generalized form of Witten-
Bell smoothing, which performed better than other
approaches to smoothing in extensive English tri-
als (Carpenter 2005). Language models provide
a probability distribution P(σ) defined for strings
σ ∈ E* over a fixed alphabet of characters E. We
begin with Markovian language models normal-
ized as random processes. This means the sum of
the probabilities for strings of a fixed length is 1.0.
The chain rule factors P(σc) = P(σ) · P(c|σ)
for a character c and string σ. The n-gram Marko-
vian assumption restricts the context to the previ-
ous n −1 characters, taking P(cn|σc1 · · · cn−1) =
P(cn|c1 ··· cn−1).
The maximum likelihood estimator for n-grams
is PML(c|σ) = count(σc)/extCount(σ), where
count(σ) is the number of times the sequence σ
was observed in the training data and extCount(σ)
</bodyText>
<page confidence="0.983385">
169
</page>
<bodyText confidence="0.8055605">
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 169–172,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</bodyText>
<equation confidence="0.6577945">
is the number of single-character extensions of σ
observed: extCount(σ) = &amp; count(σc).
</equation>
<bodyText confidence="0.996176">
Witten-Bell smoothing uses linear interpolation
to form a mixture model of all orders of maximum
likelihood estimates down to the uniform estimate
PU(c) = 1/|E|. The interpolation ratio λ(dσ)
ranges between 0 and 1 depending on the context:
</bodyText>
<equation confidence="0.901041">
P�(c|dσ) = λ(dσ)PML(c|dσ)
+ (1 − λ(dσ)) P�(c|σ)
P(c) = λ()PML(c)
+ (1 − λ())(1/|E|)
Generalized Witten-Bell smoothing defines the
interpolation ratio with a hyperparameter θ:
= extCount(σ )
λ(σ) extCount(σ) + θ · numExts(σ)
</equation>
<bodyText confidence="0.9998486">
We take numExts(σ) = |1c|count(σc) &gt; 01 |to be
the number of different symbols observed follow-
ing σ in the training data. The original Witten-Bell
estimator set the hyperparameter θ = 1. Ling-
Pipe’s default sets θ equal to the n-gram order.
</bodyText>
<subsectionHeader confidence="0.997525">
3.2 Noisy Channel Spelling Correction
</subsectionHeader>
<bodyText confidence="0.999953054054054">
LingPipe performs spelling correction with a
noisy-channel model. A noisy-channel model
consists of a source model Ps(µ) defining the
probability of message µ, coupled with a chan-
nel model Pc(σ|µ) defining the likelihood of a sig-
nal σ given a message µ. In LingPipe, the source
model Ps is a character language model. The
channel model Pc is a (probabilistically normal-
ized) weighted edit distance (with transposition).
LingPipe’s decoder finds the most likely message
µ to have produced a signal σ: argmaxµP(µ|σ) =
argmaxµP(µ) · P(σ|µ).
For spelling correction, the channel Pc(σ|µ) is
a model of what is likely to be typed given an in-
tended message. Uniform models work fairly well
and ones tuned to brainos and typos work even bet-
ter. The source model is typically estimated from
a corpus of ordinary text.
For Chinese word segmentation, the source
model is trained over the corpus with spaces in-
serted. The noisy channel deterministically elim-
inates spaces so that Pc(σ|µ) = 1.0 if σ is
identical to µ with all of the spaces removed,
and 0.0 otherwise. This channel is easily imple-
mented as a weighted edit distance where dele-
tion of a single space is 100% likely (log proba-
bility edit “cost” is zero) and matching a charac-
ter is 100% likely, with any other operation be-
ing 0% likely (infinite cost). This makes any seg-
mentation equally likely according to the channel
model, reducing decoding to finding the highest
likelihood hypothesis consisting of the test string
with spaces inserted. This approach reduces to
the cross-entropy/compression-based approach of
(Teahan et al. 2000). Experiments showed that
skewing these space-insertion/matching probabil-
ities reduces decoding accuracy.
</bodyText>
<subsectionHeader confidence="0.997698">
3.3 LingPipe’s Named Entity Recognition
</subsectionHeader>
<bodyText confidence="0.999966842105263">
LingPipe 2.1 introduced a hidden Markov
model interface with several decoders: first-best
(Viterbi), n-best (Viterbi forward, A* backward
with exact Viterbi estimates), and confidence-
based (forward-backward).
LingPipe 2.2 introduced a chunking implemen-
tation that codes a chunking problem as an HMM
tagging problem using a refinement of the stan-
dard BIO coding. The refinement both introduces
context and greatly simplifies confidence estima-
tion over the approach using standard BIO cod-
ing in (Culotta and McCallum 2004). The tags
are B-T for the first character in a multi-character
entity of type T, M-T for a middle character in a
multi-character entity, E-T for the end character in
a multi-character entity, and W-T for a single char-
acter entity. The out tags are similarly contextual-
ized, with additional information on the start/end
tags to model their context. Specifically, the tags
used are B-O-T for a character not in an entity
following an entity of type T, I-O for any mid-
dle character not in an entity, and E-O-T for a
character not in an entity but preceding a charac-
ter in an entity of type T, and finally, W-O-T for
a character that is a single character between two
entities, the following entity being of type T. Fi-
nally, the first tag is conditioned on the begin-of-
sentence tag (BOS) and after the last tag, the end-
of-sentence tag (EOS) is generated. Thus the prob-
abilities normalize to model string/tag joint prob-
abilities.
In the HMM implementation considered here,
transitions between states (tags) in the HMM are
modeled by a maximum likelihood estimate over
the training data. Tag emissions are generated by
bounded character language models. Rather than
the process estimate P(X), we use P(X#|#) ,
where # is a distinguished boundary character
</bodyText>
<page confidence="0.991659">
170
</page>
<table confidence="0.965430555555555">
Corpus Encod Sents Chars Uniq Words Uniq Test S Test Ch Unseen
City U HK HKSCS (trad) 57K 4.3M 5113 1.6M 76K 7.5K 364K 0.046%
Microsoft gb18030 (simp) 46K 3.4M 4768 1.3M 63K 4.4K 173K 0.046%
Ac Sinica Big5 (trad) 709K 13.2M 6123 5.5M 146K 11.0K 146K 0.560%
Penn/Colo CP936 (simp) 19K 1.3M 4294 0.5M 37K 5.1K 256K 0.160%
Figure 1: Word Segmentation Corpora
Corpus Sents Chars Uniq LOC PER ORG Test S Test Ch Unseen
City U HK 48K 2.7M 5113 48.2K 36.4K 27.8K 7.5K 364K 0.046%
Microsoft 44K 2.2M 4791 36.9K 17.6K 20.6K 4.4K 173K 0.046%
</table>
<figureCaption confidence="0.997073">
Figure 2: Named Entity Recognition Corpora
</figureCaption>
<bodyText confidence="0.999432769230769">
not in the training or test character sets. We also
train with boundaries. For Chinese at the charac-
ter level, this bounding is irrelevant as all tokens
are length 1, so probabilities are already normal-
ized and there is no contextual position to take ac-
count of within a token. In the more usual word-
tokenized case, it normalizes probabilities over all
strings and accounts for the special status of pre-
fixes and suffixes (e.g. capitalization, inflection).
Consider the chunking consisting of the string
John J. Smith lives in Seattle. with John J. Smith a
person mention and Seattle a location mention. In
the coded HMM model, the joint estimate is:
</bodyText>
<equation confidence="0.628536">
�PML(B-PER|BOS) · �PB-PER(John#|#)
</equation>
<listItem confidence="0.887635">
· PML(I-PER|B-PER) · �PI-PER(J#|#)
· PML(I-PER|I-PER) · �PI-PER(.#|#)
· PML(E-PER|I-PER) · PE-PER(Smith#|#)
· �PML(B-O-PER|E-PER) · �PB-O-PER(lives#|#)
· �PML(E-O-LOC|B-O-PER) · �PE-O-LOC(in#|#)
· �PML(W-LOC|E-O-LOC) · �PW-LOC(Seattle#|#)
· PML(W-O-EOS|W-LOC) · �PW-O-EOS(.#|#)
· �PML(EOS|W-O-EOS)
</listItem>
<bodyText confidence="0.996004125">
LingPipe 2.3 introduced an n-best chunking im-
plementation that adapts an underlying n-best
chunker via rescoring. In rescoring, each of these
outputs is scored on its own and the new best
output is returned. The rescoring model is a
longer-distance generative model that produces
alternating out/entity tags for all characters. The
joint probability of the specified chunking is:
</bodyText>
<listItem confidence="0.5883934">
�POUT(cPER|cBOS)
· PPER(John J. SmithcOUT|cOUT)
· POUT( lives in cLOC|cPER)
· �PLOC(SeattlecOUT|cOUT)
· �POUT(.cEOS|cLOC)
</listItem>
<bodyText confidence="0.9999545">
where each estimator is a character language
model, and where the cT are distinct characters
not in the training/test sets that encode begin-of-
sentence (BOS), end-of-sentence (EOS), and type
(e.g. PER, LOC, ORG). In words, we generate an
alternating sequence of OUT and type estimates,
starting and ending with an OUT estimate. We
begin by conditioning on the begin-of-sentence
tag. Because the first character is in an entity, we
do not generate any text, but rather generate a
character indicating that we are done generating
the OUT characters and ready to switch to gen-
erating person characters. We then generate the
phrase John J. Smith in the person model; note
that type estimates always begin and end with the
cOUT character, essentially making them bounded
models. After generating the name and the
character to end the entity, we revert to generating
more out characters, starting from a person and
ending with a location. Note that we are generat-
ing the phrase lives in including the preceding and
following space. All such spaces are generated in
the OUT models for English; there are no spaces in
the Chinese input. Next, we generate the location
phrase the same way as the person phrase. Next,
we generate the final period in the OUT model
and then the end-of-sentence symbol. Note that
the OUT category’s language model shoulders
the brunt of the burden of estimating contextual
effects. It conditions on the preceding type, so
that the likelihood of lives in is conditioned on
following a person entity. Furthermore, the choice
to begin an entity of type location is based on
the fact that it follows lives in. This includes
begin-of-sentence and end-of-sentence effects,
so the model is sensitive to initial capitalization
in the out model as a distribution of character
sequences likely to follow BOS. Similarly, the
</bodyText>
<page confidence="0.994863">
171
</page>
<table confidence="0.974324666666667">
Corpus R P F1 Best F1 OOV ROOV
City Uni Hong Kong .966 .957 .961 .972 4.0% .555
Microsoft Research .959 .955 .957 .963 3.4% .494
Academia Sinica .951 .935 .943 .958 4.2% .389
U Penn and U Colorado .919 .895 .907 .933 8.8% .459
Figure 3: Word Segmentation Results (Closed Category)
Corpus R P F1 Best F1 PLOC RLOC PPER RPER PORG RORG
City Uni HK .8417 .8690 .8551 .8903 .8961 .8762 .8749 .8943 .6997 .8176
MS Research .8097 .8188 .8142 .8651 .8351 .8716 .7968 .8438 .7739 .6899
</table>
<figureCaption confidence="0.99848">
Figure 4: Named Entity Recognition Results (Closed Category)
</figureCaption>
<bodyText confidence="0.99354325">
end-of-sentence is conditioned on the preceding
text, in this case a single period. The resulting
model defines a (properly normalized) joint
probability distribution over chunkings.
</bodyText>
<sectionHeader confidence="0.993668" genericHeader="method">
4 Held-out Parameter Tuning
</sectionHeader>
<bodyText confidence="0.999712346153846">
We ran preliminary tests on MUC 6 English and
City University of Hong Kong data for Chinese
and found baseline performance around 72% and
rescored performance around 82%. The underly-
ing model was designed to have good recall in gen-
erating hypotheses. Over 99% of the MUC test
sentences had their correct analysis in a 1024-best
list generated by the underlying model. Neverthe-
less, setting the number of hypotheses beyond 64
did not improve results in either English or Chi-
nese, so we reported runs with n-best set to 64.
We believe this is because the two language-model
based approaches make highly correlated ranking
decisions based on character n-grams.
Held-out scores peaked with 5-grams for Chi-
nese; 3-grams and 4-grams were not much worse
and longer n-grams performed nearly identically.
We used 7500 as the number of distinct charac-
ters, though this parameter is not at all sensitive
to within an order of magnitude. We used Ling-
Pipe’s default of setting the interpolation parame-
ter equal to the n-gram length; for the final eval-
uation 0 = 5.0. Higher interpolation ratios favor
precision over recall, lower ratios favor recall. Val-
ues within an order of magnitude performed with
1% F-measure and 2% precision/recall.
</bodyText>
<sectionHeader confidence="0.928961" genericHeader="method">
5 Bakeoff Time and Effort
</sectionHeader>
<bodyText confidence="0.999961">
The total time spent on this SIGHAN bakeoff was
about 2 hours for the word segmentation task and
10 hours for the named-entity task (not including
writing this paper). We started from a working
word segmentation system for the last SIGHAN.
Most of the time was spent munging entity data,
with the rest devoted to held out analysis. The final
code was roughly one page per task, with only a
dozen or so LingPipe-specific lines. The final run,
including unpacking, training and testing, took 45
minutes on a 512MB home PC; most of the time
was named-entity decoding.
</bodyText>
<sectionHeader confidence="0.999988" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.9998682">
Official bakeoff results for the four word segmen-
tation corpora are shown in Figure 3, and for the
two named entity corpora in Figure 4. Column
labels are R for recall, P for precision, F1 for
balanced F-measure, Best F1 for the best closed
system’s F1 score, OOV for the out-of-vocabulary
rate in the test corpus, and RppV for recall on the
out-of-vocabulary items. For the named-entity re-
sults, precision and recall are also broken down by
category.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="conclusions">
7 Distribution
</sectionHeader>
<bodyText confidence="0.999580333333333">
LingPipe may be downloaded from its homepage,
http://www.alias-i.com/lingpipe. The code
for the bakeoff is available via anonymous CVS
from the sandbox. An Apache Ant makefile is pro-
vided to generate our bakeoff submission from the
official data distribution format.
</bodyText>
<sectionHeader confidence="0.999632" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999158571428571">
Carpenter, B. 2005. Scaling high-order character language
models to gigabytes. ACL Software Workshop. Ann Arbor.
Culotta, A. and A. McCallum. 2004. Confidence estimation
for information extraction. HLT/NAACL 2004. Boston.
Teahan, W. J., Y. Wen, R. McNab, and I. H. Witten. 2000. A
compression-based algorithm for Chinese word segmenta-
tion. Computational Linguistics, 26(3):375–393.
</reference>
<page confidence="0.997877">
172
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.022374">
<title confidence="0.9981145">Character Language Models for Chinese Word Segmentation and Entity Recognition</title>
<author confidence="0.994975">Bob</author>
<affiliation confidence="0.99677">Alias-i, Inc.</affiliation>
<email confidence="0.999441">carp@alias-i.com</email>
<abstract confidence="0.998644265060241">We describe the application of the Ling- Pipe toolkit to Chinese word segmentation and named entity recognition for the 3rd SIGHAN bakeoff. 1 Word Segmentation Chinese is written without spaces between words. For the word segmentation task, four training corpora were provided with one sentence per line and a single space character between words. Test data consisted of Chinese text, one sentence per line, without spaces between words. The task is to insert single space characters between the words. For this task and named entity recognition, we the Unicode versions of the corpora converted from their native formats by the bakeoff organizers. 2 Named Entity Recognition Named entities consist of proper noun mentions persons locations and organiza- Two training corpora were provided. Each line consists of a single character, a single space character, and then a tag. The tags were in standard BIO (begin/in/out) encoding. the first character in a person entity, subsequent characters in a person, and characters not part of entities. We segmented the data into sentences by taking Unicode character which is rendered as a baseline-aligned circle, as marking end of sentence As judged by our own sentence numbers (see Figures 1 and 2), this missed around 20% of the sentence boundaries in the City U NE corpus and 5% of the boundaries in the Microsoft NE corpus. Test data is in the same format as the word segmentation task. 3 LingPipe LingPipe is a Java-based natural language processing toolkit distributed with source code by Alias-i (2006). For this bakeoff, we used two LingPipe Chinese segmentation and for named-entity extraction. Both of these depend on the character language modeling packand the chunker also depends on the hidden Markov model package The experiments reported in this paper were carried out in May 2006 using (a prerelease version of) LingPipe 2.3.0. 3.1 LingPipe’s Character Language Models provides based character language models with a generalized form of Witten- Bell smoothing, which performed better than other approaches to smoothing in extensive English trials (Carpenter 2005). Language models provide probability distribution for strings ∈ over a fixed alphabet of characters We begin with Markovian language models normalized as random processes. This means the sum of the probabilities for strings of a fixed length is 1.0. chain rule factors = a character string The Markovian assumption restricts the context to the previtaking · · = maximum likelihood estimator for = where the number of times the sequence observed in the training data and 169 of the Fifth SIGHAN Workshop on Chinese Language pages July 2006. Association for Computational Linguistics the number of single-character extensions of = Witten-Bell smoothing uses linear interpolation to form a mixture model of all orders of maximum likelihood estimates down to the uniform estimate = The interpolation ratio ranges between 0 and 1 depending on the context: = (1 = (1 Generalized Witten-Bell smoothing defines the ratio with a hyperparameter + take = be the number of different symbols observed followthe training data. The original Witten-Bell set the hyperparameter Lingdefault sets to the order. 3.2 Noisy Channel Spelling Correction LingPipe performs spelling correction with a noisy-channel model. A noisy-channel model of a source model the of message coupled with a chanmodel the likelihood of a siga message In LingPipe, the source is a character language model. The model is a (probabilistically normalized) weighted edit distance (with transposition). LingPipe’s decoder finds the most likely message have produced a signal = spelling correction, the channel a model of what is likely to be typed given an intended message. Uniform models work fairly well and ones tuned to brainos and typos work even better. The source model is typically estimated from a corpus of ordinary text. For Chinese word segmentation, the source model is trained over the corpus with spaces inserted. The noisy channel deterministically elimspaces so that = to all of the spaces removed, This channel is easily implemented as a weighted edit distance where deletion of a single space is 100% likely (log probability edit “cost” is zero) and matching a character is 100% likely, with any other operation being 0% likely (infinite cost). This makes any segmentation equally likely according to the channel model, reducing decoding to finding the highest likelihood hypothesis consisting of the test string with spaces inserted. This approach reduces to the cross-entropy/compression-based approach of (Teahan et al. 2000). Experiments showed that skewing these space-insertion/matching probabilities reduces decoding accuracy. 3.3 LingPipe’s Named Entity Recognition LingPipe 2.1 introduced a hidden Markov model interface with several decoders: first-best (Viterbi), n-best (Viterbi forward, A* backward with exact Viterbi estimates), and confidencebased (forward-backward). LingPipe 2.2 introduced a chunking implementation that codes a chunking problem as an HMM tagging problem using a refinement of the standard BIO coding. The refinement both introduces context and greatly simplifies confidence estimation over the approach using standard BIO coding in (Culotta and McCallum 2004). The tags the first character in a multi-character of type a middle character in a entity, the end character in multi-character entity, and a single character entity. The out tags are similarly contextualized, with additional information on the start/end tags to model their context. Specifically, the tags are a character not in an entity an entity of type T, any midcharacter not in an entity, and a character not in an entity but preceding a characin an entity of type and finally, a character that is a single character between two the following entity being of type Finally, the first tag is conditioned on the begin-oftag and after the last tag, the endtag is generated. Thus the probabilities normalize to model string/tag joint probabilities. In the HMM implementation considered here, transitions between states (tags) in the HMM are modeled by a maximum likelihood estimate over the training data. Tag emissions are generated by bounded character language models. Rather than process estimate we use a distinguished boundary character</abstract>
<note confidence="0.920199181818182">170 Corpus Encod Sents Chars Uniq Words Uniq Test S Test Ch Unseen City U HK HKSCS (trad) 57K 4.3M 5113 1.6M 76K 7.5K 364K 0.046% Microsoft gb18030 (simp) 46K 3.4M 4768 1.3M 63K 4.4K 173K 0.046% Ac Sinica Big5 (trad) 709K 13.2M 6123 5.5M 146K 11.0K 146K 0.560% Penn/Colo CP936 (simp) 19K 1.3M 4294 0.5M 37K 5.1K 256K 0.160% Figure 1: Word Segmentation Corpora Corpus Sents Chars Uniq LOC PER ORG Test S Test Ch Unseen City U HK 48K 2.7M 5113 48.2K 36.4K 27.8K 7.5K 364K 0.046% Microsoft 44K 2.2M 4791 36.9K 17.6K 20.6K 4.4K 173K 0.046% Figure 2: Named Entity Recognition Corpora</note>
<abstract confidence="0.9892514">not in the training or test character sets. We also train with boundaries. For Chinese at the character level, this bounding is irrelevant as all tokens are length 1, so probabilities are already normalized and there is no contextual position to take account of within a token. In the more usual wordtokenized case, it normalizes probabilities over all strings and accounts for the special status of prefixes and suffixes (e.g. capitalization, inflection). Consider the chunking consisting of the string J. Smith lives in Seattle. J. Smith mention and location mention. In the coded HMM model, the joint estimate is: · · · · · · · · 2.3 introduced an chunking imthat adapts an underlying chunker via rescoring. In rescoring, each of these outputs is scored on its own and the new best output is returned. The rescoring model is a longer-distance generative model that produces alternating out/entity tags for all characters. The joint probability of the specified chunking is: · J. · in · · where each estimator is a character language and where the distinct characters not in the training/test sets that encode begin-ofend-of-sentence and type In words, we generate an sequence of type estimates, and ending with an We begin by conditioning on the begin-of-sentence tag. Because the first character is in an entity, we do not generate any text, but rather generate a character indicating that we are done generating and ready to switch to generating person characters. We then generate the J. Smith the person model; note that type estimates always begin and end with the essentially making them bounded models. After generating the name and the character to end the entity, we revert to generating more out characters, starting from a person and ending with a location. Note that we are generatthe phrase in the preceding and following space. All such spaces are generated in for English; there are no spaces in the Chinese input. Next, we generate the location phrase the same way as the person phrase. Next, generate the final period in the and then the end-of-sentence symbol. Note that language model shoulders the brunt of the burden of estimating contextual effects. It conditions on the preceding type, so the likelihood of in conditioned on following a person entity. Furthermore, the choice to begin an entity of type location is based on fact that it follows This includes begin-of-sentence and end-of-sentence effects, so the model is sensitive to initial capitalization in the out model as a distribution of character</abstract>
<note confidence="0.904472416666667">likely to follow Similarly, the 171 Corpus R P OOV City Uni Hong Kong .966 .957 .961 .972 4.0% .555 Microsoft Research .959 .955 .957 .963 3.4% .494 Academia Sinica .951 .935 .943 .958 4.2% .389 U Penn and U Colorado .919 .895 .907 .933 8.8% .459 Figure 3: Word Segmentation Results (Closed Category) Corpus R P City Uni HK .8417 .8690 .8551 .8903 .8961 .8762 .8749 .8943 .6997 .8176 MS Research .8097 .8188 .8142 .8651 .8351 .8716 .7968 .8438 .7739 .6899 Figure 4: Named Entity Recognition Results (Closed Category)</note>
<abstract confidence="0.982708347826087">end-of-sentence is conditioned on the preceding text, in this case a single period. The resulting model defines a (properly normalized) joint probability distribution over chunkings. 4 Held-out Parameter Tuning We ran preliminary tests on MUC 6 English and City University of Hong Kong data for Chinese and found baseline performance around 72% and rescored performance around 82%. The underlying model was designed to have good recall in generating hypotheses. Over 99% of the MUC test sentences had their correct analysis in a 1024-best list generated by the underlying model. Nevertheless, setting the number of hypotheses beyond 64 did not improve results in either English or Chiso we reported runs with set to 64. We believe this is because the two language-model based approaches make highly correlated ranking based on character Held-out scores peaked with 5-grams for Chinese; 3-grams and 4-grams were not much worse longer performed nearly identically. We used 7500 as the number of distinct characters, though this parameter is not at all sensitive to within an order of magnitude. We used Ling- Pipe’s default of setting the interpolation parameequal to the length; for the final eval- Higher interpolation ratios favor precision over recall, lower ratios favor recall. Values within an order of magnitude performed with 1% F-measure and 2% precision/recall. 5 Bakeoff Time and Effort The total time spent on this SIGHAN bakeoff was about 2 hours for the word segmentation task and 10 hours for the named-entity task (not including writing this paper). We started from a working word segmentation system for the last SIGHAN. Most of the time was spent munging entity data, with the rest devoted to held out analysis. The final code was roughly one page per task, with only a dozen or so LingPipe-specific lines. The final run, including unpacking, training and testing, took 45 minutes on a 512MB home PC; most of the time was named-entity decoding. 6 Results Official bakeoff results for the four word segmentation corpora are shown in Figure 3, and for the two named entity corpora in Figure 4. Column are recall, precision, the best closed the out-of-vocabulary in the test corpus, and recall on the out-of-vocabulary items. For the named-entity results, precision and recall are also broken down by category. 7 Distribution LingPipe may be downloaded from its homepage, The code for the bakeoff is available via anonymous CVS from the sandbox. An Apache Ant makefile is provided to generate our bakeoff submission from the official data distribution format. References Carpenter, B. 2005. Scaling high-order character language to gigabytes. Software Ann Arbor. Culotta, A. and A. McCallum. 2004. Confidence estimation information extraction. Boston. Teahan, W. J., Y. Wen, R. McNab, and I. H. Witten. 2000. A compression-based algorithm for Chinese word segmenta-</abstract>
<intro confidence="0.527942">172</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Carpenter</author>
</authors>
<title>Scaling high-order character language models to gigabytes. ACL Software Workshop.</title>
<date>2005</date>
<location>Ann Arbor.</location>
<contexts>
<context position="2454" citStr="Carpenter 2005" startWordPosition="384" endWordPosition="385">liasi.spell for Chinese word segmentation and com.aliasi.chunk for named-entity extraction. Both of these depend on the character language modeling package com.aliasi.lm, and the chunker also depends on the hidden Markov model package com.alias.hmm. The experiments reported in this paper were carried out in May 2006 using (a prerelease version of) LingPipe 2.3.0. 3.1 LingPipe’s Character Language Models LingPipe provides n-gram based character language models with a generalized form of WittenBell smoothing, which performed better than other approaches to smoothing in extensive English trials (Carpenter 2005). Language models provide a probability distribution P(σ) defined for strings σ ∈ E* over a fixed alphabet of characters E. We begin with Markovian language models normalized as random processes. This means the sum of the probabilities for strings of a fixed length is 1.0. The chain rule factors P(σc) = P(σ) · P(c|σ) for a character c and string σ. The n-gram Markovian assumption restricts the context to the previous n −1 characters, taking P(cn|σc1 · · · cn−1) = P(cn|c1 ··· cn−1). The maximum likelihood estimator for n-grams is PML(c|σ) = count(σc)/extCount(σ), where count(σ) is the number of</context>
</contexts>
<marker>Carpenter, 2005</marker>
<rawString>Carpenter, B. 2005. Scaling high-order character language models to gigabytes. ACL Software Workshop. Ann Arbor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Culotta</author>
<author>A McCallum</author>
</authors>
<title>Confidence estimation for information extraction. HLT/NAACL</title>
<date>2004</date>
<location>Boston.</location>
<contexts>
<context position="6369" citStr="Culotta and McCallum 2004" startWordPosition="1007" endWordPosition="1010">space-insertion/matching probabilities reduces decoding accuracy. 3.3 LingPipe’s Named Entity Recognition LingPipe 2.1 introduced a hidden Markov model interface with several decoders: first-best (Viterbi), n-best (Viterbi forward, A* backward with exact Viterbi estimates), and confidencebased (forward-backward). LingPipe 2.2 introduced a chunking implementation that codes a chunking problem as an HMM tagging problem using a refinement of the standard BIO coding. The refinement both introduces context and greatly simplifies confidence estimation over the approach using standard BIO coding in (Culotta and McCallum 2004). The tags are B-T for the first character in a multi-character entity of type T, M-T for a middle character in a multi-character entity, E-T for the end character in a multi-character entity, and W-T for a single character entity. The out tags are similarly contextualized, with additional information on the start/end tags to model their context. Specifically, the tags used are B-O-T for a character not in an entity following an entity of type T, I-O for any middle character not in an entity, and E-O-T for a character not in an entity but preceding a character in an entity of type T, and final</context>
</contexts>
<marker>Culotta, McCallum, 2004</marker>
<rawString>Culotta, A. and A. McCallum. 2004. Confidence estimation for information extraction. HLT/NAACL 2004. Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J Teahan</author>
<author>Y Wen</author>
<author>R McNab</author>
<author>I H Witten</author>
</authors>
<title>A compression-based algorithm for Chinese word segmentation.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>3</issue>
<contexts>
<context position="5703" citStr="Teahan et al. 2000" startWordPosition="915" endWordPosition="918">Pc(σ|µ) = 1.0 if σ is identical to µ with all of the spaces removed, and 0.0 otherwise. This channel is easily implemented as a weighted edit distance where deletion of a single space is 100% likely (log probability edit “cost” is zero) and matching a character is 100% likely, with any other operation being 0% likely (infinite cost). This makes any segmentation equally likely according to the channel model, reducing decoding to finding the highest likelihood hypothesis consisting of the test string with spaces inserted. This approach reduces to the cross-entropy/compression-based approach of (Teahan et al. 2000). Experiments showed that skewing these space-insertion/matching probabilities reduces decoding accuracy. 3.3 LingPipe’s Named Entity Recognition LingPipe 2.1 introduced a hidden Markov model interface with several decoders: first-best (Viterbi), n-best (Viterbi forward, A* backward with exact Viterbi estimates), and confidencebased (forward-backward). LingPipe 2.2 introduced a chunking implementation that codes a chunking problem as an HMM tagging problem using a refinement of the standard BIO coding. The refinement both introduces context and greatly simplifies confidence estimation over the</context>
</contexts>
<marker>Teahan, Wen, McNab, Witten, 2000</marker>
<rawString>Teahan, W. J., Y. Wen, R. McNab, and I. H. Witten. 2000. A compression-based algorithm for Chinese word segmentation. Computational Linguistics, 26(3):375–393.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>