<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998942">
A Statistical Parser for Czech*
</title>
<author confidence="0.847387">
Michael Collins
</author>
<affiliation confidence="0.5785565">
AT&amp;T Labs—Research,
Shannon Laboratory,
</affiliation>
<address confidence="0.8923615">
180 Park Avenue,
Florham Park, NJ 07932
</address>
<email confidence="0.995108">
mcollins@research.att
</email>
<author confidence="0.853689">
Lance Ramshaw
</author>
<affiliation confidence="0.393011">
BBN Technologies,
</affiliation>
<address confidence="0.853718">
70 Fawcett St.,
Cambridge, MA 02138
1 ramshaw@bbn . corn
</address>
<author confidence="0.804868">
Jan Haj
</author>
<affiliation confidence="0.488733333333333">
Institute of Formal and Applied Linguistics
Charles University,
Prague, Czech Republic
</affiliation>
<title confidence="0.434127">
hajic@ufal .mff cuni .cz
</title>
<author confidence="0.955416">
Christoph Tillmann
</author>
<affiliation confidence="0.648637">
Lehrstuhl für Informatik VI,
RVVTH Aachen
D-52056 Aachen, Germany
</affiliation>
<email confidence="0.988497">
tillmann@informatik.rwth—aachen.de
</email>
<sectionHeader confidence="0.995517" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999282545454545">
This paper considers statistical parsing of Czech,
which differs radically from English in at least two
respects: (1) it is a highly inflected language, and
(2) it has relatively free word order. These dif-
ferences are likely to pose new problems for tech-
niques that have been developed on English. We
describe our experience in building on the parsing
model of (Collins 97). Our final results — 80% de-
pendency accuracy — represent good progress to-
wards the 91% accuracy of the parser on English
(Wall Street Journal) text.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999924">
Much of the recent research on statistical parsing
has focused on English; languages other than En-
glish are likely to pose new problems for statisti-
cal methods. This paper considers statistical pars-
ing of Czech, using the Prague Dependency Tree-
bank (PDT) (Haji, 1998) as a source of training and
test data (the PDT contains around 480,000 words
of general news, business news, and science articles
</bodyText>
<footnote confidence="0.8837876">
* This material is based upon work supported by the National
Science Foundation under Grant No. (#IIS-9732388), and was
carried out at the 1998 Workshop on Language Engineering,
Center for Language and Speech Processing, Johns Hopkins
University. Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of the authors
and do not necessarily reflect the views of the National Sci-
ence Foundation or The Johns Hopkins University. The project
has also had support at various levels from the following grants
and programs: Grant Agency of the Czech Republic grants No.
405/96/0198 and 405/96/K214 and Ministry of Education of
the Czech Republic Project No. VS96151. We would also like
to thank Eric Brill, Barbora Hladka, Frederick Jelinek, Doug
Jones, Cynthia Kuo, Oren Schwartz, and Daniel Zeman for
many useful discussions during and after the workshop.
</footnote>
<listItem confidence="0.789589666666667">
annotated for dependency structure). Czech differs
radically from English in at least two respects:
• It is a highly inflected (HI) language. Words
in Czech can inflect for a number of syntac-
tic features: case, number, gender, negation
and so on. This leads to a very large number
of possible word forms, and consequent sparse
data problems when parameters are associated
with lexical items. On the positive side, inflec-
tional information should provide strong cues
to parse structure; an important question is how
to parameterize a statistical parsing model in a
way that makes good use of inflectional infor-
mation.
• It has relatively free word order (FWO). For
example, a subject-verb-object triple in Czech
can generally appear in all 6 possible surface
orders (SVO, SOY, VSO etc.).
</listItem>
<bodyText confidence="0.9741990625">
Other Slavic languages (such as Polish, Russian,
Slovak, Slovene, Serbo-croatian, Ukrainian) also
show these characteristics. Many European lan-
guages exhibit FWO and HI phenomena to a lesser
extent. Thus the techniques and results found for
Czech should be relevant to parsing several other
languages.
This paper first describes a baseline approach,
based on the parsing model of (Collins 97), which
recovers dependencies with 72% accuracy. We then
describe a series of refinements to the model, giv-
ing an improvement to 80% accuracy, with around
82% accuracy on newspaper/business articles. (As
a point of comparison, the parser achieves 91% de-
pendency accuracy on English (Wall Street Journal)
text.)
</bodyText>
<page confidence="0.997602">
505
</page>
<sectionHeader confidence="0.936777" genericHeader="introduction">
2 Data and Evaluation
</sectionHeader>
<bodyText confidence="0.999900303030303">
The Prague Dependency Treebank PDT (Hap,
1998) has been modeled after the Penn Treebank
(Marcus et al. 93), with one important excep-
tion: following the Praguian linguistic tradition,
the syntactic annotation is based on dependencies
rather than phrase structures. Thus instead of &amp;quot;non-
terminal&amp;quot; symbols used at the non-leaves of the tree,
the PDT uses so-called analytical functions captur-
ing the type of relation between a dependent and
its governing node. Thus the number of nodes is
equal to the number of tokens (words + punctuation)
plus one (an artificial root node with rather techni-
cal function is added to each sentence). The PDT
contains also a traditional morpho-syntactic anno-
tation (tags) at each word position (together with a
lemma, uniquely representing the underlying lexical
unit). As Czech is a HI language, the size of the set
of possible tags is unusually high: more than 3,000
tags may be assigned by the Czech morphological
analyzer. The PDT also contains machine-assigned
tags and lemmas for each word (using a tagger de-
scribed in (Haji 6 and Hladka, 1998)).
For evaluation purposes, the PDT has been di-
vided into a training set (19k sentences) and a de-
velopment/evaluation test set pair (about 3,500 sen-
tences each). Parsing accuracy is defined as the ratio
of correct dependency links vs. the total number of
dependency links in a sentence (which equals, with
the one artificial root node added, to the number of
tokens in a sentence). As usual, with the develop-
ment test set being available during the development
phase, all final results has been obtained on the eval-
uation test set, which nobody could see beforehand.
</bodyText>
<sectionHeader confidence="0.963127" genericHeader="method">
3 A Sketch of the Parsing Model
</sectionHeader>
<bodyText confidence="0.9975564">
The parsing model builds on Model 1 of (Collins
97); this section briefly describes the model. The
parser uses a lexicalized grammar — each non-
terminal has an associated head-word and part-of-
speech (POS). We write non-terminals as X (x): X
is the non-terminal label, and x is a (w, t) pair where
w is the associated head-word, and t as the POS tag.
See figure 1 for an example lexicalized tree, and a
list of the lexicalized rules that it contains.
Each rule has the fonnl :
</bodyText>
<equation confidence="0.9990075">
P(h) (h)Ri(ri)...Rm(rni)
(1)
</equation>
<bodyText confidence="0.7999855">
&apos;With the exception of the top rule in the tree, which has the
form TOP H (h) .
H is the head-child of the phrase, which inher-
its the head-word h from its parent P. L1...Ln
and RI are left and right modifiers of
H. Either n or m may be zero, and n =
</bodyText>
<equation confidence="0.99098875">
171 = 0 for unary rules. For example,
in S (bought , VBD) --+ NP (yesterday, NN)
NP ( IBM, NNP) VP (bought , VBD) :
n = 2 m = 0
P=s FL= VP
L1 = NP L2 = NP
= (IBM, NNP) /2 = (yesterday , NN)
h = (bought , VBD)
</equation>
<bodyText confidence="0.999545857142857">
The model can be considered to be a variant
of Probabilistic Context-Free Grammar (PCFG). In
PCFGs each rule a —&gt; in the CFG underlying
the PCFG has an associated probability P(/31a).
In (Collins 97), P (01a) is defined as a product of
terms, by assuming that the right-hand-side of the
rule is generated in three steps:
</bodyText>
<listItem confidence="0.996040333333333">
1. Generate the head constituent label of the
phrase, with probability PH (H I P, h).
2. Generate modifiers to the left of the head with
</listItem>
<bodyText confidence="0.9976594">
probability I P, h, H),
where Ln+i (in+1) = STOP. The STOP
symbol is added to the vocabulary of non-
terminals, and the model stops generating left
modifiers when it is generated.
</bodyText>
<listItem confidence="0.896022333333333">
3. Generate modifiers to the right of the head with
probability PR (Ri (ri) I P, h, H).
Rm.fi (rm+i) is defined as STOP.
For example, the probability of S (bought , VBD)
-&gt; NP (yesterday, NN) NP ( IBM, NNP )
VP (bought , VBD) is defined as
</listItem>
<equation confidence="0.9869664">
Ph(VP I S , bought , VBD) x
(NP ( IBM, NNP ) I S , VP , bought , VBD) x
(NP (yesterday, NN) I S , VP , bought , VBD) x
Pi (STOP I S , VP , bought , VI3D) x
Pr (STOP I S , VP , bought , VBD)
</equation>
<bodyText confidence="0.997438777777778">
Other rules in the tree contribute similar sets of
probabilities. The probability for the entire tree is
calculated as the product of all these terms.
(Collins 97) describes a series of refinements to
this basic model: the addition of &amp;quot;distance&amp;quot; (a con-
ditioning feature indicating whether or not a mod-
ifier is adjacent to the head); the addition of sub-
categorization parameters (Model 2), and parame-
ters that model wh-movement (Model 3); estimation
</bodyText>
<page confidence="0.91308">
506
</page>
<equation confidence="0.9661574">
TOP
S(bought,VBD)
NP(yesterday,NN) NP(IBM,NNP) VP(bought,VBD)
NN NNP VBD NP(Lotus,NNP)
yesterday IBM bought NNP
Lotus
TOP
S(bought,VBD)
NP(yesterday,NN)
NP(IBM,NNP)
VP(bought,VBD)
NP(Lotus,NNP)
S(bought,VBD)
NP(yesterday,NN)
NN(yesterday)
NNP(IBM)
VBD(bought)
NNP(Lotus)
NP(IBM,NNP) VP(bought,VBD)
NP(Lotus,NNP)
</equation>
<figureCaption confidence="0.999395">
Figure 1: A lexicalized parse tree, and a list of the rules it contains.
</figureCaption>
<bodyText confidence="0.999917166666667">
techniques that smooth various levels of back-off (in
particular using POS tags as word-classes, allow-
ing the model to learn generalizations about POS
classes of words). Search for the highest probabil-
ity tree for a sentence is achieved using a CKY-style
parsing algorithm.
</bodyText>
<sectionHeader confidence="0.998694" genericHeader="method">
4 ParsingtheCzechPDT
</sectionHeader>
<bodyText confidence="0.99997180952381">
Many statistical parsing methods developed for En-
glish use lexicalized trees as a representation (e.g.,
(Jelinek et al. 94; Magerman 95; Ratnaparkhi 97;
Charniak 97; Collins 96; Collins 97)); several (e.g.,
(Eisner 96; Collins 96; Collins 97; Charniak 97))
emphasize the use of parameters associated with
dependencies between pairs of words. The Czech
PDT contains dependency annotations, but no tree
structures. For parsing Czech we considered a strat-
egy of converting dependency structures in training
data to lexicalized trees, then running the parsing
algorithms originally developed for English. A key
point is that the mapping from lexicalized trees to
dependency structures is many-to-one. As an exam-
ple, figure 2 shows an input dependency structure,
and three different lexicalized trees with this depen-
dency structure.
The choice of tree structure is crucial in determin-
ing the independence assumptions that the parsing
model makes. There are at least 3 degrees of free-
dom when deciding on the tree structures:
</bodyText>
<listItem confidence="0.929074142857143">
1. How &amp;quot;flat&amp;quot; should the trees be? The trees could
be as flat as possible (as in figure 2(a)), or bi-
nary branching (as in trees (b) or (c)), or some-
where between these two extremes.
2. What non-terminal labels should the internal
nodes have?
3. What set of POS tags should be used?
</listItem>
<subsectionHeader confidence="0.953552">
4.1 A Baseline Approach
</subsectionHeader>
<bodyText confidence="0.968759">
To provide a baseline result we implemented what is
probably the simplest possible conversion scheme:
</bodyText>
<listItem confidence="0.8551188">
1. The trees were as flat as possible, as in fig-
ure 2(a).
2. The non-terminal labels were &amp;quot;XP&amp;quot;, where X
is the first letter of the POS tag of the head-
word for the constituent. See figure 3 for an
example.
3. The part of speech tags were the major cate-
gory for each word (the first letter of the Czech
POS set, which corresponds to broad category
distinctions such as verb, noun etc.).
</listItem>
<bodyText confidence="0.9986145">
The baseline approach gave a result of 71.9% accu-
racy on the development test set.
</bodyText>
<page confidence="0.968579">
507
</page>
<figure confidence="0.975286208333333">
Input:
sentence with part of speech tags: TIN saw/V the/D man/N (N=noun, V=verb, D=determiner)
dependencies (word = Parent): (I = saw), (saw START), (the = man), (man saw)
Output: a lexicalized tree
(b) X(saw)
N X(saw)
I V
saw
1 D N
X(man)
..„..-----..„
I 1
the man
(c) X(saw)
X(saw) X(man)
.......---..,.. „..----..„
NV D N
1 1 1 1
I saw the man
(a) X(saw) X(man)
V ....„----...„
1 D N
saw 1 1
the man
</figure>
<figureCaption confidence="0.97809575">
Figure 2: Converting dependency structures to lexicalized trees with equivalent dependencies. The trees
(a), (b) and (c) all have the input dependency structure: (a) is the &amp;quot;flattest&amp;quot; possible tree; (b) and (c) are
binary branching structures. Any labels for the non-terminals (marked X) would preserve the dependency
structure.
</figureCaption>
<figure confidence="0.730558">
VP(saw
</figure>
<figureCaption confidence="0.994603666666667">
Figure 3: The baseline approach for non-terminal
labels. Each label is XP, where X is the POS tag for
the head-word of the constituent.
</figureCaption>
<subsectionHeader confidence="0.976117">
4.2 Modifications to the Baseline Trees
</subsectionHeader>
<bodyText confidence="0.9987962">
While the baseline approach is reasonably success-
ful, there are some linguistic phenomena that lead
to clear problems. This section describes some tree
transformations that are linguistically motivated,
and lead to improvements in parsing accuracy.
</bodyText>
<subsubsectionHeader confidence="0.488256">
4.2.1 Relative Clauses
</subsubsectionHeader>
<bodyText confidence="0.99987375">
In the PDT the verb is taken to be the head of both
sentences and relative clauses. Figure 4 illustrates
how the baseline transformation method can lead to
parsing errors in relative clause cases. Figure 4(c)
shows the solution to the problem: the label of the
relative clause is changed to SBAR, and an addi-
tional VP level is added to the right of the relative
pronoun. Similar transformations were applied for
relative clauses involving Wh-PPs (e.g., &amp;quot;the man
to whom I gave a book&amp;quot;), Wh-NPs (e.g., &amp;quot;the man
whose book I read&amp;quot;) and Wh-Adverbials (e.g., &amp;quot;the
place where I live&amp;quot;).
</bodyText>
<subsectionHeader confidence="0.469262">
4.2.2 Coordination
</subsectionHeader>
<bodyText confidence="0.999934523809524">
The PDT takes the conjunct to be the head of coor-
dination structures (for example, and would be the
head of the NP dogs and cats). In these cases the
baseline approach gives tree structures such as that
in figure 5(a). The non-terminal label for the phrase
is JP (because the head of the phrase, the conjunct
and, is tagged as J).
This choice of non-terminal is problematic for
two reasons: (1) the JP label is assigned to all co-
ordinated phrases, for example hiding the fact that
the constituent in figure 5(a) is an NP; (2) the model
assumes that left and right modifiers are generated
independently of each other, and as it stands will
give unreasonably high probability to two unlike
phrases being coordinated. To fix these problems,
the non-terminal label in coordination cases was al-
tered to be the same as that of the second conjunct
(the phrase directly to the right of the head of the
phrase). See figure 5. A similar transformation was
made for cases where a comma was the head of a
phrase.
</bodyText>
<subsectionHeader confidence="0.636132">
4.2.3 Punctuation
</subsectionHeader>
<bodyText confidence="0.944996">
Figure 6 shows an additional change concerning
commas. This change increases the sensitivity of
the model to punctuation.
</bodyText>
<subsectionHeader confidence="0.996815">
4.3 Model Alterations
</subsectionHeader>
<bodyText confidence="0.9996845">
This section describes some modifications to the pa-
rameterization of the model.
</bodyText>
<figure confidence="0.99607252">
NP(man)
,.....---...„
D N
1 1
the man
1
saw
508
(a) VP
V
likes
NP
John
(h)
NP
Mary VP
V NP
who likes Tim
VP
Mary SBAR
Z P VP
I I .......--■,,
who V NP
I I
likes Tim
</figure>
<figureCaption confidence="0.999767">
Figure 4: (a) The baseline approach does not distin-
</figureCaption>
<bodyText confidence="0.819281625">
guish main clauses from relative clauses: both have
a verb as the head, so both are labeled VP. (b) A typ-
ical parsing error due to relative and main clauses
not being distinguished. (note that two main clauses
can be coordinated by a comma, as in John likes
Mary, Mary likes Tim). (c) The solution to the prob-
lem: a modification to relative clause structures in
training data.
</bodyText>
<sectionHeader confidence="0.4306925" genericHeader="method">
4.3.1 Preferences for dependencies that do not
cross verbs
</sectionHeader>
<bodyText confidence="0.9999398">
The model of (Collins 97) had conditioning vari-
ables that allowed the model to learn a preference
for dependencies which do not cross verbs. From
the results in table 3, adding this condition improved
accuracy by about 0.9% on the development set.
</bodyText>
<subsubsectionHeader confidence="0.347477">
4.3.2 Punctuation for phrasal boundaries
</subsubsectionHeader>
<bodyText confidence="0.999909">
The parser of (Collins 96) used punctuation as an in-
dication of phrasal boundaries. It was found that if a
constituent Z (...XY...) has two children X and
Y separated by a punctuation mark, then Y is gen-
erally followed by a punctuation mark or the end of
</bodyText>
<figure confidence="0.734153666666667">
JP(a) Is) NP(a)
NP(h j) J NP(h 2 ) NP(h 1) J NP(h 2 )
and and
</figure>
<figureCaption confidence="0.6676085">
Figure 5: An example of coordination. The base-
line approach (a) labels the phrase as a JP; the re-
finement (b) takes the second conjunct&apos;s label as the
non-terminal for the whole phrase.
</figureCaption>
<figure confidence="0.596274">
NP(h) NPX(h)
Z(,) N(h) Z(,) NP(h)
I ... I
N(h)
</figure>
<figureCaption confidence="0.966278">
Figure 6: An additional change, triggered by a
comma that is the left-most child of a phrase: a new
non-terminal NPX is introduced.
</figureCaption>
<bodyText confidence="0.9810215">
sentence marker. The parsers of (Collins 96,97) en-
coded this as a hard constraint. In the Czech parser
we added a cost of -2.5 (log probability)2 to struc-
tures that violated this constraint.
</bodyText>
<subsectionHeader confidence="0.598747">
4.3.3 First-Order (Bigram) Dependencies
</subsectionHeader>
<bodyText confidence="0.99970475">
The model of section 3 made the assumption that
modifiers are generated independently of each other.
This section describes a hi gram model, where the
context is increased to consider the previously gen-
erated modifier ((Eisner 96) also describes use of
bigram statistics). The right-hand-side of a rule is
now assumed to be generated in the following three
step process:
</bodyText>
<listItem confidence="0.9976215">
1. Generate the head label, with probability
RH(H f P, h)
2. Generate left modifiers with probability
II Pc (Li(ii) I Li_i , P,h,H)
</listItem>
<bodyText confidence="0.9999345">
where Lo is defined as a special NULL sym-
bol. Thus the previous modifier, Li_1, is
added to the conditioning context (in the pre-
vious model the left modifiers had probability
</bodyText>
<equation confidence="0.628291">
fli=1..n+1 P (L (1i) I 13,11,11).)
</equation>
<listItem confidence="0.5869575">
3. Generate right modifiers using a similar bi-
gram process.
</listItem>
<bodyText confidence="0.977656666666667">
Introducing bigram-dependencies into the parsing
model improved parsing accuracy by about 0.9 %
(as shown in Table 3).
</bodyText>
<footnote confidence="0.736772">
2This value was optimized on the development set
</footnote>
<figure confidence="0.917873">
NP V NP
John likes Mary
who likes Tim
VP
</figure>
<page confidence="0.933304">
509
</page>
<table confidence="0.9946285">
1. main part of 8. person
speech
2. detailed part of 9. tense
speech
3. gender 10. degree of compar-
ison
4. number 11. negativeness
5. case 12. voice
6. possessor&apos;s 13. variant/register
gender
7. possessor&apos;s num-
ber
</table>
<tableCaption confidence="0.988186">
Table 1: The 13-character encoding of the Czech
POS tags.
</tableCaption>
<subsectionHeader confidence="0.99492">
4.4 Alternative Part-of-Speech Tagsets
</subsectionHeader>
<bodyText confidence="0.999945055555556">
Part of speech (POS) tags serve an important role
in statistical parsing by providing the model with a
level of generalization as to how classes of words
tend to behave, what roles they play in sentences,
and what other classes they tend to combine with.
Statistical parsers of English typically make use of
the roughly 50 POS tags used in the Penn Treebank
corpus, but the Czech PDT corpus provides a much
richer set of POS tags, with over 3000 possible tags
defined by the tagging system and over 1000 tags
actually found in the corpus. Using that large a
tagset with a training corpus of only 19,000 sen-
tences would lead to serious sparse data problems.
It is also clear that some of the distinctions being
made by the tags are more important than others
for parsing. We therefore explored different ways
of extracting smaller but still maximally informative
POS tagsets.
</bodyText>
<subsectionHeader confidence="0.907429">
4.4.1 Description of the Czech Tagset
</subsectionHeader>
<bodyText confidence="0.999734066666667">
The POS tags in the Czech PDT corpus (Haji 6 and
Hladka, 1997) are encoded in 13-character strings.
Table 1 shows the role of each character. For exam-
ple, the tag NNMP1 A would be used for a
word that had &amp;quot;noun&amp;quot; as both its main and detailed
part of speech, that was masculine, plural, nomina-
tive (case 1), and whose negativeness value was &amp;quot;af-
firmative&amp;quot;.
Within the corpus, each word was annotated with
all of the POS tags that would be possible given its
spelling, using the output of a morphological analy-
sis program, and also with the single one of those
tags that a statistical POS tagging program had
predicted to be the correct tag (Hake and Hladka,
1998). Table 2 shows a phrase from the corpus, with
</bodyText>
<table confidence="0.9987495">
Form Dictionary Tags Machine Tag
poslanci NNMP1 A-- NNMP1 A- -
NNMP5 A--
NNMP7 A--
NNMS3 A--
NNMS6 A- -
Parlamentu NNIS2 A-- NNIS2 A--
NNIS3 A- -
NNIS6 A-1
schvalili VpMP---XR-AA- VpMP---XR-AA-
</table>
<tableCaption confidence="0.826883">
Table 2: Corpus POS tags for &amp;quot;the representatives
of the Parliament approved&amp;quot;.
</tableCaption>
<bodyText confidence="0.99862175">
the alternative possible tags and machine-selected
tag for each word. In the training portion of the cor-
pus, the correct tag as judged by human annotators
was also provided.
</bodyText>
<subsectionHeader confidence="0.980945">
4.4.2 Selection of a More Informative Tagset
</subsectionHeader>
<bodyText confidence="0.999992818181818">
In the baseline approach, the first letter, or &amp;quot;main
part of speech&amp;quot;, of the full POS strings was used as
the tag. This resulted in a tagset with 13 possible
values.
A number of alternative, richer tagsets were ex-
plored, using various combinations of character po-
sitions from the tag string. The most successful al-
ternative was a two-letter tag whose first letter was
always the main POS, and whose second letter was
the case field if the main POS was one that dis-
plays case, while otherwise the second letter was
the detailed POS. (The detailed POS was used for
the main POS values D, J, V, and X; the case field
was used for the other possible main POS values.)
This two-letter scheme resulted in 58 tags, and pro-
vided about a 1.1% parsing improvement over the
baseline on the development set.
Even richer tagsets that also included the per-
son, gender, and number values were tested without
yielding any further improvement, presumably be-
cause the damage from sparse data outweighed the
value of the additional information present.
</bodyText>
<subsectionHeader confidence="0.917684">
4.4.3 Explorations toward Clustered Tagsets
</subsectionHeader>
<bodyText confidence="0.999976666666667">
An entirely different approach, rather than search-
ing by hand for effective tagsets, would be to use
clustering to derive them automatically. We ex-
plored two different methods, bottom-up and top-
down, for automatically deriving POS tag sets based
on counts of governing and dependent tags extracted
from the parse trees that the parser constructs from
the training data. Neither tested approach resulted
in any improvement in parsing performance com-
</bodyText>
<page confidence="0.986809">
510
</page>
<bodyText confidence="0.9997515">
pared to the hand-designed &amp;quot;two letter&amp;quot; tagset, but
the implementations of each were still only prelim-
inary, and a clustered tagset more adroitly derived
might do better.
</bodyText>
<subsectionHeader confidence="0.978437">
4.4.4 Dealing with Tag Ambiguity
</subsectionHeader>
<bodyText confidence="0.99999345">
One final issue regarding POS tags was how to deal
with the ambiguity between possible tags, both in
training and test. In the training data, there was a
choice between using the output of the POS tagger
or the human annotator&apos;s judgment as to the correct
tag. In test data, the correct answer was not avail-
able, but the POS tagger output could be used if de-
sired. This turns out to matter only for unknown
words, as the parser is designed to do its own tag-
ging, for words that it has seen in training at least
5 times, ignoring any tag supplied with the input.
For &amp;quot;unknown&amp;quot; words (seen less than 5 times), the
parser can be set either to believe the tag supplied
by the POS tagger or to allow equally any of the
dictionary-derived possible tags for the word, effec-
tively allowing the parse context to make the choice.
(Note that the rich inflectional morphology of Czech
leads to a higher rate of &amp;quot;unknown&amp;quot; word forms than
would be true in English; in one test, 29.5% of the
words in test data were &amp;quot;unknown&amp;quot;)
Our tests indicated that if unknown words are
treated by believing the POS tagger&apos;s suggestion,
then scores are better if the parser is also trained
on the POS tagger&apos;s suggestions, rather than on the
human annotator&apos;s correct tags. Training on the cor-
rect tags results in 1% worse performance. Even
though the POS tagger&apos;s tags are less accurate, they
are more like what the parser will be using in the test
data, and that turns out to be the key point. On the
other hand, if the parser allows all possible dictio-
nary tags for unknown words in test material, then
it pays to train on the actual correct tags.
In initial tests, this combination of training on the
correct tags and allowing all dictionary tags for un-
known test words somewhat outperformed the alter-
native of using the POS tagger&apos;s predictions both for
training and for unknown test words. When tested
with the final version of the parser on the full de-
velopment set, those two strategies performed at the
same level.
</bodyText>
<sectionHeader confidence="0.999971" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.99876725">
We ran three versions of the parser over the final
test set: the baseline version, the full model with
all additions, and the full model with everything but
the bigram model. The baseline system on the fi-
</bodyText>
<table confidence="0.9879865">
Modification Improvement
Coordination +2.6%
Relative clauses +1.5%
Punctuation -0.1%??
Enriched POS tags +1.1%
Punctuation +0.4%
Verb crossing +0.9%
Bigram +0.9%
Total change +7.4%
Total Relative Error reduction 26%
</table>
<tableCaption confidence="0.989068">
Table 3: A breakdown of the results on the develop-
ment set.
</tableCaption>
<table confidence="0.999445">
Genre Proportion Accuracy
(Sentences/
Dependencies)
Newspaper 50%/44% 81.4%
Business 25%/19% 81.4%
Science 25%/38% 76.0%
</table>
<tableCaption confidence="0.960019">
Table 4: Breakdown of the results by genre. Note
</tableCaption>
<bodyText confidence="0.999609777777778">
that although the Science section only contributes
25% of the sentences in test data, it contains much
longer sentences than the other sections and there-
fore accounts for 38% of the dependencies in test
data.
nal test set achieved 72.3% accuracy. The final sys-
tem achieved 80.0% accuracy3: a 7.7% absolute im-
provement and a 27.8% relative improvement.
The development set showed very similar results:
a baseline accuracy of 71.9% and a final accuracy of
79.3%. Table 3 shows the relative improvement of
each component of the mode14. Table 4 shows the
results on the development set by genre. It is inter-
esting to see that the performance on newswire text
is over 2% better than the averaged performance.
The Science section of the development set is con-
siderably harder to parse (presumably because of
longer sentences and more open vocabulary).
</bodyText>
<footnote confidence="0.599780076923077">
3The parser fails to give an analysis on some sentences, be-
cause the search space becomes too large. The baseline system
missed 5 sentences; the full system missed 21 sentences; the
full system minus bigrams missed 2 sentences. To score the
full system we took the output from the full system minus bi-
grams when the full system produced no output (to prevent a
heavy penalty due to the 21 missed sentences). The remaining
2 unparsed sentences (5 in the baseline case) had all dependen-
cies attached to the root.
4We were surprised to see this slight drop in accuracy for
the punctuation tree modification. Earlier tests on a different
development set, with less training data and fewer other model
alterations had shown a good improvement for this feature.
</footnote>
<page confidence="0.988782">
511
</page>
<subsectionHeader confidence="0.985496">
5.1 Comparison to Previous Results
</subsectionHeader>
<bodyText confidence="0.99998128">
The main piece of previous work on parsing Czech
that we are aware of is described in (Kuboli 99).
This is a rule-based system which is based on a man-
ually designed set of rules. The system&apos;s accuracy
is not evaluated on a test corpus, so it is difficult
to compare our results to theirs. We can, however,
make some comparison of the results in this paper
to those on parsing English. (Collins 99) describes
results of 91% accuracy in recovering dependen-
cies on section 0 of the Penn Wall Street Journal
Treebank, using Model 2 of (Collins 97). This task
is almost certainly easier for a number of reasons:
there was more training data (40,000 sentences as
opposed to 19,000); Wall Street Journal may be an
easier domain than the PDT, as a reasonable pro-
portion of sentences come from a sub-domain, fi-
nancial news, which is relatively restricted. Unlike
model 1, model 2 of the parser takes subcategoriza-
tion information into account, which gives some im-
provement on English and might well also improve
results on Czech. Given these differences, it is dif-
ficult to make a direct comparison, but the overall
conclusion seems to be that the Czech accuracy is
approaching results on English, although it is still
somewhat behind.
</bodyText>
<sectionHeader confidence="0.999784" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999723125">
The 80% dependency accuracy of the parser repre-
sents good progress towards English parsing perfor-
mance. A major area for future work is likely to
be an improved treatment of morphology; a natural
approach to this problem is to consider more care-
fully how POS tags are used as word classes by
the model. We have begun to investigate this is-
sue, through the automatic derivation of POS tags
through clustering or &amp;quot;splitting&amp;quot; approaches. It
might also be possible to exploit the internal struc-
ture of the POS tags, for example through incremen-
tal prediction of the POS tag being generated; or to
exploit the use of word lemmas, effectively split-
ting word—word relations into syntactic dependen-
cies (POS tag—POS tag relations) and more seman-
tic (lemma—lemma) dependencies.
</bodyText>
<sectionHeader confidence="0.999476" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999874964912281">
E. Charniak. 1997. Statistical Parsing with a
Context-free Grammar and Word Statistics. Pro-
ceedings of the Fourteenth National Conference
on Artificial Intelligence, AAAI Press/MIT Press,
Menlo Park (1997).
M. Collins. 1996. A New Statistical Parser Based
on Bigram Lexical Dependencies. Proceedings of
the 34th Annual Meeting of the Association for
Computational Linguistics, pages 184-191.
M. Collins. 1997. Three Generative, Lexicalised
Models for Statistical Parsing. In Proceedings of
the 35th Annual Meeting of the Association for
Computational Linguistics and 8th Conference
of the European Chapter of the Association for
Computational Linguistics, pages 16-23.
M. Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. Thesis, Uni-
versity of Pennsylvania.
J. Eisner. 1996. Three New Probabilistic Models for
Dependency Parsing: An Exploration. Proceed-
ings of COLING-96, pages 340-345.
Jan Hajie. 1998. Building a Syntactically Anno-
tated Corpus: The Prague Dependency Treebank.
Issues of Valency and Meaning (Festschrift for
Jarmila Panevova). Carolina, Charles University,
Prague. pp. 106-132.
Jan Haji 6 and Barbora Hladka. 1997. Tagging of In-
flective Languages: a Comparison. In Proceed-
ings of the ANLP&apos;97, pages 136-143, Washing-
ton, DC.
Jan Hajie and Barbora Hladka. 1998. Tagging In-
flective Languages: Prediction of Morphological
Categories for a Rich, Structured Tagset. In Pro-
ceedings of ACl/Coling&apos;98, Montreal, Canada,
Aug. 5-9, pp. 483-490.
F. Jelinek, J. Lafferty, D. Magerman, R. Mercer,
A. Ratnaparkhi, S. Roukos. 1994. Decision Tree
Parsing using a Hidden Derivation Model. Pro-
ceedings of the 1994 Human Language Technol-
ogy Workshop, pages 272-277.
V. Kubofi. 1999. A Robust Parser for Czech.
Technical Report 6/1999, UFAL, Matematicko-
fyzikalni fakulta Karlovy univerzity, Prague.
D. Magerman. 1995. Statistical Decision-Tree Mod-
els for Parsing. Proceedings of the 33rd Annual
Meeting of the Association for Computational
Linguistics, pages 276-283.
M. Marcus, B. Santorini and M. Marcinkiewicz.
1993. Building a Large Annotated Corpus of En-
glish: the Penn Treebank. Computational Lin-
guistics, 19(2):313-330.
A. Ratnaparkhi. 1997. A Linear Observed Time Sta-
tistical Parser Based on Maximum Entropy Mod-
els. In Proceedings of the Second Conference
on Empirical Methods in Natural Language Pro-
cessing, Brown University, Providence, Rhode
Island.
</reference>
<page confidence="0.997063">
512
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.061183">
<title confidence="0.999872">A Statistical Parser for Czech*</title>
<author confidence="0.999653">Michael Collins</author>
<affiliation confidence="0.9966065">AT&amp;T Labs—Research, Shannon Laboratory,</affiliation>
<address confidence="0.9994645">180 Park Avenue, Florham Park, NJ 07932</address>
<email confidence="0.992154">mcollins@research.att</email>
<author confidence="0.987532">Lance Ramshaw</author>
<affiliation confidence="0.874829">Technologies,</affiliation>
<address confidence="0.9976815">70 Fawcett St., Cambridge, MA 02138</address>
<note confidence="0.376961">1 ramshaw@bbn . corn</note>
<author confidence="0.999434">Jan Haj</author>
<affiliation confidence="0.9996205">Institute of Formal and Applied Linguistics Charles University,</affiliation>
<address confidence="0.814289">Prague, Czech Republic</address>
<email confidence="0.835002">hajic@ufal.mffcuni.cz</email>
<author confidence="0.996721">Christoph Tillmann</author>
<affiliation confidence="0.861662">Lehrstuhl für Informatik VI,</affiliation>
<address confidence="0.7312365">RVVTH Aachen D-52056 Aachen, Germany</address>
<email confidence="0.999387">tillmann@informatik.rwth—aachen.de</email>
<abstract confidence="0.963321583333333">This paper considers statistical parsing of Czech, which differs radically from English in at least two (1) it is a inflected and it has relatively word order. differences are likely to pose new problems for techniques that have been developed on English. We describe our experience in building on the parsing model of (Collins 97). Our final results — 80% dependency accuracy — represent good progress towards the 91% accuracy of the parser on English (Wall Street Journal) text.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Statistical Parsing with a Context-free Grammar and Word Statistics.</title>
<date>1997</date>
<booktitle>Proceedings of the Fourteenth National Conference on Artificial Intelligence, AAAI</booktitle>
<publisher>Press/MIT Press,</publisher>
<location>Menlo Park</location>
<marker>Charniak, 1997</marker>
<rawString>E. Charniak. 1997. Statistical Parsing with a Context-free Grammar and Word Statistics. Proceedings of the Fourteenth National Conference on Artificial Intelligence, AAAI Press/MIT Press, Menlo Park (1997).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>A New Statistical Parser Based on Bigram Lexical Dependencies.</title>
<date>1996</date>
<booktitle>Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>184--191</pages>
<marker>Collins, 1996</marker>
<rawString>M. Collins. 1996. A New Statistical Parser Based on Bigram Lexical Dependencies. Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pages 184-191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Three Generative, Lexicalised Models for Statistical Parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>16--23</pages>
<marker>Collins, 1997</marker>
<rawString>M. Collins. 1997. Three Generative, Lexicalised Models for Statistical Parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics, pages 16-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. Thesis,</tech>
<institution>University of Pennsylvania.</institution>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. Thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Three New Probabilistic Models for Dependency Parsing: An Exploration.</title>
<date>1996</date>
<booktitle>Proceedings of COLING-96,</booktitle>
<pages>340--345</pages>
<marker>Eisner, 1996</marker>
<rawString>J. Eisner. 1996. Three New Probabilistic Models for Dependency Parsing: An Exploration. Proceedings of COLING-96, pages 340-345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajie</author>
</authors>
<title>Building a Syntactically Annotated Corpus: The Prague Dependency Treebank. Issues of Valency and Meaning (Festschrift for Jarmila Panevova).</title>
<date>1998</date>
<pages>106--132</pages>
<location>Carolina, Charles University, Prague.</location>
<marker>Hajie, 1998</marker>
<rawString>Jan Hajie. 1998. Building a Syntactically Annotated Corpus: The Prague Dependency Treebank. Issues of Valency and Meaning (Festschrift for Jarmila Panevova). Carolina, Charles University, Prague. pp. 106-132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbora Hladka</author>
</authors>
<title>Tagging of Inflective Languages: a Comparison.</title>
<date>1997</date>
<booktitle>In Proceedings of the ANLP&apos;97,</booktitle>
<pages>136--143</pages>
<location>Washington, DC.</location>
<contexts>
<context position="17729" citStr="Hladka, 1997" startWordPosition="3025" endWordPosition="3026">us, but the Czech PDT corpus provides a much richer set of POS tags, with over 3000 possible tags defined by the tagging system and over 1000 tags actually found in the corpus. Using that large a tagset with a training corpus of only 19,000 sentences would lead to serious sparse data problems. It is also clear that some of the distinctions being made by the tags are more important than others for parsing. We therefore explored different ways of extracting smaller but still maximally informative POS tagsets. 4.4.1 Description of the Czech Tagset The POS tags in the Czech PDT corpus (Haji 6 and Hladka, 1997) are encoded in 13-character strings. Table 1 shows the role of each character. For example, the tag NNMP1 A would be used for a word that had &amp;quot;noun&amp;quot; as both its main and detailed part of speech, that was masculine, plural, nominative (case 1), and whose negativeness value was &amp;quot;affirmative&amp;quot;. Within the corpus, each word was annotated with all of the POS tags that would be possible given its spelling, using the output of a morphological analysis program, and also with the single one of those tags that a statistical POS tagging program had predicted to be the correct tag (Hake and Hladka, 1998).</context>
</contexts>
<marker>Hladka, 1997</marker>
<rawString>Jan Haji 6 and Barbora Hladka. 1997. Tagging of Inflective Languages: a Comparison. In Proceedings of the ANLP&apos;97, pages 136-143, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajie</author>
<author>Barbora Hladka</author>
</authors>
<title>Tagging Inflective Languages: Prediction of Morphological Categories for a Rich, Structured Tagset.</title>
<date>1998</date>
<booktitle>In Proceedings of ACl/Coling&apos;98,</booktitle>
<pages>483--490</pages>
<location>Montreal, Canada,</location>
<marker>Hajie, Hladka, 1998</marker>
<rawString>Jan Hajie and Barbora Hladka. 1998. Tagging Inflective Languages: Prediction of Morphological Categories for a Rich, Structured Tagset. In Proceedings of ACl/Coling&apos;98, Montreal, Canada, Aug. 5-9, pp. 483-490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>J Lafferty</author>
<author>D Magerman</author>
<author>R Mercer</author>
<author>A Ratnaparkhi</author>
<author>S Roukos</author>
</authors>
<title>Decision Tree Parsing using a Hidden Derivation Model.</title>
<date>1994</date>
<booktitle>Proceedings of the 1994 Human Language Technology Workshop,</booktitle>
<pages>272--277</pages>
<marker>Jelinek, Lafferty, Magerman, Mercer, Ratnaparkhi, Roukos, 1994</marker>
<rawString>F. Jelinek, J. Lafferty, D. Magerman, R. Mercer, A. Ratnaparkhi, S. Roukos. 1994. Decision Tree Parsing using a Hidden Derivation Model. Proceedings of the 1994 Human Language Technology Workshop, pages 272-277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Kubofi</author>
</authors>
<title>A Robust Parser for Czech.</title>
<date>1999</date>
<booktitle>UFAL, Matematickofyzikalni fakulta Karlovy univerzity,</booktitle>
<tech>Technical Report 6/1999,</tech>
<location>Prague.</location>
<marker>Kubofi, 1999</marker>
<rawString>V. Kubofi. 1999. A Robust Parser for Czech. Technical Report 6/1999, UFAL, Matematickofyzikalni fakulta Karlovy univerzity, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Magerman</author>
</authors>
<title>Statistical Decision-Tree Models for Parsing.</title>
<date>1995</date>
<booktitle>Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>276--283</pages>
<marker>Magerman, 1995</marker>
<rawString>D. Magerman. 1995. Statistical Decision-Tree Models for Parsing. Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 276-283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: the Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini and M. Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A Linear Observed Time Statistical Parser Based on Maximum Entropy Models.</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<institution>Brown University,</institution>
<location>Providence, Rhode Island.</location>
<marker>Ratnaparkhi, 1997</marker>
<rawString>A. Ratnaparkhi. 1997. A Linear Observed Time Statistical Parser Based on Maximum Entropy Models. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, Brown University, Providence, Rhode Island.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>