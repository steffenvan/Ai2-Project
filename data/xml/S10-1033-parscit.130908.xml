<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.084029">
<title confidence="0.957696">
SJTULTLAB: Chunk Based Method for Keyphrase Extraction
</title>
<author confidence="0.997715">
Letian Wang
</author>
<affiliation confidence="0.95496575">
Department of
Computer Science &amp; Engineering
Shanghai Jiao Tong University
Shanghai, China
</affiliation>
<email confidence="0.995911">
koh@sjtu.edu.cn
</email>
<sectionHeader confidence="0.993895" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999082142857143">
In this paper we present a chunk based
keyphrase extraction method for scientific
articles. Different from most previous sys-
tems, supervised machine learning algo-
rithms are not used in our system. Instead,
document structure information is used to
remove unimportant contents; Chunk ex-
traction and filtering is used to reduce the
quantity of candidates; Keywords are used
to filter the candidates before generating
final keyphrases. Our experimental results
on test data show that the method works
better than the baseline systems and is
comparable with other known algorithms.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999441304347826">
Keyphrases are sequences of words which cap-
ture the main topics discussed in a document.
Keyphrases are very useful in many natural lan-
guage processing (NLP) applications such as doc-
ument summarization, classification and cluster-
ing. But it is an expensive and time-consuming job
for users to tag keyphrases of a document. These
needs motivate methods for automatic keyphrase
extraction.
Most existing algorithms for keyphrase extrac-
tion treat this task as a supervised classifica-
tion task. The KEA algorithm (Gordon et al.,
1999) identifies candidate keyphrases using lex-
ical methods, calculates feature values for each
candidate, and uses a machine-learning algorithm
to predict which candidates are good keyphrases.
A domain-specific method (Frank et al., 1999)
was proposed based on the Naive Bayes learn-
ing scheme. Turney (Turney, 2000) treated a
document as a set of phrases, which the learn-
ing algorithm must learn to classify as positive or
negative examples of keyphrases. Turney (Tur-
ney, 2003) also presented enhancements to the
</bodyText>
<author confidence="0.9373">
Fang Li
</author>
<affiliation confidence="0.9636095">
Department of
Computer Science &amp; Engineering
Shanghai Jiao Tong University
Shanghai, China
</affiliation>
<email confidence="0.924624">
fli@sjtu.edu.cn
</email>
<bodyText confidence="0.999893272727273">
KEA keyphrase extraction algorithm that are de-
signed to increase the coherence of the extracted
keyphrases. Nguyen and yen Kan (Nguyen and
yen Kan, 2007) presented a keyphrase extraction
algorithm for scientific publications. They also in-
troduced two features that capture the positions
of phrases and salient morphological phenom-
ena. Wu and Agogino (Wu and Agogino, 2004)
proposed an automated keyphrase extraction al-
gorithm using a nondominated sorting multi-
objective genetic algorithm. Kumar and Srinathan
(Kumar and Srinathan, 2008) used n-gram filtra-
tion technique and weight of words for keyphrase
extraction from scientific articles.
For this evaluation task, Kim and Kan (Kim
and Kan, 2009) tackled two major issues in au-
tomatic keyphrase extraction using scientific ar-
ticles: candidate selection and feature engineer-
ing. They also re-examined the existing features
broadly used for the supervised approach.
Different from previous systems, our system
uses a chunk based method to extract keyphrases
from scientific articles. Domain-specific informa-
tion is used to find out useful parts in a document.
The chunk based method is used to extract candi-
dates of keyphrases in a document. Keywords of a
document are used to select keyphrases from can-
didates.
In the following, Section 2 will describe the ar-
chitecture of the system. Section 3 will introduce
functions and implementation of each part in the
system. Experiment results will be showed in Sec-
tion 4. The conclusion will be given in Section 5.
</bodyText>
<sectionHeader confidence="0.946828" genericHeader="method">
2 System Architecture
</sectionHeader>
<bodyText confidence="0.980916833333333">
Figure 1 shows the architecture of our system. The
system accepts a document as input (go through
arrows with solid lines), then does the preprocess-
ing job and identifies the structure of the docu-
ment. After these two steps, the formatted doc-
ument is sent to the candidate selection module
</bodyText>
<page confidence="0.957352">
158
</page>
<note confidence="0.760504">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 158–161,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999906">
Figure 1: System architecture
</figureCaption>
<bodyText confidence="0.999958736842105">
which first extracts chunks from the document,
then uses some rules to filter the extracted chunks.
After candidate selection, the system will choose
top fifteen (ordered by the position of the first oc-
currence in the original document) chunks from
the candidates as the keyphrases and output the
result (“Output1” in Figure 1) which is our sub-
mitted result. The candidates will also be sent
to keyphrase selection module which first extracts
keywords from the formatted document, then uses
keywords to choose keyphrases from the candi-
dates. Keywords extraction needs some training
data (go through arrows with dotted lines) which
also needs first two steps of our system. The result
of keywords selection module will be sent to “Out-
put2” as the final result after choosing top fifteen
chunks.
OpenNLP1 and KEA2 are used in chunk extrac-
tion and keywords extraction respectively.
</bodyText>
<sectionHeader confidence="0.988183" genericHeader="method">
3 System Description
</sectionHeader>
<subsectionHeader confidence="0.997009">
3.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999079">
In preprocessing, our system first deletes line
breaks between each broken lines to reconnect the
</bodyText>
<footnote confidence="0.999805">
1http://opennlp.sourceforge.net/
2http://nzdl.org/Kea/
</footnote>
<bodyText confidence="0.999947916666667">
broken sentences while line breaks after title and
section titles will be reserved. Title and section
titles are recognized through some heuristic rules
that title occupies first few lines of a document
and section titles are started with numbers except
abstract and reference. The system then deletes
brackets blocks in the documents to make sure
no keyphrases will be splitted by brackets blocks
(e.g., the brackets in “natural language processing
(NLP) applications” could be an obstacle to ex-
tracting phrase “natural language processing ap-
plications”).
</bodyText>
<subsectionHeader confidence="0.999941">
3.2 Document Structure Identification
</subsectionHeader>
<bodyText confidence="0.996900135135135">
Scientific articles often have similar structures
which start with title, abstract and end with con-
clusion, reference. The structure information is
used in our system to remove unimportant con-
tents in the input document. Based on the anal-
ysis of training documents, we assume that each
article can be divided into several parts: Title, Ab-
stract, Introduction, Related Work, Content, Ex-
periment, Conclusion, Acknowledgement and Ref-
erence, where Content often contains the descrip-
tion of theories, methods or algorithms.
To implement the identification of document
structure, our system first maps each section ti-
tle (including document title) to one of the parts
in the document structure with some rules derived
from the analysis of training documents. For each
part except Content, we have a pattern to map the
section titles. For example, the section title of Ab-
stract should be equal to “abstract”, the section ti-
tle of Introduction should contain “introduction”,
the section title of Related Work should contain
“related work” or “background”, the section title
of Experiment should contain “experiment”, “re-
sult” or “evaluation”, the section title of Conclu-
sion should contain “conclusion” or “discussion”.
Section titles which do not match any of the pat-
terns will be mapped to the Content part. After
mapping section titles, the content between two
section titles will be mapped to the same part as
the first section title (e.g., the content between the
section title “1. Introduction” and “2. Related
Work” will be mapped to the Introduction part).
In our keyphrase analysis, we observed that
most keyphrases appear in the first few parts of
a document, such as Title, Abstract, and Introduc-
tion. We also found that parts like Experiment,
Acknowledgement and Reference almost have no
</bodyText>
<figure confidence="0.998824681818182">
Output1
System
Candidate
Selection
Chunk Extraction
Chunk Selection
Chunk Filtering
Preprocessing
Formatted
Document(s)
Document
Structure
Identification
Candidates
Output2
Input
Document
Keyphrase Selection
Keywords
Extraction
Training
Data
</figure>
<page confidence="0.99217">
159
</page>
<bodyText confidence="0.9949">
keyphrases. Thus, Experiment, Acknowledgement
and Reference are removed by our system and
other parts are sorted in their original order and
outputted as formatted document(s) (see in Fig-
ure 1) for further process.
</bodyText>
<subsectionHeader confidence="0.999134">
3.3 Candidate Selection
</subsectionHeader>
<bodyText confidence="0.998896541666667">
The purpose of candidate selection is to find out
potential keyphrases in a document. Traditional
approaches just choose all the possible words se-
quences and filters them with part-of-speech tags.
This approach may result in huge amount of candi-
dates and lots of meaningless candidates for each
document.
Our system uses chunk based method to solve
these problems.
“A chunk is a textual unit of adjacent
word tokens which can be mutually
linked through unambiguously identi-
fied dependency chains with no recourse
to idiosyncratic lexical information.”3
Our approach significantly reduces the quantity
of candidates and keep the meanings of origi-
nal documents. For example, for an article ti-
tle, “Evaluating adaptive resource management
for distributed real-time embedded systems”, the
traditional method will extract lots of meaning-
less candidates like “adaptive resource” and “dis-
tributed real-time”, while our method just extract
“adaptive resource management” and “distributed
real-time embedded systems” as candidates.
</bodyText>
<subsectionHeader confidence="0.89902">
3.3.1 Chunk Extraction
</subsectionHeader>
<bodyText confidence="0.9999193">
The first step of candidate selection is chunk ex-
traction which extract chunks from a document.
Four tools in OpenNLP, SentenceDetector, Tok-
enizer, PosTagger and TreebankChunker, are uti-
lized in our system. The system first evokes Sen-
tenceDetector to split the formatted document into
sentences. Then uses Tokenizer and PosTagger to
label all the words with part-of-speech tag. At last,
TreebankChunker is used to extract chunks from
the document.
</bodyText>
<subsectionHeader confidence="0.925858">
3.3.2 Chunk filtering
</subsectionHeader>
<bodyText confidence="0.995143">
Not all the extracted chunks can be the candidates
of keyphrases. Our system uses some heuristic
rules to select candidates from extracted chunks.
</bodyText>
<footnote confidence="0.599438">
3http://www.ilc.cnr.it/sparkle/wp1-prefinal/node24.html
</footnote>
<bodyText confidence="0.99969575">
The types of rules range from statistic informa-
tion to syntactic structures. The rules that our sys-
tem uses are based on some traditional methods
for candidate filtering. They are:
</bodyText>
<listItem confidence="0.997173785714286">
1. Any chunks in candidates should have less
than 5 words.
2. Any single word chunks in candidates should
be found at least twice in a document.
3. Any chunks in candidates should be noun
phrases.
4. Any chunks in candidates must start with the
word with the part-of-speech tag (defined in
OpenNLP) NN, NNS, NNP, NNPS, JJ, JJR
or JJS and end with the word with the part-of-
speech tag NN, NNS, NNP or NNPS. Chunks
that do not match these rules will be removed.
Chunks that haven’t been removed will be the
candidate keyphrases of the document.
</listItem>
<subsectionHeader confidence="0.996516">
3.4 Keyphrase Selection
</subsectionHeader>
<bodyText confidence="0.9999835">
Our analysis shows that keywords are helpful to
extract keyphrases from a document. Thus, key-
words are used to select keyphrases from candi-
date chunks.
</bodyText>
<subsectionHeader confidence="0.877441">
3.4.1 Keywords Extraction
</subsectionHeader>
<bodyText confidence="0.999963666666667">
KEA is a keyphrase extraction tool, it can also be
used to extract keywords with some appropriate
parameters. We observed that most keyphrases
extracted by KEA only contain one word or two
words which describe the key meaning of the doc-
ument, even when the max length is set to 5 or
more. There are four parameters to be set, in or-
der to get best results, we set maximum length of
a keyphrase to 2, minimum length of a keyphrase
to 1, minimum occurrence of a phrase to 1 and
number of keyphrases to extract to 30. Then, the
output of the KEA system contains thirty keywords
per document.
As showed in Figure 1, KEA needs training data
(provided by the task owner). Our system uses for-
matted documents (generated by the first two steps
of our system) of training data as the input training
data to KEA.
</bodyText>
<subsectionHeader confidence="0.929988">
3.4.2 Chunk Selection
</subsectionHeader>
<bodyText confidence="0.995043333333333">
After extracting thirty keywords from each docu-
ment, our system uses these keywords to filter out
non-keyphrase chunks from the candidates. The
</bodyText>
<page confidence="0.991895">
160
</page>
<bodyText confidence="0.999943142857143">
system completes the task in two steps: 1) Re-
move candidates of a document that do not have
any keywords of the document extracted by KEA;
2) Choose the top fifteen (ordered by the position
of the first occurrence in the orginal document)
keyphrases as the answer of a document (“Out-
put2” in Figure 1).
</bodyText>
<sectionHeader confidence="0.997153" genericHeader="method">
4 Experiment Result
</sectionHeader>
<bodyText confidence="0.995618866666667">
Table 1 shows the F-score of two outputs of our
system and some baseline systems. The first three
methods are the baselines provided by the task
owner. TFIDF is an unsupervised method to rank
the candidates based on TFIDF scores. NB and
ME are supervised methods using Navie Bayes
and maximum entropy in WEKA4. KEA refers to
the KEA system with the parameters that can out-
put the best results. OP1 is our system with
the “Output1” as result and OP2 is our system
with the “Output2” as result (see Figure 1). In
second column, “R” means to use the reader-
assigned keyphrases set as gold-standard data and
“C” means to use both author-assigned and reader-
assigned keyphrases sets as answers.
</bodyText>
<table confidence="0.999834615384615">
Method by Top05 Top10 Top15
TFIDF R 10.44% 12.61% 12.87%
C 11.19% 14.35% 15.10%
NB R 9.86% 12.07% 12.65%
C 10.89% 14.03% 14.70%
ME R 9.86% 12.07% 12.65%
C 10.89% 14.03% 14.70%
KEA R 14.55% 17.24% 16.42%
C 14.45% 17.68% 17.74%
OP1 R 15.61% 17.60% 17.31%
C 15.36% 18.41% 18.61%
OP2 R 16.08% 18.42% 18.05%
C 17.91% 20.52% 20.36%
</table>
<tableCaption confidence="0.926896">
Table 1: The comparison of F-score of our system
with other systems.
</tableCaption>
<bodyText confidence="0.999593857142857">
From the table, we can see that, both two out-
puts of our system made an improvement over the
baseline systems and got better results than the
well known KEA system.
We submitted both results of OP1 and OP2 to
the evaluation task. Because of some misunder-
standing over the result upload system, only the
</bodyText>
<footnote confidence="0.861867">
4http://www.cs.waikato.ac.nz/ml/weka/
</footnote>
<bodyText confidence="0.990199">
result of OP1 (with bold style) was successfully
submitted.
</bodyText>
<sectionHeader confidence="0.997614" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999886111111111">
We proposed a chunk based method for keyphrase
extraction in this paper. In our system, document
structure information of scientific articles is used
to pick up significant contents, chunk based candi-
date selection is used to reduce the quantity of can-
didates and reserve their original meanings, key-
words are used to select keyphrases from a docu-
ment. All these factors contribute to the result of
our system.
</bodyText>
<sectionHeader confidence="0.999424" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999597868421052">
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-manning. 1999.
Domain-specific keyphrase extraction. pages 668–
673. Morgan Kaufmann Publishers.
Ian Witten Gordon, Gordon W. Paynter, Eibe Frank,
Carl Gutwin, and Craig G. Nevill-manning. 1999.
Kea: Practical automatic keyphrase extraction. In
Proceedings of Digital Libraries 99 (DL’99, pages
254–255. ACM Press.
Su Nam Kim and Min-Yen Kan. 2009. Re-examining
automatic keyphrase extraction approaches in scien-
tific articles. In Proceedings of the Workshop on
Multiword Expressions: Identification, Interpreta-
tion, Disambiguation and Applications, pages 9–16,
Singapore, August. Association for Computational
Linguistics.
Niraj Kumar and Kannan Srinathan. 2008. Automatic
keyphrase extraction from scientific documents us-
ing n-gram filtration technique. In DocEng ’08:
Proceeding of the eighth ACM symposium on Doc-
ument engineering, pages 199–208, New York, NY,
USA. ACM.
Thuy Dung Nguyen and Min yen Kan. 2007.
Keyphrase extraction in scientific publications. In
In Proc. of International Conference on Asian Digi-
tal Libraries (ICADL 07, pages 317–326. Springer.
Peter Turney. 2000. Learning algorithms for keyphrase
extraction. Information Retrieval, 2:303–336.
Peter Turney. 2003. Coherent keyphrase extraction via
web mining. In In Proceedings ofIJCAI, pages 434–
439.
Jia-Long Wu and Alice M. Agogino. 2004. Au-
tomating keyphrase extraction with multi-objective
genetic algorithms. In HICSS ’04: Proceedings of
the Proceedings of the 37th Annual Hawaii Interna-
tional Conference on System Sciences (HICSS’04) -
Track 4, page 40104.3, Washington, DC, USA. IEEE
Computer Society.
</reference>
<page confidence="0.998212">
161
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.946248">
<title confidence="0.999762">SJTULTLAB: Chunk Based Method for Keyphrase Extraction</title>
<author confidence="0.999738">Letian Wang</author>
<affiliation confidence="0.988011333333333">Department of Computer Science &amp; Engineering Shanghai Jiao Tong University</affiliation>
<address confidence="0.998113">Shanghai, China</address>
<email confidence="0.990551">koh@sjtu.edu.cn</email>
<abstract confidence="0.9995322">In this paper we present a chunk based keyphrase extraction method for scientific articles. Different from most previous systems, supervised machine learning algorithms are not used in our system. Instead, document structure information is used to remove unimportant contents; Chunk extraction and filtering is used to reduce the quantity of candidates; Keywords are used to filter the candidates before generating final keyphrases. Our experimental results on test data show that the method works better than the baseline systems and is comparable with other known algorithms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eibe Frank</author>
<author>Gordon W Paynter</author>
<author>Ian H Witten</author>
<author>Carl Gutwin</author>
<author>Craig G Nevill-manning</author>
</authors>
<title>Domain-specific keyphrase extraction.</title>
<date>1999</date>
<pages>668--673</pages>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="1540" citStr="Frank et al., 1999" startWordPosition="224" endWordPosition="227">essing (NLP) applications such as document summarization, classification and clustering. But it is an expensive and time-consuming job for users to tag keyphrases of a document. These needs motivate methods for automatic keyphrase extraction. Most existing algorithms for keyphrase extraction treat this task as a supervised classification task. The KEA algorithm (Gordon et al., 1999) identifies candidate keyphrases using lexical methods, calculates feature values for each candidate, and uses a machine-learning algorithm to predict which candidates are good keyphrases. A domain-specific method (Frank et al., 1999) was proposed based on the Naive Bayes learning scheme. Turney (Turney, 2000) treated a document as a set of phrases, which the learning algorithm must learn to classify as positive or negative examples of keyphrases. Turney (Turney, 2003) also presented enhancements to the Fang Li Department of Computer Science &amp; Engineering Shanghai Jiao Tong University Shanghai, China fli@sjtu.edu.cn KEA keyphrase extraction algorithm that are designed to increase the coherence of the extracted keyphrases. Nguyen and yen Kan (Nguyen and yen Kan, 2007) presented a keyphrase extraction algorithm for scientifi</context>
</contexts>
<marker>Frank, Paynter, Witten, Gutwin, Nevill-manning, 1999</marker>
<rawString>Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl Gutwin, and Craig G. Nevill-manning. 1999. Domain-specific keyphrase extraction. pages 668– 673. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Witten Gordon</author>
<author>Gordon W Paynter</author>
<author>Eibe Frank</author>
<author>Carl Gutwin</author>
<author>Craig G Nevill-manning</author>
</authors>
<title>Kea: Practical automatic keyphrase extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of Digital Libraries 99 (DL’99,</booktitle>
<pages>254--255</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="1306" citStr="Gordon et al., 1999" startWordPosition="192" endWordPosition="195">r than the baseline systems and is comparable with other known algorithms. 1 Introduction Keyphrases are sequences of words which capture the main topics discussed in a document. Keyphrases are very useful in many natural language processing (NLP) applications such as document summarization, classification and clustering. But it is an expensive and time-consuming job for users to tag keyphrases of a document. These needs motivate methods for automatic keyphrase extraction. Most existing algorithms for keyphrase extraction treat this task as a supervised classification task. The KEA algorithm (Gordon et al., 1999) identifies candidate keyphrases using lexical methods, calculates feature values for each candidate, and uses a machine-learning algorithm to predict which candidates are good keyphrases. A domain-specific method (Frank et al., 1999) was proposed based on the Naive Bayes learning scheme. Turney (Turney, 2000) treated a document as a set of phrases, which the learning algorithm must learn to classify as positive or negative examples of keyphrases. Turney (Turney, 2003) also presented enhancements to the Fang Li Department of Computer Science &amp; Engineering Shanghai Jiao Tong University Shanghai</context>
</contexts>
<marker>Gordon, Paynter, Frank, Gutwin, Nevill-manning, 1999</marker>
<rawString>Ian Witten Gordon, Gordon W. Paynter, Eibe Frank, Carl Gutwin, and Craig G. Nevill-manning. 1999. Kea: Practical automatic keyphrase extraction. In Proceedings of Digital Libraries 99 (DL’99, pages 254–255. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Min-Yen Kan</author>
</authors>
<title>Re-examining automatic keyphrase extraction approaches in scientific articles.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Multiword Expressions: Identification, Interpretation, Disambiguation and Applications,</booktitle>
<pages>9--16</pages>
<institution>Singapore, August. Association for Computational Linguistics.</institution>
<contexts>
<context position="2628" citStr="Kim and Kan, 2009" startWordPosition="390" endWordPosition="393">f the extracted keyphrases. Nguyen and yen Kan (Nguyen and yen Kan, 2007) presented a keyphrase extraction algorithm for scientific publications. They also introduced two features that capture the positions of phrases and salient morphological phenomena. Wu and Agogino (Wu and Agogino, 2004) proposed an automated keyphrase extraction algorithm using a nondominated sorting multiobjective genetic algorithm. Kumar and Srinathan (Kumar and Srinathan, 2008) used n-gram filtration technique and weight of words for keyphrase extraction from scientific articles. For this evaluation task, Kim and Kan (Kim and Kan, 2009) tackled two major issues in automatic keyphrase extraction using scientific articles: candidate selection and feature engineering. They also re-examined the existing features broadly used for the supervised approach. Different from previous systems, our system uses a chunk based method to extract keyphrases from scientific articles. Domain-specific information is used to find out useful parts in a document. The chunk based method is used to extract candidates of keyphrases in a document. Keywords of a document are used to select keyphrases from candidates. In the following, Section 2 will des</context>
</contexts>
<marker>Kim, Kan, 2009</marker>
<rawString>Su Nam Kim and Min-Yen Kan. 2009. Re-examining automatic keyphrase extraction approaches in scientific articles. In Proceedings of the Workshop on Multiword Expressions: Identification, Interpretation, Disambiguation and Applications, pages 9–16, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niraj Kumar</author>
<author>Kannan Srinathan</author>
</authors>
<title>Automatic keyphrase extraction from scientific documents using n-gram filtration technique.</title>
<date>2008</date>
<booktitle>In DocEng ’08: Proceeding of the eighth ACM symposium on Document engineering,</booktitle>
<pages>199--208</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2466" citStr="Kumar and Srinathan, 2008" startWordPosition="364" endWordPosition="367">puter Science &amp; Engineering Shanghai Jiao Tong University Shanghai, China fli@sjtu.edu.cn KEA keyphrase extraction algorithm that are designed to increase the coherence of the extracted keyphrases. Nguyen and yen Kan (Nguyen and yen Kan, 2007) presented a keyphrase extraction algorithm for scientific publications. They also introduced two features that capture the positions of phrases and salient morphological phenomena. Wu and Agogino (Wu and Agogino, 2004) proposed an automated keyphrase extraction algorithm using a nondominated sorting multiobjective genetic algorithm. Kumar and Srinathan (Kumar and Srinathan, 2008) used n-gram filtration technique and weight of words for keyphrase extraction from scientific articles. For this evaluation task, Kim and Kan (Kim and Kan, 2009) tackled two major issues in automatic keyphrase extraction using scientific articles: candidate selection and feature engineering. They also re-examined the existing features broadly used for the supervised approach. Different from previous systems, our system uses a chunk based method to extract keyphrases from scientific articles. Domain-specific information is used to find out useful parts in a document. The chunk based method is </context>
</contexts>
<marker>Kumar, Srinathan, 2008</marker>
<rawString>Niraj Kumar and Kannan Srinathan. 2008. Automatic keyphrase extraction from scientific documents using n-gram filtration technique. In DocEng ’08: Proceeding of the eighth ACM symposium on Document engineering, pages 199–208, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thuy Dung Nguyen</author>
<author>Min yen Kan</author>
</authors>
<title>Keyphrase extraction in scientific publications. In</title>
<date>2007</date>
<booktitle>In Proc. of International Conference on Asian Digital Libraries (ICADL 07,</booktitle>
<pages>317--326</pages>
<publisher>Springer.</publisher>
<marker>Nguyen, Kan, 2007</marker>
<rawString>Thuy Dung Nguyen and Min yen Kan. 2007. Keyphrase extraction in scientific publications. In In Proc. of International Conference on Asian Digital Libraries (ICADL 07, pages 317–326. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Learning algorithms for keyphrase extraction. Information Retrieval,</title>
<date>2000</date>
<pages>2--303</pages>
<contexts>
<context position="1617" citStr="Turney, 2000" startWordPosition="239" endWordPosition="240">ring. But it is an expensive and time-consuming job for users to tag keyphrases of a document. These needs motivate methods for automatic keyphrase extraction. Most existing algorithms for keyphrase extraction treat this task as a supervised classification task. The KEA algorithm (Gordon et al., 1999) identifies candidate keyphrases using lexical methods, calculates feature values for each candidate, and uses a machine-learning algorithm to predict which candidates are good keyphrases. A domain-specific method (Frank et al., 1999) was proposed based on the Naive Bayes learning scheme. Turney (Turney, 2000) treated a document as a set of phrases, which the learning algorithm must learn to classify as positive or negative examples of keyphrases. Turney (Turney, 2003) also presented enhancements to the Fang Li Department of Computer Science &amp; Engineering Shanghai Jiao Tong University Shanghai, China fli@sjtu.edu.cn KEA keyphrase extraction algorithm that are designed to increase the coherence of the extracted keyphrases. Nguyen and yen Kan (Nguyen and yen Kan, 2007) presented a keyphrase extraction algorithm for scientific publications. They also introduced two features that capture the positions </context>
</contexts>
<marker>Turney, 2000</marker>
<rawString>Peter Turney. 2000. Learning algorithms for keyphrase extraction. Information Retrieval, 2:303–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Coherent keyphrase extraction via web mining. In</title>
<date>2003</date>
<booktitle>In Proceedings ofIJCAI,</booktitle>
<pages>434--439</pages>
<contexts>
<context position="1779" citStr="Turney, 2003" startWordPosition="266" endWordPosition="268">st existing algorithms for keyphrase extraction treat this task as a supervised classification task. The KEA algorithm (Gordon et al., 1999) identifies candidate keyphrases using lexical methods, calculates feature values for each candidate, and uses a machine-learning algorithm to predict which candidates are good keyphrases. A domain-specific method (Frank et al., 1999) was proposed based on the Naive Bayes learning scheme. Turney (Turney, 2000) treated a document as a set of phrases, which the learning algorithm must learn to classify as positive or negative examples of keyphrases. Turney (Turney, 2003) also presented enhancements to the Fang Li Department of Computer Science &amp; Engineering Shanghai Jiao Tong University Shanghai, China fli@sjtu.edu.cn KEA keyphrase extraction algorithm that are designed to increase the coherence of the extracted keyphrases. Nguyen and yen Kan (Nguyen and yen Kan, 2007) presented a keyphrase extraction algorithm for scientific publications. They also introduced two features that capture the positions of phrases and salient morphological phenomena. Wu and Agogino (Wu and Agogino, 2004) proposed an automated keyphrase extraction algorithm using a nondominated so</context>
</contexts>
<marker>Turney, 2003</marker>
<rawString>Peter Turney. 2003. Coherent keyphrase extraction via web mining. In In Proceedings ofIJCAI, pages 434– 439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia-Long Wu</author>
<author>Alice M Agogino</author>
</authors>
<title>Automating keyphrase extraction with multi-objective genetic algorithms.</title>
<date>2004</date>
<booktitle>In HICSS ’04: Proceedings of the Proceedings of the 37th Annual Hawaii International Conference on System Sciences (HICSS’04) -Track 4,</booktitle>
<pages>40104--3</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="2302" citStr="Wu and Agogino, 2004" startWordPosition="342" endWordPosition="345">rithm must learn to classify as positive or negative examples of keyphrases. Turney (Turney, 2003) also presented enhancements to the Fang Li Department of Computer Science &amp; Engineering Shanghai Jiao Tong University Shanghai, China fli@sjtu.edu.cn KEA keyphrase extraction algorithm that are designed to increase the coherence of the extracted keyphrases. Nguyen and yen Kan (Nguyen and yen Kan, 2007) presented a keyphrase extraction algorithm for scientific publications. They also introduced two features that capture the positions of phrases and salient morphological phenomena. Wu and Agogino (Wu and Agogino, 2004) proposed an automated keyphrase extraction algorithm using a nondominated sorting multiobjective genetic algorithm. Kumar and Srinathan (Kumar and Srinathan, 2008) used n-gram filtration technique and weight of words for keyphrase extraction from scientific articles. For this evaluation task, Kim and Kan (Kim and Kan, 2009) tackled two major issues in automatic keyphrase extraction using scientific articles: candidate selection and feature engineering. They also re-examined the existing features broadly used for the supervised approach. Different from previous systems, our system uses a chunk</context>
</contexts>
<marker>Wu, Agogino, 2004</marker>
<rawString>Jia-Long Wu and Alice M. Agogino. 2004. Automating keyphrase extraction with multi-objective genetic algorithms. In HICSS ’04: Proceedings of the Proceedings of the 37th Annual Hawaii International Conference on System Sciences (HICSS’04) -Track 4, page 40104.3, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>