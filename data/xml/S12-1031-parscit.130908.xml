<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.085158">
<title confidence="0.950935">
The Effects of Semantic Annotations on Precision Parse Ranking
</title>
<author confidence="0.963953">
Andrew MacKinlay♦♥, Rebecca Dridan♠∗, Diana McCarthy♣† and Timothy Baldwin♦♥
</author>
<affiliation confidence="0.65658025">
♦ Dept. of Computing and Information Systems, University of Melbourne, Australia
♥ NICTA Victoria Research Laboratories, University of Melbourne, Australia
♠ Department of Informatics, University of Oslo, Norway
♣ Computational Linguistics and Phonetics, Saarland University, Germany
</affiliation>
<email confidence="0.9843215">
amack@csse.unimelb.edu.au,rdridan@ifi.uio.no,
diana@dianamccarthy.co.uk,tb@ldwin.net
</email>
<sectionHeader confidence="0.993583" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999861846153846">
We investigate the effects of adding semantic
annotations including word sense hypernyms
to the source text for use as an extra source
of information in HPSG parse ranking for the
English Resource Grammar. The semantic an-
notations are coarse semantic categories or en-
tries from a distributional thesaurus, assigned
either heuristically or by a pre-trained tagger.
We test this using two test corpora in different
domains with various sources of training data.
The best reduces error rate in dependency F-
score by 1% on average, while some methods
produce substantial decreases in performance.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.964046136363636">
Most start-of-the-art natural language parsers (Char-
niak, 2000; Clark and Curran, 2004; Collins, 1997)
use lexicalised features for parse ranking. These are
important to achieve optimal parsing accuracy, and
yet these are also the features which by their nature
suffer from data-sparseness problems in the training
data. In the absence of reliable fine-grained statis-
tics for a given token, various strategies are possible.
There will often be statistics available for coarser
categories, such as the POS of the particular token.
However, it is possible that these coarser represen-
tations discard too much, missing out information
which could be valuable to the parse ranking. An
intermediate level of representation could provide
valuable additional information here. For example,
*This research was conducted while the second author was
a postdoctoral researcher within NICTA VRL.
†The third author is a visiting scholar on the Erasmus
Mundus Masters Program in ‘Language and Communication
Technologies’ (LCT, 2007–0060).
assume we wish to correctly attach the prepositional
phrases in the following examples:
</bodyText>
<listItem confidence="0.8038895">
(1) I saw a tree with my telescope
(2) I saw a tree with no leaves
</listItem>
<bodyText confidence="0.999979857142857">
The most obvious interpretation in each case has the
prepositional phrase headed by with attaching in dif-
ferent places: to the verb phrase in the first example,
and to the noun tree in the second. Such distinctions
are difficult for a parser to make when the training
data is sparse, but imagine we had seen examples
such as the following in the training corpus:
</bodyText>
<listItem confidence="0.997263">
(3) Kim saw a eucalypt with his binoculars
(4) Sandy observed a willow with plentiful foliage
</listItem>
<bodyText confidence="0.999947333333333">
There are few lexical items in common, but in each
case the prepositional phrase attachment follows the
same pattern: in (3) it attaches to the verb, and in
(4) to the noun. A conventional lexicalised parser
would have no knowledge of the semantic similarity
between eucalypt and tree, willow and tree, binoc-
ulars and telescope, or foliage and leaves, so would
not be able to make any conclusions about the earlier
examples on the basis of this training data. However
if the parse ranker has also been supplied with in-
formation about synonyms or hypernyms of the lex-
emes in the training data, it could possibly have gen-
eralised, to learn that PPs containing nouns related
to seeing instruments often modify verbs relating to
observation (in preference to nouns denoting inani-
mate objects), while plant flora can often be modi-
fied by PPs relating to appendages of plants such as
leaves. This is not necessarily applicable only to PP
attachment, but may help in a range of other syntac-
tic phenomena, such as distinguishing between com-
plements and modifiers of verbs.
</bodyText>
<page confidence="0.969226">
228
</page>
<note confidence="0.9727405">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 228–236,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999727454545455">
The synonyms or hypernyms could take the form
of any grouping which relates word forms with se-
mantic or syntactic commonality – such as a label
from the WordNet (Miller, 1995) hierarchy, a sub-
categorisation frame (for verbs) or closely related
terms from a distributional thesaurus (Lin, 1998).
We present work here on using various levels
of semantic generalisation as an attempt to im-
prove parse selection accuracy with the English Re-
source Grammar (ERG: Flickinger (2000)), a preci-
sion HPSG-based grammar of English.
</bodyText>
<sectionHeader confidence="0.999923" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.992129">
2.1 Parse Selection for Precision Grammars
</subsectionHeader>
<bodyText confidence="0.999940033333333">
The focus of this work is on parsing using hand-
crafted precision HPSG-based grammars, and in
particular the ERG. While these grammars are care-
fully crafted to avoid overgeneration, the ambiguity
of natural languages means that there will unavoid-
ably be multiple candidate parses licensed by the
grammar for any non-trivial sentence. For the ERG,
the number of parses postulated for a given sentence
can be anywhere from zero to tens of thousands. It
is the job of the parse selection model to select the
best parse from all of these candidates as accurately
as possible, for some definition of ‘best’, as we dis-
cuss in Section 3.2.
Parse selection is usually performed by training
discriminative parse selection models, which ‘dis-
criminate’ between the set of all candidate parses.
A widely-used method to achieve this is outlined
in Velldal (2007). We feed both correct and incor-
rect parses licensed by the grammar to the TADM
toolkit (Malouf, 2002), and learn a maximum en-
tropy model. This method is used by Zhang et al.
(2007) and MacKinlay et al. (2011) inter alia. One
important implementation detail is that rather than
exhaustively ranking all candidates out of possibly
many thousands of trees, Zhang et al. (2007) showed
that it was possible to use ‘selective unpacking’,
which means that the exhaustive parse forest can be
represented compactly as a ‘packed forest’, and the
top-ranked trees can be successively reconstructed,
enabling faster parsing using less memory.
</bodyText>
<subsectionHeader confidence="0.998977">
2.2 Semantic Generalisation for parse ranking
</subsectionHeader>
<bodyText confidence="0.9995223">
Above, we outlined a number of reasons why
semantic generalisation of lexemes could enable
parsers to make more efficient use of training data,
and indeed, there has been some prior work investi-
gating this possibility. Agirre et al. (2008) applied
two state-of-the-art treebank parsers to the sense-
tagged subset of the Brown corpus version of the
Penn Treebank (Marcus et al., 1993), and added
sense annotation to the training data to evaluate their
impact on parse selection and specifically on PP-
attachment. The annotations they used were oracle
sense annotations, automatic sense recognition and
the first sense heuristic, and it was this last method
which was the best performer in general. The sense
annotations were either the WordNet synset ID or
the coarse semantic file, which we explain in more
detail below, and replaced the original tokens in
the training data. The largest improvement in pars-
ing F-score was a 6.9% reduction in error rate for
the Bikel parser (Bikel, 2002), boosting the F-score
from 0.841 to 0.852, using the noun supersense only.
More recently, Agirre et al. (2011) largely repro-
duced these results with a dependency parser.
Fujita et al. (2007) add sense information to im-
prove parse ranking with JaCy (Siegel and Bender,
2002), an HPSG-based grammar which uses simi-
lar machinery to the ERG. They use baseline syn-
tactic features, and also add semantic features based
on dependency triples extracted from the semantic
representations of the sentence trees output by the
parser. The dataset they use has human-assigned
sense tags from a Japanese lexical hierarchy, which
they use as a source of annotations. The dependency
triples are modified in each feature set by replacing
elements of the semantic triples with corresponding
senses or hypernyms. In the best-performing con-
figuration, they use both syntactic and semantic fea-
tures with multiple levels of the the semantic hier-
archy from combined feature sets. They achieve a
5.6% improvement in exact match parsing accuracy.
</bodyText>
<sectionHeader confidence="0.998612" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.9995105">
We performed experiments in HPSG parse rank-
ing using the ERG, evaluating the impact on parse
selection of semantic annotations such as coarse
sense labels or synonyms from a distributional the-
</bodyText>
<page confidence="0.998107">
229
</page>
<table confidence="0.999368285714286">
WESCIENCE LOGON
Total Sentences 9632 9410
Parseable Sentences 9249 8799
Validated Sentences 7631 8550
Train/Test Sentences 6149/1482 6823/1727
Tokens/sentence 15.0 13.6
Training Tokens 92.5k 92.8k
</table>
<tableCaption confidence="0.97716125">
Table 1: Corpora used in our experiments, with total sen-
tences, how many of those can be parsed, how many of
the parseable sentences have a single gold parse (and are
used in these experiments), and average sentence length
</tableCaption>
<bodyText confidence="0.999594916666667">
saurus. Our work here differs from the aforemen-
tioned work of Fujita et al. (2007) in a number of
ways. Firstly, we use purely syntactic parse selec-
tion features based on the derivation tree of the sen-
tence (see Section 3.4.3), rather than ranking using
dependency triples, meaning that our method is in
principle able to be integrated into a parser more eas-
ily, where the final set of dependencies would not be
known in advance. Secondly, we do not use human-
created sense annotations, instead relying on heuris-
tics or trained sense-taggers, which is closer to the
reality of real-world parsing tasks.
</bodyText>
<subsectionHeader confidence="0.996205">
3.1 Corpora
</subsectionHeader>
<bodyText confidence="0.99995424137931">
Following MacKinlay et al. (2011), we use two pri-
mary training corpora. First, we use the LOGON
corpus (Oepen et al., 2004), a collection of En-
glish translations of Norwegian hiking texts. The
LOGON corpus contains 8550 sentences with ex-
actly one gold parse, which we partitioned ran-
domly by sentence into 10 approximately equal sec-
tions, reserving two sections as test data, and us-
ing the remainder as our training corpus. These
sentences were randomly divided into training and
development data. Secondly, we use the We-
Science (Ytrestøl et al., 2009) corpus, a collection
of Wikipedia articles related to computational lin-
guistics. The corpus contains 11558 sentences, from
which we randomly chose 9632, preserving the re-
mainder for future work. This left 7631 sentences
with a single gold tree, which we divided into a
training set and a development set in the same way.
The corpora are summarised in Table 1.
With these corpora, we are able to investigate in-
domain and cross-domain effects, by testing on a
different corpus to the training corpus, so we can
examine whether sense-tagging alleviates the cross-
domain performance penalty noted in MacKinlay et
al. (2011). We can also use a subset of each training
corpus to simulate the common situation of sparse
training data, so we can investigate whether sense-
tagging enables the learner to make better use of a
limited quantity of training data.
</bodyText>
<subsectionHeader confidence="0.99744">
3.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.9999882">
Our primary evaluation metric is Elementary De-
pendency Match (Dridan and Oepen, 2011). This
converts the semantic output of the ERG into a set
of dependency-like triples, and scores these triples
using precision, recall and F-score as is conven-
tional for other dependency evaluation. Following
MacKinlay et al. (2011), we use the EDMNA mode
of evaluation, which provides a good level of com-
parability while still reflecting most the semantically
salient information from the grammar.
Other work on the ERG and related grammars has
tended to focus on exact tree match, but the granu-
lar EDM metric is a better fit for our needs here –
among other reasons, it is more sensitive in terms
of error rate reduction to changes in parse selection
models (MacKinlay et al., 2011). Additionally, it is
desirable to be able to choose between two different
parses which do not match the gold standard exactly
but when one of the parses is a closer match than the
other; this is not possible with exact match accuracy.
</bodyText>
<subsectionHeader confidence="0.993225">
3.3 Reranking for parse selection
</subsectionHeader>
<bodyText confidence="0.9997935625">
The features we are adding to the parse selection
procedure could all in principle be applied by the
parser during the selective unpacking stage, since
they all depend on information which can be pre-
computed. However, we wish to avoid the need for
multiple expensive parsing runs, and more impor-
tantly the need to modify the relatively complex in-
ternals of the parse ranking machinery in the PET
parser (Callmeier, 2000). So instead of performing
the parse ranking in conjunction with parsing, as is
the usual practice, we use a pre-parsed forest of the
top-500 trees for each corpus, and rerank the forest
afterwards for each configuration shown.
The pre-parsed forests use the same models which
were used in treebanking. Using reranking means
that the set of candidate trees is held constant, which
</bodyText>
<page confidence="0.962229">
230
</page>
<bodyText confidence="0.999850714285714">
means that parse selection models never get the
chance to introduce a new tree which was not in
the original parse forest from which the gold tree
was annotated, which may provide a very small per-
formance boost (although when the parse selection
models are similar as is the case for most of the mod-
els here, this effect is likely to be very small).
</bodyText>
<subsectionHeader confidence="0.995181">
3.4 Word Sense Annotations
3.4.1 Using the WordNet Hierarchy
</subsectionHeader>
<bodyText confidence="0.998524255813954">
Most experiments we report on here make some
use of the WordNet sense inventory. Obviously we
need to determine the best sense and corresponding
WordNet synset for a given token. We return to this
in Section 3.4.2, but for now assume that the sense
disambiguation is done.
As we are concerned primarily with making
commonalities between lemmas with different base
forms apparent to the parse selection model, the fine-
grained synset ID will do relatively little to provide
a coarser identifier for the token – indeed, if two
tokens with identical forms were assigned different
synset IDs, we would be obscuring the similarity.1
We can of course make use of the WordNet hier-
archy, and use hypernyms from the hierarchy to tag
each candidate token, but there are a large number
of ways this can be achieved, particularly when it
is possibly to assign multiple labels per token as is
the case here (which we discuss in Section 3.4.3).
We apply two relatively simple strategies. We noted
in Section 2.2 that Agirre et al. (2008) found that
the semantic file was useful. This is the coarse lex-
icographic category label, elsewhere denoted super-
sense (Ciaramita and Altun, 2006), which is the
terminology we use. Nouns are divided into 26
coarse categories such as ‘animal’, ‘quantity’ or
‘phenomenon’, and verbs into 15 categories such as
‘perception’ or ‘consumption’. In some configura-
tions, denoted SS, we tag each open-class token with
one of the supersense labels.
Another configuration attempts to avoid making
assumptions about which level of the hierarchy will
be most useful for parse disambiguation, instead
leaving it the MaxEnt parse ranker to pick those la-
bels from the hierarchy which are most useful. Each
&apos;This could be useful for verbs since senses interact strongly
subcategorisation frames, but that is not our focus here.
open class token is labelled with multiple synsets,
starting with the assigned leaf synset and travelling
as high as possible up the hierarchy, with no distinc-
tion made between the different levels in the hier-
archy. Configurations using this are designated HP,
for ‘hypernym path’.
</bodyText>
<subsectionHeader confidence="0.957203">
3.4.2 Disambiguating senses
</subsectionHeader>
<bodyText confidence="0.999989666666666">
We return now to the question of determination
of the synset for a given token. One frequently-
used and robust strategy is to lemmatise and POS-
tag each token, and assign it the first-listed sense
from WordNet (which may or may not be based on
actual frequency counts). We POS-tag using TnT
(Brants, 2000) and lemmatise using WordNet’s na-
tive lemmatiser. This yields a leaf-level synset, mak-
ing it suitable as a source of annotations for both SS
and HP. We denote this ‘WNF’ for ‘WordNet First’
(shown in parentheses after SS or HP).
Secondly, to evaluate whether a more informed
approach to sense-tagging helps beyond the naive
WNF method, in the ‘SST’ method, we use the out-
puts of SuperSense Tagger (Ciaramita and Altun,
2006), which is optimised for assigning the super-
senses described above, and can outperform a WNF-
style baseline on at least some datasets. Since this
only gives us coarse supersense labels, it can only
provide SS annotations, as we do not get the leaf
synsets needed for HP. The input we feed in is POS-
tagged with TnT as above, for comparability with
the WNF method, and to ensure that it is compati-
ble with the configuration in which the corpora were
parsed – specifically, the unknown-word handling
uses a version of the sentences tagged with TnT. We
ignore multi-token named entity outputs from Su-
perSense Tagger, as these would introduce a con-
founding factor in our experiments and also reduce
comparability of the results with the WNF method.
</bodyText>
<subsectionHeader confidence="0.937682">
3.4.3 A distributional thesaurus method
</subsectionHeader>
<bodyText confidence="0.99972875">
A final configuration attempts to avoid the need
for curated resources such as WordNet, instead us-
ing an automatically-constructed distributional the-
saurus (Lin, 1998). We use the thesaurus from
McCarthy et al. (2004), constructed along these
lines using the grammatical relations from RASP
(Briscoe and Carroll, 2002) applied to 90 millions
words of text from the British National Corpus.
</bodyText>
<page confidence="0.979978">
231
</page>
<figure confidence="0.982482857142857">
root_frag
np_frg_c
hdn_bnp_c
aj-hdn_norm_c
legal_a1 n_pl_olr
&amp;quot;legal&amp;quot; issue_n1
&amp;quot;issues&amp;quot;
</figure>
<figureCaption confidence="0.997683">
Figure 1: ERG derivation tree for the phrase Legal issues
</figureCaption>
<figure confidence="0.9836765">
[n_-_c_le &amp;quot;issues&amp;quot;]
[n_pl_olr n_-_c_le &amp;quot;issues&amp;quot;]
[aj-hdn_norm_c n_pl_olr n_-_c_le &amp;quot;issues&amp;quot;]
(a) Original features
[n_-_c_le noun.cognition]
[n_pl_olr n_-_c_le noun.cognition]
[aj-hdn_norm_c n_pl_olr n_-_c_le noun.cognition]
(b) Additional features in leaf mode, which augment the original
features
[noun.cognition &amp;quot;issues&amp;quot;]
[n_pl_olr noun.cognition &amp;quot;issues&amp;quot;]
[aj-hdn_norm_c n_pl_olr noun.cognition &amp;quot;issues&amp;quot;]
(c) Additional features in leaf-parent (‘P’) mode, which augment
the original features
</figure>
<figureCaption confidence="0.999733666666667">
Figure 2: Examples of features extracted from for
&amp;quot;issues&amp;quot; node in Figure 1 with grandparenting level
of 2 or less
</figureCaption>
<bodyText confidence="0.99532373015873">
To apply the mapping, we POS-tag the text with
TnT as usual, and for each noun, verb and adjec-
tive we lemmatise the token (with WordNet again,
falling back to the surface form if this fails), and
look up the corresponding entry in the thesaurus. If
there is a match, we select the top five most simi-
lar entries (or fewer if there are less than five), and
use these new entries to create additional features,
as well as adding a feature for the lemma itself in all
cases. This method is denoted LDT for ‘Lin Distri-
butional Thesaurus’. We note that many other meth-
ods could be used to select these, such as different
numbers of synonyms, or dynamically changing the
number of synonyms based on a threshold against
the top similarity score, but this is not something we
evaluate in this preliminary investigation.
Adding Word Sense to Parse Selection Models
We noted above that parse selection using the
methodology established by Velldal (2007) uses
human-annotated incorrect and correct derivation
trees to train a maximum entropy parse selection
model. More specifically, the model is trained using
features extracted from the candidate HPSG deriva-
tion trees, using the labels of each node (which are
the rule names from the grammar) and those of a
limited number of ancestor nodes.
As an example, we examine the noun phrase Le-
gal issues from the WESCIENCE corpus, for which
the correct ERG derivation tree is shown in Figure 1.
Features are created by examining each node in the
tree and at least its parent, with the feature name set
to the concatenation of the node labels. We also gen-
erally make used of grandparenting features, where
we examine earlier ancestors in the derivation tree.
A grandparenting level of one means we would also
use the label of the grandparent (i.e. the parent’s par-
ent) of the node, a level of two means we would add
in the great-grandparent label, and so on. Our exper-
iments here use a maximum grandparenting level of
three. There is also an additional transformation ap-
plied to the tree – the immediate parent of each leaf
is, which is usually a lexeme, is replaced with the
corresponding lexical type, which is a broader par-
ent category from the type hierarchy of the grammar,
although the details of this are not relevant here.
For the node labelled &amp;quot;issues&amp;quot; in Figure 1 with
grandparenting levels from zero to two, we would
extract the features as shown in Figure 2(a) (where
the parent node issue_n1 has already been re-
placed with its lexical type n_-c_le).
In this work here, we create variants of these fea-
tures. A preprocessing script runs over the training
or test data, and for each sentence lists variants of
each token using standoff markup indexed by char-
acter span, which are created from the set of addi-
tional semantic tags assigned to each token by the
word sense configuration (from those described in
Section 3.4) which is currently in use. These sets of
semantic tags for a given word could be a single su-
persense tag, as in SS, a set of synset IDs as in HP
or a set of replacement lemmas in LDT. In all cases,
the set of semantic tags could also be empty – if ei-
ther the word has a part of speech which we are not
</bodyText>
<page confidence="0.988469">
232
</page>
<table confidence="0.9997409">
Test Train P/ R/ F SS (WNF) OF SSP(WNF) OF
P/ R/ F P/ R/ F
LOG WESC (23k) 85.02/82.22/83.60 85.09/82.33/83.69 +0.09 84.81/82.20/83.48 −0.11
WESC (92k) 86.56/83.58/85.05 86.83/84.04/85.41 +0.36 87.03/83.96/85.47 +0.42
LOG (23k) 88.60/87.23/87.91 88.72/87.20/87.95 +0.04 88.43/87.00/87.71 −0.21
LOG (92k) 91.74/90.15/90.94 91.82/90.07/90.94 −0.00 91.90/90.13/91.01 +0.07
WESC WESC (23k) 86.80/84.43/85.60 87.12/84.44/85.76 +0.16 87.18/84.50/85.82 +0.22
WESC (92k) 89.34/86.81/88.06 89.54/86.76/88.13 +0.07 89.43/87.23/88.32 +0.26
LOG (23k) 83.74/81.41/82.56 84.02/81.43/82.71 +0.15 84.10/81.67/82.86 +0.31
LOG (92k) 85.98/82.93/84.43 86.02/82.69/84.32 −0.11 85.89/82.76/84.30 −0.13
</table>
<tableCaption confidence="0.899662666666667">
Table 2: Results for SS (WNF) (supersense from first WordNet sense), evaluated on 23k tokens (approx 1500
sentences) of either WESCIENCE or LOGON, and trained on various sizes of in-domain and cross-domain training
data. Subscript ‘P’ indicates mappings were applied to leaf parents rather than leaves.
</tableCaption>
<table confidence="0.9998079">
Test Train P/ R/ F SS (SST) OF SSP(SST) OF
P/ R/ F P/ R/ F
LOG WESC (23k) 85.02/82.22/83.60 84.97/82.38/83.65 +0.06 85.32/82.66/83.97 +0.37
WESC (92k) 86.56/83.58/85.05 87.05/84.47/85.74 +0.70 86.98/83.87/85.40 +0.35
LOG (23k) 88.60/87.23/87.91 88.93/87.50/88.21 +0.29 88.84/87.40/88.11 +0.20
LOG (92k) 91.74/90.15/90.94 91.67/90.02/90.83 −0.10 91.47/89.96/90.71 −0.23
WESC WESC (23k) 86.80/84.43/85.60 86.88/84.29/85.56 −0.04 87.32/84.48/85.88 +0.27
WESC (92k) 89.34/86.81/88.06 89.53/86.54/88.01 −0.05 89.50/86.56/88.00 −0.05
LOG (23k) 83.74/81.41/82.56 84.06/81.30/82.66 +0.10 83.96/81.64/82.78 +0.23
LOG (92k) 85.98/82.93/84.43 86.13/82.96/84.51 +0.08 85.76/82.84/84.28 −0.16
</table>
<tableCaption confidence="0.943178">
Table 3: Results for SS (SST) (supersense from SuperSense Tagger)
</tableCaption>
<table confidence="0.9997852">
Test Train P/ R/ F HPWNF OF HPP(WNF) OF
P/ R/ F P/ R/ F
LOG WESC (23k) 85.02/82.22/83.60 84.56/82.03/83.28 −0.32 84.74/82.20/83.45 −0.15
WESC (92k) 86.56/83.58/85.05 86.65/84.22/85.42 +0.37 86.41/83.65/85.01 −0.04
LOG (23k) 88.60/87.23/87.91 88.58/87.26/87.92 +0.00 88.58/87.35/87.96 +0.05
LOG (92k) 91.74/90.15/90.94 91.68/90.19/90.93 −0.01 91.66/89.85/90.75 −0.19
WESC WESC (23k) 86.80/84.43/85.60 86.89/84.19/85.52 −0.08 87.18/84.43/85.78 +0.18
WESC (92k) 89.34/86.81/88.06 89.74/86.96/88.33 +0.27 89.23/86.88/88.04 −0.01
LOG (23k) 83.74/81.41/82.56 83.87/81.20/82.51 −0.04 83.47/81.00/82.22 −0.33
LOG (92k) 85.98/82.93/84.43 85.89/82.38/84.10 −0.33 85.75/83.03/84.37 −0.06
</table>
<tableCaption confidence="0.9447">
Table 4: Results for HPWNF (hypernym path from first WordNet sense)
</tableCaption>
<table confidence="0.9997291">
Test Train P/ R/ F LDTP(5)
P/ R/ F OF
LOG WESC (23k) 85.02/82.22/83.60 84.48/82.18/83.31 −0.28
WESC (92k) 86.56/83.58/85.05 86.36/84.14/85.23 +0.19
LOG (23k) 88.60/87.23/87.91 88.28/86.99/87.63 −0.28
LOG (92k) 91.74/90.15/90.94 91.01/89.25/90.12 −0.82
WESC WESC (23k) 86.80/84.43/85.60 86.17/83.51/84.82 −0.78
WESC (92k) 89.34/86.81/88.06 88.31/85.61/86.94 −1.12
LOG (23k) 83.74/81.41/82.56 83.60/81.18/82.37 −0.19
LOG (92k) 85.98/82.93/84.43 85.74/82.96/84.33 −0.11
</table>
<tableCaption confidence="0.997472">
Table 5: Results for LDT (5) (Lin-style distributional thesaurus, expanding each term with the top-5 most similar)
</tableCaption>
<page confidence="0.998367">
233
</page>
<bodyText confidence="0.999913310344828">
attempting to tag semantically, or if our method has
no knowledge of the particular word.
The mapping is applied at the point of feature ex-
traction from the set of derivation trees – at model
construction time for the training set and at rerank-
ing time for the development set. If a given leaf to-
ken has some set of corresponding semantic tags, we
add a set of variant features for each semantic tag,
duplicated and modified from the matching “core”
features described above. There are two ways these
mappings can be applied, since it is not immedi-
ately apparent where the extra lexical generalisation
would be most useful. The ‘leaf’ variant applies to
the leaf node itself, so that in each feature involving
the leaf node, add a variant where the leaf node sur-
face string has been replaced with the new seman-
tic tag. The ‘parent’ variant, which has a subscript
‘P’ (e.g. SSP(WNF) ) applies the mapping to the
immediate parent of the leaf node, leaving the leaf
itself unchanged, but creating variant features with
the parent nodes replaced with the tag.
For our example here, we assume that we have
an SS mapping for Figure 2(a), and that this has
mapped the token for &amp;quot;issues&amp;quot; to the WordNet
supersense noun.cognition. For the leaf vari-
ant, the extra features that would be added (either for
considering inclusion in the model, or for scoring a
sentence when reranking) are shown in Figure 2(b),
while those for the parent variant are in Figure 2(c).
</bodyText>
<subsectionHeader confidence="0.9776345">
3.4.4 Evaluating the contribution of sense
annotations
</subsectionHeader>
<bodyText confidence="0.999993944444445">
We wish to evaluate whether adding sense annota-
tions improve parser accuracy against the baseline of
training a model in the conventional way using only
syntactic features. As noted above, we suspect that
this semantic generalisation may help in cases where
appropriate training data is sparse – that is, where
the training data is from a different domain or only
a small amount exists. So to evaluate the various
methods in these conditions, we train models from
small (23k token) training sets and large (96k token)
training sets created from subsets of each corpus
(WESCIENCE and LOGON). For the baseline, we
train these models without modification. For each
of the various methods of adding semantic tags, we
then re-use each of these training sets to create new
models after adding the appropriate additional fea-
tures as described above, to evaluate whether these
additional features improve parsing accuracy
</bodyText>
<sectionHeader confidence="0.999755" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999984976744186">
We present an extensive summary of the results ob-
tained using the various methods in Tables 2, 3, 4
and 5. In each case we show results for applying
to the leaf and to the parent. Aggregating the re-
sults for each method, the differences range between
substantially negative and modestly positive, with a
large number of fluctuations due to statistical noise.
LDT is the least promising performer, with only
one very modest improvement, and the largest de-
creases in performance, of around 1%. The HP-
WNF and HPP(WNF) methods make changes in
either direction – on average, over all four train-
ing/test combinations, there are very small drops
in F-score of 0.02% for HPWNF, and 0.06% for
HPP(WNF), which indicates that neither of the
methods is likely to be useful in reliably improving
parser performance.
The SS methods are more promising. SS (WNF)
and SSP(WNF) methods yield an average im-
provement of 0.10% each, while SS (SST) and
SSP(SST) give average improvements of 0.12%
and 0.13% respectively (representing an error rate
reduction of around 1%). Interestingly, the increase
in tagging accuracy we might expect using Super-
Sense Tagger only translates to a modest (and prob-
ably not significant) increase in parser performance,
possibly because the tagger is not optimised for the
domains in question. Amongst the statistical noise
it is hard to discern overall trends; surprisingly, it
seems that the size of the training corpus has rela-
tively little to do with the success of adding these su-
persense annotations, and that the corpus being from
an unmatched domain doesn’t necessarily mean that
sense-tagging will improve accuracy either. There
may be a slight trend for sense annotations to be
more useful when WESCIENCE is the training cor-
pus (either in the small or the large size).
To gain a better insight into how the effects
change as the size of the training corpus changes for
the different domains, we created learning curves for
the best-performing method, SSP(SST) (although
as noted above, all SS methods give similar levels
of improvement), shown in Figure 3. Overall, these
</bodyText>
<page confidence="0.994713">
234
</page>
<figure confidence="0.999934757575758">
0 20 40 60 80 100
Training Tokens (thousands)
(a) LOGON
0 20 40 60 80 100
Training Tokens (thousands)
(b) WESCIENCE
0.84
0.90
0.88
0.86
0.80
0.78
0.76
0.82
Test Corpus
LOGON*
LOGON* +SS
WeSc
WeSc +SS
W
0.86
0.84
0.76
0.90
0.88
0.80
0.78
0.82
Test Corpus
LOGON
LOGON +SS
WeSc*
WeSc* +SS
</figure>
<figureCaption confidence="0.9994815">
Figure 3: EDMNA learning curves for SS (SST) (supersense from SuperSense Tagger). ‘*’ denotes in-domain
training corpus.
</figureCaption>
<bodyText confidence="0.9999275">
graphs support the same conclusions as the tables
– the gains we see are very modest and there is a
slight tendency for WESCIENCE models to benefit
more from the semantic generalisation, but no strong
tendencies for this to work better for cross-domain
training data or small training sets.
</bodyText>
<sectionHeader confidence="0.998835" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999994888888889">
We have presented an initial study evaluat-
ing whether a fairly simple approach to using
automatically-created coarse semantic annotations
can improve HPSG parse selection accuracy using
the English Resource Grammar. We have provided
some weak evidence that adding features based on
semantic annotations, and in particular word super-
sense, can provide modest improvements in parse
selection performance in terms of dependency F-
score, with the best-performing method SSP(SST)
providing an average reduction in error rate over 4
training/test corpus combinations of 1%. Other ap-
proaches were less promising. In all configurations,
there were instances of F-score decreases, some-
times substantial.
It is somewhat surprising that we did not achieve
reliable performance gains which were seen in the
related work described above. One possible expla-
nation is that the model training parameters were
suboptimal for this data set since the characteris-
tics of the data are somewhat different than with-
out sense annotations. The failure to improve some-
what mirrors the results of Clark (2001), who was at-
tempting to improve the parse ranking performance
of the unification-based based probabilistic parser of
Carroll and Briscoe (1996). Clark (2001) used de-
pendencies to rank parses, and WordNet-based tech-
niques to generalise this model and learn selectional
preferences, but failed to improve performance over
the structural (i.e. non-dependency) ranking in the
original parser. Additionally, perhaps the changes
we applied in this work to the parse ranking could
possibly have been more effective with features
based on semantic dependences as used by Fujita
et al. (2007), although we outlined reasons why we
wished to avoid this approach.
This work is preliminary and there is room for
more exploration in this space. There is scope for
much more feature engineering on the semantic an-
notations, such as using different levels of the se-
mantic hierarchy, or replacing the purely lexical fea-
tures instead of augmenting them. Additionally,
more error analysis would reveal whether this ap-
proach was more useful for avoiding certain kinds
of parser errors (such as PP-attachment).
</bodyText>
<sectionHeader confidence="0.996512" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9985866">
NICTA is funded by the Australian Government as
represented by the Department of Broadband, Com-
munications and the Digital Economy and the Aus-
tralian Research Council through the ICT Centre of
Excellence program.
</bodyText>
<page confidence="0.99722">
235
</page>
<sectionHeader confidence="0.990313" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999323398058253">
E. Agirre, T. Baldwin, and D. Martinez. 2008. Improv-
ing parsing and PP attachment performance with sense
information. In Proceedings of ACL-08: HLT, pages
317–325, Columbus, Ohio, June.
Eneko Agirre, Kepa Bengoetxea, Koldo Gojenola, and
Joakim Nivre. 2011. Improving dependency parsing
with semantic classes. In Proceedings of the 49th An-
nual Meeting of the Association of Computational Lin-
guistics, ACL-HLT 2011 Short Paper, Portland, Ore-
gon”.
D. M. Bikel. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In Proceed-
ings of the second international conference on Human
Language Technology Research, pages 178–182, San
Francisco, CA, USA.
T. Brants. 2000. Tnt – a statistical part-of-speech tag-
ger. In Proceedings of the Sixth Conference on Ap-
plied Natural Language Processing, pages 224–231,
Seattle, Washington, USA, April.
T. Briscoe and J. Carroll. 2002. Robust accurate statis-
tical annotation of general text. In Proceedings of the
3rd International Conference on Language Resources
and Evaluation, pages 1499–1504.
U. Callmeier. 2000. Pet – a platform for experimenta-
tion with efficient HPSG processing techniques. Nat.
Lang. Eng., 6(1):99–107.
J. Carroll and E. Briscoe. 1996. Apportioning devel-
opment effort in a probabilistic lr pars- ing system
through evaluation. In Proceedings of the SIGDAT
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 92–100, Philadelphia, PA.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of the 1st North American chapter of
the Association for Computational Linguistics confer-
ence, pages 132–139.
M. Ciaramita and Y. Altun. 2006. Broad-coverage sense
disambiguation and information extraction with a su-
persense sequence tagger. In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 594–602, Sydney, Australia,
July.
S. Clark and J.R. Curran. 2004. Parsing the WSJ us-
ing CCG and log-linear models. In Proceedings of the
42nd Meeting of the ACL, pages 104–111.
S. Clark. 2001. Class-based Statistical Models for Lex-
ical Knowledge Acquisition. Ph.D. thesis, University
of Sussex.
M. Collins. 1997. Three generative, lexicalised mod-
els for statistical parsing. In Proceedings of the 35th
Annual Meeting of the Association for Computational
Linguistics, pages 16–23, Madrid, Spain, July.
R. Dridan and S. Oepen. 2011. Parser evaluation us-
ing elementary dependency matching. In Proceedings
of the 12th International Conference on Parsing Tech-
nologies, pages 225–230, Dublin, Ireland, October.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Nat. Lang. Eng., 6
(1):15–28.
S. Fujita, F. Bond, S. Oepen, and T. Tanaka. 2007. Ex-
ploiting semantic information for HPSG parse selec-
tion. In ACL 2007 Workshop on Deep Linguistic Pro-
cessing, pages 25–32, Prague, Czech Republic, June.
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proceedings of the 17th international
conference on Computational linguistics-Volume 2,
pages 768–774.
A. MacKinlay, R. Dridan, D. Flickinger, and T. Baldwin.
2011. Cross-domain effects on parse selection for pre-
cision grammars. Research on Language &amp; Computa-
tion, 8(4):299–340.
R. Malouf. 2002. A comparison of algorithms for maxi-
mum entropy parameter estimation. In Proceedings of
the Sixth Conference on Natural Language Learning
(CoNLL-2002), pages 49–55.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of english:
the penn treebank. Comput. Linguist., 19(2):313–330.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2004.
Finding predominant word senses in untagged text. In
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, pages 279–es.
G.A. Miller. 1995. WordNet: a lexical database for En-
glish. Communications of the ACM, 38(11):39–41.
S. Oepen, D. Flickinger, K. Toutanova, and C.D. Man-
ning. 2004. LinGO Redwoods: A rich and dynamic
treebank for HPSG. Research on Language &amp; Com-
putation, 2(4):575–596.
M. Siegel and E.M. Bender. 2002. Efficient deep pro-
cessing of japanese. In Proceedings of the 3rd work-
shop on Asian language resources and international
standardization-Volume 12, pages 1–8.
E. Velldal. 2007. Empirical Realization Ranking. Ph.D.
thesis, University of Oslo Department of Informatics.
G. Ytrestøl, D. Flickinger, and S. Oepen. 2009. Ex-
tracting and annotating Wikipedia sub-domains – to-
wards a new eScience community resourc. In Pro-
ceedings of the Seventh International Workshop on
Treebanks and Linguistic Theories, Groeningen, The
Netherlands, January.
Y. Zhang, S. Oepen, and J. Carroll. 2007. Efficiency in
unification-based n-best parsing. In IWPT ’07: Pro-
ceedings of the 10th International Conference on Pars-
ing Technologies, pages 48–59, Morristown, NJ, USA.
</reference>
<page confidence="0.998501">
236
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.242435">
<title confidence="0.999891">The Effects of Semantic Annotations on Precision Parse Ranking</title>
<author confidence="0.999235">Rebecca Diana Timothy</author>
<affiliation confidence="0.961347">Dept. of Computing and Information Systems, University of Melbourne, Australia</affiliation>
<address confidence="0.681848666666667">Victoria Research Laboratories, University of Melbourne, Australia of Informatics, University of Oslo, Norway Linguistics and Phonetics, Saarland University, Germany</address>
<abstract confidence="0.994081357142857">We investigate the effects of adding semantic annotations including word sense hypernyms to the source text for use as an extra source of information in HPSG parse ranking for the English Resource Grammar. The semantic annotations are coarse semantic categories or entries from a distributional thesaurus, assigned either heuristically or by a pre-trained tagger. We test this using two test corpora in different domains with various sources of training data. The best reduces error rate in dependency Fscore by 1% on average, while some methods produce substantial decreases in performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>T Baldwin</author>
<author>D Martinez</author>
</authors>
<title>Improving parsing and PP attachment performance with sense information.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>317--325</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="6342" citStr="Agirre et al. (2008)" startWordPosition="991" endWordPosition="994">g all candidates out of possibly many thousands of trees, Zhang et al. (2007) showed that it was possible to use ‘selective unpacking’, which means that the exhaustive parse forest can be represented compactly as a ‘packed forest’, and the top-ranked trees can be successively reconstructed, enabling faster parsing using less memory. 2.2 Semantic Generalisation for parse ranking Above, we outlined a number of reasons why semantic generalisation of lexemes could enable parsers to make more efficient use of training data, and indeed, there has been some prior work investigating this possibility. Agirre et al. (2008) applied two state-of-the-art treebank parsers to the sensetagged subset of the Brown corpus version of the Penn Treebank (Marcus et al., 1993), and added sense annotation to the training data to evaluate their impact on parse selection and specifically on PPattachment. The annotations they used were oracle sense annotations, automatic sense recognition and the first sense heuristic, and it was this last method which was the best performer in general. The sense annotations were either the WordNet synset ID or the coarse semantic file, which we explain in more detail below, and replaced the ori</context>
<context position="14050" citStr="Agirre et al. (2008)" startWordPosition="2272" endWordPosition="2275">model, the finegrained synset ID will do relatively little to provide a coarser identifier for the token – indeed, if two tokens with identical forms were assigned different synset IDs, we would be obscuring the similarity.1 We can of course make use of the WordNet hierarchy, and use hypernyms from the hierarchy to tag each candidate token, but there are a large number of ways this can be achieved, particularly when it is possibly to assign multiple labels per token as is the case here (which we discuss in Section 3.4.3). We apply two relatively simple strategies. We noted in Section 2.2 that Agirre et al. (2008) found that the semantic file was useful. This is the coarse lexicographic category label, elsewhere denoted supersense (Ciaramita and Altun, 2006), which is the terminology we use. Nouns are divided into 26 coarse categories such as ‘animal’, ‘quantity’ or ‘phenomenon’, and verbs into 15 categories such as ‘perception’ or ‘consumption’. In some configurations, denoted SS, we tag each open-class token with one of the supersense labels. Another configuration attempts to avoid making assumptions about which level of the hierarchy will be most useful for parse disambiguation, instead leaving it t</context>
</contexts>
<marker>Agirre, Baldwin, Martinez, 2008</marker>
<rawString>E. Agirre, T. Baldwin, and D. Martinez. 2008. Improving parsing and PP attachment performance with sense information. In Proceedings of ACL-08: HLT, pages 317–325, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Kepa Bengoetxea</author>
<author>Koldo Gojenola</author>
<author>Joakim Nivre</author>
</authors>
<title>Improving dependency parsing with semantic classes.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association of Computational Linguistics, ACL-HLT 2011 Short Paper,</booktitle>
<location>Portland, Oregon”.</location>
<contexts>
<context position="7200" citStr="Agirre et al. (2011)" startWordPosition="1131" endWordPosition="1134">nd specifically on PPattachment. The annotations they used were oracle sense annotations, automatic sense recognition and the first sense heuristic, and it was this last method which was the best performer in general. The sense annotations were either the WordNet synset ID or the coarse semantic file, which we explain in more detail below, and replaced the original tokens in the training data. The largest improvement in parsing F-score was a 6.9% reduction in error rate for the Bikel parser (Bikel, 2002), boosting the F-score from 0.841 to 0.852, using the noun supersense only. More recently, Agirre et al. (2011) largely reproduced these results with a dependency parser. Fujita et al. (2007) add sense information to improve parse ranking with JaCy (Siegel and Bender, 2002), an HPSG-based grammar which uses similar machinery to the ERG. They use baseline syntactic features, and also add semantic features based on dependency triples extracted from the semantic representations of the sentence trees output by the parser. The dataset they use has human-assigned sense tags from a Japanese lexical hierarchy, which they use as a source of annotations. The dependency triples are modified in each feature set by</context>
</contexts>
<marker>Agirre, Bengoetxea, Gojenola, Nivre, 2011</marker>
<rawString>Eneko Agirre, Kepa Bengoetxea, Koldo Gojenola, and Joakim Nivre. 2011. Improving dependency parsing with semantic classes. In Proceedings of the 49th Annual Meeting of the Association of Computational Linguistics, ACL-HLT 2011 Short Paper, Portland, Oregon”.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Bikel</author>
</authors>
<title>Design of a multi-lingual, parallelprocessing statistical parsing engine.</title>
<date>2002</date>
<booktitle>In Proceedings of the second international conference on Human Language Technology Research,</booktitle>
<pages>178--182</pages>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="7089" citStr="Bikel, 2002" startWordPosition="1115" endWordPosition="1116">., 1993), and added sense annotation to the training data to evaluate their impact on parse selection and specifically on PPattachment. The annotations they used were oracle sense annotations, automatic sense recognition and the first sense heuristic, and it was this last method which was the best performer in general. The sense annotations were either the WordNet synset ID or the coarse semantic file, which we explain in more detail below, and replaced the original tokens in the training data. The largest improvement in parsing F-score was a 6.9% reduction in error rate for the Bikel parser (Bikel, 2002), boosting the F-score from 0.841 to 0.852, using the noun supersense only. More recently, Agirre et al. (2011) largely reproduced these results with a dependency parser. Fujita et al. (2007) add sense information to improve parse ranking with JaCy (Siegel and Bender, 2002), an HPSG-based grammar which uses similar machinery to the ERG. They use baseline syntactic features, and also add semantic features based on dependency triples extracted from the semantic representations of the sentence trees output by the parser. The dataset they use has human-assigned sense tags from a Japanese lexical h</context>
</contexts>
<marker>Bikel, 2002</marker>
<rawString>D. M. Bikel. 2002. Design of a multi-lingual, parallelprocessing statistical parsing engine. In Proceedings of the second international conference on Human Language Technology Research, pages 178–182, San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
</authors>
<title>Tnt – a statistical part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the Sixth Conference on Applied Natural Language Processing,</booktitle>
<pages>224--231</pages>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="15475" citStr="Brants, 2000" startWordPosition="2504" endWordPosition="2505">open class token is labelled with multiple synsets, starting with the assigned leaf synset and travelling as high as possible up the hierarchy, with no distinction made between the different levels in the hierarchy. Configurations using this are designated HP, for ‘hypernym path’. 3.4.2 Disambiguating senses We return now to the question of determination of the synset for a given token. One frequentlyused and robust strategy is to lemmatise and POStag each token, and assign it the first-listed sense from WordNet (which may or may not be based on actual frequency counts). We POS-tag using TnT (Brants, 2000) and lemmatise using WordNet’s native lemmatiser. This yields a leaf-level synset, making it suitable as a source of annotations for both SS and HP. We denote this ‘WNF’ for ‘WordNet First’ (shown in parentheses after SS or HP). Secondly, to evaluate whether a more informed approach to sense-tagging helps beyond the naive WNF method, in the ‘SST’ method, we use the outputs of SuperSense Tagger (Ciaramita and Altun, 2006), which is optimised for assigning the supersenses described above, and can outperform a WNFstyle baseline on at least some datasets. Since this only gives us coarse supersense</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>T. Brants. 2000. Tnt – a statistical part-of-speech tagger. In Proceedings of the Sixth Conference on Applied Natural Language Processing, pages 224–231, Seattle, Washington, USA, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Robust accurate statistical annotation of general text.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation,</booktitle>
<pages>1499--1504</pages>
<contexts>
<context position="17000" citStr="Briscoe and Carroll, 2002" startWordPosition="2754" endWordPosition="2757">nknown-word handling uses a version of the sentences tagged with TnT. We ignore multi-token named entity outputs from SuperSense Tagger, as these would introduce a confounding factor in our experiments and also reduce comparability of the results with the WNF method. 3.4.3 A distributional thesaurus method A final configuration attempts to avoid the need for curated resources such as WordNet, instead using an automatically-constructed distributional thesaurus (Lin, 1998). We use the thesaurus from McCarthy et al. (2004), constructed along these lines using the grammatical relations from RASP (Briscoe and Carroll, 2002) applied to 90 millions words of text from the British National Corpus. 231 root_frag np_frg_c hdn_bnp_c aj-hdn_norm_c legal_a1 n_pl_olr &amp;quot;legal&amp;quot; issue_n1 &amp;quot;issues&amp;quot; Figure 1: ERG derivation tree for the phrase Legal issues [n_-_c_le &amp;quot;issues&amp;quot;] [n_pl_olr n_-_c_le &amp;quot;issues&amp;quot;] [aj-hdn_norm_c n_pl_olr n_-_c_le &amp;quot;issues&amp;quot;] (a) Original features [n_-_c_le noun.cognition] [n_pl_olr n_-_c_le noun.cognition] [aj-hdn_norm_c n_pl_olr n_-_c_le noun.cognition] (b) Additional features in leaf mode, which augment the original features [noun.cognition &amp;quot;issues&amp;quot;] [n_pl_olr noun.cognition &amp;quot;issues&amp;quot;] [aj-hdn_norm_c n_pl_</context>
</contexts>
<marker>Briscoe, Carroll, 2002</marker>
<rawString>T. Briscoe and J. Carroll. 2002. Robust accurate statistical annotation of general text. In Proceedings of the 3rd International Conference on Language Resources and Evaluation, pages 1499–1504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Callmeier</author>
</authors>
<title>Pet – a platform for experimentation with efficient HPSG processing techniques.</title>
<date>2000</date>
<journal>Nat. Lang. Eng.,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="12232" citStr="Callmeier, 2000" startWordPosition="1962" endWordPosition="1963"> parses which do not match the gold standard exactly but when one of the parses is a closer match than the other; this is not possible with exact match accuracy. 3.3 Reranking for parse selection The features we are adding to the parse selection procedure could all in principle be applied by the parser during the selective unpacking stage, since they all depend on information which can be precomputed. However, we wish to avoid the need for multiple expensive parsing runs, and more importantly the need to modify the relatively complex internals of the parse ranking machinery in the PET parser (Callmeier, 2000). So instead of performing the parse ranking in conjunction with parsing, as is the usual practice, we use a pre-parsed forest of the top-500 trees for each corpus, and rerank the forest afterwards for each configuration shown. The pre-parsed forests use the same models which were used in treebanking. Using reranking means that the set of candidate trees is held constant, which 230 means that parse selection models never get the chance to introduce a new tree which was not in the original parse forest from which the gold tree was annotated, which may provide a very small performance boost (alt</context>
</contexts>
<marker>Callmeier, 2000</marker>
<rawString>U. Callmeier. 2000. Pet – a platform for experimentation with efficient HPSG processing techniques. Nat. Lang. Eng., 6(1):99–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>E Briscoe</author>
</authors>
<title>Apportioning development effort in a probabilistic lr pars- ing system through evaluation.</title>
<date>1996</date>
<booktitle>In Proceedings of the SIGDAT Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>92--100</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="30526" citStr="Carroll and Briscoe (1996)" startWordPosition="4849" endWordPosition="4852">ising. In all configurations, there were instances of F-score decreases, sometimes substantial. It is somewhat surprising that we did not achieve reliable performance gains which were seen in the related work described above. One possible explanation is that the model training parameters were suboptimal for this data set since the characteristics of the data are somewhat different than without sense annotations. The failure to improve somewhat mirrors the results of Clark (2001), who was attempting to improve the parse ranking performance of the unification-based based probabilistic parser of Carroll and Briscoe (1996). Clark (2001) used dependencies to rank parses, and WordNet-based techniques to generalise this model and learn selectional preferences, but failed to improve performance over the structural (i.e. non-dependency) ranking in the original parser. Additionally, perhaps the changes we applied in this work to the parse ranking could possibly have been more effective with features based on semantic dependences as used by Fujita et al. (2007), although we outlined reasons why we wished to avoid this approach. This work is preliminary and there is room for more exploration in this space. There is sco</context>
</contexts>
<marker>Carroll, Briscoe, 1996</marker>
<rawString>J. Carroll and E. Briscoe. 1996. Apportioning development effort in a probabilistic lr pars- ing system through evaluation. In Proceedings of the SIGDAT Conference on Empirical Methods in Natural Language Processing, pages 92–100, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="1187" citStr="Charniak, 2000" startWordPosition="157" endWordPosition="159">ns including word sense hypernyms to the source text for use as an extra source of information in HPSG parse ranking for the English Resource Grammar. The semantic annotations are coarse semantic categories or entries from a distributional thesaurus, assigned either heuristically or by a pre-trained tagger. We test this using two test corpora in different domains with various sources of training data. The best reduces error rate in dependency Fscore by 1% on average, while some methods produce substantial decreases in performance. 1 Introduction Most start-of-the-art natural language parsers (Charniak, 2000; Clark and Curran, 2004; Collins, 1997) use lexicalised features for parse ranking. These are important to achieve optimal parsing accuracy, and yet these are also the features which by their nature suffer from data-sparseness problems in the training data. In the absence of reliable fine-grained statistics for a given token, various strategies are possible. There will often be statistics available for coarser categories, such as the POS of the particular token. However, it is possible that these coarser representations discard too much, missing out information which could be valuable to the </context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ciaramita</author>
<author>Y Altun</author>
</authors>
<title>Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>594--602</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="14197" citStr="Ciaramita and Altun, 2006" startWordPosition="2295" endWordPosition="2298">al forms were assigned different synset IDs, we would be obscuring the similarity.1 We can of course make use of the WordNet hierarchy, and use hypernyms from the hierarchy to tag each candidate token, but there are a large number of ways this can be achieved, particularly when it is possibly to assign multiple labels per token as is the case here (which we discuss in Section 3.4.3). We apply two relatively simple strategies. We noted in Section 2.2 that Agirre et al. (2008) found that the semantic file was useful. This is the coarse lexicographic category label, elsewhere denoted supersense (Ciaramita and Altun, 2006), which is the terminology we use. Nouns are divided into 26 coarse categories such as ‘animal’, ‘quantity’ or ‘phenomenon’, and verbs into 15 categories such as ‘perception’ or ‘consumption’. In some configurations, denoted SS, we tag each open-class token with one of the supersense labels. Another configuration attempts to avoid making assumptions about which level of the hierarchy will be most useful for parse disambiguation, instead leaving it the MaxEnt parse ranker to pick those labels from the hierarchy which are most useful. Each &apos;This could be useful for verbs since senses interact st</context>
<context position="15899" citStr="Ciaramita and Altun, 2006" startWordPosition="2574" endWordPosition="2577">d robust strategy is to lemmatise and POStag each token, and assign it the first-listed sense from WordNet (which may or may not be based on actual frequency counts). We POS-tag using TnT (Brants, 2000) and lemmatise using WordNet’s native lemmatiser. This yields a leaf-level synset, making it suitable as a source of annotations for both SS and HP. We denote this ‘WNF’ for ‘WordNet First’ (shown in parentheses after SS or HP). Secondly, to evaluate whether a more informed approach to sense-tagging helps beyond the naive WNF method, in the ‘SST’ method, we use the outputs of SuperSense Tagger (Ciaramita and Altun, 2006), which is optimised for assigning the supersenses described above, and can outperform a WNFstyle baseline on at least some datasets. Since this only gives us coarse supersense labels, it can only provide SS annotations, as we do not get the leaf synsets needed for HP. The input we feed in is POStagged with TnT as above, for comparability with the WNF method, and to ensure that it is compatible with the configuration in which the corpora were parsed – specifically, the unknown-word handling uses a version of the sentences tagged with TnT. We ignore multi-token named entity outputs from SuperSe</context>
</contexts>
<marker>Ciaramita, Altun, 2006</marker>
<rawString>M. Ciaramita and Y. Altun. 2006. Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 594–602, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J R Curran</author>
</authors>
<title>Parsing the WSJ using CCG and log-linear models.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the ACL,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="1211" citStr="Clark and Curran, 2004" startWordPosition="160" endWordPosition="163">d sense hypernyms to the source text for use as an extra source of information in HPSG parse ranking for the English Resource Grammar. The semantic annotations are coarse semantic categories or entries from a distributional thesaurus, assigned either heuristically or by a pre-trained tagger. We test this using two test corpora in different domains with various sources of training data. The best reduces error rate in dependency Fscore by 1% on average, while some methods produce substantial decreases in performance. 1 Introduction Most start-of-the-art natural language parsers (Charniak, 2000; Clark and Curran, 2004; Collins, 1997) use lexicalised features for parse ranking. These are important to achieve optimal parsing accuracy, and yet these are also the features which by their nature suffer from data-sparseness problems in the training data. In the absence of reliable fine-grained statistics for a given token, various strategies are possible. There will often be statistics available for coarser categories, such as the POS of the particular token. However, it is possible that these coarser representations discard too much, missing out information which could be valuable to the parse ranking. An interm</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>S. Clark and J.R. Curran. 2004. Parsing the WSJ using CCG and log-linear models. In Proceedings of the 42nd Meeting of the ACL, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
</authors>
<title>Class-based Statistical Models for Lexical Knowledge Acquisition.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Sussex.</institution>
<contexts>
<context position="30383" citStr="Clark (2001)" startWordPosition="4830" endWordPosition="4831">SST) providing an average reduction in error rate over 4 training/test corpus combinations of 1%. Other approaches were less promising. In all configurations, there were instances of F-score decreases, sometimes substantial. It is somewhat surprising that we did not achieve reliable performance gains which were seen in the related work described above. One possible explanation is that the model training parameters were suboptimal for this data set since the characteristics of the data are somewhat different than without sense annotations. The failure to improve somewhat mirrors the results of Clark (2001), who was attempting to improve the parse ranking performance of the unification-based based probabilistic parser of Carroll and Briscoe (1996). Clark (2001) used dependencies to rank parses, and WordNet-based techniques to generalise this model and learn selectional preferences, but failed to improve performance over the structural (i.e. non-dependency) ranking in the original parser. Additionally, perhaps the changes we applied in this work to the parse ranking could possibly have been more effective with features based on semantic dependences as used by Fujita et al. (2007), although we out</context>
</contexts>
<marker>Clark, 2001</marker>
<rawString>S. Clark. 2001. Class-based Statistical Models for Lexical Knowledge Acquisition. Ph.D. thesis, University of Sussex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>16--23</pages>
<location>Madrid, Spain,</location>
<contexts>
<context position="1227" citStr="Collins, 1997" startWordPosition="164" endWordPosition="165"> source text for use as an extra source of information in HPSG parse ranking for the English Resource Grammar. The semantic annotations are coarse semantic categories or entries from a distributional thesaurus, assigned either heuristically or by a pre-trained tagger. We test this using two test corpora in different domains with various sources of training data. The best reduces error rate in dependency Fscore by 1% on average, while some methods produce substantial decreases in performance. 1 Introduction Most start-of-the-art natural language parsers (Charniak, 2000; Clark and Curran, 2004; Collins, 1997) use lexicalised features for parse ranking. These are important to achieve optimal parsing accuracy, and yet these are also the features which by their nature suffer from data-sparseness problems in the training data. In the absence of reliable fine-grained statistics for a given token, various strategies are possible. There will often be statistics available for coarser categories, such as the POS of the particular token. However, it is possible that these coarser representations discard too much, missing out information which could be valuable to the parse ranking. An intermediate level of </context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>M. Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 16–23, Madrid, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dridan</author>
<author>S Oepen</author>
</authors>
<title>Parser evaluation using elementary dependency matching.</title>
<date>2011</date>
<booktitle>In Proceedings of the 12th International Conference on Parsing Technologies,</booktitle>
<pages>225--230</pages>
<location>Dublin, Ireland,</location>
<contexts>
<context position="10858" citStr="Dridan and Oepen, 2011" startWordPosition="1726" endWordPosition="1729">orpora are summarised in Table 1. With these corpora, we are able to investigate indomain and cross-domain effects, by testing on a different corpus to the training corpus, so we can examine whether sense-tagging alleviates the crossdomain performance penalty noted in MacKinlay et al. (2011). We can also use a subset of each training corpus to simulate the common situation of sparse training data, so we can investigate whether sensetagging enables the learner to make better use of a limited quantity of training data. 3.2 Evaluation Our primary evaluation metric is Elementary Dependency Match (Dridan and Oepen, 2011). This converts the semantic output of the ERG into a set of dependency-like triples, and scores these triples using precision, recall and F-score as is conventional for other dependency evaluation. Following MacKinlay et al. (2011), we use the EDMNA mode of evaluation, which provides a good level of comparability while still reflecting most the semantically salient information from the grammar. Other work on the ERG and related grammars has tended to focus on exact tree match, but the granular EDM metric is a better fit for our needs here – among other reasons, it is more sensitive in terms o</context>
</contexts>
<marker>Dridan, Oepen, 2011</marker>
<rawString>R. Dridan and S. Oepen. 2011. Parser evaluation using elementary dependency matching. In Proceedings of the 12th International Conference on Parsing Technologies, pages 225–230, Dublin, Ireland, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Flickinger</author>
</authors>
<title>On building a more efficient grammar by exploiting types.</title>
<date>2000</date>
<journal>Nat. Lang. Eng.,</journal>
<volume>6</volume>
<pages>1--15</pages>
<contexts>
<context position="4473" citStr="Flickinger (2000)" startWordPosition="690" endWordPosition="691">Lexical and Computational Semantics (*SEM), pages 228–236, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics The synonyms or hypernyms could take the form of any grouping which relates word forms with semantic or syntactic commonality – such as a label from the WordNet (Miller, 1995) hierarchy, a subcategorisation frame (for verbs) or closely related terms from a distributional thesaurus (Lin, 1998). We present work here on using various levels of semantic generalisation as an attempt to improve parse selection accuracy with the English Resource Grammar (ERG: Flickinger (2000)), a precision HPSG-based grammar of English. 2 Related Work 2.1 Parse Selection for Precision Grammars The focus of this work is on parsing using handcrafted precision HPSG-based grammars, and in particular the ERG. While these grammars are carefully crafted to avoid overgeneration, the ambiguity of natural languages means that there will unavoidably be multiple candidate parses licensed by the grammar for any non-trivial sentence. For the ERG, the number of parses postulated for a given sentence can be anywhere from zero to tens of thousands. It is the job of the parse selection model to sel</context>
</contexts>
<marker>Flickinger, 2000</marker>
<rawString>Dan Flickinger. 2000. On building a more efficient grammar by exploiting types. Nat. Lang. Eng., 6 (1):15–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Fujita</author>
<author>F Bond</author>
<author>S Oepen</author>
<author>T Tanaka</author>
</authors>
<title>Exploiting semantic information for HPSG parse selection.</title>
<date>2007</date>
<booktitle>In ACL 2007 Workshop on Deep Linguistic Processing,</booktitle>
<pages>25--32</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="7280" citStr="Fujita et al. (2007)" startWordPosition="1144" endWordPosition="1147">otations, automatic sense recognition and the first sense heuristic, and it was this last method which was the best performer in general. The sense annotations were either the WordNet synset ID or the coarse semantic file, which we explain in more detail below, and replaced the original tokens in the training data. The largest improvement in parsing F-score was a 6.9% reduction in error rate for the Bikel parser (Bikel, 2002), boosting the F-score from 0.841 to 0.852, using the noun supersense only. More recently, Agirre et al. (2011) largely reproduced these results with a dependency parser. Fujita et al. (2007) add sense information to improve parse ranking with JaCy (Siegel and Bender, 2002), an HPSG-based grammar which uses similar machinery to the ERG. They use baseline syntactic features, and also add semantic features based on dependency triples extracted from the semantic representations of the sentence trees output by the parser. The dataset they use has human-assigned sense tags from a Japanese lexical hierarchy, which they use as a source of annotations. The dependency triples are modified in each feature set by replacing elements of the semantic triples with corresponding senses or hyperny</context>
<context position="8824" citStr="Fujita et al. (2007)" startWordPosition="1388" endWordPosition="1391">mpact on parse selection of semantic annotations such as coarse sense labels or synonyms from a distributional the229 WESCIENCE LOGON Total Sentences 9632 9410 Parseable Sentences 9249 8799 Validated Sentences 7631 8550 Train/Test Sentences 6149/1482 6823/1727 Tokens/sentence 15.0 13.6 Training Tokens 92.5k 92.8k Table 1: Corpora used in our experiments, with total sentences, how many of those can be parsed, how many of the parseable sentences have a single gold parse (and are used in these experiments), and average sentence length saurus. Our work here differs from the aforementioned work of Fujita et al. (2007) in a number of ways. Firstly, we use purely syntactic parse selection features based on the derivation tree of the sentence (see Section 3.4.3), rather than ranking using dependency triples, meaning that our method is in principle able to be integrated into a parser more easily, where the final set of dependencies would not be known in advance. Secondly, we do not use humancreated sense annotations, instead relying on heuristics or trained sense-taggers, which is closer to the reality of real-world parsing tasks. 3.1 Corpora Following MacKinlay et al. (2011), we use two primary training corpo</context>
<context position="30966" citStr="Fujita et al. (2007)" startWordPosition="4916" endWordPosition="4919">t mirrors the results of Clark (2001), who was attempting to improve the parse ranking performance of the unification-based based probabilistic parser of Carroll and Briscoe (1996). Clark (2001) used dependencies to rank parses, and WordNet-based techniques to generalise this model and learn selectional preferences, but failed to improve performance over the structural (i.e. non-dependency) ranking in the original parser. Additionally, perhaps the changes we applied in this work to the parse ranking could possibly have been more effective with features based on semantic dependences as used by Fujita et al. (2007), although we outlined reasons why we wished to avoid this approach. This work is preliminary and there is room for more exploration in this space. There is scope for much more feature engineering on the semantic annotations, such as using different levels of the semantic hierarchy, or replacing the purely lexical features instead of augmenting them. Additionally, more error analysis would reveal whether this approach was more useful for avoiding certain kinds of parser errors (such as PP-attachment). Acknowledgements NICTA is funded by the Australian Government as represented by the Departmen</context>
</contexts>
<marker>Fujita, Bond, Oepen, Tanaka, 2007</marker>
<rawString>S. Fujita, F. Bond, S. Oepen, and T. Tanaka. 2007. Exploiting semantic information for HPSG parse selection. In ACL 2007 Workshop on Deep Linguistic Processing, pages 25–32, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguistics-Volume 2,</booktitle>
<pages>768--774</pages>
<contexts>
<context position="4292" citStr="Lin, 1998" startWordPosition="661" endWordPosition="662">only to PP attachment, but may help in a range of other syntactic phenomena, such as distinguishing between complements and modifiers of verbs. 228 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 228–236, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics The synonyms or hypernyms could take the form of any grouping which relates word forms with semantic or syntactic commonality – such as a label from the WordNet (Miller, 1995) hierarchy, a subcategorisation frame (for verbs) or closely related terms from a distributional thesaurus (Lin, 1998). We present work here on using various levels of semantic generalisation as an attempt to improve parse selection accuracy with the English Resource Grammar (ERG: Flickinger (2000)), a precision HPSG-based grammar of English. 2 Related Work 2.1 Parse Selection for Precision Grammars The focus of this work is on parsing using handcrafted precision HPSG-based grammars, and in particular the ERG. While these grammars are carefully crafted to avoid overgeneration, the ambiguity of natural languages means that there will unavoidably be multiple candidate parses licensed by the grammar for any non-</context>
<context position="16849" citStr="Lin, 1998" startWordPosition="2733" endWordPosition="2734"> with the WNF method, and to ensure that it is compatible with the configuration in which the corpora were parsed – specifically, the unknown-word handling uses a version of the sentences tagged with TnT. We ignore multi-token named entity outputs from SuperSense Tagger, as these would introduce a confounding factor in our experiments and also reduce comparability of the results with the WNF method. 3.4.3 A distributional thesaurus method A final configuration attempts to avoid the need for curated resources such as WordNet, instead using an automatically-constructed distributional thesaurus (Lin, 1998). We use the thesaurus from McCarthy et al. (2004), constructed along these lines using the grammatical relations from RASP (Briscoe and Carroll, 2002) applied to 90 millions words of text from the British National Corpus. 231 root_frag np_frg_c hdn_bnp_c aj-hdn_norm_c legal_a1 n_pl_olr &amp;quot;legal&amp;quot; issue_n1 &amp;quot;issues&amp;quot; Figure 1: ERG derivation tree for the phrase Legal issues [n_-_c_le &amp;quot;issues&amp;quot;] [n_pl_olr n_-_c_le &amp;quot;issues&amp;quot;] [aj-hdn_norm_c n_pl_olr n_-_c_le &amp;quot;issues&amp;quot;] (a) Original features [n_-_c_le noun.cognition] [n_pl_olr n_-_c_le noun.cognition] [aj-hdn_norm_c n_pl_olr n_-_c_le noun.cognition] (b) </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th international conference on Computational linguistics-Volume 2, pages 768–774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A MacKinlay</author>
<author>R Dridan</author>
<author>D Flickinger</author>
<author>T Baldwin</author>
</authors>
<title>Cross-domain effects on parse selection for precision grammars.</title>
<date>2011</date>
<journal>Research on Language &amp; Computation,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="5634" citStr="MacKinlay et al. (2011)" startWordPosition="883" endWordPosition="886">f thousands. It is the job of the parse selection model to select the best parse from all of these candidates as accurately as possible, for some definition of ‘best’, as we discuss in Section 3.2. Parse selection is usually performed by training discriminative parse selection models, which ‘discriminate’ between the set of all candidate parses. A widely-used method to achieve this is outlined in Velldal (2007). We feed both correct and incorrect parses licensed by the grammar to the TADM toolkit (Malouf, 2002), and learn a maximum entropy model. This method is used by Zhang et al. (2007) and MacKinlay et al. (2011) inter alia. One important implementation detail is that rather than exhaustively ranking all candidates out of possibly many thousands of trees, Zhang et al. (2007) showed that it was possible to use ‘selective unpacking’, which means that the exhaustive parse forest can be represented compactly as a ‘packed forest’, and the top-ranked trees can be successively reconstructed, enabling faster parsing using less memory. 2.2 Semantic Generalisation for parse ranking Above, we outlined a number of reasons why semantic generalisation of lexemes could enable parsers to make more efficient use of tr</context>
<context position="9389" citStr="MacKinlay et al. (2011)" startWordPosition="1483" endWordPosition="1486">ffers from the aforementioned work of Fujita et al. (2007) in a number of ways. Firstly, we use purely syntactic parse selection features based on the derivation tree of the sentence (see Section 3.4.3), rather than ranking using dependency triples, meaning that our method is in principle able to be integrated into a parser more easily, where the final set of dependencies would not be known in advance. Secondly, we do not use humancreated sense annotations, instead relying on heuristics or trained sense-taggers, which is closer to the reality of real-world parsing tasks. 3.1 Corpora Following MacKinlay et al. (2011), we use two primary training corpora. First, we use the LOGON corpus (Oepen et al., 2004), a collection of English translations of Norwegian hiking texts. The LOGON corpus contains 8550 sentences with exactly one gold parse, which we partitioned randomly by sentence into 10 approximately equal sections, reserving two sections as test data, and using the remainder as our training corpus. These sentences were randomly divided into training and development data. Secondly, we use the WeScience (Ytrestøl et al., 2009) corpus, a collection of Wikipedia articles related to computational linguistics.</context>
<context position="11090" citStr="MacKinlay et al. (2011)" startWordPosition="1762" endWordPosition="1765">sdomain performance penalty noted in MacKinlay et al. (2011). We can also use a subset of each training corpus to simulate the common situation of sparse training data, so we can investigate whether sensetagging enables the learner to make better use of a limited quantity of training data. 3.2 Evaluation Our primary evaluation metric is Elementary Dependency Match (Dridan and Oepen, 2011). This converts the semantic output of the ERG into a set of dependency-like triples, and scores these triples using precision, recall and F-score as is conventional for other dependency evaluation. Following MacKinlay et al. (2011), we use the EDMNA mode of evaluation, which provides a good level of comparability while still reflecting most the semantically salient information from the grammar. Other work on the ERG and related grammars has tended to focus on exact tree match, but the granular EDM metric is a better fit for our needs here – among other reasons, it is more sensitive in terms of error rate reduction to changes in parse selection models (MacKinlay et al., 2011). Additionally, it is desirable to be able to choose between two different parses which do not match the gold standard exactly but when one of the p</context>
</contexts>
<marker>MacKinlay, Dridan, Flickinger, Baldwin, 2011</marker>
<rawString>A. MacKinlay, R. Dridan, D. Flickinger, and T. Baldwin. 2011. Cross-domain effects on parse selection for precision grammars. Research on Language &amp; Computation, 8(4):299–340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Sixth Conference on Natural Language Learning (CoNLL-2002),</booktitle>
<pages>49--55</pages>
<contexts>
<context position="5527" citStr="Malouf, 2002" startWordPosition="864" endWordPosition="865">the ERG, the number of parses postulated for a given sentence can be anywhere from zero to tens of thousands. It is the job of the parse selection model to select the best parse from all of these candidates as accurately as possible, for some definition of ‘best’, as we discuss in Section 3.2. Parse selection is usually performed by training discriminative parse selection models, which ‘discriminate’ between the set of all candidate parses. A widely-used method to achieve this is outlined in Velldal (2007). We feed both correct and incorrect parses licensed by the grammar to the TADM toolkit (Malouf, 2002), and learn a maximum entropy model. This method is used by Zhang et al. (2007) and MacKinlay et al. (2011) inter alia. One important implementation detail is that rather than exhaustively ranking all candidates out of possibly many thousands of trees, Zhang et al. (2007) showed that it was possible to use ‘selective unpacking’, which means that the exhaustive parse forest can be represented compactly as a ‘packed forest’, and the top-ranked trees can be successively reconstructed, enabling faster parsing using less memory. 2.2 Semantic Generalisation for parse ranking Above, we outlined a num</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>R. Malouf. 2002. A comparison of algorithms for maximum entropy parameter estimation. In Proceedings of the Sixth Conference on Natural Language Learning (CoNLL-2002), pages 49–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>M A Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of english: the penn treebank.</title>
<date>1993</date>
<journal>Comput. Linguist.,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="6485" citStr="Marcus et al., 1993" startWordPosition="1014" endWordPosition="1017">eans that the exhaustive parse forest can be represented compactly as a ‘packed forest’, and the top-ranked trees can be successively reconstructed, enabling faster parsing using less memory. 2.2 Semantic Generalisation for parse ranking Above, we outlined a number of reasons why semantic generalisation of lexemes could enable parsers to make more efficient use of training data, and indeed, there has been some prior work investigating this possibility. Agirre et al. (2008) applied two state-of-the-art treebank parsers to the sensetagged subset of the Brown corpus version of the Penn Treebank (Marcus et al., 1993), and added sense annotation to the training data to evaluate their impact on parse selection and specifically on PPattachment. The annotations they used were oracle sense annotations, automatic sense recognition and the first sense heuristic, and it was this last method which was the best performer in general. The sense annotations were either the WordNet synset ID or the coarse semantic file, which we explain in more detail below, and replaced the original tokens in the training data. The largest improvement in parsing F-score was a 6.9% reduction in error rate for the Bikel parser (Bikel, 2</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of english: the penn treebank. Comput. Linguist., 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
<author>R Koeling</author>
<author>J Weeds</author>
<author>J Carroll</author>
</authors>
<title>Finding predominant word senses in untagged text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>279</pages>
<contexts>
<context position="16899" citStr="McCarthy et al. (2004)" startWordPosition="2740" endWordPosition="2743">at it is compatible with the configuration in which the corpora were parsed – specifically, the unknown-word handling uses a version of the sentences tagged with TnT. We ignore multi-token named entity outputs from SuperSense Tagger, as these would introduce a confounding factor in our experiments and also reduce comparability of the results with the WNF method. 3.4.3 A distributional thesaurus method A final configuration attempts to avoid the need for curated resources such as WordNet, instead using an automatically-constructed distributional thesaurus (Lin, 1998). We use the thesaurus from McCarthy et al. (2004), constructed along these lines using the grammatical relations from RASP (Briscoe and Carroll, 2002) applied to 90 millions words of text from the British National Corpus. 231 root_frag np_frg_c hdn_bnp_c aj-hdn_norm_c legal_a1 n_pl_olr &amp;quot;legal&amp;quot; issue_n1 &amp;quot;issues&amp;quot; Figure 1: ERG derivation tree for the phrase Legal issues [n_-_c_le &amp;quot;issues&amp;quot;] [n_pl_olr n_-_c_le &amp;quot;issues&amp;quot;] [aj-hdn_norm_c n_pl_olr n_-_c_le &amp;quot;issues&amp;quot;] (a) Original features [n_-_c_le noun.cognition] [n_pl_olr n_-_c_le noun.cognition] [aj-hdn_norm_c n_pl_olr n_-_c_le noun.cognition] (b) Additional features in leaf mode, which augment th</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2004. Finding predominant word senses in untagged text. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, pages 279–es.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>WordNet: a lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="4174" citStr="Miller, 1995" startWordPosition="644" endWordPosition="645">t flora can often be modified by PPs relating to appendages of plants such as leaves. This is not necessarily applicable only to PP attachment, but may help in a range of other syntactic phenomena, such as distinguishing between complements and modifiers of verbs. 228 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 228–236, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics The synonyms or hypernyms could take the form of any grouping which relates word forms with semantic or syntactic commonality – such as a label from the WordNet (Miller, 1995) hierarchy, a subcategorisation frame (for verbs) or closely related terms from a distributional thesaurus (Lin, 1998). We present work here on using various levels of semantic generalisation as an attempt to improve parse selection accuracy with the English Resource Grammar (ERG: Flickinger (2000)), a precision HPSG-based grammar of English. 2 Related Work 2.1 Parse Selection for Precision Grammars The focus of this work is on parsing using handcrafted precision HPSG-based grammars, and in particular the ERG. While these grammars are carefully crafted to avoid overgeneration, the ambiguity of</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>G.A. Miller. 1995. WordNet: a lexical database for English. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oepen</author>
<author>D Flickinger</author>
<author>K Toutanova</author>
<author>C D Manning</author>
</authors>
<title>LinGO Redwoods: A rich and dynamic treebank for HPSG.</title>
<date>2004</date>
<journal>Research on Language &amp; Computation,</journal>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="9479" citStr="Oepen et al., 2004" startWordPosition="1500" endWordPosition="1503">e purely syntactic parse selection features based on the derivation tree of the sentence (see Section 3.4.3), rather than ranking using dependency triples, meaning that our method is in principle able to be integrated into a parser more easily, where the final set of dependencies would not be known in advance. Secondly, we do not use humancreated sense annotations, instead relying on heuristics or trained sense-taggers, which is closer to the reality of real-world parsing tasks. 3.1 Corpora Following MacKinlay et al. (2011), we use two primary training corpora. First, we use the LOGON corpus (Oepen et al., 2004), a collection of English translations of Norwegian hiking texts. The LOGON corpus contains 8550 sentences with exactly one gold parse, which we partitioned randomly by sentence into 10 approximately equal sections, reserving two sections as test data, and using the remainder as our training corpus. These sentences were randomly divided into training and development data. Secondly, we use the WeScience (Ytrestøl et al., 2009) corpus, a collection of Wikipedia articles related to computational linguistics. The corpus contains 11558 sentences, from which we randomly chose 9632, preserving the re</context>
</contexts>
<marker>Oepen, Flickinger, Toutanova, Manning, 2004</marker>
<rawString>S. Oepen, D. Flickinger, K. Toutanova, and C.D. Manning. 2004. LinGO Redwoods: A rich and dynamic treebank for HPSG. Research on Language &amp; Computation, 2(4):575–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Siegel</author>
<author>E M Bender</author>
</authors>
<title>Efficient deep processing of japanese.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd workshop on Asian language resources and international standardization-Volume 12,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="7363" citStr="Siegel and Bender, 2002" startWordPosition="1158" endWordPosition="1161"> this last method which was the best performer in general. The sense annotations were either the WordNet synset ID or the coarse semantic file, which we explain in more detail below, and replaced the original tokens in the training data. The largest improvement in parsing F-score was a 6.9% reduction in error rate for the Bikel parser (Bikel, 2002), boosting the F-score from 0.841 to 0.852, using the noun supersense only. More recently, Agirre et al. (2011) largely reproduced these results with a dependency parser. Fujita et al. (2007) add sense information to improve parse ranking with JaCy (Siegel and Bender, 2002), an HPSG-based grammar which uses similar machinery to the ERG. They use baseline syntactic features, and also add semantic features based on dependency triples extracted from the semantic representations of the sentence trees output by the parser. The dataset they use has human-assigned sense tags from a Japanese lexical hierarchy, which they use as a source of annotations. The dependency triples are modified in each feature set by replacing elements of the semantic triples with corresponding senses or hypernyms. In the best-performing configuration, they use both syntactic and semantic feat</context>
</contexts>
<marker>Siegel, Bender, 2002</marker>
<rawString>M. Siegel and E.M. Bender. 2002. Efficient deep processing of japanese. In Proceedings of the 3rd workshop on Asian language resources and international standardization-Volume 12, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Velldal</author>
</authors>
<title>Empirical Realization Ranking.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Oslo Department of Informatics.</institution>
<contexts>
<context position="5425" citStr="Velldal (2007)" startWordPosition="846" endWordPosition="847">ill unavoidably be multiple candidate parses licensed by the grammar for any non-trivial sentence. For the ERG, the number of parses postulated for a given sentence can be anywhere from zero to tens of thousands. It is the job of the parse selection model to select the best parse from all of these candidates as accurately as possible, for some definition of ‘best’, as we discuss in Section 3.2. Parse selection is usually performed by training discriminative parse selection models, which ‘discriminate’ between the set of all candidate parses. A widely-used method to achieve this is outlined in Velldal (2007). We feed both correct and incorrect parses licensed by the grammar to the TADM toolkit (Malouf, 2002), and learn a maximum entropy model. This method is used by Zhang et al. (2007) and MacKinlay et al. (2011) inter alia. One important implementation detail is that rather than exhaustively ranking all candidates out of possibly many thousands of trees, Zhang et al. (2007) showed that it was possible to use ‘selective unpacking’, which means that the exhaustive parse forest can be represented compactly as a ‘packed forest’, and the top-ranked trees can be successively reconstructed, enabling fa</context>
<context position="18773" citStr="Velldal (2007)" startWordPosition="3026" endWordPosition="3027">e are less than five), and use these new entries to create additional features, as well as adding a feature for the lemma itself in all cases. This method is denoted LDT for ‘Lin Distributional Thesaurus’. We note that many other methods could be used to select these, such as different numbers of synonyms, or dynamically changing the number of synonyms based on a threshold against the top similarity score, but this is not something we evaluate in this preliminary investigation. Adding Word Sense to Parse Selection Models We noted above that parse selection using the methodology established by Velldal (2007) uses human-annotated incorrect and correct derivation trees to train a maximum entropy parse selection model. More specifically, the model is trained using features extracted from the candidate HPSG derivation trees, using the labels of each node (which are the rule names from the grammar) and those of a limited number of ancestor nodes. As an example, we examine the noun phrase Legal issues from the WESCIENCE corpus, for which the correct ERG derivation tree is shown in Figure 1. Features are created by examining each node in the tree and at least its parent, with the feature name set to the</context>
</contexts>
<marker>Velldal, 2007</marker>
<rawString>E. Velldal. 2007. Empirical Realization Ranking. Ph.D. thesis, University of Oslo Department of Informatics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ytrestøl</author>
<author>D Flickinger</author>
<author>S Oepen</author>
</authors>
<title>Extracting and annotating Wikipedia sub-domains – towards a new eScience community resourc.</title>
<date>2009</date>
<booktitle>In Proceedings of the Seventh International Workshop on Treebanks and Linguistic Theories, Groeningen, The Netherlands,</booktitle>
<contexts>
<context position="9908" citStr="Ytrestøl et al., 2009" startWordPosition="1570" endWordPosition="1573">ich is closer to the reality of real-world parsing tasks. 3.1 Corpora Following MacKinlay et al. (2011), we use two primary training corpora. First, we use the LOGON corpus (Oepen et al., 2004), a collection of English translations of Norwegian hiking texts. The LOGON corpus contains 8550 sentences with exactly one gold parse, which we partitioned randomly by sentence into 10 approximately equal sections, reserving two sections as test data, and using the remainder as our training corpus. These sentences were randomly divided into training and development data. Secondly, we use the WeScience (Ytrestøl et al., 2009) corpus, a collection of Wikipedia articles related to computational linguistics. The corpus contains 11558 sentences, from which we randomly chose 9632, preserving the remainder for future work. This left 7631 sentences with a single gold tree, which we divided into a training set and a development set in the same way. The corpora are summarised in Table 1. With these corpora, we are able to investigate indomain and cross-domain effects, by testing on a different corpus to the training corpus, so we can examine whether sense-tagging alleviates the crossdomain performance penalty noted in MacK</context>
</contexts>
<marker>Ytrestøl, Flickinger, Oepen, 2009</marker>
<rawString>G. Ytrestøl, D. Flickinger, and S. Oepen. 2009. Extracting and annotating Wikipedia sub-domains – towards a new eScience community resourc. In Proceedings of the Seventh International Workshop on Treebanks and Linguistic Theories, Groeningen, The Netherlands, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>S Oepen</author>
<author>J Carroll</author>
</authors>
<title>Efficiency in unification-based n-best parsing.</title>
<date>2007</date>
<booktitle>In IWPT ’07: Proceedings of the 10th International Conference on Parsing Technologies,</booktitle>
<pages>48--59</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="5606" citStr="Zhang et al. (2007)" startWordPosition="878" endWordPosition="881">here from zero to tens of thousands. It is the job of the parse selection model to select the best parse from all of these candidates as accurately as possible, for some definition of ‘best’, as we discuss in Section 3.2. Parse selection is usually performed by training discriminative parse selection models, which ‘discriminate’ between the set of all candidate parses. A widely-used method to achieve this is outlined in Velldal (2007). We feed both correct and incorrect parses licensed by the grammar to the TADM toolkit (Malouf, 2002), and learn a maximum entropy model. This method is used by Zhang et al. (2007) and MacKinlay et al. (2011) inter alia. One important implementation detail is that rather than exhaustively ranking all candidates out of possibly many thousands of trees, Zhang et al. (2007) showed that it was possible to use ‘selective unpacking’, which means that the exhaustive parse forest can be represented compactly as a ‘packed forest’, and the top-ranked trees can be successively reconstructed, enabling faster parsing using less memory. 2.2 Semantic Generalisation for parse ranking Above, we outlined a number of reasons why semantic generalisation of lexemes could enable parsers to m</context>
</contexts>
<marker>Zhang, Oepen, Carroll, 2007</marker>
<rawString>Y. Zhang, S. Oepen, and J. Carroll. 2007. Efficiency in unification-based n-best parsing. In IWPT ’07: Proceedings of the 10th International Conference on Parsing Technologies, pages 48–59, Morristown, NJ, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>