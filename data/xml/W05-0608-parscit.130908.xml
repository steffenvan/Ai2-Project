<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000013">
<title confidence="0.919903">
Domain Kernels for Text Categorization
</title>
<note confidence="0.917360333333333">
Alfio Gliozzo and Carlo Strapparava
ITC-Irst
via Sommarive, I-38050, Trento, ITALY
</note>
<email confidence="0.992989">
{gliozzo,strappa}@itc.it
</email>
<sectionHeader confidence="0.996821" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9869308">
In this paper we propose and evaluate
a technique to perform semi-supervised
learning for Text Categorization. In
particular we defined a kernel function,
namely the Domain Kernel, that allowed
us to plug “external knowledge” into the
supervised learning process. External
knowledge is acquired from unlabeled
data in a totally unsupervised way, and it
is represented by means of Domain Mod-
els.
We evaluated the Domain Kernel in two
standard benchmarks for Text Categoriza-
tion with good results, and we compared
its performance with a kernel function that
exploits a standard bag-of-words feature
representation. The learning curves show
that the Domain Kernel allows us to re-
duce drastically the amount of training
data required for learning.
</bodyText>
<sectionHeader confidence="0.998885" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999599340909091">
Text Categorization (TC) deals with the problem of
assigning a set of category labels to documents. Cat-
egories are usually defined according to a variety
of topics (e.g. SPORT vs. POLITICS) and a set of
hand tagged examples is provided for training. In the
state-of-the-art TC settings supervised classifiers are
used for learning and texts are represented by means
of bag-of-words.
Even if, in principle, supervised approaches reach
the best performance in many Natural Language
Processing (NLP) tasks, in practice it is not always
easy to apply them to concrete applicative settings.
In fact, supervised systems for TC require to be
trained a large amount of hand tagged texts. This
situation is usually feasible only when there is some-
one (e.g. a big company) that can easily provide al-
ready classified documents to train the system.
In most of the cases this scenario is quite unprac-
tical, if not infeasible. An example is the task of
categorizing personal documents, in which the cate-
gories can be modified according to the user’s inter-
ests: new categories are often introduced and, pos-
sibly, the available labeled training for them is very
limited.
In the NLP literature the problem of providing
large amounts of manually annotated data is known
as the Knowledge Acquisition Bottleneck. Cur-
rent research in supervised approaches to NLP often
deals with defining methodologies and algorithms to
reduce the amount of human effort required for col-
lecting labeled examples.
A promising direction to solve this problem is to
provide unlabeled data together with labeled texts
to help supervision. In the Machine Learning lit-
erature this learning schema has been called semi-
supervised learning. It has been applied to the
TC problem using different techniques: co-training
(Blum and Mitchell, 1998), EM-algorithm (Nigam
et al., 2000), Transduptive SVM (Joachims, 1999b)
and Latent Semantic Indexing (Zelikovitz and Hirsh,
2001).
In this paper we propose a novel technique to per-
form semi-supervised learning for TC. The under-
lying idea behind our approach is that lexical co-
</bodyText>
<page confidence="0.968766">
56
</page>
<note confidence="0.960062">
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 56–63, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.99497056097561">
herence (i.e. co-occurence in texts of semantically
related terms) (Magnini et al., 2002) is an inherent
property of corpora, and it can be exploited to help a
supervised classifier to build a better categorization
hypothesis, even if the amount of labeled training
data provided for learning is very low.
Our proposal consists of defining a Domain
Kernel and exploiting it inside a Support Vector
Machine (SVM) classification framework for TC
(Joachims, 2002). The Domain Kernel relies on the
notion of Domain Model, which is a shallow repre-
sentation for lexical ambiguity and variability. Do-
main Models can be acquired in an unsupervised
way from unlabeled data, and then exploited to de-
fine a Domain Kernel (i.e. a generalized similarity
function among documents)&apos;.
We evaluated the Domain Kernel in two stan-
dard benchmarks for TC (i.e. Reuters and 20News-
groups), and we compared its performance with a
kernel function that exploits a more standard Bag-
of-Words (BoW) feature representation. The use of
the Domain Kernel got a significant improvement in
the learning curves of both tasks. In particular, there
is a notable increment of the recall, especially with
few learning examples. In addition, F1 measure in-
creases by 2.8 points in the Reuters task at full learn-
ing, achieving the state-of-the-art results.
The paper is structured as follows. Section 2 in-
troduces the notion of Domain Model and describes
an automatic acquisition technique based on Latent
Semantic Analysis (LSA). In Section 3 we illustrate
the SVM approach to TC, and we define a Domain
Kernel that exploits Domain Models to estimate sim-
ilarity among documents. In Section 4 the perfor-
mance of the Domain Kernel are compared with a
standard bag-of-words feature representation, show-
ing the improvements in the learning curves. Section
5 describes the previous attempts to exploit semi-
supervised learning for TC, while section 6 con-
cludes the paper and proposes some directions for
future research.
</bodyText>
<sectionHeader confidence="0.985635" genericHeader="introduction">
2 Domain Models
</sectionHeader>
<bodyText confidence="0.9996178">
The simplest methodology to estimate the similar-
ity among the topics of two texts is to represent
them by means of vectors in the Vector Space Model
(VSM), and to exploit the cosine similarity. More
formally, let T = {t1, t2, ... , t�} be a corpus, let
V = {w1, w2, ... , wk} be its vocabulary, let T be
the k x n term-by-document matrix representing T,
such that tij is the frequency of word wi into the text
tj. The VSM is a k-dimensional space Rk, in which
the text tj E T is represented by means of the vec-
tor t� such that the ith component of �tj is tij. The
similarity among two texts in the VSM is estimated
by computing the cosine.
However this approach does not deal well with
lexical variability and ambiguity. For example the
two sentences “he is affected by AIDS” and “HIV is
a virus” do not have any words in common. In the
VSM their similarity is zero because they have or-
thogonal vectors, even if the concepts they express
are very closely related. On the other hand, the sim-
ilarity between the two sentences “the laptop has
been infected by a virus” and “HIV is a virus” would
turn out very high, due to the ambiguity of the word
virus.
To overcome this problem we introduce the notion
of Domain Model (DM), and we show how to use it
in order to define a domain VSM, in which texts and
terms are represented in a uniform way.
A Domain Model is composed by soft clusters of
terms. Each cluster represents a semantic domain
(Gliozzo et al., 2004), i.e. a set of terms that often
co-occur in texts having similar topics. A Domain
Model is represented by a k x k&apos; rectangular matrix
D, containing the degree of association among terms
and domains, as illustrated in Table 1.
</bodyText>
<table confidence="0.7797384">
MEDICINE COMPUTER SCIENCE
HIV 1 0
AIDS 1 0
virus 0.5 0.5
laptop 0 1
</table>
<tableCaption confidence="0.855913857142857">
Table 1: Example of Domain Matrix
&apos;The idea of exploiting a Domain Kernel to help a super-
vised classification framework, has been profitably used also in
other NLP tasks such as word sense disambiguation (see for ex-
ample (Strapparava et al., 2004)).
Domain Models can be used to describe lexical
ambiguity and variability. Lexical ambiguity is rep-
</tableCaption>
<page confidence="0.99832">
57
</page>
<bodyText confidence="0.999882769230769">
resented by associating one term to more than one
domain, while variability is represented by associat-
ing different terms to the same domain. For example
the term virus is associated to both the domain
COMPUTER SCIENCE and the domain MEDICINE
(ambiguity) while the domain MEDICINE is associ-
ated to both the terms AIDS and HIV (variability).
More formally, let D = {D1, D2, ..., Dk0} be
a set of domains, such that k&apos; « k. A Domain
Model is fully defined by a k x k&apos; domain matrix
D representing in each cell di,z the domain rele-
vance of term wi with respect to the domain Dz.
The domain matrix D is used to define a function
</bodyText>
<equation confidence="0.917314">
D : Rk —* Rk0, that maps the vectors tj, expressed
�
</equation>
<bodyText confidence="0.9943305">
into the classical VSM, into the vectors t&apos;j in the do-
main VSM. D is defined by2
</bodyText>
<equation confidence="0.837509">
D(t�) = t�(IIDFD) = �t&apos; (1)
j
where IIDF is a diagonal matrix such that iIDF
i,i =
</equation>
<bodyText confidence="0.99704516">
IDF(wi), t� is represented as a row vector, and
IDF(wi) is the Inverse Document Frequency of wi.
Vectors in the domain VSM are called Domain
Vectors. Domain Vectors for texts are estimated by
exploiting formula 1, while the Domain Vector �w&apos;i,
corresponding to the word wi E V , is the ith row of
the domain matrix D. To be a valid domain matrix
such vectors should be normalized (i.e. (�w&apos;i, �w&apos;i) =
1).
In the Domain VSM the similarity among Domain
Vectors is estimated by taking into account second
order relations among terms. For example the simi-
larity of the two sentences “He is affected by AIDS”
and “HIV is a virus” is very high, because the terms
AIDS, HIV and virus are highly associated to the
domain MEDICINE.
In this work we propose the use of Latent Se-
mantic Analysis (LSA) (Deerwester et al., 1990) to
induce Domain Models from corpora. LSA is an
unsupervised technique for estimating the similar-
ity among texts and terms in a corpus. LSA is per-
formed by means of a Singular Value Decomposi-
tion (SVD) of the term-by-document matrix T de-
scribing the corpus. The SVD algorithm can be ex-
ploited to acquire a domain matrix D from a large
</bodyText>
<subsectionHeader confidence="0.368936666666667">
2In (Wong et al., 1985) a similar schema is adopted to define
a Generalized Vector Space Model, of which the Domain VSM
is a particular instance.
</subsectionHeader>
<bodyText confidence="0.998652875">
corpus T in a totally unsupervised way. SVD de-
composes the term-by-document matrix T into three
matrixes T — VEk0UT where Ek0 is the diagonal
k x k matrix containing the highest k&apos; « k eigen-
values of T, and all the remaining elements set to
0. The parameter k&apos; is the dimensionality of the Do-
main VSM and can be fixed in advance3. Under this
setting we define the domain matrix DLSA4 as
</bodyText>
<equation confidence="0.825102">
�
DLSA = INV Ek0 (2)
</equation>
<bodyText confidence="0.99707212">
where IN is a diagonal matrix such that iNi,i =
Kernel Methods are the state-of-the-art supervised
framework for learning, and they have been success-
fully adopted to approach the TC task (Joachims,
1999a).
The basic idea behind kernel methods is to embed
the data into a suitable feature space F via a map-
ping function 0 : X — F, and then use a linear
algorithm for discovering nonlinear patterns. Kernel
methods allow us to build a modular system, as the
kernel function acts as an interface between the data
and the learning algorithm. Thus the kernel function
becomes the only domain specific module of the sys-
tem, while the learning algorithm is a general pur-
pose component. Potentially a kernel function can
work with any kernel-based algorithm, such as for
example SVM.
During the learning phase SVMs assign a weight
Ai &gt; 0 to any example xi E X. All the labeled
instances xi such that Ai &gt; 0 are called support vec-
tors. The support vectors lie close to the best sepa-
rating hyper-plane between positive and negative ex-
amples. New examples are then assigned to the class
of its closest support vectors, according to equation
3.
</bodyText>
<footnote confidence="0.700235666666667">
3It is not clear how to choose the right dimensionality. In
our experiments we used 400 dimensions.
4When DLSA is substituted in Equation 1 the Domain VSM
</footnote>
<construct confidence="0.847760714285714">
is equivalent to a Latent Semantic Space (Deerwester et al.,
1990). The only difference in our formulation is that the vectors
representinmhe terms in the Domain VSM are normalized by
the matrix I , and then rescaled, according to their IDF value,
by matrix IIDF. Note the analogy with the tf idf term weighting
schema (Salton and McGill, 1983), widely adopted in Informa-
tion Retrieval.
</construct>
<figure confidence="0.893616">
V1 , wi&apos; is the ith row of the matrix V&apos;\/Ek0.
(~w0�,~w0�)
3 The Domain Kernel
</figure>
<page confidence="0.985731">
58
</page>
<sectionHeader confidence="0.980394" genericHeader="background">
4 Evaluation
</sectionHeader>
<equation confidence="0.9789">
n
f(x) = AiK(xi, x) + A0 (3)
i=1
</equation>
<bodyText confidence="0.999582176470588">
The kernel function K returns the similarity be-
tween two instances in the input space X, and can
be designed in order to capture the relevant aspects
to estimate similarity, just by taking care of satis-
fying set of formal requirements, as described in
(Sch¨olkopf and Smola, 2001).
In this paper we define the Domain Kernel and we
apply it to TC tasks. The Domain Kernel, denoted
by KD, can be exploited to estimate the topic simi-
larity among two texts while taking into account the
external knowledge provided by a Domain Model
(see section 2). It is a variation of the Latent Seman-
tic Kernel (Shawe-Taylor and Cristianini, 2004), in
which a Domain Model is exploited to define an ex-
plicit mapping D : Rk —* Rk&apos; from the classical
VSM into the domain VSM. The Domain Kernel is
defined by
</bodyText>
<equation confidence="0.998566666666667">
(D(ti), D(tl))
KD(ti, tl) = (4)
�(D(tl ), D(tl))(D(ti), D(ti))
</equation>
<bodyText confidence="0.993719689655173">
where D is the Domain Mapping defined in equa-
tion 1. To be fully defined, the Domain Kernel re-
quires a Domain Matrix D. In principle, D can be
acquired from any corpora by exploiting any (soft)
term clustering algorithm. Anyway, we belive that
adequate Domain Models for particular tasks can be
better acquired from collections of documents from
the same source. For this reason, for the experi-
ments reported in this paper, we acquired the matrix
DLSA, defined by equation 2, using the whole (un-
labeled) training corpora available for each task, so
tuning the Domain Model on the particular task in
which it will be applied.
A more traditional approach to measure topic sim-
ilarity among text consists of extracting BoW fea-
tures and to compare them in a vector space. The
BoW kernel, denoted by KBoW, is a particular case
of the Domain Kernel, in which D = I, and I is the
identity matrix. The BoW Kernel does not require
a Domain Model, so we can consider this setting
as “purely” supervised, in which no external knowl-
edge source is provided.
We compared the performance of both KD and
KBoW on two standard TC benchmarks. In sub-
section 4.1 we describe the evaluation tasks and the
preprocessing steps, in 4.2 we describe some algo-
rithmic details of the TC system adopted. Finally
in subsection 4.3 we compare the learning curves of
KD and KBoW.
</bodyText>
<subsectionHeader confidence="0.976433">
4.1 Text Categorization tasks
</subsectionHeader>
<bodyText confidence="0.99973428125">
For the experiments reported in this paper, we se-
lected two evaluation benchmarks typically used in
the TC literature (Sebastiani, 2002): the 20news-
groups and the Reuters corpora. In both the data sets
we tagged the texts for part of speech and we consid-
ered only the noun, verb, adjective, and adverb parts
of speech, representing them by vectors containing
the frequencies of each disambiguated lemma. The
only feature selection step we performed was to re-
move all the closed-class words from the document
index.
20newsgroups. The 20Newsgroups data set5 is
a collection of approximately 20,000 newsgroup
documents, partitioned (nearly) evenly across 20
different newsgroups. This collection has become
a popular data set for experiments in text appli-
cations of machine learning techniques, such as
text classification and text clustering. Some of
the newsgroups are very closely related to each
other (e.g. comp.sys.ibm.pc.hardware
/ comp.sys.mac.hardware), while others
are highly unrelated (e.g. misc.forsale /
soc.religion.christian). We removed
cross-posts (duplicates), newsgroup-identifying
headers (i.e. Xref, Newsgroups, Path, Followup-To,
Date), and empty documents from the original
corpus, so to obtain 18,941 documents. Then we
randomly divided it into training (80%) and test
(20%) sets, containing respectively 15,153 and
3,788 documents.
Reuters. We used the Reuters-21578 collec-
tion6, and we splitted it into training and test
</bodyText>
<footnote confidence="0.98085825">
5Available at http://www.ai.mit.edu-
/people/jrennie/20Newsgroups/.
6Available at http://kdd.ics.uci.edu/databases/-
reuters21578/reuters21578.html.
</footnote>
<page confidence="0.999361">
59
</page>
<bodyText confidence="0.9999842">
partitions according to the standard ModApt`e
split. It includes 12,902 documents for 90 cat-
egories, with a fixed splitting between training
and test data. We conducted our experiments by
considering only the 10 most frequent categories,
</bodyText>
<note confidence="0.641438">
i.e. Earn, Acquisition, Money-fx,
Grain, Crude, Trade, Interest,
</note>
<bodyText confidence="0.992570333333333">
Ship, Wheat and Corn, and we included in
our dataset all the non empty documents labeled
with at least one of those categories. Thus the final
dataset includes 9295 document, of which 6680 are
included in the training partition, and 2615 are in
the test set.
</bodyText>
<subsectionHeader confidence="0.985214">
4.2 Implementation details
</subsectionHeader>
<bodyText confidence="0.9996388">
As a supervised learning device, we used the SVM
implementation described in (Joachims, 1999a).
The Domain Kernel is implemented by defining an
explicit feature mapping according to formula 1, and
by normalizing each vector to obtain vectors of uni-
tary length. All the experiments have been per-
formed on the standard parameter settings, using a
linear kernel.
We acquired a different Domain Model for each
corpus by performing the SVD processes on the
term-by-document matrices representing the whole
training partitions, and we considered only the first
400 domains (i.e. k&apos; = 400)1.
As far as the Reuters task is concerned, the TC
problem has been approached as a set of binary fil-
tering problems, allowing the TC system to pro-
vide more than one category label to each document.
For the 20newsgroups task, we implemented a one-
versus-all classification schema, in order to assign a
single category to each news.
</bodyText>
<subsectionHeader confidence="0.996342">
4.3 Domain Kernel versus BoW Kernel
</subsectionHeader>
<bodyText confidence="0.970791272727273">
Figure 1 and Figure 2 report the learning curves for
both KD and KBoW, evaluated respectively on the
Reuters and the 20newgroups task. Results clearly
show that KD always outperforms KBoW, espe-
cially when very limited amount of labeled data is
provided for learning.
7To perform the SVD operation we adopted
LIBSVDC, an optimized package for sparse ma-
trix that allows to perform this step in few minutes
even for large corpora. It can be downloaded from
http://tedlab.mit.edu/∼dr/SVDLIBC/.
</bodyText>
<figure confidence="0.868009">
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Fraction of labeled training data
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Fraction of labeled training data
</figure>
<figureCaption confidence="0.916236">
Figure 2: Micro-F1 learning curves for 20news-
groups
</figureCaption>
<bodyText confidence="0.9948952">
Table 2 compares the performances of the two
kernels at full learning. KD achieves a better micro-
F1 than KBoW in both tasks. The improvement is
particularly significant in the Reuters task (+ 2.8 %).
Tables 3 shows the number of labeled examples
required by KD and KBoW to achieve the same
micro-F1 in the Reuters task. KD requires only
146 examples to obtain a micro-F1 of 0.84, while
KBoW requires 1380 examples to achieve the same
performance. In the same task, KD surpass the per-
formance of KBoW at full learning using only the
10% of the labeled data. The last column of the ta-
ble shows clearly that KD requires 90% less labeled
data than KBoW to achieve the same performances.
A similar behavior is reported in Table 4 for the
</bodyText>
<figure confidence="0.97659">
Domain Kernel
BoW Kernel
</figure>
<figureCaption confidence="0.976698">
Figure 1: Micro-F1 learning curves for Reuters
</figureCaption>
<figure confidence="0.999595318181818">
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
F1 measure
Domain Kernel
BoW Kernel
F1 measure 1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
</figure>
<page confidence="0.970421">
60
</page>
<table confidence="0.992418714285714">
F1 Domain Kernel Bow Kernel
Reuters 0.928 0.900
20newsgroups 0.886 0.880
F1 Domain Kernel Bow Kernel Ratio
.50 30 500 6%
.70 98 1182 8%
.85 2272 7879 29%
</table>
<tableCaption confidence="0.931631">
Table 2: Micro-F1 with full learning
</tableCaption>
<table confidence="0.9994335">
F1 Domain Kernel Bow Kernel Ratio
.54 14 267 5%
.84 146 1380 10%
.90 668 6680 10%
</table>
<tableCaption confidence="0.999565">
Table 3: Number of training examples needed by
</tableCaption>
<bodyText confidence="0.944809047619048">
KD and KBoW to reach the same micro-F1 on the
Reuters task
20newsgroups task. It is important to notice that the
number of labeled documents is higher in this corpus
than in the previous one. The benefits of using Do-
main Models are then less evident at full learning,
even if they are significant when very few labeled
data are provided.
Figures 3 and 4 report a more detailed analysis
by comparing the micro-precision and micro-recall
learning curves of both kernels in the Reuters task8.
It is clear from the graphs that the main contribute
of KD is about increasing recall, while precision is
similar in both cases9. This last result confirms our
hypothesis that the information provided by the Do-
main Models allows the system to generalize in a
more effective way over the training examples, al-
lowing to estimate the similarity among texts even if
they have just few words in common.
Finally, KD achieves the state-of-the-art in the
Reuters task, as reported in section 5.
</bodyText>
<sectionHeader confidence="0.999291" genericHeader="related work">
5 Related Works
</sectionHeader>
<bodyText confidence="0.9998916">
To our knowledge, the first attempt to apply the
semi-supervised learning schema to TC has been
reported in (Blum and Mitchell, 1998). Their co-
training algorithm was able to reduce significantly
the error rate, if compared to a strictly supervised
</bodyText>
<footnote confidence="0.484194285714286">
8For the 20-newsgroups task both micro-precision and
micro-recall are equal to micro-F1 because a single category
label has been assigned to every instance.
9It is worth noting that KD gets a F1 measure of 0.54 (Preci-
sion/Recall of 0.93/0.38) using just 14 training examples, sug-
gesting that it can be profitably exploited for a bootstrapping
process.
</footnote>
<tableCaption confidence="0.667501">
Table 4: Number of training examples needed by
KD and KBoW to reach the same micro-F1 on the
20newsgroups task
</tableCaption>
<table confidence="0.64143525">
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Fraction of labeled training data
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Fraction of labeled training data
</table>
<figureCaption confidence="0.992941">
Figure 4: Learning curves for Reuters (Recall)
</figureCaption>
<bodyText confidence="0.996378714285714">
classifier.
(Nigam et al., 2000) adopted an Expectation Max-
imization (EM) schema to deal with the same prob-
lem, evaluating extensively their approach on sev-
eral datasets. They compared their algorithm with
a standard probabilistic approach to TC, reporting
substantial improvements in the learning curve.
</bodyText>
<figure confidence="0.998650833333333">
Precision
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
1
Domain Kernel
BoW Kernel
</figure>
<figureCaption confidence="0.709952">
Figure 3: Learning curves for Reuters (Precision)
</figureCaption>
<figure confidence="0.999234923076923">
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
Recall
Domain Kernel
BoW Kernel
</figure>
<page confidence="0.994011">
61
</page>
<bodyText confidence="0.999963311111111">
A similar evaluation is also reported in (Joachims,
1999b), where a transduptive SVM is compared
to a state-of-the-art TC classifier based on SVM.
The semi-supervised approach obtained better re-
sults than the standard with few learning data, while
at full learning results seem to converge.
(Bekkerman et al., 2002) adopted a SVM classi-
fier in which texts have been represented by their as-
sociations to a set of Distributional Word Clusters.
Even if this approach is very similar to ours, it is not
a semi-supervised learning schema, because authors
did not exploit any additional unlabeled data to in-
duce word clusters.
In (Zelikovitz and Hirsh, 2001) background
knowledge (i.e. the unlabeled data) is exploited to-
gether with labeled data to estimate document sim-
ilarity in a Latent Semantic Space (Deerwester et
al., 1990). Their approach differs from the one pro-
posed in this paper because a different categoriza-
tion algorithm has been adopted. Authors compared
their algorithm with an EM schema (Nigam et al.,
2000) on the same dataset, reporting better results
only with very few labeled data, while EM performs
better with more training.
All the semi-supervised approaches in the liter-
ature reports better results than strictly supervised
ones with few learning, while with more data the
learning curves tend to converge.
A comparative evaluation among semi-supervised
TC algorithms is quite difficult, because the used
data sets, the preprocessing steps and the splitting
partitions adopted affect sensibly the final results.
Anyway, we reported the best F1 measure on the
Reuters corpus: to our knowledge, the state-of-the-
art on the 10 top most frequent categories of the
ModApte split at full learning is F1 92.0 (Bekker-
man et al., 2002) while we obtained 92.8. It is im-
portant to notice here that this results has been ob-
tained thanks to the improvements of the Domain
Kernel. In addition, on the 20newsgroups task, our
methods requires about 100 documents (i.e. five
documents per category) to achieve 70% F1, while
both EM (Nigam et al., 2000) and LSI (Zelikovitz
and Hirsh, 2001) requires more than 400 to achieve
the same performance.
</bodyText>
<sectionHeader confidence="0.985634" genericHeader="conclusions">
6 Conclusion and Future Works
</sectionHeader>
<bodyText confidence="0.999655931034483">
In this paper a novel technique to perform semi-
supervised learning for TC has been proposed and
evaluated. We defined a Domain Kernel that allows
us to improve the similarity estimation among docu-
ments by exploiting Domain Models. Domain Mod-
els are acquired from large collections of non anno-
tated texts in a totally unsupervised way.
An extensive evaluation on two standard bench-
marks shows that the Domain Kernel allows us to re-
duce drastically the amount of training data required
for learning. In particular the recall increases sen-
sibly, while preserving a very good accuracy. We
explained this phenomenon by showing that the sim-
ilarity scores evaluated by the Domain Kernel takes
into account both variability and ambiguity, being
able to estimate similarity even among texts that do
not have any word in common.
As future work, we plan to apply our semi-
supervised learning method to some concrete ap-
plicative scenarios, such as user modeling and cat-
egorization of personal documents in mail clients.
In addition, we are going deeper in the direction of
semi-supervised learning, by acquiring more com-
plex structures than clusters (e.g. synonymy, hyper-
onymy) to represent domain models. Furthermore,
we are working to adapt the general framework pro-
vided by the Domain Models to a multilingual sce-
nario, in order to apply the Domain Kernel to a Cross
Language TC task.
</bodyText>
<sectionHeader confidence="0.99843" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9979582">
This work has been partially supported by the ON-
TOTEXT (From Text to Knowledge for the Se-
mantic Web) project, funded by the Autonomous
Province of Trento under the FUP-2004 research
program.
</bodyText>
<sectionHeader confidence="0.999179" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999789125">
R. Bekkerman, R. El-Yaniv, N. Tishby, and Y. Win-
ter. 2002. Distributional word clusters vs. words for
text categorization. Journal of Machine Learning Re-
search, 1:1183–1208.
A. Blum and T. Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In COLT: Proceed-
ings of the Workshop on Computational Learning The-
ory, Morgan Kaufmann Publishers.
</reference>
<page confidence="0.980849">
62
</page>
<reference confidence="0.999828679245283">
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and
R. Harshman. 1990. Indexing by latent semantic anal-
ysis. Journal of the American Society of Information
Science.
A. Gliozzo, C. Strapparava, and I. Dagan. 2004. Unsu-
pervised and supervised exploitation of semantic do-
mains in lexical disambiguation. Computer Speech
and Language, 18:275–299.
T. Joachims. 1999a. Making large-scale SVM learning
practical. In B. Sch¨olkopf, C. Burges, and A. Smola,
editors, Advances in kernel methods: support vector
learning, chapter 11, pages 169 – 184. MIT Press,
Cambridge, MA, USA.
T. Joachims. 1999b. Transductive inference for text
classification using support vector machines. In Pro-
ceedings of ICML-99, 16th International Conference
on Machine Learning, pages 200–209. Morgan Kauf-
mann Publishers, San Francisco, US.
T. Joachims. 2002. Learning to Classify Text using Sup-
port Vector Machines. Kluwer Academic Publishers.
B. Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo.
2002. The role of domain information in word
sense disambiguation. Natural Language Engineer-
ing, 8(4):359–373.
K. Nigam, A. K. McCallum, S. Thrun, and T. M.
Mitchell. 2000. Text classification from labeled and
unlabeled documents using EM. Machine Learning,
39(2/3):103–134.
G. Salton and M.H. McGill. 1983. Introduction to mod-
ern information retrieval. McGraw-Hill, New York.
B. Sch¨olkopf and A. J. Smola. 2001. Learning with Ker-
nels. Support Vector Machines, Regularization, Opti-
mization, and Beyond. MIT Press.
F. Sebastiani. 2002. Machine learning in automated text
categorization. ACM Computing Surveys, 34(1):1–47.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel Meth-
ods for Pattern Analysis. Cambridge University Press.
C. Strapparava, A. Gliozzo, and C. Giuliano. 2004. Pat-
tern abstraction and term similarity for word sense
disambiguation: Irst at senseval-3. In Proc. of
SENSEVAL-3 Third International Workshop on Eval-
uation of Systems for the Semantic Analysis of Text,
pages 229–234, Barcelona, Spain, July.
S.K.M. Wong, W. Ziarko, and P.C.N. Wong. 1985. Gen-
eralized vector space model in information retrieval.
In Proceedings of the 81h ACM SIGIR Conference.
S. Zelikovitz and H. Hirsh. 2001. Using LSI for text clas-
sification in the presence of background text. In Hen-
rique Paques, Ling Liu, and David Grossman, editors,
Proceedings of CIKM-01, 10th ACM International
Conference on Information and Knowledge Manage-
ment, pages 113–118, Atlanta, US. ACM Press, New
York, US.
</reference>
<page confidence="0.999464">
63
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.400810">
<title confidence="0.998525">Domain Kernels for Text Categorization</title>
<author confidence="0.925109">Alfio Gliozzo</author>
<author confidence="0.925109">Carlo</author>
<affiliation confidence="0.433744">ITC-Irst</affiliation>
<address confidence="0.633598">via Sommarive, I-38050, Trento, ITALY</address>
<abstract confidence="0.999437142857143">In this paper we propose and a technique to perform semi-supervised learning for Text Categorization. particular we defined a kernel function, namely the Domain Kernel, that allowed us to plug “external knowledge” into the supervised learning process. External knowledge is acquired from unlabeled data in a totally unsupervised way, and it is represented by means of Domain Models. We evaluated the Domain Kernel in two standard benchmarks for Text Categorization with good results, and we compared its performance with a kernel function that exploits a standard bag-of-words feature representation. The learning curves show that the Domain Kernel allows us to reduce drastically the amount of training data required for learning.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Bekkerman</author>
<author>R El-Yaniv</author>
<author>N Tishby</author>
<author>Y Winter</author>
</authors>
<title>Distributional word clusters vs. words for text categorization.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>1--1183</pages>
<contexts>
<context position="21615" citStr="Bekkerman et al., 2002" startWordPosition="3666" endWordPosition="3669">r algorithm with a standard probabilistic approach to TC, reporting substantial improvements in the learning curve. Precision 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 1 Domain Kernel BoW Kernel Figure 3: Learning curves for Reuters (Precision) 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 Recall Domain Kernel BoW Kernel 61 A similar evaluation is also reported in (Joachims, 1999b), where a transduptive SVM is compared to a state-of-the-art TC classifier based on SVM. The semi-supervised approach obtained better results than the standard with few learning data, while at full learning results seem to converge. (Bekkerman et al., 2002) adopted a SVM classifier in which texts have been represented by their associations to a set of Distributional Word Clusters. Even if this approach is very similar to ours, it is not a semi-supervised learning schema, because authors did not exploit any additional unlabeled data to induce word clusters. In (Zelikovitz and Hirsh, 2001) background knowledge (i.e. the unlabeled data) is exploited together with labeled data to estimate document similarity in a Latent Semantic Space (Deerwester et al., 1990). Their approach differs from the one proposed in this paper because a different categoriza</context>
<context position="23046" citStr="Bekkerman et al., 2002" startWordPosition="3896" endWordPosition="3900"> with more training. All the semi-supervised approaches in the literature reports better results than strictly supervised ones with few learning, while with more data the learning curves tend to converge. A comparative evaluation among semi-supervised TC algorithms is quite difficult, because the used data sets, the preprocessing steps and the splitting partitions adopted affect sensibly the final results. Anyway, we reported the best F1 measure on the Reuters corpus: to our knowledge, the state-of-theart on the 10 top most frequent categories of the ModApte split at full learning is F1 92.0 (Bekkerman et al., 2002) while we obtained 92.8. It is important to notice here that this results has been obtained thanks to the improvements of the Domain Kernel. In addition, on the 20newsgroups task, our methods requires about 100 documents (i.e. five documents per category) to achieve 70% F1, while both EM (Nigam et al., 2000) and LSI (Zelikovitz and Hirsh, 2001) requires more than 400 to achieve the same performance. 6 Conclusion and Future Works In this paper a novel technique to perform semisupervised learning for TC has been proposed and evaluated. We defined a Domain Kernel that allows us to improve the sim</context>
</contexts>
<marker>Bekkerman, El-Yaniv, Tishby, Winter, 2002</marker>
<rawString>R. Bekkerman, R. El-Yaniv, N. Tishby, and Y. Winter. 2002. Distributional word clusters vs. words for text categorization. Journal of Machine Learning Research, 1:1183–1208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>T Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In COLT: Proceedings of the Workshop on Computational Learning Theory,</booktitle>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="2713" citStr="Blum and Mitchell, 1998" startWordPosition="420" endWordPosition="423">re the problem of providing large amounts of manually annotated data is known as the Knowledge Acquisition Bottleneck. Current research in supervised approaches to NLP often deals with defining methodologies and algorithms to reduce the amount of human effort required for collecting labeled examples. A promising direction to solve this problem is to provide unlabeled data together with labeled texts to help supervision. In the Machine Learning literature this learning schema has been called semisupervised learning. It has been applied to the TC problem using different techniques: co-training (Blum and Mitchell, 1998), EM-algorithm (Nigam et al., 2000), Transduptive SVM (Joachims, 1999b) and Latent Semantic Indexing (Zelikovitz and Hirsh, 2001). In this paper we propose a novel technique to perform semi-supervised learning for TC. The underlying idea behind our approach is that lexical co56 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), pages 56–63, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics herence (i.e. co-occurence in texts of semantically related terms) (Magnini et al., 2002) is an inherent property of corpora, and it can be exploited t</context>
<context position="20030" citStr="Blum and Mitchell, 1998" startWordPosition="3402" endWordPosition="3405">from the graphs that the main contribute of KD is about increasing recall, while precision is similar in both cases9. This last result confirms our hypothesis that the information provided by the Domain Models allows the system to generalize in a more effective way over the training examples, allowing to estimate the similarity among texts even if they have just few words in common. Finally, KD achieves the state-of-the-art in the Reuters task, as reported in section 5. 5 Related Works To our knowledge, the first attempt to apply the semi-supervised learning schema to TC has been reported in (Blum and Mitchell, 1998). Their cotraining algorithm was able to reduce significantly the error rate, if compared to a strictly supervised 8For the 20-newsgroups task both micro-precision and micro-recall are equal to micro-F1 because a single category label has been assigned to every instance. 9It is worth noting that KD gets a F1 measure of 0.54 (Precision/Recall of 0.93/0.38) using just 14 training examples, suggesting that it can be profitably exploited for a bootstrapping process. Table 4: Number of training examples needed by KD and KBoW to reach the same micro-F1 on the 20newsgroups task 0 0.1 0.2 0.3 0.4 0.5 </context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>A. Blum and T. Mitchell. 1998. Combining labeled and unlabeled data with co-training. In COLT: Proceedings of the Workshop on Computational Learning Theory, Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S Dumais</author>
<author>G Furnas</author>
<author>T Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society of Information Science.</journal>
<contexts>
<context position="8913" citStr="Deerwester et al., 1990" startWordPosition="1509" endWordPosition="1512">y exploiting formula 1, while the Domain Vector �w&apos;i, corresponding to the word wi E V , is the ith row of the domain matrix D. To be a valid domain matrix such vectors should be normalized (i.e. (�w&apos;i, �w&apos;i) = 1). In the Domain VSM the similarity among Domain Vectors is estimated by taking into account second order relations among terms. For example the similarity of the two sentences “He is affected by AIDS” and “HIV is a virus” is very high, because the terms AIDS, HIV and virus are highly associated to the domain MEDICINE. In this work we propose the use of Latent Semantic Analysis (LSA) (Deerwester et al., 1990) to induce Domain Models from corpora. LSA is an unsupervised technique for estimating the similarity among texts and terms in a corpus. LSA is performed by means of a Singular Value Decomposition (SVD) of the term-by-document matrix T describing the corpus. The SVD algorithm can be exploited to acquire a domain matrix D from a large 2In (Wong et al., 1985) a similar schema is adopted to define a Generalized Vector Space Model, of which the Domain VSM is a particular instance. corpus T in a totally unsupervised way. SVD decomposes the term-by-document matrix T into three matrixes T — VEk0UT wh</context>
<context position="11157" citStr="Deerwester et al., 1990" startWordPosition="1913" endWordPosition="1916">nel-based algorithm, such as for example SVM. During the learning phase SVMs assign a weight Ai &gt; 0 to any example xi E X. All the labeled instances xi such that Ai &gt; 0 are called support vectors. The support vectors lie close to the best separating hyper-plane between positive and negative examples. New examples are then assigned to the class of its closest support vectors, according to equation 3. 3It is not clear how to choose the right dimensionality. In our experiments we used 400 dimensions. 4When DLSA is substituted in Equation 1 the Domain VSM is equivalent to a Latent Semantic Space (Deerwester et al., 1990). The only difference in our formulation is that the vectors representinmhe terms in the Domain VSM are normalized by the matrix I , and then rescaled, according to their IDF value, by matrix IIDF. Note the analogy with the tf idf term weighting schema (Salton and McGill, 1983), widely adopted in Information Retrieval. V1 , wi&apos; is the ith row of the matrix V&apos;\/Ek0. (~w0�,~w0�) 3 The Domain Kernel 58 4 Evaluation n f(x) = AiK(xi, x) + A0 (3) i=1 The kernel function K returns the similarity between two instances in the input space X, and can be designed in order to capture the relevant aspects t</context>
<context position="22124" citStr="Deerwester et al., 1990" startWordPosition="3750" endWordPosition="3753">than the standard with few learning data, while at full learning results seem to converge. (Bekkerman et al., 2002) adopted a SVM classifier in which texts have been represented by their associations to a set of Distributional Word Clusters. Even if this approach is very similar to ours, it is not a semi-supervised learning schema, because authors did not exploit any additional unlabeled data to induce word clusters. In (Zelikovitz and Hirsh, 2001) background knowledge (i.e. the unlabeled data) is exploited together with labeled data to estimate document similarity in a Latent Semantic Space (Deerwester et al., 1990). Their approach differs from the one proposed in this paper because a different categorization algorithm has been adopted. Authors compared their algorithm with an EM schema (Nigam et al., 2000) on the same dataset, reporting better results only with very few labeled data, while EM performs better with more training. All the semi-supervised approaches in the literature reports better results than strictly supervised ones with few learning, while with more data the learning curves tend to converge. A comparative evaluation among semi-supervised TC algorithms is quite difficult, because the use</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society of Information Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gliozzo</author>
<author>C Strapparava</author>
<author>I Dagan</author>
</authors>
<title>Unsupervised and supervised exploitation of semantic domains in lexical disambiguation. Computer Speech and Language,</title>
<date>2004</date>
<pages>18--275</pages>
<contexts>
<context position="6612" citStr="Gliozzo et al., 2004" startWordPosition="1085" endWordPosition="1088">M their similarity is zero because they have orthogonal vectors, even if the concepts they express are very closely related. On the other hand, the similarity between the two sentences “the laptop has been infected by a virus” and “HIV is a virus” would turn out very high, due to the ambiguity of the word virus. To overcome this problem we introduce the notion of Domain Model (DM), and we show how to use it in order to define a domain VSM, in which texts and terms are represented in a uniform way. A Domain Model is composed by soft clusters of terms. Each cluster represents a semantic domain (Gliozzo et al., 2004), i.e. a set of terms that often co-occur in texts having similar topics. A Domain Model is represented by a k x k&apos; rectangular matrix D, containing the degree of association among terms and domains, as illustrated in Table 1. MEDICINE COMPUTER SCIENCE HIV 1 0 AIDS 1 0 virus 0.5 0.5 laptop 0 1 Table 1: Example of Domain Matrix &apos;The idea of exploiting a Domain Kernel to help a supervised classification framework, has been profitably used also in other NLP tasks such as word sense disambiguation (see for example (Strapparava et al., 2004)). Domain Models can be used to describe lexical ambiguity</context>
</contexts>
<marker>Gliozzo, Strapparava, Dagan, 2004</marker>
<rawString>A. Gliozzo, C. Strapparava, and I. Dagan. 2004. Unsupervised and supervised exploitation of semantic domains in lexical disambiguation. Computer Speech and Language, 18:275–299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>Advances in kernel methods: support vector learning, chapter 11,</booktitle>
<pages>169--184</pages>
<editor>In B. Sch¨olkopf, C. Burges, and A. Smola, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="2782" citStr="Joachims, 1999" startWordPosition="431" endWordPosition="432">s the Knowledge Acquisition Bottleneck. Current research in supervised approaches to NLP often deals with defining methodologies and algorithms to reduce the amount of human effort required for collecting labeled examples. A promising direction to solve this problem is to provide unlabeled data together with labeled texts to help supervision. In the Machine Learning literature this learning schema has been called semisupervised learning. It has been applied to the TC problem using different techniques: co-training (Blum and Mitchell, 1998), EM-algorithm (Nigam et al., 2000), Transduptive SVM (Joachims, 1999b) and Latent Semantic Indexing (Zelikovitz and Hirsh, 2001). In this paper we propose a novel technique to perform semi-supervised learning for TC. The underlying idea behind our approach is that lexical co56 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), pages 56–63, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics herence (i.e. co-occurence in texts of semantically related terms) (Magnini et al., 2002) is an inherent property of corpora, and it can be exploited to help a supervised classifier to build a better categorization hypot</context>
<context position="10005" citStr="Joachims, 1999" startWordPosition="1711" endWordPosition="1712">ce. corpus T in a totally unsupervised way. SVD decomposes the term-by-document matrix T into three matrixes T — VEk0UT where Ek0 is the diagonal k x k matrix containing the highest k&apos; « k eigenvalues of T, and all the remaining elements set to 0. The parameter k&apos; is the dimensionality of the Domain VSM and can be fixed in advance3. Under this setting we define the domain matrix DLSA4 as � DLSA = INV Ek0 (2) where IN is a diagonal matrix such that iNi,i = Kernel Methods are the state-of-the-art supervised framework for learning, and they have been successfully adopted to approach the TC task (Joachims, 1999a). The basic idea behind kernel methods is to embed the data into a suitable feature space F via a mapping function 0 : X — F, and then use a linear algorithm for discovering nonlinear patterns. Kernel methods allow us to build a modular system, as the kernel function acts as an interface between the data and the learning algorithm. Thus the kernel function becomes the only domain specific module of the system, while the learning algorithm is a general purpose component. Potentially a kernel function can work with any kernel-based algorithm, such as for example SVM. During the learning phase </context>
<context position="16110" citStr="Joachims, 1999" startWordPosition="2718" endWordPosition="2719">,902 documents for 90 categories, with a fixed splitting between training and test data. We conducted our experiments by considering only the 10 most frequent categories, i.e. Earn, Acquisition, Money-fx, Grain, Crude, Trade, Interest, Ship, Wheat and Corn, and we included in our dataset all the non empty documents labeled with at least one of those categories. Thus the final dataset includes 9295 document, of which 6680 are included in the training partition, and 2615 are in the test set. 4.2 Implementation details As a supervised learning device, we used the SVM implementation described in (Joachims, 1999a). The Domain Kernel is implemented by defining an explicit feature mapping according to formula 1, and by normalizing each vector to obtain vectors of unitary length. All the experiments have been performed on the standard parameter settings, using a linear kernel. We acquired a different Domain Model for each corpus by performing the SVD processes on the term-by-document matrices representing the whole training partitions, and we considered only the first 400 domains (i.e. k&apos; = 400)1. As far as the Reuters task is concerned, the TC problem has been approached as a set of binary filtering pr</context>
<context position="21356" citStr="Joachims, 1999" startWordPosition="3628" endWordPosition="3629">training data Figure 4: Learning curves for Reuters (Recall) classifier. (Nigam et al., 2000) adopted an Expectation Maximization (EM) schema to deal with the same problem, evaluating extensively their approach on several datasets. They compared their algorithm with a standard probabilistic approach to TC, reporting substantial improvements in the learning curve. Precision 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 1 Domain Kernel BoW Kernel Figure 3: Learning curves for Reuters (Precision) 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 Recall Domain Kernel BoW Kernel 61 A similar evaluation is also reported in (Joachims, 1999b), where a transduptive SVM is compared to a state-of-the-art TC classifier based on SVM. The semi-supervised approach obtained better results than the standard with few learning data, while at full learning results seem to converge. (Bekkerman et al., 2002) adopted a SVM classifier in which texts have been represented by their associations to a set of Distributional Word Clusters. Even if this approach is very similar to ours, it is not a semi-supervised learning schema, because authors did not exploit any additional unlabeled data to induce word clusters. In (Zelikovitz and Hirsh, 2001) bac</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999a. Making large-scale SVM learning practical. In B. Sch¨olkopf, C. Burges, and A. Smola, editors, Advances in kernel methods: support vector learning, chapter 11, pages 169 – 184. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Transductive inference for text classification using support vector machines.</title>
<date>1999</date>
<booktitle>In Proceedings of ICML-99, 16th International Conference on Machine Learning,</booktitle>
<pages>200--209</pages>
<publisher>Morgan Kaufmann Publishers,</publisher>
<location>San Francisco, US.</location>
<contexts>
<context position="2782" citStr="Joachims, 1999" startWordPosition="431" endWordPosition="432">s the Knowledge Acquisition Bottleneck. Current research in supervised approaches to NLP often deals with defining methodologies and algorithms to reduce the amount of human effort required for collecting labeled examples. A promising direction to solve this problem is to provide unlabeled data together with labeled texts to help supervision. In the Machine Learning literature this learning schema has been called semisupervised learning. It has been applied to the TC problem using different techniques: co-training (Blum and Mitchell, 1998), EM-algorithm (Nigam et al., 2000), Transduptive SVM (Joachims, 1999b) and Latent Semantic Indexing (Zelikovitz and Hirsh, 2001). In this paper we propose a novel technique to perform semi-supervised learning for TC. The underlying idea behind our approach is that lexical co56 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), pages 56–63, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics herence (i.e. co-occurence in texts of semantically related terms) (Magnini et al., 2002) is an inherent property of corpora, and it can be exploited to help a supervised classifier to build a better categorization hypot</context>
<context position="10005" citStr="Joachims, 1999" startWordPosition="1711" endWordPosition="1712">ce. corpus T in a totally unsupervised way. SVD decomposes the term-by-document matrix T into three matrixes T — VEk0UT where Ek0 is the diagonal k x k matrix containing the highest k&apos; « k eigenvalues of T, and all the remaining elements set to 0. The parameter k&apos; is the dimensionality of the Domain VSM and can be fixed in advance3. Under this setting we define the domain matrix DLSA4 as � DLSA = INV Ek0 (2) where IN is a diagonal matrix such that iNi,i = Kernel Methods are the state-of-the-art supervised framework for learning, and they have been successfully adopted to approach the TC task (Joachims, 1999a). The basic idea behind kernel methods is to embed the data into a suitable feature space F via a mapping function 0 : X — F, and then use a linear algorithm for discovering nonlinear patterns. Kernel methods allow us to build a modular system, as the kernel function acts as an interface between the data and the learning algorithm. Thus the kernel function becomes the only domain specific module of the system, while the learning algorithm is a general purpose component. Potentially a kernel function can work with any kernel-based algorithm, such as for example SVM. During the learning phase </context>
<context position="16110" citStr="Joachims, 1999" startWordPosition="2718" endWordPosition="2719">,902 documents for 90 categories, with a fixed splitting between training and test data. We conducted our experiments by considering only the 10 most frequent categories, i.e. Earn, Acquisition, Money-fx, Grain, Crude, Trade, Interest, Ship, Wheat and Corn, and we included in our dataset all the non empty documents labeled with at least one of those categories. Thus the final dataset includes 9295 document, of which 6680 are included in the training partition, and 2615 are in the test set. 4.2 Implementation details As a supervised learning device, we used the SVM implementation described in (Joachims, 1999a). The Domain Kernel is implemented by defining an explicit feature mapping according to formula 1, and by normalizing each vector to obtain vectors of unitary length. All the experiments have been performed on the standard parameter settings, using a linear kernel. We acquired a different Domain Model for each corpus by performing the SVD processes on the term-by-document matrices representing the whole training partitions, and we considered only the first 400 domains (i.e. k&apos; = 400)1. As far as the Reuters task is concerned, the TC problem has been approached as a set of binary filtering pr</context>
<context position="21356" citStr="Joachims, 1999" startWordPosition="3628" endWordPosition="3629">training data Figure 4: Learning curves for Reuters (Recall) classifier. (Nigam et al., 2000) adopted an Expectation Maximization (EM) schema to deal with the same problem, evaluating extensively their approach on several datasets. They compared their algorithm with a standard probabilistic approach to TC, reporting substantial improvements in the learning curve. Precision 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 1 Domain Kernel BoW Kernel Figure 3: Learning curves for Reuters (Precision) 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 Recall Domain Kernel BoW Kernel 61 A similar evaluation is also reported in (Joachims, 1999b), where a transduptive SVM is compared to a state-of-the-art TC classifier based on SVM. The semi-supervised approach obtained better results than the standard with few learning data, while at full learning results seem to converge. (Bekkerman et al., 2002) adopted a SVM classifier in which texts have been represented by their associations to a set of Distributional Word Clusters. Even if this approach is very similar to ours, it is not a semi-supervised learning schema, because authors did not exploit any additional unlabeled data to induce word clusters. In (Zelikovitz and Hirsh, 2001) bac</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999b. Transductive inference for text classification using support vector machines. In Proceedings of ICML-99, 16th International Conference on Machine Learning, pages 200–209. Morgan Kaufmann Publishers, San Francisco, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Learning to Classify Text using Support Vector Machines.</title>
<date>2002</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="3622" citStr="Joachims, 2002" startWordPosition="560" endWordPosition="561">the 9th Conference on Computational Natural Language Learning (CoNLL), pages 56–63, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics herence (i.e. co-occurence in texts of semantically related terms) (Magnini et al., 2002) is an inherent property of corpora, and it can be exploited to help a supervised classifier to build a better categorization hypothesis, even if the amount of labeled training data provided for learning is very low. Our proposal consists of defining a Domain Kernel and exploiting it inside a Support Vector Machine (SVM) classification framework for TC (Joachims, 2002). The Domain Kernel relies on the notion of Domain Model, which is a shallow representation for lexical ambiguity and variability. Domain Models can be acquired in an unsupervised way from unlabeled data, and then exploited to define a Domain Kernel (i.e. a generalized similarity function among documents)&apos;. We evaluated the Domain Kernel in two standard benchmarks for TC (i.e. Reuters and 20Newsgroups), and we compared its performance with a kernel function that exploits a more standard Bagof-Words (BoW) feature representation. The use of the Domain Kernel got a significant improvement in the </context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>T. Joachims. 2002. Learning to Classify Text using Support Vector Machines. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Magnini</author>
<author>C Strapparava</author>
<author>G Pezzulo</author>
<author>A Gliozzo</author>
</authors>
<title>The role of domain information in word sense disambiguation.</title>
<date>2002</date>
<journal>Natural Language Engineering,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="3251" citStr="Magnini et al., 2002" startWordPosition="499" endWordPosition="502">to the TC problem using different techniques: co-training (Blum and Mitchell, 1998), EM-algorithm (Nigam et al., 2000), Transduptive SVM (Joachims, 1999b) and Latent Semantic Indexing (Zelikovitz and Hirsh, 2001). In this paper we propose a novel technique to perform semi-supervised learning for TC. The underlying idea behind our approach is that lexical co56 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), pages 56–63, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics herence (i.e. co-occurence in texts of semantically related terms) (Magnini et al., 2002) is an inherent property of corpora, and it can be exploited to help a supervised classifier to build a better categorization hypothesis, even if the amount of labeled training data provided for learning is very low. Our proposal consists of defining a Domain Kernel and exploiting it inside a Support Vector Machine (SVM) classification framework for TC (Joachims, 2002). The Domain Kernel relies on the notion of Domain Model, which is a shallow representation for lexical ambiguity and variability. Domain Models can be acquired in an unsupervised way from unlabeled data, and then exploited to de</context>
</contexts>
<marker>Magnini, Strapparava, Pezzulo, Gliozzo, 2002</marker>
<rawString>B. Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo. 2002. The role of domain information in word sense disambiguation. Natural Language Engineering, 8(4):359–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Nigam</author>
<author>A K McCallum</author>
<author>S Thrun</author>
<author>T M Mitchell</author>
</authors>
<title>Text classification from labeled and unlabeled documents using EM.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="2748" citStr="Nigam et al., 2000" startWordPosition="425" endWordPosition="428">s of manually annotated data is known as the Knowledge Acquisition Bottleneck. Current research in supervised approaches to NLP often deals with defining methodologies and algorithms to reduce the amount of human effort required for collecting labeled examples. A promising direction to solve this problem is to provide unlabeled data together with labeled texts to help supervision. In the Machine Learning literature this learning schema has been called semisupervised learning. It has been applied to the TC problem using different techniques: co-training (Blum and Mitchell, 1998), EM-algorithm (Nigam et al., 2000), Transduptive SVM (Joachims, 1999b) and Latent Semantic Indexing (Zelikovitz and Hirsh, 2001). In this paper we propose a novel technique to perform semi-supervised learning for TC. The underlying idea behind our approach is that lexical co56 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), pages 56–63, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics herence (i.e. co-occurence in texts of semantically related terms) (Magnini et al., 2002) is an inherent property of corpora, and it can be exploited to help a supervised classifier to b</context>
<context position="20835" citStr="Nigam et al., 2000" startWordPosition="3540" endWordPosition="3543">qual to micro-F1 because a single category label has been assigned to every instance. 9It is worth noting that KD gets a F1 measure of 0.54 (Precision/Recall of 0.93/0.38) using just 14 training examples, suggesting that it can be profitably exploited for a bootstrapping process. Table 4: Number of training examples needed by KD and KBoW to reach the same micro-F1 on the 20newsgroups task 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Fraction of labeled training data 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Fraction of labeled training data Figure 4: Learning curves for Reuters (Recall) classifier. (Nigam et al., 2000) adopted an Expectation Maximization (EM) schema to deal with the same problem, evaluating extensively their approach on several datasets. They compared their algorithm with a standard probabilistic approach to TC, reporting substantial improvements in the learning curve. Precision 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 1 Domain Kernel BoW Kernel Figure 3: Learning curves for Reuters (Precision) 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 Recall Domain Kernel BoW Kernel 61 A similar evaluation is also reported in (Joachims, 1999b), where a transduptive SVM is compared to a state-of-the-art TC classifier ba</context>
<context position="22319" citStr="Nigam et al., 2000" startWordPosition="3782" endWordPosition="3785">to a set of Distributional Word Clusters. Even if this approach is very similar to ours, it is not a semi-supervised learning schema, because authors did not exploit any additional unlabeled data to induce word clusters. In (Zelikovitz and Hirsh, 2001) background knowledge (i.e. the unlabeled data) is exploited together with labeled data to estimate document similarity in a Latent Semantic Space (Deerwester et al., 1990). Their approach differs from the one proposed in this paper because a different categorization algorithm has been adopted. Authors compared their algorithm with an EM schema (Nigam et al., 2000) on the same dataset, reporting better results only with very few labeled data, while EM performs better with more training. All the semi-supervised approaches in the literature reports better results than strictly supervised ones with few learning, while with more data the learning curves tend to converge. A comparative evaluation among semi-supervised TC algorithms is quite difficult, because the used data sets, the preprocessing steps and the splitting partitions adopted affect sensibly the final results. Anyway, we reported the best F1 measure on the Reuters corpus: to our knowledge, the s</context>
</contexts>
<marker>Nigam, McCallum, Thrun, Mitchell, 2000</marker>
<rawString>K. Nigam, A. K. McCallum, S. Thrun, and T. M. Mitchell. 2000. Text classification from labeled and unlabeled documents using EM. Machine Learning, 39(2/3):103–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M H McGill</author>
</authors>
<title>Introduction to modern information retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill,</publisher>
<location>New York.</location>
<contexts>
<context position="11435" citStr="Salton and McGill, 1983" startWordPosition="1961" endWordPosition="1964"> and negative examples. New examples are then assigned to the class of its closest support vectors, according to equation 3. 3It is not clear how to choose the right dimensionality. In our experiments we used 400 dimensions. 4When DLSA is substituted in Equation 1 the Domain VSM is equivalent to a Latent Semantic Space (Deerwester et al., 1990). The only difference in our formulation is that the vectors representinmhe terms in the Domain VSM are normalized by the matrix I , and then rescaled, according to their IDF value, by matrix IIDF. Note the analogy with the tf idf term weighting schema (Salton and McGill, 1983), widely adopted in Information Retrieval. V1 , wi&apos; is the ith row of the matrix V&apos;\/Ek0. (~w0�,~w0�) 3 The Domain Kernel 58 4 Evaluation n f(x) = AiK(xi, x) + A0 (3) i=1 The kernel function K returns the similarity between two instances in the input space X, and can be designed in order to capture the relevant aspects to estimate similarity, just by taking care of satisfying set of formal requirements, as described in (Sch¨olkopf and Smola, 2001). In this paper we define the Domain Kernel and we apply it to TC tasks. The Domain Kernel, denoted by KD, can be exploited to estimate the topic sim</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>G. Salton and M.H. McGill. 1983. Introduction to modern information retrieval. McGraw-Hill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Sch¨olkopf</author>
<author>A J Smola</author>
</authors>
<title>Learning with Kernels. Support Vector Machines, Regularization, Optimization, and Beyond.</title>
<date>2001</date>
<publisher>MIT Press.</publisher>
<marker>Sch¨olkopf, Smola, 2001</marker>
<rawString>B. Sch¨olkopf and A. J. Smola. 2001. Learning with Kernels. Support Vector Machines, Regularization, Optimization, and Beyond. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sebastiani</author>
</authors>
<title>Machine learning in automated text categorization.</title>
<date>2002</date>
<journal>ACM Computing Surveys,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="13970" citStr="Sebastiani, 2002" startWordPosition="2412" endWordPosition="2413">W Kernel does not require a Domain Model, so we can consider this setting as “purely” supervised, in which no external knowledge source is provided. We compared the performance of both KD and KBoW on two standard TC benchmarks. In subsection 4.1 we describe the evaluation tasks and the preprocessing steps, in 4.2 we describe some algorithmic details of the TC system adopted. Finally in subsection 4.3 we compare the learning curves of KD and KBoW. 4.1 Text Categorization tasks For the experiments reported in this paper, we selected two evaluation benchmarks typically used in the TC literature (Sebastiani, 2002): the 20newsgroups and the Reuters corpora. In both the data sets we tagged the texts for part of speech and we considered only the noun, verb, adjective, and adverb parts of speech, representing them by vectors containing the frequencies of each disambiguated lemma. The only feature selection step we performed was to remove all the closed-class words from the document index. 20newsgroups. The 20Newsgroups data set5 is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. This collection has become a popular data set for experimen</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>F. Sebastiani. 2002. Machine learning in automated text categorization. ACM Computing Surveys, 34(1):1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Shawe-Taylor</author>
<author>N Cristianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2004</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="12236" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="2105" endWordPosition="2108"> i=1 The kernel function K returns the similarity between two instances in the input space X, and can be designed in order to capture the relevant aspects to estimate similarity, just by taking care of satisfying set of formal requirements, as described in (Sch¨olkopf and Smola, 2001). In this paper we define the Domain Kernel and we apply it to TC tasks. The Domain Kernel, denoted by KD, can be exploited to estimate the topic similarity among two texts while taking into account the external knowledge provided by a Domain Model (see section 2). It is a variation of the Latent Semantic Kernel (Shawe-Taylor and Cristianini, 2004), in which a Domain Model is exploited to define an explicit mapping D : Rk —* Rk&apos; from the classical VSM into the domain VSM. The Domain Kernel is defined by (D(ti), D(tl)) KD(ti, tl) = (4) �(D(tl ), D(tl))(D(ti), D(ti)) where D is the Domain Mapping defined in equation 1. To be fully defined, the Domain Kernel requires a Domain Matrix D. In principle, D can be acquired from any corpora by exploiting any (soft) term clustering algorithm. Anyway, we belive that adequate Domain Models for particular tasks can be better acquired from collections of documents from the same source. For this reason</context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>J. Shawe-Taylor and N. Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Strapparava</author>
<author>A Gliozzo</author>
<author>C Giuliano</author>
</authors>
<title>Pattern abstraction and term similarity for word sense disambiguation: Irst at senseval-3.</title>
<date>2004</date>
<booktitle>In Proc. of SENSEVAL-3 Third International Workshop on Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>229--234</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="7154" citStr="Strapparava et al., 2004" startWordPosition="1182" endWordPosition="1185">clusters of terms. Each cluster represents a semantic domain (Gliozzo et al., 2004), i.e. a set of terms that often co-occur in texts having similar topics. A Domain Model is represented by a k x k&apos; rectangular matrix D, containing the degree of association among terms and domains, as illustrated in Table 1. MEDICINE COMPUTER SCIENCE HIV 1 0 AIDS 1 0 virus 0.5 0.5 laptop 0 1 Table 1: Example of Domain Matrix &apos;The idea of exploiting a Domain Kernel to help a supervised classification framework, has been profitably used also in other NLP tasks such as word sense disambiguation (see for example (Strapparava et al., 2004)). Domain Models can be used to describe lexical ambiguity and variability. Lexical ambiguity is rep57 resented by associating one term to more than one domain, while variability is represented by associating different terms to the same domain. For example the term virus is associated to both the domain COMPUTER SCIENCE and the domain MEDICINE (ambiguity) while the domain MEDICINE is associated to both the terms AIDS and HIV (variability). More formally, let D = {D1, D2, ..., Dk0} be a set of domains, such that k&apos; « k. A Domain Model is fully defined by a k x k&apos; domain matrix D representing in</context>
</contexts>
<marker>Strapparava, Gliozzo, Giuliano, 2004</marker>
<rawString>C. Strapparava, A. Gliozzo, and C. Giuliano. 2004. Pattern abstraction and term similarity for word sense disambiguation: Irst at senseval-3. In Proc. of SENSEVAL-3 Third International Workshop on Evaluation of Systems for the Semantic Analysis of Text, pages 229–234, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S K M Wong</author>
<author>W Ziarko</author>
<author>P C N Wong</author>
</authors>
<title>Generalized vector space model in information retrieval.</title>
<date>1985</date>
<booktitle>In Proceedings of the 81h ACM SIGIR Conference.</booktitle>
<contexts>
<context position="9272" citStr="Wong et al., 1985" startWordPosition="1575" endWordPosition="1578">ty of the two sentences “He is affected by AIDS” and “HIV is a virus” is very high, because the terms AIDS, HIV and virus are highly associated to the domain MEDICINE. In this work we propose the use of Latent Semantic Analysis (LSA) (Deerwester et al., 1990) to induce Domain Models from corpora. LSA is an unsupervised technique for estimating the similarity among texts and terms in a corpus. LSA is performed by means of a Singular Value Decomposition (SVD) of the term-by-document matrix T describing the corpus. The SVD algorithm can be exploited to acquire a domain matrix D from a large 2In (Wong et al., 1985) a similar schema is adopted to define a Generalized Vector Space Model, of which the Domain VSM is a particular instance. corpus T in a totally unsupervised way. SVD decomposes the term-by-document matrix T into three matrixes T — VEk0UT where Ek0 is the diagonal k x k matrix containing the highest k&apos; « k eigenvalues of T, and all the remaining elements set to 0. The parameter k&apos; is the dimensionality of the Domain VSM and can be fixed in advance3. Under this setting we define the domain matrix DLSA4 as � DLSA = INV Ek0 (2) where IN is a diagonal matrix such that iNi,i = Kernel Methods are th</context>
</contexts>
<marker>Wong, Ziarko, Wong, 1985</marker>
<rawString>S.K.M. Wong, W. Ziarko, and P.C.N. Wong. 1985. Generalized vector space model in information retrieval. In Proceedings of the 81h ACM SIGIR Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Zelikovitz</author>
<author>H Hirsh</author>
</authors>
<title>Using LSI for text classification in the presence of background text.</title>
<date>2001</date>
<booktitle>In Henrique Paques, Ling Liu, and</booktitle>
<pages>113--118</pages>
<editor>David Grossman, editors,</editor>
<publisher>ACM Press,</publisher>
<location>Atlanta, US.</location>
<contexts>
<context position="2842" citStr="Zelikovitz and Hirsh, 2001" startWordPosition="437" endWordPosition="440"> research in supervised approaches to NLP often deals with defining methodologies and algorithms to reduce the amount of human effort required for collecting labeled examples. A promising direction to solve this problem is to provide unlabeled data together with labeled texts to help supervision. In the Machine Learning literature this learning schema has been called semisupervised learning. It has been applied to the TC problem using different techniques: co-training (Blum and Mitchell, 1998), EM-algorithm (Nigam et al., 2000), Transduptive SVM (Joachims, 1999b) and Latent Semantic Indexing (Zelikovitz and Hirsh, 2001). In this paper we propose a novel technique to perform semi-supervised learning for TC. The underlying idea behind our approach is that lexical co56 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), pages 56–63, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics herence (i.e. co-occurence in texts of semantically related terms) (Magnini et al., 2002) is an inherent property of corpora, and it can be exploited to help a supervised classifier to build a better categorization hypothesis, even if the amount of labeled training data provided </context>
<context position="21952" citStr="Zelikovitz and Hirsh, 2001" startWordPosition="3723" endWordPosition="3726">lso reported in (Joachims, 1999b), where a transduptive SVM is compared to a state-of-the-art TC classifier based on SVM. The semi-supervised approach obtained better results than the standard with few learning data, while at full learning results seem to converge. (Bekkerman et al., 2002) adopted a SVM classifier in which texts have been represented by their associations to a set of Distributional Word Clusters. Even if this approach is very similar to ours, it is not a semi-supervised learning schema, because authors did not exploit any additional unlabeled data to induce word clusters. In (Zelikovitz and Hirsh, 2001) background knowledge (i.e. the unlabeled data) is exploited together with labeled data to estimate document similarity in a Latent Semantic Space (Deerwester et al., 1990). Their approach differs from the one proposed in this paper because a different categorization algorithm has been adopted. Authors compared their algorithm with an EM schema (Nigam et al., 2000) on the same dataset, reporting better results only with very few labeled data, while EM performs better with more training. All the semi-supervised approaches in the literature reports better results than strictly supervised ones wi</context>
<context position="23392" citStr="Zelikovitz and Hirsh, 2001" startWordPosition="3957" endWordPosition="3960">the splitting partitions adopted affect sensibly the final results. Anyway, we reported the best F1 measure on the Reuters corpus: to our knowledge, the state-of-theart on the 10 top most frequent categories of the ModApte split at full learning is F1 92.0 (Bekkerman et al., 2002) while we obtained 92.8. It is important to notice here that this results has been obtained thanks to the improvements of the Domain Kernel. In addition, on the 20newsgroups task, our methods requires about 100 documents (i.e. five documents per category) to achieve 70% F1, while both EM (Nigam et al., 2000) and LSI (Zelikovitz and Hirsh, 2001) requires more than 400 to achieve the same performance. 6 Conclusion and Future Works In this paper a novel technique to perform semisupervised learning for TC has been proposed and evaluated. We defined a Domain Kernel that allows us to improve the similarity estimation among documents by exploiting Domain Models. Domain Models are acquired from large collections of non annotated texts in a totally unsupervised way. An extensive evaluation on two standard benchmarks shows that the Domain Kernel allows us to reduce drastically the amount of training data required for learning. In particular t</context>
</contexts>
<marker>Zelikovitz, Hirsh, 2001</marker>
<rawString>S. Zelikovitz and H. Hirsh. 2001. Using LSI for text classification in the presence of background text. In Henrique Paques, Ling Liu, and David Grossman, editors, Proceedings of CIKM-01, 10th ACM International Conference on Information and Knowledge Management, pages 113–118, Atlanta, US. ACM Press, New York, US.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>