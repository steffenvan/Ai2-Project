<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.963133">
Generation from Lexical Conceptual Structures
</title>
<author confidence="0.850025">
David Traurn and Nizar Habash
</author>
<affiliation confidence="0.887253">
UMIACS, University of Maryland
</affiliation>
<note confidence="0.367107">
{tram, habash}Ocs . umd . edu
</note>
<sectionHeader confidence="0.926213" genericHeader="method">
I Introduction
</sectionHeader>
<bodyText confidence="0.999633785714286">
This paper describes a system for generating natural
language sentences from an interlingual representa-
tion, Lexical Conceptual Structure (LCS). This sys-
tem has been developed as part of a Chinese-English
Machin.e Translation system, however, it promises
to be useful for many other MT language pairs.
The generation system has also been used in Cross-
Language information retrieval research (Levow et
al., 2000).
One of the big challenges in Natural Language
processing efforts is to be able to make use of ex-
isting resources, a big difficulty being the sometimes
large differences in syntax, semantics, and ontoto-
gies of such resources. A case in point is the in-
terlbagua representations used for machine transla-
tion and cross-language processing. Such represen-
tations are becoming fairly popular, yet there are
widely different views about what these languages
should be composed of, varying from purely concep-
tual knowledge-representations, having little to do
with the structure of language, to very syntactic rep-
resentations, maintaining most of the idiosyncrasies
of the source languages. In our generation system we
make use of resources associated with two different
(kinds of) interlingua structures: Lexical Conceptual
Structure (LCS), and the Abstract Meaning Repre-
sentations used at USC/ISI (Langkilde and Knight,
1998a).
</bodyText>
<sectionHeader confidence="0.975004" genericHeader="method">
2 Lexical Conceptual Structure
</sectionHeader>
<bodyText confidence="0.999520710526316">
Lexical Conceptual Structure is a compositional
abstraction with language-independent properties
that transcend structural idiosyncrasies (Jackendoff,
1983; Jackendoff, 1990; Jackendoff, 1996). This rep-
resentation has been used as the interlingua of sev-
eral projects such as UN1TRAN (Dorr et at., 1993)
and MILT (Dorr, 1997).
An LCS is &amp;directed graph with a root. Each node
is associated with certain information, including a
type, a primitive and a field. The type of an LOS
node is one of Event, State, Path, Manner, Property
or Thing, loosely correlated with verbs prepositions,
adverbs, adjectives and nouns. Within each of these
types, there are a number of conceptual primitives
of that type, which are the basic building blocks of
LCS structures. There are two general classes of
primitives: closed class or structural primitive (e.g.,
CAUSE, GO, BE, TO) and CONSTANTS, correspond-
ing to the primitives for open lexical classes (e.g.,
reduce+ed, textile+, slash+ingly),I. Exam-
ples of fields include Locational, Possessional,
Identif icational. Children are also designated
as to whether they are subject, argument, or
modifier position.
An LCS captures the semantics of a lexical item
through a combination of semantic structure (spec-
ified by the shape of the graph and its structural
primitives and fields) and semantic content (speci-
fied through constants). The semantic structure of
a verb is the same for all members of a verb class
(Levin and Rappaport Hovav, 1995) whereas the
content is specific to the verb itself. So, all the verbs
in the &amp;quot;Cut Verbs - Change of State&amp;quot; class have the
same semantic structure but vary in their semantic
content (for example, chip, cut, saw, scrape, slash
and scratch).
The lexicon entry or Root LCS (RLCS) of one
sense of the Chinese verb ruel_jian3 is as follows:
</bodyText>
<equation confidence="0.996001714285714">
(1)
(act_on loc
(* thing 1)
(* thing 2)
((* (on] 23) loc (*head*) (thing 24))
(cut+ingly 26)
(down+/m))
</equation>
<bodyText confidence="0.999105875">
The top node in the. RIGS has the structural
primitive ACT_ON in the locational field. Its sub-
ject is a star-marked LCS, meaning a subordinate
RLCS needs to be filled in here to form a complete
event. It also has the restriction that the filler LOS
be of the type thing. The number &amp;quot;1&amp;quot; in that node
specifies the thematic role: in this case, agent. The
second child node, in argument position, needs to
</bodyText>
<footnote confidence="0.948587">
1Suffixes such as +, +ed, +ingly are Markers of the open
class of primitives, indicating the type
</footnote>
<page confidence="0.998147">
52
</page>
<bodyText confidence="0.999113263157895">
be of type thing too. The number &amp;quot;2&amp;quot; stands for
theme. The last two children specify the manner of
the locational act_on, that is &amp;quot;cutting in a down-
ward manner&amp;quot;. The RLCS for nouns are generally
much simpler since they usually include only one
root node with a primitive. For instance (US+) or
(quota+).
The meaning of complex phrases is represented as
a composed LCS (CLCS). This is constructed &amp;quot;com-
posed&amp;quot; from several RLCSes corresponding to in-
dividual words. In the composition process, which
starts with a parse tree of the input sentence, all
the obligatory positions in the root and subordinate
RLCS corresponding to lexical items are filled with
other RLCSes from appropriately placed items in the
parse tree. For example, the three RLCSes we have
seen already can compose to give the CLCS in (2),
corresponding to the English sentence: United states
cut down (the) quota.
</bodyText>
<equation confidence="0.996855333333333">
(act_on loc
(us+)
(quota+)
((* [on] 23) loc (*head*) (thing 24))
(cut+ingly 26)
(down/m))
</equation>
<bodyText confidence="0.99822775">
CLCS structures can be composed of different
sorts of RLCS structures, corresponding to differ-
ent words. A CLCS can also be decomposed on the
generation side in different ways depending on the
RLCSes of the lexical items in the target language.
For example, the CLCS above will match a single
verb and two arguments when generated in Chinese
(regardless of the input language). But it will match
four lexical items in English: cut, US, quota, and
down, since the RLCS for the verb &amp;quot;cut&amp;quot; in the En-
glish lexicon, as shown in (3), does not include the
modifier down.
</bodyText>
<equation confidence="0.9925506">
(act_on loc
(* thing 1)
(* thing 2)
((* [on] 23) 10 (*head*) (thing 24))
(cut+ingly 26))
</equation>
<bodyText confidence="0.999441666666667">
The rest of the examples in this paper will refer
to the slightly more complex CLCS shown in (4),
corresponding to the English sentence The United
States unilaterally reduced the China textile export
quota This LCS is presented without all the addi-
tional features for sake of clarity. Also, it is actually
one of eight possible LCS compositions produced by
the analysis component from the input Chinese sen-
tence.
</bodyText>
<equation confidence="0.997371307692308">
(cause (us+)
(go ident (quota+ (china+)
(textile+)
(export+))
(to ident (quota+ (china+)
(textile+)
(export-f))
(at ident (quota+ (china+)
(textile+)
(export+))
(reduce+ed))))
(with instr (*HEAD*) nil)
(unilaterally+/m))
</equation>
<sectionHeader confidence="0.956116" genericHeader="method">
3 The Generation System
</sectionHeader>
<bodyText confidence="0.998648333333333">
Since this generation system was developed in tan-
dem with the most recent LCS composition system,
and LCS-language and specific lexicon extensions,
a premium was put on the ability for experimenta-
tion along a number of parameters and rapid ad-
justment on the basis of intermediate inputs and re-
sults to the generation system. This goal encour-
aged a modular design, and made lisp a convenient
language for implementation. We were also able to
successfully integrate components from the Nitrogen
Generation System (Langldlde and Knight, 1998a;
Langkilde and Knight, 1998b).
</bodyText>
<figure confidence="0.53614">
English
</figure>
<figureCaption confidence="0.999813">
Figure 1: Generation System Architecture
</figureCaption>
<bodyText confidence="0.999945636363636">
The architecture of the generation system is
shown in Figure 1, showing the main modules and
sub-modules and flow of information between them.
The first main component translates, with the use of
a language specific lexicon, from the LCS interlingua
to a language-specific representation of the sentence
in a modified form of the AMR-interlingua, using
words and features specific to the target language,
but also including syntactic and semantic informa-
tion from the LCS representation. The second main
component produces target language sentences from
</bodyText>
<figure confidence="0.992060538461539">
CLCS Pre-Processing Linereization
154
Morphology
NITROGEN
Ingram preferences
Realization
f UCS-Amr
Lexical Access
Alignincny
Decomposition
LCS-AMR
Creation
Lexical Choice
</figure>
<page confidence="0.997763">
53
</page>
<bodyText confidence="0.999238275862069">
this intermediate representation. We will now de-
scribe each of these components in more detail.
The input to the generation component is a text-
representation of a CLCS, the Lexical Conceptual
Structure corresponding to a natural language sen-
tence. The particular format, known as long-hand
is equivalent to the form shown in (4), but mak-
ing certain information more explicit and regular
(at the price of increased verbosity). The Long-
hand CLCS can either be a fully language-neutral
interlingua representation, or one which still incor-
porates some aspects of the source-language inter-
pretation process. This latter may include grammat-
ical features on LOS nodes, but also nodes, known as
functional nodes, which correspond to words in the
source language but are not LCS-nodes themselves,
serving merely as place-holders for feature informa-
tion. Examples of these nodes include punctuation
markers, coordinating conjunctions, grammatical as-
pect markers, and determiners. An additional exten-
sion of the LOS input language, beyond traditional
LOS is the in-place representation of an ambiguous
sub-tree as a POSSIBLES node, which has the various
possibilities represented as its own children.
Thus, for example, the following structure (with
some aspects elided for brevity) represents a node
that could be one of three possibilities. In the second
one, the root of the sub-tree is a functional node,
passing its features to its child, COUNTRY+:
</bodyText>
<figure confidence="0.576909833333333">
(5)
(:POSSIBLES -2589104
(MIDDLE+ (COUNTRY+ ( DEVELOPING+/P)))
(FUNCTIONAL (POSTPOSITION AMONG)
(COUNTRY+ (DEVELOPING+/P)))
(CHINA+ (COUNTRY+ ( DEVELOPING+/P)))
</figure>
<subsectionHeader confidence="0.995734">
3.1 Lexical Choice
</subsectionHeader>
<bodyText confidence="0.986905227272727">
The first major component, divided into four
pipelined sub-modules, as shown in Figure 1 trans-
forms a CLCS structure to what we call an LCS-
AMR structure, using the syntax of the abstract
meaning representation (AMR.), used in the Nitro-
gen generation system, but with words already cho-
sen (rather than more abstract Sensus ontology con-
cepts), and also augmented with information from
the LOS that is useful for target language realiza-
tion.
3.1.1 Pre-Processing
The pre-processing phase converts the text input for-
mat into internal graph representations, for efficient
access of components (with links for parents as well
as children), also doing away with extraneous source-
language features, converting, for example, (5) to re-
move the functional node and promote COUNTRY+ to
be one of the possible sub-trees. This involves a top-
down traversal of the tree, including some complex-
ities when functional nodes without children (which
then assign features to their parents) are direct chil-
dren of possibles nodes.
</bodyText>
<subsubsectionHeader confidence="0.825504">
3.1.2 Lexical Access
</subsubsectionHeader>
<bodyText confidence="0.957037772727273">
The lexical access phase compares the internal CLCS
form to the target language lexicon, decorating the
CLOS tree with the RLCSes of target language
words which are likely to match sub-structures of
the CLCS. In an off-line processing phase, the tar-
get language lexicon is stored in a hash-table, with
each entry keyed on a designated primitive which
would be a most distinguishing node in the RLCS.
On-line decoration then proceeds in two step pro-
cess, for each node in the CLOS:
(6) a. look for RLCSes stored in the lexicon under
the CLCS node&apos;s primitives
b. store retrieved RLCSes at the node in the
CLCS that matches the root of this RLCS
Figure 2 shows some of the English entries match-
ing the CLCS in (4). For most of these words, the
designated primitive is the only node in the corre-
sponding LOS for that entry. For reduce, however,
reduce+ed is the designated primitive. While this
will be retrieved in step (6) while examining the
reduce+ed node from (4), in (6)b, the LCS for &amp;quot;re-
duce&amp;quot; will be stored at the root node of (4) (cause).
</bodyText>
<construct confidence="0.750230166666667">
DEFJORD &amp;quot;reduce&amp;quot;
:CLASS &amp;quot;45.4.a&amp;quot;
:THETA_ROLES ((1 &amp;quot;_ag_th,instr(with)&amp;quot;))
:LCS (cause (* thing 1)
(go ident (* thing 2)
(toward ident (thing 2)
(at ident (thing 2)
(reduce+ed 9))))
((* with 19) instr (*head*)
(thing 20)))
:VAR_SPEC ((1 (animate +))))
(:DEF_WORD PUS&amp;quot; :LCS (US+ 0))
(:DEF_WORD &amp;quot;China&amp;quot; :LCS (China+ 0))
(:DEF_WORD &amp;quot;quota&amp;quot; :LCS (quota+ 0))
(:DEF_WORD &amp;quot;WITH&amp;quot;
:LCS (with instr (thing 2) (* thing 20)))
( DEF_WORD &amp;quot;unilaterally&amp;quot;
:LCS (unilaterally+/m 0))
</construct>
<figureCaption confidence="0.986309">
Figure 2: Lexicon entries
</figureCaption>
<page confidence="0.989343">
54
</page>
<bodyText confidence="0.99979025">
The current English lexicon contains over 11000
RLCS entries such as those in Figure 2, including
over 4000 verbs and 6200 unique primitive keys in
the hash-table.
</bodyText>
<subsectionHeader confidence="0.955043">
3.1.3 Alignment/Decomposition
</subsectionHeader>
<bodyText confidence="0.972191423076923">
The heart of the lexical access algorithm is the de-
composition process. This algorithm attempts to
align RLCSes selected by the lexical access portion
with parts of the CLCS, to find a complete cover-
ing of the CLCS graph. The main algorithm is very
similar to that described in (Darr, 1993), however
with some extensions to be able to also deal with
the in-place ambiguity represented by the possibles
nodes.
The algorithm recursively checks a CLCS node
against corresponding RLCS nodes coming from the
lexical entries-retrieved and stored in the previous
phase. If significant incompatibilities are found, the
lexical entry is discarded. If all (obligatory) nodes
in the RLCS match against nodes in the CLCS,
then the rest of the CLCS is recursively checked
against other lexical entries stored at the remain-
ing unmatched CLCS nodes. Some nodes, indicated
with a &amp;quot;*&amp;quot;, as in Figure 2, require not just. a match
against the corresponding CLOS node, but also a
match against another lexical entry. Some CLCS
nodes must thus match multiple RLCS nodes. A
CLCS node matches an RLCS node, if the following
conditions hold:
(7) a. the primitives are the same (or primitive for
one is a wild-card, represented as nil)
</bodyText>
<listItem confidence="0.845657333333333">
b. the types (e.g., thing, event, state, etc.) are
the same
c. the fields (e.g., identificational, possessive,
locational, etc) are the same
d. the positions (e.g., subject, argument, or
modifier) are the same
e. all obligatory children of the RLCS node
have corresponding matches to children of
the CLCS
</listItem>
<bodyText confidence="0.999014964285714">
Subject and argument children of an RLCS node
are obligatory unless specified as optional, whereas
modifiers are optional unless specified as obliga-
tory. In the RLCS for &amp;quot; reduce, 2 in Figure 2,
the nodes corresponding to agent and theme (num-
bered 1 and 2, respectively) are obligatory, while
the instrument (the node numbered 19) is optional.
Thus, even though in (4) there is no matching lexical
entry for the node in Figure 2 numbered 20
marked in the RLCS for &amp;quot;with&amp;quot;), the main RLCS
for &amp;quot; reduce&amp;quot; is allowed to match, though with-
out any realization for the instrument.
A complexity in the algorithm occurs when there
are multiple possibilities filling in a position in a
CLOS. In this case, only one of these possibilities
is required to match all the corresponding RLCS
nodes in order for a lexical entry to match. In the
case where there are some of these possibilities that
do not match any RLCS nodes (meaning there are
no target-language realizations for these constructs),
these possibilities can be pruned at this stage. On
the other hand, ambiguity can also be introduced at
the decomposition stage, if multiple lexical entries
can match a single structure
The result of the decomposition process is a
match-structure indicating the hierarchical relation-
ship between all lexical entries, which, together cover
the input CLCS.
</bodyText>
<sectionHeader confidence="0.60148" genericHeader="method">
3.1.4 LCS-AMR Creation
</sectionHeader>
<bodyText confidence="0.999483714285714">
The match structure resulting from decomposition
is then converted into the appropriate input format
used by the Nitrogen generation system. Nitrogen&apos;s
input, Abstract Meaning Representation (AMR), is
a labeled directed graph written using the syntax
for the PENMAN Sentence Plan Language (Penman
1989). the structure of an AMR is basically as in (8),
</bodyText>
<listItem confidence="0.5939645">
(8) AMR = &lt;concept&gt; (&lt;label&gt; {&lt;role&gt;
&lt;AMR&gt;)+)
</listItem>
<bodyText confidence="0.999984571428571">
Since the roles expected by Nitrogen&apos;s English
generation grammar do not match well with the the-
matic roles and features of a CLCS, we have ex-
tended the AMR language with LCS-specific rela-
tions, calling the result, an LCS-AMR. To distin-
guish the LCS relations from those used by Nitro-
gen, we mark most of the new roles with the prefix
:LCS-. Figure 3 shows the LCS-AMR corresponding
to the CLCS in (4).
In the above example, the basic role / is used
to specify an instance. So, the LCS-AMR can be
read as an instance of the concept induce whose
category is a verb and is in the active voice. More-
over, &apos;reduce&apos; has two thematic roles related to it, an
agent and a theme; and it is modified by the concept
junilaterallyl. The different roles modifying &apos;reduce&apos;
come from different origins. The :LCS-NODE value
comes directly from the unique node number in the
input CLCS. The category, voice and telicity are de-
rived from features of the LCS entry for the verb
&apos;reduce&apos; in the English lexicon. The specifications
of agent and theme come from the LCS represen-
tation of the verb reduce in the English lexicon as
well, as can be seen by the node numbers 1 and 2, in
the lexicon entry in Figure 2. The role :LCS-MOD-
MANNER is derived by combining the fact that the
corresponding AMR had a modifier role in the CLCS
and because its type is a Manner.
</bodyText>
<subsectionHeader confidence="0.989573">
3.2 Realization
</subsectionHeader>
<bodyText confidence="0.9987525">
The LCS-AMR representation is then passed to the
realization module. The strategy used by Nitrogen is
</bodyText>
<page confidence="0.990156">
55
</page>
<figure confidence="0.984953">
(a7537 / Ireducel
:LCS-NODE 6253520
:LCS-VOICE ACTIVE
:CAT V
:TELIC +
:LCS-AG (a7538 / Waited Statesf
:LCS-NODE 6278216
:CAT N)
:LCS-TH (a7539 / lquotal
:LCS-NODE 6278804
:CAT N
:LCS-MOD-THING (a7540 !china&apos;
:LCS-NODE 6108872
:CAT N)
:LCS-MOD-THING (a7541 / itextilel
:LCS-NODE 6111224
-- :CAT N)
:LCS-MOD-THING (a7542 / lexperti
:LCS-NODE 6112400
:CAT N))
:LCS-MOD-MANNER (a7543 / lunilaterally1
:LCS-NODE 6279392
:CAT ADV))
</figure>
<figureCaption confidence="0.999191">
Figure 3: LCS-AMR
</figureCaption>
<bodyText confidence="0.999946384615385">
to over-generate possible sequences of English from
the ambiguous or under-specified AMFts and then
decide amongst them based on bigram frequency.
The interface between the Linearization module and
the Statistical Extraction module is a word lattice
of possible renderings. The Nitrogen package of-
fers support for both subtasks, Linearization and
Statistical Extraction. Initially, we used the Nitro-
gen grammar to do Linearization. But complexities
in recasting the LCS-AMR roles as standard AMR
roles as well as efficiency considerations compelled
us to create our own English grammar implemented
in Lisp to generate the word lattices.
</bodyText>
<subsectionHeader confidence="0.579842">
3.2.1 Linearization
</subsectionHeader>
<bodyText confidence="0.999437">
In this module, we force linear order on the un-
ordered parts of an LCS-AMR. This is done by
recursively calling subroutines that create various
phrase types (NP,PP, etc.) from aspects of the LCS-
AMR. The result of the linearization phase is a word
lattice specifying the sequence of words that make
up the resulting sentence and the points of ambigu-
ity where different generation paths are taken. (9)
shows the word lattice corresponding to the LCS-
AMR in (8).
</bodyText>
<listItem confidence="0.9646464">
(9) (SEQ (WRD &amp;quot;start-sentence*&amp;quot; BOS) (WRD
&amp;quot;united states&amp;quot; NOUN) (WRD &amp;quot;unilaterally&amp;quot;
ADJ) (WRD &amp;quot;reduced&amp;quot; VERB) (OR (WRD
&amp;quot;the&amp;quot; ART) (WRD &amp;quot;a&amp;quot; ART) (WRD &amp;quot;an&amp;quot;
ART)) (WRD &amp;quot;china&amp;quot; ADJ) (OR (SEQ (WRD
</listItem>
<bodyText confidence="0.904982416666667">
&amp;quot;export&amp;quot; ADJ) (WRD &amp;quot;textile&amp;quot; ADJ)) (SEQ
(WRD &amp;quot;textile&amp;quot; ADJ) (WRD &amp;quot;export&amp;quot; ADJ)))
(WRD &amp;quot;quota&amp;quot; NOUN) (WRD &amp;quot;.&amp;quot; PUNC)
(WRD &amp;quot;*end-sentence*&amp;quot; EOS))
The keyword SEQ specifies that what follows it is
a list of words in their correct linear order. The key-
word OR specifies the existence of different paths for
generation. In the above example, the word &apos;quota&apos;
gets all possible determiners since its definiteness is
not specified. Also, the relative order of the words
&apos;textile&apos; and &apos;export&apos; is not resolved so both possi-
bilities are generated.
Sentences were realized according to the pattern
in (10). That is, first subordinating conjunctions,
if any, then modifiers in the temporal field (e.g.,
&amp;quot;now&amp;quot;, &amp;quot;in 1978&amp;quot;), then the first thematic role, then
most other modifiers, the verb (with collocations if
any) then spatial modifiers (&amp;quot;up&amp;quot;, &amp;quot;down&amp;quot;), then the
second and third thematic roles, followed by prepo-
sitional phrases and relative sentences. Nitrogen&apos;s
morphology component was also used, e.g., to give
tense to the head verb. In the example above, since
there was no tense specified in the input LCS, past
tense was used on the basis of the telicity of the verb.
</bodyText>
<listItem confidence="0.8773895">
(10) (Sconj ,) (temp-mod)* Thi (Mods)* V (coll)
(smod)* (Th2)+ (Th3)÷ (PP)* (RelS)*
</listItem>
<bodyText confidence="0.9984986">
There is no one-to-one mapping between a partic-
ular thematic role and an argument position. For
example, a theme can be the subject in some cases
and it can be the object in others or even an oblique.
Observe &amp;quot;cookie&amp;quot; in (11).
</bodyText>
<listItem confidence="0.895788666666667">
(11) a. John ate a cookie (object)
b. the cookie contains chocolate (subject)
c. she nibbled at a cookie (oblique)
</listItem>
<bodyText confidence="0.998712">
Thematic roles are numbered for their correct re-
alization order, according to the hierarchy for argu-
ments shown in (12).
</bodyText>
<listItem confidence="0.9165755">
(12) agent &gt; instrument &gt; theme &gt; perceived &gt;
(everythinget se)
</listItem>
<bodyText confidence="0.99909825">
So, in the case of the occurrence of theme alone,
it is mapped to first argument position. If a theme
and an agent occur, the agent is mapped to first ar-
gument position and the theme is mapped to second
argument position. A more detailed discussion is
available in (Dorr et al., 1998). For the LCS-AMR in
Figure 3, the thematic hierarchy is what determined
that the junited states is the subject and &apos;quoted is
the object of the verb !reduce!.
In our input CLCSs, in most cases little hierarchi-
cal information was given about multiple modifiers
of a noun. Our initial, brute force, solution was to
</bodyText>
<page confidence="0.992447">
56
</page>
<bodyText confidence="0.988370454545455">
generate all permutations and depend on statisti-
cal extraction to decide. This technique worked for
noun phrases of about 6 words, but was too costly
for larger phrases (of which there were several ex-
amples in our test corpus). This cost was alleviated
to some degree, also providing slightly better results
than pure bigram selection by labelling adjectives in
the English lexicon as belonging to one of several
ordered classes, inspired by the adjective ordering
scheme in (Quirk et al., 1985). This is shown in
(13).
</bodyText>
<listItem confidence="0.993195692307692">
(13) a. Determiner (all, few, several, some, etc.)
b. Most Adjectival (important, practical, eco-
nomic, etc.)
c. Age (old, young, etc.)
d. Color (black, red, etc.)
e. Participle (confusing, adjusted, convincing,
decided)
f. Provenance (China, southern, etc.)
g. Noun (Ba-nk_of_China, difference, memoran-
dum, etc.)
h. Denominal (nouns made into adjectives by
adding -al, e.g., individual, coastal, annual,
etc.)
</listItem>
<bodyText confidence="0.999909555555555">
If multiple words fall within the same group, per-
mutations are generated for them. This situation
can be seen for the LCA-AMR in Figure 3 with the
ordering of the modifiers of the word &apos;quota&apos;: &apos;china&apos;,
&apos;export&apos; and &apos;textile&apos;. &apos;china&apos; fell within the Prove-
nance class of modifiers which gives it precedence
over the other two words. They, on the other hand,
fell in the Noun class and therefore both permuta-
tions were passed on to the statistical component.
</bodyText>
<subsectionHeader confidence="0.763549">
3.2.2 Statistical Preferences
</subsectionHeader>
<bodyText confidence="0.998436666666667">
The final step, extracting a preferred sentence from
the word lattice of possibilities is done using Ni-
trogen&apos;s Statistical Extractor without any changes.
Sentences are scored using uni and bigram frequen-
cies calculated based on two years of Wall Street
Journal (Langkilde and Knight, 1998b),
</bodyText>
<sectionHeader confidence="0.800303" genericHeader="method">
4 Dealing with Ambiguity
</sectionHeader>
<bodyText confidence="0.999163">
A major issue in sentence generation from an inter-
lingua or conceptual structure, especially as part of a
machine translation project, is how and when to deal
with ambiguity. There are several different sources
of ambiguity in the generation process outlined in
the previous section. Some of these include:
</bodyText>
<listItem confidence="0.95418264">
• ambiguity in source language analysis (as repre-
sented by possibles nodes in the CLCS input to
the Generation system). This can include am-
biguity between multiple concepts, such as the
example in (5), LCS type/structure (e.g., thing
or event, which field), or structural ambiguity
(subject, argument or modifier).
• ambiguity introduced in lexical choice (when
multiple match structures can cover a single
CLCS)
• ambiguity introduced in realization (when mul-
tiple orderings are possible, also multiple mor-
phological realizations)
There are also several types of strategies for ad-
dressing ambiguity at various phases, including:
• passing all possible structures down for further
processing stages to deal with
• filtering based on &amp;quot;soft&amp;quot; preferences (only pass
the highest set of candidates, according to some
metric)
• quota-based filtering, passing only the top N
candidates
• threshold filtering, passing only candidates that
exceed a fixed threshold (either score or binary
test)
</listItem>
<bodyText confidence="0.99996772">
The generation system uses a combination of these
strategies, at different phases in the processing. Am-
biguous CLCS sub-trees are sometimes annotated
with scores based on preference of attachment as an
argument rather than a modifier. The alignment al-
gorithm can be run in either of two modes, one which
selects only the top scoring possibility for which a
matching structure can be found, and one in which
all possible structures are passed on, regardless of
score. The former method is the only one feasible
when given very large (e.g., over 1 megabyte text
files) CLCS inputs. Also at the decomposition level,
soft preferences are used in that missing lexical en-
tries can be hypothesized to cover parts of the CLCS
(essentially &amp;quot;making up&amp;quot; words in the target lan-
guage). This is done, however, only when no le-
gitimate matches are found using only the available
lexical entries. At the linearization phase, there are
often many choices for ordering of modifiers at the
sarne level. As mentioned in the previous section,
we are experimenting with separating these into po-
sitional classes, but our last resort is to pass along
all permutations of elements in each sub-class. The
ultimate arbiter is the statistical extractor, which
orders and presents the top scoring realizations.
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="method">
5 Interlingual representation issues
</sectionHeader>
<bodyText confidence="0.999378">
One issue that needs to be confronted in an Inter-
lingua such as LCS is what to do when linguistic
structure of languages vary widely, and useful con-
ceptual structure may also diverge from these. A
</bodyText>
<page confidence="0.994672">
57
</page>
<bodyText confidence="0.9997374">
case in point is the representation of numbers. Lan-
guages diverge widely as to which numbers are prim-
itive terms, and how larger numbers are built com-
positionally through modification (e.g., multiplica-
tion and addition). One question that immediately
comes up is whether an interiingua such as LCS
should represent numbers according to the linguis-
tic structure of the source language (or some partic-
ular designated natural language) or as some other
internal numerical form, (e.g. decimal numerals).
Likewise, on generation into a target language, how
much of the structure of the source language should
be kept, especially when this is not the most nat-
ural way to group things in the target language.
One might be tempted to always convert to a stan-
dard interlingua representation of numbers, however
this does lose some possible classification into groups
that might be present in the input (contrast in En-
glish: &amp;quot;12 pair&amp;quot; with &amp;quot;2 dozen&amp;quot;.
In our Chinese-English efforts, such issues came
up, since the natural multiplication points in Chi-
nese were 100, 10,000, and 100,000,000, rather than
100, 1000, and 1,000,000, as in English. Our provi-
sional solution is to propogate the source language
modification structure all the way through the LCS-
AMR stage, and include special purpose rules look-
ing for the &amp;quot;Chinese&amp;quot; numbers and multiplying them
together to get numerals, and then divide and real-
ize in the English fashion. E.g., using the words
thousand, million, and billion.
</bodyText>
<sectionHeader confidence="0.999375" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.9998405">
So far most of the evaluation has been fairly small-
scale and fairly subjective, generating English sen-
tences from CLCSs produced from about 80 sen-
tences. Evaluation in this case is difficult, because
the ultimate criteria is translation quality, which
can, itself, be difficult to judge, but, moreover, it
can be hard to attribute specific deficits to the anal-
ysis phase, the lexical resources, or the generation
system proper. So far results have been mostly ad-
equate, even for large and&apos; fairly complex sentences,
taking less than 1 minute for generation up to inputs
of about 1 megabyte input CLCS files. Ambiguity
and complexity beyond that level tends to overtax
the generation system.
For the most part, the over-generation strategy of
Nitrogen, coupled with the bigram preferences works
very well. There are still some difficulties, however.
One major one is that, especially with its bias for
shorter sentences, fluency is given preference over
translation fidelity. Thus, if there are options of
whether or not to express some optional informa-
tion, this will tend to be left out. Also, bigrams are
obviously inadequate for capturing long-distance de-
pendencies, and so, if things like agreement are not
carefully controlled in the symbolic component, they
will be incorrect in some cases.
The generation component has also been used on
a broader scale, generating thousands of simple sen-
tences - at least one for each verb in the English
LCS lexicon, creating sentence templates to be used
in a Cross-Language information retrieval system
(Levow et al., 2000).
</bodyText>
<sectionHeader confidence="0.999399" genericHeader="discussions">
7 Future Work
</sectionHeader>
<bodyText confidence="0.999975914285714">
The biggest remaining step is a more careful evalu-
ation of different sub-systems and preference strate-
gies to more efficiently process very ambiguous and
complex inputs, without substantially sacrificing
translation quality. Also a current research topic
is how to combine other metrics coming from vari-
ous points in the generation process with the bigram
statistics, to result in better overall outputs.
Another topic of interest is developing other lan-
guage outputs. Most of the subcomponents are
language-independent. The realization components
being an obvious exception. In particular, the
pre-processing algorithm is completely language-
independent. The lexical access algorithm is lan-
guage independent, although it requires a target-
language lexicon, which of course is language de-
pendent. The alignment algorithm is also com-
pletely language independent. The lcs-amr creation
language is mostly language independent, however
there may not be sufficient features added to the
language and extracted from the LCS-AMR for full
generation of some other languages. Some target
languages might require some extensions to the out-
put language and new rules to extract this informa-
tion from the LCS. The realization process is mostly
language dependent. The current linearizaton mod-
ule is very dependent on the structure of English.
We are, however working on a future version of this
component splitting the linearization task into lan-
guage independent processes and grammar compil-
ers, and independent language-specific output gram-
mars. Nitrogen&apos;s realizer, also, is algorithmically
language-independent, however one would need a
target language database for realization in another
language.
</bodyText>
<sectionHeader confidence="0.996519" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999934818181818">
This work was supported by the US Department of
Defense through contract MDA904-96-R-0738. The
Nitrogen system used in the realization process was
provided by USC/ISI, we would like to thank Keven
Knight and Irene Langkilde for help and advice in
using it. The adjective classifications described in
Section 3 were devised by Carol Van Ess-Dykema.
David Clark and Noah Smith worked on previous
versions of the system, and we are indebted to some
of their ideas for the current implementation. We
would also like to thank the CLIP group at UM-
</bodyText>
<page confidence="0.993154">
58
</page>
<bodyText confidence="0.82395575">
veristy of Maryland, especially Ron Dolan, Bonnie
Darr, Gina Levow, Mari Olsen, Wade Shin, Amy
Weinberg, for helpful input and feedback on the gen-
eration system.
</bodyText>
<sectionHeader confidence="0.941329" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999202063829787">
Bonnie J. Dorr, James Bendier, Scott Blanksteen,
and Barrie Migdaloff. 1993. Use of Lexical Con-
ceptual Structure for Intelligent Tutoring. Tech-
nical Report UMIACS TR 93-108, CS TR 3161,
University of Maryland.
Bonnie J. Dorr, Nizar Habash, and David Traum.
1998. A Thematic Hierarchy for Efficient Gener-
ation from Lexical-Conceptal Structure. In Pro-
ceedings of the Third Conference of the Associ-
ation for Machine Translation in the Americas,
AMTA-98, in Lecture Notes in Artificial Intelli-
gence, 1529, pages 333-343, Langhorne, PA, Oc-
tober 28-31.
Bonnie J. Dorr. 1993. Machine Translation: A View
from the Lexicon. The MIT Press.
Bonnie J. Dorr. 1997. Large-Scale Acquisition of
LCS-Based Lexicons for Foreign Language Tutor-
ing. In Proceedings of the ACL Fifth Conference
on Applied Natural Language Processing (ANLP),
pages 139-146, Washington, DC.
Ray Jackendoff. 1983. Semantics and Cognition.
The MIT Press, Cambridge, MA.
Ray Jackendoff. 1990. Semantic Structures. The
MIT Press, Cambridge, MA.
Ray Jackendoff, 1996. The Proper Treatment of
Measuring Out, Telicity, and Perhaps Even Quan-
tification in English. Natural Language and Lin-
guistic Theory, 14:305-354.
Irene Langkilde and Kevin Knight. 1998a. Gen-
eration that Exploits Corpus-Based Statistical
Knowledge. In Proceedings of COLING-ACL &apos;98,
pages 704-710.
Irene Langlcilde and Kevin Knight. 1998b. The
Practical Value of N-Grams in Generation. In In-
ternational Natural Language Generation Work-
shop.
Beth Levin and Malka Rappaport Hovav. 1995. Un-
accusativity: At the Syntax-Lexical Semantics In-
terface. The MIT Press, Cambridge, MA. LI
Monograph 26.
Gina Levow, Bonnie J. Dorr, and Dekang Lin. 2000.
Construction of chinese-english semantic hierar-
chy for cross-language retrieval. forthcoming.
Randolph Quirk, Sidney Greenbaum, Geoffrey
Leech, and Jan Svartvik, 1985. A Comprehen-
sive Grammar of the English Language. Longman,
London.
</reference>
<page confidence="0.999231">
59
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.669813">
<title confidence="0.999764">Generation from Lexical Conceptual Structures</title>
<author confidence="0.982604">David Traurn</author>
<author confidence="0.982604">Nizar</author>
<affiliation confidence="0.938267">UMIACS, University of</affiliation>
<intro confidence="0.696369">umd . edu</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bonnie J Dorr</author>
<author>James Bendier</author>
<author>Scott Blanksteen</author>
<author>Barrie Migdaloff</author>
</authors>
<title>Use of Lexical Conceptual Structure for Intelligent Tutoring.</title>
<date>1993</date>
<tech>Technical Report UMIACS TR 93-108, CS TR 3161,</tech>
<institution>University of Maryland.</institution>
<marker>Dorr, Bendier, Blanksteen, Migdaloff, 1993</marker>
<rawString>Bonnie J. Dorr, James Bendier, Scott Blanksteen, and Barrie Migdaloff. 1993. Use of Lexical Conceptual Structure for Intelligent Tutoring. Technical Report UMIACS TR 93-108, CS TR 3161, University of Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie J Dorr</author>
<author>Nizar Habash</author>
<author>David Traum</author>
</authors>
<title>A Thematic Hierarchy for Efficient Generation from Lexical-Conceptal Structure.</title>
<date>1998</date>
<booktitle>In Proceedings of the Third Conference of the Association for Machine Translation in the Americas, AMTA-98, in Lecture Notes in Artificial Intelligence,</booktitle>
<volume>1529</volume>
<pages>333--343</pages>
<location>Langhorne, PA,</location>
<contexts>
<context position="20500" citStr="Dorr et al., 1998" startWordPosition="3316" endWordPosition="3319">blique. Observe &amp;quot;cookie&amp;quot; in (11). (11) a. John ate a cookie (object) b. the cookie contains chocolate (subject) c. she nibbled at a cookie (oblique) Thematic roles are numbered for their correct realization order, according to the hierarchy for arguments shown in (12). (12) agent &gt; instrument &gt; theme &gt; perceived &gt; (everythinget se) So, in the case of the occurrence of theme alone, it is mapped to first argument position. If a theme and an agent occur, the agent is mapped to first argument position and the theme is mapped to second argument position. A more detailed discussion is available in (Dorr et al., 1998). For the LCS-AMR in Figure 3, the thematic hierarchy is what determined that the junited states is the subject and &apos;quoted is the object of the verb !reduce!. In our input CLCSs, in most cases little hierarchical information was given about multiple modifiers of a noun. Our initial, brute force, solution was to 56 generate all permutations and depend on statistical extraction to decide. This technique worked for noun phrases of about 6 words, but was too costly for larger phrases (of which there were several examples in our test corpus). This cost was alleviated to some degree, also providing</context>
</contexts>
<marker>Dorr, Habash, Traum, 1998</marker>
<rawString>Bonnie J. Dorr, Nizar Habash, and David Traum. 1998. A Thematic Hierarchy for Efficient Generation from Lexical-Conceptal Structure. In Proceedings of the Third Conference of the Association for Machine Translation in the Americas, AMTA-98, in Lecture Notes in Artificial Intelligence, 1529, pages 333-343, Langhorne, PA, October 28-31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie J Dorr</author>
</authors>
<title>Machine Translation: A View from the Lexicon.</title>
<date>1993</date>
<publisher>The MIT Press.</publisher>
<marker>Dorr, 1993</marker>
<rawString>Bonnie J. Dorr. 1993. Machine Translation: A View from the Lexicon. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie J Dorr</author>
</authors>
<title>Large-Scale Acquisition of LCS-Based Lexicons for Foreign Language Tutoring.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL Fifth Conference on Applied Natural Language Processing (ANLP),</booktitle>
<pages>139--146</pages>
<location>Washington, DC.</location>
<contexts>
<context position="1841" citStr="Dorr, 1997" startWordPosition="269" endWordPosition="270">ource languages. In our generation system we make use of resources associated with two different (kinds of) interlingua structures: Lexical Conceptual Structure (LCS), and the Abstract Meaning Representations used at USC/ISI (Langkilde and Knight, 1998a). 2 Lexical Conceptual Structure Lexical Conceptual Structure is a compositional abstraction with language-independent properties that transcend structural idiosyncrasies (Jackendoff, 1983; Jackendoff, 1990; Jackendoff, 1996). This representation has been used as the interlingua of several projects such as UN1TRAN (Dorr et at., 1993) and MILT (Dorr, 1997). An LCS is &amp;directed graph with a root. Each node is associated with certain information, including a type, a primitive and a field. The type of an LOS node is one of Event, State, Path, Manner, Property or Thing, loosely correlated with verbs prepositions, adverbs, adjectives and nouns. Within each of these types, there are a number of conceptual primitives of that type, which are the basic building blocks of LCS structures. There are two general classes of primitives: closed class or structural primitive (e.g., CAUSE, GO, BE, TO) and CONSTANTS, corresponding to the primitives for open lexic</context>
</contexts>
<marker>Dorr, 1997</marker>
<rawString>Bonnie J. Dorr. 1997. Large-Scale Acquisition of LCS-Based Lexicons for Foreign Language Tutoring. In Proceedings of the ACL Fifth Conference on Applied Natural Language Processing (ANLP), pages 139-146, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>Semantics and Cognition.</title>
<date>1983</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1672" citStr="Jackendoff, 1983" startWordPosition="241" endWordPosition="242">y conceptual knowledge-representations, having little to do with the structure of language, to very syntactic representations, maintaining most of the idiosyncrasies of the source languages. In our generation system we make use of resources associated with two different (kinds of) interlingua structures: Lexical Conceptual Structure (LCS), and the Abstract Meaning Representations used at USC/ISI (Langkilde and Knight, 1998a). 2 Lexical Conceptual Structure Lexical Conceptual Structure is a compositional abstraction with language-independent properties that transcend structural idiosyncrasies (Jackendoff, 1983; Jackendoff, 1990; Jackendoff, 1996). This representation has been used as the interlingua of several projects such as UN1TRAN (Dorr et at., 1993) and MILT (Dorr, 1997). An LCS is &amp;directed graph with a root. Each node is associated with certain information, including a type, a primitive and a field. The type of an LOS node is one of Event, State, Path, Manner, Property or Thing, loosely correlated with verbs prepositions, adverbs, adjectives and nouns. Within each of these types, there are a number of conceptual primitives of that type, which are the basic building blocks of LCS structures. </context>
</contexts>
<marker>Jackendoff, 1983</marker>
<rawString>Ray Jackendoff. 1983. Semantics and Cognition. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>Semantic Structures.</title>
<date>1990</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1690" citStr="Jackendoff, 1990" startWordPosition="243" endWordPosition="244">edge-representations, having little to do with the structure of language, to very syntactic representations, maintaining most of the idiosyncrasies of the source languages. In our generation system we make use of resources associated with two different (kinds of) interlingua structures: Lexical Conceptual Structure (LCS), and the Abstract Meaning Representations used at USC/ISI (Langkilde and Knight, 1998a). 2 Lexical Conceptual Structure Lexical Conceptual Structure is a compositional abstraction with language-independent properties that transcend structural idiosyncrasies (Jackendoff, 1983; Jackendoff, 1990; Jackendoff, 1996). This representation has been used as the interlingua of several projects such as UN1TRAN (Dorr et at., 1993) and MILT (Dorr, 1997). An LCS is &amp;directed graph with a root. Each node is associated with certain information, including a type, a primitive and a field. The type of an LOS node is one of Event, State, Path, Manner, Property or Thing, loosely correlated with verbs prepositions, adverbs, adjectives and nouns. Within each of these types, there are a number of conceptual primitives of that type, which are the basic building blocks of LCS structures. There are two gene</context>
</contexts>
<marker>Jackendoff, 1990</marker>
<rawString>Ray Jackendoff. 1990. Semantic Structures. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>The Proper Treatment of Measuring Out, Telicity, and Perhaps Even Quantification</title>
<date>1996</date>
<booktitle>in English. Natural Language and Linguistic Theory,</booktitle>
<pages>14--305</pages>
<contexts>
<context position="1709" citStr="Jackendoff, 1996" startWordPosition="245" endWordPosition="246">ns, having little to do with the structure of language, to very syntactic representations, maintaining most of the idiosyncrasies of the source languages. In our generation system we make use of resources associated with two different (kinds of) interlingua structures: Lexical Conceptual Structure (LCS), and the Abstract Meaning Representations used at USC/ISI (Langkilde and Knight, 1998a). 2 Lexical Conceptual Structure Lexical Conceptual Structure is a compositional abstraction with language-independent properties that transcend structural idiosyncrasies (Jackendoff, 1983; Jackendoff, 1990; Jackendoff, 1996). This representation has been used as the interlingua of several projects such as UN1TRAN (Dorr et at., 1993) and MILT (Dorr, 1997). An LCS is &amp;directed graph with a root. Each node is associated with certain information, including a type, a primitive and a field. The type of an LOS node is one of Event, State, Path, Manner, Property or Thing, loosely correlated with verbs prepositions, adverbs, adjectives and nouns. Within each of these types, there are a number of conceptual primitives of that type, which are the basic building blocks of LCS structures. There are two general classes of prim</context>
</contexts>
<marker>Jackendoff, 1996</marker>
<rawString>Ray Jackendoff, 1996. The Proper Treatment of Measuring Out, Telicity, and Perhaps Even Quantification in English. Natural Language and Linguistic Theory, 14:305-354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
<author>Kevin Knight</author>
</authors>
<title>Generation that Exploits Corpus-Based Statistical Knowledge.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL &apos;98,</booktitle>
<pages>704--710</pages>
<contexts>
<context position="1482" citStr="Langkilde and Knight, 1998" startWordPosition="219" endWordPosition="222">e translation and cross-language processing. Such representations are becoming fairly popular, yet there are widely different views about what these languages should be composed of, varying from purely conceptual knowledge-representations, having little to do with the structure of language, to very syntactic representations, maintaining most of the idiosyncrasies of the source languages. In our generation system we make use of resources associated with two different (kinds of) interlingua structures: Lexical Conceptual Structure (LCS), and the Abstract Meaning Representations used at USC/ISI (Langkilde and Knight, 1998a). 2 Lexical Conceptual Structure Lexical Conceptual Structure is a compositional abstraction with language-independent properties that transcend structural idiosyncrasies (Jackendoff, 1983; Jackendoff, 1990; Jackendoff, 1996). This representation has been used as the interlingua of several projects such as UN1TRAN (Dorr et at., 1993) and MILT (Dorr, 1997). An LCS is &amp;directed graph with a root. Each node is associated with certain information, including a type, a primitive and a field. The type of an LOS node is one of Event, State, Path, Manner, Property or Thing, loosely correlated with ve</context>
<context position="6786" citStr="Langkilde and Knight, 1998" startWordPosition="1089" endWordPosition="1092">r (*HEAD*) nil) (unilaterally+/m)) 3 The Generation System Since this generation system was developed in tandem with the most recent LCS composition system, and LCS-language and specific lexicon extensions, a premium was put on the ability for experimentation along a number of parameters and rapid adjustment on the basis of intermediate inputs and results to the generation system. This goal encouraged a modular design, and made lisp a convenient language for implementation. We were also able to successfully integrate components from the Nitrogen Generation System (Langldlde and Knight, 1998a; Langkilde and Knight, 1998b). English Figure 1: Generation System Architecture The architecture of the generation system is shown in Figure 1, showing the main modules and sub-modules and flow of information between them. The first main component translates, with the use of a language specific lexicon, from the LCS interlingua to a language-specific representation of the sentence in a modified form of the AMR-interlingua, using words and features specific to the target language, but also including syntactic and semantic information from the LCS representation. The second main component produces target language sentence</context>
<context position="22524" citStr="Langkilde and Knight, 1998" startWordPosition="3642" endWordPosition="3645">ring of the modifiers of the word &apos;quota&apos;: &apos;china&apos;, &apos;export&apos; and &apos;textile&apos;. &apos;china&apos; fell within the Provenance class of modifiers which gives it precedence over the other two words. They, on the other hand, fell in the Noun class and therefore both permutations were passed on to the statistical component. 3.2.2 Statistical Preferences The final step, extracting a preferred sentence from the word lattice of possibilities is done using Nitrogen&apos;s Statistical Extractor without any changes. Sentences are scored using uni and bigram frequencies calculated based on two years of Wall Street Journal (Langkilde and Knight, 1998b), 4 Dealing with Ambiguity A major issue in sentence generation from an interlingua or conceptual structure, especially as part of a machine translation project, is how and when to deal with ambiguity. There are several different sources of ambiguity in the generation process outlined in the previous section. Some of these include: • ambiguity in source language analysis (as represented by possibles nodes in the CLCS input to the Generation system). This can include ambiguity between multiple concepts, such as the example in (5), LCS type/structure (e.g., thing or event, which field), or str</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Irene Langkilde and Kevin Knight. 1998a. Generation that Exploits Corpus-Based Statistical Knowledge. In Proceedings of COLING-ACL &apos;98, pages 704-710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langlcilde</author>
<author>Kevin Knight</author>
</authors>
<title>The Practical Value of N-Grams in Generation.</title>
<date>1998</date>
<booktitle>In International Natural Language Generation Workshop.</booktitle>
<marker>Langlcilde, Knight, 1998</marker>
<rawString>Irene Langlcilde and Kevin Knight. 1998b. The Practical Value of N-Grams in Generation. In International Natural Language Generation Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
<author>Malka Rappaport Hovav</author>
</authors>
<title>Unaccusativity: At the Syntax-Lexical Semantics Interface.</title>
<date>1995</date>
<journal>LI Monograph</journal>
<volume>26</volume>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Levin, Hovav, 1995</marker>
<rawString>Beth Levin and Malka Rappaport Hovav. 1995. Unaccusativity: At the Syntax-Lexical Semantics Interface. The MIT Press, Cambridge, MA. LI Monograph 26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gina Levow</author>
<author>Bonnie J Dorr</author>
<author>Dekang Lin</author>
</authors>
<title>Construction of chinese-english semantic hierarchy for cross-language retrieval.</title>
<date>2000</date>
<tech>forthcoming.</tech>
<contexts>
<context position="28421" citStr="Levow et al., 2000" startWordPosition="4592" endWordPosition="4595">over translation fidelity. Thus, if there are options of whether or not to express some optional information, this will tend to be left out. Also, bigrams are obviously inadequate for capturing long-distance dependencies, and so, if things like agreement are not carefully controlled in the symbolic component, they will be incorrect in some cases. The generation component has also been used on a broader scale, generating thousands of simple sentences - at least one for each verb in the English LCS lexicon, creating sentence templates to be used in a Cross-Language information retrieval system (Levow et al., 2000). 7 Future Work The biggest remaining step is a more careful evaluation of different sub-systems and preference strategies to more efficiently process very ambiguous and complex inputs, without substantially sacrificing translation quality. Also a current research topic is how to combine other metrics coming from various points in the generation process with the bigram statistics, to result in better overall outputs. Another topic of interest is developing other language outputs. Most of the subcomponents are language-independent. The realization components being an obvious exception. In parti</context>
</contexts>
<marker>Levow, Dorr, Lin, 2000</marker>
<rawString>Gina Levow, Bonnie J. Dorr, and Dekang Lin. 2000. Construction of chinese-english semantic hierarchy for cross-language retrieval. forthcoming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randolph Quirk</author>
<author>Sidney Greenbaum</author>
<author>Geoffrey Leech</author>
<author>Jan Svartvik</author>
</authors>
<title>A Comprehensive Grammar of the English Language.</title>
<date>1985</date>
<location>Longman, London.</location>
<contexts>
<context position="21312" citStr="Quirk et al., 1985" startWordPosition="3452" endWordPosition="3455"> little hierarchical information was given about multiple modifiers of a noun. Our initial, brute force, solution was to 56 generate all permutations and depend on statistical extraction to decide. This technique worked for noun phrases of about 6 words, but was too costly for larger phrases (of which there were several examples in our test corpus). This cost was alleviated to some degree, also providing slightly better results than pure bigram selection by labelling adjectives in the English lexicon as belonging to one of several ordered classes, inspired by the adjective ordering scheme in (Quirk et al., 1985). This is shown in (13). (13) a. Determiner (all, few, several, some, etc.) b. Most Adjectival (important, practical, economic, etc.) c. Age (old, young, etc.) d. Color (black, red, etc.) e. Participle (confusing, adjusted, convincing, decided) f. Provenance (China, southern, etc.) g. Noun (Ba-nk_of_China, difference, memorandum, etc.) h. Denominal (nouns made into adjectives by adding -al, e.g., individual, coastal, annual, etc.) If multiple words fall within the same group, permutations are generated for them. This situation can be seen for the LCA-AMR in Figure 3 with the ordering of the mo</context>
</contexts>
<marker>Quirk, Greenbaum, Leech, Svartvik, 1985</marker>
<rawString>Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, and Jan Svartvik, 1985. A Comprehensive Grammar of the English Language. Longman, London.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>