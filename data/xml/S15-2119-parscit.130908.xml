<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007955">
<title confidence="0.9340065">
UPF-taln: SemEval 2015 Tasks 10 and 11
Sentiment Analysis of Literal and Figurative Language in Twitter ∗
</title>
<author confidence="0.995706">
Francesco Barbieri, Francesco Ronzano, Horacio Saggion
</author>
<affiliation confidence="0.933125">
Universitat Pompeu Fabra, Barcelona, Spain
</affiliation>
<email confidence="0.998057">
name.surname@upf.edu
</email>
<sectionHeader confidence="0.995631" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997955">
In this paper, we describe the approach used
by the UPF-taln team for tasks 10 and 11 of
SemEval 2015 that respectively focused on
“Sentiment Analysis in Twitter” and “Sen-
timent Analysis of Figurative Language in
Twitter”. Our approach achieved satisfac-
tory results in the figurative language analy-
sis task, obtaining the second best result. In
task 10, our approach obtained acceptable per-
formances. We experimented with both word-
based features and domain-independent intrin-
sic word features. We exploited two ma-
chine learning methods: the supervised algo-
rithm Support Vector Machines for task 10,
and Random-Sub-Space with M5P as base al-
gorithm for task 11.
</bodyText>
<sectionHeader confidence="0.99002" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.973942571428571">
During the last decade the study and characterisa-
tion of sentiments and emotions in on-line user-
generated content has attracted more and more in-
terest. Since 2013 several tasks dealing with Sen-
timent Analysis have been organised in the context
of SemEval. These tasks have been mainly focused
on the analysis of short texts like SMS or tweets.
In this paper we describe the approach adopted by
UPF-taln team for tasks 10 and 11 of SemEval 2015,
both dealing with the analysis of English tweets.
Task 10 concerned “Sentiment Analysis in Twitter”
∗The research described in this paper is partially funded
by the Spanish fellowship RYC-2009-04291, the SKATER-
TALN UPF project (TIN2012-38584-C06-03), and the EU
project Dr. Inventor (n. 611383).
and included different subtasks. We participated in
the subtask B, named “Sentiment Polarity Classifi-
cation”. Given a message, we were asked to classify
whether the message was of positive, negative, or
neutral sentiment. In Task 11 the participants were
asked to determine the polarity score (between -5 to
+5) of tweets rich in metaphor and irony. Our model
reaches satisfactory results in the figurative language
task 11, however it has suboptimal performance in
task 10.
We exploited an extended version of the tweet
classification features and approach described in
(Barbieri and Saggion, 2014). In particular, we ex-
perimented the use of intrinsic word features, char-
acterising each word in a tweet to try to model and
thus automatically determine its polarity. Thanks to
intrinsic word features, we aimed to detect two as-
pects of tweets: the style used (e.g. register used,
frequent or rare words, positive or negative words,
etc.) and the unexpectedness in the use of words,
particularly important for figurative language. We
also exploited textual features (like word occur-
rences, bigrams, skipgrams or other word patterns)
in order to capture the way words are used in positive
and negative tweets. As machine learning approach
we choose the supervised method Support Vector
Machines (Platt, 1999) for task 10 and the regres-
sion algorithm Random-Sub-Space (Ho, 1998) with
M5P (Quinlan, 2014) as base algorithm for task 11.
In Section 2 and 3 we describe the dataset used
and the tools we employed to process the tweets.
In Section 4 we introduce the features we built our
model on. In Section 5 we discuss the performance
of our model in SemEval 2015 and in Section 6 we
</bodyText>
<page confidence="0.976954">
704
</page>
<bodyText confidence="0.6971105">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 704–708,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
conclude with a recap of our approach and sugges-
tions for further research.
</bodyText>
<sectionHeader confidence="0.985907" genericHeader="introduction">
2 Dataset
</sectionHeader>
<bodyText confidence="0.999969285714286">
In order to train our systems we used in each task
only the dataset provided by the organisers. For
task 10 we were able to retrieve 9689 tweets, tagged
as positive, negative and neutral (Rosenthal et al.,
2015). For task 11 the dataset was a collection
of 8000 figurative tweets annotated with sentiment
scores from -5 to +5 (Li et al., 2015).
</bodyText>
<sectionHeader confidence="0.749683" genericHeader="method">
3 Text Analysis and Tools
</sectionHeader>
<bodyText confidence="0.999973909090909">
In order to deal with the noisy text of Twitter
we made use of the GATE application TwitIE
(Bontcheva et al., 2013) where we modified the
normaliser, adding new abbreviations, new slang
words, removing URLs and changing the normalisa-
tion rules. Besides the tweet normalisation we also
employed TwitIE for tokenisation, Part of Speech
tagging and lemmatisation. We also used Word-
Net (Miller, 1995) to extract synonyms and synsets.
We employed two sentiment lexicons, SentiWord-
Net3.0 (Baccianella et al., 2010) and the NRC Hash-
tag Sentiment Lexicon (Mohammad et al., 2013) and
two emotion lexicons NRC Hashtag Emotion Lexi-
con (Mohammad, 2012) and Depeche Mood (Sta-
iano and Guerini, 2014). As frequency data for de-
termining how often a word is used in English, we
relied on the American National Corpus (Ide and Su-
derman, 2004); we also exploited the VU Amster-
dam Metaphors Corpus (Steen et al., 2010) to find
out how often a word is used in metaphors. Finally,
the machine learning tool we used was Weka (Hall
et al., 2009).
</bodyText>
<sectionHeader confidence="0.993124" genericHeader="method">
4 Our Method
</sectionHeader>
<bodyText confidence="0.9994382">
We employed different machine learning methods
for the two tasks. In task 10, as the classes were only
three (positive, negative and neutral) we opted for
a supervised learning method, and from our exper-
iments with several classifiers, Support Vector Ma-
chines resulted to be the best one. On the other hand,
in task 11 tweets were classified as belonging to one
of 11 polarity classes associated with values rang-
ing from -5 to 5, hence a regression approach was
more suitable. The regression method employed was
Random-Sub-Space with M5P as base algorithm.
We also tried different mixed techniques, like using
a supervised method to classify positive (0 to 5) and
negative (-5 to 0), then a regression method (over
the two subsets) but with no luck: pure regression
methods fitted better task 11.
In both tasks we characterised each tweet using
nine groups of related features all describing both in-
trinsic aspects of the words and word patterns. These
groups of features are the following:
</bodyText>
<listItem confidence="0.999979333333333">
• Sentiments and Emotional Lexicons
• Frequency
• Lemma-Based
• Ambiguity
• Synonyms
• Adjective/ Adverb Intensity
• Characters
• Part of Speech
• Bad Words
</listItem>
<subsectionHeader confidence="0.984544">
4.1 Sentiments and Emotional Lexicons
</subsectionHeader>
<bodyText confidence="0.999746818181818">
Using sentiment lexicons in Sentiment Analysis has
been a common and rewarding practice (Mohammad
et al., 2013; Kiritchenko et al., 2014). The char-
acterisation of the sentiment associated to words in
tweets is important for two reasons: to detect the
global sentiment (e.g. if tweets contain mainly pos-
itive or negative terms) and, in the case of figura-
tive language, to capture unexpectedness created by
a negative word in a positive context and viceversa.
Using the two sentiment lexicons and two emotional
lexicons mentioned in Section 3, we computed the
number of positive / negative words, the sum of the
intensities of the positive /negative scores of words,
the mean of positive / negative score of words, the
greatest positive / negative score, the gap between
the greatest positive / negative score and the posi-
tive / negative mean. These features are computed
including all the words of each tweet. We also de-
termined these features by considering separately
Nouns, Verbs, Adjectives, and Adverbs (we calcu-
late the features by considering only words charac-
terised by a specific Part of Speech).
</bodyText>
<page confidence="0.990331">
705
</page>
<subsectionHeader confidence="0.947166">
4.2 Frequency
</subsectionHeader>
<bodyText confidence="0.9997377">
To design the Frequency feature we used two fre-
quency corpora: the American National Corpus and
the VU Amsterdam Metaphors Corpus. From these
corpora we extracted three features: rarest word fre-
quency (frequency of the rarest word included in the
tweet), frequency mean (word frequency arithmetic
average) and frequency gap (the difference between
the two previous features). As previously done, we
computed these features by considering only Nouns,
Verbs, Adjectives, and Adverbs.
</bodyText>
<subsectionHeader confidence="0.999412">
4.3 Lemma-Based
</subsectionHeader>
<bodyText confidence="0.999941125">
We designed this group of features to detect com-
mon word-patterns in positive and negative tweets.
The lemma-based features are three: lemma+pos
(the combination of each lemma and its Part of
Speech in the tweet), bigrams (combination of two
lemmas in a sequence) and skip one gram, combina-
tion of two lemmas with distance one (two lemmas
separated by one lemma).
</bodyText>
<subsectionHeader confidence="0.990988">
4.4 Ambiguity
</subsectionHeader>
<bodyText confidence="0.999963">
Ambiguity is modelled with WordNet. Our hypoth-
esis is that if a word has many meanings (synset as-
sociated) it is more likely to be used in an ambigu-
ous way. For each tweet we calculated the maximum
number of synsets associated to a single word, the
mean synset number of all the words, and the synset
gap—the difference between the two previous fea-
tures. We determine the value of these features by
including all the words of a tweet as well as by con-
sidering only Nouns, Verbs, Adjectives or Adverbs.
</bodyText>
<subsectionHeader confidence="0.965381">
4.5 Synonyms
</subsectionHeader>
<bodyText confidence="0.999656133333333">
We carried out an analysis of the choice of synonyms
as follows: for each word in the tweet we retrieve
its list of synonyms, then we computed, across all
the words of the tweet: the greatest / lowest num-
ber of synonyms with frequency higher than the one
present in the tweet, the mean number of synonyms
with frequency greater / lower than the frequency of
the related word present in the tweet. We determine
also the greatest / lowest number of synonyms and
the mean number of synonyms of the words with fre-
quency greater / lower than the one present in the the
tweet (gap feature). We computed the set of Syn-
onyms features by considering both all the words
and also restricting the calculation to words with the
Part of Speech tags as above.
</bodyText>
<subsectionHeader confidence="0.981839">
4.6 Adjective/ Adverb Intensity
</subsectionHeader>
<bodyText confidence="0.999954">
Using the Potts (2011) intensity scores of Adjectives
and Adverbs, we calculated three features: the most
intense adjective/adverb and the intensity mean of
the adjective/adverb of the tweet.
</bodyText>
<subsectionHeader confidence="0.976422">
4.7 Characters
</subsectionHeader>
<bodyText confidence="0.999977555555555">
We also wanted to capture the punctuation style of
the author of a tweet. Punctuation and type of char-
acters used are very important in social networks:
a full stop at the end of a subjective message may
change the polarity of the message. Each feature
is a count of specific punctuation marks, including:
“.”, “#”, “!”, “?”, “$”, “%”, “&amp;”, “+”, “-”, “=”, “/”.
Moreover we count as well number of uppercase and
lowercase character.
</bodyText>
<subsectionHeader confidence="0.999505">
4.8 Part of Speech
</subsectionHeader>
<bodyText confidence="0.999390875">
The features included in the Part of Speech group
are designed to capture the structure of positive and
negative tweets. The features of this group are eight
and each one of them counts the number of oc-
currences of words characterised by a certain Part
of Speech. The eight Part of Speech considered
are Verbs, Nouns, Adjectives, Adverbs, Interjections,
Determiners, Pronouns, and Appositions.
</bodyText>
<subsectionHeader confidence="0.996037">
4.9 Bad Words
</subsectionHeader>
<bodyText confidence="0.998157333333333">
Since Twitter messages often include bad words1,
we count them as they may be used more often in
negative messages.
</bodyText>
<sectionHeader confidence="0.996641" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999262">
In this section we present our results in the two tasks
(see Table 1 and Table 2). We only report final re-
sults (mean of Precision, Recall and F-Measure of
each class), for more details please refer to the task
10 and task 11 papers (Rosenthal et al., 2015; Li et
al., 2015).
</bodyText>
<footnote confidence="0.916983666666667">
1We enriched with more variants this list:
https://github.com/shutterstock/List-of-Dirty-Naughty-
Obscene-and-Otherwise-Bad-Words
</footnote>
<page confidence="0.995998">
706
</page>
<subsectionHeader confidence="0.867281">
5.1 Task 10-B
</subsectionHeader>
<bodyText confidence="0.985774483870968">
Given a message, classify whether the message is of
positive, negative, or neutral sentiment. Our model
scores at position 27th out of 40 groups. Systems
were evaluated with the mean of the F-measures of
Positive, Negative and Neutral classes. Our score
is 9 points less than the best system. A consider-
able number of tweets in the test set were considered
sarcastic tweets complicating the sentiment analysis
task. With this test subset our system improves its
performances globally scoring at the 11th position.
See Table 1 for the results in each test set. The fea-
tures that perform better are from the group Senti-
ments and Emotion Lexicons, that achieve informa-
tion gain scores of 0.133. Even if less influent, the
Frequency group obtains a score of 0.09. The other
group of features are not very important for this task,
and the information gain scores are less than 0.3.
F-Measure Rank
participants. We obtained a cosine similarity of
0.710 and a Mean Squared Error (MSE) of 2.458.
The best system cosine and MSE scores were re-
spectively 0.758 and 2.117. In Table 2 the reader
can find all the results.
In Table 3 we show experiments to analyse the
contribution of each type of feature to the final re-
sults. The most important contribution is given by
the Sentiment lexicons NRC and SentiWordNet (see
Section 4.1). Also the Synonyms feature is impor-
tant with a cosine similarity of 0.564. The feature
that was less influent to the final classification was
Intensity of Adjectives and Adverbs.
</bodyText>
<table confidence="0.949984">
MSE Cosine
Overall 2.458 0.711
Sarcasm 0.934 0.903
Irony 1.041 0.873
Metaphor 4.186 0.520
Other 3.772 0.486
65.05 27th
50.93 11th
66.15 17th
57.84 31st
64.5 31st
</table>
<tableCaption confidence="0.9619645">
Table 2: Task 11 results measured by the Cosine Similar-
ity and the Mean Square Error over the test set (Overall)
and for its subsets: sarcasm, irony, metaphor and other
(non-figurative tweets).
</tableCaption>
<table confidence="0.3522404">
Twitter 2014
Sarcasm
Twitter 2013
SMS 2013
LiveJournal 2014
</table>
<tableCaption confidence="0.993458">
Table 1: Task 10 results. For each test set we report F-
Measure and ranking comparing to other systems.
</tableCaption>
<subsectionHeader confidence="0.996352">
5.2 Task 11
</subsectionHeader>
<bodyText confidence="0.999774882352941">
Given a set of tweets that are rich in metaphor and
irony, the goal is to determine whether the user has
expressed a positive, negative or neutral sentiment
in each, and the degree to which this sentiment has
been communicated.
A vector space model was used to evaluate the
similarity of the predictions of each participating
system to the human-annotated gold standard. The
list of expected gold-standard sentiment scores was
used to construct a normalised gold-standard vector,
while a comparable vector will be constructed from
the predictions of a participating system. The cosine
distance between vectors was then used as a mea-
sure of how well the participating system estimates
the gold-standard sentiment scores for the whole of
the test set (Li et al., 2015).
In this task our model ranked second out of 15
</bodyText>
<table confidence="0.999406307692308">
Feature Cosine Similarity
NRC H. Sentiment 0.578
SentiWordNet 0.562
Synonyms 0.564
Characters 0.550
Part of Speech 0.550
Depeche Mood 0.550
Lemma-Based 0.547
NRC H. Emotion 0.547
Bad Words 0.547
Frequency 0.546
Ambiguity 0.546
Intensity 0.544
</table>
<tableCaption confidence="0.762464">
Table 3: Task 11 contribution of each group of feature.
The best feature group was Sentiment, in particular the
features computed with the NRC Hashtag Sentiment Lex-
icon, see Section 4.1.
</tableCaption>
<sectionHeader confidence="0.999067" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.949095">
In this paper we have described our participation
to the SemEval task 10 and 11. Besides the word-
</bodyText>
<page confidence="0.989513">
707
</page>
<bodyText confidence="0.999959823529412">
based features, we experimented the use of intrinsic
word features to characterise positive and negative
tweets. In task 10 our system obtains average perfor-
mances leaving room for important improvements to
our approach. Our system obtains very good results
in task 11, ranking second out of 15 participating
teams. The difference in performance in the two
tasks was expected since our model is the adaption
to sentiment analysis of a model for irony (Barbi-
eri and Saggion, 2014) and sarcasm (Barbieri et al.,
2014) detection in Twitter, thus it fits better the figu-
rative language identification task. Yet, both models
can be improved and we are planning to add new
features (vector space models and distributional se-
mantics among others) and experiment new machine
learning techniques (e.g. cascade classifiers for task
10 or different regression algorithms for task 11).
</bodyText>
<sectionHeader confidence="0.998681" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9984590875">
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
LREC, volume 10, pages 2200–2204.
Francesco Barbieri and Horacio Saggion. 2014. Mod-
elling Irony in Twitter. In Proceedings of the Student
Research Workshop at the 14th Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, Gothenburg, Sweden, April.
Francesco Barbieri, Horacio Saggion, and Ronzano
Francesco. 2014. Modelling sarcasm in twitter, a
novel approach. ACL Workshop on Sentiment Anal-
ysis: WASSA.
Kalina Bontcheva, Leon Derczynski, Adam Funk,
Mark A. Greenwood, Diana Maynard, and Niraj
Aswani. 2013. TwitIE: An Open-Source Information
Extraction Pipeline for Microblog Text. In Proceed-
ings ofRecent Advances in Natural Language Process-
ing Conferemce.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten. 2009.
The WEKA data mining software: an update. ACM
SIGKDD explorations newsletter, 11(1):10–18.
Tin Kam Ho. 1998. The random subspace method
for constructing decision forests. Pattern Analy-
sis and Machine Intelligence, IEEE Transactions on,
20(8):832–844.
Nancy Ide and Keith Suderman. 2004. The American
National Corpus First Release. In Proceedings of the
Language Resources and Evaluation Conference.
Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, and
Saif Mohammad. 2014. NRC-Canada-2014: Detect-
ing Aspects and Sentiment in Customer Reviews. In
Proceedings of the 8th International Workshop on Se-
mantic Evaluation (SemEval 2014), Dublin, Ireland,
August.
Guofu Li, Aniruddha Ghosh, Tony Veale, Paolo Rosso,
Ekaterina Shutova, Antonio Reyes, and John Barnden.
2015. Task 11: Sentiment Analysis of Figurative Lan-
guage in Twitter. Denver, Colorado, USA, June, 4-5.
George A Miller. 1995. WordNet: a lexical database for
English. Communications of the ACM, 38(11):39–41.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the State-of-the-
Art in Sentiment Analysis of Tweets. In Proceedings
of the seventh international workshop on Semantic
Evaluation Exercises (SemEval-2013), Atlanta, Geor-
gia, USA, June.
Saif Mohammad. 2012. #Emotional Tweets. In *SEM
2012: The First Joint Conference on Lexical and Com-
putational Semantics – Volume 1: Proceedings of the
main conference and the shared task, and Volume
2: Proceedings of the Sixth International Workshop
on Semantic Evaluation (SemEval 2012), Montr´eal,
Canada, 7-8 June.
John Platt. 1999. Fast Training of Support Vector Ma-
chines Using Sequential Minimal Optimization. Ad-
vances in kernel methodssupport vector learning, 3.
Christopher Potts. 2011. Developing adjective scales
from user-supplied textual metadata. NSF Workshop
on Restructuring Adjectives in WordNet. Arlington,VA.
J Ross Quinlan. 2014. C4. 5: programs for machine
learning. Elsevier.
Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko,
Saif M Mohammad, Alan Ritter, and Veselin Stoy-
anov. 2015. Semeval-2015 task 10: Sentiment analy-
sis in twitter. In Proceedings of the 9th International
Workshop on Semantic Evaluation, SemEval ’2015,
Denver, Colorado, USA, June.
Jacopo Staiano and Marco Guerini. 2014. De-
pecheMood: a Lexicon for Emotion Analysis from
Crowd-Annotated News. In 52nd Annual Meeting of
the Association for Computational Linguistics (Short
Papers), page 427433, Baltimore, Maryland, USA,,
June.
Gerard J Steen, Aletta G Dorst, J Berenike Herrmann,
Anna Kaal, Tina Krennmayr, and Trijntje Pasma.
2010. A method for linguistic metaphor identification:
From MIP to MIPVU, volume 14. John Benjamins
Publishing.
</reference>
<page confidence="0.997004">
708
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9490215">UPF-taln: SemEval 2015 Tasks 10 and 11 Analysis of Literal and Figurative Language in Twitter</title>
<author confidence="0.998916">Francesco Barbieri</author>
<author confidence="0.998916">Francesco Ronzano</author>
<author confidence="0.998916">Horacio</author>
<affiliation confidence="0.981533">Universitat Pompeu Fabra, Barcelona,</affiliation>
<email confidence="0.999833">name.surname@upf.edu</email>
<abstract confidence="0.994877838709677">In this paper, we describe the approach used by the UPF-taln team for tasks 10 and 11 of SemEval 2015 that respectively focused on “Sentiment Analysis in Twitter” and “Sentiment Analysis of Figurative Language in Twitter”. Our approach achieved satisfactory results in the figurative language analysis task, obtaining the second best result. In task 10, our approach obtained acceptable performances. We experimented with both wordbased features and domain-independent intrinsic word features. We exploited two machine learning methods: the supervised algorithm Support Vector Machines for task 10, and Random-Sub-Space with M5P as base algorithm for task 11. 1 Motivation During the last decade the study and characterisation of sentiments and emotions in on-line usergenerated content has attracted more and more interest. Since 2013 several tasks dealing with Sentiment Analysis have been organised in the context of SemEval. These tasks have been mainly focused on the analysis of short texts like SMS or tweets. In this paper we describe the approach adopted by UPF-taln team for tasks 10 and 11 of SemEval 2015, both dealing with the analysis of English tweets. Task 10 concerned “Sentiment Analysis in Twitter” research described in this paper is partially funded by the Spanish fellowship RYC-2009-04291, the SKATER- TALN UPF project (TIN2012-38584-C06-03), and the EU project Dr. Inventor (n. 611383). and included different subtasks. We participated in the subtask B, named “Sentiment Polarity Classification”. Given a message, we were asked to classify whether the message was of positive, negative, or neutral sentiment. In Task 11 the participants were asked to determine the polarity score (between -5 to +5) of tweets rich in metaphor and irony. Our model reaches satisfactory results in the figurative language task 11, however it has suboptimal performance in task 10. We exploited an extended version of the tweet classification features and approach described in (Barbieri and Saggion, 2014). In particular, we experimented the use of intrinsic word features, characterising each word in a tweet to try to model and thus automatically determine its polarity. Thanks to intrinsic word features, we aimed to detect two aspects of tweets: the style used (e.g. register used, frequent or rare words, positive or negative words, etc.) and the unexpectedness in the use of words, particularly important for figurative language. We also exploited textual features (like word occurrences, bigrams, skipgrams or other word patterns) in order to capture the way words are used in positive and negative tweets. As machine learning approach we choose the supervised method Support Vector Machines (Platt, 1999) for task 10 and the regression algorithm Random-Sub-Space (Ho, 1998) with M5P (Quinlan, 2014) as base algorithm for task 11. In Section 2 and 3 we describe the dataset used and the tools we employed to process the tweets. In Section 4 we introduce the features we built our model on. In Section 5 we discuss the performance of our model in SemEval 2015 and in Section 6 we 704 of the 9th International Workshop on Semantic Evaluation (SemEval pages 704–708, Colorado, June 4-5, 2015. Association for Computational Linguistics conclude with a recap of our approach and suggestions for further research. 2 Dataset In order to train our systems we used in each task only the dataset provided by the organisers. For task 10 we were able to retrieve 9689 tweets, tagged as positive, negative and neutral (Rosenthal et al., 2015). For task 11 the dataset was a collection of 8000 figurative tweets annotated with sentiment scores from -5 to +5 (Li et al., 2015). 3 Text Analysis and Tools In order to deal with the noisy text of Twitter we made use of the GATE application TwitIE (Bontcheva et al., 2013) where we modified the normaliser, adding new abbreviations, new slang words, removing URLs and changing the normalisation rules. Besides the tweet normalisation we also employed TwitIE for tokenisation, Part of Speech tagging and lemmatisation. We also used Word- Net (Miller, 1995) to extract synonyms and synsets. We employed two sentiment lexicons, SentiWord- Net3.0 (Baccianella et al., 2010) and the NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013) and two emotion lexicons NRC Hashtag Emotion Lexicon (Mohammad, 2012) and Depeche Mood (Staiano and Guerini, 2014). As frequency data for determining how often a word is used in English, we relied on the American National Corpus (Ide and Suderman, 2004); we also exploited the VU Amsterdam Metaphors Corpus (Steen et al., 2010) to find out how often a word is used in metaphors. Finally, the machine learning tool we used was Weka (Hall et al., 2009). 4 Our Method We employed different machine learning methods for the two tasks. In task 10, as the classes were only three (positive, negative and neutral) we opted for a supervised learning method, and from our experiments with several classifiers, Support Vector Machines resulted to be the best one. On the other hand, in task 11 tweets were classified as belonging to one of 11 polarity classes associated with values ranging from -5 to 5, hence a regression approach was more suitable. The regression method employed was Random-Sub-Space with M5P as base algorithm. We also tried different mixed techniques, like using a supervised method to classify positive (0 to 5) and negative (-5 to 0), then a regression method (over the two subsets) but with no luck: pure regression methods fitted better task 11. In both tasks we characterised each tweet using nine groups of related features all describing both intrinsic aspects of the words and word patterns. These groups of features are the following:</abstract>
<title confidence="0.917620222222222">Sentiments and Emotional Lexicons • Frequency • Lemma-Based • Ambiguity • Synonyms • Adjective/ Adverb Intensity • Characters • Part of Speech • Bad Words</title>
<abstract confidence="0.986637957142857">4.1 Sentiments and Emotional Lexicons Using sentiment lexicons in Sentiment Analysis has been a common and rewarding practice (Mohammad et al., 2013; Kiritchenko et al., 2014). The characterisation of the sentiment associated to words in tweets is important for two reasons: to detect the sentiment if tweets contain mainly positive or negative terms) and, in the case of figuralanguage, to capture by a negative word in a positive context and viceversa. Using the two sentiment lexicons and two emotional lexicons mentioned in Section 3, we computed the of positive / negative the of the of the positive /negative scores of of positive / negative score of the positive / negative the between the greatest positive / negative score and the posi- / negative These features are computed including all the words of each tweet. We also determined these features by considering separately Nouns, Verbs, Adjectives, and Adverbs (we calculate the features by considering only words characterised by a specific Part of Speech). 705 4.2 Frequency To design the Frequency feature we used two frequency corpora: the American National Corpus and the VU Amsterdam Metaphors Corpus. From these we extracted three features: word freof the rarest word included in the mean frequency arithmetic and gap difference between the two previous features). As previously done, we computed these features by considering only Nouns, Verbs, Adjectives, and Adverbs. 4.3 Lemma-Based We designed this group of features to detect common word-patterns in positive and negative tweets. lemma-based features are three: (the combination of each lemma and its Part of in the tweet), of two in a sequence) and one combination of two lemmas with distance one (two lemmas separated by one lemma). 4.4 Ambiguity Ambiguity is modelled with WordNet. Our hypothesis is that if a word has many meanings (synset associated) it is more likely to be used in an ambiguway. For each tweet we calculated the of synsets to a single word, the synset number all the words, and the difference between the two previous features. We determine the value of these features by including all the words of a tweet as well as by considering only Nouns, Verbs, Adjectives or Adverbs. 4.5 Synonyms We carried out an analysis of the choice of synonyms as follows: for each word in the tweet we retrieve its list of synonyms, then we computed, across all words of the tweet: the / lowest numof synonyms frequency higher than the one in the tweet, the number of synonyms with frequency greater / lower than the frequency of the related word present in the tweet. We determine also the greatest / lowest number of synonyms and the mean number of synonyms of the words with frequency greater / lower than the one present in the the We computed the set of Synonyms features by considering both all the words and also restricting the calculation to words with the Part of Speech tags as above. 4.6 Adjective/ Adverb Intensity Using the Potts (2011) intensity scores of Adjectives Adverbs, we calculated three features: the and the mean the adjective/adverb of the tweet. 4.7 Characters We also wanted to capture the punctuation style of the author of a tweet. Punctuation and type of characters used are very important in social networks: a full stop at the end of a subjective message may change the polarity of the message. Each feature is a count of specific punctuation marks, including: “.”, “#”, “!”, “?”, “$”, “%”, “&amp;”, “+”, “-”, “=”, “/”. we count as well number of 4.8 Part of Speech The features included in the Part of Speech group are designed to capture the structure of positive and negative tweets. The features of this group are eight and each one of them counts the number of occurrences of words characterised by a certain Part of Speech. The eight Part of Speech considered and 4.9 Bad Words Twitter messages often include we count them as they may be used more often in negative messages. 5 Experiments and Results In this section we present our results in the two tasks (see Table 1 and Table 2). We only report final results (mean of Precision, Recall and F-Measure of each class), for more details please refer to the task 10 and task 11 papers (Rosenthal et al., 2015; Li et al., 2015). enriched with more variants this list: https://github.com/shutterstock/List-of-Dirty-Naughty- Obscene-and-Otherwise-Bad-Words 706 5.1 Task 10-B Given a message, classify whether the message is of negative, or neutral Our model at position out of 40 groups. Systems were evaluated with the mean of the F-measures of Positive, Negative and Neutral classes. Our score is 9 points less than the best system. A considerable number of tweets in the test set were considered sarcastic tweets complicating the sentiment analysis task. With this test subset our system improves its globally scoring at the position. See Table 1 for the results in each test set. The features that perform better are from the group Sentiments and Emotion Lexicons, that achieve information gain scores of 0.133. Even if less influent, the Frequency group obtains a score of 0.09. The other group of features are not very important for this task, and the information gain scores are less than 0.3. F-Measure Rank participants. We obtained a cosine similarity of 0.710 and a Mean Squared Error (MSE) of 2.458. The best system cosine and MSE scores were respectively 0.758 and 2.117. In Table 2 the reader can find all the results. In Table 3 we show experiments to analyse the contribution of each type of feature to the final results. The most important contribution is given by the Sentiment lexicons NRC and SentiWordNet (see Section 4.1). Also the Synonyms feature is important with a cosine similarity of 0.564. The feature that was less influent to the final classification was Intensity of Adjectives and Adverbs.</abstract>
<affiliation confidence="0.617791">MSE Cosine</affiliation>
<address confidence="0.546341375">Overall 2.458 0.711 Sarcasm 0.934 0.903 Irony 1.041 0.873 Metaphor 4.186 0.520 Other 3.772 0.486 50.93 66.15 57.84</address>
<abstract confidence="0.957663571428571">64.5 Table 2: Task 11 results measured by the Cosine Similarity and the Mean Square Error over the test set (Overall) and for its subsets: sarcasm, irony, metaphor and other (non-figurative tweets). Twitter Twitter SMS LiveJournal 2014 Table 1: Task 10 results. For each test set we report F- Measure and ranking comparing to other systems. 5.2 Task 11 Given a set of tweets that are rich in metaphor and irony, the goal is to determine whether the user has expressed a positive, negative or neutral sentiment in each, and the degree to which this sentiment has been communicated. A vector space model was used to evaluate the similarity of the predictions of each participating system to the human-annotated gold standard. The list of expected gold-standard sentiment scores was used to construct a normalised gold-standard vector, while a comparable vector will be constructed from the predictions of a participating system. The cosine distance between vectors was then used as a measure of how well the participating system estimates the gold-standard sentiment scores for the whole of the test set (Li et al., 2015).</abstract>
<note confidence="0.748059">In this task our model ranked second out of 15</note>
<title confidence="0.835548">Feature Cosine Similarity</title>
<note confidence="0.804512823529412">NRC H. Sentiment 0.578 SentiWordNet 0.562 Synonyms 0.564 Characters 0.550 Part of Speech 0.550 Depeche Mood 0.550 Lemma-Based 0.547 NRC H. Emotion 0.547 Bad Words 0.547 Frequency 0.546 Ambiguity 0.546 Intensity 0.544 Table 3: Task 11 contribution of each group of feature. The best feature group was Sentiment, in particular the features computed with the NRC Hashtag Sentiment Lexicon, see Section 4.1. 6 Conclusions</note>
<abstract confidence="0.97068675">In this paper we have described our participation the SemEval task 10 and 11. Besides the word- 707 based features, we experimented the use of intrinsic word features to characterise positive and negative tweets. In task 10 our system obtains average performances leaving room for important improvements to our approach. Our system obtains very good results in task 11, ranking second out of 15 participating teams. The difference in performance in the two tasks was expected since our model is the adaption to sentiment analysis of a model for irony (Barbieri and Saggion, 2014) and sarcasm (Barbieri et al., 2014) detection in Twitter, thus it fits better the figurative language identification task. Yet, both models can be improved and we are planning to add new features (vector space models and distributional semantics among others) and experiment new machine learning techniques (e.g. cascade classifiers for task 10 or different regression algorithms for task 11).</abstract>
<title confidence="0.920035">References</title>
<author confidence="0.924719">Stefano Baccianella</author>
<author confidence="0.924719">Andrea Esuli</author>
<author confidence="0.924719">Fabrizio Sebas-</author>
<abstract confidence="0.580602333333333">tiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In volume 10, pages 2200–2204.</abstract>
<note confidence="0.917808714285714">Francesco Barbieri and Horacio Saggion. 2014. Mod- Irony in Twitter. In of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Gothenburg, Sweden, April. Francesco Barbieri, Horacio Saggion, and Ronzano Francesco. 2014. Modelling sarcasm in twitter, a</note>
<title confidence="0.903135">approach. Workshop on Sentiment Anal-</title>
<author confidence="0.801163">Kalina Bontcheva</author>
<author confidence="0.801163">Leon Derczynski</author>
<author confidence="0.801163">Adam Funk</author>
<author confidence="0.801163">Mark A Greenwood</author>
<author confidence="0.801163">Diana Maynard</author>
<author confidence="0.801163">Niraj</author>
<title confidence="0.581450333333333">Aswani. 2013. TwitIE: An Open-Source Information Pipeline for Microblog Text. In Proceedings ofRecent Advances in Natural Language Process-</title>
<author confidence="0.689981">Mark Hall</author>
<author confidence="0.689981">Eibe Frank</author>
<author confidence="0.689981">Geoffrey Holmes</author>
<author confidence="0.689981">Bernhard</author>
<abstract confidence="0.816871666666667">Pfahringer, Peter Reutemann, and Ian H Witten. 2009. WEKA data mining software: an update. explorations 11(1):10–18. Tin Kam Ho. 1998. The random subspace method constructing decision forests. Analyand Machine Intelligence, IEEE Transactions</abstract>
<note confidence="0.905241666666667">20(8):832–844. Nancy Ide and Keith Suderman. 2004. The American Corpus First Release. In of the Resources and Evaluation Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, and Saif Mohammad. 2014. NRC-Canada-2014: Detecting Aspects and Sentiment in Customer Reviews. In Proceedings of the 8th International Workshop on Se- Evaluation (SemEval Dublin, Ireland, August. Guofu Li, Aniruddha Ghosh, Tony Veale, Paolo Rosso, Ekaterina Shutova, Antonio Reyes, and John Barnden. 2015. Task 11: Sentiment Analysis of Figurative Language in Twitter. Denver, Colorado, USA, June, 4-5. George A Miller. 1995. WordNet: a lexical database for of the 38(11):39–41. Saif Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. NRC-Canada: Building the State-of-thein Sentiment Analysis of Tweets. In of the seventh international workshop on Semantic Exercises Atlanta, Georgia, USA, June. Mohammad. 2012. #Emotional Tweets. In 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop</note>
<affiliation confidence="0.390022">Semantic Evaluation (SemEval Montr´eal,</affiliation>
<address confidence="0.557878">Canada, 7-8 June.</address>
<author confidence="0.553661">Fast Training of Support Vector Ma-</author>
<affiliation confidence="0.658218">Using Sequential Minimal Optimization. Ad-</affiliation>
<abstract confidence="0.833375">in kernel methodssupport vector 3. Christopher Potts. 2011. Developing adjective scales user-supplied textual metadata. Workshop on Restructuring Adjectives in WordNet. Arlington,VA.</abstract>
<note confidence="0.82291295">Ross Quinlan. 2014. 5: programs for machine Elsevier. Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko, Saif M Mohammad, Alan Ritter, and Veselin Stoyanov. 2015. Semeval-2015 task 10: Sentiment analyin twitter. In of the 9th International on Semantic SemEval ’2015, Denver, Colorado, USA, June. Staiano and Marco Guerini. 2014. pecheMood: a Lexicon for Emotion Analysis from News. In Annual Meeting of the Association for Computational Linguistics (Short page 427433, Baltimore, Maryland, USA,, June. Gerard J Steen, Aletta G Dorst, J Berenike Herrmann, Anna Kaal, Tina Krennmayr, and Trijntje Pasma. method for linguistic metaphor identification: MIP to volume 14. John Benjamins Publishing. 708</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In LREC,</booktitle>
<volume>10</volume>
<pages>2200--2204</pages>
<contexts>
<context position="4480" citStr="Baccianella et al., 2010" startWordPosition="715" endWordPosition="718">of 8000 figurative tweets annotated with sentiment scores from -5 to +5 (Li et al., 2015). 3 Text Analysis and Tools In order to deal with the noisy text of Twitter we made use of the GATE application TwitIE (Bontcheva et al., 2013) where we modified the normaliser, adding new abbreviations, new slang words, removing URLs and changing the normalisation rules. Besides the tweet normalisation we also employed TwitIE for tokenisation, Part of Speech tagging and lemmatisation. We also used WordNet (Miller, 1995) to extract synonyms and synsets. We employed two sentiment lexicons, SentiWordNet3.0 (Baccianella et al., 2010) and the NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013) and two emotion lexicons NRC Hashtag Emotion Lexicon (Mohammad, 2012) and Depeche Mood (Staiano and Guerini, 2014). As frequency data for determining how often a word is used in English, we relied on the American National Corpus (Ide and Suderman, 2004); we also exploited the VU Amsterdam Metaphors Corpus (Steen et al., 2010) to find out how often a word is used in metaphors. Finally, the machine learning tool we used was Weka (Hall et al., 2009). 4 Our Method We employed different machine learning methods for the two tasks. In tas</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In LREC, volume 10, pages 2200–2204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francesco Barbieri</author>
<author>Horacio Saggion</author>
</authors>
<title>Modelling Irony in Twitter.</title>
<date>2014</date>
<booktitle>In Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Gothenburg, Sweden,</location>
<contexts>
<context position="2247" citStr="Barbieri and Saggion, 2014" startWordPosition="346" endWordPosition="349">t Dr. Inventor (n. 611383). and included different subtasks. We participated in the subtask B, named “Sentiment Polarity Classification”. Given a message, we were asked to classify whether the message was of positive, negative, or neutral sentiment. In Task 11 the participants were asked to determine the polarity score (between -5 to +5) of tweets rich in metaphor and irony. Our model reaches satisfactory results in the figurative language task 11, however it has suboptimal performance in task 10. We exploited an extended version of the tweet classification features and approach described in (Barbieri and Saggion, 2014). In particular, we experimented the use of intrinsic word features, characterising each word in a tweet to try to model and thus automatically determine its polarity. Thanks to intrinsic word features, we aimed to detect two aspects of tweets: the style used (e.g. register used, frequent or rare words, positive or negative words, etc.) and the unexpectedness in the use of words, particularly important for figurative language. We also exploited textual features (like word occurrences, bigrams, skipgrams or other word patterns) in order to capture the way words are used in positive and negative</context>
</contexts>
<marker>Barbieri, Saggion, 2014</marker>
<rawString>Francesco Barbieri and Horacio Saggion. 2014. Modelling Irony in Twitter. In Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, Gothenburg, Sweden, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francesco Barbieri</author>
<author>Horacio Saggion</author>
<author>Ronzano Francesco</author>
</authors>
<title>Modelling sarcasm in twitter, a novel approach.</title>
<date>2014</date>
<booktitle>ACL Workshop on Sentiment Analysis: WASSA.</booktitle>
<marker>Barbieri, Saggion, Francesco, 2014</marker>
<rawString>Francesco Barbieri, Horacio Saggion, and Ronzano Francesco. 2014. Modelling sarcasm in twitter, a novel approach. ACL Workshop on Sentiment Analysis: WASSA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kalina Bontcheva</author>
<author>Leon Derczynski</author>
<author>Adam Funk</author>
<author>Mark A Greenwood</author>
<author>Diana Maynard</author>
<author>Niraj Aswani</author>
</authors>
<title>TwitIE: An Open-Source Information Extraction Pipeline for Microblog Text.</title>
<date>2013</date>
<booktitle>In Proceedings ofRecent Advances in Natural Language Processing Conferemce.</booktitle>
<contexts>
<context position="4087" citStr="Bontcheva et al., 2013" startWordPosition="657" endWordPosition="660">. c�2015 Association for Computational Linguistics conclude with a recap of our approach and suggestions for further research. 2 Dataset In order to train our systems we used in each task only the dataset provided by the organisers. For task 10 we were able to retrieve 9689 tweets, tagged as positive, negative and neutral (Rosenthal et al., 2015). For task 11 the dataset was a collection of 8000 figurative tweets annotated with sentiment scores from -5 to +5 (Li et al., 2015). 3 Text Analysis and Tools In order to deal with the noisy text of Twitter we made use of the GATE application TwitIE (Bontcheva et al., 2013) where we modified the normaliser, adding new abbreviations, new slang words, removing URLs and changing the normalisation rules. Besides the tweet normalisation we also employed TwitIE for tokenisation, Part of Speech tagging and lemmatisation. We also used WordNet (Miller, 1995) to extract synonyms and synsets. We employed two sentiment lexicons, SentiWordNet3.0 (Baccianella et al., 2010) and the NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013) and two emotion lexicons NRC Hashtag Emotion Lexicon (Mohammad, 2012) and Depeche Mood (Staiano and Guerini, 2014). As frequency data for determ</context>
</contexts>
<marker>Bontcheva, Derczynski, Funk, Greenwood, Maynard, Aswani, 2013</marker>
<rawString>Kalina Bontcheva, Leon Derczynski, Adam Funk, Mark A. Greenwood, Diana Maynard, and Niraj Aswani. 2013. TwitIE: An Open-Source Information Extraction Pipeline for Microblog Text. In Proceedings ofRecent Advances in Natural Language Processing Conferemce.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: an update.</title>
<date>2009</date>
<journal>ACM SIGKDD explorations newsletter,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="4993" citStr="Hall et al., 2009" startWordPosition="808" endWordPosition="811">ract synonyms and synsets. We employed two sentiment lexicons, SentiWordNet3.0 (Baccianella et al., 2010) and the NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013) and two emotion lexicons NRC Hashtag Emotion Lexicon (Mohammad, 2012) and Depeche Mood (Staiano and Guerini, 2014). As frequency data for determining how often a word is used in English, we relied on the American National Corpus (Ide and Suderman, 2004); we also exploited the VU Amsterdam Metaphors Corpus (Steen et al., 2010) to find out how often a word is used in metaphors. Finally, the machine learning tool we used was Weka (Hall et al., 2009). 4 Our Method We employed different machine learning methods for the two tasks. In task 10, as the classes were only three (positive, negative and neutral) we opted for a supervised learning method, and from our experiments with several classifiers, Support Vector Machines resulted to be the best one. On the other hand, in task 11 tweets were classified as belonging to one of 11 polarity classes associated with values ranging from -5 to 5, hence a regression approach was more suitable. The regression method employed was Random-Sub-Space with M5P as base algorithm. We also tried different mixe</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H Witten. 2009. The WEKA data mining software: an update. ACM SIGKDD explorations newsletter, 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tin Kam Ho</author>
</authors>
<title>The random subspace method for constructing decision forests.</title>
<date>1998</date>
<journal>Pattern Analysis and Machine Intelligence, IEEE Transactions on,</journal>
<volume>20</volume>
<issue>8</issue>
<contexts>
<context position="3023" citStr="Ho, 1998" startWordPosition="472" endWordPosition="473"> Thanks to intrinsic word features, we aimed to detect two aspects of tweets: the style used (e.g. register used, frequent or rare words, positive or negative words, etc.) and the unexpectedness in the use of words, particularly important for figurative language. We also exploited textual features (like word occurrences, bigrams, skipgrams or other word patterns) in order to capture the way words are used in positive and negative tweets. As machine learning approach we choose the supervised method Support Vector Machines (Platt, 1999) for task 10 and the regression algorithm Random-Sub-Space (Ho, 1998) with M5P (Quinlan, 2014) as base algorithm for task 11. In Section 2 and 3 we describe the dataset used and the tools we employed to process the tweets. In Section 4 we introduce the features we built our model on. In Section 5 we discuss the performance of our model in SemEval 2015 and in Section 6 we 704 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 704–708, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics conclude with a recap of our approach and suggestions for further research. 2 Dataset In order to train our </context>
</contexts>
<marker>Ho, 1998</marker>
<rawString>Tin Kam Ho. 1998. The random subspace method for constructing decision forests. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 20(8):832–844.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Keith Suderman</author>
</authors>
<title>The American National Corpus First Release.</title>
<date>2004</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference.</booktitle>
<contexts>
<context position="4796" citStr="Ide and Suderman, 2004" startWordPosition="770" endWordPosition="774"> URLs and changing the normalisation rules. Besides the tweet normalisation we also employed TwitIE for tokenisation, Part of Speech tagging and lemmatisation. We also used WordNet (Miller, 1995) to extract synonyms and synsets. We employed two sentiment lexicons, SentiWordNet3.0 (Baccianella et al., 2010) and the NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013) and two emotion lexicons NRC Hashtag Emotion Lexicon (Mohammad, 2012) and Depeche Mood (Staiano and Guerini, 2014). As frequency data for determining how often a word is used in English, we relied on the American National Corpus (Ide and Suderman, 2004); we also exploited the VU Amsterdam Metaphors Corpus (Steen et al., 2010) to find out how often a word is used in metaphors. Finally, the machine learning tool we used was Weka (Hall et al., 2009). 4 Our Method We employed different machine learning methods for the two tasks. In task 10, as the classes were only three (positive, negative and neutral) we opted for a supervised learning method, and from our experiments with several classifiers, Support Vector Machines resulted to be the best one. On the other hand, in task 11 tweets were classified as belonging to one of 11 polarity classes ass</context>
</contexts>
<marker>Ide, Suderman, 2004</marker>
<rawString>Nancy Ide and Keith Suderman. 2004. The American National Corpus First Release. In Proceedings of the Language Resources and Evaluation Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
<author>Colin Cherry</author>
<author>Saif Mohammad</author>
</authors>
<title>NRC-Canada-2014: Detecting Aspects and Sentiment in Customer Reviews.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014),</booktitle>
<location>Dublin, Ireland,</location>
<contexts>
<context position="6331" citStr="Kiritchenko et al., 2014" startWordPosition="1029" endWordPosition="1032">ion method (over the two subsets) but with no luck: pure regression methods fitted better task 11. In both tasks we characterised each tweet using nine groups of related features all describing both intrinsic aspects of the words and word patterns. These groups of features are the following: • Sentiments and Emotional Lexicons • Frequency • Lemma-Based • Ambiguity • Synonyms • Adjective/ Adverb Intensity • Characters • Part of Speech • Bad Words 4.1 Sentiments and Emotional Lexicons Using sentiment lexicons in Sentiment Analysis has been a common and rewarding practice (Mohammad et al., 2013; Kiritchenko et al., 2014). The characterisation of the sentiment associated to words in tweets is important for two reasons: to detect the global sentiment (e.g. if tweets contain mainly positive or negative terms) and, in the case of figurative language, to capture unexpectedness created by a negative word in a positive context and viceversa. Using the two sentiment lexicons and two emotional lexicons mentioned in Section 3, we computed the number of positive / negative words, the sum of the intensities of the positive /negative scores of words, the mean of positive / negative score of words, the greatest positive / </context>
</contexts>
<marker>Kiritchenko, Zhu, Cherry, Mohammad, 2014</marker>
<rawString>Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, and Saif Mohammad. 2014. NRC-Canada-2014: Detecting Aspects and Sentiment in Customer Reviews. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guofu Li</author>
<author>Aniruddha Ghosh</author>
<author>Tony Veale</author>
<author>Paolo Rosso</author>
<author>Ekaterina Shutova</author>
<author>Antonio Reyes</author>
<author>John Barnden</author>
</authors>
<title>Task 11: Sentiment Analysis of Figurative Language in Twitter.</title>
<date>2015</date>
<pages>4--5</pages>
<location>Denver, Colorado, USA,</location>
<contexts>
<context position="3944" citStr="Li et al., 2015" startWordPosition="630" endWordPosition="633">704 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 704–708, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics conclude with a recap of our approach and suggestions for further research. 2 Dataset In order to train our systems we used in each task only the dataset provided by the organisers. For task 10 we were able to retrieve 9689 tweets, tagged as positive, negative and neutral (Rosenthal et al., 2015). For task 11 the dataset was a collection of 8000 figurative tweets annotated with sentiment scores from -5 to +5 (Li et al., 2015). 3 Text Analysis and Tools In order to deal with the noisy text of Twitter we made use of the GATE application TwitIE (Bontcheva et al., 2013) where we modified the normaliser, adding new abbreviations, new slang words, removing URLs and changing the normalisation rules. Besides the tweet normalisation we also employed TwitIE for tokenisation, Part of Speech tagging and lemmatisation. We also used WordNet (Miller, 1995) to extract synonyms and synsets. We employed two sentiment lexicons, SentiWordNet3.0 (Baccianella et al., 2010) and the NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013) a</context>
<context position="10967" citStr="Li et al., 2015" startWordPosition="1817" endWordPosition="1820">ces of words characterised by a certain Part of Speech. The eight Part of Speech considered are Verbs, Nouns, Adjectives, Adverbs, Interjections, Determiners, Pronouns, and Appositions. 4.9 Bad Words Since Twitter messages often include bad words1, we count them as they may be used more often in negative messages. 5 Experiments and Results In this section we present our results in the two tasks (see Table 1 and Table 2). We only report final results (mean of Precision, Recall and F-Measure of each class), for more details please refer to the task 10 and task 11 papers (Rosenthal et al., 2015; Li et al., 2015). 1We enriched with more variants this list: https://github.com/shutterstock/List-of-Dirty-NaughtyObscene-and-Otherwise-Bad-Words 706 5.1 Task 10-B Given a message, classify whether the message is of positive, negative, or neutral sentiment. Our model scores at position 27th out of 40 groups. Systems were evaluated with the mean of the F-measures of Positive, Negative and Neutral classes. Our score is 9 points less than the best system. A considerable number of tweets in the test set were considered sarcastic tweets complicating the sentiment analysis task. With this test subset our system imp</context>
<context position="13911" citStr="Li et al., 2015" startWordPosition="2300" endWordPosition="2303">timent in each, and the degree to which this sentiment has been communicated. A vector space model was used to evaluate the similarity of the predictions of each participating system to the human-annotated gold standard. The list of expected gold-standard sentiment scores was used to construct a normalised gold-standard vector, while a comparable vector will be constructed from the predictions of a participating system. The cosine distance between vectors was then used as a measure of how well the participating system estimates the gold-standard sentiment scores for the whole of the test set (Li et al., 2015). In this task our model ranked second out of 15 Feature Cosine Similarity NRC H. Sentiment 0.578 SentiWordNet 0.562 Synonyms 0.564 Characters 0.550 Part of Speech 0.550 Depeche Mood 0.550 Lemma-Based 0.547 NRC H. Emotion 0.547 Bad Words 0.547 Frequency 0.546 Ambiguity 0.546 Intensity 0.544 Table 3: Task 11 contribution of each group of feature. The best feature group was Sentiment, in particular the features computed with the NRC Hashtag Sentiment Lexicon, see Section 4.1. 6 Conclusions In this paper we have described our participation to the SemEval task 10 and 11. Besides the word707 based </context>
</contexts>
<marker>Li, Ghosh, Veale, Rosso, Shutova, Reyes, Barnden, 2015</marker>
<rawString>Guofu Li, Aniruddha Ghosh, Tony Veale, Paolo Rosso, Ekaterina Shutova, Antonio Reyes, and John Barnden. 2015. Task 11: Sentiment Analysis of Figurative Language in Twitter. Denver, Colorado, USA, June, 4-5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: a lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="4368" citStr="Miller, 1995" startWordPosition="701" endWordPosition="702">s positive, negative and neutral (Rosenthal et al., 2015). For task 11 the dataset was a collection of 8000 figurative tweets annotated with sentiment scores from -5 to +5 (Li et al., 2015). 3 Text Analysis and Tools In order to deal with the noisy text of Twitter we made use of the GATE application TwitIE (Bontcheva et al., 2013) where we modified the normaliser, adding new abbreviations, new slang words, removing URLs and changing the normalisation rules. Besides the tweet normalisation we also employed TwitIE for tokenisation, Part of Speech tagging and lemmatisation. We also used WordNet (Miller, 1995) to extract synonyms and synsets. We employed two sentiment lexicons, SentiWordNet3.0 (Baccianella et al., 2010) and the NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013) and two emotion lexicons NRC Hashtag Emotion Lexicon (Mohammad, 2012) and Depeche Mood (Staiano and Guerini, 2014). As frequency data for determining how often a word is used in English, we relied on the American National Corpus (Ide and Suderman, 2004); we also exploited the VU Amsterdam Metaphors Corpus (Steen et al., 2010) to find out how often a word is used in metaphors. Finally, the machine learning tool we used was</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A Miller. 1995. WordNet: a lexical database for English. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>NRC-Canada: Building the State-of-theArt in Sentiment Analysis of Tweets.</title>
<date>2013</date>
<booktitle>In Proceedings of the seventh international workshop on Semantic Evaluation Exercises (SemEval-2013),</booktitle>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="4542" citStr="Mohammad et al., 2013" startWordPosition="726" endWordPosition="729"> to +5 (Li et al., 2015). 3 Text Analysis and Tools In order to deal with the noisy text of Twitter we made use of the GATE application TwitIE (Bontcheva et al., 2013) where we modified the normaliser, adding new abbreviations, new slang words, removing URLs and changing the normalisation rules. Besides the tweet normalisation we also employed TwitIE for tokenisation, Part of Speech tagging and lemmatisation. We also used WordNet (Miller, 1995) to extract synonyms and synsets. We employed two sentiment lexicons, SentiWordNet3.0 (Baccianella et al., 2010) and the NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013) and two emotion lexicons NRC Hashtag Emotion Lexicon (Mohammad, 2012) and Depeche Mood (Staiano and Guerini, 2014). As frequency data for determining how often a word is used in English, we relied on the American National Corpus (Ide and Suderman, 2004); we also exploited the VU Amsterdam Metaphors Corpus (Steen et al., 2010) to find out how often a word is used in metaphors. Finally, the machine learning tool we used was Weka (Hall et al., 2009). 4 Our Method We employed different machine learning methods for the two tasks. In task 10, as the classes were only three (positive, negative and n</context>
<context position="6304" citStr="Mohammad et al., 2013" startWordPosition="1025" endWordPosition="1028">5 to 0), then a regression method (over the two subsets) but with no luck: pure regression methods fitted better task 11. In both tasks we characterised each tweet using nine groups of related features all describing both intrinsic aspects of the words and word patterns. These groups of features are the following: • Sentiments and Emotional Lexicons • Frequency • Lemma-Based • Ambiguity • Synonyms • Adjective/ Adverb Intensity • Characters • Part of Speech • Bad Words 4.1 Sentiments and Emotional Lexicons Using sentiment lexicons in Sentiment Analysis has been a common and rewarding practice (Mohammad et al., 2013; Kiritchenko et al., 2014). The characterisation of the sentiment associated to words in tweets is important for two reasons: to detect the global sentiment (e.g. if tweets contain mainly positive or negative terms) and, in the case of figurative language, to capture unexpectedness created by a negative word in a positive context and viceversa. Using the two sentiment lexicons and two emotional lexicons mentioned in Section 3, we computed the number of positive / negative words, the sum of the intensities of the positive /negative scores of words, the mean of positive / negative score of word</context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. NRC-Canada: Building the State-of-theArt in Sentiment Analysis of Tweets. In Proceedings of the seventh international workshop on Semantic Evaluation Exercises (SemEval-2013), Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
</authors>
<title>Emotional Tweets.</title>
<date>2012</date>
<booktitle>In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012),</booktitle>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="4612" citStr="Mohammad, 2012" startWordPosition="739" endWordPosition="740"> noisy text of Twitter we made use of the GATE application TwitIE (Bontcheva et al., 2013) where we modified the normaliser, adding new abbreviations, new slang words, removing URLs and changing the normalisation rules. Besides the tweet normalisation we also employed TwitIE for tokenisation, Part of Speech tagging and lemmatisation. We also used WordNet (Miller, 1995) to extract synonyms and synsets. We employed two sentiment lexicons, SentiWordNet3.0 (Baccianella et al., 2010) and the NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013) and two emotion lexicons NRC Hashtag Emotion Lexicon (Mohammad, 2012) and Depeche Mood (Staiano and Guerini, 2014). As frequency data for determining how often a word is used in English, we relied on the American National Corpus (Ide and Suderman, 2004); we also exploited the VU Amsterdam Metaphors Corpus (Steen et al., 2010) to find out how often a word is used in metaphors. Finally, the machine learning tool we used was Weka (Hall et al., 2009). 4 Our Method We employed different machine learning methods for the two tasks. In task 10, as the classes were only three (positive, negative and neutral) we opted for a supervised learning method, and from our experi</context>
</contexts>
<marker>Mohammad, 2012</marker>
<rawString>Saif Mohammad. 2012. #Emotional Tweets. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), Montr´eal, Canada, 7-8 June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Platt</author>
</authors>
<title>Fast Training of Support Vector Machines Using Sequential Minimal Optimization. Advances in kernel methodssupport vector learning,</title>
<date>1999</date>
<pages>3</pages>
<contexts>
<context position="2954" citStr="Platt, 1999" startWordPosition="461" endWordPosition="462">n a tweet to try to model and thus automatically determine its polarity. Thanks to intrinsic word features, we aimed to detect two aspects of tweets: the style used (e.g. register used, frequent or rare words, positive or negative words, etc.) and the unexpectedness in the use of words, particularly important for figurative language. We also exploited textual features (like word occurrences, bigrams, skipgrams or other word patterns) in order to capture the way words are used in positive and negative tweets. As machine learning approach we choose the supervised method Support Vector Machines (Platt, 1999) for task 10 and the regression algorithm Random-Sub-Space (Ho, 1998) with M5P (Quinlan, 2014) as base algorithm for task 11. In Section 2 and 3 we describe the dataset used and the tools we employed to process the tweets. In Section 4 we introduce the features we built our model on. In Section 5 we discuss the performance of our model in SemEval 2015 and in Section 6 we 704 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 704–708, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics conclude with a recap of our approach a</context>
</contexts>
<marker>Platt, 1999</marker>
<rawString>John Platt. 1999. Fast Training of Support Vector Machines Using Sequential Minimal Optimization. Advances in kernel methodssupport vector learning, 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Potts</author>
</authors>
<title>Developing adjective scales from user-supplied textual metadata.</title>
<date>2011</date>
<booktitle>NSF Workshop on Restructuring Adjectives in WordNet. Arlington,VA.</booktitle>
<contexts>
<context position="9507" citStr="Potts (2011)" startWordPosition="1570" endWordPosition="1571">test / lowest number of synonyms with frequency higher than the one present in the tweet, the mean number of synonyms with frequency greater / lower than the frequency of the related word present in the tweet. We determine also the greatest / lowest number of synonyms and the mean number of synonyms of the words with frequency greater / lower than the one present in the the tweet (gap feature). We computed the set of Synonyms features by considering both all the words and also restricting the calculation to words with the Part of Speech tags as above. 4.6 Adjective/ Adverb Intensity Using the Potts (2011) intensity scores of Adjectives and Adverbs, we calculated three features: the most intense adjective/adverb and the intensity mean of the adjective/adverb of the tweet. 4.7 Characters We also wanted to capture the punctuation style of the author of a tweet. Punctuation and type of characters used are very important in social networks: a full stop at the end of a subjective message may change the polarity of the message. Each feature is a count of specific punctuation marks, including: “.”, “#”, “!”, “?”, “$”, “%”, “&amp;”, “+”, “-”, “=”, “/”. Moreover we count as well number of uppercase and lowe</context>
</contexts>
<marker>Potts, 2011</marker>
<rawString>Christopher Potts. 2011. Developing adjective scales from user-supplied textual metadata. NSF Workshop on Restructuring Adjectives in WordNet. Arlington,VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross Quinlan</author>
</authors>
<title>C4. 5: programs for machine learning.</title>
<date>2014</date>
<publisher>Elsevier.</publisher>
<contexts>
<context position="3048" citStr="Quinlan, 2014" startWordPosition="476" endWordPosition="477"> word features, we aimed to detect two aspects of tweets: the style used (e.g. register used, frequent or rare words, positive or negative words, etc.) and the unexpectedness in the use of words, particularly important for figurative language. We also exploited textual features (like word occurrences, bigrams, skipgrams or other word patterns) in order to capture the way words are used in positive and negative tweets. As machine learning approach we choose the supervised method Support Vector Machines (Platt, 1999) for task 10 and the regression algorithm Random-Sub-Space (Ho, 1998) with M5P (Quinlan, 2014) as base algorithm for task 11. In Section 2 and 3 we describe the dataset used and the tools we employed to process the tweets. In Section 4 we introduce the features we built our model on. In Section 5 we discuss the performance of our model in SemEval 2015 and in Section 6 we 704 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 704–708, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics conclude with a recap of our approach and suggestions for further research. 2 Dataset In order to train our systems we used in each t</context>
</contexts>
<marker>Quinlan, 2014</marker>
<rawString>J Ross Quinlan. 2014. C4. 5: programs for machine learning. Elsevier.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Preslav Nakov</author>
<author>Svetlana Kiritchenko</author>
<author>Saif M Mohammad</author>
<author>Alan Ritter</author>
<author>Veselin Stoyanov</author>
</authors>
<title>Semeval-2015 task 10: Sentiment analysis in twitter.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’2015,</booktitle>
<location>Denver, Colorado, USA,</location>
<contexts>
<context position="3812" citStr="Rosenthal et al., 2015" startWordPosition="606" endWordPosition="609"> we introduce the features we built our model on. In Section 5 we discuss the performance of our model in SemEval 2015 and in Section 6 we 704 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 704–708, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics conclude with a recap of our approach and suggestions for further research. 2 Dataset In order to train our systems we used in each task only the dataset provided by the organisers. For task 10 we were able to retrieve 9689 tweets, tagged as positive, negative and neutral (Rosenthal et al., 2015). For task 11 the dataset was a collection of 8000 figurative tweets annotated with sentiment scores from -5 to +5 (Li et al., 2015). 3 Text Analysis and Tools In order to deal with the noisy text of Twitter we made use of the GATE application TwitIE (Bontcheva et al., 2013) where we modified the normaliser, adding new abbreviations, new slang words, removing URLs and changing the normalisation rules. Besides the tweet normalisation we also employed TwitIE for tokenisation, Part of Speech tagging and lemmatisation. We also used WordNet (Miller, 1995) to extract synonyms and synsets. We employe</context>
<context position="10949" citStr="Rosenthal et al., 2015" startWordPosition="1813" endWordPosition="1816">s the number of occurrences of words characterised by a certain Part of Speech. The eight Part of Speech considered are Verbs, Nouns, Adjectives, Adverbs, Interjections, Determiners, Pronouns, and Appositions. 4.9 Bad Words Since Twitter messages often include bad words1, we count them as they may be used more often in negative messages. 5 Experiments and Results In this section we present our results in the two tasks (see Table 1 and Table 2). We only report final results (mean of Precision, Recall and F-Measure of each class), for more details please refer to the task 10 and task 11 papers (Rosenthal et al., 2015; Li et al., 2015). 1We enriched with more variants this list: https://github.com/shutterstock/List-of-Dirty-NaughtyObscene-and-Otherwise-Bad-Words 706 5.1 Task 10-B Given a message, classify whether the message is of positive, negative, or neutral sentiment. Our model scores at position 27th out of 40 groups. Systems were evaluated with the mean of the F-measures of Positive, Negative and Neutral classes. Our score is 9 points less than the best system. A considerable number of tweets in the test set were considered sarcastic tweets complicating the sentiment analysis task. With this test sub</context>
</contexts>
<marker>Rosenthal, Nakov, Kiritchenko, Mohammad, Ritter, Stoyanov, 2015</marker>
<rawString>Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko, Saif M Mohammad, Alan Ritter, and Veselin Stoyanov. 2015. Semeval-2015 task 10: Sentiment analysis in twitter. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’2015, Denver, Colorado, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacopo Staiano</author>
<author>Marco Guerini</author>
</authors>
<title>DepecheMood: a Lexicon for Emotion Analysis from Crowd-Annotated News.</title>
<date>2014</date>
<booktitle>In 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers),</booktitle>
<pages>427433</pages>
<location>Baltimore, Maryland, USA,,</location>
<contexts>
<context position="4657" citStr="Staiano and Guerini, 2014" startWordPosition="744" endWordPosition="748"> of the GATE application TwitIE (Bontcheva et al., 2013) where we modified the normaliser, adding new abbreviations, new slang words, removing URLs and changing the normalisation rules. Besides the tweet normalisation we also employed TwitIE for tokenisation, Part of Speech tagging and lemmatisation. We also used WordNet (Miller, 1995) to extract synonyms and synsets. We employed two sentiment lexicons, SentiWordNet3.0 (Baccianella et al., 2010) and the NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013) and two emotion lexicons NRC Hashtag Emotion Lexicon (Mohammad, 2012) and Depeche Mood (Staiano and Guerini, 2014). As frequency data for determining how often a word is used in English, we relied on the American National Corpus (Ide and Suderman, 2004); we also exploited the VU Amsterdam Metaphors Corpus (Steen et al., 2010) to find out how often a word is used in metaphors. Finally, the machine learning tool we used was Weka (Hall et al., 2009). 4 Our Method We employed different machine learning methods for the two tasks. In task 10, as the classes were only three (positive, negative and neutral) we opted for a supervised learning method, and from our experiments with several classifiers, Support Vecto</context>
</contexts>
<marker>Staiano, Guerini, 2014</marker>
<rawString>Jacopo Staiano and Marco Guerini. 2014. DepecheMood: a Lexicon for Emotion Analysis from Crowd-Annotated News. In 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), page 427433, Baltimore, Maryland, USA,, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard J Steen</author>
<author>Aletta G Dorst</author>
<author>J Berenike Herrmann</author>
<author>Anna Kaal</author>
<author>Tina Krennmayr</author>
<author>Trijntje Pasma</author>
</authors>
<title>A method for linguistic metaphor identification: From MIP to MIPVU, volume 14.</title>
<date>2010</date>
<publisher>John Benjamins Publishing.</publisher>
<contexts>
<context position="4870" citStr="Steen et al., 2010" startWordPosition="784" endWordPosition="787"> also employed TwitIE for tokenisation, Part of Speech tagging and lemmatisation. We also used WordNet (Miller, 1995) to extract synonyms and synsets. We employed two sentiment lexicons, SentiWordNet3.0 (Baccianella et al., 2010) and the NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013) and two emotion lexicons NRC Hashtag Emotion Lexicon (Mohammad, 2012) and Depeche Mood (Staiano and Guerini, 2014). As frequency data for determining how often a word is used in English, we relied on the American National Corpus (Ide and Suderman, 2004); we also exploited the VU Amsterdam Metaphors Corpus (Steen et al., 2010) to find out how often a word is used in metaphors. Finally, the machine learning tool we used was Weka (Hall et al., 2009). 4 Our Method We employed different machine learning methods for the two tasks. In task 10, as the classes were only three (positive, negative and neutral) we opted for a supervised learning method, and from our experiments with several classifiers, Support Vector Machines resulted to be the best one. On the other hand, in task 11 tweets were classified as belonging to one of 11 polarity classes associated with values ranging from -5 to 5, hence a regression approach was </context>
</contexts>
<marker>Steen, Dorst, Herrmann, Kaal, Krennmayr, Pasma, 2010</marker>
<rawString>Gerard J Steen, Aletta G Dorst, J Berenike Herrmann, Anna Kaal, Tina Krennmayr, and Trijntje Pasma. 2010. A method for linguistic metaphor identification: From MIP to MIPVU, volume 14. John Benjamins Publishing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>