<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000780">
<title confidence="0.9963605">
Accurate Word Segmentation using Transliteration
and Language Model Projection
</title>
<author confidence="0.988971">
Masato Hagiwara Satoshi Sekine
</author>
<affiliation confidence="0.987346">
Rakuten Institute of Technology, New York
</affiliation>
<address confidence="0.854738">
215 Park Avenue South, New York, NY
</address>
<email confidence="0.99743">
fmasato.hagiwara, satoshi.b.sekine}@mail.rakuten.com
</email>
<sectionHeader confidence="0.980086" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999850214285714">
Transliterated compound nouns not
separated by whitespaces pose diffi-
culty on word segmentation (WS). Of-
fline approaches have been proposed to
split them using word statistics, but
they rely on static lexicon, limiting
their use. We propose an online ap-
proach, integrating source LM, and/or,
back-transliteration and English LM.
The experiments on Japanese and Chi-
nese WS have shown that the pro-
posed models achieve significant im-
provement over state-of-the-art, reduc-
ing 16% errors in Japanese.
</bodyText>
<sectionHeader confidence="0.995497" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99990425">
Accurate word segmentation (WS) is the
key components in successful language pro-
cessing. The problem is pronounced in lan-
guages such as Japanese and Chinese, where
words are not separated by whitespaces. In
particular, compound nouns pose difficulties
to WS since they are productive, and often
consist of unknown words.
In Japanese, transliterated foreign com-
pound words written in Katakana are ex-
tremely difficult to split up into components
without proper lexical knowledge. For ex-
ample, when splitting a compound noun ブ
ラキッシュレッド burakisshureddo, a traditional
word segmenter can easily segment this as ブ
ラキッ/シュレッド “*blacki shred” since シュレッ
ド shureddo “shred” is a known, frequent word.
It is only the knowledge that ブラキッburaki
(*“blacki”) is not a valid word which prevents
this. Knowing that the back-transliterated un-
igram “blacki” and bigram “blacki shred” are
unlikely in English can promote the correct
WS, ブラキッシュ/レッド “blackish red”. In Chi-
nese, the problem can be more severe since
the language does not have a separate script
to represent transliterated words.
Kaji and Kitsuregawa (2011) tackled
Katakana compound splitting using back-
transliteration and paraphrasing. Their ap-
proach falls into an offline approach, which
focuses on creating dictionaries by extract-
ing new words from large corpora separately
before WS. However, offline approaches have
limitation unless the lexicon is constantly
updated. Moreover, they only deal with
Katakana, but their method is not directly ap-
plicable to Chinese since the language lacks a
separate script for transliterated words.
Instead, we adopt an online approach, which
deals with unknown words simultaneously as
the model analyzes the input. Our ap-
proach is based on semi-Markov discrimina-
tive structure prediction, and it incorporates
English back-transliteration and English lan-
guage models (LMs) into WS in a seamless
way. We refer to this process of transliterat-
ing unknown words into another language and
using the target LM as LM projection. Since
the model employs a general transliteration
model and a general English LM, it achieves
robust WS for unknown words. To the best
of our knowledge, this paper is the first to use
transliteration and projected LMs in an online,
seamlessly integrated fashion for WS.
To show the effectiveness of our approach,
we test our models on a Japanese balanced cor-
pus and an electronic commerce domain cor-
pus, and a balanced Chinese corpus. The re-
sults show that we achieved a significant im-
provement in WS accuracy in both languages.
</bodyText>
<sectionHeader confidence="0.996778" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.95212">
In Japanese WS, unknown words are usu-
ally dealt with in an online manner with the
unknown word model, which uses heuristics
</bodyText>
<page confidence="0.987743">
183
</page>
<bodyText confidence="0.964003285714286">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 183–189,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
depending on character types (Kudo et al.,
2004). Nagata (1999) proposed a Japanese un-
known word model which considers PoS (part
of speech), word length model and orthog-
raphy. Uchimoto et al. (2001) proposed a
maximum entropy morphological analyzer ro-
bust to unknown words. In Chinese, Peng et
al. (2004) used CRF confidence to detect new
words.
For offline approaches, Mori and Nagao
(1996) extracted unknown word and estimated
their PoS from a corpus through distributional
analysis. Asahara and Matsumoto (2004)
built a character-based chunking model using
SVM for Japanese unknown word detection.
Kaji and Kitsuregawa (2011)’s approach is
the closest to ours. They built a model
to split Katakana compounds using back-
transliteration and paraphrasing mined from
large corpora. Nakazawa et al. (2005) is
a similar approach, using a Ja-En dictionary
to translate compound components and check
their occurrence in an English corpus. Sim-
ilar approaches are proposed for other lan-
guages, such as German (Koehn and Knight,
2003) and Urdu-Hindi (Lehal, 2010). Correct
splitting of compound nouns has a positive ef-
fect on MT (Koehn and Knight, 2003) and IR
(Braschler and Ripplinger, 2004).
A similar problem can be seen in Korean,
German etc. where compounds may not be
explicitly split by whitespaces. Koehn and
Knight (2003) tackled the splitting problem in
German, by using word statistics in a mono-
lingual corpus. They also used the informa-
tion whether translations of compound parts
appear in a German-English bilingual corpus.
Lehal (2010) used Urdu-Devnagri translitera-
tion and a Hindi corpus for handling the space
omission problem in Urdu compound words.
</bodyText>
<sectionHeader confidence="0.994795" genericHeader="method">
3 Word Segmentation Model
</sectionHeader>
<bodyText confidence="0.999870444444445">
Out baseline model is a semi-Markov struc-
ture prediction model which estimates WS and
the PoS sequence simultaneously (Kudo et al.,
2004; Zhang and Clark, 2008). This model
finds the best output y* from the input sen-
tence string x as: y* = arg maxyEY(x) w *y).
Here, Y (x) denotes all the possible sequences
of words derived from x. The best analysis is
determined by the feature function φ(y) the
</bodyText>
<equation confidence="0.748750428571429">
ID Feature ID Feature
1 wi 13 w1i−1w1i
2 t1 14 t1 i−1t1
3* i 16* i
4* t1i t215* 17* t1 i−1t2 i−1t1 i t2
5* i 18* i
6* t1 i t2 it3 19 t1 i−1t2 i−1t3 i−1t1 i t2 i t3
7 i 20 i
8* t1 i t2 i t5 i t6 21 t1 i−1t2 i−1t5 i−1t6 i−1t1 i t2 i t5 i t6
9* i 22 i
10* t1 i t2 i t6 t1 i−1t2 i−1t6 i−1t1 i t2 i t6
11* i i
12 wit1 φLMS
i 1 (wi)
wit1 i t2 φLMS
i 2 (wi−1, wi)
wit1 i t2 i t3 φLMP
i 1 (wi)
wit1 i t2 i t5 i t6 i φLMP
wit1it2it6i 2 (wi−1, wi)
c(wi)l(wi)
</equation>
<tableCaption confidence="0.88443">
Table 1: Features for WS &amp; PoS tagging
</tableCaption>
<bodyText confidence="0.992646375">
weight vector w. WS is conducted by stan-
dard Viterbi search based on lattice, which
is illustrated in Figure 1. We limit the fea-
tures to word unigram and bigram features,
i.e., φ(y) = ∑i[φ1(wi) + φ2(wi−1,wi)] for y =
w1...w,,,. By factoring the feature function into
these two subsets, argmax can be efficiently
searched by the Viterbi algorithm, with its
computational complexity proportional to the
input length. We list all the baseline features
in Table 11. The asterisks (*) indicate the fea-
ture is used for Japanese (JA) but not for Chi-
nese (ZH) WS. Here, wi and wi−1 denote the
current and previous word in question, and t1i
and t�i−1 are level-j PoS tags assigned to them.
l(w) and c(w) are the length and the set of
character types of word w.
If there is a substring for which no dic-
tionary entries are found, the unknown word
model is invoked. In Japanese, our unknown
word model relies on heuristics based on char-
acter types and word length to generate word
nodes, similar to that of MeCab (Kudo et
al., 2004). In Chinese, we aggregated con-
secutive 1 to 4 characters add them as “n
(common noun)”, “ns (place name)”, “nr (per-
sonal name)”, and “nz (other proper nouns),”
since most of the unknown words in Chinese
are proper nouns. Also, we aggregated up to
20 consecutive numerical characters, making
them a single node, and assign “m” (number).
For other character types, a single node with
PoS “w (others)” is created.
&apos;The Japanese dictionary and the corpus we used
have 6 levels of PoS tag hierarchy, while the Chinese
ones have only one level, which is why some of the
PoS features are not included in Chinese. As character
type, Hiragana (JA), Katakana (JA), Latin alphabet,
Number, Chinese characters, and Others, are distin-
guished. Word length is in Unicode.
</bodyText>
<page confidence="0.997228">
184
</page>
<figureCaption confidence="0.999724">
Figure 1: Example lattice with LM projection
</figureCaption>
<bodyText confidence="0.9342467">
4 Use of Language Model
Language Model Augmentation Analo-
gous to Koehn and Knight (2003), we can ex-
ploit the fact that レッド reddo (red) in the
example ブラキッシュレッド is such a common
word that one can expect it appears frequently
in the training corpus. To incorporate this
intuition, we used log probability of n-gram
as features, which are included in Table 1
(ID 19 and 20): φLMS
</bodyText>
<equation confidence="0.890565">
1 (wi) = logp(wi) and
φLMS
2 (wi−1, wi) = logp(wi−1, wi). Here the
</equation>
<bodyText confidence="0.9372483">
empirical probability p(wi) and p(wi−1, wi) are
computed from the source language corpus. In
Japanese, we applied this source language aug-
mentation only to Katakana words. In Chi-
nese, we did not limit the target.
4.1 Language Model Projection
As we mentioned in Section 2, English
LM knowledge helps split transliterated com-
pounds. We use (LM) projection, which is
a combination of back-transliteration and an
English model, by extending the normal lat-
tice building process as follows:
Firstly, when the lattice is being built, each
node is back-transliterated and the resulting
nodes are associated with it, as shown in
Figure 1 as the shaded nodes. Then, edges
are spanned between these extended English
nodes, instead of between the original nodes,
by additionally taking into consideration En-
glish LM features (ID 21 and 22 in Table 1):
φLMP 1(wi) = logp(wi) and φLMP 2(wi−1, wi) =
logp(wi−1, wi). Here the empirical probabil-
ity p(wi) and p(wi−1,wi) are computed from
the English corpus. For example, Feature 21
is set to φLMP 1(“blackish”) for node (a), to
φLMP
1 (“red”) for node (b), and Feature 22 is
set to φLMP
2 (“blackish”, “red”) for edge (c) in
Figure 1. If no transliterations were generated,
or the n-grams do not appear in the English
corpus, a small frequency ε is assumed.
Finally, the created edges are traversed from
EOS, and associated original nodes are chosen
as the WS result. In Figure 1, the bold edges
are traversed at the final step, and the corre-
sponding nodes “大 - 人気 - 色 - ブラキッシュ-
レッド” are chosen as the final WS result.
For Japanese, we only expand and project
Katakana noun nodes (whether they are
known or unknown words) since transliterated
words are almost always written in Katakana.
For Chinese, only “ns (place name)”, “nr (per-
sonal name)”, and “nz (other proper noun)”
nodes whose surface form is more than 1-
character long are transliterated. As the En-
glish LM, we used Google Web 1T 5-gram Ver-
sion 1 (Brants and Franz, 2006), limiting it to
unigrams occurring more than 2000 times and
bigrams occurring more than 500 times.
</bodyText>
<sectionHeader confidence="0.98795" genericHeader="method">
5 Transliteration
</sectionHeader>
<bodyText confidence="0.999763538461539">
For transliterating Japanese/Chinese words
back to English, we adopted the Joint Source
Channel (JSC) Model (Li et al., 2004), a gen-
erative model widely used as a simple yet pow-
erful baseline in previous research e.g., (Hagi-
wara and Sekine, 2012; Finch and Sumita,
2010).2 The JSC model, given an input
of source word s and target word t, de-
fines the transliteration probability based on
transliteration units (TUs) ui = (si, ti) as:
PJSC((s, t)) = ∏fi=1 P(ui|ui−n+1, ..., ui−1),
where f is the number of TUs in a given source
/ target word pair. TUs are atomic pair units
of source / target words, such as “la/ラ” and
“ish/ッシュ”. The TU n-gram probabilities are
learned from a training corpus by following it-
erative updates similar to the EM algorithm3.
In order to generate transliteration candidates,
we used a stack decoder described in (Hagi-
wara and Sekine, 2012). We used the training
data of the NEWS 2009 workshop (Li et al.,
2009a; Li et al., 2009b).
As reference, we measured the performance
on its own, using NEWS 2009 (Li et al., 2009b)
data. The percentage of correctly transliter-
ated words are 37.9% for Japanese and 25.6%
</bodyText>
<footnote confidence="0.983984">
2Note that one could also adopt other generative /
discriminative transliteration models, such as (Jiampo-
jamarn et al., 2007; Jiampojamarn et al., 2008).
3We only allow TUs whose length is shorter than or
equal to 3, both in the source and target side.
</footnote>
<figure confidence="0.997447451612903">
Input: )�, A % 10- .;&amp;quot; 7 zk- l ✓ z L l 1
very popular color blackish red
BOS
大 人 気
大 人 気
大 人 気 色
Transliteration
Model
色
ブ bu
ブ ラ
bla bra
ブ ラ キ
brakiblaki
ブ ラ キ ッ
brakiblaki
ブ キ ッbr blackish シ
.
node (a)
キ ッ シ ュ
kish
キ ッ
ki
シ ュ レッ ド
shrad
shred
. . .
edge (c)
レ ドlede
rea os red
node (b)
</figure>
<page confidence="0.99624">
185
</page>
<bodyText confidence="0.999935625">
for Chinese. Although the numbers seem low
at a first glance, Chinese back-transliteration
itself is a very hard task, mostly because
Chinese phonology is so different from En-
glish that some sounds may be dropped when
transliterated. Therefore, we can regard this
performance as a lower bound of the translit-
eration module performance we used for WS.
</bodyText>
<sectionHeader confidence="0.999188" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.997727">
6.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.99944516">
Corpora For Japanese, we used (1) EC
corpus, consists of 1,230 product titles and
descriptions randomly sampled from Rakuten
(Rakuten-Inc., 2012). The corpus is manually
annotated with the BCCWJ style WS (Ogura
et al., 2011). It consists of 118,355 tokens, and
has a relatively high percentage of Katakana
words (11.2%). (2) BCCWJ (Maekawa, 2008)
CORE (60,374 sentences, 1,286,899 tokens,
out of which approx. 3.58% are Katakana
words). As the dictionary, we used UniDic
(Den et al., 2007). For Chinese, we used
LCMC (McEnery and Xiao, 2004) (45,697 sen-
tences and 1,001,549 tokens). As the dictio-
nary, we used CC-CEDICT (MDGB, 2011)4.
Training and Evaluation We used Aver-
aged Perceptron (Collins, 2002) (3 iterations)
for training, with five-fold cross-validation. As
for the evaluation metrics, we used Precision
(Prec.), Recall (Rec.), and F-measure (F). We
additionally evaluated the performance lim-
ited to Katakana (JA) or proper nouns (ZH)
in order to see the impact of compound split-
ting. We also used word error rate (WER) to
see the relative change of errors.
</bodyText>
<subsectionHeader confidence="0.995596">
6.2 Japanese WS Results
</subsectionHeader>
<bodyText confidence="0.997028571428571">
We compared the baseline model, the
augmented model with the source language
(+LM-S) and the projected model (+LM-P).
Table 3 shows the result of the proposed mod-
els and major open-source Japanese WS sys-
tems, namely, MeCab 0.98 (Kudo et al., 2004),
JUMAN 7.0 (Kurohashi and Nagao, 1994),
</bodyText>
<footnote confidence="0.6612705">
4Since the dictionary is not explicitly annotated
with PoS tags, we firstly took the intersection of the
training corpus and the dictionary words, and assigned
all the possible PoS tags to the words which appeared
in the corpus. All the other words which do not appear
in the training corpus are discarded.
</footnote>
<bodyText confidence="0.999988292682927">
and KyTea 0.4.2 (Neubig et al., 2011) 5. We
observed slight improvement by incorporat-
ing the source LM, and observed a 0.48 point
F-value increase over baseline, which trans-
lates to 4.65 point Katakana F-value change
and 16.0% (3.56% to 2.99 %) WER reduc-
tion, mainly due to its higher Katakana word
rate (11.2%). Here, MeCab+UniDic achieved
slightly better Katakana WS than the pro-
posed models. This may be because it is
trained on a much larger training corpus (the
whole BCCWJ). The same trend is observed
for BCCWJ corpus (Table 2), where we gained
statistically significant 1 point F-measure in-
crease on Katakana word.
Many of the improvements of +LM-S over
Baseline come from finer grained splitting,
for example, * レインスーツ reinsuutsu “rain
suits” to レイン/スーツ, while there is wrong
over-splitting, e.g., テレキャスターterekyasutaa
“Telecaster” to * テレ/キャスター. This type of
error is reduced by +LM-P, e.g., * プラス/チッ
ク purasu chikku “*plus tick” to プラスチック
purasuchikku “plastic” due to LM projection.
+LM-P also improved compounds whose com-
ponents do not appear in the training data,
such as * ルーカスフィルム ruukasufirumu to
ルーカス/フィルム “Lucus Film.” Indeed, we
randomly extracted 30 Katakana differences
between +LM-S and +LM-P, and found out
that 25 out of 30 (83%) are true improvement.
One of the proposed method’s advantages is
that it is very robust to variations, such as
アクティベイティッド akutibeitiddo “activated,”
even though only the original form, アクティベ
イト akutibeito “activate” is in the dictionary.
One type of errors can be attributed
to non-English words such as ス ノ コ ベッド
sunokobeddo, which is a compound of Japanese
word スノコ sunoko “duckboard” and an En-
glish word ベッド beddo “bed.”
</bodyText>
<subsectionHeader confidence="0.996029">
6.3 Chinese WS Results
</subsectionHeader>
<bodyText confidence="0.999609333333333">
We compare the results on Chinese WS,
with Stanford Segmenter (Tseng et al., 2005)
(Table 4) 6. Including +LM-S decreased the
</bodyText>
<footnote confidence="0.999541142857143">
5Because MeCab+UniDic and KyTea models are
actually trained on BCCWJ itself, this evaluation is
not meaningful but just for reference. The WS granu-
larity of IPADic, JUMAN, and KyTea is also different
from the BCCWJ style.
6Note that the comparison might not be fair since
(1) Stanford segmenter’s criteria are different from
</footnote>
<page confidence="0.990964">
186
</page>
<table confidence="0.999499625">
Model Prec. (O) Rec. (O) F (O) Prec. (K) Rec. (K) F (K) WER
MeCab+IPADic 91.28 89.87 90.57 88.74 82.32 85.41 12.87
MeCab+UniDic* (98.84) (99.33) (99.08) (96.51) (97.34) (96.92) (1.31)
JUMAN 85.66 78.15 81.73 91.68 88.41 90.01 23.49
KyTea* (81.84) (90.12) (85.78) (99.57) (99.73) (99.65) (20.02)
Baseline 96.36 96.57 96.47 84.83 84.36 84.59 4.54
+LM-S 96.36 96.57 96.47 84.81 84.36 84.59 4.54
+LM-S+LM-P 96.39 96.61 96.50 85.59 85.40 85.50 4.50
</table>
<tableCaption confidence="0.956181">
Table 2: Japanese WS Performance (%) on BCCWJ — Overall (O) and Katakana (K)
</tableCaption>
<table confidence="0.99973325">
Model Prec. (O) Rec. (O) F (O) Prec. (K) Rec. (K) F (K) WER
MeCab+IPADic 84.36 87.31 85.81 86.65 73.47 79.52 20.34
MeCab+UniDic 95.14 97.55 96.33 93.88 93.22 93.55 5.46
JUMAN 90.99 87.13 89.2 92.37 88.02 90.14 14.56
KyTea 82.00 86.53 84.21 93.47 90.32 91.87 21.90
Baseline 97.50 97.00 97.25 89.61 85.40 87.45 3.56
+LM-S 97.79 97.37 97.58 92.58 88.99 90.75 3.17
+LM-S+LM-P 97.90 97.55 97.73 93.62 90.64 92.10 2.99
</table>
<tableCaption confidence="0.994904">
Table 3: Japanese WS Performance (%) on the EC domain corpus
</tableCaption>
<table confidence="0.9995236">
Model Prec. (O) Rec. (O) F (O) Prec. (P) Rec. (P) F (P) WER
Stanford Segmenter 87.06 86.38 86.72 — — — 17.45
Baseline 90.65 90.87 90.76 83.29 51.45 63.61 12.21
+LM-S 90.54 90.78 90.66 72.69 43.28 54.25 12.32
+LM-P 90.90 91.48 91.19 75.04 52.11 61.51 11.90
</table>
<tableCaption confidence="0.999818">
Table 4: Chinese WS Performance (%) — Overall (O) and Proper Nouns (P)
</tableCaption>
<bodyText confidence="0.999920628571429">
performance, which may be because one can-
not limit where the source LM features are
applied. This is why the result of +LM-
S+LM-P is not shown for Chinese. On the
other hand, replacing LM-S with LM-P im-
proved the performance significantly. We
found positive changes such as * 欧 麦/尔
萨 利 赫 oumai/ersalihe to 欧 麦 尔/萨 利 赫
oumaier/salihe “Umar Saleh” and * 领导/人
曼德拉 lingdao/renmandela to 领导人/曼德拉
lingdaoren/mandela“Leader Mandela”. How-
ever, considering the overall F-measure in-
crease and proper noun F-measure decrease
suggests that the effect of LM projection is
not limited to proper nouns but also promoted
finer granularity because we observed proper
noun recall increase.
One of the reasons which make Chinese LM
projection difficult is the corpus allows sin-
gle tokens with a transliterated part and Chi-
nese affices, e.g., 马克思主义者 makesizhuy-
izhe “Marxists” (马克思 makesi “Marx” + 主
义者 zhuyizhe “-ist (believers)”) and 尼罗河
niluohe “Nile River” ( 尼罗 niluo “Nile” +
河 he “-river”). Another source of errors is
transliteration accuracy. For example, no ap-
ours, and (2) our model only uses the intersection of
the training set and the dictionary. Proper noun per-
formance for the Stanford segmenter is not shown since
it does not assign PoS tags.
propriate transliterations were generated for
维娜斯 weinasi “Venus,” which is commonly
spelled 维纳斯 weinasi. Improving the JSC
model could improve the LM projection per-
formance.
</bodyText>
<sectionHeader confidence="0.948359" genericHeader="conclusions">
7 Conclusion and Future Works
</sectionHeader>
<bodyText confidence="0.999981095238095">
In this paper, we proposed a novel, on-
line WS model for the Japanese/Chinese
compound word splitting problem, by seam-
lessly incorporating the knowledge that back-
transliteration of properly segmented words
also appear in an English LM. The experi-
mental results show that the model achieves
a significant improvement over the baseline
and LM augmentation, achieving 16% WER
reduction in the EC domain.
The concept of LM projection is general
enough to be used for splitting other com-
pound nouns. For example, for Japanese per-
sonal names such as 仲里依紗 Naka Riisa, if
we could successfully estimate the pronuncia-
tion Nakariisa and look up possible splits in
an English LM, one is expected to find a cor-
rect WS Naka Riisa because the first and/or
the last name are mentioned in the LM. Seek-
ing broader application of LM projection is a
future work.
</bodyText>
<page confidence="0.996744">
187
</page>
<sectionHeader confidence="0.977557" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999525952380953">
Masayuki Asahara and Yuji Matsumoto. 2004.
Japanese unknown word identification by
character-based chunking. In Proceedings of
COLING 2004, pages 459–465.
Thorsten Brants and Alex Franz. 2006. Web 1T
5-gram Version 1. Linguistic Data Consortium.
Martin Braschler and Bärbel Ripplinger. 2004.
How effective is stemming and decompounding
for german text retrieval? Information Re-
trieval, pages 291–316.
Michael Collins. 2002. Discriminative training
methods for hidden markov models: theory and
experiments with perceptron algorithms. In
Proceedings of EMNLP 2012, pages 1–8.
Yasuharu Den, Toshinobu Ogiso, Hideki Ogura,
Atsushi Yamada, Nobuaki Minematsu, Kiy-
otaka Uchimoto, and Hanae Koiso. 2007.
The development of an electronic dictionary
for morphological analysis and its application
to Japanese corpus linguistics (in Japanese).
Japanese linguistics, 22:101–122.
Andrew Finch and Eiichiro Sumita. 2010. A
bayesian model of bilingual segmentation for
transliteration. In Proceedings of IWSLT 2010,
pages 259–266.
Masato Hagiwara and Satoshi Sekine. 2012. La-
tent class transliteration based on source lan-
guage origin. In Proceedings of NEWS 2012,
pages 30–37.
Sittichai Jiampojamarn, Grzegorz Kondrak, and
Tarek Sherif. 2007. Applying many-to-many
alignments and hidden markov models to letter-
to-phoneme conversion. In Proceedings of
NAACL-HLT 2007, pages 372–379.
Sittichai Jiampojamarn, Colin Cherry, and Grze-
gorz Kondrak. 2008. Joint processing and dis-
criminative training for letter-to-phoneme con-
version. In Proceedings of ACL 2008, pages
905–913.
Nobuhiro Kaji and Masaru Kitsuregawa. 2011.
Splitting noun compounds via monolingual and
bilingual paraphrasing: A study on japanese
katakana words. In Proceedings of the EMNLP
2011, pages 959–969.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In Proceedings
of EACL 2003, pages 187–193.
Taku Kudo, Kaoru Yamamoto, and Yuji Mat-
sumoto. 2004. Applying conditional random
fields to Japanese morphological analysis. In
Proceedings of EMNLP 2004, pages 230–237.
Sadao Kurohashi and Makoto Nagao. 1994. Im-
provements of Japanese morphological analyzer
juman. In Proceedings of the International
Workshop on Sharable Natural Language Re-
sources, pages 22–38.
Gurpreet Singh Lehal. 2010. A word segmentation
system for handling space omission problem in
urdu script. In Proceedings of the 1st Workshop
on South and Southeast Asian Natural Language
Processing (WSSANLP), pages 43–50.
Haizhou Li, Zhang Min, and Su Jian. 2004. A
joint source-channel model for machine translit-
eration. In Proceedings of ACL 2004, pages
159–166.
Haizhou Li, A Kumaran, Vladimir Pervouchine,
and Min Zhang. 2009a. Report of news 2009
machine transliteration shared task. In Proceed-
ings of NEWS 2009, pages 1–18.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2009b. Whitepaper of news 2009
machine transliteration shared task. In Proceed-
ings of NEWS 2009, pages 19–26.
Kikuo Maekawa. 2008. Compilation of
the Kotonoha-BCCWJ corpus (in Japanese).
Nihongo no kenkyu (Studies in Japanese),
4(1):82–95.
Anthony McEnery and Zhonghua Xiao. 2004. The
lancaster corpus of mandarin chinese: A corpus
for monolingual and contrastive language study.
In Proceedings of LREC 2004, pages 1175–1178.
MDGB. 2011. CC-CEDICT,
Retreived August, 2012 from
http://www.mdbg.net/chindict/chindict.php?
page=cedict.
Shinsuke Mori and Makoto Nagao. 1996. Word
extraction from corpora and its part-of-speech
estimation using distributional analysis. In Pro-
ceedings of COLING 2006, pages 1119–1122.
Masaaki Nagata. 1999. A part of speech estima-
tion method for Japanese unknown words using
a statistical model of morphology and context.
In Proceedings of ACL 1999, pages 277–284.
Toshiaki Nakazawa, Daisuke Kawahara, and Sadao
Kurohashi. 2005. Automatic acquisition of ba-
sic katakana lexicon from a given corpus. In
Proceedings of IJCNLP 2005, pages 682–693.
Graham Neubig, Yosuke Nakata, and Shinsuke
Mori. 2011. Pointwise prediction for robust,
adaptable Japanese morphological analysis. In
Proceedings of ACL-HLT 2011, pages 529–533.
Hideki Ogura, Hanae Koiso, Yumi Fujike, Sayaka
Miyauchi, and Yutaka Hara. 2011. Mor-
phological Information Guildeline for BCCWJ:
Balanced Corpus of Contemporary Written
</reference>
<page confidence="0.982353">
188
</page>
<reference confidence="0.999738782608696">
Japanese, 4th Edition. National Institute for
Japanese Language and Linguistics.
Fuchun Peng, Fangfang Feng, and Andrew McCal-
lum. 2004. Chinese segmentation and new word
detection using conditional random fields. In
Proceedings COLING 2004.
Rakuten-Inc. 2012. Rakuten Ichiba
http://www.rakuten.co.jp/.
Huihsin Tseng, Pichuan Chang, Galen Andrew,
Daniel Jurafsky, and Christopher Manning.
2005. A conditional random field word seg-
menter. In Fourth SIGHAN Workshop on Chi-
nese Language Processing.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi
Isahara. 2001. Morphological analysis based
on a maximum entropy model — an ap-
proach to the unknown word problem — (in
Japanese). Journal of Natural Language Pro-
cessing, 8:127–141.
Yue Zhang and Stephen Clark. 2008. Joint word
segmentation and pos tagging using a single per-
ceptron. In Proceedings of ACL 2008, pages
888–896.
</reference>
<page confidence="0.998925">
189
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.842741">
<title confidence="0.9981765">Accurate Word Segmentation using and Language Model Projection</title>
<author confidence="0.971894">Masato Hagiwara Satoshi Sekine</author>
<affiliation confidence="0.997745">Rakuten Institute of Technology, New</affiliation>
<address confidence="0.999771">215 Park Avenue South, New York, NY</address>
<email confidence="0.999043">fmasato.hagiwara,satoshi.b.sekine}@mail.rakuten.com</email>
<abstract confidence="0.9908068">Transliterated compound nouns not separated by whitespaces pose diffion word segmentation (WS). Ofhave been proposed to split them using word statistics, but they rely on static lexicon, limiting use. We propose an approach, integrating source LM, and/or, back-transliteration and English LM. The experiments on Japanese and Chinese WS have shown that the proposed models achieve significant improvement over state-of-the-art, reducing 16% errors in Japanese.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Masayuki Asahara</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese unknown word identification by character-based chunking.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING</booktitle>
<pages>459--465</pages>
<contexts>
<context position="4139" citStr="Asahara and Matsumoto (2004)" startWordPosition="634" endWordPosition="637">al Linguistics, pages 183–189, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics depending on character types (Kudo et al., 2004). Nagata (1999) proposed a Japanese unknown word model which considers PoS (part of speech), word length model and orthography. Uchimoto et al. (2001) proposed a maximum entropy morphological analyzer robust to unknown words. In Chinese, Peng et al. (2004) used CRF confidence to detect new words. For offline approaches, Mori and Nagao (1996) extracted unknown word and estimated their PoS from a corpus through distributional analysis. Asahara and Matsumoto (2004) built a character-based chunking model using SVM for Japanese unknown word detection. Kaji and Kitsuregawa (2011)’s approach is the closest to ours. They built a model to split Katakana compounds using backtransliteration and paraphrasing mined from large corpora. Nakazawa et al. (2005) is a similar approach, using a Ja-En dictionary to translate compound components and check their occurrence in an English corpus. Similar approaches are proposed for other languages, such as German (Koehn and Knight, 2003) and Urdu-Hindi (Lehal, 2010). Correct splitting of compound nouns has a positive effect </context>
</contexts>
<marker>Asahara, Matsumoto, 2004</marker>
<rawString>Masayuki Asahara and Yuji Matsumoto. 2004. Japanese unknown word identification by character-based chunking. In Proceedings of COLING 2004, pages 459–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1T 5-gram Version 1. Linguistic Data Consortium.</title>
<date>2006</date>
<contexts>
<context position="10436" citStr="Brants and Franz, 2006" startWordPosition="1762" endWordPosition="1765">d associated original nodes are chosen as the WS result. In Figure 1, the bold edges are traversed at the final step, and the corresponding nodes “大 - 人気 - 色 - ブラキッシュ- レッド” are chosen as the final WS result. For Japanese, we only expand and project Katakana noun nodes (whether they are known or unknown words) since transliterated words are almost always written in Katakana. For Chinese, only “ns (place name)”, “nr (personal name)”, and “nz (other proper noun)” nodes whose surface form is more than 1- character long are transliterated. As the English LM, we used Google Web 1T 5-gram Version 1 (Brants and Franz, 2006), limiting it to unigrams occurring more than 2000 times and bigrams occurring more than 500 times. 5 Transliteration For transliterating Japanese/Chinese words back to English, we adopted the Joint Source Channel (JSC) Model (Li et al., 2004), a generative model widely used as a simple yet powerful baseline in previous research e.g., (Hagiwara and Sekine, 2012; Finch and Sumita, 2010).2 The JSC model, given an input of source word s and target word t, defines the transliteration probability based on transliteration units (TUs) ui = (si, ti) as: PJSC((s, t)) = ∏fi=1 P(ui|ui−n+1, ..., ui−1), wh</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram Version 1. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Braschler</author>
<author>Bärbel Ripplinger</author>
</authors>
<title>How effective is stemming and decompounding for german text retrieval? Information Retrieval,</title>
<date>2004</date>
<pages>291--316</pages>
<contexts>
<context position="4809" citStr="Braschler and Ripplinger, 2004" startWordPosition="739" endWordPosition="742"> using SVM for Japanese unknown word detection. Kaji and Kitsuregawa (2011)’s approach is the closest to ours. They built a model to split Katakana compounds using backtransliteration and paraphrasing mined from large corpora. Nakazawa et al. (2005) is a similar approach, using a Ja-En dictionary to translate compound components and check their occurrence in an English corpus. Similar approaches are proposed for other languages, such as German (Koehn and Knight, 2003) and Urdu-Hindi (Lehal, 2010). Correct splitting of compound nouns has a positive effect on MT (Koehn and Knight, 2003) and IR (Braschler and Ripplinger, 2004). A similar problem can be seen in Korean, German etc. where compounds may not be explicitly split by whitespaces. Koehn and Knight (2003) tackled the splitting problem in German, by using word statistics in a monolingual corpus. They also used the information whether translations of compound parts appear in a German-English bilingual corpus. Lehal (2010) used Urdu-Devnagri transliteration and a Hindi corpus for handling the space omission problem in Urdu compound words. 3 Word Segmentation Model Out baseline model is a semi-Markov structure prediction model which estimates WS and the PoS sequ</context>
</contexts>
<marker>Braschler, Ripplinger, 2004</marker>
<rawString>Martin Braschler and Bärbel Ripplinger. 2004. How effective is stemming and decompounding for german text retrieval? Information Retrieval, pages 291–316.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP 2012,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="13327" citStr="Collins, 2002" startWordPosition="2278" endWordPosition="2279">ptions randomly sampled from Rakuten (Rakuten-Inc., 2012). The corpus is manually annotated with the BCCWJ style WS (Ogura et al., 2011). It consists of 118,355 tokens, and has a relatively high percentage of Katakana words (11.2%). (2) BCCWJ (Maekawa, 2008) CORE (60,374 sentences, 1,286,899 tokens, out of which approx. 3.58% are Katakana words). As the dictionary, we used UniDic (Den et al., 2007). For Chinese, we used LCMC (McEnery and Xiao, 2004) (45,697 sentences and 1,001,549 tokens). As the dictionary, we used CC-CEDICT (MDGB, 2011)4. Training and Evaluation We used Averaged Perceptron (Collins, 2002) (3 iterations) for training, with five-fold cross-validation. As for the evaluation metrics, we used Precision (Prec.), Recall (Rec.), and F-measure (F). We additionally evaluated the performance limited to Katakana (JA) or proper nouns (ZH) in order to see the impact of compound splitting. We also used word error rate (WER) to see the relative change of errors. 6.2 Japanese WS Results We compared the baseline model, the augmented model with the source language (+LM-S) and the projected model (+LM-P). Table 3 shows the result of the proposed models and major open-source Japanese WS systems, n</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms. In Proceedings of EMNLP 2012, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yasuharu Den</author>
</authors>
<title>Toshinobu Ogiso, Hideki Ogura, Atsushi Yamada, Nobuaki Minematsu, Kiyotaka Uchimoto, and Hanae Koiso.</title>
<date>2007</date>
<pages>22--101</pages>
<marker>Den, 2007</marker>
<rawString>Yasuharu Den, Toshinobu Ogiso, Hideki Ogura, Atsushi Yamada, Nobuaki Minematsu, Kiyotaka Uchimoto, and Hanae Koiso. 2007. The development of an electronic dictionary for morphological analysis and its application to Japanese corpus linguistics (in Japanese). Japanese linguistics, 22:101–122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Finch</author>
<author>Eiichiro Sumita</author>
</authors>
<title>A bayesian model of bilingual segmentation for transliteration.</title>
<date>2010</date>
<booktitle>In Proceedings of IWSLT</booktitle>
<pages>259--266</pages>
<contexts>
<context position="10824" citStr="Finch and Sumita, 2010" startWordPosition="1825" endWordPosition="1828">, only “ns (place name)”, “nr (personal name)”, and “nz (other proper noun)” nodes whose surface form is more than 1- character long are transliterated. As the English LM, we used Google Web 1T 5-gram Version 1 (Brants and Franz, 2006), limiting it to unigrams occurring more than 2000 times and bigrams occurring more than 500 times. 5 Transliteration For transliterating Japanese/Chinese words back to English, we adopted the Joint Source Channel (JSC) Model (Li et al., 2004), a generative model widely used as a simple yet powerful baseline in previous research e.g., (Hagiwara and Sekine, 2012; Finch and Sumita, 2010).2 The JSC model, given an input of source word s and target word t, defines the transliteration probability based on transliteration units (TUs) ui = (si, ti) as: PJSC((s, t)) = ∏fi=1 P(ui|ui−n+1, ..., ui−1), where f is the number of TUs in a given source / target word pair. TUs are atomic pair units of source / target words, such as “la/ラ” and “ish/ッシュ”. The TU n-gram probabilities are learned from a training corpus by following iterative updates similar to the EM algorithm3. In order to generate transliteration candidates, we used a stack decoder described in (Hagiwara and Sekine, 2012). We</context>
</contexts>
<marker>Finch, Sumita, 2010</marker>
<rawString>Andrew Finch and Eiichiro Sumita. 2010. A bayesian model of bilingual segmentation for transliteration. In Proceedings of IWSLT 2010, pages 259–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masato Hagiwara</author>
<author>Satoshi Sekine</author>
</authors>
<title>Latent class transliteration based on source language origin.</title>
<date>2012</date>
<booktitle>In Proceedings of NEWS 2012,</booktitle>
<pages>30--37</pages>
<contexts>
<context position="10799" citStr="Hagiwara and Sekine, 2012" startWordPosition="1820" endWordPosition="1824">en in Katakana. For Chinese, only “ns (place name)”, “nr (personal name)”, and “nz (other proper noun)” nodes whose surface form is more than 1- character long are transliterated. As the English LM, we used Google Web 1T 5-gram Version 1 (Brants and Franz, 2006), limiting it to unigrams occurring more than 2000 times and bigrams occurring more than 500 times. 5 Transliteration For transliterating Japanese/Chinese words back to English, we adopted the Joint Source Channel (JSC) Model (Li et al., 2004), a generative model widely used as a simple yet powerful baseline in previous research e.g., (Hagiwara and Sekine, 2012; Finch and Sumita, 2010).2 The JSC model, given an input of source word s and target word t, defines the transliteration probability based on transliteration units (TUs) ui = (si, ti) as: PJSC((s, t)) = ∏fi=1 P(ui|ui−n+1, ..., ui−1), where f is the number of TUs in a given source / target word pair. TUs are atomic pair units of source / target words, such as “la/ラ” and “ish/ッシュ”. The TU n-gram probabilities are learned from a training corpus by following iterative updates similar to the EM algorithm3. In order to generate transliteration candidates, we used a stack decoder described in (Hagiw</context>
</contexts>
<marker>Hagiwara, Sekine, 2012</marker>
<rawString>Masato Hagiwara and Satoshi Sekine. 2012. Latent class transliteration based on source language origin. In Proceedings of NEWS 2012, pages 30–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sittichai Jiampojamarn</author>
<author>Grzegorz Kondrak</author>
<author>Tarek Sherif</author>
</authors>
<title>Applying many-to-many alignments and hidden markov models to letterto-phoneme conversion.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL-HLT</booktitle>
<pages>372--379</pages>
<contexts>
<context position="11813" citStr="Jiampojamarn et al., 2007" startWordPosition="1995" endWordPosition="1999">U n-gram probabilities are learned from a training corpus by following iterative updates similar to the EM algorithm3. In order to generate transliteration candidates, we used a stack decoder described in (Hagiwara and Sekine, 2012). We used the training data of the NEWS 2009 workshop (Li et al., 2009a; Li et al., 2009b). As reference, we measured the performance on its own, using NEWS 2009 (Li et al., 2009b) data. The percentage of correctly transliterated words are 37.9% for Japanese and 25.6% 2Note that one could also adopt other generative / discriminative transliteration models, such as (Jiampojamarn et al., 2007; Jiampojamarn et al., 2008). 3We only allow TUs whose length is shorter than or equal to 3, both in the source and target side. Input: )�, A % 10- .;&amp;quot; 7 zk- l ✓ z L l 1 very popular color blackish red BOS 大 人 気 大 人 気 大 人 気 色 Transliteration Model 色 ブ bu ブ ラ bla bra ブ ラ キ brakiblaki ブ ラ キ ッ brakiblaki ブ キ ッbr blackish シ . node (a) キ ッ シ ュ kish キ ッ ki シ ュ レッ ド shrad shred . . . edge (c) レ ドlede rea os red node (b) 185 for Chinese. Although the numbers seem low at a first glance, Chinese back-transliteration itself is a very hard task, mostly because Chinese phonology is so different from Englis</context>
</contexts>
<marker>Jiampojamarn, Kondrak, Sherif, 2007</marker>
<rawString>Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek Sherif. 2007. Applying many-to-many alignments and hidden markov models to letterto-phoneme conversion. In Proceedings of NAACL-HLT 2007, pages 372–379.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sittichai Jiampojamarn</author>
<author>Colin Cherry</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Joint processing and discriminative training for letter-to-phoneme conversion.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>905--913</pages>
<contexts>
<context position="11841" citStr="Jiampojamarn et al., 2008" startWordPosition="2000" endWordPosition="2003">learned from a training corpus by following iterative updates similar to the EM algorithm3. In order to generate transliteration candidates, we used a stack decoder described in (Hagiwara and Sekine, 2012). We used the training data of the NEWS 2009 workshop (Li et al., 2009a; Li et al., 2009b). As reference, we measured the performance on its own, using NEWS 2009 (Li et al., 2009b) data. The percentage of correctly transliterated words are 37.9% for Japanese and 25.6% 2Note that one could also adopt other generative / discriminative transliteration models, such as (Jiampojamarn et al., 2007; Jiampojamarn et al., 2008). 3We only allow TUs whose length is shorter than or equal to 3, both in the source and target side. Input: )�, A % 10- .;&amp;quot; 7 zk- l ✓ z L l 1 very popular color blackish red BOS 大 人 気 大 人 気 大 人 気 色 Transliteration Model 色 ブ bu ブ ラ bla bra ブ ラ キ brakiblaki ブ ラ キ ッ brakiblaki ブ キ ッbr blackish シ . node (a) キ ッ シ ュ kish キ ッ ki シ ュ レッ ド shrad shred . . . edge (c) レ ドlede rea os red node (b) 185 for Chinese. Although the numbers seem low at a first glance, Chinese back-transliteration itself is a very hard task, mostly because Chinese phonology is so different from English that some sounds may be dr</context>
</contexts>
<marker>Jiampojamarn, Cherry, Kondrak, 2008</marker>
<rawString>Sittichai Jiampojamarn, Colin Cherry, and Grzegorz Kondrak. 2008. Joint processing and discriminative training for letter-to-phoneme conversion. In Proceedings of ACL 2008, pages 905–913.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nobuhiro Kaji</author>
<author>Masaru Kitsuregawa</author>
</authors>
<title>Splitting noun compounds via monolingual and bilingual paraphrasing: A study on japanese katakana words.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP 2011,</booktitle>
<pages>959--969</pages>
<contexts>
<context position="1861" citStr="Kaji and Kitsuregawa (2011)" startWordPosition="275" endWordPosition="278">dge. For example, when splitting a compound noun ブ ラキッシュレッド burakisshureddo, a traditional word segmenter can easily segment this as ブ ラキッ/シュレッド “*blacki shred” since シュレッ ド shureddo “shred” is a known, frequent word. It is only the knowledge that ブラキッburaki (*“blacki”) is not a valid word which prevents this. Knowing that the back-transliterated unigram “blacki” and bigram “blacki shred” are unlikely in English can promote the correct WS, ブラキッシュ/レッド “blackish red”. In Chinese, the problem can be more severe since the language does not have a separate script to represent transliterated words. Kaji and Kitsuregawa (2011) tackled Katakana compound splitting using backtransliteration and paraphrasing. Their approach falls into an offline approach, which focuses on creating dictionaries by extracting new words from large corpora separately before WS. However, offline approaches have limitation unless the lexicon is constantly updated. Moreover, they only deal with Katakana, but their method is not directly applicable to Chinese since the language lacks a separate script for transliterated words. Instead, we adopt an online approach, which deals with unknown words simultaneously as the model analyzes the input. O</context>
<context position="4253" citStr="Kaji and Kitsuregawa (2011)" startWordPosition="650" endWordPosition="653">epending on character types (Kudo et al., 2004). Nagata (1999) proposed a Japanese unknown word model which considers PoS (part of speech), word length model and orthography. Uchimoto et al. (2001) proposed a maximum entropy morphological analyzer robust to unknown words. In Chinese, Peng et al. (2004) used CRF confidence to detect new words. For offline approaches, Mori and Nagao (1996) extracted unknown word and estimated their PoS from a corpus through distributional analysis. Asahara and Matsumoto (2004) built a character-based chunking model using SVM for Japanese unknown word detection. Kaji and Kitsuregawa (2011)’s approach is the closest to ours. They built a model to split Katakana compounds using backtransliteration and paraphrasing mined from large corpora. Nakazawa et al. (2005) is a similar approach, using a Ja-En dictionary to translate compound components and check their occurrence in an English corpus. Similar approaches are proposed for other languages, such as German (Koehn and Knight, 2003) and Urdu-Hindi (Lehal, 2010). Correct splitting of compound nouns has a positive effect on MT (Koehn and Knight, 2003) and IR (Braschler and Ripplinger, 2004). A similar problem can be seen in Korean, G</context>
</contexts>
<marker>Kaji, Kitsuregawa, 2011</marker>
<rawString>Nobuhiro Kaji and Masaru Kitsuregawa. 2011. Splitting noun compounds via monolingual and bilingual paraphrasing: A study on japanese katakana words. In Proceedings of the EMNLP 2011, pages 959–969.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Empirical methods for compound splitting.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL</booktitle>
<pages>187--193</pages>
<contexts>
<context position="4650" citStr="Koehn and Knight, 2003" startWordPosition="713" endWordPosition="716">unknown word and estimated their PoS from a corpus through distributional analysis. Asahara and Matsumoto (2004) built a character-based chunking model using SVM for Japanese unknown word detection. Kaji and Kitsuregawa (2011)’s approach is the closest to ours. They built a model to split Katakana compounds using backtransliteration and paraphrasing mined from large corpora. Nakazawa et al. (2005) is a similar approach, using a Ja-En dictionary to translate compound components and check their occurrence in an English corpus. Similar approaches are proposed for other languages, such as German (Koehn and Knight, 2003) and Urdu-Hindi (Lehal, 2010). Correct splitting of compound nouns has a positive effect on MT (Koehn and Knight, 2003) and IR (Braschler and Ripplinger, 2004). A similar problem can be seen in Korean, German etc. where compounds may not be explicitly split by whitespaces. Koehn and Knight (2003) tackled the splitting problem in German, by using word statistics in a monolingual corpus. They also used the information whether translations of compound parts appear in a German-English bilingual corpus. Lehal (2010) used Urdu-Devnagri transliteration and a Hindi corpus for handling the space omissi</context>
<context position="8112" citStr="Koehn and Knight (2003)" startWordPosition="1357" endWordPosition="1360"> characters, making them a single node, and assign “m” (number). For other character types, a single node with PoS “w (others)” is created. &apos;The Japanese dictionary and the corpus we used have 6 levels of PoS tag hierarchy, while the Chinese ones have only one level, which is why some of the PoS features are not included in Chinese. As character type, Hiragana (JA), Katakana (JA), Latin alphabet, Number, Chinese characters, and Others, are distinguished. Word length is in Unicode. 184 Figure 1: Example lattice with LM projection 4 Use of Language Model Language Model Augmentation Analogous to Koehn and Knight (2003), we can exploit the fact that レッド reddo (red) in the example ブラキッシュレッド is such a common word that one can expect it appears frequently in the training corpus. To incorporate this intuition, we used log probability of n-gram as features, which are included in Table 1 (ID 19 and 20): φLMS 1 (wi) = logp(wi) and φLMS 2 (wi−1, wi) = logp(wi−1, wi). Here the empirical probability p(wi) and p(wi−1, wi) are computed from the source language corpus. In Japanese, we applied this source language augmentation only to Katakana words. In Chinese, we did not limit the target. 4.1 Language Model Projection A</context>
</contexts>
<marker>Koehn, Knight, 2003</marker>
<rawString>Philipp Koehn and Kevin Knight. 2003. Empirical methods for compound splitting. In Proceedings of EACL 2003, pages 187–193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Kaoru Yamamoto</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Applying conditional random fields to Japanese morphological analysis.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>230--237</pages>
<contexts>
<context position="3673" citStr="Kudo et al., 2004" startWordPosition="561" endWordPosition="564">veness of our approach, we test our models on a Japanese balanced corpus and an electronic commerce domain corpus, and a balanced Chinese corpus. The results show that we achieved a significant improvement in WS accuracy in both languages. 2 Related Work In Japanese WS, unknown words are usually dealt with in an online manner with the unknown word model, which uses heuristics 183 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 183–189, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics depending on character types (Kudo et al., 2004). Nagata (1999) proposed a Japanese unknown word model which considers PoS (part of speech), word length model and orthography. Uchimoto et al. (2001) proposed a maximum entropy morphological analyzer robust to unknown words. In Chinese, Peng et al. (2004) used CRF confidence to detect new words. For offline approaches, Mori and Nagao (1996) extracted unknown word and estimated their PoS from a corpus through distributional analysis. Asahara and Matsumoto (2004) built a character-based chunking model using SVM for Japanese unknown word detection. Kaji and Kitsuregawa (2011)’s approach is the c</context>
<context position="5447" citStr="Kudo et al., 2004" startWordPosition="840" endWordPosition="843">an be seen in Korean, German etc. where compounds may not be explicitly split by whitespaces. Koehn and Knight (2003) tackled the splitting problem in German, by using word statistics in a monolingual corpus. They also used the information whether translations of compound parts appear in a German-English bilingual corpus. Lehal (2010) used Urdu-Devnagri transliteration and a Hindi corpus for handling the space omission problem in Urdu compound words. 3 Word Segmentation Model Out baseline model is a semi-Markov structure prediction model which estimates WS and the PoS sequence simultaneously (Kudo et al., 2004; Zhang and Clark, 2008). This model finds the best output y* from the input sentence string x as: y* = arg maxyEY(x) w *y). Here, Y (x) denotes all the possible sequences of words derived from x. The best analysis is determined by the feature function φ(y) the ID Feature ID Feature 1 wi 13 w1i−1w1i 2 t1 14 t1 i−1t1 3* i 16* i 4* t1i t215* 17* t1 i−1t2 i−1t1 i t2 5* i 18* i 6* t1 i t2 it3 19 t1 i−1t2 i−1t3 i−1t1 i t2 i t3 7 i 20 i 8* t1 i t2 i t5 i t6 21 t1 i−1t2 i−1t5 i−1t6 i−1t1 i t2 i t5 i t6 9* i 22 i 10* t1 i t2 i t6 t1 i−1t2 i−1t6 i−1t1 i t2 i t6 11* i i 12 wit1 φLMS i 1 (wi) wit1 i t2 φ</context>
<context position="7217" citStr="Kudo et al., 2004" startWordPosition="1206" endWordPosition="1209">nput length. We list all the baseline features in Table 11. The asterisks (*) indicate the feature is used for Japanese (JA) but not for Chinese (ZH) WS. Here, wi and wi−1 denote the current and previous word in question, and t1i and t�i−1 are level-j PoS tags assigned to them. l(w) and c(w) are the length and the set of character types of word w. If there is a substring for which no dictionary entries are found, the unknown word model is invoked. In Japanese, our unknown word model relies on heuristics based on character types and word length to generate word nodes, similar to that of MeCab (Kudo et al., 2004). In Chinese, we aggregated consecutive 1 to 4 characters add them as “n (common noun)”, “ns (place name)”, “nr (personal name)”, and “nz (other proper nouns),” since most of the unknown words in Chinese are proper nouns. Also, we aggregated up to 20 consecutive numerical characters, making them a single node, and assign “m” (number). For other character types, a single node with PoS “w (others)” is created. &apos;The Japanese dictionary and the corpus we used have 6 levels of PoS tag hierarchy, while the Chinese ones have only one level, which is why some of the PoS features are not included in Ch</context>
<context position="13964" citStr="Kudo et al., 2004" startWordPosition="2381" endWordPosition="2384"> training, with five-fold cross-validation. As for the evaluation metrics, we used Precision (Prec.), Recall (Rec.), and F-measure (F). We additionally evaluated the performance limited to Katakana (JA) or proper nouns (ZH) in order to see the impact of compound splitting. We also used word error rate (WER) to see the relative change of errors. 6.2 Japanese WS Results We compared the baseline model, the augmented model with the source language (+LM-S) and the projected model (+LM-P). Table 3 shows the result of the proposed models and major open-source Japanese WS systems, namely, MeCab 0.98 (Kudo et al., 2004), JUMAN 7.0 (Kurohashi and Nagao, 1994), 4Since the dictionary is not explicitly annotated with PoS tags, we firstly took the intersection of the training corpus and the dictionary words, and assigned all the possible PoS tags to the words which appeared in the corpus. All the other words which do not appear in the training corpus are discarded. and KyTea 0.4.2 (Neubig et al., 2011) 5. We observed slight improvement by incorporating the source LM, and observed a 0.48 point F-value increase over baseline, which translates to 4.65 point Katakana F-value change and 16.0% (3.56% to 2.99 %) WER red</context>
</contexts>
<marker>Kudo, Yamamoto, Matsumoto, 2004</marker>
<rawString>Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Applying conditional random fields to Japanese morphological analysis. In Proceedings of EMNLP 2004, pages 230–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<title>Improvements of Japanese morphological analyzer juman.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Workshop on Sharable Natural Language Resources,</booktitle>
<pages>22--38</pages>
<contexts>
<context position="14003" citStr="Kurohashi and Nagao, 1994" startWordPosition="2387" endWordPosition="2390">-validation. As for the evaluation metrics, we used Precision (Prec.), Recall (Rec.), and F-measure (F). We additionally evaluated the performance limited to Katakana (JA) or proper nouns (ZH) in order to see the impact of compound splitting. We also used word error rate (WER) to see the relative change of errors. 6.2 Japanese WS Results We compared the baseline model, the augmented model with the source language (+LM-S) and the projected model (+LM-P). Table 3 shows the result of the proposed models and major open-source Japanese WS systems, namely, MeCab 0.98 (Kudo et al., 2004), JUMAN 7.0 (Kurohashi and Nagao, 1994), 4Since the dictionary is not explicitly annotated with PoS tags, we firstly took the intersection of the training corpus and the dictionary words, and assigned all the possible PoS tags to the words which appeared in the corpus. All the other words which do not appear in the training corpus are discarded. and KyTea 0.4.2 (Neubig et al., 2011) 5. We observed slight improvement by incorporating the source LM, and observed a 0.48 point F-value increase over baseline, which translates to 4.65 point Katakana F-value change and 16.0% (3.56% to 2.99 %) WER reduction, mainly due to its higher Kataka</context>
</contexts>
<marker>Kurohashi, Nagao, 1994</marker>
<rawString>Sadao Kurohashi and Makoto Nagao. 1994. Improvements of Japanese morphological analyzer juman. In Proceedings of the International Workshop on Sharable Natural Language Resources, pages 22–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gurpreet Singh Lehal</author>
</authors>
<title>A word segmentation system for handling space omission problem in urdu script.</title>
<date>2010</date>
<booktitle>In Proceedings of the 1st Workshop on South and Southeast Asian Natural Language Processing (WSSANLP),</booktitle>
<pages>43--50</pages>
<contexts>
<context position="4679" citStr="Lehal, 2010" startWordPosition="719" endWordPosition="720">m a corpus through distributional analysis. Asahara and Matsumoto (2004) built a character-based chunking model using SVM for Japanese unknown word detection. Kaji and Kitsuregawa (2011)’s approach is the closest to ours. They built a model to split Katakana compounds using backtransliteration and paraphrasing mined from large corpora. Nakazawa et al. (2005) is a similar approach, using a Ja-En dictionary to translate compound components and check their occurrence in an English corpus. Similar approaches are proposed for other languages, such as German (Koehn and Knight, 2003) and Urdu-Hindi (Lehal, 2010). Correct splitting of compound nouns has a positive effect on MT (Koehn and Knight, 2003) and IR (Braschler and Ripplinger, 2004). A similar problem can be seen in Korean, German etc. where compounds may not be explicitly split by whitespaces. Koehn and Knight (2003) tackled the splitting problem in German, by using word statistics in a monolingual corpus. They also used the information whether translations of compound parts appear in a German-English bilingual corpus. Lehal (2010) used Urdu-Devnagri transliteration and a Hindi corpus for handling the space omission problem in Urdu compound w</context>
</contexts>
<marker>Lehal, 2010</marker>
<rawString>Gurpreet Singh Lehal. 2010. A word segmentation system for handling space omission problem in urdu script. In Proceedings of the 1st Workshop on South and Southeast Asian Natural Language Processing (WSSANLP), pages 43–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>Zhang Min</author>
<author>Su Jian</author>
</authors>
<title>A joint source-channel model for machine transliteration.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>159--166</pages>
<contexts>
<context position="10679" citStr="Li et al., 2004" startWordPosition="1799" endWordPosition="1802">atakana noun nodes (whether they are known or unknown words) since transliterated words are almost always written in Katakana. For Chinese, only “ns (place name)”, “nr (personal name)”, and “nz (other proper noun)” nodes whose surface form is more than 1- character long are transliterated. As the English LM, we used Google Web 1T 5-gram Version 1 (Brants and Franz, 2006), limiting it to unigrams occurring more than 2000 times and bigrams occurring more than 500 times. 5 Transliteration For transliterating Japanese/Chinese words back to English, we adopted the Joint Source Channel (JSC) Model (Li et al., 2004), a generative model widely used as a simple yet powerful baseline in previous research e.g., (Hagiwara and Sekine, 2012; Finch and Sumita, 2010).2 The JSC model, given an input of source word s and target word t, defines the transliteration probability based on transliteration units (TUs) ui = (si, ti) as: PJSC((s, t)) = ∏fi=1 P(ui|ui−n+1, ..., ui−1), where f is the number of TUs in a given source / target word pair. TUs are atomic pair units of source / target words, such as “la/ラ” and “ish/ッシュ”. The TU n-gram probabilities are learned from a training corpus by following iterative updates si</context>
</contexts>
<marker>Li, Min, Jian, 2004</marker>
<rawString>Haizhou Li, Zhang Min, and Su Jian. 2004. A joint source-channel model for machine transliteration. In Proceedings of ACL 2004, pages 159–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>A Kumaran</author>
<author>Vladimir Pervouchine</author>
<author>Min Zhang</author>
</authors>
<title>Report of news 2009 machine transliteration shared task.</title>
<date>2009</date>
<booktitle>In Proceedings of NEWS</booktitle>
<pages>1--18</pages>
<contexts>
<context position="11490" citStr="Li et al., 2009" startWordPosition="1943" endWordPosition="1946">nd target word t, defines the transliteration probability based on transliteration units (TUs) ui = (si, ti) as: PJSC((s, t)) = ∏fi=1 P(ui|ui−n+1, ..., ui−1), where f is the number of TUs in a given source / target word pair. TUs are atomic pair units of source / target words, such as “la/ラ” and “ish/ッシュ”. The TU n-gram probabilities are learned from a training corpus by following iterative updates similar to the EM algorithm3. In order to generate transliteration candidates, we used a stack decoder described in (Hagiwara and Sekine, 2012). We used the training data of the NEWS 2009 workshop (Li et al., 2009a; Li et al., 2009b). As reference, we measured the performance on its own, using NEWS 2009 (Li et al., 2009b) data. The percentage of correctly transliterated words are 37.9% for Japanese and 25.6% 2Note that one could also adopt other generative / discriminative transliteration models, such as (Jiampojamarn et al., 2007; Jiampojamarn et al., 2008). 3We only allow TUs whose length is shorter than or equal to 3, both in the source and target side. Input: )�, A % 10- .;&amp;quot; 7 zk- l ✓ z L l 1 very popular color blackish red BOS 大 人 気 大 人 気 大 人 気 色 Transliteration Model 色 ブ bu ブ ラ bla bra ブ ラ キ brak</context>
</contexts>
<marker>Li, Kumaran, Pervouchine, Zhang, 2009</marker>
<rawString>Haizhou Li, A Kumaran, Vladimir Pervouchine, and Min Zhang. 2009a. Report of news 2009 machine transliteration shared task. In Proceedings of NEWS 2009, pages 1–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>A Kumaran</author>
<author>Min Zhang</author>
<author>Vladimir Pervouchine</author>
</authors>
<title>Whitepaper of news 2009 machine transliteration shared task.</title>
<date>2009</date>
<booktitle>In Proceedings of NEWS</booktitle>
<pages>pages</pages>
<contexts>
<context position="11490" citStr="Li et al., 2009" startWordPosition="1943" endWordPosition="1946">nd target word t, defines the transliteration probability based on transliteration units (TUs) ui = (si, ti) as: PJSC((s, t)) = ∏fi=1 P(ui|ui−n+1, ..., ui−1), where f is the number of TUs in a given source / target word pair. TUs are atomic pair units of source / target words, such as “la/ラ” and “ish/ッシュ”. The TU n-gram probabilities are learned from a training corpus by following iterative updates similar to the EM algorithm3. In order to generate transliteration candidates, we used a stack decoder described in (Hagiwara and Sekine, 2012). We used the training data of the NEWS 2009 workshop (Li et al., 2009a; Li et al., 2009b). As reference, we measured the performance on its own, using NEWS 2009 (Li et al., 2009b) data. The percentage of correctly transliterated words are 37.9% for Japanese and 25.6% 2Note that one could also adopt other generative / discriminative transliteration models, such as (Jiampojamarn et al., 2007; Jiampojamarn et al., 2008). 3We only allow TUs whose length is shorter than or equal to 3, both in the source and target side. Input: )�, A % 10- .;&amp;quot; 7 zk- l ✓ z L l 1 very popular color blackish red BOS 大 人 気 大 人 気 大 人 気 色 Transliteration Model 色 ブ bu ブ ラ bla bra ブ ラ キ brak</context>
</contexts>
<marker>Li, Kumaran, Zhang, Pervouchine, 2009</marker>
<rawString>Haizhou Li, A Kumaran, Min Zhang, and Vladimir Pervouchine. 2009b. Whitepaper of news 2009 machine transliteration shared task. In Proceedings of NEWS 2009, pages 19–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kikuo Maekawa</author>
</authors>
<title>Compilation of the Kotonoha-BCCWJ corpus (in Japanese). Nihongo no kenkyu (Studies</title>
<date>2008</date>
<note>in Japanese), 4(1):82–95.</note>
<contexts>
<context position="12971" citStr="Maekawa, 2008" startWordPosition="2221" endWordPosition="2222">ly because Chinese phonology is so different from English that some sounds may be dropped when transliterated. Therefore, we can regard this performance as a lower bound of the transliteration module performance we used for WS. 6 Experiments 6.1 Experimental Settings Corpora For Japanese, we used (1) EC corpus, consists of 1,230 product titles and descriptions randomly sampled from Rakuten (Rakuten-Inc., 2012). The corpus is manually annotated with the BCCWJ style WS (Ogura et al., 2011). It consists of 118,355 tokens, and has a relatively high percentage of Katakana words (11.2%). (2) BCCWJ (Maekawa, 2008) CORE (60,374 sentences, 1,286,899 tokens, out of which approx. 3.58% are Katakana words). As the dictionary, we used UniDic (Den et al., 2007). For Chinese, we used LCMC (McEnery and Xiao, 2004) (45,697 sentences and 1,001,549 tokens). As the dictionary, we used CC-CEDICT (MDGB, 2011)4. Training and Evaluation We used Averaged Perceptron (Collins, 2002) (3 iterations) for training, with five-fold cross-validation. As for the evaluation metrics, we used Precision (Prec.), Recall (Rec.), and F-measure (F). We additionally evaluated the performance limited to Katakana (JA) or proper nouns (ZH) i</context>
</contexts>
<marker>Maekawa, 2008</marker>
<rawString>Kikuo Maekawa. 2008. Compilation of the Kotonoha-BCCWJ corpus (in Japanese). Nihongo no kenkyu (Studies in Japanese), 4(1):82–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony McEnery</author>
<author>Zhonghua Xiao</author>
</authors>
<title>The lancaster corpus of mandarin chinese: A corpus for monolingual and contrastive language study.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC</booktitle>
<pages>1175--1178</pages>
<contexts>
<context position="13166" citStr="McEnery and Xiao, 2004" startWordPosition="2251" endWordPosition="2254">ration module performance we used for WS. 6 Experiments 6.1 Experimental Settings Corpora For Japanese, we used (1) EC corpus, consists of 1,230 product titles and descriptions randomly sampled from Rakuten (Rakuten-Inc., 2012). The corpus is manually annotated with the BCCWJ style WS (Ogura et al., 2011). It consists of 118,355 tokens, and has a relatively high percentage of Katakana words (11.2%). (2) BCCWJ (Maekawa, 2008) CORE (60,374 sentences, 1,286,899 tokens, out of which approx. 3.58% are Katakana words). As the dictionary, we used UniDic (Den et al., 2007). For Chinese, we used LCMC (McEnery and Xiao, 2004) (45,697 sentences and 1,001,549 tokens). As the dictionary, we used CC-CEDICT (MDGB, 2011)4. Training and Evaluation We used Averaged Perceptron (Collins, 2002) (3 iterations) for training, with five-fold cross-validation. As for the evaluation metrics, we used Precision (Prec.), Recall (Rec.), and F-measure (F). We additionally evaluated the performance limited to Katakana (JA) or proper nouns (ZH) in order to see the impact of compound splitting. We also used word error rate (WER) to see the relative change of errors. 6.2 Japanese WS Results We compared the baseline model, the augmented mod</context>
</contexts>
<marker>McEnery, Xiao, 2004</marker>
<rawString>Anthony McEnery and Zhonghua Xiao. 2004. The lancaster corpus of mandarin chinese: A corpus for monolingual and contrastive language study. In Proceedings of LREC 2004, pages 1175–1178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MDGB</author>
</authors>
<date>2011</date>
<publisher>CC-CEDICT,</publisher>
<contexts>
<context position="13257" citStr="MDGB, 2011" startWordPosition="2268" endWordPosition="2269"> we used (1) EC corpus, consists of 1,230 product titles and descriptions randomly sampled from Rakuten (Rakuten-Inc., 2012). The corpus is manually annotated with the BCCWJ style WS (Ogura et al., 2011). It consists of 118,355 tokens, and has a relatively high percentage of Katakana words (11.2%). (2) BCCWJ (Maekawa, 2008) CORE (60,374 sentences, 1,286,899 tokens, out of which approx. 3.58% are Katakana words). As the dictionary, we used UniDic (Den et al., 2007). For Chinese, we used LCMC (McEnery and Xiao, 2004) (45,697 sentences and 1,001,549 tokens). As the dictionary, we used CC-CEDICT (MDGB, 2011)4. Training and Evaluation We used Averaged Perceptron (Collins, 2002) (3 iterations) for training, with five-fold cross-validation. As for the evaluation metrics, we used Precision (Prec.), Recall (Rec.), and F-measure (F). We additionally evaluated the performance limited to Katakana (JA) or proper nouns (ZH) in order to see the impact of compound splitting. We also used word error rate (WER) to see the relative change of errors. 6.2 Japanese WS Results We compared the baseline model, the augmented model with the source language (+LM-S) and the projected model (+LM-P). Table 3 shows the resu</context>
</contexts>
<marker>MDGB, 2011</marker>
<rawString>MDGB. 2011. CC-CEDICT,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Retreived August</author>
</authors>
<date>2012</date>
<note>from http://www.mdbg.net/chindict/chindict.php? page=cedict.</note>
<marker>August, 2012</marker>
<rawString>Retreived August, 2012 from http://www.mdbg.net/chindict/chindict.php? page=cedict.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shinsuke Mori</author>
<author>Makoto Nagao</author>
</authors>
<title>Word extraction from corpora and its part-of-speech estimation using distributional analysis.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING</booktitle>
<pages>1119--1122</pages>
<contexts>
<context position="4016" citStr="Mori and Nagao (1996)" startWordPosition="617" endWordPosition="620">nown word model, which uses heuristics 183 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 183–189, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics depending on character types (Kudo et al., 2004). Nagata (1999) proposed a Japanese unknown word model which considers PoS (part of speech), word length model and orthography. Uchimoto et al. (2001) proposed a maximum entropy morphological analyzer robust to unknown words. In Chinese, Peng et al. (2004) used CRF confidence to detect new words. For offline approaches, Mori and Nagao (1996) extracted unknown word and estimated their PoS from a corpus through distributional analysis. Asahara and Matsumoto (2004) built a character-based chunking model using SVM for Japanese unknown word detection. Kaji and Kitsuregawa (2011)’s approach is the closest to ours. They built a model to split Katakana compounds using backtransliteration and paraphrasing mined from large corpora. Nakazawa et al. (2005) is a similar approach, using a Ja-En dictionary to translate compound components and check their occurrence in an English corpus. Similar approaches are proposed for other languages, such </context>
</contexts>
<marker>Mori, Nagao, 1996</marker>
<rawString>Shinsuke Mori and Makoto Nagao. 1996. Word extraction from corpora and its part-of-speech estimation using distributional analysis. In Proceedings of COLING 2006, pages 1119–1122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
</authors>
<title>A part of speech estimation method for Japanese unknown words using a statistical model of morphology and context.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>277--284</pages>
<contexts>
<context position="3688" citStr="Nagata (1999)" startWordPosition="565" endWordPosition="566">ch, we test our models on a Japanese balanced corpus and an electronic commerce domain corpus, and a balanced Chinese corpus. The results show that we achieved a significant improvement in WS accuracy in both languages. 2 Related Work In Japanese WS, unknown words are usually dealt with in an online manner with the unknown word model, which uses heuristics 183 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 183–189, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics depending on character types (Kudo et al., 2004). Nagata (1999) proposed a Japanese unknown word model which considers PoS (part of speech), word length model and orthography. Uchimoto et al. (2001) proposed a maximum entropy morphological analyzer robust to unknown words. In Chinese, Peng et al. (2004) used CRF confidence to detect new words. For offline approaches, Mori and Nagao (1996) extracted unknown word and estimated their PoS from a corpus through distributional analysis. Asahara and Matsumoto (2004) built a character-based chunking model using SVM for Japanese unknown word detection. Kaji and Kitsuregawa (2011)’s approach is the closest to ours.</context>
</contexts>
<marker>Nagata, 1999</marker>
<rawString>Masaaki Nagata. 1999. A part of speech estimation method for Japanese unknown words using a statistical model of morphology and context. In Proceedings of ACL 1999, pages 277–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toshiaki Nakazawa</author>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Automatic acquisition of basic katakana lexicon from a given corpus.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCNLP</booktitle>
<pages>682--693</pages>
<contexts>
<context position="4427" citStr="Nakazawa et al. (2005)" startWordPosition="677" endWordPosition="680">moto et al. (2001) proposed a maximum entropy morphological analyzer robust to unknown words. In Chinese, Peng et al. (2004) used CRF confidence to detect new words. For offline approaches, Mori and Nagao (1996) extracted unknown word and estimated their PoS from a corpus through distributional analysis. Asahara and Matsumoto (2004) built a character-based chunking model using SVM for Japanese unknown word detection. Kaji and Kitsuregawa (2011)’s approach is the closest to ours. They built a model to split Katakana compounds using backtransliteration and paraphrasing mined from large corpora. Nakazawa et al. (2005) is a similar approach, using a Ja-En dictionary to translate compound components and check their occurrence in an English corpus. Similar approaches are proposed for other languages, such as German (Koehn and Knight, 2003) and Urdu-Hindi (Lehal, 2010). Correct splitting of compound nouns has a positive effect on MT (Koehn and Knight, 2003) and IR (Braschler and Ripplinger, 2004). A similar problem can be seen in Korean, German etc. where compounds may not be explicitly split by whitespaces. Koehn and Knight (2003) tackled the splitting problem in German, by using word statistics in a monoling</context>
</contexts>
<marker>Nakazawa, Kawahara, Kurohashi, 2005</marker>
<rawString>Toshiaki Nakazawa, Daisuke Kawahara, and Sadao Kurohashi. 2005. Automatic acquisition of basic katakana lexicon from a given corpus. In Proceedings of IJCNLP 2005, pages 682–693.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Yosuke Nakata</author>
<author>Shinsuke Mori</author>
</authors>
<title>Pointwise prediction for robust, adaptable Japanese morphological analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT 2011,</booktitle>
<pages>529--533</pages>
<contexts>
<context position="14349" citStr="Neubig et al., 2011" startWordPosition="2446" endWordPosition="2449">the baseline model, the augmented model with the source language (+LM-S) and the projected model (+LM-P). Table 3 shows the result of the proposed models and major open-source Japanese WS systems, namely, MeCab 0.98 (Kudo et al., 2004), JUMAN 7.0 (Kurohashi and Nagao, 1994), 4Since the dictionary is not explicitly annotated with PoS tags, we firstly took the intersection of the training corpus and the dictionary words, and assigned all the possible PoS tags to the words which appeared in the corpus. All the other words which do not appear in the training corpus are discarded. and KyTea 0.4.2 (Neubig et al., 2011) 5. We observed slight improvement by incorporating the source LM, and observed a 0.48 point F-value increase over baseline, which translates to 4.65 point Katakana F-value change and 16.0% (3.56% to 2.99 %) WER reduction, mainly due to its higher Katakana word rate (11.2%). Here, MeCab+UniDic achieved slightly better Katakana WS than the proposed models. This may be because it is trained on a much larger training corpus (the whole BCCWJ). The same trend is observed for BCCWJ corpus (Table 2), where we gained statistically significant 1 point F-measure increase on Katakana word. Many of the im</context>
</contexts>
<marker>Neubig, Nakata, Mori, 2011</marker>
<rawString>Graham Neubig, Yosuke Nakata, and Shinsuke Mori. 2011. Pointwise prediction for robust, adaptable Japanese morphological analysis. In Proceedings of ACL-HLT 2011, pages 529–533.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Ogura</author>
<author>Hanae Koiso</author>
<author>Yumi Fujike</author>
<author>Sayaka Miyauchi</author>
<author>Yutaka Hara</author>
</authors>
<date>2011</date>
<booktitle>Morphological Information Guildeline for BCCWJ: Balanced Corpus of Contemporary Written Japanese, 4th Edition. National Institute for Japanese Language and Linguistics.</booktitle>
<contexts>
<context position="12849" citStr="Ogura et al., 2011" startWordPosition="2200" endWordPosition="2203">185 for Chinese. Although the numbers seem low at a first glance, Chinese back-transliteration itself is a very hard task, mostly because Chinese phonology is so different from English that some sounds may be dropped when transliterated. Therefore, we can regard this performance as a lower bound of the transliteration module performance we used for WS. 6 Experiments 6.1 Experimental Settings Corpora For Japanese, we used (1) EC corpus, consists of 1,230 product titles and descriptions randomly sampled from Rakuten (Rakuten-Inc., 2012). The corpus is manually annotated with the BCCWJ style WS (Ogura et al., 2011). It consists of 118,355 tokens, and has a relatively high percentage of Katakana words (11.2%). (2) BCCWJ (Maekawa, 2008) CORE (60,374 sentences, 1,286,899 tokens, out of which approx. 3.58% are Katakana words). As the dictionary, we used UniDic (Den et al., 2007). For Chinese, we used LCMC (McEnery and Xiao, 2004) (45,697 sentences and 1,001,549 tokens). As the dictionary, we used CC-CEDICT (MDGB, 2011)4. Training and Evaluation We used Averaged Perceptron (Collins, 2002) (3 iterations) for training, with five-fold cross-validation. As for the evaluation metrics, we used Precision (Prec.), R</context>
</contexts>
<marker>Ogura, Koiso, Fujike, Miyauchi, Hara, 2011</marker>
<rawString>Hideki Ogura, Hanae Koiso, Yumi Fujike, Sayaka Miyauchi, and Yutaka Hara. 2011. Morphological Information Guildeline for BCCWJ: Balanced Corpus of Contemporary Written Japanese, 4th Edition. National Institute for Japanese Language and Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Fangfang Feng</author>
<author>Andrew McCallum</author>
</authors>
<title>Chinese segmentation and new word detection using conditional random fields.</title>
<date>2004</date>
<booktitle>In Proceedings COLING</booktitle>
<contexts>
<context position="3929" citStr="Peng et al. (2004)" startWordPosition="603" endWordPosition="606">n Japanese WS, unknown words are usually dealt with in an online manner with the unknown word model, which uses heuristics 183 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 183–189, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics depending on character types (Kudo et al., 2004). Nagata (1999) proposed a Japanese unknown word model which considers PoS (part of speech), word length model and orthography. Uchimoto et al. (2001) proposed a maximum entropy morphological analyzer robust to unknown words. In Chinese, Peng et al. (2004) used CRF confidence to detect new words. For offline approaches, Mori and Nagao (1996) extracted unknown word and estimated their PoS from a corpus through distributional analysis. Asahara and Matsumoto (2004) built a character-based chunking model using SVM for Japanese unknown word detection. Kaji and Kitsuregawa (2011)’s approach is the closest to ours. They built a model to split Katakana compounds using backtransliteration and paraphrasing mined from large corpora. Nakazawa et al. (2005) is a similar approach, using a Ja-En dictionary to translate compound components and check their occu</context>
</contexts>
<marker>Peng, Feng, McCallum, 2004</marker>
<rawString>Fuchun Peng, Fangfang Feng, and Andrew McCallum. 2004. Chinese segmentation and new word detection using conditional random fields. In Proceedings COLING 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rakuten-Inc</author>
</authors>
<date>2012</date>
<note>Rakuten Ichiba http://www.rakuten.co.jp/.</note>
<marker>Rakuten-Inc, 2012</marker>
<rawString>Rakuten-Inc. 2012. Rakuten Ichiba http://www.rakuten.co.jp/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Pichuan Chang</author>
<author>Galen Andrew</author>
<author>Daniel Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A conditional random field word segmenter.</title>
<date>2005</date>
<booktitle>In Fourth SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="16095" citStr="Tseng et al., 2005" startWordPosition="2733" endWordPosition="2736">ed 30 Katakana differences between +LM-S and +LM-P, and found out that 25 out of 30 (83%) are true improvement. One of the proposed method’s advantages is that it is very robust to variations, such as アクティベイティッド akutibeitiddo “activated,” even though only the original form, アクティベ イト akutibeito “activate” is in the dictionary. One type of errors can be attributed to non-English words such as ス ノ コ ベッド sunokobeddo, which is a compound of Japanese word スノコ sunoko “duckboard” and an English word ベッド beddo “bed.” 6.3 Chinese WS Results We compare the results on Chinese WS, with Stanford Segmenter (Tseng et al., 2005) (Table 4) 6. Including +LM-S decreased the 5Because MeCab+UniDic and KyTea models are actually trained on BCCWJ itself, this evaluation is not meaningful but just for reference. The WS granularity of IPADic, JUMAN, and KyTea is also different from the BCCWJ style. 6Note that the comparison might not be fair since (1) Stanford segmenter’s criteria are different from 186 Model Prec. (O) Rec. (O) F (O) Prec. (K) Rec. (K) F (K) WER MeCab+IPADic 91.28 89.87 90.57 88.74 82.32 85.41 12.87 MeCab+UniDic* (98.84) (99.33) (99.08) (96.51) (97.34) (96.92) (1.31) JUMAN 85.66 78.15 81.73 91.68 88.41 90.01 2</context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A conditional random field word segmenter. In Fourth SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyotaka Uchimoto</author>
<author>Satoshi Sekine</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Morphological analysis based on a maximum entropy model — an approach to the unknown word problem — (in Japanese).</title>
<date>2001</date>
<journal>Journal of Natural Language Processing,</journal>
<pages>8--127</pages>
<contexts>
<context position="3823" citStr="Uchimoto et al. (2001)" startWordPosition="586" endWordPosition="589">The results show that we achieved a significant improvement in WS accuracy in both languages. 2 Related Work In Japanese WS, unknown words are usually dealt with in an online manner with the unknown word model, which uses heuristics 183 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 183–189, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics depending on character types (Kudo et al., 2004). Nagata (1999) proposed a Japanese unknown word model which considers PoS (part of speech), word length model and orthography. Uchimoto et al. (2001) proposed a maximum entropy morphological analyzer robust to unknown words. In Chinese, Peng et al. (2004) used CRF confidence to detect new words. For offline approaches, Mori and Nagao (1996) extracted unknown word and estimated their PoS from a corpus through distributional analysis. Asahara and Matsumoto (2004) built a character-based chunking model using SVM for Japanese unknown word detection. Kaji and Kitsuregawa (2011)’s approach is the closest to ours. They built a model to split Katakana compounds using backtransliteration and paraphrasing mined from large corpora. Nakazawa et al. (2</context>
</contexts>
<marker>Uchimoto, Sekine, Isahara, 2001</marker>
<rawString>Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara. 2001. Morphological analysis based on a maximum entropy model — an approach to the unknown word problem — (in Japanese). Journal of Natural Language Processing, 8:127–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Joint word segmentation and pos tagging using a single perceptron.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>888--896</pages>
<contexts>
<context position="5471" citStr="Zhang and Clark, 2008" startWordPosition="844" endWordPosition="847">n, German etc. where compounds may not be explicitly split by whitespaces. Koehn and Knight (2003) tackled the splitting problem in German, by using word statistics in a monolingual corpus. They also used the information whether translations of compound parts appear in a German-English bilingual corpus. Lehal (2010) used Urdu-Devnagri transliteration and a Hindi corpus for handling the space omission problem in Urdu compound words. 3 Word Segmentation Model Out baseline model is a semi-Markov structure prediction model which estimates WS and the PoS sequence simultaneously (Kudo et al., 2004; Zhang and Clark, 2008). This model finds the best output y* from the input sentence string x as: y* = arg maxyEY(x) w *y). Here, Y (x) denotes all the possible sequences of words derived from x. The best analysis is determined by the feature function φ(y) the ID Feature ID Feature 1 wi 13 w1i−1w1i 2 t1 14 t1 i−1t1 3* i 16* i 4* t1i t215* 17* t1 i−1t2 i−1t1 i t2 5* i 18* i 6* t1 i t2 it3 19 t1 i−1t2 i−1t3 i−1t1 i t2 i t3 7 i 20 i 8* t1 i t2 i t5 i t6 21 t1 i−1t2 i−1t5 i−1t6 i−1t1 i t2 i t5 i t6 9* i 22 i 10* t1 i t2 i t6 t1 i−1t2 i−1t6 i−1t1 i t2 i t6 11* i i 12 wit1 φLMS i 1 (wi) wit1 i t2 φLMS i 2 (wi−1, wi) wit1 </context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. Joint word segmentation and pos tagging using a single perceptron. In Proceedings of ACL 2008, pages 888–896.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>