<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.994107">
Learning Word Representations from Scarce and Noisy Data with
Embedding Sub-spaces
</title>
<author confidence="0.699627">
Ramon F. Astudillo, Silvio Amir, Wang Lin, M´ario Silva, Isabel Trancoso
</author>
<bodyText confidence="0.58667925">
Instituto de Engenharia de Sistemas e Computadores Investigac¸˜ao e Desenvolvimento (INESC-ID)
Rua Alves Redol 9
Lisbon, Portugal
{ramon.astudillo, samir, wlin, mjs, isabel.trancoso}@inesc-id.pt
</bodyText>
<sectionHeader confidence="0.970976" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999934409090909">
We investigate a technique to adapt unsu-
pervised word embeddings to specific ap-
plications, when only small and noisy la-
beled datasets are available. Current meth-
ods use pre-trained embeddings to initial-
ize model parameters, and then use the la-
beled data to tailor them for the intended
task. However, this approach is prone to
overfitting when the training is performed
with scarce and noisy data. To overcome
this issue, we use the supervised data to
find an embedding subspace that fits the
task complexity. All the word representa-
tions are adapted through a projection into
this task-specific subspace, even if they do
not occur on the labeled dataset. This ap-
proach was recently used in the SemEval
2015 Twitter sentiment analysis challenge,
attaining state-of-the-art results. Here we
show results improving those of the chal-
lenge, as well as additional experiments in
a Twitter Part-Of-Speech tagging task.
</bodyText>
<sectionHeader confidence="0.998879" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999952625">
The success of supervised systems largely depends
on the amount and quality of the available train-
ing data, oftentimes, even more than the particu-
lar choice of learning algorithm (Banko and Brill,
2001). Labeled data is, however, expensive to ob-
tain, while unlabeled data is widely available. In
order to exploit this fact, semi-supervised learn-
ing methods can be used. In particular, it is pos-
sible to derive word representations by exploiting
word co-occurrence patterns in large samples of
unlabeled text. Based on this idea, several meth-
ods have been recently proposed to efficiently es-
timate word embeddings from raw text, leverag-
ing neural language models (Huang et al., 2012;
Mikolov et al., 2013; Pennington et al., 2014; Ling
et al., 2015). These models work by maximizing
the probability that words within a given window
size are predicted correctly. The resulting embed-
dings are low-dimensional dense vectors that en-
code syntactic and semantic properties of words.
Using these word representations, Turian et al.
(2010) were able to improve near state-of-the-art
systems for several tasks, by simply plugging in
the learned word representations as additional fea-
tures. However, because these features are esti-
mated by minimizing the prediction errors made
on a generic, unsupervised, task they might be
suboptimal for the intended purposes.
Ideally, word features should be adapted to the
specific supervised task. One of the reasons for
the success of deep learning models for language
problems, is the use unsupervised word embed-
dings to initialize the word projection layer. Then,
during training, the errors made in the predictions
are backpropagated to update the embeddings, so
that they better predict the supervised signal (Col-
lobert et al., 2011; dos Santos and Gatti, 2014a).
However, this strategy faces an additional chal-
lenge in noisy domains, such as social media.
The lexical variation caused by the typos, use of
slang and abbreviations leads to a great number
of singletons and out-of-vocabulary words. For
these words, the embeddings will be poorly re-
estimated. Even worse, words not present on the
training set will never get their embeddings up-
dated.
In this paper, we describe a strategy to adapt un-
supervised word embeddings when dealing with
small and noisy labeled datasets. The intuition be-
hind our approach is the following. For a given
task, only a subset of all the latent aspects captured
by the word embeddings will be useful. Therefore,
instead of updating the embeddings directly with
the available labeled data, we estimate a projec-
tion of these embeddings into a low dimensional
sub-space. This simple method brings two funda-
</bodyText>
<page confidence="0.97308">
1074
</page>
<note confidence="0.977625">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1074–1084,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999641">
mental advantages. On the one hand, we obtain
low dimensional embeddings fitting the complex-
ity of the target task. On the other hand, we are
able to learn new representations for all the words,
even if they do not occur in the labeled dataset.
To estimate the low dimensional sub-space, we
propose a simple non-linear model equivalent to a
neural network with one single hidden layer. The
model is trained in supervised fashion on the la-
beled dataset, learning jointly the sub-space pro-
jection and a classifier for the target task. Using
this model, we built a system to participate in the
SemEval 2015 Twitter sentiment analysis bench-
mark (Rosenthal et al., 2015). Our submission at-
tained state-of-the-art results without hand-coded
features or linguistic resources (Astudillo et al.,
2015). Here, we further investigate this approach
and compare it against several state-of-the-art sys-
tems for Twitter sentiment classification. We also
report on additional experiments to assess the ad-
equacy of this strategy in other natural language
problems. To this end, we apply the embedding
sub-space layer to Ling et al. (2015) deep learning
model for part-of-speech tagging. Even though
the gains were not as significant as in the senti-
ment polarity prediction task, the results suggest
that our method is indeed generalizable to other
problems.
The rest of the paper is organized as follows: the
related work is reviewed in Section 2. Section 3,
briefly describes the model used to pre-train the
word embeddings. In Section 4, we introduce the
concept of embedding sub-space, as well as the
the non-linear sub-space model for text classifica-
tion. Section 5, details the experiments performed
with the SemEval corpora. Section 6 describes ad-
ditional experiments applying the embedding sub-
space method to a Part-of-Speech tagging model
for Twitter data. Finally, Section 7 draws the con-
clusions.
</bodyText>
<sectionHeader confidence="0.999764" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999974966666667">
NLP systems can benefit from a very large pool
of unlabeled data. While raw documents are usu-
ally not annotated, they contain structure, which
can be leveraged to learn word features. Con-
text is one strong indicator for word similarity,
as related words tend to occur in similar con-
texts (Firth, 1968). Approaches that are based on
this concept include, Latent Semantic Analysis,
where words are represented as rows in the low-
rank approximation of a term co-occurrence ma-
trix (Dumais et al., 1988), word clusters obtained
with hierarchical clustering algorithms based on
Hidden Markov Models (Brown et al., 1992), and
continuous word vectors learned with neural lan-
guage models (Bengio et al., 2003). The result-
ing clusters and vectors, can then be used as more
generalizable features in supervised tasks, as they
also provide representations for words not present
in the labeled data (Bespalov et al., 2011; Owoputi
et al., 2013; Chen and Manning, 2014).
A great amount of work has been done on the
problem of learning better word representations
from unsupervised data. However, not many stud-
ies have discussed the best ways to use them in
supervised tasks. Typically, in these cases, word
representations are directly used as features or to
initialize the parameters of more complex mod-
els. In some tasks, this approach is however prone
to overfitting. The work presented here aims to
provide a simple approach to overcome this last
scenario. It is thus directly related to Labutov
and Lipson (2013), where a method to learn task-
specific representations from general pre-trained
embeddings was presented. In this work, new fea-
tures were estimated with a convex objective func-
tion that combined the log-likelihood of the train-
ing data, with regularization penalizing the Frobe-
nius norm of the distortion matrix. That is, the ma-
trix of the differences between the original and the
new embeddings. Even though the adapted em-
beddings performed better than the purely unsu-
pervised features, both were significantly outper-
formed by a simple bag-of-words baseline.
Most other approaches, simply rely on addi-
tional training data to fine tune the embeddings for
a given supervised task. In Bansal et al. (2014),
better word embeddings for dependency parsing
were obtained by using a corpus created to cap-
ture dependency context. This technique requires,
nevertheless, of a pre-existing dependency parser
or, at least a parsed corpus. For some other tasks,
it is possible to collect weakly labeled corpora by
making strong assumptions about the data. In Go
et al. (2009) a corpus for Twitter sentiment anal-
ysis was built by assuming that tweets with posi-
tive emoticons imply positive sentiment, whereas
tweets with negative emoticons imply negative
sentiment. Using a similar corpus, Tang et al.
(2014b) induced sentiment specific word embed-
dings, for the Twitter domain. The embeddings
</bodyText>
<page confidence="0.990677">
1075
</page>
<bodyText confidence="0.9998594">
were estimated with a neural network that mini-
mized a linear combination of two loss functions,
one penalized the errors made at predicting the
center word within a sequence of words, while the
other penalized mistakes made at deciding the sen-
timent label. Weakly labeled data has also been
used to refine unsupervised embeddings, by re-
training them to predict the noisy labels before us-
ing the actual task-specific supervised data (Sev-
eryn and Moschitti, 2015).
</bodyText>
<sectionHeader confidence="0.564048" genericHeader="method">
3 Unsupervised Structured Skip-Gram
Word Embeddings
</sectionHeader>
<bodyText confidence="0.99975652">
Word embeddings are generally trained by opti-
mizing an objective function that can be measured
without annotations. One popular approach is to
estimate the embeddings by maximizing the prob-
ability that the words within a given window size
are predicted correctly. Our previous work has
compared several such models, namely the skip-
gram and CBOW architectures (Mikolov et al.,
2013), GloVe (Pennington et al., 2014), and the
structured skip-gram approach (Ling et al., 2015),
suggesting that they all have comparable capabil-
ities. Thus, in this study we only use embeddings
derived with the structured skip-gram approach, a
modification of the skip-gram architecture that has
been shown to outperform the original model in
syntax based tasks such as, part-of-speech tagging
and dependency parsing.
Central to the structured skip-gram is a log lin-
ear model of word prediction. Let w = i denote
that a word at a given position of a sentence is
the i-th word on a vocabulary of size v, and let
wp = j denote that the word p positions further
in the sentence is the j-th word on the vocabu-
lary. The structured skip-gram models the follow-
ing probability:
</bodyText>
<equation confidence="0.982054">
( j · E · wi)
p(wp = j|w = i) a exp Cp (1)
</equation>
<bodyText confidence="0.999142965517241">
Here, wi E 11, 01v×1 is a one-hot representa-
tion of w = i. That is, a vector of zeros of the
size of the vocabulary v with a 1 on the i-th entry
of the vector. The symbol · denotes internal prod-
uct and exp() acts element-wise. The log-linear
model is parametrized by the following matrices:
E E Re×v, is the embedding matrix, transform-
ing the one-hot representation into a compact real
valued space of size e, Cpj E Rv×e is a set of out-
put matrices, one for each relative word position p,
projecting the real-valued representation to a vec-
tor with the size of the vocabulary v. By learn-
ing a different matrix Cp for each relative word
position, the model captures word order informa-
tion, unlike the original skip-gram approach that
uses only one output matrix. Finally, a distribution
over all possible words is attained by exponentiat-
ing and normalizing over the v possible options. In
practice, negative sampling is used to avoid having
to normalize over the whole vocabulary (Goldberg
and Levy, 2014).
As most other neural network models, the struc-
tured skip-gram is trained with gradient-based
methods. After a model has been trained, the low
dimensional embedding E · wi E Re×1 encapsu-
lates the information about each word wi and its
surrounding contexts. This embbeding can thus
be used as input to other learning algorithms to
further enhance performance.
</bodyText>
<sectionHeader confidence="0.87329" genericHeader="method">
4 Adapting Embeddings with Sub-space
Projections
</sectionHeader>
<bodyText confidence="0.999991736842105">
As detailed in the introduction and related work,
word embeddings are a useful unsupervised tech-
nique to attain initial model values or features
prior to supervised training. These models can
be then retrained using the available labeled data.
However, even if the embeddings provide a com-
pact real valued representation of each word in a
vocabulary, the total number of parameters in the
model can be rather high. If, as it is often the
case, only a small amount of supervised data is
available, this can lead to severe overfitting. Even
if regularization is used to reduce the overfitting
risk, only a reduced subset of the words will actu-
ally be present in the labeled dataset. Words not
seen during training will never get their embed-
dings updated. Furthermore, rare words will re-
ceive very few updates, and thus their embeddings
will be poorly adapted for the intended task. We
propose a simple solution to avoid this problem.
</bodyText>
<subsectionHeader confidence="0.99705">
4.1 Embedding Sub-space
</subsectionHeader>
<bodyText confidence="0.9999775">
Let E E Re×v denote the original embedding
matrix obtained, e.g. with the structured skip-
gram model described in Equation 1. We define
the adapted embedding matrix as the factorization
S · E, where S E Rs×e, with s « e. We estimate
the parameters of the matrix S using the labeled
dataset, while E is kept fixed. In other words, we
determine the optimal projection of the embedding
</bodyText>
<page confidence="0.962218">
1076
</page>
<bodyText confidence="0.973821666666667">
matrix E into a sub-space of dimension s.
The idea of embedding sub-space rests on two
fundamental principles:
</bodyText>
<listItem confidence="0.792568285714286">
1. With dimensionality reduction of the embed-
dings, the model can better fit the complexity
of the task at hand or the amount of available
data.
2. Using a projection, all the embeddings are
indirectly updated, not only those of words
present in the labeled dataset.
</listItem>
<bodyText confidence="0.999969695652174">
One question that arises from this approach, is
if the estimated projection is also optimal for the
words not present in the labeled dataset. We as-
sume that the words on the labeled dataset are, to
some extent, representative of the words found in
the unlabeled corpus. This is a reasonable assump-
tion since both datasets can be seen as samples
drawn from the same power-law distribution. If
this holds, for every unknown word, there will be
some other word sufficiently close it in the embed-
ding space. Consequently, the projection matrix
S will also be approximately valid for those un-
seen words. It is often the case that a relatively
small number of words of the labeled dataset are
not present on the unlabeled corpus. These words
are not represented in E. One way to deal with this
case, is to simply set the embeddings of unknown
words to zero. But in this case, the embeddings
will not be adapted during training. Random ini-
tializations of the embeddings seems to be help-
ful for tasks that have a higher penalty for missing
words, although it remains unclear if better initial-
ization strategies exist.
</bodyText>
<subsectionHeader confidence="0.996692">
4.2 Non-Linear Embedding Sub-space Model
</subsectionHeader>
<bodyText confidence="0.999995090909091">
The concept of embedding sub-space can be ap-
plied to log-linear classifiers or any deep learning
architecture that uses embeddings. We now de-
scribe an application of this method for short text
classification tasks. In what follows, we will refer
to this approach as Non-Linear Sub-space Embed-
ding (NLSE) model. The NLSE can be interpreted
as a simple feed-forward neural network model
(Rumelhart et al., 1985) with one single hidden
layer utilizing the embedding sub-space approach,
as depicted in Fig. 1. Let
</bodyText>
<equation confidence="0.979041">
m = [w1 ··· wn] (2)
</equation>
<bodyText confidence="0.997196857142857">
denote a message of n words. Each column
w E {0,1}vx1 of m represents a word in one-
hot form, as described in Section 3. Let y de-
note a categorical random variable over K classes.
The NLSE model, estimates thus the probability of
each possible category y = k E K given a mes-
sage m as
</bodyText>
<equation confidence="0.940627">
p(y = k1m) a exp (Yk · h · 1) . (3)
</equation>
<bodyText confidence="0.9983335">
Here, h E {0,1}exn are the activations of the hid-
den layer for each word, given by
</bodyText>
<equation confidence="0.976069">
h = Q (S · E · m) (4)
</equation>
<bodyText confidence="0.999867666666667">
where Q() is a sigmoid function acting on each
element of the matrix. The matrix Y E R3xs
maps the embedding sub-space to the classifica-
tion space and 1 E 1nx1 is a matrix of ones that
sums the scores for all words together, prior to nor-
malization. This is equivalent to a bag-of-words
assumption. Finally, the model computes a prob-
ability distribution over the K classes, using the
softmax function.
Compared to a conventional feed-forward net-
work employing embeddings for natural language
classification tasks, two main differences arise.
First, the input layer is factorized into two com-
ponents, the embeddings attained in unsupervised
form, E, and the projection matrix S. Second, the
size of the sub-space, in which the embeddings are
projected, is much smaller than that of the origi-
nal embeddings with typical reductions above one
order of magnitude. As usual in this kind of mod-
els, all the parameters can be trained with gradient
methods, using the backpropagation update rule.
</bodyText>
<sectionHeader confidence="0.997826" genericHeader="method">
5 NLSE for Twitter Sentiment Analysis
</sectionHeader>
<bodyText confidence="0.999887571428571">
In this section, we apply the NLSE model to the
message polarity classification task proposed by
SemEval, for their well-known Twitter sentiment
analysis challenge (Nakov et al., 2013). Given a
message, the goal is to decide whether it expresses
a positive, negative, or neutral sentiment. Most
of the top performing systems that participated in
this challenge, relied on linear classification mod-
els and the bag-of-words assumption, representing
messages as sparse vectors of the size of the vo-
cabulary. In the case of social media, this approach
is particularly inefficient, due to the large vocabu-
laries necessary to account for all the lexical vari-
ation found in this domain. Thus, these models
</bodyText>
<page confidence="0.978092">
1077
</page>
<figure confidence="0.864547">
Negative
</figure>
<figureCaption confidence="0.990461">
Figure 1: Illustration of the NLSE model, applied
to sentiment polarity prediction.
</figureCaption>
<table confidence="0.9997102">
Positive Neutral Negative
Development 3230 4109 1265
Tweets 2015 1032 983 364
Tweets 2014 982 669 202
Tweets 2013 1572 1640 601
</table>
<tableCaption confidence="0.996234">
Table 1: Number of examples per class in each
</tableCaption>
<bodyText confidence="0.990498736842105">
SemEval dataset. The first row shows the training
data; the other rows are sets used for testing.
need to be enriched with additional hand-crafted
features that try to capture more discriminative as-
pects of the content, most of which require exter-
nal tools (e.g., part-of-speech taggers and parsers)
or linguistic resources (e.g., dictionaries and sen-
timent lexicons) (Miura et al., 2014; Kiritchenko
et al., 2014). With the embedding sub-space ap-
proach, however, we are able to attain state-of-
the-art performance while requiring only minimal
processing of the data and few hyperparameters.
To make our results comparable to other systems
for this task, we adopted the guidelines from the
benchmark. Our system was trained and tuned
using only the development data. The evaluation
was performed on the test sets, shown in Table 1,
and we report the results in terms of the average
F-measure for the positive and negative classes.
</bodyText>
<subsectionHeader confidence="0.952138">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999997525">
The first step of our approach requires a corpus of
raw text for the unsupervised pre-training of the
embedding matrix E. We resorted to the corpus of
52 million tweets used in (Owoputi et al., 2013)
and the tokenizer described in the same work. The
messages were previously pre-processed as fol-
lows: lower-casing, replacing Twitter user men-
tions and URLs with special tokens and reducing
any character repetition to at most 3 characters.
Words occurring less than 40 times in the cor-
pus were discarded, resulting in a vocabulary of
around 210,000 types. Then, a modified version
of the word2vec tool1 was used to compute the
word embeddings, as described in Section 3. The
window size and negative sampling rate were set
to 5 and 25 words, respectively, and embeddings
of 50, 200, 400 and 600 dimensions were built.
Our system accepts as input a sentence rep-
resented as a matrix, obtained by concatenating
the one-hot vectors that represent each individual
word. Therefore, we first performed the afore-
mentioned normalization and tokenization steps
and then, converted each tweet into this represen-
tation. The development set was split into 80%
for parameter learning and 20% for model evalu-
ation and selection, maintaining the original rela-
tive class proportions in each set. All the weights
were initialized uniformly at random, as proposed
in (Glorot and Bengio, 2010). The model was
trained with conventional Stochastic Gradient De-
scent (Rumelhart et al., 1985) with a fixed learning
rate of 0.01, and the weights were updated after
each message was processed. Variations of learn-
ing rate to smaller values, e.g. 0.005, were con-
sidered but did not lead to a clear pattern. We ex-
plored different configurations of the hyperparam-
eters a (embedding size) and s (sub-space size).
Model selection was done by early stopping, i.e.,
we kept the configuration with best F-measure on
the evaluation set after 5-8 iterations.
</bodyText>
<subsectionHeader confidence="0.593131">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999892857142857">
In general, the NLSE model showed consistent
and fast convergence towards the optimum in very
few iterations. Despite using class log-likelihood
as training criterion, it showed good performance
in terms of the average F-measure for positive
and negative sentiments. We found that all em-
bedding sizes yield comparable performances, al-
</bodyText>
<footnote confidence="0.695982">
1https://github.com/wlin12/wang2vec
</footnote>
<figure confidence="0.99797">
Word Embeddings
Subspace
Logistic
Sigmoid
Some
ppl r juz unrealiable
Bow
Softmax
over
Polarity
</figure>
<page confidence="0.771452">
1078
</page>
<figureCaption confidence="0.96945825">
Figure 2: Average F-measure on the SemEval
test sets varying with embedding sub-space size
s. Sub-space size 0 used to denote the baseline
(log-linear model).
</figureCaption>
<bodyText confidence="0.998928705882353">
though larger embeddings tend to perform better.
Therefore, we only report results obtained with the
600 dimensional vectors. In Figure 2, we show the
variation of system performance with sub-space
size s. The baseline is a log-linear using the em-
beddings in E as features. As it can be seen, the
performance is sharply improved when the em-
bedding sub-spaces are used. By choosing dif-
ferent values of s, we can adjust the model to the
complexity of the task and the amount of labeled
data available. Given the small size of the train-
ing set, the best results were attained with the use
of smaller sub-spaces, in the range of 5-10 dimen-
sions.
Figure 3, presents the main results of the ex-
perimental evaluation. As baselines, we consid-
ered two simple approaches: LOG-LINEAR, which
uses the unsupervised embeddings directly as fea-
tures in a log-linear classifier, and LOG-LINEAR*,
also using the unsupervised embeddings as fea-
tures in a log-linear classifier, but updating the em-
beddings with the training data. These baselines,
were compared against two variations of the non-
linear sub-space embedding model: NLSE, where
we only train the S and Y weights while the em-
beddings are kept fixed, and NLSE*, where we
also update the embedding matrix during training.
For these experiments, we set s = 10. The re-
sults in Figure 3a, show that our model largely
outperforms the simpler baselines. Furthermore,
we observe that updating the embeddings always
leads to inferior results. This suggests that pre-
computed embeddings should be kept fixed, when
little labeled data is available to re-train them.
</bodyText>
<subsectionHeader confidence="0.445871">
Comparison with the state-of-the-art
</subsectionHeader>
<bodyText confidence="0.778915808510638">
We now compare the NLSE model with state-of-
the-art systems, including the best submissions to
previous SemEval benchmarks. We also include
two other approaches that are related to the one
here proposed, where a neural network initialized
with pre-trained word embeddings is used to learn
relevant features. Specifically, we compare the
following systems:
• NRC (Kiritchenko et al., 2014), a support
vector machine classifier with a large set
of hand-crafted features, including word and
character n-grams, brown clusters, POS tags,
morphological features, and a set of features
based on five sentiment lexicons. Most of the
performance was due to the combination of
these lexicons. This was the top system in
the 2013 edition of SemEval.
• TEAMX (Miura et al., 2014), a logistic re-
gression classifier using a similar set of fea-
tures. Additional features based on two dif-
ferent POS taggers and a word sense dis-
ambiguator were also included in the model.
This approach attained the highest ranking in
the 2014 edition.
• CHARSCNN (dos Santos and Gatti, 2014b),
a deep learning architecture with two con-
volutional layers that exploit character-level
and word-level information. The features are
extracted by converting a sentence into a se-
quence of word embeddings, and the individ-
ual words into sequences of character embed-
dings. Convolution filters followed by max
pooling are applied to these sequences, to
produce fixed size vectors. These vectors are
then combined and transfered to a set of non-
linear activation functions, to generate more
complex representations of the input. The
predictions, based on these learned features
are computed with a softmax classifier.
• COOOOLLL (Tang et al., 2014a), a support
vector machine classifier that leverages the
sentiment specific word embeddings, dis-
cussed in Section 2. The embeddings are
also processed with a convolution filter, but
the output of this operation is used to pro-
duce three representations obtained with dif-
ferent strategies, namely with max, min and
</bodyText>
<page confidence="0.993803">
1079
</page>
<figure confidence="0.961313">
(a) Comparison of two baselines with two variations (b) Performance of state-of-the-art systems for Twitter senti-
of the NLSE model ment prediction
</figure>
<figureCaption confidence="0.999939">
Figure 3: Average F-measure on the SemEval test sets
</figureCaption>
<bodyText confidence="0.954399666666667">
average pooling. The final feature vector is
obtained by concatenating these representa-
tions and Kiritchenko et al. (2014) feature set.
</bodyText>
<listItem confidence="0.944387461538461">
• UNITN (Severyn and Moschitti, 2015), an-
other deep convolutional neural network that
jointly learns internal representations and a
softmax classifier. The network is trained
in three steps: (i) unsupervised pre-training
of embeddings, (ii) refinement of the em-
beddings using a weakly labeled corpus, and
(iii) fine tuning the model with the labeled
data from SemEval. It should be noted that
the system was trained with a labeled corpus
65% larger than ours2. This system made the
best submission on the 2015 edition of the
benchmark.
</listItem>
<bodyText confidence="0.9999265">
The results in Figure 3b, show that despite be-
ing simpler and requiring less resources and la-
beled data, the NLSE model is extremely compet-
itive, even outperforming most other systems, in
predicting the sentiment polarity of Twitter mes-
sages.
</bodyText>
<sectionHeader confidence="0.945829" genericHeader="method">
6 Generalization to Other Tasks
</sectionHeader>
<bodyText confidence="0.999471142857143">
While the embedding sub-space method works
well for the sentiment prediction task, we would
like to know its impact in other settings that are
known to benefit from unsupervised embeddings.
Thus, we decided to replicate the part-of-speech
tagging work in (Ling et al., 2015), where pre-
training embeddings have been shown to improve
</bodyText>
<footnote confidence="0.950073">
2The UNITN system was trained with around 11,400 la-
beled examples, whereas we used only 6,900.
the quality of the results significantly.
</footnote>
<subsectionHeader confidence="0.683758">
6.1 Sub-space Window Model
</subsectionHeader>
<bodyText confidence="0.8973778">
Part-of-speech tagging is a word labeling task,
where each word is to be labeled with its syntactic
function in the sentence. More formally, given an
input sentence w1, ... , wn of n words, we wish to
predict a sequence of labels y1, ... , yn, which are
the POS tags of each of the words. This task is
scored by the ratio between the number of correct
labels and the number of words to be labeled.
We modified (Collobert et al., 2011) window
model, to include the sub-space matrix S. The
probability of labeling the word wt with the POS
tag k is given by
p(y = k|mt+p
t−p) a exp (Yk • ht + b) , (5)
where
</bodyText>
<equation confidence="0.987097">
t+p (6)
mt−p = [wt−p ... wt ... wt+pl
</equation>
<bodyText confidence="0.997547666666667">
denotes a context window of words around the
t-th word, with a total span of 2p + 1 words. ht
denotes the activations of a hidden layer given by
</bodyText>
<equation confidence="0.98296075">
ht = tanh ⎛ H - ⎡ S • E • wt+p 1) . (7)
⎜ ⎜ ⎜ ⎜ ⎝ ⎢ ⎢ ⎢ ⎢ ⎣ S•E•wt
� � �
S • E • wt−p
</equation>
<bodyText confidence="0.9727315">
Here tanh denotes the hyperbolic tangent, act-
ing element-wise. Aside from embedding E and
</bodyText>
<page confidence="0.988976">
1080
</page>
<bodyText confidence="0.995693333333333">
sub-space S matrices, the model is parametrized
by the weights H ∈ Rh×pa and Y ∈ Rv×h as well
as a bias b ∈ Rv×1.
Note that if S is set to the identity matrix, this
would be equivalent to the original Collobert et al.
(2011) model.
</bodyText>
<figureCaption confidence="0.808123">
Figure 4: Illustration of the window model
by (Collobert et al., 2011) using a sub-space layer.
</figureCaption>
<subsectionHeader confidence="0.995992">
6.2 Experiments
</subsectionHeader>
<bodyText confidence="0.993507863636364">
Tests were performed in Gimpel et al. (2011) Twit-
ter POS dataset, which uses the universal POS tag
set composed by 25 different labels (Petrov et al.,
2012). The dataset contains 1000 annotated tweets
for training, 327 tweets for tuning and 500 tweets
for testing. The number of word tokens in these
sets are 15000, 5000 and 7000, respectively. There
are 5000, 2000 and 3000 word types.
Once again, we initialized the embeddings
with unsupervised pre-training using the struc-
tured skip-gram approach. As for the hyperpa-
rameters of the model, we used embeddings with
e = 50 dimensions, a hidden layer with h = 200
dimensions and a context of p = 2 as used in (Ling
et al., 2015). Training employed mini-batch gradi-
ent descent, with mini batches of 100 sentences
and a momentum of 0.95. The learning rate was
set to 0.2. Finally, we used early stopping by
choosing the epoch with the highest accuracy in
the tuning set. As for the sub-space layer size, we
tried three different hyperparameterizations: 10,
30 and 50 dimensions.
</bodyText>
<subsectionHeader confidence="0.955279">
6.3 Results
</subsectionHeader>
<bodyText confidence="0.999982842105263">
Figure 5 displays the results. Using the setup that
led to the best results in the sentiment prediction
task (FIX), that is, fixing E and updating S, leads
to lower accuracies than the baseline (TRAIN-ALL,
s = 0). We also see that different values of s do
not have a very strong impact in the final results.
Sentiment polarity prediction and POS tagging
differ in multiple aspects and there may be more
than one reason for this poorer performance. One
particularly relevant aspect, in our opinion, is the
way words that have no pre-trained embedding are
treated. In the case of sentiment prediction, these
words were set to having and embedding of zero.
This fits the use of the bag-of-words assumption
and the fact that only one label is produced per
message, as there are many other words to draw
evidence from. In the case of POS tagging a hy-
pothesis must be drawn for each word, using a
shorter context. Thus, ignoring a word means that
context is used instead, which is a frequent cause
of errors.
One way around this problem would be to up-
date the parameters of S and E, but this leads to
results similar to the experiment without the sub-
space projections (TRAIN-ALL). This is expected
as the sub-space layer was designed to work on
fixed word embeddings, if these are updated its
benefits are lost. Thus, we solve this problem
by fixing all the embeddings, except for the word
types not found in the pre-training corpus. That
is, instead of leaving the unknown words as the
zero vector, we use the labeled data to learn a bet-
ter representation. Using this setup (TRAIN-OOV),
we can obtain a small but consistent improvement
over the baseline. While these improvements are
not significant, as this task is not as prone to over-
fitting as in sentiment analysis, this is a good check
of the validity of our method.
</bodyText>
<sectionHeader confidence="0.994591" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999289666666667">
We presented a new approach to use unsupervised
word embeddings based on the idea of finding a
sub-space projection of the embeddings for a given
task. This approach offers two main advantages.
On the one hand, it allows to indirectly update
embeddings unseen during training. On the other
</bodyText>
<figure confidence="0.9963266">
Verb
Word Embeddings
Window
Tanh
Subspace
Softmax
over
Tags
Some
ppl r juz unrealiable
</figure>
<page confidence="0.753313">
1081
</page>
<figureCaption confidence="0.8950286">
Figure 5: Results for the part-of-speech task on
the ARK POS dataset, for different strategies to
update the embeddings and with variations of the
sub-space size. Sub-space size 0 used to denote
the baseline (window model).
</figureCaption>
<bodyText confidence="0.999918931034483">
hand, it reduces the number of model parameters
to fit the complexity of the task. These properties
make this method particularly useful for the cases
where only small amounts of noisy data are avail-
able to train the model.
Experiments on the SemEval challenge corpora
validated these ideas, showing that such a simple
approach can attain state-of-the-art results compa-
rable with the best systems of past SemEval edi-
tions and often outperforming them in all datasets.
It should be noted that this is attained while keep-
ing the original embedding matrix E fixed and
only learning the projection S with the supervised
data. Additional experiments on the Twitter POS
tagging task indicate however that, the technique
is not always as effective as in the sentiment clas-
sification task. One possible explanation for the
different behavior is the use of embeddings of ze-
ros for words without pre-trained embedding. It is
plausible that this has a stronger effect on the POS
tagging task. Another aspect to be taken into ac-
count is the fact that both tasks could have a differ-
ent complexity which would explain why adapting
E in the POS taks yields better results. Optimality
of the embeddings for each of the tasks might also
come into play here.
The implementation of the proposed method
and our Twitter Sentiment Analysis system has
been made publicly available3.
</bodyText>
<footnote confidence="0.9734015">
3https://github.com/ramon-astudillo/
NLSE
</footnote>
<sectionHeader confidence="0.989282" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999729">
This work was partially supported by Fundac¸˜ao
para a Ciˆencia e Tecnologia (FCT), through
contracts UID/CEC/50021/2013, EXCL/EEI-
ESS/0257/2012 (DataStorm), research project
with reference UTAP-EXPL/EEI-ESS/0031/2014,
EU project SPEDIAL (FP7 611396), grant
number SFRH/BPD/68428/2010 and Ph.D.
scholarship SFRH/BD/89020/2012.
</bodyText>
<sectionHeader confidence="0.997856" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998798023255814">
Ramon F. Astudillo, Silvio Amir, Wang Ling, Bruno
Martins, M´ario Silva, and Isabel Trancoso. 2015.
Inesc-id: Sentiment analysis without hand-coded
features or liguistic resources using embedding sub-
spaces. In Proceedings of the 9th International
Workshop on Semantic Evaluation, SemEval ’2015,
Denver, Colorado, June. Association for Computa-
tional Linguistics.
Michele Banko and Eric Brill. 2001. Scaling to very
very large corpora for natural language disambigua-
tion. In Proceedings of the 39th Annual Meeting
on Association for Computational Linguistics, pages
26–33. Association for Computational Linguistics.
Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring continuous word representations for
dependency parsing. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137–1155.
Dmitriy Bespalov, Bing Bai, Yanjun Qi, and Ali Shok-
oufandeh. 2011. Sentiment classification based on
supervised latent n-gram analysis. In Proceedings of
the 20th ACM international conference on Informa-
tion and knowledge management, pages 375–382.
ACM.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Comput. Linguist., 18(4):467–479, Decem-
ber.
Danqi Chen and Christopher D Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 740–750.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
</reference>
<page confidence="0.963177">
1082
</page>
<reference confidence="0.997291854545454">
Cicero dos Santos and Maira Gatti. 2014a. Deep
convolutional neural networks for sentiment analy-
sis of short texts. In Proceedings of COLING 2014,
the 25th International Conference on Computational
Linguistics: Technical Papers, pages 69–78, Dublin,
Ireland, August. Dublin City University and Associ-
ation for Computational Linguistics.
Cıcero Nogueira dos Santos and Maıra Gatti. 2014b.
Deep convolutional neural networks for sentiment
analysis of short texts. In Proceedings of the 25th In-
ternational Conference on Computational Linguis-
tics (COLING), Dublin, Ireland.
Susan T Dumais, George W Furnas, Thomas K Lan-
dauer, Scott Deerwester, and Richard Harshman.
1988. Using latent semantic analysis to improve ac-
cess to textual information. In Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 281–285. ACM.
John Rupert Firth. 1968. Selected papers of JR Firth,
1952-59. Indiana University Press.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: annotation, features, and experiments. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers - Volume 2, HLT
’11, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neural
networks. In International conference on artificial
intelligence and statistics, pages 249–256.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, pages 1–12.
Yoav Goldberg and Omer Levy. 2014. word2vec
explained: deriving mikolov et al.’s negative-
sampling word-embedding method. arXiv preprint
arXiv:1402.3722.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers - Volume 1, ACL ’12, pages 873–
882, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Svetlana Kiritchenko, Xiaodan Zhu, and Saif M Mo-
hammad. 2014. Sentiment analysis of short in-
formal texts. Journal of Artificial Intelligence Re-
search, pages 723–762.
Igor Labutov and Hod Lipson. 2013. Re-embedding
words. In Proceedings of the 51st annual meeting of
the ACL, pages 489–493.
Wang Ling, Chris Dyer, Alan Black, and Isabel
Trancoso. 2015. Two/too simple adaptations of
word2vec for syntax problems. In Proceedings of
the 2015 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies. Association
for Computational Linguistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In 27th Annual Conference on Neural Infor-
mation Processing Systems.
Yasuhide Miura, Shigeyuki Sakaki, Keigo Hattori, and
Tomoko Ohkuma. 2014. Teamx: A sentiment ana-
lyzer with enhanced lexicon mapping and weighting
scheme for unbalanced data. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval 2014), pages 628–632, Dublin, Ireland,
August. Association for Computational Linguistics.
Preslav Nakov, Zornitsa Kozareva, Alan Ritter, Sara
Rosenthal, Veselin Stoyanov, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter.
Olutobi Owoputi, Chris Dyer, Kevin Gimpel, Nathan
Schneider, and Noah A. Smith. 2013. Improved
part-of-speech tagging for online conversational text
with word clusters. In In Proceedings of NAACL.
Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for
word representation. Proceedings of the Empiricial
Methods in Natural Language Processing (EMNLP
2014), 12.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
the Eight International Conference on Language Re-
sources and Evaluation (LREC’12). European Lan-
guage Resources Association (ELRA), may.
Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko,
Saif M Mohammad, Alan Ritter, and Veselin Stoy-
anov. 2015. Semeval-2015 task 10: Sentiment
analysis in twitter. In Proceedings of the 9th In-
ternational Workshop on Semantic Evaluation, Se-
mEval ’2015, Denver, Colorado, June. Association
for Computational Linguistics.
David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams. 1985. Learning internal representations
by error propagation. Technical report, DTIC Doc-
ument.
Aliaksei Severyn and Alessandro Moschitti. 2015.
Unitn: Training deep convolutional neural network
for twitter sentiment classification. In Proceedings
of the 9th International Workshop on Semantic Eval-
uation, SemEval ’2015, Denver, Colorado, June. As-
sociation for Computational Linguistics.
</reference>
<page confidence="0.666467">
1083
</page>
<reference confidence="0.999561375">
Duyu Tang, Furu Wei, Bing Qin, Ting Liu, and Ming
Zhou. 2014a. Coooolll: A deep learning system for
twitter sentiment classification. SemEval 2014, page
208.
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014b. Learning sentiment-
specific word embedding for twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1555–1565.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th annual meeting of the association for compu-
tational linguistics, pages 384–394. Association for
Computational Linguistics.
</reference>
<page confidence="0.996119">
1084
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.356795">
<title confidence="0.9997725">Learning Word Representations from Scarce and Noisy Data Embedding Sub-spaces</title>
<author confidence="0.998516">Ramon F Astudillo</author>
<author confidence="0.998516">Silvio Amir</author>
<author confidence="0.998516">Wang Lin</author>
<author confidence="0.998516">M´ario Silva</author>
<author confidence="0.998516">Isabel</author>
<affiliation confidence="0.704345">de Engenharia de Sistemas e Computadores e Desenvolvimento Rua Alves Redol Lisbon,</affiliation>
<email confidence="0.983224">samir,wlin,mjs,</email>
<abstract confidence="0.998475739130435">We investigate a technique to adapt unsupervised word embeddings to specific applications, when only small and noisy labeled datasets are available. Current methods use pre-trained embeddings to initialize model parameters, and then use the labeled data to tailor them for the intended task. However, this approach is prone to overfitting when the training is performed with scarce and noisy data. To overcome this issue, we use the supervised data to find an embedding subspace that fits the task complexity. All the word representations are adapted through a projection into this task-specific subspace, even if they do not occur on the labeled dataset. This approach was recently used in the SemEval 2015 Twitter sentiment analysis challenge, attaining state-of-the-art results. Here we show results improving those of the challenge, as well as additional experiments in a Twitter Part-Of-Speech tagging task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ramon F Astudillo</author>
<author>Silvio Amir</author>
<author>Wang Ling</author>
<author>Bruno Martins</author>
<author>M´ario Silva</author>
<author>Isabel Trancoso</author>
</authors>
<title>Inesc-id: Sentiment analysis without hand-coded features or liguistic resources using embedding subspaces.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’2015,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Denver, Colorado,</location>
<contexts>
<context position="5018" citStr="Astudillo et al., 2015" startWordPosition="781" endWordPosition="784">ations for all the words, even if they do not occur in the labeled dataset. To estimate the low dimensional sub-space, we propose a simple non-linear model equivalent to a neural network with one single hidden layer. The model is trained in supervised fashion on the labeled dataset, learning jointly the sub-space projection and a classifier for the target task. Using this model, we built a system to participate in the SemEval 2015 Twitter sentiment analysis benchmark (Rosenthal et al., 2015). Our submission attained state-of-the-art results without hand-coded features or linguistic resources (Astudillo et al., 2015). Here, we further investigate this approach and compare it against several state-of-the-art systems for Twitter sentiment classification. We also report on additional experiments to assess the adequacy of this strategy in other natural language problems. To this end, we apply the embedding sub-space layer to Ling et al. (2015) deep learning model for part-of-speech tagging. Even though the gains were not as significant as in the sentiment polarity prediction task, the results suggest that our method is indeed generalizable to other problems. The rest of the paper is organized as follows: the </context>
</contexts>
<marker>Astudillo, Amir, Ling, Martins, Silva, Trancoso, 2015</marker>
<rawString>Ramon F. Astudillo, Silvio Amir, Wang Ling, Bruno Martins, M´ario Silva, and Isabel Trancoso. 2015. Inesc-id: Sentiment analysis without hand-coded features or liguistic resources using embedding subspaces. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’2015, Denver, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Eric Brill</author>
</authors>
<title>Scaling to very very large corpora for natural language disambiguation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>26--33</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1490" citStr="Banko and Brill, 2001" startWordPosition="223" endWordPosition="226">. All the word representations are adapted through a projection into this task-specific subspace, even if they do not occur on the labeled dataset. This approach was recently used in the SemEval 2015 Twitter sentiment analysis challenge, attaining state-of-the-art results. Here we show results improving those of the challenge, as well as additional experiments in a Twitter Part-Of-Speech tagging task. 1 Introduction The success of supervised systems largely depends on the amount and quality of the available training data, oftentimes, even more than the particular choice of learning algorithm (Banko and Brill, 2001). Labeled data is, however, expensive to obtain, while unlabeled data is widely available. In order to exploit this fact, semi-supervised learning methods can be used. In particular, it is possible to derive word representations by exploiting word co-occurrence patterns in large samples of unlabeled text. Based on this idea, several methods have been recently proposed to efficiently estimate word embeddings from raw text, leveraging neural language models (Huang et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Ling et al., 2015). These models work by maximizing the probability that</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>Michele Banko and Eric Brill. 2001. Scaling to very very large corpora for natural language disambiguation. In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, pages 26–33. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Kevin Gimpel</author>
<author>Karen Livescu</author>
</authors>
<title>Tailoring continuous word representations for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8335" citStr="Bansal et al. (2014)" startWordPosition="1316" endWordPosition="1319">beddings was presented. In this work, new features were estimated with a convex objective function that combined the log-likelihood of the training data, with regularization penalizing the Frobenius norm of the distortion matrix. That is, the matrix of the differences between the original and the new embeddings. Even though the adapted embeddings performed better than the purely unsupervised features, both were significantly outperformed by a simple bag-of-words baseline. Most other approaches, simply rely on additional training data to fine tune the embeddings for a given supervised task. In Bansal et al. (2014), better word embeddings for dependency parsing were obtained by using a corpus created to capture dependency context. This technique requires, nevertheless, of a pre-existing dependency parser or, at least a parsed corpus. For some other tasks, it is possible to collect weakly labeled corpora by making strong assumptions about the data. In Go et al. (2009) a corpus for Twitter sentiment analysis was built by assuming that tweets with positive emoticons imply positive sentiment, whereas tweets with negative emoticons imply negative sentiment. Using a similar corpus, Tang et al. (2014b) induced</context>
</contexts>
<marker>Bansal, Gimpel, Livescu, 2014</marker>
<rawString>Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014. Tailoring continuous word representations for dependency parsing. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="6830" citStr="Bengio et al., 2003" startWordPosition="1071" endWordPosition="1074">uments are usually not annotated, they contain structure, which can be leveraged to learn word features. Context is one strong indicator for word similarity, as related words tend to occur in similar contexts (Firth, 1968). Approaches that are based on this concept include, Latent Semantic Analysis, where words are represented as rows in the lowrank approximation of a term co-occurrence matrix (Dumais et al., 1988), word clusters obtained with hierarchical clustering algorithms based on Hidden Markov Models (Brown et al., 1992), and continuous word vectors learned with neural language models (Bengio et al., 2003). The resulting clusters and vectors, can then be used as more generalizable features in supervised tasks, as they also provide representations for words not present in the labeled data (Bespalov et al., 2011; Owoputi et al., 2013; Chen and Manning, 2014). A great amount of work has been done on the problem of learning better word representations from unsupervised data. However, not many studies have discussed the best ways to use them in supervised tasks. Typically, in these cases, word representations are directly used as features or to initialize the parameters of more complex models. In so</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. The Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Bespalov</author>
<author>Bing Bai</author>
<author>Yanjun Qi</author>
<author>Ali Shokoufandeh</author>
</authors>
<title>Sentiment classification based on supervised latent n-gram analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th ACM international conference on Information and knowledge management,</booktitle>
<pages>375--382</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7038" citStr="Bespalov et al., 2011" startWordPosition="1105" endWordPosition="1108">s (Firth, 1968). Approaches that are based on this concept include, Latent Semantic Analysis, where words are represented as rows in the lowrank approximation of a term co-occurrence matrix (Dumais et al., 1988), word clusters obtained with hierarchical clustering algorithms based on Hidden Markov Models (Brown et al., 1992), and continuous word vectors learned with neural language models (Bengio et al., 2003). The resulting clusters and vectors, can then be used as more generalizable features in supervised tasks, as they also provide representations for words not present in the labeled data (Bespalov et al., 2011; Owoputi et al., 2013; Chen and Manning, 2014). A great amount of work has been done on the problem of learning better word representations from unsupervised data. However, not many studies have discussed the best ways to use them in supervised tasks. Typically, in these cases, word representations are directly used as features or to initialize the parameters of more complex models. In some tasks, this approach is however prone to overfitting. The work presented here aims to provide a simple approach to overcome this last scenario. It is thus directly related to Labutov and Lipson (2013), whe</context>
</contexts>
<marker>Bespalov, Bai, Qi, Shokoufandeh, 2011</marker>
<rawString>Dmitriy Bespalov, Bing Bai, Yanjun Qi, and Ali Shokoufandeh. 2011. Sentiment classification based on supervised latent n-gram analysis. In Proceedings of the 20th ACM international conference on Information and knowledge management, pages 375–382. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Comput. Linguist.,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="6743" citStr="Brown et al., 1992" startWordPosition="1057" endWordPosition="1060">d Work NLP systems can benefit from a very large pool of unlabeled data. While raw documents are usually not annotated, they contain structure, which can be leveraged to learn word features. Context is one strong indicator for word similarity, as related words tend to occur in similar contexts (Firth, 1968). Approaches that are based on this concept include, Latent Semantic Analysis, where words are represented as rows in the lowrank approximation of a term co-occurrence matrix (Dumais et al., 1988), word clusters obtained with hierarchical clustering algorithms based on Hidden Markov Models (Brown et al., 1992), and continuous word vectors learned with neural language models (Bengio et al., 2003). The resulting clusters and vectors, can then be used as more generalizable features in supervised tasks, as they also provide representations for words not present in the labeled data (Bespalov et al., 2011; Owoputi et al., 2013; Chen and Manning, 2014). A great amount of work has been done on the problem of learning better word representations from unsupervised data. However, not many studies have discussed the best ways to use them in supervised tasks. Typically, in these cases, word representations are </context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based n-gram models of natural language. Comput. Linguist., 18(4):467–479, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Christopher D Manning</author>
</authors>
<title>A fast and accurate dependency parser using neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>740--750</pages>
<contexts>
<context position="7085" citStr="Chen and Manning, 2014" startWordPosition="1113" endWordPosition="1116"> this concept include, Latent Semantic Analysis, where words are represented as rows in the lowrank approximation of a term co-occurrence matrix (Dumais et al., 1988), word clusters obtained with hierarchical clustering algorithms based on Hidden Markov Models (Brown et al., 1992), and continuous word vectors learned with neural language models (Bengio et al., 2003). The resulting clusters and vectors, can then be used as more generalizable features in supervised tasks, as they also provide representations for words not present in the labeled data (Bespalov et al., 2011; Owoputi et al., 2013; Chen and Manning, 2014). A great amount of work has been done on the problem of learning better word representations from unsupervised data. However, not many studies have discussed the best ways to use them in supervised tasks. Typically, in these cases, word representations are directly used as features or to initialize the parameters of more complex models. In some tasks, this approach is however prone to overfitting. The work presented here aims to provide a simple approach to overcome this last scenario. It is thus directly related to Labutov and Lipson (2013), where a method to learn taskspecific representatio</context>
</contexts>
<marker>Chen, Manning, 2014</marker>
<rawString>Danqi Chen and Christopher D Manning. 2014. A fast and accurate dependency parser using neural networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740–750.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="3053" citStr="Collobert et al., 2011" startWordPosition="467" endWordPosition="471">resentations as additional features. However, because these features are estimated by minimizing the prediction errors made on a generic, unsupervised, task they might be suboptimal for the intended purposes. Ideally, word features should be adapted to the specific supervised task. One of the reasons for the success of deep learning models for language problems, is the use unsupervised word embeddings to initialize the word projection layer. Then, during training, the errors made in the predictions are backpropagated to update the embeddings, so that they better predict the supervised signal (Collobert et al., 2011; dos Santos and Gatti, 2014a). However, this strategy faces an additional challenge in noisy domains, such as social media. The lexical variation caused by the typos, use of slang and abbreviations leads to a great number of singletons and out-of-vocabulary words. For these words, the embeddings will be poorly reestimated. Even worse, words not present on the training set will never get their embeddings updated. In this paper, we describe a strategy to adapt unsupervised word embeddings when dealing with small and noisy labeled datasets. The intuition behind our approach is the following. For</context>
<context position="27199" citStr="Collobert et al., 2011" startWordPosition="4450" endWordPosition="4453">ve been shown to improve 2The UNITN system was trained with around 11,400 labeled examples, whereas we used only 6,900. the quality of the results significantly. 6.1 Sub-space Window Model Part-of-speech tagging is a word labeling task, where each word is to be labeled with its syntactic function in the sentence. More formally, given an input sentence w1, ... , wn of n words, we wish to predict a sequence of labels y1, ... , yn, which are the POS tags of each of the words. This task is scored by the ratio between the number of correct labels and the number of words to be labeled. We modified (Collobert et al., 2011) window model, to include the sub-space matrix S. The probability of labeling the word wt with the POS tag k is given by p(y = k|mt+p t−p) a exp (Yk • ht + b) , (5) where t+p (6) mt−p = [wt−p ... wt ... wt+pl denotes a context window of words around the t-th word, with a total span of 2p + 1 words. ht denotes the activations of a hidden layer given by ht = tanh ⎛ H - ⎡ S • E • wt+p 1) . (7) ⎜ ⎜ ⎜ ⎜ ⎝ ⎢ ⎢ ⎢ ⎢ ⎣ S•E•wt � � � S • E • wt−p Here tanh denotes the hyperbolic tangent, acting element-wise. Aside from embedding E and 1080 sub-space S matrices, the model is parametrized by the weights H </context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cicero dos Santos</author>
<author>Maira Gatti</author>
</authors>
<title>Deep convolutional neural networks for sentiment analysis of short texts.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,</booktitle>
<pages>69--78</pages>
<institution>Dublin City University and Association for Computational Linguistics.</institution>
<location>Dublin, Ireland,</location>
<contexts>
<context position="3081" citStr="Santos and Gatti, 2014" startWordPosition="473" endWordPosition="476">eatures. However, because these features are estimated by minimizing the prediction errors made on a generic, unsupervised, task they might be suboptimal for the intended purposes. Ideally, word features should be adapted to the specific supervised task. One of the reasons for the success of deep learning models for language problems, is the use unsupervised word embeddings to initialize the word projection layer. Then, during training, the errors made in the predictions are backpropagated to update the embeddings, so that they better predict the supervised signal (Collobert et al., 2011; dos Santos and Gatti, 2014a). However, this strategy faces an additional challenge in noisy domains, such as social media. The lexical variation caused by the typos, use of slang and abbreviations leads to a great number of singletons and out-of-vocabulary words. For these words, the embeddings will be poorly reestimated. Even worse, words not present on the training set will never get their embeddings updated. In this paper, we describe a strategy to adapt unsupervised word embeddings when dealing with small and noisy labeled datasets. The intuition behind our approach is the following. For a given task, only a subset</context>
<context position="24158" citStr="Santos and Gatti, 2014" startWordPosition="3957" endWordPosition="3960">arge set of hand-crafted features, including word and character n-grams, brown clusters, POS tags, morphological features, and a set of features based on five sentiment lexicons. Most of the performance was due to the combination of these lexicons. This was the top system in the 2013 edition of SemEval. • TEAMX (Miura et al., 2014), a logistic regression classifier using a similar set of features. Additional features based on two different POS taggers and a word sense disambiguator were also included in the model. This approach attained the highest ranking in the 2014 edition. • CHARSCNN (dos Santos and Gatti, 2014b), a deep learning architecture with two convolutional layers that exploit character-level and word-level information. The features are extracted by converting a sentence into a sequence of word embeddings, and the individual words into sequences of character embeddings. Convolution filters followed by max pooling are applied to these sequences, to produce fixed size vectors. These vectors are then combined and transfered to a set of nonlinear activation functions, to generate more complex representations of the input. The predictions, based on these learned features are computed with a softm</context>
</contexts>
<marker>Santos, Gatti, 2014</marker>
<rawString>Cicero dos Santos and Maira Gatti. 2014a. Deep convolutional neural networks for sentiment analysis of short texts. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 69–78, Dublin, Ireland, August. Dublin City University and Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cıcero Nogueira dos Santos</author>
<author>Maıra Gatti</author>
</authors>
<title>Deep convolutional neural networks for sentiment analysis of short texts.</title>
<date>2014</date>
<booktitle>In Proceedings of the 25th International Conference on Computational Linguistics (COLING),</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="3081" citStr="Santos and Gatti, 2014" startWordPosition="473" endWordPosition="476">eatures. However, because these features are estimated by minimizing the prediction errors made on a generic, unsupervised, task they might be suboptimal for the intended purposes. Ideally, word features should be adapted to the specific supervised task. One of the reasons for the success of deep learning models for language problems, is the use unsupervised word embeddings to initialize the word projection layer. Then, during training, the errors made in the predictions are backpropagated to update the embeddings, so that they better predict the supervised signal (Collobert et al., 2011; dos Santos and Gatti, 2014a). However, this strategy faces an additional challenge in noisy domains, such as social media. The lexical variation caused by the typos, use of slang and abbreviations leads to a great number of singletons and out-of-vocabulary words. For these words, the embeddings will be poorly reestimated. Even worse, words not present on the training set will never get their embeddings updated. In this paper, we describe a strategy to adapt unsupervised word embeddings when dealing with small and noisy labeled datasets. The intuition behind our approach is the following. For a given task, only a subset</context>
<context position="24158" citStr="Santos and Gatti, 2014" startWordPosition="3957" endWordPosition="3960">arge set of hand-crafted features, including word and character n-grams, brown clusters, POS tags, morphological features, and a set of features based on five sentiment lexicons. Most of the performance was due to the combination of these lexicons. This was the top system in the 2013 edition of SemEval. • TEAMX (Miura et al., 2014), a logistic regression classifier using a similar set of features. Additional features based on two different POS taggers and a word sense disambiguator were also included in the model. This approach attained the highest ranking in the 2014 edition. • CHARSCNN (dos Santos and Gatti, 2014b), a deep learning architecture with two convolutional layers that exploit character-level and word-level information. The features are extracted by converting a sentence into a sequence of word embeddings, and the individual words into sequences of character embeddings. Convolution filters followed by max pooling are applied to these sequences, to produce fixed size vectors. These vectors are then combined and transfered to a set of nonlinear activation functions, to generate more complex representations of the input. The predictions, based on these learned features are computed with a softm</context>
</contexts>
<marker>Santos, Gatti, 2014</marker>
<rawString>Cıcero Nogueira dos Santos and Maıra Gatti. 2014b. Deep convolutional neural networks for sentiment analysis of short texts. In Proceedings of the 25th International Conference on Computational Linguistics (COLING), Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Scott Deerwester</author>
<author>Richard Harshman</author>
</authors>
<title>Using latent semantic analysis to improve access to textual information.</title>
<date>1988</date>
<booktitle>In Proceedings of the SIGCHI conference on Human factors in computing systems,</booktitle>
<pages>281--285</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6628" citStr="Dumais et al., 1988" startWordPosition="1041" endWordPosition="1044">bspace method to a Part-of-Speech tagging model for Twitter data. Finally, Section 7 draws the conclusions. 2 Related Work NLP systems can benefit from a very large pool of unlabeled data. While raw documents are usually not annotated, they contain structure, which can be leveraged to learn word features. Context is one strong indicator for word similarity, as related words tend to occur in similar contexts (Firth, 1968). Approaches that are based on this concept include, Latent Semantic Analysis, where words are represented as rows in the lowrank approximation of a term co-occurrence matrix (Dumais et al., 1988), word clusters obtained with hierarchical clustering algorithms based on Hidden Markov Models (Brown et al., 1992), and continuous word vectors learned with neural language models (Bengio et al., 2003). The resulting clusters and vectors, can then be used as more generalizable features in supervised tasks, as they also provide representations for words not present in the labeled data (Bespalov et al., 2011; Owoputi et al., 2013; Chen and Manning, 2014). A great amount of work has been done on the problem of learning better word representations from unsupervised data. However, not many studies</context>
</contexts>
<marker>Dumais, Furnas, Landauer, Deerwester, Harshman, 1988</marker>
<rawString>Susan T Dumais, George W Furnas, Thomas K Landauer, Scott Deerwester, and Richard Harshman. 1988. Using latent semantic analysis to improve access to textual information. In Proceedings of the SIGCHI conference on Human factors in computing systems, pages 281–285. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Rupert Firth</author>
</authors>
<date>1968</date>
<journal>Selected papers of JR Firth,</journal>
<pages>1952--59</pages>
<publisher>Indiana University Press.</publisher>
<contexts>
<context position="6432" citStr="Firth, 1968" startWordPosition="1011" endWordPosition="1012">-linear sub-space model for text classification. Section 5, details the experiments performed with the SemEval corpora. Section 6 describes additional experiments applying the embedding subspace method to a Part-of-Speech tagging model for Twitter data. Finally, Section 7 draws the conclusions. 2 Related Work NLP systems can benefit from a very large pool of unlabeled data. While raw documents are usually not annotated, they contain structure, which can be leveraged to learn word features. Context is one strong indicator for word similarity, as related words tend to occur in similar contexts (Firth, 1968). Approaches that are based on this concept include, Latent Semantic Analysis, where words are represented as rows in the lowrank approximation of a term co-occurrence matrix (Dumais et al., 1988), word clusters obtained with hierarchical clustering algorithms based on Hidden Markov Models (Brown et al., 1992), and continuous word vectors learned with neural language models (Bengio et al., 2003). The resulting clusters and vectors, can then be used as more generalizable features in supervised tasks, as they also provide representations for words not present in the labeled data (Bespalov et al.</context>
</contexts>
<marker>Firth, 1968</marker>
<rawString>John Rupert Firth. 1968. Selected papers of JR Firth, 1952-59. Indiana University Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for twitter: annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech tagging for twitter: annotation, features, and experiments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Glorot</author>
<author>Yoshua Bengio</author>
</authors>
<title>Understanding the difficulty of training deep feedforward neural networks.</title>
<date>2010</date>
<booktitle>In International conference on artificial intelligence and statistics,</booktitle>
<pages>249--256</pages>
<contexts>
<context position="20282" citStr="Glorot and Bengio, 2010" startWordPosition="3333" endWordPosition="3336">ively, and embeddings of 50, 200, 400 and 600 dimensions were built. Our system accepts as input a sentence represented as a matrix, obtained by concatenating the one-hot vectors that represent each individual word. Therefore, we first performed the aforementioned normalization and tokenization steps and then, converted each tweet into this representation. The development set was split into 80% for parameter learning and 20% for model evaluation and selection, maintaining the original relative class proportions in each set. All the weights were initialized uniformly at random, as proposed in (Glorot and Bengio, 2010). The model was trained with conventional Stochastic Gradient Descent (Rumelhart et al., 1985) with a fixed learning rate of 0.01, and the weights were updated after each message was processed. Variations of learning rate to smaller values, e.g. 0.005, were considered but did not lead to a clear pattern. We explored different configurations of the hyperparameters a (embedding size) and s (sub-space size). Model selection was done by early stopping, i.e., we kept the configuration with best F-measure on the evaluation set after 5-8 iterations. 5.2 Results In general, the NLSE model showed consi</context>
</contexts>
<marker>Glorot, Bengio, 2010</marker>
<rawString>Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In International conference on artificial intelligence and statistics, pages 249–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alec Go</author>
<author>Richa Bhayani</author>
<author>Lei Huang</author>
</authors>
<title>Twitter sentiment classification using distant supervision. CS224N Project Report,</title>
<date>2009</date>
<pages>1--12</pages>
<location>Stanford,</location>
<contexts>
<context position="8694" citStr="Go et al. (2009)" startWordPosition="1373" endWordPosition="1376">er than the purely unsupervised features, both were significantly outperformed by a simple bag-of-words baseline. Most other approaches, simply rely on additional training data to fine tune the embeddings for a given supervised task. In Bansal et al. (2014), better word embeddings for dependency parsing were obtained by using a corpus created to capture dependency context. This technique requires, nevertheless, of a pre-existing dependency parser or, at least a parsed corpus. For some other tasks, it is possible to collect weakly labeled corpora by making strong assumptions about the data. In Go et al. (2009) a corpus for Twitter sentiment analysis was built by assuming that tweets with positive emoticons imply positive sentiment, whereas tweets with negative emoticons imply negative sentiment. Using a similar corpus, Tang et al. (2014b) induced sentiment specific word embeddings, for the Twitter domain. The embeddings 1075 were estimated with a neural network that minimized a linear combination of two loss functions, one penalized the errors made at predicting the center word within a sequence of words, while the other penalized mistakes made at deciding the sentiment label. Weakly labeled data h</context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, pages 1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Omer Levy</author>
</authors>
<title>word2vec explained: deriving mikolov et al.’s negativesampling word-embedding method. arXiv preprint arXiv:1402.3722.</title>
<date>2014</date>
<contexts>
<context position="11726" citStr="Goldberg and Levy, 2014" startWordPosition="1889" endWordPosition="1892">t real valued space of size e, Cpj E Rv×e is a set of output matrices, one for each relative word position p, projecting the real-valued representation to a vector with the size of the vocabulary v. By learning a different matrix Cp for each relative word position, the model captures word order information, unlike the original skip-gram approach that uses only one output matrix. Finally, a distribution over all possible words is attained by exponentiating and normalizing over the v possible options. In practice, negative sampling is used to avoid having to normalize over the whole vocabulary (Goldberg and Levy, 2014). As most other neural network models, the structured skip-gram is trained with gradient-based methods. After a model has been trained, the low dimensional embedding E · wi E Re×1 encapsulates the information about each word wi and its surrounding contexts. This embbeding can thus be used as input to other learning algorithms to further enhance performance. 4 Adapting Embeddings with Sub-space Projections As detailed in the introduction and related work, word embeddings are a useful unsupervised technique to attain initial model values or features prior to supervised training. These models can</context>
</contexts>
<marker>Goldberg, Levy, 2014</marker>
<rawString>Yoav Goldberg and Omer Levy. 2014. word2vec explained: deriving mikolov et al.’s negativesampling word-embedding method. arXiv preprint arXiv:1402.3722.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12,</booktitle>
<pages>873--882</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1969" citStr="Huang et al., 2012" startWordPosition="300" endWordPosition="303"> and quality of the available training data, oftentimes, even more than the particular choice of learning algorithm (Banko and Brill, 2001). Labeled data is, however, expensive to obtain, while unlabeled data is widely available. In order to exploit this fact, semi-supervised learning methods can be used. In particular, it is possible to derive word representations by exploiting word co-occurrence patterns in large samples of unlabeled text. Based on this idea, several methods have been recently proposed to efficiently estimate word embeddings from raw text, leveraging neural language models (Huang et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Ling et al., 2015). These models work by maximizing the probability that words within a given window size are predicted correctly. The resulting embeddings are low-dimensional dense vectors that encode syntactic and semantic properties of words. Using these word representations, Turian et al. (2010) were able to improve near state-of-the-art systems for several tasks, by simply plugging in the learned word representations as additional features. However, because these features are estimated by minimizing the prediction errors made on a generic, </context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H. Huang, Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12, pages 873– 882, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
<author>Saif M Mohammad</author>
</authors>
<title>Sentiment analysis of short informal texts.</title>
<date>2014</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>723--762</pages>
<contexts>
<context position="18371" citStr="Kiritchenko et al., 2014" startWordPosition="3021" endWordPosition="3024">ed to sentiment polarity prediction. Positive Neutral Negative Development 3230 4109 1265 Tweets 2015 1032 983 364 Tweets 2014 982 669 202 Tweets 2013 1572 1640 601 Table 1: Number of examples per class in each SemEval dataset. The first row shows the training data; the other rows are sets used for testing. need to be enriched with additional hand-crafted features that try to capture more discriminative aspects of the content, most of which require external tools (e.g., part-of-speech taggers and parsers) or linguistic resources (e.g., dictionaries and sentiment lexicons) (Miura et al., 2014; Kiritchenko et al., 2014). With the embedding sub-space approach, however, we are able to attain state-ofthe-art performance while requiring only minimal processing of the data and few hyperparameters. To make our results comparable to other systems for this task, we adopted the guidelines from the benchmark. Our system was trained and tuned using only the development data. The evaluation was performed on the test sets, shown in Table 1, and we report the results in terms of the average F-measure for the positive and negative classes. 5.1 Experimental Setup The first step of our approach requires a corpus of raw text </context>
<context position="23490" citStr="Kiritchenko et al., 2014" startWordPosition="3845" endWordPosition="3848">rmore, we observe that updating the embeddings always leads to inferior results. This suggests that precomputed embeddings should be kept fixed, when little labeled data is available to re-train them. Comparison with the state-of-the-art We now compare the NLSE model with state-ofthe-art systems, including the best submissions to previous SemEval benchmarks. We also include two other approaches that are related to the one here proposed, where a neural network initialized with pre-trained word embeddings is used to learn relevant features. Specifically, we compare the following systems: • NRC (Kiritchenko et al., 2014), a support vector machine classifier with a large set of hand-crafted features, including word and character n-grams, brown clusters, POS tags, morphological features, and a set of features based on five sentiment lexicons. Most of the performance was due to the combination of these lexicons. This was the top system in the 2013 edition of SemEval. • TEAMX (Miura et al., 2014), a logistic regression classifier using a similar set of features. Additional features based on two different POS taggers and a word sense disambiguator were also included in the model. This approach attained the highest</context>
<context position="25444" citStr="Kiritchenko et al. (2014)" startWordPosition="4155" endWordPosition="4158">tor machine classifier that leverages the sentiment specific word embeddings, discussed in Section 2. The embeddings are also processed with a convolution filter, but the output of this operation is used to produce three representations obtained with different strategies, namely with max, min and 1079 (a) Comparison of two baselines with two variations (b) Performance of state-of-the-art systems for Twitter sentiof the NLSE model ment prediction Figure 3: Average F-measure on the SemEval test sets average pooling. The final feature vector is obtained by concatenating these representations and Kiritchenko et al. (2014) feature set. • UNITN (Severyn and Moschitti, 2015), another deep convolutional neural network that jointly learns internal representations and a softmax classifier. The network is trained in three steps: (i) unsupervised pre-training of embeddings, (ii) refinement of the embeddings using a weakly labeled corpus, and (iii) fine tuning the model with the labeled data from SemEval. It should be noted that the system was trained with a labeled corpus 65% larger than ours2. This system made the best submission on the 2015 edition of the benchmark. The results in Figure 3b, show that despite being </context>
</contexts>
<marker>Kiritchenko, Zhu, Mohammad, 2014</marker>
<rawString>Svetlana Kiritchenko, Xiaodan Zhu, and Saif M Mohammad. 2014. Sentiment analysis of short informal texts. Journal of Artificial Intelligence Research, pages 723–762.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Labutov</author>
<author>Hod Lipson</author>
</authors>
<title>Re-embedding words.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st annual meeting of the ACL,</booktitle>
<pages>489--493</pages>
<contexts>
<context position="7633" citStr="Labutov and Lipson (2013)" startWordPosition="1204" endWordPosition="1207">ed data (Bespalov et al., 2011; Owoputi et al., 2013; Chen and Manning, 2014). A great amount of work has been done on the problem of learning better word representations from unsupervised data. However, not many studies have discussed the best ways to use them in supervised tasks. Typically, in these cases, word representations are directly used as features or to initialize the parameters of more complex models. In some tasks, this approach is however prone to overfitting. The work presented here aims to provide a simple approach to overcome this last scenario. It is thus directly related to Labutov and Lipson (2013), where a method to learn taskspecific representations from general pre-trained embeddings was presented. In this work, new features were estimated with a convex objective function that combined the log-likelihood of the training data, with regularization penalizing the Frobenius norm of the distortion matrix. That is, the matrix of the differences between the original and the new embeddings. Even though the adapted embeddings performed better than the purely unsupervised features, both were significantly outperformed by a simple bag-of-words baseline. Most other approaches, simply rely on add</context>
</contexts>
<marker>Labutov, Lipson, 2013</marker>
<rawString>Igor Labutov and Hod Lipson. 2013. Re-embedding words. In Proceedings of the 51st annual meeting of the ACL, pages 489–493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wang Ling</author>
<author>Chris Dyer</author>
<author>Alan Black</author>
<author>Isabel Trancoso</author>
</authors>
<title>Two/too simple adaptations of word2vec for syntax problems.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association</booktitle>
<contexts>
<context position="2036" citStr="Ling et al., 2015" startWordPosition="312" endWordPosition="315">than the particular choice of learning algorithm (Banko and Brill, 2001). Labeled data is, however, expensive to obtain, while unlabeled data is widely available. In order to exploit this fact, semi-supervised learning methods can be used. In particular, it is possible to derive word representations by exploiting word co-occurrence patterns in large samples of unlabeled text. Based on this idea, several methods have been recently proposed to efficiently estimate word embeddings from raw text, leveraging neural language models (Huang et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Ling et al., 2015). These models work by maximizing the probability that words within a given window size are predicted correctly. The resulting embeddings are low-dimensional dense vectors that encode syntactic and semantic properties of words. Using these word representations, Turian et al. (2010) were able to improve near state-of-the-art systems for several tasks, by simply plugging in the learned word representations as additional features. However, because these features are estimated by minimizing the prediction errors made on a generic, unsupervised, task they might be suboptimal for the intended purpos</context>
<context position="5347" citStr="Ling et al. (2015)" startWordPosition="832" endWordPosition="835">assifier for the target task. Using this model, we built a system to participate in the SemEval 2015 Twitter sentiment analysis benchmark (Rosenthal et al., 2015). Our submission attained state-of-the-art results without hand-coded features or linguistic resources (Astudillo et al., 2015). Here, we further investigate this approach and compare it against several state-of-the-art systems for Twitter sentiment classification. We also report on additional experiments to assess the adequacy of this strategy in other natural language problems. To this end, we apply the embedding sub-space layer to Ling et al. (2015) deep learning model for part-of-speech tagging. Even though the gains were not as significant as in the sentiment polarity prediction task, the results suggest that our method is indeed generalizable to other problems. The rest of the paper is organized as follows: the related work is reviewed in Section 2. Section 3, briefly describes the model used to pre-train the word embeddings. In Section 4, we introduce the concept of embedding sub-space, as well as the the non-linear sub-space model for text classification. Section 5, details the experiments performed with the SemEval corpora. Section</context>
<context position="10004" citStr="Ling et al., 2015" startWordPosition="1578" endWordPosition="1581"> labels before using the actual task-specific supervised data (Severyn and Moschitti, 2015). 3 Unsupervised Structured Skip-Gram Word Embeddings Word embeddings are generally trained by optimizing an objective function that can be measured without annotations. One popular approach is to estimate the embeddings by maximizing the probability that the words within a given window size are predicted correctly. Our previous work has compared several such models, namely the skipgram and CBOW architectures (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and the structured skip-gram approach (Ling et al., 2015), suggesting that they all have comparable capabilities. Thus, in this study we only use embeddings derived with the structured skip-gram approach, a modification of the skip-gram architecture that has been shown to outperform the original model in syntax based tasks such as, part-of-speech tagging and dependency parsing. Central to the structured skip-gram is a log linear model of word prediction. Let w = i denote that a word at a given position of a sentence is the i-th word on a vocabulary of size v, and let wp = j denote that the word p positions further in the sentence is the j-th word on</context>
<context position="26543" citStr="Ling et al., 2015" startWordPosition="4332" endWordPosition="4335">ystem made the best submission on the 2015 edition of the benchmark. The results in Figure 3b, show that despite being simpler and requiring less resources and labeled data, the NLSE model is extremely competitive, even outperforming most other systems, in predicting the sentiment polarity of Twitter messages. 6 Generalization to Other Tasks While the embedding sub-space method works well for the sentiment prediction task, we would like to know its impact in other settings that are known to benefit from unsupervised embeddings. Thus, we decided to replicate the part-of-speech tagging work in (Ling et al., 2015), where pretraining embeddings have been shown to improve 2The UNITN system was trained with around 11,400 labeled examples, whereas we used only 6,900. the quality of the results significantly. 6.1 Sub-space Window Model Part-of-speech tagging is a word labeling task, where each word is to be labeled with its syntactic function in the sentence. More formally, given an input sentence w1, ... , wn of n words, we wish to predict a sequence of labels y1, ... , yn, which are the POS tags of each of the words. This task is scored by the ratio between the number of correct labels and the number of w</context>
<context position="28755" citStr="Ling et al., 2015" startWordPosition="4759" endWordPosition="4762">taset, which uses the universal POS tag set composed by 25 different labels (Petrov et al., 2012). The dataset contains 1000 annotated tweets for training, 327 tweets for tuning and 500 tweets for testing. The number of word tokens in these sets are 15000, 5000 and 7000, respectively. There are 5000, 2000 and 3000 word types. Once again, we initialized the embeddings with unsupervised pre-training using the structured skip-gram approach. As for the hyperparameters of the model, we used embeddings with e = 50 dimensions, a hidden layer with h = 200 dimensions and a context of p = 2 as used in (Ling et al., 2015). Training employed mini-batch gradient descent, with mini batches of 100 sentences and a momentum of 0.95. The learning rate was set to 0.2. Finally, we used early stopping by choosing the epoch with the highest accuracy in the tuning set. As for the sub-space layer size, we tried three different hyperparameterizations: 10, 30 and 50 dimensions. 6.3 Results Figure 5 displays the results. Using the setup that led to the best results in the sentiment prediction task (FIX), that is, fixing E and updating S, leads to lower accuracies than the baseline (TRAIN-ALL, s = 0). We also see that differen</context>
</contexts>
<marker>Ling, Dyer, Black, Trancoso, 2015</marker>
<rawString>Wang Ling, Chris Dyer, Alan Black, and Isabel Trancoso. 2015. Two/too simple adaptations of word2vec for syntax problems. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In 27th Annual Conference on Neural Information Processing Systems.</booktitle>
<contexts>
<context position="1991" citStr="Mikolov et al., 2013" startWordPosition="304" endWordPosition="307">available training data, oftentimes, even more than the particular choice of learning algorithm (Banko and Brill, 2001). Labeled data is, however, expensive to obtain, while unlabeled data is widely available. In order to exploit this fact, semi-supervised learning methods can be used. In particular, it is possible to derive word representations by exploiting word co-occurrence patterns in large samples of unlabeled text. Based on this idea, several methods have been recently proposed to efficiently estimate word embeddings from raw text, leveraging neural language models (Huang et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Ling et al., 2015). These models work by maximizing the probability that words within a given window size are predicted correctly. The resulting embeddings are low-dimensional dense vectors that encode syntactic and semantic properties of words. Using these word representations, Turian et al. (2010) were able to improve near state-of-the-art systems for several tasks, by simply plugging in the learned word representations as additional features. However, because these features are estimated by minimizing the prediction errors made on a generic, unsupervised, task the</context>
<context position="9912" citStr="Mikolov et al., 2013" startWordPosition="1564" endWordPosition="1567">a has also been used to refine unsupervised embeddings, by retraining them to predict the noisy labels before using the actual task-specific supervised data (Severyn and Moschitti, 2015). 3 Unsupervised Structured Skip-Gram Word Embeddings Word embeddings are generally trained by optimizing an objective function that can be measured without annotations. One popular approach is to estimate the embeddings by maximizing the probability that the words within a given window size are predicted correctly. Our previous work has compared several such models, namely the skipgram and CBOW architectures (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and the structured skip-gram approach (Ling et al., 2015), suggesting that they all have comparable capabilities. Thus, in this study we only use embeddings derived with the structured skip-gram approach, a modification of the skip-gram architecture that has been shown to outperform the original model in syntax based tasks such as, part-of-speech tagging and dependency parsing. Central to the structured skip-gram is a log linear model of word prediction. Let w = i denote that a word at a given position of a sentence is the i-th word on a vocabulary of size v,</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In 27th Annual Conference on Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yasuhide Miura</author>
<author>Shigeyuki Sakaki</author>
<author>Keigo Hattori</author>
<author>Tomoko Ohkuma</author>
</authors>
<title>Teamx: A sentiment analyzer with enhanced lexicon mapping and weighting scheme for unbalanced data.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>628--632</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Dublin, Ireland,</location>
<contexts>
<context position="18344" citStr="Miura et al., 2014" startWordPosition="3017" endWordPosition="3020">he NLSE model, applied to sentiment polarity prediction. Positive Neutral Negative Development 3230 4109 1265 Tweets 2015 1032 983 364 Tweets 2014 982 669 202 Tweets 2013 1572 1640 601 Table 1: Number of examples per class in each SemEval dataset. The first row shows the training data; the other rows are sets used for testing. need to be enriched with additional hand-crafted features that try to capture more discriminative aspects of the content, most of which require external tools (e.g., part-of-speech taggers and parsers) or linguistic resources (e.g., dictionaries and sentiment lexicons) (Miura et al., 2014; Kiritchenko et al., 2014). With the embedding sub-space approach, however, we are able to attain state-ofthe-art performance while requiring only minimal processing of the data and few hyperparameters. To make our results comparable to other systems for this task, we adopted the guidelines from the benchmark. Our system was trained and tuned using only the development data. The evaluation was performed on the test sets, shown in Table 1, and we report the results in terms of the average F-measure for the positive and negative classes. 5.1 Experimental Setup The first step of our approach req</context>
<context position="23869" citStr="Miura et al., 2014" startWordPosition="3907" endWordPosition="3910">r approaches that are related to the one here proposed, where a neural network initialized with pre-trained word embeddings is used to learn relevant features. Specifically, we compare the following systems: • NRC (Kiritchenko et al., 2014), a support vector machine classifier with a large set of hand-crafted features, including word and character n-grams, brown clusters, POS tags, morphological features, and a set of features based on five sentiment lexicons. Most of the performance was due to the combination of these lexicons. This was the top system in the 2013 edition of SemEval. • TEAMX (Miura et al., 2014), a logistic regression classifier using a similar set of features. Additional features based on two different POS taggers and a word sense disambiguator were also included in the model. This approach attained the highest ranking in the 2014 edition. • CHARSCNN (dos Santos and Gatti, 2014b), a deep learning architecture with two convolutional layers that exploit character-level and word-level information. The features are extracted by converting a sentence into a sequence of word embeddings, and the individual words into sequences of character embeddings. Convolution filters followed by max po</context>
</contexts>
<marker>Miura, Sakaki, Hattori, Ohkuma, 2014</marker>
<rawString>Yasuhide Miura, Shigeyuki Sakaki, Keigo Hattori, and Tomoko Ohkuma. 2014. Teamx: A sentiment analyzer with enhanced lexicon mapping and weighting scheme for unbalanced data. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 628–632, Dublin, Ireland, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Zornitsa Kozareva</author>
<author>Alan Ritter</author>
<author>Sara Rosenthal</author>
<author>Veselin Stoyanov</author>
<author>Theresa Wilson</author>
</authors>
<date>2013</date>
<note>Semeval-2013 task 2: Sentiment analysis in twitter.</note>
<contexts>
<context position="17170" citStr="Nakov et al., 2013" startWordPosition="2828" endWordPosition="2831">ponents, the embeddings attained in unsupervised form, E, and the projection matrix S. Second, the size of the sub-space, in which the embeddings are projected, is much smaller than that of the original embeddings with typical reductions above one order of magnitude. As usual in this kind of models, all the parameters can be trained with gradient methods, using the backpropagation update rule. 5 NLSE for Twitter Sentiment Analysis In this section, we apply the NLSE model to the message polarity classification task proposed by SemEval, for their well-known Twitter sentiment analysis challenge (Nakov et al., 2013). Given a message, the goal is to decide whether it expresses a positive, negative, or neutral sentiment. Most of the top performing systems that participated in this challenge, relied on linear classification models and the bag-of-words assumption, representing messages as sparse vectors of the size of the vocabulary. In the case of social media, this approach is particularly inefficient, due to the large vocabularies necessary to account for all the lexical variation found in this domain. Thus, these models 1077 Negative Figure 1: Illustration of the NLSE model, applied to sentiment polarity</context>
</contexts>
<marker>Nakov, Kozareva, Ritter, Rosenthal, Stoyanov, Wilson, 2013</marker>
<rawString>Preslav Nakov, Zornitsa Kozareva, Alan Ritter, Sara Rosenthal, Veselin Stoyanov, and Theresa Wilson. 2013. Semeval-2013 task 2: Sentiment analysis in twitter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters. In</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="7060" citStr="Owoputi et al., 2013" startWordPosition="1109" endWordPosition="1112">ches that are based on this concept include, Latent Semantic Analysis, where words are represented as rows in the lowrank approximation of a term co-occurrence matrix (Dumais et al., 1988), word clusters obtained with hierarchical clustering algorithms based on Hidden Markov Models (Brown et al., 1992), and continuous word vectors learned with neural language models (Bengio et al., 2003). The resulting clusters and vectors, can then be used as more generalizable features in supervised tasks, as they also provide representations for words not present in the labeled data (Bespalov et al., 2011; Owoputi et al., 2013; Chen and Manning, 2014). A great amount of work has been done on the problem of learning better word representations from unsupervised data. However, not many studies have discussed the best ways to use them in supervised tasks. Typically, in these cases, word representations are directly used as features or to initialize the parameters of more complex models. In some tasks, this approach is however prone to overfitting. The work presented here aims to provide a simple approach to overcome this last scenario. It is thus directly related to Labutov and Lipson (2013), where a method to learn t</context>
<context position="19109" citStr="Owoputi et al., 2013" startWordPosition="3144" endWordPosition="3147">nly minimal processing of the data and few hyperparameters. To make our results comparable to other systems for this task, we adopted the guidelines from the benchmark. Our system was trained and tuned using only the development data. The evaluation was performed on the test sets, shown in Table 1, and we report the results in terms of the average F-measure for the positive and negative classes. 5.1 Experimental Setup The first step of our approach requires a corpus of raw text for the unsupervised pre-training of the embedding matrix E. We resorted to the corpus of 52 million tweets used in (Owoputi et al., 2013) and the tokenizer described in the same work. The messages were previously pre-processed as follows: lower-casing, replacing Twitter user mentions and URLs with special tokens and reducing any character repetition to at most 3 characters. Words occurring less than 40 times in the corpus were discarded, resulting in a vocabulary of around 210,000 types. Then, a modified version of the word2vec tool1 was used to compute the word embeddings, as described in Section 3. The window size and negative sampling rate were set to 5 and 25 words, respectively, and embeddings of 50, 200, 400 and 600 dimen</context>
</contexts>
<marker>Owoputi, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),</booktitle>
<pages>12</pages>
<contexts>
<context position="2016" citStr="Pennington et al., 2014" startWordPosition="308" endWordPosition="311">a, oftentimes, even more than the particular choice of learning algorithm (Banko and Brill, 2001). Labeled data is, however, expensive to obtain, while unlabeled data is widely available. In order to exploit this fact, semi-supervised learning methods can be used. In particular, it is possible to derive word representations by exploiting word co-occurrence patterns in large samples of unlabeled text. Based on this idea, several methods have been recently proposed to efficiently estimate word embeddings from raw text, leveraging neural language models (Huang et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Ling et al., 2015). These models work by maximizing the probability that words within a given window size are predicted correctly. The resulting embeddings are low-dimensional dense vectors that encode syntactic and semantic properties of words. Using these word representations, Turian et al. (2010) were able to improve near state-of-the-art systems for several tasks, by simply plugging in the learned word representations as additional features. However, because these features are estimated by minimizing the prediction errors made on a generic, unsupervised, task they might be suboptimal for</context>
<context position="9945" citStr="Pennington et al., 2014" startWordPosition="1569" endWordPosition="1572"> unsupervised embeddings, by retraining them to predict the noisy labels before using the actual task-specific supervised data (Severyn and Moschitti, 2015). 3 Unsupervised Structured Skip-Gram Word Embeddings Word embeddings are generally trained by optimizing an objective function that can be measured without annotations. One popular approach is to estimate the embeddings by maximizing the probability that the words within a given window size are predicted correctly. Our previous work has compared several such models, namely the skipgram and CBOW architectures (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and the structured skip-gram approach (Ling et al., 2015), suggesting that they all have comparable capabilities. Thus, in this study we only use embeddings derived with the structured skip-gram approach, a modification of the skip-gram architecture that has been shown to outperform the original model in syntax based tasks such as, part-of-speech tagging and dependency parsing. Central to the structured skip-gram is a log linear model of word prediction. Let w = i denote that a word at a given position of a sentence is the i-th word on a vocabulary of size v, and let wp = j denote that the w</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan McDonald</author>
</authors>
<title>A universal part-of-speech tagset.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12). European Language Resources Association (ELRA),</booktitle>
<contexts>
<context position="28234" citStr="Petrov et al., 2012" startWordPosition="4667" endWordPosition="4670">•E•wt � � � S • E • wt−p Here tanh denotes the hyperbolic tangent, acting element-wise. Aside from embedding E and 1080 sub-space S matrices, the model is parametrized by the weights H ∈ Rh×pa and Y ∈ Rv×h as well as a bias b ∈ Rv×1. Note that if S is set to the identity matrix, this would be equivalent to the original Collobert et al. (2011) model. Figure 4: Illustration of the window model by (Collobert et al., 2011) using a sub-space layer. 6.2 Experiments Tests were performed in Gimpel et al. (2011) Twitter POS dataset, which uses the universal POS tag set composed by 25 different labels (Petrov et al., 2012). The dataset contains 1000 annotated tweets for training, 327 tweets for tuning and 500 tweets for testing. The number of word tokens in these sets are 15000, 5000 and 7000, respectively. There are 5000, 2000 and 3000 word types. Once again, we initialized the embeddings with unsupervised pre-training using the structured skip-gram approach. As for the hyperparameters of the model, we used embeddings with e = 50 dimensions, a hidden layer with h = 200 dimensions and a context of p = 2 as used in (Ling et al., 2015). Training employed mini-batch gradient descent, with mini batches of 100 sente</context>
</contexts>
<marker>Petrov, Das, McDonald, 2012</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12). European Language Resources Association (ELRA), may.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Preslav Nakov</author>
<author>Svetlana Kiritchenko</author>
<author>Saif M Mohammad</author>
<author>Alan Ritter</author>
<author>Veselin Stoyanov</author>
</authors>
<title>Semeval-2015 task 10: Sentiment analysis in twitter.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’2015,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Denver, Colorado,</location>
<contexts>
<context position="4891" citStr="Rosenthal et al., 2015" startWordPosition="765" endWordPosition="768">ain low dimensional embeddings fitting the complexity of the target task. On the other hand, we are able to learn new representations for all the words, even if they do not occur in the labeled dataset. To estimate the low dimensional sub-space, we propose a simple non-linear model equivalent to a neural network with one single hidden layer. The model is trained in supervised fashion on the labeled dataset, learning jointly the sub-space projection and a classifier for the target task. Using this model, we built a system to participate in the SemEval 2015 Twitter sentiment analysis benchmark (Rosenthal et al., 2015). Our submission attained state-of-the-art results without hand-coded features or linguistic resources (Astudillo et al., 2015). Here, we further investigate this approach and compare it against several state-of-the-art systems for Twitter sentiment classification. We also report on additional experiments to assess the adequacy of this strategy in other natural language problems. To this end, we apply the embedding sub-space layer to Ling et al. (2015) deep learning model for part-of-speech tagging. Even though the gains were not as significant as in the sentiment polarity prediction task, the</context>
</contexts>
<marker>Rosenthal, Nakov, Kiritchenko, Mohammad, Ritter, Stoyanov, 2015</marker>
<rawString>Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko, Saif M Mohammad, Alan Ritter, and Veselin Stoyanov. 2015. Semeval-2015 task 10: Sentiment analysis in twitter. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’2015, Denver, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David E Rumelhart</author>
<author>Geoffrey E Hinton</author>
<author>Ronald J Williams</author>
</authors>
<title>Learning internal representations by error propagation.</title>
<date>1985</date>
<tech>Technical report, DTIC Document.</tech>
<contexts>
<context position="15416" citStr="Rumelhart et al., 1985" startWordPosition="2513" endWordPosition="2516">zations of the embeddings seems to be helpful for tasks that have a higher penalty for missing words, although it remains unclear if better initialization strategies exist. 4.2 Non-Linear Embedding Sub-space Model The concept of embedding sub-space can be applied to log-linear classifiers or any deep learning architecture that uses embeddings. We now describe an application of this method for short text classification tasks. In what follows, we will refer to this approach as Non-Linear Sub-space Embedding (NLSE) model. The NLSE can be interpreted as a simple feed-forward neural network model (Rumelhart et al., 1985) with one single hidden layer utilizing the embedding sub-space approach, as depicted in Fig. 1. Let m = [w1 ··· wn] (2) denote a message of n words. Each column w E {0,1}vx1 of m represents a word in onehot form, as described in Section 3. Let y denote a categorical random variable over K classes. The NLSE model, estimates thus the probability of each possible category y = k E K given a message m as p(y = k1m) a exp (Yk · h · 1) . (3) Here, h E {0,1}exn are the activations of the hidden layer for each word, given by h = Q (S · E · m) (4) where Q() is a sigmoid function acting on each element </context>
<context position="20376" citStr="Rumelhart et al., 1985" startWordPosition="3347" endWordPosition="3350">t a sentence represented as a matrix, obtained by concatenating the one-hot vectors that represent each individual word. Therefore, we first performed the aforementioned normalization and tokenization steps and then, converted each tweet into this representation. The development set was split into 80% for parameter learning and 20% for model evaluation and selection, maintaining the original relative class proportions in each set. All the weights were initialized uniformly at random, as proposed in (Glorot and Bengio, 2010). The model was trained with conventional Stochastic Gradient Descent (Rumelhart et al., 1985) with a fixed learning rate of 0.01, and the weights were updated after each message was processed. Variations of learning rate to smaller values, e.g. 0.005, were considered but did not lead to a clear pattern. We explored different configurations of the hyperparameters a (embedding size) and s (sub-space size). Model selection was done by early stopping, i.e., we kept the configuration with best F-measure on the evaluation set after 5-8 iterations. 5.2 Results In general, the NLSE model showed consistent and fast convergence towards the optimum in very few iterations. Despite using class log</context>
</contexts>
<marker>Rumelhart, Hinton, Williams, 1985</marker>
<rawString>David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1985. Learning internal representations by error propagation. Technical report, DTIC Document.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aliaksei Severyn</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Unitn: Training deep convolutional neural network for twitter sentiment classification.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’2015,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Denver, Colorado,</location>
<contexts>
<context position="9477" citStr="Severyn and Moschitti, 2015" startWordPosition="1497" endWordPosition="1501">moticons imply negative sentiment. Using a similar corpus, Tang et al. (2014b) induced sentiment specific word embeddings, for the Twitter domain. The embeddings 1075 were estimated with a neural network that minimized a linear combination of two loss functions, one penalized the errors made at predicting the center word within a sequence of words, while the other penalized mistakes made at deciding the sentiment label. Weakly labeled data has also been used to refine unsupervised embeddings, by retraining them to predict the noisy labels before using the actual task-specific supervised data (Severyn and Moschitti, 2015). 3 Unsupervised Structured Skip-Gram Word Embeddings Word embeddings are generally trained by optimizing an objective function that can be measured without annotations. One popular approach is to estimate the embeddings by maximizing the probability that the words within a given window size are predicted correctly. Our previous work has compared several such models, namely the skipgram and CBOW architectures (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and the structured skip-gram approach (Ling et al., 2015), suggesting that they all have comparable capabilities. Thus, in this st</context>
<context position="25495" citStr="Severyn and Moschitti, 2015" startWordPosition="4163" endWordPosition="4166">ent specific word embeddings, discussed in Section 2. The embeddings are also processed with a convolution filter, but the output of this operation is used to produce three representations obtained with different strategies, namely with max, min and 1079 (a) Comparison of two baselines with two variations (b) Performance of state-of-the-art systems for Twitter sentiof the NLSE model ment prediction Figure 3: Average F-measure on the SemEval test sets average pooling. The final feature vector is obtained by concatenating these representations and Kiritchenko et al. (2014) feature set. • UNITN (Severyn and Moschitti, 2015), another deep convolutional neural network that jointly learns internal representations and a softmax classifier. The network is trained in three steps: (i) unsupervised pre-training of embeddings, (ii) refinement of the embeddings using a weakly labeled corpus, and (iii) fine tuning the model with the labeled data from SemEval. It should be noted that the system was trained with a labeled corpus 65% larger than ours2. This system made the best submission on the 2015 edition of the benchmark. The results in Figure 3b, show that despite being simpler and requiring less resources and labeled da</context>
</contexts>
<marker>Severyn, Moschitti, 2015</marker>
<rawString>Aliaksei Severyn and Alessandro Moschitti. 2015. Unitn: Training deep convolutional neural network for twitter sentiment classification. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’2015, Denver, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duyu Tang</author>
<author>Furu Wei</author>
<author>Bing Qin</author>
<author>Ting Liu</author>
<author>Ming Zhou</author>
</authors>
<title>Coooolll: A deep learning system for twitter sentiment classification. SemEval</title>
<date>2014</date>
<pages>208</pages>
<contexts>
<context position="8925" citStr="Tang et al. (2014" startWordPosition="1409" endWordPosition="1412">sk. In Bansal et al. (2014), better word embeddings for dependency parsing were obtained by using a corpus created to capture dependency context. This technique requires, nevertheless, of a pre-existing dependency parser or, at least a parsed corpus. For some other tasks, it is possible to collect weakly labeled corpora by making strong assumptions about the data. In Go et al. (2009) a corpus for Twitter sentiment analysis was built by assuming that tweets with positive emoticons imply positive sentiment, whereas tweets with negative emoticons imply negative sentiment. Using a similar corpus, Tang et al. (2014b) induced sentiment specific word embeddings, for the Twitter domain. The embeddings 1075 were estimated with a neural network that minimized a linear combination of two loss functions, one penalized the errors made at predicting the center word within a sequence of words, while the other penalized mistakes made at deciding the sentiment label. Weakly labeled data has also been used to refine unsupervised embeddings, by retraining them to predict the noisy labels before using the actual task-specific supervised data (Severyn and Moschitti, 2015). 3 Unsupervised Structured Skip-Gram Word Embed</context>
<context position="24802" citStr="Tang et al., 2014" startWordPosition="4056" endWordPosition="4059">cture with two convolutional layers that exploit character-level and word-level information. The features are extracted by converting a sentence into a sequence of word embeddings, and the individual words into sequences of character embeddings. Convolution filters followed by max pooling are applied to these sequences, to produce fixed size vectors. These vectors are then combined and transfered to a set of nonlinear activation functions, to generate more complex representations of the input. The predictions, based on these learned features are computed with a softmax classifier. • COOOOLLL (Tang et al., 2014a), a support vector machine classifier that leverages the sentiment specific word embeddings, discussed in Section 2. The embeddings are also processed with a convolution filter, but the output of this operation is used to produce three representations obtained with different strategies, namely with max, min and 1079 (a) Comparison of two baselines with two variations (b) Performance of state-of-the-art systems for Twitter sentiof the NLSE model ment prediction Figure 3: Average F-measure on the SemEval test sets average pooling. The final feature vector is obtained by concatenating these rep</context>
</contexts>
<marker>Tang, Wei, Qin, Liu, Zhou, 2014</marker>
<rawString>Duyu Tang, Furu Wei, Bing Qin, Ting Liu, and Ming Zhou. 2014a. Coooolll: A deep learning system for twitter sentiment classification. SemEval 2014, page 208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duyu Tang</author>
<author>Furu Wei</author>
<author>Nan Yang</author>
<author>Ming Zhou</author>
<author>Ting Liu</author>
<author>Bing Qin</author>
</authors>
<title>Learning sentimentspecific word embedding for twitter sentiment classification.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1555--1565</pages>
<contexts>
<context position="8925" citStr="Tang et al. (2014" startWordPosition="1409" endWordPosition="1412">sk. In Bansal et al. (2014), better word embeddings for dependency parsing were obtained by using a corpus created to capture dependency context. This technique requires, nevertheless, of a pre-existing dependency parser or, at least a parsed corpus. For some other tasks, it is possible to collect weakly labeled corpora by making strong assumptions about the data. In Go et al. (2009) a corpus for Twitter sentiment analysis was built by assuming that tweets with positive emoticons imply positive sentiment, whereas tweets with negative emoticons imply negative sentiment. Using a similar corpus, Tang et al. (2014b) induced sentiment specific word embeddings, for the Twitter domain. The embeddings 1075 were estimated with a neural network that minimized a linear combination of two loss functions, one penalized the errors made at predicting the center word within a sequence of words, while the other penalized mistakes made at deciding the sentiment label. Weakly labeled data has also been used to refine unsupervised embeddings, by retraining them to predict the noisy labels before using the actual task-specific supervised data (Severyn and Moschitti, 2015). 3 Unsupervised Structured Skip-Gram Word Embed</context>
<context position="24802" citStr="Tang et al., 2014" startWordPosition="4056" endWordPosition="4059">cture with two convolutional layers that exploit character-level and word-level information. The features are extracted by converting a sentence into a sequence of word embeddings, and the individual words into sequences of character embeddings. Convolution filters followed by max pooling are applied to these sequences, to produce fixed size vectors. These vectors are then combined and transfered to a set of nonlinear activation functions, to generate more complex representations of the input. The predictions, based on these learned features are computed with a softmax classifier. • COOOOLLL (Tang et al., 2014a), a support vector machine classifier that leverages the sentiment specific word embeddings, discussed in Section 2. The embeddings are also processed with a convolution filter, but the output of this operation is used to produce three representations obtained with different strategies, namely with max, min and 1079 (a) Comparison of two baselines with two variations (b) Performance of state-of-the-art systems for Twitter sentiof the NLSE model ment prediction Figure 3: Average F-measure on the SemEval test sets average pooling. The final feature vector is obtained by concatenating these rep</context>
</contexts>
<marker>Tang, Wei, Yang, Zhou, Liu, Qin, 2014</marker>
<rawString>Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu, and Bing Qin. 2014b. Learning sentimentspecific word embedding for twitter sentiment classification. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1555–1565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th annual meeting of the association for computational linguistics,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2318" citStr="Turian et al. (2010)" startWordPosition="354" endWordPosition="357">word representations by exploiting word co-occurrence patterns in large samples of unlabeled text. Based on this idea, several methods have been recently proposed to efficiently estimate word embeddings from raw text, leveraging neural language models (Huang et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Ling et al., 2015). These models work by maximizing the probability that words within a given window size are predicted correctly. The resulting embeddings are low-dimensional dense vectors that encode syntactic and semantic properties of words. Using these word representations, Turian et al. (2010) were able to improve near state-of-the-art systems for several tasks, by simply plugging in the learned word representations as additional features. However, because these features are estimated by minimizing the prediction errors made on a generic, unsupervised, task they might be suboptimal for the intended purposes. Ideally, word features should be adapted to the specific supervised task. One of the reasons for the success of deep learning models for language problems, is the use unsupervised word embeddings to initialize the word projection layer. Then, during training, the errors made in</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384–394. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>