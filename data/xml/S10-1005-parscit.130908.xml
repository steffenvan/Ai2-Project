<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.901961">
SemEval-2010 Task 7: Argument Selection and Coercion
</title>
<author confidence="0.983901">
James Pustejovsky and Anna Rumshisky and Alex Plotnick
</author>
<affiliation confidence="0.847453666666667">
Dept. of Computer Science
Brandeis University
Waltham, MA, USA
</affiliation>
<author confidence="0.983766">
Elisabetta Jezek Olga Batiukova Valeria Quochi
</author>
<affiliation confidence="0.966078666666667">
Dept. of Linguistics Dept. of Humanities ILC-CNR
University of Pavia Carlos III University of Madrid Pisa, Italy
Pavia, Italy Madrid, Spain
</affiliation>
<sectionHeader confidence="0.974041" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999989846153846">
We describe the Argument Selection and
Coercion task for the SemEval-2010 eval-
uation exercise. This task involves char-
acterizing the type of compositional oper-
ation that exists between a predicate and
the arguments it selects. Specifically, the
goal is to identify whether the type that
a verb selects is satisfied directly by the
argument, or whether the argument must
change type to satisfy the verb typing. We
discuss the problem in detail, describe the
data preparation for the task, and analyze
the results of the submissions.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999953193548387">
In recent years, a number of annotation schemes
that encode semantic information have been de-
veloped and used to produce data sets for training
machine learning algorithms. Semantic markup
schemes that have focused on annotating entity
types and, more generally, word senses, have
been extended to include semantic relationships
between sentence elements, such as the seman-
tic role (or label) assigned to the argument by the
predicate (Palmer et al., 2005; Ruppenhofer et al.,
2006; Kipper, 2005; Burchardt et al., 2006; Subi-
rats, 2004).
In this task, we take this one step further and
attempt to capture the “compositional history” of
the argument selection relative to the predicate. In
particular, this task attempts to identify the oper-
ations of type adjustment induced by a predicate
over its arguments when they do not match its se-
lectional properties. The task is defined as fol-
lows: for each argument of a predicate, identify
whether the entity in that argument position satis-
fies the type expected by the predicate. If not, then
identify how the entity in that position satisfies the
typing expected by the predicate; that is, identify
the source and target types in a type-shifting or co-
ercion operation.
Consider the example below, where the verb re-
port normally selects for a human in subject po-
sition, as in (1a). Notice, however, that through
a metonymic interpretation, this constraint can be
violated, as demonstrated in (1b).
</bodyText>
<listItem confidence="0.624692">
(1) a. John reported in late from Washington.
b. Washington reported in late.
</listItem>
<bodyText confidence="0.999902692307692">
Neither the surface annotation of entity extents
and types nor assigning semantic roles associated
with the predicate would reflect in this case a cru-
cial point: namely, that in order for the typing
requirements of the predicate to be satisfied, a
type coercion or a metonymy (Hobbs et al., 1993;
Pustejovsky, 1991; Nunberg, 1979; Egg, 2005)
has taken place.
The SemEval Metonymy task (Markert and Nis-
sim, 2007) was a good attempt to annotate such
metonymic relations over a larger data set. This
task involved two types with their metonymic
variants: categories-for-locations (e.g., place-
for-people) and categories-for-organizations (e.g.,
organization-for-members). One of the limitations
of this approach, however, is that while appropri-
ate for these specialized metonymy relations, the
annotation specification and resulting corpus are
not an informative guide for extending the annota-
tion of argument selection more broadly.
In fact, the metonymy example in (1) is an in-
stance of a much more pervasive phenomenon of
type shifting and coercion in argument selection.
For example, in (2) below, the sense annotation
for the verb enjoy should arguably assign similar
values to both (2a) and (2b).
</bodyText>
<page confidence="0.984575">
27
</page>
<note confidence="0.7577555">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 27–32,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<figureCaption confidence="0.650323">
Figure 1: The MATTER Methodology
(2) a. Mary enjoyed drinking her beer.
b. Mary enjoyed her beer.
</figureCaption>
<bodyText confidence="0.999885666666667">
The consequence of this is that under current sense
and role annotation strategies, the mapping to a
syntactic realization for a given sense is made
more complex, and is in fact perplexing for a clus-
tering or learning algorithm operating over subcat-
egorization types for the verb.
</bodyText>
<sectionHeader confidence="0.860459" genericHeader="method">
2 Methodology of Annotation
</sectionHeader>
<bodyText confidence="0.903869361111111">
Before introducing the specifics of the argument
selection and coercion task, we will briefly review
our assumptions regarding the role of annotation
in computational linguistic systems.
We assume that the features we use for encoding
a specific linguistic phenomenon are rich enough
to capture the desired behavior. These linguistic
descriptions are typically distilled from extensive
theoretical modeling of the phenomenon. The de-
scriptions in turn form the basis for the annota-
tion values of the specification language, which
are themselves the features used in a development
cycle for training and testing a labeling algorithm
over a text. Finally, based on an analysis and eval-
uation of the performance of a system, the model
of the phenomenon may be revised.
We call this cycle of development the MATTER
methodology (Fig. 1):
Model: Structural descriptions provide theoretically in-
formed attributes derived from empirical observations
over the data;
Annotate: Annotation scheme assumes a feature set that en-
codes specific structural descriptions and properties of
the input data;
Train: Algorithm is trained over a corpus annotated with the
target feature set;
Test: Algorithm is tested against held-out data;
Evaluate: Standardized evaluation of results;
Revise: Revisit the model, annotation specification, or algo-
rithm, in order to make the annotation more robust and
reliable.
Some of the current and completed annotation ef-
forts that have undergone such a development cy-
cle include PropBank (Palmer et al., 2005), Nom-
Bank (Meyers et al., 2004), and TimeBank (Puste-
jovsky et al., 2005).
</bodyText>
<sectionHeader confidence="0.98804" genericHeader="method">
3 Task Description
</sectionHeader>
<bodyText confidence="0.998085125">
The argument selection and coercion (ASC) task
involves identifying the selectional mechanism
used by the predicate over a particular argument.1
For the purposes of this task, the possible relations
between the predicate and a given argument are re-
stricted to selection and coercion. In selection, the
argument NP satisfies the typing requirements of
the predicate, as in (3):
</bodyText>
<listItem confidence="0.6176274">
(3) a. The spokesman denied the statement (PROPOSI-
TION).
b. The child threw the stone (PHYSICAL OBJECT).
c. The audience didn’t believe the rumor (PROPOSI-
TION).
</listItem>
<bodyText confidence="0.94316735">
Coercion occurs when a type-shifting operation
must be performed on the complement NP in order
to satisfy selectional requirements of the predicate,
as in (4). Note that coercion operations may apply
to any argument position in a sentence, including
the subject, as seen in (4b). Coercion can also be
seen as an object of a proposition, as in (4c).
(4) a. The president denied the attack (EVENT → PROPO-
SITION).
b. The White House (LOCATION → HUMAN) denied
this statement.
c. The Boston office called with an update (EVENT →
INFO).
In order to determine whether type-shifting has
taken place, the classification task must then in-
volve (1) identifying the verb sense and the asso-
ciated syntactic frame, (2) identifying selectional
requirements imposed by that verb sense on the
target argument, and (3) identifying the semantic
type of the target argument.
</bodyText>
<sectionHeader confidence="0.990952" genericHeader="method">
4 Resources and Corpus Development
</sectionHeader>
<bodyText confidence="0.999180666666667">
We prepared the data for this task in two phases:
the data set construction phase and the annotation
phase (see Fig. 2). The first phase consisted of
</bodyText>
<listItem confidence="0.8617914">
(1) selecting the target verbs to be annotated and
compiling a sense inventory for each target, and
(2) data extraction and preprocessing. The pre-
pared data was then loaded into the annotation in-
terface. During the annotation phase, the annota-
</listItem>
<bodyText confidence="0.933073666666667">
tion judgments were entered into the database, and
an adjudicator resolved disagreements. The result-
ing database was then exported in an XML format.
</bodyText>
<footnote confidence="0.957975">
1This task is part of a larger effort to annotate text with
compositional operations (Pustejovsky et al., 2009).
</footnote>
<page confidence="0.997951">
28
</page>
<figureCaption confidence="0.995491">
Figure 2: Corpus Development Architecture
</figureCaption>
<subsectionHeader confidence="0.986894">
4.1 Data Set Construction Phase: English
</subsectionHeader>
<bodyText confidence="0.797153111111111">
For the English data set, the data construction
phase was combined with the annotation phase.
The data for the task was created using the fol-
lowing steps:
1. The verbs were selected by examining the data
from the BNC, using the Sketch Engine (Kilgar-
riff et al., 2004) as described in (Rumshisky and
Batiukova, 2008). Verbs that consistently im-
pose semantic typing on one of their arguments
in at least one of their senses (strongly coercive
verbs) were included into the final data set: ar-
rive (at), cancel, deny, finish, and hear.
2. Sense inventories were compiled for each verb,
with the senses mapped to OntoNotes (Pradhan
et al., 2007) whenever possible. For each sense,
a set of type templates was compiled using a
modification of the CPA technique (Hanks and
Pustejovsky, 2005; Pustejovsky et al., 2004):
every argument in the syntactic pattern asso-
ciated with a given sense was assigned a type
specification. Although a particular sense is
often compatible with more than one semantic
type for a given argument, this was never the
case in our data set, where no disjoint types
were tested. The coercive senses of the chosen
verbs were associated with the following type
templates:
</bodyText>
<listItem confidence="0.975741111111111">
a. Arrive (at), sense reach a destination or goal: HU-
MAN arrive at LOCATION
b. Cancel, sense call off: HUMAN cancel EVENT
c. Deny, sense state or maintain that something is un-
true: HUMAN deny PROPOSITION
d. Finish, sense complete an activity: HUMAN finish
EVENT
e. Hear, sense perceive physical sound: HUMAN hear
SOUND
</listItem>
<bodyText confidence="0.887955">
We used a subset of semantic types from the
Brandeis Shallow Ontology (BSO), which is a
shallow hierarchy of types developed as a part
of the CPA effort (Hanks, 2009; Pustejovsky
et al., 2004; Rumshisky et al., 2006). Types
were selected for their prevalence in manually
identified selection context patterns developed
for several hundred English verbs. That is,
they capture common semantic distinctions as-
sociated with the selectional properties of many
verbs. The types used for annotation were:
ABSTRACT ENTITY, ANIMATE, ARTIFACT, ATTITUDE,
DOCUMENT, DRINK, EMOTION, ENTITY, EVENT, FOOD,
HUMAN, HUMAN GROUP, IDEA, INFORMATION, LOCA-
TION, OBLIGATION, ORGANIZATION, PATH, PHYSICAL
OBJECT, PROPERTY, PROPOSITION, RULE, SENSATION,
SOUND, SUBSTANCE, TIME PERIOD, VEHICLE
This set of types is purposefully shallow and
non-hierarchical. For example, HUMAN is a
subtype of both ANIMATE and PHYSICAL OB-
JECT, but annotators and system developers
were instructed to choose the most relevant type
(e.g., HUMAN) and to ignore inheritance.
3. A set of sentences was randomly extracted for
each target verb from the BNC (Burnard, 1995).
The extracted sentences were parsed automati-
cally, and the sentences organized according to
the grammatical relation the target verb was in-
volved in. Sentences were excluded from the set
if the target argument was expressed as anaphor,
or was not present in the sentence. The seman-
tic head for the target grammatical relation was
identified in each case.
</bodyText>
<listItem confidence="0.584945714285714">
4. Word sense disambiguation of the target predi-
cate was performed manually on each extracted
sentence, matching the target against the sense
inventory and the corresponding type templates
as described above. The appropriate senses
were then saved into the database along with the
associated type template.
</listItem>
<bodyText confidence="0.946079333333333">
5. The sentences containing coercive senses of the
target verbs were loaded into the Brandeis An-
notation Tool (Verhagen, 2010). Annotators
were presented with a list of sentences and
asked to determine whether the argument in
the specified grammatical relation to the target
belongs to the type associated with that sense
in the corresponding template. Disagreements
were resolved by adjudication.
</bodyText>
<page confidence="0.994563">
29
</page>
<table confidence="0.999859875">
Coerion Type Verb Train Test
EVENT→LOCATION arrive at 38 37
ARTIFACT→EVENT cancel 35 35
finish 91 92
EVENT→PROPOSITION deny 56 54
ARTIFACT→SOUND hear 28 30
EVENT→SOUND hear 24 26
DOCUMENT→EVENT finish 39 40
</table>
<tableCaption confidence="0.999966">
Table 1: Coercions in the English data set
</tableCaption>
<bodyText confidence="0.9553908">
6. To guarantee robustness of the data, two addi-
tional steps were taken. First, only the six most
recurrent coercion types were selected; these
are given in table 1. Preference was given to
cross-domain coercions, where the source and
the target types are not related ontologically.
Second, the distribution of selection and co-
ercion instances were skewed to increase the
number of coercions. The final English data set
contains about 30% coercions.
</bodyText>
<listItem confidence="0.6715024">
7. Finally, the data set was randomly split in half
into a training set and a test set. The training
data has 1032 instances, 311 of which are co-
ercions, and the test data has 1039 instances,
314 of which are coercions.
</listItem>
<subsectionHeader confidence="0.996439">
4.2 Data Set Construction Phase: Italian
</subsectionHeader>
<bodyText confidence="0.982381133333333">
In constructing the Italian data set, we adopted the
same methodology used for the English data set,
with the following differences:
1. The list of coercive verbs was selected by exam-
ining data from the ItWaC (Baroni and Kilgar-
riff, 2006) using the Sketch Engine (Kilgarriff
et al., 2004):
accusare ‘accuse’, annunciare ‘announce’, arrivare ‘ar-
rive’, ascoltare ‘listen’, avvisare ‘inform’, chiamare
‘call’, cominciare ‘begin’, completare ‘complete’, con-
cludere ‘conclude’, contattare ‘contact’, divorare ‘de-
vour’, echeggiare ‘echo’, finire ‘finish’, informare ‘in-
form’, interrompere ‘interrupt’, leggere ‘read’, raggiun-
gere ‘reach’, recar(si) ‘go to’, rimbombare ‘resound’,
sentire ‘hear’, udire ‘hear’, visitare ‘visit’.
</bodyText>
<listItem confidence="0.895391125">
2. The coercive senses of the chosen verbs were
associated with type templates, some of which
are listed listed below. Whenever possible,
senses and type templates were adapted from
the Italian Pattern Dictionary (Hanks and Jezek,
2007) and mapped to their SIMPLE equiva-
lents (Lenci et al., 2000).
a. arrivare, sense reach a location: HUMAN arriva
[prep] LOCATION
b. cominciare, sense initiate an undertaking: HUMAN
comincia EVENT
c. completare, sense finish an activity: HUMAN com-
pleta EVENT
d. udire, sense perceive a sound: HUMAN ode SOUND
e. visitare, sense visit a place: HUMAN visita LOCA-
TION
</listItem>
<bodyText confidence="0.9658125">
The following types were used to annotate
the Italian dataset:
</bodyText>
<construct confidence="0.3930575">
ABSTRACT ENTITY, ANIMATE, ARTIFACT, ATTITUDE,
CONTAINER, DOCUMENT, DRINK, EMOTION, ENTITY,
EVENT, FOOD, HUMAN, HUMAN GROUP, IDEA, IN-
FORMATION, LIQUID, LOCATION, ORGANIZATION,
PHYSICAL OBJECT, PROPERTY, SENSATION, SOUND,
TIME PERIOD, VEHICLE
</construct>
<bodyText confidence="0.95606015">
The annotators were provided with a set of def-
initions and examples of each type.
3. A set of sentences for each target verb was ex-
tracted and parsed from the PAROLE sottoin-
sieme corpus (Bindi et al., 2000). They were
skimmed to ensure that the final data set con-
tained a sufficient number of coercions, with
proportionally more selections than coercions.
Sentences were preselected to include instances
representing one of the chosen senses.
4. In order to exclude instances that may have been
wrongly selected, a judge performed word sense
disambiguation of the target predicate in the ex-
tracted sentences.
5. Annotators were presented with a list of sen-
tences and asked to determine the usual seman-
tic type associated with the argument in the
specified grammatical relation. Every sentence
was annotated by two annotators and one judge,
who resolved disagreements.
</bodyText>
<listItem confidence="0.9989236">
6. Some of the coercion types selected for Italian
were:
a. LOCATION → HUMAN (accusare, annunciare)
b. ARTIFACT → HUMAN (annunciare, avvisare)
c. EVENT → LOCATION (arrivare, raggiungere)
d. ARTIFACT → EVENT (cominciare, completare)
e. EVENT → DOCUMENT (leggere, divorare)
f. HUMAN → DOCUMENT (leggere, divorare)
g. EVENT → SOUND (ascoltare, echeggiare)
h. ARTIFACT → SOUND (ascoltare, echeggiare)
</listItem>
<bodyText confidence="0.912239">
7. The Italian training data contained 1466 in-
stances, 381 of which are coercions; the test
data had 1463 instances, with 384 coercions.
</bodyText>
<sectionHeader confidence="0.98186" genericHeader="method">
5 Data Format
</sectionHeader>
<bodyText confidence="0.99994575">
The test and training data were provided in XML.
The relation between the predicate (viewed as
a function) and its argument were represented
by composition link elements (CompLink), as
</bodyText>
<page confidence="0.995823">
30
</page>
<bodyText confidence="0.996792333333333">
shown below. The test data differed from the train-
ing data in the omission of CompLink elements.
In case of coercion, there is a mismatch between
the source and the target types, and both types
need to be identified; e.g., The State Department
repeatedly denied the attack:
</bodyText>
<table confidence="0.880471714285714">
The State Department repeatedly
&lt;SELECTOR sid=&amp;quot;s1&amp;quot;&gt;denied&lt;/SELECTOR&gt;
the &lt;TARGET id=&amp;quot;t1&amp;quot;&gt;attack&lt;/TARGET&gt;.
&lt;CompLink cid=&amp;quot;cid1&amp;quot;
compType=&amp;quot;COERCION&amp;quot;
selector_id=&amp;quot;s1&amp;quot;
relatedToTarget=&amp;quot;t1&amp;quot;
sourceType=&amp;quot;EVENT&amp;quot;
targetType=&amp;quot;PROPOSITION&amp;quot;/&gt;
When the compositional operation is selection,
the source and target types must match; e.g., The
State Department repeatedly denied the statement:
The State Department repeatedly
&lt;SELECTOR sid=&amp;quot;s2&amp;quot;&gt;denied&lt;/SELECTOR&gt;
the &lt;TARGET id=&amp;quot;t2&amp;quot;&gt;statement&lt;/TARGET&gt;.
&lt;CompLink cid=&amp;quot;cid2&amp;quot;
compType=&amp;quot;SELECTION&amp;quot;
selector_id=&amp;quot;s2&amp;quot;
relatedToTarget=&amp;quot;t2&amp;quot;
sourceType=&amp;quot;PROPOSITION&amp;quot;
targetType=&amp;quot;PROPOSITION&amp;quot;/&gt;
</table>
<sectionHeader confidence="0.980888" genericHeader="evaluation">
6 Results &amp; Analysis
</sectionHeader>
<bodyText confidence="0.99936524">
We received only a single submission for the
ASC task. The UTDMet system was an SVM-
based system with features derived from two main
sources: a PageRank-style algorithm over Word-
Net hypernyms used to define semantic classes,
and statistics from a PropBank-style parse of some
8 million documents from the English Gigaword
corpus. The results, shown in Table 2, were
computed from confusion matrices constructed for
each of four classification tasks for the 1039 link
instances in the English test data: determination
of argument selection or coercion, identification of
the argument source type, identification of the ar-
gument target type, and the joint identification of
the source/target type pair.
Clearly, the UTDMet system did quite well at
this task. The one immediately noticeable outlier
is the macro-averaged precision for the joint type,
which reflects a small number of miscategoriza-
tions of rare types. For example, eliminating the
single miscategorized ARTIFACT-LOCATION link
in the submitted test data bumps this score up to
a respectable 94%. This large discrepancy can ex-
plained by the lack of any coercions with those
types in the gold-standard data.
</bodyText>
<table confidence="0.999089125">
Prec. Recall Averaging
Selection vs. 95 96 (macro)
Coercion: 96 96 (micro)
Source Type: 96 96 (macro)
96 96 (micro)
Target Type: 100 100 (both)
Joint Type: 86 95 (macro)
96 96 (micro)
</table>
<tableCaption confidence="0.996223">
Table 2: Results for the UTDMet submission.
</tableCaption>
<bodyText confidence="0.999975095238095">
In the absence of any other submissions, it is
difficult to provide a point of comparison for this
performance. However, we can provide a base-
line by taking each link to be a selection whose
source and target types are the most common type
(EVENT for the gold-standard English data). This
yields micro-averaged precision scores of 69% for
selection vs. coercion, 33% for source type iden-
tification, 37% for the target type identification,
and 22% for the joint type.
The performance of the UTDMet system sug-
gests that most of the type coercions were identifi-
able based largely on examination of lexical clues
associated with selection contexts. This is in fact
to be expected for the type coercions that were the
focus of the English data set. It will be interesting
to see how systems perform on the Italian data set
and an expanded corpus for English and Italian,
where more subtle and complex type exploitations
and manipulations are at play. These will hope-
fully be explored in future competitions.
</bodyText>
<sectionHeader confidence="0.998509" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999940714285714">
In this paper, we have described the Argument Se-
lection and Coercion task for SemEval-2010. This
task involves identifying the relation between a
predicate and its argument as one that encodes
the compositional history of the selection process.
This allows us to distinguish surface forms that di-
rectly satisfy the selectional (type) requirements of
a predicate from those that are coerced in context.
We described some details of a specification lan-
guage for selection, the annotation task using this
specification to identify argument selection behav-
ior, and the preparation of the data for the task.
Finally, we analyzed the results of the task sub-
missions.
</bodyText>
<page confidence="0.999823">
31
</page>
<sectionHeader confidence="0.989631" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999667393617021">
M. Baroni and A. Kilgarriff. 2006. Large
linguistically-processed web corpora for multiple
languages. In Proceedings of European ACL.
R. Bindi, P. Baroni, M. Monachini, and E. Gola. 2000.
PAROLE-Sottoinsieme. ILC-CNR Internal Report.
Aljoscha Burchardt, Katrin Erk, Anette Frank, An-
drea Kowalski, Sebastian Pado, and Manfred Pinkal.
2006. The salsa corpus: a german corpus resource
for lexical semantics. In Proceedings of LREC,
Genoa, Italy.
L. Burnard, 1995. Users’ Reference Guide, British Na-
tional Corpus. British National Corpus Consortium,
Oxford, England.
Marcus Egg. 2005. Flexible semantics for reinterpre-
tation phenomena. CSLI, Stanford.
P. Hanks and E. Jezek. 2007. Building Pattern Dictio-
naries with Corpus Analysis. In International Col-
loquium on Possible Dictionaries, Rome, June, 6-7.
Oral Presentation.
P. Hanks and J. Pustejovsky. 2005. A pattern dic-
tionary for natural language processing. Revue
Franc¸aise de Linguistique Appliqu´ee.
P. Hanks. 2009. Corpus pattern analysis. CPA
Project Page. Retrieved April 11, 2009, from
http://nlp.fi.muni.cz/projekty/cpa/.
J. R. Hobbs, M. Stickel, and P. Martin. 1993. Interpre-
tation as abduction. Artificial Intelligence, 63:69–
142.
A. Kilgarriff, P. Rychly, P. Smrz, and D. Tugwell.
2004. The Sketch Engine. Proceedings of Euralex,
Lorient, France, pages 105–116.
Karin Kipper. 2005. VerbNet: A broad-coverage, com-
prehensive verb lexicon. Phd dissertation, Univer-
sity of Pennsylvania, PA.
A. Lenci, N. Bel, F. Busa, N. Calzolari, E. Gola,
M. Monachini, A. Ogonowski, I. Peters, W. Peters,
N. Ruimy, et al. 2000. SIMPLE: A general frame-
work for the development of multilingual lexicons.
International Journal of Lexicography, 13(4):249.
K. Markert and M. Nissim. 2007. SemEval-2007
task 8: Metonymy resolution. In Eneko Agirre,
Llu´ıs M`arquez, and Richard Wicentowski, editors,
Proceedings of the Fourth International Workshop
on Semantic Evaluations (SemEval-2007), Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
A. Meyers, R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004.
The NomBank project: An interim report. In HLT-
NAACL 2004 Workshop: Frontiers in Corpus Anno-
tation, pages 24–31.
Geoffrey Nunberg. 1979. The non-uniqueness of se-
mantic solutions: Polysemy. Linguistics and Phi-
losophy, 3:143–184.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71–106.
S. Pradhan, E. Hovy, MS Marcus, M. Palmer,
L. Ramshaw, and R. Weischedel. 2007. Ontonotes:
A unified relational semantic representation. In
International Conference on Semantic Computing,
2007, pages 517–526.
J. Pustejovsky, P. Hanks, and A. Rumshisky. 2004.
Automated Induction of Sense in Context. In COL-
ING 2004, Geneva, Switzerland, pages 924–931.
J. Pustejovsky, R. Knippen, J. Littman, and R. Sauri.
2005. Temporal and event information in natural
language text. Language Resources and Evaluation,
39(2):123–164.
J. Pustejovsky, A. Rumshisky, J. Moszkowicz, and
O. Batiukova. 2009. GLML: Annotating argument
selection and coercion. IWCS-8: Eighth Interna-
tional Conference on Computational Semantics.
J. Pustejovsky. 1991. The generative lexicon. Compu-
tational Linguistics, 17(4).
A. Rumshisky and O. Batiukova. 2008. Polysemy
in verbs: systematic relations between senses and
their effect on annotation. In COLING Workshop
on Human Judgement in Computational Linguistics
(HJCL-2008), Manchester, England.
A. Rumshisky, P. Hanks, C. Havasi, and J. Pustejovsky.
2006. Constructing a corpus-based ontology using
model bias. In The 19th International FLAIRS Con-
ference, FLAIRS 2006, Melbourne Beach, Florida,
USA.
J. Ruppenhofer, M. Ellsworth, M. Petruck, C. Johnson,
and J. Scheffczyk. 2006. FrameNet II: Extended
Theory and Practice.
Carlos Subirats. 2004. FrameNet Espa˜nol. Una red
sem´antica de marcos conceptuales. In VI Interna-
tional Congress of Hispanic Linguistics, Leipzig.
Marc Verhagen. 2010. The Brandeis Annotation Tool.
In Language Resources and Evaluation Conference,
LREC 2010, Malta.
</reference>
<page confidence="0.999294">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.754790">
<title confidence="0.994575">SemEval-2010 Task 7: Argument Selection and Coercion</title>
<author confidence="0.978236">Pustejovsky Rumshisky Plotnick</author>
<affiliation confidence="0.9997965">Dept. of Computer Science Brandeis University</affiliation>
<address confidence="0.999906">Waltham, MA, USA</address>
<author confidence="0.997969">Elisabetta Jezek Olga Batiukova Valeria Quochi</author>
<affiliation confidence="0.9935355">Dept. of Linguistics Dept. of Humanities ILC-CNR University of Pavia Carlos III University of Madrid Pisa, Italy</affiliation>
<address confidence="0.788703">Pavia, Italy Madrid, Spain</address>
<abstract confidence="0.999781928571429">describe the Selection and for the SemEval-2010 evaluation exercise. This task involves characterizing the type of compositional operation that exists between a predicate and the arguments it selects. Specifically, the goal is to identify whether the type that a verb selects is satisfied directly by the argument, or whether the argument must change type to satisfy the verb typing. We discuss the problem in detail, describe the data preparation for the task, and analyze the results of the submissions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>A Kilgarriff</author>
</authors>
<title>Large linguistically-processed web corpora for multiple languages.</title>
<date>2006</date>
<booktitle>In Proceedings of European ACL.</booktitle>
<contexts>
<context position="12900" citStr="Baroni and Kilgarriff, 2006" startWordPosition="2046" endWordPosition="2050">and coercion instances were skewed to increase the number of coercions. The final English data set contains about 30% coercions. 7. Finally, the data set was randomly split in half into a training set and a test set. The training data has 1032 instances, 311 of which are coercions, and the test data has 1039 instances, 314 of which are coercions. 4.2 Data Set Construction Phase: Italian In constructing the Italian data set, we adopted the same methodology used for the English data set, with the following differences: 1. The list of coercive verbs was selected by examining data from the ItWaC (Baroni and Kilgarriff, 2006) using the Sketch Engine (Kilgarriff et al., 2004): accusare ‘accuse’, annunciare ‘announce’, arrivare ‘arrive’, ascoltare ‘listen’, avvisare ‘inform’, chiamare ‘call’, cominciare ‘begin’, completare ‘complete’, concludere ‘conclude’, contattare ‘contact’, divorare ‘devour’, echeggiare ‘echo’, finire ‘finish’, informare ‘inform’, interrompere ‘interrupt’, leggere ‘read’, raggiungere ‘reach’, recar(si) ‘go to’, rimbombare ‘resound’, sentire ‘hear’, udire ‘hear’, visitare ‘visit’. 2. The coercive senses of the chosen verbs were associated with type templates, some of which are listed listed belo</context>
</contexts>
<marker>Baroni, Kilgarriff, 2006</marker>
<rawString>M. Baroni and A. Kilgarriff. 2006. Large linguistically-processed web corpora for multiple languages. In Proceedings of European ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bindi</author>
<author>P Baroni</author>
<author>M Monachini</author>
<author>E Gola</author>
</authors>
<date>2000</date>
<tech>PAROLE-Sottoinsieme. ILC-CNR Internal Report.</tech>
<contexts>
<context position="14493" citStr="Bindi et al., 2000" startWordPosition="2279" endWordPosition="2282">NT d. udire, sense perceive a sound: HUMAN ode SOUND e. visitare, sense visit a place: HUMAN visita LOCATION The following types were used to annotate the Italian dataset: ABSTRACT ENTITY, ANIMATE, ARTIFACT, ATTITUDE, CONTAINER, DOCUMENT, DRINK, EMOTION, ENTITY, EVENT, FOOD, HUMAN, HUMAN GROUP, IDEA, INFORMATION, LIQUID, LOCATION, ORGANIZATION, PHYSICAL OBJECT, PROPERTY, SENSATION, SOUND, TIME PERIOD, VEHICLE The annotators were provided with a set of definitions and examples of each type. 3. A set of sentences for each target verb was extracted and parsed from the PAROLE sottoinsieme corpus (Bindi et al., 2000). They were skimmed to ensure that the final data set contained a sufficient number of coercions, with proportionally more selections than coercions. Sentences were preselected to include instances representing one of the chosen senses. 4. In order to exclude instances that may have been wrongly selected, a judge performed word sense disambiguation of the target predicate in the extracted sentences. 5. Annotators were presented with a list of sentences and asked to determine the usual semantic type associated with the argument in the specified grammatical relation. Every sentence was annotated</context>
</contexts>
<marker>Bindi, Baroni, Monachini, Gola, 2000</marker>
<rawString>R. Bindi, P. Baroni, M. Monachini, and E. Gola. 2000. PAROLE-Sottoinsieme. ILC-CNR Internal Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aljoscha Burchardt</author>
<author>Katrin Erk</author>
<author>Anette Frank</author>
<author>Andrea Kowalski</author>
<author>Sebastian Pado</author>
<author>Manfred Pinkal</author>
</authors>
<title>The salsa corpus: a german corpus resource for lexical semantics.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="1433" citStr="Burchardt et al., 2006" startWordPosition="217" endWordPosition="220">describe the data preparation for the task, and analyze the results of the submissions. 1 Introduction In recent years, a number of annotation schemes that encode semantic information have been developed and used to produce data sets for training machine learning algorithms. Semantic markup schemes that have focused on annotating entity types and, more generally, word senses, have been extended to include semantic relationships between sentence elements, such as the semantic role (or label) assigned to the argument by the predicate (Palmer et al., 2005; Ruppenhofer et al., 2006; Kipper, 2005; Burchardt et al., 2006; Subirats, 2004). In this task, we take this one step further and attempt to capture the “compositional history” of the argument selection relative to the predicate. In particular, this task attempts to identify the operations of type adjustment induced by a predicate over its arguments when they do not match its selectional properties. The task is defined as follows: for each argument of a predicate, identify whether the entity in that argument position satisfies the type expected by the predicate. If not, then identify how the entity in that position satisfies the typing expected by the pre</context>
</contexts>
<marker>Burchardt, Erk, Frank, Kowalski, Pado, Pinkal, 2006</marker>
<rawString>Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea Kowalski, Sebastian Pado, and Manfred Pinkal. 2006. The salsa corpus: a german corpus resource for lexical semantics. In Proceedings of LREC, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Burnard</author>
</authors>
<title>Users’ Reference Guide,</title>
<date>1995</date>
<institution>British National Corpus. British National Corpus Consortium,</institution>
<location>Oxford, England.</location>
<contexts>
<context position="10636" citStr="Burnard, 1995" startWordPosition="1682" endWordPosition="1683">NTITY, ANIMATE, ARTIFACT, ATTITUDE, DOCUMENT, DRINK, EMOTION, ENTITY, EVENT, FOOD, HUMAN, HUMAN GROUP, IDEA, INFORMATION, LOCATION, OBLIGATION, ORGANIZATION, PATH, PHYSICAL OBJECT, PROPERTY, PROPOSITION, RULE, SENSATION, SOUND, SUBSTANCE, TIME PERIOD, VEHICLE This set of types is purposefully shallow and non-hierarchical. For example, HUMAN is a subtype of both ANIMATE and PHYSICAL OBJECT, but annotators and system developers were instructed to choose the most relevant type (e.g., HUMAN) and to ignore inheritance. 3. A set of sentences was randomly extracted for each target verb from the BNC (Burnard, 1995). The extracted sentences were parsed automatically, and the sentences organized according to the grammatical relation the target verb was involved in. Sentences were excluded from the set if the target argument was expressed as anaphor, or was not present in the sentence. The semantic head for the target grammatical relation was identified in each case. 4. Word sense disambiguation of the target predicate was performed manually on each extracted sentence, matching the target against the sense inventory and the corresponding type templates as described above. The appropriate senses were then s</context>
</contexts>
<marker>Burnard, 1995</marker>
<rawString>L. Burnard, 1995. Users’ Reference Guide, British National Corpus. British National Corpus Consortium, Oxford, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcus Egg</author>
</authors>
<title>Flexible semantics for reinterpretation phenomena.</title>
<date>2005</date>
<publisher>CSLI, Stanford.</publisher>
<contexts>
<context position="2776" citStr="Egg, 2005" startWordPosition="442" endWordPosition="443"> verb report normally selects for a human in subject position, as in (1a). Notice, however, that through a metonymic interpretation, this constraint can be violated, as demonstrated in (1b). (1) a. John reported in late from Washington. b. Washington reported in late. Neither the surface annotation of entity extents and types nor assigning semantic roles associated with the predicate would reflect in this case a crucial point: namely, that in order for the typing requirements of the predicate to be satisfied, a type coercion or a metonymy (Hobbs et al., 1993; Pustejovsky, 1991; Nunberg, 1979; Egg, 2005) has taken place. The SemEval Metonymy task (Markert and Nissim, 2007) was a good attempt to annotate such metonymic relations over a larger data set. This task involved two types with their metonymic variants: categories-for-locations (e.g., placefor-people) and categories-for-organizations (e.g., organization-for-members). One of the limitations of this approach, however, is that while appropriate for these specialized metonymy relations, the annotation specification and resulting corpus are not an informative guide for extending the annotation of argument selection more broadly. In fact, th</context>
</contexts>
<marker>Egg, 2005</marker>
<rawString>Marcus Egg. 2005. Flexible semantics for reinterpretation phenomena. CSLI, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hanks</author>
<author>E Jezek</author>
</authors>
<title>Building Pattern Dictionaries with Corpus Analysis.</title>
<date>2007</date>
<booktitle>In International Colloquium on Possible Dictionaries,</booktitle>
<pages>6--7</pages>
<institution>Oral Presentation.</institution>
<location>Rome,</location>
<contexts>
<context position="13620" citStr="Hanks and Jezek, 2007" startWordPosition="2142" endWordPosition="2145">vare ‘arrive’, ascoltare ‘listen’, avvisare ‘inform’, chiamare ‘call’, cominciare ‘begin’, completare ‘complete’, concludere ‘conclude’, contattare ‘contact’, divorare ‘devour’, echeggiare ‘echo’, finire ‘finish’, informare ‘inform’, interrompere ‘interrupt’, leggere ‘read’, raggiungere ‘reach’, recar(si) ‘go to’, rimbombare ‘resound’, sentire ‘hear’, udire ‘hear’, visitare ‘visit’. 2. The coercive senses of the chosen verbs were associated with type templates, some of which are listed listed below. Whenever possible, senses and type templates were adapted from the Italian Pattern Dictionary (Hanks and Jezek, 2007) and mapped to their SIMPLE equivalents (Lenci et al., 2000). a. arrivare, sense reach a location: HUMAN arriva [prep] LOCATION b. cominciare, sense initiate an undertaking: HUMAN comincia EVENT c. completare, sense finish an activity: HUMAN completa EVENT d. udire, sense perceive a sound: HUMAN ode SOUND e. visitare, sense visit a place: HUMAN visita LOCATION The following types were used to annotate the Italian dataset: ABSTRACT ENTITY, ANIMATE, ARTIFACT, ATTITUDE, CONTAINER, DOCUMENT, DRINK, EMOTION, ENTITY, EVENT, FOOD, HUMAN, HUMAN GROUP, IDEA, INFORMATION, LIQUID, LOCATION, ORGANIZATION,</context>
</contexts>
<marker>Hanks, Jezek, 2007</marker>
<rawString>P. Hanks and E. Jezek. 2007. Building Pattern Dictionaries with Corpus Analysis. In International Colloquium on Possible Dictionaries, Rome, June, 6-7. Oral Presentation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hanks</author>
<author>J Pustejovsky</author>
</authors>
<title>A pattern dictionary for natural language processing. Revue Franc¸aise de Linguistique Appliqu´ee.</title>
<date>2005</date>
<contexts>
<context position="8788" citStr="Hanks and Pustejovsky, 2005" startWordPosition="1391" endWordPosition="1394">. The verbs were selected by examining the data from the BNC, using the Sketch Engine (Kilgarriff et al., 2004) as described in (Rumshisky and Batiukova, 2008). Verbs that consistently impose semantic typing on one of their arguments in at least one of their senses (strongly coercive verbs) were included into the final data set: arrive (at), cancel, deny, finish, and hear. 2. Sense inventories were compiled for each verb, with the senses mapped to OntoNotes (Pradhan et al., 2007) whenever possible. For each sense, a set of type templates was compiled using a modification of the CPA technique (Hanks and Pustejovsky, 2005; Pustejovsky et al., 2004): every argument in the syntactic pattern associated with a given sense was assigned a type specification. Although a particular sense is often compatible with more than one semantic type for a given argument, this was never the case in our data set, where no disjoint types were tested. The coercive senses of the chosen verbs were associated with the following type templates: a. Arrive (at), sense reach a destination or goal: HUMAN arrive at LOCATION b. Cancel, sense call off: HUMAN cancel EVENT c. Deny, sense state or maintain that something is untrue: HUMAN deny PR</context>
</contexts>
<marker>Hanks, Pustejovsky, 2005</marker>
<rawString>P. Hanks and J. Pustejovsky. 2005. A pattern dictionary for natural language processing. Revue Franc¸aise de Linguistique Appliqu´ee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hanks</author>
</authors>
<title>Corpus pattern analysis.</title>
<date>2009</date>
<journal>CPA</journal>
<institution>Project Page. Retrieved</institution>
<note>from http://nlp.fi.muni.cz/projekty/cpa/.</note>
<contexts>
<context position="9678" citStr="Hanks, 2009" startWordPosition="1544" endWordPosition="1545">here no disjoint types were tested. The coercive senses of the chosen verbs were associated with the following type templates: a. Arrive (at), sense reach a destination or goal: HUMAN arrive at LOCATION b. Cancel, sense call off: HUMAN cancel EVENT c. Deny, sense state or maintain that something is untrue: HUMAN deny PROPOSITION d. Finish, sense complete an activity: HUMAN finish EVENT e. Hear, sense perceive physical sound: HUMAN hear SOUND We used a subset of semantic types from the Brandeis Shallow Ontology (BSO), which is a shallow hierarchy of types developed as a part of the CPA effort (Hanks, 2009; Pustejovsky et al., 2004; Rumshisky et al., 2006). Types were selected for their prevalence in manually identified selection context patterns developed for several hundred English verbs. That is, they capture common semantic distinctions associated with the selectional properties of many verbs. The types used for annotation were: ABSTRACT ENTITY, ANIMATE, ARTIFACT, ATTITUDE, DOCUMENT, DRINK, EMOTION, ENTITY, EVENT, FOOD, HUMAN, HUMAN GROUP, IDEA, INFORMATION, LOCATION, OBLIGATION, ORGANIZATION, PATH, PHYSICAL OBJECT, PROPERTY, PROPOSITION, RULE, SENSATION, SOUND, SUBSTANCE, TIME PERIOD, VEHI</context>
</contexts>
<marker>Hanks, 2009</marker>
<rawString>P. Hanks. 2009. Corpus pattern analysis. CPA Project Page. Retrieved April 11, 2009, from http://nlp.fi.muni.cz/projekty/cpa/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Hobbs</author>
<author>M Stickel</author>
<author>P Martin</author>
</authors>
<title>Interpretation as abduction.</title>
<date>1993</date>
<journal>Artificial Intelligence,</journal>
<volume>63</volume>
<pages>142</pages>
<contexts>
<context position="2730" citStr="Hobbs et al., 1993" startWordPosition="434" endWordPosition="437">rcion operation. Consider the example below, where the verb report normally selects for a human in subject position, as in (1a). Notice, however, that through a metonymic interpretation, this constraint can be violated, as demonstrated in (1b). (1) a. John reported in late from Washington. b. Washington reported in late. Neither the surface annotation of entity extents and types nor assigning semantic roles associated with the predicate would reflect in this case a crucial point: namely, that in order for the typing requirements of the predicate to be satisfied, a type coercion or a metonymy (Hobbs et al., 1993; Pustejovsky, 1991; Nunberg, 1979; Egg, 2005) has taken place. The SemEval Metonymy task (Markert and Nissim, 2007) was a good attempt to annotate such metonymic relations over a larger data set. This task involved two types with their metonymic variants: categories-for-locations (e.g., placefor-people) and categories-for-organizations (e.g., organization-for-members). One of the limitations of this approach, however, is that while appropriate for these specialized metonymy relations, the annotation specification and resulting corpus are not an informative guide for extending the annotation o</context>
</contexts>
<marker>Hobbs, Stickel, Martin, 1993</marker>
<rawString>J. R. Hobbs, M. Stickel, and P. Martin. 1993. Interpretation as abduction. Artificial Intelligence, 63:69– 142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
<author>P Rychly</author>
<author>P Smrz</author>
<author>D Tugwell</author>
</authors>
<title>The Sketch Engine.</title>
<date>2004</date>
<booktitle>Proceedings of Euralex,</booktitle>
<pages>105--116</pages>
<location>Lorient, France,</location>
<contexts>
<context position="8272" citStr="Kilgarriff et al., 2004" startWordPosition="1305" endWordPosition="1309">he annotation judgments were entered into the database, and an adjudicator resolved disagreements. The resulting database was then exported in an XML format. 1This task is part of a larger effort to annotate text with compositional operations (Pustejovsky et al., 2009). 28 Figure 2: Corpus Development Architecture 4.1 Data Set Construction Phase: English For the English data set, the data construction phase was combined with the annotation phase. The data for the task was created using the following steps: 1. The verbs were selected by examining the data from the BNC, using the Sketch Engine (Kilgarriff et al., 2004) as described in (Rumshisky and Batiukova, 2008). Verbs that consistently impose semantic typing on one of their arguments in at least one of their senses (strongly coercive verbs) were included into the final data set: arrive (at), cancel, deny, finish, and hear. 2. Sense inventories were compiled for each verb, with the senses mapped to OntoNotes (Pradhan et al., 2007) whenever possible. For each sense, a set of type templates was compiled using a modification of the CPA technique (Hanks and Pustejovsky, 2005; Pustejovsky et al., 2004): every argument in the syntactic pattern associated with</context>
<context position="12950" citStr="Kilgarriff et al., 2004" startWordPosition="2055" endWordPosition="2058">ber of coercions. The final English data set contains about 30% coercions. 7. Finally, the data set was randomly split in half into a training set and a test set. The training data has 1032 instances, 311 of which are coercions, and the test data has 1039 instances, 314 of which are coercions. 4.2 Data Set Construction Phase: Italian In constructing the Italian data set, we adopted the same methodology used for the English data set, with the following differences: 1. The list of coercive verbs was selected by examining data from the ItWaC (Baroni and Kilgarriff, 2006) using the Sketch Engine (Kilgarriff et al., 2004): accusare ‘accuse’, annunciare ‘announce’, arrivare ‘arrive’, ascoltare ‘listen’, avvisare ‘inform’, chiamare ‘call’, cominciare ‘begin’, completare ‘complete’, concludere ‘conclude’, contattare ‘contact’, divorare ‘devour’, echeggiare ‘echo’, finire ‘finish’, informare ‘inform’, interrompere ‘interrupt’, leggere ‘read’, raggiungere ‘reach’, recar(si) ‘go to’, rimbombare ‘resound’, sentire ‘hear’, udire ‘hear’, visitare ‘visit’. 2. The coercive senses of the chosen verbs were associated with type templates, some of which are listed listed below. Whenever possible, senses and type templates we</context>
</contexts>
<marker>Kilgarriff, Rychly, Smrz, Tugwell, 2004</marker>
<rawString>A. Kilgarriff, P. Rychly, P. Smrz, and D. Tugwell. 2004. The Sketch Engine. Proceedings of Euralex, Lorient, France, pages 105–116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper</author>
</authors>
<title>VerbNet: A broad-coverage, comprehensive verb lexicon.</title>
<date>2005</date>
<institution>University of Pennsylvania, PA.</institution>
<note>Phd dissertation,</note>
<contexts>
<context position="1409" citStr="Kipper, 2005" startWordPosition="215" endWordPosition="216">em in detail, describe the data preparation for the task, and analyze the results of the submissions. 1 Introduction In recent years, a number of annotation schemes that encode semantic information have been developed and used to produce data sets for training machine learning algorithms. Semantic markup schemes that have focused on annotating entity types and, more generally, word senses, have been extended to include semantic relationships between sentence elements, such as the semantic role (or label) assigned to the argument by the predicate (Palmer et al., 2005; Ruppenhofer et al., 2006; Kipper, 2005; Burchardt et al., 2006; Subirats, 2004). In this task, we take this one step further and attempt to capture the “compositional history” of the argument selection relative to the predicate. In particular, this task attempts to identify the operations of type adjustment induced by a predicate over its arguments when they do not match its selectional properties. The task is defined as follows: for each argument of a predicate, identify whether the entity in that argument position satisfies the type expected by the predicate. If not, then identify how the entity in that position satisfies the ty</context>
</contexts>
<marker>Kipper, 2005</marker>
<rawString>Karin Kipper. 2005. VerbNet: A broad-coverage, comprehensive verb lexicon. Phd dissertation, University of Pennsylvania, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lenci</author>
<author>N Bel</author>
<author>F Busa</author>
<author>N Calzolari</author>
<author>E Gola</author>
<author>M Monachini</author>
<author>A Ogonowski</author>
<author>I Peters</author>
<author>W Peters</author>
<author>N Ruimy</author>
</authors>
<title>SIMPLE: A general framework for the development of multilingual lexicons.</title>
<date>2000</date>
<journal>International Journal of Lexicography,</journal>
<volume>13</volume>
<issue>4</issue>
<contexts>
<context position="13680" citStr="Lenci et al., 2000" startWordPosition="2153" endWordPosition="2156">‘call’, cominciare ‘begin’, completare ‘complete’, concludere ‘conclude’, contattare ‘contact’, divorare ‘devour’, echeggiare ‘echo’, finire ‘finish’, informare ‘inform’, interrompere ‘interrupt’, leggere ‘read’, raggiungere ‘reach’, recar(si) ‘go to’, rimbombare ‘resound’, sentire ‘hear’, udire ‘hear’, visitare ‘visit’. 2. The coercive senses of the chosen verbs were associated with type templates, some of which are listed listed below. Whenever possible, senses and type templates were adapted from the Italian Pattern Dictionary (Hanks and Jezek, 2007) and mapped to their SIMPLE equivalents (Lenci et al., 2000). a. arrivare, sense reach a location: HUMAN arriva [prep] LOCATION b. cominciare, sense initiate an undertaking: HUMAN comincia EVENT c. completare, sense finish an activity: HUMAN completa EVENT d. udire, sense perceive a sound: HUMAN ode SOUND e. visitare, sense visit a place: HUMAN visita LOCATION The following types were used to annotate the Italian dataset: ABSTRACT ENTITY, ANIMATE, ARTIFACT, ATTITUDE, CONTAINER, DOCUMENT, DRINK, EMOTION, ENTITY, EVENT, FOOD, HUMAN, HUMAN GROUP, IDEA, INFORMATION, LIQUID, LOCATION, ORGANIZATION, PHYSICAL OBJECT, PROPERTY, SENSATION, SOUND, TIME PERIOD, V</context>
</contexts>
<marker>Lenci, Bel, Busa, Calzolari, Gola, Monachini, Ogonowski, Peters, Peters, Ruimy, 2000</marker>
<rawString>A. Lenci, N. Bel, F. Busa, N. Calzolari, E. Gola, M. Monachini, A. Ogonowski, I. Peters, W. Peters, N. Ruimy, et al. 2000. SIMPLE: A general framework for the development of multilingual lexicons. International Journal of Lexicography, 13(4):249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Markert</author>
<author>M Nissim</author>
</authors>
<title>SemEval-2007 task 8: Metonymy resolution.</title>
<date>2007</date>
<booktitle>Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<editor>In Eneko Agirre, Llu´ıs M`arquez, and Richard Wicentowski, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2846" citStr="Markert and Nissim, 2007" startWordPosition="451" endWordPosition="455">sition, as in (1a). Notice, however, that through a metonymic interpretation, this constraint can be violated, as demonstrated in (1b). (1) a. John reported in late from Washington. b. Washington reported in late. Neither the surface annotation of entity extents and types nor assigning semantic roles associated with the predicate would reflect in this case a crucial point: namely, that in order for the typing requirements of the predicate to be satisfied, a type coercion or a metonymy (Hobbs et al., 1993; Pustejovsky, 1991; Nunberg, 1979; Egg, 2005) has taken place. The SemEval Metonymy task (Markert and Nissim, 2007) was a good attempt to annotate such metonymic relations over a larger data set. This task involved two types with their metonymic variants: categories-for-locations (e.g., placefor-people) and categories-for-organizations (e.g., organization-for-members). One of the limitations of this approach, however, is that while appropriate for these specialized metonymy relations, the annotation specification and resulting corpus are not an informative guide for extending the annotation of argument selection more broadly. In fact, the metonymy example in (1) is an instance of a much more pervasive phen</context>
</contexts>
<marker>Markert, Nissim, 2007</marker>
<rawString>K. Markert and M. Nissim. 2007. SemEval-2007 task 8: Metonymy resolution. In Eneko Agirre, Llu´ıs M`arquez, and Richard Wicentowski, editors, Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Meyers</author>
<author>R Reeves</author>
<author>C Macleod</author>
<author>R Szekely</author>
<author>V Zielinska</author>
<author>B Young</author>
<author>R Grishman</author>
</authors>
<title>The NomBank project: An interim report.</title>
<date>2004</date>
<booktitle>In HLTNAACL 2004 Workshop: Frontiers in Corpus Annotation,</booktitle>
<pages>24--31</pages>
<contexts>
<context position="5774" citStr="Meyers et al., 2004" startWordPosition="898" endWordPosition="901">ations over the data; Annotate: Annotation scheme assumes a feature set that encodes specific structural descriptions and properties of the input data; Train: Algorithm is trained over a corpus annotated with the target feature set; Test: Algorithm is tested against held-out data; Evaluate: Standardized evaluation of results; Revise: Revisit the model, annotation specification, or algorithm, in order to make the annotation more robust and reliable. Some of the current and completed annotation efforts that have undergone such a development cycle include PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004), and TimeBank (Pustejovsky et al., 2005). 3 Task Description The argument selection and coercion (ASC) task involves identifying the selectional mechanism used by the predicate over a particular argument.1 For the purposes of this task, the possible relations between the predicate and a given argument are restricted to selection and coercion. In selection, the argument NP satisfies the typing requirements of the predicate, as in (3): (3) a. The spokesman denied the statement (PROPOSITION). b. The child threw the stone (PHYSICAL OBJECT). c. The audience didn’t believe the rumor (PROPOSITION). </context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielinska, B. Young, and R. Grishman. 2004. The NomBank project: An interim report. In HLTNAACL 2004 Workshop: Frontiers in Corpus Annotation, pages 24–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Nunberg</author>
</authors>
<title>The non-uniqueness of semantic solutions: Polysemy. Linguistics and Philosophy,</title>
<date>1979</date>
<pages>3--143</pages>
<contexts>
<context position="2764" citStr="Nunberg, 1979" startWordPosition="440" endWordPosition="441">elow, where the verb report normally selects for a human in subject position, as in (1a). Notice, however, that through a metonymic interpretation, this constraint can be violated, as demonstrated in (1b). (1) a. John reported in late from Washington. b. Washington reported in late. Neither the surface annotation of entity extents and types nor assigning semantic roles associated with the predicate would reflect in this case a crucial point: namely, that in order for the typing requirements of the predicate to be satisfied, a type coercion or a metonymy (Hobbs et al., 1993; Pustejovsky, 1991; Nunberg, 1979; Egg, 2005) has taken place. The SemEval Metonymy task (Markert and Nissim, 2007) was a good attempt to annotate such metonymic relations over a larger data set. This task involved two types with their metonymic variants: categories-for-locations (e.g., placefor-people) and categories-for-organizations (e.g., organization-for-members). One of the limitations of this approach, however, is that while appropriate for these specialized metonymy relations, the annotation specification and resulting corpus are not an informative guide for extending the annotation of argument selection more broadly.</context>
</contexts>
<marker>Nunberg, 1979</marker>
<rawString>Geoffrey Nunberg. 1979. The non-uniqueness of semantic solutions: Polysemy. Linguistics and Philosophy, 3:143–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="1369" citStr="Palmer et al., 2005" startWordPosition="207" endWordPosition="210">o satisfy the verb typing. We discuss the problem in detail, describe the data preparation for the task, and analyze the results of the submissions. 1 Introduction In recent years, a number of annotation schemes that encode semantic information have been developed and used to produce data sets for training machine learning algorithms. Semantic markup schemes that have focused on annotating entity types and, more generally, word senses, have been extended to include semantic relationships between sentence elements, such as the semantic role (or label) assigned to the argument by the predicate (Palmer et al., 2005; Ruppenhofer et al., 2006; Kipper, 2005; Burchardt et al., 2006; Subirats, 2004). In this task, we take this one step further and attempt to capture the “compositional history” of the argument selection relative to the predicate. In particular, this task attempts to identify the operations of type adjustment induced by a predicate over its arguments when they do not match its selectional properties. The task is defined as follows: for each argument of a predicate, identify whether the entity in that argument position satisfies the type expected by the predicate. If not, then identify how the </context>
<context position="5743" citStr="Palmer et al., 2005" startWordPosition="892" endWordPosition="895">s derived from empirical observations over the data; Annotate: Annotation scheme assumes a feature set that encodes specific structural descriptions and properties of the input data; Train: Algorithm is trained over a corpus annotated with the target feature set; Test: Algorithm is tested against held-out data; Evaluate: Standardized evaluation of results; Revise: Revisit the model, annotation specification, or algorithm, in order to make the annotation more robust and reliable. Some of the current and completed annotation efforts that have undergone such a development cycle include PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004), and TimeBank (Pustejovsky et al., 2005). 3 Task Description The argument selection and coercion (ASC) task involves identifying the selectional mechanism used by the predicate over a particular argument.1 For the purposes of this task, the possible relations between the predicate and a given argument are restricted to selection and coercion. In selection, the argument NP satisfies the typing requirements of the predicate, as in (3): (3) a. The spokesman denied the statement (PROPOSITION). b. The child threw the stone (PHYSICAL OBJECT). c. The audience didn’t be</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>M. Palmer, D. Gildea, and P. Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pradhan</author>
<author>E Hovy</author>
<author>MS Marcus</author>
<author>M Palmer</author>
<author>L Ramshaw</author>
<author>R Weischedel</author>
</authors>
<title>Ontonotes: A unified relational semantic representation.</title>
<date>2007</date>
<booktitle>In International Conference on Semantic Computing,</booktitle>
<pages>517--526</pages>
<contexts>
<context position="8645" citStr="Pradhan et al., 2007" startWordPosition="1368" endWordPosition="1371">a set, the data construction phase was combined with the annotation phase. The data for the task was created using the following steps: 1. The verbs were selected by examining the data from the BNC, using the Sketch Engine (Kilgarriff et al., 2004) as described in (Rumshisky and Batiukova, 2008). Verbs that consistently impose semantic typing on one of their arguments in at least one of their senses (strongly coercive verbs) were included into the final data set: arrive (at), cancel, deny, finish, and hear. 2. Sense inventories were compiled for each verb, with the senses mapped to OntoNotes (Pradhan et al., 2007) whenever possible. For each sense, a set of type templates was compiled using a modification of the CPA technique (Hanks and Pustejovsky, 2005; Pustejovsky et al., 2004): every argument in the syntactic pattern associated with a given sense was assigned a type specification. Although a particular sense is often compatible with more than one semantic type for a given argument, this was never the case in our data set, where no disjoint types were tested. The coercive senses of the chosen verbs were associated with the following type templates: a. Arrive (at), sense reach a destination or goal: </context>
</contexts>
<marker>Pradhan, Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2007</marker>
<rawString>S. Pradhan, E. Hovy, MS Marcus, M. Palmer, L. Ramshaw, and R. Weischedel. 2007. Ontonotes: A unified relational semantic representation. In International Conference on Semantic Computing, 2007, pages 517–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>P Hanks</author>
<author>A Rumshisky</author>
</authors>
<title>Automated Induction of Sense in Context.</title>
<date>2004</date>
<booktitle>In COLING 2004,</booktitle>
<pages>924--931</pages>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="8815" citStr="Pustejovsky et al., 2004" startWordPosition="1395" endWordPosition="1398">examining the data from the BNC, using the Sketch Engine (Kilgarriff et al., 2004) as described in (Rumshisky and Batiukova, 2008). Verbs that consistently impose semantic typing on one of their arguments in at least one of their senses (strongly coercive verbs) were included into the final data set: arrive (at), cancel, deny, finish, and hear. 2. Sense inventories were compiled for each verb, with the senses mapped to OntoNotes (Pradhan et al., 2007) whenever possible. For each sense, a set of type templates was compiled using a modification of the CPA technique (Hanks and Pustejovsky, 2005; Pustejovsky et al., 2004): every argument in the syntactic pattern associated with a given sense was assigned a type specification. Although a particular sense is often compatible with more than one semantic type for a given argument, this was never the case in our data set, where no disjoint types were tested. The coercive senses of the chosen verbs were associated with the following type templates: a. Arrive (at), sense reach a destination or goal: HUMAN arrive at LOCATION b. Cancel, sense call off: HUMAN cancel EVENT c. Deny, sense state or maintain that something is untrue: HUMAN deny PROPOSITION d. Finish, sense </context>
</contexts>
<marker>Pustejovsky, Hanks, Rumshisky, 2004</marker>
<rawString>J. Pustejovsky, P. Hanks, and A. Rumshisky. 2004. Automated Induction of Sense in Context. In COLING 2004, Geneva, Switzerland, pages 924–931.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>R Knippen</author>
<author>J Littman</author>
<author>R Sauri</author>
</authors>
<title>Temporal and event information in natural language text.</title>
<date>2005</date>
<journal>Language Resources and Evaluation,</journal>
<volume>39</volume>
<issue>2</issue>
<contexts>
<context position="5815" citStr="Pustejovsky et al., 2005" startWordPosition="904" endWordPosition="908">tation scheme assumes a feature set that encodes specific structural descriptions and properties of the input data; Train: Algorithm is trained over a corpus annotated with the target feature set; Test: Algorithm is tested against held-out data; Evaluate: Standardized evaluation of results; Revise: Revisit the model, annotation specification, or algorithm, in order to make the annotation more robust and reliable. Some of the current and completed annotation efforts that have undergone such a development cycle include PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004), and TimeBank (Pustejovsky et al., 2005). 3 Task Description The argument selection and coercion (ASC) task involves identifying the selectional mechanism used by the predicate over a particular argument.1 For the purposes of this task, the possible relations between the predicate and a given argument are restricted to selection and coercion. In selection, the argument NP satisfies the typing requirements of the predicate, as in (3): (3) a. The spokesman denied the statement (PROPOSITION). b. The child threw the stone (PHYSICAL OBJECT). c. The audience didn’t believe the rumor (PROPOSITION). Coercion occurs when a type-shifting oper</context>
</contexts>
<marker>Pustejovsky, Knippen, Littman, Sauri, 2005</marker>
<rawString>J. Pustejovsky, R. Knippen, J. Littman, and R. Sauri. 2005. Temporal and event information in natural language text. Language Resources and Evaluation, 39(2):123–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>A Rumshisky</author>
<author>J Moszkowicz</author>
<author>O Batiukova</author>
</authors>
<title>GLML: Annotating argument selection and coercion.</title>
<date>2009</date>
<booktitle>IWCS-8: Eighth International Conference on Computational Semantics.</booktitle>
<contexts>
<context position="7917" citStr="Pustejovsky et al., 2009" startWordPosition="1246" endWordPosition="1249">s task in two phases: the data set construction phase and the annotation phase (see Fig. 2). The first phase consisted of (1) selecting the target verbs to be annotated and compiling a sense inventory for each target, and (2) data extraction and preprocessing. The prepared data was then loaded into the annotation interface. During the annotation phase, the annotation judgments were entered into the database, and an adjudicator resolved disagreements. The resulting database was then exported in an XML format. 1This task is part of a larger effort to annotate text with compositional operations (Pustejovsky et al., 2009). 28 Figure 2: Corpus Development Architecture 4.1 Data Set Construction Phase: English For the English data set, the data construction phase was combined with the annotation phase. The data for the task was created using the following steps: 1. The verbs were selected by examining the data from the BNC, using the Sketch Engine (Kilgarriff et al., 2004) as described in (Rumshisky and Batiukova, 2008). Verbs that consistently impose semantic typing on one of their arguments in at least one of their senses (strongly coercive verbs) were included into the final data set: arrive (at), cancel, deny</context>
</contexts>
<marker>Pustejovsky, Rumshisky, Moszkowicz, Batiukova, 2009</marker>
<rawString>J. Pustejovsky, A. Rumshisky, J. Moszkowicz, and O. Batiukova. 2009. GLML: Annotating argument selection and coercion. IWCS-8: Eighth International Conference on Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
</authors>
<title>The generative lexicon.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>4</issue>
<contexts>
<context position="2749" citStr="Pustejovsky, 1991" startWordPosition="438" endWordPosition="439">sider the example below, where the verb report normally selects for a human in subject position, as in (1a). Notice, however, that through a metonymic interpretation, this constraint can be violated, as demonstrated in (1b). (1) a. John reported in late from Washington. b. Washington reported in late. Neither the surface annotation of entity extents and types nor assigning semantic roles associated with the predicate would reflect in this case a crucial point: namely, that in order for the typing requirements of the predicate to be satisfied, a type coercion or a metonymy (Hobbs et al., 1993; Pustejovsky, 1991; Nunberg, 1979; Egg, 2005) has taken place. The SemEval Metonymy task (Markert and Nissim, 2007) was a good attempt to annotate such metonymic relations over a larger data set. This task involved two types with their metonymic variants: categories-for-locations (e.g., placefor-people) and categories-for-organizations (e.g., organization-for-members). One of the limitations of this approach, however, is that while appropriate for these specialized metonymy relations, the annotation specification and resulting corpus are not an informative guide for extending the annotation of argument selectio</context>
</contexts>
<marker>Pustejovsky, 1991</marker>
<rawString>J. Pustejovsky. 1991. The generative lexicon. Computational Linguistics, 17(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rumshisky</author>
<author>O Batiukova</author>
</authors>
<title>Polysemy in verbs: systematic relations between senses and their effect on annotation.</title>
<date>2008</date>
<booktitle>In COLING Workshop on Human Judgement in Computational Linguistics (HJCL-2008),</booktitle>
<location>Manchester, England.</location>
<contexts>
<context position="8320" citStr="Rumshisky and Batiukova, 2008" startWordPosition="1313" endWordPosition="1316">the database, and an adjudicator resolved disagreements. The resulting database was then exported in an XML format. 1This task is part of a larger effort to annotate text with compositional operations (Pustejovsky et al., 2009). 28 Figure 2: Corpus Development Architecture 4.1 Data Set Construction Phase: English For the English data set, the data construction phase was combined with the annotation phase. The data for the task was created using the following steps: 1. The verbs were selected by examining the data from the BNC, using the Sketch Engine (Kilgarriff et al., 2004) as described in (Rumshisky and Batiukova, 2008). Verbs that consistently impose semantic typing on one of their arguments in at least one of their senses (strongly coercive verbs) were included into the final data set: arrive (at), cancel, deny, finish, and hear. 2. Sense inventories were compiled for each verb, with the senses mapped to OntoNotes (Pradhan et al., 2007) whenever possible. For each sense, a set of type templates was compiled using a modification of the CPA technique (Hanks and Pustejovsky, 2005; Pustejovsky et al., 2004): every argument in the syntactic pattern associated with a given sense was assigned a type specification</context>
</contexts>
<marker>Rumshisky, Batiukova, 2008</marker>
<rawString>A. Rumshisky and O. Batiukova. 2008. Polysemy in verbs: systematic relations between senses and their effect on annotation. In COLING Workshop on Human Judgement in Computational Linguistics (HJCL-2008), Manchester, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rumshisky</author>
<author>P Hanks</author>
<author>C Havasi</author>
<author>J Pustejovsky</author>
</authors>
<title>Constructing a corpus-based ontology using model bias.</title>
<date>2006</date>
<booktitle>In The 19th International FLAIRS Conference, FLAIRS 2006,</booktitle>
<location>Melbourne Beach, Florida, USA.</location>
<contexts>
<context position="9729" citStr="Rumshisky et al., 2006" startWordPosition="1550" endWordPosition="1553"> coercive senses of the chosen verbs were associated with the following type templates: a. Arrive (at), sense reach a destination or goal: HUMAN arrive at LOCATION b. Cancel, sense call off: HUMAN cancel EVENT c. Deny, sense state or maintain that something is untrue: HUMAN deny PROPOSITION d. Finish, sense complete an activity: HUMAN finish EVENT e. Hear, sense perceive physical sound: HUMAN hear SOUND We used a subset of semantic types from the Brandeis Shallow Ontology (BSO), which is a shallow hierarchy of types developed as a part of the CPA effort (Hanks, 2009; Pustejovsky et al., 2004; Rumshisky et al., 2006). Types were selected for their prevalence in manually identified selection context patterns developed for several hundred English verbs. That is, they capture common semantic distinctions associated with the selectional properties of many verbs. The types used for annotation were: ABSTRACT ENTITY, ANIMATE, ARTIFACT, ATTITUDE, DOCUMENT, DRINK, EMOTION, ENTITY, EVENT, FOOD, HUMAN, HUMAN GROUP, IDEA, INFORMATION, LOCATION, OBLIGATION, ORGANIZATION, PATH, PHYSICAL OBJECT, PROPERTY, PROPOSITION, RULE, SENSATION, SOUND, SUBSTANCE, TIME PERIOD, VEHICLE This set of types is purposefully shallow and n</context>
</contexts>
<marker>Rumshisky, Hanks, Havasi, Pustejovsky, 2006</marker>
<rawString>A. Rumshisky, P. Hanks, C. Havasi, and J. Pustejovsky. 2006. Constructing a corpus-based ontology using model bias. In The 19th International FLAIRS Conference, FLAIRS 2006, Melbourne Beach, Florida, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ruppenhofer</author>
<author>M Ellsworth</author>
<author>M Petruck</author>
<author>C Johnson</author>
<author>J Scheffczyk</author>
</authors>
<title>FrameNet II: Extended Theory and Practice.</title>
<date>2006</date>
<contexts>
<context position="1395" citStr="Ruppenhofer et al., 2006" startWordPosition="211" endWordPosition="214">ping. We discuss the problem in detail, describe the data preparation for the task, and analyze the results of the submissions. 1 Introduction In recent years, a number of annotation schemes that encode semantic information have been developed and used to produce data sets for training machine learning algorithms. Semantic markup schemes that have focused on annotating entity types and, more generally, word senses, have been extended to include semantic relationships between sentence elements, such as the semantic role (or label) assigned to the argument by the predicate (Palmer et al., 2005; Ruppenhofer et al., 2006; Kipper, 2005; Burchardt et al., 2006; Subirats, 2004). In this task, we take this one step further and attempt to capture the “compositional history” of the argument selection relative to the predicate. In particular, this task attempts to identify the operations of type adjustment induced by a predicate over its arguments when they do not match its selectional properties. The task is defined as follows: for each argument of a predicate, identify whether the entity in that argument position satisfies the type expected by the predicate. If not, then identify how the entity in that position sa</context>
</contexts>
<marker>Ruppenhofer, Ellsworth, Petruck, Johnson, Scheffczyk, 2006</marker>
<rawString>J. Ruppenhofer, M. Ellsworth, M. Petruck, C. Johnson, and J. Scheffczyk. 2006. FrameNet II: Extended Theory and Practice.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Subirats</author>
</authors>
<title>FrameNet Espa˜nol. Una red sem´antica de marcos conceptuales.</title>
<date>2004</date>
<booktitle>In VI International Congress of Hispanic Linguistics,</booktitle>
<location>Leipzig.</location>
<contexts>
<context position="1450" citStr="Subirats, 2004" startWordPosition="221" endWordPosition="223">ation for the task, and analyze the results of the submissions. 1 Introduction In recent years, a number of annotation schemes that encode semantic information have been developed and used to produce data sets for training machine learning algorithms. Semantic markup schemes that have focused on annotating entity types and, more generally, word senses, have been extended to include semantic relationships between sentence elements, such as the semantic role (or label) assigned to the argument by the predicate (Palmer et al., 2005; Ruppenhofer et al., 2006; Kipper, 2005; Burchardt et al., 2006; Subirats, 2004). In this task, we take this one step further and attempt to capture the “compositional history” of the argument selection relative to the predicate. In particular, this task attempts to identify the operations of type adjustment induced by a predicate over its arguments when they do not match its selectional properties. The task is defined as follows: for each argument of a predicate, identify whether the entity in that argument position satisfies the type expected by the predicate. If not, then identify how the entity in that position satisfies the typing expected by the predicate; that is, </context>
</contexts>
<marker>Subirats, 2004</marker>
<rawString>Carlos Subirats. 2004. FrameNet Espa˜nol. Una red sem´antica de marcos conceptuales. In VI International Congress of Hispanic Linguistics, Leipzig.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Verhagen</author>
</authors>
<title>The Brandeis Annotation Tool.</title>
<date>2010</date>
<booktitle>In Language Resources and Evaluation Conference, LREC</booktitle>
<contexts>
<context position="11426" citStr="Verhagen, 2010" startWordPosition="1806" endWordPosition="1807"> from the set if the target argument was expressed as anaphor, or was not present in the sentence. The semantic head for the target grammatical relation was identified in each case. 4. Word sense disambiguation of the target predicate was performed manually on each extracted sentence, matching the target against the sense inventory and the corresponding type templates as described above. The appropriate senses were then saved into the database along with the associated type template. 5. The sentences containing coercive senses of the target verbs were loaded into the Brandeis Annotation Tool (Verhagen, 2010). Annotators were presented with a list of sentences and asked to determine whether the argument in the specified grammatical relation to the target belongs to the type associated with that sense in the corresponding template. Disagreements were resolved by adjudication. 29 Coerion Type Verb Train Test EVENT→LOCATION arrive at 38 37 ARTIFACT→EVENT cancel 35 35 finish 91 92 EVENT→PROPOSITION deny 56 54 ARTIFACT→SOUND hear 28 30 EVENT→SOUND hear 24 26 DOCUMENT→EVENT finish 39 40 Table 1: Coercions in the English data set 6. To guarantee robustness of the data, two additional steps were taken. Fi</context>
</contexts>
<marker>Verhagen, 2010</marker>
<rawString>Marc Verhagen. 2010. The Brandeis Annotation Tool. In Language Resources and Evaluation Conference, LREC 2010, Malta.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>