<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000604">
<title confidence="0.99915">
A Hearer-oriented Evaluation of Referring Expression Generation *
</title>
<author confidence="0.99812">
Imtiaz H. Khan, Kees van Deemter, Graeme Ritchie, Albert Gatt, Alexandra A. Cleland
</author>
<affiliation confidence="0.999686">
University of Aberdeen, Aberdeen, Scotland, United Kingdom
</affiliation>
<email confidence="0.991181">
{i.h.khan,k.vdeemter,g.ritchie,a.gatt,a.cleland}@abdn.ac.uk
</email>
<sectionHeader confidence="0.993677" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999909">
This paper discusses the evaluation of a
Generation of Referring Expressions algo-
rithm that takes structural ambiguity into
account. We describe an ongoing study
with human readers.
</bodyText>
<sectionHeader confidence="0.999267" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999721612903226">
In recent years, the NLG community has seen a
substantial number of studies to evaluate Gener-
ation of Referring Expressions (GRE) algorithms,
but it is still far from clear what would constitute
an optimal evaluation method. Two limitations
stand out in the bulk of existing work. Firstly,
most existing evaluations are essentially speaker-
oriented, focussing on the degree of “human-
likeness” of the generated descriptions, disre-
garding their effectiveness (e.g. Mellish and Dale
(1998), Gupta and Stent (2005), van Deemter et al.
(2006), Belz and Kilgarriff (2006), Belz and Re-
iter (2006), Paris et al. (2006), Viethen and Dale
(2006), Gatt and Belz (2008)). The limited num-
ber of exceptions to this rule indicate that the dif-
ferences between the two approaches to evaluation
can be substantial (Gatt and Belz, 2008). Sec-
ondly, most evaluations have focussed on the se-
mantic content of the generated descriptions, as
produced by the Content Determination stage of
a GRE algorithm; this means that linguistic re-
alisation (i.e. the choice of words and linguistic
constructions) is usually not addressed (exceptions
are: Stone and Webber (1998), Krahmer and The-
une (2002), Siddharthan and Copestake (2004)).
Our aim is to build GRE algorithms that produce
referring expressions that are of optimal benefit to
a hearer. That is, we are interested in generating
descriptions that are easy to read and understand.
But the readability and intelligibility of a descrip-
tion can crucially depend on the way in which it is
</bodyText>
<note confidence="0.542507">
* This work is supported by a University of Aberdeen
Sixth Century Studentship, and EPSRC grant EP/E011764/1.
</note>
<bodyText confidence="0.999544888888889">
worded. This happens particularly when there is
potential for misunderstanding, as can happen in
the case of attachment and scope ambiguities.
Suppose, for example, one wants to make it
clear that all radical students and all radical teach-
ers are in agreement with a certain idea. It might
be risky to express this as ‘the radical students and
teachers are agreed’, since the reader1 might be
inclined to interpret this as pertaining to all teach-
ers rather than only the radical ones. For this rea-
son, a GRE program might opt for the longer noun
phrase ‘the radical students and the radical teach-
ers’. But because this expression is lengthier, the
choice involves a compromise between compre-
hensibiliity and brevity, a special case of a diffi-
cult trade-off that is typical of generation as well
as interpretation of language (van Deemter, 2004).
We previously reported the design of an algo-
rithm (based on an earlier work on expressions re-
ferring to sets (Gatt, 2007)), which was derived
from experiments in which readers were asked to
express their preference between different descrip-
tions and to respond to instructions which used a
variety of phrasings (Khan et al., 2008). Here we
discuss the issues that arise when such an algo-
rithm is evaluated in terms of its benefits for read-
ers.
</bodyText>
<sectionHeader confidence="0.486334" genericHeader="method">
2 Summary of the algorithm
</sectionHeader>
<bodyText confidence="0.9873139">
In order to study specific data, we have focussed
on the construction illustrated in Section 1 above:
potentially ambiguous Noun Phrases of the gen-
eral form the Adj Nouni and Noun�. For such
phrases, there are potentially two interpretations:
wide scope (Adj modifies both Nouni and Noun�)
or narrow scope (Adj modifies Nouni but not
Noun�).
Our algorithm starts from an unambiguous set-
theoretic formula over lexical items (i.e. words
</bodyText>
<footnote confidence="0.72524175">
1In this paper, we use the word reader and hearer inter-
changeably.
Proceedings of the 12th European Workshop on Natural Language Generation, pages 98–101,
Athens, Greece, 30 – 31 March 2009. c�2009 Association for Computational Linguistics
</footnote>
<page confidence="0.996432">
98
</page>
<bodyText confidence="0.999534930232558">
have already been chosen), and thus has to choose
between a number of different realisations. The
possible phrasings for the wide scope meaning are:
(1) the Adj Noun1 and Noun2, (2) the Adj Noun2
and Noun1, (3) the Adj Noun1 and the Adj Noun2,
and (4) the Adj Noun2 and the Adj Noun1. For nar-
row scope, the possibilities are: (1) the Adj Noun1
and Noun2, (2) the Noun2 and Adj Noun1, (3) the
Adj Noun1 and the Noun2, and (4) the Noun2 and
the Adj Noun1. For our purposes, (1) and (2) are
designated as ‘brief’, (3) and (4) as ‘non-brief’
(that is, ’brevity’ has a specialised sense involv-
ing the presence/absence of ‘the’ and possibly Adj
before the second Noun). Importantly, the ‘non-
brief’ expressions are syntactically unambiguous,
but the ‘brief’ NPs are potentially ambiguous, and
hence are the focus of attention in this work.
Our algorithm is based on certain specific hy-
potheses (from the earlier experiments) which
make crucial use of corpus data concerning the
frequency of two types of collocations: the col-
location between an adjective and a noun, and the
collocation between two nouns. At a broader level,
we hypothesise: the most likely reading of an NP
can be predicted using corpus data (Word Sketches
(Kilgarriff, 2003)). The more specific hypotheses
derive from earlier work by Kilgarriff (2003) and
Chantree et al. (2006), and were further developed
and tested in our previous experiments. The cen-
tral idea is that this statistical information can be
used to predict a ‘most likely’ scoping (and hence
interpretation) for the adjective in the ‘brief’ (i.e.
potentially ambiguous) NPs. We define an NP to
be predictable if our model predicts a single read-
ing for it; otherwise it is unpredictable. Hence, all
‘non-brief’ NPs are predictable (being unambigu-
ous), but only some of the ‘brief’ ones are pre-
dictable.
In a nutshell, the model underlying our algo-
rithm prefers predictable expressions to unpre-
dictable ones, but if several of the expressions are
predictable then brief expressions are preferred
over non-brief.
</bodyText>
<sectionHeader confidence="0.856361" genericHeader="method">
3 Aims of the study
</sectionHeader>
<bodyText confidence="0.996435236363636">
We want to find out whether our generator
makes the best possible choices (for hearers) from
amongst the different ways in which a given de-
scription can be realised. But although our al-
gorithm uses sophisticated strategies for avoiding
noun phrases that it believes to be liable to mis-
understanding, misunderstandings cannot be ruled
out, and if a hearer misunderstands a noun phrase
then secondary aspects such as reading (and/or
comprehension) speed are of little consequence.
We therefore plan first to find out the likelihood of
misunderstanding. For this reason, we will report
on the degree of accuracy, as a percentage of times
that a participant’s understanding of an expression
that we label as predictable fails to match the in-
terpretation assigned by our model. Additionally,
we shall statistically test two hypotheses:
Comprehension Accuracy 1: Predictable ex-
pressions are more often interpreted in
agreement than in disagreement with the
model.
Comprehension Accuracy 2: There is more
agreement among participants on the inter-
pretation of predictable expressions than of
unpredictable expressions.
We will not only test the comprehensibility of the
expressions generated by our algorithm, but their
readability and intelligibility as well. This is nec-
essary because the experiments which led to the
algorithm design considered only certain aspects
of the hearer’s reaction to NPs (e.g. metalinguistic
judgements about a participant’s preferences) and
we wish to check these comprehensibility/brevity
facets from a different, perhaps psycholinguisti-
cally more valid, perspective. It is also necessary
because avoidance of misunderstandings is not the
only decisive factor: if several of the expressions
are predictable then our algorithm chooses be-
tween them by preferring brevity. But why is brief
better than non-brief? Taking readability and intel-
ligibility together as ‘processing speed’, our third
hypothesis is:
Processing speed: Subjects process
predictable brief expressions more
quickly than predictable non-brief ones.
Confirmation of this hypothesis would be a strong
indication that our algorithm is on the right track,
particularly if the degree of accuracy (see above)
turns out to be high. Processing speed is a com-
plex concept, but we could decompose it as ‘read-
ing speed’ and ‘comprehension speed’, permitting
us to examine reading and comprehension sepa-
rately. We intend to see what evidence there is for
the following additional propositions, which will
be tested solely to aid our understanding.
</bodyText>
<page confidence="0.990526">
99
</page>
<listItem confidence="0.683379266666667">
Reading Speed:
RS1: Subjects read predictable brief NPs more
quickly than unpredictable brief ones.
RS2: Subjects read unpredictable brief NPs more
quickly than predictable non-brief ones.
RS3: Subjects read predictable brief NPs more
quickly than predictable non-brief ones.
Comprehension Speed:
CS1: Subjects comprehend predictable brief NPs
more quickly than unpredictable brief ones.
CS2: Subjects comprehend predictable non-brief
NPs more quickly than unpredictable brief ones.
CS3: Subjects do not comprehend predictable
non-brief NPs more quickly than predictable brief
ones.
</listItem>
<bodyText confidence="0.71445875">
(Remember that, in our restricted set of NPs, a
phrase cannot be both ‘unpredictable’ and ‘non-
brief’.) Rejection of any of these statements will
not count against our algorithm.
</bodyText>
<sectionHeader confidence="0.927303" genericHeader="method">
4 Sketch of experimental procedure
</sectionHeader>
<bodyText confidence="0.999736714285714">
Participants will be presented with a sequence of
trials (on a computer screen), each of which con-
sists of a lead-in sentence followed by a target sen-
tence and a comprehension question that relates to
the two sentences together. The target sentence
might for example say ‘the radical students and
teachers were waving their hands’. The compre-
hension question in this case could be ‘Were the
moderate teachers waving their hands?’. As both
the target sentence and the comprehension ques-
tion make use of definite NPs (e.g. ‘the moderate
teachers’), it is necessary to ensure any presuppo-
sitions about the existence of the referent set are
met, without biasing the answer. For this reason,
the target sentence is preceded by a lead-in sen-
tence to establish the existence of the sets within
the discourse (here, ‘there were radical and mod-
erate people in a rally’).
Given this set-up we are confident that we
can identify, from a participant’s yes/no answer,
whether the NP in the target sentence was assigned
a narrow-scope or a wide-scope reading for the ad-
jective. The computer will record the participant’s
response as well as the length of time that the par-
ticipant took to answer the question. We will use
Linger2 for presentation of stimuli. Pilots sug-
gest that the complexity of the trials makes it ad-
visable to use masked sentence-based self-paced
</bodyText>
<footnote confidence="0.801824">
2http://tedlab.mit.edu/˜dr/Linger/
</footnote>
<bodyText confidence="0.999710666666667">
reading, in which every press of the space bar re-
veals the next sentence and the previous sentence
is replaced by dashes.
The choice of nouns and adjectives (to construct
NPs) is motivated by the fact that there is a bal-
anced distribution of NPs in each of the follow-
ing three classes. Wide scope class is the one for
which our model predicts a wide-scope reading;
narrow scope class is the one for which our model
predicts a narrow-scope reading; and ambiguous
class is the one for which our model fails to pre-
dict a single reading (Khan et al., 2008).
</bodyText>
<sectionHeader confidence="0.824263" genericHeader="method">
5 Issues emerging from this study
</sectionHeader>
<bodyText confidence="0.940416263157895">
The design of this experiment raised some difficult
questions, some quite unexpected:
1. The quality of the output of a generation al-
gorithm might appear to be a simple and well-
understood concept. However, output quality is
multi-faceted, because an expression may be easy
to read but difficult to process semantically, or the
other way round. A thorough output evaluation
should address both aspects of quality, in our view.
2. If both reading and understanding are ad-
dressed, this raises the question of how these
two dimensions should be traded off against each
other. If one algorithm’s output was read more
quickly than that of another, but understood more
slowly than the second, which of the two should be
preferred? Perhaps there is a legitimate role here
for metalinguistic judgments after all, in which
participants are asked to express their preference
between expressions (see Paraboni et al. (2006) for
</bodyText>
<listItem confidence="0.838753764705882">
discussion)? An alternative point of view is that
these questions are impossible to answer indepen-
dent of a realistic setting in which participants ut-
ter sentences with a concrete communicative pur-
pose in mind. If utterances were made in order to
accomplish a concrete task (e.g., to win a game)
then task-based evaluation would be possible.
3. Even though this paper has not focussed on de-
tails of experimental design and analysis, one diffi-
culty is worth mentioning: given the grammatical
options between which the generator is choosing,
only three types of situations are represented: a de-
scription can be brief and predictable (e.g. using
‘the old men and women’ to convey wide scope,
since the adjective is predicted by our algorithm
to have wide scope), brief and unpredictable (e.g.
‘the rowing boats and ships’ for wide scope, given
</listItem>
<page confidence="0.985533">
100
</page>
<bodyText confidence="0.999858">
a prediction of narrow scope), or non-brief and
predictable (e.g. ‘the old men and the old women’
for wide scope). It might appear that there exists
a fourth option: non-brief and unpredictable. But
this is ruled out by our technical sense of ‘non-
brief’: as noted earlier, ‘non-brief’ NPs do not
have the scope ambiguity. Because of this “miss-
ing cell”, it will not be possible to analyse our data
using an ANOVA test, which would have automat-
ically taken care of all possible interactions be-
tween comprehensibility and brevity. A number
of different tests will be used instead, with Bon-
ferroni corrections where necessary.
</bodyText>
<sectionHeader confidence="0.999169" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999887714285714">
Human-based evaluation is gaining considerable
popularity in the NLG community. Whereas eval-
uation of GRE has mostly been speaker-oriented,
the present paper has explored a plan for an ex-
perimental hearer-oriented evaluation. The main
conclusion is that hearer-based evaluation is diffi-
cult because the quality of a generated expression
can be measured in different ways, whose results
cannot be assumed to match. One factor we have
not examined is the notion offluency: it is possible
that our algorithm will sometimes choose a word
order (e.g. ‘the women and old men’) that is rela-
tively infrequent, and therefore lacking in fluency.
Such situations might lead to longer reading times.
</bodyText>
<sectionHeader confidence="0.993133" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.969626726027397">
A. Belz and A. Kilgarriff. 2006. Shared-task evalu-
ations in HLT: Lessons for NLG. In Proceedings
of the 4th International Conference on Natural Lan-
guage Generation, pages 133–135.
A. Belz and E. Reiter. 2006. Comparing automatic
and human evaluation of NLG systems. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 313–320, Trento, Italy, 3-7 April.
F. Chantree, B. Nuseibeh, A. de Roeck, and A. Willis.
2006. Identifying nocuous ambiguities in require-
ments specifications. In Proceedings of 14th IEEE
International Requirements Engineering conference
(RE’06), Minneapolis/St. Paul, Minnesota, U.S.A.
A. Gatt and A. Belz. 2008. Attribute selection for re-
ferring expression generation: New algorithms and
evaluation methods. In Proceedings of the 5th Inter-
national Conference on NLG.
A. Gatt. 2007. Generating Coherent References to
Multiple Entities. Ph.D. thesis, University of Ab-
erdeen, Aberdeen, Scotland.
S. Gupta and A. Stent. 2005. Automatic evaluation
of referring expression generation using corpora. In
Proceedings of the Workshop on Using Corpora for
Natural Language Generation, pages 1–6.
I. H. Khan, K. van Deemter, and G. Ritchie. 2008.
Generation of referring expressions: Managing
structural ambiguities. In Proceedings of the 22nd
International Conference on Computational Lin-
guistics (COLING-8), pages 433–440, Manchester.
A. Kilgarriff. 2003. Thesauruses for natural language
processing. In Proceedings of NLP-KE, pages 5–13,
Beijing, China.
E. Krahmer and M. Theune. 2002. Efficient context-
sensitive generation of referring expressions. In
K. van Deemter and R. Kibble, editors, Information
Sharing: Reference and Presupposition in Language
Generation and Interpretation, CSLI Publications,
pages 223–264.
C. Mellish and R. Dale. 1998. Evaluation in the
context of natural language generation. Computer
Speech and Language, 12(4):349–373.
I. Paraboni, J. Masthoff, and K. van Deemter. 2006.
Overspecified reference in hierarchical domain:
measuring the benefits for readers. In Proceedings
of the Fourth International Conference on Natural
Language Generation(INLG), pages 55–62.
C. Paris, N. Colineau, and R. Wilkinson. 2006. Eval-
uations of NLG systems: Common corpus and tasks
or common dimensions and metrics? In Proceed-
ings of the 4th International Conference on Natural
Language Generation, pages 127–129.
A. Siddharthan and A. Copestake. 2004. Generating
referring expressions in open domains. In Proceed-
ings of the 42nd Meeting of the Association for Com-
putational Linguistics Annual Conference (ACL-04).
M. Stone and B. Webber. 1998. Textual economy
through close coupling of syntax and semantics. In
Proceedings of the Ninth International Workshop on
Natural Language Generation, pages 178–187, New
Brunswick, New Jersey.
K. van Deemter, I. van der Sluis, and A. Gatt. 2006.
Building a semantically transparent corpus for the
generation of referring expressions. In Proceedings
of the 4th International Conference on Natural Lan-
guage Generation, pages 130–132.
K. van Deemter. 2004. Towards a probabilistic version
of bidirectional OT syntax and semantics. Journal
of Semantics, 21(3):251–281.
J. Viethen and R. Dale. 2006. Towards the evaluation
of referring expression generation. In Proceedings
of the 4th Australasian Language Technology Work-
shop, pages 115–122, Sydney, Australia.
</reference>
<page confidence="0.998608">
101
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.794391">
<title confidence="0.996071">Hearer-oriented Evaluation of Referring Expression Generation</title>
<author confidence="0.912515">Imtiaz H Khan</author>
<author confidence="0.912515">Kees van_Deemter</author>
<author confidence="0.912515">Graeme Ritchie</author>
<author confidence="0.912515">Albert Gatt</author>
<author confidence="0.912515">A Alexandra</author>
<affiliation confidence="0.833571">University of Aberdeen, Aberdeen, Scotland, United</affiliation>
<abstract confidence="0.998437666666667">This paper discusses the evaluation of a Generation of Referring Expressions algorithm that takes structural ambiguity into account. We describe an ongoing study with human readers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Belz</author>
<author>A Kilgarriff</author>
</authors>
<title>Shared-task evaluations in HLT: Lessons for NLG.</title>
<date>2006</date>
<booktitle>In Proceedings of the 4th International Conference on Natural Language Generation,</booktitle>
<pages>133--135</pages>
<contexts>
<context position="1039" citStr="Belz and Kilgarriff (2006)" startWordPosition="142" endWordPosition="145"> describe an ongoing study with human readers. 1 Introduction In recent years, the NLG community has seen a substantial number of studies to evaluate Generation of Referring Expressions (GRE) algorithms, but it is still far from clear what would constitute an optimal evaluation method. Two limitations stand out in the bulk of existing work. Firstly, most existing evaluations are essentially speakeroriented, focussing on the degree of “humanlikeness” of the generated descriptions, disregarding their effectiveness (e.g. Mellish and Dale (1998), Gupta and Stent (2005), van Deemter et al. (2006), Belz and Kilgarriff (2006), Belz and Reiter (2006), Paris et al. (2006), Viethen and Dale (2006), Gatt and Belz (2008)). The limited number of exceptions to this rule indicate that the differences between the two approaches to evaluation can be substantial (Gatt and Belz, 2008). Secondly, most evaluations have focussed on the semantic content of the generated descriptions, as produced by the Content Determination stage of a GRE algorithm; this means that linguistic realisation (i.e. the choice of words and linguistic constructions) is usually not addressed (exceptions are: Stone and Webber (1998), Krahmer and Theune (2</context>
</contexts>
<marker>Belz, Kilgarriff, 2006</marker>
<rawString>A. Belz and A. Kilgarriff. 2006. Shared-task evaluations in HLT: Lessons for NLG. In Proceedings of the 4th International Conference on Natural Language Generation, pages 133–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Belz</author>
<author>E Reiter</author>
</authors>
<title>Comparing automatic and human evaluation of NLG systems.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>313--320</pages>
<location>Trento,</location>
<contexts>
<context position="1063" citStr="Belz and Reiter (2006)" startWordPosition="146" endWordPosition="150">ith human readers. 1 Introduction In recent years, the NLG community has seen a substantial number of studies to evaluate Generation of Referring Expressions (GRE) algorithms, but it is still far from clear what would constitute an optimal evaluation method. Two limitations stand out in the bulk of existing work. Firstly, most existing evaluations are essentially speakeroriented, focussing on the degree of “humanlikeness” of the generated descriptions, disregarding their effectiveness (e.g. Mellish and Dale (1998), Gupta and Stent (2005), van Deemter et al. (2006), Belz and Kilgarriff (2006), Belz and Reiter (2006), Paris et al. (2006), Viethen and Dale (2006), Gatt and Belz (2008)). The limited number of exceptions to this rule indicate that the differences between the two approaches to evaluation can be substantial (Gatt and Belz, 2008). Secondly, most evaluations have focussed on the semantic content of the generated descriptions, as produced by the Content Determination stage of a GRE algorithm; this means that linguistic realisation (i.e. the choice of words and linguistic constructions) is usually not addressed (exceptions are: Stone and Webber (1998), Krahmer and Theune (2002), Siddharthan and Co</context>
</contexts>
<marker>Belz, Reiter, 2006</marker>
<rawString>A. Belz and E. Reiter. 2006. Comparing automatic and human evaluation of NLG systems. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, pages 313–320, Trento, Italy, 3-7 April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Chantree</author>
<author>B Nuseibeh</author>
<author>A de Roeck</author>
<author>A Willis</author>
</authors>
<title>Identifying nocuous ambiguities in requirements specifications.</title>
<date>2006</date>
<booktitle>In Proceedings of 14th IEEE International Requirements Engineering conference (RE’06),</booktitle>
<location>Minneapolis/St. Paul, Minnesota, U.S.A.</location>
<marker>Chantree, Nuseibeh, de Roeck, Willis, 2006</marker>
<rawString>F. Chantree, B. Nuseibeh, A. de Roeck, and A. Willis. 2006. Identifying nocuous ambiguities in requirements specifications. In Proceedings of 14th IEEE International Requirements Engineering conference (RE’06), Minneapolis/St. Paul, Minnesota, U.S.A.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gatt</author>
<author>A Belz</author>
</authors>
<title>Attribute selection for referring expression generation: New algorithms and evaluation methods.</title>
<date>2008</date>
<booktitle>In Proceedings of the 5th International Conference on NLG.</booktitle>
<contexts>
<context position="1131" citStr="Gatt and Belz (2008)" startWordPosition="159" endWordPosition="162">as seen a substantial number of studies to evaluate Generation of Referring Expressions (GRE) algorithms, but it is still far from clear what would constitute an optimal evaluation method. Two limitations stand out in the bulk of existing work. Firstly, most existing evaluations are essentially speakeroriented, focussing on the degree of “humanlikeness” of the generated descriptions, disregarding their effectiveness (e.g. Mellish and Dale (1998), Gupta and Stent (2005), van Deemter et al. (2006), Belz and Kilgarriff (2006), Belz and Reiter (2006), Paris et al. (2006), Viethen and Dale (2006), Gatt and Belz (2008)). The limited number of exceptions to this rule indicate that the differences between the two approaches to evaluation can be substantial (Gatt and Belz, 2008). Secondly, most evaluations have focussed on the semantic content of the generated descriptions, as produced by the Content Determination stage of a GRE algorithm; this means that linguistic realisation (i.e. the choice of words and linguistic constructions) is usually not addressed (exceptions are: Stone and Webber (1998), Krahmer and Theune (2002), Siddharthan and Copestake (2004)). Our aim is to build GRE algorithms that produce ref</context>
</contexts>
<marker>Gatt, Belz, 2008</marker>
<rawString>A. Gatt and A. Belz. 2008. Attribute selection for referring expression generation: New algorithms and evaluation methods. In Proceedings of the 5th International Conference on NLG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gatt</author>
</authors>
<title>Generating Coherent References to Multiple Entities.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Aberdeen,</institution>
<location>Aberdeen, Scotland.</location>
<contexts>
<context position="3064" citStr="Gatt, 2007" startWordPosition="481" endWordPosition="482"> agreed’, since the reader1 might be inclined to interpret this as pertaining to all teachers rather than only the radical ones. For this reason, a GRE program might opt for the longer noun phrase ‘the radical students and the radical teachers’. But because this expression is lengthier, the choice involves a compromise between comprehensibiliity and brevity, a special case of a difficult trade-off that is typical of generation as well as interpretation of language (van Deemter, 2004). We previously reported the design of an algorithm (based on an earlier work on expressions referring to sets (Gatt, 2007)), which was derived from experiments in which readers were asked to express their preference between different descriptions and to respond to instructions which used a variety of phrasings (Khan et al., 2008). Here we discuss the issues that arise when such an algorithm is evaluated in terms of its benefits for readers. 2 Summary of the algorithm In order to study specific data, we have focussed on the construction illustrated in Section 1 above: potentially ambiguous Noun Phrases of the general form the Adj Nouni and Noun�. For such phrases, there are potentially two interpretations: wide sc</context>
</contexts>
<marker>Gatt, 2007</marker>
<rawString>A. Gatt. 2007. Generating Coherent References to Multiple Entities. Ph.D. thesis, University of Aberdeen, Aberdeen, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gupta</author>
<author>A Stent</author>
</authors>
<title>Automatic evaluation of referring expression generation using corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the Workshop on Using Corpora for Natural Language Generation,</booktitle>
<pages>1--6</pages>
<contexts>
<context position="984" citStr="Gupta and Stent (2005)" startWordPosition="133" endWordPosition="136">hm that takes structural ambiguity into account. We describe an ongoing study with human readers. 1 Introduction In recent years, the NLG community has seen a substantial number of studies to evaluate Generation of Referring Expressions (GRE) algorithms, but it is still far from clear what would constitute an optimal evaluation method. Two limitations stand out in the bulk of existing work. Firstly, most existing evaluations are essentially speakeroriented, focussing on the degree of “humanlikeness” of the generated descriptions, disregarding their effectiveness (e.g. Mellish and Dale (1998), Gupta and Stent (2005), van Deemter et al. (2006), Belz and Kilgarriff (2006), Belz and Reiter (2006), Paris et al. (2006), Viethen and Dale (2006), Gatt and Belz (2008)). The limited number of exceptions to this rule indicate that the differences between the two approaches to evaluation can be substantial (Gatt and Belz, 2008). Secondly, most evaluations have focussed on the semantic content of the generated descriptions, as produced by the Content Determination stage of a GRE algorithm; this means that linguistic realisation (i.e. the choice of words and linguistic constructions) is usually not addressed (excepti</context>
</contexts>
<marker>Gupta, Stent, 2005</marker>
<rawString>S. Gupta and A. Stent. 2005. Automatic evaluation of referring expression generation using corpora. In Proceedings of the Workshop on Using Corpora for Natural Language Generation, pages 1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Khan</author>
<author>K van Deemter</author>
<author>G Ritchie</author>
</authors>
<title>Generation of referring expressions: Managing structural ambiguities.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (COLING-8),</booktitle>
<pages>433--440</pages>
<location>Manchester.</location>
<marker>Khan, van Deemter, Ritchie, 2008</marker>
<rawString>I. H. Khan, K. van Deemter, and G. Ritchie. 2008. Generation of referring expressions: Managing structural ambiguities. In Proceedings of the 22nd International Conference on Computational Linguistics (COLING-8), pages 433–440, Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
</authors>
<title>Thesauruses for natural language processing.</title>
<date>2003</date>
<booktitle>In Proceedings of NLP-KE,</booktitle>
<pages>5--13</pages>
<location>Beijing, China.</location>
<contexts>
<context position="5328" citStr="Kilgarriff, 2003" startWordPosition="861" endWordPosition="862">e’ and possibly Adj before the second Noun). Importantly, the ‘nonbrief’ expressions are syntactically unambiguous, but the ‘brief’ NPs are potentially ambiguous, and hence are the focus of attention in this work. Our algorithm is based on certain specific hypotheses (from the earlier experiments) which make crucial use of corpus data concerning the frequency of two types of collocations: the collocation between an adjective and a noun, and the collocation between two nouns. At a broader level, we hypothesise: the most likely reading of an NP can be predicted using corpus data (Word Sketches (Kilgarriff, 2003)). The more specific hypotheses derive from earlier work by Kilgarriff (2003) and Chantree et al. (2006), and were further developed and tested in our previous experiments. The central idea is that this statistical information can be used to predict a ‘most likely’ scoping (and hence interpretation) for the adjective in the ‘brief’ (i.e. potentially ambiguous) NPs. We define an NP to be predictable if our model predicts a single reading for it; otherwise it is unpredictable. Hence, all ‘non-brief’ NPs are predictable (being unambiguous), but only some of the ‘brief’ ones are predictable. In a </context>
</contexts>
<marker>Kilgarriff, 2003</marker>
<rawString>A. Kilgarriff. 2003. Thesauruses for natural language processing. In Proceedings of NLP-KE, pages 5–13, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Krahmer</author>
<author>M Theune</author>
</authors>
<title>Efficient contextsensitive generation of referring expressions.</title>
<date>2002</date>
<booktitle>Information Sharing: Reference and Presupposition in Language Generation and Interpretation, CSLI Publications,</booktitle>
<pages>223--264</pages>
<editor>In K. van Deemter and R. Kibble, editors,</editor>
<contexts>
<context position="1643" citStr="Krahmer and Theune (2002)" startWordPosition="240" endWordPosition="244">and Kilgarriff (2006), Belz and Reiter (2006), Paris et al. (2006), Viethen and Dale (2006), Gatt and Belz (2008)). The limited number of exceptions to this rule indicate that the differences between the two approaches to evaluation can be substantial (Gatt and Belz, 2008). Secondly, most evaluations have focussed on the semantic content of the generated descriptions, as produced by the Content Determination stage of a GRE algorithm; this means that linguistic realisation (i.e. the choice of words and linguistic constructions) is usually not addressed (exceptions are: Stone and Webber (1998), Krahmer and Theune (2002), Siddharthan and Copestake (2004)). Our aim is to build GRE algorithms that produce referring expressions that are of optimal benefit to a hearer. That is, we are interested in generating descriptions that are easy to read and understand. But the readability and intelligibility of a description can crucially depend on the way in which it is * This work is supported by a University of Aberdeen Sixth Century Studentship, and EPSRC grant EP/E011764/1. worded. This happens particularly when there is potential for misunderstanding, as can happen in the case of attachment and scope ambiguities. Sup</context>
</contexts>
<marker>Krahmer, Theune, 2002</marker>
<rawString>E. Krahmer and M. Theune. 2002. Efficient contextsensitive generation of referring expressions. In K. van Deemter and R. Kibble, editors, Information Sharing: Reference and Presupposition in Language Generation and Interpretation, CSLI Publications, pages 223–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Mellish</author>
<author>R Dale</author>
</authors>
<title>Evaluation in the context of natural language generation.</title>
<date>1998</date>
<journal>Computer Speech and Language,</journal>
<volume>12</volume>
<issue>4</issue>
<contexts>
<context position="960" citStr="Mellish and Dale (1998)" startWordPosition="129" endWordPosition="132">rring Expressions algorithm that takes structural ambiguity into account. We describe an ongoing study with human readers. 1 Introduction In recent years, the NLG community has seen a substantial number of studies to evaluate Generation of Referring Expressions (GRE) algorithms, but it is still far from clear what would constitute an optimal evaluation method. Two limitations stand out in the bulk of existing work. Firstly, most existing evaluations are essentially speakeroriented, focussing on the degree of “humanlikeness” of the generated descriptions, disregarding their effectiveness (e.g. Mellish and Dale (1998), Gupta and Stent (2005), van Deemter et al. (2006), Belz and Kilgarriff (2006), Belz and Reiter (2006), Paris et al. (2006), Viethen and Dale (2006), Gatt and Belz (2008)). The limited number of exceptions to this rule indicate that the differences between the two approaches to evaluation can be substantial (Gatt and Belz, 2008). Secondly, most evaluations have focussed on the semantic content of the generated descriptions, as produced by the Content Determination stage of a GRE algorithm; this means that linguistic realisation (i.e. the choice of words and linguistic constructions) is usuall</context>
</contexts>
<marker>Mellish, Dale, 1998</marker>
<rawString>C. Mellish and R. Dale. 1998. Evaluation in the context of natural language generation. Computer Speech and Language, 12(4):349–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Paraboni</author>
<author>J Masthoff</author>
<author>K van Deemter</author>
</authors>
<title>Overspecified reference in hierarchical domain: measuring the benefits for readers.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fourth International Conference on Natural Language Generation(INLG),</booktitle>
<pages>55--62</pages>
<marker>Paraboni, Masthoff, van Deemter, 2006</marker>
<rawString>I. Paraboni, J. Masthoff, and K. van Deemter. 2006. Overspecified reference in hierarchical domain: measuring the benefits for readers. In Proceedings of the Fourth International Conference on Natural Language Generation(INLG), pages 55–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Paris</author>
<author>N Colineau</author>
<author>R Wilkinson</author>
</authors>
<title>Evaluations of NLG systems: Common corpus and tasks or common dimensions and metrics?</title>
<date>2006</date>
<booktitle>In Proceedings of the 4th International Conference on Natural Language Generation,</booktitle>
<pages>127--129</pages>
<contexts>
<context position="1084" citStr="Paris et al. (2006)" startWordPosition="151" endWordPosition="154">roduction In recent years, the NLG community has seen a substantial number of studies to evaluate Generation of Referring Expressions (GRE) algorithms, but it is still far from clear what would constitute an optimal evaluation method. Two limitations stand out in the bulk of existing work. Firstly, most existing evaluations are essentially speakeroriented, focussing on the degree of “humanlikeness” of the generated descriptions, disregarding their effectiveness (e.g. Mellish and Dale (1998), Gupta and Stent (2005), van Deemter et al. (2006), Belz and Kilgarriff (2006), Belz and Reiter (2006), Paris et al. (2006), Viethen and Dale (2006), Gatt and Belz (2008)). The limited number of exceptions to this rule indicate that the differences between the two approaches to evaluation can be substantial (Gatt and Belz, 2008). Secondly, most evaluations have focussed on the semantic content of the generated descriptions, as produced by the Content Determination stage of a GRE algorithm; this means that linguistic realisation (i.e. the choice of words and linguistic constructions) is usually not addressed (exceptions are: Stone and Webber (1998), Krahmer and Theune (2002), Siddharthan and Copestake (2004)). Our </context>
</contexts>
<marker>Paris, Colineau, Wilkinson, 2006</marker>
<rawString>C. Paris, N. Colineau, and R. Wilkinson. 2006. Evaluations of NLG systems: Common corpus and tasks or common dimensions and metrics? In Proceedings of the 4th International Conference on Natural Language Generation, pages 127–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Siddharthan</author>
<author>A Copestake</author>
</authors>
<title>Generating referring expressions in open domains.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics Annual Conference (ACL-04).</booktitle>
<contexts>
<context position="1677" citStr="Siddharthan and Copestake (2004)" startWordPosition="245" endWordPosition="248"> and Reiter (2006), Paris et al. (2006), Viethen and Dale (2006), Gatt and Belz (2008)). The limited number of exceptions to this rule indicate that the differences between the two approaches to evaluation can be substantial (Gatt and Belz, 2008). Secondly, most evaluations have focussed on the semantic content of the generated descriptions, as produced by the Content Determination stage of a GRE algorithm; this means that linguistic realisation (i.e. the choice of words and linguistic constructions) is usually not addressed (exceptions are: Stone and Webber (1998), Krahmer and Theune (2002), Siddharthan and Copestake (2004)). Our aim is to build GRE algorithms that produce referring expressions that are of optimal benefit to a hearer. That is, we are interested in generating descriptions that are easy to read and understand. But the readability and intelligibility of a description can crucially depend on the way in which it is * This work is supported by a University of Aberdeen Sixth Century Studentship, and EPSRC grant EP/E011764/1. worded. This happens particularly when there is potential for misunderstanding, as can happen in the case of attachment and scope ambiguities. Suppose, for example, one wants to ma</context>
</contexts>
<marker>Siddharthan, Copestake, 2004</marker>
<rawString>A. Siddharthan and A. Copestake. 2004. Generating referring expressions in open domains. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics Annual Conference (ACL-04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stone</author>
<author>B Webber</author>
</authors>
<title>Textual economy through close coupling of syntax and semantics.</title>
<date>1998</date>
<booktitle>In Proceedings of the Ninth International Workshop on Natural Language Generation,</booktitle>
<pages>178--187</pages>
<location>New Brunswick, New Jersey.</location>
<contexts>
<context position="1616" citStr="Stone and Webber (1998)" startWordPosition="236" endWordPosition="239">mter et al. (2006), Belz and Kilgarriff (2006), Belz and Reiter (2006), Paris et al. (2006), Viethen and Dale (2006), Gatt and Belz (2008)). The limited number of exceptions to this rule indicate that the differences between the two approaches to evaluation can be substantial (Gatt and Belz, 2008). Secondly, most evaluations have focussed on the semantic content of the generated descriptions, as produced by the Content Determination stage of a GRE algorithm; this means that linguistic realisation (i.e. the choice of words and linguistic constructions) is usually not addressed (exceptions are: Stone and Webber (1998), Krahmer and Theune (2002), Siddharthan and Copestake (2004)). Our aim is to build GRE algorithms that produce referring expressions that are of optimal benefit to a hearer. That is, we are interested in generating descriptions that are easy to read and understand. But the readability and intelligibility of a description can crucially depend on the way in which it is * This work is supported by a University of Aberdeen Sixth Century Studentship, and EPSRC grant EP/E011764/1. worded. This happens particularly when there is potential for misunderstanding, as can happen in the case of attachment</context>
</contexts>
<marker>Stone, Webber, 1998</marker>
<rawString>M. Stone and B. Webber. 1998. Textual economy through close coupling of syntax and semantics. In Proceedings of the Ninth International Workshop on Natural Language Generation, pages 178–187, New Brunswick, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K van Deemter</author>
<author>I van der Sluis</author>
<author>A Gatt</author>
</authors>
<title>Building a semantically transparent corpus for the generation of referring expressions.</title>
<date>2006</date>
<booktitle>In Proceedings of the 4th International Conference on Natural Language Generation,</booktitle>
<pages>130--132</pages>
<marker>van Deemter, van der Sluis, Gatt, 2006</marker>
<rawString>K. van Deemter, I. van der Sluis, and A. Gatt. 2006. Building a semantically transparent corpus for the generation of referring expressions. In Proceedings of the 4th International Conference on Natural Language Generation, pages 130–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K van Deemter</author>
</authors>
<title>Towards a probabilistic version of bidirectional OT syntax and semantics.</title>
<date>2004</date>
<journal>Journal of Semantics,</journal>
<volume>21</volume>
<issue>3</issue>
<marker>van Deemter, 2004</marker>
<rawString>K. van Deemter. 2004. Towards a probabilistic version of bidirectional OT syntax and semantics. Journal of Semantics, 21(3):251–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Viethen</author>
<author>R Dale</author>
</authors>
<title>Towards the evaluation of referring expression generation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 4th Australasian Language Technology Workshop,</booktitle>
<pages>115--122</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="1109" citStr="Viethen and Dale (2006)" startWordPosition="155" endWordPosition="158">ears, the NLG community has seen a substantial number of studies to evaluate Generation of Referring Expressions (GRE) algorithms, but it is still far from clear what would constitute an optimal evaluation method. Two limitations stand out in the bulk of existing work. Firstly, most existing evaluations are essentially speakeroriented, focussing on the degree of “humanlikeness” of the generated descriptions, disregarding their effectiveness (e.g. Mellish and Dale (1998), Gupta and Stent (2005), van Deemter et al. (2006), Belz and Kilgarriff (2006), Belz and Reiter (2006), Paris et al. (2006), Viethen and Dale (2006), Gatt and Belz (2008)). The limited number of exceptions to this rule indicate that the differences between the two approaches to evaluation can be substantial (Gatt and Belz, 2008). Secondly, most evaluations have focussed on the semantic content of the generated descriptions, as produced by the Content Determination stage of a GRE algorithm; this means that linguistic realisation (i.e. the choice of words and linguistic constructions) is usually not addressed (exceptions are: Stone and Webber (1998), Krahmer and Theune (2002), Siddharthan and Copestake (2004)). Our aim is to build GRE algor</context>
</contexts>
<marker>Viethen, Dale, 2006</marker>
<rawString>J. Viethen and R. Dale. 2006. Towards the evaluation of referring expression generation. In Proceedings of the 4th Australasian Language Technology Workshop, pages 115–122, Sydney, Australia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>