<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003453">
<note confidence="0.99613">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
</note>
<title confidence="0.997322">
Monolingual Web-based Factoid Question Answering in Chinese,
Swedish, English and Japanese
</title>
<author confidence="0.930935">
E.W.D. Whittaker J. Hamonic D. Yang T. Klingberg S. Furui
</author>
<affiliation confidence="0.9996425">
Dept. of Computer Science
Tokyo Institute of Technology
</affiliation>
<address confidence="0.8854915">
2-12-1, Ookayama, Meguro-ku
Tokyo 152-8552 Japan
</address>
<email confidence="0.999733">
{edw,yuuki,raymond,tor,furui}@furui.cs.titech.ac.jp
</email>
<sectionHeader confidence="0.996685" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999500166666667">
In this paper we extend the application
of our statistical pattern classification ap-
proach to question answering (QA) which
has previously been applied successfully
to English and Japanese to develop two
prototype QA systems in Chinese and
Swedish. We show what data is necessary
to achieve this and also evaluate the per-
formance of the two new systems using a
translation of the TREC 2003 factoid QA
task. While performance for Chinese and
Swedish is found to be lower than that for
the more developed English and Japanese
systems we explain why this is the case
and offer solutions for their improvement.
All systems form the basis of our pub-
licly accessible web-based multilingual
QA system at http://asked.jp.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999950592592593">
Much of the research into automatic question an-
swering (QA) has understandably concentrated on
the English language with little regard to portabil-
ity or efficacy in other languages. It is only rela-
tively recently, with the introduction of the CLEF
and NTCIR QA evaluations, that researchers have
started to look at porting and evaluating the tech-
niques that have been shown to work well for En-
glish to other languages.
One of the major drawbacks of porting an En-
glish language QA system or approach to other
languages is often the lack of the corresponding
NLP tools in the target language. For instance,
parsers and named-entity (NE)-taggers, which are
typical components in many QA systems, are cer-
tainly not available for all the world’s languages.
Trainable parsers and NE-taggers similarly require
appropriate training data which, if not available,
is costly and requires specialized knowledge to
produce. Language-specific databases are also a
common feature of many systems, some of which
have taken many man-years to construct and ver-
ify. Such a component in many English-language
systems, for example, is WordNet. While porting
WordNet to other languages has been started in the
Euro and Global WordNet projects they still only
cover a relatively small number of languages.
In this paper we describe the application of our
data-driven approach to QA which was developed
right from the outset with the aim of portability
and robustness in mind. This statistical pattern
classification approach to QA is essentially lan-
guage independent and trainable given appropriate
language-specific training data. No assumptions
about the language are made by the model except
that some notion of words or space-separated to-
kens must exist or be introduced where it is ab-
sent. Our only other requirements to build a QA
system in a new target language are: a large col-
lection of text data in the target language that can
be searched for answers (e.g. the web), and a list
of example questions-and-answers (q-and-a) in the
target language. Given these data sources the re-
maining components can be obtained automati-
cally for each language.
So, in contrast to other contemporary ap-
proaches to QA our English language system
does not use WordNet as in (Hovy et al., 2001;
Moldovan et al., 2002), NE extraction, or any
other linguistic information e.g. from semantic
analysis (Hovy et al., 2001) or from question pars-
ing (Hovy et al., 2001; Moldovan et al., 2002) and
uses capitalised (where appropriate for the lan-
guage) word tokens as the only features for mod-
</bodyText>
<page confidence="0.997965">
45
</page>
<note confidence="0.984094">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
</note>
<bodyText confidence="0.99997679710145">
elling. For our Japanese system, although we cur-
rently use Chasen to segment Japanese charac-
ter sequences into units that resemble words, we
make no use of any morphological information
as used for example in (Fuchigami et al., 2004).
Moreover, it should be noted that our approach
is at the same time very different to other purely
web-based approaches such as askMSR (Brill et
al., 2002) and Aranea (Lin et al., 2002). For exam-
ple, we use entire documents rather than the snip-
pets of text returned by web search engines; we do
not use structured document sources or databases
and we do not transform the query in any way ei-
ther by term re-ordering or by modifying the tense
of verbs. These basic principles apply to each of
our language-specific QA systems thus simplify-
ing and accelerating development.
The approach to QA that we adopt has previ-
ously been described in (Whittaker et al., 2005a;
Whittaker et al., 2005b; Whittaker et al., 2005c)
where the details of the mathematical model and
how it was trained for English and Japanese were
given. Our approach has also been successfully
evaluated in the text retrieval conference (TREC)
2005 QA track evaluation (Voorhees, 2003) where
our group placed eleventh out of thirty partici-
pants (Whittaker et al., 2005a). Although the
TREC QA task is substantially different to web-
based QA the TREC evaluation confirmed that our
approach works and also provides an objective as-
sessment of its quality. Similarly, for our Japanese
language system we have previously evaluated the
performance of our approach on the NTCIR-3
QAC-1 task (Whittaker et al., 2005c). Although
our Japanese experiments were applied retrospec-
tively, the results would have placed us in the mid-
range of participating systems in that year’s eval-
uation. In this paper we present additional ex-
periments on Chinese and Swedish and explain
how our statistical pattern classification approach
to QA was successfully applied to these two new
languages. Using our approach and given ap-
propriate training data it is found that a reason-
ably proficient developer can build a QA system
in a new language in around 10 hours. Evalua-
tion of the Chinese and Swedish systems is per-
formed using a translation of the first 200 fac-
toid questions from the TREC 2003 evaluation
which we have also made available online. We
compare these results both qualitatively and quan-
titatively against results obtained previously for
English and Japanese. The systems, built us-
ing this method, form the basis of our multi-
language web demo which is publicly available at
http://asked.jp.
An outline of the remainder of this paper is as
follows: we briefly describe our statistical pattern
classification approach to QA in Section 2 repeat-
ing the important elements of our approach as nec-
essary to understand the remainder of the paper.
In Section 3 we describe the basic building blocks
of our QA system and how they can typically be
trained. We also give a breakdown of the data
used to train each language specific QA system.
In Section 4 we present the results of experiments
on Chinese, Swedish, English and Japanese and in
Section 5 we compare and analyse these results.
We wrap up with a conclusion and further work in
Sections 6 and 7.
</bodyText>
<sectionHeader confidence="0.79773" genericHeader="introduction">
2 Statistical pattern classification
approach to QA
</sectionHeader>
<bodyText confidence="0.999448625">
The answer to a question depends primarily on
the question itself but also on many other factors
such as the person asking the question, the loca-
tion of the person, what questions the person has
asked before, and so on. Although such factors
are clearly relevant in a real-world scenario they
are difficult to model and also to test in an off-
line mode, for example, in the context of the NT-
CIR and TREC evaluations. We therefore choose
to consider only the dependence of an answer A
on the question Q, where each is considered to
be a string of lA words A = al, ... , a1A and lQ
words Q = ql, ... , q1Q, respectively. In particu-
lar, we hypothesize that the answer A depends on
two sets of features W = W(Q) and X = X (Q)
as follows:
</bodyText>
<equation confidence="0.992422">
P(A I Q) = P(A I W, X), (1)
</equation>
<bodyText confidence="0.995048818181818">
where W = wl, ... , w1w can be thought of as a
set of lW features describing the “question-type”
part of Q such as who, when, where, which, etc.
and X = xl, ... , x1x is a set of lX features
comprising the “information-bearing” part of Q
i.e. what the question is actually about and what it
refers to. For example, in the questions, “Where
was Tom Cruise married?” and “When was
Tom Cruise married?” the information-bearing
component is identical in both cases whereas the
question-type component is different.
</bodyText>
<page confidence="0.998835">
46
</page>
<note confidence="0.850267">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
</note>
<bodyText confidence="0.925743">
Finding the best answer A involves a search
over all A for the one which maximizes the prob-
ability of the above model:
</bodyText>
<equation confidence="0.920560666666667">
�
A = arg max P(A I W, X). (2)
A
</equation>
<bodyText confidence="0.987687555555556">
Using Bayes rule, making further conditional
independence assumptions and assuming uniform
prior probabilities, which therefore do not affect
the optimisation criterion, we obtain the final op-
timisation criterion (see (Whittaker et al., 2005a)
for more details):
Some examples include: HOW, MANY, WHEN,
WHO, UNTIL etc. The question-type mapping
function W(Q) extracts n-tuples (n = 1, 2, ...) of
question-type features from the question Q, such
as HOW MANY and UNTIL WHEN.
Modelling the complex relationship between W
and A directly is non-trivial. We therefore intro-
duce an intermediate variable representing classes
of example questions-and-answers (q-and-a) (� for
e = 1... ICEI drawn from the set CE, and to fa-
cilitate modelling we say that W is conditionally
independent of A given (� as follows:
</bodyText>
<figure confidence="0.898657916666667">
arg max P(A I X)
A v
retrieval
model
- P(WIA)
� v �
ffilter
model
~ (3)
ICEIE P(W I ce) - P(ce I A)• (4)
e=1
P(WIA)=
</figure>
<bodyText confidence="0.998142375">
The P(A I X) model is essentially a language
model which models the probability of an answer
sequence A given a set of information-bearing fea-
tures X. It models the proximity of A to features
in X. We call this model the retrieval model and
examine it further in Section 2.1.
The P (W I A) model matches an answer A
with features in the question-type set W. Roughly
speaking this model relates ways of asking a ques-
tion with classes of valid answers. For example, it
associates dates, or days of the week with when-
type questions. In general, there are many valid
and equiprobable A for a given W so this compo-
nent can only re-rank candidate answers retrieved
by the retrieval model. Consequently, we call it the
filter model and examine it further in Section 2.2.
</bodyText>
<subsectionHeader confidence="0.938335">
2.1 Retrieval model
</subsectionHeader>
<bodyText confidence="0.999844615384615">
The retrieval model essentially models the prox-
imity of A to features in X. Since A =
a1, ... , alA we are actually modelling the distri-
bution of multi-word sequences. This should be
borne in mind in the following discussion when-
ever A is used. As mentioned above, we currently
use a deterministic information-feature mapping
function X = X (Q). This mapping only gener-
ates word m-tuples (m = 1, 2, ...) from single
words in Q that are not present in a stoplist of 50-
100 high-frequency words. For more details on
the exact form of the retrieval model please refer
to (Whittaker et al., 2005a).
</bodyText>
<subsectionHeader confidence="0.973711">
2.2 Filter model
</subsectionHeader>
<bodyText confidence="0.906987703703704">
A set of IVWI single-word features is extracted
based on frequency of occurrence in question data.
Given a set E of example q-and-a tj for
I. = 1... IEI where tj = (qj, aj) =
(~~1 � � qj , a1 � � a��A. ) we define a mapping
function f E CE by f(tj) = e. Each
class ce = (w1, ... , we �� � �a1, ... , a&apos; ��� ) is then
obtained by ce = U
j:f(tj)=e
experiments in this paper no clustering of the q-
and-a is actually performed so each q-and-a ex-
ample forms its own unique class i.e. each c, cor-
responds to a single tj and vice-versa.
Assuming conditional independence of the an-
swer words in class ce given A and making the
modelling assumption that the jth answer word aje
in the example class ce is dependent only on the
jth answer word in A we obtain:
P(WIA)=
where ca is a concrete class in the set of ICAI an-
swer classes CA, and assuming a� is conditionally
independent of ea given aj. The system using the
formulation of filter model given by Equation (5)
is referred to as model ONE. The model given by
Equation (4) is referred to as model TWO, how-
ever, we are only concerned with model ONE in
this paper.
</bodyText>
<equation confidence="0.987755071428571">
lAJ a2. In all the
U
W(~~)
i=1
lAe
11
j=1
P(W I ce) -
P(a&apos; I aj)
ICEIE
e=1
= ICEIE P(W I ee) lAe �CA1E P(aj I ea)P(ca I aj),
e=1 11 a=1 (5)
j=1
</equation>
<page confidence="0.98677">
47
</page>
<note confidence="0.916851">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
</note>
<bodyText confidence="0.999909945454545">
Answer typing, such as it exists in our model
ONE system, is performed by the filter model and
is effected by matching the input query against
our set of example questions CE. Each one of
these example questions ce has an answer asso-
ciated with it which in turn is expanded via the
CA classes into a set of possible answers for each
question. At each step this matching process is
entirely probabilistic as given by Equation (5). To
take a simple example, if we are presented with
the input query “Who was the U.S. President who
first used Camp David?” the first step effectively
matches the words (and bigrams, trigrams) in that
query against the words (and bigrams, trigrams)
in our example questions with a probability of its
match assigned to all example questions. Suppose,
for example, that the top-scoring example question
in our set is “Who was the U.S. President who re-
signed as a result of the Watergate affair?” since
the first six words in each question match each
other and will likely result in a high probability
being assigned. The corresponding answer to this
example question is “Richard Nixon”. So the next
step expands “Richard” using CA and results in
a high score being assigned to a class of words
which tend to share the feature of being male first
names, one of which is “Franklin”. Expanding
the second word in the answer “Nixon” using CA
will possibly result in a high score being assigned
to a class of words that share the feature of being
the surnames of U.S. presidents, one of which is
“Roosevelt”. In this way, we end up with a high
score assigned by the filter model to “Franklin
Roosevelt” but also to “Abraham Lincoln”, “Bill
Clinton”, etc. In combination with the retrieval
model, we hope that the documents obtained for
this query assign a higher retrieval model score
to “Franklin Roosevelt” over the names of other
U.S. presidents and thus output it in first place
with the highest overall probability. While this ap-
proach works well for names, dates and short place
names it does fall down, for example, on names
of books, plays and films where there is typically
less of a clear correspondence between the words
in any given position of two answers. This situa-
tion could be avoided by using multi-word answer
strings and not making the position-dependence
modelling assumption that was made to arrive at
Equation (5) but this has its own drawbacks.
The above description of the operation of the
filter model highlights the need for homogeneous
classes of CA of sufficiently wide coverage. In the
next section we describe a way in which this can
be achieved in an efficient, data-driven manner for
essentially any language.
</bodyText>
<subsectionHeader confidence="0.995804">
2.3 Obtaining CA
</subsectionHeader>
<bodyText confidence="0.999979605263158">
As we saw in the previous section the CA are the
closest thing we have to named entities since they
define classes of words that share some similarity
with each other. However, in some sense they are
more flexible than named entities since any word
can actually belong to any class but with a certain
probability. In this way we don’t rule out with a
zero probability the possibility of a word belong-
ing to some class, just that a word is more likely
to belong to some classes than others. In addition,
the entities are not actually named i.e. we do not
impose our own label on the classes so we do not
explicitly have a class of first names, or a class of
days of the week. Although we hope that we will
end up with classes containing such similar words
we do not make it explicit and we do not label what
each class of words is supposed to represent.
In keeping with our data-driven philosophy
and related objective to make our approach as
language-independent as possible we use an ag-
glomerative clustering algorithm to derive classes
automatically from data. The set of potential an-
swer words VC, that are clustered, should ideally
cover all possible words that might ever be an-
swers to questions. We therefore take the most fre-
quent IVcAI words from a language-specific cor-
pus T comprising ITI word tokens as our set of
potential answers. The “seeds” for the clusters are
chosen to be the most frequent ICAI words in T.
The algorithm then uses the co-occurrence proba-
bilities of words in T to group together words with
similar co-occurrence statistics. For each word a
in VcA the co-occurrence probability P(ai I a, d)
is the probability of ai given a occurring d words
away. If d is positive, a2 occurs after a, and if
negative, before a. We then construct a vector
of co-occurrences with maximum separation be-
tween words D, as follows:
</bodyText>
<equation confidence="0.990761333333333">
a� = [P(ai I a, 1), ... ,P(aR I a, 1), P(ai I a, ~1), . . . ,
P(aR I a, —1), ...... , P(ai I a, D), ... , P(aR I a, D),
P(ai I a, —D),..., P(aR I a, —D)]. (6)
</equation>
<bodyText confidence="0.9816825">
Rather than storing 2DVCA elements we can com-
pute most terms efficiently and on-the-fly using
</bodyText>
<page confidence="0.998519">
48
</page>
<note confidence="0.865086">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
</note>
<table confidence="0.8758472">
Language T T VCA CE CA
Chinese TREC Mandarin (Rogers, 2000) 68M 33k 7k 1000
Swedish PAROLE (University, 1997) 19M 367k 5k 1000
English AQUAINT (Voorhees, 2002) 300M 215k 290k 500
Japanese MAINICHI (Fukumoto et al., 2002) 150M 300k 270k 5000
</table>
<tableCaption confidence="0.999669">
Table 1: Description of each of the four monolingual QA systems.
</tableCaption>
<bodyText confidence="0.9717998">
the Katz back-off method (Katz, 1987) and ab-
solute discounting for estimating the probabilities
of unobserved events. To find the distance be-
tween two vectors, for efficiency, we use an Ll
distance metric: D(a27 aj) _ a2 — aj . Merging
two vectors then involves a straightforward update
of the co-occurrence counts for a cluster and re-
computing the affected conditional probabilities
and back-off weights. The clustering algorithm is
described below:
</bodyText>
<figure confidence="0.477443">
Algorithm 2.1 Cluster words to generate CA
initialize most frequent words to CAJ
classes
for i:= 1 to IVC„
for j:= 1 to �CAJ
compute ������ �c��
move d, to �
c
~~~
�
update �
c
~~~
�
</figure>
<bodyText confidence="0.9883856">
Although the algorithm is currently applied off-
line before system deployment it could also be eas-
ily adapted to handle multi-word sequences and
could also be applied at run-time thus reducing the
effects of out-of-vocabulary answers.
</bodyText>
<sectionHeader confidence="0.982373" genericHeader="method">
3 System components
</sectionHeader>
<bodyText confidence="0.999954020408163">
There are four basic ingredients to building a QA
system using our approach: (1) a collection of ex-
ample q-and-a (CE) pairs used for answer-typing
(answers need not necessarily be correct but must
be of the correct answer type); (2) a classification
of words (or word-like units cf. Japanese/Chinese)
into classes of similar words (CA) e.g. a class of
country names, of given names, of numbers etc.;
(3) a list of question words (qlist) such as “WHO”,
“WHERE”, “WHEN” etc.; and (4) a stop list
of words that should be ignored by the retrieval
model (stoplist) as described in Section 2.1.
The q-and-a (CE) for different languages can
often be found on the web or in commercial quiz
software. To date, we have not examined how
many or what distribution of types of example q-
and-a are important for good performance. How-
ever, intuitively, one expects that the richer and
more varied the questions the better the perfor-
mance will be. For the time being we aim to in-
clude as many examples as possible and hope this
covers the kinds of questions that are asked in re-
ality (or at least in the evaluations). In principle, in
a working system it should also be possible to feed
a user’s question together with the user’s response
back into the system to improve performance in an
unsupervised manner, so that performance would
gradually improve over time.
To obtain the classes of answers CA for each
language Algorithm 2.1 is applied. The qlist is
generated by taking the most frequently occurring
terms from the question portion of the example
q-and-a. The stoplist is formed from the 50-100
most frequently occurring words in T.
Google is used to retrieve documents for all
questions, except Japanese, where an index of the
NTCIR 2001 web snapshot is used instead. The
question is passed as-is to Google after the re-
moval of stop words. The top U documents are
downloaded in their entirety, HTML markup re-
moved, the text cleaned and upper-cased (where
appropriate). For consistency, all data in our sys-
tem is encoded using UTF-8.
The data source and relevant system details for
each language-specific QA system are given in Ta-
ble 1. For the Japanese system Chasen1 is used to
segment character sequences into word-like units.
For Chinese each sentence is mapped to a se-
quence of space-separated characters.
</bodyText>
<sectionHeader confidence="0.999906" genericHeader="method">
4 Experimental work
</sectionHeader>
<bodyText confidence="0.999902833333333">
A substantial amount of time has gone into in-
vestigating combinations and ranges of param-
eters which gave good performance on English
development questions. These same parameters
were largely used as-is for the Japanese system
with only minor optimisations performed on de-
</bodyText>
<footnote confidence="0.981881">
1http://chasen.naist.jp/hiki/ChaSen
</footnote>
<page confidence="0.997831">
49
</page>
<note confidence="0.854368">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
</note>
<table confidence="0.993791125">
Top Chinese Swedish English Japanese
Answers “TREC 2003” “TREC 2003” QAC-1
TREC 2003 TREC 2005
1 14 (7%) 23 (11.5%) 99 (24.0%) 64 (17.7%) 37 (18.5%)
5 24 (12.0%) 34 (17.0%) 175 (42.4%) 121 (33.4%) 61 (30.5%)
10 33 (16.5%) 46 (23.0%) 196 (47.5%) 151 (41.7%) 69 (34.5%)
20 39 (19.5%) 50 (25.0%) 216 (52.3%) 167 (46.1%) 81 (40.5%)
Total Questions 200 200 413 362 200
</table>
<tableCaption confidence="0.999665">
Table 2: Performance of each language-specific system in the top 1, 5, 10 and 20 answers output.
</tableCaption>
<bodyText confidence="0.999910620689655">
velopment questions disjoint from any evaluation
set. For the two new QA systems in Chinese and
Swedish we used exactly the same parameters that
had been found to work well for the English sys-
tem and performed no optimisation, mainly due to
a lack of questions that we could hold-out for de-
velopment. This is clearly sub-optimal but will be
addressed in future work.
It has been found for English and Japanese
that system performance consistently increases the
more documents that are used to search for an an-
swer. Experiments with English used U = 500
documents and for Japanese U = 5000 documents
was used. Due to time constraints, however, for
the Swedish experiments we used a maximum of
U = 100 documents. For the Chinese experiments
we were forced to use a maximum of only U = 20
documents. This was because the segmentation
of Chinese text into individual characters resulted
in a huge number of character combinations be-
ing generated in the retrieval model and due to
memory limitations the only way round this prob-
lem was to limit the number of documents used.
In addition, for an answer to be output the num-
ber of times it had to occur in the data was set to
5 for Swedish and 2 for Chinese. These values
were based on the number of documents we were
using and our prior experience with English and
Japanese.
</bodyText>
<subsectionHeader confidence="0.999939">
4.1 Test data
</subsectionHeader>
<bodyText confidence="0.99994046875">
For evaluating the English and Japanese systems
there were obvious candidates for evaluation sets
well-known to the QA community since the TREC
and NTCIR QA evaluations have been running
now for several years. For the English experi-
ments we performed two sets of experiments us-
ing the TREC 2003 and TREC 2005 factoid QA
questions. For the Japanese experiments we used
the formal run of the NTCIR-3 QAC-1 task.
For Chinese and Swedish there were no such
standard test sets available. We therefore decided
to translate the first 200 questions in the TREC
2003 QA factoid task into Chinese and Swedish.
For Swedish this was relatively straightforward.
For Chinese, we didn’t know the Chinese trans-
lation of the names, places etc. for about 40%
of the original TREC questions. Such questions
were therefore “translated” into a roughly equiv-
alent question about something similar in China.
However, the final set of Chinese questions still
contained 17% of questions that had a U.S. back-
ground. Correct answers were found by a hu-
man to 91% and 93% of questions for Chinese
and Swedish, respectively. We hope that using
the TREC questions should at least give the reader
some idea of the kind of the questions that were
asked so they can appreciate the difficulty of find-
ing the answers in Chinese and Swedish web data.
To aid comprehension of the task we are tackling
we have made the questions and the answers we
used for evaluation freely available2 for anyone
else interested in running similar experiments.
</bodyText>
<subsectionHeader confidence="0.82495">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.9997216">
The TREC, NTCIR and CLEF QA evaluations
have all converged on the use of exact and sup-
ported answers as a metric of QA system perfor-
mance. In web-based QA we are also concerned
with document support but primarily with answer
correctness so for now we only use answer correct-
ness as our evaluation metric. However, we also
adopt the notion of inexact answers to mark as in-
correct, answers that have extraneous or missing
parts such as units of measurement or in the case
of, say Chinese, an extra or missing character.
In Table 2 we present the number and percent-
age of answers that were marked correct in the top
1, 5, 10 and 20 answers output by each of the four
monolingual systems.
</bodyText>
<footnote confidence="0.964896">
2http://asked.jp/About.html
</footnote>
<page confidence="0.982129">
50
</page>
<note confidence="0.981004">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
</note>
<sectionHeader confidence="0.994323" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.99997844">
Probably the first observation one makes looking
at Table 2 is that the performance of the Chinese
and Swedish systems is significantly lower than
the English and Japanese systems. On the face
of it this is not so surprising since far more docu-
ments and many more hours went into developing
the English and Japanese systems for which the
system parameters were also optimised on held-
out question data prior to evaluation.
One significant issue common to our evaluation
of the Chinese and Swedish systems is the use of
a translation of the TREC 2003 questions. Al-
though it was hoped this would aid comprehen-
sion of the task we were attempting it had the
predictable problem that not enough, or any (in
some cases), relevant Chinese and Swedish web
data could be found to answer some questions.
Particularly problematic were U.S. centric ques-
tions about baseball and U.S. geography of which
there were around 5% and 10%, respectively, in
the Swedish test set. However, aside from the
choice of test data there are several other possi-
ble reasons for the drop in performance and we’ll
examine them separately for Chinese and Swedish
in the following sections.
</bodyText>
<subsectionHeader confidence="0.954372">
5.1 Chinese
</subsectionHeader>
<bodyText confidence="0.999951096774194">
One of the most important issues in building a Chi-
nese QA system was deciding on the segmenta-
tion for text in which there are no spaces between
words. In contrast to our Japanese system where
we used a freely available segmentation tool we
chose to segment Chinese text into single char-
acters and to not even try to recover a notion of
words from the text. Clearly this is far from satis-
factory but the redundancy aspect of our approach
does compensate for this to some extent by favour-
ing frequent character sequences which are more
likely to represent words. The second major prob-
lem we had was downloading sufficient and rel-
evant data for each question. The largest culprit
here was our restriction on using only 20 docu-
ments, the reasons for which were explained in
Section 4. However, the quality of the documents
themselves was also a problem: only 47.5% of
questions had data in which the correct answer was
observed at least once in the data and this was re-
duced to 43.5% when an answer had to occur 2 or
more times. This places a much lower value on the
performance upper bound (i.e. 47.5%) compared
to English where it is typically 80-90% and con-
sequently puts the results we obtained in a more
positive light. Another interesting observation was
that there were very few inexact answers (10% in
the top 20) which shows that the modelling as-
sumption made in Equation (5) was not detrimen-
tal for Chinese even when segmenting by charac-
ters.
</bodyText>
<subsectionHeader confidence="0.995927">
5.2 Swedish
</subsectionHeader>
<bodyText confidence="0.999984277777778">
The general structure of Swedish and its charac-
ter set are very similar to English so there were
very few modifications that needed to be made
to port the English system to Swedish. In some
senses Swedish (and indeed many other Euro-
pean languages) are much easier than English for
a purely statistical keyword-based system since
they do not have the same phenomenon that En-
glish has where many3 questions use does/did as
in “Who/when/where does/did ...?” etc. questions.
Swedish does, however, suffer from some of the
same problems as English such as the need to
re-order words in a question to produce a more
statement-like formulation. A more significant
problem is that Swedish is a language spoken by
only about 9 million people and correspondingly
there were far fewer training questions and an-
swers freely available on the web and much less
web-data available for finding answers. Although
our maximum number of documents was set to
100, we only downloaded 48 documents per ques-
tion on average (s.d. of 30). In a data-driven ap-
proach that relies to a large extent on data redun-
dancy and the philosophy that “the more data the
better” to achieve good performance this is always
likely to be problematic. Indeed, of the documents
that were downloaded for each question 67.5%
contained a correct answer. This was reduced
to 50% for correct answers occuring the thresh-
old number of 5 times or more. Clearly a more
linguistics-based system suffers less from this lack
of redundancy but as pointed out earlier the prob-
lem is simply transferred to the development stage
instead, where the initial data or linguistic knowl-
edge needed to train a system might well be lack-
ing.
</bodyText>
<sectionHeader confidence="0.999278" genericHeader="method">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9830445">
In this paper we have shown that usable perfor-
mance from two prototype QA systems developed
</bodyText>
<footnote confidence="0.988842">
3For example, 28% of TREC 2003 factoid questions.
</footnote>
<page confidence="0.991453">
51
</page>
<note confidence="0.941857">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
</note>
<bodyText confidence="0.99986">
for Chinese and Swedish could be obtained in a
matter of hours using our data-driven approach
to question answering. While the performance of
such systems was found to be below that of more
developed systems that used the same approach
we feel that this demonstrates the effectiveness of
our language independent approach for web-based
QA. One of the interesting observations from our
experiments is the inherent difficulty of finding an-
swers to questions when there is very little data
available in the target language in which to search
for answers. While one could argue that some-
one in Sweden or China may not be interested in
finding out the winning team of the 1996 World
Series this clearly misses the point and instead
makes a compelling argument for cross-language
QA (CLQA) whose profile has been steadily in-
creasing due to the recent CLEF and NTCIR eval-
uations. While most attempts at CLQA have so
far concentrated on translating the query into En-
glish and performing monolingual English QA on
the translated query, we feel that an approach that
also combines a simple system trained in the man-
ner described in this paper would almost certainly
benefit overall performance.
</bodyText>
<sectionHeader confidence="0.999674" genericHeader="method">
7 Further work
</sectionHeader>
<bodyText confidence="0.997697470588235">
In the future we aim to improve the Chinese and
Swedish systems by increasing the amount and
quality of data used for training and also the data
used for searching for answers as well as solving
the explosion in memory usage that occurred with
large amounts of Chinese text. We also intend to
optimise the system parameters for each system
using the questions and answers used for training
with a rotating form of cross-validation so as to
maximise use of the little data we have available.
Given the rapid development time and reasonable
performance of the approach outlined in this paper
combined with the fact that specialized linguistic
knowledge is unnecessary we also plan the devel-
opment of yet more monolingual systems in other
languages and hope to participate in the upcoming
CLEF QA track.
</bodyText>
<sectionHeader confidence="0.998977" genericHeader="conclusions">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9995308">
This research was supported by JSPS and the
Japanese government 21st century COE pro-
gramme. The authors also with to thank Dietrich
Klakow for his contributions to the model devel-
opment.
</bodyText>
<sectionHeader confidence="0.994284" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999930961538462">
E. Brill, S. Dumais, and M. Banko. 2002. An
Analysis of the AskMSR Question-answering Sys-
tem. In Proceedings of the 2002 Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
M. Fuchigami, H. Ohnuma, and A. Ikeno. 2004. Oki
QA System for QAC-2. In Proceedings ofNTCIR-4
Workshop.
J. Fukumoto, T. Kato, and F. Masui. 2002. Ques-
tion Answering Challenge (QAC-1) An Evaluation
of Question Answering Task at NTCIR Workshop 3.
In Proceedings ofNTCIR-3 Workshop.
E. Hovy, U. Hermjakob, and Lin C-Y. 2001. The Use
of External Knowledge in Factoid QA. In Proceed-
ings of the TREC 2001 Conference.
S. M. Katz. 1987. Estimation of Probabilities from
Sparse Data for the Language Model Component of
a Speech Recognizer. IEEE Transactions on Acous-
tic, Speech, and Signal Processing, 35(3):400–401.
J. Lin, A. Fernandes, B. Katz, G. Marton, and S. Tellex.
2002. Extracting Answers from the Web Us-
ing Knowledge Annotation and Knowledge Min-
ing Techniques. In Proceedings of the Eleventh
Text REtrieval Conference (TREC2002), Gaithers-
burg, Maryland, November.
D. Moldovan, S. Harabagiu, R. Girju, P. Morarescu,
F. Lacatusu, A. Novischi, A. Badulescu, and
O. Bolohan. 2002. LCC Tools for Question An-
swering. In Proceedings of the TREC 2002 Confer-
ence.
Willie Rogers. 2000. TREC Mandarin. Linguistic
Data Consortium.
Sprakbanken Gothenburg University. 1997.
PAROLE Swedish Language Corpus.
http://spraakbanken.gu.se/lbeng.html.
E. Voorhees. 2002. Overview of the TREC 2002 Ques-
tion Answering Track. In Proceedings of the TREC
2002 Conference.
E. Voorhees. 2003. Overview of the TREC 2003 Ques-
tion Answering Track. In Proceedings of the TREC
2003 Conference.
E.W.D. Whittaker, P. Chatain, S. Furui, and D. Klakow.
2005a. TREC2005 Question Answering Experi-
ments at Tokyo Institute of Technology. In Proceed-
ings of the 14th Text Retrieval Conference.
E.W.D. Whittaker, S. Furui, and D. Klakow. 2005b.
A Statistical Pattern Recognition Approach to Ques-
tion Answering using Web Data. In Proceedings of
Cyberworlds.
E.W.D. Whittaker, J. Hamonic, and S. Furui. 2005c. A
Unified Approach to Japanese and English Question
Answering. In Proceedings ofNTCIR-5.
</reference>
<page confidence="0.998859">
52
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.427241">
<note confidence="0.658498">EACL 2006 Workshop on Multilingual Question Answering - MLQA06</note>
<title confidence="0.9694275">Monolingual Web-based Factoid Question Answering in Swedish, English and Japanese</title>
<author confidence="0.999123">E W D Whittaker J Hamonic D Yang T Klingberg S Furui</author>
<affiliation confidence="0.9974">Dept. of Computer Tokyo Institute of</affiliation>
<address confidence="0.911244">2-12-1, Ookayama, Tokyo 152-8552</address>
<abstract confidence="0.990408210526316">In this paper we extend the application of our statistical pattern classification approach to question answering (QA) which has previously been applied successfully to English and Japanese to develop two prototype QA systems in Chinese and Swedish. We show what data is necessary to achieve this and also evaluate the performance of the two new systems using a translation of the TREC 2003 factoid QA task. While performance for Chinese and Swedish is found to be lower than that for the more developed English and Japanese systems we explain why this is the case and offer solutions for their improvement. All systems form the basis of our publicly accessible web-based multilingual system at</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>S Dumais</author>
<author>M Banko</author>
</authors>
<title>An Analysis of the AskMSR Question-answering System.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="4113" citStr="Brill et al., 2002" startWordPosition="656" endWordPosition="659"> from question parsing (Hovy et al., 2001; Moldovan et al., 2002) and uses capitalised (where appropriate for the language) word tokens as the only features for mod45 EACL 2006 Workshop on Multilingual Question Answering - MLQA06 elling. For our Japanese system, although we currently use Chasen to segment Japanese character sequences into units that resemble words, we make no use of any morphological information as used for example in (Fuchigami et al., 2004). Moreover, it should be noted that our approach is at the same time very different to other purely web-based approaches such as askMSR (Brill et al., 2002) and Aranea (Lin et al., 2002). For example, we use entire documents rather than the snippets of text returned by web search engines; we do not use structured document sources or databases and we do not transform the query in any way either by term re-ordering or by modifying the tense of verbs. These basic principles apply to each of our language-specific QA systems thus simplifying and accelerating development. The approach to QA that we adopt has previously been described in (Whittaker et al., 2005a; Whittaker et al., 2005b; Whittaker et al., 2005c) where the details of the mathematical mod</context>
</contexts>
<marker>Brill, Dumais, Banko, 2002</marker>
<rawString>E. Brill, S. Dumais, and M. Banko. 2002. An Analysis of the AskMSR Question-answering System. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fuchigami</author>
<author>H Ohnuma</author>
<author>A Ikeno</author>
</authors>
<date>2004</date>
<booktitle>Oki QA System for QAC-2. In Proceedings ofNTCIR-4 Workshop.</booktitle>
<contexts>
<context position="3957" citStr="Fuchigami et al., 2004" startWordPosition="629" endWordPosition="632"> WordNet as in (Hovy et al., 2001; Moldovan et al., 2002), NE extraction, or any other linguistic information e.g. from semantic analysis (Hovy et al., 2001) or from question parsing (Hovy et al., 2001; Moldovan et al., 2002) and uses capitalised (where appropriate for the language) word tokens as the only features for mod45 EACL 2006 Workshop on Multilingual Question Answering - MLQA06 elling. For our Japanese system, although we currently use Chasen to segment Japanese character sequences into units that resemble words, we make no use of any morphological information as used for example in (Fuchigami et al., 2004). Moreover, it should be noted that our approach is at the same time very different to other purely web-based approaches such as askMSR (Brill et al., 2002) and Aranea (Lin et al., 2002). For example, we use entire documents rather than the snippets of text returned by web search engines; we do not use structured document sources or databases and we do not transform the query in any way either by term re-ordering or by modifying the tense of verbs. These basic principles apply to each of our language-specific QA systems thus simplifying and accelerating development. The approach to QA that we </context>
</contexts>
<marker>Fuchigami, Ohnuma, Ikeno, 2004</marker>
<rawString>M. Fuchigami, H. Ohnuma, and A. Ikeno. 2004. Oki QA System for QAC-2. In Proceedings ofNTCIR-4 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Fukumoto</author>
<author>T Kato</author>
<author>F Masui</author>
</authors>
<title>Question Answering Challenge (QAC-1) An Evaluation of Question Answering Task at NTCIR Workshop 3.</title>
<date>2002</date>
<booktitle>In Proceedings ofNTCIR-3 Workshop.</booktitle>
<contexts>
<context position="17193" citStr="Fukumoto et al., 2002" startWordPosition="3009" endWordPosition="3012">ct a vector of co-occurrences with maximum separation between words D, as follows: a� = [P(ai I a, 1), ... ,P(aR I a, 1), P(ai I a, ~1), . . . , P(aR I a, —1), ...... , P(ai I a, D), ... , P(aR I a, D), P(ai I a, —D),..., P(aR I a, —D)]. (6) Rather than storing 2DVCA elements we can compute most terms efficiently and on-the-fly using 48 EACL 2006 Workshop on Multilingual Question Answering - MLQA06 Language T T VCA CE CA Chinese TREC Mandarin (Rogers, 2000) 68M 33k 7k 1000 Swedish PAROLE (University, 1997) 19M 367k 5k 1000 English AQUAINT (Voorhees, 2002) 300M 215k 290k 500 Japanese MAINICHI (Fukumoto et al., 2002) 150M 300k 270k 5000 Table 1: Description of each of the four monolingual QA systems. the Katz back-off method (Katz, 1987) and absolute discounting for estimating the probabilities of unobserved events. To find the distance between two vectors, for efficiency, we use an Ll distance metric: D(a27 aj) _ a2 — aj . Merging two vectors then involves a straightforward update of the co-occurrence counts for a cluster and recomputing the affected conditional probabilities and back-off weights. The clustering algorithm is described below: Algorithm 2.1 Cluster words to generate CA initialize most freq</context>
</contexts>
<marker>Fukumoto, Kato, Masui, 2002</marker>
<rawString>J. Fukumoto, T. Kato, and F. Masui. 2002. Question Answering Challenge (QAC-1) An Evaluation of Question Answering Task at NTCIR Workshop 3. In Proceedings ofNTCIR-3 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>U Hermjakob</author>
<author>C-Y Lin</author>
</authors>
<title>The Use of External Knowledge in Factoid QA.</title>
<date>2001</date>
<booktitle>In Proceedings of the TREC 2001 Conference.</booktitle>
<contexts>
<context position="3367" citStr="Hovy et al., 2001" startWordPosition="531" endWordPosition="534">are made by the model except that some notion of words or space-separated tokens must exist or be introduced where it is absent. Our only other requirements to build a QA system in a new target language are: a large collection of text data in the target language that can be searched for answers (e.g. the web), and a list of example questions-and-answers (q-and-a) in the target language. Given these data sources the remaining components can be obtained automatically for each language. So, in contrast to other contemporary approaches to QA our English language system does not use WordNet as in (Hovy et al., 2001; Moldovan et al., 2002), NE extraction, or any other linguistic information e.g. from semantic analysis (Hovy et al., 2001) or from question parsing (Hovy et al., 2001; Moldovan et al., 2002) and uses capitalised (where appropriate for the language) word tokens as the only features for mod45 EACL 2006 Workshop on Multilingual Question Answering - MLQA06 elling. For our Japanese system, although we currently use Chasen to segment Japanese character sequences into units that resemble words, we make no use of any morphological information as used for example in (Fuchigami et al., 2004). Moreover</context>
</contexts>
<marker>Hovy, Hermjakob, Lin, 2001</marker>
<rawString>E. Hovy, U. Hermjakob, and Lin C-Y. 2001. The Use of External Knowledge in Factoid QA. In Proceedings of the TREC 2001 Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Katz</author>
</authors>
<title>Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustic, Speech, and Signal Processing,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="17316" citStr="Katz, 1987" startWordPosition="3032" endWordPosition="3033">), . . . , P(aR I a, —1), ...... , P(ai I a, D), ... , P(aR I a, D), P(ai I a, —D),..., P(aR I a, —D)]. (6) Rather than storing 2DVCA elements we can compute most terms efficiently and on-the-fly using 48 EACL 2006 Workshop on Multilingual Question Answering - MLQA06 Language T T VCA CE CA Chinese TREC Mandarin (Rogers, 2000) 68M 33k 7k 1000 Swedish PAROLE (University, 1997) 19M 367k 5k 1000 English AQUAINT (Voorhees, 2002) 300M 215k 290k 500 Japanese MAINICHI (Fukumoto et al., 2002) 150M 300k 270k 5000 Table 1: Description of each of the four monolingual QA systems. the Katz back-off method (Katz, 1987) and absolute discounting for estimating the probabilities of unobserved events. To find the distance between two vectors, for efficiency, we use an Ll distance metric: D(a27 aj) _ a2 — aj . Merging two vectors then involves a straightforward update of the co-occurrence counts for a cluster and recomputing the affected conditional probabilities and back-off weights. The clustering algorithm is described below: Algorithm 2.1 Cluster words to generate CA initialize most frequent words to CAJ classes for i:= 1 to IVC„ for j:= 1 to �CAJ compute ������ �c�� move d, to � c ~~~ � update � c ~~~ � Alt</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>S. M. Katz. 1987. Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer. IEEE Transactions on Acoustic, Speech, and Signal Processing, 35(3):400–401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
<author>A Fernandes</author>
<author>B Katz</author>
<author>G Marton</author>
<author>S Tellex</author>
</authors>
<title>Extracting Answers from the Web Using Knowledge Annotation and Knowledge Mining Techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the Eleventh Text REtrieval Conference (TREC2002),</booktitle>
<location>Gaithersburg, Maryland,</location>
<contexts>
<context position="4143" citStr="Lin et al., 2002" startWordPosition="662" endWordPosition="665">al., 2001; Moldovan et al., 2002) and uses capitalised (where appropriate for the language) word tokens as the only features for mod45 EACL 2006 Workshop on Multilingual Question Answering - MLQA06 elling. For our Japanese system, although we currently use Chasen to segment Japanese character sequences into units that resemble words, we make no use of any morphological information as used for example in (Fuchigami et al., 2004). Moreover, it should be noted that our approach is at the same time very different to other purely web-based approaches such as askMSR (Brill et al., 2002) and Aranea (Lin et al., 2002). For example, we use entire documents rather than the snippets of text returned by web search engines; we do not use structured document sources or databases and we do not transform the query in any way either by term re-ordering or by modifying the tense of verbs. These basic principles apply to each of our language-specific QA systems thus simplifying and accelerating development. The approach to QA that we adopt has previously been described in (Whittaker et al., 2005a; Whittaker et al., 2005b; Whittaker et al., 2005c) where the details of the mathematical model and how it was trained for </context>
</contexts>
<marker>Lin, Fernandes, Katz, Marton, Tellex, 2002</marker>
<rawString>J. Lin, A. Fernandes, B. Katz, G. Marton, and S. Tellex. 2002. Extracting Answers from the Web Using Knowledge Annotation and Knowledge Mining Techniques. In Proceedings of the Eleventh Text REtrieval Conference (TREC2002), Gaithersburg, Maryland, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>S Harabagiu</author>
<author>R Girju</author>
<author>P Morarescu</author>
<author>F Lacatusu</author>
<author>A Novischi</author>
<author>A Badulescu</author>
<author>O Bolohan</author>
</authors>
<title>LCC Tools for Question Answering.</title>
<date>2002</date>
<booktitle>In Proceedings of the TREC 2002 Conference.</booktitle>
<contexts>
<context position="3391" citStr="Moldovan et al., 2002" startWordPosition="535" endWordPosition="538">el except that some notion of words or space-separated tokens must exist or be introduced where it is absent. Our only other requirements to build a QA system in a new target language are: a large collection of text data in the target language that can be searched for answers (e.g. the web), and a list of example questions-and-answers (q-and-a) in the target language. Given these data sources the remaining components can be obtained automatically for each language. So, in contrast to other contemporary approaches to QA our English language system does not use WordNet as in (Hovy et al., 2001; Moldovan et al., 2002), NE extraction, or any other linguistic information e.g. from semantic analysis (Hovy et al., 2001) or from question parsing (Hovy et al., 2001; Moldovan et al., 2002) and uses capitalised (where appropriate for the language) word tokens as the only features for mod45 EACL 2006 Workshop on Multilingual Question Answering - MLQA06 elling. For our Japanese system, although we currently use Chasen to segment Japanese character sequences into units that resemble words, we make no use of any morphological information as used for example in (Fuchigami et al., 2004). Moreover, it should be noted tha</context>
</contexts>
<marker>Moldovan, Harabagiu, Girju, Morarescu, Lacatusu, Novischi, Badulescu, Bolohan, 2002</marker>
<rawString>D. Moldovan, S. Harabagiu, R. Girju, P. Morarescu, F. Lacatusu, A. Novischi, A. Badulescu, and O. Bolohan. 2002. LCC Tools for Question Answering. In Proceedings of the TREC 2002 Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Willie Rogers</author>
</authors>
<title>TREC Mandarin. Linguistic Data Consortium.</title>
<date>2000</date>
<contexts>
<context position="17032" citStr="Rogers, 2000" startWordPosition="2985" endWordPosition="2986">ty P(ai I a, d) is the probability of ai given a occurring d words away. If d is positive, a2 occurs after a, and if negative, before a. We then construct a vector of co-occurrences with maximum separation between words D, as follows: a� = [P(ai I a, 1), ... ,P(aR I a, 1), P(ai I a, ~1), . . . , P(aR I a, —1), ...... , P(ai I a, D), ... , P(aR I a, D), P(ai I a, —D),..., P(aR I a, —D)]. (6) Rather than storing 2DVCA elements we can compute most terms efficiently and on-the-fly using 48 EACL 2006 Workshop on Multilingual Question Answering - MLQA06 Language T T VCA CE CA Chinese TREC Mandarin (Rogers, 2000) 68M 33k 7k 1000 Swedish PAROLE (University, 1997) 19M 367k 5k 1000 English AQUAINT (Voorhees, 2002) 300M 215k 290k 500 Japanese MAINICHI (Fukumoto et al., 2002) 150M 300k 270k 5000 Table 1: Description of each of the four monolingual QA systems. the Katz back-off method (Katz, 1987) and absolute discounting for estimating the probabilities of unobserved events. To find the distance between two vectors, for efficiency, we use an Ll distance metric: D(a27 aj) _ a2 — aj . Merging two vectors then involves a straightforward update of the co-occurrence counts for a cluster and recomputing the affe</context>
</contexts>
<marker>Rogers, 2000</marker>
<rawString>Willie Rogers. 2000. TREC Mandarin. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<date>1997</date>
<journal>PAROLE Swedish Language Corpus. http://spraakbanken.gu.se/lbeng.html.</journal>
<institution>Sprakbanken Gothenburg University.</institution>
<marker>1997</marker>
<rawString>Sprakbanken Gothenburg University. 1997. PAROLE Swedish Language Corpus. http://spraakbanken.gu.se/lbeng.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2002</date>
<booktitle>In Proceedings of the TREC 2002 Conference.</booktitle>
<contexts>
<context position="17132" citStr="Voorhees, 2002" startWordPosition="3001" endWordPosition="3002">rs after a, and if negative, before a. We then construct a vector of co-occurrences with maximum separation between words D, as follows: a� = [P(ai I a, 1), ... ,P(aR I a, 1), P(ai I a, ~1), . . . , P(aR I a, —1), ...... , P(ai I a, D), ... , P(aR I a, D), P(ai I a, —D),..., P(aR I a, —D)]. (6) Rather than storing 2DVCA elements we can compute most terms efficiently and on-the-fly using 48 EACL 2006 Workshop on Multilingual Question Answering - MLQA06 Language T T VCA CE CA Chinese TREC Mandarin (Rogers, 2000) 68M 33k 7k 1000 Swedish PAROLE (University, 1997) 19M 367k 5k 1000 English AQUAINT (Voorhees, 2002) 300M 215k 290k 500 Japanese MAINICHI (Fukumoto et al., 2002) 150M 300k 270k 5000 Table 1: Description of each of the four monolingual QA systems. the Katz back-off method (Katz, 1987) and absolute discounting for estimating the probabilities of unobserved events. To find the distance between two vectors, for efficiency, we use an Ll distance metric: D(a27 aj) _ a2 — aj . Merging two vectors then involves a straightforward update of the co-occurrence counts for a cluster and recomputing the affected conditional probabilities and back-off weights. The clustering algorithm is described below: Al</context>
</contexts>
<marker>Voorhees, 2002</marker>
<rawString>E. Voorhees. 2002. Overview of the TREC 2002 Question Answering Track. In Proceedings of the TREC 2002 Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2003</date>
<booktitle>In Proceedings of the TREC 2003 Conference.</booktitle>
<contexts>
<context position="4907" citStr="Voorhees, 2003" startWordPosition="793" endWordPosition="794">databases and we do not transform the query in any way either by term re-ordering or by modifying the tense of verbs. These basic principles apply to each of our language-specific QA systems thus simplifying and accelerating development. The approach to QA that we adopt has previously been described in (Whittaker et al., 2005a; Whittaker et al., 2005b; Whittaker et al., 2005c) where the details of the mathematical model and how it was trained for English and Japanese were given. Our approach has also been successfully evaluated in the text retrieval conference (TREC) 2005 QA track evaluation (Voorhees, 2003) where our group placed eleventh out of thirty participants (Whittaker et al., 2005a). Although the TREC QA task is substantially different to webbased QA the TREC evaluation confirmed that our approach works and also provides an objective assessment of its quality. Similarly, for our Japanese language system we have previously evaluated the performance of our approach on the NTCIR-3 QAC-1 task (Whittaker et al., 2005c). Although our Japanese experiments were applied retrospectively, the results would have placed us in the midrange of participating systems in that year’s evaluation. In this pa</context>
</contexts>
<marker>Voorhees, 2003</marker>
<rawString>E. Voorhees. 2003. Overview of the TREC 2003 Question Answering Track. In Proceedings of the TREC 2003 Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E W D Whittaker</author>
<author>P Chatain</author>
<author>S Furui</author>
<author>D Klakow</author>
</authors>
<title>TREC2005 Question Answering Experiments at Tokyo Institute of Technology.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th Text Retrieval Conference.</booktitle>
<contexts>
<context position="4619" citStr="Whittaker et al., 2005" startWordPosition="746" endWordPosition="749">approach is at the same time very different to other purely web-based approaches such as askMSR (Brill et al., 2002) and Aranea (Lin et al., 2002). For example, we use entire documents rather than the snippets of text returned by web search engines; we do not use structured document sources or databases and we do not transform the query in any way either by term re-ordering or by modifying the tense of verbs. These basic principles apply to each of our language-specific QA systems thus simplifying and accelerating development. The approach to QA that we adopt has previously been described in (Whittaker et al., 2005a; Whittaker et al., 2005b; Whittaker et al., 2005c) where the details of the mathematical model and how it was trained for English and Japanese were given. Our approach has also been successfully evaluated in the text retrieval conference (TREC) 2005 QA track evaluation (Voorhees, 2003) where our group placed eleventh out of thirty participants (Whittaker et al., 2005a). Although the TREC QA task is substantially different to webbased QA the TREC evaluation confirmed that our approach works and also provides an objective assessment of its quality. Similarly, for our Japanese language system w</context>
<context position="8737" citStr="Whittaker et al., 2005" startWordPosition="1463" endWordPosition="1466">Cruise married?” and “When was Tom Cruise married?” the information-bearing component is identical in both cases whereas the question-type component is different. 46 EACL 2006 Workshop on Multilingual Question Answering - MLQA06 Finding the best answer A involves a search over all A for the one which maximizes the probability of the above model: � A = arg max P(A I W, X). (2) A Using Bayes rule, making further conditional independence assumptions and assuming uniform prior probabilities, which therefore do not affect the optimisation criterion, we obtain the final optimisation criterion (see (Whittaker et al., 2005a) for more details): Some examples include: HOW, MANY, WHEN, WHO, UNTIL etc. The question-type mapping function W(Q) extracts n-tuples (n = 1, 2, ...) of question-type features from the question Q, such as HOW MANY and UNTIL WHEN. Modelling the complex relationship between W and A directly is non-trivial. We therefore introduce an intermediate variable representing classes of example questions-and-answers (q-and-a) (� for e = 1... ICEI drawn from the set CE, and to facilitate modelling we say that W is conditionally independent of A given (� as follows: arg max P(A I X) A v retrieval model - </context>
<context position="10791" citStr="Whittaker et al., 2005" startWordPosition="1833" endWordPosition="1836">tion 2.2. 2.1 Retrieval model The retrieval model essentially models the proximity of A to features in X. Since A = a1, ... , alA we are actually modelling the distribution of multi-word sequences. This should be borne in mind in the following discussion whenever A is used. As mentioned above, we currently use a deterministic information-feature mapping function X = X (Q). This mapping only generates word m-tuples (m = 1, 2, ...) from single words in Q that are not present in a stoplist of 50- 100 high-frequency words. For more details on the exact form of the retrieval model please refer to (Whittaker et al., 2005a). 2.2 Filter model A set of IVWI single-word features is extracted based on frequency of occurrence in question data. Given a set E of example q-and-a tj for I. = 1... IEI where tj = (qj, aj) = (~~1 � � qj , a1 � � a��A. ) we define a mapping function f E CE by f(tj) = e. Each class ce = (w1, ... , we �� � �a1, ... , a&apos; ��� ) is then obtained by ce = U j:f(tj)=e experiments in this paper no clustering of the qand-a is actually performed so each q-and-a example forms its own unique class i.e. each c, corresponds to a single tj and vice-versa. Assuming conditional independence of the answer wo</context>
</contexts>
<marker>Whittaker, Chatain, Furui, Klakow, 2005</marker>
<rawString>E.W.D. Whittaker, P. Chatain, S. Furui, and D. Klakow. 2005a. TREC2005 Question Answering Experiments at Tokyo Institute of Technology. In Proceedings of the 14th Text Retrieval Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E W D Whittaker</author>
<author>S Furui</author>
<author>D Klakow</author>
</authors>
<title>A Statistical Pattern Recognition Approach to Question Answering using Web Data.</title>
<date>2005</date>
<booktitle>In Proceedings of Cyberworlds.</booktitle>
<contexts>
<context position="4619" citStr="Whittaker et al., 2005" startWordPosition="746" endWordPosition="749">approach is at the same time very different to other purely web-based approaches such as askMSR (Brill et al., 2002) and Aranea (Lin et al., 2002). For example, we use entire documents rather than the snippets of text returned by web search engines; we do not use structured document sources or databases and we do not transform the query in any way either by term re-ordering or by modifying the tense of verbs. These basic principles apply to each of our language-specific QA systems thus simplifying and accelerating development. The approach to QA that we adopt has previously been described in (Whittaker et al., 2005a; Whittaker et al., 2005b; Whittaker et al., 2005c) where the details of the mathematical model and how it was trained for English and Japanese were given. Our approach has also been successfully evaluated in the text retrieval conference (TREC) 2005 QA track evaluation (Voorhees, 2003) where our group placed eleventh out of thirty participants (Whittaker et al., 2005a). Although the TREC QA task is substantially different to webbased QA the TREC evaluation confirmed that our approach works and also provides an objective assessment of its quality. Similarly, for our Japanese language system w</context>
<context position="8737" citStr="Whittaker et al., 2005" startWordPosition="1463" endWordPosition="1466">Cruise married?” and “When was Tom Cruise married?” the information-bearing component is identical in both cases whereas the question-type component is different. 46 EACL 2006 Workshop on Multilingual Question Answering - MLQA06 Finding the best answer A involves a search over all A for the one which maximizes the probability of the above model: � A = arg max P(A I W, X). (2) A Using Bayes rule, making further conditional independence assumptions and assuming uniform prior probabilities, which therefore do not affect the optimisation criterion, we obtain the final optimisation criterion (see (Whittaker et al., 2005a) for more details): Some examples include: HOW, MANY, WHEN, WHO, UNTIL etc. The question-type mapping function W(Q) extracts n-tuples (n = 1, 2, ...) of question-type features from the question Q, such as HOW MANY and UNTIL WHEN. Modelling the complex relationship between W and A directly is non-trivial. We therefore introduce an intermediate variable representing classes of example questions-and-answers (q-and-a) (� for e = 1... ICEI drawn from the set CE, and to facilitate modelling we say that W is conditionally independent of A given (� as follows: arg max P(A I X) A v retrieval model - </context>
<context position="10791" citStr="Whittaker et al., 2005" startWordPosition="1833" endWordPosition="1836">tion 2.2. 2.1 Retrieval model The retrieval model essentially models the proximity of A to features in X. Since A = a1, ... , alA we are actually modelling the distribution of multi-word sequences. This should be borne in mind in the following discussion whenever A is used. As mentioned above, we currently use a deterministic information-feature mapping function X = X (Q). This mapping only generates word m-tuples (m = 1, 2, ...) from single words in Q that are not present in a stoplist of 50- 100 high-frequency words. For more details on the exact form of the retrieval model please refer to (Whittaker et al., 2005a). 2.2 Filter model A set of IVWI single-word features is extracted based on frequency of occurrence in question data. Given a set E of example q-and-a tj for I. = 1... IEI where tj = (qj, aj) = (~~1 � � qj , a1 � � a��A. ) we define a mapping function f E CE by f(tj) = e. Each class ce = (w1, ... , we �� � �a1, ... , a&apos; ��� ) is then obtained by ce = U j:f(tj)=e experiments in this paper no clustering of the qand-a is actually performed so each q-and-a example forms its own unique class i.e. each c, corresponds to a single tj and vice-versa. Assuming conditional independence of the answer wo</context>
</contexts>
<marker>Whittaker, Furui, Klakow, 2005</marker>
<rawString>E.W.D. Whittaker, S. Furui, and D. Klakow. 2005b. A Statistical Pattern Recognition Approach to Question Answering using Web Data. In Proceedings of Cyberworlds.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E W D Whittaker</author>
<author>J Hamonic</author>
<author>S Furui</author>
</authors>
<title>A Unified Approach to Japanese and English Question Answering.</title>
<date>2005</date>
<booktitle>In Proceedings ofNTCIR-5.</booktitle>
<contexts>
<context position="4619" citStr="Whittaker et al., 2005" startWordPosition="746" endWordPosition="749">approach is at the same time very different to other purely web-based approaches such as askMSR (Brill et al., 2002) and Aranea (Lin et al., 2002). For example, we use entire documents rather than the snippets of text returned by web search engines; we do not use structured document sources or databases and we do not transform the query in any way either by term re-ordering or by modifying the tense of verbs. These basic principles apply to each of our language-specific QA systems thus simplifying and accelerating development. The approach to QA that we adopt has previously been described in (Whittaker et al., 2005a; Whittaker et al., 2005b; Whittaker et al., 2005c) where the details of the mathematical model and how it was trained for English and Japanese were given. Our approach has also been successfully evaluated in the text retrieval conference (TREC) 2005 QA track evaluation (Voorhees, 2003) where our group placed eleventh out of thirty participants (Whittaker et al., 2005a). Although the TREC QA task is substantially different to webbased QA the TREC evaluation confirmed that our approach works and also provides an objective assessment of its quality. Similarly, for our Japanese language system w</context>
<context position="8737" citStr="Whittaker et al., 2005" startWordPosition="1463" endWordPosition="1466">Cruise married?” and “When was Tom Cruise married?” the information-bearing component is identical in both cases whereas the question-type component is different. 46 EACL 2006 Workshop on Multilingual Question Answering - MLQA06 Finding the best answer A involves a search over all A for the one which maximizes the probability of the above model: � A = arg max P(A I W, X). (2) A Using Bayes rule, making further conditional independence assumptions and assuming uniform prior probabilities, which therefore do not affect the optimisation criterion, we obtain the final optimisation criterion (see (Whittaker et al., 2005a) for more details): Some examples include: HOW, MANY, WHEN, WHO, UNTIL etc. The question-type mapping function W(Q) extracts n-tuples (n = 1, 2, ...) of question-type features from the question Q, such as HOW MANY and UNTIL WHEN. Modelling the complex relationship between W and A directly is non-trivial. We therefore introduce an intermediate variable representing classes of example questions-and-answers (q-and-a) (� for e = 1... ICEI drawn from the set CE, and to facilitate modelling we say that W is conditionally independent of A given (� as follows: arg max P(A I X) A v retrieval model - </context>
<context position="10791" citStr="Whittaker et al., 2005" startWordPosition="1833" endWordPosition="1836">tion 2.2. 2.1 Retrieval model The retrieval model essentially models the proximity of A to features in X. Since A = a1, ... , alA we are actually modelling the distribution of multi-word sequences. This should be borne in mind in the following discussion whenever A is used. As mentioned above, we currently use a deterministic information-feature mapping function X = X (Q). This mapping only generates word m-tuples (m = 1, 2, ...) from single words in Q that are not present in a stoplist of 50- 100 high-frequency words. For more details on the exact form of the retrieval model please refer to (Whittaker et al., 2005a). 2.2 Filter model A set of IVWI single-word features is extracted based on frequency of occurrence in question data. Given a set E of example q-and-a tj for I. = 1... IEI where tj = (qj, aj) = (~~1 � � qj , a1 � � a��A. ) we define a mapping function f E CE by f(tj) = e. Each class ce = (w1, ... , we �� � �a1, ... , a&apos; ��� ) is then obtained by ce = U j:f(tj)=e experiments in this paper no clustering of the qand-a is actually performed so each q-and-a example forms its own unique class i.e. each c, corresponds to a single tj and vice-versa. Assuming conditional independence of the answer wo</context>
</contexts>
<marker>Whittaker, Hamonic, Furui, 2005</marker>
<rawString>E.W.D. Whittaker, J. Hamonic, and S. Furui. 2005c. A Unified Approach to Japanese and English Question Answering. In Proceedings ofNTCIR-5.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>