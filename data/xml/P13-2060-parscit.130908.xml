<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000210">
<title confidence="0.962551">
Stacking for Statistical Machine Translation∗
</title>
<author confidence="0.986421">
Majid Razmara and Anoop Sarkar
</author>
<affiliation confidence="0.819137666666667">
School of Computing Science
Simon Fraser University
Burnaby, BC, Canada
</affiliation>
<email confidence="0.996617">
{razmara,anoop}@sfu.ca
</email>
<sectionHeader confidence="0.976692" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999649">
We propose the use of stacking, an ensem-
ble learning technique, to the statistical machine
translation (SMT) models. A diverse ensem-
ble of weak learners is created using the same
SMT engine (a hierarchical phrase-based sys-
tem) by manipulating the training data and a
strong model is created by combining the weak
models on-the-fly. Experimental results on two
language pairs and three different sizes of train-
ing data show significant improvements of up
to 4 BLEU points over a conventionally trained
SMT model.
</bodyText>
<sectionHeader confidence="0.999391" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.967294317073171">
Ensemble-based methods have been widely used
in machine learning with the aim of reduc-
ing the instability of classifiers and regressors
and/or increase their bias. The idea behind
ensemble learning is to combine multiple mod-
els, weak learners, in an attempt to produce a
strong model with less error. It has also been
successfully applied to a wide variety of tasks in
NLP (Tomeh et al., 2010; Surdeanu and Man-
ning, 2010; F. T. Martins et al., 2008; Sang, 2002)
and recently has attracted attention in the statis-
tical machine translation community in various
work (Xiao et al., 2013; Song et al., 2011; Xiao
et al., 2010; Lagarda and Casacuberta, 2008).
In this paper, we propose a method to adopt
stacking (Wolpert, 1992), an ensemble learning
technique, to SMT. We manipulate the full set of
training data, creating k disjoint sets of held-out
and held-in data sets as in k-fold cross-validation
and build a model on each partition. This creates
a diverse ensemble of statistical machine transla-
tion models where each member of the ensemble
has different feature function values for the SMT
log-linear model (Koehn, 2010). The weights of
model are then tuned using minimum error rate
training (Och, 2003) on the held-out fold to pro-
vide k weak models. We then create a strong
∗This research was partially supported by an NSERC,
Canada (RGPIN: 264905) grant and a Google Faculty Award
to the second author.
model by stacking another meta-learner on top of
weak models to combine them into a single model.
The particular second-tier model we use is a model
combination approach called ensemble decoding
which combines hypotheses from the weak mod-
els on-the-fly in the decoder.
Using this approach, we take advantage of the
diversity created by manipulating the training data
and obtain a significant and consistent improve-
ment over a conventionally trained SMT model
with a fixed training and tuning set.
</bodyText>
<sectionHeader confidence="0.9958" genericHeader="method">
2 Ensemble Learning Methods
</sectionHeader>
<bodyText confidence="0.999221655172414">
Two well-known instances of general framework
of ensemble learning are bagging and boosting.
Bagging (Breiman, 1996a) (bootstrap aggregat-
ing) takes a number of samples with replacement
from a training set. The generated sample set
may have 0, 1 or more instances of each origi-
nal training instance. This procedure is repeated
a number of times and the base learner is ap-
plied to each sample to produce a weak learner.
These models are aggregated by doing a uniform
voting for classification or averaging the predic-
tions for regression. Bagging reduces the vari-
ance of the base model while leaving the bias rela-
tively unchanged and is most useful when a small
change in the training data affects the prediction
of the model (i.e. the model is unstable) (Breiman,
1996a). Bagging has been recently applied to
SMT (Xiao et al., 2013; Song et al., 2011)
Boosting (Schapire, 1990) constructs a strong
learner by repeatedly choosing a weak learner
and applying it on a re-weighted training set. In
each iteration, a weak model is learned on the
training data, whose instance weights are modi-
fied from the previous iteration to concentrate on
examples on which the model predictions were
poor. By putting more weight on the wrongly
predicted examples, a diverse ensemble of weak
learners is created. Boosting has also been used in
SMT (Xiao et al., 2013; Xiao et al., 2010; Lagarda
</bodyText>
<page confidence="0.986508">
334
</page>
<note confidence="0.4748895">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 334–339,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figure confidence="0.2130372">
Algorithm 1: Stacking for SMT
Input: D = {(fj, ej�}N &gt; A parallel corpus
j=1
Input: k &gt; # of folds (i.e. weak learners)
Output: STRONGMODEL s
</figure>
<listItem confidence="0.9903542">
1: D1, ... ,Dk ← SPLIT(D, k)
2: for i = 1 → k do
3: Ti ← D − Di &gt; Use all but current partition as
training set.
4: φi ← TRAIN(Ti) &gt; Train feature functions.
5: Mi ← TUNE(φi, Di) &gt; Tune the model on the
current partition.
6: end for
7: s ← COMBINEMODELS(M1 , ..., Mk) &gt; Combine all
the base models to produce a strong stacked model.
</listItem>
<bodyText confidence="0.989887491228071">
and Casacuberta, 2008).
Stacking (or stacked generalization) (Wolpert,
1992) is another ensemble learning algorithm that
uses a second-level learning algorithm on top of
the base learners to reduce the bias. The first
level consists of predictors g1, ... , gk where gi :
Rd -+ R, receiving input x E Rd and produc-
ing a prediction gi(x). The next level consists
of a single function h : Rd+k -+ R that takes
(x, g1(x), ... , gk(x)) as input and produces an en-
semble prediction yˆ = h(x, g1(x), ... , gk(x)).
Two categories of ensemble learning are ho-
mogeneous learning and heterogeneous learning.
In homogeneous learning, a single base learner
is used, and diversity is generated by data sam-
pling, feature sampling, randomization and pa-
rameter settings, among other strategies. In het-
erogeneous learning different learning algorithms
are applied to the same training data to create a
pool of diverse models. In this paper, we focus on
homogeneous ensemble learning by manipulating
the training data.
In the primary form of stacking (Wolpert,
1992), the training data is split into multiple dis-
joint sets of held-out and held-in data sets using
k-fold cross-validation and k models are trained
on the held-in partitions and run on held-out par-
titions. Then a meta-learner uses the predictions
of all models on their held-out sets and the actual
labels to learn a final model. The details of the
first-layer and second-layer predictors are consid-
ered to be a “black art” (Wolpert, 1992).
Breiman (1996b) linearly combines the weak
learners in the stacking framework. The weights
of the base learners are learned using ridge regres-
sion: s(x) = Ek αkmk(x), where mk is a base
model trained on the k-th partition of the data and
s is the resulting strong model created by linearly
interpolating the weak learners.
Stacking (aka blending) has been used in the
system that won the Netflix Prize1, which used a
multi-level stacking algorithm.
Stacking has been actively used in statistical
parsing: Nivre and McDonald (2008) integrated
two models for dependency parsing by letting one
model learn from features generated by the other;
F. T. Martins et al. (2008) further formalized the
stacking algorithm and improved on Nivre and
McDonald (2008); Surdeanu and Manning (2010)
includes a detailed analysis of ensemble models
for statistical parsing: i) the diversity of base
parsers is more important than the complexity of
the models; ii) unweighted voting performs as well
as weighted voting; and iii) ensemble models that
combine at decoding time significantly outperform
models that combine multiple models at training
time.
</bodyText>
<sectionHeader confidence="0.988474" genericHeader="method">
3 Our Approach
</sectionHeader>
<bodyText confidence="0.999984454545455">
In this paper, we propose a method to apply stack-
ing to statistical machine translation (SMT) and
our method is the first to successfully exploit
stacking for statistical machine translation. We
use a standard statistical machine translation en-
gine and produce multiple diverse models by par-
titioning the training set using the k-fold cross-
validation technique. A diverse ensemble of weak
systems is created by learning a model on each
k −1 fold and tuning the statistical machine trans-
lation log-linear weights on the remaining fold.
However, instead of learning a model on the output
of base models as in (Wolpert, 1992), we combine
hypotheses from the base models in the decoder
with uniform weights. For the base learner, we
use Kriya (Sankaran et al., 2012), an in-house hier-
archical phrase-based machine translation system,
to produce multiple weak models. These mod-
els are combined together using Ensemble Decod-
ing (Razmara et al., 2012) to produce a strong
model in the decoder. This method is briefly ex-
plained in next section.
</bodyText>
<subsectionHeader confidence="0.99802">
3.1 Ensemble Decoding
</subsectionHeader>
<bodyText confidence="0.912086666666667">
SMT Log-linear models (Koehn, 2010) find the
most likely target language output a given the
source language input f using a vector of feature
functions 0:
( )
p(e|f) a exp w 0
</bodyText>
<footnote confidence="0.978931">
1http://www.netflixprize.com/
</footnote>
<page confidence="0.998258">
335
</page>
<bodyText confidence="0.9994498">
Ensemble decoding combines several models
dynamically at decoding time. The scores are
combined for each partial hypothesis using a
user-defined mixture operation ⊗ over component
models.
</bodyText>
<equation confidence="0.984514">
� �
p(e|f) ∝ exp w1 · φ1 ⊗ w2 · φ2 ⊗ ...
</equation>
<bodyText confidence="0.995158916666667">
We previously successfully applied ensemble
decoding to domain adaptation in SMT and
showed that it performed better than approaches
that pre-compute linear mixtures of different mod-
els (Razmara et al., 2012). Several mixture oper-
ations were proposed, allowing the user to encode
belief about the relative strengths of the compo-
nent models. These mixture operations receive
two or more probabilities and return the mixture
probability p(¯e  |f) for each rule ¯e, f used in the
decoder. Different options for these operations
are:
</bodyText>
<listItem confidence="0.992333">
• Weighted Sum (wsum) is defined as:
</listItem>
<equation confidence="0.992820333333333">
M
p(¯e  |f) ∝ E λm exp (wm · φm)
m
</equation>
<bodyText confidence="0.999896333333333">
where m denotes the index of component
models, M is the total number of them and
λm is the weight for component m.
</bodyText>
<listItem confidence="0.99386275">
• Weighted Max (wmax) is defined as:
¯f) ∝ max (λm exp (wm · φm))
m
• Prod or log-wsum is defined as:
</listItem>
<equation confidence="0.958940666666667">
M
p(¯e  |¯f) ∝ exp ( E λm (wm · φm))
m
</equation>
<listItem confidence="0.9769466">
• Model Switching (Switch): Each cell in the
CKY chart is populated only by rules from
one of the models and the other models’ rules
are discarded. Each component model is con-
sidered as an expert on different spans of the
</listItem>
<table confidence="0.989776">
Train size Src tokens Tgt tokens
0+dev 67K 58K
Fr - En 10k+dev 365K 327K
100k+dev 3M 2.8M
0+dev 60K 58K
Es - En 10k+dev 341K 326K
100k+dev 2.9M 2.8M
</table>
<tableCaption confidence="0.983337">
Table 1: Statistics of the training set for different systems and
different language pairs.
</tableCaption>
<bodyText confidence="0.995138">
(SW:MAX), i.e. for each cell, the model that
has the highest weighted score wins:
</bodyText>
<equation confidence="0.9758215">
ψ( f,n) = λn max
e
</equation>
<bodyText confidence="0.9357281">
Alternatively, we can pick the model with
highest weighted sum of the probabilities of
the rules (SW:SUM). This sum has to take into
account the translation table limit (ttl), on the
number of rules suggested by each model for
each cell:
exp (wn · φn(¯e,¯f))
The probability of each phrase-pair (¯e, f) is
then:
p(¯e |
</bodyText>
<sectionHeader confidence="0.997796" genericHeader="method">
4 Experiments &amp; Results
</sectionHeader>
<bodyText confidence="0.999903294117647">
We experimented with two language pairs: French
to English and Spanish to English on the Europarl
corpus (v7) (Koehn, 2005) and used ACL/WMT
2005 2 data for dev and test sets.
For the base models, we used an in-house
implementation of hierarchical phrase-based sys-
tems, Kriya (Sankaran et al., 2012), which uses
the same features mentioned in (Chiang, 2005):
forward and backward relative-frequency and lex-
ical TM probabilities; LM; word, phrase and glue-
rules penalty. GIZA++ (Och and Ney, 2003) has
been used for word alignment with phrase length
limit of 10. Feature weights were optimized using
MERT (Och, 2003). We built a 5-gram language
model on the English side of Europarl and used the
Kneser-Ney smoothing method and SRILM (Stol-
cke, 2002) as the language model toolkit.
</bodyText>
<footnote confidence="0.572353">
2http://www.statmt.org/wpt05/mt-shared-task/
</footnote>
<figure confidence="0.962147833333333">
p(¯e |
source. A binary indicator function δ( f, m)
picks a component model for each span:
δ( f,m) = { 1, m = argmax ψ( f, n)
n∈M
0, otherwise
</figure>
<bodyText confidence="0.9118255">
The criteria for choosing a model for each
cell, ψ( ¯f, n), could be based on max
</bodyText>
<equation confidence="0.9981714">
(wn · φn(¯e, f))
E¯f,n) = λn
e¯
ψ(
f,m) pm(¯e |f)
δ(
f) =
M
E
m
</equation>
<page confidence="0.997806">
336
</page>
<table confidence="0.999894571428572">
Direction k-fold Resub Mean WSUM
2 18.08 19.67 22.32
Fr - En 4 18.08 21.80 23.14
8 18.08 22.47 23.76
2 18.61 19.23 21.62
Es - En 4 18.61 21.52 23.42
8 18.61 22.20 23.69
WMAX PROD SW:MAX SW:SUM
22.48 22.06 21.70 21.81
23.48 23.55 22.83 22.95
23.75 23.78 23.02 23.47
21.33 21.49 21.48 21.51
22.81 22.91 22.81 22.92
23.89 23.51 22.92 23.26
</table>
<tableCaption confidence="0.973668">
Table 2: Testset BLEU scores when applying stacking on the devset only (using no specific training set).
</tableCaption>
<table confidence="0.951928142857143">
Direction Corpus k-fold Baseline BMA WSUM WMAX PROD SW:MAX SW:SUM
10k+dev 6 28.75 29.49 29.87 29.78 29.21 29.69 29.59
Fr - En
100k+dev 11 / 51 29.53 29.75 34.00 34.07 33.11 34.05 33.96
10k+dev 6 28.21 28.76 29.59 29.51 29.15 29.10 29.21
Es - En
100k+dev 11 / 51 33.25 33.44 34.21 34.00 33.17 34.19 34.22
</table>
<tableCaption confidence="0.999785">
Table 3: Testset BLEU scores when using 10k and 100k sentence training sets along with the devset.
</tableCaption>
<subsectionHeader confidence="0.99331">
4.1 Training on devset
</subsectionHeader>
<bodyText confidence="0.999572791666666">
We first consider the scenario in which there is
no parallel data between a language pair except
a small bi-text used as a devset. We use no spe-
cific training data and construct a SMT system
completely on the devset by using our approach
and compare to two different baselines. A natu-
ral baseline when having a limited parallel text is
to do re-substitution validation where the model
is trained on the whole devset and is tuned on the
same set. This validation process suffers seriously
from over-fitting. The second baseline is the mean
of BLEU scores of all base models.
Table 2 summarizes the BLEU scores on the
testset when using stacking only on the devset on
two different language pairs. As the table shows,
increasing the number of folds results in higher
BLEU scores. However, doing such will generally
lead to higher variance among base learners.
Figure 1 shows the BLEU score of each of the
base models resulted from a 20-fold partitioning
of the devset along with the strong models’ BLEU
scores. As the figure shows, the strong models are
generally superior to the base models whose mean
is represented as a horizontal line.
</bodyText>
<subsectionHeader confidence="0.997998">
4.2 Training on train+dev
</subsectionHeader>
<bodyText confidence="0.999588931034483">
When we have some training data, we can use
the cross-validation-style partitioning to create k
splits. We then train a system on k − 1 folds and
tune on the devset. However, each system eventu-
ally wastes a fold of the training data. In order to
take advantage of that remaining fold, we concate-
nate the devset to the training set and partition the
whole union. In this way, we use all data available
to us. We experimented with two sizes of train-
ing data: 10k sentence pairs and 100k, that with
the addition of the devset, we have 12k and 102k
sentence-pair corpora.
Table 1 summarizes statistics of the data sets
used in this scenario. Table 3 reports the BLEU
scores when using stacking on these two corpus
sizes. The baselines are the conventional systems
which are built on the training-set only and tuned
on the devset as well as Bayesian Model Averaging
(BMA, see §5). For the 100k+dev corpus, we sam-
pled 11 partitions from all 51 possible partitions
by taking every fifth partition as training data. The
results in Table 3 show that stacking can improve
over the baseline BLEU scores by up to 4 points.
Examining the performance of the different
mixture operations, we can see that WSUM and
WMAX typically outperform other mixture oper-
ations. Different mixture operations can be domi-
nant in different language pairs and different sizes
of training sets.
</bodyText>
<sectionHeader confidence="0.999925" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999943142857143">
Xiao et al. (2013) have applied both boosting
and bagging on three different statistical machine
translation engines: phrase-based (Koehn et al.,
2003), hierarchical phrase-based (Chiang, 2005)
and syntax-based (Galley et al., 2006) and showed
SMT can benefit from these methods as well.
Duan et al. (2009) creates an ensemble of mod-
els by using feature subspace method in the ma-
chine learning literature (Ho, 1998). Each mem-
ber of the ensemble is built by removing one non-
LM feature in the log-linear framework or varying
the order of language model. Finally they use a
sentence-level system combination on the outputs
of the base models to pick the best system for each
</bodyText>
<page confidence="0.996612">
337
</page>
<figureCaption confidence="0.988869">
Figure 1: BLEU scores for all the base models and stacked models on the Fr-En devset with 20-fold cross validation. The
horizontal line shows the mean of base models’ scores.
</figureCaption>
<bodyText confidence="0.995747272727273">
sentence. Though, they do not combine the hy-
potheses search spaces of individual base models.
Our work is most similar to that of Duan et
al. (2010) which uses Bayesian model averaging
(BMA) (Hoeting et al., 1999) for SMT. They used
sampling without replacement to create a num-
ber of base models whose phrase-tables are com-
bined with that of the baseline (trained on the full
training-set) using linear mixture models (Foster
and Kuhn, 2007).
Our approach differs from this approach in a
number of ways: i) we use cross-validation-style
partitioning for creating training subsets while
they do sampling without replacement (80% of the
training set); ii) in our approach a number of base
models are trained and tuned and they are com-
bined on-the-fly in the decoder using ensemble de-
coding which has been shown to be more effective
than offline combination of phrase-table-only fea-
tures; iii) in Duan et al. (2010)’s method, each sys-
tem gives up 20% of the training data in exchange
for more diversity, but in contrast, our method not
only uses all available data for training, but pro-
motes diversity through allowing each model to
tune on a different data set; iv) our approach takes
advantage of held out data (the tuning set) in the
training of base models which is beneficial espe-
cially when little parallel data is available or tun-
ing/test sets and training sets are from different do-
mains.
Empirical results (Table 3) also show that our
approach outperforms the Bayesian model averag-
ing approach (BMA).
</bodyText>
<sectionHeader confidence="0.996414" genericHeader="conclusions">
6 Conclusion &amp; Future Work
</sectionHeader>
<bodyText confidence="0.999926">
In this paper, we proposed a novel method on ap-
plying stacking to the statistical machine transla-
tion task. The results when using no, 10k and 100k
sentence-pair training sets (along with a develop-
ment set for tuning) show that stacking can yield
an improvement of up to 4 BLEU points over con-
ventionally trained SMT models which use a fixed
training and tuning set.
Future work includes experimenting with larger
training sets to investigate how useful this ap-
proach can be when having different sizes of train-
ing data.
</bodyText>
<sectionHeader confidence="0.999145" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995023052631579">
Leo Breiman. 1996a. Bagging predictors. Machine
Learning, 24(2):123–140, August.
Leo Breiman. 1996b. Stacked regressions. Machine
Learning, 24(1):49–64, July.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In ACL
’05: Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 263–
270, Morristown, NJ, USA. ACL.
Nan Duan, Mu Li, Tong Xiao, and Ming Zhou. 2009.
The feature subspace method for smt system combi-
nation. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 3 - Volume 3, EMNLP ’09, pages 1096–
1104, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Nan Duan, Hong Sun, and Ming Zhou. 2010. Transla-
tion model generalization using probability averag-
ing for machine translation. In Proceedings of the
</reference>
<page confidence="0.993822">
338
</page>
<reference confidence="0.997330055045871">
23rd International Conference on Computational
Linguistics, COLING ’10, pages 304–312, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Andr´e F. T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
157–166, Honolulu, Hawaii, October. Association
for Computational Linguistics.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for smt. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
StatMT ’07, pages 128–135, Stroudsburg, PA, USA.
ACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training
of context-rich syntactic translation models. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, ACL-44, pages 961–968, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Tin Kam Ho. 1998. The random subspace method for
constructing decision forests. IEEE Trans. Pattern
Anal. Mach. Intell., 20(8):832–844, August.
Jennifer A. Hoeting, David Madigan, Adrian E.
Raftery, and Chris T. Volinsky. 1999. Bayesian
Model Averaging: A Tutorial. Statistical Science,
14(4):382–401.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the Human Language Technology Con-
ference of the NAACL, pages 127–133, Edmonton,
May. NAACL.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT summit, volume 5.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York, NY, USA,
1st edition.
Antonio Lagarda and Francisco Casacuberta. 2008.
Applying boosting to statistical machine translation.
In Annual Meeting of European Association for Ma-
chine Translation (EAMT), pages 88–96.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings of ACL-08: HLT, pages
950–958, Columbus, Ohio, June. Association for
Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19–51, March.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings of
the 41th Annual Meeting of the ACL, Sapporo, July.
ACL.
Majid Razmara, George Foster, Baskaran Sankaran,
and Anoop Sarkar. 2012. Mixing multiple transla-
tion models in statistical machine translation. In The
50th Annual Meeting of the Association for Compu-
tational Linguistics, Proceedings of the Conference,
July 8-14, 2012, Jeju Island, Korea - Volume 1: Long
Papers, pages 940–949. The Association for Com-
puter Linguistics.
Erik F. Tjong Kim Sang. 2002. Memory-based shal-
low parsing. J. Mach. Learn. Res., 2:559–594,
March.
Baskaran Sankaran, Majid Razmara, and Anoop
Sarkar. 2012. Kriya an end-to-end hierarchical
phrase-based mt system. The Prague Bulletin of
Mathematical Linguistics, 97(97), April.
Robert E. Schapire. 1990. The strength of weak learn-
ability. Mach. Learn., 5(2):197–227, July.
Linfeng Song, Haitao Mi, Yajuan L¨u, and Qun Liu.
2011. Bagging-based system combination for do-
main adaption. In Proceedings of the 13th Machine
Translation Summit (MT Summit XIII), pages 293–
299. International Association for Machine Transla-
tion, September.
Andreas Stolcke. 2002. SRILM – an extensible lan-
guage modeling toolkit. In Proceedings Interna-
tional Conference on Spoken Language Processing,
pages 257–286.
Mihai Surdeanu and Christopher D. Manning. 2010.
Ensemble models for dependency parsing: cheap
and good? In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ’10, pages 649–652, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Nadi Tomeh, Alexandre Allauzen, Guillaume Wis-
niewski, and Franc¸ois Yvon. 2010. Refining word
alignment with discriminative training. In Proceed-
ings of The Ninth Conference of the Association for
Machine Translation in the Americas (AMTA 2010).
David H. Wolpert. 1992. Stacked generalization. Neu-
ral Networks, 5:241–259.
Tong Xiao, Jingbo Zhu, Muhua Zhu, and Huizhen
Wang. 2010. Boosting-based system combina-
tion for machine translation. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ’10, pages 739–748,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Tong Xiao, Jingbo Zhu, and Tongran Liu. 2013. Bag-
ging and boosting statistical machine translation sys-
tems. Artificial Intelligence, 195:496–527, Febru-
ary.
</reference>
<page confidence="0.999216">
339
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.191696">
<title confidence="0.531349">for Statistical Machine Razmara School of Computing</title>
<author confidence="0.575419">Simon Fraser Burnaby</author>
<author confidence="0.575419">BC</author>
<abstract confidence="0.986231538461539">propose the use of an ensemble learning technique, to the statistical machine translation (SMT) models. A diverse ensemble of weak learners is created using the same SMT engine (a hierarchical phrase-based system) by manipulating the training data and a strong model is created by combining the weak models on-the-fly. Experimental results on two language pairs and three different sizes of training data show significant improvements of up points over a conventionally trained SMT model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Bagging predictors.</title>
<date>1996</date>
<booktitle>Machine Learning,</booktitle>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="2756" citStr="Breiman, 1996" startWordPosition="441" endWordPosition="442">eta-learner on top of weak models to combine them into a single model. The particular second-tier model we use is a model combination approach called ensemble decoding which combines hypotheses from the weak models on-the-fly in the decoder. Using this approach, we take advantage of the diversity created by manipulating the training data and obtain a significant and consistent improvement over a conventionally trained SMT model with a fixed training and tuning set. 2 Ensemble Learning Methods Two well-known instances of general framework of ensemble learning are bagging and boosting. Bagging (Breiman, 1996a) (bootstrap aggregating) takes a number of samples with replacement from a training set. The generated sample set may have 0, 1 or more instances of each original training instance. This procedure is repeated a number of times and the base learner is applied to each sample to produce a weak learner. These models are aggregated by doing a uniform voting for classification or averaging the predictions for regression. Bagging reduces the variance of the base model while leaving the bias relatively unchanged and is most useful when a small change in the training data affects the prediction of th</context>
<context position="6180" citStr="Breiman (1996" startWordPosition="1031" endWordPosition="1032">ol of diverse models. In this paper, we focus on homogeneous ensemble learning by manipulating the training data. In the primary form of stacking (Wolpert, 1992), the training data is split into multiple disjoint sets of held-out and held-in data sets using k-fold cross-validation and k models are trained on the held-in partitions and run on held-out partitions. Then a meta-learner uses the predictions of all models on their held-out sets and the actual labels to learn a final model. The details of the first-layer and second-layer predictors are considered to be a “black art” (Wolpert, 1992). Breiman (1996b) linearly combines the weak learners in the stacking framework. The weights of the base learners are learned using ridge regression: s(x) = Ek αkmk(x), where mk is a base model trained on the k-th partition of the data and s is the resulting strong model created by linearly interpolating the weak learners. Stacking (aka blending) has been used in the system that won the Netflix Prize1, which used a multi-level stacking algorithm. Stacking has been actively used in statistical parsing: Nivre and McDonald (2008) integrated two models for dependency parsing by letting one model learn from featu</context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>Leo Breiman. 1996a. Bagging predictors. Machine Learning, 24(2):123–140, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Stacked regressions.</title>
<date>1996</date>
<booktitle>Machine Learning,</booktitle>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="2756" citStr="Breiman, 1996" startWordPosition="441" endWordPosition="442">eta-learner on top of weak models to combine them into a single model. The particular second-tier model we use is a model combination approach called ensemble decoding which combines hypotheses from the weak models on-the-fly in the decoder. Using this approach, we take advantage of the diversity created by manipulating the training data and obtain a significant and consistent improvement over a conventionally trained SMT model with a fixed training and tuning set. 2 Ensemble Learning Methods Two well-known instances of general framework of ensemble learning are bagging and boosting. Bagging (Breiman, 1996a) (bootstrap aggregating) takes a number of samples with replacement from a training set. The generated sample set may have 0, 1 or more instances of each original training instance. This procedure is repeated a number of times and the base learner is applied to each sample to produce a weak learner. These models are aggregated by doing a uniform voting for classification or averaging the predictions for regression. Bagging reduces the variance of the base model while leaving the bias relatively unchanged and is most useful when a small change in the training data affects the prediction of th</context>
<context position="6180" citStr="Breiman (1996" startWordPosition="1031" endWordPosition="1032">ol of diverse models. In this paper, we focus on homogeneous ensemble learning by manipulating the training data. In the primary form of stacking (Wolpert, 1992), the training data is split into multiple disjoint sets of held-out and held-in data sets using k-fold cross-validation and k models are trained on the held-in partitions and run on held-out partitions. Then a meta-learner uses the predictions of all models on their held-out sets and the actual labels to learn a final model. The details of the first-layer and second-layer predictors are considered to be a “black art” (Wolpert, 1992). Breiman (1996b) linearly combines the weak learners in the stacking framework. The weights of the base learners are learned using ridge regression: s(x) = Ek αkmk(x), where mk is a base model trained on the k-th partition of the data and s is the resulting strong model created by linearly interpolating the weak learners. Stacking (aka blending) has been used in the system that won the Netflix Prize1, which used a multi-level stacking algorithm. Stacking has been actively used in statistical parsing: Nivre and McDonald (2008) integrated two models for dependency parsing by letting one model learn from featu</context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>Leo Breiman. 1996b. Stacked regressions. Machine Learning, 24(1):49–64, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>263--270</pages>
<publisher>ACL.</publisher>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="10933" citStr="Chiang, 2005" startWordPosition="1841" endWordPosition="1842">the rules (SW:SUM). This sum has to take into account the translation table limit (ttl), on the number of rules suggested by each model for each cell: exp (wn · φn(¯e,¯f)) The probability of each phrase-pair (¯e, f) is then: p(¯e | 4 Experiments &amp; Results We experimented with two language pairs: French to English and Spanish to English on the Europarl corpus (v7) (Koehn, 2005) and used ACL/WMT 2005 2 data for dev and test sets. For the base models, we used an in-house implementation of hierarchical phrase-based systems, Kriya (Sankaran et al., 2012), which uses the same features mentioned in (Chiang, 2005): forward and backward relative-frequency and lexical TM probabilities; LM; word, phrase and gluerules penalty. GIZA++ (Och and Ney, 2003) has been used for word alignment with phrase length limit of 10. Feature weights were optimized using MERT (Och, 2003). We built a 5-gram language model on the English side of Europarl and used the Kneser-Ney smoothing method and SRILM (Stolcke, 2002) as the language model toolkit. 2http://www.statmt.org/wpt05/mt-shared-task/ p(¯e | source. A binary indicator function δ( f, m) picks a component model for each span: δ( f,m) = { 1, m = argmax ψ( f, n) n∈M 0, </context>
<context position="15295" citStr="Chiang, 2005" startWordPosition="2602" endWordPosition="2603"> by taking every fifth partition as training data. The results in Table 3 show that stacking can improve over the baseline BLEU scores by up to 4 points. Examining the performance of the different mixture operations, we can see that WSUM and WMAX typically outperform other mixture operations. Different mixture operations can be dominant in different language pairs and different sizes of training sets. 5 Related Work Xiao et al. (2013) have applied both boosting and bagging on three different statistical machine translation engines: phrase-based (Koehn et al., 2003), hierarchical phrase-based (Chiang, 2005) and syntax-based (Galley et al., 2006) and showed SMT can benefit from these methods as well. Duan et al. (2009) creates an ensemble of models by using feature subspace method in the machine learning literature (Ho, 1998). Each member of the ensemble is built by removing one nonLM feature in the log-linear framework or varying the order of language model. Finally they use a sentence-level system combination on the outputs of the base models to pick the best system for each 337 Figure 1: BLEU scores for all the base models and stacked models on the Fr-En devset with 20-fold cross validation. T</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 263– 270, Morristown, NJ, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nan Duan</author>
<author>Mu Li</author>
<author>Tong Xiao</author>
<author>Ming Zhou</author>
</authors>
<title>The feature subspace method for smt system combination.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3 - Volume 3, EMNLP ’09,</booktitle>
<pages>1096--1104</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="15408" citStr="Duan et al. (2009)" startWordPosition="2620" endWordPosition="2623">r the baseline BLEU scores by up to 4 points. Examining the performance of the different mixture operations, we can see that WSUM and WMAX typically outperform other mixture operations. Different mixture operations can be dominant in different language pairs and different sizes of training sets. 5 Related Work Xiao et al. (2013) have applied both boosting and bagging on three different statistical machine translation engines: phrase-based (Koehn et al., 2003), hierarchical phrase-based (Chiang, 2005) and syntax-based (Galley et al., 2006) and showed SMT can benefit from these methods as well. Duan et al. (2009) creates an ensemble of models by using feature subspace method in the machine learning literature (Ho, 1998). Each member of the ensemble is built by removing one nonLM feature in the log-linear framework or varying the order of language model. Finally they use a sentence-level system combination on the outputs of the base models to pick the best system for each 337 Figure 1: BLEU scores for all the base models and stacked models on the Fr-En devset with 20-fold cross validation. The horizontal line shows the mean of base models’ scores. sentence. Though, they do not combine the hypotheses se</context>
</contexts>
<marker>Duan, Li, Xiao, Zhou, 2009</marker>
<rawString>Nan Duan, Mu Li, Tong Xiao, and Ming Zhou. 2009. The feature subspace method for smt system combination. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3 - Volume 3, EMNLP ’09, pages 1096– 1104, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nan Duan</author>
<author>Hong Sun</author>
<author>Ming Zhou</author>
</authors>
<title>Translation model generalization using probability averaging for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10,</booktitle>
<pages>304--312</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="16101" citStr="Duan et al. (2010)" startWordPosition="2743" endWordPosition="2746">ine learning literature (Ho, 1998). Each member of the ensemble is built by removing one nonLM feature in the log-linear framework or varying the order of language model. Finally they use a sentence-level system combination on the outputs of the base models to pick the best system for each 337 Figure 1: BLEU scores for all the base models and stacked models on the Fr-En devset with 20-fold cross validation. The horizontal line shows the mean of base models’ scores. sentence. Though, they do not combine the hypotheses search spaces of individual base models. Our work is most similar to that of Duan et al. (2010) which uses Bayesian model averaging (BMA) (Hoeting et al., 1999) for SMT. They used sampling without replacement to create a number of base models whose phrase-tables are combined with that of the baseline (trained on the full training-set) using linear mixture models (Foster and Kuhn, 2007). Our approach differs from this approach in a number of ways: i) we use cross-validation-style partitioning for creating training subsets while they do sampling without replacement (80% of the training set); ii) in our approach a number of base models are trained and tuned and they are combined on-the-fly</context>
</contexts>
<marker>Duan, Sun, Zhou, 2010</marker>
<rawString>Nan Duan, Hong Sun, and Ming Zhou. 2010. Translation model generalization using probability averaging for machine translation. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10, pages 304–312, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Stacking dependency parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>157--166</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="1156" citStr="Martins et al., 2008" startWordPosition="181" endWordPosition="184"> two language pairs and three different sizes of training data show significant improvements of up to 4 BLEU points over a conventionally trained SMT model. 1 Introduction Ensemble-based methods have been widely used in machine learning with the aim of reducing the instability of classifiers and regressors and/or increase their bias. The idea behind ensemble learning is to combine multiple models, weak learners, in an attempt to produce a strong model with less error. It has also been successfully applied to a wide variety of tasks in NLP (Tomeh et al., 2010; Surdeanu and Manning, 2010; F. T. Martins et al., 2008; Sang, 2002) and recently has attracted attention in the statistical machine translation community in various work (Xiao et al., 2013; Song et al., 2011; Xiao et al., 2010; Lagarda and Casacuberta, 2008). In this paper, we propose a method to adopt stacking (Wolpert, 1992), an ensemble learning technique, to SMT. We manipulate the full set of training data, creating k disjoint sets of held-out and held-in data sets as in k-fold cross-validation and build a model on each partition. This creates a diverse ensemble of statistical machine translation models where each member of the ensemble has d</context>
<context position="6835" citStr="Martins et al. (2008)" startWordPosition="1137" endWordPosition="1140">ners in the stacking framework. The weights of the base learners are learned using ridge regression: s(x) = Ek αkmk(x), where mk is a base model trained on the k-th partition of the data and s is the resulting strong model created by linearly interpolating the weak learners. Stacking (aka blending) has been used in the system that won the Netflix Prize1, which used a multi-level stacking algorithm. Stacking has been actively used in statistical parsing: Nivre and McDonald (2008) integrated two models for dependency parsing by letting one model learn from features generated by the other; F. T. Martins et al. (2008) further formalized the stacking algorithm and improved on Nivre and McDonald (2008); Surdeanu and Manning (2010) includes a detailed analysis of ensemble models for statistical parsing: i) the diversity of base parsers is more important than the complexity of the models; ii) unweighted voting performs as well as weighted voting; and iii) ensemble models that combine at decoding time significantly outperform models that combine multiple models at training time. 3 Our Approach In this paper, we propose a method to apply stacking to statistical machine translation (SMT) and our method is the fir</context>
</contexts>
<marker>Martins, Das, Smith, Xing, 2008</marker>
<rawString>Andr´e F. T. Martins, Dipanjan Das, Noah A. Smith, and Eric P. Xing. 2008. Stacking dependency parsers. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157–166, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Mixturemodel adaptation for smt.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07,</booktitle>
<pages>128--135</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="16394" citStr="Foster and Kuhn, 2007" startWordPosition="2791" endWordPosition="2794">h 337 Figure 1: BLEU scores for all the base models and stacked models on the Fr-En devset with 20-fold cross validation. The horizontal line shows the mean of base models’ scores. sentence. Though, they do not combine the hypotheses search spaces of individual base models. Our work is most similar to that of Duan et al. (2010) which uses Bayesian model averaging (BMA) (Hoeting et al., 1999) for SMT. They used sampling without replacement to create a number of base models whose phrase-tables are combined with that of the baseline (trained on the full training-set) using linear mixture models (Foster and Kuhn, 2007). Our approach differs from this approach in a number of ways: i) we use cross-validation-style partitioning for creating training subsets while they do sampling without replacement (80% of the training set); ii) in our approach a number of base models are trained and tuned and they are combined on-the-fly in the decoder using ensemble decoding which has been shown to be more effective than offline combination of phrase-table-only features; iii) in Duan et al. (2010)’s method, each system gives up 20% of the training data in exchange for more diversity, but in contrast, our method not only use</context>
</contexts>
<marker>Foster, Kuhn, 2007</marker>
<rawString>George Foster and Roland Kuhn. 2007. Mixturemodel adaptation for smt. In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07, pages 128–135, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>961--968</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="15334" citStr="Galley et al., 2006" startWordPosition="2606" endWordPosition="2609"> as training data. The results in Table 3 show that stacking can improve over the baseline BLEU scores by up to 4 points. Examining the performance of the different mixture operations, we can see that WSUM and WMAX typically outperform other mixture operations. Different mixture operations can be dominant in different language pairs and different sizes of training sets. 5 Related Work Xiao et al. (2013) have applied both boosting and bagging on three different statistical machine translation engines: phrase-based (Koehn et al., 2003), hierarchical phrase-based (Chiang, 2005) and syntax-based (Galley et al., 2006) and showed SMT can benefit from these methods as well. Duan et al. (2009) creates an ensemble of models by using feature subspace method in the machine learning literature (Ho, 1998). Each member of the ensemble is built by removing one nonLM feature in the log-linear framework or varying the order of language model. Finally they use a sentence-level system combination on the outputs of the base models to pick the best system for each 337 Figure 1: BLEU scores for all the base models and stacked models on the Fr-En devset with 20-fold cross validation. The horizontal line shows the mean of ba</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44, pages 961–968, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tin Kam Ho</author>
</authors>
<title>The random subspace method for constructing decision forests.</title>
<date>1998</date>
<journal>IEEE Trans. Pattern Anal. Mach. Intell.,</journal>
<volume>20</volume>
<issue>8</issue>
<contexts>
<context position="15517" citStr="Ho, 1998" startWordPosition="2641" endWordPosition="2642">e that WSUM and WMAX typically outperform other mixture operations. Different mixture operations can be dominant in different language pairs and different sizes of training sets. 5 Related Work Xiao et al. (2013) have applied both boosting and bagging on three different statistical machine translation engines: phrase-based (Koehn et al., 2003), hierarchical phrase-based (Chiang, 2005) and syntax-based (Galley et al., 2006) and showed SMT can benefit from these methods as well. Duan et al. (2009) creates an ensemble of models by using feature subspace method in the machine learning literature (Ho, 1998). Each member of the ensemble is built by removing one nonLM feature in the log-linear framework or varying the order of language model. Finally they use a sentence-level system combination on the outputs of the base models to pick the best system for each 337 Figure 1: BLEU scores for all the base models and stacked models on the Fr-En devset with 20-fold cross validation. The horizontal line shows the mean of base models’ scores. sentence. Though, they do not combine the hypotheses search spaces of individual base models. Our work is most similar to that of Duan et al. (2010) which uses Baye</context>
</contexts>
<marker>Ho, 1998</marker>
<rawString>Tin Kam Ho. 1998. The random subspace method for constructing decision forests. IEEE Trans. Pattern Anal. Mach. Intell., 20(8):832–844, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer A Hoeting</author>
<author>David Madigan</author>
<author>Adrian E Raftery</author>
<author>Chris T Volinsky</author>
</authors>
<title>Bayesian Model Averaging: A Tutorial.</title>
<date>1999</date>
<journal>Statistical Science,</journal>
<volume>14</volume>
<issue>4</issue>
<contexts>
<context position="16166" citStr="Hoeting et al., 1999" startWordPosition="2753" endWordPosition="2756">e is built by removing one nonLM feature in the log-linear framework or varying the order of language model. Finally they use a sentence-level system combination on the outputs of the base models to pick the best system for each 337 Figure 1: BLEU scores for all the base models and stacked models on the Fr-En devset with 20-fold cross validation. The horizontal line shows the mean of base models’ scores. sentence. Though, they do not combine the hypotheses search spaces of individual base models. Our work is most similar to that of Duan et al. (2010) which uses Bayesian model averaging (BMA) (Hoeting et al., 1999) for SMT. They used sampling without replacement to create a number of base models whose phrase-tables are combined with that of the baseline (trained on the full training-set) using linear mixture models (Foster and Kuhn, 2007). Our approach differs from this approach in a number of ways: i) we use cross-validation-style partitioning for creating training subsets while they do sampling without replacement (80% of the training set); ii) in our approach a number of base models are trained and tuned and they are combined on-the-fly in the decoder using ensemble decoding which has been shown to b</context>
</contexts>
<marker>Hoeting, Madigan, Raftery, Volinsky, 1999</marker>
<rawString>Jennifer A. Hoeting, David Madigan, Adrian E. Raftery, and Chris T. Volinsky. 1999. Bayesian Model Averaging: A Tutorial. Statistical Science, 14(4):382–401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL,</booktitle>
<pages>127--133</pages>
<publisher>NAACL.</publisher>
<location>Edmonton,</location>
<contexts>
<context position="15253" citStr="Koehn et al., 2003" startWordPosition="2596" endWordPosition="2599">ed 11 partitions from all 51 possible partitions by taking every fifth partition as training data. The results in Table 3 show that stacking can improve over the baseline BLEU scores by up to 4 points. Examining the performance of the different mixture operations, we can see that WSUM and WMAX typically outperform other mixture operations. Different mixture operations can be dominant in different language pairs and different sizes of training sets. 5 Related Work Xiao et al. (2013) have applied both boosting and bagging on three different statistical machine translation engines: phrase-based (Koehn et al., 2003), hierarchical phrase-based (Chiang, 2005) and syntax-based (Galley et al., 2006) and showed SMT can benefit from these methods as well. Duan et al. (2009) creates an ensemble of models by using feature subspace method in the machine learning literature (Ho, 1998). Each member of the ensemble is built by removing one nonLM feature in the log-linear framework or varying the order of language model. Finally they use a sentence-level system combination on the outputs of the base models to pick the best system for each 337 Figure 1: BLEU scores for all the base models and stacked models on the Fr-</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the Human Language Technology Conference of the NAACL, pages 127–133, Edmonton, May. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT summit,</booktitle>
<volume>5</volume>
<contexts>
<context position="10699" citStr="Koehn, 2005" startWordPosition="1802" endWordPosition="1803">ferent systems and different language pairs. (SW:MAX), i.e. for each cell, the model that has the highest weighted score wins: ψ( f,n) = λn max e Alternatively, we can pick the model with highest weighted sum of the probabilities of the rules (SW:SUM). This sum has to take into account the translation table limit (ttl), on the number of rules suggested by each model for each cell: exp (wn · φn(¯e,¯f)) The probability of each phrase-pair (¯e, f) is then: p(¯e | 4 Experiments &amp; Results We experimented with two language pairs: French to English and Spanish to English on the Europarl corpus (v7) (Koehn, 2005) and used ACL/WMT 2005 2 data for dev and test sets. For the base models, we used an in-house implementation of hierarchical phrase-based systems, Kriya (Sankaran et al., 2012), which uses the same features mentioned in (Chiang, 2005): forward and backward relative-frequency and lexical TM probabilities; LM; word, phrase and gluerules penalty. GIZA++ (Och and Ney, 2003) has been used for word alignment with phrase length limit of 10. Feature weights were optimized using MERT (Och, 2003). We built a 5-gram language model on the English side of Europarl and used the Kneser-Ney smoothing method a</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>P. Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT summit, volume 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Machine Translation.</title>
<date>2010</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA,</location>
<contexts>
<context position="1831" citStr="Koehn, 2010" startWordPosition="292" endWordPosition="293">stical machine translation community in various work (Xiao et al., 2013; Song et al., 2011; Xiao et al., 2010; Lagarda and Casacuberta, 2008). In this paper, we propose a method to adopt stacking (Wolpert, 1992), an ensemble learning technique, to SMT. We manipulate the full set of training data, creating k disjoint sets of held-out and held-in data sets as in k-fold cross-validation and build a model on each partition. This creates a diverse ensemble of statistical machine translation models where each member of the ensemble has different feature function values for the SMT log-linear model (Koehn, 2010). The weights of model are then tuned using minimum error rate training (Och, 2003) on the held-out fold to provide k weak models. We then create a strong ∗This research was partially supported by an NSERC, Canada (RGPIN: 264905) grant and a Google Faculty Award to the second author. model by stacking another meta-learner on top of weak models to combine them into a single model. The particular second-tier model we use is a model combination approach called ensemble decoding which combines hypotheses from the weak models on-the-fly in the decoder. Using this approach, we take advantage of the </context>
<context position="8410" citStr="Koehn, 2010" startWordPosition="1389" endWordPosition="1390">ranslation log-linear weights on the remaining fold. However, instead of learning a model on the output of base models as in (Wolpert, 1992), we combine hypotheses from the base models in the decoder with uniform weights. For the base learner, we use Kriya (Sankaran et al., 2012), an in-house hierarchical phrase-based machine translation system, to produce multiple weak models. These models are combined together using Ensemble Decoding (Razmara et al., 2012) to produce a strong model in the decoder. This method is briefly explained in next section. 3.1 Ensemble Decoding SMT Log-linear models (Koehn, 2010) find the most likely target language output a given the source language input f using a vector of feature functions 0: ( ) p(e|f) a exp w 0 1http://www.netflixprize.com/ 335 Ensemble decoding combines several models dynamically at decoding time. The scores are combined for each partial hypothesis using a user-defined mixture operation ⊗ over component models. � � p(e|f) ∝ exp w1 · φ1 ⊗ w2 · φ2 ⊗ ... We previously successfully applied ensemble decoding to domain adaptation in SMT and showed that it performed better than approaches that pre-compute linear mixtures of different models (Razmara e</context>
</contexts>
<marker>Koehn, 2010</marker>
<rawString>Philipp Koehn. 2010. Statistical Machine Translation. Cambridge University Press, New York, NY, USA, 1st edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Lagarda</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Applying boosting to statistical machine translation.</title>
<date>2008</date>
<booktitle>In Annual Meeting of European Association for Machine Translation (EAMT),</booktitle>
<pages>88--96</pages>
<contexts>
<context position="1360" citStr="Lagarda and Casacuberta, 2008" startWordPosition="214" endWordPosition="217"> have been widely used in machine learning with the aim of reducing the instability of classifiers and regressors and/or increase their bias. The idea behind ensemble learning is to combine multiple models, weak learners, in an attempt to produce a strong model with less error. It has also been successfully applied to a wide variety of tasks in NLP (Tomeh et al., 2010; Surdeanu and Manning, 2010; F. T. Martins et al., 2008; Sang, 2002) and recently has attracted attention in the statistical machine translation community in various work (Xiao et al., 2013; Song et al., 2011; Xiao et al., 2010; Lagarda and Casacuberta, 2008). In this paper, we propose a method to adopt stacking (Wolpert, 1992), an ensemble learning technique, to SMT. We manipulate the full set of training data, creating k disjoint sets of held-out and held-in data sets as in k-fold cross-validation and build a model on each partition. This creates a diverse ensemble of statistical machine translation models where each member of the ensemble has different feature function values for the SMT log-linear model (Koehn, 2010). The weights of model are then tuned using minimum error rate training (Och, 2003) on the held-out fold to provide k weak models</context>
</contexts>
<marker>Lagarda, Casacuberta, 2008</marker>
<rawString>Antonio Lagarda and Francisco Casacuberta. 2008. Applying boosting to statistical machine translation. In Annual Meeting of European Association for Machine Translation (EAMT), pages 88–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Ryan McDonald</author>
</authors>
<title>Integrating graph-based and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>950--958</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="6697" citStr="Nivre and McDonald (2008)" startWordPosition="1114" endWordPosition="1117">he first-layer and second-layer predictors are considered to be a “black art” (Wolpert, 1992). Breiman (1996b) linearly combines the weak learners in the stacking framework. The weights of the base learners are learned using ridge regression: s(x) = Ek αkmk(x), where mk is a base model trained on the k-th partition of the data and s is the resulting strong model created by linearly interpolating the weak learners. Stacking (aka blending) has been used in the system that won the Netflix Prize1, which used a multi-level stacking algorithm. Stacking has been actively used in statistical parsing: Nivre and McDonald (2008) integrated two models for dependency parsing by letting one model learn from features generated by the other; F. T. Martins et al. (2008) further formalized the stacking algorithm and improved on Nivre and McDonald (2008); Surdeanu and Manning (2010) includes a detailed analysis of ensemble models for statistical parsing: i) the diversity of base parsers is more important than the complexity of the models; ii) unweighted voting performs as well as weighted voting; and iii) ensemble models that combine at decoding time significantly outperform models that combine multiple models at training ti</context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In Proceedings of ACL-08: HLT, pages 950–958, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Comput. Linguist.,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="11071" citStr="Och and Ney, 2003" startWordPosition="1860" endWordPosition="1863">el for each cell: exp (wn · φn(¯e,¯f)) The probability of each phrase-pair (¯e, f) is then: p(¯e | 4 Experiments &amp; Results We experimented with two language pairs: French to English and Spanish to English on the Europarl corpus (v7) (Koehn, 2005) and used ACL/WMT 2005 2 data for dev and test sets. For the base models, we used an in-house implementation of hierarchical phrase-based systems, Kriya (Sankaran et al., 2012), which uses the same features mentioned in (Chiang, 2005): forward and backward relative-frequency and lexical TM probabilities; LM; word, phrase and gluerules penalty. GIZA++ (Och and Ney, 2003) has been used for word alignment with phrase length limit of 10. Feature weights were optimized using MERT (Och, 2003). We built a 5-gram language model on the English side of Europarl and used the Kneser-Ney smoothing method and SRILM (Stolcke, 2002) as the language model toolkit. 2http://www.statmt.org/wpt05/mt-shared-task/ p(¯e | source. A binary indicator function δ( f, m) picks a component model for each span: δ( f,m) = { 1, m = argmax ψ( f, n) n∈M 0, otherwise The criteria for choosing a model for each cell, ψ( ¯f, n), could be based on max (wn · φn(¯e, f)) E¯f,n) = λn e¯ ψ( f,m) pm(¯e </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Comput. Linguist., 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41th Annual Meeting of the ACL,</booktitle>
<publisher>ACL.</publisher>
<location>Sapporo,</location>
<contexts>
<context position="1914" citStr="Och, 2003" startWordPosition="306" endWordPosition="307"> 2011; Xiao et al., 2010; Lagarda and Casacuberta, 2008). In this paper, we propose a method to adopt stacking (Wolpert, 1992), an ensemble learning technique, to SMT. We manipulate the full set of training data, creating k disjoint sets of held-out and held-in data sets as in k-fold cross-validation and build a model on each partition. This creates a diverse ensemble of statistical machine translation models where each member of the ensemble has different feature function values for the SMT log-linear model (Koehn, 2010). The weights of model are then tuned using minimum error rate training (Och, 2003) on the held-out fold to provide k weak models. We then create a strong ∗This research was partially supported by an NSERC, Canada (RGPIN: 264905) grant and a Google Faculty Award to the second author. model by stacking another meta-learner on top of weak models to combine them into a single model. The particular second-tier model we use is a model combination approach called ensemble decoding which combines hypotheses from the weak models on-the-fly in the decoder. Using this approach, we take advantage of the diversity created by manipulating the training data and obtain a significant and co</context>
<context position="11190" citStr="Och, 2003" startWordPosition="1882" endWordPosition="1883">xperimented with two language pairs: French to English and Spanish to English on the Europarl corpus (v7) (Koehn, 2005) and used ACL/WMT 2005 2 data for dev and test sets. For the base models, we used an in-house implementation of hierarchical phrase-based systems, Kriya (Sankaran et al., 2012), which uses the same features mentioned in (Chiang, 2005): forward and backward relative-frequency and lexical TM probabilities; LM; word, phrase and gluerules penalty. GIZA++ (Och and Ney, 2003) has been used for word alignment with phrase length limit of 10. Feature weights were optimized using MERT (Och, 2003). We built a 5-gram language model on the English side of Europarl and used the Kneser-Ney smoothing method and SRILM (Stolcke, 2002) as the language model toolkit. 2http://www.statmt.org/wpt05/mt-shared-task/ p(¯e | source. A binary indicator function δ( f, m) picks a component model for each span: δ( f,m) = { 1, m = argmax ψ( f, n) n∈M 0, otherwise The criteria for choosing a model for each cell, ψ( ¯f, n), could be based on max (wn · φn(¯e, f)) E¯f,n) = λn e¯ ψ( f,m) pm(¯e |f) δ( f) = M E m 336 Direction k-fold Resub Mean WSUM 2 18.08 19.67 22.32 Fr - En 4 18.08 21.80 23.14 8 18.08 22.47 23</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training for statistical machine translation. In Proceedings of the 41th Annual Meeting of the ACL, Sapporo, July. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Majid Razmara</author>
<author>George Foster</author>
<author>Baskaran Sankaran</author>
<author>Anoop Sarkar</author>
</authors>
<title>Mixing multiple translation models in statistical machine translation.</title>
<date>2012</date>
<booktitle>In The 50th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference,</booktitle>
<pages>940--949</pages>
<publisher>The Association</publisher>
<institution>for Computer Linguistics.</institution>
<contexts>
<context position="8260" citStr="Razmara et al., 2012" startWordPosition="1362" endWordPosition="1365"> the k-fold crossvalidation technique. A diverse ensemble of weak systems is created by learning a model on each k −1 fold and tuning the statistical machine translation log-linear weights on the remaining fold. However, instead of learning a model on the output of base models as in (Wolpert, 1992), we combine hypotheses from the base models in the decoder with uniform weights. For the base learner, we use Kriya (Sankaran et al., 2012), an in-house hierarchical phrase-based machine translation system, to produce multiple weak models. These models are combined together using Ensemble Decoding (Razmara et al., 2012) to produce a strong model in the decoder. This method is briefly explained in next section. 3.1 Ensemble Decoding SMT Log-linear models (Koehn, 2010) find the most likely target language output a given the source language input f using a vector of feature functions 0: ( ) p(e|f) a exp w 0 1http://www.netflixprize.com/ 335 Ensemble decoding combines several models dynamically at decoding time. The scores are combined for each partial hypothesis using a user-defined mixture operation ⊗ over component models. � � p(e|f) ∝ exp w1 · φ1 ⊗ w2 · φ2 ⊗ ... We previously successfully applied ensemble de</context>
</contexts>
<marker>Razmara, Foster, Sankaran, Sarkar, 2012</marker>
<rawString>Majid Razmara, George Foster, Baskaran Sankaran, and Anoop Sarkar. 2012. Mixing multiple translation models in statistical machine translation. In The 50th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, July 8-14, 2012, Jeju Island, Korea - Volume 1: Long Papers, pages 940–949. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
</authors>
<title>Memory-based shallow parsing.</title>
<date>2002</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>2--559</pages>
<contexts>
<context position="1169" citStr="Sang, 2002" startWordPosition="185" endWordPosition="186">d three different sizes of training data show significant improvements of up to 4 BLEU points over a conventionally trained SMT model. 1 Introduction Ensemble-based methods have been widely used in machine learning with the aim of reducing the instability of classifiers and regressors and/or increase their bias. The idea behind ensemble learning is to combine multiple models, weak learners, in an attempt to produce a strong model with less error. It has also been successfully applied to a wide variety of tasks in NLP (Tomeh et al., 2010; Surdeanu and Manning, 2010; F. T. Martins et al., 2008; Sang, 2002) and recently has attracted attention in the statistical machine translation community in various work (Xiao et al., 2013; Song et al., 2011; Xiao et al., 2010; Lagarda and Casacuberta, 2008). In this paper, we propose a method to adopt stacking (Wolpert, 1992), an ensemble learning technique, to SMT. We manipulate the full set of training data, creating k disjoint sets of held-out and held-in data sets as in k-fold cross-validation and build a model on each partition. This creates a diverse ensemble of statistical machine translation models where each member of the ensemble has different feat</context>
</contexts>
<marker>Sang, 2002</marker>
<rawString>Erik F. Tjong Kim Sang. 2002. Memory-based shallow parsing. J. Mach. Learn. Res., 2:559–594, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baskaran Sankaran</author>
<author>Majid Razmara</author>
<author>Anoop Sarkar</author>
</authors>
<title>Kriya an end-to-end hierarchical phrase-based mt system.</title>
<date>2012</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<volume>97</volume>
<issue>97</issue>
<contexts>
<context position="8078" citStr="Sankaran et al., 2012" startWordPosition="1335" endWordPosition="1338">ploit stacking for statistical machine translation. We use a standard statistical machine translation engine and produce multiple diverse models by partitioning the training set using the k-fold crossvalidation technique. A diverse ensemble of weak systems is created by learning a model on each k −1 fold and tuning the statistical machine translation log-linear weights on the remaining fold. However, instead of learning a model on the output of base models as in (Wolpert, 1992), we combine hypotheses from the base models in the decoder with uniform weights. For the base learner, we use Kriya (Sankaran et al., 2012), an in-house hierarchical phrase-based machine translation system, to produce multiple weak models. These models are combined together using Ensemble Decoding (Razmara et al., 2012) to produce a strong model in the decoder. This method is briefly explained in next section. 3.1 Ensemble Decoding SMT Log-linear models (Koehn, 2010) find the most likely target language output a given the source language input f using a vector of feature functions 0: ( ) p(e|f) a exp w 0 1http://www.netflixprize.com/ 335 Ensemble decoding combines several models dynamically at decoding time. The scores are combin</context>
<context position="10875" citStr="Sankaran et al., 2012" startWordPosition="1830" endWordPosition="1833">n pick the model with highest weighted sum of the probabilities of the rules (SW:SUM). This sum has to take into account the translation table limit (ttl), on the number of rules suggested by each model for each cell: exp (wn · φn(¯e,¯f)) The probability of each phrase-pair (¯e, f) is then: p(¯e | 4 Experiments &amp; Results We experimented with two language pairs: French to English and Spanish to English on the Europarl corpus (v7) (Koehn, 2005) and used ACL/WMT 2005 2 data for dev and test sets. For the base models, we used an in-house implementation of hierarchical phrase-based systems, Kriya (Sankaran et al., 2012), which uses the same features mentioned in (Chiang, 2005): forward and backward relative-frequency and lexical TM probabilities; LM; word, phrase and gluerules penalty. GIZA++ (Och and Ney, 2003) has been used for word alignment with phrase length limit of 10. Feature weights were optimized using MERT (Och, 2003). We built a 5-gram language model on the English side of Europarl and used the Kneser-Ney smoothing method and SRILM (Stolcke, 2002) as the language model toolkit. 2http://www.statmt.org/wpt05/mt-shared-task/ p(¯e | source. A binary indicator function δ( f, m) picks a component model</context>
</contexts>
<marker>Sankaran, Razmara, Sarkar, 2012</marker>
<rawString>Baskaran Sankaran, Majid Razmara, and Anoop Sarkar. 2012. Kriya an end-to-end hierarchical phrase-based mt system. The Prague Bulletin of Mathematical Linguistics, 97(97), April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
</authors>
<title>The strength of weak learnability.</title>
<date>1990</date>
<journal>Mach. Learn.,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="3516" citStr="Schapire, 1990" startWordPosition="572" endWordPosition="573">es of each original training instance. This procedure is repeated a number of times and the base learner is applied to each sample to produce a weak learner. These models are aggregated by doing a uniform voting for classification or averaging the predictions for regression. Bagging reduces the variance of the base model while leaving the bias relatively unchanged and is most useful when a small change in the training data affects the prediction of the model (i.e. the model is unstable) (Breiman, 1996a). Bagging has been recently applied to SMT (Xiao et al., 2013; Song et al., 2011) Boosting (Schapire, 1990) constructs a strong learner by repeatedly choosing a weak learner and applying it on a re-weighted training set. In each iteration, a weak model is learned on the training data, whose instance weights are modified from the previous iteration to concentrate on examples on which the model predictions were poor. By putting more weight on the wrongly predicted examples, a diverse ensemble of weak learners is created. Boosting has also been used in SMT (Xiao et al., 2013; Xiao et al., 2010; Lagarda 334 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 3</context>
</contexts>
<marker>Schapire, 1990</marker>
<rawString>Robert E. Schapire. 1990. The strength of weak learnability. Mach. Learn., 5(2):197–227, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linfeng Song</author>
<author>Haitao Mi</author>
<author>Yajuan L¨u</author>
<author>Qun Liu</author>
</authors>
<title>Bagging-based system combination for domain adaption.</title>
<date>2011</date>
<booktitle>In Proceedings of the 13th Machine Translation Summit (MT Summit XIII),</booktitle>
<pages>293--299</pages>
<institution>International Association for Machine Translation,</institution>
<marker>Song, Mi, L¨u, Liu, 2011</marker>
<rawString>Linfeng Song, Haitao Mi, Yajuan L¨u, and Qun Liu. 2011. Bagging-based system combination for domain adaption. In Proceedings of the 13th Machine Translation Summit (MT Summit XIII), pages 293– 299. International Association for Machine Translation, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings International Conference on Spoken Language Processing,</booktitle>
<pages>257--286</pages>
<contexts>
<context position="11323" citStr="Stolcke, 2002" startWordPosition="1904" endWordPosition="1906">ACL/WMT 2005 2 data for dev and test sets. For the base models, we used an in-house implementation of hierarchical phrase-based systems, Kriya (Sankaran et al., 2012), which uses the same features mentioned in (Chiang, 2005): forward and backward relative-frequency and lexical TM probabilities; LM; word, phrase and gluerules penalty. GIZA++ (Och and Ney, 2003) has been used for word alignment with phrase length limit of 10. Feature weights were optimized using MERT (Och, 2003). We built a 5-gram language model on the English side of Europarl and used the Kneser-Ney smoothing method and SRILM (Stolcke, 2002) as the language model toolkit. 2http://www.statmt.org/wpt05/mt-shared-task/ p(¯e | source. A binary indicator function δ( f, m) picks a component model for each span: δ( f,m) = { 1, m = argmax ψ( f, n) n∈M 0, otherwise The criteria for choosing a model for each cell, ψ( ¯f, n), could be based on max (wn · φn(¯e, f)) E¯f,n) = λn e¯ ψ( f,m) pm(¯e |f) δ( f) = M E m 336 Direction k-fold Resub Mean WSUM 2 18.08 19.67 22.32 Fr - En 4 18.08 21.80 23.14 8 18.08 22.47 23.76 2 18.61 19.23 21.62 Es - En 4 18.61 21.52 23.42 8 18.61 22.20 23.69 WMAX PROD SW:MAX SW:SUM 22.48 22.06 21.70 21.81 23.48 23.55 2</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Proceedings International Conference on Spoken Language Processing, pages 257–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Christopher D Manning</author>
</authors>
<title>Ensemble models for dependency parsing: cheap and good?</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>649--652</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1128" citStr="Surdeanu and Manning, 2010" startWordPosition="174" endWordPosition="178">n-the-fly. Experimental results on two language pairs and three different sizes of training data show significant improvements of up to 4 BLEU points over a conventionally trained SMT model. 1 Introduction Ensemble-based methods have been widely used in machine learning with the aim of reducing the instability of classifiers and regressors and/or increase their bias. The idea behind ensemble learning is to combine multiple models, weak learners, in an attempt to produce a strong model with less error. It has also been successfully applied to a wide variety of tasks in NLP (Tomeh et al., 2010; Surdeanu and Manning, 2010; F. T. Martins et al., 2008; Sang, 2002) and recently has attracted attention in the statistical machine translation community in various work (Xiao et al., 2013; Song et al., 2011; Xiao et al., 2010; Lagarda and Casacuberta, 2008). In this paper, we propose a method to adopt stacking (Wolpert, 1992), an ensemble learning technique, to SMT. We manipulate the full set of training data, creating k disjoint sets of held-out and held-in data sets as in k-fold cross-validation and build a model on each partition. This creates a diverse ensemble of statistical machine translation models where each </context>
<context position="6948" citStr="Surdeanu and Manning (2010)" startWordPosition="1153" endWordPosition="1156"> Ek αkmk(x), where mk is a base model trained on the k-th partition of the data and s is the resulting strong model created by linearly interpolating the weak learners. Stacking (aka blending) has been used in the system that won the Netflix Prize1, which used a multi-level stacking algorithm. Stacking has been actively used in statistical parsing: Nivre and McDonald (2008) integrated two models for dependency parsing by letting one model learn from features generated by the other; F. T. Martins et al. (2008) further formalized the stacking algorithm and improved on Nivre and McDonald (2008); Surdeanu and Manning (2010) includes a detailed analysis of ensemble models for statistical parsing: i) the diversity of base parsers is more important than the complexity of the models; ii) unweighted voting performs as well as weighted voting; and iii) ensemble models that combine at decoding time significantly outperform models that combine multiple models at training time. 3 Our Approach In this paper, we propose a method to apply stacking to statistical machine translation (SMT) and our method is the first to successfully exploit stacking for statistical machine translation. We use a standard statistical machine tr</context>
</contexts>
<marker>Surdeanu, Manning, 2010</marker>
<rawString>Mihai Surdeanu and Christopher D. Manning. 2010. Ensemble models for dependency parsing: cheap and good? In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 649–652, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadi Tomeh</author>
<author>Alexandre Allauzen</author>
<author>Guillaume Wisniewski</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Refining word alignment with discriminative training.</title>
<date>2010</date>
<booktitle>In Proceedings of The Ninth Conference of the Association for Machine Translation in the Americas (AMTA</booktitle>
<contexts>
<context position="1100" citStr="Tomeh et al., 2010" startWordPosition="170" endWordPosition="173">ng the weak models on-the-fly. Experimental results on two language pairs and three different sizes of training data show significant improvements of up to 4 BLEU points over a conventionally trained SMT model. 1 Introduction Ensemble-based methods have been widely used in machine learning with the aim of reducing the instability of classifiers and regressors and/or increase their bias. The idea behind ensemble learning is to combine multiple models, weak learners, in an attempt to produce a strong model with less error. It has also been successfully applied to a wide variety of tasks in NLP (Tomeh et al., 2010; Surdeanu and Manning, 2010; F. T. Martins et al., 2008; Sang, 2002) and recently has attracted attention in the statistical machine translation community in various work (Xiao et al., 2013; Song et al., 2011; Xiao et al., 2010; Lagarda and Casacuberta, 2008). In this paper, we propose a method to adopt stacking (Wolpert, 1992), an ensemble learning technique, to SMT. We manipulate the full set of training data, creating k disjoint sets of held-out and held-in data sets as in k-fold cross-validation and build a model on each partition. This creates a diverse ensemble of statistical machine tr</context>
</contexts>
<marker>Tomeh, Allauzen, Wisniewski, Yvon, 2010</marker>
<rawString>Nadi Tomeh, Alexandre Allauzen, Guillaume Wisniewski, and Franc¸ois Yvon. 2010. Refining word alignment with discriminative training. In Proceedings of The Ninth Conference of the Association for Machine Translation in the Americas (AMTA 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David H Wolpert</author>
</authors>
<title>Stacked generalization.</title>
<date>1992</date>
<journal>Neural Networks,</journal>
<pages>5--241</pages>
<contexts>
<context position="1430" citStr="Wolpert, 1992" startWordPosition="228" endWordPosition="229"> classifiers and regressors and/or increase their bias. The idea behind ensemble learning is to combine multiple models, weak learners, in an attempt to produce a strong model with less error. It has also been successfully applied to a wide variety of tasks in NLP (Tomeh et al., 2010; Surdeanu and Manning, 2010; F. T. Martins et al., 2008; Sang, 2002) and recently has attracted attention in the statistical machine translation community in various work (Xiao et al., 2013; Song et al., 2011; Xiao et al., 2010; Lagarda and Casacuberta, 2008). In this paper, we propose a method to adopt stacking (Wolpert, 1992), an ensemble learning technique, to SMT. We manipulate the full set of training data, creating k disjoint sets of held-out and held-in data sets as in k-fold cross-validation and build a model on each partition. This creates a diverse ensemble of statistical machine translation models where each member of the ensemble has different feature function values for the SMT log-linear model (Koehn, 2010). The weights of model are then tuned using minimum error rate training (Och, 2003) on the held-out fold to provide k weak models. We then create a strong ∗This research was partially supported by an</context>
<context position="4758" citStr="Wolpert, 1992" startWordPosition="790" endWordPosition="791">ust 4-9 2013. c�2013 Association for Computational Linguistics Algorithm 1: Stacking for SMT Input: D = {(fj, ej�}N &gt; A parallel corpus j=1 Input: k &gt; # of folds (i.e. weak learners) Output: STRONGMODEL s 1: D1, ... ,Dk ← SPLIT(D, k) 2: for i = 1 → k do 3: Ti ← D − Di &gt; Use all but current partition as training set. 4: φi ← TRAIN(Ti) &gt; Train feature functions. 5: Mi ← TUNE(φi, Di) &gt; Tune the model on the current partition. 6: end for 7: s ← COMBINEMODELS(M1 , ..., Mk) &gt; Combine all the base models to produce a strong stacked model. and Casacuberta, 2008). Stacking (or stacked generalization) (Wolpert, 1992) is another ensemble learning algorithm that uses a second-level learning algorithm on top of the base learners to reduce the bias. The first level consists of predictors g1, ... , gk where gi : Rd -+ R, receiving input x E Rd and producing a prediction gi(x). The next level consists of a single function h : Rd+k -+ R that takes (x, g1(x), ... , gk(x)) as input and produces an ensemble prediction yˆ = h(x, g1(x), ... , gk(x)). Two categories of ensemble learning are homogeneous learning and heterogeneous learning. In homogeneous learning, a single base learner is used, and diversity is generat</context>
<context position="6165" citStr="Wolpert, 1992" startWordPosition="1029" endWordPosition="1030">a to create a pool of diverse models. In this paper, we focus on homogeneous ensemble learning by manipulating the training data. In the primary form of stacking (Wolpert, 1992), the training data is split into multiple disjoint sets of held-out and held-in data sets using k-fold cross-validation and k models are trained on the held-in partitions and run on held-out partitions. Then a meta-learner uses the predictions of all models on their held-out sets and the actual labels to learn a final model. The details of the first-layer and second-layer predictors are considered to be a “black art” (Wolpert, 1992). Breiman (1996b) linearly combines the weak learners in the stacking framework. The weights of the base learners are learned using ridge regression: s(x) = Ek αkmk(x), where mk is a base model trained on the k-th partition of the data and s is the resulting strong model created by linearly interpolating the weak learners. Stacking (aka blending) has been used in the system that won the Netflix Prize1, which used a multi-level stacking algorithm. Stacking has been actively used in statistical parsing: Nivre and McDonald (2008) integrated two models for dependency parsing by letting one model l</context>
<context position="7938" citStr="Wolpert, 1992" startWordPosition="1313" endWordPosition="1314">paper, we propose a method to apply stacking to statistical machine translation (SMT) and our method is the first to successfully exploit stacking for statistical machine translation. We use a standard statistical machine translation engine and produce multiple diverse models by partitioning the training set using the k-fold crossvalidation technique. A diverse ensemble of weak systems is created by learning a model on each k −1 fold and tuning the statistical machine translation log-linear weights on the remaining fold. However, instead of learning a model on the output of base models as in (Wolpert, 1992), we combine hypotheses from the base models in the decoder with uniform weights. For the base learner, we use Kriya (Sankaran et al., 2012), an in-house hierarchical phrase-based machine translation system, to produce multiple weak models. These models are combined together using Ensemble Decoding (Razmara et al., 2012) to produce a strong model in the decoder. This method is briefly explained in next section. 3.1 Ensemble Decoding SMT Log-linear models (Koehn, 2010) find the most likely target language output a given the source language input f using a vector of feature functions 0: ( ) p(e|</context>
</contexts>
<marker>Wolpert, 1992</marker>
<rawString>David H. Wolpert. 1992. Stacked generalization. Neural Networks, 5:241–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Xiao</author>
<author>Jingbo Zhu</author>
<author>Muhua Zhu</author>
<author>Huizhen Wang</author>
</authors>
<title>Boosting-based system combination for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>739--748</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1328" citStr="Xiao et al., 2010" startWordPosition="210" endWordPosition="213">emble-based methods have been widely used in machine learning with the aim of reducing the instability of classifiers and regressors and/or increase their bias. The idea behind ensemble learning is to combine multiple models, weak learners, in an attempt to produce a strong model with less error. It has also been successfully applied to a wide variety of tasks in NLP (Tomeh et al., 2010; Surdeanu and Manning, 2010; F. T. Martins et al., 2008; Sang, 2002) and recently has attracted attention in the statistical machine translation community in various work (Xiao et al., 2013; Song et al., 2011; Xiao et al., 2010; Lagarda and Casacuberta, 2008). In this paper, we propose a method to adopt stacking (Wolpert, 1992), an ensemble learning technique, to SMT. We manipulate the full set of training data, creating k disjoint sets of held-out and held-in data sets as in k-fold cross-validation and build a model on each partition. This creates a diverse ensemble of statistical machine translation models where each member of the ensemble has different feature function values for the SMT log-linear model (Koehn, 2010). The weights of model are then tuned using minimum error rate training (Och, 2003) on the held-o</context>
<context position="4006" citStr="Xiao et al., 2010" startWordPosition="653" endWordPosition="656">le) (Breiman, 1996a). Bagging has been recently applied to SMT (Xiao et al., 2013; Song et al., 2011) Boosting (Schapire, 1990) constructs a strong learner by repeatedly choosing a weak learner and applying it on a re-weighted training set. In each iteration, a weak model is learned on the training data, whose instance weights are modified from the previous iteration to concentrate on examples on which the model predictions were poor. By putting more weight on the wrongly predicted examples, a diverse ensemble of weak learners is created. Boosting has also been used in SMT (Xiao et al., 2013; Xiao et al., 2010; Lagarda 334 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 334–339, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Algorithm 1: Stacking for SMT Input: D = {(fj, ej�}N &gt; A parallel corpus j=1 Input: k &gt; # of folds (i.e. weak learners) Output: STRONGMODEL s 1: D1, ... ,Dk ← SPLIT(D, k) 2: for i = 1 → k do 3: Ti ← D − Di &gt; Use all but current partition as training set. 4: φi ← TRAIN(Ti) &gt; Train feature functions. 5: Mi ← TUNE(φi, Di) &gt; Tune the model on the current partition. 6: end for 7: s ← COMBINEMODELS(M1 </context>
</contexts>
<marker>Xiao, Zhu, Zhu, Wang, 2010</marker>
<rawString>Tong Xiao, Jingbo Zhu, Muhua Zhu, and Huizhen Wang. 2010. Boosting-based system combination for machine translation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 739–748, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Xiao</author>
<author>Jingbo Zhu</author>
<author>Tongran Liu</author>
</authors>
<title>Bagging and boosting statistical machine translation systems.</title>
<date>2013</date>
<journal>Artificial Intelligence,</journal>
<volume>195</volume>
<contexts>
<context position="1290" citStr="Xiao et al., 2013" startWordPosition="202" endWordPosition="205"> trained SMT model. 1 Introduction Ensemble-based methods have been widely used in machine learning with the aim of reducing the instability of classifiers and regressors and/or increase their bias. The idea behind ensemble learning is to combine multiple models, weak learners, in an attempt to produce a strong model with less error. It has also been successfully applied to a wide variety of tasks in NLP (Tomeh et al., 2010; Surdeanu and Manning, 2010; F. T. Martins et al., 2008; Sang, 2002) and recently has attracted attention in the statistical machine translation community in various work (Xiao et al., 2013; Song et al., 2011; Xiao et al., 2010; Lagarda and Casacuberta, 2008). In this paper, we propose a method to adopt stacking (Wolpert, 1992), an ensemble learning technique, to SMT. We manipulate the full set of training data, creating k disjoint sets of held-out and held-in data sets as in k-fold cross-validation and build a model on each partition. This creates a diverse ensemble of statistical machine translation models where each member of the ensemble has different feature function values for the SMT log-linear model (Koehn, 2010). The weights of model are then tuned using minimum error r</context>
<context position="3470" citStr="Xiao et al., 2013" startWordPosition="563" endWordPosition="566">nerated sample set may have 0, 1 or more instances of each original training instance. This procedure is repeated a number of times and the base learner is applied to each sample to produce a weak learner. These models are aggregated by doing a uniform voting for classification or averaging the predictions for regression. Bagging reduces the variance of the base model while leaving the bias relatively unchanged and is most useful when a small change in the training data affects the prediction of the model (i.e. the model is unstable) (Breiman, 1996a). Bagging has been recently applied to SMT (Xiao et al., 2013; Song et al., 2011) Boosting (Schapire, 1990) constructs a strong learner by repeatedly choosing a weak learner and applying it on a re-weighted training set. In each iteration, a weak model is learned on the training data, whose instance weights are modified from the previous iteration to concentrate on examples on which the model predictions were poor. By putting more weight on the wrongly predicted examples, a diverse ensemble of weak learners is created. Boosting has also been used in SMT (Xiao et al., 2013; Xiao et al., 2010; Lagarda 334 Proceedings of the 51st Annual Meeting of the Asso</context>
<context position="15120" citStr="Xiao et al. (2013)" startWordPosition="2578" endWordPosition="2581">n the training-set only and tuned on the devset as well as Bayesian Model Averaging (BMA, see §5). For the 100k+dev corpus, we sampled 11 partitions from all 51 possible partitions by taking every fifth partition as training data. The results in Table 3 show that stacking can improve over the baseline BLEU scores by up to 4 points. Examining the performance of the different mixture operations, we can see that WSUM and WMAX typically outperform other mixture operations. Different mixture operations can be dominant in different language pairs and different sizes of training sets. 5 Related Work Xiao et al. (2013) have applied both boosting and bagging on three different statistical machine translation engines: phrase-based (Koehn et al., 2003), hierarchical phrase-based (Chiang, 2005) and syntax-based (Galley et al., 2006) and showed SMT can benefit from these methods as well. Duan et al. (2009) creates an ensemble of models by using feature subspace method in the machine learning literature (Ho, 1998). Each member of the ensemble is built by removing one nonLM feature in the log-linear framework or varying the order of language model. Finally they use a sentence-level system combination on the output</context>
</contexts>
<marker>Xiao, Zhu, Liu, 2013</marker>
<rawString>Tong Xiao, Jingbo Zhu, and Tongran Liu. 2013. Bagging and boosting statistical machine translation systems. Artificial Intelligence, 195:496–527, February.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>