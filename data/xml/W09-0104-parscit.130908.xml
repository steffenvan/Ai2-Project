<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.405587">
<title confidence="0.994488">
Computational Linguistics and Generative Linguistics:
The Triumph of Hope over Experience
</title>
<author confidence="0.99803">
Geoffrey K. Pullum
</author>
<affiliation confidence="0.9990755">
School of Philosophy, Psychology, and Language Sciences
University of Edinburgh
</affiliation>
<email confidence="0.994599">
gpullum@ling.ed.ac.ul
</email>
<sectionHeader confidence="0.993827" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999975066666667">
It is remarkable if any relationship at all
persists between computational linguists
(CL) and that part of general linguis-
tics comprising the mainstream of MIT
transformational-generative (TG) theoreti-
cal syntax. If the lines are still open, it rep-
resents something of a tribute to CL prac-
titioners’ tolerance — a triumph of hope
and goodwill over the experience of abuse
— because the TG community has shown
considerable hostility toward CL and ev-
erything it stands for over the past fifty
years. I offer some brief historical notes,
and hint at prospects for a better basis for
collaboration in the future.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999983746031746">
The theme of this workshop is the interaction be-
tween computational linguistics (CL) and general
linguistics. The organizers ask whether it has
it been virtuous, vicious, or vacuous. They use
only three of the rather extraordinary number of
v-initial adjectives. Is the relationship vital, valu-
able, venturesome, visionary, versatile, and vi-
brant? Or vague, variable, verbose, and sometimes
vexatious? Has it perhaps been merely vestigial
and vicarious, with hardly any general linguists
really participating? Or vain, venal, vaporous, vir-
ginal, volatile, and voguish, yet vulnerable, a re-
lationship at risk? Or would the best description
use adjectives like vengeful, venomous, vilifica-
tory, villainous, vindictive, violent, vitriolic, vo-
ciferous, and vulpine?
I will argue that at least with respect to that part
of general linguistics comprising the mainstream
of American theoretical syntax, it would be quite
remarkable if any relationship with computational
linguistics (CL) had thrived. It would represent (as
Samuel Johnson remarked cynically, and wrongly,
about second marriages) a triumph of hope over
experience. It seems to me that the relation-
ship that could have been was at least somewhat
blighted by the negative and defensive stance that
MIT-influenced transformational-generative (TG)
syntacticians have adopted on a diverse array of
topics highly relevant to CL.
There was never any need for such attitudes.
And at the conclusion of these brief remarks I will
suggest a basis for thinking that relations could be
much more satisfactory in the future. But I think
it is worth taking a sober look at the half-century
of history from 1959 to 2009, during which al-
most everything about the course of theoretical
syntax, at least in the USA, where I worked dur-
ing the latter half of the period, has been tacitly
guided by a single line of thinking. ‘Generative
grammar’ is commonly used to denote it, but that
will not do. First, ‘generative grammar’ is often
used to mean ‘MIT-influenced transformational-
generative grammar’. For that I will use the ab-
breviation TG. And second, it is sometimes (in-
correctly) claimed that ‘generative’ means noth-
ing more or less than ‘explicit’ (see Chomsky
1966, 12: ‘a generative grammar (that is, an ex-
plicit grammar that makes no appeal to the reader’s
“facul´e de langage” but rather attempts to incorpo-
rate the mechanisms of this faculty)’).
We need more precise terminology in order to
home in on what I am talking about. As Seuren
(2004) has stressed, the relevant vision of what a
grammar is like, built into most linguistic theoriza-
tion today at a level so deep that most linguists are
incapable of seeing past it or out of it, is not just
that it is explicit, but that a grammar is and must be
a syntax-centered random generator. I will there-
fore refer to language specification by random
generation (LSRG).
The definitive technical paper defining gram-
mars in LSRG terms is Chomsky (1959). This
was a fine paper, which would have earned its
</bodyText>
<note confidence="0.941104">
Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Linguistics, pages 12–21,
Athens, Greece, 30 March, 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.998212">
12
</page>
<bodyText confidence="0.999419181818182">
writer tenure in any department of linguistics,
logic, computer science, or mathematics that knew
what it was doing and could see the possibilities.
But it brought into linguistics two things that were
not going to go away for half a century. One
was the notion that any formally precise linguis-
tics had to be limited to LSRG. And the other was
the combative and insular personality of the pa-
per’s author, which had such a great influence on
the personality of his extraordinarily important de-
partment at MIT.
</bodyText>
<sectionHeader confidence="0.890224" genericHeader="introduction">
2 Obsession with random generation
</sectionHeader>
<bodyText confidence="0.999945047619048">
The sense of ‘generate’ relevant to LSRG goes
back to the work of the great mathematical lo-
gician Emil Post (as acknowledged by Chomsky
1959, 137n). Post’s project was initially to for-
malize the notion of proof in logical systems —
originally, the propositional logic that was infor-
mally used but not formally defined in Whitehead
and Russell’s Principia Mathematica. He repre-
sented well-formed formulae (‘enunciations’, in-
cluding the ‘primitive assertions’, i.e. axioms, and
the ‘assertions’ i.e. theorems) to be simply strings
over a finite set of symbols, and rules of inference
(‘productions’) as instructions for deriving a new
string (the conclusion) from a set of strings already
in hand (the premises). He then studied the ques-
tion of what kinds of sets of strings could be gen-
erated if a set of initial strings were closed under
the operation of applying inference rules. Post’s
rather ungainly general presentation of the general
concept of rules of inference, or in his terms pro-
ductions, looks like this:
</bodyText>
<equation confidence="0.9987976">
g11 Pi01 g12 Pi02 ··· g1m1 Pi0m1 g1m1+1
g21 Pi00 1 g22 Pi00 2 · · · g2m2 Pi00 m2 g2m2+1
gk1 Pi0k 1 gk2 Pi0k 2 ··· gkmk Pi0k mk gkmk+1
produce
g1 Pi1 g2 Pi2 ··· gm Pim gm+1
</equation>
<bodyText confidence="0.99796105">
In specific instances of productions the g metavari-
ables in this schema are replaced by actual strings
over what is now known as the terminal vocabu-
lary. The P metavariables function as cover sym-
bols for arbitrary stretches of material — they are
string variables, some of which may be repeated
to copy material into the conclusion. A produc-
tion provides a license, given a set of strings that
match patterns of the form g0P1g1P2 · · · Pkgk, to
produce a certain other string composed in some
way out of the various gi and Pi.
Post defined a class of canonical systems, each
consisting of a set of initial strings and a finite
set of productions. Sets generated by canonical
production systems he called canonical sets. Post
had realized early on that the canonical sets were
nothing more or less than the sets definable by re-
cursive functions or Turing machines; that is, they
were just the recursively enumerable (r. e.) sets.
He proceeded to prove that even if you restrict
the number and distribution of the gi and Pi ex-
tremely tightly, expressive power may not be re-
duced. Specifically, he proved that no reduction
in the definable sets is obtained even if you set the
number of P variables and the number of premises
at 1, and require that every production has the form
‘g0P produces Pg1’. Such very restricted sys-
tems were called normal systems. Normal sys-
tems can still derive every canonical set, provided
you are allowed to use extra symbols that appear
in derivations but not in the ultimately generated
strings (these extra symbols are what would be-
come known to formal language theorists within
computer science as variables and to linguists as
nonterminals).
In a notation more familiar to linguists, the re-
sult amounts to showing that every r. e. subset of
E+ can be generated by some generative grammar
using a symbol vocabulary V = E ∪ N in which
all rules have the form ‘xW → W y’ for speci-
fied strings x, y ∈ V * and some fixed W ∈ V *.
This was the first weak generative capacity result:
normal systems are equivalent in weak generative
capacity to full canonical systems.
In a later paper, settling a conjecture of Thue,
Post showed (1947) that you can derive every
canonical set if your productions all have the
form ‘P1giP2 produces P1gjP2’. This amounts
to showing that every canonical subset of E+ can
be generated by (what would later be called) a
generative grammar using a symbol vocabulary
V = E ∪ N in which all rules have the form
‘WxZ → WyZ’ for specified strings x, y ∈ V *
and fixed W, Z ∈ V *.
Hence the first demonstration that unrestricted
rewriting systems (Chomsky’s ‘type-0’ grammars)
can derive any r. e. set was not original with Chom-
sky (1959). It had been published twelve years
earlier by Post.
Post had in effect invented what could be called
</bodyText>
<page confidence="0.997663">
13
</page>
<bodyText confidence="0.999844293333333">
top-down random generators. These randomly
generate r. e. sets of symbols by expanding an ini-
tial axiomatic string, which can be just a single
symbol. Their equivalence to Turing machines is
obvious (Kozen 1997, 256–257).
Between the time of Post’s doctoral work in
1920 and the 1943 paper in which he published
his result on canonical systems (already present
in compressed form in his thesis), Ajdukiewicz
(1935) had proposed a different style of genera-
tive grammar, also motivated by the development
of a better understanding of proof. Adjukiewicz’s
invention was categorial grammar, the first kind of
bottom-up random generators. It composes ex-
pressions of the generated language by combining
parts — initially primitive categorized symbols,
and then previously composed subparts.
When Chomsky and Lasnik (1977) start talk-
ing about the ‘computational system’ of human
language (a mode of speaking that rapidly caught
on, and persists in current ‘minimalist’ work), the
‘computation’ of which they spoke was one that
takes place nowhere: no such computations are
ever done, except perhaps using pencil and paper
as a syntactic theorist tries to figure out how or
whether a certain string can be derived. This ‘com-
putational system’ attempts randomly and nonde-
terministically to find some way to apply rules in
order to build a particular structure, starting from
an arbitrary syntactic starting point.
In the case of pre-1990 work the starting point
was apparently a start symbol; in post-1990 ‘min-
imalist’ work it is a numeration: a randomly cho-
sen multiset of categorized items from the lexi-
con. The concept of a ‘numeration’ is a reflection
of how firmly embedded the random-generation
idea is. The numeration serves no real purpose.
It would be possible to formalize a grammar as a
set of combinatory principles for putting together
words in a string as encountered, from first to last,
so that it was in effect a parser. Categorial gram-
mars seem ideally suited to that role (Steedman,
2000), and minimalist grammars are really just a
variety of categorial grammar, stripped of some of
the formal coherence and links to logic and seman-
tics.
Chomsky has often written as if it were a neces-
sary truth that a grammar must be a random gener-
ator. For example: ‘Clearly, a grammar must con-
tain ... a ‘syntactic component’ that generates an
infinite number of strings representing grammat-
ical sentences ... This is the classical model for
grammar’ (Chomsky 1962, 539). This says that a
grammar must be a random generator. But this is
not true. A grammar could in principle be formu-
lated as, say a transducer mapping phonetic rep-
resentation inputs to corresponding sets of logical
forms. (Presumably this must be possible, given
what human beings do.)
It is particularly strange to see Chomsky ignor-
ing this possibility and yet asserting in Knowl-
edge of Language (Chomsky, 1986b) that a per-
son’s internalized grammar ‘assigns a status to ev-
ery relevant physical event, say, every sound wave’
(p. 26). The claim is false, simply because ran-
dom generators are not transducers or functions:
they do not take inputs. A random generator only
‘assigns a status’ to a string by generating it with
a derivation that associates it with certain prop-
erties. And surely it is not a sensible hypothesis
about human linguistic competence to posit that in
the brain of every human being there is an inter-
nalized random generator generating every phys-
ically possible sequence of sounds, from a ship’s
foghorn to Mahler’s ninth symphony.
</bodyText>
<sectionHeader confidence="0.900678" genericHeader="method">
3 Downplaying expressive power
</sectionHeader>
<bodyText confidence="0.99995596">
Perhaps the most centrally important reason for
linguists’ concern with the possibility of excess
expressive power in grammar formalisms was
their sense that it should be guaranteed by the gen-
eral theory of grammar that linguistic behaviors
such as understanding a sentence should be repre-
sented as at least possible. This meant that gram-
mars had to be defined in a way that at least made
the general membership problem (‘Given gram-
mar G, is string w grammatical?’) decidable.
It was in Chomsky’s 1959 paper that progress
was first made toward restricting the expressive
power of production systems in ways that achieved
this, and the early work on topics like pushdown
automata and finite state machines shows that
those topics were of interest.
As is well known, Chomsky showed that if
productions of the general form XcpZ —* X0Z
(where X, 0, Z are strings in V* and cp E V +) are
limited by the condition that 0 is no shorter than
cp, we are no longer able to derive every r. e. set of
strings over the alphabet; we get only the context-
sensitive stringsets. If the further limitation that
cp E N is imposed, we get only the context-free
stringsets. And if on top of that the requirement
</bodyText>
<page confidence="0.994383">
14
</page>
<bodyText confidence="0.999911434782609">
that 0 E (E U EN) is imposed, we get only the
regular sets.
Chomsky’s 1959 position was that the set of
all grammatical English word sequences was not
a regular stringset over the set of English words,
and that if any context-free grammar for English
could be constructed, it would not be an elegant or
revealing one. The search for intuitively adequate
grammars therefore had to range over the class of
grammars generating context-sensitive stringsets.
This is a large class of grammars, but at least it is
a proper subset of the class of grammars for which
the membership recognition problem is decidable.
Casting around outside that range was probably
not sensible, since natural languages surely had to
be decidable (it was taken to be quite obvious that
native speakers could rapidly recognize whether or
not a string of words was a sentence in their lan-
guage).
As I have detailed elsewhere in somewhat
tongue-in-cheek fashion (Pullum, 1989), Chom-
sky pulled back sharply from his initial interest
in mathematical study of linguistic formalisms as
it became clear that TG theories were being criti-
cized for their Turing-equivalence, and began dis-
missing precise studies of the generative capac-
ity of grammars as trivial and ridiculous. This, it
seems to me, was one more clear sign of distanc-
ing from the concerns of CL. It was mainly com-
putational linguists who showed interest in Gaz-
dar’s observation that a theory limited to gener-
ating context-free languages could guarantee not
just recognition but recognition in polynomial (in-
deed, better than cubic) time, and in the related
observation that none of the arguments for non-
context-free characteristics in human languages
seemed to be good ones (Pullum and Gazdar,
1982).
The MIT reaction to Gazdar’s suggestion was
to mount a major effort to find intractability
in Gazdar-style (GPSG) grammars — to repre-
sent the recognition problem as NP-hard even
for context-free-equivalent theories of grammar
(Barton et al., 1987). This was something of
a confidence trick. First, the results depended
on switching attention from the fixed-grammar
arbitrary-string recognition problem (the analog
of what Vardi (1982) calls data complexity) to
the variable-grammar arbitrary-string recognition
problem (what Vardi calls combined complexity).
Second, it seemed to be vaguely assumed that only
GPSG had any charges to answer, and that the
GB theory of that time (Chomsky, 1981) would
not suffer from similar computational complexity
problems, but GB eventually turned out to be, in-
sofar as it was well defined, strongly equivalent to
Gazdar’s framework (Rogers, 1998).
For pre-GB varieties of TG, however, the prob-
lem had mainly been not that recognition was NP-
hard but that it was not computable at all: trans-
formational grammars from 1957 on kept proving
to be Turing-equivalent. That was what seems to
have driven the denigration of mathematical lin-
guistics, and the downplaying of the relevance of
decidability to such an extreme degree (see e.g.
Chomsky 1980: 120ff, where the very idea that
recognition is decidable is dismissed as an unim-
portant detail, and not necessarily even a true
claim).
</bodyText>
<sectionHeader confidence="0.943478" genericHeader="method">
4 Hostility to machine testing
</sectionHeader>
<bodyText confidence="0.999970774193548">
With many versions of TG offering no guaran-
tee that there was any parser for the language
even in principle, it was not clear that machine
testing of grammatical theories by algorithmic
checking of claims made about grammaticality of
selected strings was a plausible idea. Perhaps
machine theorem-proving algorithms could have
been adapted to showing that a certain grammar
could indeed derive a certain string, but in prac-
tice early transformational grammar was vastly too
complex to permit the building of tools for gram-
mar testing, and later transformational grammar
far too vague.
I know of only one success story in grammar
evaluation by implementing random generation,
in fact. Ed Stabler (1992) coded up a Prolog
grammaticality-proving system based on the Bar-
riers theory of transformational grammar (Chom-
sky, 1986a), which (Pullum, 1989) had mocked for
sloppiness of statement. The Barriers system had
in particular abandoned the usual practice of defin-
ing trees in a way that had dominance as a reflexive
relation. Chomsky casually asserted that he would
take it to be irreflexive. Moreover, Stabler’s care-
ful and sympathetic reconstruction of Chomsky’s
intent defines the notion of ‘exclusion’ in such a
way that every node excludes itself (Chomsky’s
definition said that ‘α excludes Q if [and only if]
no segment of α dominates Q’, and of course a
given α never dominates itself). And sure enough,
the Stabler implementation revealed that this sys-
</bodyText>
<page confidence="0.989688">
15
</page>
<bodyText confidence="0.997761555555556">
tem of definitions had a problem: unbounded de- I am annoying to myself is prefixed with an as-
pendency constructions that Chomsky took to be terisk to show that it is ungrammatical. Searching
allowed were in fact blocked by his theoretical ma- for this exact strings using Google, as we can do
chinery. today, reveals that it gets 229 hits. I take this multi-
Stabler concluded from his discovery ‘that the ple attestation to shift the burden overwhelmingly
project of implementing GB theories transparently against the linguist who claims that it is barred by
is both manageable and worthwhile’. But his pa- the grammar of the language. But anyone who has
per has essentially never been referred to by any experience (as I do) with trying to talk TG lin-
mainstream syntacticians. It was not exactly what guists out of their beliefs by citing attested sen-
they wanted to hear. Nor has anyone, to my knowl- tences will know that it is between the difficult and
edge, utilized Stabler’s experience in doing syn- the impossible. From ‘There are many errors in
tactic research using the Barriers framework. published works’ to ‘It may be OK for him, but
There has in any case traditionally been con- it’s not for me’, there are many ways in which the
siderable resistance to machine testing of theo- linguist can escape from the conclusion that a ma-
ries. I have heard a story told by MIT linguists chine has proved superior in assessing the data.
of how one early graduate student devised a com- Hostility to corpus work has probably to some
puter program to test the rule system of SPE, and extent paved the way for the present situation,
told Morris Halle about some of the bugs he had where the machine translation teams at Google’s
thereby found, but Halle had already noticed all of research labs has no linguists, the work depending
them. The moral of the story is clearly supposed entirely on heavily numerical tracking of statisti-
to be that machine testing is unneeded and of no cal parallels seen in aligned bilingual texts.
value. And an unwholesome split is visible in the lin-
Mark Johnson as an undergraduate did some guistics community between those who broadly
work showing that the Unix stream editor sed want nothing to do with corpora and think personal
could serve as an excellent tool for implementing intuitions are fine as a basis for data gathering, and
systems of ordered historical sound changes for the people that I have called corpus fetishists who
the assistance of comparative-historical linguists; treat all facts as unclean and unholy unless they
but this very sensible idea never led to widespread come direct and unedited out of a corpus. At the
testing of synchronic phonological ordered-rule extremes, we get a divide between dreamers and
analyses. token-counters — on the one hand, people who
In short, computational testbeds, however en- think that speculations on how universal principles
thusiastically developed in some areas of science might account for subtle shades of their own inner
(chemistry, astrophysics, ecology, molecular biol- reactions to particular sentences, and on the other,
ogy), simply never (yet) took off in linguistic sci- people who think that counting the different pro-
ence. nouns in ten million words of text and tabulating
the results is a contribution to science.
</bodyText>
<sectionHeader confidence="0.9914075" genericHeader="method">
5 Loathing of corpora
6 Aversion to the stochastic
</sectionHeader>
<bodyText confidence="0.999952111111111">
There has traditionally been hostility even to
machine data-hunting or language study through
computer-searchable corpora. This is fading away
as a new generation of young linguists who do ev-
erything by searching the web do their data by web
search too; but it held back collaboration for a long
time. Early proposals for amassing computer cor-
pora were treated with contempt by TG grammar-
ians (‘I’m a native speaker, I have intuitions; why
do I need your arbitrary collection of computer-
searchable text?’).
And quite often evidence from attested sen-
tences is simply dismissed. To take a random ex-
ample, on page 48 of Postal (1971) the sentences
Mention of statistics reminds us that stochastic
methods have revolutionized CL since the 1980s,
but have made few inroads into general linguis-
tics, and none into TG linguistics. This is despite
the excellent introduction to probabilistic genera-
tive grammars provided in Levelt’s excellent and
far-sighted introduction to mathematical linguis-
tics (Levelt, 1974), the first volume of which has
now been republished separately (Levelt, 2008).
The reason for the extraordinarily low profile of
probabilistic grammars within the ranks of TG lin-
guists has to do with the very successful attack on
the very possibility of their relevance in Syntac-
</bodyText>
<page confidence="0.987045">
16
</page>
<bodyText confidence="0.999919964285714">
tic Structures (Chomsky, 1957). Insisting that any
statistical model for grammaticality would have
to treat Colorless green ideas sleep furiously and
Furiously sleep ideas green colorless in exactly
the same terms, as they are word strings with the
same (pre-1957) frequency of zero, Chomsky ar-
gued that probability of a string had no conceiv-
able relevance to its grammaticality.
Unfortunately he had made a mistake. He was
tacitly assuming that the probability of an event
type that has not yet occurred must be zero. Max-
imum likelihood estimation (MLE) does indeed
yield that result; but Chomsky was not obliged to
adopt MLE. The technique now known as smooth-
ing had been developed during the Second World
War by Alan Turing and I. J. Good, and although
it took a while to become known, Good had pub-
lished on it by 1953. Chomsky was simply not
acquainted with the statistical literature and not in-
terested in applying statistical methods to linguis-
tic material. Most linguists for the next forty years
followed him in his disdain for such work. But
when Pereira (2000) finally applied Good-Turing
estimation (smoothing) to the question of how dif-
ferent the probabilities of the two famous word se-
quences are from normal English text, he found
that the first (the syntactically well-formed one)
had a probability 200,000 times that of the second.
</bodyText>
<sectionHeader confidence="0.993393" genericHeader="method">
7 Contempt for applications
</sectionHeader>
<bodyText confidence="0.999964444444445">
Theoretical linguists have tended to have an al-
most total lack of interest in anything that might
offer a practical application for their theories.
Most kinds of science tend eventually to support
some sort of engineering or practical techniques:
physics led to jet planes; geology gave us oil lo-
cation methods; biology brought forth gene splic-
ing; even logic and psychology have applications
in factories and other workplaces. But not main-
stream theoretical linguistics. Its theories do not
seem to yield applications of any sort.
Very early on, Chomsky found that he had to
distance himself from computers altogether: note
the remark in Chomsky (1966, 9) that ‘Quite a
few commentators have assumed that recent work
in generative grammar is somehow an outgrowth
of an interest in the use of computers for one
or another purpose, or that it has some engineer-
ing motivation’, and note that he calls such views
both ‘incomprehensible’ and ‘entirely false’. Be-
ing taken to have ambitions relating to natural lan-
guage processing was at that time clearly anath-
ema for the leader of the TG community.
What takes the place of application of theories
to practical domains today, since nothing has come
of any computational TG linguistics, is an attempt
to derive conclusions about human brain organiza-
tion and mental anatomy. Linguists claim to be bi-
ologists rather than psychologists (psycholinguis-
tics developed its own experimental paradigms
and began its own steady progress away from in-
teraction with TG linguistics). There is a journal
called Biolinguistics now, and much talk about in-
terfaces and evolution and perfection. Linguists
somehow live with the fact that the real biologists
and neurophysiologists are not getting involved.
It is probably this pretense at uncovering deep
principles of structure in a putative mental organ
(and pretense is what it is) that is responsible for
the dramatic falling off of interest in precise de-
scription of languages. Getting the details right —
what was described as ‘observational adequacy’ in
Aspects (Chomsky, 1965) — is taken to be a low-
prestige occupation when compared to one that is
alleged to offer glimpses of universal principles
that hold the key to language acquisition and the
innate cognitive abilities of the species.
Yet these universal principles are never actually
presented for examination in the way that genuine
results in science are. It is as if what is impor-
tant to the hunter after universal principles is the
hunt itself, the call of the horn and the thrill of the
chase, but not the grubby business of examining
and weighing the kill. The fact is that no really
robust and carefully formulated universals of lan-
guage have been discovered, described, promul-
gated, confirmed, and widely accepted as correct
in the fifty years that universals have been sought.
The notion that linguists have discovered innate
principles that solve the mystery of first language
acquisition (Scholz and Pullum, 2006) is partic-
ularly pernicious. The position generally advo-
cated by TG linguists is widely known as linguis-
tic nativism, and it says that some significant as-
pects of knowledge of language are not derived
from any experience but are innately known. But
when pressed on the question of what the evidence
shows about linguistic nativism, about whether
it can really be defended against its plausible ri-
vals, nativists tend to react by drawing back very
sharply into a trivial form of the thesis: of course
linguistic nativism must be true, they insist, be-
</bodyText>
<page confidence="0.997918">
17
</page>
<bodyText confidence="0.999972045454545">
cause when you raise a baby and a kitten in the
same household under the same conditions it is
only the baby ends up with knowledge of lan-
guage. They therefore differ in some respect, in-
nately. ‘Universal grammar’ is simply one name
that linguists use for that which separates them:
whatever it is that human infants have but kittens
and monkeys and bricks don’t.
But of course, that makes the thesis trivial: it
is true in virtue of being merely a restatement of
the observation that led to linguistic nativism be-
ing put forward. We know that it is only human
neonates who accomplish the language acquisition
task, and that is why we are seeking an explanatory
theory of how humans accomplish the task. To say
that there must be something special about them is
certainly true, but that does not count as a scien-
tific discovery. We need specifics. Serious scien-
tists are like the private sector as characterized in
the immortal line uttered by Ray Stantz (played
by Dan Ackroyd) in Ghostsbusters: ‘I’ve worked
in the private sector. They expect results!’
</bodyText>
<sectionHeader confidence="0.902775" genericHeader="method">
8 Hope for the future
</sectionHeader>
<bodyText confidence="0.999872128205128">
It is absolutely not the case that general and the-
oretical linguistics should continue to act as if the
main object were to prevent any interaction with
CL. Let me point to a few hopeful developments.
Over the period from about 1989 to 2001, a
team of linguists worked on and completed a truly
comprehensive informal grammar of the English
language. It was published as Huddleston and Pul-
lum et al. (2002), henceforth CGEL. It is an infor-
mal grammar, intended for serious academic users
but not limited to those with a linguistics back-
ground. And it comes close to being fully exhaus-
tive in its coverage of Standard English grammat-
ical constructions and morphology.
It should not be forgotten that the era of TG,
though it produced (in my view) no theories that
are really worth having, an enormous number of
interesting data discoveries about English were
made. CGEL profited greatly from those, as the
Further Reading section makes clear. But does not
attempt to develop theoretical conclusions or par-
ticipate in theoretical disputes. Wherever possible,
CGEL takes a largely pretheoretic or at least basi-
cally neutral stance.
Where theoretical commitments have to be
made explicit, they are, but they are then imple-
mented in consistent terms across the entire book.
Although more than a dozen linguists were in-
volved, it is not an anthology; Huddleston and Pul-
lum provide a unitary authorial voice for the book
and rewrote every part of the book at least once.
When disputes about analyses arose between the
authors who drafted different chapters, they were
settled one way or the other by recourse to evi-
dence, and not permitted to create departures from
consistency in the book as a whole.
CGEL was preceded by large-scale 3-volume
grammars for Italian (Renzi et al., 2001) and for
Spanish (Bosque and Demonte, 1999), and now a
grammar of French on a similar scale, the Grande
Grammaire du franc¸ais is being written by a team
of linguists in Paris under the leadership of Anne
Abeill´e (Paris 7), Annie Delaveau (Paris 10), and
Dani`ele Godard (CNRS). In 2006 I visited Paris at
the request of that team to give a workshop on the
making of CGEL. Work continues, and the book is
now planned for publication by Editions Bayard in
2010. If anything the scope of this work is broader
than CGEL’s, since CGEL did not aim to cover
uncontroversially non-standard dialects of English
(for example, those that have negative concord),
whereas the Grande Grammaire explicitly aims
to cover regional and non-standard varieties of
French. Additionally, an effort to produce a com-
parable grammar of Mandarin Chinese is now be-
ing mounted in Hong Kong under the directorship
of Professor Chu-Ren Huang, the dean of the new
Faculty of Humanities at Hong Kong Polytechnic
University. I gave a workshop on CGEL there (in
March 2009) too.
The importance of these projects is simply that
they bear witness to the fact that, at least in some
areas, there are linguists — and not just isolated
individuals but teams of experienced linguists —
who are prepared to get involved in detailed lan-
guage description of the type that will be a prereq-
uisite to any future computational linguistics that
relies on details of syntax and semantics (rather
than probabilistic number-crunching on n-grams
and raw text, which has its own interest but does
not involve input from linguistics or even a rudi-
mentary knowledge of the language being pro-
cessed). Among them are both traditional general
linguists like Huddleston and people with serious
CL experience like Abeill´e and Huang.
But there is more. I have made a prelimi-
nary analysis of the inventory of syntactic cate-
gories used in the tagging for labelling trees in the
</bodyText>
<page confidence="0.997971">
18
</page>
<bodyText confidence="0.999921284313726">
Penn Treebank (Marcus et al., 1993), comparing
them to the categories used in CGEL. I would de-
scribe the fit as not perfect, but within negotiating
range. In some ways the fit is remarkable, given
the complete independence of the two projects
(the Treebank under Mitch Marcus in Philadelphia
was largely complete by 1992, when the CGEL
project under the direction of Rodney Huddleston
in Australia was only just getting up to speed, but
Huddleston and Marcus did not know about each
other’s work).
The biggest discrepancy in categorization is in
the problematic area of prepositions, adverbs, and
subordinating conjunctions, where the Treebank
has remained much too close to the confused older
tradition (where many prepositions are claimed to
have second lives as adverbs and quite a few are
also included on the list of subordinating conjunc-
tions, so that a word like since has one mean-
ing but three grammatical categories). The heart
of the problem is that the sage counsel of Jes-
persen (1924, 87–90) and the cogent arguments
of Emonds (1972) were not taken under consid-
eration by the devisers of the Treebank’s tagging
categories. But fixing that would involve nothing
more than undoing some unmotivated partitioning
of the preposition category.
Since there are few if any significant disagree-
ments about bracketing, and the category systems
could be brought into alignment, I believe it would
not be a major project to convert the entire Penn
Treebank into an alternate form where it was to-
tally compatible with CGEL in the syntactic anal-
yses it presupposed. There could be considerable
value in a complex of reference tools that included
a treebank of some 4.5 million words that is fully
compatible in its syntactic assumptions with an
1,860-page reference grammar of high reliability
and consistency.
And there is yet more. Here I will be brief,
and things will get slightly technical. The question
naturally arises of how one might formalize CGEL
to get it in a form where it was explicit enough for
use as a database that natural language process-
ing systems could in principle make use of. James
Rogers and I have recently considered that ques-
tion (Pullum and Rogers, 2008) within the con-
text of model-theoretic syntax, a line of work that
first began to receive sophisticated formulations
here at the EACL in various papers of the early
1990s (e.g. Blackburn et al. (1993), Kracht (1993),
Blackburn &amp; Gardent (1995); see Pullum (2007)
for a brief historical survey, and Pullum &amp; Scholz
(2001) for a deeper treatment of relevant theoreti-
cal issues).
One thing that might appear to be a stumbling-
block to formalizing CGEL, and an obstacle to the
relationship with treebanks as well, is that strictly
speaking CGEL’s assumed syntactic representa-
tions are not (or not all) trees. They are graphs that
depart from being ordinary constituent-structure
trees in at least two respects.
First, they are annotated not just with cate-
gories labelling the nodes, but also with syntactic
functions (grammatical relations like Subject-of,
Determiner-of, Head-of, Complement-of, etc.)
that are perhaps best conceptualized as labelling
the edges of the graph (the lines between the nodes
in the diagrams).
Second, and perhaps more seriously, there is oc-
casional downward convergence of branches: it
is permitted for a given constituent, under certain
conditions, to bear two different grammatical rela-
tions to two different superordinate nodes. (A de-
terminative like some, for example, may be both
the Determiner of an NP and the Head of the
Nominal that is the phrasal head of that NP.) Often
(as in HPSG work) the introduction of re-entrancy
had dramatic consequences for key properties like
decidability of satisfiability for descriptions, or
even for model-checking. (I take it that the for-
mal issues around HPSG are very well known to
the EACL community. In this short paper I do not
try to deal with HPSG at all. There is plenty to be
said, but also plenty of excellent HPSG specialists
in Europe who are more competent than I am to
treat the topic.)
Pullum &amp; Rogers (2008) shows, however, that
given certain very weak conditions, which seem
almost certainly to be satisfied by the kinds of
grammatical analysis posited in grammars of the
CGEL sort, there is a way of constructing a com-
patible directed ordered spanning tree for any
CGEL-style syntactic structure in such a way
that no information is lost and reachability via
edge chains is preserved. Moreover, the map-
ping between CGEL structures and spanning trees
is definable in weak monadic second-order logic
(wMSO).
Put this together with the results of Rogers
(1998) on definability of trees in wMSO, and there
is a clear prospect of the CGEL analysis of En-
</bodyText>
<page confidence="0.997291">
19
</page>
<bodyText confidence="0.999971659574468">
glish syntax being reconstructible in terms of the
wMSO theory of trees. And what that means for
parsing is clear from results of nearly 40 years
ago (Doner, 1970): there is a strong equivalence
via tree automata to context-free grammars, which
means that all the technology of context-free pars-
ing can potentially be brought to bear on process-
ing them.
This does not mean it would be a crisis if some
language of interest is found to be non-context-
free, incidentally. By the results of Rogers (2003),
wMSO theories interpreted on tree-like structures
of higher dimensionality than 2 could be em-
ployed. For example, where the structures are 3-
dimensional (so that individual nodes are allowed
to bear the parent-of relation to all of the nodes
in entire 2-dimensional trees), the string yield of
the set of all structures satisfying a given wMSO
sentence is always a tree-adjoining language, and
for every tree-adjoining language there is such a
characterizing wMSO sentence.
Notice, by the way, that the theoretical tools
of use here are coming out of currently very ac-
tive subdisciplines of computational logic and au-
tomata theory, such as finite model theory, descrip-
tive complexity theory, and database theory. The
very tools that linguistics needs in order to for-
malize syntactic theories in a revealing way are
the ones that theoretical computer science is in-
tensively working on because their investigation is
intrinsically interesting.
To sum up, what this is all telling us is that
there is no reason for anyone to continue be-
ing guided by the TG bias toward isolating the-
oretical linguistics from CL. There is not neces-
sarily a major gulf between (i) cutting-edge cur-
rent theoretical developments like model-theoretic
syntax, (ii) large-scale descriptive grammars like
CGEL, and (iii) feasible computational natural-
language engineering. Given the excellent per-
sonal relations between general linguists and com-
putational linguists in some European locations
(Edinburgh being an excellent example), it seems
to me that developments in interdisciplinary rela-
tions that would integrate the two disciplines quite
thoroughly could probably happen quite fast. Per-
haps it is happening already.
</bodyText>
<sectionHeader confidence="0.998622" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999817333333333">
I am very grateful to Barbara Scholz for her de-
tailed criticisms of a draft of this paper. I have
taken account of many of her helpful suggestions,
but since she still does not agree with what I say
here, none of the failings or errors above should be
blamed on her.
</bodyText>
<sectionHeader confidence="0.990279" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999782127659574">
Kazimierz Ajdukiewicz. 1935. Die syntaktische kon-
nexit¨at. Studia Philosophica, 1:1–27. Reprinted in
Storrs McCall, ed., Polish Logic 1920–1939, 207–
231. Oxford: Oxford University Press.
G. Edward Barton, Robert C. Berwick, and Eric Sven
Ristad. 1987. Computational Complexity and Natu-
ral Language. MIT Press, Cambridge, MA.
Patrick Blackburn and Claire Gardent. 1995. A spec-
ification language for lexical functional grammars.
In Seventh Conference of the European Chapter of
the Association for Computational Linguistics: Pro-
ceedings of the Conference, pages 39–44, Morris-
town, NJ. European Association for Computational
Linguistics.
Patrick Blackburn, Claire Gardent, and Wilfried
Meyer-Viol. 1993. Talking about trees. In Sixth
Conference of the European Chapter of the Associa-
tion for Computational Linguistics: Proceedings of
the Conference, pages 21–29, Morristown, NJ. Eu-
ropean Association for Computational Linguistics.
Ignacio Bosque and Violeta Demonte, editors. 1999.
Gram´atica Descriptiva de La Lengua Espa˜nola.
Real Academia Espa˜nola / Espasa Calpe, Madrid. 3
volumes.
Noam Chomsky and Howard Lasnik. 1977. Filters and
control. Linguistic Inquiry, 8:425–504.
Noam Chomsky. 1957. Syntactic Structures. Mouton,
The Hague.
Noam Chomsky. 1959. On certain formal properties
of grammars. Information and Control, 2:137–167.
Reprinted in Readings in Mathematical Psychology,
Volume II, ed. by R. Duncan Luce, Robert R. Bush,
and Eugene Galanter, 125–155, New York: John Wi-
ley &amp; Sons, 1965 (citation to the original on p. 125
of this reprinting is incorrect).
Noam Chomsky. 1962. Explanatory models in linguis-
tics. In Ernest Nagel, Patrick Suppes, and Alfred
Tarski, editors, Logic, Methodology and Philosophy
of Science: Proceedings of the 1960 International
Congress, pages 528–550, Stanford, CA. Stanford
University Press.
Noam Chomsky. 1965. Aspects of the Theory of Syn-
tax. MIT Press, Cambridge, MA.
Noam Chomsky. 1966. Topics in the Theory of Gener-
ative Grammar. Mouton, The Hague.
Noam Chomsky. 1980. Rules and Representations.
Basil Blackwell, Oxford.
</reference>
<page confidence="0.902636">
20
</page>
<reference confidence="0.999910842105263">
Noam Chomsky. 1981. Lectures on Government and
Binding. Foris, Dordrecht.
Noam Chomsky. 1986a. Barriers. MIT Press, Cam-
bridge, MA.
Noam Chomsky. 1986b. Knowledge of Language: Its
Origins, Nature, and Use. Praeger, New York.
John Doner. 1970. Tree acceptors and some of their
applications. Journal of Computer and System Sci-
ences, 4:406–451.
Joseph E. Emonds. 1972. Evidence that indirect ob-
ject movement is a structure-preserving rule. Foun-
dations of Language, 8:546–561.
Rodney Huddleston, Geoffrey K. Pullum, et al. 2002.
The Cambridge Grammar of the English Language.
Cambridge University Press, Cambridge.
Otto Jespersen. 1924. The Philosophy of Grammar.
Holt, New York.
Dexter Kozen. 1997. Automata and Computability.
Springer, Berlin.
Marcus Kracht. 1993. Mathematical aspects of com-
mand relations. In Sixth Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics: Proceedings of the Conference, pages
240–249, Morristown, NJ. Association for Compu-
tational Linguistics.
W. J. M. Levelt. 1974. Formal Grammars in Lin-
guistics and Psycholinguistics. Volume I: An Intro-
duction to the Theory of Formal Languages and
Automata; Volume II: Applications in Linguistic
Theory; Volume III: Psycholinguistic Applications.
Mouton, The Hague.
W. J. M. Levelt. 2008. An Introduction to the Theory of
Formal Languages and Automata. John Benjamins,
Amsterdam.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.
Emil Post. 1947. Recursive unsolvability of a problem
of thue. Journal of Symbolic Logic, 12:1–11.
Paul M. Postal. 1971. Crossover Phenomena. Holt,
Rinehart and Winston, New York.
Geoffrey K. Pullum and Gerald Gazdar. 1982. Natural
languages and context-free languages. Linguistics
and Philosophy, 4:471–504.
Geoffrey K. Pullum and James Rogers. 2008. Ex-
pressive power of the syntactic theory implicit in the
cambridge grammar of the english language. Pa-
per presented at the annual meeting of the Linguis-
tics Assocition of Great Britain, University of Es-
sex, September 2008. Online at http://ling.ed.ac.
uk/-gpullum/EssexLAGB.pdf.
Geoffrey K. Pullum and Barbara C. Scholz. 2001.
On the distinction between model-theoretic and
generative-enumerative syntactic frameworks. In
Philippe de Groote, Glyn Morrill, and Christian
Retor´e, editors, Logical Aspects of Computational
Linguistics: 4th International Conference, num-
ber 2099 in Lecture Notes in Artificial Intelligence,
pages 17–43, Berlin and New York. Springer.
Geoffrey K. Pullum. 1989. Formal linguistics meets
the Boojum. Natural Language &amp; Linguistic The-
ory, 7:137–143.
Geoffrey K. Pullum. 2007. The evolution of model-
theoretic frameworks in linguistics. In James Rogers
and Stephan Kepser, editors, Model-Theoretic Syn-
tax at 10: ESSLLI2007 Workshop, pages 1–10, Trin-
ity College Dublin, Ireland. Association for Logic,
Language and Information.
Lorenzo Renzi, Giampaolo Salvi, and Anna Cardi-
naletti. 2001. Grande grammatica italiana di con-
sultazione. Il Mulino, Bologna. 3 volumes.
James Rogers. 1998. A Descriptive Approach to
Language-Theoretic Complexity. CSLI Publica-
tions, Stanford, CA.
James Rogers. 2003. wMSO theories as grammar for-
malisms. Theoretical Computer Science, 293:291–
320.
Barbara C. Scholz and Geoffrey K. Pullum. 2006. Ir-
rational nativist exuberance. In Robert Stainton, ed-
itor, Contemporary Debates in Cognitive Science,
pages 59–80. Basil Blackwell, Oxford.
Pieter A. M. Seuren. 2004. Chomsky’s Minimalism.
Oxford University Press, Oxford.
Edward P. Stabler, Jr. 1992. Implementing government
binding theories. In Robert Levine, editor, Formal
Grammar: Theory and Implementation, pages 243–
289. Oxford University Press, New York.
Mark Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, MA.
Moshe Y. Vardi. 1982. The complexity of relational
query languages. In Proceedings of the 14th ACM
Symposium on Theory of Computing, pages 137–
146, New York. Association for Computing Machin-
ery.
</reference>
<page confidence="0.999437">
21
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.885264">
<title confidence="0.9888925">Computational Linguistics and Generative The Triumph of Hope over Experience</title>
<author confidence="0.999931">K Geoffrey</author>
<affiliation confidence="0.99926">School of Philosophy, Psychology, and Language University of</affiliation>
<email confidence="0.950011">gpullum@ling.ed.ac.ul</email>
<abstract confidence="0.9970158125">It is remarkable if any relationship at all persists between computational linguists (CL) and that part of general linguistics comprising the mainstream of MIT transformational-generative (TG) theoretical syntax. If the lines are still open, it represents something of a tribute to CL practitioners’ tolerance — a triumph of hope and goodwill over the experience of abuse — because the TG community has shown considerable hostility toward CL and everything it stands for over the past fifty years. I offer some brief historical notes, and hint at prospects for a better basis for collaboration in the future.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kazimierz Ajdukiewicz</author>
</authors>
<title>Die syntaktische konnexit¨at.</title>
<date>1935</date>
<booktitle>Studia Philosophica, 1:1–27. Reprinted in Storrs McCall, ed., Polish Logic 1920–1939, 207– 231. Oxford:</booktitle>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="8988" citStr="Ajdukiewicz (1935)" startWordPosition="1510" endWordPosition="1511">ystems (Chomsky’s ‘type-0’ grammars) can derive any r. e. set was not original with Chomsky (1959). It had been published twelve years earlier by Post. Post had in effect invented what could be called 13 top-down random generators. These randomly generate r. e. sets of symbols by expanding an initial axiomatic string, which can be just a single symbol. Their equivalence to Turing machines is obvious (Kozen 1997, 256–257). Between the time of Post’s doctoral work in 1920 and the 1943 paper in which he published his result on canonical systems (already present in compressed form in his thesis), Ajdukiewicz (1935) had proposed a different style of generative grammar, also motivated by the development of a better understanding of proof. Adjukiewicz’s invention was categorial grammar, the first kind of bottom-up random generators. It composes expressions of the generated language by combining parts — initially primitive categorized symbols, and then previously composed subparts. When Chomsky and Lasnik (1977) start talking about the ‘computational system’ of human language (a mode of speaking that rapidly caught on, and persists in current ‘minimalist’ work), the ‘computation’ of which they spoke was one</context>
</contexts>
<marker>Ajdukiewicz, 1935</marker>
<rawString>Kazimierz Ajdukiewicz. 1935. Die syntaktische konnexit¨at. Studia Philosophica, 1:1–27. Reprinted in Storrs McCall, ed., Polish Logic 1920–1939, 207– 231. Oxford: Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Edward Barton</author>
<author>Robert C Berwick</author>
<author>Eric Sven Ristad</author>
</authors>
<date>1987</date>
<booktitle>Computational Complexity and Natural Language.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="15295" citStr="Barton et al., 1987" startWordPosition="2561" endWordPosition="2564">uists who showed interest in Gazdar’s observation that a theory limited to generating context-free languages could guarantee not just recognition but recognition in polynomial (indeed, better than cubic) time, and in the related observation that none of the arguments for noncontext-free characteristics in human languages seemed to be good ones (Pullum and Gazdar, 1982). The MIT reaction to Gazdar’s suggestion was to mount a major effort to find intractability in Gazdar-style (GPSG) grammars — to represent the recognition problem as NP-hard even for context-free-equivalent theories of grammar (Barton et al., 1987). This was something of a confidence trick. First, the results depended on switching attention from the fixed-grammar arbitrary-string recognition problem (the analog of what Vardi (1982) calls data complexity) to the variable-grammar arbitrary-string recognition problem (what Vardi calls combined complexity). Second, it seemed to be vaguely assumed that only GPSG had any charges to answer, and that the GB theory of that time (Chomsky, 1981) would not suffer from similar computational complexity problems, but GB eventually turned out to be, insofar as it was well defined, strongly equivalent t</context>
</contexts>
<marker>Barton, Berwick, Ristad, 1987</marker>
<rawString>G. Edward Barton, Robert C. Berwick, and Eric Sven Ristad. 1987. Computational Complexity and Natural Language. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Blackburn</author>
<author>Claire Gardent</author>
</authors>
<title>A specification language for lexical functional grammars.</title>
<date>1995</date>
<booktitle>In Seventh Conference of the European Chapter of the Association for Computational Linguistics: Proceedings of the Conference,</booktitle>
<pages>39--44</pages>
<publisher>European Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ.</location>
<contexts>
<context position="34713" citStr="Blackburn &amp; Gardent (1995)" startWordPosition="5783" endWordPosition="5786">istency. And there is yet more. Here I will be brief, and things will get slightly technical. The question naturally arises of how one might formalize CGEL to get it in a form where it was explicit enough for use as a database that natural language processing systems could in principle make use of. James Rogers and I have recently considered that question (Pullum and Rogers, 2008) within the context of model-theoretic syntax, a line of work that first began to receive sophisticated formulations here at the EACL in various papers of the early 1990s (e.g. Blackburn et al. (1993), Kracht (1993), Blackburn &amp; Gardent (1995); see Pullum (2007) for a brief historical survey, and Pullum &amp; Scholz (2001) for a deeper treatment of relevant theoretical issues). One thing that might appear to be a stumblingblock to formalizing CGEL, and an obstacle to the relationship with treebanks as well, is that strictly speaking CGEL’s assumed syntactic representations are not (or not all) trees. They are graphs that depart from being ordinary constituent-structure trees in at least two respects. First, they are annotated not just with categories labelling the nodes, but also with syntactic functions (grammatical relations like Sub</context>
</contexts>
<marker>Blackburn, Gardent, 1995</marker>
<rawString>Patrick Blackburn and Claire Gardent. 1995. A specification language for lexical functional grammars. In Seventh Conference of the European Chapter of the Association for Computational Linguistics: Proceedings of the Conference, pages 39–44, Morristown, NJ. European Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Blackburn</author>
<author>Claire Gardent</author>
<author>Wilfried Meyer-Viol</author>
</authors>
<title>Talking about trees.</title>
<date>1993</date>
<booktitle>In Sixth Conference of the European Chapter of the Association for Computational Linguistics: Proceedings of the Conference,</booktitle>
<pages>21--29</pages>
<publisher>European Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ.</location>
<contexts>
<context position="34670" citStr="Blackburn et al. (1993)" startWordPosition="5777" endWordPosition="5780">nce grammar of high reliability and consistency. And there is yet more. Here I will be brief, and things will get slightly technical. The question naturally arises of how one might formalize CGEL to get it in a form where it was explicit enough for use as a database that natural language processing systems could in principle make use of. James Rogers and I have recently considered that question (Pullum and Rogers, 2008) within the context of model-theoretic syntax, a line of work that first began to receive sophisticated formulations here at the EACL in various papers of the early 1990s (e.g. Blackburn et al. (1993), Kracht (1993), Blackburn &amp; Gardent (1995); see Pullum (2007) for a brief historical survey, and Pullum &amp; Scholz (2001) for a deeper treatment of relevant theoretical issues). One thing that might appear to be a stumblingblock to formalizing CGEL, and an obstacle to the relationship with treebanks as well, is that strictly speaking CGEL’s assumed syntactic representations are not (or not all) trees. They are graphs that depart from being ordinary constituent-structure trees in at least two respects. First, they are annotated not just with categories labelling the nodes, but also with syntacti</context>
</contexts>
<marker>Blackburn, Gardent, Meyer-Viol, 1993</marker>
<rawString>Patrick Blackburn, Claire Gardent, and Wilfried Meyer-Viol. 1993. Talking about trees. In Sixth Conference of the European Chapter of the Association for Computational Linguistics: Proceedings of the Conference, pages 21–29, Morristown, NJ. European Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<date>1999</date>
<booktitle>Gram´atica Descriptiva de La Lengua Espa˜nola. Real Academia Espa˜nola / Espasa Calpe,</booktitle>
<volume>3</volume>
<pages>volumes.</pages>
<editor>Ignacio Bosque and Violeta Demonte, editors.</editor>
<location>Madrid.</location>
<marker>1999</marker>
<rawString>Ignacio Bosque and Violeta Demonte, editors. 1999. Gram´atica Descriptiva de La Lengua Espa˜nola. Real Academia Espa˜nola / Espasa Calpe, Madrid. 3 volumes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
<author>Howard Lasnik</author>
</authors>
<title>Filters and control.</title>
<date>1977</date>
<booktitle>Linguistic Inquiry,</booktitle>
<pages>8--425</pages>
<contexts>
<context position="9389" citStr="Chomsky and Lasnik (1977)" startWordPosition="1566" endWordPosition="1569"> obvious (Kozen 1997, 256–257). Between the time of Post’s doctoral work in 1920 and the 1943 paper in which he published his result on canonical systems (already present in compressed form in his thesis), Ajdukiewicz (1935) had proposed a different style of generative grammar, also motivated by the development of a better understanding of proof. Adjukiewicz’s invention was categorial grammar, the first kind of bottom-up random generators. It composes expressions of the generated language by combining parts — initially primitive categorized symbols, and then previously composed subparts. When Chomsky and Lasnik (1977) start talking about the ‘computational system’ of human language (a mode of speaking that rapidly caught on, and persists in current ‘minimalist’ work), the ‘computation’ of which they spoke was one that takes place nowhere: no such computations are ever done, except perhaps using pencil and paper as a syntactic theorist tries to figure out how or whether a certain string can be derived. This ‘computational system’ attempts randomly and nondeterministically to find some way to apply rules in order to build a particular structure, starting from an arbitrary syntactic starting point. In the cas</context>
</contexts>
<marker>Chomsky, Lasnik, 1977</marker>
<rawString>Noam Chomsky and Howard Lasnik. 1977. Filters and control. Linguistic Inquiry, 8:425–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Syntactic Structures.</title>
<date>1957</date>
<publisher>Mouton, The Hague.</publisher>
<contexts>
<context position="22648" citStr="Chomsky, 1957" startWordPosition="3764" endWordPosition="3765">e revolutionized CL since the 1980s, but have made few inroads into general linguistics, and none into TG linguistics. This is despite the excellent introduction to probabilistic generative grammars provided in Levelt’s excellent and far-sighted introduction to mathematical linguistics (Levelt, 1974), the first volume of which has now been republished separately (Levelt, 2008). The reason for the extraordinarily low profile of probabilistic grammars within the ranks of TG linguists has to do with the very successful attack on the very possibility of their relevance in Syntac16 tic Structures (Chomsky, 1957). Insisting that any statistical model for grammaticality would have to treat Colorless green ideas sleep furiously and Furiously sleep ideas green colorless in exactly the same terms, as they are word strings with the same (pre-1957) frequency of zero, Chomsky argued that probability of a string had no conceivable relevance to its grammaticality. Unfortunately he had made a mistake. He was tacitly assuming that the probability of an event type that has not yet occurred must be zero. Maximum likelihood estimation (MLE) does indeed yield that result; but Chomsky was not obliged to adopt MLE. Th</context>
</contexts>
<marker>Chomsky, 1957</marker>
<rawString>Noam Chomsky. 1957. Syntactic Structures. Mouton, The Hague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>On certain formal properties of grammars. Information and Control,</title>
<date>1959</date>
<booktitle>Reprinted in Readings in Mathematical Psychology, Volume II,</booktitle>
<pages>2--137</pages>
<editor>ed. by R. Duncan Luce, Robert R. Bush, and Eugene Galanter,</editor>
<publisher>John Wiley &amp; Sons,</publisher>
<location>New York:</location>
<contexts>
<context position="3790" citStr="Chomsky (1959)" startWordPosition="605" endWordPosition="606">her attempts to incorporate the mechanisms of this faculty)’). We need more precise terminology in order to home in on what I am talking about. As Seuren (2004) has stressed, the relevant vision of what a grammar is like, built into most linguistic theorization today at a level so deep that most linguists are incapable of seeing past it or out of it, is not just that it is explicit, but that a grammar is and must be a syntax-centered random generator. I will therefore refer to language specification by random generation (LSRG). The definitive technical paper defining grammars in LSRG terms is Chomsky (1959). This was a fine paper, which would have earned its Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Linguistics, pages 12–21, Athens, Greece, 30 March, 2009. c�2009 Association for Computational Linguistics 12 writer tenure in any department of linguistics, logic, computer science, or mathematics that knew what it was doing and could see the possibilities. But it brought into linguistics two things that were not going to go away for half a century. One was the notion that any formally precise linguistics had to be limited to LSRG. And the other w</context>
<context position="8468" citStr="Chomsky (1959)" startWordPosition="1423" endWordPosition="1425">l canonical systems. In a later paper, settling a conjecture of Thue, Post showed (1947) that you can derive every canonical set if your productions all have the form ‘P1giP2 produces P1gjP2’. This amounts to showing that every canonical subset of E+ can be generated by (what would later be called) a generative grammar using a symbol vocabulary V = E ∪ N in which all rules have the form ‘WxZ → WyZ’ for specified strings x, y ∈ V * and fixed W, Z ∈ V *. Hence the first demonstration that unrestricted rewriting systems (Chomsky’s ‘type-0’ grammars) can derive any r. e. set was not original with Chomsky (1959). It had been published twelve years earlier by Post. Post had in effect invented what could be called 13 top-down random generators. These randomly generate r. e. sets of symbols by expanding an initial axiomatic string, which can be just a single symbol. Their equivalence to Turing machines is obvious (Kozen 1997, 256–257). Between the time of Post’s doctoral work in 1920 and the 1943 paper in which he published his result on canonical systems (already present in compressed form in his thesis), Ajdukiewicz (1935) had proposed a different style of generative grammar, also motivated by the dev</context>
</contexts>
<marker>Chomsky, 1959</marker>
<rawString>Noam Chomsky. 1959. On certain formal properties of grammars. Information and Control, 2:137–167. Reprinted in Readings in Mathematical Psychology, Volume II, ed. by R. Duncan Luce, Robert R. Bush, and Eugene Galanter, 125–155, New York: John Wiley &amp; Sons, 1965 (citation to the original on p. 125 of this reprinting is incorrect).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Explanatory models in linguistics.</title>
<date>1962</date>
<booktitle>Logic, Methodology and Philosophy of Science: Proceedings of the 1960 International Congress,</booktitle>
<pages>528--550</pages>
<editor>In Ernest Nagel, Patrick Suppes, and Alfred Tarski, editors,</editor>
<publisher>Stanford University Press.</publisher>
<location>Stanford, CA.</location>
<contexts>
<context position="11035" citStr="Chomsky 1962" startWordPosition="1845" endWordPosition="1846">a string as encountered, from first to last, so that it was in effect a parser. Categorial grammars seem ideally suited to that role (Steedman, 2000), and minimalist grammars are really just a variety of categorial grammar, stripped of some of the formal coherence and links to logic and semantics. Chomsky has often written as if it were a necessary truth that a grammar must be a random generator. For example: ‘Clearly, a grammar must contain ... a ‘syntactic component’ that generates an infinite number of strings representing grammatical sentences ... This is the classical model for grammar’ (Chomsky 1962, 539). This says that a grammar must be a random generator. But this is not true. A grammar could in principle be formulated as, say a transducer mapping phonetic representation inputs to corresponding sets of logical forms. (Presumably this must be possible, given what human beings do.) It is particularly strange to see Chomsky ignoring this possibility and yet asserting in Knowledge of Language (Chomsky, 1986b) that a person’s internalized grammar ‘assigns a status to every relevant physical event, say, every sound wave’ (p. 26). The claim is false, simply because random generators are not </context>
</contexts>
<marker>Chomsky, 1962</marker>
<rawString>Noam Chomsky. 1962. Explanatory models in linguistics. In Ernest Nagel, Patrick Suppes, and Alfred Tarski, editors, Logic, Methodology and Philosophy of Science: Proceedings of the 1960 International Congress, pages 528–550, Stanford, CA. Stanford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Aspects of the Theory of Syntax.</title>
<date>1965</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="26055" citStr="Chomsky, 1965" startWordPosition="4321" endWordPosition="4322"> its own steady progress away from interaction with TG linguistics). There is a journal called Biolinguistics now, and much talk about interfaces and evolution and perfection. Linguists somehow live with the fact that the real biologists and neurophysiologists are not getting involved. It is probably this pretense at uncovering deep principles of structure in a putative mental organ (and pretense is what it is) that is responsible for the dramatic falling off of interest in precise description of languages. Getting the details right — what was described as ‘observational adequacy’ in Aspects (Chomsky, 1965) — is taken to be a lowprestige occupation when compared to one that is alleged to offer glimpses of universal principles that hold the key to language acquisition and the innate cognitive abilities of the species. Yet these universal principles are never actually presented for examination in the way that genuine results in science are. It is as if what is important to the hunter after universal principles is the hunt itself, the call of the horn and the thrill of the chase, but not the grubby business of examining and weighing the kill. The fact is that no really robust and carefully formulat</context>
</contexts>
<marker>Chomsky, 1965</marker>
<rawString>Noam Chomsky. 1965. Aspects of the Theory of Syntax. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Topics in the Theory of Generative Grammar.</title>
<date>1966</date>
<publisher>Mouton, The Hague.</publisher>
<contexts>
<context position="3053" citStr="Chomsky 1966" startWordPosition="475" endWordPosition="476">ing a sober look at the half-century of history from 1959 to 2009, during which almost everything about the course of theoretical syntax, at least in the USA, where I worked during the latter half of the period, has been tacitly guided by a single line of thinking. ‘Generative grammar’ is commonly used to denote it, but that will not do. First, ‘generative grammar’ is often used to mean ‘MIT-influenced transformationalgenerative grammar’. For that I will use the abbreviation TG. And second, it is sometimes (incorrectly) claimed that ‘generative’ means nothing more or less than ‘explicit’ (see Chomsky 1966, 12: ‘a generative grammar (that is, an explicit grammar that makes no appeal to the reader’s “facul´e de langage” but rather attempts to incorporate the mechanisms of this faculty)’). We need more precise terminology in order to home in on what I am talking about. As Seuren (2004) has stressed, the relevant vision of what a grammar is like, built into most linguistic theorization today at a level so deep that most linguists are incapable of seeing past it or out of it, is not just that it is explicit, but that a grammar is and must be a syntax-centered random generator. I will therefore refe</context>
<context position="24646" citStr="Chomsky (1966" startWordPosition="4094" endWordPosition="4095">ve an almost total lack of interest in anything that might offer a practical application for their theories. Most kinds of science tend eventually to support some sort of engineering or practical techniques: physics led to jet planes; geology gave us oil location methods; biology brought forth gene splicing; even logic and psychology have applications in factories and other workplaces. But not mainstream theoretical linguistics. Its theories do not seem to yield applications of any sort. Very early on, Chomsky found that he had to distance himself from computers altogether: note the remark in Chomsky (1966, 9) that ‘Quite a few commentators have assumed that recent work in generative grammar is somehow an outgrowth of an interest in the use of computers for one or another purpose, or that it has some engineering motivation’, and note that he calls such views both ‘incomprehensible’ and ‘entirely false’. Being taken to have ambitions relating to natural language processing was at that time clearly anathema for the leader of the TG community. What takes the place of application of theories to practical domains today, since nothing has come of any computational TG linguistics, is an attempt to der</context>
</contexts>
<marker>Chomsky, 1966</marker>
<rawString>Noam Chomsky. 1966. Topics in the Theory of Generative Grammar. Mouton, The Hague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Rules and Representations.</title>
<date>1980</date>
<publisher>Basil Blackwell,</publisher>
<location>Oxford.</location>
<contexts>
<context position="16326" citStr="Chomsky 1980" startWordPosition="2724" endWordPosition="2725"> that time (Chomsky, 1981) would not suffer from similar computational complexity problems, but GB eventually turned out to be, insofar as it was well defined, strongly equivalent to Gazdar’s framework (Rogers, 1998). For pre-GB varieties of TG, however, the problem had mainly been not that recognition was NPhard but that it was not computable at all: transformational grammars from 1957 on kept proving to be Turing-equivalent. That was what seems to have driven the denigration of mathematical linguistics, and the downplaying of the relevance of decidability to such an extreme degree (see e.g. Chomsky 1980: 120ff, where the very idea that recognition is decidable is dismissed as an unimportant detail, and not necessarily even a true claim). 4 Hostility to machine testing With many versions of TG offering no guarantee that there was any parser for the language even in principle, it was not clear that machine testing of grammatical theories by algorithmic checking of claims made about grammaticality of selected strings was a plausible idea. Perhaps machine theorem-proving algorithms could have been adapted to showing that a certain grammar could indeed derive a certain string, but in practice ear</context>
</contexts>
<marker>Chomsky, 1980</marker>
<rawString>Noam Chomsky. 1980. Rules and Representations. Basil Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<date>1981</date>
<booktitle>Lectures on Government and Binding. Foris,</booktitle>
<location>Dordrecht.</location>
<contexts>
<context position="15740" citStr="Chomsky, 1981" startWordPosition="2628" endWordPosition="2629">tractability in Gazdar-style (GPSG) grammars — to represent the recognition problem as NP-hard even for context-free-equivalent theories of grammar (Barton et al., 1987). This was something of a confidence trick. First, the results depended on switching attention from the fixed-grammar arbitrary-string recognition problem (the analog of what Vardi (1982) calls data complexity) to the variable-grammar arbitrary-string recognition problem (what Vardi calls combined complexity). Second, it seemed to be vaguely assumed that only GPSG had any charges to answer, and that the GB theory of that time (Chomsky, 1981) would not suffer from similar computational complexity problems, but GB eventually turned out to be, insofar as it was well defined, strongly equivalent to Gazdar’s framework (Rogers, 1998). For pre-GB varieties of TG, however, the problem had mainly been not that recognition was NPhard but that it was not computable at all: transformational grammars from 1957 on kept proving to be Turing-equivalent. That was what seems to have driven the denigration of mathematical linguistics, and the downplaying of the relevance of decidability to such an extreme degree (see e.g. Chomsky 1980: 120ff, where</context>
</contexts>
<marker>Chomsky, 1981</marker>
<rawString>Noam Chomsky. 1981. Lectures on Government and Binding. Foris, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<date>1986</date>
<publisher>Barriers. MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="11450" citStr="Chomsky, 1986" startWordPosition="1915" endWordPosition="1916">Clearly, a grammar must contain ... a ‘syntactic component’ that generates an infinite number of strings representing grammatical sentences ... This is the classical model for grammar’ (Chomsky 1962, 539). This says that a grammar must be a random generator. But this is not true. A grammar could in principle be formulated as, say a transducer mapping phonetic representation inputs to corresponding sets of logical forms. (Presumably this must be possible, given what human beings do.) It is particularly strange to see Chomsky ignoring this possibility and yet asserting in Knowledge of Language (Chomsky, 1986b) that a person’s internalized grammar ‘assigns a status to every relevant physical event, say, every sound wave’ (p. 26). The claim is false, simply because random generators are not transducers or functions: they do not take inputs. A random generator only ‘assigns a status’ to a string by generating it with a derivation that associates it with certain properties. And surely it is not a sensible hypothesis about human linguistic competence to posit that in the brain of every human being there is an internalized random generator generating every physically possible sequence of sounds, from a</context>
<context position="17316" citStr="Chomsky, 1986" startWordPosition="2881" endWordPosition="2883">e about grammaticality of selected strings was a plausible idea. Perhaps machine theorem-proving algorithms could have been adapted to showing that a certain grammar could indeed derive a certain string, but in practice early transformational grammar was vastly too complex to permit the building of tools for grammar testing, and later transformational grammar far too vague. I know of only one success story in grammar evaluation by implementing random generation, in fact. Ed Stabler (1992) coded up a Prolog grammaticality-proving system based on the Barriers theory of transformational grammar (Chomsky, 1986a), which (Pullum, 1989) had mocked for sloppiness of statement. The Barriers system had in particular abandoned the usual practice of defining trees in a way that had dominance as a reflexive relation. Chomsky casually asserted that he would take it to be irreflexive. Moreover, Stabler’s careful and sympathetic reconstruction of Chomsky’s intent defines the notion of ‘exclusion’ in such a way that every node excludes itself (Chomsky’s definition said that ‘α excludes Q if [and only if] no segment of α dominates Q’, and of course a given α never dominates itself). And sure enough, the Stabler </context>
</contexts>
<marker>Chomsky, 1986</marker>
<rawString>Noam Chomsky. 1986a. Barriers. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Knowledge of Language: Its Origins, Nature, and Use.</title>
<date>1986</date>
<publisher>Praeger,</publisher>
<location>New York.</location>
<contexts>
<context position="11450" citStr="Chomsky, 1986" startWordPosition="1915" endWordPosition="1916">Clearly, a grammar must contain ... a ‘syntactic component’ that generates an infinite number of strings representing grammatical sentences ... This is the classical model for grammar’ (Chomsky 1962, 539). This says that a grammar must be a random generator. But this is not true. A grammar could in principle be formulated as, say a transducer mapping phonetic representation inputs to corresponding sets of logical forms. (Presumably this must be possible, given what human beings do.) It is particularly strange to see Chomsky ignoring this possibility and yet asserting in Knowledge of Language (Chomsky, 1986b) that a person’s internalized grammar ‘assigns a status to every relevant physical event, say, every sound wave’ (p. 26). The claim is false, simply because random generators are not transducers or functions: they do not take inputs. A random generator only ‘assigns a status’ to a string by generating it with a derivation that associates it with certain properties. And surely it is not a sensible hypothesis about human linguistic competence to posit that in the brain of every human being there is an internalized random generator generating every physically possible sequence of sounds, from a</context>
<context position="17316" citStr="Chomsky, 1986" startWordPosition="2881" endWordPosition="2883">e about grammaticality of selected strings was a plausible idea. Perhaps machine theorem-proving algorithms could have been adapted to showing that a certain grammar could indeed derive a certain string, but in practice early transformational grammar was vastly too complex to permit the building of tools for grammar testing, and later transformational grammar far too vague. I know of only one success story in grammar evaluation by implementing random generation, in fact. Ed Stabler (1992) coded up a Prolog grammaticality-proving system based on the Barriers theory of transformational grammar (Chomsky, 1986a), which (Pullum, 1989) had mocked for sloppiness of statement. The Barriers system had in particular abandoned the usual practice of defining trees in a way that had dominance as a reflexive relation. Chomsky casually asserted that he would take it to be irreflexive. Moreover, Stabler’s careful and sympathetic reconstruction of Chomsky’s intent defines the notion of ‘exclusion’ in such a way that every node excludes itself (Chomsky’s definition said that ‘α excludes Q if [and only if] no segment of α dominates Q’, and of course a given α never dominates itself). And sure enough, the Stabler </context>
</contexts>
<marker>Chomsky, 1986</marker>
<rawString>Noam Chomsky. 1986b. Knowledge of Language: Its Origins, Nature, and Use. Praeger, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Doner</author>
</authors>
<title>Tree acceptors and some of their applications.</title>
<date>1970</date>
<journal>Journal of Computer and System Sciences,</journal>
<pages>4--406</pages>
<contexts>
<context position="37169" citStr="Doner, 1970" startWordPosition="6193" endWordPosition="6194">nstructing a compatible directed ordered spanning tree for any CGEL-style syntactic structure in such a way that no information is lost and reachability via edge chains is preserved. Moreover, the mapping between CGEL structures and spanning trees is definable in weak monadic second-order logic (wMSO). Put this together with the results of Rogers (1998) on definability of trees in wMSO, and there is a clear prospect of the CGEL analysis of En19 glish syntax being reconstructible in terms of the wMSO theory of trees. And what that means for parsing is clear from results of nearly 40 years ago (Doner, 1970): there is a strong equivalence via tree automata to context-free grammars, which means that all the technology of context-free parsing can potentially be brought to bear on processing them. This does not mean it would be a crisis if some language of interest is found to be non-contextfree, incidentally. By the results of Rogers (2003), wMSO theories interpreted on tree-like structures of higher dimensionality than 2 could be employed. For example, where the structures are 3- dimensional (so that individual nodes are allowed to bear the parent-of relation to all of the nodes in entire 2-dimens</context>
</contexts>
<marker>Doner, 1970</marker>
<rawString>John Doner. 1970. Tree acceptors and some of their applications. Journal of Computer and System Sciences, 4:406–451.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph E Emonds</author>
</authors>
<title>Evidence that indirect object movement is a structure-preserving rule. Foundations of Language,</title>
<date>1972</date>
<pages>8--546</pages>
<contexts>
<context position="33332" citStr="Emonds (1972)" startWordPosition="5555" endWordPosition="5556"> but Huddleston and Marcus did not know about each other’s work). The biggest discrepancy in categorization is in the problematic area of prepositions, adverbs, and subordinating conjunctions, where the Treebank has remained much too close to the confused older tradition (where many prepositions are claimed to have second lives as adverbs and quite a few are also included on the list of subordinating conjunctions, so that a word like since has one meaning but three grammatical categories). The heart of the problem is that the sage counsel of Jespersen (1924, 87–90) and the cogent arguments of Emonds (1972) were not taken under consideration by the devisers of the Treebank’s tagging categories. But fixing that would involve nothing more than undoing some unmotivated partitioning of the preposition category. Since there are few if any significant disagreements about bracketing, and the category systems could be brought into alignment, I believe it would not be a major project to convert the entire Penn Treebank into an alternate form where it was totally compatible with CGEL in the syntactic analyses it presupposed. There could be considerable value in a complex of reference tools that included a</context>
</contexts>
<marker>Emonds, 1972</marker>
<rawString>Joseph E. Emonds. 1972. Evidence that indirect object movement is a structure-preserving rule. Foundations of Language, 8:546–561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodney Huddleston</author>
<author>Geoffrey K Pullum</author>
</authors>
<title>The Cambridge Grammar of the English Language.</title>
<date>2002</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<marker>Huddleston, Pullum, 2002</marker>
<rawString>Rodney Huddleston, Geoffrey K. Pullum, et al. 2002. The Cambridge Grammar of the English Language. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Otto Jespersen</author>
</authors>
<title>The Philosophy of Grammar.</title>
<date>1924</date>
<location>Holt, New York.</location>
<contexts>
<context position="33282" citStr="Jespersen (1924" startWordPosition="5546" endWordPosition="5548">ston in Australia was only just getting up to speed, but Huddleston and Marcus did not know about each other’s work). The biggest discrepancy in categorization is in the problematic area of prepositions, adverbs, and subordinating conjunctions, where the Treebank has remained much too close to the confused older tradition (where many prepositions are claimed to have second lives as adverbs and quite a few are also included on the list of subordinating conjunctions, so that a word like since has one meaning but three grammatical categories). The heart of the problem is that the sage counsel of Jespersen (1924, 87–90) and the cogent arguments of Emonds (1972) were not taken under consideration by the devisers of the Treebank’s tagging categories. But fixing that would involve nothing more than undoing some unmotivated partitioning of the preposition category. Since there are few if any significant disagreements about bracketing, and the category systems could be brought into alignment, I believe it would not be a major project to convert the entire Penn Treebank into an alternate form where it was totally compatible with CGEL in the syntactic analyses it presupposed. There could be considerable val</context>
</contexts>
<marker>Jespersen, 1924</marker>
<rawString>Otto Jespersen. 1924. The Philosophy of Grammar. Holt, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dexter Kozen</author>
</authors>
<title>Automata and Computability.</title>
<date>1997</date>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<contexts>
<context position="8784" citStr="Kozen 1997" startWordPosition="1477" endWordPosition="1478"> using a symbol vocabulary V = E ∪ N in which all rules have the form ‘WxZ → WyZ’ for specified strings x, y ∈ V * and fixed W, Z ∈ V *. Hence the first demonstration that unrestricted rewriting systems (Chomsky’s ‘type-0’ grammars) can derive any r. e. set was not original with Chomsky (1959). It had been published twelve years earlier by Post. Post had in effect invented what could be called 13 top-down random generators. These randomly generate r. e. sets of symbols by expanding an initial axiomatic string, which can be just a single symbol. Their equivalence to Turing machines is obvious (Kozen 1997, 256–257). Between the time of Post’s doctoral work in 1920 and the 1943 paper in which he published his result on canonical systems (already present in compressed form in his thesis), Ajdukiewicz (1935) had proposed a different style of generative grammar, also motivated by the development of a better understanding of proof. Adjukiewicz’s invention was categorial grammar, the first kind of bottom-up random generators. It composes expressions of the generated language by combining parts — initially primitive categorized symbols, and then previously composed subparts. When Chomsky and Lasnik (</context>
</contexts>
<marker>Kozen, 1997</marker>
<rawString>Dexter Kozen. 1997. Automata and Computability. Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcus Kracht</author>
</authors>
<title>Mathematical aspects of command relations.</title>
<date>1993</date>
<booktitle>In Sixth Conference of the European Chapter of the Association for Computational Linguistics: Proceedings of the Conference,</booktitle>
<pages>240--249</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ.</location>
<contexts>
<context position="34685" citStr="Kracht (1993)" startWordPosition="5781" endWordPosition="5782">bility and consistency. And there is yet more. Here I will be brief, and things will get slightly technical. The question naturally arises of how one might formalize CGEL to get it in a form where it was explicit enough for use as a database that natural language processing systems could in principle make use of. James Rogers and I have recently considered that question (Pullum and Rogers, 2008) within the context of model-theoretic syntax, a line of work that first began to receive sophisticated formulations here at the EACL in various papers of the early 1990s (e.g. Blackburn et al. (1993), Kracht (1993), Blackburn &amp; Gardent (1995); see Pullum (2007) for a brief historical survey, and Pullum &amp; Scholz (2001) for a deeper treatment of relevant theoretical issues). One thing that might appear to be a stumblingblock to formalizing CGEL, and an obstacle to the relationship with treebanks as well, is that strictly speaking CGEL’s assumed syntactic representations are not (or not all) trees. They are graphs that depart from being ordinary constituent-structure trees in at least two respects. First, they are annotated not just with categories labelling the nodes, but also with syntactic functions (gr</context>
</contexts>
<marker>Kracht, 1993</marker>
<rawString>Marcus Kracht. 1993. Mathematical aspects of command relations. In Sixth Conference of the European Chapter of the Association for Computational Linguistics: Proceedings of the Conference, pages 240–249, Morristown, NJ. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J M Levelt</author>
</authors>
<title>Formal Grammars in Linguistics and Psycholinguistics. Volume I: An Introduction to the Theory of Formal Languages and Automata; Volume II: Applications in Linguistic Theory; Volume III: Psycholinguistic Applications.</title>
<date>1974</date>
<publisher>Mouton, The Hague.</publisher>
<contexts>
<context position="22335" citStr="Levelt, 1974" startWordPosition="3713" endWordPosition="3714">(‘I’m a native speaker, I have intuitions; why do I need your arbitrary collection of computersearchable text?’). And quite often evidence from attested sentences is simply dismissed. To take a random example, on page 48 of Postal (1971) the sentences Mention of statistics reminds us that stochastic methods have revolutionized CL since the 1980s, but have made few inroads into general linguistics, and none into TG linguistics. This is despite the excellent introduction to probabilistic generative grammars provided in Levelt’s excellent and far-sighted introduction to mathematical linguistics (Levelt, 1974), the first volume of which has now been republished separately (Levelt, 2008). The reason for the extraordinarily low profile of probabilistic grammars within the ranks of TG linguists has to do with the very successful attack on the very possibility of their relevance in Syntac16 tic Structures (Chomsky, 1957). Insisting that any statistical model for grammaticality would have to treat Colorless green ideas sleep furiously and Furiously sleep ideas green colorless in exactly the same terms, as they are word strings with the same (pre-1957) frequency of zero, Chomsky argued that probability o</context>
</contexts>
<marker>Levelt, 1974</marker>
<rawString>W. J. M. Levelt. 1974. Formal Grammars in Linguistics and Psycholinguistics. Volume I: An Introduction to the Theory of Formal Languages and Automata; Volume II: Applications in Linguistic Theory; Volume III: Psycholinguistic Applications. Mouton, The Hague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J M Levelt</author>
</authors>
<title>An Introduction to the Theory of Formal Languages and Automata. John Benjamins,</title>
<date>2008</date>
<location>Amsterdam.</location>
<contexts>
<context position="22413" citStr="Levelt, 2008" startWordPosition="3725" endWordPosition="3726">tion of computersearchable text?’). And quite often evidence from attested sentences is simply dismissed. To take a random example, on page 48 of Postal (1971) the sentences Mention of statistics reminds us that stochastic methods have revolutionized CL since the 1980s, but have made few inroads into general linguistics, and none into TG linguistics. This is despite the excellent introduction to probabilistic generative grammars provided in Levelt’s excellent and far-sighted introduction to mathematical linguistics (Levelt, 1974), the first volume of which has now been republished separately (Levelt, 2008). The reason for the extraordinarily low profile of probabilistic grammars within the ranks of TG linguists has to do with the very successful attack on the very possibility of their relevance in Syntac16 tic Structures (Chomsky, 1957). Insisting that any statistical model for grammaticality would have to treat Colorless green ideas sleep furiously and Furiously sleep ideas green colorless in exactly the same terms, as they are word strings with the same (pre-1957) frequency of zero, Chomsky argued that probability of a string had no conceivable relevance to its grammaticality. Unfortunately h</context>
</contexts>
<marker>Levelt, 2008</marker>
<rawString>W. J. M. Levelt. 2008. An Introduction to the Theory of Formal Languages and Automata. John Benjamins, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="32322" citStr="Marcus et al., 1993" startWordPosition="5385" endWordPosition="5388">be a prerequisite to any future computational linguistics that relies on details of syntax and semantics (rather than probabilistic number-crunching on n-grams and raw text, which has its own interest but does not involve input from linguistics or even a rudimentary knowledge of the language being processed). Among them are both traditional general linguists like Huddleston and people with serious CL experience like Abeill´e and Huang. But there is more. I have made a preliminary analysis of the inventory of syntactic categories used in the tagging for labelling trees in the 18 Penn Treebank (Marcus et al., 1993), comparing them to the categories used in CGEL. I would describe the fit as not perfect, but within negotiating range. In some ways the fit is remarkable, given the complete independence of the two projects (the Treebank under Mitch Marcus in Philadelphia was largely complete by 1992, when the CGEL project under the direction of Rodney Huddleston in Australia was only just getting up to speed, but Huddleston and Marcus did not know about each other’s work). The biggest discrepancy in categorization is in the problematic area of prepositions, adverbs, and subordinating conjunctions, where the </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emil Post</author>
</authors>
<title>Recursive unsolvability of a problem of thue.</title>
<date>1947</date>
<journal>Journal of Symbolic Logic,</journal>
<pages>12--1</pages>
<marker>Post, 1947</marker>
<rawString>Emil Post. 1947. Recursive unsolvability of a problem of thue. Journal of Symbolic Logic, 12:1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul M Postal</author>
</authors>
<title>Crossover Phenomena.</title>
<date>1971</date>
<location>Holt, Rinehart and Winston, New York.</location>
<contexts>
<context position="21959" citStr="Postal (1971)" startWordPosition="3659" endWordPosition="3660">onally been hostility even to machine data-hunting or language study through computer-searchable corpora. This is fading away as a new generation of young linguists who do everything by searching the web do their data by web search too; but it held back collaboration for a long time. Early proposals for amassing computer corpora were treated with contempt by TG grammarians (‘I’m a native speaker, I have intuitions; why do I need your arbitrary collection of computersearchable text?’). And quite often evidence from attested sentences is simply dismissed. To take a random example, on page 48 of Postal (1971) the sentences Mention of statistics reminds us that stochastic methods have revolutionized CL since the 1980s, but have made few inroads into general linguistics, and none into TG linguistics. This is despite the excellent introduction to probabilistic generative grammars provided in Levelt’s excellent and far-sighted introduction to mathematical linguistics (Levelt, 1974), the first volume of which has now been republished separately (Levelt, 2008). The reason for the extraordinarily low profile of probabilistic grammars within the ranks of TG linguists has to do with the very successful att</context>
</contexts>
<marker>Postal, 1971</marker>
<rawString>Paul M. Postal. 1971. Crossover Phenomena. Holt, Rinehart and Winston, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey K Pullum</author>
<author>Gerald Gazdar</author>
</authors>
<title>Natural languages and context-free languages. Linguistics and Philosophy,</title>
<date>1982</date>
<pages>4--471</pages>
<contexts>
<context position="15046" citStr="Pullum and Gazdar, 1982" startWordPosition="2523" endWordPosition="2526">ed for their Turing-equivalence, and began dismissing precise studies of the generative capacity of grammars as trivial and ridiculous. This, it seems to me, was one more clear sign of distancing from the concerns of CL. It was mainly computational linguists who showed interest in Gazdar’s observation that a theory limited to generating context-free languages could guarantee not just recognition but recognition in polynomial (indeed, better than cubic) time, and in the related observation that none of the arguments for noncontext-free characteristics in human languages seemed to be good ones (Pullum and Gazdar, 1982). The MIT reaction to Gazdar’s suggestion was to mount a major effort to find intractability in Gazdar-style (GPSG) grammars — to represent the recognition problem as NP-hard even for context-free-equivalent theories of grammar (Barton et al., 1987). This was something of a confidence trick. First, the results depended on switching attention from the fixed-grammar arbitrary-string recognition problem (the analog of what Vardi (1982) calls data complexity) to the variable-grammar arbitrary-string recognition problem (what Vardi calls combined complexity). Second, it seemed to be vaguely assumed</context>
</contexts>
<marker>Pullum, Gazdar, 1982</marker>
<rawString>Geoffrey K. Pullum and Gerald Gazdar. 1982. Natural languages and context-free languages. Linguistics and Philosophy, 4:471–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey K Pullum</author>
<author>James Rogers</author>
</authors>
<title>Expressive power of the syntactic theory implicit in the cambridge grammar of the english language.</title>
<date>2008</date>
<booktitle>Paper presented at the annual meeting of the Linguistics Assocition of</booktitle>
<institution>Great Britain, University of Essex,</institution>
<contexts>
<context position="34470" citStr="Pullum and Rogers, 2008" startWordPosition="5743" endWordPosition="5746">sed. There could be considerable value in a complex of reference tools that included a treebank of some 4.5 million words that is fully compatible in its syntactic assumptions with an 1,860-page reference grammar of high reliability and consistency. And there is yet more. Here I will be brief, and things will get slightly technical. The question naturally arises of how one might formalize CGEL to get it in a form where it was explicit enough for use as a database that natural language processing systems could in principle make use of. James Rogers and I have recently considered that question (Pullum and Rogers, 2008) within the context of model-theoretic syntax, a line of work that first began to receive sophisticated formulations here at the EACL in various papers of the early 1990s (e.g. Blackburn et al. (1993), Kracht (1993), Blackburn &amp; Gardent (1995); see Pullum (2007) for a brief historical survey, and Pullum &amp; Scholz (2001) for a deeper treatment of relevant theoretical issues). One thing that might appear to be a stumblingblock to formalizing CGEL, and an obstacle to the relationship with treebanks as well, is that strictly speaking CGEL’s assumed syntactic representations are not (or not all) tre</context>
<context position="36360" citStr="Pullum &amp; Rogers (2008)" startWordPosition="6054" endWordPosition="6057">tive like some, for example, may be both the Determiner of an NP and the Head of the Nominal that is the phrasal head of that NP.) Often (as in HPSG work) the introduction of re-entrancy had dramatic consequences for key properties like decidability of satisfiability for descriptions, or even for model-checking. (I take it that the formal issues around HPSG are very well known to the EACL community. In this short paper I do not try to deal with HPSG at all. There is plenty to be said, but also plenty of excellent HPSG specialists in Europe who are more competent than I am to treat the topic.) Pullum &amp; Rogers (2008) shows, however, that given certain very weak conditions, which seem almost certainly to be satisfied by the kinds of grammatical analysis posited in grammars of the CGEL sort, there is a way of constructing a compatible directed ordered spanning tree for any CGEL-style syntactic structure in such a way that no information is lost and reachability via edge chains is preserved. Moreover, the mapping between CGEL structures and spanning trees is definable in weak monadic second-order logic (wMSO). Put this together with the results of Rogers (1998) on definability of trees in wMSO, and there is </context>
</contexts>
<marker>Pullum, Rogers, 2008</marker>
<rawString>Geoffrey K. Pullum and James Rogers. 2008. Expressive power of the syntactic theory implicit in the cambridge grammar of the english language. Paper presented at the annual meeting of the Linguistics Assocition of Great Britain, University of Essex, September 2008. Online at http://ling.ed.ac. uk/-gpullum/EssexLAGB.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey K Pullum</author>
<author>Barbara C Scholz</author>
</authors>
<title>On the distinction between model-theoretic and generative-enumerative syntactic frameworks.</title>
<date>2001</date>
<booktitle>Logical Aspects of Computational Linguistics: 4th International Conference, number 2099 in Lecture Notes in Artificial Intelligence,</booktitle>
<pages>17--43</pages>
<editor>In Philippe de Groote, Glyn Morrill, and Christian Retor´e, editors,</editor>
<publisher>Springer.</publisher>
<location>Berlin and New York.</location>
<contexts>
<context position="34790" citStr="Pullum &amp; Scholz (2001)" startWordPosition="5796" endWordPosition="5799">y technical. The question naturally arises of how one might formalize CGEL to get it in a form where it was explicit enough for use as a database that natural language processing systems could in principle make use of. James Rogers and I have recently considered that question (Pullum and Rogers, 2008) within the context of model-theoretic syntax, a line of work that first began to receive sophisticated formulations here at the EACL in various papers of the early 1990s (e.g. Blackburn et al. (1993), Kracht (1993), Blackburn &amp; Gardent (1995); see Pullum (2007) for a brief historical survey, and Pullum &amp; Scholz (2001) for a deeper treatment of relevant theoretical issues). One thing that might appear to be a stumblingblock to formalizing CGEL, and an obstacle to the relationship with treebanks as well, is that strictly speaking CGEL’s assumed syntactic representations are not (or not all) trees. They are graphs that depart from being ordinary constituent-structure trees in at least two respects. First, they are annotated not just with categories labelling the nodes, but also with syntactic functions (grammatical relations like Subject-of, Determiner-of, Head-of, Complement-of, etc.) that are perhaps best c</context>
</contexts>
<marker>Pullum, Scholz, 2001</marker>
<rawString>Geoffrey K. Pullum and Barbara C. Scholz. 2001. On the distinction between model-theoretic and generative-enumerative syntactic frameworks. In Philippe de Groote, Glyn Morrill, and Christian Retor´e, editors, Logical Aspects of Computational Linguistics: 4th International Conference, number 2099 in Lecture Notes in Artificial Intelligence, pages 17–43, Berlin and New York. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey K Pullum</author>
</authors>
<date>1989</date>
<booktitle>Formal linguistics meets the Boojum. Natural Language &amp; Linguistic Theory,</booktitle>
<pages>7--137</pages>
<contexts>
<context position="14264" citStr="Pullum, 1989" startWordPosition="2398" endWordPosition="2399">intuitively adequate grammars therefore had to range over the class of grammars generating context-sensitive stringsets. This is a large class of grammars, but at least it is a proper subset of the class of grammars for which the membership recognition problem is decidable. Casting around outside that range was probably not sensible, since natural languages surely had to be decidable (it was taken to be quite obvious that native speakers could rapidly recognize whether or not a string of words was a sentence in their language). As I have detailed elsewhere in somewhat tongue-in-cheek fashion (Pullum, 1989), Chomsky pulled back sharply from his initial interest in mathematical study of linguistic formalisms as it became clear that TG theories were being criticized for their Turing-equivalence, and began dismissing precise studies of the generative capacity of grammars as trivial and ridiculous. This, it seems to me, was one more clear sign of distancing from the concerns of CL. It was mainly computational linguists who showed interest in Gazdar’s observation that a theory limited to generating context-free languages could guarantee not just recognition but recognition in polynomial (indeed, bett</context>
<context position="17340" citStr="Pullum, 1989" startWordPosition="2885" endWordPosition="2886">f selected strings was a plausible idea. Perhaps machine theorem-proving algorithms could have been adapted to showing that a certain grammar could indeed derive a certain string, but in practice early transformational grammar was vastly too complex to permit the building of tools for grammar testing, and later transformational grammar far too vague. I know of only one success story in grammar evaluation by implementing random generation, in fact. Ed Stabler (1992) coded up a Prolog grammaticality-proving system based on the Barriers theory of transformational grammar (Chomsky, 1986a), which (Pullum, 1989) had mocked for sloppiness of statement. The Barriers system had in particular abandoned the usual practice of defining trees in a way that had dominance as a reflexive relation. Chomsky casually asserted that he would take it to be irreflexive. Moreover, Stabler’s careful and sympathetic reconstruction of Chomsky’s intent defines the notion of ‘exclusion’ in such a way that every node excludes itself (Chomsky’s definition said that ‘α excludes Q if [and only if] no segment of α dominates Q’, and of course a given α never dominates itself). And sure enough, the Stabler implementation revealed </context>
</contexts>
<marker>Pullum, 1989</marker>
<rawString>Geoffrey K. Pullum. 1989. Formal linguistics meets the Boojum. Natural Language &amp; Linguistic Theory, 7:137–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey K Pullum</author>
</authors>
<title>The evolution of modeltheoretic frameworks in linguistics.</title>
<date>2007</date>
<booktitle>Model-Theoretic Syntax at 10: ESSLLI2007 Workshop,</booktitle>
<pages>1--10</pages>
<editor>In James Rogers and Stephan Kepser, editors,</editor>
<publisher>Association</publisher>
<location>Trinity College Dublin, Ireland.</location>
<contexts>
<context position="34732" citStr="Pullum (2007)" startWordPosition="5788" endWordPosition="5789">Here I will be brief, and things will get slightly technical. The question naturally arises of how one might formalize CGEL to get it in a form where it was explicit enough for use as a database that natural language processing systems could in principle make use of. James Rogers and I have recently considered that question (Pullum and Rogers, 2008) within the context of model-theoretic syntax, a line of work that first began to receive sophisticated formulations here at the EACL in various papers of the early 1990s (e.g. Blackburn et al. (1993), Kracht (1993), Blackburn &amp; Gardent (1995); see Pullum (2007) for a brief historical survey, and Pullum &amp; Scholz (2001) for a deeper treatment of relevant theoretical issues). One thing that might appear to be a stumblingblock to formalizing CGEL, and an obstacle to the relationship with treebanks as well, is that strictly speaking CGEL’s assumed syntactic representations are not (or not all) trees. They are graphs that depart from being ordinary constituent-structure trees in at least two respects. First, they are annotated not just with categories labelling the nodes, but also with syntactic functions (grammatical relations like Subject-of, Determiner</context>
</contexts>
<marker>Pullum, 2007</marker>
<rawString>Geoffrey K. Pullum. 2007. The evolution of modeltheoretic frameworks in linguistics. In James Rogers and Stephan Kepser, editors, Model-Theoretic Syntax at 10: ESSLLI2007 Workshop, pages 1–10, Trinity College Dublin, Ireland. Association for Logic, Language and Information.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lorenzo Renzi</author>
<author>Giampaolo Salvi</author>
<author>Anna Cardinaletti</author>
</authors>
<title>Grande grammatica italiana di consultazione. Il Mulino,</title>
<date>2001</date>
<volume>3</volume>
<pages>volumes.</pages>
<location>Bologna.</location>
<contexts>
<context position="30366" citStr="Renzi et al., 2001" startWordPosition="5053" endWordPosition="5056">have to be made explicit, they are, but they are then implemented in consistent terms across the entire book. Although more than a dozen linguists were involved, it is not an anthology; Huddleston and Pullum provide a unitary authorial voice for the book and rewrote every part of the book at least once. When disputes about analyses arose between the authors who drafted different chapters, they were settled one way or the other by recourse to evidence, and not permitted to create departures from consistency in the book as a whole. CGEL was preceded by large-scale 3-volume grammars for Italian (Renzi et al., 2001) and for Spanish (Bosque and Demonte, 1999), and now a grammar of French on a similar scale, the Grande Grammaire du franc¸ais is being written by a team of linguists in Paris under the leadership of Anne Abeill´e (Paris 7), Annie Delaveau (Paris 10), and Dani`ele Godard (CNRS). In 2006 I visited Paris at the request of that team to give a workshop on the making of CGEL. Work continues, and the book is now planned for publication by Editions Bayard in 2010. If anything the scope of this work is broader than CGEL’s, since CGEL did not aim to cover uncontroversially non-standard dialects of Engl</context>
</contexts>
<marker>Renzi, Salvi, Cardinaletti, 2001</marker>
<rawString>Lorenzo Renzi, Giampaolo Salvi, and Anna Cardinaletti. 2001. Grande grammatica italiana di consultazione. Il Mulino, Bologna. 3 volumes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Rogers</author>
</authors>
<title>A Descriptive Approach to Language-Theoretic Complexity.</title>
<date>1998</date>
<publisher>CSLI Publications,</publisher>
<location>Stanford, CA.</location>
<contexts>
<context position="15930" citStr="Rogers, 1998" startWordPosition="2657" endWordPosition="2658">f a confidence trick. First, the results depended on switching attention from the fixed-grammar arbitrary-string recognition problem (the analog of what Vardi (1982) calls data complexity) to the variable-grammar arbitrary-string recognition problem (what Vardi calls combined complexity). Second, it seemed to be vaguely assumed that only GPSG had any charges to answer, and that the GB theory of that time (Chomsky, 1981) would not suffer from similar computational complexity problems, but GB eventually turned out to be, insofar as it was well defined, strongly equivalent to Gazdar’s framework (Rogers, 1998). For pre-GB varieties of TG, however, the problem had mainly been not that recognition was NPhard but that it was not computable at all: transformational grammars from 1957 on kept proving to be Turing-equivalent. That was what seems to have driven the denigration of mathematical linguistics, and the downplaying of the relevance of decidability to such an extreme degree (see e.g. Chomsky 1980: 120ff, where the very idea that recognition is decidable is dismissed as an unimportant detail, and not necessarily even a true claim). 4 Hostility to machine testing With many versions of TG offering n</context>
<context position="36912" citStr="Rogers (1998)" startWordPosition="6145" endWordPosition="6146">mpetent than I am to treat the topic.) Pullum &amp; Rogers (2008) shows, however, that given certain very weak conditions, which seem almost certainly to be satisfied by the kinds of grammatical analysis posited in grammars of the CGEL sort, there is a way of constructing a compatible directed ordered spanning tree for any CGEL-style syntactic structure in such a way that no information is lost and reachability via edge chains is preserved. Moreover, the mapping between CGEL structures and spanning trees is definable in weak monadic second-order logic (wMSO). Put this together with the results of Rogers (1998) on definability of trees in wMSO, and there is a clear prospect of the CGEL analysis of En19 glish syntax being reconstructible in terms of the wMSO theory of trees. And what that means for parsing is clear from results of nearly 40 years ago (Doner, 1970): there is a strong equivalence via tree automata to context-free grammars, which means that all the technology of context-free parsing can potentially be brought to bear on processing them. This does not mean it would be a crisis if some language of interest is found to be non-contextfree, incidentally. By the results of Rogers (2003), wMSO</context>
</contexts>
<marker>Rogers, 1998</marker>
<rawString>James Rogers. 1998. A Descriptive Approach to Language-Theoretic Complexity. CSLI Publications, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Rogers</author>
</authors>
<title>wMSO theories as grammar formalisms.</title>
<date>2003</date>
<journal>Theoretical Computer Science,</journal>
<volume>293</volume>
<pages>320</pages>
<contexts>
<context position="37506" citStr="Rogers (2003)" startWordPosition="6251" endWordPosition="6252">ts of Rogers (1998) on definability of trees in wMSO, and there is a clear prospect of the CGEL analysis of En19 glish syntax being reconstructible in terms of the wMSO theory of trees. And what that means for parsing is clear from results of nearly 40 years ago (Doner, 1970): there is a strong equivalence via tree automata to context-free grammars, which means that all the technology of context-free parsing can potentially be brought to bear on processing them. This does not mean it would be a crisis if some language of interest is found to be non-contextfree, incidentally. By the results of Rogers (2003), wMSO theories interpreted on tree-like structures of higher dimensionality than 2 could be employed. For example, where the structures are 3- dimensional (so that individual nodes are allowed to bear the parent-of relation to all of the nodes in entire 2-dimensional trees), the string yield of the set of all structures satisfying a given wMSO sentence is always a tree-adjoining language, and for every tree-adjoining language there is such a characterizing wMSO sentence. Notice, by the way, that the theoretical tools of use here are coming out of currently very active subdisciplines of comput</context>
</contexts>
<marker>Rogers, 2003</marker>
<rawString>James Rogers. 2003. wMSO theories as grammar formalisms. Theoretical Computer Science, 293:291– 320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara C Scholz</author>
<author>Geoffrey K Pullum</author>
</authors>
<title>Irrational nativist exuberance.</title>
<date>2006</date>
<booktitle>Contemporary Debates in Cognitive Science,</booktitle>
<pages>59--80</pages>
<editor>In Robert Stainton, editor,</editor>
<publisher>Basil Blackwell,</publisher>
<location>Oxford.</location>
<contexts>
<context position="26960" citStr="Scholz and Pullum, 2006" startWordPosition="4470" endWordPosition="4473">amination in the way that genuine results in science are. It is as if what is important to the hunter after universal principles is the hunt itself, the call of the horn and the thrill of the chase, but not the grubby business of examining and weighing the kill. The fact is that no really robust and carefully formulated universals of language have been discovered, described, promulgated, confirmed, and widely accepted as correct in the fifty years that universals have been sought. The notion that linguists have discovered innate principles that solve the mystery of first language acquisition (Scholz and Pullum, 2006) is particularly pernicious. The position generally advocated by TG linguists is widely known as linguistic nativism, and it says that some significant aspects of knowledge of language are not derived from any experience but are innately known. But when pressed on the question of what the evidence shows about linguistic nativism, about whether it can really be defended against its plausible rivals, nativists tend to react by drawing back very sharply into a trivial form of the thesis: of course linguistic nativism must be true, they insist, be17 cause when you raise a baby and a kitten in the </context>
</contexts>
<marker>Scholz, Pullum, 2006</marker>
<rawString>Barbara C. Scholz and Geoffrey K. Pullum. 2006. Irrational nativist exuberance. In Robert Stainton, editor, Contemporary Debates in Cognitive Science, pages 59–80. Basil Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pieter A M Seuren</author>
</authors>
<title>Chomsky’s Minimalism.</title>
<date>2004</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford.</location>
<contexts>
<context position="3336" citStr="Seuren (2004)" startWordPosition="525" endWordPosition="526">mar’ is commonly used to denote it, but that will not do. First, ‘generative grammar’ is often used to mean ‘MIT-influenced transformationalgenerative grammar’. For that I will use the abbreviation TG. And second, it is sometimes (incorrectly) claimed that ‘generative’ means nothing more or less than ‘explicit’ (see Chomsky 1966, 12: ‘a generative grammar (that is, an explicit grammar that makes no appeal to the reader’s “facul´e de langage” but rather attempts to incorporate the mechanisms of this faculty)’). We need more precise terminology in order to home in on what I am talking about. As Seuren (2004) has stressed, the relevant vision of what a grammar is like, built into most linguistic theorization today at a level so deep that most linguists are incapable of seeing past it or out of it, is not just that it is explicit, but that a grammar is and must be a syntax-centered random generator. I will therefore refer to language specification by random generation (LSRG). The definitive technical paper defining grammars in LSRG terms is Chomsky (1959). This was a fine paper, which would have earned its Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computationa</context>
</contexts>
<marker>Seuren, 2004</marker>
<rawString>Pieter A. M. Seuren. 2004. Chomsky’s Minimalism. Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward P Stabler</author>
</authors>
<title>Implementing government binding theories.</title>
<date>1992</date>
<booktitle>Formal Grammar: Theory and Implementation,</booktitle>
<pages>243--289</pages>
<editor>In Robert Levine, editor,</editor>
<publisher>Oxford University Press,</publisher>
<location>New York.</location>
<contexts>
<context position="17196" citStr="Stabler (1992)" startWordPosition="2864" endWordPosition="2865">ge even in principle, it was not clear that machine testing of grammatical theories by algorithmic checking of claims made about grammaticality of selected strings was a plausible idea. Perhaps machine theorem-proving algorithms could have been adapted to showing that a certain grammar could indeed derive a certain string, but in practice early transformational grammar was vastly too complex to permit the building of tools for grammar testing, and later transformational grammar far too vague. I know of only one success story in grammar evaluation by implementing random generation, in fact. Ed Stabler (1992) coded up a Prolog grammaticality-proving system based on the Barriers theory of transformational grammar (Chomsky, 1986a), which (Pullum, 1989) had mocked for sloppiness of statement. The Barriers system had in particular abandoned the usual practice of defining trees in a way that had dominance as a reflexive relation. Chomsky casually asserted that he would take it to be irreflexive. Moreover, Stabler’s careful and sympathetic reconstruction of Chomsky’s intent defines the notion of ‘exclusion’ in such a way that every node excludes itself (Chomsky’s definition said that ‘α excludes Q if [a</context>
</contexts>
<marker>Stabler, 1992</marker>
<rawString>Edward P. Stabler, Jr. 1992. Implementing government binding theories. In Robert Levine, editor, Formal Grammar: Theory and Implementation, pages 243– 289. Oxford University Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="10572" citStr="Steedman, 2000" startWordPosition="1766" endWordPosition="1767">actic starting point. In the case of pre-1990 work the starting point was apparently a start symbol; in post-1990 ‘minimalist’ work it is a numeration: a randomly chosen multiset of categorized items from the lexicon. The concept of a ‘numeration’ is a reflection of how firmly embedded the random-generation idea is. The numeration serves no real purpose. It would be possible to formalize a grammar as a set of combinatory principles for putting together words in a string as encountered, from first to last, so that it was in effect a parser. Categorial grammars seem ideally suited to that role (Steedman, 2000), and minimalist grammars are really just a variety of categorial grammar, stripped of some of the formal coherence and links to logic and semantics. Chomsky has often written as if it were a necessary truth that a grammar must be a random generator. For example: ‘Clearly, a grammar must contain ... a ‘syntactic component’ that generates an infinite number of strings representing grammatical sentences ... This is the classical model for grammar’ (Chomsky 1962, 539). This says that a grammar must be a random generator. But this is not true. A grammar could in principle be formulated as, say a t</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Y Vardi</author>
</authors>
<title>The complexity of relational query languages.</title>
<date>1982</date>
<booktitle>In Proceedings of the 14th ACM Symposium on Theory of Computing,</booktitle>
<pages>137--146</pages>
<publisher>Association for Computing Machinery.</publisher>
<location>New York.</location>
<contexts>
<context position="15482" citStr="Vardi (1982)" startWordPosition="2589" endWordPosition="2590">n cubic) time, and in the related observation that none of the arguments for noncontext-free characteristics in human languages seemed to be good ones (Pullum and Gazdar, 1982). The MIT reaction to Gazdar’s suggestion was to mount a major effort to find intractability in Gazdar-style (GPSG) grammars — to represent the recognition problem as NP-hard even for context-free-equivalent theories of grammar (Barton et al., 1987). This was something of a confidence trick. First, the results depended on switching attention from the fixed-grammar arbitrary-string recognition problem (the analog of what Vardi (1982) calls data complexity) to the variable-grammar arbitrary-string recognition problem (what Vardi calls combined complexity). Second, it seemed to be vaguely assumed that only GPSG had any charges to answer, and that the GB theory of that time (Chomsky, 1981) would not suffer from similar computational complexity problems, but GB eventually turned out to be, insofar as it was well defined, strongly equivalent to Gazdar’s framework (Rogers, 1998). For pre-GB varieties of TG, however, the problem had mainly been not that recognition was NPhard but that it was not computable at all: transformation</context>
</contexts>
<marker>Vardi, 1982</marker>
<rawString>Moshe Y. Vardi. 1982. The complexity of relational query languages. In Proceedings of the 14th ACM Symposium on Theory of Computing, pages 137– 146, New York. Association for Computing Machinery.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>