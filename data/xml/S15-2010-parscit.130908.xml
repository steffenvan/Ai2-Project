<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000216">
<title confidence="0.9925635">
RTM-DCU: Predicting Semantic Similarity with Referential Translation
Machines
</title>
<author confidence="0.971634">
Ergun Bic¸ici
</author>
<affiliation confidence="0.978929666666667">
ADAPT CNGL Centre for Global Intelligent Content
School of Computing
Dublin City University, Dublin, Ireland.
</affiliation>
<email confidence="0.996708">
ergun.bicici@computing.dcu.ie
</email>
<sectionHeader confidence="0.996631" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99268125">
We use referential translation machines
(RTMs) for predicting the semantic similarity
of text. RTMs are a computational model
effectively judging monolingual and bilingual
similarity while identifying translation acts
between any two data sets with respect to
interpretants. RTMs pioneer a language
independent approach to all similarity tasks
and remove the need to access any task or
domain specific information or resource.
RTMs become the 2nd system out of 13
systems participating in Paraphrase and
Semantic Similarity in Twitter, 6th out of 16
submissions in Semantic Textual Similarity
Spanish, and 50th out of 73 submissions in
Semantic Textual Similarity English.
</bodyText>
<sectionHeader confidence="0.979714" genericHeader="categories and subject descriptors">
1 Referential Translation Machine (RTM)
</sectionHeader>
<bodyText confidence="0.94901084">
We present positive results from a fully automated
judge for semantic similarity based on Referential
Translation Machines (Bic¸ici and Way, 2014b) in
two semantic similarity tasks at SemEval-2015, Se-
mantic Evaluation Exercises - International Work-
shop on Semantic Evaluation (Nakov et al., 2015).
Referential translation machine (RTM) is a compu-
tational model for identifying the acts of translation
for translating between any given two data sets with
respect to a reference corpus selected in the same
domain. An RTM model is based on the selection
of interpretants, training data close to both the train-
ing set and the test set, which allow shared seman-
tics by providing context for similarity judgments.
Each RTM model is a data translation and transla-
tion prediction model between the instances in the
training set and the test set and translation acts are
indicators of the data transformation and translation.
RTMs present an accurate and language independent
solution for making semantic similarity judgments.
RTMs pioneer a computational model for qual-
ity and semantic similarity judgments in monolin-
gual and bilingual settings using retrieval of relevant
training data (Bic¸ici and Yuret, 2015) as interpre-
tants for reaching shared semantics. RTMs achieve
(i) top performance when predicting the quality of
translations (Bic¸ici, 2013; Bic¸ici and Way, 2014a);
(ii) top performance when predicting monolingual
cross-level semantic similarity; (iii) second perfor-
mance when predicting paraphrase and semantic
similarity in Twitter (iv) good performance when
judging the semantic similarity of sentences; (iv)
good performance when evaluating the semantic re-
latedness of sentences and their entailment (Bic¸ici
and Way, 2014b).
RTMs use Machine Translation Performance Pre-
diction (MTPP) System (Bic¸ici et al., 2013; Bic¸ici
and Way, 2014b), which is a state-of-the-art (SoA)
performance predictor of translation even without
using the translation. MTPP system measures the
coverage of individual test sentence features found
in the training set and derives indicators of the close-
ness of test sentences to the available training data,
the difficulty of translating the sentence, and the
presence of acts of translation for data transforma-
tion. MTPP features for translation acts are provided
in (Bic¸ici and Way, 2014b). RTMs become the 2nd
system out of 13 systems participating in Paraphrase
and Semantic Similarity in Twitter (Task 1) (Xu et
al., 2015) and achieve good results in Semantic Tex-
</bodyText>
<page confidence="0.986031">
56
</page>
<note confidence="0.7501965">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 56–63,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<figureCaption confidence="0.526618142857143">
Figure 1: RTM depiction.
Algorithm 1: Referential Translation Machine
Input: Training set train, test set test,
corpus C, and learning model M.
Data: Features of train and test, Ttrain
and Ttest.
Output: Predictions of similarity scores on the
</figureCaption>
<equation confidence="0.941161666666667">
test ˆq.
1 FDA5(train, test, C) — Z
2 MTPPSystem(Z, train) — Ttrain
3 MTPPSystem(Z, test) — Ttest
4 learn(M, Ttrain) — M
5 predict(M,Ttest) — yˆ
</equation>
<bodyText confidence="0.9972211875">
tual Similarity (Task 2) (Agirre et al., 2015) becom-
ing 6th out of 16 submissions in Spanish.
We use the Parallel FDA5 instance selection
model for selecting the interpretants (Bic¸ici et al.,
2014; Bic¸ici and Yuret, 2015), which allows efficient
parameterization, optimization, and implementation
of Feature Decay Algorithms (FDA), and build an
MTPP model. We view that acts of translation are
ubiquitously used during communication:
Every act of communication is an act of
translation (Bliss, 2012).
Translation need not be between different languages
and paraphrasing or communication also contain
acts of translation. When creating sentences, we use
our background knowledge and translate informa-
tion content according to the current context.
</bodyText>
<figureCaption confidence="0.687722">
Figure 1 depicts RTM and Algorithm 1 describes
</figureCaption>
<table confidence="0.998292833333334">
Task Setting Train LM
Task 1, ParSS English 313 7813
Task 2, STS English 441 6441
Task 2, STS English headlines 531 8031
Task 2, STS English images 411 6411
Task 2, STS Spanish 409 6409
</table>
<tableCaption confidence="0.997009">
Table 1: Number of sentences in Z (in thousands) se-
lected for each task.
</tableCaption>
<bodyText confidence="0.9952885">
the RTM algorithm. Our encouraging results in the
semantic similarity tasks increase our understanding
of the acts of translation we ubiquitously use when
communicating and how they can be used to pre-
dict semantic similarity. RTMs are powerful enough
to be applicable in different domains and tasks with
good performance. We describe the tasks we partic-
ipated as follows:
ParSS Paraphrase and Semantic Similarity in
Twitter (ParSS) (Xu et al., 2015):
Given two sentences S1 and S2 in the
same language, produce a similarity score
indicating whether they express a similar
meaning: a discrete real number in [0, 1].
We model as sentence MTPP between S1 to S2.
STS Semantic Textual Similarity (STS) (Agirre
et al., 2015):
Given two sentences S1 and S2 in the same
language, quantify the degree of similar-
ity: a real number in [0, 5].
STS is in English and Spanish (a real number in
[0, 4]). We model as sentence MTPP of S1 and S2.
</bodyText>
<sectionHeader confidence="0.968175" genericHeader="general terms">
2 SemEval-15 Results
</sectionHeader>
<bodyText confidence="0.9999652">
We develop individual RTM models for each task
and subtask that we participate at SemEval-2015
with the RTM-DCU team name. Interpretants are
selected from the LM corpora distributed by the
translation task of WMT14 (Bojar et al., 2014) and
LDC for English (Parker et al., 2011) and Span-
ish ( ˆAngelo Mendonc¸a et al., 2011) 1. We use the
Stanford POS tagger (Toutanova et al., 2003) to ob-
tain the lemmatized corpora for the ParSS task. The
number of instances we select for the interpretants
</bodyText>
<footnote confidence="0.943539">
1English Gigaword 5th, Spanish Gigaword 3rd edition.
</footnote>
<page confidence="0.995276">
57
</page>
<table confidence="0.9830888">
RTM-DCU results
Data Model Fl Precision Recall maxFl mPrecision mRecall rP MAE RAE MAER MRAER Rank
R SVR 0.54 0.883 0.389 0.693 0.695 0.691 0.5697 0.1953 0.7918 0.4278 0.8694 3
R PLS-SVR 0.562 0.859 0.417 0.678 0.649 0.709 0.564 0.2001 0.8109 0.4442 0.9105 4
RTM results with further optimization
Data Model Fl Precision Recall maxFl mPrecision mRecall rP MAE RAE MAER MRAER
R PLS-SVR 0.502 0.938 0.343 0.674 0.686 0.663 0.5798 0.1912 0.775 0.6901 0.838
R RR 0.521 0.94 0.36 0.681 0.735 0.634 0.5777 0.1866 0.7564 0.7438 0.7944
R+L SVR 0.53 0.892 0.377 0.669 0.652 0.686 0.5719 0.1944 0.7879 0.6788 0.8615
R+L PLS-SVR 0.5 0.884 0.349 0.642 0.649 0.634 0.5245 0.2028 0.8218 0.7425 0.8864
</table>
<tableCaption confidence="0.999703">
Table 2: ParSS test results.
</tableCaption>
<bodyText confidence="0.995575260869565">
in each task is given in Table 1.
We use ridge regression (RR), support vector re-
gression (SVR), and extremely randomized trees
(TREE) (Geurts et al., 2006) as the learning mod-
els. These models learn a regression function using
the features to estimate a numerical target value. We
also use them after a dimensionality reduction and
mapping step with partial least squares (PLS) (Spe-
cia et al., 2009). We optimize the learning parame-
ters, the number of dimensions used for PLS, and the
parameters for parallel FDA5. More details about
the optimization processes are in (Bic¸ici and Way,
2014b; Bic¸ici et al., 2014). We optimize the learning
parameters by selecting e close to the standard devi-
ation of the noise in the training set (Bic¸ici, 2013)
since the optimal value for e is shown to have lin-
ear dependence to the noise level for different noise
models (Smola et al., 1998). At testing time, the
predictions are bounded to obtain scores in the cor-
responding ranges.
We use Pearson’s correlation (rP), mean absolute
error (MAE), and relative absolute error (RAE) for
evaluation:
</bodyText>
<equation confidence="0.9522265">
 |ˆyi − yi|
MAE(ˆy, y) =
|¯y − yi|
(1)
</equation>
<bodyText confidence="0.999951">
We define MAER and MRAER for easier replica-
tion and comparability with relative errors for each
</bodyText>
<equation confidence="0.957080333333333">
instance:
MAER(ˆy, y) =
(2)
n |ˆyi − yi|
i=1 L|¯y − yi|JE
n
</equation>
<bodyText confidence="0.990697518518519">
MAER is the mean absolute error relative to the
magnitude of the target and MRAER is the mean
absolute error relative to the absolute error of a pre-
dictor always predicting the target mean assuming
that target mean is known. MAER and MRAER are
capped from below2 with c = MAE(ˆy, y)/2, which
is the measurement error and it is estimated as half
of the mean absolute error or deviation of the pre-
dictions from target mean. c represents half of the
score step with which a decision about a change
in measurement’s value can be made. c is simi-
lar to half of the standard deviation, Q, of the data
but over absolute differences. For discrete target
step size
scores, c = 2 . A method for learning deci-
sion thresholds for mimicking the human decision
process when determining whether two translations
are equivalent is described in (Bic¸ici, 2013).
MAER and MRAER are able to capture averaged
fluctuations at the instance level and they may eval-
uate the performance of a predictor at performance
prediction tasks at the instance level (e.g. perfor-
mance of the similarity of sentences, performance
of translation of different translation instances) bet-
ter. RAE compares sums of prediction errors and
MRAER averages instance prediction error compar-
isons.
</bodyText>
<equation confidence="0.980585">
2We use L . J, to cap the argument from below to c.
n
 |ˆyi − yi|
i=1
n RAE(ˆy, y) =
n
i=1
�n
i=1
n |ˆyi − yi|
i=1 L|yi|Jc
n
MRAER(ˆy, y) =
</equation>
<page confidence="0.987195">
58
</page>
<table confidence="0.990792875">
RTM-DCU rP results
Model answers-forums answers-students belief headlines images Weighted rP Rank
PLS-TREE 0.5484 0.5549 0.6223 0.7281 0.7189 0.6468 50
RTM top result rP selected according to Weighted rP among top 3 results with further optimization
Model answers-forums answers-students belief headlines images Weighted rP
TREE 0.5517 0.6729 0.6750 0.7812 0.7830 0.7126
Rank
48 38 39 29 49 38
</table>
<tableCaption confidence="0.993382">
Table 4: STS English test rp results for each domain.
</tableCaption>
<table confidence="0.999822631578947">
Data Model Fl rp MAE RAE MAER MRAER Model RTM-DCU rP results
Wikipedia News Weighted rP Rank
R PLS-SVR .4740 .6183 .2106 .6963 1.5408 .9223
R RR .4920 .6165 .2174 .7188 1.8609 .9132
R PLS-TREE .5330 .6156 .2201 .7276 1.939 .9144
R SVR .4800 .6152 .2107 .6965 1.5012 .9306
R PLS RR .5110 .6140 .2170 .7175 1.8443..9240
TREE 0.5823 0.5251 0.5443 6
RTM top result rP selected according to Weighted
rP among top 3 results with further optimization
Model Wikipedia News Weighted rP Rank
R+L SVR .5040 .6216 .2085 .6893 1.4723 .9344
R+L PLS-SVR .4970 .6209 .2093 .6919 1.5402 .9226
R+L PLS-TREE .5410 .6205 .2177 .7196 1.8834 .9161
R+L RR .4970 .6194 .2164 .7154 1.8448 .9096
R PLS-SVR .4740 .6183 .2106 .6963 1.5408 .9223
TREE 0.6622 0.5833 0.6096 5
Rank
4 5
</table>
<tableCaption confidence="0.9908305">
Table 3: ParSS training results of top 5 RTM systems
with further optimization.
</tableCaption>
<subsectionHeader confidence="0.8709325">
2.1 Task 1: Paraphrase and Semantic
Similarity in Twitter (ParSS)
</subsectionHeader>
<bodyText confidence="0.996324684210526">
ParSS contains sentences provided by Twitter 3 (Xu
et al., 2015). Official evaluation metric is Pearson’s
correlation score, which we use to select the top
systems on the training set. RTM-DCU results on
the ParSS test set are given in Table 2. The set-
ting R using SVR becomes 2nd out of 13 systems
and 3rd out of 25 submissions. Looking at MAE
and MAER allows us to obtain explanations to train
and test performance differences for example with-
out knowing their target distribution. Even though
MAE of PLS-SVR is about %5 smaller on the ParSS
test set, MAER is %55 smaller due to test set con-
taining fewer zero entries (%16 vs. %39 on the train
set). Lower test MAE than training MAE may be
attributed to RTMs.
We obtained results with lemmatized datasets and
further optimized the learning model parameters af-
ter the challenge. We present the performance of the
top 5 individual RTM models on the training set in
</bodyText>
<tableCaption confidence="0.612626">
Table 3. R uses the regular truecase (Koehn et al.,
</tableCaption>
<footnote confidence="0.813229">
3www.twitter.com
</footnote>
<tableCaption confidence="0.985943">
Table 5: STS Spanish test results.
</tableCaption>
<bodyText confidence="0.741481">
2007; Koehn, 2010) corpora and L uses the lemma-
tized truecased corpora. R+L correspond to using
the features from both R and L, which doubles the
number of features.
</bodyText>
<subsectionHeader confidence="0.987604">
2.2 Task 2: Semantic Textual Similarity (STS)
</subsectionHeader>
<bodyText confidence="0.993634736842105">
STS contains sentence pairs from different do-
mains: answers-forums, answers-students, belief,
headlines, and images for English and wikipedia
and newswire for Spanish. Official evaluation met-
ric in STS is the Pearson’s correlation score. We
build separate RTM models for headlines and im-
ages domains for STS English. Domain specific
RTM models obtain improved performance in those
domains (Bic¸ici and Way, 2014b). STS English test
set contains 2000, 1500, 2000, 1500, and 1500 sen-
tences respectively from the specified domains how-
ever for evaluation, STS use a subset of the test set,
375, 750, 375, 750, and 750 instances respectively
from the corresponding domains. This may lower
the performance of RTMs by causing FDA5 to se-
lect more domain specific data and less task specific
since RTMs use the test set to select interpretants
and build a task specific RTM prediction model.
Table 4 and Table 5 list the results on the test set
</bodyText>
<page confidence="0.997706">
59
</page>
<bodyText confidence="0.9997322">
along with their ranks out of 73 and 16 submissions
respectively for English STS and Spanish STS.
RTM top test results selected according to
Weighted rp among top 3 results on STS for each
subtask as well as top RTM-DCU results in STS
2014 (Bic¸ici and Way, 2014b) are presented in Ta-
ble 6, where we have used the top results from do-
main specific RTM models for headlines and images
domains in the overall model results. Top 3 individ-
ual RTM model performance on the training set with
further optimized learning model parameters after
the challenge are presented in Table 7. Better rp,
RAE, and MRAER on the test set than on the train-
ing set in STS 2015 English may be attributed to
RTMs.
</bodyText>
<subsectionHeader confidence="0.990366">
2.3 RTMs Across Tasks and Years
</subsectionHeader>
<bodyText confidence="0.999059552631579">
We compare the difficulty of tasks according to
MRAER where the correlation of RAE and MRAER
is 0.89. In Table 8, we list the RAE, MAER,
and MRAER obtained for different tasks and sub-
tasks, also listing RTM results from SemEval-
2013 (Bic¸ici and van Genabith, 2013), from
SemEval-2014 (Bic¸ici and Way, 2014b), and and
from quality estimation task (QET) (Bic¸ici and Way,
2014a) of machine translation (Bojar et al., 2014).
RTMs at SemEval-2013 contain results from STS.
RTMs at SemEval-2014 contain results from STS,
semantic relatedness and entailment (SRE) (Marelli
et al., 2014), and cross-level semantic similarity
(CLSS) tasks (Jurgens et al., 2014). RTMs at
WMT2014 QET contain tasks involving the predic-
tion of an integer in [1, 3] representing post-editing
effort (PEE), a real number in [0, 1] represent-
ing human-targeted translation edit rate (HTER), or
an integer representing post-editing time (PET) of
translations.
The best results are obtained for the CLSS para-
graph to sentence subtask, which may be due to the
larger contextual information that paragraphs can
provide for the RTM models. For the ParSS task,
we can only reduce the error with respect to know-
ing and predicting the mean by about 22.5%. Predic-
tion of bilingual similarity as in quality estimation of
translation can be expected to be harder and RTMs
achieve SoA performance in this task as well (Bic¸ici
and Way, 2014a). Table 8 can be used to evaluate
the difficulty of various tasks and domains based on
our SoA predictor RTM. MRAER considers both the
predictor’s error and the target scores’ fluctuations
at the instance level. We separated the results hav-
ing MRAER greater than 1 as in these tasks and sub-
tasks RTM does not perform significantly better than
mean predictor and fluctuations render these as tasks
that may require more work.
</bodyText>
<sectionHeader confidence="0.997851" genericHeader="conclusions">
3 Conclusion
</sectionHeader>
<bodyText confidence="0.999918631578947">
Referential translation machines pioneer a clean
and intuitive computational model for automatically
measuring semantic similarity by measuring the acts
of translation involved and achieve to become the
2nd system out of 13 systems participating in Para-
phrase and Semantic Similarity in Twitter, 6th out
of 16 submissions in Semantic Textual Similarity
Spanish, and 50th out of 73 submissions in Semantic
Textual Similarity English. RTMs make quality and
semantic similarity judgments possible based on the
retrieval of relevant training data as interpretants for
reaching shared semantics. We define MAER, mean
absolute error relative to the magnitude of the target,
and MRAER, mean absolute error relative to the ab-
solute error of a predictor always predicting the tar-
get mean assuming that target mean is known. RTM
test performance on various tasks sorted according
to MRAER can identify which tasks and subtasks
may require more work.
</bodyText>
<sectionHeader confidence="0.985049" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999870363636364">
This work is supported in part by SFI
(13/TIDA/I2740) for the project “Monolingual and
Bilingual Text Quality Judgments with Translation
Performance Prediction” (www.computing.dcu.
ie/˜ebicici/Projects/TIDA_RTM.html)
and in part by SFI (12/CE/I2267) as part of the
ADAPT CNGL Centre for Global Intelligent
Content (www.adaptcentre.ie) at Dublin City
University. We also thank the SFI/HEA Irish Centre
for High-End Computing (ICHEC) for the provision
of computational facilities and support.
</bodyText>
<sectionHeader confidence="0.96667" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.815732333333333">
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo,
Rada Mihalcea, German Rigau, and Janyce Wiebe.
</reference>
<page confidence="0.992965">
60
</page>
<table confidence="0.99993812">
Domain Model rp MAE RAE MAER MRAER
answers-forums PLS-SVR 0.6215 1.2239 1.1675 1.5369 1.3449
answers-students PLS-SVR 0.6125 0.9635 0.7819 0.5542 0.8404
English belief PLS-SVR 0.5879 1.3625 1.1825 1.5749 1.4119
headlines RR 0.7812 0.8318 0.5894 0.4844 0.6380
images TREE 0.7830 0.8502 0.5885 0.5424 0.6229
ALL PLS-SVR 0.6739 0.9847 0.7224 0.7379 0.7883
News TREE 0.5303 0.6315 0.9426 0.4096 1.1052
Spanish Wikipedia TREE 0.5867 0.6448 0.9499 0.4844 1.2062
ALL TREE 0.5618 0.6360 0.9459 0.4348 1.1344
deft-forum TREE 0.4341 1.1609 1.0908 0.7724 1.216
deft-news TREE 0.6974 0.9032 0.8716 0.6271 0.881
headlines TREE 0.6199 0.9254 0.7845 0.6711 0.7854
English images TREE 0.6995 0.9499 0.7395 0.8338 0.7246
OnWN TREE 0.8058 1.0028 0.5585 0.7975 0.546
tweet-news TREE 0.6882 0.831 0.8093 0.4601 0.875
ALL TREE 0.6473 0.9534 0.7449 0.7274 0.7566
News TREE 0.7 1.351 1.4141 0.5994 1.8053
Spanish Wikipedia TREE 0.4216 1.298 1.3579 0.65 1.6612
ALL TREE 0.62 1.3296 1.3823 0.6191 1.7719
headlines 0.6552 1.2763 1.0231 1.0456 1.1444
English OnWN L+S SVR 0.6943 1.3545 0.8255 1.2875 0.8605
SMT 0.3005 0.6886 1.6132 0.1669 2.0718
FNWN 0.2016 1.0604 1.2633 1.5087 1.4048
ALL L+S SVR 0.5844 1.0818 0.7791 0.8494 0.77
</table>
<tableCaption confidence="0.92201">
Table 6: RTM top test results selected according to Weighted rp among top 3 results on STS as well as top RTM-DCU
results in STS 2013 and STS 2014 (Bic¸ici and Way, 2014b). ALL presents results over all of the test set.
</tableCaption>
<table confidence="0.999640538461539">
Model rp MAE RAE MAER MRAER
PLS-SVR 0.7477 0.7679 0.6050 0.4444 0.6947
SVR 0.7452 0.7688 0.6058 0.4504 0.686
TREE 0.7265 0.8093 0.6377 0.504 0.6812
RR 0.7453 0.7559 0.6215 0.4389 0.6835
PLS-SVR 0.7411 0.7619 0.6265 0.4298 0.7087
TREE 0.7386 0.7710 0.6340 0.4726 0.6686
TREE 0.7600 0.8020 0.6248 0.5308 0.7013
PLS-SVR 0.7574 0.7839 0.6106 0.4898 0.724
RR 0.7564 0.7945 0.6189 0.5025 0.7161
TREE 0.8390 0.5154 0.5115 0.4145 0.5931
RR 0.8260 0.5473 0.5431 0.4571 0.6208
PLS-SVR 0.8218 0.5363 0.5322 0.4171 0.635
</table>
<tableCaption confidence="0.999209">
Table 7: RTM training results of top 3 systems on STS English, English images, English headlines, and Spanish tasks.
</tableCaption>
<reference confidence="0.362797666666667">
2015. SemEval-2015 Task 2: Semantic textual sim- orado, USA, June.
ilarity. In Proc. of the 9th International Workshop ˆAngelo Mendonc¸a, Daniel Jaquette, David Graff, and
on Semantic Evaluation (SemEval 2015), Denver, Col- Denise DiPersio. 2011. Spanish Gigaword third edi-
</reference>
<figure confidence="0.984978">
STS 2015
STS 2014
STS 2013
Lang
English
headlines
images
Spanish
</figure>
<reference confidence="0.997848147058824">
tion, Linguistic Data Consortium.
Ergun Bic¸ici and Josef van Genabith. 2013. CNGL-
CORE: Referential translation machines for measuring
semantic similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics,
pages 234–240, Atlanta, Georgia, USA, 13-14 June.
Ergun Bic¸ici and Andy Way. 2014a. Referential trans-
lation machines for predicting translation quality. In
Proc. of the Ninth Workshop on Statistical Machine
Translation, pages 313–321, Baltimore, Maryland,
USA, June.
Ergun Bic¸ici and Andy Way. 2014b. RTM-DCU: Ref-
erential translation machines for semantic similarity.
In Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval-2014), pages 487–496,
Dublin, Ireland, 23-24 August.
Ergun Bic¸ici and Deniz Yuret. 2015. Optimizing in-
stance selection for statistical machine translation with
feature decay algorithms. IEEE/ACM Transactions
On Audio, Speech, and Language Processing (TASLP),
23:339–350.
Ergun Bic¸ici, Declan Groves, and Josef van Genabith.
2013. Predicting sentence translation quality using ex-
trinsic and language independent features. Machine
Translation, 27:171–192, December.
Ergun Bic¸ici, Qun Liu, and Andy Way. 2014. Parallel
FDA5 for fast deployment of accurate statistical ma-
chine translation systems. In Proc. of the Ninth Work-
shop on Statistical Machine Translation, pages 59–65,
Baltimore, USA, June.
Ergun Bic¸ici. 2013. Referential translation machines for
quality estimation. In Proc. of the Eighth Workshop on
Statistical Machine Translation, pages 343–351, Sofia,
Bulgaria, August.
Chris Bliss. 2012. Comedy is transla-
tion, February. http://www.ted.com/talks/
chris bliss comedy is translation.html.
Ondrej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Johannes Leveling,
Christof Monz, Pavel Pecina, Matt Post, Herve Saint-
Amand, Radu Soricut, Lucia Specia, and Aleˇs Tam-
chyna. 2014. Findings of the 2014 workshop on statis-
tical machine translation. In Proceedings of the Ninth
Workshop on Statistical Machine Translation, pages
12–58, Baltimore, Maryland, USA, June.
Pierre Geurts, Damien Ernst, and Louis Wehenkel. 2006.
Extremely randomized trees. Machine Learning,
63(1):3–42.
David Jurgens, Mohammad Taher Pilehvar, and Roberto
Navigli. 2014. SemEval-2014 Task 3: Cross-level
semantic similarity. In Proc. of the 8th International
Workshop on Semantic Evaluation (SemEval 2014),
pages 17–26, Dublin, Ireland, August.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
pages 177–180, Prague, Czech Republic, June.
Philipp Koehn. 2010. An experimental management sys-
tem. The Prague Bulletin ofMathematical Linguistics,
94:87–96.
Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zam-
parelli. 2014. SemEval-2014 Task 1: Evaluation of
compositional distributional semantic models on full
sentences through semantic relatedness and textual en-
tailment. In Proc. of the 8th International Workshop
on Semantic Evaluation (SemEval 2014), pages 1–8,
Dublin, Ireland, August.
Preslav Nakov, Torsten Zesch, Daniel Cer, and David Ju-
rgens, editors. 2015. Proc. of the 9th International
Workshop on Semantic Evaluation (SemEval 2015).
Denver, Colorado, USA, 4-5 June.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English Gigaword fifth edi-
tion, Linguistic Data Consortium.
A. J. Smola, N. Murata, B. Sch¨olkopf, and K.-R. M¨uller.
1998. Asymptotically optimal choice of ε-loss for sup-
port vector machines. In L. Niklasson, M. Boden,
and T. Ziemke, editors, Proc. of the International Con-
ference on Artificial Neural Networks, Perspectives in
Neural Computing, pages 105–110, Berlin.
Lucia Specia, Nicola Cancedda, Marc Dymetman, Marco
Turchi, and Nello Cristianini. 2009. Estimating the
sentence-level quality of machine translation systems.
In Proc. of the 13th Annual Conference of the Eu-
ropean Association for Machine Translation (EAMT),
pages 28–35, Barcelona, Spain, May.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proc. of
the 2003 Conference of the North American Chapter of
the Association for Computational Linguistics on Hu-
man Language Technology - Volume 1, NAACL ’03,
pages 173–180, Stroudsburg, PA, USA.
Wei Xu, Chris Callison-Burch, and William B. Dolan.
2015. SemEval-2015 Task 1: Paraphrase and semantic
similarity in Twitter (PIT). In Proc. of the 9th Interna-
tional Workshop on Semantic Evaluation (SemEval),
Denver, Colorado, USA, June.
</reference>
<page confidence="0.99832">
62
</page>
<table confidence="0.999447516129032">
Task Subtask Domain Model RAE MAER MRAER
CLSS 2014 Paragraph to Sentence Mixed TREE 0.4579 0.5112 0.5037
STS 2014 English OnWN TREE 0.5585 0.7975 0.546
QET 2014 English-Spanish PEE Europarl PLS-TREE 1.0794 0.304 0.614
STS 2015 English Images TREE 0.5885 0.5424 0.6229
STS 2015 English Headlines RR 0.5894 0.4844 0.6380
CLSS 2014 Sentence to Phrase Mixed TREE 0.6255 0.6857 0.6444
QET 2014 German-English PEE Europarl RR 0.8204 0.3575 0.679
QET 2014 English-German PEE Europarl TREE 0.8602 0.3692 0.6985
STS 2014 English Images TREE 0.7395 0.8338 0.7246
QET 2014 Spanish-English PEE Europarl FS-RR 0.9 0.3798 0.7491
QET 2014 English-Spanish PET Europarl SVR 0.7223 0.4651 0.7786
STS 2014 English Headlines TREE 0.7845 0.6711 0.7854
SRE 2014 English SICK R+L PLS-SVR 0.6645 0.1827 0.8177
ParSS 2015 English Tweets SVR 0.775 0.6901 0.838
STS 2015 English Answers-students PLS-SVR 0.7819 0.5542 0.8404
CLSS 2014 Phrase to Word Mixed TREE 0.9488 1.1454 0.8483
STS 2013 English OnWN L+S SVR 0.8255 1.2875 0.8605
STS 2014 English Tweet-news TREE 0.8093 0.4601 0.875
QET 2014 English-Spanish HTER Europarl SVR 0.8532 0.7727 0.8758
STS 2014 English Deft-news TREE 0.8716 0.6271 0.881
STS 2015 Spanish News TREE 0.9426 0.4096 1.1052
STS 2013 English Headlines L+S SVR 1.0231 1.0456 1.1444
STS 2015 Spanish Wikipedia TREE 0.9499 0.4844 1.2062
STS 2014 English Deft-forum TREE 1.0908 0.7724 1.216
STS 2015 English Answers-forums PLS-SVR 1.1675 1.5369 1.3449
STS 2013 English FNWN L+S SVR 1.2633 1.5087 1.4048
STS 2015 English Belief PLS-SVR 1.1825 1.5749 1.4119
STS 2014 Spanish Wikipedia TREE 1.3579 0.65 1.6612
STS 2014 Spanish News TREE 1.4141 0.5994 1.8053
STS 2013 English SMT L+S SVR 1.6132 0.1669 2.0718
</table>
<tableCaption confidence="0.997904">
Table 8: Best RTM test results for different tasks and subtasks sorted according to MRAER.
</tableCaption>
<page confidence="0.999061">
63
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.969734">RTM-DCU: Predicting Semantic Similarity with Referential Translation Machines</title>
<author confidence="0.846731">ADAPT CNGL Centre for Global Intelligent</author>
<affiliation confidence="0.992266">School of Dublin City University, Dublin,</affiliation>
<email confidence="0.993093">ergun.bicici@computing.dcu.ie</email>
<abstract confidence="0.921762666666666">We use referential translation machines (RTMs) for predicting the semantic similarity of text. RTMs are a computational model effectively judging monolingual and bilingual similarity while identifying translation acts between any two data sets with respect to interpretants. RTMs pioneer a language independent approach to all similarity tasks and remove the need to access any task or domain specific information or resource. RTMs become the 2nd system out of 13 systems participating in Paraphrase and Semantic Similarity in Twitter, 6th out of 16 submissions in Semantic Textual Similarity Spanish, and 50th out of 73 submissions in Semantic Textual Similarity English. 1 Referential Translation Machine (RTM) We present positive results from a fully automated judge for semantic similarity based on Referential Machines and Way, 2014b) in two semantic similarity tasks at SemEval-2015, Semantic Evaluation Exercises - International Workshop on Semantic Evaluation (Nakov et al., 2015). Referential translation machine (RTM) is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain. An RTM model is based on the selection of interpretants, training data close to both the training set and the test set, which allow shared semantics by providing context for similarity judgments. Each RTM model is a data translation and translation prediction model between the instances in the training set and the test set and translation acts are indicators of the data transformation and translation. RTMs present an accurate and language independent solution for making semantic similarity judgments. RTMs pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant data and Yuret, 2015) as interpretants for reaching shared semantics. RTMs achieve (i) top performance when predicting the quality of 2013; and Way, 2014a); (ii) top performance when predicting monolingual cross-level semantic similarity; (iii) second performance when predicting paraphrase and semantic similarity in Twitter (iv) good performance when judging the semantic similarity of sentences; (iv) good performance when evaluating the semantic reof sentences and their entailment and Way, 2014b). RTMs use Machine Translation Performance Pre- (MTPP) System et al., 2013; and Way, 2014b), which is a state-of-the-art (SoA) performance predictor of translation even without using the translation. MTPP system measures the coverage of individual test sentence features found in the training set and derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation. MTPP features for translation acts are provided and Way, 2014b). RTMs become the 2nd system out of 13 systems participating in Paraphrase and Semantic Similarity in Twitter (Task 1) (Xu et 2015) and achieve good results in Semantic Tex- 56 of the 9th International Workshop on Semantic Evaluation (SemEval pages 56–63, Colorado, June 4-5, 2015. Association for Computational Linguistics Figure 1: RTM depiction. 1: Translation Machine Training set test set and learning model Features of Predictions of similarity scores on the Z M tual Similarity (Task 2) (Agirre et al., 2015) becoming 6th out of 16 submissions in Spanish. We use the Parallel FDA5 instance selection for selecting the interpretants et al., and Yuret, 2015), which allows efficient parameterization, optimization, and implementation of Feature Decay Algorithms (FDA), and build an MTPP model. We view that acts of translation are ubiquitously used during communication: Every act of communication is an act 2012). Translation need not be between different languages and paraphrasing or communication also contain acts of translation. When creating sentences, we use our background knowledge and translate information content according to the current context. Figure 1 depicts RTM and Algorithm 1 describes Task Setting Train LM Task 1, ParSS English 313 7813 Task 2, STS English 441 6441 Task 2, STS English headlines 531 8031 Task 2, STS English images 411 6411 Task 2, STS Spanish 409 6409 1: Number of sentences in thousands) selected for each task. the RTM algorithm. Our encouraging results in the semantic similarity tasks increase our understanding of the acts of translation we ubiquitously use when communicating and how they can be used to predict semantic similarity. RTMs are powerful enough to be applicable in different domains and tasks with good performance. We describe the tasks we participated as follows: ParSS Paraphrase and Semantic Similarity in Twitter (ParSS) (Xu et al., 2015): two sentences the same language, produce a similarity score indicating whether they express a similar a discrete real number in 1]. model as sentence MTPP between STS Semantic Textual Similarity (STS) (Agirre et al., 2015): two sentences the same language, quantify the degree of similara real number in 5]. STS is in English and Spanish (a real number in We model as sentence MTPP of 2 SemEval-15 Results We develop individual RTM models for each task and subtask that we participate at SemEval-2015 with the RTM-DCU team name. Interpretants are selected from the LM corpora distributed by the translation task of WMT14 (Bojar et al., 2014) and LDC for English (Parker et al., 2011) and Span- ( et al., 2011) We use the Stanford POS tagger (Toutanova et al., 2003) to obtain the lemmatized corpora for the ParSS task. The number of instances we select for the interpretants Gigaword 5th, Spanish Gigaword 3rd edition. 57 RTM-DCU results Data Model Recall mPrecision mRecall MAE RAE MAER MRAER Rank R SVR 0.54 0.883 0.389 0.693 0.695 0.691 0.5697 0.1953 0.7918 0.4278 0.8694 3 R PLS-SVR 0.562 0.859 0.417 0.678 0.649 0.709 0.564 0.2001 0.8109 0.4442 0.9105 4 RTM results with further optimization Model Precision Recall mPrecision mRecall RAE MAER MRAER R PLS-SVR 0.502 0.938 0.343 0.674 0.686 0.663 0.5798 0.1912 0.775 0.6901 0.838 R RR 0.521 0.94 0.36 0.681 0.735 0.634 0.5777 0.1866 0.7564 0.7438 0.7944 R+L SVR 0.53 0.892 0.377 0.669 0.652 0.686 0.5719 0.1944 0.7879 0.6788 0.8615 R+L PLS-SVR 0.5 0.884 0.349 0.642 0.649 0.634 0.5245 0.2028 0.8218 0.7425 0.8864 Table 2: ParSS test results. in each task is given in Table 1. We use ridge regression (RR), support vector regression (SVR), and extremely randomized trees (TREE) (Geurts et al., 2006) as the learning models. These models learn a regression function using the features to estimate a numerical target value. We also use them after a dimensionality reduction and mapping step with partial least squares (PLS) (Specia et al., 2009). We optimize the learning parameters, the number of dimensions used for PLS, and the parameters for parallel FDA5. More details about optimization processes are in and Way, et al., 2014). We optimize the learning by selecting to the standard deviof the noise in the training set 2013) the optimal value for shown to have linear dependence to the noise level for different noise models (Smola et al., 1998). At testing time, the predictions are bounded to obtain scores in the corresponding ranges. use Pearson’s correlation mean absolute error (MAE), and relative absolute error (RAE) for evaluation: = (1) We define MAER and MRAER for easier replication and comparability with relative errors for each instance: = (2) n MAER is the mean absolute error relative to the magnitude of the target and MRAER is the mean absolute error relative to the absolute error of a predictor always predicting the target mean assuming that target mean is known. MAER and MRAER are from = is the measurement error and it is estimated as half of the mean absolute error or deviation of the prefrom target mean. half of the score step with which a decision about a change measurement’s value can be made. simito half of the standard deviation, the data but over absolute differences. For discrete target step size = A method for learning decision thresholds for mimicking the human decision process when determining whether two translations equivalent is described in 2013). MAER and MRAER are able to capture averaged fluctuations at the instance level and they may evaluate the performance of a predictor at performance prediction tasks at the instance level (e.g. performance of the similarity of sentences, performance of translation of different translation instances) better. RAE compares sums of prediction errors and MRAER averages instance prediction error comparisons. use L cap the argument from below to n = n n = 58 Model answers-forums answers-students belief headlines images Rank PLS-TREE 0.5484 0.5549 0.6223 0.7281 0.7189 0.6468 50 top result according to Weighted top 3 results with further optimization Model answers-forums answers-students belief headlines images TREE Rank 0.5517 0.6729 0.6750 0.7812 0.7830 0.7126 48 38 39 29 49 38 4: STS English test for each domain.</abstract>
<title confidence="0.489681">Data Model MAE RAE MAER MRAER Model News Weighted</title>
<note confidence="0.857494777777778">R PLS-SVR .4740 .4920 .5330 .4800 .5110 .6183 .6165 .6156 .6152 .6140 .2106 .2174 .2201 .2107 .2170 .6963 .7188 .7276 .6965 .7175 1.5408 .9223 R RR 1.8609 .9132 R PLS-TREE 1.939 .9144 R SVR 1.5012 .9306 R PLS RR 1.8443..9240 TREE 0.5823 0.5251 0.5443 6 RTM top Model according to Weighted top 3 results with further optimization News Weighted R+L SVR .5040 .4970 .5410 .4970 .4740 .6216 .6209 .6205 .6194 .6183 .2085 .2093 .2177 .2164 .2106 .6893 .6919 .7196 .7154 .6963 1.4723 1.5402 1.8834 1.8448 1.5408 .9344 .9226 .9161 .9096 .9223</note>
<title confidence="0.826443">R+L PLS-SVR R+L PLS-TREE R+L RR</title>
<author confidence="0.418161">R PLS-SVR</author>
<abstract confidence="0.989329793650794">TREE Rank 0.6622 0.5833 0.6096 5 4 5 Table 3: ParSS training results of top 5 RTM systems with further optimization. 2.1 Task 1: Paraphrase and Semantic Similarity in Twitter (ParSS) contains sentences provided by Twitter 3(Xu et al., 2015). Official evaluation metric is Pearson’s correlation score, which we use to select the top systems on the training set. RTM-DCU results on the ParSS test set are given in Table 2. The setting R using SVR becomes 2nd out of 13 systems and 3rd out of 25 submissions. Looking at MAE and MAER allows us to obtain explanations to train and test performance differences for example without knowing their target distribution. Even though of PLS-SVR is about on the ParSS set, MAER is due to test set confewer zero entries the train set). Lower test MAE than training MAE may be attributed to RTMs. We obtained results with lemmatized datasets and further optimized the learning model parameters after the challenge. We present the performance of the top 5 individual RTM models on the training set in Table 3. R uses the regular truecase (Koehn et al., Table 5: STS Spanish test results. 2007; Koehn, 2010) corpora and L uses the lemmatized truecased corpora. R+L correspond to using the features from both R and L, which doubles the number of features. 2.2 Task 2: Semantic Textual Similarity (STS) STS contains sentence pairs from different domains: answers-forums, answers-students, belief, headlines, and images for English and wikipedia and newswire for Spanish. Official evaluation metric in STS is the Pearson’s correlation score. We build separate RTM models for headlines and images domains for STS English. Domain specific RTM models obtain improved performance in those and Way, 2014b). STS English test set contains 2000, 1500, 2000, 1500, and 1500 sentences respectively from the specified domains however for evaluation, STS use a subset of the test set, 375, 750, 375, 750, and 750 instances respectively from the corresponding domains. This may lower the performance of RTMs by causing FDA5 to select more domain specific data and less task specific since RTMs use the test set to select interpretants and build a task specific RTM prediction model. Table 4 and Table 5 list the results on the test set 59 along with their ranks out of 73 and 16 submissions respectively for English STS and Spanish STS. RTM top test results selected according to top 3 results on STS for each subtask as well as top RTM-DCU results in STS and Way, 2014b) are presented in Table 6, where we have used the top results from domain specific RTM models for headlines and images domains in the overall model results. Top 3 individual RTM model performance on the training set with further optimized learning model parameters after challenge are presented in Table 7. Better RAE, and MRAER on the test set than on the training set in STS 2015 English may be attributed to RTMs. 2.3 RTMs Across Tasks and Years We compare the difficulty of tasks according to MRAER where the correlation of RAE and MRAER Table 8, we list the RAE, MAER, and MRAER obtained for different tasks and subtasks, also listing RTM results from SemEvaland van Genabith, 2013), from and Way, 2014b), and and quality estimation task (QET) and Way, 2014a) of machine translation (Bojar et al., 2014). RTMs at SemEval-2013 contain results from STS. RTMs at SemEval-2014 contain results from STS, semantic relatedness and entailment (SRE) (Marelli et al., 2014), and cross-level semantic similarity (CLSS) tasks (Jurgens et al., 2014). RTMs at WMT2014 QET contain tasks involving the predicof an integer in post-editing (PEE), a real number in representing human-targeted translation edit rate (HTER), or an integer representing post-editing time (PET) of translations. The best results are obtained for the CLSS paragraph to sentence subtask, which may be due to the larger contextual information that paragraphs can provide for the RTM models. For the ParSS task, we can only reduce the error with respect to knowand predicting the mean by about Prediction of bilingual similarity as in quality estimation of translation can be expected to be harder and RTMs SoA performance in this task as well and Way, 2014a). Table 8 can be used to evaluate the difficulty of various tasks and domains based on our SoA predictor RTM. MRAER considers both the predictor’s error and the target scores’ fluctuations at the instance level. We separated the results hav- MRAER greater than in these tasks and subtasks RTM does not perform significantly better than mean predictor and fluctuations render these as tasks that may require more work. 3 Conclusion Referential translation machines pioneer a clean and intuitive computational model for automatically measuring semantic similarity by measuring the acts of translation involved and achieve to become the 2nd system out of 13 systems participating in Paraphrase and Semantic Similarity in Twitter, 6th out of 16 submissions in Semantic Textual Similarity Spanish, and 50th out of 73 submissions in Semantic Textual Similarity English. RTMs make quality and semantic similarity judgments possible based on the retrieval of relevant training data as interpretants for reaching shared semantics. We define MAER, mean absolute error relative to the magnitude of the target, and MRAER, mean absolute error relative to the absolute error of a predictor always predicting the target mean assuming that target mean is known. RTM test performance on various tasks sorted according to MRAER can identify which tasks and subtasks may require more work.</abstract>
<note confidence="0.609891384615385">Acknowledgments This work is supported in part by SFI (13/TIDA/I2740) for the project “Monolingual and Bilingual Text Quality Judgments with Translation Prediction” and in part by SFI (12/CE/I2267) as part of the ADAPT CNGL Centre for Global Intelligent at Dublin City University. We also thank the SFI/HEA Irish Centre for High-End Computing (ICHEC) for the provision of computational facilities and support. References Eneko Agirre, Carmen Banea, Claire Cardie, Daniel</note>
<affiliation confidence="0.949708">Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo,</affiliation>
<address confidence="0.6419195">Rada Mihalcea, German Rigau, and Janyce Wiebe. 60</address>
<note confidence="0.86802237037037">Domain Model MAE RAE MAER MRAER answers-forums PLS-SVR 0.6215 1.2239 1.1675 1.5369 1.3449 answers-students PLS-SVR 0.6125 0.9635 0.7819 0.5542 0.8404 English belief PLS-SVR 0.5879 1.3625 1.1825 1.5749 1.4119 headlines RR 0.7812 0.8318 0.5894 0.4844 0.6380 images TREE 0.7830 0.8502 0.5885 0.5424 0.6229 ALL PLS-SVR 0.6739 0.9847 0.7224 0.7379 0.7883 News TREE 0.5303 0.6315 0.9426 0.4096 1.1052 Spanish Wikipedia TREE 0.5867 0.6448 0.9499 0.4844 1.2062 ALL TREE 0.5618 0.6360 0.9459 0.4348 1.1344 deft-forum TREE 0.4341 1.1609 1.0908 0.7724 1.216 deft-news TREE 0.6974 0.9032 0.8716 0.6271 0.881 headlines TREE 0.6199 0.9254 0.7845 0.6711 0.7854 English images TREE 0.6995 0.9499 0.7395 0.8338 0.7246 OnWN TREE 0.8058 1.0028 0.5585 0.7975 0.546 tweet-news TREE 0.6882 0.831 0.8093 0.4601 0.875 ALL TREE 0.6473 0.9534 0.7449 0.7274 0.7566 News TREE 0.7 1.351 1.4141 0.5994 1.8053 Spanish Wikipedia TREE 0.4216 1.298 1.3579 0.65 1.6612 ALL TREE 0.62 1.3296 1.3823 0.6191 1.7719 headlines 0.6552 1.2763 1.0231 1.0456 1.1444 English OnWN SMT L+S SVR 0.6943 1.3545 0.8255 1.2875 0.8605 0.3005 0.6886 1.6132 0.1669 2.0718 FNWN 0.2016 1.0604 1.2633 1.5087 1.4048 ALL L+S SVR 0.5844 1.0818 0.7791 0.8494 0.77 6: RTM top test results selected according to Weighted top 3 results on STS as well as top RTM-DCU in STS 2013 and STS 2014 and Way, 2014b). ALL presents results over all of the test set.</note>
<title confidence="0.728378">Model MAE RAE MAER MRAER</title>
<phone confidence="0.775677727272727">PLS-SVR 0.7477 0.7679 0.6050 0.4444 0.6947 SVR 0.7452 0.7688 0.6058 0.4504 0.686 TREE 0.7265 0.8093 0.6377 0.504 0.6812 RR 0.7453 0.7559 0.6215 0.4389 0.6835 PLS-SVR 0.7411 0.7619 0.6265 0.4298 0.7087 TREE 0.7386 0.7710 0.6340 0.4726 0.6686 TREE 0.7600 0.8020 0.6248 0.5308 0.7013 PLS-SVR 0.7574 0.7839 0.6106 0.4898 0.724 RR 0.7564 0.7945 0.6189 0.5025 0.7161 TREE 0.8390 0.5154 0.5115 0.4145 0.5931 RR 0.8260 0.5473 0.5431 0.4571 0.6208</phone>
<note confidence="0.791226666666667">PLS-SVR 0.8218 0.5363 0.5322 0.4171 0.635 Table 7: RTM training results of top 3 systems on STS English, English images, English headlines, and Spanish tasks. 2015. SemEval-2015 Task 2: Semantic textual simorado, USA, June. In of the 9th International Workshop Daniel Jaquette, David Graff, and Semantic Evaluation (SemEval Denver, Col- Denise DiPersio. 2011. Spanish Gigaword third edi- STS 2015</note>
<date confidence="0.273621">STS 2014</date>
<abstract confidence="0.86746958974359">STS 2013 Lang English headlines images Spanish tion, Linguistic Data Consortium. and Josef van Genabith. 2013. CNGL- CORE: Referential translation machines for measuring similarity. In 2013: The Second Joint on Lexical and Computational pages 234–240, Atlanta, Georgia, USA, 13-14 June. and Andy Way. 2014a. Referential translation machines for predicting translation quality. In Proc. of the Ninth Workshop on Statistical Machine pages 313–321, Baltimore, Maryland, USA, June. and Andy Way. 2014b. RTM-DCU: Referential translation machines for semantic similarity. of the 8th International Workshop on Evaluation pages 487–496, Dublin, Ireland, 23-24 August. and Deniz Yuret. 2015. Optimizing instance selection for statistical machine translation with decay algorithms. Transactions Audio, Speech, and Language Processing 23:339–350. Declan Groves, and Josef van Genabith. 2013. Predicting sentence translation quality using exand language independent features. 27:171–192, December. Qun Liu, and Andy Way. 2014. Parallel FDA5 for fast deployment of accurate statistical matranslation systems. In of the Ninth Workon Statistical Machine pages 59–65, Baltimore, USA, June. 2013. Referential translation machines for estimation. In of the Eighth Workshop on Machine pages 343–351, Sofia,</abstract>
<address confidence="0.873689">Bulgaria, August.</address>
<abstract confidence="0.503778">Bliss. 2012. Comedy is translation, February. chris bliss comedy is translation.html.</abstract>
<author confidence="0.698634333333333">Ondrej Bojar</author>
<author confidence="0.698634333333333">Christian Buck</author>
<author confidence="0.698634333333333">Christian Federmann</author>
<author confidence="0.698634333333333">Barry Haddow</author>
<author confidence="0.698634333333333">Philipp Koehn</author>
<author confidence="0.698634333333333">Johannes Leveling</author>
<author confidence="0.698634333333333">Christof Monz</author>
<author confidence="0.698634333333333">Pavel Pecina</author>
<author confidence="0.698634333333333">Matt Post</author>
<author confidence="0.698634333333333">Herve Saint-</author>
<affiliation confidence="0.305996">Amand, Radu Soricut, Lucia Specia, and Aleˇs Tam-</affiliation>
<abstract confidence="0.821037166666667">chyna. 2014. Findings of the 2014 workshop on statismachine translation. In of the Ninth on Statistical Machine pages 12–58, Baltimore, Maryland, USA, June. Pierre Geurts, Damien Ernst, and Louis Wehenkel. 2006. randomized trees.</abstract>
<note confidence="0.708049">63(1):3–42. David Jurgens, Mohammad Taher Pilehvar, and Roberto Navigli. 2014. SemEval-2014 Task 3: Cross-level similarity. In of the 8th International on Semantic Evaluation (SemEval pages 17–26, Dublin, Ireland, August. Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source for statistical machine translation. In pages 177–180, Prague, Czech Republic, June. Philipp Koehn. 2010. An experimental management sys- Prague Bulletin ofMathematical 94:87–96.</note>
<author confidence="0.929258">Marco Marelli</author>
<author confidence="0.929258">Luisa Bentivogli</author>
<author confidence="0.929258">Marco Baroni</author>
<author confidence="0.929258">Raffaella Bernardi</author>
<author confidence="0.929258">Stefano Menini</author>
<author confidence="0.929258">Roberto Zam-</author>
<abstract confidence="0.954671">parelli. 2014. SemEval-2014 Task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual en-</abstract>
<note confidence="0.816192590909091">In of the 8th International Workshop Semantic Evaluation (SemEval pages 1–8, Dublin, Ireland, August. Preslav Nakov, Torsten Zesch, Daniel Cer, and David Jueditors. 2015. of the 9th International on Semantic Evaluation (SemEval Denver, Colorado, USA, 4-5 June. Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2011. English Gigaword fifth edition, Linguistic Data Consortium. A. J. Smola, N. Murata, B. Sch¨olkopf, and K.-R. M¨uller. Asymptotically optimal choice of for support vector machines. In L. Niklasson, M. Boden, T. Ziemke, editors, of the International Conon Artificial Neural Perspectives in Neural Computing, pages 105–110, Berlin. Lucia Specia, Nicola Cancedda, Marc Dymetman, Marco Turchi, and Nello Cristianini. 2009. Estimating the sentence-level quality of machine translation systems. of the 13th Annual Conference of the Eu- Association for Machine Translation pages 28–35, Barcelona, Spain, May.</note>
<author confidence="0.805711">Kristina Toutanova</author>
<author confidence="0.805711">Dan Klein</author>
<author confidence="0.805711">Christopher D Manning</author>
<abstract confidence="0.91016">and Yoram Singer. 2003. Feature-rich part-of-speech with a cyclic dependency network. In of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Hu-</abstract>
<affiliation confidence="0.600952">Language Technology - Volume NAACL ’03,</affiliation>
<address confidence="0.876264">pages 173–180, Stroudsburg, PA, USA.</address>
<author confidence="0.624504">Wei Xu</author>
<author confidence="0.624504">Chris Callison-Burch</author>
<author confidence="0.624504">William B Dolan</author>
<note confidence="0.295219552631579">2015. SemEval-2015 Task 1: Paraphrase and semantic in Twitter (PIT). In of the 9th Interna- Workshop on Semantic Evaluation Denver, Colorado, USA, June. 62 Task Subtask Domain Model RAE MAER MRAER CLSS 2014 Paragraph to Sentence Mixed TREE 0.4579 0.5112 0.5037 STS 2014 English OnWN TREE 0.5585 0.7975 0.546 QET 2014 English-Spanish PEE Europarl PLS-TREE 1.0794 0.304 0.614 STS 2015 English Images TREE 0.5885 0.5424 0.6229 STS 2015 English Headlines RR 0.5894 0.4844 0.6380 CLSS 2014 Sentence to Phrase Mixed TREE 0.6255 0.6857 0.6444 QET 2014 German-English PEE Europarl RR 0.8204 0.3575 0.679 QET 2014 English-German PEE Europarl TREE 0.8602 0.3692 0.6985 STS 2014 English Images TREE 0.7395 0.8338 0.7246 QET 2014 Spanish-English PEE Europarl FS-RR 0.9 0.3798 0.7491 QET 2014 English-Spanish PET Europarl SVR 0.7223 0.4651 0.7786 STS 2014 English Headlines TREE 0.7845 0.6711 0.7854 SRE 2014 English SICK R+L PLS-SVR 0.6645 0.1827 0.8177 ParSS 2015 English Tweets SVR 0.775 0.6901 0.838 STS 2015 English Answers-students PLS-SVR 0.7819 0.5542 0.8404 CLSS 2014 Phrase to Word Mixed TREE 0.9488 1.1454 0.8483 STS 2013 English OnWN L+S SVR 0.8255 1.2875 0.8605 STS 2014 English Tweet-news TREE 0.8093 0.4601 0.875 QET 2014 English-Spanish HTER Europarl SVR 0.8532 0.7727 0.8758 STS 2014 English Deft-news TREE 0.8716 0.6271 0.881 STS 2015 Spanish News TREE 0.9426 0.4096 1.1052 STS 2013 English Headlines L+S SVR 1.0231 1.0456 1.1444 STS 2015 Spanish Wikipedia TREE 0.9499 0.4844 1.2062 STS 2014 English Deft-forum TREE 1.0908 0.7724 1.216 STS 2015 English Answers-forums PLS-SVR 1.1675 1.5369 1.3449 STS 2013 English FNWN L+S SVR 1.2633 1.5087 1.4048 STS 2015 English Belief PLS-SVR 1.1825 1.5749 1.4119 STS 2014 Spanish Wikipedia TREE 1.3579 0.65 1.6612 STS 2014 Spanish News TREE 1.4141 0.5994 1.8053 STS 2013 English SMT L+S SVR 1.6132 0.1669 2.0718 Table 8: Best RTM test results for different tasks and subtasks sorted according to MRAER. 63</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Banea</author>
<author>Claire Cardie</author>
<author>Daniel Cer</author>
</authors>
<location>Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German</location>
<marker>Agirre, Banea, Cardie, Cer, </marker>
<rawString>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe.</rawString>
</citation>
<citation valid="true">
<title>SemEval-2015 Task 2: Semantic textual sim- orado,</title>
<date>2015</date>
<booktitle>In Proc. of the 9th International Workshop ˆAngelo Mendonc¸a, Daniel Jaquette, David Graff, and on Semantic Evaluation (SemEval 2015), Denver, Col- Denise DiPersio.</booktitle>
<location>USA,</location>
<marker>2015</marker>
<rawString>2015. SemEval-2015 Task 2: Semantic textual sim- orado, USA, June. ilarity. In Proc. of the 9th International Workshop ˆAngelo Mendonc¸a, Daniel Jaquette, David Graff, and on Semantic Evaluation (SemEval 2015), Denver, Col- Denise DiPersio. 2011. Spanish Gigaword third edition, Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Josef van Genabith</author>
</authors>
<title>CNGLCORE: Referential translation machines for measuring semantic similarity.</title>
<date>2013</date>
<booktitle>In *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>234--240</pages>
<location>Atlanta, Georgia, USA,</location>
<marker>Bic¸ici, van Genabith, 2013</marker>
<rawString>Ergun Bic¸ici and Josef van Genabith. 2013. CNGLCORE: Referential translation machines for measuring semantic similarity. In *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics, pages 234–240, Atlanta, Georgia, USA, 13-14 June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Andy Way</author>
</authors>
<title>Referential translation machines for predicting translation quality.</title>
<date>2014</date>
<booktitle>In Proc. of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<pages>313--321</pages>
<location>Baltimore, Maryland, USA,</location>
<marker>Bic¸ici, Way, 2014</marker>
<rawString>Ergun Bic¸ici and Andy Way. 2014a. Referential translation machines for predicting translation quality. In Proc. of the Ninth Workshop on Statistical Machine Translation, pages 313–321, Baltimore, Maryland, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Andy Way</author>
</authors>
<title>RTM-DCU: Referential translation machines for semantic similarity.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014),</booktitle>
<pages>487--496</pages>
<location>Dublin,</location>
<marker>Bic¸ici, Way, 2014</marker>
<rawString>Ergun Bic¸ici and Andy Way. 2014b. RTM-DCU: Referential translation machines for semantic similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014), pages 487–496, Dublin, Ireland, 23-24 August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Deniz Yuret</author>
</authors>
<title>Optimizing instance selection for statistical machine translation with feature decay algorithms.</title>
<date>2015</date>
<journal>IEEE/ACM Transactions On Audio, Speech, and Language Processing (TASLP),</journal>
<pages>23--339</pages>
<marker>Bic¸ici, Yuret, 2015</marker>
<rawString>Ergun Bic¸ici and Deniz Yuret. 2015. Optimizing instance selection for statistical machine translation with feature decay algorithms. IEEE/ACM Transactions On Audio, Speech, and Language Processing (TASLP), 23:339–350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Declan Groves</author>
<author>Josef van Genabith</author>
</authors>
<title>Predicting sentence translation quality using extrinsic and language independent features.</title>
<date>2013</date>
<journal>Machine Translation,</journal>
<pages>27--171</pages>
<marker>Bic¸ici, Groves, van Genabith, 2013</marker>
<rawString>Ergun Bic¸ici, Declan Groves, and Josef van Genabith. 2013. Predicting sentence translation quality using extrinsic and language independent features. Machine Translation, 27:171–192, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Qun Liu</author>
<author>Andy Way</author>
</authors>
<title>Parallel FDA5 for fast deployment of accurate statistical machine translation systems.</title>
<date>2014</date>
<booktitle>In Proc. of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<pages>59--65</pages>
<location>Baltimore, USA,</location>
<marker>Bic¸ici, Liu, Way, 2014</marker>
<rawString>Ergun Bic¸ici, Qun Liu, and Andy Way. 2014. Parallel FDA5 for fast deployment of accurate statistical machine translation systems. In Proc. of the Ninth Workshop on Statistical Machine Translation, pages 59–65, Baltimore, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
</authors>
<title>Referential translation machines for quality estimation.</title>
<date>2013</date>
<booktitle>In Proc. of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>343--351</pages>
<location>Sofia, Bulgaria,</location>
<marker>Bic¸ici, 2013</marker>
<rawString>Ergun Bic¸ici. 2013. Referential translation machines for quality estimation. In Proc. of the Eighth Workshop on Statistical Machine Translation, pages 343–351, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Bliss</author>
</authors>
<title>Comedy is translation, February. http://www.ted.com/talks/ chris bliss comedy is translation.html.</title>
<date>2012</date>
<contexts>
<context position="4534" citStr="Bliss, 2012" startWordPosition="673" endWordPosition="674">Z 2 MTPPSystem(Z, train) — Ttrain 3 MTPPSystem(Z, test) — Ttest 4 learn(M, Ttrain) — M 5 predict(M,Ttest) — yˆ tual Similarity (Task 2) (Agirre et al., 2015) becoming 6th out of 16 submissions in Spanish. We use the Parallel FDA5 instance selection model for selecting the interpretants (Bic¸ici et al., 2014; Bic¸ici and Yuret, 2015), which allows efficient parameterization, optimization, and implementation of Feature Decay Algorithms (FDA), and build an MTPP model. We view that acts of translation are ubiquitously used during communication: Every act of communication is an act of translation (Bliss, 2012). Translation need not be between different languages and paraphrasing or communication also contain acts of translation. When creating sentences, we use our background knowledge and translate information content according to the current context. Figure 1 depicts RTM and Algorithm 1 describes Task Setting Train LM Task 1, ParSS English 313 7813 Task 2, STS English 441 6441 Task 2, STS English headlines 531 8031 Task 2, STS English images 411 6411 Task 2, STS Spanish 409 6409 Table 1: Number of sentences in Z (in thousands) selected for each task. the RTM algorithm. Our encouraging results in t</context>
</contexts>
<marker>Bliss, 2012</marker>
<rawString>Chris Bliss. 2012. Comedy is translation, February. http://www.ted.com/talks/ chris bliss comedy is translation.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Christian Buck</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Johannes Leveling</author>
<author>Christof Monz</author>
<author>Pavel Pecina</author>
<author>Matt Post</author>
</authors>
<title>Herve SaintAmand, Radu Soricut, Lucia Specia, and Aleˇs Tamchyna.</title>
<date>2014</date>
<journal>Findings of the</journal>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<pages>12--58</pages>
<location>Baltimore, Maryland, USA,</location>
<contexts>
<context position="6272" citStr="Bojar et al., 2014" startWordPosition="964" endWordPosition="967">milar meaning: a discrete real number in [0, 1]. We model as sentence MTPP between S1 to S2. STS Semantic Textual Similarity (STS) (Agirre et al., 2015): Given two sentences S1 and S2 in the same language, quantify the degree of similarity: a real number in [0, 5]. STS is in English and Spanish (a real number in [0, 4]). We model as sentence MTPP of S1 and S2. 2 SemEval-15 Results We develop individual RTM models for each task and subtask that we participate at SemEval-2015 with the RTM-DCU team name. Interpretants are selected from the LM corpora distributed by the translation task of WMT14 (Bojar et al., 2014) and LDC for English (Parker et al., 2011) and Spanish ( ˆAngelo Mendonc¸a et al., 2011) 1. We use the Stanford POS tagger (Toutanova et al., 2003) to obtain the lemmatized corpora for the ParSS task. The number of instances we select for the interpretants 1English Gigaword 5th, Spanish Gigaword 3rd edition. 57 RTM-DCU results Data Model Fl Precision Recall maxFl mPrecision mRecall rP MAE RAE MAER MRAER Rank R SVR 0.54 0.883 0.389 0.693 0.695 0.691 0.5697 0.1953 0.7918 0.4278 0.8694 3 R PLS-SVR 0.562 0.859 0.417 0.678 0.649 0.709 0.564 0.2001 0.8109 0.4442 0.9105 4 RTM results with further opt</context>
<context position="14630" citStr="Bojar et al., 2014" startWordPosition="2404" endWordPosition="2407">eters after the challenge are presented in Table 7. Better rp, RAE, and MRAER on the test set than on the training set in STS 2015 English may be attributed to RTMs. 2.3 RTMs Across Tasks and Years We compare the difficulty of tasks according to MRAER where the correlation of RAE and MRAER is 0.89. In Table 8, we list the RAE, MAER, and MRAER obtained for different tasks and subtasks, also listing RTM results from SemEval2013 (Bic¸ici and van Genabith, 2013), from SemEval-2014 (Bic¸ici and Way, 2014b), and and from quality estimation task (QET) (Bic¸ici and Way, 2014a) of machine translation (Bojar et al., 2014). RTMs at SemEval-2013 contain results from STS. RTMs at SemEval-2014 contain results from STS, semantic relatedness and entailment (SRE) (Marelli et al., 2014), and cross-level semantic similarity (CLSS) tasks (Jurgens et al., 2014). RTMs at WMT2014 QET contain tasks involving the prediction of an integer in [1, 3] representing post-editing effort (PEE), a real number in [0, 1] representing human-targeted translation edit rate (HTER), or an integer representing post-editing time (PET) of translations. The best results are obtained for the CLSS paragraph to sentence subtask, which may be due t</context>
</contexts>
<marker>Bojar, Buck, Federmann, Haddow, Koehn, Leveling, Monz, Pecina, Post, 2014</marker>
<rawString>Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve SaintAmand, Radu Soricut, Lucia Specia, and Aleˇs Tamchyna. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 12–58, Baltimore, Maryland, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Geurts</author>
<author>Damien Ernst</author>
<author>Louis Wehenkel</author>
</authors>
<title>Extremely randomized trees.</title>
<date>2006</date>
<booktitle>Machine Learning,</booktitle>
<pages>63--1</pages>
<contexts>
<context position="7457" citStr="Geurts et al., 2006" startWordPosition="1165" endWordPosition="1168">9105 4 RTM results with further optimization Data Model Fl Precision Recall maxFl mPrecision mRecall rP MAE RAE MAER MRAER R PLS-SVR 0.502 0.938 0.343 0.674 0.686 0.663 0.5798 0.1912 0.775 0.6901 0.838 R RR 0.521 0.94 0.36 0.681 0.735 0.634 0.5777 0.1866 0.7564 0.7438 0.7944 R+L SVR 0.53 0.892 0.377 0.669 0.652 0.686 0.5719 0.1944 0.7879 0.6788 0.8615 R+L PLS-SVR 0.5 0.884 0.349 0.642 0.649 0.634 0.5245 0.2028 0.8218 0.7425 0.8864 Table 2: ParSS test results. in each task is given in Table 1. We use ridge regression (RR), support vector regression (SVR), and extremely randomized trees (TREE) (Geurts et al., 2006) as the learning models. These models learn a regression function using the features to estimate a numerical target value. We also use them after a dimensionality reduction and mapping step with partial least squares (PLS) (Specia et al., 2009). We optimize the learning parameters, the number of dimensions used for PLS, and the parameters for parallel FDA5. More details about the optimization processes are in (Bic¸ici and Way, 2014b; Bic¸ici et al., 2014). We optimize the learning parameters by selecting e close to the standard deviation of the noise in the training set (Bic¸ici, 2013) since t</context>
</contexts>
<marker>Geurts, Ernst, Wehenkel, 2006</marker>
<rawString>Pierre Geurts, Damien Ernst, and Louis Wehenkel. 2006. Extremely randomized trees. Machine Learning, 63(1):3–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Mohammad Taher Pilehvar</author>
<author>Roberto Navigli</author>
</authors>
<title>SemEval-2014 Task 3: Cross-level semantic similarity.</title>
<date>2014</date>
<booktitle>In Proc. of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>17--26</pages>
<location>Dublin, Ireland,</location>
<contexts>
<context position="14863" citStr="Jurgens et al., 2014" startWordPosition="2437" endWordPosition="2440">s according to MRAER where the correlation of RAE and MRAER is 0.89. In Table 8, we list the RAE, MAER, and MRAER obtained for different tasks and subtasks, also listing RTM results from SemEval2013 (Bic¸ici and van Genabith, 2013), from SemEval-2014 (Bic¸ici and Way, 2014b), and and from quality estimation task (QET) (Bic¸ici and Way, 2014a) of machine translation (Bojar et al., 2014). RTMs at SemEval-2013 contain results from STS. RTMs at SemEval-2014 contain results from STS, semantic relatedness and entailment (SRE) (Marelli et al., 2014), and cross-level semantic similarity (CLSS) tasks (Jurgens et al., 2014). RTMs at WMT2014 QET contain tasks involving the prediction of an integer in [1, 3] representing post-editing effort (PEE), a real number in [0, 1] representing human-targeted translation edit rate (HTER), or an integer representing post-editing time (PET) of translations. The best results are obtained for the CLSS paragraph to sentence subtask, which may be due to the larger contextual information that paragraphs can provide for the RTM models. For the ParSS task, we can only reduce the error with respect to knowing and predicting the mean by about 22.5%. Prediction of bilingual similarity a</context>
</contexts>
<marker>Jurgens, Pilehvar, Navigli, 2014</marker>
<rawString>David Jurgens, Mohammad Taher Pilehvar, and Roberto Navigli. 2014. SemEval-2014 Task 3: Cross-level semantic similarity. In Proc. of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 17–26, Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL, pages 177–180, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>An experimental management system.</title>
<date>2010</date>
<booktitle>The Prague Bulletin ofMathematical Linguistics,</booktitle>
<pages>94--87</pages>
<contexts>
<context position="12355" citStr="Koehn, 2010" startWordPosition="2016" endWordPosition="2017">ces for example without knowing their target distribution. Even though MAE of PLS-SVR is about %5 smaller on the ParSS test set, MAER is %55 smaller due to test set containing fewer zero entries (%16 vs. %39 on the train set). Lower test MAE than training MAE may be attributed to RTMs. We obtained results with lemmatized datasets and further optimized the learning model parameters after the challenge. We present the performance of the top 5 individual RTM models on the training set in Table 3. R uses the regular truecase (Koehn et al., 3www.twitter.com Table 5: STS Spanish test results. 2007; Koehn, 2010) corpora and L uses the lemmatized truecased corpora. R+L correspond to using the features from both R and L, which doubles the number of features. 2.2 Task 2: Semantic Textual Similarity (STS) STS contains sentence pairs from different domains: answers-forums, answers-students, belief, headlines, and images for English and wikipedia and newswire for Spanish. Official evaluation metric in STS is the Pearson’s correlation score. We build separate RTM models for headlines and images domains for STS English. Domain specific RTM models obtain improved performance in those domains (Bic¸ici and Way,</context>
</contexts>
<marker>Koehn, 2010</marker>
<rawString>Philipp Koehn. 2010. An experimental management system. The Prague Bulletin ofMathematical Linguistics, 94:87–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Marelli</author>
<author>Luisa Bentivogli</author>
<author>Marco Baroni</author>
<author>Raffaella Bernardi</author>
<author>Stefano Menini</author>
<author>Roberto Zamparelli</author>
</authors>
<title>SemEval-2014 Task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment.</title>
<date>2014</date>
<booktitle>In Proc. of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>1--8</pages>
<location>Dublin, Ireland,</location>
<contexts>
<context position="14790" citStr="Marelli et al., 2014" startWordPosition="2427" endWordPosition="2430">o RTMs. 2.3 RTMs Across Tasks and Years We compare the difficulty of tasks according to MRAER where the correlation of RAE and MRAER is 0.89. In Table 8, we list the RAE, MAER, and MRAER obtained for different tasks and subtasks, also listing RTM results from SemEval2013 (Bic¸ici and van Genabith, 2013), from SemEval-2014 (Bic¸ici and Way, 2014b), and and from quality estimation task (QET) (Bic¸ici and Way, 2014a) of machine translation (Bojar et al., 2014). RTMs at SemEval-2013 contain results from STS. RTMs at SemEval-2014 contain results from STS, semantic relatedness and entailment (SRE) (Marelli et al., 2014), and cross-level semantic similarity (CLSS) tasks (Jurgens et al., 2014). RTMs at WMT2014 QET contain tasks involving the prediction of an integer in [1, 3] representing post-editing effort (PEE), a real number in [0, 1] representing human-targeted translation edit rate (HTER), or an integer representing post-editing time (PET) of translations. The best results are obtained for the CLSS paragraph to sentence subtask, which may be due to the larger contextual information that paragraphs can provide for the RTM models. For the ParSS task, we can only reduce the error with respect to knowing and</context>
</contexts>
<marker>Marelli, Bentivogli, Baroni, Bernardi, Menini, Zamparelli, 2014</marker>
<rawString>Marco Marelli, Luisa Bentivogli, Marco Baroni, Raffaella Bernardi, Stefano Menini, and Roberto Zamparelli. 2014. SemEval-2014 Task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. In Proc. of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 1–8, Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<date>2015</date>
<booktitle>Proc. of the 9th International Workshop on Semantic Evaluation (SemEval 2015).</booktitle>
<pages>4--5</pages>
<editor>Preslav Nakov, Torsten Zesch, Daniel Cer, and David Jurgens, editors.</editor>
<location>Denver, Colorado, USA,</location>
<marker>2015</marker>
<rawString>Preslav Nakov, Torsten Zesch, Daniel Cer, and David Jurgens, editors. 2015. Proc. of the 9th International Workshop on Semantic Evaluation (SemEval 2015). Denver, Colorado, USA, 4-5 June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Parker</author>
<author>David Graff</author>
<author>Junbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<title>English Gigaword fifth edition, Linguistic Data Consortium.</title>
<date>2011</date>
<contexts>
<context position="6314" citStr="Parker et al., 2011" startWordPosition="972" endWordPosition="975">[0, 1]. We model as sentence MTPP between S1 to S2. STS Semantic Textual Similarity (STS) (Agirre et al., 2015): Given two sentences S1 and S2 in the same language, quantify the degree of similarity: a real number in [0, 5]. STS is in English and Spanish (a real number in [0, 4]). We model as sentence MTPP of S1 and S2. 2 SemEval-15 Results We develop individual RTM models for each task and subtask that we participate at SemEval-2015 with the RTM-DCU team name. Interpretants are selected from the LM corpora distributed by the translation task of WMT14 (Bojar et al., 2014) and LDC for English (Parker et al., 2011) and Spanish ( ˆAngelo Mendonc¸a et al., 2011) 1. We use the Stanford POS tagger (Toutanova et al., 2003) to obtain the lemmatized corpora for the ParSS task. The number of instances we select for the interpretants 1English Gigaword 5th, Spanish Gigaword 3rd edition. 57 RTM-DCU results Data Model Fl Precision Recall maxFl mPrecision mRecall rP MAE RAE MAER MRAER Rank R SVR 0.54 0.883 0.389 0.693 0.695 0.691 0.5697 0.1953 0.7918 0.4278 0.8694 3 R PLS-SVR 0.562 0.859 0.417 0.678 0.649 0.709 0.564 0.2001 0.8109 0.4442 0.9105 4 RTM results with further optimization Data Model Fl Precision Recall m</context>
</contexts>
<marker>Parker, Graff, Kong, Chen, Maeda, 2011</marker>
<rawString>Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2011. English Gigaword fifth edition, Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Smola</author>
<author>N Murata</author>
<author>B Sch¨olkopf</author>
<author>K-R M¨uller</author>
</authors>
<title>Asymptotically optimal choice of ε-loss for support vector machines. In</title>
<date>1998</date>
<booktitle>Proc. of the International Conference on Artificial Neural Networks, Perspectives in Neural Computing,</booktitle>
<pages>105--110</pages>
<editor>L. Niklasson, M. Boden, and T. Ziemke, editors,</editor>
<location>Berlin.</location>
<marker>Smola, Murata, Sch¨olkopf, M¨uller, 1998</marker>
<rawString>A. J. Smola, N. Murata, B. Sch¨olkopf, and K.-R. M¨uller. 1998. Asymptotically optimal choice of ε-loss for support vector machines. In L. Niklasson, M. Boden, and T. Ziemke, editors, Proc. of the International Conference on Artificial Neural Networks, Perspectives in Neural Computing, pages 105–110, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Nicola Cancedda</author>
<author>Marc Dymetman</author>
<author>Marco Turchi</author>
<author>Nello Cristianini</author>
</authors>
<title>Estimating the sentence-level quality of machine translation systems.</title>
<date>2009</date>
<booktitle>In Proc. of the 13th Annual Conference of the European Association for Machine Translation (EAMT),</booktitle>
<pages>28--35</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="7701" citStr="Specia et al., 2009" startWordPosition="1205" endWordPosition="1209">777 0.1866 0.7564 0.7438 0.7944 R+L SVR 0.53 0.892 0.377 0.669 0.652 0.686 0.5719 0.1944 0.7879 0.6788 0.8615 R+L PLS-SVR 0.5 0.884 0.349 0.642 0.649 0.634 0.5245 0.2028 0.8218 0.7425 0.8864 Table 2: ParSS test results. in each task is given in Table 1. We use ridge regression (RR), support vector regression (SVR), and extremely randomized trees (TREE) (Geurts et al., 2006) as the learning models. These models learn a regression function using the features to estimate a numerical target value. We also use them after a dimensionality reduction and mapping step with partial least squares (PLS) (Specia et al., 2009). We optimize the learning parameters, the number of dimensions used for PLS, and the parameters for parallel FDA5. More details about the optimization processes are in (Bic¸ici and Way, 2014b; Bic¸ici et al., 2014). We optimize the learning parameters by selecting e close to the standard deviation of the noise in the training set (Bic¸ici, 2013) since the optimal value for e is shown to have linear dependence to the noise level for different noise models (Smola et al., 1998). At testing time, the predictions are bounded to obtain scores in the corresponding ranges. We use Pearson’s correlatio</context>
</contexts>
<marker>Specia, Cancedda, Dymetman, Turchi, Cristianini, 2009</marker>
<rawString>Lucia Specia, Nicola Cancedda, Marc Dymetman, Marco Turchi, and Nello Cristianini. 2009. Estimating the sentence-level quality of machine translation systems. In Proc. of the 13th Annual Conference of the European Association for Machine Translation (EAMT), pages 28–35, Barcelona, Spain, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proc. of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03,</booktitle>
<pages>173--180</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6419" citStr="Toutanova et al., 2003" startWordPosition="992" endWordPosition="995">l., 2015): Given two sentences S1 and S2 in the same language, quantify the degree of similarity: a real number in [0, 5]. STS is in English and Spanish (a real number in [0, 4]). We model as sentence MTPP of S1 and S2. 2 SemEval-15 Results We develop individual RTM models for each task and subtask that we participate at SemEval-2015 with the RTM-DCU team name. Interpretants are selected from the LM corpora distributed by the translation task of WMT14 (Bojar et al., 2014) and LDC for English (Parker et al., 2011) and Spanish ( ˆAngelo Mendonc¸a et al., 2011) 1. We use the Stanford POS tagger (Toutanova et al., 2003) to obtain the lemmatized corpora for the ParSS task. The number of instances we select for the interpretants 1English Gigaword 5th, Spanish Gigaword 3rd edition. 57 RTM-DCU results Data Model Fl Precision Recall maxFl mPrecision mRecall rP MAE RAE MAER MRAER Rank R SVR 0.54 0.883 0.389 0.693 0.695 0.691 0.5697 0.1953 0.7918 0.4278 0.8694 3 R PLS-SVR 0.562 0.859 0.417 0.678 0.649 0.709 0.564 0.2001 0.8109 0.4442 0.9105 4 RTM results with further optimization Data Model Fl Precision Recall maxFl mPrecision mRecall rP MAE RAE MAER MRAER R PLS-SVR 0.502 0.938 0.343 0.674 0.686 0.663 0.5798 0.1912</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proc. of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03, pages 173–180, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Chris Callison-Burch</author>
<author>William B Dolan</author>
</authors>
<title>SemEval-2015 Task 1: Paraphrase and semantic similarity in Twitter (PIT).</title>
<date>2015</date>
<booktitle>In Proc. of the 9th International Workshop on Semantic Evaluation (SemEval),</booktitle>
<location>Denver, Colorado, USA,</location>
<contexts>
<context position="3419" citStr="Xu et al., 2015" startWordPosition="499" endWordPosition="502">, which is a state-of-the-art (SoA) performance predictor of translation even without using the translation. MTPP system measures the coverage of individual test sentence features found in the training set and derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation. MTPP features for translation acts are provided in (Bic¸ici and Way, 2014b). RTMs become the 2nd system out of 13 systems participating in Paraphrase and Semantic Similarity in Twitter (Task 1) (Xu et al., 2015) and achieve good results in Semantic Tex56 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 56–63, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics Figure 1: RTM depiction. Algorithm 1: Referential Translation Machine Input: Training set train, test set test, corpus C, and learning model M. Data: Features of train and test, Ttrain and Ttest. Output: Predictions of similarity scores on the test ˆq. 1 FDA5(train, test, C) — Z 2 MTPPSystem(Z, train) — Ttrain 3 MTPPSystem(Z, test) — Ttest 4 learn(M, Ttrain) — M 5 predict(</context>
<context position="5536" citStr="Xu et al., 2015" startWordPosition="834" endWordPosition="837">S English headlines 531 8031 Task 2, STS English images 411 6411 Task 2, STS Spanish 409 6409 Table 1: Number of sentences in Z (in thousands) selected for each task. the RTM algorithm. Our encouraging results in the semantic similarity tasks increase our understanding of the acts of translation we ubiquitously use when communicating and how they can be used to predict semantic similarity. RTMs are powerful enough to be applicable in different domains and tasks with good performance. We describe the tasks we participated as follows: ParSS Paraphrase and Semantic Similarity in Twitter (ParSS) (Xu et al., 2015): Given two sentences S1 and S2 in the same language, produce a similarity score indicating whether they express a similar meaning: a discrete real number in [0, 1]. We model as sentence MTPP between S1 to S2. STS Semantic Textual Similarity (STS) (Agirre et al., 2015): Given two sentences S1 and S2 in the same language, quantify the degree of similarity: a real number in [0, 5]. STS is in English and Spanish (a real number in [0, 4]). We model as sentence MTPP of S1 and S2. 2 SemEval-15 Results We develop individual RTM models for each task and subtask that we participate at SemEval-2015 with</context>
<context position="11382" citStr="Xu et al., 2015" startWordPosition="1843" endWordPosition="1846">M top result rP selected according to Weighted rP among top 3 results with further optimization Model Wikipedia News Weighted rP Rank R+L SVR .5040 .6216 .2085 .6893 1.4723 .9344 R+L PLS-SVR .4970 .6209 .2093 .6919 1.5402 .9226 R+L PLS-TREE .5410 .6205 .2177 .7196 1.8834 .9161 R+L RR .4970 .6194 .2164 .7154 1.8448 .9096 R PLS-SVR .4740 .6183 .2106 .6963 1.5408 .9223 TREE 0.6622 0.5833 0.6096 5 Rank 4 5 Table 3: ParSS training results of top 5 RTM systems with further optimization. 2.1 Task 1: Paraphrase and Semantic Similarity in Twitter (ParSS) ParSS contains sentences provided by Twitter 3 (Xu et al., 2015). Official evaluation metric is Pearson’s correlation score, which we use to select the top systems on the training set. RTM-DCU results on the ParSS test set are given in Table 2. The setting R using SVR becomes 2nd out of 13 systems and 3rd out of 25 submissions. Looking at MAE and MAER allows us to obtain explanations to train and test performance differences for example without knowing their target distribution. Even though MAE of PLS-SVR is about %5 smaller on the ParSS test set, MAER is %55 smaller due to test set containing fewer zero entries (%16 vs. %39 on the train set). Lower test M</context>
</contexts>
<marker>Xu, Callison-Burch, Dolan, 2015</marker>
<rawString>Wei Xu, Chris Callison-Burch, and William B. Dolan. 2015. SemEval-2015 Task 1: Paraphrase and semantic similarity in Twitter (PIT). In Proc. of the 9th International Workshop on Semantic Evaluation (SemEval), Denver, Colorado, USA, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>