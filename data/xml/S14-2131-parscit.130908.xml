<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9728895">
UNAL-NLP: Combining Soft Cardinality Features for Semantic
Textual Similarity, Relatedness and Entailment
</title>
<note confidence="0.8260614">
Sergio Jimenez, George Due˜nas,
and Julia Baquero
Universidad Nacional de Colombia
Ciudad Universitaria, edificio 453,
oficina 114, Bogot´a, Colombia
</note>
<email confidence="0.6694465">
[sgjimenezv,geduenasl,
jmbaquerov]@unal.edu.co
</email>
<sectionHeader confidence="0.99383" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999929714285714">
This paper describes our participation in
the SemEval-2014 tasks 1, 3 and 10. We
used an uniform approach for addressing
all the tasks using the soft cardinality for
extracting features from text pairs, and
machine learning for predicting the gold
standards. Our submitted systems ranked
among the top systems in all the task and
sub-tasks in which we participated. These
results confirm the results obtained in pre-
vious SemEval campaigns suggesting that
the soft cardinality is a simple and useful
tool for addressing a wide range of natural
language processing problems.
</bodyText>
<sectionHeader confidence="0.99843" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99998365">
The semantic textual similarity is a core prob-
lem in the computational linguistic field. Con-
sequently, the previous evaluation campaigns of
this task in SemEval have attracted the attention
of many research groups worldwide (Agirre et al.,
2012; Agirre et al., 2013).This year, 3 tasks related
to this problem have been proposed exploring dif-
ferent facets such as semantic relatedness, entail-
ment , multilingualism, lack of training data and
imbalance in the amount of information.
The soft cardinality (Jimenez et al., 2010) is a
simple concept that generalizes the classical set
cardinality by considering the similarities among
the elements in a collection for a more intuitive
quantification of the number of elements in that
collection. This approach can be applied to text
applications representing texts as collections of
words and providing a similarity function that
compares two words. Varying this word-to-word
similarity function the soft cardinality can reflect
</bodyText>
<footnote confidence="0.8540475">
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
</footnote>
<note confidence="0.970254142857143">
Alexander Gelbukh
Center for Computing Research (CIC),
Instituto Polit´ecnico Nacional (IPN),
Av. Juan Dios B´atiz, Av. Mendiz´abal,
Col. Nueva Industrial Vallejo,
Mexico City, Mexico
www.gelbukh.com
</note>
<bodyText confidence="0.9979294">
notions of syntactic similarity, semantic related-
ness, among others. We (and others) have used
this approach to address with success the semantic
textual similarity and other tasks in previous Se-
mEval editions (Jimenez et al., 2012b; Jimenez et
al., 2012a; Jimenez et al., 2013a; Jimenez et al.,
2013b; Jimenez et al., 2013c; Croce et al., 2013).
In this paper we describe our participating sys-
tems in the SemEval-2014 tasks 1, 3, and 10,
which used the soft cardinality as core approach.
</bodyText>
<sectionHeader confidence="0.993057" genericHeader="method">
2 Features from Soft Cardinalities
</sectionHeader>
<bodyText confidence="0.999961782608696">
The cardinality of a collection of elements is the
counting of non-repeated elements in it. This def-
inition is intrinsically associated with the notion
of set, which is a collection of non-repeated ele-
ments.Thus, the cardinality of a collection or set
A is denoted as |A|. Clearly, the cardinality of a
collection with repeated elements treats groups of
identical elements as a single instance contribut-
ing only with a unit (1) to the element counting.
Jimenez et al. (2010) proposed the soft cardinal-
ity that uses a notion of similarity among elements
for grouping not only identical elements but simi-
lar too. That notion of similarity among elements
is provided by a similarity function that compares
two elements ai and aj and returns a score in [0,1]
interval, having sim(ai, ai) = 1. Although, it
is not necessary that sim fulfills another metric
properties aside of identity, symmetry is also de-
sirable. Thus, the soft cardinality of a collection
A, whose elements a1, a2, ... , a|A |are compara-
ble with a similarity function sim(ai, aj), is de-
noted as |A|sim. This soft cardinality is given by
the following expression:
</bodyText>
<equation confidence="0.9768325">
(1)
E|A|j=1 sim(ai, aj)p
</equation>
<bodyText confidence="0.8112565">
It is trivial to see that |A |= |A|sim either if
p — oc or when the function sim is a crisp com-
</bodyText>
<equation confidence="0.95438375">
� |A|
|A|sim =
i=1
wai
</equation>
<page confidence="0.961951">
732
</page>
<note confidence="0.784111">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 732–742,
Dublin, Ireland, August 23-24, 2014.
Basic Derived
|A ||A n B |= |A |+ |B |− |A U B|
|B ||A o B |= |A U B |− |A n B||
|A U B ||A \ B |= |A |− |A n B|
|B \ A |= |B |− |A n B|
</note>
<tableCaption confidence="0.842244">
Table 1: The 7 basic and derived cardinalities for
two sets comparison.
</tableCaption>
<bodyText confidence="0.998795571428571">
parator, i.e. one that returns 1 for identical ele-
ments and 0 otherwise. This property shows that
the soft cardinality generalizes the classical cardi-
nality and that the parameter p controls its degree
of “softness”, whose default value is 1. The values
waz are optional “importance” weights associated
with each element ai, by default those weights can
be assigned to 1.
For the tasks at hand, we represent each short
text (lets say A) as a collection of words ai and
the sim function can be any operator that com-
pares pairs of words. The motivation for using the
soft cardinality is that the sim function can reflect
any dimension of word similarity (e.g. syntactic,
semantic) and the soft cardinality projects that no-
tion at sentence level. For instance, if sim pro-
vides the degree of semantic relatedness between
two words using WordNet, two texts A and B
could be compared by computing |A|sim, |B|sim
and |A U B|sim. Given that A n B could be empty,
the soft cardinality of the intersection must be ap-
proximated by |A n B|sim ^ |A|sim + |B|sim −
|A U B|sim instead of being computed directly
from A n B using equation 1. Using that approx-
imation, the commonality (intersection) between
A and B is induced by the pair-wise similarities
provided by sim among the words in A and B.
Since more than a century when Jaccard (1901)
proposed his well-known index, the classical set
cardinality has been used to build similarity func-
tions for set comparison. Any binary-cardinality-
based similarity function is an algebraic combina-
tion of |A|, |B |and either |A n B |or |A U B|
(e.g. Jaccard, Dice, Tversky, overlap and cosine
indexes). These three cardinalities describes un-
ambiguously all the regions in the Venn’s diagram
when comparing two sets. Thus, in this scenario 4
possible cardinalities can be derived from these 3
basic cardinalities, see Table 1. Clearly, the same
set of cardinalities can be obtained for the soft car-
dinality.
When training data is available, which is the
</bodyText>
<figure confidence="0.987202454545454">
# Feature expression
1 |A|/|AUB|
2 |A|−|AnB|/|A|
3 |A|−|AnB|/|AUB|
4 |B|/|AUB|
5 |B|−|AnB|/|B|
6 |B|−|AnB|/|AUB|
7 |AnB|/|A|
8 |AnB|/|B|
9 |AnB|/|AUB|
10 |AUB|−|AnB|/|AnB|
</figure>
<tableCaption confidence="0.993141">
Table 2: Extended set of 10 rational features.
</tableCaption>
<bodyText confidence="0.999972864864865">
case for tasks 1, 3 and 10 in SemEval 2014, it
is possible to think that instead of using an ad-
hoc expression (e.g. Jaccard, Dice) the similar-
ity function can be obtained using the cardinalities
in Table 1 as features for a machine-learning re-
gression algorithm. Our hypothesis is that such
learnt function should predict in a more accurate
way the gold standard variable than any other ad-
hoc function. However, these cardinality features
are intrinsically correlated with the length of the
texts where they were obtained. This correlation
makes that the performance of the learnt similar-
ity function could be dependent of the length of
the texts. For instance, if the function was trained
using long texts it is plausible to think that this
function would be more effective when tested with
long texts than with shorter ones. Having this in
mind, an extended set of rational features is pro-
posed, whose values are standardized in [0,1] in-
terval aiming to reduce the effect of the length of
the texts. These features are presented in Table 2.
The soft cardinality has proven to overcome
the classic cardinality in the semantic textual
similarity (STS) task in previous SemEval cam-
paigns (Jimenez et al., 2012b; Jimenez et al.,
2013a). Even using a simplistic function sim
based on q-grams of characters, the soft cardinal-
ity method ranked third among 89 participating
systems (Agirre et al., 2012). Thus, our participat-
ing systems in the SemEval 2014 campaign were
based on the previously described set of 17 fea-
tures, obtained from the soft cardinality with dif-
ferent sim functions for comparing pairs of words.
Each sim function produced a different set of fea-
tures, which were combined with a regression al-
gorithm for similarity and relatedness tasks. Sim-
ilarly, a classification algorithm was used for the
</bodyText>
<page confidence="0.992857">
733
</page>
<figure confidence="0.653399">
entailment task. pairs of words w1 and w2, represented each one as
a collection of 3-grams of characters, is given by
3 Systems Description the following expression:
</figure>
<bodyText confidence="0.9998958">
In this section the different feature sets used for
each submitted system to the different task and
subtask are described. Besides, the data used for
training, parameters and other preprocessing de-
tails are described for each system.
</bodyText>
<subsectionHeader confidence="0.9994565">
3.1 Task 1: Textual Relatedness and
Entailment
</subsectionHeader>
<bodyText confidence="0.999977227272727">
The task 1 is based on the SICK (Sentences
Involving Compositional Knowledge) data set
(Marelli et al., 2014), which contains nearly
10,000 pairs of sentences manually labeled by re-
latedness and entailment. The relatedness gold la-
bels range from 1 to 5, having 1 the minimum level
of relatedness between the texts and 5 for the max-
imum. The entailment labels have three categori-
cal values: neutral, contradiction and entailment.
The two sub tasks consist of predicting the related-
ness and entailment gold standards using approxi-
mately the 50% of the text pairs as training and the
other part as test bed.
Our overall approach consists in extracting 4
different sets of features using the method pre-
sented in section 2 and training a machine learn-
ing algorithm for predicting the gold standard la-
bels in the test data. Each feature set is described
in the following 4 subsections and the subsection
3.1.6 provides details of the used combination of
features, machine learning algorithm and prepro-
cessing details.
</bodyText>
<subsectionHeader confidence="0.905313">
3.1.1 String-Matching Features
</subsectionHeader>
<bodyText confidence="0.9999464">
First, all texts in the SICK data set where prepro-
cessed by lower casing, tokenizing and stop-word
removal (using the NLTK1). Then each word was
reduced to its stem using the Porter’s algorithm
(Porter, 1980) and a idf weight (Jones, 2004) was
associated to each stem (wai weights in eq. 1) us-
ing the very SICK data set as document collec-
tion. Next, for each instance in the data, which
is composed of two texts A and B, the 17 fea-
tures listed in Tables 1 and 2 where extracted using
eq.1. The used word-to-word similarity function
sim decomposes each word in bags of 3-grams
of characters, which are compared using the sym-
metrical Tversky’s index (Tversky, 1977; Jimenez
et al., 2013a). Thus, the similarity between two
</bodyText>
<footnote confidence="0.536614">
1http://www.nltk.org/
</footnote>
<equation confidence="0.997403857142857">
|c|
sim(w1, w2) =
β(α|wmin |+ (1 − α)|wmax|) + |c|
(2)
|c |= |w1 n w2 |+ biassim,
|wmin |= min[|w1 \ w2|, |w2 \ w1|],
|wmax |= max[|w1 \ w2|, |w2 \ w1|].
</equation>
<bodyText confidence="0.997578206896552">
The values used for the parameters were α =
1.9, β = 2.36, bias = −0.97, and p = 0.39
(wherep corresponds to eq.1). The motivation and
justification for these parameters can be found in
(Jimenez et al., 2013a). These values were ob-
tained by building a text similarity function us-
ing the Dice’s coefficient and the soft cardinali-
ties plugging eq.2 in eq.1. Next, this text similar-
ity function is evaluated in the 5,000 training text
pairs and the obtained scores are compared against
the relatedness gold-standard using the Pearson’s
correlation.
waiare not training parameters, but they are
weights associated with the words. These weights
could have been obtained from a larger corpus,
but we use the training texts to obtain them. This
process is repeated iteratively exploring the search
space defined by these 4 parameters using a hill-
climbing approach until a maximum correlation is
reached. We observe that the optimal values of the
parameters p, α, β, and bias vary considerably be-
tween the data sets and for the different sim func-
tions of word-to-word similarity. We do not yet
understand from which factors of the data and the
sim functions depend on these parameters. This
issue will be the objective of further research.
Henceforth, the set of 17 string-based features
described in this subsection will be referred as
SM.
</bodyText>
<subsectionHeader confidence="0.552395">
3.1.2 ESA Features
</subsectionHeader>
<bodyText confidence="0.999983777777778">
For this set of features we used the idea proposed
by Gabrilovich and Markovitch (2007) of enrich-
ing the representation of a text by representing
each word by its textual definition in a knowl-
edge base, i.e. explicit semantic analysis (ESA).
For that, we used as knowledge base the synset’s
textual definitions provided by WordNet. First,
in order to determine the textual definition asso-
ciated to each word, the texts were tagged using
</bodyText>
<page confidence="0.989392">
734
</page>
<bodyText confidence="0.999388041666667">
the maximum entropy POS tagger included in the
NLTK. Next, the adapted Lesk algorithm (Baner-
jee and Pedersen, 2002) for word sense disam-
biguation was applied in the texts disambiguating
one word at the time. The software package used
for this disambiguation process was pywsd2. The
arguments needed for the disambiguation of each
word are the POS tag of the target word and the
entire sentence as context. Once all the words are
disambiguated with their corresponding WordNet
synsets, each word is replaced by all the words in
their textual definition jointly with the same word
and its lemma. The final result of this stage is that
each text in the data set is replaced by a longer
text including the original text and some related
words. The motivation of this procedure is that the
extended versions of each pair of texts have more
chance of sharing common words that the original
texts.
The extended versions of these texts were used
to obtain another 17 features with the same proce-
dure described in the previous subsection (3.1.1).
This feature subset will henceforth be referred as
ESA.
</bodyText>
<sectionHeader confidence="0.9541115" genericHeader="method">
3.1.3 Features for each part-of-speech
category
</sectionHeader>
<bodyText confidence="0.999779956521739">
This set of features is motivated by the idea pro-
posed by Corley and Mihalcea (2005) of group-
ing words by their POS category before being
compared for semantic textual similarity. Our ap-
proach consist in provide a version of each text
pair in the data set for each POS category in-
cluding only the words belonging to that cate-
gory. For instance, the pair of texts {“A beauti-
ful girl is playing tennis”, “A nice and handsome
boy is playing football”} produce new pairs such
as: {“beautiful”, “nice handsome”} for the ADJ
tag, {“girl tennis”, “boy football”} for NOUN and
{“is playing”, “is playing”} for VERB.
Again, the POS tags were provided by the
NLTK’s max entropy tagger. The 28 POS cate-
gories were simplified to 9 categories in order to
avoid an excessive number of features and hence
sparseness; the used mapping is shown in Table 3.
Next, for each one of the 9 new POS categories a
set of 17 features (SM) is extracted reusing again
the method proposed in subsection 3.1.1. The only
difference with the method described in that sub-
section is that the stop-words were not removed
</bodyText>
<footnote confidence="0.944223">
2https://github.com/alvations/pywsd
</footnote>
<table confidence="0.996768">
Reduced tag set NLTK’s POS tag set
ADJ JJ,JJR,JJS
NOUN NN,NNP,NNPS,NNS
ADV RB,RBR,RBS,WRB
VERB VB,VBD,VBG,VBN,VBP,VBZ
PRO WP,WP$,PRP,PRP$
PREP RP,IN
DET PDT,DT,WDT
EX EX
CC CC
</table>
<tableCaption confidence="0.999857">
Table 3: Mapping reduction of the POS tag set.
</tableCaption>
<bodyText confidence="0.999934692307692">
and the stemming process was not performed. The
motivation for generating this feature sets by POS
category is that the machine learning algorithms
could weight differently each category. The intu-
ition behind this is that it is reasonable that cat-
egories such as VERB and NOUN could play a
more important role for the task at hand than oth-
ers such as ADJ or PREP. Using these categorized
features, such discrimination among POS cate-
gories can be discovered from the training data.
Finally, the total number of features in this set is
153 (17 features x 9 POS categories). This feature
set will be referred as POS.
</bodyText>
<subsectionHeader confidence="0.708758">
3.1.4 Features From Dependencies
</subsectionHeader>
<bodyText confidence="0.999885727272727">
The syntactic soft cardinality (Croce et al., 2012;
Croce et al., 2013) extend the soft cardinality
approach by representing texts as bags of de-
pendencies instead of bags of words. Each de-
pendency is a 3-tuple composed of two syntac-
tically related words and the type of their rela-
tionship. For instance, the sentence “The boy
plays football” can be represented with 3 depen-
dencies: [det,“boy”,“The”], [subj,“plays”,“boy”]
and [obj,“plays”,“football”]. Clearly, this repre-
sentation distinguish pairs of texts such as {“The
dog bites a boy”,“The boy bites a dog”}, which
are indistinguishable when they are represented as
bags of words. This representation can be obtained
automatically using the Stanford Parser (De Marn-
effe et al., 2006), which in addition provides a de-
pendency identifying the root word in a sentence.
We used the version 3.3.13 of that parser to obtain
such representation.
Once the texts are represented as bags of de-
pendencies, it is necessary to provide a similar-
ity function between two dependency tuples in or-
</bodyText>
<footnote confidence="0.953008">
3http://nlp.stanford.edu/software/lex-parser.shtml
</footnote>
<page confidence="0.99587">
735
</page>
<bodyText confidence="0.999946533333333">
der to use the soft cardinality (eq. 1) and hence
to obtain the 17 cardinality features in Tables 1
and 2. Such function can be obtained using the
sim function (eq. 2) for comparing the first and
second words between the dependencies and even
the labels of the dependency types. Let’s consider
two dependencies tuples d = [ddep, dw1, dw2] and
p = [pdep, pw1, pw2] where ddep and pdep are the
labels of the dependency type; dw1 and pw1 are
the first words on each dependency tuple; and dw2
and pw2 are the second words. The similarity func-
tion for comparing two dependency tuples can be a
linear combination of the sim scores between the
corresponding elements of the dependency tuples
by the following expression:
</bodyText>
<equation confidence="0.9975225">
simdep(d, P) =
γsim(ddep, Pdep) + δsim(dw1, Pw2) + Asim(dw2, Pw2)
</equation>
<bodyText confidence="0.997755263157895">
Although, it is unusual to compare the depen-
dencies’ type labels ddep and pdep with a similar-
ity function designed for words, we observed ex-
perimentally that this approach yield better overall
performance in the relatedness task in comparison
with a simple crisp comparison. The optimal val-
ues for the parameters -y = −3, 6 = 10 and λ = 3
were determined with the same methodology used
in subsection 3.1.1 for determining α, Q and bias.
Clearly, the fact that 6 &gt; λ means that the first
words in the dependency tuples plays a more im-
portant role than the second ones for the task at
hand. However, the fact that -y &lt; 0 is counter intu-
itive because it means that the lower the similarity
between the dependency type labels is, the larger
the similarity between the two dependencies. Up
to date we have been unable to find a plausible ex-
planation for this phenomenon. This set of 17 fea-
tures will be referred hereinafter as DEP.
</bodyText>
<subsectionHeader confidence="0.866941">
3.1.5 Additional Features
</subsectionHeader>
<bodyText confidence="0.999852641509434">
In addition to the feature sets based in soft car-
dinality, we designed some features aimed to ad-
dress linguistic phenomena such as antonymy, hy-
pernymy and negation.
Antonymy: Consider the following text pair
from the test data {“A man is emptying a container
made of plastic”,“A man is filling a container
made of plastic” }, which is labeled as a contra-
diction with a relatedness score of 3.91. Clearly,
these labels are explained by the antonymy rela-
tion between “emptying” and “filling”. Given that
none of the features presented above address this
issue, a list of 11,028 pairs of antonym words was
gathered from several web sites (see Table 4) and
from the antonymy relationships in WordNet, in
order to detect these cases. That list was used to
count the number of occurrences of pairs antonym
words between pairs of texts and in each one of
the texts. Thus, for any pair of texts A and B (rep-
resented as sets of words), three features (referred
henceforth as ANT) were extracted:
antonym AB Counts the number of occurrences
of pairs of antonyms in A x B (Cartesian
product) or in B x A .
antonym AA Counts the number of occurrences
of pairs of antonyms in A x A.
antonym BB Counts the number of occurrences
of pairs of antonyms in B x B.
Hypernymy: Consider the following text pair
from the test data {“A man is sitting comfortably
at a table”,“A person is sitting comfortably at the
table” }, which is labeled as an entailment with
a relatedness score of 3.96. In this case, the en-
tailment is based on the hypernymy between “per-
son” and “man”. In order to capture this linguis-
tic factor 3 features similar to the previously de-
scribed antonym features were proposed. First,
word sense disambiguation was performed (as de-
scribed in subsection 3.1.2) for obtaining a synset
label for each word. Secondly, we build a bi-
nary function hyp(ss1, ss2) that takes two Word-
Net synsets as arguments and returns 1 if ss1 is
a hypernym of ss2 with a maximum depth in the
WordNet’s is-a hierarchy of 6 steps, and 0 oth-
erwise. This hypernymy function was build us-
ing the WordNet interface provided by the NLTK.
Next, based on that synset-to-synset function, a
text-to-text function that captures the degree or hy-
pernymy in a text or in a pair of texts was build us-
ing the Monge-Elkan measure (Monge and Elkan,
1996). Thus, for two texts A and B represented
as sets of synset labels, the following expression
measures their degree of hypernymy:
</bodyText>
<equation confidence="0.99104">
|A|
HY P(A, B) = 1 max hyp(ai, bj)
�A� j=1
i=1
</equation>
<bodyText confidence="0.84482225">
Using the function HY P(*, *), 3 features are
extracted from each pair of text (referred hence-
forth as HYP):
hypernym AB from HYP(A, B)
</bodyText>
<page confidence="0.919001">
736
</page>
<figure confidence="0.995804727272727">
http://www.myenglishpages.com/site php files/vocabulary-lesson-opposites-adjectives.php
http://www.allaboutspace.com/wordlist/opposites.shtml
http://www.michigan-proficiency-exams.com/antonym-list.html
http://examples.yourdictionary.com/examples-of-antonyms.html
http://www.synonyms-antonyms.com/antonyms.html
http://englishwilleasy.com/word-must-know/vocabulary/vocabulary-list-by-opposites-or-antonyms/
http://www.meridianschools.org/staff/districtcurriculum/moreresources/languagearts/all grades/antonyms.doc
http://mrsbrower.weebly.com/uploads/1/3/2/4/13243672/antonymlist.pdf
https://foxhugh.wordpress.com/word-lists/list-of-antonyms/
http://www.paulnoll.com/Books/Clear-English/English-antonyms-1.html
http://wordnet.princeton.edu/wordnet/download/
</figure>
<tableCaption confidence="0.981578">
Table 4: URLs used for the list of 11,028 antonym pairs (accessed on March 20, 2014).
</tableCaption>
<construct confidence="0.2593095">
hypernym AA from HYP(A, A)
hypernym BB from HYP(B, B)
</construct>
<bodyText confidence="0.9925500625">
Negation: Negations play an important role in
the task at hand. For instance, consider this pair
of texts {“A person is rinsing a steak with wa-
ter”,“A man is not rinsing a large steak”} labeled
as a contradiction. In that example the negation of
the verb “rising” is the main factor of contradic-
tion. In order to capture this linguistic feature we
build a simple function that detects the occurrence
of a verb negation if the text contains one of the
following words: “not”, “n’t”, “nor”, “null”, “nei-
ther”, “either”, “barely”, “scarcely” and “hardly”.
Similarly, noun negation is detected looking for
the words: “no”, “none”, “nobody”, “nowhere”,
“nothing” and “never”. Thus, for two texts A and
B, 4 features are extracted (referred henceforth as
NEG):
</bodyText>
<listItem confidence="0.803964875">
verb neg A if verb negation is detected in A
verb neg B if verb negation is detected in B
noun neg A if noun negation is detected in A
noun neg B if noun negation is detected in B
3.1.6 Submitted Runs and Results
RUN1 (PRIMARY) This system produced pre-
dictions by extracting all the features described
previously (SM, ESA, POS, DEP, ANT,
</listItem>
<bodyText confidence="0.994162930232558">
HYP and NEG) from all the texts in the SICK
data set. Next, two machine learning models were
obtained (WEKA (Hall et al., 2009) was used
for that) using the training part of SICK, one for
regression (relatedness) and another for classifi-
cation (entailment). The regression model was
a reduced-error pruning tree (REPtree) (Quin-
lan, 1987) boosted with 20 iterations of bagging
(Breiman, 1996). The classification model was a
J48Graft tree also boosted with 20 bagging itera-
tions. These two models produced the predictions
for the test part of SICK.
RUN2 This system is similar to the one used
in RUN1, but it used only the feature sets SM and
NEG. Another difference is that a linear regres-
sion was used instead of the REPtree and no bag-
ging was performed.
RUN3 The same as RUN1, but again, linear
regression was used instead of the REPtree and no
bagging was performed.
RUN4 The same as RUN2, but the models
were boosted with 20 iterations of bagging.
RUN5 The same as RUN3, but 30 iterations of
bagging were used instead of 20.
The official results obtained by these systems
(prefixed UNAL-NLP) are shown in Table 5
jointly with those obtained by other 3 top sys-
tems among the 18 participating systems. Our
primary run (RUN1) obtained pretty competitive
results ranking 3th and 4th in the entailment and
relatedness tasks. The RUN4 obtained a remark-
able performance (it would be ranked 6th for en-
tailment and 8th for relatedness) in spite of the
fact that is a system purely based on string match-
ing. The comparison of our runs 1, 3 and 5, which
mainly differs by the use of bagging, shows that
this boosting method provides considerable im-
provements. In fact, comparing RUN3 (all fea-
tures, no bagging) and RUN4 (SM and NEG fea-
ture sets boosted with bagging), they performed
similarly in spite of the considerable larger num-
ber of features used in RUN3. Besides, the RUN5
slightly outperformed our primary run (RUN1) us-
</bodyText>
<page confidence="0.993949">
737
</page>
<table confidence="0.9993654">
Entailment Relatedness
system accuracy official rank Pearson Spearman MSE official rank
UNAL-NLP run1 (primary) 83.05% 3rd/18 0.8043 0.7458 0.3593 4th/17
UNAL-NLP run2 79.81% - 0.7482 0.7033 0.4487 -
UNAL-NLP run3 80.15% - 0.7747 0.7286 0.4081 -
UNAL-NLP run4 80.21% - 0.7662 0.7142 0.4210 -
UNAL-NLP run5 83.24% - 0.8070 0.7489 0.3550 -
ECNU run1 83.64% 2nd/18 0.8280 0.7689 0.3250 1st/17
Stanford run5 74.49% 12th/18 0.8272 0.7559 0.3230 2nd/17
Illinois-LH run1 84.58% 1st/18 0.7993 0.7538 0.3692 5th/17
</table>
<tableCaption confidence="0.993032">
Table 5: Results for task 1.
</tableCaption>
<bodyText confidence="0.837453">
ing 10 additional iterations of bagging.
</bodyText>
<sectionHeader confidence="0.774055" genericHeader="method">
3.1.7 Error Analysis
</sectionHeader>
<bodyText confidence="0.999082388888889">
Our primary run for the task 1 failed in 835 pairs of
sentences out of 4,927 in the entailment subtask.
We wanted to understand in why our system failed
in these 835 instances, so we classified manually
these instances in 4 error categories (each instance
could be assigned to several categories).
Paraphrase not detected (NP): exam-
ple={“Two groups ofpeople are playing football”,
“Two teams are competing in a football match”},
gold standard=entailment, prediction=neutral,
number of occurrences= 420 (50.3%). The system
failed to detect the paraphrase between “groups of
people” and “teams”.
Negation not detected (NN) : exam-
ple={“There is no one playing the guitar”,
“Someone is playing the guitar”}, gold stan-
dard=contradiction, prediction=neutral, number
of occurrences=94 (11.3%). The system failed to
detect that the contradiction is due to the negation
in the first text.
False similarity between words (NSS) : ex-
ample={“Two dogs are playing by a tree”,
“Two dogs are sleeping by a tree”}, gold stan-
dard=neutral, prediction=entailment, number of
occurrences=413 (49.5%). The only difference
between these 2 sentences is the gerund “playing”
vs. “sleeping”, which the system erroneously con-
sidered as similar.
Antonym not detected (NA): exam-
ple={“Three children are running down hill”,
“Three children are running up hill”}, gold
standard=contradiction, prediction=entailment,
number of occurrences=40 (4.8%). The only
difference between these 2 sentences is the
words “down” vs. “up”. In spite that this pair
of antonyms was included in the antonym list,
</bodyText>
<table confidence="0.997545">
Error category NP NN NSS NA
NP 420 5 125 0
NN - 94 1 0
NSS - - 413 22
NA - - - 40
</table>
<tableCaption confidence="0.873962">
Table 6: Co-ocurrences of types of errors in RUN1
(task1).
</tableCaption>
<bodyText confidence="0.9880372">
the system failed to distinguish the contradiction
between the texts.
The matrix in Table 6 reports the number of
co-occurrences of error categories in the 835 in-
stances erroneously classified.
</bodyText>
<subsectionHeader confidence="0.999235">
3.2 Task 3: Cross-level Semantic Similarity
</subsectionHeader>
<bodyText confidence="0.999989153846154">
The SemEval 2014 task 3 (cross-level semantic
similarity) (Jurgens et al., 2014) proposed the se-
mantic textual similarity task but across differ-
ent textual levels, namely paragraph-to-sentence,
sentence-to-phrase, phrase-to-word and word-to-
sense. As usual, the goal is to predict the gold sim-
ilarity scores for each pair of texts. For each one
of these cross-level comparison types there were
proposed a separated training and test data sets.
Basically, we addressed this task using the set of
features SM presented in subsection 3.1.1 in com-
bination with a text expansion approach similar to
the method presented in subsection 3.1.2.
</bodyText>
<subsectionHeader confidence="0.801888">
3.2.1 Paragraph-to-sentence and
Sentence-to-phrase
</subsectionHeader>
<bodyText confidence="0.999792">
For these two cross-level comparison types we
extracted the SM feature set using the pro-
vided texts. The model parameters obtained for
paragraph-to-sentence were α = 0.1, Q = 1.75,
bias = −1.35, p = 1.55; and for sentence-to-
phrase were α = 0.68, Q = 0.92, bias = −0.92,
p = 2.49.
</bodyText>
<page confidence="0.99493">
738
</page>
<bodyText confidence="0.999254058823529">
The system for the RUN2 used the SM fea-
ture set and a machine learning model build with
the provided training data for generating the simi-
larity score predictions for the test data. For the
paragraph-to-sentence data set the model was a
REPtree for regression boosted with 40 bagging it-
erations. Similarly, the model for the sentence-to-
phrase data set was a linear regressor also boosted
with 40 bagging iterations.
Unlike RUN2, RUN1 does not make use of any
machine learning algorithm. Instead, we used the
only the basic cardinalities (see Table 1) from the
SM feature set in combination with an ad-hoc re-
semblance coefficient, i.e. the Dice’s coefficient
2|A∩B|/|A|+|B |for the paragraph-to-sentence data
set. In turn, for sentence-to-phrase the overlap co-
efficient, i.e. |A∩B|/min[|A|,|B|], was used.
</bodyText>
<subsubsectionHeader confidence="0.67261">
3.2.2 Phrase-to-word and Word-to-sense
</subsubsectionHeader>
<bodyText confidence="0.999254962962963">
Before applying the same procedure used in the
previous subsection, the texts in the phrase-to-
word and word-to-sense data sets were expanded
with a similar approach to that was used in subsec-
tion 3.1.2.
Phrase-to-word expansion: First, the “word”
was expanded finding its corresponding WordNet
synset using the adapted Lesk’s algorithm provid-
ing as context the “phrase”. Then, once the word’s
synset is obtained, the “word” text is extended
with the textual definition of the synset. Simi-
larity, this procedure is repeated for each word in
the “phase” obtaining and extended version of the
phrase. Finally, these two texts are used for ex-
tracting the SM feature set. The model param-
eters were α = 0.8, 0 = 1.9, bias = −0.8,
p = 1.5.
Word-to-sense expansion: First, the “sense”
(i.e. synset) is replaced by its textual definition
and its lemma. At this point the pair word-sense
becomes a pair word-sentence. Then, the synset
of the “word” is obtained performing the adapted
Lesk’s algorithm. Next, the “word” is extended
with textual definition of the synset. Finally, these
two texts are used for extracting the SM feature
set obtaining the following model parameters were
α = 0.59, 0 = 0.9, bias = −0.89, p = 3.91.
</bodyText>
<sectionHeader confidence="0.574051" genericHeader="evaluation">
3.2.3 Results
</sectionHeader>
<bodyText confidence="0.98704275">
The official results obtained by the two submitted
runs jointly with other 3 top systems are shown in
Table 7. Our submissions (prefixed with UNAL-
NLP) ranked 3rd and 5th among 38 participating
</bodyText>
<table confidence="0.999854363636364">
test data train data
OnWN (en) OnWN 2012/2013 test
headlines (en) headlines 2013 test
images (en) MSRvid 2012 train and test
deft-news (en) MSRpar 2013 train and test
deft-forum (en) MSRvid 2012 train and test
OnWN 2012/2013 test
tweet-news (en) SMTeuroparl 2012 test
SMTnews 2012 test
Wikipedia (es) SMTeuroparl 2012 train
news (es) SMTeuroparl 2012 train
</table>
<tableCaption confidence="0.9670505">
Table 8: Training data used for the STS-2014 data
sets (task 10).
</tableCaption>
<bodyText confidence="0.999633">
systems, showing that the SM (string-matching)
feature set is effective for the prediction of sim-
ilarity scores. Particularly, in the paragraph-to-
sentence data set, which has the longest text,
RUN2 obtained the best official score. In contrast,
the scores obtained for the phrase-to-word and
word-to-sense data sets were considerably lower
in comparison with the top system, but still com-
petitive against most of the other participating sys-
tems.
</bodyText>
<subsectionHeader confidence="0.9986615">
3.3 Task 10: Multilingual Semantic
Similarity
</subsectionHeader>
<bodyText confidence="0.999981571428571">
The SemEval-2014 task 10 (multilingual seman-
tic similarity) (Agirre et al., 2014) is the sequel of
the semantic textual similarity (STS) evaluations
at SemEval in the past two years (Agirre et al.,
2012; Agirre et al., 2013). This year 6 test data
sets were proposed in English and 2 data sets in
Spanish. Similarly to the 2013 campaign, there is
not explicit training data for each data set. Conse-
quently, different data sets from the previous STS
evaluations were selected to be used as training
data for the new data sets. The selection criterion
was the average character length and type of the
texts. The Table 8 shows the training data used for
each test data set.
</bodyText>
<subsectionHeader confidence="0.651895">
3.3.1 English Subtask
</subsectionHeader>
<bodyText confidence="0.99933575">
The RUN1 for the English data sets was produced
with a parameterized similarity function based on
the SM feature set and the symmetrized Tversky’s
index (Tversky, 1977; Jimenez et al., 2013a). For
a detailed description of this function and its pa-
rameters, please refer to the STSsim feature in
the system description paper of the NTNU team
(Lynum et al., 2014). The parameters used in that
</bodyText>
<page confidence="0.994152">
739
</page>
<table confidence="0.999815333333333">
System Para-2-Sent Sent-2-Phr Phr-2-Word Word-2-Sense Official Rank
SimCompass run1 0.811 0.742 0.415 0.356 1st/38
ECNU run1 0.834 0.771 0.315 0.269 2nd/38
UNAL-NLP run2 0.837 0.738 0.274 0.256 3rd/38
SemantiKLUE run1 0.817 0.754 0.215 0.314 4th/38
UNAL-NLP run1 0.817 0.739 0.252 0.249 5th/38
</table>
<tableCaption confidence="0.884056">
Table 7: Official results for task 3 (Pearson’s correlation).
</tableCaption>
<table confidence="0.999881428571428">
Data α β bias p α&apos; β&apos; bias&apos;
OnWN 0.53 -0.53 1.01 1.00 -4.89 0.52 0.46
headlines 0.36 -0.29 4.17 0.85 -4.50 0.43 0.19
images 1.12 -1.11 0.93 0.64 -0.98 0.50 0.11
deft-news 3.36 -0.64 1.37 0.44 2.36 0.72 0.02
deft-forum 1.01 -1.01 0.24 0.93 -2.71 0.42 1.63
tweet-news 0.13 0.14 2.80 0.01 2.66 1.74 0.45
</table>
<tableCaption confidence="0.859453">
Table 9: Optimal parameters used for task 10 in
English.
</tableCaption>
<bodyText confidence="0.998992142857143">
function are reported in Table 9. Unlike subsec-
tion 3.1.1 where the Dice’s coefficient was used as
the text similarity function, here the symmetrical
Tversky’s index (eq. 2) was reused generating the
three additional parameters marked with apostro-
phe (α&apos;, Q&apos; and bias&apos;).
For the RUN2 the SM feature set was extracted
from all the data sets in English (en) listed in Table
8. Then, a REPtree (Quinlan, 1987) boosted with
50 bagging iterations (Breiman, 1996) was trained
using the training data sets selected for each test
data set. Finally, these machine learning models
produced the similarity score predictions for each
test data set.
The RUN3 was identical to the RUN2 but in-
cluded additional feature sets apart from SM,
namely: ESA, POS and WN. The WN feature
set is the same as SM, but replacing the word-to-
word similarity function in eq. 2 by the path mea-
sure from the WordNet::Similarity package (Ped-
ersen et al., 2004).
</bodyText>
<subsectionHeader confidence="0.866953">
3.3.2 Spanish Subtask
</subsectionHeader>
<bodyText confidence="0.9999605">
The Spanish system was based entirely in the SM
feature set with some small changes for adapt-
ing the system to Spanish. Basically, the list of
English stop-words was replaced by the Spanish
stop-words provided by the NLTK. In addition,
the Porter stemmer was replaced by its Spanish
equivalent, i.e. the Snowball stemmer for Span-
ish. The RUN1 is equivalent to the RUN1 for the
</bodyText>
<table confidence="0.974575846153846">
data set run1 run2 run3
deft-forum 0.5043 0.3826 0.4607
deft-news 0.7205 0.7305 0.7216
headlines 0.7616 0.7645 0.7605
images 0.8071 0.7706 0.7782
OnWN 0.7823 0.8268 0.8426
tweet-news 0.6145 0.4028 0.6583
mean (en) 0.7113 0.6573 0.7209
official rank (en) 12th/38 22th/38 9th/38
Wikipedia 0.7804 0.7566 0.6894
news 0.8154 0.7829 0.7965
mean (es) 0.8013 0.7723 0.7533
official rank (es) 3rd/22 9th/22 12th/22
</table>
<tableCaption confidence="0.643688">
Table 10: Official results for the task 10 (Pearson’s
correlation).
</tableCaption>
<bodyText confidence="0.999474">
English subtask described in the previous subsec-
tion. The parameters used for the text similarity
function were α = 1.16, Q = 1.08, bias = 0.02,
p = 1.02, α&apos; = 1.54, Q&apos; = 0.08 and bias&apos; = 1.37.
The description and meaning of these parameters
can be found in (Lynum et al., 2014) associated to
the STSsim feature.
The RUN2 was obtained using the SM feature
set and a linear regressor for generating the simi-
larity score predictions. Similarity, RUN3 used the
same feature set SM in combination with a REP-
tree boosted with 30 bagging iterations.
</bodyText>
<sectionHeader confidence="0.901452" genericHeader="evaluation">
3.3.3 Results
</sectionHeader>
<bodyText confidence="0.999963555555556">
The results for the 3 submitted runs correspond-
ing to the 2 sub tasks (English and Spanish) are
shown in Table 10. It is important to note that
the RUN1 for the Wikipedia data set in Spanish
was the top system among 22 participating sys-
tems. This result is remarkable given that this sys-
tem was trained with a data set in English showing
the domain adaptation ability of the soft cardinal-
ity approach.
</bodyText>
<page confidence="0.99354">
740
</page>
<sectionHeader confidence="0.999452" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.9999934">
We participated in the SemEval-2014 task 1, 3 and
10 with an uniform approach based on soft cardi-
nality features, obtaining pretty satisfactory results
in all data sets, tasks and sub tasks. This approach
has been used since SemEval-2012 in all versions
of the following tasks: semantic textual similar-
ity (Jimenez et al., 2012b; Jimenez et al., 2013a),
typed similarity (Croce et al., 2013), cross-lingual
textual entailment (Jimenez et al., 2012a; Jimenez
et al., 2013c), student response analysis (Jimenez
et al., 2013b), and multilingual semantic textual
similarity (Lynum et al., 2014). In the majority
of the cases, the systems based on soft cardinality,
built by us and other teams, have been among the
top systems. Given the uniformity of the approach,
the consistency of the results, the few computa-
tional resources required and the overall concep-
tual simplicity, the soft cardinality is established
as a useful tool for a wide spectrum of applications
in natural language processing.
</bodyText>
<sectionHeader confidence="0.997046" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99967313253012">
Eneko Agirre, Daniel Cer, Mona Diab, and Gonzalez-
Agirre Aitor. 2012. SemEval-2012 task 6: A pilot
on semantic textual similarity. In Proceedings of the
6th International Workshop on Semantic Evaluation
(SemEval@*SEM 2012), Montreal,Canada.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic textual similarity, including a pilot
on typed-similarity. Atlanta, Georgia, USA.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Weibe. 2014. SemEval-2014 task 10: Multilingual
semantic textual similarity. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland, August.
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted lesk algorithm for word sense disambigua-
tion using WordNet. In Computational linguis-
tics and intelligent text processing, page 136–145.
Springer.
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123–140.
Courtney Corley and Rada Mihalcea. 2005. Measur-
ing the semantic similarity of texts. In Proceedings
of the ACL Workshop on Empirical Modeling of Se-
mantic Equivalence and Entailment, EMSEE ’05,
page 13–18, Stroudsburg, PA, USA.
Danilo Croce, Valerio Storch, P. Annesi, and Roberto
Basili. 2012. Distributional compositional seman-
tics and text similarity. In 2012 IEEE Sixth Interna-
tional Conference on Semantic Computing (ICSC),
pages 242–249, September.
Danilo Croce, Valerio Storch, and Roberto Basili.
2013. UNITOR-CORE TYPED: Combining text
similarity and semantic filters through SV regres-
sion. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 1: Pro-
ceedings of the Main Conference and the Shared
Task: SemanticTextual Similarity, page 59, Atlanta,
Georgia, USA.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, volume 6, page 449–454,
Genoa, Italy, May.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In Proceedings of
the 20th International Joint Conference on Artifical
Intelligence, IJCAI’07, page 1606–1611, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.
Mark Hall, Frank Eibe, Geoffrey Holmes, and Bern-
hard Pfahringer. 2009. The WEKA data min-
ing software: An update. SIGKDD Explorations,
11(1):10–18.
Paul Jaccard. 1901. Etude comparative de la distribu-
tion florare dans une portion des alpes et des jura.
Bulletin de la Soci´et´e Vaudoise des Sciences Na-
turelles, pages 547–579.
Sergio Jimenez, Fabio Gonzalez, and Alexander Gel-
bukh. 2010. Text comparison using soft cardi-
nality. In Edgar Chavez and Stefano Lonardi, ed-
itors, String Processing and Information Retrieval,
volume 6393 of LNCS, pages 297–302. Springer,
Berlin, Heidelberg.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012a. Soft cardinality: A parameterized
similarity function for text comparison. In SemEval
2012, Montreal, Canada.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012b. Soft cardinality+ ML: Learning adap-
tive similarity functions for cross-lingual textual en-
tailment. In SemEval 2012, Montreal, Canada.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013a. SOFTCARDINALITY-CORE:Im-
proving text overlap with distributional measures for
semantic textual similarity. In *SEM 2013, Atlanta,
Georgia, USA, June.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013b. SOFTCARDINALITY: Hierarchical
text overlap for student response analysis. In Se-
mEval 2013, Atlanta, Georgia, USA, June.
</reference>
<page confidence="0.970221">
741
</page>
<reference confidence="0.999842804878049">
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013c. SOFTCARDINALITY: Learning
to identify directional cross-lingual entailment from
cardinalities and SMT. In SemEval 2013, Atlanta,
Georgia, USA, June.
Karen Sp¨arck Jones. 2004. A statistical interpretation
of term specificity and its application in retrieval.
Journal of Documentation, 60(5):493–502, October.
David Jurgens, Mohammad T. Pilehvar, and Roberto
Navigli. 2014. SemEval-2014 task 3: Cross-
level semantic similarity. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland, August.
Andr´e Lynum, Partha Pakray, Bj¨orn Gamb¨ack, and
Sergio Jimenez. 2014. NTNU: Measuring se-
mantic similarity with sublexical feature represen-
tations and soft cardinalty. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland, August.
Marco Marelli, Stefano Menini, Marco Baroni, Lucia
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014. A SICK cure for the evaluation of
compositional distributional semantic models. In
Proceedings of LREC, Reykjavik, Iceland, May.
Alvaro E. Monge and Charles Elkan. 1996. The field
matching problem: Algorithms and applications. In
Proceeding of the 2nd International Conference on
Knowledge Discovery and Data Mining (KDD-96),
pages 267–270, Portland, OR.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::similarity: measuring the
relatedness of concepts. In Proceedings HLT-
NAACL–Demonstration Papers, Stroudsburg, PA,
USA.
Martin Porter. 1980. An algorithm for suffix stripping.
Program, 3(14):130–137, October.
J. Ross Quinlan. 1987. Simplifying decision
trees. International journal of man-machine studies,
27(3):221–234.
Amos Tversky. 1977. Features of similarity. Psycho-
logical Review, 84(4):327–352, July.
</reference>
<page confidence="0.997286">
742
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.119369">
<title confidence="0.988532">UNAL-NLP: Combining Soft Cardinality Features for</title>
<author confidence="0.355538">Textual Similarity</author>
<author confidence="0.355538">Relatedness</author>
<author confidence="0.355538">Entailment</author>
<affiliation confidence="0.695865666666667">Jimenez, George Baquero Universidad Nacional de Colombia</affiliation>
<address confidence="0.7601475">Ciudad Universitaria, edificio 453, oficina 114, Bogot´a, Colombia</address>
<email confidence="0.9528685">[sgjimenezv,geduenasl,jmbaquerov]@unal.edu.co</email>
<abstract confidence="0.999879">This paper describes our participation in the SemEval-2014 tasks 1, 3 and 10. We used an uniform approach for addressing all the tasks using the soft cardinality for extracting features from text pairs, and machine learning for predicting the gold standards. Our submitted systems ranked among the top systems in all the task and sub-tasks in which we participated. These results confirm the results obtained in previous SemEval campaigns suggesting that the soft cardinality is a simple and useful tool for addressing a wide range of natural language processing problems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>GonzalezAgirre Aitor</author>
</authors>
<title>SemEval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval@*SEM 2012),</booktitle>
<location>Montreal,Canada.</location>
<contexts>
<context position="1144" citStr="Agirre et al., 2012" startWordPosition="161" endWordPosition="164">e learning for predicting the gold standards. Our submitted systems ranked among the top systems in all the task and sub-tasks in which we participated. These results confirm the results obtained in previous SemEval campaigns suggesting that the soft cardinality is a simple and useful tool for addressing a wide range of natural language processing problems. 1 Introduction The semantic textual similarity is a core problem in the computational linguistic field. Consequently, the previous evaluation campaigns of this task in SemEval have attracted the attention of many research groups worldwide (Agirre et al., 2012; Agirre et al., 2013).This year, 3 tasks related to this problem have been proposed exploring different facets such as semantic relatedness, entailment , multilingualism, lack of training data and imbalance in the amount of information. The soft cardinality (Jimenez et al., 2010) is a simple concept that generalizes the classical set cardinality by considering the similarities among the elements in a collection for a more intuitive quantification of the number of elements in that collection. This approach can be applied to text applications representing texts as collections of words and provi</context>
<context position="8029" citStr="Agirre et al., 2012" startWordPosition="1318" endWordPosition="1321">en tested with long texts than with shorter ones. Having this in mind, an extended set of rational features is proposed, whose values are standardized in [0,1] interval aiming to reduce the effect of the length of the texts. These features are presented in Table 2. The soft cardinality has proven to overcome the classic cardinality in the semantic textual similarity (STS) task in previous SemEval campaigns (Jimenez et al., 2012b; Jimenez et al., 2013a). Even using a simplistic function sim based on q-grams of characters, the soft cardinality method ranked third among 89 participating systems (Agirre et al., 2012). Thus, our participating systems in the SemEval 2014 campaign were based on the previously described set of 17 features, obtained from the soft cardinality with different sim functions for comparing pairs of words. Each sim function produced a different set of features, which were combined with a regression algorithm for similarity and relatedness tasks. Similarly, a classification algorithm was used for the 733 entailment task. pairs of words w1 and w2, represented each one as a collection of 3-grams of characters, is given by 3 Systems Description the following expression: In this section t</context>
<context position="32010" citStr="Agirre et al., 2012" startWordPosition="5235" endWordPosition="5238"> the prediction of similarity scores. Particularly, in the paragraph-tosentence data set, which has the longest text, RUN2 obtained the best official score. In contrast, the scores obtained for the phrase-to-word and word-to-sense data sets were considerably lower in comparison with the top system, but still competitive against most of the other participating systems. 3.3 Task 10: Multilingual Semantic Similarity The SemEval-2014 task 10 (multilingual semantic similarity) (Agirre et al., 2014) is the sequel of the semantic textual similarity (STS) evaluations at SemEval in the past two years (Agirre et al., 2012; Agirre et al., 2013). This year 6 test data sets were proposed in English and 2 data sets in Spanish. Similarly to the 2013 campaign, there is not explicit training data for each data set. Consequently, different data sets from the previous STS evaluations were selected to be used as training data for the new data sets. The selection criterion was the average character length and type of the texts. The Table 8 shows the training data used for each test data set. 3.3.1 English Subtask The RUN1 for the English data sets was produced with a parameterized similarity function based on the SM feat</context>
</contexts>
<marker>Agirre, Cer, Diab, Aitor, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and GonzalezAgirre Aitor. 2012. SemEval-2012 task 6: A pilot on semantic textual similarity. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval@*SEM 2012), Montreal,Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor GonzalezAgirre</author>
<author>Weiwei Guo</author>
</authors>
<title>shared task: Semantic textual similarity, including a pilot on typed-similarity.</title>
<date>2013</date>
<publisher>SEM</publisher>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="1166" citStr="Agirre et al., 2013" startWordPosition="165" endWordPosition="168">ting the gold standards. Our submitted systems ranked among the top systems in all the task and sub-tasks in which we participated. These results confirm the results obtained in previous SemEval campaigns suggesting that the soft cardinality is a simple and useful tool for addressing a wide range of natural language processing problems. 1 Introduction The semantic textual similarity is a core problem in the computational linguistic field. Consequently, the previous evaluation campaigns of this task in SemEval have attracted the attention of many research groups worldwide (Agirre et al., 2012; Agirre et al., 2013).This year, 3 tasks related to this problem have been proposed exploring different facets such as semantic relatedness, entailment , multilingualism, lack of training data and imbalance in the amount of information. The soft cardinality (Jimenez et al., 2010) is a simple concept that generalizes the classical set cardinality by considering the similarities among the elements in a collection for a more intuitive quantification of the number of elements in that collection. This approach can be applied to text applications representing texts as collections of words and providing a similarity func</context>
<context position="32032" citStr="Agirre et al., 2013" startWordPosition="5239" endWordPosition="5242">milarity scores. Particularly, in the paragraph-tosentence data set, which has the longest text, RUN2 obtained the best official score. In contrast, the scores obtained for the phrase-to-word and word-to-sense data sets were considerably lower in comparison with the top system, but still competitive against most of the other participating systems. 3.3 Task 10: Multilingual Semantic Similarity The SemEval-2014 task 10 (multilingual semantic similarity) (Agirre et al., 2014) is the sequel of the semantic textual similarity (STS) evaluations at SemEval in the past two years (Agirre et al., 2012; Agirre et al., 2013). This year 6 test data sets were proposed in English and 2 data sets in Spanish. Similarly to the 2013 campaign, there is not explicit training data for each data set. Consequently, different data sets from the previous STS evaluations were selected to be used as training data for the new data sets. The selection criterion was the average character length and type of the texts. The Table 8 shows the training data used for each test data set. 3.3.1 English Subtask The RUN1 for the English data sets was produced with a parameterized similarity function based on the SM feature set and the symmet</context>
</contexts>
<marker>Agirre, Cer, Diab, GonzalezAgirre, Guo, 2013</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, and Weiwei Guo. 2013. *SEM 2013 shared task: Semantic textual similarity, including a pilot on typed-similarity. Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Banea</author>
<author>Claire Cardie</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
<author>Weiwei Guo</author>
<author>Rada Mihalcea</author>
<author>German Rigau</author>
<author>Janyce Weibe</author>
</authors>
<title>SemEval-2014 task 10: Multilingual semantic textual similarity.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014),</booktitle>
<location>Dublin, Ireland,</location>
<contexts>
<context position="31889" citStr="Agirre et al., 2014" startWordPosition="5214" endWordPosition="5217">ata used for the STS-2014 data sets (task 10). systems, showing that the SM (string-matching) feature set is effective for the prediction of similarity scores. Particularly, in the paragraph-tosentence data set, which has the longest text, RUN2 obtained the best official score. In contrast, the scores obtained for the phrase-to-word and word-to-sense data sets were considerably lower in comparison with the top system, but still competitive against most of the other participating systems. 3.3 Task 10: Multilingual Semantic Similarity The SemEval-2014 task 10 (multilingual semantic similarity) (Agirre et al., 2014) is the sequel of the semantic textual similarity (STS) evaluations at SemEval in the past two years (Agirre et al., 2012; Agirre et al., 2013). This year 6 test data sets were proposed in English and 2 data sets in Spanish. Similarly to the 2013 campaign, there is not explicit training data for each data set. Consequently, different data sets from the previous STS evaluations were selected to be used as training data for the new data sets. The selection criterion was the average character length and type of the texts. The Table 8 shows the training data used for each test data set. 3.3.1 Engl</context>
</contexts>
<marker>Agirre, Banea, Cardie, Cer, Diab, Gonzalez-Agirre, Guo, Mihalcea, Rigau, Weibe, 2014</marker>
<rawString>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Weibe. 2014. SemEval-2014 task 10: Multilingual semantic textual similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014), Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>An adapted lesk algorithm for word sense disambiguation using WordNet.</title>
<date>2002</date>
<booktitle>In Computational linguistics and intelligent text processing,</booktitle>
<pages>136--145</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="12741" citStr="Banerjee and Pedersen, 2002" startWordPosition="2110" endWordPosition="2114">described in this subsection will be referred as SM. 3.1.2 ESA Features For this set of features we used the idea proposed by Gabrilovich and Markovitch (2007) of enriching the representation of a text by representing each word by its textual definition in a knowledge base, i.e. explicit semantic analysis (ESA). For that, we used as knowledge base the synset’s textual definitions provided by WordNet. First, in order to determine the textual definition associated to each word, the texts were tagged using 734 the maximum entropy POS tagger included in the NLTK. Next, the adapted Lesk algorithm (Banerjee and Pedersen, 2002) for word sense disambiguation was applied in the texts disambiguating one word at the time. The software package used for this disambiguation process was pywsd2. The arguments needed for the disambiguation of each word are the POS tag of the target word and the entire sentence as context. Once all the words are disambiguated with their corresponding WordNet synsets, each word is replaced by all the words in their textual definition jointly with the same word and its lemma. The final result of this stage is that each text in the data set is replaced by a longer text including the original text</context>
</contexts>
<marker>Banerjee, Pedersen, 2002</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2002. An adapted lesk algorithm for word sense disambiguation using WordNet. In Computational linguistics and intelligent text processing, page 136–145. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Bagging predictors.</title>
<date>1996</date>
<booktitle>Machine Learning,</booktitle>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="23583" citStr="Breiman, 1996" startWordPosition="3866" endWordPosition="3867">on is detected in A noun neg B if noun negation is detected in B 3.1.6 Submitted Runs and Results RUN1 (PRIMARY) This system produced predictions by extracting all the features described previously (SM, ESA, POS, DEP, ANT, HYP and NEG) from all the texts in the SICK data set. Next, two machine learning models were obtained (WEKA (Hall et al., 2009) was used for that) using the training part of SICK, one for regression (relatedness) and another for classification (entailment). The regression model was a reduced-error pruning tree (REPtree) (Quinlan, 1987) boosted with 20 iterations of bagging (Breiman, 1996). The classification model was a J48Graft tree also boosted with 20 bagging iterations. These two models produced the predictions for the test part of SICK. RUN2 This system is similar to the one used in RUN1, but it used only the feature sets SM and NEG. Another difference is that a linear regression was used instead of the REPtree and no bagging was performed. RUN3 The same as RUN1, but again, linear regression was used instead of the REPtree and no bagging was performed. RUN4 The same as RUN2, but the models were boosted with 20 iterations of bagging. RUN5 The same as RUN3, but 30 iteration</context>
<context position="34067" citStr="Breiman, 1996" startWordPosition="5584" endWordPosition="5585"> 0.02 deft-forum 1.01 -1.01 0.24 0.93 -2.71 0.42 1.63 tweet-news 0.13 0.14 2.80 0.01 2.66 1.74 0.45 Table 9: Optimal parameters used for task 10 in English. function are reported in Table 9. Unlike subsection 3.1.1 where the Dice’s coefficient was used as the text similarity function, here the symmetrical Tversky’s index (eq. 2) was reused generating the three additional parameters marked with apostrophe (α&apos;, Q&apos; and bias&apos;). For the RUN2 the SM feature set was extracted from all the data sets in English (en) listed in Table 8. Then, a REPtree (Quinlan, 1987) boosted with 50 bagging iterations (Breiman, 1996) was trained using the training data sets selected for each test data set. Finally, these machine learning models produced the similarity score predictions for each test data set. The RUN3 was identical to the RUN2 but included additional feature sets apart from SM, namely: ESA, POS and WN. The WN feature set is the same as SM, but replacing the word-toword similarity function in eq. 2 by the path measure from the WordNet::Similarity package (Pedersen et al., 2004). 3.3.2 Spanish Subtask The Spanish system was based entirely in the SM feature set with some small changes for adapting the system</context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>Leo Breiman. 1996. Bagging predictors. Machine Learning, 24(2):123–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Corley</author>
<author>Rada Mihalcea</author>
</authors>
<title>Measuring the semantic similarity of texts.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, EMSEE ’05,</booktitle>
<pages>13--18</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="13852" citStr="Corley and Mihalcea (2005)" startWordPosition="2300" endWordPosition="2303">nal result of this stage is that each text in the data set is replaced by a longer text including the original text and some related words. The motivation of this procedure is that the extended versions of each pair of texts have more chance of sharing common words that the original texts. The extended versions of these texts were used to obtain another 17 features with the same procedure described in the previous subsection (3.1.1). This feature subset will henceforth be referred as ESA. 3.1.3 Features for each part-of-speech category This set of features is motivated by the idea proposed by Corley and Mihalcea (2005) of grouping words by their POS category before being compared for semantic textual similarity. Our approach consist in provide a version of each text pair in the data set for each POS category including only the words belonging to that category. For instance, the pair of texts {“A beautiful girl is playing tennis”, “A nice and handsome boy is playing football”} produce new pairs such as: {“beautiful”, “nice handsome”} for the ADJ tag, {“girl tennis”, “boy football”} for NOUN and {“is playing”, “is playing”} for VERB. Again, the POS tags were provided by the NLTK’s max entropy tagger. The 28 P</context>
</contexts>
<marker>Corley, Mihalcea, 2005</marker>
<rawString>Courtney Corley and Rada Mihalcea. 2005. Measuring the semantic similarity of texts. In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, EMSEE ’05, page 13–18, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Croce</author>
<author>Valerio Storch</author>
<author>P Annesi</author>
<author>Roberto Basili</author>
</authors>
<title>Distributional compositional semantics and text similarity.</title>
<date>2012</date>
<booktitle>In 2012 IEEE Sixth International Conference on Semantic Computing (ICSC),</booktitle>
<pages>242--249</pages>
<contexts>
<context position="15810" citStr="Croce et al., 2012" startWordPosition="2631" endWordPosition="2634">feature sets by POS category is that the machine learning algorithms could weight differently each category. The intuition behind this is that it is reasonable that categories such as VERB and NOUN could play a more important role for the task at hand than others such as ADJ or PREP. Using these categorized features, such discrimination among POS categories can be discovered from the training data. Finally, the total number of features in this set is 153 (17 features x 9 POS categories). This feature set will be referred as POS. 3.1.4 Features From Dependencies The syntactic soft cardinality (Croce et al., 2012; Croce et al., 2013) extend the soft cardinality approach by representing texts as bags of dependencies instead of bags of words. Each dependency is a 3-tuple composed of two syntactically related words and the type of their relationship. For instance, the sentence “The boy plays football” can be represented with 3 dependencies: [det,“boy”,“The”], [subj,“plays”,“boy”] and [obj,“plays”,“football”]. Clearly, this representation distinguish pairs of texts such as {“The dog bites a boy”,“The boy bites a dog”}, which are indistinguishable when they are represented as bags of words. This representa</context>
</contexts>
<marker>Croce, Storch, Annesi, Basili, 2012</marker>
<rawString>Danilo Croce, Valerio Storch, P. Annesi, and Roberto Basili. 2012. Distributional compositional semantics and text similarity. In 2012 IEEE Sixth International Conference on Semantic Computing (ICSC), pages 242–249, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Croce</author>
<author>Valerio Storch</author>
<author>Roberto Basili</author>
</authors>
<title>UNITOR-CORE TYPED: Combining text similarity and semantic filters through SV regression.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: SemanticTextual Similarity,</booktitle>
<pages>59</pages>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="2632" citStr="Croce et al., 2013" startWordPosition="379" endWordPosition="382"> organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ Alexander Gelbukh Center for Computing Research (CIC), Instituto Polit´ecnico Nacional (IPN), Av. Juan Dios B´atiz, Av. Mendiz´abal, Col. Nueva Industrial Vallejo, Mexico City, Mexico www.gelbukh.com notions of syntactic similarity, semantic relatedness, among others. We (and others) have used this approach to address with success the semantic textual similarity and other tasks in previous SemEval editions (Jimenez et al., 2012b; Jimenez et al., 2012a; Jimenez et al., 2013a; Jimenez et al., 2013b; Jimenez et al., 2013c; Croce et al., 2013). In this paper we describe our participating systems in the SemEval-2014 tasks 1, 3, and 10, which used the soft cardinality as core approach. 2 Features from Soft Cardinalities The cardinality of a collection of elements is the counting of non-repeated elements in it. This definition is intrinsically associated with the notion of set, which is a collection of non-repeated elements.Thus, the cardinality of a collection or set A is denoted as |A|. Clearly, the cardinality of a collection with repeated elements treats groups of identical elements as a single instance contributing only with a un</context>
<context position="15831" citStr="Croce et al., 2013" startWordPosition="2635" endWordPosition="2638">category is that the machine learning algorithms could weight differently each category. The intuition behind this is that it is reasonable that categories such as VERB and NOUN could play a more important role for the task at hand than others such as ADJ or PREP. Using these categorized features, such discrimination among POS categories can be discovered from the training data. Finally, the total number of features in this set is 153 (17 features x 9 POS categories). This feature set will be referred as POS. 3.1.4 Features From Dependencies The syntactic soft cardinality (Croce et al., 2012; Croce et al., 2013) extend the soft cardinality approach by representing texts as bags of dependencies instead of bags of words. Each dependency is a 3-tuple composed of two syntactically related words and the type of their relationship. For instance, the sentence “The boy plays football” can be represented with 3 dependencies: [det,“boy”,“The”], [subj,“plays”,“boy”] and [obj,“plays”,“football”]. Clearly, this representation distinguish pairs of texts such as {“The dog bites a boy”,“The boy bites a dog”}, which are indistinguishable when they are represented as bags of words. This representation can be obtained </context>
<context position="36780" citStr="Croce et al., 2013" startWordPosition="6042" endWordPosition="6045">as the top system among 22 participating systems. This result is remarkable given that this system was trained with a data set in English showing the domain adaptation ability of the soft cardinality approach. 740 4 Conclusions We participated in the SemEval-2014 task 1, 3 and 10 with an uniform approach based on soft cardinality features, obtaining pretty satisfactory results in all data sets, tasks and sub tasks. This approach has been used since SemEval-2012 in all versions of the following tasks: semantic textual similarity (Jimenez et al., 2012b; Jimenez et al., 2013a), typed similarity (Croce et al., 2013), cross-lingual textual entailment (Jimenez et al., 2012a; Jimenez et al., 2013c), student response analysis (Jimenez et al., 2013b), and multilingual semantic textual similarity (Lynum et al., 2014). In the majority of the cases, the systems based on soft cardinality, built by us and other teams, have been among the top systems. Given the uniformity of the approach, the consistency of the results, the few computational resources required and the overall conceptual simplicity, the soft cardinality is established as a useful tool for a wide spectrum of applications in natural language processin</context>
</contexts>
<marker>Croce, Storch, Basili, 2013</marker>
<rawString>Danilo Croce, Valerio Storch, and Roberto Basili. 2013. UNITOR-CORE TYPED: Combining text similarity and semantic filters through SV regression. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: SemanticTextual Similarity, page 59, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<volume>6</volume>
<pages>449--454</pages>
<location>Genoa, Italy,</location>
<marker>De Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine De Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC, volume 6, page 449–454, Genoa, Italy, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing semantic relatedness using wikipediabased explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI’07,</booktitle>
<pages>1606--1611</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="12272" citStr="Gabrilovich and Markovitch (2007)" startWordPosition="2033" endWordPosition="2036">y these 4 parameters using a hillclimbing approach until a maximum correlation is reached. We observe that the optimal values of the parameters p, α, β, and bias vary considerably between the data sets and for the different sim functions of word-to-word similarity. We do not yet understand from which factors of the data and the sim functions depend on these parameters. This issue will be the objective of further research. Henceforth, the set of 17 string-based features described in this subsection will be referred as SM. 3.1.2 ESA Features For this set of features we used the idea proposed by Gabrilovich and Markovitch (2007) of enriching the representation of a text by representing each word by its textual definition in a knowledge base, i.e. explicit semantic analysis (ESA). For that, we used as knowledge base the synset’s textual definitions provided by WordNet. First, in order to determine the textual definition associated to each word, the texts were tagged using 734 the maximum entropy POS tagger included in the NLTK. Next, the adapted Lesk algorithm (Banerjee and Pedersen, 2002) for word sense disambiguation was applied in the texts disambiguating one word at the time. The software package used for this dis</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using wikipediabased explicit semantic analysis. In Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI’07, page 1606–1611, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Frank Eibe</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
</authors>
<title>The WEKA data mining software: An update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="23319" citStr="Hall et al., 2009" startWordPosition="3824" endWordPosition="3827">the words: “no”, “none”, “nobody”, “nowhere”, “nothing” and “never”. Thus, for two texts A and B, 4 features are extracted (referred henceforth as NEG): verb neg A if verb negation is detected in A verb neg B if verb negation is detected in B noun neg A if noun negation is detected in A noun neg B if noun negation is detected in B 3.1.6 Submitted Runs and Results RUN1 (PRIMARY) This system produced predictions by extracting all the features described previously (SM, ESA, POS, DEP, ANT, HYP and NEG) from all the texts in the SICK data set. Next, two machine learning models were obtained (WEKA (Hall et al., 2009) was used for that) using the training part of SICK, one for regression (relatedness) and another for classification (entailment). The regression model was a reduced-error pruning tree (REPtree) (Quinlan, 1987) boosted with 20 iterations of bagging (Breiman, 1996). The classification model was a J48Graft tree also boosted with 20 bagging iterations. These two models produced the predictions for the test part of SICK. RUN2 This system is similar to the one used in RUN1, but it used only the feature sets SM and NEG. Another difference is that a linear regression was used instead of the REPtree a</context>
</contexts>
<marker>Hall, Eibe, Holmes, Pfahringer, 2009</marker>
<rawString>Mark Hall, Frank Eibe, Geoffrey Holmes, and Bernhard Pfahringer. 2009. The WEKA data mining software: An update. SIGKDD Explorations, 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Jaccard</author>
</authors>
<title>Etude comparative de la distribution florare dans une portion des alpes et des jura.</title>
<date>1901</date>
<booktitle>Bulletin de la Soci´et´e Vaudoise des Sciences Naturelles,</booktitle>
<pages>547--579</pages>
<contexts>
<context position="5760" citStr="Jaccard (1901)" startWordPosition="948" endWordPosition="949">at notion at sentence level. For instance, if sim provides the degree of semantic relatedness between two words using WordNet, two texts A and B could be compared by computing |A|sim, |B|sim and |A U B|sim. Given that A n B could be empty, the soft cardinality of the intersection must be approximated by |A n B|sim ^ |A|sim + |B|sim − |A U B|sim instead of being computed directly from A n B using equation 1. Using that approximation, the commonality (intersection) between A and B is induced by the pair-wise similarities provided by sim among the words in A and B. Since more than a century when Jaccard (1901) proposed his well-known index, the classical set cardinality has been used to build similarity functions for set comparison. Any binary-cardinalitybased similarity function is an algebraic combination of |A|, |B |and either |A n B |or |A U B| (e.g. Jaccard, Dice, Tversky, overlap and cosine indexes). These three cardinalities describes unambiguously all the regions in the Venn’s diagram when comparing two sets. Thus, in this scenario 4 possible cardinalities can be derived from these 3 basic cardinalities, see Table 1. Clearly, the same set of cardinalities can be obtained for the soft cardin</context>
</contexts>
<marker>Jaccard, 1901</marker>
<rawString>Paul Jaccard. 1901. Etude comparative de la distribution florare dans une portion des alpes et des jura. Bulletin de la Soci´et´e Vaudoise des Sciences Naturelles, pages 547–579.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Jimenez</author>
<author>Fabio Gonzalez</author>
<author>Alexander Gelbukh</author>
</authors>
<title>Text comparison using soft cardinality.</title>
<date>2010</date>
<booktitle>String Processing and Information Retrieval,</booktitle>
<volume>6393</volume>
<pages>297--302</pages>
<editor>In Edgar Chavez and Stefano Lonardi, editors,</editor>
<publisher>Springer,</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="1425" citStr="Jimenez et al., 2010" startWordPosition="205" endWordPosition="208">useful tool for addressing a wide range of natural language processing problems. 1 Introduction The semantic textual similarity is a core problem in the computational linguistic field. Consequently, the previous evaluation campaigns of this task in SemEval have attracted the attention of many research groups worldwide (Agirre et al., 2012; Agirre et al., 2013).This year, 3 tasks related to this problem have been proposed exploring different facets such as semantic relatedness, entailment , multilingualism, lack of training data and imbalance in the amount of information. The soft cardinality (Jimenez et al., 2010) is a simple concept that generalizes the classical set cardinality by considering the similarities among the elements in a collection for a more intuitive quantification of the number of elements in that collection. This approach can be applied to text applications representing texts as collections of words and providing a similarity function that compares two words. Varying this word-to-word similarity function the soft cardinality can reflect This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers.</context>
<context position="3285" citStr="Jimenez et al. (2010)" startWordPosition="488" endWordPosition="491">participating systems in the SemEval-2014 tasks 1, 3, and 10, which used the soft cardinality as core approach. 2 Features from Soft Cardinalities The cardinality of a collection of elements is the counting of non-repeated elements in it. This definition is intrinsically associated with the notion of set, which is a collection of non-repeated elements.Thus, the cardinality of a collection or set A is denoted as |A|. Clearly, the cardinality of a collection with repeated elements treats groups of identical elements as a single instance contributing only with a unit (1) to the element counting. Jimenez et al. (2010) proposed the soft cardinality that uses a notion of similarity among elements for grouping not only identical elements but similar too. That notion of similarity among elements is provided by a similarity function that compares two elements ai and aj and returns a score in [0,1] interval, having sim(ai, ai) = 1. Although, it is not necessary that sim fulfills another metric properties aside of identity, symmetry is also desirable. Thus, the soft cardinality of a collection A, whose elements a1, a2, ... , a|A |are comparable with a similarity function sim(ai, aj), is denoted as |A|sim. This so</context>
</contexts>
<marker>Jimenez, Gonzalez, Gelbukh, 2010</marker>
<rawString>Sergio Jimenez, Fabio Gonzalez, and Alexander Gelbukh. 2010. Text comparison using soft cardinality. In Edgar Chavez and Stefano Lonardi, editors, String Processing and Information Retrieval, volume 6393 of LNCS, pages 297–302. Springer, Berlin, Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Jimenez</author>
<author>Claudia Becerra</author>
<author>Alexander Gelbukh</author>
</authors>
<title>Soft cardinality: A parameterized similarity function for text comparison.</title>
<date>2012</date>
<booktitle>In SemEval 2012,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="2518" citStr="Jimenez et al., 2012" startWordPosition="359" endWordPosition="362">nder a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ Alexander Gelbukh Center for Computing Research (CIC), Instituto Polit´ecnico Nacional (IPN), Av. Juan Dios B´atiz, Av. Mendiz´abal, Col. Nueva Industrial Vallejo, Mexico City, Mexico www.gelbukh.com notions of syntactic similarity, semantic relatedness, among others. We (and others) have used this approach to address with success the semantic textual similarity and other tasks in previous SemEval editions (Jimenez et al., 2012b; Jimenez et al., 2012a; Jimenez et al., 2013a; Jimenez et al., 2013b; Jimenez et al., 2013c; Croce et al., 2013). In this paper we describe our participating systems in the SemEval-2014 tasks 1, 3, and 10, which used the soft cardinality as core approach. 2 Features from Soft Cardinalities The cardinality of a collection of elements is the counting of non-repeated elements in it. This definition is intrinsically associated with the notion of set, which is a collection of non-repeated elements.Thus, the cardinality of a collection or set A is denoted as |A|. Clearly, the cardinality of a coll</context>
<context position="7840" citStr="Jimenez et al., 2012" startWordPosition="1288" endWordPosition="1291">ity function could be dependent of the length of the texts. For instance, if the function was trained using long texts it is plausible to think that this function would be more effective when tested with long texts than with shorter ones. Having this in mind, an extended set of rational features is proposed, whose values are standardized in [0,1] interval aiming to reduce the effect of the length of the texts. These features are presented in Table 2. The soft cardinality has proven to overcome the classic cardinality in the semantic textual similarity (STS) task in previous SemEval campaigns (Jimenez et al., 2012b; Jimenez et al., 2013a). Even using a simplistic function sim based on q-grams of characters, the soft cardinality method ranked third among 89 participating systems (Agirre et al., 2012). Thus, our participating systems in the SemEval 2014 campaign were based on the previously described set of 17 features, obtained from the soft cardinality with different sim functions for comparing pairs of words. Each sim function produced a different set of features, which were combined with a regression algorithm for similarity and relatedness tasks. Similarly, a classification algorithm was used for th</context>
<context position="36716" citStr="Jimenez et al., 2012" startWordPosition="6032" endWordPosition="6035">ant to note that the RUN1 for the Wikipedia data set in Spanish was the top system among 22 participating systems. This result is remarkable given that this system was trained with a data set in English showing the domain adaptation ability of the soft cardinality approach. 740 4 Conclusions We participated in the SemEval-2014 task 1, 3 and 10 with an uniform approach based on soft cardinality features, obtaining pretty satisfactory results in all data sets, tasks and sub tasks. This approach has been used since SemEval-2012 in all versions of the following tasks: semantic textual similarity (Jimenez et al., 2012b; Jimenez et al., 2013a), typed similarity (Croce et al., 2013), cross-lingual textual entailment (Jimenez et al., 2012a; Jimenez et al., 2013c), student response analysis (Jimenez et al., 2013b), and multilingual semantic textual similarity (Lynum et al., 2014). In the majority of the cases, the systems based on soft cardinality, built by us and other teams, have been among the top systems. Given the uniformity of the approach, the consistency of the results, the few computational resources required and the overall conceptual simplicity, the soft cardinality is established as a useful tool f</context>
</contexts>
<marker>Jimenez, Becerra, Gelbukh, 2012</marker>
<rawString>Sergio Jimenez, Claudia Becerra, and Alexander Gelbukh. 2012a. Soft cardinality: A parameterized similarity function for text comparison. In SemEval 2012, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Jimenez</author>
<author>Claudia Becerra</author>
<author>Alexander Gelbukh</author>
</authors>
<title>Soft cardinality+ ML: Learning adaptive similarity functions for cross-lingual textual entailment.</title>
<date>2012</date>
<booktitle>In SemEval 2012,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="2518" citStr="Jimenez et al., 2012" startWordPosition="359" endWordPosition="362">nder a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ Alexander Gelbukh Center for Computing Research (CIC), Instituto Polit´ecnico Nacional (IPN), Av. Juan Dios B´atiz, Av. Mendiz´abal, Col. Nueva Industrial Vallejo, Mexico City, Mexico www.gelbukh.com notions of syntactic similarity, semantic relatedness, among others. We (and others) have used this approach to address with success the semantic textual similarity and other tasks in previous SemEval editions (Jimenez et al., 2012b; Jimenez et al., 2012a; Jimenez et al., 2013a; Jimenez et al., 2013b; Jimenez et al., 2013c; Croce et al., 2013). In this paper we describe our participating systems in the SemEval-2014 tasks 1, 3, and 10, which used the soft cardinality as core approach. 2 Features from Soft Cardinalities The cardinality of a collection of elements is the counting of non-repeated elements in it. This definition is intrinsically associated with the notion of set, which is a collection of non-repeated elements.Thus, the cardinality of a collection or set A is denoted as |A|. Clearly, the cardinality of a coll</context>
<context position="7840" citStr="Jimenez et al., 2012" startWordPosition="1288" endWordPosition="1291">ity function could be dependent of the length of the texts. For instance, if the function was trained using long texts it is plausible to think that this function would be more effective when tested with long texts than with shorter ones. Having this in mind, an extended set of rational features is proposed, whose values are standardized in [0,1] interval aiming to reduce the effect of the length of the texts. These features are presented in Table 2. The soft cardinality has proven to overcome the classic cardinality in the semantic textual similarity (STS) task in previous SemEval campaigns (Jimenez et al., 2012b; Jimenez et al., 2013a). Even using a simplistic function sim based on q-grams of characters, the soft cardinality method ranked third among 89 participating systems (Agirre et al., 2012). Thus, our participating systems in the SemEval 2014 campaign were based on the previously described set of 17 features, obtained from the soft cardinality with different sim functions for comparing pairs of words. Each sim function produced a different set of features, which were combined with a regression algorithm for similarity and relatedness tasks. Similarly, a classification algorithm was used for th</context>
<context position="36716" citStr="Jimenez et al., 2012" startWordPosition="6032" endWordPosition="6035">ant to note that the RUN1 for the Wikipedia data set in Spanish was the top system among 22 participating systems. This result is remarkable given that this system was trained with a data set in English showing the domain adaptation ability of the soft cardinality approach. 740 4 Conclusions We participated in the SemEval-2014 task 1, 3 and 10 with an uniform approach based on soft cardinality features, obtaining pretty satisfactory results in all data sets, tasks and sub tasks. This approach has been used since SemEval-2012 in all versions of the following tasks: semantic textual similarity (Jimenez et al., 2012b; Jimenez et al., 2013a), typed similarity (Croce et al., 2013), cross-lingual textual entailment (Jimenez et al., 2012a; Jimenez et al., 2013c), student response analysis (Jimenez et al., 2013b), and multilingual semantic textual similarity (Lynum et al., 2014). In the majority of the cases, the systems based on soft cardinality, built by us and other teams, have been among the top systems. Given the uniformity of the approach, the consistency of the results, the few computational resources required and the overall conceptual simplicity, the soft cardinality is established as a useful tool f</context>
</contexts>
<marker>Jimenez, Becerra, Gelbukh, 2012</marker>
<rawString>Sergio Jimenez, Claudia Becerra, and Alexander Gelbukh. 2012b. Soft cardinality+ ML: Learning adaptive similarity functions for cross-lingual textual entailment. In SemEval 2012, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Jimenez</author>
<author>Claudia Becerra</author>
<author>Alexander Gelbukh</author>
</authors>
<title>SOFTCARDINALITY-CORE:Improving text overlap with distributional measures for semantic textual similarity.</title>
<date>2013</date>
<booktitle>In *SEM 2013,</booktitle>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="2564" citStr="Jimenez et al., 2013" startWordPosition="367" endWordPosition="370">ational Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ Alexander Gelbukh Center for Computing Research (CIC), Instituto Polit´ecnico Nacional (IPN), Av. Juan Dios B´atiz, Av. Mendiz´abal, Col. Nueva Industrial Vallejo, Mexico City, Mexico www.gelbukh.com notions of syntactic similarity, semantic relatedness, among others. We (and others) have used this approach to address with success the semantic textual similarity and other tasks in previous SemEval editions (Jimenez et al., 2012b; Jimenez et al., 2012a; Jimenez et al., 2013a; Jimenez et al., 2013b; Jimenez et al., 2013c; Croce et al., 2013). In this paper we describe our participating systems in the SemEval-2014 tasks 1, 3, and 10, which used the soft cardinality as core approach. 2 Features from Soft Cardinalities The cardinality of a collection of elements is the counting of non-repeated elements in it. This definition is intrinsically associated with the notion of set, which is a collection of non-repeated elements.Thus, the cardinality of a collection or set A is denoted as |A|. Clearly, the cardinality of a collection with repeated elements treats groups of</context>
<context position="7863" citStr="Jimenez et al., 2013" startWordPosition="1292" endWordPosition="1295">ependent of the length of the texts. For instance, if the function was trained using long texts it is plausible to think that this function would be more effective when tested with long texts than with shorter ones. Having this in mind, an extended set of rational features is proposed, whose values are standardized in [0,1] interval aiming to reduce the effect of the length of the texts. These features are presented in Table 2. The soft cardinality has proven to overcome the classic cardinality in the semantic textual similarity (STS) task in previous SemEval campaigns (Jimenez et al., 2012b; Jimenez et al., 2013a). Even using a simplistic function sim based on q-grams of characters, the soft cardinality method ranked third among 89 participating systems (Agirre et al., 2012). Thus, our participating systems in the SemEval 2014 campaign were based on the previously described set of 17 features, obtained from the soft cardinality with different sim functions for comparing pairs of words. Each sim function produced a different set of features, which were combined with a regression algorithm for similarity and relatedness tasks. Similarly, a classification algorithm was used for the 733 entailment task. </context>
<context position="10620" citStr="Jimenez et al., 2013" startWordPosition="1751" endWordPosition="1754">r casing, tokenizing and stop-word removal (using the NLTK1). Then each word was reduced to its stem using the Porter’s algorithm (Porter, 1980) and a idf weight (Jones, 2004) was associated to each stem (wai weights in eq. 1) using the very SICK data set as document collection. Next, for each instance in the data, which is composed of two texts A and B, the 17 features listed in Tables 1 and 2 where extracted using eq.1. The used word-to-word similarity function sim decomposes each word in bags of 3-grams of characters, which are compared using the symmetrical Tversky’s index (Tversky, 1977; Jimenez et al., 2013a). Thus, the similarity between two 1http://www.nltk.org/ |c| sim(w1, w2) = β(α|wmin |+ (1 − α)|wmax|) + |c| (2) |c |= |w1 n w2 |+ biassim, |wmin |= min[|w1 \ w2|, |w2 \ w1|], |wmax |= max[|w1 \ w2|, |w2 \ w1|]. The values used for the parameters were α = 1.9, β = 2.36, bias = −0.97, and p = 0.39 (wherep corresponds to eq.1). The motivation and justification for these parameters can be found in (Jimenez et al., 2013a). These values were obtained by building a text similarity function using the Dice’s coefficient and the soft cardinalities plugging eq.2 in eq.1. Next, this text similarity func</context>
<context position="32690" citStr="Jimenez et al., 2013" startWordPosition="5353" endWordPosition="5356">osed in English and 2 data sets in Spanish. Similarly to the 2013 campaign, there is not explicit training data for each data set. Consequently, different data sets from the previous STS evaluations were selected to be used as training data for the new data sets. The selection criterion was the average character length and type of the texts. The Table 8 shows the training data used for each test data set. 3.3.1 English Subtask The RUN1 for the English data sets was produced with a parameterized similarity function based on the SM feature set and the symmetrized Tversky’s index (Tversky, 1977; Jimenez et al., 2013a). For a detailed description of this function and its parameters, please refer to the STSsim feature in the system description paper of the NTNU team (Lynum et al., 2014). The parameters used in that 739 System Para-2-Sent Sent-2-Phr Phr-2-Word Word-2-Sense Official Rank SimCompass run1 0.811 0.742 0.415 0.356 1st/38 ECNU run1 0.834 0.771 0.315 0.269 2nd/38 UNAL-NLP run2 0.837 0.738 0.274 0.256 3rd/38 SemantiKLUE run1 0.817 0.754 0.215 0.314 4th/38 UNAL-NLP run1 0.817 0.739 0.252 0.249 5th/38 Table 7: Official results for task 3 (Pearson’s correlation). Data α β bias p α&apos; β&apos; bias&apos; OnWN 0.53 </context>
<context position="36739" citStr="Jimenez et al., 2013" startWordPosition="6036" endWordPosition="6039">N1 for the Wikipedia data set in Spanish was the top system among 22 participating systems. This result is remarkable given that this system was trained with a data set in English showing the domain adaptation ability of the soft cardinality approach. 740 4 Conclusions We participated in the SemEval-2014 task 1, 3 and 10 with an uniform approach based on soft cardinality features, obtaining pretty satisfactory results in all data sets, tasks and sub tasks. This approach has been used since SemEval-2012 in all versions of the following tasks: semantic textual similarity (Jimenez et al., 2012b; Jimenez et al., 2013a), typed similarity (Croce et al., 2013), cross-lingual textual entailment (Jimenez et al., 2012a; Jimenez et al., 2013c), student response analysis (Jimenez et al., 2013b), and multilingual semantic textual similarity (Lynum et al., 2014). In the majority of the cases, the systems based on soft cardinality, built by us and other teams, have been among the top systems. Given the uniformity of the approach, the consistency of the results, the few computational resources required and the overall conceptual simplicity, the soft cardinality is established as a useful tool for a wide spectrum of a</context>
</contexts>
<marker>Jimenez, Becerra, Gelbukh, 2013</marker>
<rawString>Sergio Jimenez, Claudia Becerra, and Alexander Gelbukh. 2013a. SOFTCARDINALITY-CORE:Improving text overlap with distributional measures for semantic textual similarity. In *SEM 2013, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Jimenez</author>
<author>Claudia Becerra</author>
<author>Alexander Gelbukh</author>
</authors>
<title>SOFTCARDINALITY: Hierarchical text overlap for student response analysis.</title>
<date>2013</date>
<booktitle>In SemEval 2013,</booktitle>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="2564" citStr="Jimenez et al., 2013" startWordPosition="367" endWordPosition="370">ational Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ Alexander Gelbukh Center for Computing Research (CIC), Instituto Polit´ecnico Nacional (IPN), Av. Juan Dios B´atiz, Av. Mendiz´abal, Col. Nueva Industrial Vallejo, Mexico City, Mexico www.gelbukh.com notions of syntactic similarity, semantic relatedness, among others. We (and others) have used this approach to address with success the semantic textual similarity and other tasks in previous SemEval editions (Jimenez et al., 2012b; Jimenez et al., 2012a; Jimenez et al., 2013a; Jimenez et al., 2013b; Jimenez et al., 2013c; Croce et al., 2013). In this paper we describe our participating systems in the SemEval-2014 tasks 1, 3, and 10, which used the soft cardinality as core approach. 2 Features from Soft Cardinalities The cardinality of a collection of elements is the counting of non-repeated elements in it. This definition is intrinsically associated with the notion of set, which is a collection of non-repeated elements.Thus, the cardinality of a collection or set A is denoted as |A|. Clearly, the cardinality of a collection with repeated elements treats groups of</context>
<context position="7863" citStr="Jimenez et al., 2013" startWordPosition="1292" endWordPosition="1295">ependent of the length of the texts. For instance, if the function was trained using long texts it is plausible to think that this function would be more effective when tested with long texts than with shorter ones. Having this in mind, an extended set of rational features is proposed, whose values are standardized in [0,1] interval aiming to reduce the effect of the length of the texts. These features are presented in Table 2. The soft cardinality has proven to overcome the classic cardinality in the semantic textual similarity (STS) task in previous SemEval campaigns (Jimenez et al., 2012b; Jimenez et al., 2013a). Even using a simplistic function sim based on q-grams of characters, the soft cardinality method ranked third among 89 participating systems (Agirre et al., 2012). Thus, our participating systems in the SemEval 2014 campaign were based on the previously described set of 17 features, obtained from the soft cardinality with different sim functions for comparing pairs of words. Each sim function produced a different set of features, which were combined with a regression algorithm for similarity and relatedness tasks. Similarly, a classification algorithm was used for the 733 entailment task. </context>
<context position="10620" citStr="Jimenez et al., 2013" startWordPosition="1751" endWordPosition="1754">r casing, tokenizing and stop-word removal (using the NLTK1). Then each word was reduced to its stem using the Porter’s algorithm (Porter, 1980) and a idf weight (Jones, 2004) was associated to each stem (wai weights in eq. 1) using the very SICK data set as document collection. Next, for each instance in the data, which is composed of two texts A and B, the 17 features listed in Tables 1 and 2 where extracted using eq.1. The used word-to-word similarity function sim decomposes each word in bags of 3-grams of characters, which are compared using the symmetrical Tversky’s index (Tversky, 1977; Jimenez et al., 2013a). Thus, the similarity between two 1http://www.nltk.org/ |c| sim(w1, w2) = β(α|wmin |+ (1 − α)|wmax|) + |c| (2) |c |= |w1 n w2 |+ biassim, |wmin |= min[|w1 \ w2|, |w2 \ w1|], |wmax |= max[|w1 \ w2|, |w2 \ w1|]. The values used for the parameters were α = 1.9, β = 2.36, bias = −0.97, and p = 0.39 (wherep corresponds to eq.1). The motivation and justification for these parameters can be found in (Jimenez et al., 2013a). These values were obtained by building a text similarity function using the Dice’s coefficient and the soft cardinalities plugging eq.2 in eq.1. Next, this text similarity func</context>
<context position="32690" citStr="Jimenez et al., 2013" startWordPosition="5353" endWordPosition="5356">osed in English and 2 data sets in Spanish. Similarly to the 2013 campaign, there is not explicit training data for each data set. Consequently, different data sets from the previous STS evaluations were selected to be used as training data for the new data sets. The selection criterion was the average character length and type of the texts. The Table 8 shows the training data used for each test data set. 3.3.1 English Subtask The RUN1 for the English data sets was produced with a parameterized similarity function based on the SM feature set and the symmetrized Tversky’s index (Tversky, 1977; Jimenez et al., 2013a). For a detailed description of this function and its parameters, please refer to the STSsim feature in the system description paper of the NTNU team (Lynum et al., 2014). The parameters used in that 739 System Para-2-Sent Sent-2-Phr Phr-2-Word Word-2-Sense Official Rank SimCompass run1 0.811 0.742 0.415 0.356 1st/38 ECNU run1 0.834 0.771 0.315 0.269 2nd/38 UNAL-NLP run2 0.837 0.738 0.274 0.256 3rd/38 SemantiKLUE run1 0.817 0.754 0.215 0.314 4th/38 UNAL-NLP run1 0.817 0.739 0.252 0.249 5th/38 Table 7: Official results for task 3 (Pearson’s correlation). Data α β bias p α&apos; β&apos; bias&apos; OnWN 0.53 </context>
<context position="36739" citStr="Jimenez et al., 2013" startWordPosition="6036" endWordPosition="6039">N1 for the Wikipedia data set in Spanish was the top system among 22 participating systems. This result is remarkable given that this system was trained with a data set in English showing the domain adaptation ability of the soft cardinality approach. 740 4 Conclusions We participated in the SemEval-2014 task 1, 3 and 10 with an uniform approach based on soft cardinality features, obtaining pretty satisfactory results in all data sets, tasks and sub tasks. This approach has been used since SemEval-2012 in all versions of the following tasks: semantic textual similarity (Jimenez et al., 2012b; Jimenez et al., 2013a), typed similarity (Croce et al., 2013), cross-lingual textual entailment (Jimenez et al., 2012a; Jimenez et al., 2013c), student response analysis (Jimenez et al., 2013b), and multilingual semantic textual similarity (Lynum et al., 2014). In the majority of the cases, the systems based on soft cardinality, built by us and other teams, have been among the top systems. Given the uniformity of the approach, the consistency of the results, the few computational resources required and the overall conceptual simplicity, the soft cardinality is established as a useful tool for a wide spectrum of a</context>
</contexts>
<marker>Jimenez, Becerra, Gelbukh, 2013</marker>
<rawString>Sergio Jimenez, Claudia Becerra, and Alexander Gelbukh. 2013b. SOFTCARDINALITY: Hierarchical text overlap for student response analysis. In SemEval 2013, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Jimenez</author>
<author>Claudia Becerra</author>
<author>Alexander Gelbukh</author>
</authors>
<title>SOFTCARDINALITY: Learning to identify directional cross-lingual entailment from cardinalities and SMT.</title>
<date>2013</date>
<booktitle>In SemEval 2013,</booktitle>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="2564" citStr="Jimenez et al., 2013" startWordPosition="367" endWordPosition="370">ational Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ Alexander Gelbukh Center for Computing Research (CIC), Instituto Polit´ecnico Nacional (IPN), Av. Juan Dios B´atiz, Av. Mendiz´abal, Col. Nueva Industrial Vallejo, Mexico City, Mexico www.gelbukh.com notions of syntactic similarity, semantic relatedness, among others. We (and others) have used this approach to address with success the semantic textual similarity and other tasks in previous SemEval editions (Jimenez et al., 2012b; Jimenez et al., 2012a; Jimenez et al., 2013a; Jimenez et al., 2013b; Jimenez et al., 2013c; Croce et al., 2013). In this paper we describe our participating systems in the SemEval-2014 tasks 1, 3, and 10, which used the soft cardinality as core approach. 2 Features from Soft Cardinalities The cardinality of a collection of elements is the counting of non-repeated elements in it. This definition is intrinsically associated with the notion of set, which is a collection of non-repeated elements.Thus, the cardinality of a collection or set A is denoted as |A|. Clearly, the cardinality of a collection with repeated elements treats groups of</context>
<context position="7863" citStr="Jimenez et al., 2013" startWordPosition="1292" endWordPosition="1295">ependent of the length of the texts. For instance, if the function was trained using long texts it is plausible to think that this function would be more effective when tested with long texts than with shorter ones. Having this in mind, an extended set of rational features is proposed, whose values are standardized in [0,1] interval aiming to reduce the effect of the length of the texts. These features are presented in Table 2. The soft cardinality has proven to overcome the classic cardinality in the semantic textual similarity (STS) task in previous SemEval campaigns (Jimenez et al., 2012b; Jimenez et al., 2013a). Even using a simplistic function sim based on q-grams of characters, the soft cardinality method ranked third among 89 participating systems (Agirre et al., 2012). Thus, our participating systems in the SemEval 2014 campaign were based on the previously described set of 17 features, obtained from the soft cardinality with different sim functions for comparing pairs of words. Each sim function produced a different set of features, which were combined with a regression algorithm for similarity and relatedness tasks. Similarly, a classification algorithm was used for the 733 entailment task. </context>
<context position="10620" citStr="Jimenez et al., 2013" startWordPosition="1751" endWordPosition="1754">r casing, tokenizing and stop-word removal (using the NLTK1). Then each word was reduced to its stem using the Porter’s algorithm (Porter, 1980) and a idf weight (Jones, 2004) was associated to each stem (wai weights in eq. 1) using the very SICK data set as document collection. Next, for each instance in the data, which is composed of two texts A and B, the 17 features listed in Tables 1 and 2 where extracted using eq.1. The used word-to-word similarity function sim decomposes each word in bags of 3-grams of characters, which are compared using the symmetrical Tversky’s index (Tversky, 1977; Jimenez et al., 2013a). Thus, the similarity between two 1http://www.nltk.org/ |c| sim(w1, w2) = β(α|wmin |+ (1 − α)|wmax|) + |c| (2) |c |= |w1 n w2 |+ biassim, |wmin |= min[|w1 \ w2|, |w2 \ w1|], |wmax |= max[|w1 \ w2|, |w2 \ w1|]. The values used for the parameters were α = 1.9, β = 2.36, bias = −0.97, and p = 0.39 (wherep corresponds to eq.1). The motivation and justification for these parameters can be found in (Jimenez et al., 2013a). These values were obtained by building a text similarity function using the Dice’s coefficient and the soft cardinalities plugging eq.2 in eq.1. Next, this text similarity func</context>
<context position="32690" citStr="Jimenez et al., 2013" startWordPosition="5353" endWordPosition="5356">osed in English and 2 data sets in Spanish. Similarly to the 2013 campaign, there is not explicit training data for each data set. Consequently, different data sets from the previous STS evaluations were selected to be used as training data for the new data sets. The selection criterion was the average character length and type of the texts. The Table 8 shows the training data used for each test data set. 3.3.1 English Subtask The RUN1 for the English data sets was produced with a parameterized similarity function based on the SM feature set and the symmetrized Tversky’s index (Tversky, 1977; Jimenez et al., 2013a). For a detailed description of this function and its parameters, please refer to the STSsim feature in the system description paper of the NTNU team (Lynum et al., 2014). The parameters used in that 739 System Para-2-Sent Sent-2-Phr Phr-2-Word Word-2-Sense Official Rank SimCompass run1 0.811 0.742 0.415 0.356 1st/38 ECNU run1 0.834 0.771 0.315 0.269 2nd/38 UNAL-NLP run2 0.837 0.738 0.274 0.256 3rd/38 SemantiKLUE run1 0.817 0.754 0.215 0.314 4th/38 UNAL-NLP run1 0.817 0.739 0.252 0.249 5th/38 Table 7: Official results for task 3 (Pearson’s correlation). Data α β bias p α&apos; β&apos; bias&apos; OnWN 0.53 </context>
<context position="36739" citStr="Jimenez et al., 2013" startWordPosition="6036" endWordPosition="6039">N1 for the Wikipedia data set in Spanish was the top system among 22 participating systems. This result is remarkable given that this system was trained with a data set in English showing the domain adaptation ability of the soft cardinality approach. 740 4 Conclusions We participated in the SemEval-2014 task 1, 3 and 10 with an uniform approach based on soft cardinality features, obtaining pretty satisfactory results in all data sets, tasks and sub tasks. This approach has been used since SemEval-2012 in all versions of the following tasks: semantic textual similarity (Jimenez et al., 2012b; Jimenez et al., 2013a), typed similarity (Croce et al., 2013), cross-lingual textual entailment (Jimenez et al., 2012a; Jimenez et al., 2013c), student response analysis (Jimenez et al., 2013b), and multilingual semantic textual similarity (Lynum et al., 2014). In the majority of the cases, the systems based on soft cardinality, built by us and other teams, have been among the top systems. Given the uniformity of the approach, the consistency of the results, the few computational resources required and the overall conceptual simplicity, the soft cardinality is established as a useful tool for a wide spectrum of a</context>
</contexts>
<marker>Jimenez, Becerra, Gelbukh, 2013</marker>
<rawString>Sergio Jimenez, Claudia Becerra, and Alexander Gelbukh. 2013c. SOFTCARDINALITY: Learning to identify directional cross-lingual entailment from cardinalities and SMT. In SemEval 2013, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sp¨arck Jones</author>
</authors>
<title>A statistical interpretation of term specificity and its application in retrieval.</title>
<date>2004</date>
<journal>Journal of Documentation,</journal>
<volume>60</volume>
<issue>5</issue>
<contexts>
<context position="10175" citStr="Jones, 2004" startWordPosition="1672" endWordPosition="1673">eatures using the method presented in section 2 and training a machine learning algorithm for predicting the gold standard labels in the test data. Each feature set is described in the following 4 subsections and the subsection 3.1.6 provides details of the used combination of features, machine learning algorithm and preprocessing details. 3.1.1 String-Matching Features First, all texts in the SICK data set where preprocessed by lower casing, tokenizing and stop-word removal (using the NLTK1). Then each word was reduced to its stem using the Porter’s algorithm (Porter, 1980) and a idf weight (Jones, 2004) was associated to each stem (wai weights in eq. 1) using the very SICK data set as document collection. Next, for each instance in the data, which is composed of two texts A and B, the 17 features listed in Tables 1 and 2 where extracted using eq.1. The used word-to-word similarity function sim decomposes each word in bags of 3-grams of characters, which are compared using the symmetrical Tversky’s index (Tversky, 1977; Jimenez et al., 2013a). Thus, the similarity between two 1http://www.nltk.org/ |c| sim(w1, w2) = β(α|wmin |+ (1 − α)|wmax|) + |c| (2) |c |= |w1 n w2 |+ biassim, |wmin |= min[|</context>
</contexts>
<marker>Jones, 2004</marker>
<rawString>Karen Sp¨arck Jones. 2004. A statistical interpretation of term specificity and its application in retrieval. Journal of Documentation, 60(5):493–502, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Mohammad T Pilehvar</author>
<author>Roberto Navigli</author>
</authors>
<title>SemEval-2014 task 3: Crosslevel semantic similarity.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014),</booktitle>
<location>Dublin, Ireland,</location>
<contexts>
<context position="27735" citStr="Jurgens et al., 2014" startWordPosition="4544" endWordPosition="4547">nces=40 (4.8%). The only difference between these 2 sentences is the words “down” vs. “up”. In spite that this pair of antonyms was included in the antonym list, Error category NP NN NSS NA NP 420 5 125 0 NN - 94 1 0 NSS - - 413 22 NA - - - 40 Table 6: Co-ocurrences of types of errors in RUN1 (task1). the system failed to distinguish the contradiction between the texts. The matrix in Table 6 reports the number of co-occurrences of error categories in the 835 instances erroneously classified. 3.2 Task 3: Cross-level Semantic Similarity The SemEval 2014 task 3 (cross-level semantic similarity) (Jurgens et al., 2014) proposed the semantic textual similarity task but across different textual levels, namely paragraph-to-sentence, sentence-to-phrase, phrase-to-word and word-tosense. As usual, the goal is to predict the gold similarity scores for each pair of texts. For each one of these cross-level comparison types there were proposed a separated training and test data sets. Basically, we addressed this task using the set of features SM presented in subsection 3.1.1 in combination with a text expansion approach similar to the method presented in subsection 3.1.2. 3.2.1 Paragraph-to-sentence and Sentence-to-p</context>
</contexts>
<marker>Jurgens, Pilehvar, Navigli, 2014</marker>
<rawString>David Jurgens, Mohammad T. Pilehvar, and Roberto Navigli. 2014. SemEval-2014 task 3: Crosslevel semantic similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014), Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e Lynum</author>
<author>Partha Pakray</author>
<author>Bj¨orn Gamb¨ack</author>
<author>Sergio Jimenez</author>
</authors>
<title>NTNU: Measuring semantic similarity with sublexical feature representations and soft cardinalty.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014),</booktitle>
<location>Dublin, Ireland,</location>
<marker>Lynum, Pakray, Gamb¨ack, Jimenez, 2014</marker>
<rawString>Andr´e Lynum, Partha Pakray, Bj¨orn Gamb¨ack, and Sergio Jimenez. 2014. NTNU: Measuring semantic similarity with sublexical feature representations and soft cardinalty. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014), Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Marelli</author>
<author>Stefano Menini</author>
<author>Marco Baroni</author>
<author>Lucia Bentivogli</author>
<author>Raffaella Bernardi</author>
<author>Roberto Zamparelli</author>
</authors>
<title>A SICK cure for the evaluation of compositional distributional semantic models.</title>
<date>2014</date>
<booktitle>In Proceedings of LREC,</booktitle>
<location>Reykjavik, Iceland,</location>
<contexts>
<context position="9002" citStr="Marelli et al., 2014" startWordPosition="1476" endWordPosition="1479">asks. Similarly, a classification algorithm was used for the 733 entailment task. pairs of words w1 and w2, represented each one as a collection of 3-grams of characters, is given by 3 Systems Description the following expression: In this section the different feature sets used for each submitted system to the different task and subtask are described. Besides, the data used for training, parameters and other preprocessing details are described for each system. 3.1 Task 1: Textual Relatedness and Entailment The task 1 is based on the SICK (Sentences Involving Compositional Knowledge) data set (Marelli et al., 2014), which contains nearly 10,000 pairs of sentences manually labeled by relatedness and entailment. The relatedness gold labels range from 1 to 5, having 1 the minimum level of relatedness between the texts and 5 for the maximum. The entailment labels have three categorical values: neutral, contradiction and entailment. The two sub tasks consist of predicting the relatedness and entailment gold standards using approximately the 50% of the text pairs as training and the other part as test bed. Our overall approach consists in extracting 4 different sets of features using the method presented in s</context>
</contexts>
<marker>Marelli, Menini, Baroni, Bentivogli, Bernardi, Zamparelli, 2014</marker>
<rawString>Marco Marelli, Stefano Menini, Marco Baroni, Lucia Bentivogli, Raffaella Bernardi, and Roberto Zamparelli. 2014. A SICK cure for the evaluation of compositional distributional semantic models. In Proceedings of LREC, Reykjavik, Iceland, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alvaro E Monge</author>
<author>Charles Elkan</author>
</authors>
<title>The field matching problem: Algorithms and applications.</title>
<date>1996</date>
<booktitle>In Proceeding of the 2nd International Conference on Knowledge Discovery and Data Mining (KDD-96),</booktitle>
<pages>267--270</pages>
<location>Portland, OR.</location>
<contexts>
<context position="20888" citStr="Monge and Elkan, 1996" startWordPosition="3512" endWordPosition="3515"> sense disambiguation was performed (as described in subsection 3.1.2) for obtaining a synset label for each word. Secondly, we build a binary function hyp(ss1, ss2) that takes two WordNet synsets as arguments and returns 1 if ss1 is a hypernym of ss2 with a maximum depth in the WordNet’s is-a hierarchy of 6 steps, and 0 otherwise. This hypernymy function was build using the WordNet interface provided by the NLTK. Next, based on that synset-to-synset function, a text-to-text function that captures the degree or hypernymy in a text or in a pair of texts was build using the Monge-Elkan measure (Monge and Elkan, 1996). Thus, for two texts A and B represented as sets of synset labels, the following expression measures their degree of hypernymy: |A| HY P(A, B) = 1 max hyp(ai, bj) �A� j=1 i=1 Using the function HY P(*, *), 3 features are extracted from each pair of text (referred henceforth as HYP): hypernym AB from HYP(A, B) 736 http://www.myenglishpages.com/site php files/vocabulary-lesson-opposites-adjectives.php http://www.allaboutspace.com/wordlist/opposites.shtml http://www.michigan-proficiency-exams.com/antonym-list.html http://examples.yourdictionary.com/examples-of-antonyms.html http://www.synonyms-a</context>
</contexts>
<marker>Monge, Elkan, 1996</marker>
<rawString>Alvaro E. Monge and Charles Elkan. 1996. The field matching problem: Algorithms and applications. In Proceeding of the 2nd International Conference on Knowledge Discovery and Data Mining (KDD-96), pages 267–270, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>WordNet::similarity: measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Proceedings HLTNAACL–Demonstration Papers,</booktitle>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="34536" citStr="Pedersen et al., 2004" startWordPosition="5663" endWordPosition="5667">as extracted from all the data sets in English (en) listed in Table 8. Then, a REPtree (Quinlan, 1987) boosted with 50 bagging iterations (Breiman, 1996) was trained using the training data sets selected for each test data set. Finally, these machine learning models produced the similarity score predictions for each test data set. The RUN3 was identical to the RUN2 but included additional feature sets apart from SM, namely: ESA, POS and WN. The WN feature set is the same as SM, but replacing the word-toword similarity function in eq. 2 by the path measure from the WordNet::Similarity package (Pedersen et al., 2004). 3.3.2 Spanish Subtask The Spanish system was based entirely in the SM feature set with some small changes for adapting the system to Spanish. Basically, the list of English stop-words was replaced by the Spanish stop-words provided by the NLTK. In addition, the Porter stemmer was replaced by its Spanish equivalent, i.e. the Snowball stemmer for Spanish. The RUN1 is equivalent to the RUN1 for the data set run1 run2 run3 deft-forum 0.5043 0.3826 0.4607 deft-news 0.7205 0.7305 0.7216 headlines 0.7616 0.7645 0.7605 images 0.8071 0.7706 0.7782 OnWN 0.7823 0.8268 0.8426 tweet-news 0.6145 0.4028 0.</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. WordNet::similarity: measuring the relatedness of concepts. In Proceedings HLTNAACL–Demonstration Papers, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>3</volume>
<issue>14</issue>
<contexts>
<context position="10144" citStr="Porter, 1980" startWordPosition="1666" endWordPosition="1667">extracting 4 different sets of features using the method presented in section 2 and training a machine learning algorithm for predicting the gold standard labels in the test data. Each feature set is described in the following 4 subsections and the subsection 3.1.6 provides details of the used combination of features, machine learning algorithm and preprocessing details. 3.1.1 String-Matching Features First, all texts in the SICK data set where preprocessed by lower casing, tokenizing and stop-word removal (using the NLTK1). Then each word was reduced to its stem using the Porter’s algorithm (Porter, 1980) and a idf weight (Jones, 2004) was associated to each stem (wai weights in eq. 1) using the very SICK data set as document collection. Next, for each instance in the data, which is composed of two texts A and B, the 17 features listed in Tables 1 and 2 where extracted using eq.1. The used word-to-word similarity function sim decomposes each word in bags of 3-grams of characters, which are compared using the symmetrical Tversky’s index (Tversky, 1977; Jimenez et al., 2013a). Thus, the similarity between two 1http://www.nltk.org/ |c| sim(w1, w2) = β(α|wmin |+ (1 − α)|wmax|) + |c| (2) |c |= |w1 </context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Martin Porter. 1980. An algorithm for suffix stripping. Program, 3(14):130–137, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross Quinlan</author>
</authors>
<title>Simplifying decision trees.</title>
<date>1987</date>
<booktitle>International journal of man-machine studies,</booktitle>
<pages>27--3</pages>
<contexts>
<context position="23529" citStr="Quinlan, 1987" startWordPosition="3857" endWordPosition="3859">rb negation is detected in B noun neg A if noun negation is detected in A noun neg B if noun negation is detected in B 3.1.6 Submitted Runs and Results RUN1 (PRIMARY) This system produced predictions by extracting all the features described previously (SM, ESA, POS, DEP, ANT, HYP and NEG) from all the texts in the SICK data set. Next, two machine learning models were obtained (WEKA (Hall et al., 2009) was used for that) using the training part of SICK, one for regression (relatedness) and another for classification (entailment). The regression model was a reduced-error pruning tree (REPtree) (Quinlan, 1987) boosted with 20 iterations of bagging (Breiman, 1996). The classification model was a J48Graft tree also boosted with 20 bagging iterations. These two models produced the predictions for the test part of SICK. RUN2 This system is similar to the one used in RUN1, but it used only the feature sets SM and NEG. Another difference is that a linear regression was used instead of the REPtree and no bagging was performed. RUN3 The same as RUN1, but again, linear regression was used instead of the REPtree and no bagging was performed. RUN4 The same as RUN2, but the models were boosted with 20 iteratio</context>
<context position="34016" citStr="Quinlan, 1987" startWordPosition="5577" endWordPosition="5578"> 0.50 0.11 deft-news 3.36 -0.64 1.37 0.44 2.36 0.72 0.02 deft-forum 1.01 -1.01 0.24 0.93 -2.71 0.42 1.63 tweet-news 0.13 0.14 2.80 0.01 2.66 1.74 0.45 Table 9: Optimal parameters used for task 10 in English. function are reported in Table 9. Unlike subsection 3.1.1 where the Dice’s coefficient was used as the text similarity function, here the symmetrical Tversky’s index (eq. 2) was reused generating the three additional parameters marked with apostrophe (α&apos;, Q&apos; and bias&apos;). For the RUN2 the SM feature set was extracted from all the data sets in English (en) listed in Table 8. Then, a REPtree (Quinlan, 1987) boosted with 50 bagging iterations (Breiman, 1996) was trained using the training data sets selected for each test data set. Finally, these machine learning models produced the similarity score predictions for each test data set. The RUN3 was identical to the RUN2 but included additional feature sets apart from SM, namely: ESA, POS and WN. The WN feature set is the same as SM, but replacing the word-toword similarity function in eq. 2 by the path measure from the WordNet::Similarity package (Pedersen et al., 2004). 3.3.2 Spanish Subtask The Spanish system was based entirely in the SM feature </context>
</contexts>
<marker>Quinlan, 1987</marker>
<rawString>J. Ross Quinlan. 1987. Simplifying decision trees. International journal of man-machine studies, 27(3):221–234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amos Tversky</author>
</authors>
<title>Features of similarity.</title>
<date>1977</date>
<journal>Psychological Review,</journal>
<volume>84</volume>
<issue>4</issue>
<contexts>
<context position="10598" citStr="Tversky, 1977" startWordPosition="1749" endWordPosition="1750">ocessed by lower casing, tokenizing and stop-word removal (using the NLTK1). Then each word was reduced to its stem using the Porter’s algorithm (Porter, 1980) and a idf weight (Jones, 2004) was associated to each stem (wai weights in eq. 1) using the very SICK data set as document collection. Next, for each instance in the data, which is composed of two texts A and B, the 17 features listed in Tables 1 and 2 where extracted using eq.1. The used word-to-word similarity function sim decomposes each word in bags of 3-grams of characters, which are compared using the symmetrical Tversky’s index (Tversky, 1977; Jimenez et al., 2013a). Thus, the similarity between two 1http://www.nltk.org/ |c| sim(w1, w2) = β(α|wmin |+ (1 − α)|wmax|) + |c| (2) |c |= |w1 n w2 |+ biassim, |wmin |= min[|w1 \ w2|, |w2 \ w1|], |wmax |= max[|w1 \ w2|, |w2 \ w1|]. The values used for the parameters were α = 1.9, β = 2.36, bias = −0.97, and p = 0.39 (wherep corresponds to eq.1). The motivation and justification for these parameters can be found in (Jimenez et al., 2013a). These values were obtained by building a text similarity function using the Dice’s coefficient and the soft cardinalities plugging eq.2 in eq.1. Next, thi</context>
<context position="32668" citStr="Tversky, 1977" startWordPosition="5351" endWordPosition="5352"> sets were proposed in English and 2 data sets in Spanish. Similarly to the 2013 campaign, there is not explicit training data for each data set. Consequently, different data sets from the previous STS evaluations were selected to be used as training data for the new data sets. The selection criterion was the average character length and type of the texts. The Table 8 shows the training data used for each test data set. 3.3.1 English Subtask The RUN1 for the English data sets was produced with a parameterized similarity function based on the SM feature set and the symmetrized Tversky’s index (Tversky, 1977; Jimenez et al., 2013a). For a detailed description of this function and its parameters, please refer to the STSsim feature in the system description paper of the NTNU team (Lynum et al., 2014). The parameters used in that 739 System Para-2-Sent Sent-2-Phr Phr-2-Word Word-2-Sense Official Rank SimCompass run1 0.811 0.742 0.415 0.356 1st/38 ECNU run1 0.834 0.771 0.315 0.269 2nd/38 UNAL-NLP run2 0.837 0.738 0.274 0.256 3rd/38 SemantiKLUE run1 0.817 0.754 0.215 0.314 4th/38 UNAL-NLP run1 0.817 0.739 0.252 0.249 5th/38 Table 7: Official results for task 3 (Pearson’s correlation). Data α β bias p </context>
</contexts>
<marker>Tversky, 1977</marker>
<rawString>Amos Tversky. 1977. Features of similarity. Psychological Review, 84(4):327–352, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>