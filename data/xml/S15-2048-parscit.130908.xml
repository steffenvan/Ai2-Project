<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.059727">
<title confidence="0.9983255">
VectorSLU: A Continuous Word Vector Approach to Answer Selection in
Community Question Answering Systems
</title>
<author confidence="0.915848">
Yonatan Belinkov, Mitra Mohtarami, Scott Cyphers, James Glass
</author>
<affiliation confidence="0.7663">
MIT Computer Science and Artificial Intelligence Laboratory, Cambridge, MA 02139, USA
</affiliation>
<email confidence="0.992661">
{belinkov, mitra, cyphers, glass}@csail.mit.edu
</email>
<sectionHeader confidence="0.995579" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998002785714286">
Continuous word and phrase vectors have
proven useful in a number of NLP tasks. Here
we describe our experience using them as a
source of features for the SemEval-2015 task
3, consisting of two community question an-
swering subtasks: Answer Selection for cate-
gorizing answers as potential, good, and bad
with regards to their corresponding questions;
and YES/NO inference for predicting a yes, no,
or unsure response to a YES/NO question us-
ing all of its good answers. Our system ranked
6th and 1st in the English answer selection and
YES/NO inference subtasks respectively, and
2nd in the Arabic answer selection subtask.
</bodyText>
<sectionHeader confidence="0.998963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999594619047619">
Continuous word and phrase vectors, in which sim-
ilar words and phrases are associated with similar
vectors, have been useful in many NLP tasks (Al-
Rfou et al., 2013; Bansal et al., 2014; Bowman et
al., 2014; Boyd-Graber et al., 2012; Chen and Rud-
nicky, 2014; Guo et al., 2014; Iyyer et al., 2014;
Levy and Goldberg, 2014; Mikolov et al., 2013c).
To evaluate the effectiveness of continuous vector
representations for Community question answering
(CQA), we focused on using simple features derived
from vector similarity as input to a multi-class linear
SVM classifier. Our approach is language indepen-
dent and was evaluated on both English and Arabic.
Most of the vectors we use are domain-independent.
CQA services provide forums for users to ask or
answer questions on any topic, resulting in high vari-
ance answer quality (M`arquez et al., 2015). Search-
ing for good answers among the many responses can
be time-consuming for participants. This is illus-
trated by the following example of a question and
subsequent answers.
</bodyText>
<listItem confidence="0.9300468">
Q: Can I obtain Driving License my QID is written
Employee?
A1: the word employee is a general term that refers
to all the staff in your company ... you are all
considered employees of your company
A2: your qid should specify what is the actual pro-
fession you have. I think for me, your chances
to have a drivers license is low.
A3: his asking if he can obtain. means he have the
driver license.
</listItem>
<bodyText confidence="0.999960944444445">
Answer selection aims to automatically catego-
rize answers as: good if they completely answer the
question, potential if they contain useful information
about the question but do not completely answer it,
and bad if irrelevant to the question. In the example,
answers A1, A2, and A3 are respectively classified
as potential, good, and bad. The Arabic answer se-
lection task uses the labels direct, related, and irrel-
evant.
YES/NO inference infers a yes, no, or unsure
response to a question through its good answers,
which might not explicitly contain yes or no key-
words. For example, the answer for Q is no with
respect to A2 that can be interpreted as a no answer
to the question.
The remainder of this paper describes our features
and our rationale for choosing them, followed by an
analysis of the results, and a conclusion.
</bodyText>
<page confidence="0.966381">
282
</page>
<note confidence="0.68647">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 282–287,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<table confidence="0.993175818181818">
Text-based features
Text-based similarities
yes/no/probably-like words existing
Vector-based features
Q&amp;A vectors
OOV Q&amp;A
yes/no/probably-based cosine similarity
Metadata-based features
Q&amp;A identical user
Rank-based features
Normalized ranking scores
</table>
<tableCaption confidence="0.999965">
Table 1: The different types of features.
</tableCaption>
<sectionHeader confidence="0.957236" genericHeader="introduction">
2 Method
</sectionHeader>
<bodyText confidence="0.99971618918919">
Continuous vector representations, described by
Sch¨utze (Sch¨utze, 1992a; Sch¨utze, 1992b), asso-
ciate similar vectors with similar words and phrases.
Most approaches to computing vector representa-
tions use the observation that similar words ap-
pear in similar contexts (Firth, 1957). The theses
of Sahlgren (Sahlgren, 2006), Mikolov (Mikolov,
2012), and Socher (Socher, 2014) provide extensive
information on vector representations.
Our system analyzes questions and answers with
a DkPro (Eckart de Castilho and Gurevych, 2014)
uimaFIT (Ogren and Bethard, 2009) pipeline. The
DkPro OpenNLP (Apache Software Foundation,
2014) segmenter and chunker tokenize and find sen-
tences and phrases in the English questions and an-
swers, followed by lemmatization with the Stanford
lemmatizer (Manning et al., 2014). In Arabic, we
only apply lemmatization, with no chunking, using
MADAMIRA (Pasha et al., 2014). Stop words are re-
moved in both languages.
As shown in Table 1, we compute text-based,
vector-based, metadata-based and rank-based fea-
tures from the pre-processed data. The features are
used for a linear SVM classifier for answer selection
and YES/NO answer inference tasks. YES/NO an-
swer inference is only performed on good YES/NO
question answers, using the YES/NO majority class,
and unsure otherwise. SVM parameters are set by
grid-search and cross-validation.
Text-based features These features are mainly
computed using text similarity metrics that mea-
sure the string overlap between questions and
answers: The Longest Common Substring mea-
sure (Gusfield, 1997) identifies uninterrupted com-
mon strings, while the Longest Common Subse-
quence measure (Allison and Dix, 1986) and the
Longest Common Subsequence Norm identify com-
mon strings with interruptions and text replace-
ments, while Greedy String Tiling measure (Wise,
1996) allows reordering of the subsequences. Other
measures which treat text as sequences of characters
and compute similarities include the Monge Elkan
Second String (Monge and Elkan, 1997) and Jaro
Second String (Jaro, 1989) measures. A Cosine
Similarity-type measure based on term frequency
within the text is also used. Sets of (1-4)-grams from
the question and answer are compared with Jaccard
coefficient (Lyon et al., 2004) and Containment mea-
sures (Broder, 1997).1
Another group of text-based features identifies an-
swers that contain yes-like (e.g., “yes”, “oh yes”,
“yeah”, “yep”), no-like (e.g., “no”, “none”, “nope”,
“never”) and unsure-like (e.g., “possibly”, “con-
ceivably”, “perhaps”, “might”) words. These word
groups were determined by selecting the top 20
nearest neighbor words to the words yes, no and
probably based on the cosine similarity of their
Word2Vec vectors. These features are particularly
useful for the YES/NO answer inference task.
Vector-based features Our vector-based features
are computed from Word2Vec vectors (Mikolov
et al., 2013a; Mikolov et al., 2013b; Mikolov et
al., 2013d). For English word vectors we use
the GoogleNews vectors dataset, available on the
Word2Vec web site,2 which has a 3,000,000 word
vocabulary of 300-dimensional word vectors trained
on about 100 billion words. For Arabic word vectors
we use Word2Vec to train 100-dimensional vec-
tors with default settings on a lemmatized version of
the Arabic Gigaword (Linguistic Data Consortium,
2011), obtaining a vocabulary of 120,000 word lem-
mas.
We also use Doc2Vec,3 an implementation
of (Le and Mikolov, 2014) in the gensim
</bodyText>
<footnote confidence="0.993484">
1These features are mostly taken from the QCRI base-
line system: http://alt.qcri.org/semeval2015/
task3/index.php?id=data-and-tools.
2https://code.google.com/p/word2vec.
3http://radimrehurek.com/gensim/models/
</footnote>
<page confidence="0.998176">
283
</page>
<bodyText confidence="0.999837">
toolkit (ˇReh˚uˇrek and Sojka, 2010). Doc2Vec pro-
vides vectors for text of arbitrary length, so it allows
us to directly model answers and questions. The
Doc2Vec vectors were trained on the CQA English
data, creating a single vector for each question or
answer. These are the only vectors that were trained
specifically for the CQA domain.
We implemented a UIMA annotator that asso-
ciates a Word2Vec word vector with each in-
vocabulary token (or lemma). No vectors are as-
signed for out of vocabulary tokens. Another an-
notator computes the average of the vectors for the
entire question or answer, with no vector assigned if
all tokens are out of vocabulary.
We initially used the cosine similarity of the ques-
tion and answer vectors as a feature for the SVM
classifier, but we found that we had better results us-
ing the normalized vectors themselves. We hypothe-
size that the SVM was able to tune the importance of
the components of the vectors, whereas cosine sim-
ilarity weights each component equally. If the ques-
tion or answer has no vector, we use a 0 vector. To
make it easier for the classifier to ignore the vectors
in these cases, we add boolean features indicating
out of vocabulary, OOV Question and OOV Answer.
Even though the bag of words approach showed
encouraging results, we found it to be too coarse, so
we also compute average vectors for each sentence.
For English, we also compute average vectors for
each chunk. Then we look for the best matches be-
tween sentences (and chunks) in the question and
answer in terms of cosine similarity, and use the
pairs of (unnormalized) vectors as features.4 More
formally, given a question with sentence vectors
{qz} and an answer with sentence vectors {aj}, we
take as features the values of the vector pair (ˆq, ˆa)
defined as:
</bodyText>
<equation confidence="0.981918">
(ˆq, ˆa) =arg max
(qiraj)
</equation>
<bodyText confidence="0.991410111111111">
We also have six features corresponding to the
greatest cosine similarity between the comment
word vectors and the vectors for the words yes, Yes,
no, No, probably and Probably. These features are
more effective for the YES/NO classification task.
doc2vec.html.
4Post-evaluation testing showed no significant difference be-
tween using normalized or unnormalized vectors.
Metadata-based features As a metadata-based
indicator, the Q&amp;A identical user identifies if the
user who posted the question is the same user who
wrote the answer. This indicator is useful for detect-
ing irrelevant dialogue answers.
Rank-based features We employ SVM Rank5 to
compute ranking scores of answers with respect to
their corresponding questions. After generating all
other features, SVM Rank is run to produce rank-
ing scores for each possible answer. For training
SVM Rank, we convert answer labels to ranks ac-
cording to the following heuristic: good answers are
ranked first, potential ones second, and bad ones
third. Ranking scores are then used as features for
the classifier. The normalization of these scores can
be used as rank-based features to provide more in-
formation to the classifier, although these scores are
also used without any other features as explained in
Section 3.
</bodyText>
<sectionHeader confidence="0.994625" genericHeader="background">
3 Evaluation and Results
</sectionHeader>
<bodyText confidence="0.999145695652174">
We evaluate our approach on the answer selection
and YES/NO answer inference tasks. We use the
CQA datasets provided by the Semeval 2015 task
that contain 2600 training and 300 development
questions and their corresponding answers (a total
number of 16,541 training and 1,645 development
answers). About 10% of these questions are of the
YES/NO type. We combined the training and de-
velopment datasets for training purposes. The test
dataset includes 329 questions and 1976 answers.
About 9% of the test questions are bipolar.
We also evaluate our performance on the Arabic
answer selection task. The dataset contains 1300
training questions, 200 development questions, and
200 test questions. This dataset does not include
YES/NO questions.
English answer selection Our approach for the
answer selection task in English ranked 6th out of
12 submissions and its results are shown in Table
2. VectorSLU-Primary shows the results when we
include all the features listed in Table 1 except the
rank-based features. VectorSLU-Contrastive shows
the results when we include all the features except
</bodyText>
<footnote confidence="0.922492">
5http://www.cs.cornell.edu/people/tj/
svm_light/svm_rank.html.
</footnote>
<table confidence="0.881876375">
qz · aj
kqzk kajk
284
Method Macro-F1 Accuracy
VectorSLU-Primary 49.10 66.45
VectorSLU-Contrastive 49.54 70.45
JAIST (best) 57.19 72.52
Baseline 22.36 50.46
</table>
<tableCaption confidence="0.604444">
Table 2: Results for the English answer selection task.
</tableCaption>
<table confidence="0.9999034">
Method Macro-F1 Accuracy
VectorSLU-Primary 70.99 76.32
VectorSLU-Contrastive 73.18 78.12
QCRI (best) 78.55 83.02
Baseline 24.03 56.34
</table>
<tableCaption confidence="0.999381">
Table 3: Results for the Arabic answer selection task.
</tableCaption>
<bodyText confidence="0.999626548387097">
the rank-based and text-based features. Interest-
ingly, VectorSLU-Contrastive leads to a better per-
formance than VectorSLU-Primary. The lower per-
formance of VectorSLU-Primary could be due to the
high overlap between text-based features in differ-
ent classes that can clearly mislead classifiers. For
example, A1, A2 and A3 (see Section 1) all have a
considerable word overlap with their question, while
only A2 is a good answer. The last two rows of the
table are respectively related to the best performance
among all submissions and the majority class base-
line that always predicts good.
Arabic answer selection Our approach for an-
swer selection in Arabic ranked 2nd out of 4 sub-
missions. Table 3 shows the results. In these ex-
periments, we employ all features listed in Table 1
except for yes/no/probably-based features, since the
Arabic task does not include YES/NO answer infer-
ence. Vectors were trained from the Arabic Giga-
word (Linguistic Data Consortium, 2011). We found
lemma vectors to work better than token vectors.
We computed ranking scores with SVM Rank
for both VectorSLU-contrastive and VectorSLU-
Primary. In the case of VectorSLU-contrastive, we
used these scores to predict labels according to the
following heuristic: the top scoring answer is la-
beled as direct, the second scoring answer as re-
lated, and all other answers as irrelevant. This de-
cision mechanism is based on the distribution in the
training and development data, and proved to work
well on the test data. However, for our primary
</bodyText>
<table confidence="0.99681375">
Method Macro-F1 Accuracy
VectorSLU-Primary (best) 63.70 72.00
VectorSLU-Contrastive 61.90 68.00
Baseline 25.00 60.00
</table>
<tableCaption confidence="0.998803">
Table 4: Results for the English YES/NO inference task.
</tableCaption>
<bodyText confidence="0.999978615384615">
submission we were interested in a more principled
mechanism. Thus, in the VectorSLU-primary system
we computed 10 extra classification features from
the ranking scores. These features are used to pro-
vide prior knowledge about relative ranking of an-
swers with respect to their corresponding questions.
To compute these features, we first rank answers
with respect to questions and then scale the resul-
tant scores into the [0,1] range. We then consider
10 binary features that indicate whether the score of
each input answer is the range of [0,0.1), [0.1,0.2),
..., [0.9,1), respectively. Note that each feature vec-
tor contains exactly one 1 and nine 0s.
The last two rows of the table are related to the
best performance and the majority class baseline that
always predicts irrelevant.
English YES/NO inference For the indirect
YES/NO answer inference task, we achieve the best
performance and ranked 1st out of 8 submissions.
Table 4 shows the results. VectorSLU-Primary and
VectorSLU-Contrastive have the same definition as
in Table 2. Both approaches with or without the text-
based features outperform the baseline that always
predicts yes as the majority class and other submis-
sions. This indicates the effectiveness of the vector-
based features.
</bodyText>
<sectionHeader confidence="0.99996" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999211222222222">
We are not aware of any previous CQA work us-
ing continuous word vectors. Our vector features
were somewhat motivated by existing text-based
features, taken from the QCRI baseline system, re-
placing text-similarity heuristics with cosine simi-
larity. Some of the approaches to classifying an-
swers can be found in the general CQA literature,
such as (Toba et al., 2014; Bian et al., 2008; Liu et
al., 2008).
</bodyText>
<page confidence="0.998001">
285
</page>
<sectionHeader confidence="0.994272" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999896571428572">
In summary, we represented words, phrases, sen-
tences and whole questions and answers in vector
space, and computed various features from them for
a classifier, for both English and Arabic. We showed
the utility of these vector-based features for address-
ing the answer selection and the YES/NO answer
inference tasks in community question answering.
</bodyText>
<sectionHeader confidence="0.983988" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.995220166666667">
This research was supported by the Qatar Comput-
ing Research Institute (QCRI). We would like to
thank Alessandro Moschitti, Preslav Nakov, Llu´ıs
M`arquez, Massimo Nicosia, and other members of
the QCRI Arabic Language Technologies group for
their collaboration on this project.
</bodyText>
<sectionHeader confidence="0.992928" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996605845238096">
Rami Al-Rfou, Bryan Perozzi, and Steven Skiena. 2013.
Polyglot: Distributed Word Representations for Multi-
lingual NLP. CoRR, abs/1307.1662.
Lloyd Allison and Trevor I. Dix. 1986. A bit-string
longest-common-subsequence algorithm. Information
Processing Letters, 23(5):305 – 310.
Apache Software Foundation. 2014. OpenNLP.
Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014.
Tailoring Continuous Word Representations for De-
pendency Parsing. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Linguis-
tics, ACL 2014, June 22-27, 2014, Baltimore, MD,
USA, Volume 2: Short Papers, pages 809–815.
Jiang Bian, Yandong Liu, Eugene Agichtein, and
Hongyuan Zha. 2008. Finding the Right Facts in the
Crowd: Factoid Question Answering over Social Me-
dia. In Proceedings of the 17th International Confer-
ence on World Wide Web, WWW ’08, pages 467–476,
New York, NY, USA.
Samuel R. Bowman, Christopher Potts, and Christo-
pher D. Manning. 2014. Recursive Neural Net-
works for Learning Logical Semantics. CoRR,
abs/1406.1827.
Jordan L. Boyd-Graber, Brianna Satinoff, He He, and
Hal Daum´e III. 2012. Besting the Quiz Master:
Crowdsourcing Incremental Classification Games. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, EMNLP-
CoNLL 2012, July 12-14, 2012, Jeju Island, Korea,
pages 1290–1301.
Andrei Z. Broder. 1997. On the Resemblance and
Containment of Documents. In Proceedings of the
Compression and Complexity of Sequences 1997, SE-
QUENCES ’97, pages 21–, Washington, DC, USA.
Yun-Nung Chen and Alexander I. Rudnicky. 2014. Dy-
namically Supporting Unexplored Domains in Con-
versational Interactions by Enriching Semantics with
Neural Word Embeddings. In Proceedings of the 2014
Spoken Language Technology Workshop, December 7-
10, 2014, South Lake Tahoe, Nevada, USA, pages 590–
595.
Richard Eckart de Castilho and Iryna Gurevych. 2014.
A broad-coverage collection of portable NLP com-
ponents for building shareable analysis pipelines. In
Nancy Ide and Jens Grivolla, editors, Proceedings of
the Workshop on Open Infrastructures and Analysis
Frameworks for HLT (OIAF4HLT) at COLING 2014,
pages 1–11, Dublin, Ireland, August.
John Firth. 1957. A Synopsis of Linguistic Theory,
1930-1955. Studies in Linguistic Analysis, pages 1–
32.
Daniel (Zhaohan) Guo, Gokhan Tur, Wen-tau Yih, and
Geoffrey Zweig. 2014. Joint Semantic Utterance
Classification and Slot Filling with Recursive Neural
Networks. pages 554–559.
Dan Gusfield. 1997. Algorithms on Strings, Trees, and
Sequences: Computer Science and Computational Bi-
ology. Cambridge University Press, New York, NY,
USA.
Mohit Iyyer, Jordan L. Boyd-Graber, Leonardo
Max Batista Claudino, Richard Socher, and
Hal Daum´e III. 2014. A Neural Network for
Factoid Question Answering over Paragraphs. In
Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2014, October 25-29, 2014, Doha, Qatar, A meeting
of SIGDAT, a Special Interest Group of the ACL, pages
633–644.
Matthew A. Jaro. 1989. Advances in Record-Linkage
Methodology as Applied to Matching the 1985 Census
of Tampa, Florida. Journal of the American Statistical
Association, 84(406):414–420.
Quoc V. Le and Tomas Mikolov. 2014. Distributed
Representations of Sentences and Documents. CoRR,
abs/1405.4053.
Omer Levy and Yoav Goldberg. 2014. Dependency-
Based Word Embeddings. In Proceedings of the 52nd
Annual Meeting of the Association for Computational
Linguistics, ACL 2014, June 22-27, 2014, Baltimore,
MD, USA, Volume 2: Short Papers, pages 302–308.
Linguistic Data Consortium. 2011. Arabic Gigaword
Fifth Edition. https://catalog.ldc.upenn.
edu/LDC2011T11.
</reference>
<page confidence="0.990934">
286
</page>
<reference confidence="0.993043888888888">
Yandong Liu, Jiang Bian, and Eugene Agichtein. 2008.
Predicting Information Seeker Satisfaction in Commu-
nity Question Answering. In Proceedings of the 31st
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval, SI-
GIR ’08, pages 483–490, New York, NY, USA.
Caroline Lyon, Ruth Barrett, and James Malcolm. 2004.
A theoretical basis to the automated detection of copy-
ing between texts, and its practical implementation in
the ferret plagiarism and collusion detector. In Plagia-
rism: Prevention, Practice and Policies 2004 Confer-
ence.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David McClosky.
2014. The Stanford CoreNLP Natural Language Pro-
cessing Toolkit. In Proceedings of 52nd Annual Meet-
ing of the Association for Computational Linguistics:
System Demonstrations, pages 55–60.
Llu´ıs M`arquez, James Glass, Walid Magdy, Alessandro
Moschitti, Preslav Nakov, and Bilal Randeree. 2015.
SemEval-2015 Task 3: Answer Selection in Commu-
nity Question Answering. In Proceedings of the 9th
International Workshop on Semantic Evaluation (Se-
mEval 2015).
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient Estimation of Word Repre-
sentations in Vector Space. CoRR, abs/1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado,
and Jeffrey Dean. 2013b. Distributed Representa-
tions of Words and Phrases and their Compositionality.
CoRR, abs/1310.4546.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013c. Distributed Rep-
resentations of Words and Phrases and their Compo-
sitionality. In Advances in Neural Information Pro-
cessing Systems 26: 27th Annual Conference on Neu-
ral Information Processing Systems 2013. Proceedings
of a meeting held December 5-8, 2013, Lake Tahoe,
Nevada, United States., pages 3111–3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013d. Linguistic Regularities in Continuous Space
Word Representations. In Human Language Tech-
nologies: Conference of the North American Chap-
ter of the Association of Computational Linguistics,
Proceedings, June 9-14, 2013, Westin Peachtree Plaza
Hotel, Atlanta, Georgia, USA, pages 746–751.
Tom`aˇs Mikolov. 2012. Statistical Language Models
Based on Neural Networks. Ph.D. thesis, Brno Uni-
versity of Technology.
Alvaro Monge and Charles Elkan. 1997. An Effi-
cient Domain-Independent Algorithm for Detecting
Approximately Duplicate Database Records.
Philip Ogren and Steven Bethard. 2009. Building Test
Suites for UIMA Components. In Proceedings of
the Workshop on Software Engineering, Testing, and
Quality Assurance for Natural Language Processing
(SETQA-NLP 2009), pages 1–4, Boulder, Colorado,
June.
Arfath Pasha, Mohamed Al-Badrashiny, Mona Diab,
Ahmed El Kholy, Ramy Eskander, Nizar Habash,
Manoj Pooleery, Owen Rambow, and Ryan Roth.
2014. MADAMIRA: A Fast, Comprehensive Tool
for Morphological Analysis and Disambiguation of
Arabic. In Proceedings of the Ninth International
Conference on Language Resources and Evaluation
(LREC’14), pages 1094–1101, Reykjavik, Iceland.
Radim ˇReh˚uˇrek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora. In Pro-
ceedings of the LREC 2010 Workshop on New Chal-
lenges for NLP Frameworks, pages 45–50, Valletta,
Malta, May.
Magnus Sahlgren. 2006. The Word-Space Model. Ph.D.
thesis, Stockholm University.
Hinrich Sch¨utze. 1992a. Dimensions of Meaning. In
Proceedings of Supercomputing ’92, pages 787–796.
Hinrich Sch¨utze. 1992b. Word Space. In NIPS, pages
895–902.
Richard Socher. 2014. Recursive Deep Learning for
Natural Language Processing and Computer Vision.
Ph.D. thesis, Stanford University.
Hapnes Toba, Zhao-Yan Ming, Mirna Adriani, and Tat-
Seng Chua. 2014. Discovering high quality an-
swers in community question answering archives us-
ing a hierarchy of classifiers. Information Sciences,
261(0):101 – 115.
Michael J. Wise. 1996. YAP3: Improved Detection Of
Similarities In Computer Program And Other Texts.
In SIGCSEB: SIGCSE Bulletin (ACM Special Interest
Group on Computer Science Education), pages 130–
134.
</reference>
<page confidence="0.997363">
287
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.433263">
<title confidence="0.9999245">VectorSLU: A Continuous Word Vector Approach to Answer Selection Community Question Answering Systems</title>
<author confidence="0.999025">Yonatan Belinkov</author>
<author confidence="0.999025">Mitra Mohtarami</author>
<author confidence="0.999025">Scott Cyphers</author>
<author confidence="0.999025">James Glass</author>
<affiliation confidence="0.978712">MIT Computer Science and Artificial Intelligence Laboratory, Cambridge, MA 02139,</affiliation>
<address confidence="0.474789">mitra, cyphers,</address>
<abstract confidence="0.9931714">Continuous word and phrase vectors have proven useful in a number of NLP tasks. Here we describe our experience using them as a source of features for the SemEval-2015 task 3, consisting of two community question ansubtasks: Selection cateanswers as and with regards to their corresponding questions; inference predicting a to a YES/NO question usall of its Our system ranked 6th and 1st in the English answer selection and YES/NO inference subtasks respectively, and 2nd in the Arabic answer selection subtask.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rami Al-Rfou</author>
<author>Bryan Perozzi</author>
<author>Steven Skiena</author>
</authors>
<title>Polyglot: Distributed Word Representations for Multilingual NLP.</title>
<date>2013</date>
<pages>1307--1662</pages>
<publisher>CoRR,</publisher>
<marker>Al-Rfou, Perozzi, Skiena, 2013</marker>
<rawString>Rami Al-Rfou, Bryan Perozzi, and Steven Skiena. 2013. Polyglot: Distributed Word Representations for Multilingual NLP. CoRR, abs/1307.1662.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lloyd Allison</author>
<author>Trevor I Dix</author>
</authors>
<title>A bit-string longest-common-subsequence algorithm.</title>
<date>1986</date>
<journal>Information Processing Letters,</journal>
<volume>23</volume>
<issue>5</issue>
<pages>310</pages>
<contexts>
<context position="5356" citStr="Allison and Dix, 1986" startWordPosition="827" endWordPosition="830">rocessed data. The features are used for a linear SVM classifier for answer selection and YES/NO answer inference tasks. YES/NO answer inference is only performed on good YES/NO question answers, using the YES/NO majority class, and unsure otherwise. SVM parameters are set by grid-search and cross-validation. Text-based features These features are mainly computed using text similarity metrics that measure the string overlap between questions and answers: The Longest Common Substring measure (Gusfield, 1997) identifies uninterrupted common strings, while the Longest Common Subsequence measure (Allison and Dix, 1986) and the Longest Common Subsequence Norm identify common strings with interruptions and text replacements, while Greedy String Tiling measure (Wise, 1996) allows reordering of the subsequences. Other measures which treat text as sequences of characters and compute similarities include the Monge Elkan Second String (Monge and Elkan, 1997) and Jaro Second String (Jaro, 1989) measures. A Cosine Similarity-type measure based on term frequency within the text is also used. Sets of (1-4)-grams from the question and answer are compared with Jaccard coefficient (Lyon et al., 2004) and Containment meas</context>
</contexts>
<marker>Allison, Dix, 1986</marker>
<rawString>Lloyd Allison and Trevor I. Dix. 1986. A bit-string longest-common-subsequence algorithm. Information Processing Letters, 23(5):305 – 310.</rawString>
</citation>
<citation valid="true">
<title>Apache Software Foundation.</title>
<date>2014</date>
<note>OpenNLP.</note>
<marker>2014</marker>
<rawString>Apache Software Foundation. 2014. OpenNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Kevin Gimpel</author>
<author>Karen Livescu</author>
</authors>
<title>Tailoring Continuous Word Representations for Dependency Parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL</booktitle>
<volume>Volume</volume>
<pages>809--815</pages>
<location>Baltimore, MD, USA,</location>
<contexts>
<context position="1131" citStr="Bansal et al., 2014" startWordPosition="171" endWordPosition="174">y question answering subtasks: Answer Selection for categorizing answers as potential, good, and bad with regards to their corresponding questions; and YES/NO inference for predicting a yes, no, or unsure response to a YES/NO question using all of its good answers. Our system ranked 6th and 1st in the English answer selection and YES/NO inference subtasks respectively, and 2nd in the Arabic answer selection subtask. 1 Introduction Continuous word and phrase vectors, in which similar words and phrases are associated with similar vectors, have been useful in many NLP tasks (AlRfou et al., 2013; Bansal et al., 2014; Bowman et al., 2014; Boyd-Graber et al., 2012; Chen and Rudnicky, 2014; Guo et al., 2014; Iyyer et al., 2014; Levy and Goldberg, 2014; Mikolov et al., 2013c). To evaluate the effectiveness of continuous vector representations for Community question answering (CQA), we focused on using simple features derived from vector similarity as input to a multi-class linear SVM classifier. Our approach is language independent and was evaluated on both English and Arabic. Most of the vectors we use are domain-independent. CQA services provide forums for users to ask or answer questions on any topic, res</context>
</contexts>
<marker>Bansal, Gimpel, Livescu, 2014</marker>
<rawString>Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014. Tailoring Continuous Word Representations for Dependency Parsing. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014, June 22-27, 2014, Baltimore, MD, USA, Volume 2: Short Papers, pages 809–815.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Bian</author>
<author>Yandong Liu</author>
<author>Eugene Agichtein</author>
<author>Hongyuan Zha</author>
</authors>
<title>Finding the Right Facts in the Crowd: Factoid Question Answering over Social Media.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th International Conference on World Wide Web, WWW ’08,</booktitle>
<pages>467--476</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="15337" citStr="Bian et al., 2008" startWordPosition="2387" endWordPosition="2390">able 2. Both approaches with or without the textbased features outperform the baseline that always predicts yes as the majority class and other submissions. This indicates the effectiveness of the vectorbased features. 4 Related Work We are not aware of any previous CQA work using continuous word vectors. Our vector features were somewhat motivated by existing text-based features, taken from the QCRI baseline system, replacing text-similarity heuristics with cosine similarity. Some of the approaches to classifying answers can be found in the general CQA literature, such as (Toba et al., 2014; Bian et al., 2008; Liu et al., 2008). 285 5 Conclusion In summary, we represented words, phrases, sentences and whole questions and answers in vector space, and computed various features from them for a classifier, for both English and Arabic. We showed the utility of these vector-based features for addressing the answer selection and the YES/NO answer inference tasks in community question answering. Acknowledgments This research was supported by the Qatar Computing Research Institute (QCRI). We would like to thank Alessandro Moschitti, Preslav Nakov, Llu´ıs M`arquez, Massimo Nicosia, and other members of the </context>
</contexts>
<marker>Bian, Liu, Agichtein, Zha, 2008</marker>
<rawString>Jiang Bian, Yandong Liu, Eugene Agichtein, and Hongyuan Zha. 2008. Finding the Right Facts in the Crowd: Factoid Question Answering over Social Media. In Proceedings of the 17th International Conference on World Wide Web, WWW ’08, pages 467–476, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel R Bowman</author>
<author>Christopher Potts</author>
<author>Christopher D Manning</author>
</authors>
<title>Recursive Neural Networks for Learning Logical Semantics.</title>
<date>2014</date>
<location>CoRR, abs/1406.1827.</location>
<contexts>
<context position="1152" citStr="Bowman et al., 2014" startWordPosition="175" endWordPosition="178">subtasks: Answer Selection for categorizing answers as potential, good, and bad with regards to their corresponding questions; and YES/NO inference for predicting a yes, no, or unsure response to a YES/NO question using all of its good answers. Our system ranked 6th and 1st in the English answer selection and YES/NO inference subtasks respectively, and 2nd in the Arabic answer selection subtask. 1 Introduction Continuous word and phrase vectors, in which similar words and phrases are associated with similar vectors, have been useful in many NLP tasks (AlRfou et al., 2013; Bansal et al., 2014; Bowman et al., 2014; Boyd-Graber et al., 2012; Chen and Rudnicky, 2014; Guo et al., 2014; Iyyer et al., 2014; Levy and Goldberg, 2014; Mikolov et al., 2013c). To evaluate the effectiveness of continuous vector representations for Community question answering (CQA), we focused on using simple features derived from vector similarity as input to a multi-class linear SVM classifier. Our approach is language independent and was evaluated on both English and Arabic. Most of the vectors we use are domain-independent. CQA services provide forums for users to ask or answer questions on any topic, resulting in high varian</context>
</contexts>
<marker>Bowman, Potts, Manning, 2014</marker>
<rawString>Samuel R. Bowman, Christopher Potts, and Christopher D. Manning. 2014. Recursive Neural Networks for Learning Logical Semantics. CoRR, abs/1406.1827.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan L Boyd-Graber</author>
<author>Brianna Satinoff</author>
<author>He He</author>
<author>Hal Daum´e</author>
</authors>
<title>Besting the Quiz Master: Crowdsourcing Incremental Classification Games.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLPCoNLL 2012,</booktitle>
<pages>1290--1301</pages>
<location>Jeju Island,</location>
<marker>Boyd-Graber, Satinoff, He, Daum´e, 2012</marker>
<rawString>Jordan L. Boyd-Graber, Brianna Satinoff, He He, and Hal Daum´e III. 2012. Besting the Quiz Master: Crowdsourcing Incremental Classification Games. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLPCoNLL 2012, July 12-14, 2012, Jeju Island, Korea, pages 1290–1301.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Z Broder</author>
</authors>
<title>On the Resemblance and Containment of Documents.</title>
<date>1997</date>
<booktitle>In Proceedings of the Compression and Complexity of Sequences 1997, SEQUENCES ’97,</booktitle>
<pages>21</pages>
<location>Washington, DC, USA.</location>
<contexts>
<context position="5975" citStr="Broder, 1997" startWordPosition="923" endWordPosition="924">he Longest Common Subsequence Norm identify common strings with interruptions and text replacements, while Greedy String Tiling measure (Wise, 1996) allows reordering of the subsequences. Other measures which treat text as sequences of characters and compute similarities include the Monge Elkan Second String (Monge and Elkan, 1997) and Jaro Second String (Jaro, 1989) measures. A Cosine Similarity-type measure based on term frequency within the text is also used. Sets of (1-4)-grams from the question and answer are compared with Jaccard coefficient (Lyon et al., 2004) and Containment measures (Broder, 1997).1 Another group of text-based features identifies answers that contain yes-like (e.g., “yes”, “oh yes”, “yeah”, “yep”), no-like (e.g., “no”, “none”, “nope”, “never”) and unsure-like (e.g., “possibly”, “conceivably”, “perhaps”, “might”) words. These word groups were determined by selecting the top 20 nearest neighbor words to the words yes, no and probably based on the cosine similarity of their Word2Vec vectors. These features are particularly useful for the YES/NO answer inference task. Vector-based features Our vector-based features are computed from Word2Vec vectors (Mikolov et al., 2013a;</context>
</contexts>
<marker>Broder, 1997</marker>
<rawString>Andrei Z. Broder. 1997. On the Resemblance and Containment of Documents. In Proceedings of the Compression and Complexity of Sequences 1997, SEQUENCES ’97, pages 21–, Washington, DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun-Nung Chen</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>Dynamically Supporting Unexplored Domains in Conversational Interactions by Enriching Semantics with Neural Word Embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Spoken Language Technology Workshop,</booktitle>
<pages>590--595</pages>
<location>Nevada, USA,</location>
<contexts>
<context position="1203" citStr="Chen and Rudnicky, 2014" startWordPosition="183" endWordPosition="187">wers as potential, good, and bad with regards to their corresponding questions; and YES/NO inference for predicting a yes, no, or unsure response to a YES/NO question using all of its good answers. Our system ranked 6th and 1st in the English answer selection and YES/NO inference subtasks respectively, and 2nd in the Arabic answer selection subtask. 1 Introduction Continuous word and phrase vectors, in which similar words and phrases are associated with similar vectors, have been useful in many NLP tasks (AlRfou et al., 2013; Bansal et al., 2014; Bowman et al., 2014; Boyd-Graber et al., 2012; Chen and Rudnicky, 2014; Guo et al., 2014; Iyyer et al., 2014; Levy and Goldberg, 2014; Mikolov et al., 2013c). To evaluate the effectiveness of continuous vector representations for Community question answering (CQA), we focused on using simple features derived from vector similarity as input to a multi-class linear SVM classifier. Our approach is language independent and was evaluated on both English and Arabic. Most of the vectors we use are domain-independent. CQA services provide forums for users to ask or answer questions on any topic, resulting in high variance answer quality (M`arquez et al., 2015). Searchin</context>
</contexts>
<marker>Chen, Rudnicky, 2014</marker>
<rawString>Yun-Nung Chen and Alexander I. Rudnicky. 2014. Dynamically Supporting Unexplored Domains in Conversational Interactions by Enriching Semantics with Neural Word Embeddings. In Proceedings of the 2014 Spoken Language Technology Workshop, December 7-10, 2014, South Lake Tahoe, Nevada, USA, pages 590– 595.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Eckart de Castilho</author>
<author>Iryna Gurevych</author>
</authors>
<title>A broad-coverage collection of portable NLP components for building shareable analysis pipelines.</title>
<date>2014</date>
<booktitle>In Nancy Ide and Jens Grivolla, editors, Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT (OIAF4HLT) at COLING 2014,</booktitle>
<pages>1--11</pages>
<location>Dublin, Ireland,</location>
<marker>de Castilho, Gurevych, 2014</marker>
<rawString>Richard Eckart de Castilho and Iryna Gurevych. 2014. A broad-coverage collection of portable NLP components for building shareable analysis pipelines. In Nancy Ide and Jens Grivolla, editors, Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT (OIAF4HLT) at COLING 2014, pages 1–11, Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Firth</author>
</authors>
<title>A Synopsis of Linguistic Theory,</title>
<date>1957</date>
<booktitle>Studies in Linguistic Analysis,</booktitle>
<pages>1--32</pages>
<contexts>
<context position="3964" citStr="Firth, 1957" startWordPosition="623" endWordPosition="624">r Computational Linguistics Text-based features Text-based similarities yes/no/probably-like words existing Vector-based features Q&amp;A vectors OOV Q&amp;A yes/no/probably-based cosine similarity Metadata-based features Q&amp;A identical user Rank-based features Normalized ranking scores Table 1: The different types of features. 2 Method Continuous vector representations, described by Sch¨utze (Sch¨utze, 1992a; Sch¨utze, 1992b), associate similar vectors with similar words and phrases. Most approaches to computing vector representations use the observation that similar words appear in similar contexts (Firth, 1957). The theses of Sahlgren (Sahlgren, 2006), Mikolov (Mikolov, 2012), and Socher (Socher, 2014) provide extensive information on vector representations. Our system analyzes questions and answers with a DkPro (Eckart de Castilho and Gurevych, 2014) uimaFIT (Ogren and Bethard, 2009) pipeline. The DkPro OpenNLP (Apache Software Foundation, 2014) segmenter and chunker tokenize and find sentences and phrases in the English questions and answers, followed by lemmatization with the Stanford lemmatizer (Manning et al., 2014). In Arabic, we only apply lemmatization, with no chunking, using MADAMIRA (Pash</context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>John Firth. 1957. A Synopsis of Linguistic Theory, 1930-1955. Studies in Linguistic Analysis, pages 1– 32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Guo</author>
<author>Gokhan Tur</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Joint Semantic Utterance Classification and Slot Filling with Recursive Neural Networks.</title>
<date>2014</date>
<pages>554--559</pages>
<contexts>
<context position="1221" citStr="Guo et al., 2014" startWordPosition="188" endWordPosition="191">and bad with regards to their corresponding questions; and YES/NO inference for predicting a yes, no, or unsure response to a YES/NO question using all of its good answers. Our system ranked 6th and 1st in the English answer selection and YES/NO inference subtasks respectively, and 2nd in the Arabic answer selection subtask. 1 Introduction Continuous word and phrase vectors, in which similar words and phrases are associated with similar vectors, have been useful in many NLP tasks (AlRfou et al., 2013; Bansal et al., 2014; Bowman et al., 2014; Boyd-Graber et al., 2012; Chen and Rudnicky, 2014; Guo et al., 2014; Iyyer et al., 2014; Levy and Goldberg, 2014; Mikolov et al., 2013c). To evaluate the effectiveness of continuous vector representations for Community question answering (CQA), we focused on using simple features derived from vector similarity as input to a multi-class linear SVM classifier. Our approach is language independent and was evaluated on both English and Arabic. Most of the vectors we use are domain-independent. CQA services provide forums for users to ask or answer questions on any topic, resulting in high variance answer quality (M`arquez et al., 2015). Searching for good answers</context>
</contexts>
<marker>Guo, Tur, Yih, Zweig, 2014</marker>
<rawString>Daniel (Zhaohan) Guo, Gokhan Tur, Wen-tau Yih, and Geoffrey Zweig. 2014. Joint Semantic Utterance Classification and Slot Filling with Recursive Neural Networks. pages 554–559.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gusfield</author>
</authors>
<title>Algorithms on Strings, Trees, and Sequences: Computer Science and Computational Biology.</title>
<date>1997</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5246" citStr="Gusfield, 1997" startWordPosition="813" endWordPosition="814"> in Table 1, we compute text-based, vector-based, metadata-based and rank-based features from the pre-processed data. The features are used for a linear SVM classifier for answer selection and YES/NO answer inference tasks. YES/NO answer inference is only performed on good YES/NO question answers, using the YES/NO majority class, and unsure otherwise. SVM parameters are set by grid-search and cross-validation. Text-based features These features are mainly computed using text similarity metrics that measure the string overlap between questions and answers: The Longest Common Substring measure (Gusfield, 1997) identifies uninterrupted common strings, while the Longest Common Subsequence measure (Allison and Dix, 1986) and the Longest Common Subsequence Norm identify common strings with interruptions and text replacements, while Greedy String Tiling measure (Wise, 1996) allows reordering of the subsequences. Other measures which treat text as sequences of characters and compute similarities include the Monge Elkan Second String (Monge and Elkan, 1997) and Jaro Second String (Jaro, 1989) measures. A Cosine Similarity-type measure based on term frequency within the text is also used. Sets of (1-4)-gra</context>
</contexts>
<marker>Gusfield, 1997</marker>
<rawString>Dan Gusfield. 1997. Algorithms on Strings, Trees, and Sequences: Computer Science and Computational Biology. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Iyyer</author>
<author>Jordan L Boyd-Graber</author>
<author>Leonardo Max Batista Claudino</author>
<author>Richard Socher</author>
<author>Hal Daum´e</author>
</authors>
<title>A Neural Network for Factoid Question Answering over Paragraphs.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP</booktitle>
<pages>633--644</pages>
<location>Doha,</location>
<marker>Iyyer, Boyd-Graber, Claudino, Socher, Daum´e, 2014</marker>
<rawString>Mohit Iyyer, Jordan L. Boyd-Graber, Leonardo Max Batista Claudino, Richard Socher, and Hal Daum´e III. 2014. A Neural Network for Factoid Question Answering over Paragraphs. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 633–644.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew A Jaro</author>
</authors>
<date>1989</date>
<journal>Journal of the American Statistical Association,</journal>
<booktitle>Advances in Record-Linkage Methodology as Applied to Matching the</booktitle>
<volume>84</volume>
<issue>406</issue>
<institution>Census of Tampa, Florida.</institution>
<contexts>
<context position="5731" citStr="Jaro, 1989" startWordPosition="885" endWordPosition="886">y metrics that measure the string overlap between questions and answers: The Longest Common Substring measure (Gusfield, 1997) identifies uninterrupted common strings, while the Longest Common Subsequence measure (Allison and Dix, 1986) and the Longest Common Subsequence Norm identify common strings with interruptions and text replacements, while Greedy String Tiling measure (Wise, 1996) allows reordering of the subsequences. Other measures which treat text as sequences of characters and compute similarities include the Monge Elkan Second String (Monge and Elkan, 1997) and Jaro Second String (Jaro, 1989) measures. A Cosine Similarity-type measure based on term frequency within the text is also used. Sets of (1-4)-grams from the question and answer are compared with Jaccard coefficient (Lyon et al., 2004) and Containment measures (Broder, 1997).1 Another group of text-based features identifies answers that contain yes-like (e.g., “yes”, “oh yes”, “yeah”, “yep”), no-like (e.g., “no”, “none”, “nope”, “never”) and unsure-like (e.g., “possibly”, “conceivably”, “perhaps”, “might”) words. These word groups were determined by selecting the top 20 nearest neighbor words to the words yes, no and probab</context>
</contexts>
<marker>Jaro, 1989</marker>
<rawString>Matthew A. Jaro. 1989. Advances in Record-Linkage Methodology as Applied to Matching the 1985 Census of Tampa, Florida. Journal of the American Statistical Association, 84(406):414–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc V Le and Tomas Mikolov</author>
</authors>
<date>2014</date>
<booktitle>Distributed Representations of Sentences and Documents. CoRR, abs/1405.4053.</booktitle>
<contexts>
<context position="7120" citStr="Mikolov, 2014" startWordPosition="1095" endWordPosition="1096">d features are computed from Word2Vec vectors (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013d). For English word vectors we use the GoogleNews vectors dataset, available on the Word2Vec web site,2 which has a 3,000,000 word vocabulary of 300-dimensional word vectors trained on about 100 billion words. For Arabic word vectors we use Word2Vec to train 100-dimensional vectors with default settings on a lemmatized version of the Arabic Gigaword (Linguistic Data Consortium, 2011), obtaining a vocabulary of 120,000 word lemmas. We also use Doc2Vec,3 an implementation of (Le and Mikolov, 2014) in the gensim 1These features are mostly taken from the QCRI baseline system: http://alt.qcri.org/semeval2015/ task3/index.php?id=data-and-tools. 2https://code.google.com/p/word2vec. 3http://radimrehurek.com/gensim/models/ 283 toolkit (ˇReh˚uˇrek and Sojka, 2010). Doc2Vec provides vectors for text of arbitrary length, so it allows us to directly model answers and questions. The Doc2Vec vectors were trained on the CQA English data, creating a single vector for each question or answer. These are the only vectors that were trained specifically for the CQA domain. We implemented a UIMA annotator </context>
</contexts>
<marker>Mikolov, 2014</marker>
<rawString>Quoc V. Le and Tomas Mikolov. 2014. Distributed Representations of Sentences and Documents. CoRR, abs/1405.4053.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>DependencyBased Word Embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL</booktitle>
<pages>302--308</pages>
<location>Baltimore, MD, USA, Volume</location>
<contexts>
<context position="1266" citStr="Levy and Goldberg, 2014" startWordPosition="196" endWordPosition="199">nding questions; and YES/NO inference for predicting a yes, no, or unsure response to a YES/NO question using all of its good answers. Our system ranked 6th and 1st in the English answer selection and YES/NO inference subtasks respectively, and 2nd in the Arabic answer selection subtask. 1 Introduction Continuous word and phrase vectors, in which similar words and phrases are associated with similar vectors, have been useful in many NLP tasks (AlRfou et al., 2013; Bansal et al., 2014; Bowman et al., 2014; Boyd-Graber et al., 2012; Chen and Rudnicky, 2014; Guo et al., 2014; Iyyer et al., 2014; Levy and Goldberg, 2014; Mikolov et al., 2013c). To evaluate the effectiveness of continuous vector representations for Community question answering (CQA), we focused on using simple features derived from vector similarity as input to a multi-class linear SVM classifier. Our approach is language independent and was evaluated on both English and Arabic. Most of the vectors we use are domain-independent. CQA services provide forums for users to ask or answer questions on any topic, resulting in high variance answer quality (M`arquez et al., 2015). Searching for good answers among the many responses can be time-consumi</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014. DependencyBased Word Embeddings. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014, June 22-27, 2014, Baltimore, MD, USA, Volume 2: Short Papers, pages 302–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linguistic Data Consortium</author>
</authors>
<title>Arabic Gigaword Fifth Edition.</title>
<date>2011</date>
<note>https://catalog.ldc.upenn. edu/LDC2011T11.</note>
<contexts>
<context position="7006" citStr="Consortium, 2011" startWordPosition="1076" endWordPosition="1077">s. These features are particularly useful for the YES/NO answer inference task. Vector-based features Our vector-based features are computed from Word2Vec vectors (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013d). For English word vectors we use the GoogleNews vectors dataset, available on the Word2Vec web site,2 which has a 3,000,000 word vocabulary of 300-dimensional word vectors trained on about 100 billion words. For Arabic word vectors we use Word2Vec to train 100-dimensional vectors with default settings on a lemmatized version of the Arabic Gigaword (Linguistic Data Consortium, 2011), obtaining a vocabulary of 120,000 word lemmas. We also use Doc2Vec,3 an implementation of (Le and Mikolov, 2014) in the gensim 1These features are mostly taken from the QCRI baseline system: http://alt.qcri.org/semeval2015/ task3/index.php?id=data-and-tools. 2https://code.google.com/p/word2vec. 3http://radimrehurek.com/gensim/models/ 283 toolkit (ˇReh˚uˇrek and Sojka, 2010). Doc2Vec provides vectors for text of arbitrary length, so it allows us to directly model answers and questions. The Doc2Vec vectors were trained on the CQA English data, creating a single vector for each question or answ</context>
<context position="12970" citStr="Consortium, 2011" startWordPosition="2012" endWordPosition="2013">onsiderable word overlap with their question, while only A2 is a good answer. The last two rows of the table are respectively related to the best performance among all submissions and the majority class baseline that always predicts good. Arabic answer selection Our approach for answer selection in Arabic ranked 2nd out of 4 submissions. Table 3 shows the results. In these experiments, we employ all features listed in Table 1 except for yes/no/probably-based features, since the Arabic task does not include YES/NO answer inference. Vectors were trained from the Arabic Gigaword (Linguistic Data Consortium, 2011). We found lemma vectors to work better than token vectors. We computed ranking scores with SVM Rank for both VectorSLU-contrastive and VectorSLUPrimary. In the case of VectorSLU-contrastive, we used these scores to predict labels according to the following heuristic: the top scoring answer is labeled as direct, the second scoring answer as related, and all other answers as irrelevant. This decision mechanism is based on the distribution in the training and development data, and proved to work well on the test data. However, for our primary Method Macro-F1 Accuracy VectorSLU-Primary (best) 63.</context>
</contexts>
<marker>Consortium, 2011</marker>
<rawString>Linguistic Data Consortium. 2011. Arabic Gigaword Fifth Edition. https://catalog.ldc.upenn. edu/LDC2011T11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yandong Liu</author>
<author>Jiang Bian</author>
<author>Eugene Agichtein</author>
</authors>
<title>Predicting Information Seeker Satisfaction in Community Question Answering.</title>
<date>2008</date>
<booktitle>In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’08,</booktitle>
<pages>483--490</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="15356" citStr="Liu et al., 2008" startWordPosition="2391" endWordPosition="2394">ches with or without the textbased features outperform the baseline that always predicts yes as the majority class and other submissions. This indicates the effectiveness of the vectorbased features. 4 Related Work We are not aware of any previous CQA work using continuous word vectors. Our vector features were somewhat motivated by existing text-based features, taken from the QCRI baseline system, replacing text-similarity heuristics with cosine similarity. Some of the approaches to classifying answers can be found in the general CQA literature, such as (Toba et al., 2014; Bian et al., 2008; Liu et al., 2008). 285 5 Conclusion In summary, we represented words, phrases, sentences and whole questions and answers in vector space, and computed various features from them for a classifier, for both English and Arabic. We showed the utility of these vector-based features for addressing the answer selection and the YES/NO answer inference tasks in community question answering. Acknowledgments This research was supported by the Qatar Computing Research Institute (QCRI). We would like to thank Alessandro Moschitti, Preslav Nakov, Llu´ıs M`arquez, Massimo Nicosia, and other members of the QCRI Arabic Languag</context>
</contexts>
<marker>Liu, Bian, Agichtein, 2008</marker>
<rawString>Yandong Liu, Jiang Bian, and Eugene Agichtein. 2008. Predicting Information Seeker Satisfaction in Community Question Answering. In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’08, pages 483–490, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Lyon</author>
<author>Ruth Barrett</author>
<author>James Malcolm</author>
</authors>
<title>A theoretical basis to the automated detection of copying between texts, and its practical implementation in the ferret plagiarism and collusion detector.</title>
<date>2004</date>
<booktitle>In Plagiarism: Prevention, Practice and Policies</booktitle>
<note>Conference.</note>
<contexts>
<context position="5935" citStr="Lyon et al., 2004" startWordPosition="915" endWordPosition="918">equence measure (Allison and Dix, 1986) and the Longest Common Subsequence Norm identify common strings with interruptions and text replacements, while Greedy String Tiling measure (Wise, 1996) allows reordering of the subsequences. Other measures which treat text as sequences of characters and compute similarities include the Monge Elkan Second String (Monge and Elkan, 1997) and Jaro Second String (Jaro, 1989) measures. A Cosine Similarity-type measure based on term frequency within the text is also used. Sets of (1-4)-grams from the question and answer are compared with Jaccard coefficient (Lyon et al., 2004) and Containment measures (Broder, 1997).1 Another group of text-based features identifies answers that contain yes-like (e.g., “yes”, “oh yes”, “yeah”, “yep”), no-like (e.g., “no”, “none”, “nope”, “never”) and unsure-like (e.g., “possibly”, “conceivably”, “perhaps”, “might”) words. These word groups were determined by selecting the top 20 nearest neighbor words to the words yes, no and probably based on the cosine similarity of their Word2Vec vectors. These features are particularly useful for the YES/NO answer inference task. Vector-based features Our vector-based features are computed from </context>
</contexts>
<marker>Lyon, Barrett, Malcolm, 2004</marker>
<rawString>Caroline Lyon, Ruth Barrett, and James Malcolm. 2004. A theoretical basis to the automated detection of copying between texts, and its practical implementation in the ferret plagiarism and collusion detector. In Plagiarism: Prevention, Practice and Policies 2004 Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP Natural Language Processing Toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="4484" citStr="Manning et al., 2014" startWordPosition="696" endWordPosition="699">ector representations use the observation that similar words appear in similar contexts (Firth, 1957). The theses of Sahlgren (Sahlgren, 2006), Mikolov (Mikolov, 2012), and Socher (Socher, 2014) provide extensive information on vector representations. Our system analyzes questions and answers with a DkPro (Eckart de Castilho and Gurevych, 2014) uimaFIT (Ogren and Bethard, 2009) pipeline. The DkPro OpenNLP (Apache Software Foundation, 2014) segmenter and chunker tokenize and find sentences and phrases in the English questions and answers, followed by lemmatization with the Stanford lemmatizer (Manning et al., 2014). In Arabic, we only apply lemmatization, with no chunking, using MADAMIRA (Pasha et al., 2014). Stop words are removed in both languages. As shown in Table 1, we compute text-based, vector-based, metadata-based and rank-based features from the pre-processed data. The features are used for a linear SVM classifier for answer selection and YES/NO answer inference tasks. YES/NO answer inference is only performed on good YES/NO question answers, using the YES/NO majority class, and unsure otherwise. SVM parameters are set by grid-search and cross-validation. Text-based features These features are </context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Llu´ıs M`arquez</author>
<author>James Glass</author>
<author>Walid Magdy</author>
<author>Alessandro Moschitti</author>
<author>Preslav Nakov</author>
<author>Bilal Randeree</author>
</authors>
<date>2015</date>
<booktitle>SemEval-2015 Task 3: Answer Selection in Community Question Answering. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval</booktitle>
<marker>M`arquez, Glass, Magdy, Moschitti, Nakov, Randeree, 2015</marker>
<rawString>Llu´ıs M`arquez, James Glass, Walid Magdy, Alessandro Moschitti, Preslav Nakov, and Bilal Randeree. 2015. SemEval-2015 Task 3: Answer Selection in Community Question Answering. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient Estimation of Word Representations in Vector Space.</title>
<date>2013</date>
<location>CoRR, abs/1301.3781.</location>
<contexts>
<context position="1288" citStr="Mikolov et al., 2013" startWordPosition="200" endWordPosition="203">NO inference for predicting a yes, no, or unsure response to a YES/NO question using all of its good answers. Our system ranked 6th and 1st in the English answer selection and YES/NO inference subtasks respectively, and 2nd in the Arabic answer selection subtask. 1 Introduction Continuous word and phrase vectors, in which similar words and phrases are associated with similar vectors, have been useful in many NLP tasks (AlRfou et al., 2013; Bansal et al., 2014; Bowman et al., 2014; Boyd-Graber et al., 2012; Chen and Rudnicky, 2014; Guo et al., 2014; Iyyer et al., 2014; Levy and Goldberg, 2014; Mikolov et al., 2013c). To evaluate the effectiveness of continuous vector representations for Community question answering (CQA), we focused on using simple features derived from vector similarity as input to a multi-class linear SVM classifier. Our approach is language independent and was evaluated on both English and Arabic. Most of the vectors we use are domain-independent. CQA services provide forums for users to ask or answer questions on any topic, resulting in high variance answer quality (M`arquez et al., 2015). Searching for good answers among the many responses can be time-consuming for participants. T</context>
<context position="6573" citStr="Mikolov et al., 2013" startWordPosition="1007" endWordPosition="1010">easures (Broder, 1997).1 Another group of text-based features identifies answers that contain yes-like (e.g., “yes”, “oh yes”, “yeah”, “yep”), no-like (e.g., “no”, “none”, “nope”, “never”) and unsure-like (e.g., “possibly”, “conceivably”, “perhaps”, “might”) words. These word groups were determined by selecting the top 20 nearest neighbor words to the words yes, no and probably based on the cosine similarity of their Word2Vec vectors. These features are particularly useful for the YES/NO answer inference task. Vector-based features Our vector-based features are computed from Word2Vec vectors (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013d). For English word vectors we use the GoogleNews vectors dataset, available on the Word2Vec web site,2 which has a 3,000,000 word vocabulary of 300-dimensional word vectors trained on about 100 billion words. For Arabic word vectors we use Word2Vec to train 100-dimensional vectors with default settings on a lemmatized version of the Arabic Gigaword (Linguistic Data Consortium, 2011), obtaining a vocabulary of 120,000 word lemmas. We also use Doc2Vec,3 an implementation of (Le and Mikolov, 2014) in the gensim 1These features are mostly taken from </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient Estimation of Word Representations in Vector Space. CoRR, abs/1301.3781.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<booktitle>2013b. Distributed Representations of Words and Phrases and their Compositionality. CoRR, abs/1310.4546.</booktitle>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, </marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013b. Distributed Representations of Words and Phrases and their Compositionality. CoRR, abs/1310.4546.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Gregory S Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed Representations of Words and Phrases and their Compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held</booktitle>
<pages>3111--3119</pages>
<location>Lake Tahoe, Nevada, United States.,</location>
<contexts>
<context position="1288" citStr="Mikolov et al., 2013" startWordPosition="200" endWordPosition="203">NO inference for predicting a yes, no, or unsure response to a YES/NO question using all of its good answers. Our system ranked 6th and 1st in the English answer selection and YES/NO inference subtasks respectively, and 2nd in the Arabic answer selection subtask. 1 Introduction Continuous word and phrase vectors, in which similar words and phrases are associated with similar vectors, have been useful in many NLP tasks (AlRfou et al., 2013; Bansal et al., 2014; Bowman et al., 2014; Boyd-Graber et al., 2012; Chen and Rudnicky, 2014; Guo et al., 2014; Iyyer et al., 2014; Levy and Goldberg, 2014; Mikolov et al., 2013c). To evaluate the effectiveness of continuous vector representations for Community question answering (CQA), we focused on using simple features derived from vector similarity as input to a multi-class linear SVM classifier. Our approach is language independent and was evaluated on both English and Arabic. Most of the vectors we use are domain-independent. CQA services provide forums for users to ask or answer questions on any topic, resulting in high variance answer quality (M`arquez et al., 2015). Searching for good answers among the many responses can be time-consuming for participants. T</context>
<context position="6573" citStr="Mikolov et al., 2013" startWordPosition="1007" endWordPosition="1010">easures (Broder, 1997).1 Another group of text-based features identifies answers that contain yes-like (e.g., “yes”, “oh yes”, “yeah”, “yep”), no-like (e.g., “no”, “none”, “nope”, “never”) and unsure-like (e.g., “possibly”, “conceivably”, “perhaps”, “might”) words. These word groups were determined by selecting the top 20 nearest neighbor words to the words yes, no and probably based on the cosine similarity of their Word2Vec vectors. These features are particularly useful for the YES/NO answer inference task. Vector-based features Our vector-based features are computed from Word2Vec vectors (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013d). For English word vectors we use the GoogleNews vectors dataset, available on the Word2Vec web site,2 which has a 3,000,000 word vocabulary of 300-dimensional word vectors trained on about 100 billion words. For Arabic word vectors we use Word2Vec to train 100-dimensional vectors with default settings on a lemmatized version of the Arabic Gigaword (Linguistic Data Consortium, 2011), obtaining a vocabulary of 120,000 word lemmas. We also use Doc2Vec,3 an implementation of (Le and Mikolov, 2014) in the gensim 1These features are mostly taken from </context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013c. Distributed Representations of Words and Phrases and their Compositionality. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States., pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic Regularities in Continuous Space Word Representations.</title>
<date>2013</date>
<booktitle>In Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings,</booktitle>
<pages>746--751</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="1288" citStr="Mikolov et al., 2013" startWordPosition="200" endWordPosition="203">NO inference for predicting a yes, no, or unsure response to a YES/NO question using all of its good answers. Our system ranked 6th and 1st in the English answer selection and YES/NO inference subtasks respectively, and 2nd in the Arabic answer selection subtask. 1 Introduction Continuous word and phrase vectors, in which similar words and phrases are associated with similar vectors, have been useful in many NLP tasks (AlRfou et al., 2013; Bansal et al., 2014; Bowman et al., 2014; Boyd-Graber et al., 2012; Chen and Rudnicky, 2014; Guo et al., 2014; Iyyer et al., 2014; Levy and Goldberg, 2014; Mikolov et al., 2013c). To evaluate the effectiveness of continuous vector representations for Community question answering (CQA), we focused on using simple features derived from vector similarity as input to a multi-class linear SVM classifier. Our approach is language independent and was evaluated on both English and Arabic. Most of the vectors we use are domain-independent. CQA services provide forums for users to ask or answer questions on any topic, resulting in high variance answer quality (M`arquez et al., 2015). Searching for good answers among the many responses can be time-consuming for participants. T</context>
<context position="6573" citStr="Mikolov et al., 2013" startWordPosition="1007" endWordPosition="1010">easures (Broder, 1997).1 Another group of text-based features identifies answers that contain yes-like (e.g., “yes”, “oh yes”, “yeah”, “yep”), no-like (e.g., “no”, “none”, “nope”, “never”) and unsure-like (e.g., “possibly”, “conceivably”, “perhaps”, “might”) words. These word groups were determined by selecting the top 20 nearest neighbor words to the words yes, no and probably based on the cosine similarity of their Word2Vec vectors. These features are particularly useful for the YES/NO answer inference task. Vector-based features Our vector-based features are computed from Word2Vec vectors (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013d). For English word vectors we use the GoogleNews vectors dataset, available on the Word2Vec web site,2 which has a 3,000,000 word vocabulary of 300-dimensional word vectors trained on about 100 billion words. For Arabic word vectors we use Word2Vec to train 100-dimensional vectors with default settings on a lemmatized version of the Arabic Gigaword (Linguistic Data Consortium, 2011), obtaining a vocabulary of 120,000 word lemmas. We also use Doc2Vec,3 an implementation of (Le and Mikolov, 2014) in the gensim 1These features are mostly taken from </context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013d. Linguistic Regularities in Continuous Space Word Representations. In Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings, June 9-14, 2013, Westin Peachtree Plaza Hotel, Atlanta, Georgia, USA, pages 746–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom`aˇs Mikolov</author>
</authors>
<title>Statistical Language Models Based on Neural Networks.</title>
<date>2012</date>
<tech>Ph.D. thesis,</tech>
<institution>Brno University of Technology.</institution>
<contexts>
<context position="4030" citStr="Mikolov, 2012" startWordPosition="632" endWordPosition="633">arities yes/no/probably-like words existing Vector-based features Q&amp;A vectors OOV Q&amp;A yes/no/probably-based cosine similarity Metadata-based features Q&amp;A identical user Rank-based features Normalized ranking scores Table 1: The different types of features. 2 Method Continuous vector representations, described by Sch¨utze (Sch¨utze, 1992a; Sch¨utze, 1992b), associate similar vectors with similar words and phrases. Most approaches to computing vector representations use the observation that similar words appear in similar contexts (Firth, 1957). The theses of Sahlgren (Sahlgren, 2006), Mikolov (Mikolov, 2012), and Socher (Socher, 2014) provide extensive information on vector representations. Our system analyzes questions and answers with a DkPro (Eckart de Castilho and Gurevych, 2014) uimaFIT (Ogren and Bethard, 2009) pipeline. The DkPro OpenNLP (Apache Software Foundation, 2014) segmenter and chunker tokenize and find sentences and phrases in the English questions and answers, followed by lemmatization with the Stanford lemmatizer (Manning et al., 2014). In Arabic, we only apply lemmatization, with no chunking, using MADAMIRA (Pasha et al., 2014). Stop words are removed in both languages. As show</context>
</contexts>
<marker>Mikolov, 2012</marker>
<rawString>Tom`aˇs Mikolov. 2012. Statistical Language Models Based on Neural Networks. Ph.D. thesis, Brno University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alvaro Monge</author>
<author>Charles Elkan</author>
</authors>
<title>An Efficient Domain-Independent Algorithm for Detecting Approximately Duplicate Database Records.</title>
<date>1997</date>
<contexts>
<context position="5695" citStr="Monge and Elkan, 1997" startWordPosition="877" endWordPosition="880">atures are mainly computed using text similarity metrics that measure the string overlap between questions and answers: The Longest Common Substring measure (Gusfield, 1997) identifies uninterrupted common strings, while the Longest Common Subsequence measure (Allison and Dix, 1986) and the Longest Common Subsequence Norm identify common strings with interruptions and text replacements, while Greedy String Tiling measure (Wise, 1996) allows reordering of the subsequences. Other measures which treat text as sequences of characters and compute similarities include the Monge Elkan Second String (Monge and Elkan, 1997) and Jaro Second String (Jaro, 1989) measures. A Cosine Similarity-type measure based on term frequency within the text is also used. Sets of (1-4)-grams from the question and answer are compared with Jaccard coefficient (Lyon et al., 2004) and Containment measures (Broder, 1997).1 Another group of text-based features identifies answers that contain yes-like (e.g., “yes”, “oh yes”, “yeah”, “yep”), no-like (e.g., “no”, “none”, “nope”, “never”) and unsure-like (e.g., “possibly”, “conceivably”, “perhaps”, “might”) words. These word groups were determined by selecting the top 20 nearest neighbor w</context>
</contexts>
<marker>Monge, Elkan, 1997</marker>
<rawString>Alvaro Monge and Charles Elkan. 1997. An Efficient Domain-Independent Algorithm for Detecting Approximately Duplicate Database Records.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Ogren</author>
<author>Steven Bethard</author>
</authors>
<title>Building Test Suites for UIMA Components.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing (SETQA-NLP</booktitle>
<pages>1--4</pages>
<location>Boulder, Colorado,</location>
<contexts>
<context position="4243" citStr="Ogren and Bethard, 2009" startWordPosition="660" endWordPosition="663">king scores Table 1: The different types of features. 2 Method Continuous vector representations, described by Sch¨utze (Sch¨utze, 1992a; Sch¨utze, 1992b), associate similar vectors with similar words and phrases. Most approaches to computing vector representations use the observation that similar words appear in similar contexts (Firth, 1957). The theses of Sahlgren (Sahlgren, 2006), Mikolov (Mikolov, 2012), and Socher (Socher, 2014) provide extensive information on vector representations. Our system analyzes questions and answers with a DkPro (Eckart de Castilho and Gurevych, 2014) uimaFIT (Ogren and Bethard, 2009) pipeline. The DkPro OpenNLP (Apache Software Foundation, 2014) segmenter and chunker tokenize and find sentences and phrases in the English questions and answers, followed by lemmatization with the Stanford lemmatizer (Manning et al., 2014). In Arabic, we only apply lemmatization, with no chunking, using MADAMIRA (Pasha et al., 2014). Stop words are removed in both languages. As shown in Table 1, we compute text-based, vector-based, metadata-based and rank-based features from the pre-processed data. The features are used for a linear SVM classifier for answer selection and YES/NO answer infer</context>
</contexts>
<marker>Ogren, Bethard, 2009</marker>
<rawString>Philip Ogren and Steven Bethard. 2009. Building Test Suites for UIMA Components. In Proceedings of the Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing (SETQA-NLP 2009), pages 1–4, Boulder, Colorado, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arfath Pasha</author>
<author>Mohamed Al-Badrashiny</author>
<author>Mona Diab</author>
<author>Ahmed El Kholy</author>
<author>Ramy Eskander</author>
<author>Nizar Habash</author>
<author>Manoj Pooleery</author>
<author>Owen Rambow</author>
<author>Ryan Roth</author>
</authors>
<title>MADAMIRA: A Fast, Comprehensive Tool for Morphological Analysis and Disambiguation of Arabic.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14),</booktitle>
<pages>1094--1101</pages>
<location>Reykjavik, Iceland.</location>
<marker>Pasha, Al-Badrashiny, Diab, El Kholy, Eskander, Habash, Pooleery, Rambow, Roth, 2014</marker>
<rawString>Arfath Pasha, Mohamed Al-Badrashiny, Mona Diab, Ahmed El Kholy, Ramy Eskander, Nizar Habash, Manoj Pooleery, Owen Rambow, and Ryan Roth. 2014. MADAMIRA: A Fast, Comprehensive Tool for Morphological Analysis and Disambiguation of Arabic. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), pages 1094–1101, Reykjavik, Iceland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radim ˇReh˚uˇrek</author>
<author>Petr Sojka</author>
</authors>
<title>Software Framework for Topic Modelling with Large Corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks,</booktitle>
<pages>45--50</pages>
<location>Valletta, Malta,</location>
<marker>ˇReh˚uˇrek, Sojka, 2010</marker>
<rawString>Radim ˇReh˚uˇrek and Petr Sojka. 2010. Software Framework for Topic Modelling with Large Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45–50, Valletta, Malta, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>The Word-Space Model.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Stockholm University.</institution>
<contexts>
<context position="4005" citStr="Sahlgren, 2006" startWordPosition="629" endWordPosition="630"> features Text-based similarities yes/no/probably-like words existing Vector-based features Q&amp;A vectors OOV Q&amp;A yes/no/probably-based cosine similarity Metadata-based features Q&amp;A identical user Rank-based features Normalized ranking scores Table 1: The different types of features. 2 Method Continuous vector representations, described by Sch¨utze (Sch¨utze, 1992a; Sch¨utze, 1992b), associate similar vectors with similar words and phrases. Most approaches to computing vector representations use the observation that similar words appear in similar contexts (Firth, 1957). The theses of Sahlgren (Sahlgren, 2006), Mikolov (Mikolov, 2012), and Socher (Socher, 2014) provide extensive information on vector representations. Our system analyzes questions and answers with a DkPro (Eckart de Castilho and Gurevych, 2014) uimaFIT (Ogren and Bethard, 2009) pipeline. The DkPro OpenNLP (Apache Software Foundation, 2014) segmenter and chunker tokenize and find sentences and phrases in the English questions and answers, followed by lemmatization with the Stanford lemmatizer (Manning et al., 2014). In Arabic, we only apply lemmatization, with no chunking, using MADAMIRA (Pasha et al., 2014). Stop words are removed i</context>
</contexts>
<marker>Sahlgren, 2006</marker>
<rawString>Magnus Sahlgren. 2006. The Word-Space Model. Ph.D. thesis, Stockholm University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Dimensions of Meaning.</title>
<date>1992</date>
<booktitle>In Proceedings of Supercomputing ’92,</booktitle>
<pages>787--796</pages>
<marker>Sch¨utze, 1992</marker>
<rawString>Hinrich Sch¨utze. 1992a. Dimensions of Meaning. In Proceedings of Supercomputing ’92, pages 787–796.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Word Space. In</title>
<date>1992</date>
<booktitle>NIPS,</booktitle>
<pages>895--902</pages>
<marker>Sch¨utze, 1992</marker>
<rawString>Hinrich Sch¨utze. 1992b. Word Space. In NIPS, pages 895–902.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
</authors>
<title>Recursive Deep Learning for Natural Language Processing and Computer Vision.</title>
<date>2014</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="4057" citStr="Socher, 2014" startWordPosition="636" endWordPosition="637"> words existing Vector-based features Q&amp;A vectors OOV Q&amp;A yes/no/probably-based cosine similarity Metadata-based features Q&amp;A identical user Rank-based features Normalized ranking scores Table 1: The different types of features. 2 Method Continuous vector representations, described by Sch¨utze (Sch¨utze, 1992a; Sch¨utze, 1992b), associate similar vectors with similar words and phrases. Most approaches to computing vector representations use the observation that similar words appear in similar contexts (Firth, 1957). The theses of Sahlgren (Sahlgren, 2006), Mikolov (Mikolov, 2012), and Socher (Socher, 2014) provide extensive information on vector representations. Our system analyzes questions and answers with a DkPro (Eckart de Castilho and Gurevych, 2014) uimaFIT (Ogren and Bethard, 2009) pipeline. The DkPro OpenNLP (Apache Software Foundation, 2014) segmenter and chunker tokenize and find sentences and phrases in the English questions and answers, followed by lemmatization with the Stanford lemmatizer (Manning et al., 2014). In Arabic, we only apply lemmatization, with no chunking, using MADAMIRA (Pasha et al., 2014). Stop words are removed in both languages. As shown in Table 1, we compute te</context>
</contexts>
<marker>Socher, 2014</marker>
<rawString>Richard Socher. 2014. Recursive Deep Learning for Natural Language Processing and Computer Vision. Ph.D. thesis, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hapnes Toba</author>
<author>Zhao-Yan Ming</author>
<author>Mirna Adriani</author>
<author>TatSeng Chua</author>
</authors>
<title>Discovering high quality answers in community question answering archives using a hierarchy of classifiers.</title>
<date>2014</date>
<journal>Information Sciences,</journal>
<volume>261</volume>
<issue>0</issue>
<pages>115</pages>
<contexts>
<context position="15318" citStr="Toba et al., 2014" startWordPosition="2383" endWordPosition="2386"> definition as in Table 2. Both approaches with or without the textbased features outperform the baseline that always predicts yes as the majority class and other submissions. This indicates the effectiveness of the vectorbased features. 4 Related Work We are not aware of any previous CQA work using continuous word vectors. Our vector features were somewhat motivated by existing text-based features, taken from the QCRI baseline system, replacing text-similarity heuristics with cosine similarity. Some of the approaches to classifying answers can be found in the general CQA literature, such as (Toba et al., 2014; Bian et al., 2008; Liu et al., 2008). 285 5 Conclusion In summary, we represented words, phrases, sentences and whole questions and answers in vector space, and computed various features from them for a classifier, for both English and Arabic. We showed the utility of these vector-based features for addressing the answer selection and the YES/NO answer inference tasks in community question answering. Acknowledgments This research was supported by the Qatar Computing Research Institute (QCRI). We would like to thank Alessandro Moschitti, Preslav Nakov, Llu´ıs M`arquez, Massimo Nicosia, and ot</context>
</contexts>
<marker>Toba, Ming, Adriani, Chua, 2014</marker>
<rawString>Hapnes Toba, Zhao-Yan Ming, Mirna Adriani, and TatSeng Chua. 2014. Discovering high quality answers in community question answering archives using a hierarchy of classifiers. Information Sciences, 261(0):101 – 115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Wise</author>
</authors>
<title>YAP3: Improved Detection Of Similarities In Computer Program And Other Texts.</title>
<date>1996</date>
<booktitle>In SIGCSEB: SIGCSE Bulletin (ACM Special Interest Group on Computer Science Education),</booktitle>
<pages>130--134</pages>
<contexts>
<context position="5510" citStr="Wise, 1996" startWordPosition="852" endWordPosition="853"> good YES/NO question answers, using the YES/NO majority class, and unsure otherwise. SVM parameters are set by grid-search and cross-validation. Text-based features These features are mainly computed using text similarity metrics that measure the string overlap between questions and answers: The Longest Common Substring measure (Gusfield, 1997) identifies uninterrupted common strings, while the Longest Common Subsequence measure (Allison and Dix, 1986) and the Longest Common Subsequence Norm identify common strings with interruptions and text replacements, while Greedy String Tiling measure (Wise, 1996) allows reordering of the subsequences. Other measures which treat text as sequences of characters and compute similarities include the Monge Elkan Second String (Monge and Elkan, 1997) and Jaro Second String (Jaro, 1989) measures. A Cosine Similarity-type measure based on term frequency within the text is also used. Sets of (1-4)-grams from the question and answer are compared with Jaccard coefficient (Lyon et al., 2004) and Containment measures (Broder, 1997).1 Another group of text-based features identifies answers that contain yes-like (e.g., “yes”, “oh yes”, “yeah”, “yep”), no-like (e.g.,</context>
</contexts>
<marker>Wise, 1996</marker>
<rawString>Michael J. Wise. 1996. YAP3: Improved Detection Of Similarities In Computer Program And Other Texts. In SIGCSEB: SIGCSE Bulletin (ACM Special Interest Group on Computer Science Education), pages 130– 134.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>