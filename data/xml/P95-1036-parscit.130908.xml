<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.975163">
Some Novel Applications of Explanation-Based Learning to
Parsing Lexicalized Tree-Adjoining Grammars.
</title>
<author confidence="0.972637">
B. Srinivas and Aravind K. Joshi
</author>
<affiliation confidence="0.877194333333333">
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104, USA
</affiliation>
<email confidence="0.997925">
{srini, joshi}@linc.cis.upenn.edu
</email>
<sectionHeader confidence="0.99751" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9938235">
In this paper we present some novel ap-
plications of Explanation-Based Learning
(EBL) technique to parsing Lexicalized
Tree-Adjoining grammars. The novel as-
pects are (a) immediate generalization of
parses in the training set, (b) generaliza-
tion over recursive structures and (c) rep-
resentation of generalized parses as Finite
State Transducers. A highly impoverished
parser called a &amp;quot;stapler&amp;quot; has also been in-
troduced. We present experimental results
using EBL for different corpora and archi-
tectures to show the effectiveness of our ap-
proach.
</bodyText>
<sectionHeader confidence="0.999659" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.976892614285715">
In this paper we present some novel applications of
the so-called Explanation-Based Learning technique
(EBL) to parsing Lexicalized Tree-Adjoining gram-
mars (LTAG). EBL techniques were originally intro-
duced in the Al literature by (Mitchell et al., 1986;
Minton, 1988; van Harmelen and Bundy, 1988). The
main idea of EBL is to keep track of problems solved
in the past and to replay those solutions to solve
new but somewhat similar problems in the future.
Although put in these general terms the approach
sounds attractive, it is by no means clear that EBL
will actually improve the performance of the system
using it, an aspect which is of great interest to us
here.
Rayner (1988) was the first to investigate this
technique in the context of natural language pars-
ing. Seen as an EBL problem, the parse of a sin-
gle sentence represents an explanation of why the
sentence is a part of the language defined by the
grammar. Parsing new sentences amounts to find-
ing analogous explanations from the training sen-
tences. As a special case of EBL, Samuelsson and
This work was partially supported by ARO grant
DAAL03-89-0031, ARPA grant N00014-90-J-1863, NSF
STC grant DIR-8920230, and Ben Franklin Partnership
Program (PA) grant 93S.3078C-6
Rayner (1991) specialize a grammar for the ATIS
domain by storing chunks of the parse trees present
in a treebank of parsed examples. The idea is to
reparse the training examples by letting the parse
tree drive the rule expansion process and halting the
expansion of a specialized rule if the current node
meets a &apos;tree-cutting&apos; criteria. However, the prob-
lem of specifying an optimal &apos;tree-cutting&apos; criteria
was not addressed in this work. Samuelsson (1994)
used the information-theoretic measure of entropy to
derive the appropriate sized tree chunks automati-
cally. Neumann (1994) also attempts to specialize
a grammar given a training corpus of parsed exam-
ples by generalizing the parse for each sentence and
storing the generalized phrasal derivations under a
suitable index.
Although our work can be considered to be in
this general direction, it is distinct in that it ex-
ploits some of the key properties of LTAG to (a)
achieve an immediate generalization of parses in the
training set of sentences, (b) achieve an additional
level of generalization of the parses in the training
set, thereby dealing with test sentences which are
not necessarily of the same length as the training
sentences and (c) represent the set of generalized
parses as a finite state transducer (FST), which is
the first such use of FST in the context of EBL, to
the best of our knowledge. Later in the paper, we
will make some additional comments on the relation-
ship between our approach and some of the earlier
approaches.
In addition to these special aspects of our work,
we will present experimental results evaluating the
effectiveness of our approach on more than one kind
of corpus. We also introduce a device called a &amp;quot;sta-
pler&amp;quot;, a considerably impoverished parser, whose
only job is to do term unification and compute alter-
nate attachments for modifiers. We achieve substan-
tial speed-up by the use of &amp;quot;stapler&amp;quot; in combination
with the output of the FST.
The paper is organized as follows. In Section 2
we provide a brief introduction to LTAG with the
help of an example. In Section 3 we discuss our
approach to using EBL and the advantages provided
</bodyText>
<page confidence="0.997702">
268
</page>
<figureCaption confidence="0.999948">
Figure 1: Substitution and Adjunction in LTAG
</figureCaption>
<figure confidence="0.9944715">
7.&gt;
(a) (b)
</figure>
<bodyText confidence="0.994827875">
by LTAG. The FST representation used for EBL is
illustrated in Section 4. In Section 5 we present the
&amp;quot;stapler&amp;quot; in some detail. The results of some of the
experiments based on our approach are presented
in Section 6. In Section 7 we discuss the relevance
of our approach to other lexicalized grammars. In
Section 8 we conclude with some directions for future
work.
</bodyText>
<sectionHeader confidence="0.9859235" genericHeader="introduction">
2 Lexicalized Tree-Adjoining
Grammar
</sectionHeader>
<bodyText confidence="0.997023555555556">
Lexicalized Tree-Adjoining Grammar (LTAG) (Sch-
abes et al., 1988; Schabes, 1990) consists of ELE-
MENTARY TREES, with each elementary tree hav-
ing a lexical item (anchor) on its frontier. An el-
ementary tree serves as a complex description of
the anchor and provides a domain of locality over
which the anchor can specify syntactic and semantic
(predicate-argument) constraints. Elementary trees
are of two kinds — (a) INITIAL TREES and (b) Aux-
</bodyText>
<sectionHeader confidence="0.635161" genericHeader="method">
ILIARY TREES.
</sectionHeader>
<bodyText confidence="0.980784288888889">
Nodes on the frontier of initial trees are marked
as substitution sites by a T. Exactly one node on
the frontier of an auxiliary tree, whose label matches
the label of the root of the tree, is marked as a foot
node by a `4,&apos;; the other nodes on the frontier of an
auxiliary tree are marked as substitution sites. El-
ementary trees are combined by Substitution and
Adjunction operations.
Each node of an elementary tree is associated with
the top and the bottom feature structures (FS). The
bottom FS contains information relating to the sub-
tree rooted at the node, and the top FS contains
information relating to the supertree at that node.&apos;
The features may get their values from three differ-
ent sources such as the morphology of anchor, the
structure of the tree itself, or by unification during
the derivation process. FS are manipulated by sub-
stitution and adjunction as shown in Figure 1.
The initial trees (as) and auxiliary trees (i3s) for
the sentence show me the flights from Boston to
Philadelphia are shown in Figure 2. Due to the lim-
ited space, we have shown only the features on the ai
tree. The result of combining the elementary trees
&apos;Nodes marked for substitution are associated with
only the top FS.
shown in Figure 2 is the derived tree, shown in Fig-
ure 2(a). The process of combining the elementary
trees to yield a parse of the sentence is represented
by the derivation tree, shown in Figure 2(b). The
nodes of the derivation tree are the tree names that
are anchored by the appropriate lexical items. The
combining operation is indicated by the nature of
the arcs—broken line for substitution and bold line
for adjunction—while the address of the operation is
indicated as part of the node label. The derivation
tree can also be interpreted as a dependency tree2
with unlabeled arcs between words of the sentence
as shown in Figure 2(c).
Elementary trees of LTAG are the domains for
specifying dependencies. Recursive structures are
specified via the auxiliary trees. The three aspects
of LTAG — (a) lexicalization, (b) extended domain of
locality and (c) factoring of recursion, provide a nat-
ural means for generalization during the EBL pro-
cess.
</bodyText>
<sectionHeader confidence="0.9962505" genericHeader="method">
3 Overview of our approach to using
EBL
</sectionHeader>
<bodyText confidence="0.999674739130435">
We are pursuing the EBL approach in the context
of a wide-coverage grammar development system
called XTAG (Doran et al., 1994). The XTAG sys-
tem consists of a morphological analyzer, a part-of-
speech tagger, a wide-coverage LTAG English gram-
mar, a predictive left-to-right Early-style parser for
LTAG (Schabes, 1990) and an X-windows interface
for grammar development (Paroubek et al., 1992).
Figure 3 shows a flowchart of the XTAG system.
The input sentence is subjected to morphological
analysis and is parts-of-speech tagged before being
sent to the parser. The parser retrieves the elemen-
tary trees that the words of the sentence anchor and
combines them by adjunction and substitution op-
erations to derive a parse of the sentence.
Given this context, the training phase of the EBL
process involves generalizing the derivation trees
generated by XTAG for a training sentence and stor-
ing these generalized parses in the generalized parse
&apos;There are some differences between derivation trees
and conventional dependency trees. However we will not
discuss these differences in this paper as they are not
relevant to the present work.
</bodyText>
<page confidence="0.988413">
269
</page>
<figure confidence="0.999521727272727">
Prom
411
Ell II 8
VP yor 42.5620.61.11
oriserre re* ma
I •
sondereur
sr soar
DetP NT
P2a,a••] patt
Maw
ai
NT
DetP N
the
a2 a3 a4
• vp.ratal
MPE
NP NP
Boehm
(ea
th a a
IT
I NA
a V PIP tatno
Ara Pre PT
&amp;quot;
Er 241/1 Pr P SP
&amp;quot; I I
D F PP or N
I I Il
la PION row N
(a)
aj [1120W]
„.
a2 (me] (2.2) as [nights] (2-3)
a3 Mel (I) II/ (from] (0) 2 Etol (f))
as [Boston] (2.2) a6 [Philadelphia] (2.2)
(b)
show
me flights
to
&amp;Man Philadel
(c)
</figure>
<figureCaption confidence="0.8687195">
Figure 2: (as and 13s) Elementary trees, (a) Derived Tree, (b) Derivation Tree, and (c) Dependency tree for
the sentence: show me the flights from Boston to Philadelphia.
</figureCaption>
<page confidence="0.839652">
270
</page>
<figure confidence="0.6506435">
Input Sentence
Derivation Structure
</figure>
<figureCaption confidence="0.9484512">
Figure 3: Flowchart of the XTAG system
COMpita
Chsanlixed Pane Ch. Paw I)
Figure 4: Flowchart of the XTAG system with
the EBL component
</figureCaption>
<bodyText confidence="0.999976076923077">
database under an index computed from the mor-
phological features of the sentence. The application
phase of EBL is shown in the flowchart in Figure 4.
An index using the morphological features of the
words in the input sentence is computed. Using this
index, a set of generalized parses is retrieved from
the generalized parse database created in the train-
ing phase. If the retrieval fails to yield any gener-
alized parse then the input sentence is parsed using
the full parser. However, if the retrieval succeeds
then the generalized parses are input to the &amp;quot;sta-
pler&amp;quot;. Section 5 provides a description of the &amp;quot;sta-
pler&amp;quot;.
</bodyText>
<subsectionHeader confidence="0.9843675">
3.1 Implications of LTAG representation
for EBL
</subsectionHeader>
<bodyText confidence="0.996281368421053">
An LTAG parse of a sentence can be seen as a se-
quence of elementary trees associated with the lexi-
cal items of the sentence along with substitution and
adjunction links among the elementary trees. Also,
the feature values in the feature structures of each
node of every elementary tree are instantiated by the
parsing process. Given an LTAG parse, the general-
ization of the parse is truly immediate in that a gen-
eralized parse is obtained by (a) uninstantiating the
particular lexical items that anchor the individual el-
ementary trees in the parse and (b) uninstantiating
the feature values contributed by the morphology of
the anchor and the derivation process. This type of
generalization is called feature-generalization.
In other EBL approaches (Rayner, 1988; Neu-
mann, 1994; Samuelsson, 1994) it is necessary to
walk up and down the parse tree to determine the
appropriate subtrees to generalize on and to sup-
press the feature values. In our approach, the pro-
cess of generalization is immediate, once we have the
output of the parser, since the elementary trees an-
chored by the words of the sentence define the sub-
trees of the parse for generalization. Replacing the
elementary trees with unistantiated feature values is
all that is needed to achieve this generalization.
The generalized parse of a sentence is stored in-
dexed on the part-of-speech (POS) sequence of the
training sentence. In the application phase, the POS
sequence of the input sentence is used to retrieve a
generalized parse(s) which is then instantiated with
the features of the sentence. This method of retriev-
ing a generalized parse allows for parsing of sen-
tences of the same lengths and the same POS se-
quence as those in the training corpus. However,
in our approach there is another generalization that
falls out of the LTAG representation which allows for
flexible matching of the index to allow the system to
parse sentences that are not necessarily of the same
length as any sentence in the training corpus.
Auxiliary trees in LTAG represent recursive struc-
tures. So if there is an auxiliary tree that is used in
an LTAG parse, then that tree with the trees for
its arguments can be repeated any number of times,
or possibly omitted altogether, to get parses of sen-
tences that differ from the sentences of the training
corpus only in the number of modifiers. This type of
generalization is called modifier-generalization. This
type of generalization is not possible in other EBL
approaches.
This implies that the POS sequence covered by
the auxiliary tree and its arguments can be repeated
zero or more times. As a result, the index of a gener-
alized parse of a sentence with modifiers is no longer
a string but a regular expression pattern on the POS
sequence and retrieval of a generalized parse involves
regular expression pattern matching on the indices.
If, for example, the training example was
</bodyText>
<listItem confidence="0.99129">
(1) Show/V me/N the/D flights/N from/P
Boston/N to/P Philadelphia/N.
then, the index of this sentence is
(2) VNDN (P N)*
</listItem>
<bodyText confidence="0.994016">
since the two prepositions in the parse of this sen-
tence would anchor (the same) auxiliary trees.
</bodyText>
<table confidence="0.776500333333333">
P.O.S Moodie I.--
Lex Prob ;DI
Syn DB
</table>
<page confidence="0.989937">
271
</page>
<bodyText confidence="0.999853909090909">
The most efficient method of performing regular
expression pattern matching is to construct a finite
state machine for each of the stored patterns and
then traverse the machine using the given test pat-
tern. If the machine reaches the final state, then the
test pattern matches one of the stored patterns.
Given that the index of a test sentence matches
one of the indices from the training phase, the gen-
eralized parse retrieved will be a parse of the test
sentence, modulo the modifiers. For example, if the
test sentence, tagged appropriately, is
</bodyText>
<equation confidence="0.546018333333333">
(3) Show/V me/N the/D flights/N from/P
Boston/N to/P Philadelphia/N on/P
Monday/N.
</equation>
<bodyText confidence="0.995168625">
then, although the index of the test sentence
matches the index of the training sentence, the gen-
eralized parse retrieved needs to be augmented to
accommodate the additional modifier.
To accommodate the additional modifiers that
may be present in the test sentences, we need to pro-
vide a mechanism that assigns the additional modi-
fiers and their arguments the following:
</bodyText>
<listItem confidence="0.994223333333333">
1. The elementary trees that they anchor and
2. The substitution and adjunction links to the
trees they substitute or adjoin into.
</listItem>
<bodyText confidence="0.999963">
We assume that the additional modifiers along
with their arguments would be assigned the same
elementary trees and the same substitution and ad-
junction links as were assigned to the modifier and
its arguments of the training example. This, of
course, means that we may not get all the possi-
ble attachments of the modifiers at this time. (but
see the discussion of the &amp;quot;stapler&amp;quot; Section 5.)
</bodyText>
<sectionHeader confidence="0.998865" genericHeader="method">
4 FST Representation
</sectionHeader>
<bodyText confidence="0.99264005">
The representation in Figure 6 combines the gener-
alized parse with the POS sequence (regular expres-
sion) that it is indexed by. The idea is to annotate
each of the finite state arcs of the regular expression
matcher with the elementary tree associated with
that POS and also indicate which elementary tree it
would be adjoined or substituted into. This results
in a Finite State Transducer (FST) representation,
illustrated by the example below. Consider the sen-
tence (4) with the derivation tree in Figure 5.
(4) show me the flights from Boston to
Philadelphia.
An alternate representation of the derivation tree
that is similar to the dependency representation,
is to associate with each word a tuple (thisAree,
head_word, headAree, number). The description of
the tuple components is given in Table 1.
Following this notation, the derivation tree in Fig-
ure 5 (without the addresses of operations) is repre-
sented as in (5).
</bodyText>
<figure confidence="0.982805083333333">
ce2 [show]
,..—s.,
.....
,...• ......
J. N
62 [me] (2.2) 64 [flights] (2.3)
----------
..---- ----
63 [the] (1) 0, [from] (0) ii2 Ito] (0)
• I
1 1
as [Boston] (2.2) 0.6 [Philadelphia] (2.2)
</figure>
<figureCaption confidence="0.979259">
Figure 5: Derivation Tree for the sentence: show me
the flights from Boston to Philadelphia
</figureCaption>
<bodyText confidence="0.9833069">
thisAree the elementary tree that the word
anchors
head_word the word on which the current
word is dependent on; &amp;quot;—&amp;quot; if the
current word does not
depend on any other word.
head_tree the tree anchored by the head word;
&amp;quot;—&amp;quot; if the current word does not
depend on any other word.
number a signed number that indicates the
direction and the ordinal position of
the particular head elementary tree
from the position of the current
word OR
an unsigned number that indicates
the Gorn-address (i.e., the node
address) in the derivation tree to
which the word attaches OR
&amp;quot;—&amp;quot; if the current word does not
depend on any other word.
</bodyText>
<tableCaption confidence="0.968229">
Table 1: Description of the tuple components
</tableCaption>
<equation confidence="0.8092422">
(5)
show/(ai , —, —, —) me/(a2, show,cti,-1)
the/(a3, flights, a4,1-1) flights/(a4,show, al, -1)
from/(/31, flights, a4, 2) Boston/(a5, from, f3i. -1)
to/(,32, flights,a4, 2) Philadelphia/(a6, to, ,62, -1)
</equation>
<bodyText confidence="0.8719025">
Generalization of this derivation tree results in the
representation in (6).
</bodyText>
<equation confidence="0.99630575">
N/(az, V,ai,-1)
al, -1)
N/(a5, P, /3, -1))*
N/(a6, P, fl, -1))*
</equation>
<bodyText confidence="0.999553">
After generalization, the trees [31 and 02 are no
longer distinct so we denote them by ii. The trees
a5 and a6 are also no longer distinct, so we denote
them by a. With this change in notation, the two
Kleene star regular expressions in (6) can be merged
into one, and the resulting representation is (7)
</bodyText>
<equation confidence="0.9957372">
VAai, —, —, —)
DI(cr3, N, a4,+1)
(P/(/31, N, a4, 2)
(P/(fl2, N, a4, 2)
(6)
</equation>
<page confidence="0.995539">
272
</page>
<figureCaption confidence="0.9399955">
Figure 6: Finite State Transducer Representation for the sentences: show me the flights from Boston to
Philadelphia, show me the flights from Boston to Philadelphia on Monday, ...
</figureCaption>
<equation confidence="0.99037475">
N/( a, P , , -1)
V/(ai, —) N/(a2, V,a1,-1)
(7) D/(a3, N, a4,-F1) N/(a4Y, al, -1)
(P/(/3, N, a4, 2) N/(a, P, f3, -1) )*
</equation>
<subsectionHeader confidence="0.445423">
which can be seen as a path in an FST as in Figure 6.
</subsectionHeader>
<bodyText confidence="0.999782625">
This FST representation is possible due to the lex-
icalized nature of the elementary trees. This repre-
sentation makes a distinction between dependencies
between modifiers and complements. The number in
the tuple associated with each word is a signed num-
ber if a complement dependency is being expressed
and is an unsigned number if a modifier dependency
is being expressed.&apos;
</bodyText>
<sectionHeader confidence="0.994206" genericHeader="method">
5 Stapler
</sectionHeader>
<bodyText confidence="0.872861641025641">
In this section, we introduce a device called &amp;quot;sta-
pler&amp;quot;, a very impoverished parser that takes as in-
put the result of the EBL lookup and returns the
parse(s) for the sentence. The output of the EBL
lookup is a sequence of elementary trees annotated
with dependency links — an almost parse. To con-
struct a complete parse, the &amp;quot;stapler&amp;quot; performs the
following tasks:
• Identify the nature of link: The dependency
links in the almost parse are to be distinguished
as either substitution links or adjunction links.
This task is extremely straightforward since the
types (initial or auxiliary) of the elementary
trees a dependency link connects identifies the
nature of the link.
• Modifier Attachment: The EBL lookup is not
guaranteed to output all possible modifier-
head dependencies for a give input, since
the modifier-generalization assigns the same
modifier-head link, as was in the training ex-
ample, to all the additional modifiers. So it is
the task of the stapler to compute all the alter-
nate attachments for modifiers.
• Address of Operation: The substitution and ad-
junction links are to be assigned a node ad-
dress to indicate the location of the operation.
The &amp;quot;stapler&amp;quot; assigns this using the structure of
31n a complement auxiliary tree the anchor subcat-
egorizes for the foot node, which is not the case for a
modifier auxiliary tree.
the elementary trees that the words anchor and
their linear order in the sentence.
• Feature Instantiation: The values of the fea-
tures on the nodes of the elementary trees are
to be instantiated by a process of unification.
Since the features in LTAGs are finite-valued
and only features within an elementary tree
can be co-indexed, the &amp;quot;stapler&amp;quot; performs term-
unification to instantiate the features.
</bodyText>
<sectionHeader confidence="0.995256" genericHeader="method">
6 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999940206896552">
We now present experimental results from two dif-
ferent sets of experiments performed to show the
effectiveness of our approach. The first set of ex-
periments, (Experiments 1(a) through 1(c)), are in-
tended to measure the coverage of the FST represen-
tation of the parses of sentences from a range of cor-
pora (ATIS, IBM-Manual and Alvey). The results
of these experiments provide a measure of repeti-
tiveness of patterns as described in this paper, at
the sentence level, in each of these corpora.
Experiment 1(a): The details of the experiment
with the ATIS corpus are as follows. A total of 465
sentences, average length of 10 words per sentence,
which had been completely parsed by the XTAG sys-
tem were randomly divided into two sets, a train-
ing set of 365 sentences and a test set of 100 sen-
tences, using a random number generator. For each
of the training sentences, the parses were ranked us-
ing heuristics4 (Srinivas et al., 1994) and the top
three derivations were generalized and stored as an
FST. The FST was tested for retrieval of a gener-
alized parse for each of the test sentences that were
pretagged with the correct POS sequence (In Ex-
periment 2, we make use of the POS tagger to do
the tagging). When a match is found, the output
of the EBL component is a generalized parse that
associates with each word the elementary tree that
it anchors and the elementary tree into which it ad-
joins or substitutes into — an almost parse.5
</bodyText>
<footnote confidence="0.92352475">
&apos;We are not using stochastic LTAGs. For work on
Stochastic LTAGs see (Resnik, 1992; Schabes, 1992).
5See (Joshi and Srinivas, 1994) for the role of almost
parse in supertag disambiguation.
</footnote>
<page confidence="0.991779">
273
</page>
<table confidence="0.9330808">
Corpus Size of # of States % Coverage Response Time
Training set (secs)
ATIS 365 6000 80% 1.00 sec/sent
IBM 1100 21000 40% 4.00 sec/sent
Alvey 80 500 50% 0.20 sec/NP
</table>
<tableCaption confidence="0.998274">
Table 2: Coverage and Retrieval times for various corpora
</tableCaption>
<bodyText confidence="0.960142901960785">
Experiment 1(b) and 1(c): Similar experiments
were conducted using the IBM-manual corpus and a
set of noun definitions from the LDOCE dictionary
that were used as the Alvey test set (Carroll, 1993).
Results of these experiments are summarized in
Table 2. The size of the FST obtained for each of the
corpora, the coverage of the FST and the traversal
time per input are shown in this table. The cover-
age of the FST is the number of inputs that were as-
signed a correct generalized parse among the parses
retrieved by traversing the FST.
Since these experiments measure the performance
of the EBL component on various corpora we will
refer to these results as the &apos;EBL-Lookup times&apos;.
The second set of experiments measure the perfor-
mance improvement obtained by using EBL within
the XTAG system on the ATIS corpus. The per-
formance was measured on the same set of 100 sen-
tences that was used as test data in Experiment 1(a).
The FST constructed from the generalized parses of
the 365 ATIS sentences used in experiment 1(a) has
been used in this experiment as well.
Experiment 2(a): The performance of XTAG on
the 100 sentences is shown in the first row of Table 3.
The coverage represents the percentage of sentences
that were assigned a parse.
Experiment 2(b): This experiment is similar to
Experiment 1(a). It attempts to measure the cov-
erage and response times for retrieving a general-
ized parse from the FST. The results are shown in
the second row of Table 3. The difference in the
response times between this experiment and Exper-
iment 1(a) is due to the fact that we have included
here the times for morphological analysis and the
POS tagging of the test sentence. As before, 80%
of the sentences were assigned a generalized parse.
However, the speedup when compared to the XTAG
system is a factor of about 60.
Experiment 2(c): The setup for this experiment is
shown in Figure 7. The almost parse from the EBL
lookup is input to the full parser of the XTAG sys-
tem. The full parser does not take advantage of the
dependency information present in the almost parse,
however it benefits from the elementary tree assign-
ment to the words in it. This information helps the
full parser, by reducing the ambiguity of assigning
a correct elementary tree sequence for the words of
the sentence. The speed up shown in the third row
of Table 3 is entirely due to this ambiguity reduc-
tion. If the EBL lookup fails to retrieve a parse,
which happens for 20% of the sentences, then the
</bodyText>
<figure confidence="0.513391">
bra
Drirssios Ilree
</figure>
<figureCaption confidence="0.995653">
Figure 7: System Setup for Experiment 2(c).
</figureCaption>
<bodyText confidence="0.998184333333333">
tree assignment ambiguity is not reduced and the
full parser parses with all the trees for the words of
the sentence. The drop in coverage is due to the fact
that for 10% of the sentences, the generalized parse
retrieved could not be instantiated to the features of
the sentence.
</bodyText>
<table confidence="0.9992505">
System Coverage % Average time
(in secs)
XTAG 100% 125.18
EBL lookup 80% 1.78
EBL+XTAG parser 90% 62.93
EBL+Stapler 70% 8.00
</table>
<tableCaption confidence="0.912261">
Table 3: Performance comparison of XTAG with
and without EBL component
</tableCaption>
<bodyText confidence="0.987231733333333">
Experiment 2(d): The setup for this experiment
is shown in Figure 4. In this experiment, the almost
parse resulting from the EBL lookup is input to the
&amp;quot;stapler&amp;quot; that generates all possible modifier attach-
ments and performs term unification thus generating
all the derivation trees. The &amp;quot;stapler&amp;quot; uses both the
elementary tree assignment information and the de-
pendency information present in the almost parse
and speeds up the performance even further, by a
factor of about 15 with further decrease in coverage
by 10% due to the same reason as mentioned in Ex-
periment 2(c). However the coverage of this system
is limited by the coverage of the EBL lookup. The
results of this experiment are shown in the fourth
row of Table 3.
</bodyText>
<figure confidence="0.995034">
P.0.1 Waft
Two Medi=
Pram
Tram/ft
OP I4(
Solosire
</figure>
<page confidence="0.994821">
274
</page>
<sectionHeader confidence="0.994039" genericHeader="method">
7 Relevance to other lexicalized
grammars
</sectionHeader>
<bodyText confidence="0.999899823529412">
Some aspects of our approach can be extended to
other lexicalized grammars, in particular to catego-
rial grammars (e.g. Combinatory Categorial Gram-
mar (CCG) (Steedman, 1987)). Since in a categorial
grammar the category for a lexical item includes its
arguments, the process of generalization of the parse
can also be immediate in the same sense of our ap-
proach. The generalization over recursive structures
in a categorial grammar, however, will require fur-
ther annotations of the proof trees in order to iden-
tify the &apos;anchor&apos; of a recursive structure. If a lexi-
cal item corresponds to a potential recursive struc-
ture then it will be necessary to encode this informa-
tion by making the result part of the functor to be
X X. Further annotation of the proof tree will
be required to keep track of dependencies in order
to represent the generalized parse as an FST.
</bodyText>
<sectionHeader confidence="0.999523" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999971230769231">
In this paper, we have presented some novel applica-
tions of EBL technique to parsing LTAG. We have
also introduced a highly impoverished parser called
the &amp;quot;stapler&amp;quot; that in conjunction with the EBL re-
sults in a speed up of a factor of about 15 over a
system without the EBL component. To show the
effectiveness of our approach we have also discussed
the performance of EBL on different corpora, and
different architectures.
As part of the future work we will extend our ap-
proach to corpora with fewer repetitive sentence pat-
terns. We propose to do this by generalizing at the
phrasal level instead of at the sentence level.
</bodyText>
<sectionHeader confidence="0.999555" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999892585714286">
John Carroll. 1993. Practical Unification-based Parsing
of Natural Language. University of Cambridge, Com-
puter Laboratory, Cambridge, England.
Christy Doran, Dania Egedi, Beth Ann Hockey, B. Srini-
vas, and Martin Zaidel. 1994. XTAG System - A Wide
Coverage Grammar for English. In Proceedings of the
17th International Conference on Computational Lin-
guistics (COLING &apos;94), Kyoto, Japan, August.
Aravind K. Joshi and B. Srinivas. 1994. Disambigua-
tion of Super Parts of Speech (or Supertags): Almost
Parsing. In Proceedings of the 17&amp;quot; International Con-
ference on Computational Linguistics (COLING
Kyoto, Japan, August.
Steve Minton. 1988. Qunatitative Results concerning
the utility of Explanation-Based Learning. In Proceed-
ings of 7th AAAI Conference, pages 564-569, Saint
Paul, Minnesota.
Tom M. Mitchell, Richard M. Keller, and Smadar T.
Kedar-Carbelli. 1986. Explanation-Based Generaliza-
tion: A Unifying View. Machine Learning I, 1:47-80.
Giinter Neumann. 1994. Application of Explanation-
based Learning for Efficient Processing of Constraint-
based Grammars. In 10th IEEE Conference on Artifi-
cial Intelligence for Applications, San Antonio, Texas.
Patrick Paroubek, Yves Schabes, and Aravind K. Joshi.
1992. Xtag — a graphical workbench for developing
tree-adjoining grammars. In Third Conference on Ap-
plied Natural Language Processing, Trento, Italy.
Manny Rayner. 1988. Applying Explanation-Based
Generalization to Natural Language Processing. In
Proceedings of the International Conference on Fifth
Generation Computer Systems, Tokyo.
Philip Resnik. 1992. Probabilistic tree-adjoining gram-
mar as a framework for statistical natural language
processing. In Proceedings of the Fourteenth In-
ternational Conference on Computational Linguistics
(COLING &apos;92), Nantes, France, July.
Christer Sainuelsson and Manny Rayner. 1991. Quan-
titative Evaluation of Explanation-Based Learning as
an Optimization Tool for Large-Scale Natural Lan-
guage System. In Proceedings of the 12th Interna-
tional Joint Conference on Artificial Intelligence, Syd-
ney,Australia.
Chister Samuelsson. 1994. Grammar Specialization
through Entropy Thresholds. In 32nd Meeting of
the Association for Computational Linguistics, Las
Cruces, New Mexico.
Yves Schabes, Anne Abeille, and Aravind K. Joshi.
1988. Parsing strategies with `lexicalized&apos; grammars:
Application to Tree Adjoining Grammars. In Pro-
ceedings of the 12&amp;quot; International Conference on Com-
putational Linguistics (COLING&apos;88), Budapest, Hun-
gary, August.
Yves Schabes. 1990. Mathematical and Computational
Aspects of Lexicalized Grammars. Ph.D. thesis, Com-
puter Science Department, University of Pennsylva-
nia.
Yves Schabes. 1992. Stochastic lexicalized tree-
adjoining grammars. In Proceedings of the Fourteenth
International Conference on Computational Linguis-
tics (COLING &apos;92), Nantes, France, July.
B. Srinivas, Christine Doran, Seth Kulick, and Anoop
Sarkar. 1994. Evaluating a wide-coverage grammar.
Manuscript, October.
Mark Steedman. 1987. Combinatory Grammars and
Parasitic Gaps. Natural Language and Linguistic The-
ory, 5:403-439.
Frank van Harmelen and Allan Bundy. 1988.
Explanation-Based Generalization = Partial Evalua-
tion. Artificial Intelligence, 36:401-412.
</reference>
<page confidence="0.998445">
275
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.662794">
<title confidence="0.9987825">Some Novel Applications of Explanation-Based Learning to Parsing Lexicalized Tree-Adjoining Grammars.</title>
<author confidence="0.999689">B Srinivas</author>
<author confidence="0.999689">Aravind K Joshi</author>
<affiliation confidence="0.999908">Department of Computer and Information Science University of Pennsylvania</affiliation>
<address confidence="0.999869">Philadelphia, PA 19104, USA</address>
<email confidence="0.999912">srini@linc.cis.upenn.edu</email>
<email confidence="0.999912">joshi@linc.cis.upenn.edu</email>
<abstract confidence="0.976370666666667">In this paper we present some novel applications of Explanation-Based Learning (EBL) technique to parsing Lexicalized Tree-Adjoining grammars. The novel aspects are (a) immediate generalization of parses in the training set, (b) generalization over recursive structures and (c) representation of generalized parses as Finite State Transducers. A highly impoverished parser called a &amp;quot;stapler&amp;quot; has also been introduced. We present experimental results using EBL for different corpora and architectures to show the effectiveness of our approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Carroll</author>
</authors>
<title>Practical Unification-based Parsing of Natural Language.</title>
<date>1993</date>
<institution>University of Cambridge, Computer Laboratory,</institution>
<location>Cambridge, England.</location>
<contexts>
<context position="21765" citStr="Carroll, 1993" startWordPosition="3684" endWordPosition="3685">&apos;We are not using stochastic LTAGs. For work on Stochastic LTAGs see (Resnik, 1992; Schabes, 1992). 5See (Joshi and Srinivas, 1994) for the role of almost parse in supertag disambiguation. 273 Corpus Size of # of States % Coverage Response Time Training set (secs) ATIS 365 6000 80% 1.00 sec/sent IBM 1100 21000 40% 4.00 sec/sent Alvey 80 500 50% 0.20 sec/NP Table 2: Coverage and Retrieval times for various corpora Experiment 1(b) and 1(c): Similar experiments were conducted using the IBM-manual corpus and a set of noun definitions from the LDOCE dictionary that were used as the Alvey test set (Carroll, 1993). Results of these experiments are summarized in Table 2. The size of the FST obtained for each of the corpora, the coverage of the FST and the traversal time per input are shown in this table. The coverage of the FST is the number of inputs that were assigned a correct generalized parse among the parses retrieved by traversing the FST. Since these experiments measure the performance of the EBL component on various corpora we will refer to these results as the &apos;EBL-Lookup times&apos;. The second set of experiments measure the performance improvement obtained by using EBL within the XTAG system on t</context>
</contexts>
<marker>Carroll, 1993</marker>
<rawString>John Carroll. 1993. Practical Unification-based Parsing of Natural Language. University of Cambridge, Computer Laboratory, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christy Doran</author>
<author>Dania Egedi</author>
<author>Beth Ann Hockey</author>
<author>B Srinivas</author>
<author>Martin Zaidel</author>
</authors>
<title>XTAG System - A Wide Coverage Grammar for English.</title>
<date>1994</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics (COLING &apos;94),</booktitle>
<location>Kyoto, Japan,</location>
<contexts>
<context position="7460" citStr="Doran et al., 1994" startWordPosition="1232" endWordPosition="1235">l. The derivation tree can also be interpreted as a dependency tree2 with unlabeled arcs between words of the sentence as shown in Figure 2(c). Elementary trees of LTAG are the domains for specifying dependencies. Recursive structures are specified via the auxiliary trees. The three aspects of LTAG — (a) lexicalization, (b) extended domain of locality and (c) factoring of recursion, provide a natural means for generalization during the EBL process. 3 Overview of our approach to using EBL We are pursuing the EBL approach in the context of a wide-coverage grammar development system called XTAG (Doran et al., 1994). The XTAG system consists of a morphological analyzer, a part-ofspeech tagger, a wide-coverage LTAG English grammar, a predictive left-to-right Early-style parser for LTAG (Schabes, 1990) and an X-windows interface for grammar development (Paroubek et al., 1992). Figure 3 shows a flowchart of the XTAG system. The input sentence is subjected to morphological analysis and is parts-of-speech tagged before being sent to the parser. The parser retrieves the elementary trees that the words of the sentence anchor and combines them by adjunction and substitution operations to derive a parse of the se</context>
</contexts>
<marker>Doran, Egedi, Hockey, Srinivas, Zaidel, 1994</marker>
<rawString>Christy Doran, Dania Egedi, Beth Ann Hockey, B. Srinivas, and Martin Zaidel. 1994. XTAG System - A Wide Coverage Grammar for English. In Proceedings of the 17th International Conference on Computational Linguistics (COLING &apos;94), Kyoto, Japan, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>B Srinivas</author>
</authors>
<title>Disambiguation of Super Parts of Speech (or Supertags): Almost Parsing.</title>
<date>1994</date>
<booktitle>In Proceedings of the 17&amp;quot; International Conference on Computational Linguistics (COLING Kyoto,</booktitle>
<location>Japan,</location>
<contexts>
<context position="21282" citStr="Joshi and Srinivas, 1994" startWordPosition="3600" endWordPosition="3603"> three derivations were generalized and stored as an FST. The FST was tested for retrieval of a generalized parse for each of the test sentences that were pretagged with the correct POS sequence (In Experiment 2, we make use of the POS tagger to do the tagging). When a match is found, the output of the EBL component is a generalized parse that associates with each word the elementary tree that it anchors and the elementary tree into which it adjoins or substitutes into — an almost parse.5 &apos;We are not using stochastic LTAGs. For work on Stochastic LTAGs see (Resnik, 1992; Schabes, 1992). 5See (Joshi and Srinivas, 1994) for the role of almost parse in supertag disambiguation. 273 Corpus Size of # of States % Coverage Response Time Training set (secs) ATIS 365 6000 80% 1.00 sec/sent IBM 1100 21000 40% 4.00 sec/sent Alvey 80 500 50% 0.20 sec/NP Table 2: Coverage and Retrieval times for various corpora Experiment 1(b) and 1(c): Similar experiments were conducted using the IBM-manual corpus and a set of noun definitions from the LDOCE dictionary that were used as the Alvey test set (Carroll, 1993). Results of these experiments are summarized in Table 2. The size of the FST obtained for each of the corpora, the c</context>
</contexts>
<marker>Joshi, Srinivas, 1994</marker>
<rawString>Aravind K. Joshi and B. Srinivas. 1994. Disambiguation of Super Parts of Speech (or Supertags): Almost Parsing. In Proceedings of the 17&amp;quot; International Conference on Computational Linguistics (COLING Kyoto, Japan, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Minton</author>
</authors>
<title>Qunatitative Results concerning the utility of Explanation-Based Learning.</title>
<date>1988</date>
<booktitle>In Proceedings of 7th AAAI Conference,</booktitle>
<pages>564--569</pages>
<location>Saint Paul, Minnesota.</location>
<contexts>
<context position="1104" citStr="Minton, 1988" startWordPosition="154" endWordPosition="155">raining set, (b) generalization over recursive structures and (c) representation of generalized parses as Finite State Transducers. A highly impoverished parser called a &amp;quot;stapler&amp;quot; has also been introduced. We present experimental results using EBL for different corpora and architectures to show the effectiveness of our approach. 1 Introduction In this paper we present some novel applications of the so-called Explanation-Based Learning technique (EBL) to parsing Lexicalized Tree-Adjoining grammars (LTAG). EBL techniques were originally introduced in the Al literature by (Mitchell et al., 1986; Minton, 1988; van Harmelen and Bundy, 1988). The main idea of EBL is to keep track of problems solved in the past and to replay those solutions to solve new but somewhat similar problems in the future. Although put in these general terms the approach sounds attractive, it is by no means clear that EBL will actually improve the performance of the system using it, an aspect which is of great interest to us here. Rayner (1988) was the first to investigate this technique in the context of natural language parsing. Seen as an EBL problem, the parse of a single sentence represents an explanation of why the sent</context>
</contexts>
<marker>Minton, 1988</marker>
<rawString>Steve Minton. 1988. Qunatitative Results concerning the utility of Explanation-Based Learning. In Proceedings of 7th AAAI Conference, pages 564-569, Saint Paul, Minnesota.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom M Mitchell</author>
<author>Richard M Keller</author>
<author>Smadar T Kedar-Carbelli</author>
</authors>
<title>Explanation-Based Generalization: A Unifying View. Machine Learning I,</title>
<date>1986</date>
<pages>1--47</pages>
<contexts>
<context position="1090" citStr="Mitchell et al., 1986" startWordPosition="150" endWordPosition="153">tion of parses in the training set, (b) generalization over recursive structures and (c) representation of generalized parses as Finite State Transducers. A highly impoverished parser called a &amp;quot;stapler&amp;quot; has also been introduced. We present experimental results using EBL for different corpora and architectures to show the effectiveness of our approach. 1 Introduction In this paper we present some novel applications of the so-called Explanation-Based Learning technique (EBL) to parsing Lexicalized Tree-Adjoining grammars (LTAG). EBL techniques were originally introduced in the Al literature by (Mitchell et al., 1986; Minton, 1988; van Harmelen and Bundy, 1988). The main idea of EBL is to keep track of problems solved in the past and to replay those solutions to solve new but somewhat similar problems in the future. Although put in these general terms the approach sounds attractive, it is by no means clear that EBL will actually improve the performance of the system using it, an aspect which is of great interest to us here. Rayner (1988) was the first to investigate this technique in the context of natural language parsing. Seen as an EBL problem, the parse of a single sentence represents an explanation o</context>
</contexts>
<marker>Mitchell, Keller, Kedar-Carbelli, 1986</marker>
<rawString>Tom M. Mitchell, Richard M. Keller, and Smadar T. Kedar-Carbelli. 1986. Explanation-Based Generalization: A Unifying View. Machine Learning I, 1:47-80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giinter Neumann</author>
</authors>
<title>Application of Explanationbased Learning for Efficient Processing of Constraintbased Grammars.</title>
<date>1994</date>
<booktitle>In 10th IEEE Conference on Artificial Intelligence for Applications,</booktitle>
<location>San Antonio, Texas.</location>
<contexts>
<context position="2654" citStr="Neumann (1994)" startWordPosition="411" endWordPosition="412"> Program (PA) grant 93S.3078C-6 Rayner (1991) specialize a grammar for the ATIS domain by storing chunks of the parse trees present in a treebank of parsed examples. The idea is to reparse the training examples by letting the parse tree drive the rule expansion process and halting the expansion of a specialized rule if the current node meets a &apos;tree-cutting&apos; criteria. However, the problem of specifying an optimal &apos;tree-cutting&apos; criteria was not addressed in this work. Samuelsson (1994) used the information-theoretic measure of entropy to derive the appropriate sized tree chunks automatically. Neumann (1994) also attempts to specialize a grammar given a training corpus of parsed examples by generalizing the parse for each sentence and storing the generalized phrasal derivations under a suitable index. Although our work can be considered to be in this general direction, it is distinct in that it exploits some of the key properties of LTAG to (a) achieve an immediate generalization of parses in the training set of sentences, (b) achieve an additional level of generalization of the parses in the training set, thereby dealing with test sentences which are not necessarily of the same length as the tra</context>
<context position="10691" citStr="Neumann, 1994" startWordPosition="1792" endWordPosition="1794">inks among the elementary trees. Also, the feature values in the feature structures of each node of every elementary tree are instantiated by the parsing process. Given an LTAG parse, the generalization of the parse is truly immediate in that a generalized parse is obtained by (a) uninstantiating the particular lexical items that anchor the individual elementary trees in the parse and (b) uninstantiating the feature values contributed by the morphology of the anchor and the derivation process. This type of generalization is called feature-generalization. In other EBL approaches (Rayner, 1988; Neumann, 1994; Samuelsson, 1994) it is necessary to walk up and down the parse tree to determine the appropriate subtrees to generalize on and to suppress the feature values. In our approach, the process of generalization is immediate, once we have the output of the parser, since the elementary trees anchored by the words of the sentence define the subtrees of the parse for generalization. Replacing the elementary trees with unistantiated feature values is all that is needed to achieve this generalization. The generalized parse of a sentence is stored indexed on the part-of-speech (POS) sequence of the tra</context>
</contexts>
<marker>Neumann, 1994</marker>
<rawString>Giinter Neumann. 1994. Application of Explanationbased Learning for Efficient Processing of Constraintbased Grammars. In 10th IEEE Conference on Artificial Intelligence for Applications, San Antonio, Texas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Paroubek</author>
<author>Yves Schabes</author>
<author>Aravind K Joshi</author>
</authors>
<title>Xtag — a graphical workbench for developing tree-adjoining grammars.</title>
<date>1992</date>
<booktitle>In Third Conference on Applied Natural Language Processing,</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="7723" citStr="Paroubek et al., 1992" startWordPosition="1271" endWordPosition="1274">iliary trees. The three aspects of LTAG — (a) lexicalization, (b) extended domain of locality and (c) factoring of recursion, provide a natural means for generalization during the EBL process. 3 Overview of our approach to using EBL We are pursuing the EBL approach in the context of a wide-coverage grammar development system called XTAG (Doran et al., 1994). The XTAG system consists of a morphological analyzer, a part-ofspeech tagger, a wide-coverage LTAG English grammar, a predictive left-to-right Early-style parser for LTAG (Schabes, 1990) and an X-windows interface for grammar development (Paroubek et al., 1992). Figure 3 shows a flowchart of the XTAG system. The input sentence is subjected to morphological analysis and is parts-of-speech tagged before being sent to the parser. The parser retrieves the elementary trees that the words of the sentence anchor and combines them by adjunction and substitution operations to derive a parse of the sentence. Given this context, the training phase of the EBL process involves generalizing the derivation trees generated by XTAG for a training sentence and storing these generalized parses in the generalized parse &apos;There are some differences between derivation tre</context>
</contexts>
<marker>Paroubek, Schabes, Joshi, 1992</marker>
<rawString>Patrick Paroubek, Yves Schabes, and Aravind K. Joshi. 1992. Xtag — a graphical workbench for developing tree-adjoining grammars. In Third Conference on Applied Natural Language Processing, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manny Rayner</author>
</authors>
<title>Applying Explanation-Based Generalization to Natural Language Processing.</title>
<date>1988</date>
<booktitle>In Proceedings of the International Conference on Fifth Generation Computer Systems,</booktitle>
<location>Tokyo.</location>
<contexts>
<context position="1519" citStr="Rayner (1988)" startWordPosition="229" endWordPosition="230">planation-Based Learning technique (EBL) to parsing Lexicalized Tree-Adjoining grammars (LTAG). EBL techniques were originally introduced in the Al literature by (Mitchell et al., 1986; Minton, 1988; van Harmelen and Bundy, 1988). The main idea of EBL is to keep track of problems solved in the past and to replay those solutions to solve new but somewhat similar problems in the future. Although put in these general terms the approach sounds attractive, it is by no means clear that EBL will actually improve the performance of the system using it, an aspect which is of great interest to us here. Rayner (1988) was the first to investigate this technique in the context of natural language parsing. Seen as an EBL problem, the parse of a single sentence represents an explanation of why the sentence is a part of the language defined by the grammar. Parsing new sentences amounts to finding analogous explanations from the training sentences. As a special case of EBL, Samuelsson and This work was partially supported by ARO grant DAAL03-89-0031, ARPA grant N00014-90-J-1863, NSF STC grant DIR-8920230, and Ben Franklin Partnership Program (PA) grant 93S.3078C-6 Rayner (1991) specialize a grammar for the ATIS</context>
<context position="10676" citStr="Rayner, 1988" startWordPosition="1790" endWordPosition="1791">d adjunction links among the elementary trees. Also, the feature values in the feature structures of each node of every elementary tree are instantiated by the parsing process. Given an LTAG parse, the generalization of the parse is truly immediate in that a generalized parse is obtained by (a) uninstantiating the particular lexical items that anchor the individual elementary trees in the parse and (b) uninstantiating the feature values contributed by the morphology of the anchor and the derivation process. This type of generalization is called feature-generalization. In other EBL approaches (Rayner, 1988; Neumann, 1994; Samuelsson, 1994) it is necessary to walk up and down the parse tree to determine the appropriate subtrees to generalize on and to suppress the feature values. In our approach, the process of generalization is immediate, once we have the output of the parser, since the elementary trees anchored by the words of the sentence define the subtrees of the parse for generalization. Replacing the elementary trees with unistantiated feature values is all that is needed to achieve this generalization. The generalized parse of a sentence is stored indexed on the part-of-speech (POS) sequ</context>
</contexts>
<marker>Rayner, 1988</marker>
<rawString>Manny Rayner. 1988. Applying Explanation-Based Generalization to Natural Language Processing. In Proceedings of the International Conference on Fifth Generation Computer Systems, Tokyo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Probabilistic tree-adjoining grammar as a framework for statistical natural language processing.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fourteenth International Conference on Computational Linguistics (COLING &apos;92),</booktitle>
<location>Nantes, France,</location>
<contexts>
<context position="21233" citStr="Resnik, 1992" startWordPosition="3595" endWordPosition="3596"> (Srinivas et al., 1994) and the top three derivations were generalized and stored as an FST. The FST was tested for retrieval of a generalized parse for each of the test sentences that were pretagged with the correct POS sequence (In Experiment 2, we make use of the POS tagger to do the tagging). When a match is found, the output of the EBL component is a generalized parse that associates with each word the elementary tree that it anchors and the elementary tree into which it adjoins or substitutes into — an almost parse.5 &apos;We are not using stochastic LTAGs. For work on Stochastic LTAGs see (Resnik, 1992; Schabes, 1992). 5See (Joshi and Srinivas, 1994) for the role of almost parse in supertag disambiguation. 273 Corpus Size of # of States % Coverage Response Time Training set (secs) ATIS 365 6000 80% 1.00 sec/sent IBM 1100 21000 40% 4.00 sec/sent Alvey 80 500 50% 0.20 sec/NP Table 2: Coverage and Retrieval times for various corpora Experiment 1(b) and 1(c): Similar experiments were conducted using the IBM-manual corpus and a set of noun definitions from the LDOCE dictionary that were used as the Alvey test set (Carroll, 1993). Results of these experiments are summarized in Table 2. The size o</context>
</contexts>
<marker>Resnik, 1992</marker>
<rawString>Philip Resnik. 1992. Probabilistic tree-adjoining grammar as a framework for statistical natural language processing. In Proceedings of the Fourteenth International Conference on Computational Linguistics (COLING &apos;92), Nantes, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christer Sainuelsson</author>
<author>Manny Rayner</author>
</authors>
<title>Quantitative Evaluation of Explanation-Based Learning as an Optimization Tool for Large-Scale Natural Language System.</title>
<date>1991</date>
<booktitle>In Proceedings of the 12th International Joint Conference on Artificial Intelligence, Sydney,Australia.</booktitle>
<marker>Sainuelsson, Rayner, 1991</marker>
<rawString>Christer Sainuelsson and Manny Rayner. 1991. Quantitative Evaluation of Explanation-Based Learning as an Optimization Tool for Large-Scale Natural Language System. In Proceedings of the 12th International Joint Conference on Artificial Intelligence, Sydney,Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chister Samuelsson</author>
</authors>
<title>Grammar Specialization through Entropy Thresholds.</title>
<date>1994</date>
<booktitle>In 32nd Meeting of the Association for Computational Linguistics,</booktitle>
<location>Las Cruces, New Mexico.</location>
<contexts>
<context position="2530" citStr="Samuelsson (1994)" startWordPosition="394" endWordPosition="395">lly supported by ARO grant DAAL03-89-0031, ARPA grant N00014-90-J-1863, NSF STC grant DIR-8920230, and Ben Franklin Partnership Program (PA) grant 93S.3078C-6 Rayner (1991) specialize a grammar for the ATIS domain by storing chunks of the parse trees present in a treebank of parsed examples. The idea is to reparse the training examples by letting the parse tree drive the rule expansion process and halting the expansion of a specialized rule if the current node meets a &apos;tree-cutting&apos; criteria. However, the problem of specifying an optimal &apos;tree-cutting&apos; criteria was not addressed in this work. Samuelsson (1994) used the information-theoretic measure of entropy to derive the appropriate sized tree chunks automatically. Neumann (1994) also attempts to specialize a grammar given a training corpus of parsed examples by generalizing the parse for each sentence and storing the generalized phrasal derivations under a suitable index. Although our work can be considered to be in this general direction, it is distinct in that it exploits some of the key properties of LTAG to (a) achieve an immediate generalization of parses in the training set of sentences, (b) achieve an additional level of generalization of</context>
<context position="10710" citStr="Samuelsson, 1994" startWordPosition="1795" endWordPosition="1796">elementary trees. Also, the feature values in the feature structures of each node of every elementary tree are instantiated by the parsing process. Given an LTAG parse, the generalization of the parse is truly immediate in that a generalized parse is obtained by (a) uninstantiating the particular lexical items that anchor the individual elementary trees in the parse and (b) uninstantiating the feature values contributed by the morphology of the anchor and the derivation process. This type of generalization is called feature-generalization. In other EBL approaches (Rayner, 1988; Neumann, 1994; Samuelsson, 1994) it is necessary to walk up and down the parse tree to determine the appropriate subtrees to generalize on and to suppress the feature values. In our approach, the process of generalization is immediate, once we have the output of the parser, since the elementary trees anchored by the words of the sentence define the subtrees of the parse for generalization. Replacing the elementary trees with unistantiated feature values is all that is needed to achieve this generalization. The generalized parse of a sentence is stored indexed on the part-of-speech (POS) sequence of the training sentence. In </context>
</contexts>
<marker>Samuelsson, 1994</marker>
<rawString>Chister Samuelsson. 1994. Grammar Specialization through Entropy Thresholds. In 32nd Meeting of the Association for Computational Linguistics, Las Cruces, New Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Anne Abeille</author>
<author>Aravind K Joshi</author>
</authors>
<title>Parsing strategies with `lexicalized&apos; grammars: Application to Tree Adjoining Grammars.</title>
<date>1988</date>
<booktitle>In Proceedings of the 12&amp;quot; International Conference on Computational Linguistics (COLING&apos;88),</booktitle>
<location>Budapest, Hungary,</location>
<contexts>
<context position="4736" citStr="Schabes et al., 1988" startWordPosition="764" endWordPosition="768">of an example. In Section 3 we discuss our approach to using EBL and the advantages provided 268 Figure 1: Substitution and Adjunction in LTAG 7.&gt; (a) (b) by LTAG. The FST representation used for EBL is illustrated in Section 4. In Section 5 we present the &amp;quot;stapler&amp;quot; in some detail. The results of some of the experiments based on our approach are presented in Section 6. In Section 7 we discuss the relevance of our approach to other lexicalized grammars. In Section 8 we conclude with some directions for future work. 2 Lexicalized Tree-Adjoining Grammar Lexicalized Tree-Adjoining Grammar (LTAG) (Schabes et al., 1988; Schabes, 1990) consists of ELEMENTARY TREES, with each elementary tree having a lexical item (anchor) on its frontier. An elementary tree serves as a complex description of the anchor and provides a domain of locality over which the anchor can specify syntactic and semantic (predicate-argument) constraints. Elementary trees are of two kinds — (a) INITIAL TREES and (b) AuxILIARY TREES. Nodes on the frontier of initial trees are marked as substitution sites by a T. Exactly one node on the frontier of an auxiliary tree, whose label matches the label of the root of the tree, is marked as a foot </context>
</contexts>
<marker>Schabes, Abeille, Joshi, 1988</marker>
<rawString>Yves Schabes, Anne Abeille, and Aravind K. Joshi. 1988. Parsing strategies with `lexicalized&apos; grammars: Application to Tree Adjoining Grammars. In Proceedings of the 12&amp;quot; International Conference on Computational Linguistics (COLING&apos;88), Budapest, Hungary, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
</authors>
<title>Mathematical and Computational Aspects of Lexicalized Grammars.</title>
<date>1990</date>
<tech>Ph.D. thesis,</tech>
<institution>Computer Science Department, University of Pennsylvania.</institution>
<contexts>
<context position="4752" citStr="Schabes, 1990" startWordPosition="769" endWordPosition="770">ion 3 we discuss our approach to using EBL and the advantages provided 268 Figure 1: Substitution and Adjunction in LTAG 7.&gt; (a) (b) by LTAG. The FST representation used for EBL is illustrated in Section 4. In Section 5 we present the &amp;quot;stapler&amp;quot; in some detail. The results of some of the experiments based on our approach are presented in Section 6. In Section 7 we discuss the relevance of our approach to other lexicalized grammars. In Section 8 we conclude with some directions for future work. 2 Lexicalized Tree-Adjoining Grammar Lexicalized Tree-Adjoining Grammar (LTAG) (Schabes et al., 1988; Schabes, 1990) consists of ELEMENTARY TREES, with each elementary tree having a lexical item (anchor) on its frontier. An elementary tree serves as a complex description of the anchor and provides a domain of locality over which the anchor can specify syntactic and semantic (predicate-argument) constraints. Elementary trees are of two kinds — (a) INITIAL TREES and (b) AuxILIARY TREES. Nodes on the frontier of initial trees are marked as substitution sites by a T. Exactly one node on the frontier of an auxiliary tree, whose label matches the label of the root of the tree, is marked as a foot node by a `4,&apos;; </context>
<context position="7648" citStr="Schabes, 1990" startWordPosition="1262" endWordPosition="1263">ifying dependencies. Recursive structures are specified via the auxiliary trees. The three aspects of LTAG — (a) lexicalization, (b) extended domain of locality and (c) factoring of recursion, provide a natural means for generalization during the EBL process. 3 Overview of our approach to using EBL We are pursuing the EBL approach in the context of a wide-coverage grammar development system called XTAG (Doran et al., 1994). The XTAG system consists of a morphological analyzer, a part-ofspeech tagger, a wide-coverage LTAG English grammar, a predictive left-to-right Early-style parser for LTAG (Schabes, 1990) and an X-windows interface for grammar development (Paroubek et al., 1992). Figure 3 shows a flowchart of the XTAG system. The input sentence is subjected to morphological analysis and is parts-of-speech tagged before being sent to the parser. The parser retrieves the elementary trees that the words of the sentence anchor and combines them by adjunction and substitution operations to derive a parse of the sentence. Given this context, the training phase of the EBL process involves generalizing the derivation trees generated by XTAG for a training sentence and storing these generalized parses </context>
</contexts>
<marker>Schabes, 1990</marker>
<rawString>Yves Schabes. 1990. Mathematical and Computational Aspects of Lexicalized Grammars. Ph.D. thesis, Computer Science Department, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
</authors>
<title>Stochastic lexicalized treeadjoining grammars.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fourteenth International Conference on Computational Linguistics (COLING &apos;92),</booktitle>
<location>Nantes, France,</location>
<contexts>
<context position="21249" citStr="Schabes, 1992" startWordPosition="3597" endWordPosition="3598">al., 1994) and the top three derivations were generalized and stored as an FST. The FST was tested for retrieval of a generalized parse for each of the test sentences that were pretagged with the correct POS sequence (In Experiment 2, we make use of the POS tagger to do the tagging). When a match is found, the output of the EBL component is a generalized parse that associates with each word the elementary tree that it anchors and the elementary tree into which it adjoins or substitutes into — an almost parse.5 &apos;We are not using stochastic LTAGs. For work on Stochastic LTAGs see (Resnik, 1992; Schabes, 1992). 5See (Joshi and Srinivas, 1994) for the role of almost parse in supertag disambiguation. 273 Corpus Size of # of States % Coverage Response Time Training set (secs) ATIS 365 6000 80% 1.00 sec/sent IBM 1100 21000 40% 4.00 sec/sent Alvey 80 500 50% 0.20 sec/NP Table 2: Coverage and Retrieval times for various corpora Experiment 1(b) and 1(c): Similar experiments were conducted using the IBM-manual corpus and a set of noun definitions from the LDOCE dictionary that were used as the Alvey test set (Carroll, 1993). Results of these experiments are summarized in Table 2. The size of the FST obtain</context>
</contexts>
<marker>Schabes, 1992</marker>
<rawString>Yves Schabes. 1992. Stochastic lexicalized treeadjoining grammars. In Proceedings of the Fourteenth International Conference on Computational Linguistics (COLING &apos;92), Nantes, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Srinivas</author>
<author>Christine Doran</author>
<author>Seth Kulick</author>
<author>Anoop Sarkar</author>
</authors>
<title>Evaluating a wide-coverage grammar.</title>
<date>1994</date>
<tech>Manuscript,</tech>
<contexts>
<context position="20645" citStr="Srinivas et al., 1994" startWordPosition="3485" endWordPosition="3488">S, IBM-Manual and Alvey). The results of these experiments provide a measure of repetitiveness of patterns as described in this paper, at the sentence level, in each of these corpora. Experiment 1(a): The details of the experiment with the ATIS corpus are as follows. A total of 465 sentences, average length of 10 words per sentence, which had been completely parsed by the XTAG system were randomly divided into two sets, a training set of 365 sentences and a test set of 100 sentences, using a random number generator. For each of the training sentences, the parses were ranked using heuristics4 (Srinivas et al., 1994) and the top three derivations were generalized and stored as an FST. The FST was tested for retrieval of a generalized parse for each of the test sentences that were pretagged with the correct POS sequence (In Experiment 2, we make use of the POS tagger to do the tagging). When a match is found, the output of the EBL component is a generalized parse that associates with each word the elementary tree that it anchors and the elementary tree into which it adjoins or substitutes into — an almost parse.5 &apos;We are not using stochastic LTAGs. For work on Stochastic LTAGs see (Resnik, 1992; Schabes, 1</context>
</contexts>
<marker>Srinivas, Doran, Kulick, Sarkar, 1994</marker>
<rawString>B. Srinivas, Christine Doran, Seth Kulick, and Anoop Sarkar. 1994. Evaluating a wide-coverage grammar. Manuscript, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>Combinatory Grammars and Parasitic Gaps. Natural Language and Linguistic Theory,</title>
<date>1987</date>
<pages>5--403</pages>
<contexts>
<context position="25577" citStr="Steedman, 1987" startWordPosition="4348" endWordPosition="4349"> present in the almost parse and speeds up the performance even further, by a factor of about 15 with further decrease in coverage by 10% due to the same reason as mentioned in Experiment 2(c). However the coverage of this system is limited by the coverage of the EBL lookup. The results of this experiment are shown in the fourth row of Table 3. P.0.1 Waft Two Medi= Pram Tram/ft OP I4( Solosire 274 7 Relevance to other lexicalized grammars Some aspects of our approach can be extended to other lexicalized grammars, in particular to categorial grammars (e.g. Combinatory Categorial Grammar (CCG) (Steedman, 1987)). Since in a categorial grammar the category for a lexical item includes its arguments, the process of generalization of the parse can also be immediate in the same sense of our approach. The generalization over recursive structures in a categorial grammar, however, will require further annotations of the proof trees in order to identify the &apos;anchor&apos; of a recursive structure. If a lexical item corresponds to a potential recursive structure then it will be necessary to encode this information by making the result part of the functor to be X X. Further annotation of the proof tree will be requi</context>
</contexts>
<marker>Steedman, 1987</marker>
<rawString>Mark Steedman. 1987. Combinatory Grammars and Parasitic Gaps. Natural Language and Linguistic Theory, 5:403-439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank van Harmelen</author>
<author>Allan Bundy</author>
</authors>
<date>1988</date>
<journal>Explanation-Based Generalization = Partial Evaluation. Artificial Intelligence,</journal>
<pages>36--401</pages>
<marker>van Harmelen, Bundy, 1988</marker>
<rawString>Frank van Harmelen and Allan Bundy. 1988. Explanation-Based Generalization = Partial Evaluation. Artificial Intelligence, 36:401-412.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>