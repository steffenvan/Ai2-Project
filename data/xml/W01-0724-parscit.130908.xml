<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017325">
<title confidence="0.887499">
Memory-Based Clause Identification
</title>
<author confidence="0.8667">
Erik F. Tjong Kim Sang
</author>
<affiliation confidence="0.8390585">
CNTS - Language Technology Group
University of Antwerp
</affiliation>
<email confidence="0.509078">
eriktOuia.ua.ac.be
</email>
<sectionHeader confidence="0.995397" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999989285714286">
We apply a memory-based learner to the
CoNLL-2001 shared task: clause identification
(Tjong Kim Sang and Dejean, 2001). The task
is divided in three parts. The first two parts are
classification tasks: identifying the positions of
clause starts and clause ends given a word, its
part-of-speech tag and the syntactic base chunk
it belongs to. Our memory-based learner can be
applied to these tasks in a straightforward way.
The third part of the shared task is identifying
complete embedded clauses. We will perform
this task by first identifying clause starts and
clause ends and then combining these to clauses
with a set of conversion rules.
</bodyText>
<sectionHeader confidence="0.991168" genericHeader="keywords">
2 Approach
</sectionHeader>
<bodyText confidence="0.999916090909091">
The first two parts of the CoNLL-2001 shared
task are similar to the CoNLL-2000 shared task:
classify words in context according to some tag-
ging scheme. We have participated in the lat-
ter shared task (Tjong Kim Sang, 2000) and
we will use a similar approach for the first two
parts of the 2001 shared task. The goal of these
parts is to predict if a word is the first word of a
clause or not (part 1) and if it is the final word
of a clause or not (part 2). We have used the
memory-based classifier TiMBL (Daelemans et
al., 2000) for predicting the most likely classi-
fication of each word. Memory-based learners
store all training data and determine a classifi-
cation for new data by examining the classifica-
tions of training data which are similar to the
new data. Each item is represented by a set of
feature-value pairs. The features have weights
which encode their relevance to the classifica-
tion of the training data items (Daelemans et
al., 2000).
Although the memory-based learner is able
to find a sensible feature weight set, it is not
guaranteed to find the best feature weight set.
In an earlier study (Tjong Kim Sang and Veen-
stra, 1999), we have reported that the number
of features supplied to the system has an influ-
ence on the performance and that the maximal
number of features not necessarily provided the
best results. In order to maximize the system&apos;s
performance, we have evaluated seven combina-
tions of the available three feature types (words,
part-of-speech (POS) tags and clause tags):
</bodyText>
<listItem confidence="0.997265142857143">
1. words only (w)
2. POS tags only (p)
3. clause tags only (c)
4. words and POS tags (wp)
5. words and clause tags (wc)
6. POS tags and clause tags (pc)
7. words, POS tags and clause tags (wpc)
</listItem>
<bodyText confidence="0.999475523809524">
In our CoNLL-2000 work, we have shown that
it is useful to evaluate combinations of clas-
sifiers since these often tend to perform bet-
ter than their best individual member (Tjong
Kim Sang, 2000). We will also use this approach
here and will evaluate majority votes of the clas-
sifier combinations 1+2+3 (8), 4+5+6 (9) and
7+8+9 (10). This means, for example, that we
will look at the output of the classifiers 1, 2 and
3 of the list above. Each of these classifiers pre-
dicts that a word starts a clause or not (task
1). We generate a new data classification (8) by
choosing for each word the classification which
is predicted most frequently.
The ten set-ups we have described above, use
information about all words. However, clauses
are a high-level structures which might not need
all this information. It might be useful to re-
place the chunks by a single token since our
fixed-context system might be able to make bet-
ter judgements when it is able to examine a
</bodyText>
<table confidence="0.983677892857143">
trainl 0 1 2 3
w 61.77 84.40 83.74 81.08
P 30.44 80.40 80.47 76.85
c 13.67 76.76 79.05 78.71
wp 62.24 87.19 84.45 81.22
wc 67.95 87.31 85.74 82.97
pc 49.29 86.65 84.92 81.72
wpc 68.66 87.92 85.93 83.28
1+2+3 38.32 85.24 86.92 85.38
4+5+6 68.04 88.83 87.44 84.98
7+8+9 68.03 88.75 87.72 85.45
w- 54.05 83.70 83.48 81.25
C- 14.26 77.70 79.30 78.50
WC- 58.47 86.53 85.74 82.77
train2 0 1 2 3
w 61.11 75.99 77.52 77.63
P 61.71 77.52 78.74 77.95
C 00.00 67.25 75.06 75.70
wp 61.25 76.52 77.92 78.12
wc 61.01 75.96 77.46 77.79
PC 61.74 77.44 78.40 77.93
wpc 61.21 76.17 77.73 78.00
1+2+3 61.67 75.93 79.60 79.94
4+5+6 61.44 77.30 79.15 79.38
7+8+9 61.44 77.20 79.25 79.60
w- 61.24 76.01 78.69 79.25
C- 61.73 76.82 78.34 80.90
wc- 61.43 76.77 80.15 81.61
</table>
<figure confidence="0.974117346153846">
1
2
3
4
5
6
7
8
9
10
11
12
13
1
2
3
4
5
6
7
8
9
10
11
12
13
</figure>
<tableCaption confidence="0.967235">
Table 1: F0_, rates obtained in 10-fold cross-
</tableCaption>
<bodyText confidence="0.998632129032258">
validation experiments with the training data
of part 1 of the shared task. We used different
combinations of information (w: words, p: POS
tags and c: chunk tags) and different context
sizes (0-3). The best results have been obtained
with a majority vote of three information pairs
while using context size 1 (row 9).
larger part of a sentence. We have tested this by
removing all chunks from the data and replac-
ing them by their head word and the chunk tag.
The head words have been generated by a set
of rules put forward by (Magerman, 1995) and
modified by (Collins, 1999)1. Words that are
outside of a base chunk receive their POS tag
as chunk tag. This approach consists of three
feature combinations: words only (w-), chunk
tags only (c-) and words and chunk tags (wc-).
The evaluation has been performed with 10-
fold cross-validation on the training data to
avoid tuning the system parameters on the test
data. This means that we have split the training
data in 10 parts and tested each of the parts af-
ter having trained with the other nine. The 10
results have been concatenated and processed
by the evaluation software for the shared task.
In this evaluation process we have also tested
different symmetric sizes of the context: 0, 1, 2
and 3. For example, while classifying the fourth
word of the phrase But analysts reckon underly-
ing support for sterling, we used only one feature
for context size 0 (underlying) and five for con-
</bodyText>
<tableCaption confidence="0.476256">
&apos;Available on http://www.research.att.comrmcollins/
papers/heads
Table 2: F0_, rates obtained in 10-fold cross-
</tableCaption>
<bodyText confidence="0.999089172413793">
validation experiments with the training data
of part 2 of the shared task. We used different
combinations of information (w: words, p: POS
tags and c: chunk tags) and different context
sizes (0-3). The best results have been obtained
with words and POS tags after compressing the
chunks and while using context size 3 (row 13).
text size 2 (analysts, reckon, underlying, sup-
port, for): the focus word and the two previous
and the two next words. In our previous work
we found that the performance increased for
larger context sizes until some task-dependent
size after which the performance dropped grad-
ually (Tjong Kim Sang and Veenstra, 1999).
In principle, the third part of the shared task
can also be interpreted as a classification task.
However, the task is more difficult since besides
predicting where clauses start and end, it re-
quires as well predicting how many clauses start
or end at a certain position. This is a sheer im-
possible task for a system which examines no
more than 7 tokens at a time but needs to pro-
cess sentences with an average length of more
than 20 tokens. Rather than solving the im-
possible, we have tried to use the results of the
other two parts of the shared task, start and
end positions of clauses, for building a complete
clause structure. For this purpose we have used
the following heuristic rules:
</bodyText>
<listItem confidence="0.987422833333333">
1. Assume that exactly one clause starts at
each clause start position.
2. Assume that exactly one clause ends at
each clause end position but
3. ignore all clause end positions when cur-
rently no clause is open, and
4. ignore all clause ends at non-sentence-final
positions which attempt to close a clause
started at the first word of the sentence.
5. If clauses are opened but not closed at the
end of the sentence then close them at the
penultimate word of the sentence.
</listItem>
<bodyText confidence="0.959795">
These rules generate complete and consistent
embedded clause structures from the clause
boundary output of our experiments.
</bodyText>
<sectionHeader confidence="0.999581" genericHeader="introduction">
3 Results
</sectionHeader>
<bodyText confidence="0.9998980625">
We have performed the evaluations described in
the previous section in a 10-fold cross-validation
experiment on the training data of parts 1 and
2 of the shared task. The results can be found
in tables 1 and 2. For the clause start predic-
tion part (1), we obtained the best performance
with a majority vote of the results for the fea-
ture combinations wp, wc and pc (row 9) for
context size 1 (F0_1 = 88.83). The clause end
prediction part (2) worked best with the com-
pressed chunk format while using feature com-
bination wc (row 13) with context size 3 (F0_1
= 81.61). Table 2 shows a monotonic increase
of the F rates for increasing context size. In-
deed, the increase goes on for context size 4 but
its maximal F rate (81.72) is probably not sig-
nificantly higher than the one for context size
3. We have combined the two results with the
heuristic rules to a complete clause structure
which obtained F0_1 = 71.34 on part 3 of the
shared task (training data only)2.
After finding the best training configurations
for the training data, we have applied these
to the development and the test data for the
shared task. The results can be found in ta-
ble 3. All F rates are better than the baseline
scores (Tjong Kim Sang and Dejean, 2001). Re-
call scores are lower than precision scores, like
we have observed in our earlier work. Predict-
ing clause ends seems to be more difficult than
predicting clause starts. We do not know what
could be causing this.
</bodyText>
<footnote confidence="0.971699833333333">
2For part 3 of the task we have also attempted to
predict clause ends while using clause start information.
This generated improved clause end results (83.50) but
the complete clause results did not change much (71.39).
3After removing a software bug, the numbers in table
3 marked by * differ from the paper proceedings version.
</footnote>
<table confidence="0.998897625">
development precision recall F0-1
part 1 92.94% 86.87% 89.80
part 2 83.80% 80.44% 82.09
part 3 76.54% 67.20% 71.57
test precision recall F0-1
part 1 92.91% 85.08% 88.82
part 2 84.72% 79.96% 82.28
part 3 76.91% 60.61% 67.79
</table>
<tableCaption confidence="0.863072666666667">
Table 3: Results obtained for the development
and the test data set for the three parts of the
shared task.
</tableCaption>
<sectionHeader confidence="0.992749" genericHeader="method">
4 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.9999857">
We have put forward a method for identifying
clauses in sentences given the words, their part-
of-speech tags and a base chunk structure of the
sentence: the CoNLL-2001 shared task. It uses
a memory-based learner for predicting positions
of clause starts and clause ends. After this, a
list of heuristic rules is used for converting these
positions to a consistent embedded clause struc-
ture. Our approach obtains F0_1 = 66.67 on the
test data of the third part of the shared task.
</bodyText>
<sectionHeader confidence="0.998288" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999827043478261">
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Processing. PhD thesis,
University of Pennsylvania.
Walter Daelemans, Jakub Zavrel, Ko van der
Sloot, and Antal van den Bosch. 2000. TiMBL:
Tilburg Memory Based Learner, version 3.0,
Reference Guide. ILK Technical Report 00-01.
http://ilk.kub.n1/.
David M. Magerman. 1995. Statistical Decision-
Tree Models for Parsing. In Proceedings of
the 33rd Annual Meeting of the Association for
Computational Linguistics (ACL&apos;95). Cambridge,
MA, USA.
Erik F. Tjong Kim Sang and Herve Dejean.
2001. Introduction to the CoNLL-2001 Shared
Task: Clause Identification. In Proceedings of the
CoNLL-2001. Toulouse, France.
Erik F. Tjong Kim Sang and Jorn Veenstra. 1999.
Representing Text Chunks. In Proceedings of
EACL&apos;99. Bergen, Norway.
Erik F. Tjong Kim Sang. 2000. Text Chunking by
System Combination. In Proceedings of CoNLL-
2000 and LLL-2000. Lisbon, Portugal.
</reference>
<page confidence="0.896172">
*3
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.505663">
<title confidence="0.999957">Memory-Based Clause Identification</title>
<author confidence="0.999232">Erik F Tjong Kim</author>
<affiliation confidence="0.9271565">CNTS - Language Technology University of</affiliation>
<intro confidence="0.591479">eriktOuia.ua.ac.be</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Processing.</title>
<date>1999</date>
<tech>PhD thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="4896" citStr="Collins, 1999" startWordPosition="880" endWordPosition="881">: F0_, rates obtained in 10-fold crossvalidation experiments with the training data of part 1 of the shared task. We used different combinations of information (w: words, p: POS tags and c: chunk tags) and different context sizes (0-3). The best results have been obtained with a majority vote of three information pairs while using context size 1 (row 9). larger part of a sentence. We have tested this by removing all chunks from the data and replacing them by their head word and the chunk tag. The head words have been generated by a set of rules put forward by (Magerman, 1995) and modified by (Collins, 1999)1. Words that are outside of a base chunk receive their POS tag as chunk tag. This approach consists of three feature combinations: words only (w-), chunk tags only (c-) and words and chunk tags (wc-). The evaluation has been performed with 10- fold cross-validation on the training data to avoid tuning the system parameters on the test data. This means that we have split the training data in 10 parts and tested each of the parts after having trained with the other nine. The 10 results have been concatenated and processed by the evaluation software for the shared task. In this evaluation proces</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Processing. PhD thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Ko van der Sloot</author>
<author>Antal van den Bosch</author>
</authors>
<title>TiMBL: Tilburg Memory Based Learner, version 3.0, Reference Guide.</title>
<date>2000</date>
<tech>ILK Technical Report 00-01. http://ilk.kub.n1/.</tech>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 2000</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch. 2000. TiMBL: Tilburg Memory Based Learner, version 3.0, Reference Guide. ILK Technical Report 00-01. http://ilk.kub.n1/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
</authors>
<title>Statistical DecisionTree Models for Parsing.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;95).</booktitle>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="4864" citStr="Magerman, 1995" startWordPosition="875" endWordPosition="876">3 4 5 6 7 8 9 10 11 12 13 Table 1: F0_, rates obtained in 10-fold crossvalidation experiments with the training data of part 1 of the shared task. We used different combinations of information (w: words, p: POS tags and c: chunk tags) and different context sizes (0-3). The best results have been obtained with a majority vote of three information pairs while using context size 1 (row 9). larger part of a sentence. We have tested this by removing all chunks from the data and replacing them by their head word and the chunk tag. The head words have been generated by a set of rules put forward by (Magerman, 1995) and modified by (Collins, 1999)1. Words that are outside of a base chunk receive their POS tag as chunk tag. This approach consists of three feature combinations: words only (w-), chunk tags only (c-) and words and chunk tags (wc-). The evaluation has been performed with 10- fold cross-validation on the training data to avoid tuning the system parameters on the test data. This means that we have split the training data in 10 parts and tested each of the parts after having trained with the other nine. The 10 results have been concatenated and processed by the evaluation software for the shared</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>David M. Magerman. 1995. Statistical DecisionTree Models for Parsing. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;95). Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>Herve Dejean</author>
</authors>
<title>Introduction to the CoNLL-2001 Shared Task: Clause Identification.</title>
<date>2001</date>
<booktitle>In Proceedings of the CoNLL-2001.</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="9052" citStr="Sang and Dejean, 2001" startWordPosition="1606" endWordPosition="1609"> rates for increasing context size. Indeed, the increase goes on for context size 4 but its maximal F rate (81.72) is probably not significantly higher than the one for context size 3. We have combined the two results with the heuristic rules to a complete clause structure which obtained F0_1 = 71.34 on part 3 of the shared task (training data only)2. After finding the best training configurations for the training data, we have applied these to the development and the test data for the shared task. The results can be found in table 3. All F rates are better than the baseline scores (Tjong Kim Sang and Dejean, 2001). Recall scores are lower than precision scores, like we have observed in our earlier work. Predicting clause ends seems to be more difficult than predicting clause starts. We do not know what could be causing this. 2For part 3 of the task we have also attempted to predict clause ends while using clause start information. This generated improved clause end results (83.50) but the complete clause results did not change much (71.39). 3After removing a software bug, the numbers in table 3 marked by * differ from the paper proceedings version. development precision recall F0-1 part 1 92.94% 86.87%</context>
</contexts>
<marker>Sang, Dejean, 2001</marker>
<rawString>Erik F. Tjong Kim Sang and Herve Dejean. 2001. Introduction to the CoNLL-2001 Shared Task: Clause Identification. In Proceedings of the CoNLL-2001. Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>Jorn Veenstra</author>
</authors>
<title>Representing Text Chunks.</title>
<date>1999</date>
<booktitle>In Proceedings of EACL&apos;99.</booktitle>
<location>Bergen,</location>
<contexts>
<context position="1949" citStr="Sang and Veenstra, 1999" startWordPosition="329" endWordPosition="333"> 2000) for predicting the most likely classification of each word. Memory-based learners store all training data and determine a classification for new data by examining the classifications of training data which are similar to the new data. Each item is represented by a set of feature-value pairs. The features have weights which encode their relevance to the classification of the training data items (Daelemans et al., 2000). Although the memory-based learner is able to find a sensible feature weight set, it is not guaranteed to find the best feature weight set. In an earlier study (Tjong Kim Sang and Veenstra, 1999), we have reported that the number of features supplied to the system has an influence on the performance and that the maximal number of features not necessarily provided the best results. In order to maximize the system&apos;s performance, we have evaluated seven combinations of the available three feature types (words, part-of-speech (POS) tags and clause tags): 1. words only (w) 2. POS tags only (p) 3. clause tags only (c) 4. words and POS tags (wp) 5. words and clause tags (wc) 6. POS tags and clause tags (pc) 7. words, POS tags and clause tags (wpc) In our CoNLL-2000 work, we have shown that i</context>
<context position="6517" citStr="Sang and Veenstra, 1999" startWordPosition="1149" endWordPosition="1152">s with the training data of part 2 of the shared task. We used different combinations of information (w: words, p: POS tags and c: chunk tags) and different context sizes (0-3). The best results have been obtained with words and POS tags after compressing the chunks and while using context size 3 (row 13). text size 2 (analysts, reckon, underlying, support, for): the focus word and the two previous and the two next words. In our previous work we found that the performance increased for larger context sizes until some task-dependent size after which the performance dropped gradually (Tjong Kim Sang and Veenstra, 1999). In principle, the third part of the shared task can also be interpreted as a classification task. However, the task is more difficult since besides predicting where clauses start and end, it requires as well predicting how many clauses start or end at a certain position. This is a sheer impossible task for a system which examines no more than 7 tokens at a time but needs to process sentences with an average length of more than 20 tokens. Rather than solving the impossible, we have tried to use the results of the other two parts of the shared task, start and end positions of clauses, for buil</context>
</contexts>
<marker>Sang, Veenstra, 1999</marker>
<rawString>Erik F. Tjong Kim Sang and Jorn Veenstra. 1999. Representing Text Chunks. In Proceedings of EACL&apos;99. Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
</authors>
<title>Text Chunking by System Combination.</title>
<date>2000</date>
<booktitle>In Proceedings of CoNLL2000 and LLL-2000.</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="1024" citStr="Sang, 2000" startWordPosition="163" endWordPosition="164"> a word, its part-of-speech tag and the syntactic base chunk it belongs to. Our memory-based learner can be applied to these tasks in a straightforward way. The third part of the shared task is identifying complete embedded clauses. We will perform this task by first identifying clause starts and clause ends and then combining these to clauses with a set of conversion rules. 2 Approach The first two parts of the CoNLL-2001 shared task are similar to the CoNLL-2000 shared task: classify words in context according to some tagging scheme. We have participated in the latter shared task (Tjong Kim Sang, 2000) and we will use a similar approach for the first two parts of the 2001 shared task. The goal of these parts is to predict if a word is the first word of a clause or not (part 1) and if it is the final word of a clause or not (part 2). We have used the memory-based classifier TiMBL (Daelemans et al., 2000) for predicting the most likely classification of each word. Memory-based learners store all training data and determine a classification for new data by examining the classifications of training data which are similar to the new data. Each item is represented by a set of feature-value pairs.</context>
<context position="2698" citStr="Sang, 2000" startWordPosition="465" endWordPosition="466">eatures not necessarily provided the best results. In order to maximize the system&apos;s performance, we have evaluated seven combinations of the available three feature types (words, part-of-speech (POS) tags and clause tags): 1. words only (w) 2. POS tags only (p) 3. clause tags only (c) 4. words and POS tags (wp) 5. words and clause tags (wc) 6. POS tags and clause tags (pc) 7. words, POS tags and clause tags (wpc) In our CoNLL-2000 work, we have shown that it is useful to evaluate combinations of classifiers since these often tend to perform better than their best individual member (Tjong Kim Sang, 2000). We will also use this approach here and will evaluate majority votes of the classifier combinations 1+2+3 (8), 4+5+6 (9) and 7+8+9 (10). This means, for example, that we will look at the output of the classifiers 1, 2 and 3 of the list above. Each of these classifiers predicts that a word starts a clause or not (task 1). We generate a new data classification (8) by choosing for each word the classification which is predicted most frequently. The ten set-ups we have described above, use information about all words. However, clauses are a high-level structures which might not need all this inf</context>
</contexts>
<marker>Sang, 2000</marker>
<rawString>Erik F. Tjong Kim Sang. 2000. Text Chunking by System Combination. In Proceedings of CoNLL2000 and LLL-2000. Lisbon, Portugal.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>