<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.945342">
Modeling of Long Distance Context Dependency in Chinese
</title>
<author confidence="0.925849">
GuoDong ZHOU
</author>
<affiliation confidence="0.92496">
Institute for Infocomm Research
</affiliation>
<address confidence="0.9709125">
21 Heng Mui Keng Terrace
Singapore, 119613
</address>
<email confidence="0.997667">
zhougd@i2r.a-star.edu.sg
</email>
<sectionHeader confidence="0.993826" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999943741935484">
Ngram modeling is simple in language
modeling and has been widely used in many
applications. However, it can only capture the
short distance context dependency within an
N-word window where the largest practical N
for natural language is three. In the meantime,
much of context dependency in natural
language occurs beyond a three-word window.
In order to incorporate this kind of long
distance context dependency, this paper
proposes a new MI-Ngram modeling approach.
The MI-Ngram model consists of two
components: an ngram model and an MI
model. The ngram model captures the short
distance context dependency within an N-word
window while the MI model captures the long
distance context dependency between the word
pairs beyond the N-word window by using the
concept of mutual information. It is found that
MI-Ngram modeling has much better
performance than ngram modeling. Evaluation
on the XINHUA new corpus of 29 million
words shows that inclusion of the best
1,600,000 word pairs decreases the perplexity
of the MI-Trigram model by 20 percent
compared with the trigram model. In the
meanwhile, evaluation on Chinese word
segmentation shows that about 35 percent of
errors can be corrected by using the
MI-Trigram model compared with the trigram
model.
</bodyText>
<sectionHeader confidence="0.998895" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999232953488372">
Language modeling is the attempt to characterize,
capture and exploit the regularities and constraints
in natural language. Among various language
modeling approaches, ngram modeling has been
widely used in many applications, such as speech
recognition, machine translation (Katz 1987;
Jelinek 1989; Gale and Church 1990; Brown et al.
1992; Yang et al. 1996; Bai et al 1998; Zhou et al
1999; Rosenfeld 2000; Gao et al 2002). Although
ngram modeling is simple in nature and easy to use,
it has obvious deficiencies. For instance, ngram
modeling can only capture the short distance
context dependency within an N-word window
where currently the largest practical N for natural
language is three.
In the meantime, it is found that there always
exist many preferred relationships between words.
Two highly associated word pairs are Ta(/ ffqR
(“not only/but also”) and X t / TP ±
(“doctor/nurse”). Psychological experiments in
Meyer et al. (1975) indicated that the human’s
reaction to a highly associated word pair was
stronger and faster than that to a poorly associated
word pair. Such preference information is very
useful for natural language processing (Church et
al. 1990; Hiddle et al. 1993; Rosenfeld 1994; Zhou
et al.1998; Zhou et al 1999). Obviously, the
preference relationships between words can expand
from a short to long distance. While we can use
traditional ngram modeling to capture the short
distance context dependency, the long distance
context dependency should also be exploited
properly.
The purpose of this paper is to propose a new
MI-Ngram modeling approach to capture the
context dependency over both a short distance and a
long distance. Experimentation shows that this new
MI-Ngram modeling approach can significantly
decrease the perplexity of the new MI-Ngram
model compared with traditional ngram model. In
the meantime, evaluation on Chinese word
segmentation shows that this new approach can
significantly reduce the error rate.
</bodyText>
<figure confidence="0.901851733333333">
log
( )
w1
P(S
)
P(S) log
= P
by re-writing equation (2.2):
(2.2)
m
∑
log
( )
w1
Pngram
</figure>
<equation confidence="0.862565846153846">
logP
+
(wi  |w1−1)
( ) log
S = P
(  |)
i −1
P w w
i 1
log
, we have:
P
(wi  |w1−1) ≈ P(wi  |wi−n+1)
</equation>
<bodyText confidence="0.9991783">
This paper is organized as follows. In section 2,
we describe the traditional ngram modeling
approach and discuss its main property. In section 3,
we propose the new MI-Ngram modeling approach
to capture context dependency over both a short
distance and a long distance. In section 4, we
measure the MI-Ngram modeling approach and
evaluate its application in Chinese word
segmentation. Finally we give a summary of this
paper in section 5.
</bodyText>
<equation confidence="0.857824913043478">
2 Ngram Modeling
Let S = w1m = w1w2 ... wm , where wi ’s are the words
that make up the hypothesis, the probability of the
word string P can be computed by using the
(S)
chain rule:
m
P S P w P wi w
( ) ( 1) (  |1 )
i 1
−
= ∏ (2.1)
i=2
And the probability P (wi  |wi− 1) can be estimated
by using maximum likelihood estimation (MLE)
principle:
C w w
( )
i − 1 i
P w w
(  |)
i i − =
1C(wi−1)
</equation>
<bodyText confidence="0.79162575">
Where C(• ) represents the number of times the
sequence occurs in the training data. In practice, due
to the data sparseness problem, some smoothing
techniques, such as linear interpolation (Jelinek
1989; Chen and Goodman 1999) and back-off
modeling (Katz 1987), are applied.
Obviously, an ngram model assumes that the
probability of the next word w is independent of
</bodyText>
<figure confidence="0.478678125">
i
word string w in the history. The difference
i − n
1
between bigram, trigram and other ngram models is
the value of N. The parameters of an ngram are thus
the probabilities:
(2.5)
</figure>
<bodyText confidence="0.7025395">
By taking log function to both sides of equation
(2.1), we have the log probability of the word
</bodyText>
<equation confidence="0.7579998">
Given S = w 1 w 2 . . . wm , an ngram model
string log P(S) :
estimates the log probability of the word string
P(wn  |w1 ... wn−1) For all w1, ww,..., wn ∈ V .
2
</equation>
<bodyText confidence="0.99975">
So, the classical task of statistical language
modeling becomes how to effectively and
efficiently predict the next word, given the previous
words, that is to say, to estimate expressions of the
</bodyText>
<equation confidence="0.9850253">
i −
form (  |1) . For convenience, P(  |1)
wi wi −
P wi w
1 1
i −1
is often written as P(wi  |h) , where h w
= , is
1
called history.
</equation>
<bodyText confidence="0.971269">
Ngram modeling has been widely used in
estimatingP(wi  |h) . Within an ngram model, the
probability of a word occurring next is estimated
based on the previous words. That is to say,
n − 1
</bodyText>
<equation confidence="0.992682">
1
P(wi  |wi−n+1 )
</equation>
<bodyText confidence="0.499048">
Where is the string length, w is the -th word
</bodyText>
<equation confidence="0.996260933333333">
m i
i
in string .
S
From equation (2.3)
P(w1−1wi) P(wi−n+1wi )
P(w1−1) ≈ P(wi−n+1 )
log
(2.6)
(  |) (  |1 )
i − ≈
1 i −
P wi w P w w (2.3)
1 i i n
− +1
</equation>
<bodyText confidence="0.791881333333333">
For example, in bigram model (n=2), the
probability of a word is assumed to depend only on
the previous word:
</bodyText>
<equation confidence="0.95421906557377">
(  |) (  |1)
− ≈ i
1
P wi w i P w w (2.4)
1 i−
i
=
n
−
+
∑
i
=
2
m
+
∑
i
=
n
P w w
( )
i − 1
1 i
log P(w1i−1)P(wi )
log P(w i−n+1 wi )
P w P w
( ) ( )
i − 1
i n
− + 1 i
(2.7)
( )
w w
i − 1
1 i
P(i−1
wi−n+1 wi )
P(wi−n+1 )P(wi )
P
≈
( ) ( )
i − 1 P w
1 i
P w
Obviously, we can get
MI (wl−1 , wi, d = 1) ≈ MI (wi−n+1 , wi, d = 1) (2.8)
model estimates the log probability of the string
P(S
) as:
i − 1
( )
P w w
(
i − 1 1 i
= =
MI ( , , 1) log
w w d
1 i i 1
P w −
1
</equation>
<bodyText confidence="0.408362">
where
</bodyText>
<figure confidence="0.876244826086957">
) ( )
P w i
log
( ) log
S = P
PTrigram
+
is
( )
w1
w2
(2.9)
log(
 |)
w1
m
the mutual information between the word string pair
(wi−11
+∑ log P(wi|wa_2 )
i=3
, wi ) and
mutual information between the word string pair
1, wi ) d
</figure>
<bodyText confidence="0.95655575">
. is the distance of two word strings
in the word string pair and is equal to 1 when the
two word strings are adjacent.
For a word string pair ( over a distance
A , B)
d where and
A B are word strings, mutual
information MI (A d)
, B, reflects the degree of
preference relationship between the two strings
over a distanced. Several properties of mutual
information are apparent:
</bodyText>
<listItem confidence="0.940499">
• For the same distance d ,
MI(A,B,d) ≠ MI(B,A,d
</listItem>
<sectionHeader confidence="0.947194" genericHeader="method">
3 MI-Ngram Modeling
</sectionHeader>
<bodyText confidence="0.729927333333333">
− 1
Given history H = wi w w w
= , we can
</bodyText>
<equation confidence="0.985653470588235">
1 1 2 ... i−1 i −
assume X = w2 1 = w2w3...wi−1 . Then we have
H = w1X (3.1)
and
P(wi  |H) = P(wi  |w1X) . (3.2)
Since
logP(wi|H) = log
P Hw
( )
i
+ log (3.3)
P H P w
( ) ( )
i
P w w
i − 1
( i )
− i n
1 − + 1
, , 1) log
w d = =
n + 1 i P w P w
( ) ( )
i − 1
i n
− + 1 i
−
is the
(wi
i
MI
(i 1
wi − n
− +
</equation>
<figure confidence="0.844066333333333">
)
P
(Hwi
P(H)
) . log P ( )
w i
• For different distances d and d2 ,
1
MI(A,B,d1 ) ≠ MI(A,B,d2
+
log P w MI w
( ) (
i
Here we assume
.
)
)
, H,1
</figure>
<listItem confidence="0.896631666666667">
• If A and B are independent over a distance
d, then MI (A, B, d) = 0.
MI (A, B, d) reflects the change of the
</listItem>
<bodyText confidence="0.925855823529412">
information content when the word strings A and
B are correlated. That is to say, the higher the
value of MI (A, B, d) , the stronger affinity and
A
B have. Therefore, we can use mutual information
to measure the preference relationship degree
between a word string pair.
From the view of mutual information, an ngram
model assumes the mutual information
independency between ( wi −n w
1 , i) . Using an
alternative view of equivalence, an ngram model is
one that partitions the data into equivalence classes
based on the last n-1 words in the history.
As trigram model is most widely used in current
research, we will mainly consider the trigram-based
model. By re-writing equation (2.6), the trigram
</bodyText>
<equation confidence="0.938196333333333">
,d=1)=MI(X,wi, d =1
(w1,wi,d = i)
i 1 −1
</equation>
<bodyText confidence="0.830005">
where H w − i
= , X w
</bodyText>
<equation confidence="0.771364">
= and i &gt; N. That is to
1 2
</equation>
<bodyText confidence="0.993998428571429">
say, the mutual information of the next word with
the history is assumed equal to the summation of
that of the next word with the first word in the
history and that of the next word with the rest word
string in the history.
We can re-writing equation (3.3) by using
equation (3.4):
</bodyText>
<equation confidence="0.997465987951807">
log P(wi  |H )
logP(wi) + MI(H, wi
log ( ) ( , ,1) ( , ,
P w MI X w MI w 1 w i
i + i + i )
MI H w
( , i
+
MI
) (3.4)
,
1
)
n
k
=i−
k
m
=i−n
n
k
m
=i−
)
MI
1
+
∑ ∑
(wk,wi,i−k
contributes to
=
n
i
+
1 k=1
(S)
log
PMI Ngram
−
P
log
(|
wi
(3.9)
,
+
1
,
i k
−
)
wi
k
m
=−
i
i
=
n
+ 2
log(  |1 )
wi w i −1
By applying equation (3.6) repeatedly, we have:
P(wi|wi−1)
1
log
wi − +
1) (
MI w
2 1
log (  |1
P w i w i −
3
+MI(w2,wi,i−1)+MI(w1,wi,i)
)
L L L
log (  |1
P wi wii−n+−
1
( , ,
wk wi i k
− + 1) (3.7)
k=
</equation>
<bodyText confidence="0.9981984">
Obviously, the first item in equation (3.7)
contributes to the log probability of ngram within an
N-word window while the second item is the
summation of mutual information which
contributes to the long distance context dependency
of the next word wi with the individual previous
word wj (1 ≤ j ≤ i − N, i &gt; N) over the long
distance outside the N-word window.
By using equation (3.7), equation (2.2) can be
re-written as:
</bodyText>
<equation confidence="0.9998165">
P(wi  |wi−2 )
(wk,wi,i−k+1) (3.8)
</equation>
<bodyText confidence="0.9999311">
In equation (3.8), the first three items are the
values computed by the trigram model as shown in
equation (2.9) and the forth item
summation of the mutual information of the next
word with the words over the long distance outside
the N-word window. That is, the new model as
shown in equation (3.8) consists of two
components: an ngram model and an MI model.
Therefore, we call equation (3.8) as an MI-Ngram
model and equation (3.8) can be re-written as:
</bodyText>
<equation confidence="0.996379041666667">
∑
log
n + 1
+
i=
MI
∑ ∑
n
+
+
i=
1 k=1
logPN
(S)
g
n
=i−
m
k
∑ ∑
n
=
+
1 k=1
</equation>
<bodyText confidence="0.9998015">
As a special case N=3, the MI-Trigram model
estimate the log probability of the string as follows:
</bodyText>
<equation confidence="0.969308142857143">
)
1
+
i
wi w
 |1)
i − Compared with traditional ngram modeling,
</equation>
<page confidence="0.664196">
1
</page>
<bodyText confidence="0.999465333333333">
MI-Ngram modeling incorporates the long distance
context dependency by computing mutual
information of the long distance dependent word
</bodyText>
<equation confidence="0.946955570175439">
)
=
n
i
 |)
i − 1
w 2
=
P w
( i
i−1
P w w
i 1
log (  |) log
(3.6)
=
i
)
i
,
i
=n
+ P w n + w
log ( 1  |2 )
n
m
i
+MI(wn+1,w1,n+1) log (  |1)
P w w
−
+ ∑ i 1
+2
log ( 1 ) log(  |1 )
i − 1
P w + ∑ w i w
L L L
2
m
P w X
( )
log i + MI w 1 w i
( , ,
i
P X
( )
= logP(wi  |X)+MI(w1,wi,i) (3.5)
Then we have
+ MI w , wi
( 1
)
, wi, i
+
MI
∑
MI
(wk
)
log(S)
MI−
ig
P
Tr
m
i 1
log ( ) log ( ) log ( |
P S P w w −
= P w + ∑
1 i 1
= logP(wi)+log P(wiX) +MI(w1,wi,i) = log P(w1)+∑
P w P X
( ) ( )
i
∑ ∑
i=4 k=1
3
(
k
w
MI
w
, i
,
m
+ P wn+ w
log ( 1  |1 )
n + ∑ log (
P
i
=
n
i
=
2
=
2
log(S)
i
PTr
gram
i
1
+
)
(3.10)
n
log P(w1) + ∑ log(wi  |w1i 1)
i
=
2
+
i k
−
i
=
</equation>
<bodyText confidence="0.999985052631579">
pairs. Since the number of possible long distance
dependent word pairs may be very huge, it is
impossible for MI-Ngram modeling to incorporate
all of them. Therefore, for MI-Ngram modeling to
be practically useful, how to select a reasonable
number of word pairs becomes very important.
Here two approaches are used (Zhou et al 1998 and
1999). One is to restrict the window size of possible
word pairs by computing and comparing the
perplexities1 (Shannon C.E. 1951) of various long
distance bigram models for different distances. It is
found that the bigram perplexities for different
distances outside the 10-word window become
stable. Therefore, we only consider MI-Ngram
modeling with a window size of 10 words. Another
is to adapt average mutual information to select a
reasonable number of long distance dependent word
pairs. Given distance d and two words A and B, its
average mutual information is computed as:
</bodyText>
<footnote confidence="0.5862565">
1 Perplexity is a measure of the average number of
possible choices there are for a random variable. The
perplexity PP of a random variable X with entropy
is defined as:
</footnote>
<equation confidence="0.907602">
PP X =
( ) 2H(X)
</equation>
<bodyText confidence="0.99934">
Entropy is a measure of uncertainty about a random
variable. If a random variable X occurs with a
probability distribution P (x) , then the entropy H
of that event is defined as:
</bodyText>
<equation confidence="0.92976675">
H(X) = −∑P x
( ) log2 P(x)
x X
∈
</equation>
<bodyText confidence="0.989873">
Since x log2 x→ 0 as x → 0 , it is conventional to
use the relation 0log2 0 = 0 when computing entropy.
The units of entropy are bits of information. This is
because the entropy of a random variable corresponds to
the average number of bits per event needed to encode a
typical sequence of event samples from that random
variable’ s distribution.
</bodyText>
<figure confidence="0.659304444444444">
AMI A B d
( , , )
P(A,B,d)
P(A)P(B)
+ P(A, B, d) log P(A, B, d) (3.11)
P(A)P(B)
+ P A B d
( , ,
+ P(A,B,d
</figure>
<bodyText confidence="0.998974222222222">
Compared with mutual information, average
mutual information takes joint probabilities into
consideration. In this way, average mutual
information prefers frequently occurred word pairs.
In our paper, different numbers of long distance
dependent word pairs will be considered in
MI-Ngram modeling within a window size of 10
words to evaluate the effect of different MI model
size.
</bodyText>
<sectionHeader confidence="0.999438" genericHeader="method">
4 Experimentation
</sectionHeader>
<bodyText confidence="0.99997215">
As trigram modeling is most widely used in current
research, only MI-Trigram modeling is studied
here. Furthermore, in order to demonstrate the
effect of different numbers of word pairs in
MI-Trigram modeling, various MI-Trigram models
with different numbers of word pairs and the same
window size of 10 words are trained on the
XINHUA news corpus of 29 million words while
the lexicon contains about 56,000 words. Finally,
various MI-Trigram models are tested on the same
task of Chinese word segmentation using the
Chinese tag bank PFR1.0 2 of 3.69M Chinese
characters (1.12M Chinese Words).
Table 1 shows the perplexities of various
MI-Trigram models and their performances on
Chinese word segmentation. Here, the precision (P)
measures the number of correct words in the answer
file over the total number of words in the answer file
and the recall (R) measures the number of correct
words in the answer file over the total number of
</bodyText>
<footnote confidence="0.953603333333333">
2 PFR1.0 is developed by Institute of Computational
Linguistics at Beijing Univ. Here, only the word
segmentation annotation is used.
</footnote>
<figure confidence="0.896418142857143">
)
H(X
)
(X
) log
P A B d
( , ,
P(A,B,d)
P
) log
(A)P(B)
) log
P(A,B,d)
P(A)P(B)
</figure>
<bodyText confidence="0.855859435897436">
words in the key file. F-measure is the weighted
harmonic mean of precision and recall:
Table 1 shows that
• The perplexity and the F-measure rise quickly
as the number of word pairs in MI-Trigram
modeling increases from 0 to 1,600,000 and
then rise slowly. Therefore, the best 1,600,000
word pairs should at least be included.
• Inclusion of the best 1,600,000 word pairs
decreases the perplexity of MI-Trigram
modeling by about 20 percent compared with
the pure trigram model.
• The performance of Chinese word
segmentation using the MI-Trigram model with
1,600,000 word pairs is 0.8 percent higher than
using the pure trigram model (MI-Trigram with
0 word pairs). That is to say, about 35 percent of
errors can be corrected by incorporating only
1,600,000 word pairs to the MI-Trigram model
compared with the pure trigram model.
• For Chinese word segmentation task, recalls are
about 0.7 percent higher than precisions. The
main reason may be the existence of unknown
words. In our experimentation, unknown words
are segmented into individual Chinese
characters. This makes the number of
segmented words in the answer file higher than
that in the key file.
It is clear that MI-Ngram modeling has much
better performance than ngram modeling. One
advantage of MI-Ngram modeling is that its number
of parameters is just a little more than that of ngram
modeling. Another advantage of MI-Ngram
modeling is that the number of the word pairs can be
reasonable in size without losing too much of its
modeling power. Compared to ngram modeling,
MI-Ngram modeling also captures the
long-distance context dependency of word pairs
using the concept of mutual information.
</bodyText>
<figure confidence="0.8667463">
( 2 1)
β + RP
β
F
with =1.
β 2
2
P
R
+
</figure>
<tableCaption confidence="0.926311">
Table 1: The effect of different numbers of word pairs in MI-Trigram modeling with the same window size
of 10 words on Chinese word segmentation
</tableCaption>
<table confidence="0.999254">
Number of word pairs Perplexity Precision Recall F-measure
0 316 97.5 98.2 97.8
100,000 295 97.9 98.4 98.1
200,000 281 98.1 98.6 98.3
400,000 269 98.2 98.7 98.4
800,000 259 98.2 98.8 98.5
1,600,000 250 98.4 98.8 98.6
3,200,000 245 98.3 98.9 98.6
6,400,000 242 98.4 98.9 98.6
</table>
<sectionHeader confidence="0.999201" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999615">
This paper proposes a new MI-Ngram modeling
approach to capture the context dependency over
both a short distance and a long distance. This is
done by incorporating long distance dependent
word pairs into traditional ngram model by using
the concept of mutual information. It is found that
MI-Ngram modeling has much better performance
than ngram modeling.
Future works include the explorations of the
new MI-Trigram modeling approach in other
applications, such as Mandarin speech recognition
and PINYIN to Chinese character conversion.
</bodyText>
<sectionHeader confidence="0.999468" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999647454545454">
Bai S.H., Li H.Z., Lin Z.M. and Yuan B.S. 1989.
Building class-based language models with
contextual statistics. Proceedings of International
Conference on Acoustics, Speech and Signal
Processing (ICASSP’1998). pages173-176.
Brown P.F. et al. 1992. Class-based ngram models of
natural language. Computational Linguistics 18(4),
467-479.
Chen S.F. and Goodman J. 1999. An empirical study
of smoothing technique for language modeling.
Computer, Speech and Language. 13(5). 359-394.
Church K.W. et al. 1991. Enhanced good Turing and
Cat-Cal: two new methods for estimating
probabilities of English bigrams. Computer, Speech
and Language 5(1), 19-54.
Gale W.A. and Church K.W. 1990. Poor estimates of
context are worse than none. Proceedings of
DARPA Speech and Natural Language Workshop,
Hidden Valley, Pennsylvania, pages293-295.
Gao J.F., Goodman J.T., Cao G.H. and Li H. 2002.
Exploring asymmetric clustering for statistical
language modelling. Proceedings of the Fortieth
Annual Meeting of the Association for
Computational Linguistics (ACL’2002).
Philadelphia. pages183-190.
Hindle D. et al. 1993. Structural ambiguity and lexical
relations. Computational Linguistics 19(1),
103-120.
Jelinek F. 1989. Self-organized language modeling for
speech recognition. In Readings in Speech
Recognition. Edited by Waibel A. and Lee K.F.
Morgan Kaufman. San Mateo. CA. pages450-506.
Katz S.M. 1987. “ Estimation of Probabilities from
Sparse Data for the Language Model Component
of a Speech Recognizer”. IEEE Transactions on
Acoustics. Speech and Signal Processing. 35.
400-401.
Meyer D. et al. 1975. Loci of contextual effects on
visual word recognition. In Attention and
Performance V, edited by P.Rabbitt and S.Dornie.
pages98-116. Acdemic Press.
Rosenfeld R. 1994. Adaptive statistical language
modeling: A Maximum Entropy Approach. Ph.D.
Thesis, Carneige Mellon University.
Rosenfeld R. 2000. Two decades of language
modelling: where do we go from here. Proceedings
of IEEE. 88:1270-1278. August.
Shannon C.E. 1951. Prediction and entropy of printed
English. Bell Systems Technical Journal 30, 50-64.
Yang Y.J. et al. 1996. Adaptive linguistic decoding
system for Mandarin speech recognition
applications. Computer Processing of Chinese &amp;
Oriental Languages 10(2), 211-224.
Zhou GuoDong and Lua Kim Teng, 1998. Word
Association and MI-Trigger-based Language
Modeling. Proceedings of the Thirtieth-sixth
Annual Meeting of the Association for
Computational Linguistics and the Seventeenth
International Conference on Computational
Linguistics (COLING-ACL’1998). Montreal,
Canada. pages10-14. August.
Zhou GuoDong and Lua KimTeng. 1999.
Interpolation of N-gram and MI-based Trigger
Pair Language Modeling in Mandarin Speech
Recognition, Computer, Speech and Language,
13(2), 123-135.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.510846">
<title confidence="0.999668">Modeling of Long Distance Context Dependency in Chinese</title>
<author confidence="0.606075">GuoDong</author>
<affiliation confidence="0.849944">Institute for Infocomm</affiliation>
<address confidence="0.8871975">21 Heng Mui Keng Singapore,</address>
<abstract confidence="0.9983463125">Ngram modeling is simple in language modeling and has been widely used in many applications. However, it can only capture the short distance context dependency within an N-word window where the largest practical N for natural language is three. In the meantime, much of context dependency in natural language occurs beyond a three-word window. In order to incorporate this kind of long distance context dependency, this paper proposes a new MI-Ngram modeling approach. The MI-Ngram model consists of two components: an ngram model and an MI model. The ngram model captures the short distance context dependency within an N-word window while the MI model captures the long distance context dependency between the word pairs beyond the N-word window by using the concept of mutual information. It is found that MI-Ngram modeling has much better performance than ngram modeling. Evaluation on the XINHUA new corpus of 29 million words shows that inclusion of the best 1,600,000 word pairs decreases the perplexity of the MI-Trigram model by 20 percent compared with the trigram model. In the meanwhile, evaluation on Chinese word segmentation shows that about 35 percent of errors can be corrected by using the MI-Trigram model compared with the trigram model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S H Bai</author>
<author>H Z Li</author>
<author>Z M Lin</author>
<author>B S Yuan</author>
</authors>
<title>Building class-based language models with contextual statistics.</title>
<date>1989</date>
<booktitle>Proceedings of International Conference on Acoustics, Speech and Signal Processing (ICASSP’1998).</booktitle>
<pages>173--176</pages>
<marker>Bai, Li, Lin, Yuan, 1989</marker>
<rawString>Bai S.H., Li H.Z., Lin Z.M. and Yuan B.S. 1989. Building class-based language models with contextual statistics. Proceedings of International Conference on Acoustics, Speech and Signal Processing (ICASSP’1998). pages173-176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
</authors>
<title>Class-based ngram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics</journal>
<volume>18</volume>
<issue>4</issue>
<pages>467--479</pages>
<marker>Brown, 1992</marker>
<rawString>Brown P.F. et al. 1992. Class-based ngram models of natural language. Computational Linguistics 18(4), 467-479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing technique for language modeling.</title>
<date>1999</date>
<journal>Computer, Speech and Language.</journal>
<volume>13</volume>
<issue>5</issue>
<pages>359--394</pages>
<contexts>
<context position="4650" citStr="Chen and Goodman 1999" startWordPosition="783" endWordPosition="786">ion 5. 2 Ngram Modeling Let S = w1m = w1w2 ... wm , where wi ’s are the words that make up the hypothesis, the probability of the word string P can be computed by using the (S) chain rule: m P S P w P wi w ( ) ( 1) ( |1 ) i 1 − = ∏ (2.1) i=2 And the probability P (wi |wi− 1) can be estimated by using maximum likelihood estimation (MLE) principle: C w w ( ) i − 1 i P w w ( |) i i − = 1C(wi−1) Where C(• ) represents the number of times the sequence occurs in the training data. In practice, due to the data sparseness problem, some smoothing techniques, such as linear interpolation (Jelinek 1989; Chen and Goodman 1999) and back-off modeling (Katz 1987), are applied. Obviously, an ngram model assumes that the probability of the next word w is independent of i word string w in the history. The difference i − n 1 between bigram, trigram and other ngram models is the value of N. The parameters of an ngram are thus the probabilities: (2.5) By taking log function to both sides of equation (2.1), we have the log probability of the word Given S = w 1 w 2 . . . wm , an ngram model string log P(S) : estimates the log probability of the word string P(wn |w1 ... wn−1) For all w1, ww,..., wn ∈ V . 2 So, the classical ta</context>
</contexts>
<marker>Chen, Goodman, 1999</marker>
<rawString>Chen S.F. and Goodman J. 1999. An empirical study of smoothing technique for language modeling. Computer, Speech and Language. 13(5). 359-394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
</authors>
<title>Enhanced good Turing and Cat-Cal: two new methods for estimating probabilities of English bigrams.</title>
<date>1991</date>
<journal>Computer, Speech and Language</journal>
<volume>5</volume>
<issue>1</issue>
<marker>Church, 1991</marker>
<rawString>Church K.W. et al. 1991. Enhanced good Turing and Cat-Cal: two new methods for estimating probabilities of English bigrams. Computer, Speech and Language 5(1), 19-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Gale</author>
<author>K W Church</author>
</authors>
<title>Poor estimates of context are worse than none.</title>
<date>1990</date>
<booktitle>Proceedings of DARPA Speech and Natural Language Workshop,</booktitle>
<pages>293--295</pages>
<location>Hidden Valley, Pennsylvania,</location>
<contexts>
<context position="1771" citStr="Gale and Church 1990" startWordPosition="266" endWordPosition="269">0 word pairs decreases the perplexity of the MI-Trigram model by 20 percent compared with the trigram model. In the meanwhile, evaluation on Chinese word segmentation shows that about 35 percent of errors can be corrected by using the MI-Trigram model compared with the trigram model. 1 Introduction Language modeling is the attempt to characterize, capture and exploit the regularities and constraints in natural language. Among various language modeling approaches, ngram modeling has been widely used in many applications, such as speech recognition, machine translation (Katz 1987; Jelinek 1989; Gale and Church 1990; Brown et al. 1992; Yang et al. 1996; Bai et al 1998; Zhou et al 1999; Rosenfeld 2000; Gao et al 2002). Although ngram modeling is simple in nature and easy to use, it has obvious deficiencies. For instance, ngram modeling can only capture the short distance context dependency within an N-word window where currently the largest practical N for natural language is three. In the meantime, it is found that there always exist many preferred relationships between words. Two highly associated word pairs are Ta(/ ffqR (“not only/but also”) and X t / TP ± (“doctor/nurse”). Psychological experiments i</context>
</contexts>
<marker>Gale, Church, 1990</marker>
<rawString>Gale W.A. and Church K.W. 1990. Poor estimates of context are worse than none. Proceedings of DARPA Speech and Natural Language Workshop, Hidden Valley, Pennsylvania, pages293-295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F Gao</author>
<author>J T Goodman</author>
<author>G H Cao</author>
<author>H Li</author>
</authors>
<title>Exploring asymmetric clustering for statistical language modelling.</title>
<date>2002</date>
<booktitle>Proceedings of the Fortieth Annual Meeting of the Association for Computational Linguistics (ACL’2002). Philadelphia.</booktitle>
<pages>183--190</pages>
<contexts>
<context position="1874" citStr="Gao et al 2002" startWordPosition="288" endWordPosition="291"> In the meanwhile, evaluation on Chinese word segmentation shows that about 35 percent of errors can be corrected by using the MI-Trigram model compared with the trigram model. 1 Introduction Language modeling is the attempt to characterize, capture and exploit the regularities and constraints in natural language. Among various language modeling approaches, ngram modeling has been widely used in many applications, such as speech recognition, machine translation (Katz 1987; Jelinek 1989; Gale and Church 1990; Brown et al. 1992; Yang et al. 1996; Bai et al 1998; Zhou et al 1999; Rosenfeld 2000; Gao et al 2002). Although ngram modeling is simple in nature and easy to use, it has obvious deficiencies. For instance, ngram modeling can only capture the short distance context dependency within an N-word window where currently the largest practical N for natural language is three. In the meantime, it is found that there always exist many preferred relationships between words. Two highly associated word pairs are Ta(/ ffqR (“not only/but also”) and X t / TP ± (“doctor/nurse”). Psychological experiments in Meyer et al. (1975) indicated that the human’s reaction to a highly associated word pair was stronger</context>
</contexts>
<marker>Gao, Goodman, Cao, Li, 2002</marker>
<rawString>Gao J.F., Goodman J.T., Cao G.H. and Li H. 2002. Exploring asymmetric clustering for statistical language modelling. Proceedings of the Fortieth Annual Meeting of the Association for Computational Linguistics (ACL’2002). Philadelphia. pages183-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Structural ambiguity and lexical relations.</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<issue>1</issue>
<pages>103--120</pages>
<marker>Hindle, 1993</marker>
<rawString>Hindle D. et al. 1993. Structural ambiguity and lexical relations. Computational Linguistics 19(1), 103-120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Self-organized language modeling for speech recognition.</title>
<date>1989</date>
<booktitle>In Readings in Speech Recognition. Edited by</booktitle>
<pages>450--506</pages>
<contexts>
<context position="1749" citStr="Jelinek 1989" startWordPosition="264" endWordPosition="265"> best 1,600,000 word pairs decreases the perplexity of the MI-Trigram model by 20 percent compared with the trigram model. In the meanwhile, evaluation on Chinese word segmentation shows that about 35 percent of errors can be corrected by using the MI-Trigram model compared with the trigram model. 1 Introduction Language modeling is the attempt to characterize, capture and exploit the regularities and constraints in natural language. Among various language modeling approaches, ngram modeling has been widely used in many applications, such as speech recognition, machine translation (Katz 1987; Jelinek 1989; Gale and Church 1990; Brown et al. 1992; Yang et al. 1996; Bai et al 1998; Zhou et al 1999; Rosenfeld 2000; Gao et al 2002). Although ngram modeling is simple in nature and easy to use, it has obvious deficiencies. For instance, ngram modeling can only capture the short distance context dependency within an N-word window where currently the largest practical N for natural language is three. In the meantime, it is found that there always exist many preferred relationships between words. Two highly associated word pairs are Ta(/ ffqR (“not only/but also”) and X t / TP ± (“doctor/nurse”). Psych</context>
<context position="4626" citStr="Jelinek 1989" startWordPosition="781" endWordPosition="782"> paper in section 5. 2 Ngram Modeling Let S = w1m = w1w2 ... wm , where wi ’s are the words that make up the hypothesis, the probability of the word string P can be computed by using the (S) chain rule: m P S P w P wi w ( ) ( 1) ( |1 ) i 1 − = ∏ (2.1) i=2 And the probability P (wi |wi− 1) can be estimated by using maximum likelihood estimation (MLE) principle: C w w ( ) i − 1 i P w w ( |) i i − = 1C(wi−1) Where C(• ) represents the number of times the sequence occurs in the training data. In practice, due to the data sparseness problem, some smoothing techniques, such as linear interpolation (Jelinek 1989; Chen and Goodman 1999) and back-off modeling (Katz 1987), are applied. Obviously, an ngram model assumes that the probability of the next word w is independent of i word string w in the history. The difference i − n 1 between bigram, trigram and other ngram models is the value of N. The parameters of an ngram are thus the probabilities: (2.5) By taking log function to both sides of equation (2.1), we have the log probability of the word Given S = w 1 w 2 . . . wm , an ngram model string log P(S) : estimates the log probability of the word string P(wn |w1 ... wn−1) For all w1, ww,..., wn ∈ V </context>
</contexts>
<marker>Jelinek, 1989</marker>
<rawString>Jelinek F. 1989. Self-organized language modeling for speech recognition. In Readings in Speech Recognition. Edited by Waibel A. and Lee K.F. Morgan Kaufman. San Mateo. CA. pages450-506.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Katz</author>
</authors>
<title>Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer”.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics. Speech and Signal Processing.</journal>
<volume>35</volume>
<pages>400--401</pages>
<contexts>
<context position="1735" citStr="Katz 1987" startWordPosition="262" endWordPosition="263">sion of the best 1,600,000 word pairs decreases the perplexity of the MI-Trigram model by 20 percent compared with the trigram model. In the meanwhile, evaluation on Chinese word segmentation shows that about 35 percent of errors can be corrected by using the MI-Trigram model compared with the trigram model. 1 Introduction Language modeling is the attempt to characterize, capture and exploit the regularities and constraints in natural language. Among various language modeling approaches, ngram modeling has been widely used in many applications, such as speech recognition, machine translation (Katz 1987; Jelinek 1989; Gale and Church 1990; Brown et al. 1992; Yang et al. 1996; Bai et al 1998; Zhou et al 1999; Rosenfeld 2000; Gao et al 2002). Although ngram modeling is simple in nature and easy to use, it has obvious deficiencies. For instance, ngram modeling can only capture the short distance context dependency within an N-word window where currently the largest practical N for natural language is three. In the meantime, it is found that there always exist many preferred relationships between words. Two highly associated word pairs are Ta(/ ffqR (“not only/but also”) and X t / TP ± (“doctor/</context>
<context position="4684" citStr="Katz 1987" startWordPosition="790" endWordPosition="791"> wm , where wi ’s are the words that make up the hypothesis, the probability of the word string P can be computed by using the (S) chain rule: m P S P w P wi w ( ) ( 1) ( |1 ) i 1 − = ∏ (2.1) i=2 And the probability P (wi |wi− 1) can be estimated by using maximum likelihood estimation (MLE) principle: C w w ( ) i − 1 i P w w ( |) i i − = 1C(wi−1) Where C(• ) represents the number of times the sequence occurs in the training data. In practice, due to the data sparseness problem, some smoothing techniques, such as linear interpolation (Jelinek 1989; Chen and Goodman 1999) and back-off modeling (Katz 1987), are applied. Obviously, an ngram model assumes that the probability of the next word w is independent of i word string w in the history. The difference i − n 1 between bigram, trigram and other ngram models is the value of N. The parameters of an ngram are thus the probabilities: (2.5) By taking log function to both sides of equation (2.1), we have the log probability of the word Given S = w 1 w 2 . . . wm , an ngram model string log P(S) : estimates the log probability of the word string P(wn |w1 ... wn−1) For all w1, ww,..., wn ∈ V . 2 So, the classical task of statistical language modelin</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Katz S.M. 1987. “ Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer”. IEEE Transactions on Acoustics. Speech and Signal Processing. 35. 400-401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Meyer</author>
</authors>
<title>Loci of contextual effects on visual word recognition. In Attention and Performance V, edited by P.Rabbitt and S.Dornie.</title>
<date>1975</date>
<pages>98--116</pages>
<publisher>Acdemic Press.</publisher>
<marker>Meyer, 1975</marker>
<rawString>Meyer D. et al. 1975. Loci of contextual effects on visual word recognition. In Attention and Performance V, edited by P.Rabbitt and S.Dornie. pages98-116. Acdemic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
</authors>
<title>Adaptive statistical language modeling: A Maximum Entropy Approach.</title>
<date>1994</date>
<tech>Ph.D. Thesis,</tech>
<institution>Carneige Mellon University.</institution>
<contexts>
<context position="2660" citStr="Rosenfeld 1994" startWordPosition="415" endWordPosition="416">ncy within an N-word window where currently the largest practical N for natural language is three. In the meantime, it is found that there always exist many preferred relationships between words. Two highly associated word pairs are Ta(/ ffqR (“not only/but also”) and X t / TP ± (“doctor/nurse”). Psychological experiments in Meyer et al. (1975) indicated that the human’s reaction to a highly associated word pair was stronger and faster than that to a poorly associated word pair. Such preference information is very useful for natural language processing (Church et al. 1990; Hiddle et al. 1993; Rosenfeld 1994; Zhou et al.1998; Zhou et al 1999). Obviously, the preference relationships between words can expand from a short to long distance. While we can use traditional ngram modeling to capture the short distance context dependency, the long distance context dependency should also be exploited properly. The purpose of this paper is to propose a new MI-Ngram modeling approach to capture the context dependency over both a short distance and a long distance. Experimentation shows that this new MI-Ngram modeling approach can significantly decrease the perplexity of the new MI-Ngram model compared with t</context>
</contexts>
<marker>Rosenfeld, 1994</marker>
<rawString>Rosenfeld R. 1994. Adaptive statistical language modeling: A Maximum Entropy Approach. Ph.D. Thesis, Carneige Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
</authors>
<title>Two decades of language modelling: where do we go from here.</title>
<date>2000</date>
<booktitle>Proceedings of IEEE.</booktitle>
<pages>88--1270</pages>
<contexts>
<context position="1857" citStr="Rosenfeld 2000" startWordPosition="286" endWordPosition="287">e trigram model. In the meanwhile, evaluation on Chinese word segmentation shows that about 35 percent of errors can be corrected by using the MI-Trigram model compared with the trigram model. 1 Introduction Language modeling is the attempt to characterize, capture and exploit the regularities and constraints in natural language. Among various language modeling approaches, ngram modeling has been widely used in many applications, such as speech recognition, machine translation (Katz 1987; Jelinek 1989; Gale and Church 1990; Brown et al. 1992; Yang et al. 1996; Bai et al 1998; Zhou et al 1999; Rosenfeld 2000; Gao et al 2002). Although ngram modeling is simple in nature and easy to use, it has obvious deficiencies. For instance, ngram modeling can only capture the short distance context dependency within an N-word window where currently the largest practical N for natural language is three. In the meantime, it is found that there always exist many preferred relationships between words. Two highly associated word pairs are Ta(/ ffqR (“not only/but also”) and X t / TP ± (“doctor/nurse”). Psychological experiments in Meyer et al. (1975) indicated that the human’s reaction to a highly associated word </context>
</contexts>
<marker>Rosenfeld, 2000</marker>
<rawString>Rosenfeld R. 2000. Two decades of language modelling: where do we go from here. Proceedings of IEEE. 88:1270-1278. August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Shannon</author>
</authors>
<title>Prediction and entropy of printed English.</title>
<date>1951</date>
<journal>Bell Systems Technical Journal</journal>
<volume>30</volume>
<pages>50--64</pages>
<marker>Shannon, 1951</marker>
<rawString>Shannon C.E. 1951. Prediction and entropy of printed English. Bell Systems Technical Journal 30, 50-64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y J Yang</author>
</authors>
<title>Adaptive linguistic decoding system for Mandarin speech recognition applications.</title>
<date>1996</date>
<journal>Computer Processing of Chinese &amp; Oriental Languages</journal>
<volume>10</volume>
<issue>2</issue>
<pages>211--224</pages>
<marker>Yang, 1996</marker>
<rawString>Yang Y.J. et al. 1996. Adaptive linguistic decoding system for Mandarin speech recognition applications. Computer Processing of Chinese &amp; Oriental Languages 10(2), 211-224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhou GuoDong</author>
<author>Lua Kim Teng</author>
</authors>
<title>Word Association and MI-Trigger-based Language Modeling.</title>
<date>1998</date>
<booktitle>Proceedings of the Thirtieth-sixth Annual Meeting of the Association for Computational Linguistics and the Seventeenth International Conference on Computational Linguistics (COLING-ACL’1998).</booktitle>
<pages>10--14</pages>
<location>Montreal,</location>
<marker>GuoDong, Teng, 1998</marker>
<rawString>Zhou GuoDong and Lua Kim Teng, 1998. Word Association and MI-Trigger-based Language Modeling. Proceedings of the Thirtieth-sixth Annual Meeting of the Association for Computational Linguistics and the Seventeenth International Conference on Computational Linguistics (COLING-ACL’1998). Montreal, Canada. pages10-14. August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhou GuoDong</author>
<author>Lua KimTeng</author>
</authors>
<title>Interpolation of N-gram and MI-based Trigger Pair Language Modeling in Mandarin Speech Recognition,</title>
<date>1999</date>
<journal>Computer, Speech and Language,</journal>
<volume>13</volume>
<issue>2</issue>
<pages>123--135</pages>
<marker>GuoDong, KimTeng, 1999</marker>
<rawString>Zhou GuoDong and Lua KimTeng. 1999. Interpolation of N-gram and MI-based Trigger Pair Language Modeling in Mandarin Speech Recognition, Computer, Speech and Language, 13(2), 123-135.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>