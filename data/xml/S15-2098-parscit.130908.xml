<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002720">
<title confidence="0.970688">
Sentibase: Sentiment Analysis in Twitter on a Budget
</title>
<author confidence="0.926868">
Satarupa Guha ,∗ Aditya Joshi ∗, Vasudeva Varma
</author>
<affiliation confidence="0.891567">
Search and Information Extraction Lab
International Institute of Information Technology, Hyderabad
Gachibowli, Hyderabad, Telengana, India
</affiliation>
<email confidence="0.9895465">
{satarupa.guha,aditya.joshi}@research.iiit.ac.in
vv@iiit.ac.in
</email>
<sectionHeader confidence="0.995517" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997951">
Like SemEval 2013 and 2014, the task Sen-
timent Analysis in Twitter found a place in
this year’s SemEval too and attracted an un-
precedented number of participations. This
task comprises of four sub-tasks. We partici-
pated in subtask 2 — Message polarity classi-
fication. Although we lie a few notches down
from the top system, we present a very simple
yet effective approach to handle this problem
that can be implemented in a single day!
</bodyText>
<sectionHeader confidence="0.998982" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996691814814815">
Social media not only acts as a proxy for the real
world society, it also offers a treasure trove of data
for different types of analyses like Trend Analysis,
Event Detection and Sentiment Analysis, to name
a few. SemEval 2015 Task 10 subtask B (Rosen-
thal et al., 2015) specifically deals with the task of
Sentiment Analysis in Twitter. Sentiment Analy-
sis in social media in general and Twitter in par-
ticular has a wide range of applications — Compa-
nies/services can gauge the public sentiment towards
the new product or service they launched, political
parties can estimate their chances of winning the
upcoming elections by monitoring what people are
saying on Twitter about them, and so on. In spite of
the availability of huge amount of data and the huge
promises they entail, working with social media data
is far more challenging than regular text data. Be-
ing user-generated, the data is noisy; there are mis-
spellings, unreliable capitalization, widespread use
∗The first two authors made equal contribution to this work
of creative acronyms, lack of grammar, and a style
of writing that is very typical of its own which makes
the problem of Sentiment Analysis on Twitter more
challenging. Also, the cues for positive or negative
sentiment in social media text are starkly different,
thereby generating a whole new domain for explo-
ration.
</bodyText>
<sectionHeader confidence="0.999724" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99937132">
SemEval 2013 (Nakov et al., 2013) and 2014 tasks
(Rosenthal et al., 2014) on Sentiment Analysis in
Twitter not only contributed to this field by mak-
ing huge amounts of annotated datasets available for
research, but also encouraged researchers to come
up with better solutions for this challenging prob-
lem. There has been numerous initiatives outside
SemEval too. (Pak and Paroubek, 2010) is one of
the early attempts at using Twitter as a corpus for
Sentiment Analysis, which shows how to automat-
ically collect a corpus for the same and performs
linguistic analysis of the collected corpus. (Bakli-
wal et al., 2012) presents a simple sentiment scoring
function which uses prior information to classify and
weight various sentiment bearing words/phrases in
tweets. (Wilson et al., 2005) demontrates an efficient
technique for automatically identifying the contex-
tual polarity for a large subset of sentiment expres-
sions. (Mohammad et al., 2013) and (Kiritchenko et
al., 2014) establishes benchmark in Sentiment Ana-
lyis in Twitter as well as in the field of Aspect Based
Sentiment Analysis by incorporating various inno-
vative linguistic features. (Agarwal et al., 2011)
introduced POS-specific prior polarity features and
(Kouloumpis et al., 2011) explored the use of a tree
</bodyText>
<page confidence="0.948819">
590
</page>
<bodyText confidence="0.8829096">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 590–594,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
kernel to obviate the need for tedious feature engi-
neering. (Kouloumpis et al., 2011) evaluate the use-
fulness of existing lexical resources as well as fea-
tures that capture information about the informal and
creative language used in microblogging. Recent
publication from (Socher et al., 2013) has further
raised the bar for Sentiment Analysis in general, but
it is not specifically designed to tackle tweets data.
</bodyText>
<sectionHeader confidence="0.999142" genericHeader="method">
3 Approach
</sectionHeader>
<subsectionHeader confidence="0.999921">
3.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999622363636364">
We acquire a list of acronyms and their expanded
forms 1. We use this list as a look-up table and
replace all occurrences of acronyms in our data by
their expanded forms. We normalize all numbers
that find a place in our data by replacing them with
the string ‘0’. We do not remove stop words be-
cause they often contribute heavily towards express-
ing sentiment/emotion. We do not also stem the
words because stemming leads to the loss of the
parts of speech information of the word and makes
the use of lexicons unnecessarily complicated.
</bodyText>
<subsectionHeader confidence="0.999755">
3.2 Vocabulary Generation
</subsectionHeader>
<bodyText confidence="0.981565647058824">
We assign a unique ID to all words occurring in our
data. All the hashtags we encounter in the data are
hashed to a single string place holder and a single
unique ID is assigned to it, as opposed to differ-
ent Ids for different hashtags. Hashtags are mostly
formed by concatenation of multiple words without
any space in between, and therefore, unless hashtags
are segmented into meaningful chunks, raw hashtags
seldom add any semantics to the sentence. Hence,
we do not distinguish between the different hashtags
and consider them as a single unit. Similarly, we
hash all mentions of the kind @user1 and @user2
to a single string placeholder and assign a single ID
to it. This is because these words prefixed by ‘@’
are all named entities and do not contribute anything
to the semantic meaning or towards the polarity of a
tweet.
</bodyText>
<footnote confidence="0.919381">
1Dowloaded from https://github.com/
TaikerLiang/Twitter/blob/master/Data/
Knowledge\_Database/Slang\%20Dictionary/a.
</footnote>
<subsectionHeader confidence="0.999407">
3.3 Feature Engineering
</subsectionHeader>
<bodyText confidence="0.995157">
The task required us to classify a tweet into positive,
negative and neutral polarity categories. This can
essentially be treated as a 2-step process
</bodyText>
<listItem confidence="0.9976365">
• Classify each tweet into subjective (posi-
tive/negative) and objective(neutral) classes.
• Classify subjective tweets into positive and
negative ones.
</listItem>
<bodyText confidence="0.999741181818182">
We keep this philosophy in mind, but do not ex-
plicitly model the problem as two sub-problems.
We treat them as a single step, but we select fea-
tures such that some of the features are best suited
for distinguishing between subjective and objective
classes, while some others are engineered to be able
to tell a positive tweet from a negative one. The
problem with treating the problem as a pipeline of
two steps is that we would have to deal with the
propagation of errors from one step to the other. If
a subjective tweet is mis-classified as an objective
one, we rob that tweet of its opportunity of being
classified any further in the next step and immedi-
ately label them as neutral. This might be detri-
mental in cases where certain features lead us to be-
lieve a tweet is objective, while a combination of all
features might rightly lean them towards positive or
negative polarities. We take aid from both extrin-
sic features like emoticons and grapheme stretching
as well as intrinsic ones like unigrams and so on.
Following is the list of features we employ and also
their underlying motivation:
</bodyText>
<listItem confidence="0.986407533333333">
• Unigrams — For each word in a tweet, we look
up the vocabulary we generated in the previous
step. If the word is present in the vocabulary,
we determine its position in the feature vector
from the unique ID assigned to it and put 1 in
its position. All other positions are 0 by default.
Unigram features contribute to understanding
both the distinction between subjective and ob-
jective tweets as well as between positive and
negative tweets.
• Number of hashtags — Inspection of the data
lead us to believe that the more the number of
hashtags in the tweets, the more the author’s
involvement with it and hence more the subjec-
tivity.
</listItem>
<page confidence="0.827859">
591
</page>
<listItem confidence="0.9019315">
• Presence of URLs — A factual tweet is often
accompanied by a URL as a proof of its valid-
</listItem>
<bodyText confidence="0.908919">
ity or for more enthusiastic of the author’s fol-
lowers to go and explore the news/fact further.
Hence, presence of URLs is likely to indicate
that the sentence is objective/neutral.
For example- “Jose Iglesias / Igleisas started
at shortstop Wednesday night for the second
http://t.co/Gkpx9Blu” and “Today In History
November 02, 1958 Elvis gave a party at his
hotel before going out on maneuvers. He sang
and... http://t.co/Za9bLTcE”
</bodyText>
<listItem confidence="0.939608484848485">
• Presence of exclamation marks — From our
observation, a subjective tweet is much more
likely to be ended by exclamation marks than a
purely factual or objective tweet. Further, posi-
tive tweets are more prone to contain exclama-
tion than negative ones.
• Presence of question marks and wh-words —
Tweets containing question marks or wh-words
like “why” and “where” are seldom objective.
Statistics tells us that this feature can act as a
strong cue for not only subjectivity, but also of
negativity.
• Number of positive/negative emoticons — An
emoticon is a representation of a facial ex-
pression such as a smile or frown, formed
by various combinations of keyboard charac-
ters and used heavily in tweets to convey the
writer’s feelings or intended tone. Quite intu-
itively, positive emoticons accompany positive
tweets and negative emotions juxtapose nega-
tive tweets. More the number of emoticons, it
leans with more confidence towards the corre-
sponding polarity. However, a lot of sarcastic
tweets also contain positive emoticons, but we
do not explicitly handle sarcasm and hence ig-
nore such possibility.
• Number of named entities — Just like URLs,
named entities act as another indication for
factual and hence neutral sentences. For ex-
ample, “Remember this? Santorum: Rom-
ney, Obama healthcare mandates one and the
same http://t.co/sIoG48TO #TheRealRomney
@userX @userY”. We extract named entities
</listItem>
<bodyText confidence="0.969979666666667">
using a python wrapper for the Stanford NER
tool (Finkel et al., 2005). However, ablation
experiments done after the submission of sys-
tem in the competition revealed that this fea-
ture actually ended up degrading performance
by more than 2% of F1 score.
</bodyText>
<listItem confidence="0.998272806451613">
• Grapheme Stretching — Words with charac-
ters repeated multiple times (at least twice)
herald strong subjectivity, most often positive.
For example, “Not only is @userZ home from
China, she’s in LA...I called her and screamed
Mandyyyyyyyyyyyyy...I’m gonna hug her for
2 hrs tomorrow!” and “daniel radcliffe was
sooo attractive in the 3rd and 5th films omg
im in love”. We used the number of grapheme
stretched words as a feature for our sentiment
classifier.
• Number of words with unusual capitalization
— Words with characters made upper-case or
lower-case out of turn might potentially convey
subjectivity. This feature also proved to slightly
degrade performance, during post-competition
ablation experiments.
• Number of words with all the characters cap-
italized — Strongly positive or strongly nega-
tive tweets often have words in all caps in or-
der to convey the excitement that normally the
loudness of a voice intones.
• Presence of numbers — Numbers are used pro-
fusely in factual tweets. For example- “13:58
Steven Pourier, Jr. (OLC) MADE the 2nd of
the 2 shot Free Throw. DaSU leads 90 - 36 in
the 2nd Half. #NAIAMBB”. Hence the pres-
ence of numbers can be used as an useful fea-
ture to distinguish between subjective and ob-
jective tweets.
• Lexicon features- We use 15 lexicon fea-
</listItem>
<bodyText confidence="0.586013857142857">
tures extracted from publicly available lexi-
cons, which prove to be one of the most pow-
erful features in our features list. Social media
data, specially tweets, have a style of language
use that is quite different from other text data.
We included lexicons which are specially tai-
lored to handle social media data, like Senti-
</bodyText>
<page confidence="0.992359">
592
</page>
<bodyText confidence="0.79020375">
ment140 and NRC Hashtag Lexicon. We elab-
orate on the lexicon features as the following:
From Sentiwordnet (Baccianella et al., 2010),
we extract
</bodyText>
<listItem confidence="0.9989424">
– Number of positive tokens
– Number of negative tokens
– Total positive sentiment score
– Total negative sentiment score
– Maximum sentiment score
</listItem>
<bodyText confidence="0.806586">
From Bing Liu’s opinion lexicon (Hu and Liu,
2004), we extract
</bodyText>
<listItem confidence="0.931305214285714">
– Number of positive tokens
– Number of negative tokens
From MPQA subjectivity lexicon (Wilson et
al., 2005), we extract
– Number of positive tokens
– Number of negative tokens
From NRC Emotion Association lexicon (Mo-
hammad and Turney, 2013), we extract
– Number of positive tokens
– Number of negative tokens
From Sentiment140 lexicon (Go et al., 2009),
we extract
– Sum of sentiment score
– Maximum sentiment score
</listItem>
<bodyText confidence="0.712003">
From NRC Hashtag Lexicon (Mohammad and
Kiritchenko, 2014), we extract
</bodyText>
<listItem confidence="0.9006015">
– Sum of sentiment score
– Maximum sentiment score
</listItem>
<subsectionHeader confidence="0.998724">
3.4 Training Classifier
</subsectionHeader>
<bodyText confidence="0.99779675">
Once we have extracted all the features, we train a
linear SVM using Python based Scikit Learn library
(Pedregosa et al., 2011) for the purpose of classifi-
cation. We experimentally ascertained the optimal
value of the parameter C to be 0.025. In order to
cope with the slight class imbalance in the data, we
automatically adjust weights inversely proportional
to class frequencies.
</bodyText>
<table confidence="0.998458">
Feature F1
all 56.67
all - number of entities 58.68 2
all - grapheme 56.52
all - exclamation 55.91
all - emoticons 56.61
all - number of hastags 56.69
all - unigrams 53.52
all - lexicons 48.40
all - wh words 56.62
all - illegal capitalization 56.90 2
</table>
<tableCaption confidence="0.997971">
Table 1: Ablation Experiment on Twitter 2015 dataset.
</tableCaption>
<table confidence="0.999767375">
Dataset Our Score Best Score
Twitter 2015 56.67 64.84
Twitter 2015 Sarcasm 62.96 65.77
Twitter 2014 63.29 74.42
Twitter 2014 Sarcasm 47.07 59.11
Twitter 2013 61.56 72.80
Live Journal 2014 67.55 75.34
SMS 2013 59.26 68.49
</table>
<tableCaption confidence="0.994512">
Table 2: Official Results for SemEval 2015.
</tableCaption>
<sectionHeader confidence="0.986952" genericHeader="evaluation">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999984615384615">
We used the official training and test sets provided
for the SemEval 2015 task to train and evaluate our
system. Tweets in the training data that were not
available any more through the Twitter API were re-
moved from the training set. For the evaluation, we
compute precision, recall and F1 measures as com-
puted by the scorer package provided for the task.
Table 1 shows the ablation experiment we carried
out, thereby highlighting the usefulness of the vari-
ous features used. Table 2 records the F1 score ob-
tained by our submission on different datasets. Our
performance on Twitter 2015 Sarcasm data set is en-
couraging - we stand 4th on the data set.
</bodyText>
<sectionHeader confidence="0.994368" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.843308666666667">
This papers details the description of the system sub-
mitted by team Sentibase for SemEval 2015 Task
10. As the title of the paper suggests, the goal
</bodyText>
<footnote confidence="0.9959065">
2The fact that this feature degrades performance became
clear during post-competition experiments
</footnote>
<page confidence="0.998109">
593
</page>
<bodyText confidence="0.999756666666667">
of this work was more to, put together a com-
plete Sentiment Analyzer for Twitter in a day’s time
that achieves competitive performance without go-
ing through complex modeling techniques, than to
up the ante in the state-of-the-work picture of Senti-
ment Analysis.
</bodyText>
<sectionHeader confidence="0.998501" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999597928571429">
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analysis
of twitter data. In Proceedings of the Workshop on
Languages in Social Media, LSM ’11, pages 30–38,
Stroudsburg, PA, USA.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
in Proc. of LREC.
Akshat Bakliwal, Piyush Arora, Senthil Madhappan,
Nikhil Kapre, Mukesh Singh, and Vasudeva Varma.
2012. Mining sentiments from tweets. In Proceedings
of the 3rd Workshop in Computational Approaches
to Subjectivity and Sentiment Analysis, WASSA ’12,
pages 11–18, Stroudsburg, PA, USA.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In In ACL, pages 363–370.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
Processing, pages 1–6.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ’04, pages
168–177, New York, NY, USA.
Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, and
Saif Mohammad. 2014. Nrc-canada-2014: Detecting
aspects and sentiment in customer reviews. In Pro-
ceedings of the 8th International Workshop on Seman-
tic Evaluation (SemEval 2014), pages 437–442.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the omg!
Edward Loper and Steven Bird. 2002. Nltk: The natural
language toolkit. In In Proceedings of the ACL Work-
shop on Effective Tools and Methodologies for Teach-
ing Natural Language Processing and Computational
Linguistics. Philadelphia: Association for Computa-
tional Linguistics.
Saif M. Mohammad and Svetlana Kiritchenko. 2014.
Using hashtags to capture fine emotion categories from
tweets. Computational Intelligence, pages n/a–n/a.
Saif M. Mohammad and Peter D. Turney. 2013.
Crowdsourcing a word-emotion association lexicon.
29(3):436–465.
Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. Nrc-canada: Building the state-of-the-art
in sentiment analysis of tweets. CoRR, abs/1308.6242.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on Se-
mantic Evaluation (SemEval 2013), pages 312–320,
Atlanta, Georgia, USA, June.
Alexander Pak and Patrick Paroubek. 2010. Twit-
ter as a corpus for sentiment analysis and opinion
mining. In Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC’10).
Fabian Pedregosa, Gal Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier Grisel,
Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-
cent Dubourg, Jake Vanderplas, Alexandre Passos,
David Cournapeau, Matthieu Brucher, Matthieu Per-
rot, and douard Duchesnay. 2011. Scikit-learn: Ma-
chine learning in Python. Journal of Machine Learn-
ing Research, 12:2825–2830.
Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Har-
ris Papageorgiou, Ion Androutsopoulos, and Suresh
Manandhar. 2014. Semeval-2014 task 4: Aspect
based sentiment analysis. In Proceedings of the 8th
International Workshop on Semantic Evaluation (Se-
mEval 2014), pages 27–35, Dublin, Ireland, August.
Sara Rosenthal, Alan Ritter, Preslav Nakov, and Veselin
Stoyanov. 2014. Semeval-2014 task 9: Sentiment
analysis in twitter. In Proceedings of the 8th Inter-
national Workshop on Semantic Evaluation (SemEval
2014), pages 73–80, Dublin, Ireland, August.
Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko,
Saif M Mohammad, Alan Ritter, and Veselin Stoy-
anov. 2015. Semeval-2015 task 10: Sentiment analy-
sis in twitter. In Proceedings of the 9th International
Workshop on Semantic Evaluation, SemEval ’2015,
Denver, Colorado, June.
Richard Socher, Alex Perelygin, Jean Y. Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng, and
Christopher Potts. 2013. Recursive deep models for
semantic compositionality over a sentiment treebank.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the Conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, HLT ’05, pages
347–354, Stroudsburg, PA, USA.
</reference>
<page confidence="0.998468">
594
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.680272">
<title confidence="0.997974">Sentibase: Sentiment Analysis in Twitter on a Budget</title>
<author confidence="0.994248">Guha Joshi Vasudeva</author>
<affiliation confidence="0.865041">Search and Information Extraction International Institute of Information Technology,</affiliation>
<address confidence="0.997778">Gachibowli, Hyderabad, Telengana,</address>
<email confidence="0.967186">vv@iiit.ac.in</email>
<abstract confidence="0.996991181818182">Like SemEval 2013 and 2014, the task Sentiment Analysis in Twitter found a place in this year’s SemEval too and attracted an unprecedented number of participations. This task comprises of four sub-tasks. We participated in subtask 2 — Message polarity classification. Although we lie a few notches down from the top system, we present a very simple yet effective approach to handle this problem that can be implemented in a single day!</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Boyi Xie</author>
<author>Ilia Vovsha</author>
<author>Owen Rambow</author>
<author>Rebecca Passonneau</author>
</authors>
<title>Sentiment analysis of twitter data.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Languages in Social Media, LSM ’11,</booktitle>
<pages>30--38</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3289" citStr="Agarwal et al., 2011" startWordPosition="520" endWordPosition="523"> linguistic analysis of the collected corpus. (Bakliwal et al., 2012) presents a simple sentiment scoring function which uses prior information to classify and weight various sentiment bearing words/phrases in tweets. (Wilson et al., 2005) demontrates an efficient technique for automatically identifying the contextual polarity for a large subset of sentiment expressions. (Mohammad et al., 2013) and (Kiritchenko et al., 2014) establishes benchmark in Sentiment Analyis in Twitter as well as in the field of Aspect Based Sentiment Analysis by incorporating various innovative linguistic features. (Agarwal et al., 2011) introduced POS-specific prior polarity features and (Kouloumpis et al., 2011) explored the use of a tree 590 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 590–594, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics kernel to obviate the need for tedious feature engineering. (Kouloumpis et al., 2011) evaluate the usefulness of existing lexical resources as well as features that capture information about the informal and creative language used in microblogging. Recent publication from (Socher et al., 2013) has further </context>
</contexts>
<marker>Agarwal, Xie, Vovsha, Rambow, Passonneau, 2011</marker>
<rawString>Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow, and Rebecca Passonneau. 2011. Sentiment analysis of twitter data. In Proceedings of the Workshop on Languages in Social Media, LSM ’11, pages 30–38, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In</title>
<date>2010</date>
<booktitle>in Proc. of LREC.</booktitle>
<contexts>
<context position="11532" citStr="Baccianella et al., 2010" startWordPosition="1886" endWordPosition="1889">B”. Hence the presence of numbers can be used as an useful feature to distinguish between subjective and objective tweets. • Lexicon features- We use 15 lexicon features extracted from publicly available lexicons, which prove to be one of the most powerful features in our features list. Social media data, specially tweets, have a style of language use that is quite different from other text data. We included lexicons which are specially tailored to handle social media data, like Senti592 ment140 and NRC Hashtag Lexicon. We elaborate on the lexicon features as the following: From Sentiwordnet (Baccianella et al., 2010), we extract – Number of positive tokens – Number of negative tokens – Total positive sentiment score – Total negative sentiment score – Maximum sentiment score From Bing Liu’s opinion lexicon (Hu and Liu, 2004), we extract – Number of positive tokens – Number of negative tokens From MPQA subjectivity lexicon (Wilson et al., 2005), we extract – Number of positive tokens – Number of negative tokens From NRC Emotion Association lexicon (Mohammad and Turney, 2013), we extract – Number of positive tokens – Number of negative tokens From Sentiment140 lexicon (Go et al., 2009), we extract – Sum of s</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In in Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akshat Bakliwal</author>
<author>Piyush Arora</author>
<author>Senthil Madhappan</author>
<author>Nikhil Kapre</author>
<author>Mukesh Singh</author>
<author>Vasudeva Varma</author>
</authors>
<title>Mining sentiments from tweets.</title>
<date>2012</date>
<booktitle>In Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis, WASSA ’12,</booktitle>
<pages>11--18</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2737" citStr="Bakliwal et al., 2012" startWordPosition="437" endWordPosition="441">ed Work SemEval 2013 (Nakov et al., 2013) and 2014 tasks (Rosenthal et al., 2014) on Sentiment Analysis in Twitter not only contributed to this field by making huge amounts of annotated datasets available for research, but also encouraged researchers to come up with better solutions for this challenging problem. There has been numerous initiatives outside SemEval too. (Pak and Paroubek, 2010) is one of the early attempts at using Twitter as a corpus for Sentiment Analysis, which shows how to automatically collect a corpus for the same and performs linguistic analysis of the collected corpus. (Bakliwal et al., 2012) presents a simple sentiment scoring function which uses prior information to classify and weight various sentiment bearing words/phrases in tweets. (Wilson et al., 2005) demontrates an efficient technique for automatically identifying the contextual polarity for a large subset of sentiment expressions. (Mohammad et al., 2013) and (Kiritchenko et al., 2014) establishes benchmark in Sentiment Analyis in Twitter as well as in the field of Aspect Based Sentiment Analysis by incorporating various innovative linguistic features. (Agarwal et al., 2011) introduced POS-specific prior polarity features</context>
</contexts>
<marker>Bakliwal, Arora, Madhappan, Kapre, Singh, Varma, 2012</marker>
<rawString>Akshat Bakliwal, Piyush Arora, Senthil Madhappan, Nikhil Kapre, Mukesh Singh, and Vasudeva Varma. 2012. Mining sentiments from tweets. In Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis, WASSA ’12, pages 11–18, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In In ACL,</booktitle>
<pages>363--370</pages>
<contexts>
<context position="9600" citStr="Finkel et al., 2005" startWordPosition="1561" endWordPosition="1564">egative tweets. More the number of emoticons, it leans with more confidence towards the corresponding polarity. However, a lot of sarcastic tweets also contain positive emoticons, but we do not explicitly handle sarcasm and hence ignore such possibility. • Number of named entities — Just like URLs, named entities act as another indication for factual and hence neutral sentences. For example, “Remember this? Santorum: Romney, Obama healthcare mandates one and the same http://t.co/sIoG48TO #TheRealRomney @userX @userY”. We extract named entities using a python wrapper for the Stanford NER tool (Finkel et al., 2005). However, ablation experiments done after the submission of system in the competition revealed that this feature actually ended up degrading performance by more than 2% of F1 score. • Grapheme Stretching — Words with characters repeated multiple times (at least twice) herald strong subjectivity, most often positive. For example, “Not only is @userZ home from China, she’s in LA...I called her and screamed Mandyyyyyyyyyyyyy...I’m gonna hug her for 2 hrs tomorrow!” and “daniel radcliffe was sooo attractive in the 3rd and 5th films omg im in love”. We used the number of grapheme stretched words a</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In In ACL, pages 363–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alec Go</author>
<author>Richa Bhayani</author>
<author>Lei Huang</author>
</authors>
<title>Twitter sentiment classification using distant supervision.</title>
<date>2009</date>
<booktitle>Processing,</booktitle>
<pages>1--6</pages>
<contexts>
<context position="12109" citStr="Go et al., 2009" startWordPosition="1982" endWordPosition="1985">Sentiwordnet (Baccianella et al., 2010), we extract – Number of positive tokens – Number of negative tokens – Total positive sentiment score – Total negative sentiment score – Maximum sentiment score From Bing Liu’s opinion lexicon (Hu and Liu, 2004), we extract – Number of positive tokens – Number of negative tokens From MPQA subjectivity lexicon (Wilson et al., 2005), we extract – Number of positive tokens – Number of negative tokens From NRC Emotion Association lexicon (Mohammad and Turney, 2013), we extract – Number of positive tokens – Number of negative tokens From Sentiment140 lexicon (Go et al., 2009), we extract – Sum of sentiment score – Maximum sentiment score From NRC Hashtag Lexicon (Mohammad and Kiritchenko, 2014), we extract – Sum of sentiment score – Maximum sentiment score 3.4 Training Classifier Once we have extracted all the features, we train a linear SVM using Python based Scikit Learn library (Pedregosa et al., 2011) for the purpose of classification. We experimentally ascertained the optimal value of the parameter C to be 0.025. In order to cope with the slight class imbalance in the data, we automatically adjust weights inversely proportional to class frequencies. Feature F</context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter sentiment classification using distant supervision. Processing, pages 1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’04,</booktitle>
<pages>168--177</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="11743" citStr="Hu and Liu, 2004" startWordPosition="1921" endWordPosition="1924"> prove to be one of the most powerful features in our features list. Social media data, specially tweets, have a style of language use that is quite different from other text data. We included lexicons which are specially tailored to handle social media data, like Senti592 ment140 and NRC Hashtag Lexicon. We elaborate on the lexicon features as the following: From Sentiwordnet (Baccianella et al., 2010), we extract – Number of positive tokens – Number of negative tokens – Total positive sentiment score – Total negative sentiment score – Maximum sentiment score From Bing Liu’s opinion lexicon (Hu and Liu, 2004), we extract – Number of positive tokens – Number of negative tokens From MPQA subjectivity lexicon (Wilson et al., 2005), we extract – Number of positive tokens – Number of negative tokens From NRC Emotion Association lexicon (Mohammad and Turney, 2013), we extract – Number of positive tokens – Number of negative tokens From Sentiment140 lexicon (Go et al., 2009), we extract – Sum of sentiment score – Maximum sentiment score From NRC Hashtag Lexicon (Mohammad and Kiritchenko, 2014), we extract – Sum of sentiment score – Maximum sentiment score 3.4 Training Classifier Once we have extracted al</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’04, pages 168–177, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
<author>Colin Cherry</author>
<author>Saif Mohammad</author>
</authors>
<title>Nrc-canada-2014: Detecting aspects and sentiment in customer reviews.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>437--442</pages>
<contexts>
<context position="3096" citStr="Kiritchenko et al., 2014" startWordPosition="490" endWordPosition="493">e SemEval too. (Pak and Paroubek, 2010) is one of the early attempts at using Twitter as a corpus for Sentiment Analysis, which shows how to automatically collect a corpus for the same and performs linguistic analysis of the collected corpus. (Bakliwal et al., 2012) presents a simple sentiment scoring function which uses prior information to classify and weight various sentiment bearing words/phrases in tweets. (Wilson et al., 2005) demontrates an efficient technique for automatically identifying the contextual polarity for a large subset of sentiment expressions. (Mohammad et al., 2013) and (Kiritchenko et al., 2014) establishes benchmark in Sentiment Analyis in Twitter as well as in the field of Aspect Based Sentiment Analysis by incorporating various innovative linguistic features. (Agarwal et al., 2011) introduced POS-specific prior polarity features and (Kouloumpis et al., 2011) explored the use of a tree 590 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 590–594, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics kernel to obviate the need for tedious feature engineering. (Kouloumpis et al., 2011) evaluate the usefulness of e</context>
</contexts>
<marker>Kiritchenko, Zhu, Cherry, Mohammad, 2014</marker>
<rawString>Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, and Saif Mohammad. 2014. Nrc-canada-2014: Detecting aspects and sentiment in customer reviews. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 437–442.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efthymios Kouloumpis</author>
<author>Theresa Wilson</author>
<author>Johanna Moore</author>
</authors>
<title>Twitter sentiment analysis: The good the bad and the omg!</title>
<date>2011</date>
<contexts>
<context position="3367" citStr="Kouloumpis et al., 2011" startWordPosition="530" endWordPosition="533">nts a simple sentiment scoring function which uses prior information to classify and weight various sentiment bearing words/phrases in tweets. (Wilson et al., 2005) demontrates an efficient technique for automatically identifying the contextual polarity for a large subset of sentiment expressions. (Mohammad et al., 2013) and (Kiritchenko et al., 2014) establishes benchmark in Sentiment Analyis in Twitter as well as in the field of Aspect Based Sentiment Analysis by incorporating various innovative linguistic features. (Agarwal et al., 2011) introduced POS-specific prior polarity features and (Kouloumpis et al., 2011) explored the use of a tree 590 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 590–594, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics kernel to obviate the need for tedious feature engineering. (Kouloumpis et al., 2011) evaluate the usefulness of existing lexical resources as well as features that capture information about the informal and creative language used in microblogging. Recent publication from (Socher et al., 2013) has further raised the bar for Sentiment Analysis in general, but it is not specifically d</context>
</contexts>
<marker>Kouloumpis, Wilson, Moore, 2011</marker>
<rawString>Efthymios Kouloumpis, Theresa Wilson, and Johanna Moore. 2011. Twitter sentiment analysis: The good the bad and the omg!</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Loper</author>
<author>Steven Bird</author>
</authors>
<title>Nltk: The natural language toolkit. In</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics. Philadelphia: Association for Computational Linguistics.</booktitle>
<marker>Loper, Bird, 2002</marker>
<rawString>Edward Loper and Steven Bird. 2002. Nltk: The natural language toolkit. In In Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics. Philadelphia: Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Svetlana Kiritchenko</author>
</authors>
<title>Using hashtags to capture fine emotion categories from tweets. Computational Intelligence,</title>
<date>2014</date>
<pages>pages n/a–n/a.</pages>
<contexts>
<context position="12230" citStr="Mohammad and Kiritchenko, 2014" startWordPosition="2001" endWordPosition="2004">s – Total positive sentiment score – Total negative sentiment score – Maximum sentiment score From Bing Liu’s opinion lexicon (Hu and Liu, 2004), we extract – Number of positive tokens – Number of negative tokens From MPQA subjectivity lexicon (Wilson et al., 2005), we extract – Number of positive tokens – Number of negative tokens From NRC Emotion Association lexicon (Mohammad and Turney, 2013), we extract – Number of positive tokens – Number of negative tokens From Sentiment140 lexicon (Go et al., 2009), we extract – Sum of sentiment score – Maximum sentiment score From NRC Hashtag Lexicon (Mohammad and Kiritchenko, 2014), we extract – Sum of sentiment score – Maximum sentiment score 3.4 Training Classifier Once we have extracted all the features, we train a linear SVM using Python based Scikit Learn library (Pedregosa et al., 2011) for the purpose of classification. We experimentally ascertained the optimal value of the parameter C to be 0.025. In order to cope with the slight class imbalance in the data, we automatically adjust weights inversely proportional to class frequencies. Feature F1 all 56.67 all - number of entities 58.68 2 all - grapheme 56.52 all - exclamation 55.91 all - emoticons 56.61 all - num</context>
</contexts>
<marker>Mohammad, Kiritchenko, 2014</marker>
<rawString>Saif M. Mohammad and Svetlana Kiritchenko. 2014. Using hashtags to capture fine emotion categories from tweets. Computational Intelligence, pages n/a–n/a.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Peter D Turney</author>
</authors>
<title>Crowdsourcing a word-emotion association lexicon.</title>
<date>2013</date>
<pages>29--3</pages>
<contexts>
<context position="11997" citStr="Mohammad and Turney, 2013" startWordPosition="1962" endWordPosition="1966">al media data, like Senti592 ment140 and NRC Hashtag Lexicon. We elaborate on the lexicon features as the following: From Sentiwordnet (Baccianella et al., 2010), we extract – Number of positive tokens – Number of negative tokens – Total positive sentiment score – Total negative sentiment score – Maximum sentiment score From Bing Liu’s opinion lexicon (Hu and Liu, 2004), we extract – Number of positive tokens – Number of negative tokens From MPQA subjectivity lexicon (Wilson et al., 2005), we extract – Number of positive tokens – Number of negative tokens From NRC Emotion Association lexicon (Mohammad and Turney, 2013), we extract – Number of positive tokens – Number of negative tokens From Sentiment140 lexicon (Go et al., 2009), we extract – Sum of sentiment score – Maximum sentiment score From NRC Hashtag Lexicon (Mohammad and Kiritchenko, 2014), we extract – Sum of sentiment score – Maximum sentiment score 3.4 Training Classifier Once we have extracted all the features, we train a linear SVM using Python based Scikit Learn library (Pedregosa et al., 2011) for the purpose of classification. We experimentally ascertained the optimal value of the parameter C to be 0.025. In order to cope with the slight cla</context>
</contexts>
<marker>Mohammad, Turney, 2013</marker>
<rawString>Saif M. Mohammad and Peter D. Turney. 2013. Crowdsourcing a word-emotion association lexicon. 29(3):436–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>Nrc-canada: Building the state-of-the-art in sentiment analysis of tweets.</title>
<date>2013</date>
<journal>CoRR,</journal>
<pages>1308--6242</pages>
<contexts>
<context position="3065" citStr="Mohammad et al., 2013" startWordPosition="485" endWordPosition="488"> numerous initiatives outside SemEval too. (Pak and Paroubek, 2010) is one of the early attempts at using Twitter as a corpus for Sentiment Analysis, which shows how to automatically collect a corpus for the same and performs linguistic analysis of the collected corpus. (Bakliwal et al., 2012) presents a simple sentiment scoring function which uses prior information to classify and weight various sentiment bearing words/phrases in tweets. (Wilson et al., 2005) demontrates an efficient technique for automatically identifying the contextual polarity for a large subset of sentiment expressions. (Mohammad et al., 2013) and (Kiritchenko et al., 2014) establishes benchmark in Sentiment Analyis in Twitter as well as in the field of Aspect Based Sentiment Analysis by incorporating various innovative linguistic features. (Agarwal et al., 2011) introduced POS-specific prior polarity features and (Kouloumpis et al., 2011) explored the use of a tree 590 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 590–594, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics kernel to obviate the need for tedious feature engineering. (Kouloumpis et al., 201</context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. Nrc-canada: Building the state-of-the-art in sentiment analysis of tweets. CoRR, abs/1308.6242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Zornitsa Kozareva</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
<author>Theresa Wilson</author>
</authors>
<title>Semeval-2013 task 2: Sentiment analysis in twitter.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>312--320</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="2156" citStr="Nakov et al., 2013" startWordPosition="342" endWordPosition="345">ntail, working with social media data is far more challenging than regular text data. Being user-generated, the data is noisy; there are misspellings, unreliable capitalization, widespread use ∗The first two authors made equal contribution to this work of creative acronyms, lack of grammar, and a style of writing that is very typical of its own which makes the problem of Sentiment Analysis on Twitter more challenging. Also, the cues for positive or negative sentiment in social media text are starkly different, thereby generating a whole new domain for exploration. 2 Related Work SemEval 2013 (Nakov et al., 2013) and 2014 tasks (Rosenthal et al., 2014) on Sentiment Analysis in Twitter not only contributed to this field by making huge amounts of annotated datasets available for research, but also encouraged researchers to come up with better solutions for this challenging problem. There has been numerous initiatives outside SemEval too. (Pak and Paroubek, 2010) is one of the early attempts at using Twitter as a corpus for Sentiment Analysis, which shows how to automatically collect a corpus for the same and performs linguistic analysis of the collected corpus. (Bakliwal et al., 2012) presents a simple </context>
</contexts>
<marker>Nakov, Rosenthal, Kozareva, Stoyanov, Ritter, Wilson, 2013</marker>
<rawString>Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva, Veselin Stoyanov, Alan Ritter, and Theresa Wilson. 2013. Semeval-2013 task 2: Sentiment analysis in twitter. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 312–320, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Pak</author>
<author>Patrick Paroubek</author>
</authors>
<title>Twitter as a corpus for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10).</booktitle>
<contexts>
<context position="2510" citStr="Pak and Paroubek, 2010" startWordPosition="398" endWordPosition="401">ch makes the problem of Sentiment Analysis on Twitter more challenging. Also, the cues for positive or negative sentiment in social media text are starkly different, thereby generating a whole new domain for exploration. 2 Related Work SemEval 2013 (Nakov et al., 2013) and 2014 tasks (Rosenthal et al., 2014) on Sentiment Analysis in Twitter not only contributed to this field by making huge amounts of annotated datasets available for research, but also encouraged researchers to come up with better solutions for this challenging problem. There has been numerous initiatives outside SemEval too. (Pak and Paroubek, 2010) is one of the early attempts at using Twitter as a corpus for Sentiment Analysis, which shows how to automatically collect a corpus for the same and performs linguistic analysis of the collected corpus. (Bakliwal et al., 2012) presents a simple sentiment scoring function which uses prior information to classify and weight various sentiment bearing words/phrases in tweets. (Wilson et al., 2005) demontrates an efficient technique for automatically identifying the contextual polarity for a large subset of sentiment expressions. (Mohammad et al., 2013) and (Kiritchenko et al., 2014) establishes b</context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>Alexander Pak and Patrick Paroubek. 2010. Twitter as a corpus for sentiment analysis and opinion mining. In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Gal Varoquaux</author>
<author>Alexandre Gramfort</author>
<author>Vincent Michel</author>
<author>Bertrand Thirion</author>
<author>Olivier Grisel</author>
<author>Mathieu Blondel</author>
<author>Peter Prettenhofer</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<location>Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David</location>
<contexts>
<context position="12445" citStr="Pedregosa et al., 2011" startWordPosition="2037" endWordPosition="2040">subjectivity lexicon (Wilson et al., 2005), we extract – Number of positive tokens – Number of negative tokens From NRC Emotion Association lexicon (Mohammad and Turney, 2013), we extract – Number of positive tokens – Number of negative tokens From Sentiment140 lexicon (Go et al., 2009), we extract – Sum of sentiment score – Maximum sentiment score From NRC Hashtag Lexicon (Mohammad and Kiritchenko, 2014), we extract – Sum of sentiment score – Maximum sentiment score 3.4 Training Classifier Once we have extracted all the features, we train a linear SVM using Python based Scikit Learn library (Pedregosa et al., 2011) for the purpose of classification. We experimentally ascertained the optimal value of the parameter C to be 0.025. In order to cope with the slight class imbalance in the data, we automatically adjust weights inversely proportional to class frequencies. Feature F1 all 56.67 all - number of entities 58.68 2 all - grapheme 56.52 all - exclamation 55.91 all - emoticons 56.61 all - number of hastags 56.69 all - unigrams 53.52 all - lexicons 48.40 all - wh words 56.62 all - illegal capitalization 56.90 2 Table 1: Ablation Experiment on Twitter 2015 dataset. Dataset Our Score Best Score Twitter 201</context>
</contexts>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, 2011</marker>
<rawString>Fabian Pedregosa, Gal Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and douard Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Pontiki</author>
<author>Dimitris Galanis</author>
<author>John Pavlopoulos</author>
<author>Harris Papageorgiou</author>
<author>Ion Androutsopoulos</author>
<author>Suresh Manandhar</author>
</authors>
<title>Semeval-2014 task 4: Aspect based sentiment analysis.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>27--35</pages>
<location>Dublin, Ireland,</location>
<marker>Pontiki, Galanis, Pavlopoulos, Papageorgiou, Androutsopoulos, Manandhar, 2014</marker>
<rawString>Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. 2014. Semeval-2014 task 4: Aspect based sentiment analysis. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 27–35, Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Alan Ritter</author>
<author>Preslav Nakov</author>
<author>Veselin Stoyanov</author>
</authors>
<title>Semeval-2014 task 9: Sentiment analysis in twitter.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>73--80</pages>
<location>Dublin, Ireland,</location>
<contexts>
<context position="2196" citStr="Rosenthal et al., 2014" startWordPosition="349" endWordPosition="352">a is far more challenging than regular text data. Being user-generated, the data is noisy; there are misspellings, unreliable capitalization, widespread use ∗The first two authors made equal contribution to this work of creative acronyms, lack of grammar, and a style of writing that is very typical of its own which makes the problem of Sentiment Analysis on Twitter more challenging. Also, the cues for positive or negative sentiment in social media text are starkly different, thereby generating a whole new domain for exploration. 2 Related Work SemEval 2013 (Nakov et al., 2013) and 2014 tasks (Rosenthal et al., 2014) on Sentiment Analysis in Twitter not only contributed to this field by making huge amounts of annotated datasets available for research, but also encouraged researchers to come up with better solutions for this challenging problem. There has been numerous initiatives outside SemEval too. (Pak and Paroubek, 2010) is one of the early attempts at using Twitter as a corpus for Sentiment Analysis, which shows how to automatically collect a corpus for the same and performs linguistic analysis of the collected corpus. (Bakliwal et al., 2012) presents a simple sentiment scoring function which uses pr</context>
</contexts>
<marker>Rosenthal, Ritter, Nakov, Stoyanov, 2014</marker>
<rawString>Sara Rosenthal, Alan Ritter, Preslav Nakov, and Veselin Stoyanov. 2014. Semeval-2014 task 9: Sentiment analysis in twitter. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 73–80, Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Preslav Nakov</author>
<author>Svetlana Kiritchenko</author>
<author>Saif M Mohammad</author>
<author>Alan Ritter</author>
<author>Veselin Stoyanov</author>
</authors>
<title>Semeval-2015 task 10: Sentiment analysis in twitter.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’2015,</booktitle>
<location>Denver, Colorado,</location>
<contexts>
<context position="1032" citStr="Rosenthal et al., 2015" startWordPosition="156" endWordPosition="160">emEval too and attracted an unprecedented number of participations. This task comprises of four sub-tasks. We participated in subtask 2 — Message polarity classification. Although we lie a few notches down from the top system, we present a very simple yet effective approach to handle this problem that can be implemented in a single day! 1 Introduction Social media not only acts as a proxy for the real world society, it also offers a treasure trove of data for different types of analyses like Trend Analysis, Event Detection and Sentiment Analysis, to name a few. SemEval 2015 Task 10 subtask B (Rosenthal et al., 2015) specifically deals with the task of Sentiment Analysis in Twitter. Sentiment Analysis in social media in general and Twitter in particular has a wide range of applications — Companies/services can gauge the public sentiment towards the new product or service they launched, political parties can estimate their chances of winning the upcoming elections by monitoring what people are saying on Twitter about them, and so on. In spite of the availability of huge amount of data and the huge promises they entail, working with social media data is far more challenging than regular text data. Being use</context>
</contexts>
<marker>Rosenthal, Nakov, Kiritchenko, Mohammad, Ritter, Stoyanov, 2015</marker>
<rawString>Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko, Saif M Mohammad, Alan Ritter, and Veselin Stoyanov. 2015. Semeval-2015 task 10: Sentiment analysis in twitter. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’2015, Denver, Colorado, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<contexts>
<context position="3876" citStr="Socher et al., 2013" startWordPosition="606" endWordPosition="609"> features. (Agarwal et al., 2011) introduced POS-specific prior polarity features and (Kouloumpis et al., 2011) explored the use of a tree 590 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 590–594, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics kernel to obviate the need for tedious feature engineering. (Kouloumpis et al., 2011) evaluate the usefulness of existing lexical resources as well as features that capture information about the informal and creative language used in microblogging. Recent publication from (Socher et al., 2013) has further raised the bar for Sentiment Analysis in general, but it is not specifically designed to tackle tweets data. 3 Approach 3.1 Preprocessing We acquire a list of acronyms and their expanded forms 1. We use this list as a look-up table and replace all occurrences of acronyms in our data by their expanded forms. We normalize all numbers that find a place in our data by replacing them with the string ‘0’. We do not remove stop words because they often contribute heavily towards expressing sentiment/emotion. We do not also stem the words because stemming leads to the loss of the parts of</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>347--354</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2907" citStr="Wilson et al., 2005" startWordPosition="462" endWordPosition="465"> of annotated datasets available for research, but also encouraged researchers to come up with better solutions for this challenging problem. There has been numerous initiatives outside SemEval too. (Pak and Paroubek, 2010) is one of the early attempts at using Twitter as a corpus for Sentiment Analysis, which shows how to automatically collect a corpus for the same and performs linguistic analysis of the collected corpus. (Bakliwal et al., 2012) presents a simple sentiment scoring function which uses prior information to classify and weight various sentiment bearing words/phrases in tweets. (Wilson et al., 2005) demontrates an efficient technique for automatically identifying the contextual polarity for a large subset of sentiment expressions. (Mohammad et al., 2013) and (Kiritchenko et al., 2014) establishes benchmark in Sentiment Analyis in Twitter as well as in the field of Aspect Based Sentiment Analysis by incorporating various innovative linguistic features. (Agarwal et al., 2011) introduced POS-specific prior polarity features and (Kouloumpis et al., 2011) explored the use of a tree 590 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 590–594, Denver, </context>
<context position="11864" citStr="Wilson et al., 2005" startWordPosition="1941" endWordPosition="1944"> of language use that is quite different from other text data. We included lexicons which are specially tailored to handle social media data, like Senti592 ment140 and NRC Hashtag Lexicon. We elaborate on the lexicon features as the following: From Sentiwordnet (Baccianella et al., 2010), we extract – Number of positive tokens – Number of negative tokens – Total positive sentiment score – Total negative sentiment score – Maximum sentiment score From Bing Liu’s opinion lexicon (Hu and Liu, 2004), we extract – Number of positive tokens – Number of negative tokens From MPQA subjectivity lexicon (Wilson et al., 2005), we extract – Number of positive tokens – Number of negative tokens From NRC Emotion Association lexicon (Mohammad and Turney, 2013), we extract – Number of positive tokens – Number of negative tokens From Sentiment140 lexicon (Go et al., 2009), we extract – Sum of sentiment score – Maximum sentiment score From NRC Hashtag Lexicon (Mohammad and Kiritchenko, 2014), we extract – Sum of sentiment score – Maximum sentiment score 3.4 Training Classifier Once we have extracted all the features, we train a linear SVM using Python based Scikit Learn library (Pedregosa et al., 2011) for the purpose of</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 347–354, Stroudsburg, PA, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>