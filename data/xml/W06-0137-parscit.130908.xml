<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015969">
<title confidence="0.9803295">
Chinese Word Segmentation based on an Approach of Maximum Entropy
Modeling
</title>
<author confidence="0.993715">
Yan Song&apos; Jiaqing Guo&apos; Dongfeng Cai2
</author>
<affiliation confidence="0.7931055">
Natural Language Processing Lab
Shenyang Institute of Aeronautical Engineering
</affiliation>
<address confidence="0.739337">
Shenyang, 110034, China
</address>
<email confidence="0.8203675">
1.{mattsure,guojiaqing}@gmail.com
2.cdf@ge-soft.com
</email>
<sectionHeader confidence="0.991629" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999418529411765">
In this paper, we described our Chinese
word segmentation system for the 3rd
SIGHAN Chinese Language Processing
Bakeoff Word Segmentation Task. Our
system deal with the Chinese character se-
quence by using the Maximum Entropy
model, which is fully automatically gen-
erated from the training data by analyz-
ing the character sequences from the train-
ing corpus. We analyze its performance
on both closed and open tracks on Mi-
crosoft Research (MSRA) and University
of Pennsylvania and University of Col-
orado (UPUC) corpus. It is shown that we
can get the results just acceptable without
using dictionary. The conclusion is also
presented.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999945">
In the 3rd SIGHAN Chinese Language Process-
ing Bakeoff Word Segmentation Task, we partici-
pated in both closed and open tracks on Microsoft
Research corpus (MSRA for short) and University
of Pennsylvania and University of Colorado cor-
pus (UPUC for short). The following sections de-
scribed how our system works and presented the
results and analysis. Finally, the conclusion is pre-
sented with discussions of the system.
</bodyText>
<sectionHeader confidence="0.991636" genericHeader="method">
2 System Overview
</sectionHeader>
<bodyText confidence="0.999244466666667">
Using Maximum Entropy approach for Chinese
Word Segmentation is not a fresh idea, some pre-
vious works (Xue and Shen, 2003; Low, Ng and
Guo, 2005) have got good performance in this
field. But what we consider in the process of
Segmentation is another way. We treat the input
text which need to be segmented as a sequence of
the Chinese characters, The segment process is, in
fact, to find where we should split the character se-
quence. The point is to get the segment probability
between 2 Chinese characters, which is different
from dealing with the character itself.
In this section, training and segmentation pro-
cess of the system is described to show how our
system works.
</bodyText>
<subsectionHeader confidence="0.999843">
2.1 Pre-Process of Training
</subsectionHeader>
<bodyText confidence="0.995678222222222">
For the first step we find the Minimal Segment
Unit (MSU for short) of a text fragment in the
training corpus. A MSU is a character or a string
which is the minimal unit in a text fragment that
cannot be segmented any more. According to the
corpus, all of the MSUs can be divided into 5
type classes: “C” - Chinese Character (such as
✴❭✵ and ✴�✵), “AB” - alphabetic string
(such as “SIGHAN”), “EN” - digit string (such
as “1234567”), “CN” - Chinese number string
(such as ✴ ➌ ③ ✓ ➏ ✵) and “P” - punctua-
tion (✴➜✵ ,✴✧✵ ,✴ ➯✵, etc). Besides the
classes above, we define a tag ”NL” as a special
MSU, which refers to the beginning or ending of a
text fragment. So, any MSU u can be described
as: uECUABUENUCNUPU{NL}. In order
to check the capability of the pure Maximum En-
tropy model, in closed tracks, we didn’t have any
type of classes, the MSU here is every character
of the text fragment, uEC&apos;U{NL}. For instance,
✴ ➲ ❶ �, ❭ ✡SIGHAN2006➞ ❝ ➀ ♠ ✧ ✵
is segmented into these MSUs: ✴ ➲/❶/�/❭/
✡/S/I/G/H/A/N/2/0/0/6/➞/❝/➀/♠/✧✵.
Once we get all the MSUs of a text fragment,
we can get the value of the Nexus Coefficient (NC
for short) of any 2 adjacent MSUs according to
the training corpus. The set of NC value can be
</bodyText>
<page confidence="0.972926">
201
</page>
<bodyText confidence="0.9637027">
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 201–204,
Sydney, July 2006. c�2006 Association for Computational Linguistics
described as: NC E 10, 11, where 0 means those
2 MSUs are segmented and 1 means they are not
segmented (Roughly, we appoint r = 0 if either
one of the 2 adjacent MSUs is NL). For example,
the NC value of these 2 MSUs &amp;quot;❭&amp;quot; and &amp;quot;ff&amp;quot;
in the text fragment &amp;quot;❭ff&amp;quot; is 0 since these 2
characters is segmented according to the training
corpus.
</bodyText>
<subsectionHeader confidence="0.999114">
2.2 Training
</subsectionHeader>
<bodyText confidence="0.99910375">
Since the segmentation is to obtain NC value of
any 2 adjacent MSUs (here we call the interspace
of the 2 adjacent MSUs a check point, illustrated
below),
</bodyText>
<equation confidence="0.74411275">
...U−3 U−2 U−1 U+1 U+2 U+3...
Check Point of U_1 and U+1
we built a tool to extract the feature as follows:
(α) U−3, U−2, U−1, U+1, U+2, U+3
(β) U−1U+1
(γ) r−2r−1
(δ) U−3r−2, U−2r−1
(E) r−2U−2, r−1U−1
</equation>
<bodyText confidence="0.999075923076923">
In these features above, U+,,, (U−,,,) refers to
the following (previous) n MSU of the check
point with the information of relative position
(Intuitively, We consider the same MSU has
different effect on the NC value of the check point
when its relative position is different to check
point). And U−1U+1 is the 2 adjacent MSUs of
the check point. r−2r−1 is the NC value of the
previous 2 check points. Similarly, the (δ) and (E)
features represent the MSUs with their adjacent
r. For instance, in the sentence ➲➫➌❻➙■❁,
we can extract these features for the check point
between the MSU ➲ and ➫:
</bodyText>
<equation confidence="0.989210833333333">
(α) NL−3,NL−2,➲−1,Z+1,_+2,❻+3,
(β) ➲−1Z+1
(γ) 00 (because ➲ is the boundary of the sen-
tence)
(δ) NL−30,NL−20
(E) 0NL−2,0➲−1
</equation>
<bodyText confidence="0.978637">
and also these features for the check point be-
tween the MSU ❻ and ➙:
</bodyText>
<listItem confidence="0.522926">
(α) Z−3,_−2,❻−1,➙+1,■+2,❁+3
(β) ❻−1➙+1
</listItem>
<figureCaption confidence="0.9999935">
Figure 1: MSRA training curve
Figure 2: UPUC training curve
</figureCaption>
<bodyText confidence="0.939693666666667">
(γ) 01 (for UPUC corpus, here the value is 00
since ➌❻ is segmented into 2 characters, but in
MSRA corpus, ➌❻ is treated as a word)
</bodyText>
<equation confidence="0.927505">
(δ) Z−30,_−21
(0 0_−2,1❻−1
</equation>
<bodyText confidence="0.999189538461539">
After the extraction of the features, we use the
ZhangLe’s Maximum Entropy Toolkit1 to train the
model with a feature cutoff of 1. In order to get
the best number of iteration, 9/10 of the training
data is used to train the model, and the other 1/10
portion of the training data is used to evaluate the
model. Figure 1 and 2 show the results of the eval-
uation on MSRA and UPUC corpus.
From the figures we can see the best iteration
number range from 555 to 575 for MSRA corpus,
and 360 to 375 for UPUC corpus. So we decide
the iteration for 560 rounds for MSRA tracks and
365 rounds for UPUC tracks, respectively.
</bodyText>
<subsectionHeader confidence="0.996975">
2.3 Segmentation
</subsectionHeader>
<bodyText confidence="0.9697335">
As we mentioned in the beginning of this section,
the segmentation is the process to obtain the value
</bodyText>
<footnote confidence="0.995949">
1Download from http://maxent.sourceforge.net
</footnote>
<page confidence="0.995349">
202
</page>
<bodyText confidence="0.999916">
of every NC in a text fragment. This process is
similar to the training process. Firstly, We scan
the text fragment from start to end to get all of
the MSUs. Then we can extract all of the features
from the text fragment and decide which check
point we should tag as r = 0 by this equation:
</bodyText>
<equation confidence="0.980133">
K αf&apos;(r|c) (1)
Ar |c) = Z H j
j=1
</equation>
<bodyText confidence="0.999167636363636">
where K is the number of features, Z is the nor-
malization constant used to ensure that a probabil-
ity distribution results, and c represents the con-
text of the check point. αj is the weight for fea-
ture fj, here {α1α2 ... αKI is generated by the
training data. We then compute P (r = 0|c) and
P(r = 1|c) by the equation (1).
After one check point is treated with value of
r, the system shifts backward to the next check
point until all of the check point in the whole text
fragment are treated. And by calculating:
</bodyText>
<equation confidence="0.9189455">
αfk(ri|ci) (2)
j
</equation>
<bodyText confidence="0.999824428571429">
to get an r sequence which can maximize P. From
this process we can see that the sequence is, in fact,
a second-order Markov Model. Thus it is easily to
think about more tags prior to the check point (as
an nth-order Markov Model) to get more accuracy,
but in this paper we only use the previous 2 tags
from the check point.
</bodyText>
<subsectionHeader confidence="0.998912">
2.4 Identification of New words
</subsectionHeader>
<bodyText confidence="0.9999351">
We perform the new word(s) identification as a
post-process by check the word formation power
(WFP) of characters. The WFP of a character is
defined as: WFP(c) = Nwc/Nc, where Nwc is
the number of times that the character c appears
in a word of at least 2 characters in the training
corpus, Nc is the number of times the character c
occurs in the training corpus. After a text fragment
is segmented by our system, we extract all consec-
utive single characters. If at least 2 consecutive
characters have the WFP larger than our threshold
of 0.88, we polymerize them together as a word.
For example, &amp;quot;➨Ö➋&amp;quot; is anew word which is
segmented as &amp;quot;➨/Ö/➋&amp;quot; by our system, WFP
of these 3 characters is 0.9517,0.9818 and 1.0 re-
spectively, then they are polymerized as one word.
Besides the WFP, during the experiments, we
find that the Maximum Entropy model can poly-
merize some MSUs as a new word (We call it poly-
merization characteristic of the model), such as ➟
➸✌➘ in the training corpus, we can extract ➟
➸✌ as the previous context feature of the check
point after ✌, in another string 0AM&apos;
,,A ✌➂, we can
extract the backward context ➂ of the check point
after ✌ with r = 1. Then in the test, a new word
➟➸✌➂ is recognized by the model since ➟
➸✌ and ➂ are polymerized if ✌➂ appears to-
gether a large number of times in the training cor-
pus.
</bodyText>
<sectionHeader confidence="0.963465" genericHeader="method">
3 Performance analysis
</sectionHeader>
<bodyText confidence="0.998268285714286">
Here Table 1 illustrates the results of all 4 tracks
we participate. The first column is the track name,
and the 2nd column presents the Recall (R), the
3rd column the Precision (P), the 4th column is
F-measure (F). The Roov refers to the recall of the
out-of-vocabulary words and the Riv refers to the
recall of the words in training corpus.
</bodyText>
<table confidence="0.9987138">
Track R P F Roo„ Ri„
MSRA Closed 0.923 0.929 0.926 0.554 0.936
MSRA Open 0.938 0.946 0.942 0.706 0.946
UPUC Closed 0.902 0.887 0.895 0.568 0.934
UPUC Open 0.926 0.906 0.917 0.660 0.954
</table>
<tableCaption confidence="0.999883">
Table 1: Results of our system in 4 tracks.
</tableCaption>
<subsectionHeader confidence="0.996411">
3.1 Closed tracks
</subsectionHeader>
<bodyText confidence="0.999977916666667">
For all of the closed tracks, we perform the seg-
mentation as we mentioned in the section above,
without any class defined. Every MSU we extract
from the training data is a character, which may be
a Chinese character, an English letter or a single
digit. We extract the features based on this kind of
MSUs to generate the models. The results show
these models are not precise.
For the UPUC closed track, the official released
training data is rather small. Then the capability
of the model is limited, this is the most reasonable
negative effect on our F-measure 0.895.
</bodyText>
<subsectionHeader confidence="0.999478">
3.2 Open tracks
</subsectionHeader>
<bodyText confidence="0.999770375">
The primary change between open tracks and
closed tracks is that we have classified 5 classes
(“C”,“AB”,“EN”,“CN” and “P”) to MSUs in or-
der to improve the accuracy of the model. The
classification really works and affects the perfor-
mance of the system in a great deal. As this text
fragment 1998❝ can be recognized as (EN)(C),
which can also presents 1644❝, thus 1644❝ can
</bodyText>
<equation confidence="0.996245272727273">
1
Z
P=
n−1H
i=1
Ari|ci) =
n−1H
i=1
K
H
j=1
</equation>
<page confidence="0.997284">
203
</page>
<bodyText confidence="0.999948235294117">
be easily recognized though there is no 1664❝ in
the training data.
The training corpus we used in UPUC open
track is the same as in UPUC closed track. With
those 5 classes, it is easily seen that the F-measure
increased by 2.2% in the open tracks.
For the MSRA open track, we adjust the class
“P” by removing the punctuation “✦” from the
class, because in the MSRA corpus, “✦” can be
a part of a organization name, such as “✦” in
✴➙✂��✦�➨�✉�➈✡➡✵. Besides,
we add the Microsoft Research training data of
SIGHAN bakeoff 2005 as extended training cor-
pus. The larger training data cooperate with the
classification method, the F-measure of the open
track increased to 0.942 as comparison with 0.926
of closed track.
</bodyText>
<subsectionHeader confidence="0.996584">
3.3 Discussion of the tracks
</subsectionHeader>
<bodyText confidence="0.9999928">
Through the tracks, we tested the performance by
using the pure Maximum Entropy model in closed
tracks and run with the improved model with clas-
sified MSUs in open tracks. It is shown that the
pure model without any additional methods can
hardly make us satisfied, for the open tracks, the
model with classes are just acceptable in segmen-
tation.
In both closed and open tracks, we use the
same new word identification process, and with
the polymerization characteristic of the model, we
find the Roo„ is better than we expected.
On the other hand, in our system, there is no dic-
tionary used as we described in the sections above,
the RZ„ of each track shows that affects the system
performance.
Another factor affects our system in the UPUC
tracks is the wrongly written characters. Consider
that our system is based on the sequence of char-
acters, this kind of mistake is fatal. For example,
in the sentence ➛❶➹★-T⑩➀❖✛❁✛④Aij,
where ④l� is written as④Aij. The model cannot
recognize it since ④Aij didn’t occur in the train-
ing corpus. In the step of new word identification,
the WFPs of the 2 characters ④➜Aij are 0.8917
and 0.8310, thus they are wrongly segmented into
2 single characters while they are treated as a word
in the gold standard corpus. Therefore, we believe
the results can increase if there are no such mis-
takes in the test data.
</bodyText>
<sectionHeader confidence="0.999576" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9999925">
We propose an approach to Chinese word seg-
mentation by using Maximum Entropy model,
which focuses on the nexus relationship of any
2 adjacent MSUs in a text fragment. We tested
our system with pure Maximum Entropy models
and models with simplex classification method.
Compare with the pure models, the models with
classified MSUs show us better performances.
However, the Maximum Entropy models of our
system still need improvement if we want to
achieve higher performance. In future works,
we will consider using more training data and
add some hybrid methods with pre- and post-
processes to improve the system.
</bodyText>
<sectionHeader confidence="0.996673" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999974636363636">
We would like to thank all the colleagues of our
Lab. Without their encouragement and help, this
work cannot be accomplished in time.
This is our first time to participate such an in-
ternational bakeoff. There are a lot of things we
haven’t experienced ever before, but with the en-
thusiastic help from the organizers, we can come
through the task. Especially, We wish to thank
Gina-Anne Levow for her patience and immediate
reply for any of our questions, and we also thank
Olivia Kwong for the advice of paper submission.
</bodyText>
<sectionHeader confidence="0.999545" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999509692307692">
Nianwen Xue and Libin Shen. 2003. Chinese Word
Segmentation as LMR tagging. In Proceedings of
the Second SIGHAN Workshop on Chinese Lan-
guage Processing, p176-179.
Maosong Sun, Ming Xiao, B K Tsou. 2004. Chinese
Word Segmentation without Using Dictionary Based
on Unsupervised Learning Strategy. Chinese Jour-
nal of Computers, Vol.27, #6, p736-742.
Jin Kiat Low, Hwee Tou Ng and Wenyuan Guo.
2005. A Maximum Entropy Approach to Chinese
Word Segmentation. In Proceedings of the Fourth
SIGHAN Workshop on Chinese Language Process-
ing, p161-164.
</reference>
<page confidence="0.998893">
204
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.651460">
<title confidence="0.962065666666667">Chinese Word Segmentation based on an Approach of Maximum Entropy Modeling Natural Language Processing</title>
<affiliation confidence="0.960007">Shenyang Institute of Aeronautical</affiliation>
<address confidence="0.989394">Shenyang, 110034,</address>
<email confidence="0.997282">2.cdf@ge-soft.com</email>
<abstract confidence="0.982817777777778">In this paper, we described our Chinese word segmentation system for the 3rd SIGHAN Chinese Language Processing Bakeoff Word Segmentation Task. Our system deal with the Chinese character sequence by using the Maximum Entropy model, which is fully automatically generated from the training data by analyzing the character sequences from the training corpus. We analyze its performance on both closed and open tracks on Microsoft Research (MSRA) and University of Pennsylvania and University of Colorado (UPUC) corpus. It is shown that we can get the results just acceptable without using dictionary. The conclusion is also presented.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Libin Shen</author>
</authors>
<title>Chinese Word Segmentation as LMR tagging.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>176--179</pages>
<contexts>
<context position="1480" citStr="Xue and Shen, 2003" startWordPosition="223" endWordPosition="226"> dictionary. The conclusion is also presented. 1 Introduction In the 3rd SIGHAN Chinese Language Processing Bakeoff Word Segmentation Task, we participated in both closed and open tracks on Microsoft Research corpus (MSRA for short) and University of Pennsylvania and University of Colorado corpus (UPUC for short). The following sections described how our system works and presented the results and analysis. Finally, the conclusion is presented with discussions of the system. 2 System Overview Using Maximum Entropy approach for Chinese Word Segmentation is not a fresh idea, some previous works (Xue and Shen, 2003; Low, Ng and Guo, 2005) have got good performance in this field. But what we consider in the process of Segmentation is another way. We treat the input text which need to be segmented as a sequence of the Chinese characters, The segment process is, in fact, to find where we should split the character sequence. The point is to get the segment probability between 2 Chinese characters, which is different from dealing with the character itself. In this section, training and segmentation process of the system is described to show how our system works. 2.1 Pre-Process of Training For the first step</context>
</contexts>
<marker>Xue, Shen, 2003</marker>
<rawString>Nianwen Xue and Libin Shen. 2003. Chinese Word Segmentation as LMR tagging. In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing, p176-179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maosong Sun</author>
<author>Ming Xiao</author>
<author>B K Tsou</author>
</authors>
<title>Chinese Word Segmentation without Using Dictionary Based on Unsupervised Learning Strategy.</title>
<date>2004</date>
<journal>Chinese Journal of Computers,</journal>
<volume>27</volume>
<pages>736--742</pages>
<marker>Sun, Xiao, Tsou, 2004</marker>
<rawString>Maosong Sun, Ming Xiao, B K Tsou. 2004. Chinese Word Segmentation without Using Dictionary Based on Unsupervised Learning Strategy. Chinese Journal of Computers, Vol.27, #6, p736-742.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Kiat Low</author>
<author>Hwee Tou Ng</author>
<author>Wenyuan Guo</author>
</authors>
<title>A Maximum Entropy Approach to Chinese Word Segmentation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>161--164</pages>
<marker>Low, Ng, Guo, 2005</marker>
<rawString>Jin Kiat Low, Hwee Tou Ng and Wenyuan Guo. 2005. A Maximum Entropy Approach to Chinese Word Segmentation. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, p161-164.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>