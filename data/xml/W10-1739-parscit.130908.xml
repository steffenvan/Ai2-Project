<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000538">
<title confidence="0.748045">
MANY: Open Source MT System Combination at WMT’10
</title>
<author confidence="0.802971">
Lo?c Barrault
</author>
<affiliation confidence="0.765877">
LIUM, University of Le Mans
</affiliation>
<address confidence="0.601786">
Le Mans, France.
</address>
<email confidence="0.95373">
FirstName.LastName@lium.univ-lemans.fr
</email>
<sectionHeader confidence="0.992756" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997744785714286">
LIUM participated in the System Combi-
nation task of the Fifth Workshop on Sta-
tistical Machine Translation (WMT 2010).
Hypotheses from 5 French/English MT
systems were combined with MANY, an
open source system combination software
based on confusion networks currently de-
veloped at LIUM.
The system combination yielded signifi-
cant improvements in BLEU score when
applied on WMT’09 data. The same be-
havior has been observed when tuning is
performed on development data of this
year evaluation.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99869465">
This year, the LIUM computer science labora-
tory has participated in the French-English sys-
tem combination task at WMT’10 evaluation cam-
paign. The system used for this task is MANY1
(Barrault, 2010), an open source system combina-
tion software based on Confusion Networks (CN).
Several improvements have been made in order to
being able to combine many systems outputs in a
decent time.
The focus has been put on the tuning step, and
more precisely how to perform system parameter
tuning. Two methods have been experimented cor-
responding to two different representations of sys-
tem combination. In the first one, system combi-
nation is considered as a whole : fed by system
hypotheses as input and generating a new hypoth-
esis as output. The second method considers that
the alignment module is independent from the de-
coder, so that the parameters from each module
can be tuned separately.
</bodyText>
<footnote confidence="0.980351">
1MANY is available at the following address http://
www-lium.univ-lemans.fr/˜barrault/MANY
</footnote>
<bodyText confidence="0.993216166666667">
Those tuning approaches are described in sec-
tion 3. Before that, a quick description of MANY,
including recent developments, can be found in
section 2. Results on WMT’09 data are pre-
sented in section 4 along results of tuning on
newssyscombtune2010.
</bodyText>
<sectionHeader confidence="0.946119" genericHeader="method">
2 System description
</sectionHeader>
<bodyText confidence="0.999651111111111">
MANY is a system combination software (Bar-
rault, 2010) based on the decoding of a lattice
made of several Confusion Networks (CN). This is
a widespread approach in MT system combination
(Rosti et al., 2007); (Shen et al., 2008); (Karakos
et al., 2008). MANY can be decomposed in two
main modules. The first one is the alignment mod-
ule which actually is a modified version of TERp
(Snover et al., 2009). Its role is to incrementally
align the hypotheses against a backbone in order to
create a confusion network. Those confusion net-
works are then connected together to create a lat-
tice. This module uses different costs (which cor-
responds to a match, an insertion, a deletion, a sub-
stitution, a shift, a synonym and a stem) to com-
pute the best alignment and incrementally build
a confusion network. In the case of confusion
network, the match (substitution, synonyms, and
stems) costs are considered when the word in the
hypothesis matches (is a substitution, a synonyms
or a stems of) at least one word of the considered
confusion sets in the CN, as shown in Figure 1.
The second module is the decoder. This decoder
is based on the token pass algorithm and it accepts
as input the lattice previously created. The proba-
bilities computed in the decoder can be expressed
as follow:
</bodyText>
<equation confidence="0.9991416">
Len(W)
log(PW) _ E {α1logPws(n) + α2logPl.(n)
n=0
}
+α3Lpen(n) + α4Npen(n) (1)
</equation>
<bodyText confidence="0.892772">
where Len(W) is the length of the hypothesis,
</bodyText>
<page confidence="0.963935">
271
</page>
<note confidence="0.6552435">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 271–275,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<figure confidence="0.310799">
Is the dinner included ?
</figure>
<figureCaption confidence="0.9426675">
Figure 1: Incremental alignment with TERp re-
sulting in a confusion network.
</figureCaption>
<bodyText confidence="0.9826626">
PTAJ3(n) is the score of the nth word in the lattice,
Pl,,t(n) is its LM probability, Lpen(n) is the length
penalty (which apply when Wn is not a null-arc),
Npen(n) is the penalty applied when crossing a
null-arc, and the αz are the features weights.
</bodyText>
<subsectionHeader confidence="0.671024">
Multithreading
</subsectionHeader>
<bodyText confidence="0.999977611111111">
One major issue with system combination con-
cerns scaling. Indeed, in order to not lose infor-
mation about word order, all system hypotheses
are considered as backbone and all other hypothe-
ses are aligned to it to create a CN. Consequently,
if we consider N system outputs, then to build N
confusion networks, N * (N −1) alignments with
modified TERp have to be performed. Moreover,
in order to get better results, the TERp costs have
to be optimized, which requires a lot of iterations,
all of which calculate N * (N − 1) alignments.
However, the building of a CN with system i as
backbone does not depend on the building of CN
with other system as backbone. Therefore multi-
threading has been integrated into MANY so that
multiple CNs can be created in parallel. From now
on, the number of thread can be specified in the
configuration file.
</bodyText>
<sectionHeader confidence="0.991295" genericHeader="method">
3 Tuning
</sectionHeader>
<bodyText confidence="0.999951">
As mentioned before, MANY is made of two main
modules : the alignment module based on a modi-
fied version of TERp and the decoder. Considering
10 systems, 19 parameters in total have to be op-
timized in order to get better results. By default,
TERp costs are set to 0.0 for match and 1.0 for
everything else. These costs are not correct, since
a shift in that case will hardly be possible. TERp
costs, system priors, fudge factor, null-arc penalty,
length penalty are tuned with Condor (a global op-
timizer based on the Powell’s algorithm, (Berghen
and Bersini, 2005)).
Two ways of tuning have been experimented.
The first one consists in optimizing the whole set
of parameters together (see section 3.1). The sec-
ond one rely on the (maybe likely) independence
of the TERp parameters towards those of the de-
coder and consists in tuning TERp parameters in
a first step and then using the optimized TERp
costs when tuning the decoder parameters (see
section 3.2).
</bodyText>
<subsectionHeader confidence="0.999528">
3.1 Tuning all parameters together
</subsectionHeader>
<bodyText confidence="0.999928291666667">
Condor is an optimizer which aims at minimizing
a certain objective function. In our case, the ob-
jective function is the whole system combination.
As input, it takes the whole set of parameters (i.e.
TERp costs except match costs (which is always
set to 0), system priors, the fudge factor, and null-
arc and length penalty) and outputs -BLEU score.
The BLEU score is one of the most robust met-
rics as presented in (Leusch et al., 2009), which is
consequently an obvious target for optimization.
Such a tuning protocol has the disadvantage
to be slower as all the confusion networks have
to be regenerated at each step because the TERp
costs provided by the optimizer will hardly be the
same for two iterations (thus, confusion networks
computed during previous iterations can hardly be
reused). Another issue with this approach is that it
is hard to converge when the parameter set is that
large. This is mainly due to the fact that we can-
not guarantee the convexity of the problem. How-
ever, one advantage is that the possible correlation
between all parameters are taken into account dur-
ing the optimization process, which is not the case
when optimizing in several steps.
</bodyText>
<subsectionHeader confidence="0.993663">
3.2 Two-step tuning
</subsectionHeader>
<bodyText confidence="0.999739363636364">
Tuning TERp parameters : In order to opti-
mize TERp parameters (i.e. del, ins, sub, shift,
stem and syn costs), we have to determine which
measure to use to evaluate a certain configuration.
We naturally considered the minimization of the
TERp score. To do so, the confusion networks are
built using the set of parameters given by the op-
timizer. TERp scores are then calculated between
the reference and each CN, and summed up.
The goal of this step is to guide the confusion
networks generation process to produce sentences
</bodyText>
<figure confidence="0.996162714285714">
Match
Match
Paraphrase Match
Supper is included ?
the dinner
included ?
Is
supper NULL
Sub
Sub
Match
Ins
Sub
Match
Do you have calculated dinner ?
Is NULL the dinner included
?
supper
NULL
Do you have
calculated
</figure>
<page confidence="0.992748">
272
</page>
<bodyText confidence="0.999888384615385">
similar to the reference. Consequently, if the con-
fusion networks generated at this step have a lower
TERp score, then this means that the decoder is
more likely to find a better hypothesis inside.
Tuning decoder parameters : Based on the
TERp configuration determined at the previous
step, this step aims at finding good parameter val-
ues. Those parameters control the final hypothe-
sis size and the importance given to the language
model probabilities compared to the translation
scores (occurring on words). The metric which is
minimized is -BLEU for the same reasons men-
tioned in section 3.1.
</bodyText>
<sectionHeader confidence="0.998387" genericHeader="evaluation">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.9935462">
During experiments, data from last year evaluation
campaign are used for testing the tuning approach.
news-dev2009a is used as development set, and
news-dev2009b as internal test, these corpora are
described in Table 1.
</bodyText>
<table confidence="0.981264">
NAME #sent. #words #tok
news-dev2009a 1025 21583 24595
news-dev2009b 1026 21837 24940
</table>
<tableCaption confidence="0.97945">
Table 1: WMT’09 corpora: number of sentences,
words and tokens calculated on the reference.
</tableCaption>
<bodyText confidence="0.895232">
For the sake of speed and simplicity, the five
best systems (ranking given by score on dev) are
considered only. Baseline systems performances
on dev and test are presented in Table 2.
</bodyText>
<table confidence="0.998349333333333">
Corpus Sys0 Sys1 Sys2 Sys3 Sys4
Dev 18.20 17.83 20.14 21.06 17.72
Test 18.53 18.33 20.43 21.35 18.15
</table>
<tableCaption confidence="0.86467575">
Table 2: Baseline systems performance on
WMT’09 data (%BLEU).
When tuning all parameters together, the set ob-
tained is presented in Table 3. The 2-step tuning
</tableCaption>
<table confidence="0.9227365">
Costs : Del Stem Syn Ins Sub Shift
0.89 0.94 1.04 0.98 0.94 0.94
Dec.: Fudge Nullpen Lenpen
0.01 0.25 1.46
Weights : Sys0 Sys1 Sys2 Sys3 Sys4
0.04 0.04 0.16 0.26 0.04
</table>
<tableCaption confidence="0.814841333333333">
Table 3: Parameters obtained with 1-step tuning.
protocol applied on news-dev2009a provides the
set of parameters presented in Table 4.
</tableCaption>
<table confidence="0.9038955">
Costs : Del Stem Syn Ins Sub Shift
9e-6 0.89 1.22 0.26 0.44 1.76
Dec.: Fudge Nullpen Lenpen
0.1 0.27 2.1
Weights : Sys0 Sys1 Sys2 Sys3 Sys4
0.07 0.09 0.09 0.09 0.11
</table>
<tableCaption confidence="0.947876">
Table 4: Parameters obtained with 2-step tuning.
</tableCaption>
<table confidence="0.921300333333333">
Results on development corpus of WMT’09
(used as test set) are presented in Table 5. We
System Dev Test
Best single 21.06 21.35
MANY 22.08 22.28
MANY-2steps 21.94 22.09
</table>
<tableCaption confidence="0.982047">
Table 5: System Combination results on WMT’09
</tableCaption>
<bodyText confidence="0.94640125">
data.
can observe that 2-step tuning provides almost 0.9
BLEU point improvement on development corpus
which is well reflected on test set with a gain of
more than 0.7 BLEU. The best results are obtain
when tuning all parameters together, which give
more than 1 BLEU point improvement on dev and
more than 0.9 on test.
</bodyText>
<subsectionHeader confidence="0.978748">
4.1 Discussion
</subsectionHeader>
<bodyText confidence="0.999632782608695">
Choosing a measure to optimize the TERp costs is
not something easy. One important remark is that
default (equal) costs are not suitable to get good
confusion networks. The goal of the confusion
networks is to make possible the generation of a
new hypothesis which can be different from those
provided by each individual system.
In these experiments, TERp calculated between
the CNs and the reference is used as the distance
to be minimized by the optimizer. We can no-
tice that for the 2-step optimization, the deletion
cost is very small. This is probably not a value
which is expected, because in this case, this means
that deletions can occur in an hypothesis without
penalizing it a lot. However, this parameter set
has a beneficial impact on the system combination
performance. Another comment is that the sys-
tem weights are not directly proportional to the re-
sults. This suggests that some phrases proposed
by weaker systems can have a higher importance
for system combination.
By contrast, optimizing parameters all together
provides more fair weights, according to the re-
</bodyText>
<page confidence="0.995874">
273
</page>
<bodyText confidence="0.988339">
sults of the single systems.
</bodyText>
<subsectionHeader confidence="0.986685">
4.2 2010 evaluation campaign
</subsectionHeader>
<bodyText confidence="0.998293307692308">
For this year system combination tasks, a de-
velopment corpus (syscombtune) and the test
(syscombtest), described in Table 6, were pro-
vided to participants.
Language model : The English target language
models has been trained on all monolingual data
provided for the translation tasks. In addition,
LDC’s Gigaword collection was used for both lan-
guages. Data corresponding to the development
and test periods were removed from the Gigaword
collections.
Tuning on syscombdev2010 corpus produced
the parameter set presented in Table 7
</bodyText>
<table confidence="0.9663564">
Costs : Del Stem Syn Ins Sub Shift
Dec.: Fudge Nullpen Lenpen
0.01 0.33 1.6
Weights : Sys0 Sys1 Sys2 Sys3 Sys4
0.11 0.21 0.04 0.15 0.15
</table>
<tableCaption confidence="0.99933">
Table 7: Parameters obtained with tuning.
</tableCaption>
<bodyText confidence="0.970116333333333">
The result provided by the system with this con-
figuration can be compared to the single systems
in Table 8.
</bodyText>
<table confidence="0.996846857142857">
System newssyscombtune2010
Sys0 27.74
Sys1 27.26
Sys2 27.15
Sys3 27.06
Sys4 27.04
MANY 28.63
</table>
<tableCaption confidence="0.9384245">
Table 8: Baseline systems performance on
WMT’10 development data (%BLEU).
</tableCaption>
<bodyText confidence="0.904568666666667">
A behavior comparable to WMT’09 evaluation
campaign is observed, which suggests that the ap-
proach is correct.
</bodyText>
<sectionHeader confidence="0.913161" genericHeader="conclusions">
5 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999980307692308">
We have shown that tuning all parameters together
is better than 2-step tuning. However, the second
method has not been fully explored. Tuning TERp
parameters targeting minimum TERp score is not
satisfying. Therefore, an alternative measure, like
ngram agreement which would be more related to
BLEU, can be considered in order to obtain better
parameters.
Further improvement for MANY will be con-
sidered like case insensitive combination then re-
casing the output using majority vote on the con-
fusion networks. This is currently a work in
progress.
</bodyText>
<sectionHeader confidence="0.999146" genericHeader="acknowledgments">
6 Acknowledgement
</sectionHeader>
<bodyText confidence="0.99949325">
This work has been partially funded by the Eu-
ropean Union under the EuroMatrix Plus project
(http://www.euromatrixplus.net, IST-2007.2.2-
FP7-231720)
</bodyText>
<sectionHeader confidence="0.990907" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.568196555555556">
Barrault, L. (2010). MANY: Open source ma-
chine translation system combination. Prague
Bulletin of Mathematical Linguistics, Special
Issue on Open Source Tools for Machine Trans-
lation, 93:147–155.
Berghen, F. V. and Bersini, H. (2005). CON-
DOR, a new parallel, constrained extension of
Powell’s UOBYQA algorithm: Experimental
results and comparison with the DFO algo-
</bodyText>
<reference confidence="0.997226055555556">
rithm. Journal of Computational and Applied
Mathematics, 181:157–175.
Karakos, D., Eisner, J., Khudanpur, S., and
Dreyer, M. (2008). Machine translation sys-
tem combination using ITG-based alignments.
In 46th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies., pages 81–84, Columbus, Ohio,
USA.
Leusch, G., Matusov, E., and Ney, H. (2009).
The RWTH system combination system for
WMT 2009. In Proceedings of the Fourth
Workshop on Statistical Machine Translation,
pages 61–65, Athens, Greece.
Rosti, A.-V., Matsoukas, S., and Schwartz, R.
(2007). Improved word-level system combina-
tion for machine translation. In Association for
Computational Linguistics, pages 312–319.
</reference>
<table confidence="0.868242666666667">
NAME #sentences #words #words tok
syscombtune 455 9348 10755
syscombtest 2034 - -
</table>
<tableCaption confidence="0.946664">
Table 6: Description of WMT’10 corpora.
</tableCaption>
<page confidence="0.996886">
274
</page>
<reference confidence="0.952738777777778">
Shen, W., Delaney, B., Anderson, T., and Slyh,
R. (2008). The MIT-LL/AFRL IWSLT-2008
MT System. In International Workshop on Spo-
ken Language Translation, Hawaii, U.S.A.
Snover, M., Madnani, N., Dorr, B., and
Schwartz, R. (2009). TER-Plus: Para-
phrase, semantic, and alignment enhancements
to translation edit rate. Machine Translation
Journal.
</reference>
<page confidence="0.998319">
275
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.159986">
<note confidence="0.4479275">MANY: Open Source MT System Combination at WMT’10 Lo?c</note>
<affiliation confidence="0.74824">LIUM, University of Le Le Mans,</affiliation>
<email confidence="0.858002">FirstName.LastName@lium.univ-lemans.fr</email>
<abstract confidence="0.997891866666667">LIUM participated in the System Combination task of the Fifth Workshop on Statistical Machine Translation (WMT 2010). Hypotheses from 5 French/English MT systems were combined with MANY, an open source system combination software based on confusion networks currently developed at LIUM. The system combination yielded significant improvements in BLEU score when applied on WMT’09 data. The same behavior has been observed when tuning is performed on development data of this year evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>rithm</author>
</authors>
<journal>Journal of Computational and Applied Mathematics,</journal>
<pages>181--157</pages>
<marker>rithm, </marker>
<rawString>rithm. Journal of Computational and Applied Mathematics, 181:157–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Karakos</author>
<author>J Eisner</author>
<author>S Khudanpur</author>
<author>M Dreyer</author>
</authors>
<title>Machine translation system combination using ITG-based alignments.</title>
<date>2008</date>
<booktitle>In 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.,</booktitle>
<pages>81--84</pages>
<location>Columbus, Ohio, USA.</location>
<contexts>
<context position="2162" citStr="Karakos et al., 2008" startWordPosition="339" endWordPosition="342">rately. 1MANY is available at the following address http:// www-lium.univ-lemans.fr/˜barrault/MANY Those tuning approaches are described in section 3. Before that, a quick description of MANY, including recent developments, can be found in section 2. Results on WMT’09 data are presented in section 4 along results of tuning on newssyscombtune2010. 2 System description MANY is a system combination software (Barrault, 2010) based on the decoding of a lattice made of several Confusion Networks (CN). This is a widespread approach in MT system combination (Rosti et al., 2007); (Shen et al., 2008); (Karakos et al., 2008). MANY can be decomposed in two main modules. The first one is the alignment module which actually is a modified version of TERp (Snover et al., 2009). Its role is to incrementally align the hypotheses against a backbone in order to create a confusion network. Those confusion networks are then connected together to create a lattice. This module uses different costs (which corresponds to a match, an insertion, a deletion, a substitution, a shift, a synonym and a stem) to compute the best alignment and incrementally build a confusion network. In the case of confusion network, the match (substitu</context>
</contexts>
<marker>Karakos, Eisner, Khudanpur, Dreyer, 2008</marker>
<rawString>Karakos, D., Eisner, J., Khudanpur, S., and Dreyer, M. (2008). Machine translation system combination using ITG-based alignments. In 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies., pages 81–84, Columbus, Ohio, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Leusch</author>
<author>E Matusov</author>
<author>H Ney</author>
</authors>
<title>The RWTH system combination system for WMT</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>61--65</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="6151" citStr="Leusch et al., 2009" startWordPosition="1030" endWordPosition="1033">r and consists in tuning TERp parameters in a first step and then using the optimized TERp costs when tuning the decoder parameters (see section 3.2). 3.1 Tuning all parameters together Condor is an optimizer which aims at minimizing a certain objective function. In our case, the objective function is the whole system combination. As input, it takes the whole set of parameters (i.e. TERp costs except match costs (which is always set to 0), system priors, the fudge factor, and nullarc and length penalty) and outputs -BLEU score. The BLEU score is one of the most robust metrics as presented in (Leusch et al., 2009), which is consequently an obvious target for optimization. Such a tuning protocol has the disadvantage to be slower as all the confusion networks have to be regenerated at each step because the TERp costs provided by the optimizer will hardly be the same for two iterations (thus, confusion networks computed during previous iterations can hardly be reused). Another issue with this approach is that it is hard to converge when the parameter set is that large. This is mainly due to the fact that we cannot guarantee the convexity of the problem. However, one advantage is that the possible correlat</context>
</contexts>
<marker>Leusch, Matusov, Ney, 2009</marker>
<rawString>Leusch, G., Matusov, E., and Ney, H. (2009). The RWTH system combination system for WMT 2009. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 61–65, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A-V Rosti</author>
<author>S Matsoukas</author>
<author>R Schwartz</author>
</authors>
<title>Improved word-level system combination for machine translation.</title>
<date>2007</date>
<booktitle>In Association for Computational Linguistics,</booktitle>
<pages>312--319</pages>
<contexts>
<context position="2117" citStr="Rosti et al., 2007" startWordPosition="331" endWordPosition="334">rameters from each module can be tuned separately. 1MANY is available at the following address http:// www-lium.univ-lemans.fr/˜barrault/MANY Those tuning approaches are described in section 3. Before that, a quick description of MANY, including recent developments, can be found in section 2. Results on WMT’09 data are presented in section 4 along results of tuning on newssyscombtune2010. 2 System description MANY is a system combination software (Barrault, 2010) based on the decoding of a lattice made of several Confusion Networks (CN). This is a widespread approach in MT system combination (Rosti et al., 2007); (Shen et al., 2008); (Karakos et al., 2008). MANY can be decomposed in two main modules. The first one is the alignment module which actually is a modified version of TERp (Snover et al., 2009). Its role is to incrementally align the hypotheses against a backbone in order to create a confusion network. Those confusion networks are then connected together to create a lattice. This module uses different costs (which corresponds to a match, an insertion, a deletion, a substitution, a shift, a synonym and a stem) to compute the best alignment and incrementally build a confusion network. In the c</context>
</contexts>
<marker>Rosti, Matsoukas, Schwartz, 2007</marker>
<rawString>Rosti, A.-V., Matsoukas, S., and Schwartz, R. (2007). Improved word-level system combination for machine translation. In Association for Computational Linguistics, pages 312–319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Shen</author>
<author>B Delaney</author>
<author>T Anderson</author>
<author>R Slyh</author>
</authors>
<date>2008</date>
<booktitle>The MIT-LL/AFRL IWSLT-2008 MT System. In International Workshop on Spoken Language Translation,</booktitle>
<location>Hawaii, U.S.A.</location>
<contexts>
<context position="2138" citStr="Shen et al., 2008" startWordPosition="335" endWordPosition="338">ule can be tuned separately. 1MANY is available at the following address http:// www-lium.univ-lemans.fr/˜barrault/MANY Those tuning approaches are described in section 3. Before that, a quick description of MANY, including recent developments, can be found in section 2. Results on WMT’09 data are presented in section 4 along results of tuning on newssyscombtune2010. 2 System description MANY is a system combination software (Barrault, 2010) based on the decoding of a lattice made of several Confusion Networks (CN). This is a widespread approach in MT system combination (Rosti et al., 2007); (Shen et al., 2008); (Karakos et al., 2008). MANY can be decomposed in two main modules. The first one is the alignment module which actually is a modified version of TERp (Snover et al., 2009). Its role is to incrementally align the hypotheses against a backbone in order to create a confusion network. Those confusion networks are then connected together to create a lattice. This module uses different costs (which corresponds to a match, an insertion, a deletion, a substitution, a shift, a synonym and a stem) to compute the best alignment and incrementally build a confusion network. In the case of confusion netw</context>
</contexts>
<marker>Shen, Delaney, Anderson, Slyh, 2008</marker>
<rawString>Shen, W., Delaney, B., Anderson, T., and Slyh, R. (2008). The MIT-LL/AFRL IWSLT-2008 MT System. In International Workshop on Spoken Language Translation, Hawaii, U.S.A.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>N Madnani</author>
<author>B Dorr</author>
</authors>
<date></date>
<marker>Snover, Madnani, Dorr, </marker>
<rawString>Snover, M., Madnani, N., Dorr, B., and</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schwartz</author>
</authors>
<title>TER-Plus: Paraphrase, semantic, and alignment enhancements to translation edit rate.</title>
<date>2009</date>
<journal>Machine Translation Journal.</journal>
<marker>Schwartz, 2009</marker>
<rawString>Schwartz, R. (2009). TER-Plus: Paraphrase, semantic, and alignment enhancements to translation edit rate. Machine Translation Journal.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>