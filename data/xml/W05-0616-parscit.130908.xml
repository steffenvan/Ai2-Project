<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.993506">
An Analogical Learner for Morphological Analysis
</title>
<author confidence="0.977824">
Nicolas Stroppa &amp; Franc¸ois Yvon
</author>
<affiliation confidence="0.559109">
GET/ENST &amp; LTCI, UMR 5141
</affiliation>
<address confidence="0.991515">
46 rue Barrault, 75013 Paris, France
</address>
<email confidence="0.999635">
{stroppa,yvon}@enst.fr
</email>
<sectionHeader confidence="0.99565" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999909181818182">
Analogical learning is based on a two-
step inference process: (i) computation
of a structural mapping between a new
and a memorized situation; (ii) transfer
of knowledge from the known to the un-
known situation. This approach requires
the ability to search for and exploit such
mappings, hence the need to properly de-
fine analogical relationships, and to effi-
ciently implement their computation.
In this paper, we propose a unified defini-
tion for the notion of (formal) analogical
proportion, which applies to a wide range
of algebraic structures. We show that this
definition is suitable for learning in do-
mains involving large databases of struc-
tured data, as is especially the case in Nat-
ural Language Processing (NLP). We then
present experimental results obtained on
two morphological analysis tasks which
demonstrate the flexibility and accuracy of
this approach.
</bodyText>
<sectionHeader confidence="0.999335" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999949275">
Analogical learning (Gentner et al., 2001) is based
on a two-step inductive process. The first step con-
sists in the construction of a structural mapping be-
tween a new instance of a problem and solved in-
stances of the same problem. Once this mapping
is established, solutions for the new instance can be
induced, based on one or several analogs. The im-
plementation of this kind of inference process re-
quires techniques for searching for, and reasoning
with, structural mappings, hence the need to prop-
erly define the notion of analogical relationships and
to efficiently implement their computation.
In Natural Language Processing (NLP), the typ-
ical dimensionality of databases, which are made
up of hundreds of thousands of instances, makes
the search for complex structural mappings a very
challenging task. It is however possible to take ad-
vantage of the specific nature of linguistic data to
work around this problem. Formal (surface) analog-
ical relationships between linguistic representations
are often a good sign of deeper analogies: a surface
similarity between the word strings write and writer
denotes a deeper (semantic) similarity between the
related concepts. Surface similarities can of course
be misleading. In order to minimize such confu-
sions, one can take advantage of other specificities
of linguistic data: (i) their systemic organization in
(pseudo)-paradigms, and (ii) their high level of re-
dundancy. In a large lexicon, we can indeed expect
to find many instances of pairs like write-writer: for
instance read-reader, review-reviewer...
Complementing surface analogies with statistical
information thus has the potential to make the search
problem tractable, while still providing with many
good analogs. Various attempts have been made to
use surface analogies in various contexts: automatic
word pronunciation (Yvon, 1999), morphological
analysis (Lepage, 1999a; Pirrelli and Yvon, 1999)
and syntactical analysis (Lepage, 1999b). These ex-
periments have mainly focused on linear represen-
</bodyText>
<page confidence="0.947166">
120
</page>
<note confidence="0.960624">
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 120–127, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.999859666666667">
tations of linguistic data, taking the form of finite
sequences of symbols, using a restrictive and some-
times ad-hoc definition of the notion of an analogy.
The first contribution of this paper is to propose a
general definition of formal analogical proportions
for algebraic structures commonly used in NLP:
attribute-value vectors, words on finite alphabets and
labeled trees. The second contribution is to show
how these formal definitions can be used within an
instance-based learning framework to learn morpho-
logical regularities.
This paper is organized as follows. In Section 2,
our interpretation of analogical learning is intro-
duced and related to other models of analogical
learning and reasoning. Section 3 presents a general
algebraic framework for defining analogical propor-
tions as well as its instantiation to the case of words
and labeled trees. This section also discusses the
algorithmic complexity of the inference procedure.
Section 4 reports the results of experiments aimed
at demonstrating the flexibility of this model and at
assessing its generalization performance. We con-
clude by discussing current limitations of this model
and by suggesting possible extensions.
</bodyText>
<subsectionHeader confidence="0.8015615">
2 Principles of analogical learning
2.1 Analogical reasoning
</subsectionHeader>
<bodyText confidence="0.999953512820513">
The ability to identify analogical relationships be-
tween what looks like unrelated situations, and to
use these relationships to solve complex problems,
lies at the core of human cognition (Gentner et al.,
2001). A number of models of this ability have
been proposed, based on symbolic (e.g. (Falken-
heimer and Gentner, 1986; Thagard et al., 1990;
Hofstadter and the Fluid Analogies Research group,
1995)) or subsymbolic (e.g. (Plate, 2000; Holyoak
and Hummel, 2001)) approaches. The main focus
of these models is the dynamic process of analogy
making, which involves the identification of a struc-
tural mappings between a memorized and a new sit-
uation. Structural mapping relates situations which,
while being apparently very different, share a set of
common high-level relationships. The building of
a structural mapping between two situations utilizes
several subparts of their descriptions and the rela-
tionships between them.
Analogy-making seems to play a central role in
our reasoning ability; it is also invoked to explain
some human skills which do not involve any sort of
conscious reasoning. This is the case for many tasks
related to the perception and production of language:
lexical access, morphological parsing, word pronun-
ciation, etc. In this context, analogical models have
been proposed as a viable alternative to rule-based
models, and many implementation of these low-
level analogical processes have been proposed such
as decision trees, neural networks or instance-based
learning methods (see e.g. (Skousen, 1989; Daele-
mans et al., 1999)). These models share an accepta-
tion of analogy which mainly relies on surface simi-
larities between instances.
Our learner tries to bridge the gap between these
approaches and attempts to remain faithful to the
idea of structural analogies, which prevails in the
AI literature, while also exploiting the intuitions of
large-scale, instance-based learning models.
</bodyText>
<subsectionHeader confidence="0.999703">
2.2 Analogical learning
</subsectionHeader>
<bodyText confidence="0.996951407407408">
We consider the following supervised learning task:
a learner is given a set S of training instances
{Xi, ... , XJ independently drawn from some un-
known distribution. Each instance Xi is a vector
containing m features: (Xi1, ... , Xi,,,). Given S,
the task is to predict the missing features of partially
informed new instances. Put in more standard terms,
the set of known (resp. unknown) features for a new
value X forms the input space (resp. output space):
the projections of X onto the input (resp. output)
space will be denoted I(X) (resp. O(X)). This set-
ting is more general than the simpler classification
task, in which only one feature (the class label) is
unknown, and covers many other interesting tasks.
The inference procedure can be sketched as fol-
lows: training examples are simply stored for fu-
ture use; no generalization (abstraction) of the data
is performed, which is characteristic of lazy learning
(Aha, 1997). Given a new instance X, we identify
formal analogical proportions involving X in the in-
put space; known objects involved in these propor-
tions are then used to infer the missing features.
An analogical proportion is a relation involv-
ing four objects A, B, C and D, denoted by
A : B :: C : D and which reads A is to B as C is
to D. The definition and computation of these pro-
portions are studied in Section 3. For the moment,
</bodyText>
<page confidence="0.996819">
121
</page>
<bodyText confidence="0.999418">
we contend that it is possible to construct analogical
proportions between (possibly partially informed)
objects in S. Let I(X) be a partially described ob-
ject not seen during training. The analogical infer-
ence process is formalized as:
</bodyText>
<listItem confidence="0.989816">
1. Construct the set T (X) C S3 defined as:
</listItem>
<equation confidence="0.9993895">
T (X) = {(A, B, C) E S3 �
I(A) : I(B) :: I(C) : I(X)}
</equation>
<listItem confidence="0.468549">
2. For each (A, B, C) E T (X), compute hy-
potheses O(X) by solving the equation:
</listItem>
<equation confidence="0.992703">
O(X) = O(A) : O(B) :: O(C) :?
</equation>
<bodyText confidence="0.999958692307692">
This inference procedure shows lots of similari-
ties with the k-nearest neighbors classifier (k-NN)
which, given a new instance, (i) searches the training
set for close neighbors, (ii) compute the unknown
class label according to the neighbors’ labels. Our
model, however, does not use any metric between
objects: we only rely on the definition of analogical
proportions, which reveal systemic, rather than su-
perficial, similarities. Moreover, inputs and outputs
are regarded in a symmetrical way: outputs are not
restricted to a set of labels, and can also be structured
objects such as sequences. The implementation of
the model still has to address two specific issues.
</bodyText>
<listItem confidence="0.999799333333333">
• When exploring S3, an exhaustive search eval-
uates XS13 triples, which can prove to be in-
tractable. Moreover, objects in S may be
unequally relevant, and we might expect the
search procedure to treat them accordingly.
• Whenever several competing hypotheses are
</listItem>
<bodyText confidence="0.998666916666667">
proposed for O(X), a ranking must be per-
formed. In our current implementation, hy-
potheses are ranked based on frequency counts.
These issues are well-known problems for k-NN
classifiers. The second one does not appear to be
critical and is usually solved based on a majority
rule. In contrast, a considerable amount of effort has
been devoted to reduce and optimize the search pro-
cess, via editing and condensing methods, as stud-
ied e.g. in (Dasarathy, 1990; Wilson and Martinez,
2000). Proposals for solving this problem are dis-
cussed in Section 3.4.
</bodyText>
<sectionHeader confidence="0.5289225" genericHeader="method">
3 An algebraic framework for analogical
proportions
</sectionHeader>
<bodyText confidence="0.9999883">
Our inductive model requires the availability of a de-
vice for computing analogical proportions on feature
vectors. We consider that an analogical proportion
holds between four feature vectors when the propor-
tion holds for all components. In this section, we
propose a unified algebraic framework for defining
analogical proportions between individual features.
After giving the general definition, we present its in-
stantiation for two types of features: words over a
finite alphabet and sets of labelled trees.
</bodyText>
<subsectionHeader confidence="0.999597">
3.1 Analogical proportions
</subsectionHeader>
<bodyText confidence="0.999344181818182">
Our starting point will be analogical proportions in
a set U, which we define as follows: Vx, y, z, t E
U, x : y :: z : t if and only if either x = y and z = t
or x = z and y = t. In the sequel, we assume that
U is additionally provided with an associative inter-
nal composition law ®, which makes (U, ®) a semi-
group. The generalization of proportions to semi-
groups involves two key ideas: the decomposition of
objects into smaller parts, subject to alternation con-
straints. To formalize the idea of decomposition, we
define the factorization of an element u in U as:
</bodyText>
<construct confidence="0.383587">
Definition 1 (Factorization)
A factorization of u E U is a sequence u1 ... un,
with Vi, ui E U, such that: u1 ® ... ® un = u.
Each term ui is a factor of u.
</construct>
<bodyText confidence="0.9228555">
The alternation constraint expresses the fact that
analogically related objects should be made of alter-
nating factors: for x : y :: z : t to hold, each factor
in x should be found alternatively in y and in z. This
yields a first definition of analogical proportions:
Definition 2 (Analogical proportion)
</bodyText>
<construct confidence="0.867683571428571">
(x, y, z, t) E U form an analogical proportion, de-
noted by x : y :: z : t if and only if there exists some
factorizations x1® ... ®xd = x, y1® ... ®yd = y,
z1 ® ... ® zd = z, t1 ® ... ® td = t such that
Vi, (yi, zi) E {(xi, ti), (ti, xi)1. The smallest d for
which such factorizations exist is termed the degree
of the analogical proportion.
</construct>
<bodyText confidence="0.99969125">
This definition is valid for any semigroup, and a
fortiori for any richer algebraic structure. Thus, it
readily applies to the case of groups, vector spaces,
free monoids, sets and attribute-value structures.
</bodyText>
<page confidence="0.898296">
122
</page>
<equation confidence="0.99870175">
w(1) : ε w(k) : ε
0 1 k
ε : w(1)
ε : w(k)
</equation>
<subsectionHeader confidence="0.9864905">
3.2 Words over Finite Alphabets
3.2.1 Analogical Proportions between Words
</subsectionHeader>
<bodyText confidence="0.982123857142857">
Let E be a finite alphabet. E* denotes the set of
finite sequences of elements of E, called words over
E. E*, provided with the concatenation operation .
is a free monoid whose identity element is the empty
word E. For w ∈ E*, w(i) denotes the ith symbol in
w. In this context, definition (2) can be re-stated as:
Definition 3 (Analogical proportion in (E*,.))
</bodyText>
<equation confidence="0.4271878">
(x, y, z, t) ∈ E* form an analogical proportion, de-
noted by x : y :: z : t if and only if there exists some
integer d and some factorizations x1 ... xd = x,
y1 ... yd = y, z1 ... zd = z, t1 ... td = t such that
∀i, (yi, zi) ∈ {(xi, ti), (ti, xi)}.
</equation>
<bodyText confidence="0.916033692307692">
An example of analogy between words is:
viewing: reviewer:: searching: researcher
with x1 = E, x2 = view, x3 = ing and t1 = re,
t2 = search, t3 = er. This definition generalizes
the proposal of (Lepage, 1998). It does not ensure
the existence of a solution to an analogical equation,
nor its uniqueness when it exists. (Lepage, 1998)
gives a set of necessary conditions for a solution to
exist. These conditions also apply here. In particu-
lar, if t is a solution of x : y :: z :?, then t contains,
in the same relative order, all the symbols in y and z
that are not in x. As a consequence, all solutions of
an equation have the same length.
</bodyText>
<subsectionHeader confidence="0.901666">
3.2.2 A Finite-state Solver
</subsectionHeader>
<bodyText confidence="0.999778352941177">
Definition (3) yields an efficient procedure for
solving analogical equations, based on finite-state
transducers. The main steps of the procedure are
sketched here. A full description can be found in
(Yvon, 2003). To start with, let us introduce the no-
tions of complementary set and shuffle product.
Complementary set If v is a subword of w, the
complementary set of v with respect to w, denoted
by w\v is the set of subwords of w obtained by re-
moving from w, in a left-to-right fashion, the sym-
bols in v. For example, eea is a complementary sub-
word of xmplr with respect to exemplar. When v is
not a subword of w, w\v is empty. This notion can
be generalized to any regular language.
The complementary set of v with respect to w is
a regular set: it is the output language of the finite-
state transducer Tw (see Figure 1) for the input v.
</bodyText>
<figureCaption confidence="0.8565855">
Figure 1: The transducer Tw computing comple-
mentary sets wrt w.
</figureCaption>
<bodyText confidence="0.962776">
Shuffle The shuffle u • v of two words u and v is
introduced e.g. in (Sakarovitch, 2003) as follows:
</bodyText>
<equation confidence="0.995351">
u • v = {u1v1u2v2 ... unvn, st. ui, vi ∈ E*,
u1 ... un = u,v1 ... vn = v}
</equation>
<bodyText confidence="0.9958563">
The shuffle of two words u and v contains all the
words w which can be composed using all the sym-
bols in u and v, subject to the condition that if a
precedes b in u (or in v), then it precedes b in w.
Taking, for instance, u = abc and v = def, the
words abcdef, abdefc, adbecf are in u • v; this
is not the case with abefcd. This operation gen-
eralizes straightforwardly to languages. The shuf-
fle of two regular languages is regular (Sakarovitch,
2003); the automaton A, computing K•L, is derived
</bodyText>
<construct confidence="0.984952857142857">
from the automata AK = (E, QK, qK, FK, SK) and
AL = (E, QL, qL, FL, SL) recognizing respectively
K and L as the product automata A = (E, QK ×
QL, (qK, q°,), FK × FL, S), where S is defined as:
S((qK, qL), a) = (rK, rL) if and only if either
SK(qK, a) = rK and qL = rL or SL(qL, a) = rL
and qK = rK.
</construct>
<bodyText confidence="0.999619333333333">
The notions of complementary set and shuffle are
related through the following property, which is a
direct consequence of the definitions.
</bodyText>
<equation confidence="0.987663">
w ∈ u • v ⇔ u ∈ w\v
</equation>
<bodyText confidence="0.9997395">
Solving analogical equations The notions of
shuffle and complementary sets yield another
characterization of analogical proportion between
words, based on the following proposition:
</bodyText>
<sectionHeader confidence="0.283081" genericHeader="method">
Proposition 1.
</sectionHeader>
<bodyText confidence="0.9002475">
∀x,y,z,t ∈ E*,x : y :: z : t ⇔ x • t ∩ y • z =6 ∅
An analogical proportion is thus established if the
symbols in x and t are also found in y and z, and ap-
pear in the same relative order. A corollary follows:
</bodyText>
<page confidence="0.989444">
123
</page>
<figure confidence="0.995954631578947">
Proposition 2.
t is a solution of x : y :: z :? #&gt; t E (y • z)\x
S
VP
NP
NP
S
VP
NP
PP
his car
AUX
by NP
VP
:
his car
have
been VP
the police
</figure>
<bodyText confidence="0.999485">
The set of solutions of an analogical equation
x : y :: z :? is a regular set, which can be computed
with a finite-state transducer. It can also be shown
that this analogical solver generalizes the approach
based on edit distance proposed in (Lepage, 1998).
</bodyText>
<subsectionHeader confidence="0.991819">
3.3 Trees
</subsectionHeader>
<bodyText confidence="0.948806096774193">
Labelled trees are very common structures in NLP
tasks: they can represent syntactic structures, or
terms in a logical representation of a sentence. To
express the definition of analogical proportion be-
tween trees, we introduce the notion of substitution.
Definition 4 (Substitution)
A (single) substitution is a pair (variable +— tree).
The application of the substitution (v +— t&apos;) to a tree
t consists in replacing each leaf of t labelled by v by
the tree t&apos;. The result of this operation is denoted:
t(v +— t&apos;). For each variable v, we define the binary
operator 4v as t 4v t&apos; = t (v +— t&apos;).
Definition 2 can then be extended as:
Definition 5 (Analogical proportion (trees))
(x, y, z, t) E U form an analogical propor-
tion, denoted by x : y :: z : t iff there exists some
variables (v1, ... , vn_1) and some factorizations
x1 4v1 ... 4vn−1 xn = x, y1 4v1 ... 4vn−1 yn = y,
z1 4v1 ... 4vn−1 zn = z, t1 4v1 ... 4vn−1 tn = t such
that Vi, (yi, zi) E {(xi, ti), (ti, xi)}.
An example of such a proportion is illustrated on
Figure 2 with syntactic parse trees.
This definition yields an effective algorithm
computing analogical proportions between trees
(Stroppa and Yvon, 2005). We consider here a sim-
pler heuristic approach, consisting in (i) linearizing
labelled trees into parenthesized sequences of sym-
bols and (ii) using the analogical solver for words
introduced above. This approach yields a faster, al-
beit approximative algorithm, which makes analogi-
cal inference tractable even for large tree databases.
</bodyText>
<subsectionHeader confidence="0.922491">
3.4 Algorithmic issues
</subsectionHeader>
<bodyText confidence="0.9996515">
We have seen how to compute analogical relation-
ships for features whose values are words and trees.
</bodyText>
<figure confidence="0.90079975">
the police have VP
impounded
impounded
eaten
</figure>
<figureCaption confidence="0.999665">
Figure 2: Analogical proportion between trees.
</figureCaption>
<bodyText confidence="0.999822333333333">
If we use, for trees, the solver based on tree lin-
earizations, the resolution of an equation amounts,
in both cases, to solving analogies on words.
The learning algorithm introduced in Section 2.2
is a two-step procedure: a search step and a trans-
fer step. The latter step only involves the resolu-
tion of (a restricted number of) analogical equations.
When x, y and z are known, solving x : y :: z :?
amounts to computing the output language of the
transducer representing (y • z)\x: the automaton
for this language has a number of states bounded by
|x|x|y |x|z |. Given the typical length of words in
our experiments, and given that the worst-case ex-
ponential bound for determinizing this automaton is
hardly met, the solving procedure is quite efficient.
The problem faced during the search procedure
is more challenging: given x, we need to retrieve
all possible triples (y, z, t) in a finite set L such
that x : y :: z : t. An exhaustive search requires
the computation of the intersection of the finite-
state automaton representing the output language of
(L • L)\x with the automaton for L. Given the size
of L in our experiments (several hundreds of thou-
sands of words), a complete search is intractable and
we resort to the following heuristic approach.
L is first split into K bins {L1, ..., LK}, with |Li |
small with respect to |L|. We then randomly select
k bins and compute, for each bin Li, the output lan-
guage of (Li • Li)\x, which is then intersected with
L: we thus only consider triples containing at least
</bodyText>
<figure confidence="0.999786380952381">
S
S
NP
VP
:
AUX
the cat
been VP
has
PP
by NP
the mouse
VP
::
NP
the mouse
VP
has VP
eaten
NP
the cat
</figure>
<page confidence="0.995286">
124
</page>
<bodyText confidence="0.999944833333333">
two words from the same bin. It has to be noted that
the bins are not randomly constructed: training ex-
amples are grouped into inflectional or derivational
families. To further speed up the search, we also im-
pose an upper bound on the degree of proportions.
All triples retrieved during these k partial searches
are then merged and considered for the transfer step.
The computation of analogical relationships has
been implemented in a generic analogical solver;
this solver is based on Vaucanson, an automata ma-
nipulation library using high performance generic
programming (Lombardy et al., 2003).
</bodyText>
<sectionHeader confidence="0.999184" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.948415">
4.1 Methodology
</subsectionHeader>
<bodyText confidence="0.999985275">
The main purpose of these experiments is to demon-
strate the flexibility of the analogical learner. We
considered two different supervised learning tasks,
both aimed at performing the lexical analysis of iso-
lated word forms. Each of these tasks represents a
possible instantiation of the learning procedure in-
troduced in Section 2.2.
The first experiment consists in computing one
or several vector(s) of morphosyntactic features to
be associated with a form. Each vector comprises
the lemma, the part-of-speech, and, based on the
part-of-speech, additional features such as number,
gender, case, tense, mood, etc. An (English) in-
put/output pair for this tasks thus looks like: in-
put=replying; output={reply; V-pp--J, where the
placeholder ’-’ denotes irrelevant features. Lexi-
cal analysis is useful for many applications: a POS
tagger, for instance, needs to “guess” the possi-
ble part(s)-of-speech of unknown words (Mikheev,
1997). For this task, we use the definition of analog-
ical proportions for “flat” feature vectors (see sec-
tion 3.1) and for word strings (section 3.2). The
training data is a list of fully informed lexical en-
tries; the test data is a list of isolated word forms
not represented in the lexicon. Bins are constructed
based on inflectional families.
The second experiment consists in computing a
morphological parse of unknown lemmas: for each
input lemma, the output of the system is one or sev-
eral parse trees representing a possible hierarchical
decomposition of the input into (morphologically
categorized) morphemes (see Figure 3). This kind
of analysis makes it possible to reconstruct the series
of morphological operations deriving a lemma, to
compute its root, its part-of-speech, and to identify
morpheme boundaries. This information is required,
for instance, to compute the pronunciation of an un-
known word; or to infer the compositional meaning
of a complex (derived or compound) lemma. Bins
gather entries sharing a common root.
</bodyText>
<figure confidence="0.834685428571428">
input=acrobatically; output =
B
✟✟✟ ❍ ❍ ❍
A BSA.
✟✟❍❍
N ASN. ally
acrobat ic
</figure>
<figureCaption confidence="0.914988">
Figure 3: Input/output pair for task 2. Bound mor-
phemes have a compositional type: BSA. denotes a
suffix that turns adjectives into adverbs.
</figureCaption>
<bodyText confidence="0.999908615384615">
These experiments use the English, German, and
Dutch morphological tables of the CELEX database
(Burnage, 1990). For task 1, these tables contain
respectively 89 000, 342 000 and 324 000 different
word forms, and the number of features to predict is
respectively 6, 12, and 10. For task 2, which was
only conducted with English lemma, the total num-
ber of different entries is 48 407.
For each experiment, we perform 10 runs, using
1 000 randomly selected entries for testing1. Gen-
eralization performance is measured as follows: the
system’s output is compared with the reference val-
ues (due to lexical ambiguity, a form may be asso-
ciated in the database with several feature vectors
or parse trees). Per instance precision is computed
as the relative number of correct hypotheses, i.e.
hypotheses which exactly match the reference: for
task 1, all features have to be correct; for task 2, the
parse tree has to be identical to the reference tree.
Per instance recall is the relative number of refer-
ence values that were actually hypothesized. Preci-
sion and recall are averaged over the test set; num-
bers reported below are averaged over the 10 runs.
Various parameters affect the performance: k, the
number of randomly selected bins considered during
the search step (see Section 3.4) and d, the upper
</bodyText>
<footnote confidence="0.780481">
1Due to lexical ambiguity, the number of tested instances is
usually greater than 1 000.
</footnote>
<page confidence="0.996289">
125
</page>
<bodyText confidence="0.941532">
bound of the degree of extracted proportions.
</bodyText>
<subsectionHeader confidence="0.921513">
4.2 Experimental results
</subsectionHeader>
<bodyText confidence="0.9970205">
Experimental results for task 1 are given in Tables 1,
2 and 3. For each main category, two recall and pre-
cision scores are computed: one for the sole lemma
and POS attributes (left column); and one for the
lemma and all the morpho-syntactic features (on the
right). In these experiments, parameters are set as
follows: k = 150 and d = 3. As k grows, both recall
and precision increase (up to a limit); k = 150 ap-
pears to be a reasonable trade-off between efficiency
and accuracy. A further increase of d does not sig-
nificantly improve accuracy: taking d = 3 or d = 4
yields very comparable results.
</bodyText>
<table confidence="0.9984264">
Lemma + POS Lemma + Features
Rec. Prec. Rec. Prec.
Nouns 76.66 94.64 75.26 95.37
Verbs 94.83 97.14 94.79 97.37
Adjectives 26.68 72.24 27.89 87.67
</table>
<tableCaption confidence="0.999216">
Table 1: Results on task 1 for English
</tableCaption>
<table confidence="0.9996624">
Lemma + POS Lemma + Features
Rec. Prec. Rec. Prec.
Nouns 71.39 92.17 54.59 74.75
Verbs 96.75 97.85 93.26 94.36
Adjectives 91.59 96.09 90.02 95.33
</table>
<tableCaption confidence="0.995337">
Table 2: Results on task 1 for Dutch
</tableCaption>
<table confidence="0.9997392">
Lemma + POS Lemma + Features
Rec. Prec. Rec. Prec.
Nouns 93.51 98.28 77.32 81.70
Verbs 99.55 99.69 90.50 90.63
Adjectives 99.14 99.28 99.01 99.15
</table>
<tableCaption confidence="0.999949">
Table 3: Results on task 1 for German
</tableCaption>
<bodyText confidence="0.999176333333333">
As a general comment, one can note that high
generalization performance is achieved for lan-
guages and categories involving rich inflectional
paradigms: this is exemplified by the performance
on all German categories. English adjectives, at
the other end of this spectrum, are very difficult to
analyze. A simple and effective workaround for
this problem consists in increasing the size the sub-
lexicons (LZ in Section 3.4) so as to incorporate in a
given bin all the members of the same derivational
(rather than inflectional) family. For Dutch, these
results are comparable with the results reported in
(van den Bosch and Daelemans, 1999), who report
an accuracy of about 92% on the task of predicting
the main syntactic category.
</bodyText>
<table confidence="0.989768666666667">
Rec. Prec.
Morphologically Complex 46.71 70.92
Others 17.00 46.86
</table>
<tableCaption confidence="0.999865">
Table 4: Results on task 2 for English
</tableCaption>
<bodyText confidence="0.999936952380952">
The second task is more challenging since the ex-
act parse tree of a lemma must be computed. For
morphologically complex lemmas (involving affixa-
tion or compounding), it is nevertheless possible to
obtain acceptable results (see Table 4, showing that
some derivational phenomena have been captured.
Further analysis is required to assess more precisely
the potential of this method.
From a theoretical perspective, it is important to
realize that our model does not commit us to a
morpheme-based approach of morphological pro-
cesses. This is obvious in task 1; and even if
task 2 aims at predicting a morphematic parse of in-
put lemmas, this goal is achieved without segment-
ing the input lemma into smaller units. For in-
stance, our learner parses the lemma enigmatically
as: [[[.N enigma][.A|N ical]]BIA. ly], that is with-
out trying to decide to which morph the orthographic
t should belong. In this model, input and output
spaces are treated symmetrically and correspond to
distinct levels of representation.
</bodyText>
<sectionHeader confidence="0.991432" genericHeader="conclusions">
5 Discussion and future work
</sectionHeader>
<bodyText confidence="0.9979135">
In this paper, we have presented a generic analog-
ical inference procedure, which applies to a wide
range of actual learning tasks, and we have detailed
its instantiation for common feature types. Prelimi-
nary experiments have been conducted on two mor-
phological analysis tasks and have shown promising
generalization performance.
These results suggest that our main hypotheses
are valid: (i) searching for triples is tractable even
with databases containing several hundred of thou-
sands instances; (ii) formal analogical proportions
are a reliable sign of deeper analogies between lin-
</bodyText>
<page confidence="0.995597">
126
</page>
<bodyText confidence="0.99978495">
guistic entities; they can thus be used to devise flex-
ible and effective learners for NLP tasks.
This work is currently being developed in various
directions: first, we are gathering additional experi-
mental results on several NLP tasks, to get a deeper
understanding of the generalization capabilities of
our analogical learner. One interesting issue, not
addressed in this paper, is the integration of vari-
ous forms of linguistic knowledge in the definition
of analogical proportions, or in the specification of
the search procedure. We are also considering al-
ternative heuristic search procedures, which could
improve or complement the approaches presented in
this paper. A possible extension would be to define
and take advantage of non-uniform distributions of
training instances, which could be used both during
the searching and ranking steps. We finally believe
that this approach might also prove useful in other
application domains involving structured data and
are willing to experiment with other kinds of data.
</bodyText>
<sectionHeader confidence="0.999116" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999963591549296">
David W. Aha. 1997. Editorial. Artificial Intelligence
Review, 11(1-5):7–10. Special Issue on Lazy Learn-
ing.
Gavin Burnage. 1990. CELEX: a guide for users. Tech-
nical report, University of Nijmegen, Center for Lexi-
cal Information, Nijmegen.
Walter Daelemans, Antal Van Den Bosch, and Jakub Za-
vrel. 1999. Forgetting exceptions is harmful in lan-
guage learning. Machine Learning, 34(1–3):11–41.
B.V. Dasarathy, editor. 1990. Nearest neighbor (NN)
Norms: NN Pattern Classification Techniques. IEEE
Computer Society Press, Los Alamitos, CA.
Brian Falkenheimer and Dedre Gentner. 1986. The
structure-mapping engine. In Proceedings of the meet-
ing of the American Association for Artificial Intelli-
gence (AAAI), pages 272–277.
Dedre Gentner, Keith J. Holyoak, and Boicho N.
Konikov, editors. 2001. The Analogical Mind. The
MIT Press, Cambridge, MA.
Douglas Hofstadter and the Fluid Analogies Research
group, editors. 1995. Fluid Concepts and Creative
Analogies. Basic Books.
Keith J. Holyoak and John E. Hummel. 2001. Under-
standing analogy within a biological symbol system.
In Dedre Gentner, Keith J. Holyoak, and Boicho N.
Konikov, editors, The analogical mind, pages 161–
195. The MIT Press, Cambridge, MA.
Yves Lepage. 1998. Solving analogies on words: An
algorithm. In Proceedings of COLING-ACL ’98, vol-
ume 2, pages 728–735, Montr´eal, Canada.
Yves Lepage. 1999a. Analogy+tables=conjugation.
In G. Friedl and H.G. Mayr, editors, Proceedings of
NLDB ’99, pages 197–201, Klagenfurt, Germany.
Yves Lepage. 1999b. Open set experiments with direct
analysis by analogy. In Proceedings of NLPRS ’99,
volume 2, pages 363–368, Beijing, China.
Sylvain Lombardy, Rapha¨el Poss, Yann R´egis-Gianas,
and Jacques Sakarovitch. 2003. Introducing Vaucan-
son. In Proceedings of CIAA 2003, pages 96–107.
Andrei Mikheev. 1997. Automatic rule induction for
unknown word guessing. Computational Linguistics,
23(3):405–423.
Vito Pirrelli and Franc¸ois Yvon. 1999. Analogy in the
lexicon: a probe into analogy-based machine learning
of language. In Proceedings of the 6th International
Symposium on Human Communication, Santiago de
Cuba, Cuba.
Tony A. Plate. 2000. Analogy retrieval and processing
with distributed vector representations. Expert sys-
tems, 17(1):29–40.
Jacques Sakarovitch. 2003. El´ements de th´eorie des au-
tomates. Vuibert, Paris.
Royal Skousen. 1989. Analogical Modeling of Lan-
guage. Kluwer, Dordrecht.
Nicolas Stroppa and Franc¸ois Yvon. 2005. Formal
models of analogical relationships. Technical report,
ENST, Paris, France.
Paul Thagard, Keith J. Holoyak, Greg Nelson, and David
Gochfeld. 1990. Analog retrieval by constraint satis-
faction. Artificial Intelligence, 46(3):259–310.
Antal van den Bosch and Walter Daelemans. 1999.
Memory-based morphological processing. In Pro-
ceedings ofACL, pages 285–292, Maryland.
D. Randall Wilson and Tony R. Martinez. 2000. Reduc-
tion techniques for instance-based learning algorithms.
Machine Learning, 38(3):257–286.
Franc¸ois Yvon. 1999. Pronouncing unknown words us-
ing multi-dimensional analogies. In Proc. Eurospeech,
volume 1, pages 199–202, Budapest, Hungary.
Franc¸ois Yvon. 2003. Finite-state machines solving
analogies on words. Technical report, ENST.
</reference>
<page confidence="0.997333">
127
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.600572">
<title confidence="0.999063">An Analogical Learner for Morphological Analysis</title>
<author confidence="0.925176">Stroppa</author>
<author confidence="0.925176">Yvon</author>
<affiliation confidence="0.656753">GET/ENST &amp; LTCI, UMR 5141</affiliation>
<address confidence="0.835618">46 rue Barrault, 75013 Paris, France</address>
<abstract confidence="0.999142217391304">Analogical learning is based on a twostep inference process: (i) computation of a structural mapping between a new and a memorized situation; (ii) transfer of knowledge from the known to the unknown situation. This approach requires the ability to search for and exploit such mappings, hence the need to properly define analogical relationships, and to efficiently implement their computation. In this paper, we propose a unified definition for the notion of (formal) analogical proportion, which applies to a wide range of algebraic structures. We show that this definition is suitable for learning in domains involving large databases of structured data, as is especially the case in Natural Language Processing (NLP). We then present experimental results obtained on two morphological analysis tasks which demonstrate the flexibility and accuracy of this approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David W Aha</author>
</authors>
<date>1997</date>
<journal>Editorial. Artificial Intelligence Review,</journal>
<pages>11--1</pages>
<note>Special Issue on Lazy Learning.</note>
<contexts>
<context position="7373" citStr="Aha, 1997" startWordPosition="1132" endWordPosition="1133">tances. Put in more standard terms, the set of known (resp. unknown) features for a new value X forms the input space (resp. output space): the projections of X onto the input (resp. output) space will be denoted I(X) (resp. O(X)). This setting is more general than the simpler classification task, in which only one feature (the class label) is unknown, and covers many other interesting tasks. The inference procedure can be sketched as follows: training examples are simply stored for future use; no generalization (abstraction) of the data is performed, which is characteristic of lazy learning (Aha, 1997). Given a new instance X, we identify formal analogical proportions involving X in the input space; known objects involved in these proportions are then used to infer the missing features. An analogical proportion is a relation involving four objects A, B, C and D, denoted by A : B :: C : D and which reads A is to B as C is to D. The definition and computation of these proportions are studied in Section 3. For the moment, 121 we contend that it is possible to construct analogical proportions between (possibly partially informed) objects in S. Let I(X) be a partially described object not seen d</context>
</contexts>
<marker>Aha, 1997</marker>
<rawString>David W. Aha. 1997. Editorial. Artificial Intelligence Review, 11(1-5):7–10. Special Issue on Lazy Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gavin Burnage</author>
</authors>
<title>CELEX: a guide for users.</title>
<date>1990</date>
<tech>Technical report,</tech>
<institution>University of Nijmegen, Center for Lexical Information,</institution>
<location>Nijmegen.</location>
<contexts>
<context position="22431" citStr="Burnage, 1990" startWordPosition="3863" endWordPosition="3864">e its root, its part-of-speech, and to identify morpheme boundaries. This information is required, for instance, to compute the pronunciation of an unknown word; or to infer the compositional meaning of a complex (derived or compound) lemma. Bins gather entries sharing a common root. input=acrobatically; output = B ✟✟✟ ❍ ❍ ❍ A BSA. ✟✟❍❍ N ASN. ally acrobat ic Figure 3: Input/output pair for task 2. Bound morphemes have a compositional type: BSA. denotes a suffix that turns adjectives into adverbs. These experiments use the English, German, and Dutch morphological tables of the CELEX database (Burnage, 1990). For task 1, these tables contain respectively 89 000, 342 000 and 324 000 different word forms, and the number of features to predict is respectively 6, 12, and 10. For task 2, which was only conducted with English lemma, the total number of different entries is 48 407. For each experiment, we perform 10 runs, using 1 000 randomly selected entries for testing1. Generalization performance is measured as follows: the system’s output is compared with the reference values (due to lexical ambiguity, a form may be associated in the database with several feature vectors or parse trees). Per instanc</context>
</contexts>
<marker>Burnage, 1990</marker>
<rawString>Gavin Burnage. 1990. CELEX: a guide for users. Technical report, University of Nijmegen, Center for Lexical Information, Nijmegen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Antal Van Den Bosch</author>
<author>Jakub Zavrel</author>
</authors>
<title>Forgetting exceptions is harmful in language learning.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<marker>Daelemans, Van Den Bosch, Zavrel, 1999</marker>
<rawString>Walter Daelemans, Antal Van Den Bosch, and Jakub Zavrel. 1999. Forgetting exceptions is harmful in language learning. Machine Learning, 34(1–3):11–41.</rawString>
</citation>
<citation valid="false">
<date>1990</date>
<editor>B.V. Dasarathy, editor.</editor>
<publisher>IEEE Computer Society Press,</publisher>
<location>Los Alamitos, CA.</location>
<marker>1990</marker>
<rawString>B.V. Dasarathy, editor. 1990. Nearest neighbor (NN) Norms: NN Pattern Classification Techniques. IEEE Computer Society Press, Los Alamitos, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Falkenheimer</author>
<author>Dedre Gentner</author>
</authors>
<title>The structure-mapping engine.</title>
<date>1986</date>
<booktitle>In Proceedings of the meeting of the American Association for Artificial Intelligence (AAAI),</booktitle>
<pages>272--277</pages>
<contexts>
<context position="4822" citStr="Falkenheimer and Gentner, 1986" startWordPosition="727" endWordPosition="731">Section 4 reports the results of experiments aimed at demonstrating the flexibility of this model and at assessing its generalization performance. We conclude by discussing current limitations of this model and by suggesting possible extensions. 2 Principles of analogical learning 2.1 Analogical reasoning The ability to identify analogical relationships between what looks like unrelated situations, and to use these relationships to solve complex problems, lies at the core of human cognition (Gentner et al., 2001). A number of models of this ability have been proposed, based on symbolic (e.g. (Falkenheimer and Gentner, 1986; Thagard et al., 1990; Hofstadter and the Fluid Analogies Research group, 1995)) or subsymbolic (e.g. (Plate, 2000; Holyoak and Hummel, 2001)) approaches. The main focus of these models is the dynamic process of analogy making, which involves the identification of a structural mappings between a memorized and a new situation. Structural mapping relates situations which, while being apparently very different, share a set of common high-level relationships. The building of a structural mapping between two situations utilizes several subparts of their descriptions and the relationships between t</context>
</contexts>
<marker>Falkenheimer, Gentner, 1986</marker>
<rawString>Brian Falkenheimer and Dedre Gentner. 1986. The structure-mapping engine. In Proceedings of the meeting of the American Association for Artificial Intelligence (AAAI), pages 272–277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dedre Gentner</author>
<author>Keith J Holyoak</author>
</authors>
<title>The Analogical Mind.</title>
<date>2001</date>
<editor>and Boicho N. Konikov, editors.</editor>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Gentner, Holyoak, 2001</marker>
<rawString>Dedre Gentner, Keith J. Holyoak, and Boicho N. Konikov, editors. 2001. The Analogical Mind. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Hofstadter</author>
</authors>
<title>and the Fluid Analogies Research group, editors.</title>
<date>1995</date>
<publisher>Basic Books.</publisher>
<marker>Hofstadter, 1995</marker>
<rawString>Douglas Hofstadter and the Fluid Analogies Research group, editors. 1995. Fluid Concepts and Creative Analogies. Basic Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith J Holyoak</author>
<author>John E Hummel</author>
</authors>
<title>Understanding analogy within a biological symbol system.</title>
<date>2001</date>
<booktitle>In Dedre Gentner,</booktitle>
<location>Keith</location>
<contexts>
<context position="4964" citStr="Holyoak and Hummel, 2001" startWordPosition="749" endWordPosition="752">We conclude by discussing current limitations of this model and by suggesting possible extensions. 2 Principles of analogical learning 2.1 Analogical reasoning The ability to identify analogical relationships between what looks like unrelated situations, and to use these relationships to solve complex problems, lies at the core of human cognition (Gentner et al., 2001). A number of models of this ability have been proposed, based on symbolic (e.g. (Falkenheimer and Gentner, 1986; Thagard et al., 1990; Hofstadter and the Fluid Analogies Research group, 1995)) or subsymbolic (e.g. (Plate, 2000; Holyoak and Hummel, 2001)) approaches. The main focus of these models is the dynamic process of analogy making, which involves the identification of a structural mappings between a memorized and a new situation. Structural mapping relates situations which, while being apparently very different, share a set of common high-level relationships. The building of a structural mapping between two situations utilizes several subparts of their descriptions and the relationships between them. Analogy-making seems to play a central role in our reasoning ability; it is also invoked to explain some human skills which do not involv</context>
</contexts>
<marker>Holyoak, Hummel, 2001</marker>
<rawString>Keith J. Holyoak and John E. Hummel. 2001. Understanding analogy within a biological symbol system. In Dedre Gentner, Keith J. Holyoak, and Boicho N.</rawString>
</citation>
<citation valid="false">
<title>The analogical mind,</title>
<pages>161--195</pages>
<editor>Konikov, editors,</editor>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker></marker>
<rawString>Konikov, editors, The analogical mind, pages 161– 195. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Lepage</author>
</authors>
<title>Solving analogies on words: An algorithm.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL ’98,</booktitle>
<volume>2</volume>
<pages>728--735</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="12842" citStr="Lepage, 1998" startWordPosition="2138" endWordPosition="2139">ty word E. For w ∈ E*, w(i) denotes the ith symbol in w. In this context, definition (2) can be re-stated as: Definition 3 (Analogical proportion in (E*,.)) (x, y, z, t) ∈ E* form an analogical proportion, denoted by x : y :: z : t if and only if there exists some integer d and some factorizations x1 ... xd = x, y1 ... yd = y, z1 ... zd = z, t1 ... td = t such that ∀i, (yi, zi) ∈ {(xi, ti), (ti, xi)}. An example of analogy between words is: viewing: reviewer:: searching: researcher with x1 = E, x2 = view, x3 = ing and t1 = re, t2 = search, t3 = er. This definition generalizes the proposal of (Lepage, 1998). It does not ensure the existence of a solution to an analogical equation, nor its uniqueness when it exists. (Lepage, 1998) gives a set of necessary conditions for a solution to exist. These conditions also apply here. In particular, if t is a solution of x : y :: z :?, then t contains, in the same relative order, all the symbols in y and z that are not in x. As a consequence, all solutions of an equation have the same length. 3.2.2 A Finite-state Solver Definition (3) yields an efficient procedure for solving analogical equations, based on finite-state transducers. The main steps of the pro</context>
<context position="16142" citStr="Lepage, 1998" startWordPosition="2796" endWordPosition="2797">1. ∀x,y,z,t ∈ E*,x : y :: z : t ⇔ x • t ∩ y • z =6 ∅ An analogical proportion is thus established if the symbols in x and t are also found in y and z, and appear in the same relative order. A corollary follows: 123 Proposition 2. t is a solution of x : y :: z :? #&gt; t E (y • z)\x S VP NP NP S VP NP PP his car AUX by NP VP : his car have been VP the police The set of solutions of an analogical equation x : y :: z :? is a regular set, which can be computed with a finite-state transducer. It can also be shown that this analogical solver generalizes the approach based on edit distance proposed in (Lepage, 1998). 3.3 Trees Labelled trees are very common structures in NLP tasks: they can represent syntactic structures, or terms in a logical representation of a sentence. To express the definition of analogical proportion between trees, we introduce the notion of substitution. Definition 4 (Substitution) A (single) substitution is a pair (variable +— tree). The application of the substitution (v +— t&apos;) to a tree t consists in replacing each leaf of t labelled by v by the tree t&apos;. The result of this operation is denoted: t(v +— t&apos;). For each variable v, we define the binary operator 4v as t 4v t&apos; = t (v </context>
</contexts>
<marker>Lepage, 1998</marker>
<rawString>Yves Lepage. 1998. Solving analogies on words: An algorithm. In Proceedings of COLING-ACL ’98, volume 2, pages 728–735, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Lepage</author>
</authors>
<date>1999</date>
<booktitle>Proceedings of NLDB ’99,</booktitle>
<pages>197--201</pages>
<editor>Analogy+tables=conjugation. In G. Friedl and H.G. Mayr, editors,</editor>
<location>Klagenfurt, Germany.</location>
<contexts>
<context position="2945" citStr="Lepage, 1999" startWordPosition="449" endWordPosition="450"> advantage of other specificities of linguistic data: (i) their systemic organization in (pseudo)-paradigms, and (ii) their high level of redundancy. In a large lexicon, we can indeed expect to find many instances of pairs like write-writer: for instance read-reader, review-reviewer... Complementing surface analogies with statistical information thus has the potential to make the search problem tractable, while still providing with many good analogs. Various attempts have been made to use surface analogies in various contexts: automatic word pronunciation (Yvon, 1999), morphological analysis (Lepage, 1999a; Pirrelli and Yvon, 1999) and syntactical analysis (Lepage, 1999b). These experiments have mainly focused on linear represen120 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), pages 120–127, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics tations of linguistic data, taking the form of finite sequences of symbols, using a restrictive and sometimes ad-hoc definition of the notion of an analogy. The first contribution of this paper is to propose a general definition of formal analogical proportions for algebraic structures commonly us</context>
</contexts>
<marker>Lepage, 1999</marker>
<rawString>Yves Lepage. 1999a. Analogy+tables=conjugation. In G. Friedl and H.G. Mayr, editors, Proceedings of NLDB ’99, pages 197–201, Klagenfurt, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Lepage</author>
</authors>
<title>Open set experiments with direct analysis by analogy.</title>
<date>1999</date>
<booktitle>In Proceedings of NLPRS ’99,</booktitle>
<volume>2</volume>
<pages>363--368</pages>
<location>Beijing, China.</location>
<contexts>
<context position="2945" citStr="Lepage, 1999" startWordPosition="449" endWordPosition="450"> advantage of other specificities of linguistic data: (i) their systemic organization in (pseudo)-paradigms, and (ii) their high level of redundancy. In a large lexicon, we can indeed expect to find many instances of pairs like write-writer: for instance read-reader, review-reviewer... Complementing surface analogies with statistical information thus has the potential to make the search problem tractable, while still providing with many good analogs. Various attempts have been made to use surface analogies in various contexts: automatic word pronunciation (Yvon, 1999), morphological analysis (Lepage, 1999a; Pirrelli and Yvon, 1999) and syntactical analysis (Lepage, 1999b). These experiments have mainly focused on linear represen120 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), pages 120–127, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics tations of linguistic data, taking the form of finite sequences of symbols, using a restrictive and sometimes ad-hoc definition of the notion of an analogy. The first contribution of this paper is to propose a general definition of formal analogical proportions for algebraic structures commonly us</context>
</contexts>
<marker>Lepage, 1999</marker>
<rawString>Yves Lepage. 1999b. Open set experiments with direct analysis by analogy. In Proceedings of NLPRS ’99, volume 2, pages 363–368, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvain Lombardy</author>
<author>Yann R´egis-Gianas Rapha¨el Poss</author>
<author>Jacques Sakarovitch</author>
</authors>
<title>Introducing Vaucanson.</title>
<date>2003</date>
<booktitle>In Proceedings of CIAA</booktitle>
<pages>96--107</pages>
<marker>Lombardy, Rapha¨el Poss, Sakarovitch, 2003</marker>
<rawString>Sylvain Lombardy, Rapha¨el Poss, Yann R´egis-Gianas, and Jacques Sakarovitch. 2003. Introducing Vaucanson. In Proceedings of CIAA 2003, pages 96–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Mikheev</author>
</authors>
<title>Automatic rule induction for unknown word guessing.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="21068" citStr="Mikheev, 1997" startWordPosition="3643" endWordPosition="3644"> in Section 2.2. The first experiment consists in computing one or several vector(s) of morphosyntactic features to be associated with a form. Each vector comprises the lemma, the part-of-speech, and, based on the part-of-speech, additional features such as number, gender, case, tense, mood, etc. An (English) input/output pair for this tasks thus looks like: input=replying; output={reply; V-pp--J, where the placeholder ’-’ denotes irrelevant features. Lexical analysis is useful for many applications: a POS tagger, for instance, needs to “guess” the possible part(s)-of-speech of unknown words (Mikheev, 1997). For this task, we use the definition of analogical proportions for “flat” feature vectors (see section 3.1) and for word strings (section 3.2). The training data is a list of fully informed lexical entries; the test data is a list of isolated word forms not represented in the lexicon. Bins are constructed based on inflectional families. The second experiment consists in computing a morphological parse of unknown lemmas: for each input lemma, the output of the system is one or several parse trees representing a possible hierarchical decomposition of the input into (morphologically categorized</context>
</contexts>
<marker>Mikheev, 1997</marker>
<rawString>Andrei Mikheev. 1997. Automatic rule induction for unknown word guessing. Computational Linguistics, 23(3):405–423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vito Pirrelli</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Analogy in the lexicon: a probe into analogy-based machine learning of language.</title>
<date>1999</date>
<booktitle>In Proceedings of the 6th International Symposium on Human Communication,</booktitle>
<location>Santiago de Cuba, Cuba.</location>
<contexts>
<context position="2972" citStr="Pirrelli and Yvon, 1999" startWordPosition="451" endWordPosition="454">ther specificities of linguistic data: (i) their systemic organization in (pseudo)-paradigms, and (ii) their high level of redundancy. In a large lexicon, we can indeed expect to find many instances of pairs like write-writer: for instance read-reader, review-reviewer... Complementing surface analogies with statistical information thus has the potential to make the search problem tractable, while still providing with many good analogs. Various attempts have been made to use surface analogies in various contexts: automatic word pronunciation (Yvon, 1999), morphological analysis (Lepage, 1999a; Pirrelli and Yvon, 1999) and syntactical analysis (Lepage, 1999b). These experiments have mainly focused on linear represen120 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), pages 120–127, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics tations of linguistic data, taking the form of finite sequences of symbols, using a restrictive and sometimes ad-hoc definition of the notion of an analogy. The first contribution of this paper is to propose a general definition of formal analogical proportions for algebraic structures commonly used in NLP: attribute-value </context>
</contexts>
<marker>Pirrelli, Yvon, 1999</marker>
<rawString>Vito Pirrelli and Franc¸ois Yvon. 1999. Analogy in the lexicon: a probe into analogy-based machine learning of language. In Proceedings of the 6th International Symposium on Human Communication, Santiago de Cuba, Cuba.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony A Plate</author>
</authors>
<title>Analogy retrieval and processing with distributed vector representations. Expert systems,</title>
<date>2000</date>
<pages>17--1</pages>
<contexts>
<context position="4937" citStr="Plate, 2000" startWordPosition="747" endWordPosition="748">performance. We conclude by discussing current limitations of this model and by suggesting possible extensions. 2 Principles of analogical learning 2.1 Analogical reasoning The ability to identify analogical relationships between what looks like unrelated situations, and to use these relationships to solve complex problems, lies at the core of human cognition (Gentner et al., 2001). A number of models of this ability have been proposed, based on symbolic (e.g. (Falkenheimer and Gentner, 1986; Thagard et al., 1990; Hofstadter and the Fluid Analogies Research group, 1995)) or subsymbolic (e.g. (Plate, 2000; Holyoak and Hummel, 2001)) approaches. The main focus of these models is the dynamic process of analogy making, which involves the identification of a structural mappings between a memorized and a new situation. Structural mapping relates situations which, while being apparently very different, share a set of common high-level relationships. The building of a structural mapping between two situations utilizes several subparts of their descriptions and the relationships between them. Analogy-making seems to play a central role in our reasoning ability; it is also invoked to explain some human</context>
</contexts>
<marker>Plate, 2000</marker>
<rawString>Tony A. Plate. 2000. Analogy retrieval and processing with distributed vector representations. Expert systems, 17(1):29–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacques Sakarovitch</author>
</authors>
<title>El´ements de th´eorie des automates.</title>
<date>2003</date>
<location>Vuibert, Paris.</location>
<contexts>
<context position="14294" citStr="Sakarovitch, 2003" startWordPosition="2410" endWordPosition="2411"> respect to w, denoted by w\v is the set of subwords of w obtained by removing from w, in a left-to-right fashion, the symbols in v. For example, eea is a complementary subword of xmplr with respect to exemplar. When v is not a subword of w, w\v is empty. This notion can be generalized to any regular language. The complementary set of v with respect to w is a regular set: it is the output language of the finitestate transducer Tw (see Figure 1) for the input v. Figure 1: The transducer Tw computing complementary sets wrt w. Shuffle The shuffle u • v of two words u and v is introduced e.g. in (Sakarovitch, 2003) as follows: u • v = {u1v1u2v2 ... unvn, st. ui, vi ∈ E*, u1 ... un = u,v1 ... vn = v} The shuffle of two words u and v contains all the words w which can be composed using all the symbols in u and v, subject to the condition that if a precedes b in u (or in v), then it precedes b in w. Taking, for instance, u = abc and v = def, the words abcdef, abdefc, adbecf are in u • v; this is not the case with abefcd. This operation generalizes straightforwardly to languages. The shuffle of two regular languages is regular (Sakarovitch, 2003); the automaton A, computing K•L, is derived from the automata</context>
</contexts>
<marker>Sakarovitch, 2003</marker>
<rawString>Jacques Sakarovitch. 2003. El´ements de th´eorie des automates. Vuibert, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Royal Skousen</author>
</authors>
<title>Analogical Modeling of Language.</title>
<date>1989</date>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="6030" citStr="Skousen, 1989" startWordPosition="913" endWordPosition="914">een them. Analogy-making seems to play a central role in our reasoning ability; it is also invoked to explain some human skills which do not involve any sort of conscious reasoning. This is the case for many tasks related to the perception and production of language: lexical access, morphological parsing, word pronunciation, etc. In this context, analogical models have been proposed as a viable alternative to rule-based models, and many implementation of these lowlevel analogical processes have been proposed such as decision trees, neural networks or instance-based learning methods (see e.g. (Skousen, 1989; Daelemans et al., 1999)). These models share an acceptation of analogy which mainly relies on surface similarities between instances. Our learner tries to bridge the gap between these approaches and attempts to remain faithful to the idea of structural analogies, which prevails in the AI literature, while also exploiting the intuitions of large-scale, instance-based learning models. 2.2 Analogical learning We consider the following supervised learning task: a learner is given a set S of training instances {Xi, ... , XJ independently drawn from some unknown distribution. Each instance Xi is a</context>
</contexts>
<marker>Skousen, 1989</marker>
<rawString>Royal Skousen. 1989. Analogical Modeling of Language. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicolas Stroppa</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Formal models of analogical relationships.</title>
<date>2005</date>
<tech>Technical report,</tech>
<location>ENST, Paris, France.</location>
<contexts>
<context position="17330" citStr="Stroppa and Yvon, 2005" startWordPosition="3014" endWordPosition="3017">nary operator 4v as t 4v t&apos; = t (v +— t&apos;). Definition 2 can then be extended as: Definition 5 (Analogical proportion (trees)) (x, y, z, t) E U form an analogical proportion, denoted by x : y :: z : t iff there exists some variables (v1, ... , vn_1) and some factorizations x1 4v1 ... 4vn−1 xn = x, y1 4v1 ... 4vn−1 yn = y, z1 4v1 ... 4vn−1 zn = z, t1 4v1 ... 4vn−1 tn = t such that Vi, (yi, zi) E {(xi, ti), (ti, xi)}. An example of such a proportion is illustrated on Figure 2 with syntactic parse trees. This definition yields an effective algorithm computing analogical proportions between trees (Stroppa and Yvon, 2005). We consider here a simpler heuristic approach, consisting in (i) linearizing labelled trees into parenthesized sequences of symbols and (ii) using the analogical solver for words introduced above. This approach yields a faster, albeit approximative algorithm, which makes analogical inference tractable even for large tree databases. 3.4 Algorithmic issues We have seen how to compute analogical relationships for features whose values are words and trees. the police have VP impounded impounded eaten Figure 2: Analogical proportion between trees. If we use, for trees, the solver based on tree li</context>
</contexts>
<marker>Stroppa, Yvon, 2005</marker>
<rawString>Nicolas Stroppa and Franc¸ois Yvon. 2005. Formal models of analogical relationships. Technical report, ENST, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Thagard</author>
<author>Keith J Holoyak</author>
<author>Greg Nelson</author>
<author>David Gochfeld</author>
</authors>
<title>Analog retrieval by constraint satisfaction.</title>
<date>1990</date>
<journal>Artificial Intelligence,</journal>
<volume>46</volume>
<issue>3</issue>
<contexts>
<context position="4844" citStr="Thagard et al., 1990" startWordPosition="732" endWordPosition="735"> experiments aimed at demonstrating the flexibility of this model and at assessing its generalization performance. We conclude by discussing current limitations of this model and by suggesting possible extensions. 2 Principles of analogical learning 2.1 Analogical reasoning The ability to identify analogical relationships between what looks like unrelated situations, and to use these relationships to solve complex problems, lies at the core of human cognition (Gentner et al., 2001). A number of models of this ability have been proposed, based on symbolic (e.g. (Falkenheimer and Gentner, 1986; Thagard et al., 1990; Hofstadter and the Fluid Analogies Research group, 1995)) or subsymbolic (e.g. (Plate, 2000; Holyoak and Hummel, 2001)) approaches. The main focus of these models is the dynamic process of analogy making, which involves the identification of a structural mappings between a memorized and a new situation. Structural mapping relates situations which, while being apparently very different, share a set of common high-level relationships. The building of a structural mapping between two situations utilizes several subparts of their descriptions and the relationships between them. Analogy-making se</context>
</contexts>
<marker>Thagard, Holoyak, Nelson, Gochfeld, 1990</marker>
<rawString>Paul Thagard, Keith J. Holoyak, Greg Nelson, and David Gochfeld. 1990. Analog retrieval by constraint satisfaction. Artificial Intelligence, 46(3):259–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antal van den Bosch</author>
<author>Walter Daelemans</author>
</authors>
<title>Memory-based morphological processing.</title>
<date>1999</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>285--292</pages>
<location>Maryland.</location>
<marker>van den Bosch, Daelemans, 1999</marker>
<rawString>Antal van den Bosch and Walter Daelemans. 1999. Memory-based morphological processing. In Proceedings ofACL, pages 285–292, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Randall Wilson</author>
<author>Tony R Martinez</author>
</authors>
<title>Reduction techniques for instance-based learning algorithms.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<volume>38</volume>
<issue>3</issue>
<contexts>
<context position="9668" citStr="Wilson and Martinez, 2000" startWordPosition="1532" endWordPosition="1535">S may be unequally relevant, and we might expect the search procedure to treat them accordingly. • Whenever several competing hypotheses are proposed for O(X), a ranking must be performed. In our current implementation, hypotheses are ranked based on frequency counts. These issues are well-known problems for k-NN classifiers. The second one does not appear to be critical and is usually solved based on a majority rule. In contrast, a considerable amount of effort has been devoted to reduce and optimize the search process, via editing and condensing methods, as studied e.g. in (Dasarathy, 1990; Wilson and Martinez, 2000). Proposals for solving this problem are discussed in Section 3.4. 3 An algebraic framework for analogical proportions Our inductive model requires the availability of a device for computing analogical proportions on feature vectors. We consider that an analogical proportion holds between four feature vectors when the proportion holds for all components. In this section, we propose a unified algebraic framework for defining analogical proportions between individual features. After giving the general definition, we present its instantiation for two types of features: words over a finite alphabe</context>
</contexts>
<marker>Wilson, Martinez, 2000</marker>
<rawString>D. Randall Wilson and Tony R. Martinez. 2000. Reduction techniques for instance-based learning algorithms. Machine Learning, 38(3):257–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franc¸ois Yvon</author>
</authors>
<title>Pronouncing unknown words using multi-dimensional analogies.</title>
<date>1999</date>
<booktitle>In Proc. Eurospeech,</booktitle>
<volume>1</volume>
<pages>199--202</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="2907" citStr="Yvon, 1999" startWordPosition="445" endWordPosition="446">inimize such confusions, one can take advantage of other specificities of linguistic data: (i) their systemic organization in (pseudo)-paradigms, and (ii) their high level of redundancy. In a large lexicon, we can indeed expect to find many instances of pairs like write-writer: for instance read-reader, review-reviewer... Complementing surface analogies with statistical information thus has the potential to make the search problem tractable, while still providing with many good analogs. Various attempts have been made to use surface analogies in various contexts: automatic word pronunciation (Yvon, 1999), morphological analysis (Lepage, 1999a; Pirrelli and Yvon, 1999) and syntactical analysis (Lepage, 1999b). These experiments have mainly focused on linear represen120 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), pages 120–127, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics tations of linguistic data, taking the form of finite sequences of symbols, using a restrictive and sometimes ad-hoc definition of the notion of an analogy. The first contribution of this paper is to propose a general definition of formal analogical proportion</context>
</contexts>
<marker>Yvon, 1999</marker>
<rawString>Franc¸ois Yvon. 1999. Pronouncing unknown words using multi-dimensional analogies. In Proc. Eurospeech, volume 1, pages 199–202, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franc¸ois Yvon</author>
</authors>
<title>Finite-state machines solving analogies on words.</title>
<date>2003</date>
<tech>Technical report, ENST.</tech>
<contexts>
<context position="13515" citStr="Yvon, 2003" startWordPosition="2258" endWordPosition="2259">cal equation, nor its uniqueness when it exists. (Lepage, 1998) gives a set of necessary conditions for a solution to exist. These conditions also apply here. In particular, if t is a solution of x : y :: z :?, then t contains, in the same relative order, all the symbols in y and z that are not in x. As a consequence, all solutions of an equation have the same length. 3.2.2 A Finite-state Solver Definition (3) yields an efficient procedure for solving analogical equations, based on finite-state transducers. The main steps of the procedure are sketched here. A full description can be found in (Yvon, 2003). To start with, let us introduce the notions of complementary set and shuffle product. Complementary set If v is a subword of w, the complementary set of v with respect to w, denoted by w\v is the set of subwords of w obtained by removing from w, in a left-to-right fashion, the symbols in v. For example, eea is a complementary subword of xmplr with respect to exemplar. When v is not a subword of w, w\v is empty. This notion can be generalized to any regular language. The complementary set of v with respect to w is a regular set: it is the output language of the finitestate transducer Tw (see </context>
</contexts>
<marker>Yvon, 2003</marker>
<rawString>Franc¸ois Yvon. 2003. Finite-state machines solving analogies on words. Technical report, ENST.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>