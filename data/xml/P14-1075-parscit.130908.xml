<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.7963824">
Perplexity on Reduced Corpora
Hayato Kobayashi∗
Yahoo Japan Corporation
9-7-1 Akasaka, Minato-ku, Tokyo 107-6211, Japan
hakobaya@yahoo-corp.jp
</note>
<sectionHeader confidence="0.98075" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999977071428571">
This paper studies the idea of remov-
ing low-frequency words from a corpus,
which is a common practice to reduce
computational costs, from a theoretical
standpoint. Based on the assumption that a
corpus follows Zipf’s law, we derive trade-
off formulae of the perplexity of k-gram
models and topic models with respect to
the size of the reduced vocabulary. In ad-
dition, we show an approximate behavior
of each formula under certain conditions.
We verify the correctness of our theory on
synthetic corpora and examine the gap be-
tween theory and practice on real corpora.
</bodyText>
<sectionHeader confidence="0.998426" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99900684375">
Removing low-frequency words from a corpus
(often called cutoff) is a common practice to save
on the computational costs involved in learning
language models and topic models. In the case
of language models, we often have to remove
low-frequency words because of a lack of com-
putational resources, since the feature space of k-
grams tends to be so large that we sometimes need
cutoffs even in a distributed environment (Brants
et al., 2007). In the case of topic models, the in-
tuition is that low-frequency words do not make a
large contribution to the statistics of the models.
Actually, when we try to roughly analyze a corpus
with topic models, a reduced corpus is enough for
the purpose (Steyvers and Griffiths, 2007).
A natural question arises: How many low-
frequency words can we remove while maintain-
ing sufficient performance? Or more generally,
by how much can we reduce a corpus/model us-
ing a certain strategy and still keep a sufficient
level of performance? There have been many stud-
∗This work was mainly carried out while the author was
with Toshiba Corporation.
ies addressing the question as it pertains to differ-
ent strategies (Stolcke, 1998; Buchsbaum et al.,
1998; Goodman and Gao, 2000; Gao and Zhang,
2002; Ha et al., 2006; Hirsimaki, 2007; Church
et al., 2007). Each of these studies experimen-
tally discusses trade-off relationships between the
size of the reduced corpus/model and its perfor-
mance measured by perplexity, word error rate,
and other factors. To our knowledge, however,
there is no theoretical study on the question and
no evidence for such a trade-off relationship, es-
pecially for topic models.
In this paper, we first address the question from
a theoretical standpoint. We focus on the cutoff
strategy for reducing a corpus, since a cutoff is
simple but powerful method that is worth study-
ing; as reported in (Goodman and Gao, 2000;
Gao and Zhang, 2002), a cutoff is competitive
with sophisticated strategies such as entropy prun-
ing. As the basis of our theory, we assume Zipf’s
law (Zipf, 1935), which is an empirical rule repre-
senting a long-tail property of words in a corpus.
Our approach is essentially the same as those in
physics, in the sense of constructing a theory while
believing experimentally observed results. For ex-
ample, we can derive the distance to the landing
point of a ball thrown up in the air with initial
speed vo and angle 0 as vol sin(20)/g by believ-
ing in the experimentally observed gravity acceler-
ation g. In a similar fashion, we will try to clarify
the trade-off relationship by believing Zipf’s law.
The rest of the paper is organized as follows. In
Section 2, we define the notation and briefly ex-
plain Zipf’s law and perplexity. In Section 3, we
theoretically derive the trade-off formulae of the
cutoff for unigram models, k-gram models, and
topic models, each of which represents its per-
plexity with respect to a reduced vocabulary, un-
der the assumption that the corpus follows Zipf’s
law. In addition, we show an approximate behav-
ior of each formula under certain conditions. In
</bodyText>
<page confidence="0.954254">
797
</page>
<note confidence="0.830908">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 797–806,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.9998495">
Section 4, we verify the correctness of our theory
on synthetic corpora and examine the gap between
theory and practice on several real corpora. Sec-
tion 5 concludes the paper.
</bodyText>
<sectionHeader confidence="0.981637" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<bodyText confidence="0.999940714285714">
Let us consider a corpus w := w1 · · · wN of cor-
pus size N and vocabulary size W. We use an
abridged notation {w} := {w ∈ w} to repre-
sent the vocabulary of w. Clearly, N = |w |and
W = |{w} |hold. When w has additional nota-
tions, N and W inherit them. For example, we
will use N′ as the size of w′ without its definition.
</bodyText>
<subsectionHeader confidence="0.983595">
2.1 Power law and Zipf’s law
</subsectionHeader>
<bodyText confidence="0.99993625">
A power law is a mathematical relationship be-
tween two quantities x and y, where y is propor-
tional to the c-th power of x, i.e., y ∝ xc, and
c is a real number. Zipf’s law (Zipf, 1935) is a
power law discovered on real corpora, wherein for
any word w ∈ w in a corpus w, its frequency (or
word count) f(w) is inversely proportional to its
frequency ranking r(w), i.e.,
</bodyText>
<equation confidence="0.831127">
f(w) = r(w).
</equation>
<bodyText confidence="0.982948916666667">
Here, f(w) := |{w′ ∈ w  |w′ = w}|, and
r(w) := |{w′ ∈ w  |f(w′) ≥ f(w)}|. From
the definition, the constant C is the maximum fre-
quency in the corpus. Taking the natural loga-
rithms ln(·) of both sides of the above equation,
we find that its plot becomes linear on a log-log
graph of r(w) and f(w). In fact, the result based
on a statistical test in (Clauset et al., 2009) reports
that the frequencies of words in a corpus com-
pletely follow a power law, whereas many datasets
with long-tail properties, such as networks, actu-
ally do not follow power laws.
</bodyText>
<subsectionHeader confidence="0.984427">
2.2 Perplexity
</subsectionHeader>
<bodyText confidence="0.999969933333334">
Perplexity is a widely used evaluation measure of
k-gram models and topic models. Let p be a pre-
dictive distribution over words, which was learned
from a training corpus w based on a certain model.
Formally, perplexity PP is defined as the geomet-
ric mean of the inverse of the per-word likelihood
on the held-out test corpus wT, i.e.,
Intuitively, PP means how many possibilities one
has for estimating the next word in a test cor-
pus. According to the definition, a lower perplex-
ity means better generalization performance of p.
Another well-known evaluation measure is cross-
entropy. Since cross-entropy is easily calculated
as log2 PP, we can apply many of the results of
this paper to cross-entropy.
</bodyText>
<sectionHeader confidence="0.945603" genericHeader="method">
3 Perplexity on Reduced Corpora
</sectionHeader>
<bodyText confidence="0.998801347826087">
Now let us consider what a cutoff is. In our study,
we simply define a corpus that has been reduced
by removing low-frequency words from the origi-
nal corpus with a certain threshold. Formally, we
say w′ is a corpus reduced from the original cor-
pus w, if w′ is the longest subsequence of w such
that maxw′Ew′ r(w′) = W′. Note that a sub-
sequence can include gaps in contrast to a sub-
string. For example, supposing we have a corpus
w = abcaba with a vocabulary {w} = {a, b, c},
w′1 = ababa is a reduced corpus, while w′2 =
aba and w′3 = acaa are not.
After learning a distribution p′ from a re-
duced corpus w′, we need to infer the distri-
bution p learned from the original corpus w.
Here, we use constant restoring (defined below),
which assumes the frequencies of the reduced low-
frequency words are a constant.
Definition 1 (Constant Restoring). Given a pos-
itive constant λ, a distribution p′ over a reduced
corpus w′, and a corpus w, we say that pˆ is
a λ-restored distribution of p′ from w′ to w, if
EwE{w} ˆp(w) = 1, and for any w ∈ w,
</bodyText>
<equation confidence="0.98891575">
�
p′(w) (w ∈ w′)
ˆp(w) ∝ λ
(w ∈/ w′).
</equation>
<bodyText confidence="0.999177909090909">
Constant restoring is similar to the additive
smoothing defined by ˆp(w) ∝ p′(w) + λ, which is
used to solve the zero-frequency problem of lan-
guage models (Chen and Goodman, 1996). The
only difference is the addition of a constant λ
only to zero-frequency words. We think con-
stant restoring is theoretically natural in our set-
ting, since we can derive the above equation by
letting each frequency of reduced words be λN′
and defining a restored frequency function as fol-
lows:
</bodyText>
<equation confidence="0.953107">
C
� � � 1 . �
1 NT f(w) (w ∈ w′)
PP := ˆf(w) =
p(w) λN′ (w ∈/ w′).
wEw�
</equation>
<page confidence="0.994127">
798
799
</page>
<bodyText confidence="0.505012">
Let
</bodyText>
<equation confidence="0.985153363636363">
(∏ )
ˆPP1 := 1
w∈w p(w)
plexity
ˆPP1 of the λ∗-restored distribution pˆ of p′
,
H(W′)
(W − W′ 1− H(W)
H(W) − H(W′))
N
be the perplexity
1
ˆPP1.
stant λ∗ minimizing
NR
N
1 + WRλ∗
NR
λ∗ N
= (1 + N′) NR
(N′ )
C
N′ ( )
∑ W′
N C lnr
exp .
N r
r=1
1
N
( ∏ )
1 ∏ 1
ˆPP1 = ˆp(w′) ˆp(wR)
w′∈w′ wR∈wR
1
N
.
1 + WRλ
NR
λN
( ∏ )
1
p′(w′)
w′∈w′
</equation>
<bodyText confidence="0.99968375">
Informally, constant restoring involves padding
the vocabulary, while additive smoothing involves
padding the corpus. Smoothing should be carried
out after restoring.
</bodyText>
<subsectionHeader confidence="0.998368">
3.1 Perplexity of Unigram Models
</subsectionHeader>
<bodyText confidence="0.980943230769231">
Let us consider the perplexity of a unigram model
learned from a reduced corpus. In unigram mod-
els, a predictive distribution p′ on a reduced cor-
pus w′ can be simply calculated as p′(w′) =
f(w′)/N′. We shall start with an analysis of
training-set perplexity, since we can derive an ex-
act formula for it, which will give us a sufficient
idea for making an approximate analysis of test-
set perplexity.
Theorem 3. For any distribution p′ on a unigram
model learned from a corpus w′ reduced from the
original corpus w following Zipf’s law, the per-
from w′ to w is calculated by
</bodyText>
<equation confidence="0.989806222222222">
(B(W ′) )
ˆPP1(W′) =H(W) exp
H(W )
1
where H(X) := ∑X x and B(X) :=
x=1
∑X ln x
x .
x=1
</equation>
<bodyText confidence="0.979551428571429">
Proof. We expand the first part of ˆPP1 in the proof
of Lemma 2 using λ∗ as follows:
of a λ-restored distribution pˆ on a unigram model.
The next lemma gives the optimal restoring con-
Lemma 2. For any λ-restored distribution pˆ of a
distribution p′ from a reduced corpus w′ to the
original corpus w, its perplexity is minimized by
</bodyText>
<equation confidence="0.977274666666667">
N − N′
λ∗ =
(W − W′)N′ .
</equation>
<bodyText confidence="0.9894562">
Proof. Let wR be the longest subsequence such
that minw′∈w′ r(w′) = W′ + 1. Since wR is the
remainder of w′, NR = N − N′ and WR = W −
W′ hold. After substituting the normalized form
of pˆ of Definition 1 into ˆPP1, we have
</bodyText>
<equation confidence="0.994985285714286">
)
∏ 1 + WRλ ∏ 1 + WRλ
p′(w′) λ
w′∈w′ wR∈wR
(=
1
N
</equation>
<bodyText confidence="0.932319111111111">
We obtain the objective formula by putting the
above two formulae together with N = CH(W)
and N′ = CH(W′), which are derived from
Zipf’s law.
We obtain the optimal smoothing factor λ∗ when
ˆPP1 a ∂∂λ(1 + WRλ)/λNRN = 0.
By using a similar argument to the one in the
above lemma, we can obtain the optimal constant
of additive smoothing as λ∗ � N−N′
</bodyText>
<equation confidence="0.506711">
W N′ , when N is
</equation>
<bodyText confidence="0.974492538461538">
sufficiently large.
The next theorem gives the exact formula of the
training-set perplexity of a unigram model learned
from a reduced corpus.
The functions H(X) and B(X) are the X-th
partial sum of the harmonic series and Bertrand
series (special form), respectively. An approxima-
tion by definite integrals yields H(X) Pz� ln X +γ,
where γ is the Euler-Mascheroni constant, and
B(X) Pz� t1 lnt X. We may omit γ from the ap-
proximate analysis.
Now let us consider an approximate form of
ˆPP1(W′) in Theorem 3. For further discussion,
</bodyText>
<equation confidence="0.95774092">
(N ) ((W − W′)N′ )1−N′ N
N′ N − N′
.
The second part of ˆPP1 is as follows:
( ∏ )
1
p′(w′)
w′∈w′
( 1 )
∏
= p′(w′)
w′∈{w′}
1 f(w′)
NN
W′ C
∏ (rN′) rN
C
r=1
W′ ′ C W′
∏(N)rN∏ rC
rN
C
r=1 r=1
∂
∂λ
</equation>
<bodyText confidence="0.683505">
we define the last part of PP1(W′) as follows:
</bodyText>
<equation confidence="0.994586">
( W − W ′ )1− H(W ′)
H(W )
F (W, W ′) := H(W ) − H(W ′)
Since W′ = SW holds for an appropriate ratio S,
we have
F(W, SW) = W − SW
(H(W) − H(SW)
1— ln (δW)
≈ W − SW 1n W
(ln W − ln (SW)
(W(1 − S))
−lnS
1
→ (W → ∞).
S
</equation>
<bodyText confidence="0.9977562">
Therefore, when W is sufficiently large, we can
use F (W, W′) ≈ WW′, since F (W, SW) ≈ δ1 holds
for any ratio S : 0 &lt; S &lt; 1. Using this fact,
we obtain an approximate formula �PP1 of PP1 as
follows:
</bodyText>
<equation confidence="0.993165333333333">
PP1 (W′) = ln W exp(2lnW)
ln2 W′ W
W′
(ln W′ − ln W)2
= √ W ln W exp .
2ln W
</equation>
<bodyText confidence="0.924142333333333">
The complexity of PP1 is quasi-polynomial,
i.e., �PP1(W′) = O(W′ln W′), which behaves as
a quadratic function on a log-log graph. Since
</bodyText>
<equation confidence="0.8921825">
�PP1(W′) is convex, i.e., ∂2
∂W ′2 �PP1(W′) &gt; 0, and
its gradient ∂
∂W ′ �PP1(W′) is zero when W′ = W,
</equation>
<bodyText confidence="0.9916272">
we infer that low-frequency words may not largely
contribute to the statistics.
Considering the special case of W′ = W, we
obtain the perplexity PP1 of the unigram model
learned from the original corpus w as
</bodyText>
<equation confidence="0.682456">
PP1 = H(W) exp (H(W)) ≈ √W ln W.
</equation>
<bodyText confidence="0.985394782608696">
Interestingly, PP1 is approximately expressed as
a simple elementary function of vocabulary size
W. This suggests that models learned from cor-
pora with the same vocabulary size theoretically
have the same perplexity.
For the test-set perplexity, we assume that both
the training corpus w and test corpus wτ are gen-
erated from the same distribution based on Zipf’s
law. This assumption is natural, considering the
situation of an in-domain test or cross validation
test. Let wτ ′ be the longest subsequence of wτ
such that for any w ∈ wτ ′, w ∈ w′ holds. For-
mally, we assume p′(w) ≈ pτ ′(w) for any w ∈ w′τ
when Wτ &gt; W′, where pτ ′ is the true distribu-
tion over wτ ′. Using similar arguments to those
of Lemma 2 and Theorem 3 for wτ, we obtain
an approximation formula for the test-set perplex-
ity, where we simply substitute W and W′ in the
exact formula for the training-set perplexity with
Wτ and Wτ ′, respectively. For simplicity, we will
only consider training-set perplexity from now on,
since we can make a similar argument for the test-
set perplexity in the later analysis.
</bodyText>
<subsectionHeader confidence="0.999911">
3.2 Perplexity of k-gram Models
</subsectionHeader>
<bodyText confidence="0.9998978">
Here, we will consider the perplexity of a k-gram
model learned from a reduced corpus as a standard
extension of a unigram model. Our theory only
assumes that the corpus is generated on the basis
of Zipf’s law. Thus, we can use a simple model
where k-grams are calculated from a random word
sequence based on Zipf’s law. This model seems
to be stupid, since we can easily notice that the
bigram “is is” is quite frequent, and the two bi-
grams “is a” and “a is” have the same frequency.
However, the experiments described later uncov-
ered the fact that the model can roughly capture
the behavior of real corpora.
The frequency fk of k-gram word wk ∈ wk in
the model is represented by the following formula:
</bodyText>
<equation confidence="0.4126275">
fk(wk) = gk(rk(wk)),
Ck
</equation>
<bodyText confidence="0.9999634">
where Ck is the maximal frequency in k-grams, rk
is the frequency ranking of wk over k-grams, and
gk expresses the frequency decay in k-grams. For
example, the decay function g2 of bigrams is as
follows:
</bodyText>
<equation confidence="0.994631">
(g2(i))i := (g2(1), g2(2), g2(3),···)
= (1 · 1, 1 · 2, 2 · 1, 1 · 3, 3 · 1, · · · )
= (1, 2, 2, 3, 3, 4, 4, 4, 5, 5, 6,·· · ).
</equation>
<bodyText confidence="0.9982546">
This is an inverse of the sum of Piltz’s divisor
functions d2(n) := Ei1·i2=n 1, which represents
the number of divisors of an integer n (cf. (OEIS,
2001)). In general, we formally define gk through
its inverse: g−1
</bodyText>
<equation confidence="0.62242125">
k (E) := 5k(E), where 5k(E) :=
Eℓ dk(n) and dk(n) := Ei1·i2···ik=n 1. Since
n=1
(gk(i))i is a sorted sequence of the elements of the
</equation>
<bodyText confidence="0.9859475">
k-th tensor power of vector (1, · · · , W), we can
calculate the maximum frequency Ck as follows.
</bodyText>
<equation confidence="0.7115222">
.
H(δW)
1− H(W)
− ln δ
ln W
</equation>
<page confidence="0.944762">
800
</page>
<construct confidence="0.804788333333333">
Lemma 4. For any corpus w following Zipf’s law,
the maximum frequency of k-grams in our model
is calculated by
</construct>
<equation confidence="0.9591895">
N − (k − 1)D
Ck = (H(W))k ,
</equation>
<bodyText confidence="0.9778957">
where D denotes the number of documents in w.
Proof. We use ∑wk fk(wk) = Ck(∑w 1/r(w))k.
The sum Sk(ℓ) of Piltz’s divisor functions can
be approximated by ℓPk(ln ℓ), where Pk(x) is a
polynomial of degree k − 1 with respect to x,
and the main term of ℓPk(ln ℓ) is given by the
ζk(s)xs
following residue Ress=1 s , where ζ(s) is
the Riemann zeta function (Li, 2005). Using this
fact, we obtain an approximation ln (g−1
</bodyText>
<listItem confidence="0.5526538">
k (ℓ)) ≈
ln ℓ + O(ln (ln ℓ)) ≈ ln ℓ, when ℓ is sufficiently
large. Thus, when the corpus is sufficiently large,
we can see that the behavior of fk is roughly linear
on a log-log graph, i.e., fk(wk) ∝ rk(wk)−1, since
</listItem>
<equation confidence="0.8446945">
if g−1
k (ℓ) ∝ ℓc holds, then fk(r) ∝ (gk(r))−1 ∝
1
r− c holds.
</equation>
<bodyText confidence="0.929866210526316">
Unfortunately, however, most corpora in the
real world are not so large that the above-
mentioned relation holds. Actually, Ha et al. (Ha
et al., 2002; Ha et al., 2006) experimentally found
that although a k-gram corpus roughly follows a
power law even when k &gt; 1, its exponent is
smaller than 1 (for Zipf’s law). They pointed out
that the exponent of bigrams is about 0.66, and
that of 5-grams is about 0.59 in the Wall Street
Journal corpus (WSJ87). Believing their claim
that there exists a constant πk such that fk(wk) ∝
rk(wk)−πk, we estimated the exponent of k-grams
in an actual situation in the form of the following
lemma.
Lemma 5. Assuming that fk(wk) ∝ rk(wk)−πk
holds for any k-gram word wk ∈ wk in a corpus
w following Zipf’s law, the optimal exponent in
our model based on the least squares criterion is
calculated by
</bodyText>
<equation confidence="0.8065395">
ln W
πk = (k − 1) ln (ln W) + ln W .
</equation>
<bodyText confidence="0.9955755">
Proof. We find the optimal exponent πk by mini-
mizing the sum of squared errors between the gra-
</bodyText>
<equation confidence="0.967021571428571">
1
dients of g−1
k (r) and r πk on a log-log graph:
∫ { ∂y (y + ln Pk(y)) − y dy,
∂ ∂
∂y πk
( 1 )}2
</equation>
<bodyText confidence="0.980752235294118">
where y = ln r.
In the case of unigrams (k = 1), the formula
exactly represents Zipf’s law. In the case of k-
grams (k &gt; 1), we found that the formula ap-
proaches Zipf’s law when W approaches infinity,
i.e., limW→∞ πk = 1.
Let us consider the perplexity of a k-gram
model learned from a reduced corpus. We im-
mediately obtain the following corollary using
Lemma 5.
Corollary 6. For any distribution p′ on a k-gram
model learned from a corpus w′ reduced from the
original corpus w following Zipf’s law, assuming
that fk(wk) ∝ rk(wk)−πk holds for any k-gram
word wk ∈ wk and the optimal exponent πk in
Lemma S, the perplexity ˆPPk of the λ∗-restored
distribution pˆ ofp′ from w′ to w is calculated by
</bodyText>
<equation confidence="0.969635454545455">
(Bπk(W ′) )
ˆPPk(W′) =Hπk(W) exp
Hπk(W)
W − W′ 1
( Hπk (W) − Hπk (W′) )
1
where Ha(X) := ∑X xa and Ba(X) :=
x=1
∑X a ln x
xa .
x=1
</equation>
<bodyText confidence="0.998311428571429">
Ha(X) is the X-th partial sum of the P-series
or hyper-harmonic series, which is a generaliza-
tion of the harmonic series H(X). Ba(X) is the
X-th partial sum of the Bertrand series (another
special form of B(X)). When 0 &lt; a &lt; 1, we can
easily calculate ˆPPk(W′) by using the following
approximations:
</bodyText>
<equation confidence="0.9991145">
Ha(X) ≈ (X + 1)1−a − 1
1 − a
a
Ba(X) ≈ 1 −a(X + 1)1−a ln(X + 1)
a 1−a a
(1 − a)2 (X + 1) + (1 − a)2 .
</equation>
<bodyText confidence="0.9989747">
By putting the approximations of Ha(X) and
Ba(X) into the formula of Corollary 6, we ob-
tain an approximation ˆPPk(W′) ≈ O(W′W′1−πk).
This implies that ˆPPk(W′) is approximately linear
on a log-log graph, when πk is close to 1, i.e., k is
relatively small and W is sufficiently large. Note
that we must use the approximation of H(X), not
Ha(X), when a = 1.
The fact that the frequency of k-grams follows
a power law leads us to an additional convenient
</bodyText>
<equation confidence="0.891005">
Hπk (W′)
Hπk (W)
,
</equation>
<page confidence="0.947472">
801
</page>
<bodyText confidence="0.9987228">
property, since the process of generating a cor-
pus in our theory can be treated as a variant of
the coupon collector’s problem. In this problem,
we consider how many trials are needed for col-
lecting all coupons whose occurrence probabilities
follow some stable distribution. According to a
well-known result about power law distributions
(Boneh and Papanicolaou, 1996), we need a cor-
pus of size kW k
1−πk ln W when πk &lt; 1, and W ln2 W
when πk = 1 for collecting all of the k-grams, the
number of which is Wk. Using results in (Atso-
nios et al., 2011), we can easily obtain a lower and
upper bound of the actual vocabulary size Wk of
k-grams from the corpus size N and vocabulary
size W as
This means that we can determine the rough
sparseness of k-grams and adjust some of the pa-
rameters such as the gram size k in learning statis-
tical language models.
</bodyText>
<subsectionHeader confidence="0.999684">
3.3 Perplexity of Topic Models
</subsectionHeader>
<bodyText confidence="0.999910583333333">
In this section, we consider the perplexity of the
widely used topic model, Latent Dirichlet Alloca-
tion (LDA) (Blei et al., 2003), by using the nota-
tion given in (Griffiths and Steyvers, 2004). LDA
is a probabilistic language model that generates a
corpus as a mixture of hidden topics, and it allows
us to infer two parameters: the document-topic
distribution θ that represents the mixture rate of
topics in each document, and the topic-word dis-
tribution ϕ that represents the occurrence rate of
words in each topic. For a given corpus w, the
model is defined as
</bodyText>
<equation confidence="0.98324325">
θdi — Dirichlet(α)
zi|θdi — Multi(θdi)
ϕzi — Dirichlet(β)
wi|zi, ϕzi — Multi(ϕzi),
</equation>
<bodyText confidence="0.99998425">
where di and zi are respectively the document
that includes the i-th word wi and the hidden
topic that is assigned to wi. In the case of infer-
ence by Gibbs sampling presented in (Griffiths and
Steyvers, 2004), we can sample a “good” topic as-
signment zi for each word wi with high probabil-
ity. Using the assignments z, we obtain the pos-
terior distributions of two parameters as �θd(z) a
</bodyText>
<equation confidence="0.9510165">
n(d)
z + α and (z(w) a n(w)
z + β, where n(d)
z and
nz respectively represent the number of times
(w)
</equation>
<bodyText confidence="0.994168485714286">
assigning topic z in document d and the number
of times topic z is assigned to word w.
Since an exact analysis is very hard, we will
place rough assumptions on ϕ� and θ� to reduce the
complexity. The assumption placed on ϕ� is that the
word distribution 0z of each topic z follows Zipf’s
law. We think this is acceptable since we can re-
gard each topic as a corpus that follows Zipf’s law.
Since �ϕz is normalized for each topic, we can as-
sume that for any two topics, z and z′, and any
two words, w and w′, �ϕz(w) ,.J �z′(w′) holds if
rz(w) = rz′(w′), where rz(w) is the frequency
ranking of w with respect to n(w)
z . Note that the
above assumption pertains to a posterior, and we
do not discuss the fact that a Pitman-Yor process
prior is better suited for a power law (Goldwater et
al., 2011).
ϕ� may not be reason-
The assumption placed on
able in the case of 0, because we can easily think
of a document with only one topic, and we usu-
ally use a small number T of topics for LDA, e.g.,
T = 20. Thus, we consider two extreme cases.
One is where each document evenly has all topics,
and the other is where each document only has one
topic. Although these two cases might be unreal-
istic, the actual (theoretical) perplexity is expected
to be between their values. We believe that analyz-
ing such extreme cases is theoretically important,
since it would be useful for bounding the compu-
tational complexity and predictive performance.
We can regard the former case as a unigram
model, since the marginal predictive distribution
∑(z)�ϕz(w) a ∑z= 1 n( T+β — f (w) is in-
</bodyText>
<equation confidence="0.706446">
z=1 θd
</equation>
<bodyText confidence="0.988794875">
dependent of d; here we have used �θd(z) = 1/T
from the assumption. In the latter case, we can
obtain an exact formula for the perplexity of LDA
when the topic assigned to each document follows
a discrete uniform distribution, as shown in the
next theorem. Note that a mixture of corpora fol-
lowing Zipf’s law can be approximately regarded
as following Zipf’s law, when W is sufficiently
large.
Theorem 7. For any distribution p′ on the LDA
model with T topics learned from a corpus w′ re-
ducedfrom the original corpus w following Zipf’s
law, assuming that each document only has one
topic which is assigned based on a discrete uni-
form distribution, the perplexity PPMix of the λ*-
restored distribution p� ofp′ from w′ to w is calcu-
</bodyText>
<equation confidence="0.979876">
/ e− Wk-, Wk �! (πk + 1) I 1—e Wk_1 wk )
πk N
Wk :5 πk — 1 (Hπk (W k)
1 NW 1−πk
πk
� (πk — 1)Hπk(Wk).
</equation>
<page confidence="0.998194">
802
</page>
<tableCaption confidence="0.9816045">
Table 1: Details of Reuters, 20news, Enwiki,
Zipf1, and ZipfMix.
</tableCaption>
<table confidence="0.996544666666667">
vocab. size corpus size doc. size
Reuters 70,258 2,754,800 18,118
20news 192,667 4,471,151 19,997
Enwiki 409,902 16,711,226 51,231
Zipf1 69,786 2,754,800 18,118
ZipfMix 70,093 2,754,800 18,118
</table>
<equation confidence="0.971510666666667">
lated by
ˆPPMix(W′) =H(W/T) exp (B(W′/T)
H(W/T) )
H(W′/T)
(W − W′ 1− H(W/T)
H(W/T) − H(W′/T) )
</equation>
<bodyText confidence="0.972257181818182">
Proof. We can prove this by using a similar argu-
ment to that of Theorem 3 for each topic.
The formula of the theorem is nearly identical
to the one of Theorem 3 for a 1/T corpus. This
implies that the growth rate of the perplexity of
LDA models is larger than that of unigram mod-
els, whereas the perplexity of LDA models for
the original corpus is smaller than that of unigram
models. In fact, a similar argument to the one in
the approximate analysis in Section 3.1 leads to an
approximate formula ˜PPMix of ˆPPMix as
</bodyText>
<equation confidence="0.97836">
� W T exp (ln W ′ − ln W )2
˜PPMix(W′) = T ln W 2 ln (W/T ) ,
</equation>
<bodyText confidence="0.903128">
when W is sufficiently large. That is, ˜PPMix(W′)
also has a quadratic behavior in a log-log graph,
i.e., ˜PPMix(W′) = O(W′ln W′).
</bodyText>
<sectionHeader confidence="0.999324" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999922907692308">
We performed experiments on three real corpora
(Reuters, 20news, and Enwiki) and two syn-
thetic corpora (Zipf1 and ZipfMix) to verify
the correctness of our theory and to examine the
gap between theory and practice. Reuters and
20news here denote corpora extracted from the
Reuters-21578 and 20 Newsgroups data sets, re-
spectively. Enwiki is a 1/100 corpus of the En-
glish Wikipedia. Zipf1 is a synthetic corpus gen-
erated by Zipf’s law, whose corpus is the same size
as Reuters, and ZipfMix is amixture of 20 syn-
thetic corpora, sizes are 1/20th of Reuters. We
used ZipfMix only for the experiments on topic
models. Table 1 lists the details of all five corpora.
Fig. 1(a) shows the word frequency of
Reuters, 20news, Enwiki, and Zipf1 versus
frequency ranking on a log-log graph. In all cor-
pora, we can regard each curve as linear with a
gradient close to 1. This means that all corpora
roughly follow Zipf’s law. Furthermore, since the
curve of Zipf1 is similar to that of Reuters,
Zipf1 can be regarded as acceptable.
Fig. 1(b) plots the perplexity of unigram mod-
els learned from Reuters, 20news, Enwiki,
and Zipf1 versus the size of reduced vocabu-
lary on a log-log graph. Each value is the aver-
age over different test sets of five-fold cross val-
idation. Theory1 is calculated using the for-
mula in Theorem 3. The graph shows that the
curve of Theory1 is nearly identical to that of
Zipf1. Since the vocabulary size WT of each test
set is small in this experiment, some errors appear
when W′ is large, i.e., WT &lt; W′. This clearly
means that our theory is theoretically correct for
an ideal corpus Zipf1. Comparing Zipf1 with
Reuters, however, we find that their perplex-
ities are quite different. The reason is that the
gap between the frequencies of low-ranking (high-
frequency) words is considerably large. For ex-
ample, the frequency of the 1st-rank word of
Reuters is f(w) = 136, 371, while that of
Zipf1 is f(w) = 234, 705. Our theory seems to
be suited for inferring the growth rate of perplexity
rather than the perplexity value itself.
As for the approximate formula ˜PP1 of Theo-
rem 3, we can surely regard the curve of Zipf1
as being roughly quadratic. The curves of real
corpora also have a similar tendency, although
their gradients are slightly steeper. This difference
might have been caused by the above-mentioned
errors. However, at least, we can ascertain the
important fact that the results for the corpora re-
duced by 1/100 are not so different from those of
the original corpora from the perspective of their
perplexity measures.
Fig. 1(c) plots the frequency of k-grams (k E
11, 2,31) in Reuters versus frequency ranking
on a log-log graph. TheoryFreq (1-3) are calcu-
lated using Ck in Lemma 4 and πk in Lemma 5.
A comparison of TheoryFreq and Zipf verifies
the correctness of our theory. However, comparing
Zipf and Reuters, we see that Ck is poorly es-
timated when the gram size is large, whereas πk is
roughly correct. This may have happened because
we did not put any assumptions on the word se-
</bodyText>
<page confidence="0.996476">
803
</page>
<figure confidence="0.996311183908046">
107
106
105
104
103
102
101
100
100 101 102 103 104 105 106
Frequency Ranking
100 101 102 103 104 105 106
Reduced Vocabulary Size
Frequency
105
Reuters
20news
Enwiki
Zipf1
105
102
101
100 101 102 103 104 105
Frequency Ranking
Reuters
Zipf1
TheoryFreq1
Reuters2
Zipf2
TheoryFreq2
Reuters3
Zipf3
TheoryFreq3
104
Frequency
103
Test set Perp exity
104
103
Reuters
20news
Enwiki
Zipf1
Theory1
(a) Frequency of unigrams (b) Perplexity of unigram models (c) Frequency of k-grams
Exponent
0.8
0.6
0.4
1.0
1.2
Reuters
TheoryExp
1 2 3 4 5 6 7 8 9 10
Gram Size
106
105
104
103
102
100 101 102 103 104 105 106 107
Reduced Vocabulary Size
105
103
100 101 102 103 104 105 106
Reduced Vocabulary Size
Test set Perp exity
Reuters
Zipf1
Theory1
Reuters2
Zipf2
Theory2
Reuters3
Zipf3
Theory3
Test set Perp exity
104
Reuters
20news
Enwiki
Zipf1
Theory1
ZipfMix
TheoryMix
TheoryAve
(d) Exponent of a power law over k- (e) Perplexity of k-gram models (f) Perplexity of topic models
grams
</figure>
<figureCaption confidence="0.821014666666667">
Figure 1: (a) Word frequency of Reuters, 20news, Enwiki, and Zipf1 versus frequency ranking.
(b) Perplexity ofunigrammodels learned from Reuters, 20news, Enwiki, and Zipf1 versus size of
reduced vocabulary. Theory1 is calculated using the formula in Theorem 3. (c) Frequency of k-grams
</figureCaption>
<bodyText confidence="0.998336955555556">
(k E 11, 2,31) in Reuters and Zipf1 versus frequency ranking. The suffix digit of each label means
its gram size. TheoryFreq (1-3) are calculated using Lemma 4 and Lemma 5. (d) Exponent of a power
law over k-grams in Reuters versus gram size. TheoryGrad is calculated using Irk in Lemma 5. (e)
Perplexity of k-gram models learned from Reuters versus size of reduced vocabulary. Theory2 and
Theory3 are calculated using the formula in Corollary 6. (f) Perplexity of topic models learned from
Reuters, 20news, Enwiki, Zipf1, and ZipfMix versus size of reduced vocabulary. TheoryMix is
calculated using the formula in Theorem 7.
quences in our simple model. The frequencies of
high-order k-grams tend to be lower than in real-
ity. We might need to place a hierarchical assump-
tion on the a power law, as in done in hierarchical
Pitman-Yor processes (Wood et al., 2011).
Fig. 1(d) plots the exponent of the power law
over k-grams in Reuters versus the gram size
on a normal graph. We estimated each exponent
of Reuters by using the least-squares method.
TheoryGrad is calculated using Irk in Lemma 5.
Surprisingly, the real exponents of Reuters are
almost the same as the theoretical estimate Irk
based on our “stupid” model that does not care
about the order of words. Note that we do not use
any information other than the vocabulary size W
and the gram size k for estimating Irk.
Fig. 1(e) plots the perplexity of k-gram mod-
els (k E 11, 2,31) learned from Reuters versus
the size of reduced vocabulary on a log-log graph.
Theory2 and Theory3 are calculated using the
formula in Corollary 6. In the case of bigrams,
the perplexities of Theory2 are almost the same
as that of Zipf2 when the size of reduced vocab-
ulary is large. However, in the case of trigrams,
the perplexities of Theory3 are far from those of
Zipf3. This difference may be due to the sparse-
ness of trigrams in Zipf3. To verify the correct-
ness of our theory for higher order k-gram models,
we need to make assumptions that include backoff
and smoothing.
Fig. 1(f) plots the perplexity of LDA models
with 20 topics learned from Reuters, 20news,
Enwiki, Zipf1, and ZipfMix versus the size of
reduced vocabulary on a log-log graph. We used
a collapsed Gibbs sampler with 100 iterations to
infer the parameters and set the hyper parameters,
α = 0.1 and Q = 0.1. In evaluating the perplexity,
we estimated a posterior document-topic distribu-
</bodyText>
<page confidence="0.998499">
804
</page>
<tableCaption confidence="0.74617525">
Table 2: Computational time and memory size
for LDA learning on the original corpus, (1/10)-
reduced corpus, and (1/20)-reduced corpus of
Reuters.
</tableCaption>
<table confidence="0.9782225">
corpus time memory perplexity
original 4m3.80s 71,548KB 500
(1/10) 3m55.70s 46,648KB 550
(1/20) 3m42.63s 34,024KB 611
</table>
<bodyText confidence="0.982796442307692">
tion Bd by using the first half of each test document
and calculated the perplexity on the second half,
as is done in (Asuncion et al., 2009). Each value
is the average over different test sets of five-fold
cross validation. Theory1 and TheoryMix
are calculated using the formulae in Theorem 3
and Theorem 7, respectively. Comparing Zipf1
with Theory1, and ZipfMix with TheoryMix,
we find that our theory of the extreme cases
discussed in Section 3.3 is theoretically cor-
rect. TheoryAve is the average of Theory1
and TheoryMix. Comparing Reuters and
TheoryAve, we see that their curves are almost
the same. If theoretical perplexity PP has a
similar tendency as real perplexity PP on a
log-log graph, i.e., lnPP(W′) Pz� ln PP(W′) + c
for some constant c, we can approximate
its deterioration rate as PP(W′)/PP(W) �
exp (ln PP(W′) + c)/ exp (ln PP(W) + c) �
PP(W′)/ PP(W). Therefore, we can use
TheoryAve as a heuristic function for estimat-
ing the perplexity of topic models. Since we
can calculate an inverse of TheoryAve from
the bisection or Newton-Raphson method, we
can maximize the reduction rate and ensure an
acceptable perplexity based on a user-specified
deterioration rate. According to the fact that the
three real corpora with different sizes have a
similar tendency, it is expected that we can use
our theory for a larger corpus.
Finally, let us examine the computational costs
for LDA learning. Table 2 shows computa-
tional time and memory size for LDA learning
on the original corpus, (1/10)-reduced corpus, and
(1/20)-reduced corpus of Reuters. Comparing
the memory used in the learning with the origi-
nal corpus and with the (1/10)-reduced corpus of
Reuters, we find that the learning on the (1/10)-
reduced corpus used 60% of the memory used by
the learning on the original corpus. While the
computational time decreased a little, we believe
that reducing the memory size helps to reduce
computational time for a larger corpus in the sense
that it can relax the constraint for in-memory com-
puting. Although we did not examine the accuracy
of real tasks in this paper, there is an interesting
report that the word error rate of language mod-
els follows a power law with respect to perplexity
(Klakow and Peters, 2002). Thus, we conjecture
that the word error rate also has a similar tendency
as perplexity with respect to the reduced vocabu-
lary size.
</bodyText>
<sectionHeader confidence="0.998709" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999996771428571">
We studied the relationship between perplexity
and vocabulary size of reduced corpora. We de-
rived trade-off formulae for the perplexity of k-
gram models and topic models with respect to the
size of reduced vocabulary and showed that each
formula approximately has a simple behavior on a
log-log graph under certain conditions. We veri-
fied the correctness of our theory on synthetic cor-
pora and examined the gap between theory and
practice on real corpora. We found that the es-
timation of the perplexity growth rate is reason-
able. This means that we can maximize the reduc-
tion rate, thereby ensuring an acceptable perplex-
ity based on a user-specified deterioration rate.
Furthermore, this suggests the possibility that we
can theoretically derive empirical parameters, or
“rules of thumb”, for different NLP problems, as-
suming that a corpus follows Zipf’s law. We be-
lieve that our theoretical estimation has the advan-
tages of computational efficiency and scalability
especially for very large corpora, although exper-
imental estimations such as cross-validation may
be more accurate.
In the future, we want to find out the cause of
the gap between theory and practice and extend
our theory to bridge the gap, in the same way that
we can construct equations of motion with air re-
sistance in the example of the landing point of
a ball in Section 1. For example, promising re-
search directions include using a general law such
as the Zipf-Mandelbrot law (Mandelbrot, 1965), a
sophisticated model that cares the order of words
such as hierarchical Pitman-Yor processes (Wood
et al., 2011), and smoothing/backoff methods to
handle the sparseness problem.
</bodyText>
<sectionHeader confidence="0.999107" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9566115">
The author would like to thank the reviewers for
their helpful comments.
</bodyText>
<page confidence="0.99786">
805
</page>
<sectionHeader confidence="0.993776" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999932663551402">
Arthur Asuncion, Max Welling, Padhraic Smyth, and
Yee Whye Teh. 2009. On smoothing and infer-
ence for topic models. In Proceedings of the 25th
Conference on Uncertainty in Artificial Intelligence
(UAI2009), pages 27–34. AUAI Press.
Ioannis Atsonios, Olivier Beaumont, Nicolas Hanusse,
and Yusik Kim. 2011. On power-law distributed
balls in bins and its applications to view size esti-
mation. In Proceedings of the 22nd International
Symposium on Algorithms and Computation (ISAAC
2011), pages 504–513. Springer-Verlag.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
Shahar Boneh and Vassilis G. Papanicolaou. 1996.
General asymptotic estimates for the coupon collec-
tor problem. Journal of Computational and Applied
Mathematics, 67(2):277–289.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large Language
Models in Machine Translation. In Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL2007), pages 858–867.
ACL.
Adam L. Buchsbaum, Raffaele Giancarlo, and Jef-
fery R. Westbrook. 1998. Shrinking Language
Models by Robust Approximation. In Proceed-
ings of the 1998 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP
1998), pages 685–688.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th annual meet-
ing on Association for Computational Linguistics
(ACL 1996), pages 310–318. ACL.
Ken Church, Ted Hart, and Jianfeng Gao. 2007. Com-
pressing Trigram Language Models with Golomb
Coding. In Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning (EMNLP-CoNLL
2007), pages 199–207. ACL.
Aaron Clauset, Cosma Rohilla Shalizi, and M. E. J.
Newman. 2009. Power-Law Distributions in Em-
pirical Data. SIAMReview, 51(4):661–703.
Jianfeng Gao and Min Zhang. 2002. Improving Lan-
guage Model Size Reduction using Better Pruning
Criteria. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2002), pages 176–182. ACL.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2011. Producing Power-Law Distribu-
tions and Damping Word Frequencies with Two-
Stage Language Models. Journal ofMachine Learn-
ing Research, 12:2335–2382.
Joshua Goodman and Jianfeng Gao. 2000. Lan-
guage Model Size Reduction by Pruning and Clus-
tering. In Proceedings of the 6th International
Conference on Spoken Language Processing (ICSLP
2000), pages 110–113. ISCA.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. In Proceedings of the National
Academy ofSciences of the United States ofAmerica
(PNAS 2004), volume 101, pages 5228–5235.
Le Quan Ha, E. I. Sicilia-Garcia, Ji Ming, and F. J.
Smith. 2002. Extension of Zipf’s Law to Words and
Phrases. In Proceedings of the 19th International
Conference on Computational Linguistics (COLING
2002), pages 1–6. ACL.
Le Quan Ha, P. Hanna, D. W. Stewart, and F. J. Smith.
2006. Reduced n-gram models for English and Chi-
nese corpora. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, Proceedings of the Conference
(COLING-ACL 2006), pages 309–315. ACL.
Teemu Hirsimaki. 2007. On Compressing N-Gram
Language Models. In Proceedings of the 2007IEEE
International Conference on Acoustics, Speech and
Signal Processing (ICASSP 2007), pages 949–952.
Dietrich Klakow and Jochen Peters. 2002. Testing the
correlation of word error rate and perplexity. Speech
Communication, 38(1):19–28.
Hailong Li. 2005. On Generalized Euler Constants
and an Integral Related to the Piltz Divisor Problem.
˘Siauliai Mathematical Seminar, 8:81–93.
Benoit B. Mandelbrot. 1965. Information Theory
and Psycholinguistics: A Theory of Word Frequen-
cies. In Scientific Psychology: Principles and Ap-
proaches. Basic Books.
OEIS. 2001. The on-line encyclopedia of inte-
ger sequences (a061017). http://oeis.org/
A061017/.
Mark Steyvers and Tom Griffiths. 2007. Probabilis-
tic Topic Models. In Handbook of Latent Semantic
Analysis, pages 424–440. Lawrence Erlbaum Asso-
ciates.
Andreas Stolcke. 1998. Entropy-based Pruning of
Backoff Language Models. In Proceedings of the
DARPA Broadcast News Transcription and Under-
standing Workshop, pages 270–274.
Frank Wood, Jan Gasthaus, C´edric Archambeau,
Lancelot James, and Yee Whye Teh. 2011. The Se-
quence Memoizer. Communications of the Associa-
tion for Computing Machines, 54(2):91–98.
George Kingsley Zipf. 1935. The Psychobiology of
Language. Houghton-Mifflin.
</reference>
<page confidence="0.998816">
806
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.529887">
<title confidence="0.995868">Perplexity on Reduced Corpora</title>
<author confidence="0.931323">Yahoo Japan</author>
<address confidence="0.657202">9-7-1 Akasaka, Minato-ku, Tokyo 107-6211,</address>
<email confidence="0.947217">hakobaya@yahoo-corp.jp</email>
<abstract confidence="0.994473333333333">This paper studies the idea of removing low-frequency words from a corpus, which is a common practice to reduce computational costs, from a theoretical standpoint. Based on the assumption that a corpus follows Zipf’s law, we derive tradeformulae of the perplexity of models and topic models with respect to the size of the reduced vocabulary. In addition, we show an approximate behavior of each formula under certain conditions. We verify the correctness of our theory on synthetic corpora and examine the gap between theory and practice on real corpora.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Arthur Asuncion</author>
<author>Max Welling</author>
<author>Padhraic Smyth</author>
<author>Yee Whye Teh</author>
</authors>
<title>On smoothing and inference for topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (UAI2009),</booktitle>
<pages>27--34</pages>
<publisher>AUAI Press.</publisher>
<contexts>
<context position="30604" citStr="Asuncion et al., 2009" startWordPosition="5688" endWordPosition="5691">aph. We used a collapsed Gibbs sampler with 100 iterations to infer the parameters and set the hyper parameters, α = 0.1 and Q = 0.1. In evaluating the perplexity, we estimated a posterior document-topic distribu804 Table 2: Computational time and memory size for LDA learning on the original corpus, (1/10)- reduced corpus, and (1/20)-reduced corpus of Reuters. corpus time memory perplexity original 4m3.80s 71,548KB 500 (1/10) 3m55.70s 46,648KB 550 (1/20) 3m42.63s 34,024KB 611 tion Bd by using the first half of each test document and calculated the perplexity on the second half, as is done in (Asuncion et al., 2009). Each value is the average over different test sets of five-fold cross validation. Theory1 and TheoryMix are calculated using the formulae in Theorem 3 and Theorem 7, respectively. Comparing Zipf1 with Theory1, and ZipfMix with TheoryMix, we find that our theory of the extreme cases discussed in Section 3.3 is theoretically correct. TheoryAve is the average of Theory1 and TheoryMix. Comparing Reuters and TheoryAve, we see that their curves are almost the same. If theoretical perplexity PP has a similar tendency as real perplexity PP on a log-log graph, i.e., lnPP(W′) Pz� ln PP(W′) + c for som</context>
</contexts>
<marker>Asuncion, Welling, Smyth, Teh, 2009</marker>
<rawString>Arthur Asuncion, Max Welling, Padhraic Smyth, and Yee Whye Teh. 2009. On smoothing and inference for topic models. In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (UAI2009), pages 27–34. AUAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Atsonios</author>
<author>Olivier Beaumont</author>
<author>Nicolas Hanusse</author>
<author>Yusik Kim</author>
</authors>
<title>On power-law distributed balls in bins and its applications to view size estimation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 22nd International Symposium on Algorithms and Computation</booktitle>
<pages>504--513</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="18572" citStr="Atsonios et al., 2011" startWordPosition="3540" endWordPosition="3544"> a power law leads us to an additional convenient Hπk (W′) Hπk (W) , 801 property, since the process of generating a corpus in our theory can be treated as a variant of the coupon collector’s problem. In this problem, we consider how many trials are needed for collecting all coupons whose occurrence probabilities follow some stable distribution. According to a well-known result about power law distributions (Boneh and Papanicolaou, 1996), we need a corpus of size kW k 1−πk ln W when πk &lt; 1, and W ln2 W when πk = 1 for collecting all of the k-grams, the number of which is Wk. Using results in (Atsonios et al., 2011), we can easily obtain a lower and upper bound of the actual vocabulary size Wk of k-grams from the corpus size N and vocabulary size W as This means that we can determine the rough sparseness of k-grams and adjust some of the parameters such as the gram size k in learning statistical language models. 3.3 Perplexity of Topic Models In this section, we consider the perplexity of the widely used topic model, Latent Dirichlet Allocation (LDA) (Blei et al., 2003), by using the notation given in (Griffiths and Steyvers, 2004). LDA is a probabilistic language model that generates a corpus as a mixtu</context>
</contexts>
<marker>Atsonios, Beaumont, Hanusse, Kim, 2011</marker>
<rawString>Ioannis Atsonios, Olivier Beaumont, Nicolas Hanusse, and Yusik Kim. 2011. On power-law distributed balls in bins and its applications to view size estimation. In Proceedings of the 22nd International Symposium on Algorithms and Computation (ISAAC 2011), pages 504–513. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="19035" citStr="Blei et al., 2003" startWordPosition="3625" endWordPosition="3628"> kW k 1−πk ln W when πk &lt; 1, and W ln2 W when πk = 1 for collecting all of the k-grams, the number of which is Wk. Using results in (Atsonios et al., 2011), we can easily obtain a lower and upper bound of the actual vocabulary size Wk of k-grams from the corpus size N and vocabulary size W as This means that we can determine the rough sparseness of k-grams and adjust some of the parameters such as the gram size k in learning statistical language models. 3.3 Perplexity of Topic Models In this section, we consider the perplexity of the widely used topic model, Latent Dirichlet Allocation (LDA) (Blei et al., 2003), by using the notation given in (Griffiths and Steyvers, 2004). LDA is a probabilistic language model that generates a corpus as a mixture of hidden topics, and it allows us to infer two parameters: the document-topic distribution θ that represents the mixture rate of topics in each document, and the topic-word distribution ϕ that represents the occurrence rate of words in each topic. For a given corpus w, the model is defined as θdi — Dirichlet(α) zi|θdi — Multi(θdi) ϕzi — Dirichlet(β) wi|zi, ϕzi — Multi(ϕzi), where di and zi are respectively the document that includes the i-th word wi and t</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shahar Boneh</author>
<author>Vassilis G Papanicolaou</author>
</authors>
<title>General asymptotic estimates for the coupon collector problem.</title>
<date>1996</date>
<journal>Journal of Computational and Applied Mathematics,</journal>
<volume>67</volume>
<issue>2</issue>
<contexts>
<context position="18391" citStr="Boneh and Papanicolaou, 1996" startWordPosition="3497" endWordPosition="3500"> close to 1, i.e., k is relatively small and W is sufficiently large. Note that we must use the approximation of H(X), not Ha(X), when a = 1. The fact that the frequency of k-grams follows a power law leads us to an additional convenient Hπk (W′) Hπk (W) , 801 property, since the process of generating a corpus in our theory can be treated as a variant of the coupon collector’s problem. In this problem, we consider how many trials are needed for collecting all coupons whose occurrence probabilities follow some stable distribution. According to a well-known result about power law distributions (Boneh and Papanicolaou, 1996), we need a corpus of size kW k 1−πk ln W when πk &lt; 1, and W ln2 W when πk = 1 for collecting all of the k-grams, the number of which is Wk. Using results in (Atsonios et al., 2011), we can easily obtain a lower and upper bound of the actual vocabulary size Wk of k-grams from the corpus size N and vocabulary size W as This means that we can determine the rough sparseness of k-grams and adjust some of the parameters such as the gram size k in learning statistical language models. 3.3 Perplexity of Topic Models In this section, we consider the perplexity of the widely used topic model, Latent Di</context>
</contexts>
<marker>Boneh, Papanicolaou, 1996</marker>
<rawString>Shahar Boneh and Vassilis G. Papanicolaou. 1996. General asymptotic estimates for the coupon collector problem. Journal of Computational and Applied Mathematics, 67(2):277–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large Language Models in Machine Translation.</title>
<date>2007</date>
<booktitle>In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL2007),</booktitle>
<pages>858--867</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="1173" citStr="Brants et al., 2007" startWordPosition="184" endWordPosition="187"> each formula under certain conditions. We verify the correctness of our theory on synthetic corpora and examine the gap between theory and practice on real corpora. 1 Introduction Removing low-frequency words from a corpus (often called cutoff) is a common practice to save on the computational costs involved in learning language models and topic models. In the case of language models, we often have to remove low-frequency words because of a lack of computational resources, since the feature space of kgrams tends to be so large that we sometimes need cutoffs even in a distributed environment (Brants et al., 2007). In the case of topic models, the intuition is that low-frequency words do not make a large contribution to the statistics of the models. Actually, when we try to roughly analyze a corpus with topic models, a reduced corpus is enough for the purpose (Steyvers and Griffiths, 2007). A natural question arises: How many lowfrequency words can we remove while maintaining sufficient performance? Or more generally, by how much can we reduce a corpus/model using a certain strategy and still keep a sufficient level of performance? There have been many stud∗This work was mainly carried out while the au</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large Language Models in Machine Translation. In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL2007), pages 858–867. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Buchsbaum</author>
<author>Raffaele Giancarlo</author>
<author>Jeffery R Westbrook</author>
</authors>
<title>Shrinking Language Models by Robust Approximation.</title>
<date>1998</date>
<booktitle>In Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP</booktitle>
<pages>685--688</pages>
<contexts>
<context position="1913" citStr="Buchsbaum et al., 1998" startWordPosition="309" endWordPosition="312">stics of the models. Actually, when we try to roughly analyze a corpus with topic models, a reduced corpus is enough for the purpose (Steyvers and Griffiths, 2007). A natural question arises: How many lowfrequency words can we remove while maintaining sufficient performance? Or more generally, by how much can we reduce a corpus/model using a certain strategy and still keep a sufficient level of performance? There have been many stud∗This work was mainly carried out while the author was with Toshiba Corporation. ies addressing the question as it pertains to different strategies (Stolcke, 1998; Buchsbaum et al., 1998; Goodman and Gao, 2000; Gao and Zhang, 2002; Ha et al., 2006; Hirsimaki, 2007; Church et al., 2007). Each of these studies experimentally discusses trade-off relationships between the size of the reduced corpus/model and its performance measured by perplexity, word error rate, and other factors. To our knowledge, however, there is no theoretical study on the question and no evidence for such a trade-off relationship, especially for topic models. In this paper, we first address the question from a theoretical standpoint. We focus on the cutoff strategy for reducing a corpus, since a cutoff is </context>
</contexts>
<marker>Buchsbaum, Giancarlo, Westbrook, 1998</marker>
<rawString>Adam L. Buchsbaum, Raffaele Giancarlo, and Jeffery R. Westbrook. 1998. Shrinking Language Models by Robust Approximation. In Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 1998), pages 685–688.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th annual meeting on Association for Computational Linguistics (ACL</booktitle>
<pages>310--318</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="7464" citStr="Chen and Goodman, 1996" startWordPosition="1313" endWordPosition="1316">stribution p learned from the original corpus w. Here, we use constant restoring (defined below), which assumes the frequencies of the reduced lowfrequency words are a constant. Definition 1 (Constant Restoring). Given a positive constant λ, a distribution p′ over a reduced corpus w′, and a corpus w, we say that pˆ is a λ-restored distribution of p′ from w′ to w, if EwE{w} ˆp(w) = 1, and for any w ∈ w, � p′(w) (w ∈ w′) ˆp(w) ∝ λ (w ∈/ w′). Constant restoring is similar to the additive smoothing defined by ˆp(w) ∝ p′(w) + λ, which is used to solve the zero-frequency problem of language models (Chen and Goodman, 1996). The only difference is the addition of a constant λ only to zero-frequency words. We think constant restoring is theoretically natural in our setting, since we can derive the above equation by letting each frequency of reduced words be λN′ and defining a restored frequency function as follows: C � � � 1 . � 1 NT f(w) (w ∈ w′) PP := ˆf(w) = p(w) λN′ (w ∈/ w′). wEw� 798 799 Let (∏ ) ˆPP1 := 1 w∈w p(w) plexity ˆPP1 of the λ∗-restored distribution pˆ of p′ , H(W′) (W − W′ 1− H(W) H(W) − H(W′)) N be the perplexity 1 ˆPP1. stant λ∗ minimizing NR N 1 + WRλ∗ NR λ∗ N = (1 + N′) NR (N′ ) C N′ ( ) ∑ W′</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proceedings of the 34th annual meeting on Association for Computational Linguistics (ACL 1996), pages 310–318. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Church</author>
<author>Ted Hart</author>
<author>Jianfeng Gao</author>
</authors>
<title>Compressing Trigram Language Models with Golomb Coding.</title>
<date>2007</date>
<booktitle>In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</booktitle>
<pages>199--207</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="2013" citStr="Church et al., 2007" startWordPosition="327" endWordPosition="330">rpus is enough for the purpose (Steyvers and Griffiths, 2007). A natural question arises: How many lowfrequency words can we remove while maintaining sufficient performance? Or more generally, by how much can we reduce a corpus/model using a certain strategy and still keep a sufficient level of performance? There have been many stud∗This work was mainly carried out while the author was with Toshiba Corporation. ies addressing the question as it pertains to different strategies (Stolcke, 1998; Buchsbaum et al., 1998; Goodman and Gao, 2000; Gao and Zhang, 2002; Ha et al., 2006; Hirsimaki, 2007; Church et al., 2007). Each of these studies experimentally discusses trade-off relationships between the size of the reduced corpus/model and its performance measured by perplexity, word error rate, and other factors. To our knowledge, however, there is no theoretical study on the question and no evidence for such a trade-off relationship, especially for topic models. In this paper, we first address the question from a theoretical standpoint. We focus on the cutoff strategy for reducing a corpus, since a cutoff is simple but powerful method that is worth studying; as reported in (Goodman and Gao, 2000; Gao and Zh</context>
</contexts>
<marker>Church, Hart, Gao, 2007</marker>
<rawString>Ken Church, Ted Hart, and Jianfeng Gao. 2007. Compressing Trigram Language Models with Golomb Coding. In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL 2007), pages 199–207. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron Clauset</author>
<author>Cosma Rohilla Shalizi</author>
<author>M E J Newman</author>
</authors>
<date>2009</date>
<booktitle>Power-Law Distributions in Empirical Data. SIAMReview,</booktitle>
<pages>51--4</pages>
<contexts>
<context position="5278" citStr="Clauset et al., 2009" startWordPosition="913" endWordPosition="916"> y ∝ xc, and c is a real number. Zipf’s law (Zipf, 1935) is a power law discovered on real corpora, wherein for any word w ∈ w in a corpus w, its frequency (or word count) f(w) is inversely proportional to its frequency ranking r(w), i.e., f(w) = r(w). Here, f(w) := |{w′ ∈ w |w′ = w}|, and r(w) := |{w′ ∈ w |f(w′) ≥ f(w)}|. From the definition, the constant C is the maximum frequency in the corpus. Taking the natural logarithms ln(·) of both sides of the above equation, we find that its plot becomes linear on a log-log graph of r(w) and f(w). In fact, the result based on a statistical test in (Clauset et al., 2009) reports that the frequencies of words in a corpus completely follow a power law, whereas many datasets with long-tail properties, such as networks, actually do not follow power laws. 2.2 Perplexity Perplexity is a widely used evaluation measure of k-gram models and topic models. Let p be a predictive distribution over words, which was learned from a training corpus w based on a certain model. Formally, perplexity PP is defined as the geometric mean of the inverse of the per-word likelihood on the held-out test corpus wT, i.e., Intuitively, PP means how many possibilities one has for estimatin</context>
</contexts>
<marker>Clauset, Shalizi, Newman, 2009</marker>
<rawString>Aaron Clauset, Cosma Rohilla Shalizi, and M. E. J. Newman. 2009. Power-Law Distributions in Empirical Data. SIAMReview, 51(4):661–703.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Min Zhang</author>
</authors>
<title>Improving Language Model Size Reduction using Better Pruning Criteria.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>176--182</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="1957" citStr="Gao and Zhang, 2002" startWordPosition="317" endWordPosition="320">oughly analyze a corpus with topic models, a reduced corpus is enough for the purpose (Steyvers and Griffiths, 2007). A natural question arises: How many lowfrequency words can we remove while maintaining sufficient performance? Or more generally, by how much can we reduce a corpus/model using a certain strategy and still keep a sufficient level of performance? There have been many stud∗This work was mainly carried out while the author was with Toshiba Corporation. ies addressing the question as it pertains to different strategies (Stolcke, 1998; Buchsbaum et al., 1998; Goodman and Gao, 2000; Gao and Zhang, 2002; Ha et al., 2006; Hirsimaki, 2007; Church et al., 2007). Each of these studies experimentally discusses trade-off relationships between the size of the reduced corpus/model and its performance measured by perplexity, word error rate, and other factors. To our knowledge, however, there is no theoretical study on the question and no evidence for such a trade-off relationship, especially for topic models. In this paper, we first address the question from a theoretical standpoint. We focus on the cutoff strategy for reducing a corpus, since a cutoff is simple but powerful method that is worth stu</context>
</contexts>
<marker>Gao, Zhang, 2002</marker>
<rawString>Jianfeng Gao and Min Zhang. 2002. Improving Language Model Size Reduction using Better Pruning Criteria. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002), pages 176–182. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Producing Power-Law Distributions and Damping Word Frequencies with TwoStage Language Models.</title>
<date>2011</date>
<journal>Journal ofMachine Learning Research,</journal>
<pages>12--2335</pages>
<contexts>
<context position="20836" citStr="Goldwater et al., 2011" startWordPosition="3962" endWordPosition="3965"> reduce the complexity. The assumption placed on ϕ� is that the word distribution 0z of each topic z follows Zipf’s law. We think this is acceptable since we can regard each topic as a corpus that follows Zipf’s law. Since �ϕz is normalized for each topic, we can assume that for any two topics, z and z′, and any two words, w and w′, �ϕz(w) ,.J �z′(w′) holds if rz(w) = rz′(w′), where rz(w) is the frequency ranking of w with respect to n(w) z . Note that the above assumption pertains to a posterior, and we do not discuss the fact that a Pitman-Yor process prior is better suited for a power law (Goldwater et al., 2011). ϕ� may not be reasonThe assumption placed on able in the case of 0, because we can easily think of a document with only one topic, and we usually use a small number T of topics for LDA, e.g., T = 20. Thus, we consider two extreme cases. One is where each document evenly has all topics, and the other is where each document only has one topic. Although these two cases might be unrealistic, the actual (theoretical) perplexity is expected to be between their values. We believe that analyzing such extreme cases is theoretically important, since it would be useful for bounding the computational co</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2011</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2011. Producing Power-Law Distributions and Damping Word Frequencies with TwoStage Language Models. Journal ofMachine Learning Research, 12:2335–2382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
<author>Jianfeng Gao</author>
</authors>
<title>Language Model Size Reduction by Pruning and Clustering.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th International Conference on Spoken Language Processing (ICSLP</booktitle>
<pages>110--113</pages>
<publisher>ISCA.</publisher>
<contexts>
<context position="1936" citStr="Goodman and Gao, 2000" startWordPosition="313" endWordPosition="316">ually, when we try to roughly analyze a corpus with topic models, a reduced corpus is enough for the purpose (Steyvers and Griffiths, 2007). A natural question arises: How many lowfrequency words can we remove while maintaining sufficient performance? Or more generally, by how much can we reduce a corpus/model using a certain strategy and still keep a sufficient level of performance? There have been many stud∗This work was mainly carried out while the author was with Toshiba Corporation. ies addressing the question as it pertains to different strategies (Stolcke, 1998; Buchsbaum et al., 1998; Goodman and Gao, 2000; Gao and Zhang, 2002; Ha et al., 2006; Hirsimaki, 2007; Church et al., 2007). Each of these studies experimentally discusses trade-off relationships between the size of the reduced corpus/model and its performance measured by perplexity, word error rate, and other factors. To our knowledge, however, there is no theoretical study on the question and no evidence for such a trade-off relationship, especially for topic models. In this paper, we first address the question from a theoretical standpoint. We focus on the cutoff strategy for reducing a corpus, since a cutoff is simple but powerful met</context>
</contexts>
<marker>Goodman, Gao, 2000</marker>
<rawString>Joshua Goodman and Jianfeng Gao. 2000. Language Model Size Reduction by Pruning and Clustering. In Proceedings of the 6th International Conference on Spoken Language Processing (ICSLP 2000), pages 110–113. ISCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>In Proceedings of the National Academy ofSciences of the United States ofAmerica (PNAS 2004),</booktitle>
<volume>101</volume>
<pages>5228--5235</pages>
<contexts>
<context position="19098" citStr="Griffiths and Steyvers, 2004" startWordPosition="3636" endWordPosition="3639"> for collecting all of the k-grams, the number of which is Wk. Using results in (Atsonios et al., 2011), we can easily obtain a lower and upper bound of the actual vocabulary size Wk of k-grams from the corpus size N and vocabulary size W as This means that we can determine the rough sparseness of k-grams and adjust some of the parameters such as the gram size k in learning statistical language models. 3.3 Perplexity of Topic Models In this section, we consider the perplexity of the widely used topic model, Latent Dirichlet Allocation (LDA) (Blei et al., 2003), by using the notation given in (Griffiths and Steyvers, 2004). LDA is a probabilistic language model that generates a corpus as a mixture of hidden topics, and it allows us to infer two parameters: the document-topic distribution θ that represents the mixture rate of topics in each document, and the topic-word distribution ϕ that represents the occurrence rate of words in each topic. For a given corpus w, the model is defined as θdi — Dirichlet(α) zi|θdi — Multi(θdi) ϕzi — Dirichlet(β) wi|zi, ϕzi — Multi(ϕzi), where di and zi are respectively the document that includes the i-th word wi and the hidden topic that is assigned to wi. In the case of inferenc</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. In Proceedings of the National Academy ofSciences of the United States ofAmerica (PNAS 2004), volume 101, pages 5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Le Quan Ha</author>
<author>E I Sicilia-Garcia</author>
<author>Ji Ming</author>
<author>F J Smith</author>
</authors>
<title>Extension of Zipf’s Law to Words and Phrases.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics (COLING</booktitle>
<pages>1--6</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="15408" citStr="Ha et al., 2002" startWordPosition="2914" endWordPosition="2917">nd the main term of ℓPk(ln ℓ) is given by the ζk(s)xs following residue Ress=1 s , where ζ(s) is the Riemann zeta function (Li, 2005). Using this fact, we obtain an approximation ln (g−1 k (ℓ)) ≈ ln ℓ + O(ln (ln ℓ)) ≈ ln ℓ, when ℓ is sufficiently large. Thus, when the corpus is sufficiently large, we can see that the behavior of fk is roughly linear on a log-log graph, i.e., fk(wk) ∝ rk(wk)−1, since if g−1 k (ℓ) ∝ ℓc holds, then fk(r) ∝ (gk(r))−1 ∝ 1 r− c holds. Unfortunately, however, most corpora in the real world are not so large that the abovementioned relation holds. Actually, Ha et al. (Ha et al., 2002; Ha et al., 2006) experimentally found that although a k-gram corpus roughly follows a power law even when k &gt; 1, its exponent is smaller than 1 (for Zipf’s law). They pointed out that the exponent of bigrams is about 0.66, and that of 5-grams is about 0.59 in the Wall Street Journal corpus (WSJ87). Believing their claim that there exists a constant πk such that fk(wk) ∝ rk(wk)−πk, we estimated the exponent of k-grams in an actual situation in the form of the following lemma. Lemma 5. Assuming that fk(wk) ∝ rk(wk)−πk holds for any k-gram word wk ∈ wk in a corpus w following Zipf’s law, the op</context>
</contexts>
<marker>Ha, Sicilia-Garcia, Ming, Smith, 2002</marker>
<rawString>Le Quan Ha, E. I. Sicilia-Garcia, Ji Ming, and F. J. Smith. 2002. Extension of Zipf’s Law to Words and Phrases. In Proceedings of the 19th International Conference on Computational Linguistics (COLING 2002), pages 1–6. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Le Quan Ha</author>
<author>P Hanna</author>
<author>D W Stewart</author>
<author>F J Smith</author>
</authors>
<title>Reduced n-gram models for English and Chinese corpora.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (COLING-ACL</booktitle>
<pages>309--315</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="1974" citStr="Ha et al., 2006" startWordPosition="321" endWordPosition="324">us with topic models, a reduced corpus is enough for the purpose (Steyvers and Griffiths, 2007). A natural question arises: How many lowfrequency words can we remove while maintaining sufficient performance? Or more generally, by how much can we reduce a corpus/model using a certain strategy and still keep a sufficient level of performance? There have been many stud∗This work was mainly carried out while the author was with Toshiba Corporation. ies addressing the question as it pertains to different strategies (Stolcke, 1998; Buchsbaum et al., 1998; Goodman and Gao, 2000; Gao and Zhang, 2002; Ha et al., 2006; Hirsimaki, 2007; Church et al., 2007). Each of these studies experimentally discusses trade-off relationships between the size of the reduced corpus/model and its performance measured by perplexity, word error rate, and other factors. To our knowledge, however, there is no theoretical study on the question and no evidence for such a trade-off relationship, especially for topic models. In this paper, we first address the question from a theoretical standpoint. We focus on the cutoff strategy for reducing a corpus, since a cutoff is simple but powerful method that is worth studying; as reporte</context>
<context position="15426" citStr="Ha et al., 2006" startWordPosition="2918" endWordPosition="2921">of ℓPk(ln ℓ) is given by the ζk(s)xs following residue Ress=1 s , where ζ(s) is the Riemann zeta function (Li, 2005). Using this fact, we obtain an approximation ln (g−1 k (ℓ)) ≈ ln ℓ + O(ln (ln ℓ)) ≈ ln ℓ, when ℓ is sufficiently large. Thus, when the corpus is sufficiently large, we can see that the behavior of fk is roughly linear on a log-log graph, i.e., fk(wk) ∝ rk(wk)−1, since if g−1 k (ℓ) ∝ ℓc holds, then fk(r) ∝ (gk(r))−1 ∝ 1 r− c holds. Unfortunately, however, most corpora in the real world are not so large that the abovementioned relation holds. Actually, Ha et al. (Ha et al., 2002; Ha et al., 2006) experimentally found that although a k-gram corpus roughly follows a power law even when k &gt; 1, its exponent is smaller than 1 (for Zipf’s law). They pointed out that the exponent of bigrams is about 0.66, and that of 5-grams is about 0.59 in the Wall Street Journal corpus (WSJ87). Believing their claim that there exists a constant πk such that fk(wk) ∝ rk(wk)−πk, we estimated the exponent of k-grams in an actual situation in the form of the following lemma. Lemma 5. Assuming that fk(wk) ∝ rk(wk)−πk holds for any k-gram word wk ∈ wk in a corpus w following Zipf’s law, the optimal exponent in </context>
</contexts>
<marker>Ha, Hanna, Stewart, Smith, 2006</marker>
<rawString>Le Quan Ha, P. Hanna, D. W. Stewart, and F. J. Smith. 2006. Reduced n-gram models for English and Chinese corpora. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (COLING-ACL 2006), pages 309–315. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teemu Hirsimaki</author>
</authors>
<title>On Compressing N-Gram Language Models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP</booktitle>
<pages>949--952</pages>
<contexts>
<context position="1991" citStr="Hirsimaki, 2007" startWordPosition="325" endWordPosition="326">els, a reduced corpus is enough for the purpose (Steyvers and Griffiths, 2007). A natural question arises: How many lowfrequency words can we remove while maintaining sufficient performance? Or more generally, by how much can we reduce a corpus/model using a certain strategy and still keep a sufficient level of performance? There have been many stud∗This work was mainly carried out while the author was with Toshiba Corporation. ies addressing the question as it pertains to different strategies (Stolcke, 1998; Buchsbaum et al., 1998; Goodman and Gao, 2000; Gao and Zhang, 2002; Ha et al., 2006; Hirsimaki, 2007; Church et al., 2007). Each of these studies experimentally discusses trade-off relationships between the size of the reduced corpus/model and its performance measured by perplexity, word error rate, and other factors. To our knowledge, however, there is no theoretical study on the question and no evidence for such a trade-off relationship, especially for topic models. In this paper, we first address the question from a theoretical standpoint. We focus on the cutoff strategy for reducing a corpus, since a cutoff is simple but powerful method that is worth studying; as reported in (Goodman and</context>
</contexts>
<marker>Hirsimaki, 2007</marker>
<rawString>Teemu Hirsimaki. 2007. On Compressing N-Gram Language Models. In Proceedings of the 2007IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2007), pages 949–952.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dietrich Klakow</author>
<author>Jochen Peters</author>
</authors>
<title>Testing the correlation of word error rate and perplexity.</title>
<date>2002</date>
<journal>Speech Communication,</journal>
<volume>38</volume>
<issue>1</issue>
<contexts>
<context position="32694" citStr="Klakow and Peters, 2002" startWordPosition="6038" endWordPosition="6041">the original corpus and with the (1/10)-reduced corpus of Reuters, we find that the learning on the (1/10)- reduced corpus used 60% of the memory used by the learning on the original corpus. While the computational time decreased a little, we believe that reducing the memory size helps to reduce computational time for a larger corpus in the sense that it can relax the constraint for in-memory computing. Although we did not examine the accuracy of real tasks in this paper, there is an interesting report that the word error rate of language models follows a power law with respect to perplexity (Klakow and Peters, 2002). Thus, we conjecture that the word error rate also has a similar tendency as perplexity with respect to the reduced vocabulary size. 5 Conclusion We studied the relationship between perplexity and vocabulary size of reduced corpora. We derived trade-off formulae for the perplexity of kgram models and topic models with respect to the size of reduced vocabulary and showed that each formula approximately has a simple behavior on a log-log graph under certain conditions. We verified the correctness of our theory on synthetic corpora and examined the gap between theory and practice on real corpora</context>
</contexts>
<marker>Klakow, Peters, 2002</marker>
<rawString>Dietrich Klakow and Jochen Peters. 2002. Testing the correlation of word error rate and perplexity. Speech Communication, 38(1):19–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hailong Li</author>
</authors>
<title>On Generalized Euler Constants and an Integral Related to the Piltz Divisor Problem. ˘Siauliai Mathematical Seminar,</title>
<date>2005</date>
<pages>8--81</pages>
<contexts>
<context position="14926" citStr="Li, 2005" startWordPosition="2821" endWordPosition="2822">ctor (1, · · · , W), we can calculate the maximum frequency Ck as follows. . H(δW) 1− H(W) − ln δ ln W 800 Lemma 4. For any corpus w following Zipf’s law, the maximum frequency of k-grams in our model is calculated by N − (k − 1)D Ck = (H(W))k , where D denotes the number of documents in w. Proof. We use ∑wk fk(wk) = Ck(∑w 1/r(w))k. The sum Sk(ℓ) of Piltz’s divisor functions can be approximated by ℓPk(ln ℓ), where Pk(x) is a polynomial of degree k − 1 with respect to x, and the main term of ℓPk(ln ℓ) is given by the ζk(s)xs following residue Ress=1 s , where ζ(s) is the Riemann zeta function (Li, 2005). Using this fact, we obtain an approximation ln (g−1 k (ℓ)) ≈ ln ℓ + O(ln (ln ℓ)) ≈ ln ℓ, when ℓ is sufficiently large. Thus, when the corpus is sufficiently large, we can see that the behavior of fk is roughly linear on a log-log graph, i.e., fk(wk) ∝ rk(wk)−1, since if g−1 k (ℓ) ∝ ℓc holds, then fk(r) ∝ (gk(r))−1 ∝ 1 r− c holds. Unfortunately, however, most corpora in the real world are not so large that the abovementioned relation holds. Actually, Ha et al. (Ha et al., 2002; Ha et al., 2006) experimentally found that although a k-gram corpus roughly follows a power law even when k &gt; 1, its</context>
</contexts>
<marker>Li, 2005</marker>
<rawString>Hailong Li. 2005. On Generalized Euler Constants and an Integral Related to the Piltz Divisor Problem. ˘Siauliai Mathematical Seminar, 8:81–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit B Mandelbrot</author>
</authors>
<title>Information Theory and Psycholinguistics: A Theory of Word Frequencies. In Scientific Psychology: Principles and Approaches.</title>
<date>1965</date>
<publisher>Basic Books.</publisher>
<marker>Mandelbrot, 1965</marker>
<rawString>Benoit B. Mandelbrot. 1965. Information Theory and Psycholinguistics: A Theory of Word Frequencies. In Scientific Psychology: Principles and Approaches. Basic Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>OEIS</author>
</authors>
<title>The on-line encyclopedia of integer sequences (a061017). http://oeis.org/ A061017/.</title>
<date>2001</date>
<contexts>
<context position="14099" citStr="OEIS, 2001" startWordPosition="2656" endWordPosition="2657">corpora. The frequency fk of k-gram word wk ∈ wk in the model is represented by the following formula: fk(wk) = gk(rk(wk)), Ck where Ck is the maximal frequency in k-grams, rk is the frequency ranking of wk over k-grams, and gk expresses the frequency decay in k-grams. For example, the decay function g2 of bigrams is as follows: (g2(i))i := (g2(1), g2(2), g2(3),···) = (1 · 1, 1 · 2, 2 · 1, 1 · 3, 3 · 1, · · · ) = (1, 2, 2, 3, 3, 4, 4, 4, 5, 5, 6,·· · ). This is an inverse of the sum of Piltz’s divisor functions d2(n) := Ei1·i2=n 1, which represents the number of divisors of an integer n (cf. (OEIS, 2001)). In general, we formally define gk through its inverse: g−1 k (E) := 5k(E), where 5k(E) := Eℓ dk(n) and dk(n) := Ei1·i2···ik=n 1. Since n=1 (gk(i))i is a sorted sequence of the elements of the k-th tensor power of vector (1, · · · , W), we can calculate the maximum frequency Ck as follows. . H(δW) 1− H(W) − ln δ ln W 800 Lemma 4. For any corpus w following Zipf’s law, the maximum frequency of k-grams in our model is calculated by N − (k − 1)D Ck = (H(W))k , where D denotes the number of documents in w. Proof. We use ∑wk fk(wk) = Ck(∑w 1/r(w))k. The sum Sk(ℓ) of Piltz’s divisor functions can </context>
</contexts>
<marker>OEIS, 2001</marker>
<rawString>OEIS. 2001. The on-line encyclopedia of integer sequences (a061017). http://oeis.org/ A061017/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steyvers</author>
<author>Tom Griffiths</author>
</authors>
<title>Probabilistic Topic Models.</title>
<date>2007</date>
<booktitle>In Handbook of Latent Semantic Analysis,</booktitle>
<pages>424--440</pages>
<contexts>
<context position="1454" citStr="Steyvers and Griffiths, 2007" startWordPosition="233" endWordPosition="236">ve on the computational costs involved in learning language models and topic models. In the case of language models, we often have to remove low-frequency words because of a lack of computational resources, since the feature space of kgrams tends to be so large that we sometimes need cutoffs even in a distributed environment (Brants et al., 2007). In the case of topic models, the intuition is that low-frequency words do not make a large contribution to the statistics of the models. Actually, when we try to roughly analyze a corpus with topic models, a reduced corpus is enough for the purpose (Steyvers and Griffiths, 2007). A natural question arises: How many lowfrequency words can we remove while maintaining sufficient performance? Or more generally, by how much can we reduce a corpus/model using a certain strategy and still keep a sufficient level of performance? There have been many stud∗This work was mainly carried out while the author was with Toshiba Corporation. ies addressing the question as it pertains to different strategies (Stolcke, 1998; Buchsbaum et al., 1998; Goodman and Gao, 2000; Gao and Zhang, 2002; Ha et al., 2006; Hirsimaki, 2007; Church et al., 2007). Each of these studies experimentally di</context>
</contexts>
<marker>Steyvers, Griffiths, 2007</marker>
<rawString>Mark Steyvers and Tom Griffiths. 2007. Probabilistic Topic Models. In Handbook of Latent Semantic Analysis, pages 424–440. Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Entropy-based Pruning of Backoff Language Models.</title>
<date>1998</date>
<booktitle>In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop,</booktitle>
<pages>270--274</pages>
<contexts>
<context position="1889" citStr="Stolcke, 1998" startWordPosition="307" endWordPosition="308">on to the statistics of the models. Actually, when we try to roughly analyze a corpus with topic models, a reduced corpus is enough for the purpose (Steyvers and Griffiths, 2007). A natural question arises: How many lowfrequency words can we remove while maintaining sufficient performance? Or more generally, by how much can we reduce a corpus/model using a certain strategy and still keep a sufficient level of performance? There have been many stud∗This work was mainly carried out while the author was with Toshiba Corporation. ies addressing the question as it pertains to different strategies (Stolcke, 1998; Buchsbaum et al., 1998; Goodman and Gao, 2000; Gao and Zhang, 2002; Ha et al., 2006; Hirsimaki, 2007; Church et al., 2007). Each of these studies experimentally discusses trade-off relationships between the size of the reduced corpus/model and its performance measured by perplexity, word error rate, and other factors. To our knowledge, however, there is no theoretical study on the question and no evidence for such a trade-off relationship, especially for topic models. In this paper, we first address the question from a theoretical standpoint. We focus on the cutoff strategy for reducing a co</context>
</contexts>
<marker>Stolcke, 1998</marker>
<rawString>Andreas Stolcke. 1998. Entropy-based Pruning of Backoff Language Models. In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, pages 270–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Wood</author>
<author>Jan Gasthaus</author>
<author>C´edric Archambeau</author>
<author>Lancelot James</author>
<author>Yee Whye Teh</author>
</authors>
<title>The Sequence Memoizer.</title>
<date>2011</date>
<journal>Communications of the Association for Computing Machines,</journal>
<volume>54</volume>
<issue>2</issue>
<contexts>
<context position="28658" citStr="Wood et al., 2011" startWordPosition="5352" endWordPosition="5355">eoryGrad is calculated using Irk in Lemma 5. (e) Perplexity of k-gram models learned from Reuters versus size of reduced vocabulary. Theory2 and Theory3 are calculated using the formula in Corollary 6. (f) Perplexity of topic models learned from Reuters, 20news, Enwiki, Zipf1, and ZipfMix versus size of reduced vocabulary. TheoryMix is calculated using the formula in Theorem 7. quences in our simple model. The frequencies of high-order k-grams tend to be lower than in reality. We might need to place a hierarchical assumption on the a power law, as in done in hierarchical Pitman-Yor processes (Wood et al., 2011). Fig. 1(d) plots the exponent of the power law over k-grams in Reuters versus the gram size on a normal graph. We estimated each exponent of Reuters by using the least-squares method. TheoryGrad is calculated using Irk in Lemma 5. Surprisingly, the real exponents of Reuters are almost the same as the theoretical estimate Irk based on our “stupid” model that does not care about the order of words. Note that we do not use any information other than the vocabulary size W and the gram size k for estimating Irk. Fig. 1(e) plots the perplexity of k-gram models (k E 11, 2,31) learned from Reuters ve</context>
</contexts>
<marker>Wood, Gasthaus, Archambeau, James, Teh, 2011</marker>
<rawString>Frank Wood, Jan Gasthaus, C´edric Archambeau, Lancelot James, and Yee Whye Teh. 2011. The Sequence Memoizer. Communications of the Association for Computing Machines, 54(2):91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Kingsley Zipf</author>
</authors>
<date>1935</date>
<journal>The Psychobiology of Language. Houghton-Mifflin.</journal>
<contexts>
<context position="2765" citStr="Zipf, 1935" startWordPosition="453" endWordPosition="454">sured by perplexity, word error rate, and other factors. To our knowledge, however, there is no theoretical study on the question and no evidence for such a trade-off relationship, especially for topic models. In this paper, we first address the question from a theoretical standpoint. We focus on the cutoff strategy for reducing a corpus, since a cutoff is simple but powerful method that is worth studying; as reported in (Goodman and Gao, 2000; Gao and Zhang, 2002), a cutoff is competitive with sophisticated strategies such as entropy pruning. As the basis of our theory, we assume Zipf’s law (Zipf, 1935), which is an empirical rule representing a long-tail property of words in a corpus. Our approach is essentially the same as those in physics, in the sense of constructing a theory while believing experimentally observed results. For example, we can derive the distance to the landing point of a ball thrown up in the air with initial speed vo and angle 0 as vol sin(20)/g by believing in the experimentally observed gravity acceleration g. In a similar fashion, we will try to clarify the trade-off relationship by believing Zipf’s law. The rest of the paper is organized as follows. In Section 2, w</context>
<context position="4713" citStr="Zipf, 1935" startWordPosition="804" endWordPosition="805">tice on several real corpora. Section 5 concludes the paper. 2 Preliminaries Let us consider a corpus w := w1 · · · wN of corpus size N and vocabulary size W. We use an abridged notation {w} := {w ∈ w} to represent the vocabulary of w. Clearly, N = |w |and W = |{w} |hold. When w has additional notations, N and W inherit them. For example, we will use N′ as the size of w′ without its definition. 2.1 Power law and Zipf’s law A power law is a mathematical relationship between two quantities x and y, where y is proportional to the c-th power of x, i.e., y ∝ xc, and c is a real number. Zipf’s law (Zipf, 1935) is a power law discovered on real corpora, wherein for any word w ∈ w in a corpus w, its frequency (or word count) f(w) is inversely proportional to its frequency ranking r(w), i.e., f(w) = r(w). Here, f(w) := |{w′ ∈ w |w′ = w}|, and r(w) := |{w′ ∈ w |f(w′) ≥ f(w)}|. From the definition, the constant C is the maximum frequency in the corpus. Taking the natural logarithms ln(·) of both sides of the above equation, we find that its plot becomes linear on a log-log graph of r(w) and f(w). In fact, the result based on a statistical test in (Clauset et al., 2009) reports that the frequencies of wo</context>
</contexts>
<marker>Zipf, 1935</marker>
<rawString>George Kingsley Zipf. 1935. The Psychobiology of Language. Houghton-Mifflin.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>