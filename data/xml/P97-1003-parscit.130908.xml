<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.815858">
Three Generative, Lexicalised Models for Statistical Parsing
</title>
<author confidence="0.87524">
Michael Collins*
</author>
<affiliation confidence="0.998539">
Dept. of Computer and Information Science
University of Pennsylvania
</affiliation>
<address confidence="0.67834">
Philadelphia, PA, 19104, U.S.A.
</address>
<email confidence="0.721084">
mcollins@gradient . cis .upenn.edu
</email>
<sectionHeader confidence="0.982962" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9987885">
In this paper we first propose a new sta-
tistical parsing model, which is a genera-
tive model of lexicalised context-free gram-
mar. We then extend the model to in-
clude a probabilistic treatment of both sub-
categorisation and wh-movement. Results
on Wall Street Journal text show that the
parser performs at 88.1/87.5% constituent
precision/recall, an average improvement
of 2.3% over (Collins 96).
</bodyText>
<sectionHeader confidence="0.997894" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999914115384615">
Generative models of syntax have been central in
linguistics since they were introduced in (Chom-
sky 57). Each sentence-tree pair (S, T) in a lan-
guage has an associated top-down derivation con-
sisting of a sequence of rule applications of a gram-
mar. These models can be extended to be statisti-
cal by defining probability distributions at points of
non-determinism in the derivations, thereby assign-
ing a probability &apos;P(S,T) to each (S, T) pair. Proba-
bilistic context free grammar (Booth and Thompson
73) was an early example of a statistical grammar.
A PCFG can be lexicalised by associating a head-
word with each non-terminal in a parse tree; thus
far, (Magerman 95; Jelinek et al. 94) and (Collins
96), which both make heavy use of lexical informa-
tion, have reported the best statistical parsing per-
formance on Wall Street Journal text. Neither of
these models is generative, instead they both esti-
mate &apos;P(T 1 S) directly.
This paper proposes three new parsing models.
Model 1 is essentially a generative version of the
model described in (Collins 96). In Model 2, we
extend the parser to make the complement/adjunct
distinction by adding probabilities over subcategori-
sation frames for head-words. In Model 3 we give
a probabilistic treatment of wh-movement, which
</bodyText>
<footnote confidence="0.5262635">
This research was supported by ARPA Grant
N6600194-C6043.
</footnote>
<bodyText confidence="0.999838352941177">
is derived from the analysis given in Generalized
Phrase Structure Grammar (Gazdar et al. 95). The
work makes two advances over previous models:
First, Model 1 performs significantly better than
(Collins 96), and Models 2 and 3 give further im-
provements — our final results are 88.1/87.5% con-
stituent precision/recall, an average improvement
of 2.3% over (Collins 96). Second, the parsers
in (Collins 96) and (NIagerman 95; Jelinek et al.
94) produce trees without information about wh-
movement or subcategorisation. Most NLP applica-
tions will need this information to extract predicate-
argument structure from parse trees.
In the remainder of this paper we describe the 3
models in section 2, discuss practical issues in sec-
tion 3, give results in section 4, and give conclusions
in section 5.
</bodyText>
<sectionHeader confidence="0.961314" genericHeader="method">
2 The Three Parsing Models
</sectionHeader>
<subsectionHeader confidence="0.948869">
2.1 Model 1
</subsectionHeader>
<bodyText confidence="0.996809428571429">
In general, a statistical parsing model defines the
conditional probability, P(T S), for each candidate
parse tree T for a sentence S. The parser itself is
an algorithm which searches for the tree, Tb„t, that
maximises &apos;P(T 1 S). A generative model uses the
observation that maximising P(T, S) is equivalent
to maximising P(T I S): 1
</bodyText>
<equation confidence="0.7830985">
arg max P(T S) = arg max P(T, S)
P(S)
= arg max 7)(T, S) (1)
P(T, S) is then estimated by attaching probabilities
</equation>
<bodyText confidence="0.953682">
to a top-down derivation of the tree. In a PCFG,
for a tree derived by n applications of context-free
re-write rules LH Si RHS, 1 i n,
</bodyText>
<equation confidence="0.977662">
P(T, S) = H P(RHS, I LHS) (2)
</equation>
<bodyText confidence="0.984562">
The re-write rules are either internal to the tree,
where LHS is a non-terminal and RHS is a string
</bodyText>
<equation confidence="0.9121208">
P(T,S)
1P(S) is constant, hence maximising i
p(s) s equiv-
alent to maximising P(T, S).
Tbest =7-
</equation>
<page confidence="0.963857">
16
</page>
<figure confidence="0.984669166666667">
TOP
S(bought)
NP (week NP(Marks) VP(bought)
JJ NN NNP VB NP(Brooks)
Last week Marks bought NNP
Brooks
TOP -&gt; S (bought)
S (bought) -&gt; NP (week) NP (Marks) VP (bought)
NP (week) -&gt; JJ (Last ) NN (week)
NP (Marks) -&gt; NNP (Marks)
VP (bought) -&gt; VB (bought) NP (Brooks)
NP (Brooks) -&gt; NNP (Brooks)
</figure>
<figureCaption confidence="0.998693">
Figure 1: A lexicalised parse tree, and a list of the rules it contains. For brevity we omit the POS tag
associated with each word.
</figureCaption>
<bodyText confidence="0.955115142857143">
of one or more non-terminals; or lexical, where LHS
is a part of speech tag and RHS is a word.
A PCFG can be lexicalised2 by associating a word
w and a part-of-speech (POS) tag t with each non-
terminal X in the tree. Thus we write a non-
terminal as X(x), where x = (w,t), and X is a
constituent label. Each rule now has the form3:
</bodyText>
<equation confidence="0.996961">
P(h) Ln(ln)...Li(li)H(h)Ri(n.)•••Rm(rm) (3)
</equation>
<bodyText confidence="0.998374133333333">
H is the head-child of the phrase, which inherits
the head-word h from its parent P. L1...L7, and
are left and right modifiers of H. Either
n or m may be zero, and n = m = 0 for unary
rules. Figure 1 shows a tree which will be used as
an example throughout this paper.
The addition of lexical heads leads to an enormous
number of potential rules, making direct estimation
of P(RHS I LHS) infeasible because of sparse data
problems. We decompose the generation of the RHS
of a rule such as (3), given the LHS, into three steps
— first generating the head, then making the inde-
pendence assumptions that the left and right mod-
ifiers are generated by separate 0th-order markov
processes 4. .
</bodyText>
<listItem confidence="0.91736275">
1. Generate the head constituent label of the
phrase, with probability PH(H I P, h).
2. Generate modifiers to the right of the head
with probability Hz=1..,,±1 PR(R2(r,) I P, h, H).
R„,44 (r,,,..Fi) is defined as STOP — the STOP
symbol is added to the vocabulary of non-
terminals, and the model stops generating right
modifiers when it is generated.
</listItem>
<footnote confidence="0.981126428571429">
2We find lexical heads in Penn treebank data using
rules which are similar to those used by (Magerman 95;
Jelinek et al. 94).
3With the exception of the top rule in the tree, which
has the form TOP H(h).
4An exception is the first rule in the tree, TOP
H(h) , which has probability PTOp(11,h1TOP)
</footnote>
<listItem confidence="0.8086315">
3. Generate modifiers to the left of the head with
probability 7
</listItem>
<equation confidence="0.998539">
-i=1...+1 (L,(1i) I P,h, H), where
Lri+i (1n-1-1) = STOP.
</equation>
<bodyText confidence="0.812913666666667">
For example, the probability of the rule S (bought)
-&gt; NP (week) NP (Marks) VP (bought) would be es-
timated as
</bodyText>
<equation confidence="0.9614325">
POP I S , bought) X Pi (NP (Marks) I S , VP , bought) x
(NP (week) I S , VP , bought) x Pi(STOP , VP ,bought) x
Pr (STOP I S , VP , bought)
We have made the 0th order markov assumptions
Pai(ii) I H,P,h,
Pi(Li(li) I H, P, (4)
Pr(Ri(ri) I H, P, h, =
Pr(Ri(ri) I H, P,h) (5)
</equation>
<bodyText confidence="0.999972375">
but in general the probabilities could be conditioned
on any of the preceding modifiers. In fact, if the
derivation order is fixed to be depth-first — that
is, each modifier recursively generates the sub-tree
below it before the next modifier is generated —
then the model can also condition on any structure
below the preceding modifiers. For the moment we
exploit this by making the approximations
</bodyText>
<equation confidence="0.9998725">
Pi(Li(ii)I H, P,h, Li(li)...L,--1(1,-1)) =
P1(Li(l2) I H, h, distancei(i - 1)) (6)
(Ri(ri) I (ri)•••Ri-i(ri--1)) =
Pr(Ri(ri) I H, P. h,distancer(i -1)) (7)
</equation>
<bodyText confidence="0.999770285714286">
where distancei and distance,. are functions of the
surface string from the head word to the edge of the
constituent (see figure 2). The distance measure is
the same as in (Collins 96), a vector with the fol-
lowing 3 elements: (1) is the string of zero length?
(Allowing the model to learn a preference for right-
branching structures); (2) does the string contain a
</bodyText>
<page confidence="0.998295">
17
</page>
<bodyText confidence="0.99965925">
verb? (Allowing the model to learn a preference for
modification of the most recent verb). (3) Does the
string contain 0, 1, 2 or &gt; 2 commas? (where a
comma is anything tagged as &amp;quot;,&amp;quot; or
</bodyText>
<equation confidence="0.931597666666667">
P(h)
R 1(rl) R2(r2) R3(r3)
distance
</equation>
<figureCaption confidence="0.985503">
Figure 2: The next child, R3(r3), is generated with
</figureCaption>
<bodyText confidence="0.855292">
probability P(R3(r3) I P, H, h, distance, (2)). The
distance is a function of the surface string from the
word after h to the last word of R2, inclusive. In
principle the model could condition on any struc-
ture dominated by H, R1 or R2.
</bodyText>
<subsectionHeader confidence="0.894875">
2.2 Model 2: The complement/adjunct
</subsectionHeader>
<bodyText confidence="0.9814145">
distinction and subcategorisation
The tree in figure 1 is an example of the importance
of the complement/adjunct distinction. It would be
useful to identify &amp;quot;Marks&amp;quot; as a subject, and &amp;quot;Last
week&amp;quot; as an adjunct (temporal modifier), but this
distinction is not made in the tree, as both NPs are
in the same position&apos; (sisters to a VP under an S
node). From here on we will identify complements
by attaching a &amp;quot;-C&amp;quot; suffix to non-terminals — fig-
ure 3 gives an example tree.
</bodyText>
<figure confidence="0.910338">
VBD NP-C(Brooks)
bought Brooks
</figure>
<figureCaption confidence="0.85711325">
Figure 3: A tree with the &amp;quot;-C&amp;quot; suffix used to identify
complements. &amp;quot;Marks&amp;quot; and &amp;quot;Brooks&amp;quot; are in subject
and object position respectively. &amp;quot;Last week&amp;quot; is an
adjunct.
</figureCaption>
<bodyText confidence="0.9999698">
A post-processing stage could add this detail to
the parser output, but we give two reasons for mak-
ing the distinction while parsing: First, identifying
complements is complex enough to warrant a prob-
abilistic treatment. Lexical information is needed
</bodyText>
<footnote confidence="0.860059">
5Except &amp;quot;Marks&amp;quot; is closer to the VP, but note that
&amp;quot;Marks&amp;quot; is also the subject in &amp;quot;Marks last week bought
Brooks&amp;quot;.
</footnote>
<bodyText confidence="0.99993475">
— for example, knowledge that &amp;quot;week&amp;quot; is likely to
be a temporal modifier. Knowledge about subcat-
egorisation preferences — for example that a verb
takes exactly one subject — is also required. These
problems are not restricted to NPs, compare &amp;quot;The
spokeswoman said (SBAR that the asbestos was
dangerous)&amp;quot; vs. &amp;quot;Bonds beat short-term invest-
ments (SBAR because the market is down)&amp;quot;, where
an SBAR headed by &amp;quot;that&amp;quot; is a complement, but an
SBAR headed by &amp;quot;because&amp;quot; is an adjunct.
The second reason for making the comple-
ment/adjunct distinction while parsing is that it
may help parsing accuracy. The assumption that
complements are generated independently of each
other often leads to incorrect parses — see figure 4
for further explanation.
</bodyText>
<subsubsectionHeader confidence="0.511996">
2.2.1 Identifying Complements and
</subsubsectionHeader>
<bodyText confidence="0.973621">
Adjuncts in the Penn Treebank
We add the &amp;quot;-C&amp;quot; suffix to all non-terminals in
training data which satisfy the following conditions:
</bodyText>
<listItem confidence="0.824719384615385">
1. The non-terminal must be: (1) an NP, SBAR,
or S whose parent is an S; (2) an NP, SBAR, S,
or VP whose parent is a VP; or (3) an S whose
parent is an SBAR.
2. The non-terminal must not have one of the fol-
lowing semantic tags: ADV, VOC, BNF, DIR,
EXT, LOC, MNR, TMP, CLR or PRP. See
(Marcus et al. 94) for an explanation of what
these tags signify. For example, the NP &amp;quot;Last
week&amp;quot; in figure 1 would have the TMP (tempo-
ral) tag; and the SBAR in &amp;quot;(SBAR because the
market is down)&amp;quot;, would have the ADV (adver-
bial) tag.
</listItem>
<bodyText confidence="0.999894">
In addition, the first child following the head of a
prepositional phrase is marked as a complement.
</bodyText>
<subsectionHeader confidence="0.6603265">
2.2.2 Probabilities over Subcategorisation
Frames
</subsectionHeader>
<bodyText confidence="0.999795">
The model could be retrained on training data
with the enhanced set of non-terminals, and it
might learn the lexical properties which distinguish
complements and adjuncts (&amp;quot;Marks&amp;quot; vs -week&amp;quot;, or
&amp;quot;that&amp;quot; vs. &amp;quot;because&amp;quot;). However, it would still suffer
from the bad independence assumptions illustrated
in figure 4. To solve these kinds of problems, the gen-
erative process is extended to include a probabilistic
choice of left and right subcategorisation frames:
</bodyText>
<listItem confidence="0.980097666666667">
1. Choose a head H with probability PH (H IP, h).
2. Choose left and right subcat frames, LC and
RC, with probabilities Pic(LC I P, H, h) and
</listItem>
<figure confidence="0.985882473684211">
H(h)
NP(week)
Last week
TOP
S(bought)
NP-C(Marks)
Marks
VP(bought)
18
I. (a) Incorrect S (b) Correct
NP-C VP
NP-C
1
Dreyfus
2. (a) Incorrect
NP-C
The issue
NP-C
the best fund
VP
was A DJP
low
VP-C
funding NP-C
Congress
NP NP was ADJP
Dreyfus the best fund low
was NP-C
NP VP
a bill funding NP-C
Congress
Was
VP
NP-C
a bill
(b) Correct
NP-C VP
The issue
</figure>
<figureCaption confidence="0.999781">
Figure 4: Two examples where the assumption that modifiers are generated independently of each
</figureCaption>
<bodyText confidence="0.983276428571428">
other leads to errors. In (1) the probability of generating both &amp;quot;Dreyfus&amp;quot; and &amp;quot;fund&amp;quot; as sub-
jects, P(NP-C(Dreyf us) I S , VP , was) * P(NP-C(fund) I S ,VP , was) is unreasonably high. (2) is similar:
P(NP-C (bill) , VP-C (funding) I VP , VB , was) = P(NP-C (bill) I VP , VB , was) *&apos;P(VP-C (funding) I VP , VB , was)
is a bad independence assumption.
Prc(RC I P,H,h). Each subcat frame is a
multiset6 specifying the complements which the
head requires in its left or right modifiers.
</bodyText>
<listItem confidence="0.814301666666667">
3. Generate the left and right modifiers with prob-
abilities Pi (Li, i I H, P, h, distancet(i — 1), LC)
and Pr (Ri, ri I H, P, h, distancer(i — 1), RC) re-
</listItem>
<bodyText confidence="0.975234166666667">
spectively. Thus the subcat requirements are
added to the conditioning context. As comple-
ments are generated they are removed from the
appropriate subcat multiset. Most importantly,
the probability of generating the STOP symbol
will be 0 when the subcat frame is non-empty,
and the probability of generating a complement
will be 0 when it is not in the subcat frame;
thus all and only the required complements will
be generated.
The probability of the phrase S (bought) -&gt;
NP (week) NP-C(Marks) VP (bought) is now:
</bodyText>
<equation confidence="0.9591265">
Ph(VP I S ,bought) x
Pic({NP-C} I S, VP , bought) x Prc({} I S , VP , bought)
(NP-C (Marks) I S , VP ,bought, {NP-C}) x
(NP (week) I S , VP , bought, {}) x
2 I (STOP I S , VP , bought, {}) x
Pr (STOP I S , VP , bought, {})
</equation>
<bodyText confidence="0.99268">
Here the head initially decides to take a sin-
gle NP-C (subject) to its left, and no complements
</bodyText>
<footnote confidence="0.96346">
6A multiset, or bag, is a set which may contain du-
plicate non-terminal labels.
</footnote>
<bodyText confidence="0.999045">
to its right. NP-C(Marks) is immediately gener-
ated as the required subject, and NP-C is removed
from LC, leaving it empty when the next modi-
fier, NP (week) is generated. The incorrect struc-
tures in figure 4 should now have low probabil-
ity because Pic({NP-C , NP-C} I S , VP ,bought) and
P„({NP-C ,VP-C} I VP , VB , was) are small.
</bodyText>
<subsectionHeader confidence="0.999014">
2.3 Model 3: Traces and Wh-Movement
</subsectionHeader>
<bodyText confidence="0.999087333333333">
Another obstacle to extracting predicate-argument
structure from parse trees is wh-movement. This
section describes a probabilistic treatment of extrac-
tion from relative clauses. Noun phrases are most of-
ten extracted from subject position, object position,
or from within PPs:
</bodyText>
<figure confidence="0.405926">
Example 1 The store (SBAR which TRACE
bought Brooks Brothers)
Example 2 The store (SBAR which Marks bought
TRACE)
x Example 3 The store (SBAR which Marks bought
Brooks Brothers from TRACE)
</figure>
<bodyText confidence="0.999787">
It might be possible to write rule-based patterns
which identify traces in a parse tree. However, we
argue again that this task is best integrated into
the parser: the task is complex enough to warrant
a probabilistic treatment, and integration may help
parsing accuracy. A couple of complexities are that
modification by an SBAR does not always involve
extraction (e.g., &amp;quot;the fact (SBAR that besoboru is
</bodyText>
<page confidence="0.996035">
19
</page>
<table confidence="0.791551214285715">
NP -&gt; NP SBAR (+gap)
SBAR(+gap) -&gt; WHNP S-C (+gap)
S (+gap) -&gt; NP-C VP (+gap)
VP (+gap) -&gt; VB TRACE NP
WHNP(that) S(bought)(+gap)
WDT
1
that
NP-C(Marks) VP(bought)(+gap)
1
Marks
VBD TRACE NP(week)
1
bought last week
</table>
<figureCaption confidence="0.96080325">
Figure 5: A +gap feature can be added to non-terminals to describe NP extraction. The top-level NP
initially generates an SBAR modifier, but specifies that it must contain an NP trace by adding the +gap
feature. The gap is then passed down through the tree, until it is discharged as a TRACE complement to
the right of bought.
</figureCaption>
<figure confidence="0.93999175">
NP(store)
NP (store
The store
SBAR(that)(+gap)
</figure>
<bodyText confidence="0.999081739130435">
played with a ball and a bat)&amp;quot;), and it is not un-
common for extraction to occur through several con-
stituents, (e.g., &amp;quot;The changes (SBAR that he said
the government was prepared to make TRACE)&amp;quot;).
The second reason for an integrated treatment
of traces is to improve the parameterisation of the
model. In particular, the subcategorisation proba-
bilities are smeared by extraction. In examples 1, 2
and 3 above &apos;bought&apos; is a transitive verb, but with-
out knowledge of traces example 2 in training data
will contribute to the probability of &apos;bought&apos; being
an intransitive verb.
Formalisms similar to GPSG (Gazdar et al. 95)
handle NP extraction by adding a gap feature to
each non-terminal in the tree, and propagating gaps
through the tree until they are finally discharged as a
trace complement (see figure 5). In extraction cases
the Penn treebank annotation co-indexes a TRACE
with the WHNP head of the SBAR, so it is straight-
forward to add this information to trees in training
data.
Given that the LHS of the rule has a gap, there
are 3 ways that the gap can be passed down to the
RHS:
Head The gap is passed to the head of the phrase,
as in rule (3) in figure 5.
Left, Right The gap is passed on recursively to one
of the left or right modifiers of the head, or is
discharged as a trace argument to the left/right
of the head. In rule (2) it is passed on to a right
modifier, the S complement. In rule (4) a trace
is generated to the right of the head VB.
We specify a parameter PG(G I P, h, H) where G
is either Head, Left or Right. The generative pro-
cess is extended to choose between these cases after
generating the head of the phrase. The rest of the
phrase is then generated in different ways depend-
ing on how the gap is propagated: In the Head
case the left and right modifiers are generated as
normal. In the Left, Right cases a gap require-
ment is added to either the left or right SUBCAT
variable. This requirement is fulfilled (and removed
from the subcat list) when a trace or a modifier
non-terminal which has the +gap feature is gener-
ated. For example, Rule (2), SBAR(that) (+gap) -&gt;
WHNP (that ) S-C (bought ) (+gap) , has probability
</bodyText>
<equation confidence="0.9929425">
P h(WHNP I SBAR, that) x PG (Right I SBAR,WHNP , that) x
Prx({} I SBAR,WHNP , that) x
PRcUS-C1 I SBAR,WHNP , that) x
PR (S-C (bought) (+gap) I SBAR, WHNP , that, {S-C , +gap}) x
P R (ST OP I SBAR,WHNP , that, {}) x
PL (STOP I SBAR ,WHNP , that, {})
Rule (4), VP (bought) (+gap) -&gt; VB (bought)
TRACE NP (week), has probability
Ph (VB I VP ,bought) x PG (Right I VP ,bought ,VB) x
PLc({} I VP , bought , VB) X PRC({NP—C} VP , bought , VB) x
PR (TRACE I VP ,bought ,VB, {NP-C, +gap}) x
PR (NP (week) I VP ,bought ,VB, x
PL(STOP I VP , bought ,VB, {}) x
P R (ST OP I VP ,bought ,VB,
</equation>
<bodyText confidence="0.9973255">
In rule (2) Right is chosen, so the +gap requirement
is added to RC. Generation of S-C (bought) (+gap)
</bodyText>
<page confidence="0.830688">
20
</page>
<equation confidence="0.973491125">
(a) H(+)
. . . .
Prob = X
(b) P(-)
H R1 ..
Prob X
P(-)
H(+)
Prob = X .x P.R (H I P, ••.)
(c) P(-) P(+)
&amp;quot; &amp;quot; &amp;quot; • •
Prob = X Prob = X xPL(STOP I ••••)
x&apos;PR(STOP I •••-)
+ Ri(+) P(-)
H R1 Ri
Prob = Y Prob =X xYx
</equation>
<figureCaption confidence="0.6519215">
Figure 6: The life of a constituent in the chart. (+) means a constituent is complete (i.e. it includes the
stop probabilities), (–) means a constituent is incomplete. (a) a new constituent is started by projecting a
complete rule upwards; (b) the constituent then takes left and right modifiers (or none if it is unary). (c)
finally, STOP probabilities are added to complete the constituent.
</figureCaption>
<table confidence="0.999489142857143">
Back-off PH (H. I ••-) PG(G I ...) PLI-(L,(/t,) I ...) - PL2(//ll,
Level Pcc(LC I ...) PRi(R,(rt,) I ...) P R2(rws I ...)
PRc (RC I .•.)
1 P, w, t P, H, w, t P, H, w, t, A, LC Ls, Its, P, H, w, t, A, LC
2 P, t P, H, t P, H, t, A, LC Ls, Its, P, H, t, A, LC
3 P P, H P, H, A, LC L,, /t,
4 — — — it,
</table>
<tableCaption confidence="0.8400965">
Table 1: The conditioning variables for each level of back-off. For example, PH estimation interpolates
ei = PH(H I P, w, t), e2 = &apos;PH(H I P,t), and e3 = PH(H I P). A is the distance measure.
</tableCaption>
<bodyText confidence="0.9883515">
:ulfills both the S-C and +gap requirements in RC.
In rule (4) Right is chosen again. Note that gen-
eration of trace satisfies both the NP-C and +gap
subcat requirements.
</bodyText>
<sectionHeader confidence="0.99662" genericHeader="method">
3 Practical Issues
</sectionHeader>
<subsectionHeader confidence="0.999855">
3.1 Smoothing and Unknown Words
</subsectionHeader>
<bodyText confidence="0.999677">
Table 1 shows the various levels of back-off for each
type of parameter in the model. Note that we de-
compose P L(L,(1ws, Its) I P, H, w, t, A, LC) (where
/wi and /t, are the word and POS tag generated
with non-terminal Ls, A is the distance measure)
into the product &apos;PLI(Li(itz) I P,H,w,t,A,LC) x
PL,2(hoi I Ls, its, P, H, w, t, A, LC), and then smooth
these two probabilities separately (Jason Eisner,
p.c.). In each case&apos; the final estimate is
</bodyText>
<equation confidence="0.973101">
e =A1e1 + (1 – Ai)(A2e2 + (1 – A2)e3)
</equation>
<bodyText confidence="0.9998996">
where e1, e2 and e3 are maximum likelihood esti-
mates with the context at levels 1, 2 and 3 in the
table, and Ai , A2 and A3 are smoothing parameters
where 0 &lt; Ai &lt; 1. All words occurring less than 5
times in training data, and words in test data which
</bodyText>
<footnote confidence="0.768193">
7Except cases L2 and R2, which have 4 levels, so that
</footnote>
<equation confidence="0.462334">
e = Aiei + (1– Ai)(A2e2 + (1– A2)(A3e3 + (1– A3)e4))•
</equation>
<bodyText confidence="0.976445666666667">
have never been seen in training, are replaced with
the &amp;quot;UNKNOWN&amp;quot; token. This allows the model to
robustly handle the statistics for rare or new words.
</bodyText>
<subsectionHeader confidence="0.999342">
3.2 Part of Speech Tagging and Parsing
</subsectionHeader>
<bodyText confidence="0.999978444444445">
Part of speech tags are generated along with the
words in this model. When parsing, the POS tags al-
lowed for each word are limited to those which have
been seen in training data for that word. For un-
known words, the output from the tagger described
in (Ratnaparkhi 96) is used as the single possible tag
for that word. A CKY style dynamic programming
chart parser is used to find the maximum probability
tree for each sentence (see figure 6).
</bodyText>
<sectionHeader confidence="0.999726" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999473833333333">
The parser was trained on sections 02 - 21 of the Wall
Street Journal portion of the Penn Treebank (Mar-
cus et al. 93) (approximately 40,000 sentences), and
tested on section 23 (2,416 sentences). We use the
PARSEVAL measures (Black et al. 91) to compare
performance:
</bodyText>
<subsectionHeader confidence="0.750522">
Labeled Precision =
</subsectionHeader>
<bodyText confidence="0.856156">
number of correct constituents in proposed parse
number of constituents in proposed parse
</bodyText>
<page confidence="0.99816">
21
</page>
<table confidence="0.770024090909091">
MODEL &lt; 40 Words (2245 sentences) &lt; 100 Words (2416 sentences)
LR LP CBs 0 CBs &lt;2 CBs LR LP CBs 0 CBs &lt;2 CBs
(Magerman 95) 84.6% 84.9% 1.26 56.6% 81.4% 84.0% 84.3% 1.46 54.0% 78.8%
(Collins 96) 85.8% 86.3% 1.14 59.9% 83.6% 85.3% 85.7% 1.32 57.2% 80.8%
Model 1 87.4% 88.1% 0.96 65.7% 86.3% 86.8% 87.6% 1.11 63.1% 84.1%
Model 2 88.1% 88.6% 0.91 66.5% 86.9% 87.5% 88.1% 1.07 63.9% 84.6%
Model 3 88.1% 88.6% 0.91 66.4% 86.9% 87.5% 88.1% 1.07 63.9% 84.6%
Table 2: Results on Section 23 of the WSJ Treebank. LR/LP = labeled recall/precision. CBs is the average
number of crossing brackets per sentence. 0 CBs, &lt; 2 CBs are the percentage of sentences with 0 or 5. 2
crossing brackets respectively.
Labeled Recall =
</table>
<bodyText confidence="0.945348291666667">
number of correct constituents in proposed parse
number of constituents in treebank parse
Crossing Brackets = number of con-
stituents which violate constituent boundaries
with a constituent in the treebank parse.
For a constituent to be &apos;correct&apos; it must span the
same set of words (ignoring punctuation, i.e. all to-
kens tagged as commas, colons or quotes) and have
the same label&apos; as a constituent in the treebank
parse. Table 2 shows the results for Models 1, 2 and
3. The precision/recall of the traces found by Model
3 was 93.3%/90.1% (out of 436 cases in section 23
of the treebank), where three criteria must be met
for a trace to be &amp;quot;correct&amp;quot;: (1) it must be an argu-
ment to the correct head-word; (2) it must be in the
correct position in relation to that head word (pre-
ceding or following); (3) it must be dominated by the
correct non-terminal label. For example, in figure 5
the trace is an argument to bought, which it fol-
lows, and it is dominated by a VP. Of the 436 cases,
342 were string-vacuous extraction from subject po-
sition, recovered with 97.1%/98.2% precision/recall;
and 94 were longer distance cases, recovered with
76%/60.6% precision/recall 9.
</bodyText>
<subsectionHeader confidence="0.987561">
4.1 Comparison to previous work
</subsectionHeader>
<bodyText confidence="0.984071970588235">
Model 1 is similar in structure to (Collins 96) -
the major differences being that the &amp;quot;score&amp;quot; for each
bigram dependency is Ps (Li, 1,IH , P, h, distances)
8(Magerman 95) collapses ADVP and PRT to the same
label, for comparison we also removed this distinction
when calculating scores.
9We exclude infinitival relative clauses from these fig-
ures, for example &amp;quot;I called a plumber TRACE to fix the
sink&amp;quot; where &apos;plumber&apos; is co-indexed with the trace sub-
ject of the infinitival. The algorithm scored 41%/18%
precision/recall on the 60 cases in section 23 - but in-
finitival relatives are extremely difficult even for human
annotators to distinguish from purpose clauses (in this
case, the infinitival could be a purpose clause modifying
&apos;called&apos;) (Ann Taylor, p.c.)
rather than Ps(Li, P, H I I, h, distances), and that
there are the additional probabilities of generat-
ing the head and the STOP symbols for each con-
stituent. However, Model 1 has some advantages
which may account for the improved performance.
The model in (Collins 96) is deficient, that is for
most sentences S, ET P(T I S) &lt; 1, because prob-
ability mass is lost to dependency structures which
violate the hard constraint that no links may cross.
For reasons we do not have space to describe here,
Model 1 has advantages in its treatment of unary
rules and the distance measure. The generative
model can condition on any structure that has been
previously generated - we exploit this in models 2
and 3 - whereas (Collins 96) is restricted to condi-
tioning on features of the surface string alone.
(Charniak 95) also uses a lexicalised genera-
tive model. In our notation, he decomposes
P(RHSi I LHS) as P(R,,...R1HL1..L, I P,h) x
</bodyText>
<equation confidence="0.632457">
P(r,I P, Ri, h) X 11.1..m P(ii P, L„ h). The
</equation>
<bodyText confidence="0.999555272727273">
Penn treebank annotation style leads to a very
large number of context-free rules, so that directly
estimating &apos;P(R7,...R1HL1..Lin I P, h) may lead to
sparse data problems, or problems with coverage
(a rule which has never been seen in training may
be required for a test data sentence). The com-
plement/adjunct distinction and traces increase the
number of rules, compounding this problem.
(Eisner 96) proposes 3 dependency models, and
gives results that show that a generative model sim-
ilar to Model 1 performs best of the three. However,
a pure dependency model omits non-terminal infor-
mation, which is important. For example, &amp;quot;hope&amp;quot; is
likely to generate a VP (TO) modifier (e.g., I hope
[VP to sleep]) whereas &amp;quot;require&amp;quot; is likely to gen-
erate an S(TO) modifier (e.g., I require [S Jim to
sleep]), but omitting non-terminals conflates these
two cases, giving high probability to incorrect struc-
tures such as &amp;quot;I hope [Jim to sleep]&amp;quot; or &amp;quot;I require [to
sleep]&amp;quot;. (Alshawi 96) extends a generative depen-
dency model to include an additional state variable
which is equivalent to having non-terminals - his
</bodyText>
<page confidence="0.986699">
22
</page>
<bodyText confidence="0.99963184">
suggestions may be close to our models 1 and 2, but
he does not fully specify the details of his model, and
doesn&apos;t give results for parsing accuracy. (Miller et
al. 96) describe a model where the RHS of a rule is
generated by a Markov process, although the pro-
cess is not head-centered. They increase the set of
non-terminals by adding semantic labels rather than
by adding lexical head-words.
(Magerman 95; Jelinek et al. 94) describe a
history-based approach which uses decision trees to
estimate P(TIS). Our models use much less sophis-
ticated n-gram estimation methods, and might well
benefit from methods such as decision-tree estima-
tion which could condition on richer history than
just surface distance.
There has recently been interest in using
dependency-based parsing models in speech recog-
nition, for example (Stolcke 96). It is interesting to
note that Models 1, 2 or 3 could be used as lan-
guage models. The probability for any sentence can
be estimated as P(S) = ET P(T, s), or (making
a Viterbi approximation for efficiency reasons) as
P(S) P(Tbest, S). We intend to perform experi-
ments to compare the perplexity of the various mod-
els, and a structurally similar &apos;pure&apos; PCFG1°.
</bodyText>
<sectionHeader confidence="0.999361" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999945285714286">
This paper has proposed a generative, lexicalised,
probabilistic parsing model. We have shown that lin-
guistically fundamental ideas, namely subcategori-
sation and wh-movement, can be given a statistical
interpretation. This improves parsing performance,
and, more importantly, adds useful information to
the parser&apos;s output.
</bodyText>
<sectionHeader confidence="0.998838" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998796">
I would like to thank Mitch Marcus, Jason Eisner,
Dan Melamed and Adwait Ratnaparkhi for many
useful discussions, and comments on earlier versions
of this paper. This work has also benefited greatly
from suggestions and advice from Scott Miller.
</bodyText>
<sectionHeader confidence="0.998759" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999630262295082">
H. Alshawi. 1996. Head Automata and Bilingual
Tiling: Translation with Minimal Representa-
tions. Proceedings of the 34th Annual Meeting
of the Association for Computational Linguistics,
pages 167-176.
E. Black et al. 1991. A Procedure for Quantita-
tively Comparing the Syntactic Coverage of En-
glish Grammars. Proceedings of the February 1991
DARPA Speech and Natural Language Workshop.
&apos;°Thanks to one of the anonymous reviewers for sug-
gesting these experiments.
T. L. Booth and R. A. Thompson. 1973. Applying
Probability Measures to Abstract Languages. IEEE
Transactions on Computers, C-22(5), pages 442-
450.
E. Charniak. 1995. Parsing with Context-Free Gram-
mars and Word Statistics. Technical Report CS-
95-28, Dept. of Computer Science, Brown Univer-
sity.
N. Chomsky. 1957. Syntactic Structures, Mouton,
The Hague.
M. J. Collins. 1996. A New Statistical Parser Based
on Bigram Lexical Dependencies. Proceedings of
the 34th Annual Meeting of the Association for
Computational Linguistics, pages 184-191.
J. Eisner. 1996. Three New Probabilistic Models for
Dependency Parsing: An Exploration. Proceed-
ings of COLING-96, pages 340-345.
G. Gazdar, E.H. Klein, G.K. Pullum, I.A. Sag. 1985.
Generalized Phrase Structure Grammar. Harvard
University Press.
F. Jelinek, J. Lafferty, D. Magerman, R. Mercer, A.
Ratnaparkhi, S. Roukos. 1994. Decision Tree Pars-
ing using a Hidden Derivation Model. Proceedings
of the 1994 Human Language Technology Work-
shop, pages 272-277.
D. Magerman. 1995. Statistical Decision-Tree Mod-
els for Parsing. Proceedings of the 33rd Annual
Meeting of the Association for Computational
Linguistics, pages 276-283.
M. Marcus, B. Santorini and M. Marcinkiewicz.
1993. Building a Large Annotated Corpus of En-
glish: the Penn Treebank. Computational Linguis-
tics, 19(2):313-330.
M. Marcus, G. Kim, M. A. Marcinkiewicz, R.
IVIacIntyre, A. Bies, M. Ferguson, K. Katz, B.
Schasberger. 1994. The Penn Treebank: Annotat-
ing Predicate Argument Structure. Proceedings of
the 1994 Human Language Technology Workshop,
pages 110-115.
S. Miller, D. Stallard and R. Schwartz. 1996. A
Fully Statistical Approach to Natural Language
Interfaces. Proceedings of the 34th Annual Meeting
of the Association for Computational Linguistics,
pages 55-61.
A. Ratnaparkhi. 1996. A Maximum Entropy Model
for Part-Of-Speech Tagging. Conference on Em-
pirical Methods in Natural Language Processing.
A. Stolcke. 1996. Linguistic Dependency Modeling.
Proceedings of ICSLP 96, Fourth International
Conference on Spoken Language Processing.
</reference>
<page confidence="0.998936">
23
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.652580">
<title confidence="0.999351">Three Generative, Lexicalised Models for Statistical Parsing</title>
<author confidence="0.999878">Michael Collins</author>
<affiliation confidence="0.999907">Dept. of Computer and Information Science University of Pennsylvania</affiliation>
<address confidence="0.999789">Philadelphia, PA, 19104, U.S.A.</address>
<email confidence="0.992947">mcollins@gradient.cis.upenn.edu</email>
<abstract confidence="0.968588454545455">In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Alshawi</author>
</authors>
<title>Head Automata and Bilingual Tiling: Translation with Minimal Representations.</title>
<date>1996</date>
<booktitle>Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>167--176</pages>
<marker>Alshawi, 1996</marker>
<rawString>H. Alshawi. 1996. Head Automata and Bilingual Tiling: Translation with Minimal Representations. Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pages 167-176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Black</author>
</authors>
<title>A Procedure for Quantitatively Comparing the Syntactic Coverage of English Grammars.</title>
<date>1991</date>
<booktitle>Proceedings of the February</booktitle>
<marker>Black, 1991</marker>
<rawString>E. Black et al. 1991. A Procedure for Quantitatively Comparing the Syntactic Coverage of English Grammars. Proceedings of the February 1991 DARPA Speech and Natural Language Workshop.</rawString>
</citation>
<citation valid="false">
<title>Thanks to one of the anonymous reviewers for suggesting these experiments.</title>
<marker></marker>
<rawString>&apos;°Thanks to one of the anonymous reviewers for suggesting these experiments.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Booth</author>
<author>R A Thompson</author>
</authors>
<title>Applying Probability Measures to Abstract Languages.</title>
<date>1973</date>
<journal>IEEE Transactions on Computers,</journal>
<volume>22</volume>
<issue>5</issue>
<pages>442--450</pages>
<marker>Booth, Thompson, 1973</marker>
<rawString>T. L. Booth and R. A. Thompson. 1973. Applying Probability Measures to Abstract Languages. IEEE Transactions on Computers, C-22(5), pages 442-450.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Parsing with Context-Free Grammars and Word Statistics.</title>
<date>1995</date>
<tech>Technical Report CS95-28,</tech>
<institution>Dept. of Computer Science, Brown University.</institution>
<marker>Charniak, 1995</marker>
<rawString>E. Charniak. 1995. Parsing with Context-Free Grammars and Word Statistics. Technical Report CS95-28, Dept. of Computer Science, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Syntactic Structures,</title>
<date>1957</date>
<location>Mouton, The Hague.</location>
<marker>Chomsky, 1957</marker>
<rawString>N. Chomsky. 1957. Syntactic Structures, Mouton, The Hague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Collins</author>
</authors>
<title>A New Statistical Parser Based on Bigram Lexical Dependencies.</title>
<date>1996</date>
<booktitle>Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>184--191</pages>
<marker>Collins, 1996</marker>
<rawString>M. J. Collins. 1996. A New Statistical Parser Based on Bigram Lexical Dependencies. Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pages 184-191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Three New Probabilistic Models for Dependency Parsing: An Exploration.</title>
<date>1996</date>
<booktitle>Proceedings of COLING-96,</booktitle>
<pages>340--345</pages>
<marker>Eisner, 1996</marker>
<rawString>J. Eisner. 1996. Three New Probabilistic Models for Dependency Parsing: An Exploration. Proceedings of COLING-96, pages 340-345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
<author>E H Klein</author>
<author>G K Pullum</author>
<author>I A Sag</author>
</authors>
<title>Generalized Phrase Structure Grammar.</title>
<date>1985</date>
<publisher>Harvard University Press.</publisher>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>G. Gazdar, E.H. Klein, G.K. Pullum, I.A. Sag. 1985. Generalized Phrase Structure Grammar. Harvard University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>J Lafferty</author>
<author>D Magerman</author>
<author>R Mercer</author>
<author>A Ratnaparkhi</author>
<author>S Roukos</author>
</authors>
<title>Decision Tree Parsing using a Hidden Derivation Model.</title>
<date>1994</date>
<booktitle>Proceedings of the 1994 Human Language Technology Workshop,</booktitle>
<pages>272--277</pages>
<marker>Jelinek, Lafferty, Magerman, Mercer, Ratnaparkhi, Roukos, 1994</marker>
<rawString>F. Jelinek, J. Lafferty, D. Magerman, R. Mercer, A. Ratnaparkhi, S. Roukos. 1994. Decision Tree Parsing using a Hidden Derivation Model. Proceedings of the 1994 Human Language Technology Workshop, pages 272-277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Magerman</author>
</authors>
<title>Statistical Decision-Tree Models for Parsing.</title>
<date>1995</date>
<booktitle>Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>276--283</pages>
<marker>Magerman, 1995</marker>
<rawString>D. Magerman. 1995. Statistical Decision-Tree Models for Parsing. Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 276-283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: the Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini and M. Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>G Kim</author>
<author>M A Marcinkiewicz</author>
<author>R IVIacIntyre</author>
<author>A Bies</author>
<author>M Ferguson</author>
<author>K Katz</author>
<author>B Schasberger</author>
</authors>
<title>The Penn Treebank: Annotating Predicate Argument Structure.</title>
<date>1994</date>
<booktitle>Proceedings of the 1994 Human Language Technology Workshop,</booktitle>
<pages>110--115</pages>
<marker>Marcus, Kim, Marcinkiewicz, IVIacIntyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>M. Marcus, G. Kim, M. A. Marcinkiewicz, R. IVIacIntyre, A. Bies, M. Ferguson, K. Katz, B. Schasberger. 1994. The Penn Treebank: Annotating Predicate Argument Structure. Proceedings of the 1994 Human Language Technology Workshop, pages 110-115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miller</author>
<author>D Stallard</author>
<author>R Schwartz</author>
</authors>
<title>A Fully Statistical Approach to Natural Language Interfaces.</title>
<date>1996</date>
<booktitle>Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>55--61</pages>
<marker>Miller, Stallard, Schwartz, 1996</marker>
<rawString>S. Miller, D. Stallard and R. Schwartz. 1996. A Fully Statistical Approach to Natural Language Interfaces. Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pages 55-61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Model for Part-Of-Speech Tagging.</title>
<date>1996</date>
<booktitle>Conference on Empirical Methods in Natural Language Processing.</booktitle>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A Maximum Entropy Model for Part-Of-Speech Tagging. Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>Linguistic Dependency Modeling.</title>
<date>1996</date>
<booktitle>Proceedings of ICSLP 96, Fourth International Conference on Spoken Language Processing.</booktitle>
<marker>Stolcke, 1996</marker>
<rawString>A. Stolcke. 1996. Linguistic Dependency Modeling. Proceedings of ICSLP 96, Fourth International Conference on Spoken Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>