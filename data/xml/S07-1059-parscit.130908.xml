<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003995">
<title confidence="0.8296025">
PU-BCD: Exponential Family Models for the Coarse- and Fine-Grained
All-Words Tasks
</title>
<author confidence="0.998001">
Jonathan Chang Miroslav Dud´ık, David M. Blei
</author>
<affiliation confidence="0.998197">
Princeton University Princeton University
Department of Electrical Engineering Department of Computer Science
</affiliation>
<email confidence="0.999117">
jcone@princeton.edu {mdudik,blei}@cs.princeton.edu
</email>
<sectionHeader confidence="0.994804" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999601">
This paper describes an exponential family
model of word sense which captures both
occurrences and co-occurrences of words
and senses in a joint probability distribution.
This statistical framework lends itself to the
task of word sense disambiguation. We eval-
uate the performance of the model in its par-
ticipation on the SemEval-2007 coarse- and
fine-grained all-words tasks under a variety
of parameters.
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999952765957447">
This paper describes an exponential family model
suited to performing word sense disambiguation.
Exponential family models are a mainstay of mod-
ern statistical modeling (Brown, 1986) and they are
widely and successfully used for example in text
classification (Berger et al., 1996). In statistical
machine learning research, a general methodology
and many algorithms were developed for undirected
graphical model representation of exponential fam-
ilies (Jordan, 2004), providing a solid basis for effi-
cient inference.
Our model differs from other probabilistic mod-
els used for word sense disambiguation in that it
captures not only word-sense co-occurrences but
also contextual sense-sense co-occurrences, thereby
breaking the naive Bayes assumption. Although
spare in the types of features, the model is extremely
expressive. Our model has parameters that control
for word-sense interaction and sense-sense similar-
ity, allowing us to capture many of the salient fea-
tures of word and sense use. After fitting the param-
eters of our model from a labeled corpus, the task
of word sense disambiguation immediately follows
by considering the posterior distribution of senses
given words.
We used this model to participate in SemEval-
2007 on the coarse- and fine-grained all-words tasks.
In both of these tasks, a series of sentences are
given with certain words tagged. Each competing
system must assign a sense from a sense inventory
to the tagged words. In both tasks, performance
was gauged by comparing the output of each system
to human-tagged senses. In the fine-grained task,
precision and recall were simply and directly com-
puted against the golden annotations. However, in
the coarse-grained task, the sense inventory was first
clustered semi-automatically with each cluster rep-
resenting an equivalence class over senses (Navigli,
2006). Precision and recall were computed against
equivalence classes.
This paper briefly derives the model and then
explores its properties for WSD. We show how
common algorithms, such as “dominant sense” and
“most frequent sense,” can be expressed in the ex-
ponential family framework. We then proceed to
present an evaluation of the developed techniques on
the SemEval-2007 tasks in which we participated.
</bodyText>
<sectionHeader confidence="0.979838" genericHeader="method">
2 The model
</sectionHeader>
<bodyText confidence="0.999965666666667">
We describe an exponential family model for word
sense disambiguation. We posit a joint distribution
over words w and senses s.
</bodyText>
<subsectionHeader confidence="0.955226">
2.1 Notation
</subsectionHeader>
<bodyText confidence="0.9992445">
We define a document d to be a sequence of words
from some lexicon W; for the participation in this
contest, a document consists of a sentence. Associ-
ated with each word is a sense from a lexicon S. In
</bodyText>
<page confidence="0.951435">
272
</page>
<bodyText confidence="0.976952538461538">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 272–276,
Prague, June 2007. c�2007 Association for Computational Linguistics
this work, our sense lexicon is the synsets of Word-
Net (Fellbaum and Miller, 2003), but our methods
easily generalize to other sense lexicons, such as
VerbNet (Kipper et al., 2000).
Formally, we denote the sequence of words in a
document d by wd = (wd,1, ... , wd,nd) and the se-
quence of synsets by sd = (sd,1, sd,2, ... , sd,nd),
where nd denotes the number of words in the docu-
ment. A corpus D is defined as a collection of doc-
uments. We also write w E s if w can be used to
represent sense s.
</bodyText>
<subsectionHeader confidence="0.999322">
2.2 An exponential family of words and senses
</subsectionHeader>
<bodyText confidence="0.9966628">
We turn our attention to an exponential family
of words and senses. The vector of parameters
7 = (r., A) consists of two blocks capturing depen-
dence on word-synset co-occurrences, and synset
co-occurrences.
</bodyText>
<equation confidence="0.99222">
p?7,n(s, w)
= expIEiκwi,si + Ei,j λsi,sj J/Z71,n .
</equation>
<bodyText confidence="0.999830642857143">
The summations are first over all positions in the
document, 1 &lt; i &lt; n, and then over all pairs of
positions in the document, 1 &lt; i, j &lt; n. We discuss
parameters of our exponential model in turn.
Word-sense parameters r. Using parameters r.
alone, it is possible to describe an arbitrary context
independent distribution between a word and its as-
signed synset.
Sense co-occurrence parameters A Parameters A
are the only parameters that establish the depen-
dence of sense on its context. More specifically,
they capture co-occurrences of synset pairs within a
context. Larger values favor, whereas smaller values
disfavor each pair of synsets.
</bodyText>
<sectionHeader confidence="0.978315" genericHeader="method">
3 Parameter estimation
</sectionHeader>
<bodyText confidence="0.981495105263158">
With the model in hand, we need to address two
problems in order to use it for problems such as
WSD. First, in parameter estimation, we find values
of the parameters that explain a labeled corpus, such
as SemCor (Miller et al., 1993). Once the parame-
ters are fit, we use posterior inference to compute the
posterior probability distribution of a set of senses
given a set of unlabeled words in a context, p(s  |w).
This distribution is used to predict the senses of the
words.
In this section, it will be useful to introduce the
notation ˜p(s, w) to denote the empirical probabili-
ties of observing the word-sense pair s, w in the en-
tire corpus:
˜p(s, w) = Ed,i δ(sd,i, s)δ(wd,i,w)/Ed nd ,
where δ(x, y) = 1 if x = y and 0 otherwise.
Similarly, we will define ˜p(s) to denote the empiri-
cal probability of observing a sense s over the entire
corpus:
</bodyText>
<equation confidence="0.912364">
˜p(s) = Ed,i δ(sd,i, s)/Ed nd .
</equation>
<subsectionHeader confidence="0.982554">
3.1 Word-sense parameters r.
</subsectionHeader>
<bodyText confidence="0.930748619047619">
= 0 if w E s and κWN
w,s = −oc
otherwise. This simply sets to zero the probability of
assigning a word w to a synset s when w E� s while
making all w E s equally likely as an assignment
to s. This forces the model to rely entirely on A
for inference. If A is also set to 0, this then forces
the system to fall back onto its arbitrary tie-breaking
mechanism such as choosing randomly or choosing
the first sense.
Most-frequent synset One approach to disam-
biguation is the technique of choosing the most fre-
quently occurring synset which the word may ex-
press. This can be implemented within the model by
settingMFS = Ins) if w E s and −oc
κ w,s = κ w,s p
otherwise.
MLE Given a labeled corpus, we would like to
find the corresponding parameters that maximize
likelihood of the data. Equivalently, we would like
to maximize the log likelihood
</bodyText>
<equation confidence="0.8618405">
07) = 11
Ed [Eiκwd,i,sd,i + Ei,j λsd,i,sd,j − ln Zq ndl .
</equation>
<bodyText confidence="0.971911142857143">
(2)
In this section, we consider a simple case when it
is possible to estimate parameters maximizing the
likelihood exactly, i.e., the case where our model
depends only on word-synset co-occurrences and is
parametrized solely by r. (setting A = 0).
Using Eq. (1), with A = 0, we obtain
</bodyText>
<equation confidence="0.923102875">
exp�E J
d,iκwd,i,sd,i
l i.
d Z.,nd
(1)
Fallback Let κWN
w,s
pr.(sD, wD) =
</equation>
<page confidence="0.981131">
273
</page>
<bodyText confidence="0.999925571428571">
Thus, p,,(sD, wD) can be viewed as a multino-
mial model with Ed nd trials and |8 |outcomes,
parametrized by κ,,,,s. The maximum likelihood es-
timates in this model are k,,,,s = ln p(s, w).
This setting of the parameters corresponds pre-
cisely to the dominant-sense model (McCarthy et al.,
2004). The resulting model is thus
</bodyText>
<equation confidence="0.583702">
p.,n(s, w) = Hi Asi, wi) . (3)
</equation>
<subsectionHeader confidence="0.999603">
3.2 Sense co-occurrence parameters λ
</subsectionHeader>
<bodyText confidence="0.999597857142857">
Unlike κ, it is impossible to find a closed-form so-
lution for the maximum-likelihood settings of λ.
Therefore, we turn to intuitive methods.
Observed synset co-occurrence One natural ad
hoc statistic to use to compute the parameters λ are
the empirical sense co-occurrences. In particular, we
may set
</bodyText>
<equation confidence="0.9996765">
λsi,sj = λSF
si,sj = ln�p(si, sj) . (4)
</equation>
<bodyText confidence="0.985598333333333">
We will observe in section 5 that the performance
of λ = λSF actually degrades the performance of
the system, especially when combined with κ = A.
This can be understood as a by-product of an un-
sympathetic interaction between κ and λ. In other
words, κ and λ overlap; by favoring a sense pair the
model will also implicitly favor each of the senses in
the pair.
Discounted observed synset co-occurrence As
</bodyText>
<equation confidence="0.754269">
λ = λSF
</equation>
<bodyText confidence="0.9721305">
we noted earlier, the combination κ = A,
actually performs worse than κ = A, λ = 0.
In order to cancel out the aforementioned over-
lap effect, we attempt to compute the number of
co-occurrences beyond what the occurrences them-
selves would imply. To do so, we set
</bodyText>
<equation confidence="0.9974245">
λ = λDSF = ln p(si)p(sj) , (5)
p(si, sj)
</equation>
<bodyText confidence="0.9995745">
a quantity which finds an analogue in the notion of
mutual information. We will see shortly that such
a setting of λ will allow sense co-occurrence to im-
prove disambiguation performance.
</bodyText>
<sectionHeader confidence="0.986308" genericHeader="method">
4 Word Sense Disambiguation
</sectionHeader>
<bodyText confidence="0.999965166666667">
Finally, we describe how to perform WSD using the
exponential family model. Our goal is to assign a
synset si to every word wi in an unlabeled document
d of length n. In this setting, the synsets are hidden
variables. Thus, we assign synsets according to their
posterior probability given the observed words:
</bodyText>
<equation confidence="0.997064666666667">
s� = argmax
sESn Es, pq,n(st, w) ,
p7/,n(s, w)
</equation>
<bodyText confidence="0.9999084">
where the sum is over all possible sequences of
synsets. This combinatorial sum renders exact infer-
ence computationally intractable. We discuss how to
obtain the sense assignment using approximate in-
ference.
</bodyText>
<subsectionHeader confidence="0.992161">
4.1 Variational Inference
</subsectionHeader>
<bodyText confidence="0.999988555555556">
To approximate the posterior over senses, we use
variational inference (Jordan et al., 1999). In vari-
ational inference, one first chooses a family of
distributions for which inference is computationlly
tractable. Then the distribution in that family which
best approximates the posterior distribution of inter-
est is found.
For our purposes, it is convenient to select q from
the family of factorized multinomial distributions:
</bodyText>
<equation confidence="0.995196">
q(s) = 11 qi(si) ,
i
</equation>
<bodyText confidence="0.944120666666667">
where each qi(si) is a multinomial distribution
over all possible senses. Observe that findings is
much simpler using q(s): one can find the argmax
of each individual qi independently.
It can be shown that the multinomial which mini-
mizes the KL-divergence must satisfy:
qi(si) a exp { �κ,,,i,si + E qj(sj)λsi,sj } (6)
jai sj
a system of transcendental equations which can
be solved iteratively to find q. This q is then used to
efficiently perform inference and hence disambigua-
tion.
</bodyText>
<sectionHeader confidence="0.998585" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999716333333333">
This section evaluates the performance of the model
and the techniques described in the previous sec-
tions with respect to the coarse- and fine-grained all-
words tasks at SemEval-2007.
In order to train the parameters, we trained our
model in a supervised fashion on SemCor (Miller et
</bodyText>
<page confidence="0.968355">
274
</page>
<equation confidence="0.93246725">
κ = κWN κ= κMFS κ = κ�
λ = 0 52.0% 45.8% 51.2%
λ = λSF 48.8% 45.3% 52.5%
λ = λDSF 47.0% 44.6% 54.2%
</equation>
<tableCaption confidence="0.7370265">
Table 1: Precision for the fine-grained all-words task. The results corresponding to the bolded value was
submitted to the competition.
</tableCaption>
<bodyText confidence="0.999706947368421">
al., 1993) with Laplace smoothing for parameter es-
timates. We utilized the POS tagging and lemma-
tization given in the coarse-grained all-words test
set. Wherever a headword was tagged differently
between the two test sets, we produced an answer
only for the coarse-grained test and not for the fine-
grained one. This led to responses on only 93.9% of
the fine-grained test words. Of the 6.1% over which
no response was given, 5.3% were tagged as “U” in
the answer key.
In order to break ties between equally likely
senses, for the fine-grained test, the system returned
the first one returned in WordNet’s sense inventory
for that lemma. For the coarse-grained test, an arbi-
trary sense was returned in case of ties.
The precision results given in this section are over
polysemous words (of all parts of speech) for which
our system gave an answer and for which the answer
key was not tagged with “U.”
</bodyText>
<subsectionHeader confidence="0.996154">
5.1 Fine-grained results (Task 17)
</subsectionHeader>
<bodyText confidence="0.999578315789474">
The fine-grained results over all permutations of the
parameters mentioned in Section 3 are given in Ta-
ble 1. Note here that the baseline number of λ =
0, κ = κWN given in the upper-left is equivalent to
simply choosing the first WordNet sense. Notably,
such a simple configuration of the model outper-
forms all but two other of the other parameter set-
tings.
When any sort of nonzero sense co-occurrence
parameter is used with κ = κWN, the performance
degrades dramatically, to 48.8% and 47.0% for λSF
and λDSF
was devised to positively interact with κ = A, it is
no surprise that it does poorly when κ is not set in
such a way. And as mentioned previously, naively
setting λ to λSF improperly conflates λ and κ, yield-
ing a poor result.
When κ = κMFS is used, the precision is
even lower, dropping to 45.8% when no sense co-
occurrence information is used. And similarly to
κ = κWN, any nonzero λ significantly degrades per-
formance. This seems to indicate the most-frequent
synset, as predicted by our earlier analysis, is an in-
ferior technique.
Finally, when κ = κ� is used (i.e. dominant sense),
the precision is 51.2%, slightly lower than but nearly
on par with that of the baseline. When sense co-
occurrence parameters are added, the performance
increases. For λSF, a precision of 52.5% is achieved;
a precision above the baseline. But again, because of
the interaction between κ and λ, here we expect it
to be possible to improve upon this performance.
And indeed, when λ = λDSF, the highest value
of the entire table, 54.2% is achieved. This is a sig-
nificant improvement over the baseline and demon-
strates that our intuitively appealing mutual informa-
tion discounting mechanism allows for κ and λ to
work cooperatively.
</bodyText>
<subsectionHeader confidence="0.997833">
5.2 Coarse-grained results (Task 7)
</subsectionHeader>
<bodyText confidence="0.999899">
In order to perform the coarse-grained task, our sys-
tem first determined the set of sense equivalence
classes. We denote a sense equivalence class by k,
where k is some sense key member of the class. The
equivalence classes were created according to the
following constraints:
</bodyText>
<listItem confidence="0.987399">
• Each sense key k may only belong to one
equivalence class k.
• All sense keys referring to the same sense s
must belong in the same class.
• All sense keys clustered together must belong
in the same class.
</listItem>
<bodyText confidence="0.9944068">
Once the clustering is complete, we can proceed
exactly as we did in the previous sections, while re-
placing all instances of s with k. Thus, training
in this case was performed on a SemCor where all
respectively. Since the discounting scheme
</bodyText>
<page confidence="0.936427">
275
</page>
<bodyText confidence="0.999982033333333">
the senses were mapped back to their corresponding
sense equivalence classes.
The model fared considerably worse on the
coarse-grained all-words task. The precision of the
system as given by the scorer was 69.7% and the
recall 62.8%. These results, while naturally much
higher than those for the fine-grained test, are low by
coarse-grained standards. While the gold standard
was not available for comparison for these results,
there are two likely causes of the lower performance
on this task.
The first is that ties were not adjudicated by
choosing the first WordNet sense. Instead, an ar-
bitrary sense was chosen thereby pushing cases in
which the model is unsure from the baseline to the
much lower random precision rate. The second is the
same number of documents are mapped to a smaller
number of “senses” (i.e. sense equivalence classes),
the number of parameters is greatly reduced. There-
fore, the expressive power of each parameter is di-
luted because it must be spread out across all senses
within the equivalence class.
We believe that both of these issues can be eas-
ily overcome and we hope to do so in future work.
Furthermore, while the model currently captures the
most salient features for word sense disambiguation,
namely word-sense occurrence and sense-sense co-
occurrence, it would be simple to extend the model
to include a larger number of features (e.g. syntactic
features).
</bodyText>
<sectionHeader confidence="0.999513" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999885">
In summary, this paper described our participation in
the the SemEval-2007 coarse- and fine-grained all-
words tasks. In particular, we described an exponen-
tial family model of word sense amenable to the task
of word sense disambiguation. The performance of
the model under a variety of parameter settings was
evaluated on both tasks and the model was shown to
be particularly effective on the fine-grained task.
</bodyText>
<sectionHeader confidence="0.99936" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9993036">
The authors would like to thank Christiane Fell-
baum, Daniel Osherson, and the members of the
CIMPL group for their helpful contributions. This
research was supported by a grant from Google Inc.
and by NSF grant CCR-0325463.
</bodyText>
<sectionHeader confidence="0.994277" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99816903125">
Adam L. Berger, Vincent J. Della Pietra, and Stephen A.
Della Pietra. 1996. A maximum entropy approach to natural
language processing. Computational Linguistics, 22(1):39–
71.
Lawrence D. Brown. 1986. Fundamentals of Statistical Expo-
nential Families. Institute of Mathematical Statistics, Hay-
ward, CA.
Christiane Fellbaum and George A. Miller. 2003. Mor-
phosemantic links in WordNet. Traitement automatique de
langue.
Michael I. Jordan, Zoubin Ghahramani, Tommi Jaakkola, and
Lawrence K. Saul. 1999. An introduction to varia-
tional methods for graphical models. Machine Learning,
37(2):183–233.
Michael I. Jordan. 2004. Graphical models. Statistical Science,
19(1):140–155.
Karin Kipper, Hoa Trang Dang, and Martha Palmer. 2000.
Class-Based Construction of a Verb Lexicon. Proceedings
of the Seventeenth National Conference on Artificial Intelli-
gence and Twelfth Conference on Innovative Applications of
Artificial Intelligence table of contents, pages 691–696.
Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll.
2004. Finding predominant senses in untagged text. In
Proceedings of the 42nd Annual Meeting of the Association
for Computational Linguistics, pages 280–287, Barcelona,
Spain.
George A. Miller, Claudia Leacock, Randee Tengi, and Ross T.
Bunker. 1993. A semantic concordance. In 3rd DARPA
Workshop on Human Language Technology.
Roberto Navigli. 2006. Meaningful clustering of senses helps
boost word sense disambiguation performance. In COLING-
ACL 2006, pages 105–112, July.
</reference>
<page confidence="0.99844">
276
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.956439">
<title confidence="0.999475">PU-BCD: Exponential Family Models for the Coarseand Fine-Grained All-Words Tasks</title>
<author confidence="0.999996">Jonathan Chang Miroslav Dud´ık</author>
<author confidence="0.999996">David M Blei</author>
<affiliation confidence="0.9963485">Princeton University Princeton University Department of Electrical Engineering Department of Computer Science</affiliation>
<abstract confidence="0.996526545454545">This paper describes an exponential family model of word sense which captures both occurrences and co-occurrences of words and senses in a joint probability distribution. This statistical framework lends itself to the task of word sense disambiguation. We evaluate the performance of the model in its participation on the SemEval-2007 coarseand fine-grained all-words tasks under a variety of parameters.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<pages>71</pages>
<contexts>
<context position="1002" citStr="Berger et al., 1996" startWordPosition="135" endWordPosition="138">occurrences and co-occurrences of words and senses in a joint probability distribution. This statistical framework lends itself to the task of word sense disambiguation. We evaluate the performance of the model in its participation on the SemEval-2007 coarse- and fine-grained all-words tasks under a variety of parameters. 1 Introduction This paper describes an exponential family model suited to performing word sense disambiguation. Exponential family models are a mainstay of modern statistical modeling (Brown, 1986) and they are widely and successfully used for example in text classification (Berger et al., 1996). In statistical machine learning research, a general methodology and many algorithms were developed for undirected graphical model representation of exponential families (Jordan, 2004), providing a solid basis for efficient inference. Our model differs from other probabilistic models used for word sense disambiguation in that it captures not only word-sense co-occurrences but also contextual sense-sense co-occurrences, thereby breaking the naive Bayes assumption. Although spare in the types of features, the model is extremely expressive. Our model has parameters that control for word-sense in</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Vincent J. Della Pietra, and Stephen A. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39– 71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence D Brown</author>
</authors>
<title>Fundamentals of Statistical Exponential Families. Institute of Mathematical Statistics,</title>
<date>1986</date>
<location>Hayward, CA.</location>
<contexts>
<context position="903" citStr="Brown, 1986" startWordPosition="121" endWordPosition="122">bstract This paper describes an exponential family model of word sense which captures both occurrences and co-occurrences of words and senses in a joint probability distribution. This statistical framework lends itself to the task of word sense disambiguation. We evaluate the performance of the model in its participation on the SemEval-2007 coarse- and fine-grained all-words tasks under a variety of parameters. 1 Introduction This paper describes an exponential family model suited to performing word sense disambiguation. Exponential family models are a mainstay of modern statistical modeling (Brown, 1986) and they are widely and successfully used for example in text classification (Berger et al., 1996). In statistical machine learning research, a general methodology and many algorithms were developed for undirected graphical model representation of exponential families (Jordan, 2004), providing a solid basis for efficient inference. Our model differs from other probabilistic models used for word sense disambiguation in that it captures not only word-sense co-occurrences but also contextual sense-sense co-occurrences, thereby breaking the naive Bayes assumption. Although spare in the types of f</context>
</contexts>
<marker>Brown, 1986</marker>
<rawString>Lawrence D. Brown. 1986. Fundamentals of Statistical Exponential Families. Institute of Mathematical Statistics, Hayward, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
<author>George A Miller</author>
</authors>
<title>Morphosemantic links in WordNet. Traitement automatique de langue.</title>
<date>2003</date>
<contexts>
<context position="3572" citStr="Fellbaum and Miller, 2003" startWordPosition="535" endWordPosition="538">e participated. 2 The model We describe an exponential family model for word sense disambiguation. We posit a joint distribution over words w and senses s. 2.1 Notation We define a document d to be a sequence of words from some lexicon W; for the participation in this contest, a document consists of a sentence. Associated with each word is a sense from a lexicon S. In 272 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 272–276, Prague, June 2007. c�2007 Association for Computational Linguistics this work, our sense lexicon is the synsets of WordNet (Fellbaum and Miller, 2003), but our methods easily generalize to other sense lexicons, such as VerbNet (Kipper et al., 2000). Formally, we denote the sequence of words in a document d by wd = (wd,1, ... , wd,nd) and the sequence of synsets by sd = (sd,1, sd,2, ... , sd,nd), where nd denotes the number of words in the document. A corpus D is defined as a collection of documents. We also write w E s if w can be used to represent sense s. 2.2 An exponential family of words and senses We turn our attention to an exponential family of words and senses. The vector of parameters 7 = (r., A) consists of two blocks capturing de</context>
</contexts>
<marker>Fellbaum, Miller, 2003</marker>
<rawString>Christiane Fellbaum and George A. Miller. 2003. Morphosemantic links in WordNet. Traitement automatique de langue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael I Jordan</author>
<author>Zoubin Ghahramani</author>
<author>Tommi Jaakkola</author>
<author>Lawrence K Saul</author>
</authors>
<title>An introduction to variational methods for graphical models.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>2</issue>
<contexts>
<context position="9469" citStr="Jordan et al., 1999" startWordPosition="1595" endWordPosition="1598">family model. Our goal is to assign a synset si to every word wi in an unlabeled document d of length n. In this setting, the synsets are hidden variables. Thus, we assign synsets according to their posterior probability given the observed words: s� = argmax sESn Es, pq,n(st, w) , p7/,n(s, w) where the sum is over all possible sequences of synsets. This combinatorial sum renders exact inference computationally intractable. We discuss how to obtain the sense assignment using approximate inference. 4.1 Variational Inference To approximate the posterior over senses, we use variational inference (Jordan et al., 1999). In variational inference, one first chooses a family of distributions for which inference is computationlly tractable. Then the distribution in that family which best approximates the posterior distribution of interest is found. For our purposes, it is convenient to select q from the family of factorized multinomial distributions: q(s) = 11 qi(si) , i where each qi(si) is a multinomial distribution over all possible senses. Observe that findings is much simpler using q(s): one can find the argmax of each individual qi independently. It can be shown that the multinomial which minimizes the KL</context>
</contexts>
<marker>Jordan, Ghahramani, Jaakkola, Saul, 1999</marker>
<rawString>Michael I. Jordan, Zoubin Ghahramani, Tommi Jaakkola, and Lawrence K. Saul. 1999. An introduction to variational methods for graphical models. Machine Learning, 37(2):183–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael I Jordan</author>
</authors>
<title>Graphical models.</title>
<date>2004</date>
<journal>Statistical Science,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="1187" citStr="Jordan, 2004" startWordPosition="161" endWordPosition="162">mance of the model in its participation on the SemEval-2007 coarse- and fine-grained all-words tasks under a variety of parameters. 1 Introduction This paper describes an exponential family model suited to performing word sense disambiguation. Exponential family models are a mainstay of modern statistical modeling (Brown, 1986) and they are widely and successfully used for example in text classification (Berger et al., 1996). In statistical machine learning research, a general methodology and many algorithms were developed for undirected graphical model representation of exponential families (Jordan, 2004), providing a solid basis for efficient inference. Our model differs from other probabilistic models used for word sense disambiguation in that it captures not only word-sense co-occurrences but also contextual sense-sense co-occurrences, thereby breaking the naive Bayes assumption. Although spare in the types of features, the model is extremely expressive. Our model has parameters that control for word-sense interaction and sense-sense similarity, allowing us to capture many of the salient features of word and sense use. After fitting the parameters of our model from a labeled corpus, the tas</context>
</contexts>
<marker>Jordan, 2004</marker>
<rawString>Michael I. Jordan. 2004. Graphical models. Statistical Science, 19(1):140–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper</author>
<author>Hoa Trang Dang</author>
<author>Martha Palmer</author>
</authors>
<title>Class-Based Construction of a Verb Lexicon.</title>
<date>2000</date>
<booktitle>Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence table of contents,</booktitle>
<pages>691--696</pages>
<contexts>
<context position="3670" citStr="Kipper et al., 2000" startWordPosition="551" endWordPosition="554">sit a joint distribution over words w and senses s. 2.1 Notation We define a document d to be a sequence of words from some lexicon W; for the participation in this contest, a document consists of a sentence. Associated with each word is a sense from a lexicon S. In 272 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 272–276, Prague, June 2007. c�2007 Association for Computational Linguistics this work, our sense lexicon is the synsets of WordNet (Fellbaum and Miller, 2003), but our methods easily generalize to other sense lexicons, such as VerbNet (Kipper et al., 2000). Formally, we denote the sequence of words in a document d by wd = (wd,1, ... , wd,nd) and the sequence of synsets by sd = (sd,1, sd,2, ... , sd,nd), where nd denotes the number of words in the document. A corpus D is defined as a collection of documents. We also write w E s if w can be used to represent sense s. 2.2 An exponential family of words and senses We turn our attention to an exponential family of words and senses. The vector of parameters 7 = (r., A) consists of two blocks capturing dependence on word-synset co-occurrences, and synset co-occurrences. p?7,n(s, w) = expIEiκwi,si + Ei</context>
</contexts>
<marker>Kipper, Dang, Palmer, 2000</marker>
<rawString>Karin Kipper, Hoa Trang Dang, and Martha Palmer. 2000. Class-Based Construction of a Verb Lexicon. Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence table of contents, pages 691–696.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Finding predominant senses in untagged text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>280--287</pages>
<location>Barcelona,</location>
<contexts>
<context position="7416" citStr="McCarthy et al., 2004" startWordPosition="1239" endWordPosition="1242">a simple case when it is possible to estimate parameters maximizing the likelihood exactly, i.e., the case where our model depends only on word-synset co-occurrences and is parametrized solely by r. (setting A = 0). Using Eq. (1), with A = 0, we obtain exp�E J d,iκwd,i,sd,i l i. d Z.,nd (1) Fallback Let κWN w,s pr.(sD, wD) = 273 Thus, p,,(sD, wD) can be viewed as a multinomial model with Ed nd trials and |8 |outcomes, parametrized by κ,,,,s. The maximum likelihood estimates in this model are k,,,,s = ln p(s, w). This setting of the parameters corresponds precisely to the dominant-sense model (McCarthy et al., 2004). The resulting model is thus p.,n(s, w) = Hi Asi, wi) . (3) 3.2 Sense co-occurrence parameters λ Unlike κ, it is impossible to find a closed-form solution for the maximum-likelihood settings of λ. Therefore, we turn to intuitive methods. Observed synset co-occurrence One natural ad hoc statistic to use to compute the parameters λ are the empirical sense co-occurrences. In particular, we may set λsi,sj = λSF si,sj = ln�p(si, sj) . (4) We will observe in section 5 that the performance of λ = λSF actually degrades the performance of the system, especially when combined with κ = A. This can be un</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2004. Finding predominant senses in untagged text. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 280–287, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Claudia Leacock</author>
<author>Randee Tengi</author>
<author>Ross T Bunker</author>
</authors>
<title>A semantic concordance.</title>
<date>1993</date>
<booktitle>In 3rd DARPA Workshop on Human Language Technology.</booktitle>
<contexts>
<context position="5188" citStr="Miller et al., 1993" startWordPosition="823" endWordPosition="826">arbitrary context independent distribution between a word and its assigned synset. Sense co-occurrence parameters A Parameters A are the only parameters that establish the dependence of sense on its context. More specifically, they capture co-occurrences of synset pairs within a context. Larger values favor, whereas smaller values disfavor each pair of synsets. 3 Parameter estimation With the model in hand, we need to address two problems in order to use it for problems such as WSD. First, in parameter estimation, we find values of the parameters that explain a labeled corpus, such as SemCor (Miller et al., 1993). Once the parameters are fit, we use posterior inference to compute the posterior probability distribution of a set of senses given a set of unlabeled words in a context, p(s |w). This distribution is used to predict the senses of the words. In this section, it will be useful to introduce the notation ˜p(s, w) to denote the empirical probabilities of observing the word-sense pair s, w in the entire corpus: ˜p(s, w) = Ed,i δ(sd,i, s)δ(wd,i,w)/Ed nd , where δ(x, y) = 1 if x = y and 0 otherwise. Similarly, we will define ˜p(s) to denote the empirical probability of observing a sense s over the e</context>
</contexts>
<marker>Miller, Leacock, Tengi, Bunker, 1993</marker>
<rawString>George A. Miller, Claudia Leacock, Randee Tengi, and Ross T. Bunker. 1993. A semantic concordance. In 3rd DARPA Workshop on Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Meaningful clustering of senses helps boost word sense disambiguation performance.</title>
<date>2006</date>
<booktitle>In COLINGACL</booktitle>
<pages>105--112</pages>
<contexts>
<context position="2559" citStr="Navigli, 2006" startWordPosition="373" endWordPosition="374">al2007 on the coarse- and fine-grained all-words tasks. In both of these tasks, a series of sentences are given with certain words tagged. Each competing system must assign a sense from a sense inventory to the tagged words. In both tasks, performance was gauged by comparing the output of each system to human-tagged senses. In the fine-grained task, precision and recall were simply and directly computed against the golden annotations. However, in the coarse-grained task, the sense inventory was first clustered semi-automatically with each cluster representing an equivalence class over senses (Navigli, 2006). Precision and recall were computed against equivalence classes. This paper briefly derives the model and then explores its properties for WSD. We show how common algorithms, such as “dominant sense” and “most frequent sense,” can be expressed in the exponential family framework. We then proceed to present an evaluation of the developed techniques on the SemEval-2007 tasks in which we participated. 2 The model We describe an exponential family model for word sense disambiguation. We posit a joint distribution over words w and senses s. 2.1 Notation We define a document d to be a sequence of w</context>
</contexts>
<marker>Navigli, 2006</marker>
<rawString>Roberto Navigli. 2006. Meaningful clustering of senses helps boost word sense disambiguation performance. In COLINGACL 2006, pages 105–112, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>