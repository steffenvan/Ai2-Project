<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.024362">
<title confidence="0.997404">
Simultaneous dialogue act segmentation and labelling using lexical and
syntactic features
</title>
<author confidence="0.998896">
Ramon Granell, Stephen Pulman Carlos-D. Martinez-Hinarejos
</author>
<affiliation confidence="0.998282">
Oxford University Computing Laboratory, Instituto Tecnol´ogico de Inform´atica,
</affiliation>
<address confidence="0.984725">
Wolfson Building, Parks Road, Universidad Polit´ecnica de Valencia,
Oxford, OX1 3QD, England Camino de Vera, s/n, 46022, Valencia, Spain
</address>
<email confidence="0.992985">
ramg@comlab.ox.ac.uk cmartine@dsic.upv.es
sgp@clg.ox.ac.uk
</email>
<sectionHeader confidence="0.993813" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997346">
Segmentation of utterances and annotation
as dialogue acts can be helpful for sev-
eral modules of dialogue systems. In this
work, we study a statistical machine learn-
ing model to perform these tasks simulta-
neously using lexical features and incorpo-
rating deterministic syntactic restrictions.
There is a slight improvement in both seg-
mentation and labelling due to these re-
strictions.
</bodyText>
<sectionHeader confidence="0.998796" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999418270270271">
Dialogue acts (DA) are linguistic abstractions that
are commonly accepted and employed by the the
dialogue community. In the framework of dia-
logue systems, they can be helpful to identify and
model user intentions and system answers by the
dialogue manager. Furthermore, in other dialogue
modules such as the automatic speech recognizer
or speech synthesiser, DA information may be also
used to increase their performance.
Many researchers have studied automatic DA
labelling using different techniques. However, in
most of this work it is common to assume that the
dialogue turns are already segmented into separate
utterances, where each utterance corresponds to
just one DA label, as in (Stolcke et al (2000); Ji
and Bilmes (2005); Webb et al (2005)). This is
not a realistic situation because the segmentation
of turns into utterances is not a trivial problem.
There have been many previous approaches to
segmentation of turns prior to DA labelling, be-
ginning with (Stolcke and Shriberg (1996)). Typ-
ically some combination of words and part of
speech (POS) tags is used to predict segmentation
boundaries. In this work we make use of a sta-
tistical model to solve both the DA labelling task
and the segmentation task simultaneously, follow-
ing (Ang et al (2005); Mart´ınez-Hinarejos et al
(2006)). Our aim is to see whether going beyond
the word n-gram models can improve accuracy,
using syntactic information (constituent structure)
obtained from the dialogue transcriptions. We ex-
amine whether this information can improve the
segmentation of the dialogue turns into DA seg-
ments. Intuitively, it seems logical to believe that
most of these segments must coincide with partic-
ular syntactic structures, and that segment bound-
aries would respect constituent boundaries.
</bodyText>
<sectionHeader confidence="0.930517" genericHeader="method">
2 Dialogue data
</sectionHeader>
<bodyText confidence="0.966454125">
The dialogue corpus used to perform the exper-
iments is the Switchboard database (SWBD). It
consists of human-human conversations by tele-
phone about generic topics. There are 1155 5-
minute conversations, comprising approximately
205000 utterances and 1.4 million words. The size
of the vocabulary is approximately 22000 words.
All this corpus has been manually annotated at
the dialogue act level using the SWBD-DAMSL
scheme, (Jurafsky et al (1997)), consisting of 42
different labels. Every dialogue turn was manu-
ally segmented into utterances. The average num-
ber of segments (utterances) per dialogue turn is
1.78 with a standard deviation of 1.41. Each utter-
ance was assigned one SWBD-DAMSL label (see
Figure 1).
</bodyText>
<sectionHeader confidence="0.829335" genericHeader="method">
3 Syntactic analysis of DA segments
</sectionHeader>
<bodyText confidence="0.984373">
An initial analysis of the syntactic structures of the
dialogue data was performed to study their possi-
ble relevance for DA segmentation.
</bodyText>
<subsubsectionHeader confidence="0.636374">
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 333–336,
</subsubsectionHeader>
<affiliation confidence="0.93688">
Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics
</affiliation>
<page confidence="0.999828">
333
</page>
<bodyText confidence="0.98529">
- $LAUGH he waits until it gets about seventeen below up here $SEG and then he calls us , $SEG
sd sd
- he waits until it gets about seventeen below up here and then he calls us .
</bodyText>
<figureCaption confidence="0.9042985">
Figure 1: The first row is an original segmented dialogue turn, where the $SEG label indicates the end
of a DA segment. The second row contains the corresponding DA label for each segment, where ”sd”
corresponds to the SWBD-DAMSL label of Statement non-opinion. The third row is the input for the
parser.
</figureCaption>
<subsectionHeader confidence="0.999982">
3.1 Parsing of spontaneous dialogues
</subsectionHeader>
<bodyText confidence="0.999988444444444">
One of the main problems we face when we try
to syntactically analyse a corpus transcribed from
spontaneous speech by different people such as
SWBD corpus, is the inconsistency of annotation
conventions for spontaneous speech phenomena
and punctuation marks. This can be problematic
for parsers, as they work at the sentence level.
Some of the dialogue turns of the SWBD corpus
are not transcribed using consistent punctuation
conventions. We therefore carried out some pre-
processing so that all turns end with proper punc-
tuation marks. Additionally, the non-verbal labels
(e.g. $LAUGH, $OVERLAP, $SEG, ...) are re-
moved. In Figure 1 there is an example of this
process.
The Stanford Parser, (Klein and Manning
(2003)) was used for the syntactic analysis of the
transcriptions of SWBD dialogues. The English
grammar used to train the parser is based on the
standard LDC Penn Treebank WSJ training sec-
tions 2-21. Is is important to remark that the nature
of the training corpus (journalistic style reports)
is different from the transcriptions of spontaneous
speech conversations. We would therefore expect
a decrease in accuracy. As output of the parsing
process, a tree that contains syntactic structures
was provided (e.g. see Figure 2).
</bodyText>
<subsectionHeader confidence="0.999938">
3.2 Syntactic features and segmentation
</subsectionHeader>
<bodyText confidence="0.99960875">
As we are interested in studying the coincidence
of syntactic structures with DA segments, we will
select two general features for each word (see Fig-
ure 3):
</bodyText>
<listItem confidence="0.992963857142857">
• Most general syntactic category that starts
with a word, (MGSS), i.e., the root of the cur-
rent subtree of the syntactic analysis, (e.g. in
Figure 2, ”CC” is the MGSS of the first word
of the second segment, ”and”).
• Most general syntactic category that ends
with a word, (MGSE), i.e., the root of the
</listItem>
<equation confidence="0.95855525">
(ROOT
(S (: -)
(S
(NP (PRP he))
(VP (VBZ waits)
(SBAR (IN until)
(S
(NP (PRP it))
(VP (VBZ gets)
(PP (IN about)
(NP (NN seventeen)))
(PP (IN below)
(ADVP (RB up) (RB here))))))))
(CC and)
(S
(ADVP (RB then))
(NP (PRP he))
(VP (VBZ calls)
(NP (PRP us))))
(. .)))
</equation>
<figureCaption confidence="0.978148">
Figure 2: Example of the syntactic analysis of the
dialogue turn that appears in Figure 1.
</figureCaption>
<bodyText confidence="0.9998546875">
subtree of the syntactic analysis that ends
with that word, (e.g. in Figure 2, ”S” is
the MGSE of last word of the first segment,
”here”).
Using these features, we have analysed the syn-
tactic categories of boundary words of segments.
Particularly, it seems interesting to study MGSE of
last word of the segment and MGSS of first word
of the segment, because it indicates which syntac-
tic structure ends before the segment boundary and
which one starts after it. As there is always the be-
ginning of a segment with the first word of the turn
and the end of a segment with the last word of the
turn, we are ignoring these for the analysis, be-
cause we are looking for intra-turn segments. Re-
sults of this analysis can be seen in Table 1.
</bodyText>
<sectionHeader confidence="0.998422" genericHeader="method">
4 The model
</sectionHeader>
<bodyText confidence="0.999838">
The statistical model used to DA label and
segment the dialogues is extensively explained
in (Mart´ınez-Hinarejos (2008)). Basically, it is
</bodyText>
<page confidence="0.994462">
334
</page>
<table confidence="0.997973222222222">
ROOT+-+: $LAUGH S+he+NP VP+waits+VBZ SBAR+until+IN S+it+NP VP+gets+VBZ
PP+about+IN NP+seventeen+PP PP+below+IN ADVP+up+RB RB+here+S $SEG
CC+and+CC S+then+ADVP NP+he+NP VP+calls+VBZ NP+us+S .+.+ROOT $SEG
Figure 3: For each word of the example turn of Figure 1, MGSS (item before the word) and MGSE (item
after the word) are obtained from the tree of Figure 2. Non-verbal labels were reincorporated.
Occ MGSE Cat Occ MGSS Cat
% %
33516 37.1 , 30318 33.5 ROOT
30640 33.9 ROOT 19988 22.1 CC
7801 8.6 : 13275 14.7 NP
7134 7.9 S 10187 11.3 S
2687 3.0 NP 3508 3.9 SBAR
2319 2.6 PRN 3421 3.8 ADVP
750 0.8 VP 2034 2.2 VP
531 0.6 ADVP 1957 2.2 INTJ
478 0.5 PP 1300 1.4 UH
465 0.5 RB 972 1.1 PP
4078 4.5 Other 3481 3.8 Other
</table>
<tableCaption confidence="0.995645">
Table 1: Occurrences and percentage of the syn-
</tableCaption>
<bodyText confidence="0.968759222222222">
tactic categories that correspond with the most fre-
quent MGSE of the last segment word (except last
segment) and MGSS of the first segment word (ex-
cept first segment).
based on a combination of a Hidden Markov
Model at lexical level and a Language Model (n-
gram) at DA level. The Viterbi algorithm is used
to find the most likely sequence of DA labels ac-
cording to the trained models. The segmentation
is obtained from the jumps between DAs of this
sequence.
The previous section has shown that the MGSE
and MGSS for the segments boundary words are
concentrated in a small set of categories (see Ta-
ble 1). Therefore, one quick and easy way to in-
corporate this information to the existing model is
to add some restrictions during the decoding pro-
cess, giving the model:
</bodyText>
<equation confidence="0.9832376">
Pr(uk|uk−1
k−n−1) �
� Pr(Wsk
sk−1+1|uk)σ(xsk)
�
</equation>
<bodyText confidence="0.9930948">
where U is the sequence of DAs that we will get
from the annotation/segmentation process. The
search process produces a segmentation s =
(s0, s1, ... , sr), that divides the word sequence
W into the segments Ws1
</bodyText>
<equation confidence="0.841876666666667">
s0+1Ws2
s1+1 . . . Wsr
sr−1+1.
</equation>
<bodyText confidence="0.9720285">
Each segment is assigned to a DA ui that forms
the DA sequence U = u1 ... ur. xi corresponds
to the syntactic features of the i word that can be
MGSE, MGSS or both of them, and
</bodyText>
<equation confidence="0.990745666666667">
1 if xi E X
σ(xi) =
0 otherwise
</equation>
<bodyText confidence="0.99916">
where X can be a subset of all the possible syn-
tactic categories that correspond to:
</bodyText>
<listItem confidence="0.980992833333333">
1. the most frequent MGSE of last segment
word, if x is MGSE.
2. the most frequent MGSS of first segment
word, if x is MGSS
3. the most frequent combinations of both pre-
vious sets.
</listItem>
<bodyText confidence="0.9993306">
It means that we will only allow a segment end-
ing when the MGSE of a word is in this set, or
a start of a segment when the MGSS of the fol-
lowing word is in the corresponding set or both
conditions at the same time.
</bodyText>
<sectionHeader confidence="0.967875" genericHeader="evaluation">
5 Experiments and results
</sectionHeader>
<bodyText confidence="0.9996243">
Ten cross-validation experiments were performed
for each model using, in each experiment a train-
ing partition composed of 1136 dialogues and
a test set of 19 dialogues, as in (Stolcke et al
(2000); Webb et al (2005); Mart´ınez-Hinarejos
et al (2006)). The N-grams were obtained using
the SLM toolkit (Rosenfeld (1998)) with Good-
Turing discounting and the HMMs were trained
using the Baum-Welch algorithm. We use the fol-
lowing evaluation measures:
</bodyText>
<listItem confidence="0.983734375">
• To evaluate the labelling, we use the DA Er-
ror Rate (equivalent to Word Error Rate) and
the percentage of error labelling of whole
turns.
• For the segment evaluation, we only check
where the segments bounds are produced
(word position in the segment), making use
of F-score obtained from precision and recall.
</listItem>
<equation confidence="0.808154777777778">
�
U = arg max
U
r
H
k=1
max
r,sr1
{
</equation>
<page confidence="0.995893">
335
</page>
<bodyText confidence="0.962679333333333">
The results from using different sizes for the set
X are shown for labelling performance in Tables 2
and 3, and F-score of the segmentation in Table 4.
</bodyText>
<table confidence="0.99077875">
Model/SizeX 5 10 20 All
MGSE 53.31 54.76 54.60 54.76
MGSS 53.35 52.76 54.92 54.76
Both 53.58 52.84 54.76 54.76
</table>
<tableCaption confidence="0.8390636">
Table 2: DAER for models using MGSE, MGSS
and both features. SizeX indicates the size of the
set of most frequent categories accepted. Without
syntactic categories (baseline) we obtain a DAER
of 54.41.
</tableCaption>
<table confidence="0.999402">
Model/SizeX 5 10 20 All
MGSE 53.61 55.41 55.34 55.77
MGSS 53.61 53.32 55.63 55.77
Both 53.46 53.10 55.19 55.77
</table>
<tableCaption confidence="0.909055666666667">
Table 3: Percentage of error of labelling of com-
plete turns for all the possible models. The base-
line value is 55.41.
</tableCaption>
<table confidence="0.99935175">
Model / SizeX 5 10 20 All
MGSE 73.08 71.18 71.44 71.17
MGSS 73.60 73.72 71.44 71.17
Both 74.36 74.08 71.75 71.16
</table>
<tableCaption confidence="0.9397895">
Table 4: F-score of segmentation. The baseline
value is 71.17.
</tableCaption>
<sectionHeader confidence="0.991537" genericHeader="conclusions">
6 Discussion and future work
</sectionHeader>
<bodyText confidence="0.999989">
In this work, we have used lexical and syntactic
features for labelling and segmenting DAs simul-
taneously. Syntactic features obtained automati-
cally were deterministically applied during the sta-
tistical decoding process. There is a slight im-
provement using syntactic information, obtaining
better results than reported in other work such
as (Martfnez-Hinarejos et al (2006)). The F-score
of the segmentation improves 3% using the syn-
tactic features, however values are slightly worse
(2%) than results in (Stolcke and Shriberg (1996)).
As future work, we think that incorporating the
syntactic information in a non-deterministic way
might further improve the annotation and segmen-
tation scores. Furthermore, it is possible to make
use of additional information from the syntactic
structure, rather than just the boundary informa-
tion we are currently using. Finally, an evalua-
tion over different corpora must be done to check
both the performance of the proposed model and
the reusability of the syntactic sets.
</bodyText>
<sectionHeader confidence="0.996557" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9994968">
This work was partially funded by the Compan-
ions project (http://www.companions-project.org)
sponsored by the European Commission as part of
the Information Society Technologies (IST) pro-
gramme under EC grant number IST-FP6-034434.
</bodyText>
<sectionHeader confidence="0.99943" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999910594594595">
Ang J., Liu Y., Shriberg E. 2005. Automatic Dialog Act
Segmentation and Classification in Multiparty Meet-
ings. Proc. ICASSP, Philadelphia, USA, pp. 1061-
1064
Ji, G and Bilmes, J. 2005. Dialog act tagging using
graphical models. Proc. ICASSP, Philadelphia, USA
Jurafsky, D. Shriberg, E., Biasca, D. 1997. Switchboard
swbd-damsl shallow- discourse-function annotation
coders manual. Tech. Rep. 97-01, University of Col-
orado Institute of Cognitive Science
Klein D. and Manning, C. D. 2003. Accurate Unlex-
icalized Parsing. Proc. ACL, Sapporo, Japan, pp.
423-430
Martfnez-Hinarejos, C. D., Granell, R., Benedf, J. M.
2006. Segmented and unsegmented dialogue-act
annotation with statistical dialogue models. Proc.
COLING/ACL Sydney, Australia, pp. 563-570
Martfnez-Hinarejos, C. D., Benedf, J. M., Granell, R.
2008. Statistical framework for a spanish spoken
dialogue corpus. Speech Communication, vol. 50,
number 11-12, pp. 992-1008
Rosenfeld, R. 1998. The cmu-cambridge statistical
language modelling toolkit v2. Technical report,
Carnegie Mellon University
Stolcke, A. and Shriberg, E. 1996. Automatic linguis-
tic segmentation of conversational speech. Proc. of
ICSLP, Philadelphia, USA
Stolcke, A., Coccaro, N., Bates, R., Taylor, P., van
Ess-Dykema, C., Ries, K., Shriberg, E., Jurafsky,
D., Martin, R., Meteer, M. 2000. Dialogue act
modelling for automatic tagging and recognition
of conversational speech. Computational Linguistics
26 (3), 1-34
Webb, N., Hepple, M., Wilks, Y. 2005. Dialogue act
classification using intra-utterance features. Proc. of
the AAAI Workshop on Spoken Language Under-
standing. Pittsburgh, USA
</reference>
<page confidence="0.999144">
336
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.848742">
<title confidence="0.9953035">Simultaneous dialogue act segmentation and labelling using lexical syntactic features</title>
<author confidence="0.997256">Ramon Granell</author>
<author confidence="0.997256">Stephen Pulman Carlos-D Martinez-Hinarejos</author>
<affiliation confidence="0.9786705">Oxford University Computing Laboratory, Instituto Tecnol´ogico de Inform´atica, Wolfson Building, Parks Road, Universidad Polit´ecnica de Valencia,</affiliation>
<address confidence="0.999785">Oxford, OX1 3QD, England Camino de Vera, s/n, 46022, Valencia, Spain</address>
<email confidence="0.9909205">ramg@comlab.ox.ac.uksgp@clg.ox.ac.uk</email>
<abstract confidence="0.991957363636363">Segmentation of utterances and annotation as dialogue acts can be helpful for several modules of dialogue systems. In this work, we study a statistical machine learning model to perform these tasks simultaneously using lexical features and incorporating deterministic syntactic restrictions. There is a slight improvement in both segmentation and labelling due to these restrictions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Ang</author>
<author>Y Liu</author>
<author>E Shriberg</author>
</authors>
<title>Automatic Dialog Act Segmentation and Classification in Multiparty Meetings.</title>
<date>2005</date>
<booktitle>Proc. ICASSP,</booktitle>
<pages>1061--1064</pages>
<location>Philadelphia, USA,</location>
<contexts>
<context position="2096" citStr="Ang et al (2005)" startWordPosition="311" endWordPosition="314">ce corresponds to just one DA label, as in (Stolcke et al (2000); Ji and Bilmes (2005); Webb et al (2005)). This is not a realistic situation because the segmentation of turns into utterances is not a trivial problem. There have been many previous approaches to segmentation of turns prior to DA labelling, beginning with (Stolcke and Shriberg (1996)). Typically some combination of words and part of speech (POS) tags is used to predict segmentation boundaries. In this work we make use of a statistical model to solve both the DA labelling task and the segmentation task simultaneously, following (Ang et al (2005); Mart´ınez-Hinarejos et al (2006)). Our aim is to see whether going beyond the word n-gram models can improve accuracy, using syntactic information (constituent structure) obtained from the dialogue transcriptions. We examine whether this information can improve the segmentation of the dialogue turns into DA segments. Intuitively, it seems logical to believe that most of these segments must coincide with particular syntactic structures, and that segment boundaries would respect constituent boundaries. 2 Dialogue data The dialogue corpus used to perform the experiments is the Switchboard datab</context>
</contexts>
<marker>Ang, Liu, Shriberg, 2005</marker>
<rawString>Ang J., Liu Y., Shriberg E. 2005. Automatic Dialog Act Segmentation and Classification in Multiparty Meetings. Proc. ICASSP, Philadelphia, USA, pp. 1061-1064</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ji</author>
<author>J Bilmes</author>
</authors>
<title>Dialog act tagging using graphical models.</title>
<date>2005</date>
<booktitle>Proc. ICASSP,</booktitle>
<location>Philadelphia, USA</location>
<contexts>
<context position="1566" citStr="Ji and Bilmes (2005)" startWordPosition="221" endWordPosition="224">ity. In the framework of dialogue systems, they can be helpful to identify and model user intentions and system answers by the dialogue manager. Furthermore, in other dialogue modules such as the automatic speech recognizer or speech synthesiser, DA information may be also used to increase their performance. Many researchers have studied automatic DA labelling using different techniques. However, in most of this work it is common to assume that the dialogue turns are already segmented into separate utterances, where each utterance corresponds to just one DA label, as in (Stolcke et al (2000); Ji and Bilmes (2005); Webb et al (2005)). This is not a realistic situation because the segmentation of turns into utterances is not a trivial problem. There have been many previous approaches to segmentation of turns prior to DA labelling, beginning with (Stolcke and Shriberg (1996)). Typically some combination of words and part of speech (POS) tags is used to predict segmentation boundaries. In this work we make use of a statistical model to solve both the DA labelling task and the segmentation task simultaneously, following (Ang et al (2005); Mart´ınez-Hinarejos et al (2006)). Our aim is to see whether going b</context>
</contexts>
<marker>Ji, Bilmes, 2005</marker>
<rawString>Ji, G and Bilmes, J. 2005. Dialog act tagging using graphical models. Proc. ICASSP, Philadelphia, USA</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Shriberg Jurafsky</author>
<author>E Biasca</author>
<author>D</author>
</authors>
<title>Switchboard swbd-damsl shallow- discourse-function annotation coders manual.</title>
<date>1997</date>
<tech>Tech. Rep. 97-01,</tech>
<institution>University of Colorado Institute of Cognitive Science</institution>
<contexts>
<context position="3068" citStr="Jurafsky et al (1997)" startWordPosition="456" endWordPosition="459">to believe that most of these segments must coincide with particular syntactic structures, and that segment boundaries would respect constituent boundaries. 2 Dialogue data The dialogue corpus used to perform the experiments is the Switchboard database (SWBD). It consists of human-human conversations by telephone about generic topics. There are 1155 5- minute conversations, comprising approximately 205000 utterances and 1.4 million words. The size of the vocabulary is approximately 22000 words. All this corpus has been manually annotated at the dialogue act level using the SWBD-DAMSL scheme, (Jurafsky et al (1997)), consisting of 42 different labels. Every dialogue turn was manually segmented into utterances. The average number of segments (utterances) per dialogue turn is 1.78 with a standard deviation of 1.41. Each utterance was assigned one SWBD-DAMSL label (see Figure 1). 3 Syntactic analysis of DA segments An initial analysis of the syntactic structures of the dialogue data was performed to study their possible relevance for DA segmentation. Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 333–336, Queen Mary University of London, </context>
</contexts>
<marker>Jurafsky, Biasca, D, 1997</marker>
<rawString>Jurafsky, D. Shriberg, E., Biasca, D. 1997. Switchboard swbd-damsl shallow- discourse-function annotation coders manual. Tech. Rep. 97-01, University of Colorado Institute of Cognitive Science</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>Proc. ACL,</booktitle>
<pages>423--430</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="4974" citStr="Klein and Manning (2003)" startWordPosition="769" endWordPosition="772">rom spontaneous speech by different people such as SWBD corpus, is the inconsistency of annotation conventions for spontaneous speech phenomena and punctuation marks. This can be problematic for parsers, as they work at the sentence level. Some of the dialogue turns of the SWBD corpus are not transcribed using consistent punctuation conventions. We therefore carried out some preprocessing so that all turns end with proper punctuation marks. Additionally, the non-verbal labels (e.g. $LAUGH, $OVERLAP, $SEG, ...) are removed. In Figure 1 there is an example of this process. The Stanford Parser, (Klein and Manning (2003)) was used for the syntactic analysis of the transcriptions of SWBD dialogues. The English grammar used to train the parser is based on the standard LDC Penn Treebank WSJ training sections 2-21. Is is important to remark that the nature of the training corpus (journalistic style reports) is different from the transcriptions of spontaneous speech conversations. We would therefore expect a decrease in accuracy. As output of the parsing process, a tree that contains syntactic structures was provided (e.g. see Figure 2). 3.2 Syntactic features and segmentation As we are interested in studying the </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Klein D. and Manning, C. D. 2003. Accurate Unlexicalized Parsing. Proc. ACL, Sapporo, Japan, pp. 423-430</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Martfnez-Hinarejos</author>
<author>R Granell</author>
<author>J M Benedf</author>
</authors>
<title>Segmented and unsegmented dialogue-act annotation with statistical dialogue models.</title>
<date>2006</date>
<booktitle>Proc. COLING/ACL</booktitle>
<pages>563--570</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="11847" citStr="Martfnez-Hinarejos et al (2006)" startWordPosition="1996" endWordPosition="1999">ll the possible models. The baseline value is 55.41. Model / SizeX 5 10 20 All MGSE 73.08 71.18 71.44 71.17 MGSS 73.60 73.72 71.44 71.17 Both 74.36 74.08 71.75 71.16 Table 4: F-score of segmentation. The baseline value is 71.17. 6 Discussion and future work In this work, we have used lexical and syntactic features for labelling and segmenting DAs simultaneously. Syntactic features obtained automatically were deterministically applied during the statistical decoding process. There is a slight improvement using syntactic information, obtaining better results than reported in other work such as (Martfnez-Hinarejos et al (2006)). The F-score of the segmentation improves 3% using the syntactic features, however values are slightly worse (2%) than results in (Stolcke and Shriberg (1996)). As future work, we think that incorporating the syntactic information in a non-deterministic way might further improve the annotation and segmentation scores. Furthermore, it is possible to make use of additional information from the syntactic structure, rather than just the boundary information we are currently using. Finally, an evaluation over different corpora must be done to check both the performance of the proposed model and t</context>
</contexts>
<marker>Martfnez-Hinarejos, Granell, Benedf, 2006</marker>
<rawString>Martfnez-Hinarejos, C. D., Granell, R., Benedf, J. M. 2006. Segmented and unsegmented dialogue-act annotation with statistical dialogue models. Proc. COLING/ACL Sydney, Australia, pp. 563-570</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Martfnez-Hinarejos</author>
<author>J M Benedf</author>
<author>R Granell</author>
</authors>
<title>Statistical framework for a spanish spoken dialogue corpus.</title>
<date>2008</date>
<journal>Speech Communication,</journal>
<volume>50</volume>
<pages>11--12</pages>
<marker>Martfnez-Hinarejos, Benedf, Granell, 2008</marker>
<rawString>Martfnez-Hinarejos, C. D., Benedf, J. M., Granell, R. 2008. Statistical framework for a spanish spoken dialogue corpus. Speech Communication, vol. 50, number 11-12, pp. 992-1008</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
</authors>
<title>The cmu-cambridge statistical language modelling toolkit v2.</title>
<date>1998</date>
<tech>Technical report,</tech>
<institution>Carnegie Mellon University</institution>
<contexts>
<context position="10092" citStr="Rosenfeld (1998)" startWordPosition="1697" endWordPosition="1698"> 3. the most frequent combinations of both previous sets. It means that we will only allow a segment ending when the MGSE of a word is in this set, or a start of a segment when the MGSS of the following word is in the corresponding set or both conditions at the same time. 5 Experiments and results Ten cross-validation experiments were performed for each model using, in each experiment a training partition composed of 1136 dialogues and a test set of 19 dialogues, as in (Stolcke et al (2000); Webb et al (2005); Mart´ınez-Hinarejos et al (2006)). The N-grams were obtained using the SLM toolkit (Rosenfeld (1998)) with GoodTuring discounting and the HMMs were trained using the Baum-Welch algorithm. We use the following evaluation measures: • To evaluate the labelling, we use the DA Error Rate (equivalent to Word Error Rate) and the percentage of error labelling of whole turns. • For the segment evaluation, we only check where the segments bounds are produced (word position in the segment), making use of F-score obtained from precision and recall. � U = arg max U r H k=1 max r,sr1 { 335 The results from using different sizes for the set X are shown for labelling performance in Tables 2 and 3, and F-sco</context>
</contexts>
<marker>Rosenfeld, 1998</marker>
<rawString>Rosenfeld, R. 1998. The cmu-cambridge statistical language modelling toolkit v2. Technical report, Carnegie Mellon University</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>E Shriberg</author>
</authors>
<title>Automatic linguistic segmentation of conversational speech.</title>
<date>1996</date>
<booktitle>Proc. of ICSLP,</booktitle>
<location>Philadelphia, USA</location>
<contexts>
<context position="1830" citStr="Stolcke and Shriberg (1996)" startWordPosition="264" endWordPosition="267">tion may be also used to increase their performance. Many researchers have studied automatic DA labelling using different techniques. However, in most of this work it is common to assume that the dialogue turns are already segmented into separate utterances, where each utterance corresponds to just one DA label, as in (Stolcke et al (2000); Ji and Bilmes (2005); Webb et al (2005)). This is not a realistic situation because the segmentation of turns into utterances is not a trivial problem. There have been many previous approaches to segmentation of turns prior to DA labelling, beginning with (Stolcke and Shriberg (1996)). Typically some combination of words and part of speech (POS) tags is used to predict segmentation boundaries. In this work we make use of a statistical model to solve both the DA labelling task and the segmentation task simultaneously, following (Ang et al (2005); Mart´ınez-Hinarejos et al (2006)). Our aim is to see whether going beyond the word n-gram models can improve accuracy, using syntactic information (constituent structure) obtained from the dialogue transcriptions. We examine whether this information can improve the segmentation of the dialogue turns into DA segments. Intuitively, </context>
<context position="12007" citStr="Stolcke and Shriberg (1996)" startWordPosition="2021" endWordPosition="2024">6 Table 4: F-score of segmentation. The baseline value is 71.17. 6 Discussion and future work In this work, we have used lexical and syntactic features for labelling and segmenting DAs simultaneously. Syntactic features obtained automatically were deterministically applied during the statistical decoding process. There is a slight improvement using syntactic information, obtaining better results than reported in other work such as (Martfnez-Hinarejos et al (2006)). The F-score of the segmentation improves 3% using the syntactic features, however values are slightly worse (2%) than results in (Stolcke and Shriberg (1996)). As future work, we think that incorporating the syntactic information in a non-deterministic way might further improve the annotation and segmentation scores. Furthermore, it is possible to make use of additional information from the syntactic structure, rather than just the boundary information we are currently using. Finally, an evaluation over different corpora must be done to check both the performance of the proposed model and the reusability of the syntactic sets. Acknowledgments This work was partially funded by the Companions project (http://www.companions-project.org) sponsored by </context>
</contexts>
<marker>Stolcke, Shriberg, 1996</marker>
<rawString>Stolcke, A. and Shriberg, E. 1996. Automatic linguistic segmentation of conversational speech. Proc. of ICSLP, Philadelphia, USA</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>N Coccaro</author>
<author>R Bates</author>
<author>P Taylor</author>
<author>C van Ess-Dykema</author>
<author>K Ries</author>
<author>E Shriberg</author>
<author>D Jurafsky</author>
<author>R Martin</author>
<author>M Meteer</author>
</authors>
<title>Dialogue act modelling for automatic tagging and recognition of conversational speech.</title>
<date>2000</date>
<journal>Computational Linguistics</journal>
<volume>26</volume>
<issue>3</issue>
<pages>1--34</pages>
<marker>Stolcke, Coccaro, Bates, Taylor, van Ess-Dykema, Ries, Shriberg, Jurafsky, Martin, Meteer, 2000</marker>
<rawString>Stolcke, A., Coccaro, N., Bates, R., Taylor, P., van Ess-Dykema, C., Ries, K., Shriberg, E., Jurafsky, D., Martin, R., Meteer, M. 2000. Dialogue act modelling for automatic tagging and recognition of conversational speech. Computational Linguistics 26 (3), 1-34</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Webb</author>
<author>M Hepple</author>
<author>Y Wilks</author>
</authors>
<title>Dialogue act classification using intra-utterance features.</title>
<date>2005</date>
<booktitle>Proc. of the AAAI Workshop on Spoken Language Understanding.</booktitle>
<location>Pittsburgh, USA</location>
<contexts>
<context position="1585" citStr="Webb et al (2005)" startWordPosition="225" endWordPosition="228">of dialogue systems, they can be helpful to identify and model user intentions and system answers by the dialogue manager. Furthermore, in other dialogue modules such as the automatic speech recognizer or speech synthesiser, DA information may be also used to increase their performance. Many researchers have studied automatic DA labelling using different techniques. However, in most of this work it is common to assume that the dialogue turns are already segmented into separate utterances, where each utterance corresponds to just one DA label, as in (Stolcke et al (2000); Ji and Bilmes (2005); Webb et al (2005)). This is not a realistic situation because the segmentation of turns into utterances is not a trivial problem. There have been many previous approaches to segmentation of turns prior to DA labelling, beginning with (Stolcke and Shriberg (1996)). Typically some combination of words and part of speech (POS) tags is used to predict segmentation boundaries. In this work we make use of a statistical model to solve both the DA labelling task and the segmentation task simultaneously, following (Ang et al (2005); Mart´ınez-Hinarejos et al (2006)). Our aim is to see whether going beyond the word n-gr</context>
<context position="9990" citStr="Webb et al (2005)" startWordPosition="1681" endWordPosition="1684"> MGSE of last segment word, if x is MGSE. 2. the most frequent MGSS of first segment word, if x is MGSS 3. the most frequent combinations of both previous sets. It means that we will only allow a segment ending when the MGSE of a word is in this set, or a start of a segment when the MGSS of the following word is in the corresponding set or both conditions at the same time. 5 Experiments and results Ten cross-validation experiments were performed for each model using, in each experiment a training partition composed of 1136 dialogues and a test set of 19 dialogues, as in (Stolcke et al (2000); Webb et al (2005); Mart´ınez-Hinarejos et al (2006)). The N-grams were obtained using the SLM toolkit (Rosenfeld (1998)) with GoodTuring discounting and the HMMs were trained using the Baum-Welch algorithm. We use the following evaluation measures: • To evaluate the labelling, we use the DA Error Rate (equivalent to Word Error Rate) and the percentage of error labelling of whole turns. • For the segment evaluation, we only check where the segments bounds are produced (word position in the segment), making use of F-score obtained from precision and recall. � U = arg max U r H k=1 max r,sr1 { 335 The results fro</context>
</contexts>
<marker>Webb, Hepple, Wilks, 2005</marker>
<rawString>Webb, N., Hepple, M., Wilks, Y. 2005. Dialogue act classification using intra-utterance features. Proc. of the AAAI Workshop on Spoken Language Understanding. Pittsburgh, USA</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>