<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.035432">
<title confidence="0.995211">
Using entity features to classify implicit discourse relations
</title>
<author confidence="0.996779">
Annie Louis, Aravind Joshi, Rashmi Prasad, Ani Nenkova
</author>
<affiliation confidence="0.998817">
University of Pennsylvania
</affiliation>
<address confidence="0.520092">
Philadelphia, PA 19104, USA
</address>
<email confidence="0.99834">
{lannie,joshi,rjprasad,nenkova}@seas.upenn.edu
</email>
<sectionHeader confidence="0.994787" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999945166666667">
We report results on predicting the sense
of implicit discourse relations between ad-
jacent sentences in text. Our investigation
concentrates on the association between
discourse relations and properties of the
referring expressions that appear in the re-
lated sentences. The properties of inter-
est include coreference information, gram-
matical role, information status and syn-
tactic form of referring expressions. Pre-
dicting the sense of implicit discourse re-
lations based on these features is consid-
erably better than a random baseline and
several of the most discriminative features
conform with linguistic intuitions. How-
ever, these features do not perform as well
as lexical features traditionally used for
sense prediction.
</bodyText>
<sectionHeader confidence="0.998781" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999951229166667">
Coherent text is described in terms of discourse re-
lations such as “cause” and “contrast” between its
constituent clauses. It is also characterized by en-
tity coherence, where the connectedness of the text
is created by virtue of the mentioned entities and
the properties of referring expressions. We aim to
investigate the association between discourse rela-
tions and the way in which references to entities
are realized. In our work, we employ features re-
lated to entity realization to automatically identify
discourse relations in text.
We focus on implicit relations that hold be-
tween adjacent sentences in the absence of dis-
course connectives such as “because” or “but”.
Previous studies on this task have zeroed in on
lexical indicators of relation sense: dependencies
between words (Marcu and Echihabi, 2001; Blair-
Goldensohn et al., 2007) and the semantic orien-
tation of words (Pitler et al., 2009), or on general
syntactic regularities (Lin et al., 2009).
The role of entities has also been hypothesized
as important for this task and entity-related fea-
tures have been used alongside others (Corston-
Oliver, 1998; Sporleder and Lascarides, 2008).
Corpus studies and reading time experiments per-
formed by Wolf and Gibson (2006) have in fact
demonstrated that the type of discourse relation
linking two clauses influences the resolution of
pronouns in them. However, the predictive power
of entity-related features has not been studied in-
dependently of other factors. Further motivation
for studying this type of features comes from new
corpus evidence (Prasad et al., 2008), that about a
quarter of all adjacent sentences are linked purely
by entity coherence, solely because they talk about
the same entity. Entity-related features would be
expected to better separate out such relations.
We present the first comprehensive study of the
connection between entity features and discourse
relations. We show that there are notable differ-
ences in properties of referring expressions across
the different relations. Sense prediction can be
done with results better than random baseline us-
ing only entity realization information. Their per-
formance, however, is lower than a knowledge-
poor approach using only the words in the sen-
tences as features. The addition of entity features
to these basic word features is also not beneficial.
</bodyText>
<sectionHeader confidence="0.985946" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999586727272727">
We use 590 Wall Street Journal (WSJ) articles
with overlapping annotations for discourse, coref-
erence and syntax from three corpora.
The Penn Discourse Treebank (PDTB) (Prasad
et al., 2008) is the largest available resource of
discourse relation annotations. In the PDTB, im-
plicit relations are annotated between adjacent
sentences in the same paragraph. They are as-
signed senses from a hierarchy containing four top
level categories–Comparison, Contingency, Tem-
poral and Expansion.
</bodyText>
<subsubsectionHeader confidence="0.6763">
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 59–62,
</subsubsectionHeader>
<affiliation confidence="0.891171">
The University of Tokyo, September 24-25, 2010. p@c 2010 Association for Computational Linguistics
</affiliation>
<page confidence="0.999713">
59
</page>
<bodyText confidence="0.9850199">
An example “Contingency” relation is shown
below. Here, the second sentence provides the
cause for the belief expressed in the first.
Ex 1. These rate indications aren’t directly comparable.
Lending practices vary widely by location.
Adjacent sentences can also become related
solely by talking about a common entity without
any of the above discourse relation links between
their propositions. Such pairs are annotated as En-
tity Relations (EntRels) in the PDTB, for example:
</bodyText>
<figureCaption confidence="0.46616">
Ex 2. Rolls-Royce Motor Cars Inc. said it expects its U.S
sales to remain steady at about 1,200 cars in 1990. The luxury
auto maker last year sold 1,214 cars in the U.S.
</figureCaption>
<bodyText confidence="0.999970866666667">
We use the coreference annotations from the
Ontonotes corpus (version 2.9) (Hovy et al., 2006)
to compute our gold-standard entity features. The
WSJ portion of this corpus contains 590 articles.
Here, nominalizations and temporal expressions
are also annotated for coreference but we use the
links between noun phrases only. We expect these
features computed on the gold-standard annota-
tions to represent an upper bound on the perfor-
mance of entity features.
Finally, the Penn Treebank corpus (Marcus et
al., 1994) is used to obtain gold-standard parse and
grammatical role information.
Only adjacent sentences within the same para-
graph are used in our experiments.
</bodyText>
<sectionHeader confidence="0.999638" genericHeader="method">
3 Entity-related features
</sectionHeader>
<bodyText confidence="0.99999275">
We associate each referring expression in a sen-
tence with a set of attributes as described below.
In Section 3.2, we detail how we combine these
attributes to compute features for a sentence pair.
</bodyText>
<subsectionHeader confidence="0.999727">
3.1 Referring expression attributes
</subsectionHeader>
<bodyText confidence="0.989609081967213">
Grammatical role. In exploratory analysis of
Comparison relations, we often observed parallel
syntactic realizations for entities in the subject po-
sition of the two sentences:
Ex 3. {Longer maturities}E1 are thought to indicate de-
clining interest rates. {Shorter maturities}E2 are considered
a sign of rising rates because portfolio managers can capture
higher rates sooner.
So, for each noun phrase, we record whether
it is the subject of a main clause (msubj), subject
of other clauses in the sentence (esubj) or a noun
phrase not in subject position (other).
Given vs. New. When an entity is first intro-
duced in the text, it is considered a new entity.
Subsequent mentions of the same entity are given
(Prince, 1992). New-given distinction could help
to identify some of the Expansion and Entity re-
lations. When a sentence elaborates on another, it
might contain a greater number of new entities.
We use the Ontonotes coreference annotations
to mark the information status for entities. For
an entity, if an antecedent is found in the previ-
ous sentences, it is marked as given, otherwise it
is a new entity.
Syntactic realization. In Entity relations, the sec-
ond sentence provides more information about a
specific entity in the first and a definite description
for this second mention seems likely. Also, given
the importance of named entities in news, entities
with proper names might be the ones frequently
described using Entity relations.
We use the part of speech (POS) tag associated
with the head of the noun phrase to assign one of
the following categories: pronoun, nominal, name
or expletive. When the head does not belong to
the above classes, we simply record its POS tag.
We also mark whether the noun phrase is a definite
description using the presence of the article ‘the’.
Modification. We expected modification proper-
ties to be most useful for predicting Comparison
relations. Also, named or new entities in Entity
relations are very likely to have post modification.
We record whether there are premodifiers or
postmodifiers in a given referring expression. In
the absence of pre- and postmodifiers, we indicate
bare head realization.
Topicalization. Preposed prepositional or ad-
verbial phrases before the subject of a sentence
indicate the topic under which the sentence is
framed. We observed that this property is frequent
in Comparison and Temporal relations. An exam-
ple Comparison is shown below.
Ex 4. {Under British rules}T1, Blue Arrow was able to
write off at once $1.15 billion in goodwill arising from the
purchase. {As a US-based company}T2, Blue Arrow would
have to amortize the good will over as many as 40 years, cre-
ating a continuing drag on reported earnings.
When the left sibling of a referring expression is
a topicalized phrase, we mark the topic attribute.
Number. Using the POS tag of the head word, we
note whether the entity is singular or plural.
</bodyText>
<subsectionHeader confidence="0.787109">
3.2 Features for classification
</subsectionHeader>
<bodyText confidence="0.9994045">
Next, for each sentence pair, we associate two sets
of features using the attributes described above.
</bodyText>
<page confidence="0.992565">
60
</page>
<bodyText confidence="0.999984351351351">
Let S1 and S2 denote the two adjacent sentences
in a relation, where S1 occurs first in the text.
Sentence level. These features characterize S1
and S2 individually. For each sentence, we add a
feature for each of the attributes described above.
The value of the feature is the number of times that
attribute is observed in the sentence; i.e., the fea-
ture S1given would have a value of 3 if there are 3
given entities in the first sentence.
Sentence pair. These features capture the interac-
tions between the entities present in S1 and S2.
Firstly, for each pair of entities (a, b), such that
a appears in S1 and b appears in S2, we assign
one of the following classes: (i) SAME: a and b
are coreferent, (ii) RELATED: their head words are
identical, (iii) DIFFERENT: neither coreferent nor
related. The RELATED category was introduced to
capture the parallelism often present in Compari-
son relations. Even though the entities themselves
are not coreferent, they share the same head word
(i.e. longer maturities and shorter maturities).
For features, we use the combination of the
class ((i), (ii) or (iii)) with the cross product of
the attributes for a and b. For example if a has
attributes {msubj, noun, ...} and b has attributes
{esubj, defdesc, ...} and a and b are corefer-
ent, we would increment the count for features–
{sameS1msubjS2esubj, sameS1msubjS2defdesc,
sameS1nounS2esubj, sameS1nounS2defdesc ...}.
Our total set of features observed for instances
in the training data is about 2000.
We experimented with two variants of fea-
tures: one using coreference annotations from
the Ontonotes corpus (gold-standard) and an-
other based on approximate coreference informa-
tion where entities with identical head words are
marked as coreferent.
</bodyText>
<sectionHeader confidence="0.997366" genericHeader="method">
4 Experimental setup
</sectionHeader>
<bodyText confidence="0.997841384615385">
We define five classification tasks which disam-
biguate if a specific PDTB relation holds between
adjacent sentences. In each task, we classify the
relation of interest (positive) versus a category
with a naturally occurring distribution of all of the
other relations (negative).
Sentence pairs from sections 0 to 22 of WSJ are
used as training data and we test on sections 23
and 24. Given the skewed distribution of positive
and negative examples for each task, we randomly
downsample the negative instances in the training
set to be equal to the positive examples. The sizes
of training sets for the tasks are
</bodyText>
<equation confidence="0.5490334">
Expansion vs other (4716)
Contingency vs other (2466)
Comparison vs other (1138)
Temporal vs other (474)
EntRel vs other (2378)
</equation>
<bodyText confidence="0.999950833333333">
Half of these examples are positive and the
other negative in each case.
The test set contains 1002 sentence pairs:
Comp. (133), Cont. (230), Temp. (34), Expn.
(369), EntRel (229), NoRel1 (7). We do not down-
sample our test set. Instead, we evaluate our pre-
dictions on the natural distribution present in the
data to get a realistic estimate of performance.
We train a linear SVM classifier (LIBLIN-
EAR2) for each task.3 The optimum regulariza-
tion parameter was chosen using cross validation
on the training data.
</bodyText>
<sectionHeader confidence="0.999966" genericHeader="evaluation">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.999965">
5.1 Feature analysis
</subsectionHeader>
<bodyText confidence="0.9996106">
We ranked the features (based on gold-standard
coreference information) in the training sets by
their information gain. We then checked which
attributes are common among the top five features
for different classification tasks.
As we had expected, the topicalization attribute
and RELATED entities frequently appear among
the top features for Comparison.
Features with the name attribute were highly
predictive of Entity relations as hypothesized.
However, while we had expected Entity relations
to have a high rate of coreference, we found coref-
erent mentions to be very indicative of Temporal
relations: all the top features involve the SAME at-
tribute. A post-analysis showed that close to 70%
of Temporal relations involve coreferent entities
compared to around 50% for the other classes.
The number of pronouns in the second sentence
was most characteristic of the Contingency rela-
tion. In the training set for Contingency task,
about 45% of sentences pairs belonging to Contin-
gency relation have a pronoun in the second sen-
tence. This is considerably larger than 32%, which
is the percentage of sentence pairs in the negative
examples with a pronoun in second sentence.
</bodyText>
<footnote confidence="0.995133833333333">
1PDTB relation for sentence pair when both entity and
discourse relations are absent, very rare about 1% of our data.
2http://www.csie.ntu.edu.tw/÷cjlin/liblinear/
3SVMs with linear kernel gave the best performance. We
also experimented with SVMs with radial basis kernel, Naive
Bayes and MaxEnt classifiers.
</footnote>
<page confidence="0.996301">
61
</page>
<table confidence="0.998902">
Task Base. EntGS EntApp WP WP+EntGS
Comp vs Oth. 13.27 24.18 24.14 27.30 26.19
Cont vs Oth. 22.95 37.57 38.16 38.17 38.99
Temp vs Oth. 3.39 7.58 5.61 11.09 10.04
Expn vs Oth. 36.82 52.42 47.82 48.54 49.06
Ent vs Oth. 22.85 38.03 36.73 38.48 38.14
</table>
<subsectionHeader confidence="0.962282">
5.2 Performance on sense prediction
</subsectionHeader>
<bodyText confidence="0.997000095238095">
The classification results (fscores) are shown in
Table 1. The random baseline (Base.) represents
the results if we predicted positive and negative re-
lations according to their proportion in the test set.
Entity features based on both gold-standard
(EntGS) and approximate coreference (EntApp)
outperform the random baseline for all the tasks.
The drop in performance without gold-standard
coreference information is strongly noticable only
for Expansion relations.
The best improvement from the baseline is seen
for predicting Contingency and Entity relations,
with around 15% absolute improvement in fscore
with both EntGS and EntApp features. The im-
provements for Comparisons and Expansions are
around 11% in the approximate case. Temporal
relations benefit least from these features. These
relations are rare, comprising 3% of the test set
and harder to isolate from other relations. Overall,
our results indicate that discourse relations and en-
tity realization have a strong association.
</bodyText>
<subsectionHeader confidence="0.999442">
5.3 Comparison with lexical features
</subsectionHeader>
<bodyText confidence="0.999977157894737">
In the context of using entity features for sense
prediction, one would also like to test how these
linguistically rich features compare with simpler
knowledge-lean approaches used in prior work.
Specifically, we compare with word pairs, a
simple yet powerful set of features introduced by
Marcu and Echihabi (2001). These features are the
cross product of words in the first sentence with
those in the second.
We trained classifiers on the word pairs from the
sentences in the PDTB training sets. In Table 1,
we report the performance of word pairs (WP) as
well as their combination with gold-standard en-
tity features (WP+EntGS). Word pairs turn out as
stronger predictors for all discourse relations com-
pared to our entity features (except for Expansion
prediction with EntGS features). Further, no ben-
efits over word pair results are obtained by com-
bining entity realization information.
</bodyText>
<sectionHeader confidence="0.999331" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999873666666667">
In this work, we used a task-based approach to
show that the two components of coherence—
discourse relations and entities—are related and
interact with each other. Coreference, givenness,
syntactic form and grammatical role of entities can
predict the implicit discourse relation between ad-
</bodyText>
<tableCaption confidence="0.971468">
Table 1: Fscore results
</tableCaption>
<bodyText confidence="0.999950777777778">
jacent sentences with results better than random
baseline. However, with respect to developing au-
tomatic discourse parsers, these entity features are
less likely to be useful. They do not outperform
or complement simpler lexical features. It would
be interesting to explore whether other aspects of
entity reference might be useful for this task, such
as bridging anaphora. But currently, annotations
and tools for these phenomena are not available.
</bodyText>
<sectionHeader confidence="0.999209" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999752857142857">
S. Blair-Goldensohn, K. McKeown, and O. Rambow.
2007. Building and refining rhetorical-semantic re-
lation models. In HLT-NAACL.
S.H. Corston-Oliver. 1998. Beyond string matching
and cue phrases: Improving efficiency and coverage
in discourse analysis. In The AAAI Spring Sympo-
sium on Intelligent Text Summarization.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. Ontonotes: the 90% solution.
In NAACL-HLT.
Z. Lin, M. Kan, and H.T. Ng. 2009. Recognizing im-
plicit discourse relations in the Penn Discourse Tree-
bank. In EMNLP.
D. Marcu and A. Echihabi. 2001. An unsupervised ap-
proach to recognizing discourse relations. In ACL.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1994.
Building a large annotated corpus of english: The
penn treebank. Computational Linguistics.
E. Pitler, A. Louis, and A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in
text. In ACL-IJCNLP.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki,
L. Robaldo, A. Joshi, and B. Webber. 2008. The
penn discourse treebank 2.0. In LREC.
E. Prince. 1992. The zpg letter: subject, definiteness,
and information status. In Discourse description:
diverse analyses of a fund raising text, pages 295–
325. John Benjamins.
C. Sporleder and A. Lascarides. 2008. Using automat-
ically labelled examples to classify rhetorical rela-
tions: An assessment. Natural Language Engineer-
ing, 14:369–416.
F. Wolf and E. Gibson. 2006. Coherence in natural
language: data structures and applications. MIT
Press.
</reference>
<page confidence="0.999184">
62
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.968276">
<title confidence="0.999766">Using entity features to classify implicit discourse relations</title>
<author confidence="0.999701">Annie Louis</author>
<author confidence="0.999701">Aravind Joshi</author>
<author confidence="0.999701">Rashmi Prasad</author>
<author confidence="0.999701">Ani</author>
<affiliation confidence="0.999863">University of</affiliation>
<address confidence="0.98423">Philadelphia, PA 19104,</address>
<abstract confidence="0.999173473684211">We report results on predicting the sense of implicit discourse relations between adjacent sentences in text. Our investigation concentrates on the association between discourse relations and properties of the referring expressions that appear in the related sentences. The properties of interest include coreference information, grammatical role, information status and syntactic form of referring expressions. Predicting the sense of implicit discourse relations based on these features is considerably better than a random baseline and several of the most discriminative features conform with linguistic intuitions. However, these features do not perform as well as lexical features traditionally used for sense prediction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Blair-Goldensohn</author>
<author>K McKeown</author>
<author>O Rambow</author>
</authors>
<title>Building and refining rhetorical-semantic relation models.</title>
<date>2007</date>
<booktitle>In HLT-NAACL.</booktitle>
<marker>Blair-Goldensohn, McKeown, Rambow, 2007</marker>
<rawString>S. Blair-Goldensohn, K. McKeown, and O. Rambow. 2007. Building and refining rhetorical-semantic relation models. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S H Corston-Oliver</author>
</authors>
<title>Beyond string matching and cue phrases: Improving efficiency and coverage in discourse analysis.</title>
<date>1998</date>
<booktitle>In The AAAI Spring Symposium on Intelligent Text Summarization.</booktitle>
<marker>Corston-Oliver, 1998</marker>
<rawString>S.H. Corston-Oliver. 1998. Beyond string matching and cue phrases: Improving efficiency and coverage in discourse analysis. In The AAAI Spring Symposium on Intelligent Text Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>M Marcus</author>
<author>M Palmer</author>
<author>L Ramshaw</author>
<author>R Weischedel</author>
</authors>
<title>Ontonotes: the 90% solution.</title>
<date>2006</date>
<booktitle>In NAACL-HLT.</booktitle>
<contexts>
<context position="4758" citStr="Hovy et al., 2006" startWordPosition="724" endWordPosition="727"> the first. Ex 1. These rate indications aren’t directly comparable. Lending practices vary widely by location. Adjacent sentences can also become related solely by talking about a common entity without any of the above discourse relation links between their propositions. Such pairs are annotated as Entity Relations (EntRels) in the PDTB, for example: Ex 2. Rolls-Royce Motor Cars Inc. said it expects its U.S sales to remain steady at about 1,200 cars in 1990. The luxury auto maker last year sold 1,214 cars in the U.S. We use the coreference annotations from the Ontonotes corpus (version 2.9) (Hovy et al., 2006) to compute our gold-standard entity features. The WSJ portion of this corpus contains 590 articles. Here, nominalizations and temporal expressions are also annotated for coreference but we use the links between noun phrases only. We expect these features computed on the gold-standard annotations to represent an upper bound on the performance of entity features. Finally, the Penn Treebank corpus (Marcus et al., 1994) is used to obtain gold-standard parse and grammatical role information. Only adjacent sentences within the same paragraph are used in our experiments. 3 Entity-related features We</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R. Weischedel. 2006. Ontonotes: the 90% solution. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Lin</author>
<author>M Kan</author>
<author>H T Ng</author>
</authors>
<title>Recognizing implicit discourse relations in the Penn Discourse Treebank.</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1931" citStr="Lin et al., 2009" startWordPosition="285" endWordPosition="288">discourse relations and the way in which references to entities are realized. In our work, we employ features related to entity realization to automatically identify discourse relations in text. We focus on implicit relations that hold between adjacent sentences in the absence of discourse connectives such as “because” or “but”. Previous studies on this task have zeroed in on lexical indicators of relation sense: dependencies between words (Marcu and Echihabi, 2001; BlairGoldensohn et al., 2007) and the semantic orientation of words (Pitler et al., 2009), or on general syntactic regularities (Lin et al., 2009). The role of entities has also been hypothesized as important for this task and entity-related features have been used alongside others (CorstonOliver, 1998; Sporleder and Lascarides, 2008). Corpus studies and reading time experiments performed by Wolf and Gibson (2006) have in fact demonstrated that the type of discourse relation linking two clauses influences the resolution of pronouns in them. However, the predictive power of entity-related features has not been studied independently of other factors. Further motivation for studying this type of features comes from new corpus evidence (Pra</context>
</contexts>
<marker>Lin, Kan, Ng, 2009</marker>
<rawString>Z. Lin, M. Kan, and H.T. Ng. 2009. Recognizing implicit discourse relations in the Penn Discourse Treebank. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>A Echihabi</author>
</authors>
<title>An unsupervised approach to recognizing discourse relations.</title>
<date>2001</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1783" citStr="Marcu and Echihabi, 2001" startWordPosition="260" endWordPosition="263">ness of the text is created by virtue of the mentioned entities and the properties of referring expressions. We aim to investigate the association between discourse relations and the way in which references to entities are realized. In our work, we employ features related to entity realization to automatically identify discourse relations in text. We focus on implicit relations that hold between adjacent sentences in the absence of discourse connectives such as “because” or “but”. Previous studies on this task have zeroed in on lexical indicators of relation sense: dependencies between words (Marcu and Echihabi, 2001; BlairGoldensohn et al., 2007) and the semantic orientation of words (Pitler et al., 2009), or on general syntactic regularities (Lin et al., 2009). The role of entities has also been hypothesized as important for this task and entity-related features have been used alongside others (CorstonOliver, 1998; Sporleder and Lascarides, 2008). Corpus studies and reading time experiments performed by Wolf and Gibson (2006) have in fact demonstrated that the type of discourse relation linking two clauses influences the resolution of pronouns in them. However, the predictive power of entity-related fea</context>
<context position="14785" citStr="Marcu and Echihabi (2001)" startWordPosition="2331" endWordPosition="2334">oximate case. Temporal relations benefit least from these features. These relations are rare, comprising 3% of the test set and harder to isolate from other relations. Overall, our results indicate that discourse relations and entity realization have a strong association. 5.3 Comparison with lexical features In the context of using entity features for sense prediction, one would also like to test how these linguistically rich features compare with simpler knowledge-lean approaches used in prior work. Specifically, we compare with word pairs, a simple yet powerful set of features introduced by Marcu and Echihabi (2001). These features are the cross product of words in the first sentence with those in the second. We trained classifiers on the word pairs from the sentences in the PDTB training sets. In Table 1, we report the performance of word pairs (WP) as well as their combination with gold-standard entity features (WP+EntGS). Word pairs turn out as stronger predictors for all discourse relations compared to our entity features (except for Expansion prediction with EntGS features). Further, no benefits over word pair results are obtained by combining entity realization information. 6 Conclusion In this wor</context>
</contexts>
<marker>Marcu, Echihabi, 2001</marker>
<rawString>D. Marcu and A. Echihabi. 2001. An unsupervised approach to recognizing discourse relations. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank. Computational Linguistics.</title>
<date>1994</date>
<contexts>
<context position="5178" citStr="Marcus et al., 1994" startWordPosition="789" endWordPosition="792">to remain steady at about 1,200 cars in 1990. The luxury auto maker last year sold 1,214 cars in the U.S. We use the coreference annotations from the Ontonotes corpus (version 2.9) (Hovy et al., 2006) to compute our gold-standard entity features. The WSJ portion of this corpus contains 590 articles. Here, nominalizations and temporal expressions are also annotated for coreference but we use the links between noun phrases only. We expect these features computed on the gold-standard annotations to represent an upper bound on the performance of entity features. Finally, the Penn Treebank corpus (Marcus et al., 1994) is used to obtain gold-standard parse and grammatical role information. Only adjacent sentences within the same paragraph are used in our experiments. 3 Entity-related features We associate each referring expression in a sentence with a set of attributes as described below. In Section 3.2, we detail how we combine these attributes to compute features for a sentence pair. 3.1 Referring expression attributes Grammatical role. In exploratory analysis of Comparison relations, we often observed parallel syntactic realizations for entities in the subject position of the two sentences: Ex 3. {Longer</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewicz. 1994. Building a large annotated corpus of english: The penn treebank. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Pitler</author>
<author>A Louis</author>
<author>A Nenkova</author>
</authors>
<title>Automatic sense prediction for implicit discourse relations in text.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP.</booktitle>
<contexts>
<context position="1874" citStr="Pitler et al., 2009" startWordPosition="276" endWordPosition="279"> expressions. We aim to investigate the association between discourse relations and the way in which references to entities are realized. In our work, we employ features related to entity realization to automatically identify discourse relations in text. We focus on implicit relations that hold between adjacent sentences in the absence of discourse connectives such as “because” or “but”. Previous studies on this task have zeroed in on lexical indicators of relation sense: dependencies between words (Marcu and Echihabi, 2001; BlairGoldensohn et al., 2007) and the semantic orientation of words (Pitler et al., 2009), or on general syntactic regularities (Lin et al., 2009). The role of entities has also been hypothesized as important for this task and entity-related features have been used alongside others (CorstonOliver, 1998; Sporleder and Lascarides, 2008). Corpus studies and reading time experiments performed by Wolf and Gibson (2006) have in fact demonstrated that the type of discourse relation linking two clauses influences the resolution of pronouns in them. However, the predictive power of entity-related features has not been studied independently of other factors. Further motivation for studying </context>
</contexts>
<marker>Pitler, Louis, Nenkova, 2009</marker>
<rawString>E. Pitler, A. Louis, and A. Nenkova. 2009. Automatic sense prediction for implicit discourse relations in text. In ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Prasad</author>
<author>N Dinesh</author>
<author>A Lee</author>
<author>E Miltsakaki</author>
<author>L Robaldo</author>
<author>A Joshi</author>
<author>B Webber</author>
</authors>
<title>The penn discourse treebank 2.0. In LREC.</title>
<date>2008</date>
<contexts>
<context position="2548" citStr="Prasad et al., 2008" startWordPosition="380" endWordPosition="383">09). The role of entities has also been hypothesized as important for this task and entity-related features have been used alongside others (CorstonOliver, 1998; Sporleder and Lascarides, 2008). Corpus studies and reading time experiments performed by Wolf and Gibson (2006) have in fact demonstrated that the type of discourse relation linking two clauses influences the resolution of pronouns in them. However, the predictive power of entity-related features has not been studied independently of other factors. Further motivation for studying this type of features comes from new corpus evidence (Prasad et al., 2008), that about a quarter of all adjacent sentences are linked purely by entity coherence, solely because they talk about the same entity. Entity-related features would be expected to better separate out such relations. We present the first comprehensive study of the connection between entity features and discourse relations. We show that there are notable differences in properties of referring expressions across the different relations. Sense prediction can be done with results better than random baseline using only entity realization information. Their performance, however, is lower than a know</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo, A. Joshi, and B. Webber. 2008. The penn discourse treebank 2.0. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Prince</author>
</authors>
<title>The zpg letter: subject, definiteness, and information status. In Discourse description: diverse analyses of a fund raising text,</title>
<date>1992</date>
<pages>295--325</pages>
<publisher>John Benjamins.</publisher>
<contexts>
<context position="6308" citStr="Prince, 1992" startWordPosition="972" endWordPosition="973">alizations for entities in the subject position of the two sentences: Ex 3. {Longer maturities}E1 are thought to indicate declining interest rates. {Shorter maturities}E2 are considered a sign of rising rates because portfolio managers can capture higher rates sooner. So, for each noun phrase, we record whether it is the subject of a main clause (msubj), subject of other clauses in the sentence (esubj) or a noun phrase not in subject position (other). Given vs. New. When an entity is first introduced in the text, it is considered a new entity. Subsequent mentions of the same entity are given (Prince, 1992). New-given distinction could help to identify some of the Expansion and Entity relations. When a sentence elaborates on another, it might contain a greater number of new entities. We use the Ontonotes coreference annotations to mark the information status for entities. For an entity, if an antecedent is found in the previous sentences, it is marked as given, otherwise it is a new entity. Syntactic realization. In Entity relations, the second sentence provides more information about a specific entity in the first and a definite description for this second mention seems likely. Also, given the </context>
</contexts>
<marker>Prince, 1992</marker>
<rawString>E. Prince. 1992. The zpg letter: subject, definiteness, and information status. In Discourse description: diverse analyses of a fund raising text, pages 295– 325. John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sporleder</author>
<author>A Lascarides</author>
</authors>
<title>Using automatically labelled examples to classify rhetorical relations: An assessment.</title>
<date>2008</date>
<journal>Natural Language Engineering,</journal>
<pages>14--369</pages>
<contexts>
<context position="2121" citStr="Sporleder and Lascarides, 2008" startWordPosition="314" endWordPosition="317">relations in text. We focus on implicit relations that hold between adjacent sentences in the absence of discourse connectives such as “because” or “but”. Previous studies on this task have zeroed in on lexical indicators of relation sense: dependencies between words (Marcu and Echihabi, 2001; BlairGoldensohn et al., 2007) and the semantic orientation of words (Pitler et al., 2009), or on general syntactic regularities (Lin et al., 2009). The role of entities has also been hypothesized as important for this task and entity-related features have been used alongside others (CorstonOliver, 1998; Sporleder and Lascarides, 2008). Corpus studies and reading time experiments performed by Wolf and Gibson (2006) have in fact demonstrated that the type of discourse relation linking two clauses influences the resolution of pronouns in them. However, the predictive power of entity-related features has not been studied independently of other factors. Further motivation for studying this type of features comes from new corpus evidence (Prasad et al., 2008), that about a quarter of all adjacent sentences are linked purely by entity coherence, solely because they talk about the same entity. Entity-related features would be expe</context>
</contexts>
<marker>Sporleder, Lascarides, 2008</marker>
<rawString>C. Sporleder and A. Lascarides. 2008. Using automatically labelled examples to classify rhetorical relations: An assessment. Natural Language Engineering, 14:369–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Wolf</author>
<author>E Gibson</author>
</authors>
<title>Coherence in natural language: data structures and applications.</title>
<date>2006</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2202" citStr="Wolf and Gibson (2006)" startWordPosition="327" endWordPosition="330">the absence of discourse connectives such as “because” or “but”. Previous studies on this task have zeroed in on lexical indicators of relation sense: dependencies between words (Marcu and Echihabi, 2001; BlairGoldensohn et al., 2007) and the semantic orientation of words (Pitler et al., 2009), or on general syntactic regularities (Lin et al., 2009). The role of entities has also been hypothesized as important for this task and entity-related features have been used alongside others (CorstonOliver, 1998; Sporleder and Lascarides, 2008). Corpus studies and reading time experiments performed by Wolf and Gibson (2006) have in fact demonstrated that the type of discourse relation linking two clauses influences the resolution of pronouns in them. However, the predictive power of entity-related features has not been studied independently of other factors. Further motivation for studying this type of features comes from new corpus evidence (Prasad et al., 2008), that about a quarter of all adjacent sentences are linked purely by entity coherence, solely because they talk about the same entity. Entity-related features would be expected to better separate out such relations. We present the first comprehensive st</context>
</contexts>
<marker>Wolf, Gibson, 2006</marker>
<rawString>F. Wolf and E. Gibson. 2006. Coherence in natural language: data structures and applications. MIT Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>