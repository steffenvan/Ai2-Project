<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002172">
<note confidence="0.895449">
In: Proceedings of CoNLL-2000 and LLL-2000, pages 73-78, Lisbon, Portugal, 2000.
</note>
<title confidence="0.991592">
Using Induced Rules as Complex Features
in Memory-Based Language Learning
</title>
<author confidence="0.459169">
Antal van den Bosch
</author>
<affiliation confidence="0.45583">
ILK / Computational Linguistics
Tilburg University, The Netherlands
</affiliation>
<email confidence="0.331539">
Antal.vdnBoschakub.n1
</email>
<sectionHeader confidence="0.977254" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999976947368421">
An extension to memory-based learning is de-
scribed in which automatically induced rules
are used as binary features. These features
have an &amp;quot;active&amp;quot; value when the left-hand side
of the underlying rule applies to the instance.
The RIPPER rule induction algorithm is adopted
for the selection of the underlying rules. The
similarity of a memory instance to a new in-
stance is measured by taking the sum of the
weights of the matching rules both instances
share. We report on experiments that indicate
that (i) the method works equally well or bet-
ter than RIPPER on various language learning
and other benchmark datasets; (ii) the method
does not necessarily perform better than default
memory-based learning, but (iii) when multi-
valued features are combined with the rule-
based features, some slight to significant im-
provements are observed.
</bodyText>
<sectionHeader confidence="0.647685" genericHeader="categories and subject descriptors">
1 Rules as features
</sectionHeader>
<bodyText confidence="0.999267206896552">
A common machine-learning solution to classi-
fication problems is rule induction (Clark and
Niblett, 1989; Quinlan, 1993; Cohen, 1995).
The goal of rule induction is generally to induce
a set of rules from data, that captures all gener-
alisable knowledge within that data, and that is
as small as possible at the same time. Classifica-
tion in rule-induction classifiers is based on the
firing of rules on a new instance, triggered by
matching feature values to the left-hand side of
the rule. Rules can be of various normal forms,
and can furthermore be ordered. The appropri-
ate content and ordering of rules can be hard
to find, and at the heart of most rule induction
systems are strong search algorithms that at-
tempt to minimise search through the space of
possible rule sets and orderings.
Although rules appear quite different from in-
stances as used in memory-based or instance-
based learning (Aha et al., 1991; Daelemans and
Van den Bosch, 1992; Daelemans et al., 1997b)
there is a continuum between them. Rules can
be seen as generalised instances; they represent
the set of training instances with the same class
that match on the conditions on the left-hand
side of the rule. Therefore, classification strate-
gies from memory-based learning can naturally
be applied to rules. For example, (Domingos,
1996) describes the RISE system, in which rules
are (carefully) generalised from instances, and
in which the k-NN classification rule searches
for nearest neighbours within these rules when
classifying new instances.
Often, the sets of instances covered by rules
overlap. In other words, seen from the instance
perspective, a single instance can match more
than one rule. Consider the schematic exam-
ple displayed in Figure 1. Three instances with
three multi-valued features match individually
with one or two of the four rules; for example,
the first instance matches with rule 1 (if fl = A
then c = Z) and with rule 3 (if f2 = C then
c = Z).
Pursuing this reasoning, it is possible to in-
dex instances by the rules that apply to them.
For example, in Figure 1, the first instance can
be indexed by the &amp;quot;active&amp;quot; rule identification
numbers 1 and 3. When the left-hand sides of
rules are seen as complex features (in which the
presence of some combination of feature values
is queried) that are strong predictors of a single
class, indexing instances by the rules that apply
to them is essentially the same as representing
instances by a set of complex features.
Note that when a rule matches an instance,
this does not guarantee that the class of the
instance is identical to the rule&apos;s predicted class
— many rules will classify with some amount of
</bodyText>
<page confidence="0.980609">
73
</page>
<figure confidence="0.906312692307692">
1 if f1 =A then c=Z
2 if f1=B and f2=B then c=Y
3 if f2=C then c=Z
4 if f3=C then c=Z
A C F 3 Z
z 1
B B D Y
Y 2
B C C 4 X
X 3
fl f2 f3 c
B B C 4 ?
? , 2
</figure>
<figureCaption confidence="0.999728">
Figure 1: Schematic visualization of the encod-
</figureCaption>
<bodyText confidence="0.98838070212766">
ing of multi-valued instances via matching rules
to rule-indexed instances, characterised by the
numbers of the rules that match them. f 1, f 2,
and f 3 represent the three features. c represents
the class label.
error. In Figure 1, the third memory instance
matches rules 3 and 4 which both predict a Z,
while the instance itself has class X.
Now when instances are represented this way,
they can be used in k-NN classification. Each
complex feature then becomes a binary feature,
that can also be assigned some weight (e.g.,
gain-ratio feature weights, chi-square, or equal
weights (Daelemans et al., 2000)); when a mem-
ory instance and a new test instance share com-
plex features, their similarity becomes the sum
of the weights of the matching features. In Fig-
ure 1, a new instance (bottom) matches rules 2
and 4, thereby (partially) matching the second
and third memory instances. If, for example,
rule 4 would have a higher overall weight than
rule 2, the third memory instance would become
the nearest neighbor. The k-NN rule then says
that the class of the nearest neighbour transfers
to the new instance, which would mean that
class X would be copied — which is a differ-
ent class than those predicted either by rule 2
or 4. This is a marked difference with classi-
fication in RIPPER, where the class is assigned
directly to the new instance by the rule that
fires first. It can be expected that many classi-
fications in this approach would be identical to
those made by RIPPER, but it is possible that
the k-NN approach has some consistent advan-
tage in the cases where classification diverges.
In this paper we investigate some effects of
recoding instances by complex features induced
by an external rule-induction algorithm, and
show that the approach is promising for lan-
guage learning tasks. We find that the method
works equally well or better than RIPPER on
various language learning and other benchmark
datasets. However, the method does not nec-
essarily perform better than default memory-
based learning. Only when the rule-indexing
features are added to the original multi-valued
features, improvements are observed.
</bodyText>
<sectionHeader confidence="0.821567" genericHeader="general terms">
2 Rule-Based Memory: algorithm
</sectionHeader>
<bodyText confidence="0.999984352941176">
A new memory-based learning variant RBM,
which stands for Rule-Based Memory, imple-
ments the ideas described in the previous sec-
tion using the following procedure: given a
training set and a test set of a certain classifi-
cation task, (1) apply RIPPER (Cohen, 1995) to
the training set, and collect the set of induced
rules; (2) recode the instances in the training
and test set according to these rules; (3) ap-
ply the basic memory-based learning algorithm
iBl-IG to the recoded training set, and k-NN-
classify the recoded test set. We describe each
of these steps briefly here.
RIPPER (Cohen, 1995) is a fast rule induction
algorithm that splits the training set in two.
On the basis of one part it induces rules in a
straightforward way, with potential overfitting.
When the induced rules classify instances in the
other part of the split training set below some
classification accuracy threshold, they are not
stored. Rules are induced per class, in a certain
class ordering. By default, the ordering is from
low-frequency classes to high frequency classes,
leaving the most frequent class as the default
rule, which is generally beneficial for the total
description length of the rule set. In our experi-
ments, we let RIPPER order the rules from high-
frequent to low-frequent, the idea being that
this method would yield more complex features.
Then, the rule set was taken as the basis
for recoding both the training and test set, as
schematically visualised in Figure 1. As with
the training material, each test set was recoded
in batch, but this could have been done on-
</bodyText>
<page confidence="0.99194">
74
</page>
<bodyText confidence="0.999799857142857">
line during classification without much compu-
tational overhead. For each language task we
experimented on, we performed 10-fold cross
validation tests, so ten different train-test par-
titions were produced (Weiss and Kulikowski,
1991) that were recoded, and then tested on.
Tests were performed with the TiMBL software
package (Daelemans et al., 2000), using the soft-
ware&apos;s dedicated routines for handling binary
features. The default ml-Io algorithm was used
(for details, consult (Aha et al., 1991; Daele-
mans and Van den Bosch, 1992; Daelemans et
al., 1997b), with gain ratio selected as feature
weighting metric.
</bodyText>
<sectionHeader confidence="0.999779" genericHeader="keywords">
3 Results
</sectionHeader>
<bodyText confidence="0.996410130952381">
We performed experiments on the following five
language data sets — More details on numbers of
features, values per features, number of classes
and number of instances are displayed in Ta-
ble 1:
Diminutive formation (henceforth DIM):
choosing the correct diminutive inflection
to Dutch nouns out of five possible: je, tje,
pje, kje, and etje, on the basis of phonemic
word transcriptions, segmented at the
level of syllable onset, nucei and coda
of the final three syllables of the word.
The data stems from a study described in
(Daelemans et al., 1997a).
Grapheme-phoneme conversion (oPsm):
the conversion of a window of nine letters
to the phonemic transcription of the
middle letter. From the original data set
described in (Van den Bosch, 1997) a 10%
subset was drawn.
Base-NP chunking (NPSM): the segmenta-
tion of sentences into non-recursive NPs.
(Veenstra, 1998) used the Base-NP tag set
as presented in (Ramshaw and Marcus,
1995): I for inside a Base-NP, 0 for out-
side a Base-NP, and B for the first word
in a Base-NP following another Base-NP.
See (Veenstra, 1998) for more details, and
(Daelemans et al., 1999) for a series of ex-
periments on the original data set from
which we have used a randomly-extracted
10%.
Part-of-speech tagging (Possm): the disam-
biguation of syntactic classes of words in
particular contexts. We assume a tagger
architecture that processes a sentence from
a disambiguated left to an ambiguous right
context , as described in (Daelemans et al.,
1996). The original data set for the part-
of-speech tagging task, extracted from the
LOB corpus, contains 1,046,151 instances;
we have used a randomly-extracted 10% of
this data.
PP attachment (PP): the attachment of a PP
in the sequence VP NP PP (VP = verb
phrase, NP = noun phrase, PP = prepo-
sitional phrase). The data consists of four-
tuples of words, extracted from the Wall
Street Journal Treebank. From the origi-
nal data set, used by (Ratnaparkhi et al.,
1994), (Collins and Brooks, 1995), and (Za-
vrel et al., 1997), (Daelemans et al., 1999)
took the train and test set together to form
the particular data also used here.
Table 2 lists the average (10-fold cross-
validation) accuracies, measured in percentages
of correctly classified test instances, of IB1-IG,
RIPPER, and RBM on these five tasks. The clear-
est overall pattern in this table is the high accu-
racy of iBl-Io, surpassed only twice by RBM on
the DIM and NPSM tasks (significantly, accord-
ing to one-tailed t-tests, with p &lt; 0.05). On
the other three tasks, IB1-Io outperforms RBM.
RIPPER performs significantly more accurately
than ml-Io only on the DIM task. Once again,
evidence is collected for the global finding that
forgetting parts of the training material, as ob-
viously happens in rule induction, tends to be
harmful to generalisation accuracy in language
learning (Daelemans et al., 1999).
A surprising result apparent in Table 2 is that
RBM never performs worse than RIPPER; in fact,
it performs significantly more accurately than
RIPPER with the GPSM, NPSM, and POSSM tasks.
There appears to be an advantage in the k-
NN approach to rule matching and voting, over
the RIPPER strategy of ordered rule firing, with
these tasks.
Another advantage, now of RBM as opposed
to ml-Io, is the reduced memory requirements
and resulting speed enhancements. As listed in
Table 3, the average number of rules in the rule
sets induced by RIPPER range between 29 and
971. Averaged over all tasks, the rules have on
</bodyText>
<page confidence="0.998315">
75
</page>
<table confidence="0.999822714285714">
Data set # 1 2 3 # Values of feature 8 9 10 11 12 # # Data set
Feat. 4 5 6 7 Class instances
DIM 11 3 51 19 40 3 61 20 79 2 64 18 43 5 3950
GPSM 9 42 42 42 42 41 42 42 42 42 61 67,575
POS 5 155 157 414 395 384 159 104,617
NP 11 5961 5911 5895 5908 51 50 55 49 3 3 3 3 25,114
PP 4 3474 4612 68 5780 2 23,898
</table>
<tableCaption confidence="0.880632666666667">
Table 1: Specifications of the five investigated language learning tasks: numbers of features, values
per feature, classes, and instances. The rightmost column gives the total number of values times
the number of classes.
</tableCaption>
<table confidence="0.993069">
Task % Correct test instances Task RIPPER / RBM Classif. time (S)
IB1-IG RIPPER RBM # rules dr f/i IB1-IG RBM
DIM 96.2 ± 0.6 96.9 ± 0.7 * 96.9 ± 0.7 * DIM 61 2.5 1.3 1 1
GPSM 88.9 ± 0.6 80.4 ± 0.5 83.3 ± 0.5 + V GPSM 971 3.9 1.5 8 17
NPSM 97.2 ± 0.3 96.9 ± 0.4 97.5 ± 0.4 * V NPSM 72 2.8 1.8 19 1
POSSM 96.6 ± 0.2 94.3 ± 0.2 95.0 ± 0.2 + V POSSM 628 2.7 1.0 32 13
PP 82.0 ± 0.5 77.0 ± 0.7 77.0 ± 0.6 + PP 29 3.0 0.3 19 1
</table>
<tableCaption confidence="0.991503">
Table 2: Average generalisation accuracies of
</tableCaption>
<bodyText confidence="0.986815307692308">
IB1-IG, RIPPER, and RBM on five language learn-
ing tasks. `*&apos; denotes significantly better accu-
racy of RBM or RIPPER over ml-IG with p &lt;
0.05. `+&apos; denotes significance in the reverse di-
rection. V denotes significantly better accuracy
of RBM over RIPPER with p &lt; 0.05.
average about two to four conditions (feature-
value tests). More importantly, as the third
column of Table 3 shows, the average number
of active rules in instances is below two for all
tasks. This means that in most instances of any
of the five tasks, only one complex feature (bit)
is active.
Especially with the smaller rule sets (DIM,
NPSM, and PP - which all have few classes, cf.
Table 1), RBM&apos;s classification is very speedy. It
reduces, for example, classification of the NPSM
test set from 19 seconds to 1 second&apos;. Large
rule sets (GPSM), however, can have adverse ef-
fects - from 8 seconds in ml-IG to 17 seconds
in RBM.
In sum, we observe two cases (DIM and NPSM)
in which RBM attains a significant generalisa-
tion accuracy improvement over ml-IG as well
as some interesting classification speedup, but
for the other tasks, for now unpredictably, gen-
</bodyText>
<note confidence="0.5777515">
&apos;Timings are measured on one partition, using a dual-
Pentium II 200 Mhz machine running Linux 2.2.
</note>
<tableCaption confidence="0.896932">
Table 3: Average number of RIPPER rules, con-
</tableCaption>
<bodyText confidence="0.985347">
ditions per rule (c/r), and coded features per
instance (f/i); and one-partition timings (s) of
classification of test material in IB1-IG and RBM,
for five language tasks.
eralisation accuracy losses and even a slowdown
are observed. The latter occurs with GPSM,
which has been analysed earlier as being ex-
tremely disjunct in class space, and therefore
highly sensitive to the &amp;quot;forgetting exceptions
is harmful&amp;quot; syndrome (Daelemans et al., 1999;
Van den Bosch, 1999a).
The complex features used in RBM are taken
as the only information available; the original
information (the feature values) are discarded.
This need not be the case; it is possible that the
recoded instances are merged with their orig-
inal feature-value vectors. We performed ex-
periments in which we made this fusion; the
results are listed in Table 4. Comparing the
column labeled &amp;quot;IB1-IG-PRBM, denoting the fu-
sion variant, with the Isl-IG column, it can be
seen that it reaches some modest error reduc-
tion percentages (rightmost column in Table 4).
In fact, with NPSM and POSSM, it performs sig-
nificantly better (again, according to one-tailed
t-tests, with p &lt; 0.05) than ml-IG. On the
other hand, adding the (average) 971 complex
features to the nine multi-valued features in the
</bodyText>
<page confidence="0.97768">
76
</page>
<table confidence="0.996970571428571">
Task % Correct test instances % Error
IB1-IG IB1-IG-F RBM reduct.
DIM 96.2 ± 0.6 96.2 ± 0.7 0.0
GP SM 88.9 ± 0.6 88.6 ± 0.4 -2.3
NPSM 97.2 ± 0.3 97.6 ± 0.4 * 6.0
P OS SM 96.6 ± 0.2 96.8 ± 0.2 * 4.6
PP 82.0 ± 0.5 82.1 ± 0.5 1.0
</table>
<tableCaption confidence="0.993078428571429">
Table 4: Average generalisation accuracies of
IB1-IG and Isl-IG + RBM, and the percentage of
error reduction, on five language learning tasks.
`,1,&apos; denotes significantly better accuracy of ml-
IG+RBm over ml-IG with p &lt; 0.05.
GPSM causes a slight drop in performance - and
a slowdown.
</tableCaption>
<sectionHeader confidence="0.996044" genericHeader="introduction">
4 Discussion
</sectionHeader>
<bodyText confidence="0.99001744">
Representing instances by complex features
that have been induced by a rule induction al-
gorithm appears, in view of the measured ac-
curacies, a viable alternative approach to us-
ing rules, as compared to standard rule induc-
tion. This result is in line with results reported
by Domingos on the RISE algorithm (Domingos,
1995; Domingos, 1996). A marked difference is
that in RISE, the rules are the instances in k-
NN classification (and due to the careful gen-
eralisation strategy of RISE, they can be very
instance-specific), while in RBM, the rules are
the features by which the original instances are
indexed. When a nearest neighbor is found to a
query instance in RBM, it is because the two in-
stances share one or more matching rules. The
actual classification that is transferred from the
memory instance to the new instance is just the
classification that this memory item is stored
with - it may well be another class than any of
its matching rules predict.
Second, the method is a potentially helpful
extension to memory-based learning of language
processing tasks. When nothing is known about
the characteristics of a language processing data
set, it is advisable to add the induced complex
features to the original features, and do k-NN
classification on the combination; it is not ad-
visable to base classification only on the induced
complex features. On its own, the method basi-
cally inherits a part of the detrimental &amp;quot;forget-
ting exceptions is harmful&amp;quot; effect from its rule-
induction source (this effect is stronger when
Table 5: Average generalisation accuracies of
ml-IG, RIPPER, and RBM on three machine-
learning benchmark tasks.
a data set is more disjunct (Daelemans et al.,
1999)). Although RBM performs equal to or bet-
ter than RIPPER, it often does not regain the
level of Isl-IG.
High disjunctivity appears to be a typical fea-
ture of language tasks (Van den Bosch, 1999b);
other non-language tasks generally display less
disjunctivity, which opens the possibility that
the RBM approach may work well for some
of these tasks. We performed pilot tests on
three machine learning benchmark classification
tasks (taken from the UCI benchmark repos-
itory (Blake and Merz, 1998)) with symbolic,
multi-valued features. Table 5 displays the re-
sults of these experiments. Although the data
set selection is small, the results of RBM and es-
pecially of ml-IG+RBm are promising; the lat-
ter algorithm is consistently better than ml-IG.
More research and comparisons are needed to
arrive at a broader picture.
An immediate point of further research lies
in the external rule induction algorithm. First,
RIPPER has options that have not been used
here, but that may be relevant for the current
issue, e.g. RIPPER&apos;S ability to represent sets
of values at left-hand side conditions, and its
flexibility in producing larger or smaller num-
bers of rules. Second, other rule induction algo-
rithms exist that may play RIPPER&apos;S role, such
as c4.5RuLEs (Quinlan, 1993).
More generally, further research should fo-
cus on the scaling properties of the approach
(including the scaling of the external rule-
induction algorithm), should investigate more
and larger language data sets, and should seek
comparisons with other existing methods that
claim to handle complex features efficiently
(Brill, 1993; Ratnaparkhi, 1997; Roth, 1998;
Brants, 2000).
</bodyText>
<table confidence="0.9909054">
% Correct test instances
Task IB1-IG RBM IM-IG-ERBM
CAR 93.9 ± 2.1 98.9 ± 0.8 97.2 ± 1.3
NURSERY 94.6 ± 0.6 98.6 ± 0.5 98.7 ± 0.2
SPLICE 91.7 ± 1.1 89.0 ± 2.1 92.7 ± 1.7
</table>
<page confidence="0.996098">
77
</page>
<sectionHeader confidence="0.996511" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999867">
The author thanks the members of the Tilburg
ILK group and the Antwerp CNTS group for
fruitful discussions. This research has been
made possible by a fellowship of the Royal
Netherlands Academy of Arts and Sciences
(KNAW).
</bodyText>
<sectionHeader confidence="0.998871" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99985083">
D.W. Aha, D. Kibler, and M. Albert. 1991.
Instance-based learning algorithms. Machine
Learning, 6:37-66.
C.L. Blake and C.J. Merz. 1998. UCI repository of
machine learning databases.
Thorsten Brants. 2000. TnT - a statistical part-of-
speech tagger. In Proceedings of the Sixth Applied
Natural Language Processing (ANLP-2000), Seat-
tle, WA.
E. Brill. 1993. A Corpus-Based Approach to Lan-
guage Learning. Dissertation, Department of
Computer and Information Science, University of
Pennsylvania.
P. Clark and T. Niblett. 1989. The CN2 rule induc-
tion algorithm. Machine Learning, 3:261-284.
W. W. Cohen. 1995. Fast effective rule induction.
In Proceedings of the Twelfth International Con-
ference on Machine Learning, Lake Tahoe, Cali-
fornia.
M.J Collins and J. Brooks. 1995. Prepositional
phrase attachment through a backed-off model.
In Proc. of Third Workshop on Very Large Cor-
pora, Cambridge.
W. Daelemans and A. Van den Bosch. 1992. Gener-
alisation performance of backpropagation learning
on a syllabification task. In M. F. J. Drossaers and
A. Nijholt, editors, Proc. of TWLT3: Connection-
ism and Natural Language Processing, pages 27-
37, Enschede. Twente University.
W. Daelemans, J. Zavrel, P. Berck, and S. Gillis.
1996. MBT: A memory-based part of speech tag-
ger generator. In E. Ejerhed and I.Dagan, editors,
Proc. of Fourth Workshop on Very Large Corpora,
pages 14-27. ACL SIGDAT.
W. Daelemans, P. Berck, and S. Gillis. 1997a. Data
mining as a method for linguistic analysis: Dutch
diminutives. Folia Linguistica, XXXI(1-2).
W. Daelemans, A. Van den Bosch, and A. Weijters.
1997b. IGTree: using trees for compression and
classification in lazy learning algorithms. Artifi-
cial Intelligence Review, 11:407-423.
W. Daelemans, A. Van den Bosch, and J. Zavrel.
1999. Forgetting exceptions is harmful in lan-
guage learning. Machine Learning, 34(1-3):11-
43.
W. Daelemans, J. Zavrel, K. Van der Sloot, and
A. Van den Bosch. 2000. TiMBL: Tilburg Mem-
ory Based Learner, version 3.0, reference manual.
Technical Report ILK-0001, ILK, Tilburg Univer-
sity.
P. Domingos. 1995. The rise 2.0 system: A case
study in multistrategy learning. Technical Re-
port 95-2, University of California at Irvine, De-
partment of Information and Computer Science,
Irvine, CA.
P. Domingos. 1996. Unifying instance-based and
rule-based induction. Machine Learning, 24:141-
168.
J.R. Quinlan. 1993. c4.5: Programs for Machine
Learning. Morgan Kaufmann, San Mateo, CA.
L.A. Ramshaw and M.P. Marcus. 1995. Text chunk-
ing using transformation-based learning. In Proc.
of Third Workshop on Very Large Corpora, pages
82-94, June.
A. Ratnaparkhi, J. Reynar, and S. Roukos. 1994. A
maximum entropy model for prepositional phrase
attachment. In Workshop on Human Language
Technology, Plainsboro, NJ, March. ARPA.
A. Ratnaparkhi. 1997. A linear observed time sta-
tistical parser based on maximum entropy models.
Technical Report cmp-lg/9706014, Computation
and Language, http://xxx.lanl.gov/list/cmp-lg/,
June.
D. Roth. 1998. Learning to resolve natural language
ambiguities: A unified approach. In Proceedings
of the National Conference on Artificial Intelli-
gence, pages 898-904.
A. Van den Bosch. 1997. Learning to pronounce
written words: A study in inductive language
learning. Ph.D. thesis, Universiteit Maastricht.
A. Van den Bosch. 1999a. Careful abstraction from
instance families in memory-based language learn-
ing. Journal for Experimental and Theoretical Ar-
tificial Intelligence, 11(3):339-368.
A. Van den Bosch. 1999b. Instance-family ab-
straction in memory-based language learning. In
I. Bratko and S. Dzeroski, editors, Machine
Learning: Proceedings of the Sixteenth Interna-
tional Conference, pages 39-48, Bled, Slovenia.
J. Veenstra. 1998. Fast NP chunking using memory-
based learning techniques. In Proceedings of
BENELEARN&apos;98, Wageningen, The Netherlands.
S. Weiss and C. Kulikowski. 1991. Computer sys-
tems that learn. San Mateo, CA: Morgan Kauf-
mann.
J. Zavrel, W. Daelemans, and J. Veenstra. 1997. Re-
solving PP attachment ambiguities with memory-
based learning. In M. Ellison, editor, Proc. of the
Workshop on Computational Language Learning
(CoNLL&apos;.97), ACL, Madrid.
</reference>
<page confidence="0.998819">
78
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000006">
<note confidence="0.982929">of CoNLL-2000 and LLL-2000, Lisbon, Portugal, 2000.</note>
<title confidence="0.9974245">Using Induced Rules as Complex in Memory-Based Language Learning</title>
<author confidence="0.997932">Antal van_den</author>
<affiliation confidence="0.939848">Tilburg University, The</affiliation>
<address confidence="0.664048">Antal.vdnBoschakub.n1</address>
<abstract confidence="0.981932451025057">An extension to memory-based learning is described in which automatically induced rules are used as binary features. These features have an &amp;quot;active&amp;quot; value when the left-hand side of the underlying rule applies to the instance. induction algorithm is adopted for the selection of the underlying rules. The similarity of a memory instance to a new instance is measured by taking the sum of the weights of the matching rules both instances share. We report on experiments that indicate that (i) the method works equally well or betthan various language learning and other benchmark datasets; (ii) the method does not necessarily perform better than default memory-based learning, but (iii) when multivalued features are combined with the rulebased features, some slight to significant improvements are observed. 1 Rules as features A common machine-learning solution to classification problems is rule induction (Clark and Niblett, 1989; Quinlan, 1993; Cohen, 1995). The goal of rule induction is generally to induce a set of rules from data, that captures all generalisable knowledge within that data, and that is as small as possible at the same time. Classification in rule-induction classifiers is based on the firing of rules on a new instance, triggered by matching feature values to the left-hand side of the rule. Rules can be of various normal forms, and can furthermore be ordered. The appropriate content and ordering of rules can be hard to find, and at the heart of most rule induction systems are strong search algorithms that attempt to minimise search through the space of possible rule sets and orderings. Although rules appear quite different from instances as used in memory-based or instancebased learning (Aha et al., 1991; Daelemans and Van den Bosch, 1992; Daelemans et al., 1997b) there is a continuum between them. Rules can be seen as generalised instances; they represent the set of training instances with the same class that match on the conditions on the left-hand side of the rule. Therefore, classification strategies from memory-based learning can naturally be applied to rules. For example, (Domingos, describes the in which rules are (carefully) generalised from instances, and which the rule searches for nearest neighbours within these rules when classifying new instances. Often, the sets of instances covered by rules overlap. In other words, seen from the instance perspective, a single instance can match more than one rule. Consider the schematic example displayed in Figure 1. Three instances with three multi-valued features match individually with one or two of the four rules; for example, first instance matches with rule 1 (if = A c = with rule 3 (if f2 = Pursuing this reasoning, it is possible to index instances by the rules that apply to them. For example, in Figure 1, the first instance can be indexed by the &amp;quot;active&amp;quot; rule identification numbers 1 and 3. When the left-hand sides of rules are seen as complex features (in which the presence of some combination of feature values is queried) that are strong predictors of a single class, indexing instances by the rules that apply to them is essentially the same as representing instances by a set of complex features. Note that when a rule matches an instance, this does not guarantee that the class of the instance is identical to the rule&apos;s predicted class — many rules will classify with some amount of 73 1 if f1 =A then c=Z 2 if f1=B and f2=B then c=Y 3 if f2=C then c=Z 4 if f3=C then c=Z A C F 3 Z z 1 B B D Y Y 2 B C C 4 X X 3 fl f2 f3 c B B C 4 ? ? Figure 1: Schematic visualization of the encoding of multi-valued instances via matching rules to rule-indexed instances, characterised by the of the rules that match them. 1, f represent the three features. the class label. error. In Figure 1, the third memory instance rules 3 and 4 which both predict a the instance itself has class Now when instances are represented this way, can be used in Each complex feature then becomes a binary feature, that can also be assigned some weight (e.g., gain-ratio feature weights, chi-square, or equal weights (Daelemans et al., 2000)); when a memory instance and a new test instance share complex features, their similarity becomes the sum of the weights of the matching features. In Figure 1, a new instance (bottom) matches rules 2 and 4, thereby (partially) matching the second and third memory instances. If, for example, rule 4 would have a higher overall weight than rule 2, the third memory instance would become the nearest neighbor. The k-NN rule then says that the class of the nearest neighbour transfers to the new instance, which would mean that be copied — which is a different class than those predicted either by rule 2 or 4. This is a marked difference with classiin the class is assigned directly to the new instance by the rule that fires first. It can be expected that many classifications in this approach would be identical to those made by RIPPER, but it is possible that has some consistent advantage in the cases where classification diverges. In this paper we investigate some effects of recoding instances by complex features induced by an external rule-induction algorithm, and show that the approach is promising for language learning tasks. We find that the method equally well or better than various language learning and other benchmark datasets. However, the method does not necessarily perform better than default memorybased learning. Only when the rule-indexing are the original multi-valued features, improvements are observed. Memory: algorithm new memory-based learning variant which stands for Rule-Based Memory, implements the ideas described in the previous section using the following procedure: given a training set and a test set of a certain classifitask, (1) apply 1995) to the training set, and collect the set of induced rules; (2) recode the instances in the training and test set according to these rules; (3) apply the basic memory-based learning algorithm to the recoded training set, and k-NNclassify the recoded test set. We describe each of these steps briefly here. 1995) is a fast rule induction algorithm that splits the training set in two. On the basis of one part it induces rules in a straightforward way, with potential overfitting. When the induced rules classify instances in the other part of the split training set below some classification accuracy threshold, they are not stored. Rules are induced per class, in a certain class ordering. By default, the ordering is from low-frequency classes to high frequency classes, leaving the most frequent class as the default rule, which is generally beneficial for the total description length of the rule set. In our experiments, we let RIPPER order the rules from highfrequent to low-frequent, the idea being that this method would yield more complex features. Then, the rule set was taken as the basis for recoding both the training and test set, as schematically visualised in Figure 1. As with the training material, each test set was recoded batch, but this could have been done on- 74 line during classification without much computational overhead. For each language task we experimented on, we performed 10-fold cross validation tests, so ten different train-test partitions were produced (Weiss and Kulikowski, 1991) that were recoded, and then tested on. Tests were performed with the TiMBL software package (Daelemans et al., 2000), using the software&apos;s dedicated routines for handling binary features. The default ml-Io algorithm was used (for details, consult (Aha et al., 1991; Daelemans and Van den Bosch, 1992; Daelemans et al., 1997b), with gain ratio selected as feature weighting metric. 3 Results We performed experiments on the following five language data sets — More details on numbers of features, values per features, number of classes and number of instances are displayed in Table 1: formation DIM): choosing the correct diminutive inflection to Dutch nouns out of five possible: je, tje, pje, kje, and etje, on the basis of phonemic word transcriptions, segmented at the level of syllable onset, nucei and coda of the final three syllables of the word. The data stems from a study described in (Daelemans et al., 1997a). conversion the conversion of a window of nine letters to the phonemic transcription of the middle letter. From the original data set described in (Van den Bosch, 1997) a 10% subset was drawn. chunking the segmentation of sentences into non-recursive NPs. (Veenstra, 1998) used the Base-NP tag set as presented in (Ramshaw and Marcus, inside a Base-NP, outa Base-NP, and the first word in a Base-NP following another Base-NP. See (Veenstra, 1998) for more details, and (Daelemans et al., 1999) for a series of experiments on the original data set from which we have used a randomly-extracted 10%. tagging the disambiguation of syntactic classes of words in particular contexts. We assume a tagger architecture that processes a sentence from a disambiguated left to an ambiguous right context , as described in (Daelemans et al., 1996). The original data set for the partof-speech tagging task, extracted from the LOB corpus, contains 1,046,151 instances; we have used a randomly-extracted 10% of this data. attachment the attachment of a PP in the sequence VP NP PP (VP = verb phrase, NP = noun phrase, PP = prepositional phrase). The data consists of fourtuples of words, extracted from the Wall Street Journal Treebank. From the original data set, used by (Ratnaparkhi et al., 1994), (Collins and Brooks, 1995), and (Zavrel et al., 1997), (Daelemans et al., 1999) took the train and test set together to form the particular data also used here. Table 2 lists the average (10-fold crossvalidation) accuracies, measured in percentages correctly classified test instances, of these five tasks. The clearest overall pattern in this table is the high accuof iBl-Io, surpassed only twice by (significantly, accordto one-tailed t-tests, with &lt; On other three tasks, IB1-Io outperforms significantly more accurately ml-Io only on the Once again, evidence is collected for the global finding that forgetting parts of the training material, as obviously happens in rule induction, tends to be harmful to generalisation accuracy in language learning (Daelemans et al., 1999). A surprising result apparent in Table 2 is that performs worse than fact, it performs significantly more accurately than the NPSM, appears to be an advantage in the kto rule matching and voting, over of ordered rule firing, with these tasks. advantage, now of opposed to ml-Io, is the reduced memory requirements and resulting speed enhancements. As listed in Table 3, the average number of rules in the rule induced by between 29 and Averaged over the rules have on 75 Data set # 1 2 3 # Values of feature 8 9 10 11 12 # # Data set instances Feat. 4 5 6 7 Class DIM 11 3 51 19 40 3 61 20 79 2 64 18 43 5 3950 GPSM 9 42 42 42 42 41 42 42 42 42 61 67,575 POS 5 155 157 414 395 384 159 104,617 NP 11 5961 5911 5895 5908 51 50 55 49 3 3 3 3 25,114 PP 4 3474 4612 68 5780 2 23,898 Table 1: Specifications of the five investigated language learning tasks: numbers of features, values per feature, classes, and instances. The rightmost column gives the total number of values times the number of classes. Task % Correct test instances Task RIPPER / RBM time IB1-IG RIPPER RBM # rules dr f/i IB1-IG RBM DIM 96.2 ± 0.6 96.9 ± 0.7 * 96.9 ± 0.7 * DIM 61 2.5 1.3 1 1 GPSM 88.9 ± 0.6 80.4 ± 0.5 83.3 ± 0.5 + V GPSM 971 3.9 1.5 8 17 NPSM 97.2 ± 0.3 96.9 ± 0.4 97.5 ± 0.4 * V NPSM 72 2.8 1.8 19 1 POSSM 96.6 ± 0.2 94.3 ± 0.2 95.0 ± 0.2 + V POSSM 628 2.7 1.0 32 13 PP 82.0 ± 0.5 77.0 ± 0.7 77.0 ± 0.6 + PP 29 3.0 0.3 19 1 Table 2: Average generalisation accuracies of RIPPER, five language learning tasks. `*&apos; denotes significantly better accuof ml-IG with &lt; 0.05. `+&apos; denotes significance in the reverse direction. V denotes significantly better accuracy &lt; average about two to four conditions (featurevalue tests). More importantly, as the third column of Table 3 shows, the average number in instances is below two for all tasks. This means that in most instances of any of the five tasks, only one complex feature (bit) is active. Especially with the smaller rule sets (DIM, all have few classes, cf. 1), is very speedy. It for example, classification of the test set from 19 seconds to 1 second&apos;. Large rule sets (GPSM), however, can have adverse effects from 8 seconds in ml-IG to 17 seconds In sum, we observe two cases (DIM and NPSM) which a significant generalisation accuracy improvement over ml-IG as well as some interesting classification speedup, but the other tasks, for now unpredictably, genmeasured on one partition, using a dual- Pentium II 200 Mhz machine running Linux 2.2. 3: Average number of conditions per rule (c/r), and coded features per instance (f/i); and one-partition timings (s) of of test material in for five language tasks. eralisation accuracy losses and even a slowdown observed. The latter occurs with which has been analysed earlier as being extremely disjunct in class space, and therefore highly sensitive to the &amp;quot;forgetting exceptions is harmful&amp;quot; syndrome (Daelemans et al., 1999; Van den Bosch, 1999a). complex features used in taken as the only information available; the original information (the feature values) are discarded. This need not be the case; it is possible that the recoded instances are merged with their original feature-value vectors. We performed experiments in which we made this fusion; the results are listed in Table 4. Comparing the labeled the fusion variant, with the Isl-IG column, it can be seen that it reaches some modest error reduction percentages (rightmost column in Table 4). fact, with performs significantly better (again, according to one-tailed with &lt; than ml-IG. On the other hand, adding the (average) 971 complex features to the nine multi-valued features in the 76 Task % Correct test instances % Error reduct. IB1-IG IB1-IG-F RBM DIM 96.2 ± 0.6 96.2 ± 0.7 0.0 GP SM 88.9 ± 0.6 88.6 ± 0.4 -2.3 NPSM 97.2 ± 0.3 97.6 ± 0.4 * 6.0 P OS SM 96.6 ± 0.2 96.8 ± 0.2 * 4.6 PP 82.0 ± 0.5 82.1 ± 0.5 1.0 Table 4: Average generalisation accuracies of Isl-IG + the percentage of error reduction, on five language learning tasks. denotes significantly better accuracy of mlover ml-IG with &lt; a slight drop in performance and a slowdown. 4 Discussion Representing instances by complex features that have been induced by a rule induction algorithm appears, in view of the measured accuracies, a viable alternative approach to using rules, as compared to standard rule induction. This result is in line with results reported Domingos on the (Domingos, 1995; Domingos, 1996). A marked difference is in rules are the k- (and due to the careful genstrategy of can be very while in rules are which the original instances are indexed. When a nearest neighbor is found to a instance in it because the two inshare or more rules. The actual classification that is transferred from the memory instance to the new instance is just the classification that this memory item is stored with it may well be another class than any of its matching rules predict. the method is a extension to memory-based learning of language processing tasks. When nothing is known about the characteristics of a language processing data it is advisable to induced complex to the original features, and do classification on the combination; it is not advisable to base classification only on the induced complex features. On its own, the method basically inherits a part of the detrimental &amp;quot;forgetting exceptions is harmful&amp;quot; effect from its ruleinduction source (this effect is stronger when Table 5: Average generalisation accuracies of three machinelearning benchmark tasks. a data set is more disjunct (Daelemans et al., Although equal to or betthan often does not regain the level of Isl-IG. High disjunctivity appears to be a typical feature of language tasks (Van den Bosch, 1999b); other non-language tasks generally display less disjunctivity, which opens the possibility that may work well for some of these tasks. We performed pilot tests on three machine learning benchmark classification tasks (taken from the UCI benchmark repository (Blake and Merz, 1998)) with symbolic, multi-valued features. Table 5 displays the reof these Although the data selection is small, the results of especially of ml-IG+RBm are promising; the latter algorithm is consistently better than ml-IG. More research and comparisons are needed to arrive at a broader picture. An immediate point of further research lies in the external rule induction algorithm. First, options that have not been used here, but that may be relevant for the current e.g. to represent sets of values at left-hand side conditions, and its flexibility in producing larger or smaller numbers of rules. Second, other rule induction algoexist that may play such as c4.5RuLEs (Quinlan, 1993). More generally, further research should focus on the scaling properties of the approach (including the scaling of the external ruleinduction algorithm), should investigate more and larger language data sets, and should seek comparisons with other existing methods that claim to handle complex features efficiently</abstract>
<note confidence="0.7272578">(Brill, 1993; Ratnaparkhi, 1997; Roth, 1998; Brants, 2000). % Correct test instances Task IB1-IG RBM IM-IG-ERBM CAR 93.9 ± 2.1 98.9 ± 0.8 97.2 ± 1.3 NURSERY 94.6 ± 0.6 98.6 ± 0.5 98.7 ± 0.2 SPLICE 91.7 ± 1.1 89.0 ± 2.1 92.7 ± 1.7 77 Acknowledgements The author thanks the members of the Tilburg</note>
<abstract confidence="0.711890425925926">ILK group and the Antwerp CNTS group for fruitful discussions. This research has been made possible by a fellowship of the Royal Netherlands Academy of Arts and Sciences (KNAW). References D.W. Aha, D. Kibler, and M. Albert. 1991. learning algorithms. C.L. Blake and C.J. Merz. 1998. UCI repository of machine learning databases. Thorsten Brants. 2000. TnT a statistical part-oftagger. In of the Sixth Applied Language Processing (ANLP-2000), Seattle, WA. E. Brill. 1993. A Corpus-Based Approach to Language Learning. Dissertation, Department of Computer and Information Science, University of Pennsylvania. P. Clark and T. Niblett. 1989. The CN2 rule inducalgorithm. Learning, W. W. Cohen. 1995. Fast effective rule induction. of the Twelfth International Conon Machine Learning, Tahoe, California. M.J Collins and J. Brooks. 1995. Prepositional phrase attachment through a backed-off model. of Third Workshop on Very Large Cor- W. Daelemans and A. Van den Bosch. 1992. Generalisation performance of backpropagation learning on a syllabification task. In M. F. J. Drossaers and Nijholt, editors, of TWLT3: Connectionand Natural Language Processing, 27- 37, Enschede. Twente University. W. Daelemans, J. Zavrel, P. Berck, and S. Gillis. 1996. MBT: A memory-based part of speech tagger generator. In E. Ejerhed and I.Dagan, editors, Proc. of Fourth Workshop on Very Large Corpora, pages 14-27. ACL SIGDAT. W. Daelemans, P. Berck, and S. Gillis. 1997a. Data mining as a method for linguistic analysis: Dutch Linguistica, W. Daelemans, A. Van den Bosch, and A. Weijters. trees for compression and in lazy learning algorithms. Artifi- Intelligence Review, W. Daelemans, A. Van den Bosch, and J. Zavrel. 1999. Forgetting exceptions is harmful in lanlearning. Learning, 34(1-3):11- 43. W. Daelemans, J. Zavrel, K. Van der Sloot, and Van den Bosch. 2000. TiMBL: Tilburg Memory Based Learner, version 3.0, reference manual. Technical Report ILK-0001, ILK, Tilburg University.</abstract>
<author confidence="0.834093">system A case study in multistrategy learning Technical Report</author>
<affiliation confidence="0.995623">partment of Information and Computer Science,</affiliation>
<address confidence="0.978599">Irvine, CA.</address>
<note confidence="0.8287342">P. Domingos. 1996. Unifying instance-based and induction. Learning, 24:141- 168. Quinlan. 1993. c4.5: for Machine Kaufmann, San Mateo, CA. L.A. Ramshaw and M.P. Marcus. 1995. Text chunkusing transformation-based learning. In Third Workshop on Very Large Corpora, 82-94, June. A. Ratnaparkhi, J. Reynar, and S. Roukos. 1994. A</note>
<abstract confidence="0.7129982">maximum entropy model for prepositional phrase In on Human Language NJ, March. ARPA. A. Ratnaparkhi. 1997. A linear observed time statistical parser based on maximum entropy models.</abstract>
<note confidence="0.762741263157895">Technical Report cmp-lg/9706014, Computation and Language, http://xxx.lanl.gov/list/cmp-lg/, June. D. Roth. 1998. Learning to resolve natural language A unified approach. In of the National Conference on Artificial Intelli- 898-904. Van den Bosch. 1997. to pronounce written words: A study in inductive language thesis, Universiteit Maastricht. A. Van den Bosch. 1999a. Careful abstraction from instance families in memory-based language learnfor Experimental and Theoretical Artificial Intelligence, 11(3):339-368. A. Van den Bosch. 1999b. Instance-family abstraction in memory-based language learning. In Bratko and S. Dzeroski, editors, Learning: Proceedings of the Sixteenth Interna- Conference, 39-48, Bled, Slovenia.</note>
<abstract confidence="0.839303181818182">J. Veenstra. 1998. Fast NP chunking using memorylearning techniques. In of The Netherlands. Weiss and C. Kulikowski. 1991. systhat learn. Mateo, CA: Morgan Kaufmann. J. Zavrel, W. Daelemans, and J. Veenstra. 1997. Resolving PP attachment ambiguities with memorylearning. In M. Ellison, editor, of the Workshop on Computational Language Learning Madrid.</abstract>
<intro confidence="0.681186">78</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D W Aha</author>
<author>D Kibler</author>
<author>M Albert</author>
</authors>
<title>Instance-based learning algorithms.</title>
<date>1991</date>
<booktitle>Machine Learning,</booktitle>
<pages>6--37</pages>
<contexts>
<context position="2044" citStr="Aha et al., 1991" startWordPosition="326" endWordPosition="329">possible at the same time. Classification in rule-induction classifiers is based on the firing of rules on a new instance, triggered by matching feature values to the left-hand side of the rule. Rules can be of various normal forms, and can furthermore be ordered. The appropriate content and ordering of rules can be hard to find, and at the heart of most rule induction systems are strong search algorithms that attempt to minimise search through the space of possible rule sets and orderings. Although rules appear quite different from instances as used in memory-based or instancebased learning (Aha et al., 1991; Daelemans and Van den Bosch, 1992; Daelemans et al., 1997b) there is a continuum between them. Rules can be seen as generalised instances; they represent the set of training instances with the same class that match on the conditions on the left-hand side of the rule. Therefore, classification strategies from memory-based learning can naturally be applied to rules. For example, (Domingos, 1996) describes the RISE system, in which rules are (carefully) generalised from instances, and in which the k-NN classification rule searches for nearest neighbours within these rules when classifying new i</context>
<context position="8220" citStr="Aha et al., 1991" startWordPosition="1391" endWordPosition="1394">ed in Figure 1. As with the training material, each test set was recoded in batch, but this could have been done on74 line during classification without much computational overhead. For each language task we experimented on, we performed 10-fold cross validation tests, so ten different train-test partitions were produced (Weiss and Kulikowski, 1991) that were recoded, and then tested on. Tests were performed with the TiMBL software package (Daelemans et al., 2000), using the software&apos;s dedicated routines for handling binary features. The default ml-Io algorithm was used (for details, consult (Aha et al., 1991; Daelemans and Van den Bosch, 1992; Daelemans et al., 1997b), with gain ratio selected as feature weighting metric. 3 Results We performed experiments on the following five language data sets — More details on numbers of features, values per features, number of classes and number of instances are displayed in Table 1: Diminutive formation (henceforth DIM): choosing the correct diminutive inflection to Dutch nouns out of five possible: je, tje, pje, kje, and etje, on the basis of phonemic word transcriptions, segmented at the level of syllable onset, nucei and coda of the final three syllables</context>
</contexts>
<marker>Aha, Kibler, Albert, 1991</marker>
<rawString>D.W. Aha, D. Kibler, and M. Albert. 1991. Instance-based learning algorithms. Machine Learning, 6:37-66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L Blake</author>
<author>C J Merz</author>
</authors>
<title>UCI repository of machine learning databases.</title>
<date>1998</date>
<contexts>
<context position="18028" citStr="Blake and Merz, 1998" startWordPosition="3123" endWordPosition="3126">uracies of ml-IG, RIPPER, and RBM on three machinelearning benchmark tasks. a data set is more disjunct (Daelemans et al., 1999)). Although RBM performs equal to or better than RIPPER, it often does not regain the level of Isl-IG. High disjunctivity appears to be a typical feature of language tasks (Van den Bosch, 1999b); other non-language tasks generally display less disjunctivity, which opens the possibility that the RBM approach may work well for some of these tasks. We performed pilot tests on three machine learning benchmark classification tasks (taken from the UCI benchmark repository (Blake and Merz, 1998)) with symbolic, multi-valued features. Table 5 displays the results of these experiments. Although the data set selection is small, the results of RBM and especially of ml-IG+RBm are promising; the latter algorithm is consistently better than ml-IG. More research and comparisons are needed to arrive at a broader picture. An immediate point of further research lies in the external rule induction algorithm. First, RIPPER has options that have not been used here, but that may be relevant for the current issue, e.g. RIPPER&apos;S ability to represent sets of values at left-hand side conditions, and it</context>
</contexts>
<marker>Blake, Merz, 1998</marker>
<rawString>C.L. Blake and C.J. Merz. 1998. UCI repository of machine learning databases.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>TnT - a statistical part-ofspeech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the Sixth Applied Natural Language Processing (ANLP-2000),</booktitle>
<location>Seattle, WA.</location>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000. TnT - a statistical part-ofspeech tagger. In Proceedings of the Sixth Applied Natural Language Processing (ANLP-2000), Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>A Corpus-Based Approach to Language Learning.</title>
<date>1993</date>
<tech>Dissertation,</tech>
<institution>Department of Computer and Information Science, University of Pennsylvania.</institution>
<marker>Brill, 1993</marker>
<rawString>E. Brill. 1993. A Corpus-Based Approach to Language Learning. Dissertation, Department of Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Clark</author>
<author>T Niblett</author>
</authors>
<date>1989</date>
<booktitle>The CN2 rule induction algorithm. Machine Learning,</booktitle>
<pages>3--261</pages>
<contexts>
<context position="1237" citStr="Clark and Niblett, 1989" startWordPosition="186" endWordPosition="189">ory instance to a new instance is measured by taking the sum of the weights of the matching rules both instances share. We report on experiments that indicate that (i) the method works equally well or better than RIPPER on various language learning and other benchmark datasets; (ii) the method does not necessarily perform better than default memory-based learning, but (iii) when multivalued features are combined with the rulebased features, some slight to significant improvements are observed. 1 Rules as features A common machine-learning solution to classification problems is rule induction (Clark and Niblett, 1989; Quinlan, 1993; Cohen, 1995). The goal of rule induction is generally to induce a set of rules from data, that captures all generalisable knowledge within that data, and that is as small as possible at the same time. Classification in rule-induction classifiers is based on the firing of rules on a new instance, triggered by matching feature values to the left-hand side of the rule. Rules can be of various normal forms, and can furthermore be ordered. The appropriate content and ordering of rules can be hard to find, and at the heart of most rule induction systems are strong search algorithms </context>
</contexts>
<marker>Clark, Niblett, 1989</marker>
<rawString>P. Clark and T. Niblett. 1989. The CN2 rule induction algorithm. Machine Learning, 3:261-284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W W Cohen</author>
</authors>
<title>Fast effective rule induction.</title>
<date>1995</date>
<booktitle>In Proceedings of the Twelfth International Conference on Machine Learning,</booktitle>
<location>Lake Tahoe, California.</location>
<contexts>
<context position="1266" citStr="Cohen, 1995" startWordPosition="192" endWordPosition="193">ed by taking the sum of the weights of the matching rules both instances share. We report on experiments that indicate that (i) the method works equally well or better than RIPPER on various language learning and other benchmark datasets; (ii) the method does not necessarily perform better than default memory-based learning, but (iii) when multivalued features are combined with the rulebased features, some slight to significant improvements are observed. 1 Rules as features A common machine-learning solution to classification problems is rule induction (Clark and Niblett, 1989; Quinlan, 1993; Cohen, 1995). The goal of rule induction is generally to induce a set of rules from data, that captures all generalisable knowledge within that data, and that is as small as possible at the same time. Classification in rule-induction classifiers is based on the firing of rules on a new instance, triggered by matching feature values to the left-hand side of the rule. Rules can be of various normal forms, and can furthermore be ordered. The appropriate content and ordering of rules can be hard to find, and at the heart of most rule induction systems are strong search algorithms that attempt to minimise sear</context>
<context position="6412" citStr="Cohen, 1995" startWordPosition="1095" endWordPosition="1096">at the method works equally well or better than RIPPER on various language learning and other benchmark datasets. However, the method does not necessarily perform better than default memorybased learning. Only when the rule-indexing features are added to the original multi-valued features, improvements are observed. 2 Rule-Based Memory: algorithm A new memory-based learning variant RBM, which stands for Rule-Based Memory, implements the ideas described in the previous section using the following procedure: given a training set and a test set of a certain classification task, (1) apply RIPPER (Cohen, 1995) to the training set, and collect the set of induced rules; (2) recode the instances in the training and test set according to these rules; (3) apply the basic memory-based learning algorithm iBl-IG to the recoded training set, and k-NNclassify the recoded test set. We describe each of these steps briefly here. RIPPER (Cohen, 1995) is a fast rule induction algorithm that splits the training set in two. On the basis of one part it induces rules in a straightforward way, with potential overfitting. When the induced rules classify instances in the other part of the split training set below some c</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>W. W. Cohen. 1995. Fast effective rule induction. In Proceedings of the Twelfth International Conference on Machine Learning, Lake Tahoe, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Collins</author>
<author>J Brooks</author>
</authors>
<title>Prepositional phrase attachment through a backed-off model.</title>
<date>1995</date>
<booktitle>In Proc. of Third Workshop on Very Large Corpora,</booktitle>
<location>Cambridge.</location>
<contexts>
<context position="10317" citStr="Collins and Brooks, 1995" startWordPosition="1742" endWordPosition="1745"> architecture that processes a sentence from a disambiguated left to an ambiguous right context , as described in (Daelemans et al., 1996). The original data set for the partof-speech tagging task, extracted from the LOB corpus, contains 1,046,151 instances; we have used a randomly-extracted 10% of this data. PP attachment (PP): the attachment of a PP in the sequence VP NP PP (VP = verb phrase, NP = noun phrase, PP = prepositional phrase). The data consists of fourtuples of words, extracted from the Wall Street Journal Treebank. From the original data set, used by (Ratnaparkhi et al., 1994), (Collins and Brooks, 1995), and (Zavrel et al., 1997), (Daelemans et al., 1999) took the train and test set together to form the particular data also used here. Table 2 lists the average (10-fold crossvalidation) accuracies, measured in percentages of correctly classified test instances, of IB1-IG, RIPPER, and RBM on these five tasks. The clearest overall pattern in this table is the high accuracy of iBl-Io, surpassed only twice by RBM on the DIM and NPSM tasks (significantly, according to one-tailed t-tests, with p &lt; 0.05). On the other three tasks, IB1-Io outperforms RBM. RIPPER performs significantly more accurately</context>
</contexts>
<marker>Collins, Brooks, 1995</marker>
<rawString>M.J Collins and J. Brooks. 1995. Prepositional phrase attachment through a backed-off model. In Proc. of Third Workshop on Very Large Corpora, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A Van den Bosch</author>
</authors>
<title>Generalisation performance of backpropagation learning on a syllabification task.</title>
<date>1992</date>
<booktitle>Proc. of TWLT3: Connectionism and Natural Language Processing,</booktitle>
<pages>27--37</pages>
<editor>In M. F. J. Drossaers and A. Nijholt, editors,</editor>
<institution>Enschede. Twente University.</institution>
<marker>Daelemans, Van den Bosch, 1992</marker>
<rawString>W. Daelemans and A. Van den Bosch. 1992. Generalisation performance of backpropagation learning on a syllabification task. In M. F. J. Drossaers and A. Nijholt, editors, Proc. of TWLT3: Connectionism and Natural Language Processing, pages 27-37, Enschede. Twente University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>P Berck</author>
<author>S Gillis</author>
</authors>
<title>MBT: A memory-based part of speech tagger generator.</title>
<date>1996</date>
<booktitle>Proc. of Fourth Workshop on Very Large Corpora,</booktitle>
<pages>14--27</pages>
<editor>In E. Ejerhed and I.Dagan, editors,</editor>
<publisher>ACL SIGDAT.</publisher>
<contexts>
<context position="9830" citStr="Daelemans et al., 1996" startWordPosition="1657" endWordPosition="1660">, 1998) used the Base-NP tag set as presented in (Ramshaw and Marcus, 1995): I for inside a Base-NP, 0 for outside a Base-NP, and B for the first word in a Base-NP following another Base-NP. See (Veenstra, 1998) for more details, and (Daelemans et al., 1999) for a series of experiments on the original data set from which we have used a randomly-extracted 10%. Part-of-speech tagging (Possm): the disambiguation of syntactic classes of words in particular contexts. We assume a tagger architecture that processes a sentence from a disambiguated left to an ambiguous right context , as described in (Daelemans et al., 1996). The original data set for the partof-speech tagging task, extracted from the LOB corpus, contains 1,046,151 instances; we have used a randomly-extracted 10% of this data. PP attachment (PP): the attachment of a PP in the sequence VP NP PP (VP = verb phrase, NP = noun phrase, PP = prepositional phrase). The data consists of fourtuples of words, extracted from the Wall Street Journal Treebank. From the original data set, used by (Ratnaparkhi et al., 1994), (Collins and Brooks, 1995), and (Zavrel et al., 1997), (Daelemans et al., 1999) took the train and test set together to form the particular</context>
</contexts>
<marker>Daelemans, Zavrel, Berck, Gillis, 1996</marker>
<rawString>W. Daelemans, J. Zavrel, P. Berck, and S. Gillis. 1996. MBT: A memory-based part of speech tagger generator. In E. Ejerhed and I.Dagan, editors, Proc. of Fourth Workshop on Very Large Corpora, pages 14-27. ACL SIGDAT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>P Berck</author>
<author>S Gillis</author>
</authors>
<title>Data mining as a method for linguistic analysis: Dutch diminutives. Folia Linguistica,</title>
<date>1997</date>
<pages>1--2</pages>
<contexts>
<context position="2103" citStr="Daelemans et al., 1997" startWordPosition="336" endWordPosition="339">uction classifiers is based on the firing of rules on a new instance, triggered by matching feature values to the left-hand side of the rule. Rules can be of various normal forms, and can furthermore be ordered. The appropriate content and ordering of rules can be hard to find, and at the heart of most rule induction systems are strong search algorithms that attempt to minimise search through the space of possible rule sets and orderings. Although rules appear quite different from instances as used in memory-based or instancebased learning (Aha et al., 1991; Daelemans and Van den Bosch, 1992; Daelemans et al., 1997b) there is a continuum between them. Rules can be seen as generalised instances; they represent the set of training instances with the same class that match on the conditions on the left-hand side of the rule. Therefore, classification strategies from memory-based learning can naturally be applied to rules. For example, (Domingos, 1996) describes the RISE system, in which rules are (carefully) generalised from instances, and in which the k-NN classification rule searches for nearest neighbours within these rules when classifying new instances. Often, the sets of instances covered by rules ove</context>
<context position="8279" citStr="Daelemans et al., 1997" startWordPosition="1402" endWordPosition="1405">est set was recoded in batch, but this could have been done on74 line during classification without much computational overhead. For each language task we experimented on, we performed 10-fold cross validation tests, so ten different train-test partitions were produced (Weiss and Kulikowski, 1991) that were recoded, and then tested on. Tests were performed with the TiMBL software package (Daelemans et al., 2000), using the software&apos;s dedicated routines for handling binary features. The default ml-Io algorithm was used (for details, consult (Aha et al., 1991; Daelemans and Van den Bosch, 1992; Daelemans et al., 1997b), with gain ratio selected as feature weighting metric. 3 Results We performed experiments on the following five language data sets — More details on numbers of features, values per features, number of classes and number of instances are displayed in Table 1: Diminutive formation (henceforth DIM): choosing the correct diminutive inflection to Dutch nouns out of five possible: je, tje, pje, kje, and etje, on the basis of phonemic word transcriptions, segmented at the level of syllable onset, nucei and coda of the final three syllables of the word. The data stems from a study described in (Dae</context>
</contexts>
<marker>Daelemans, Berck, Gillis, 1997</marker>
<rawString>W. Daelemans, P. Berck, and S. Gillis. 1997a. Data mining as a method for linguistic analysis: Dutch diminutives. Folia Linguistica, XXXI(1-2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A Van den Bosch</author>
<author>A Weijters</author>
</authors>
<title>IGTree: using trees for compression and classification in lazy learning algorithms.</title>
<date>1997</date>
<journal>Artificial Intelligence Review,</journal>
<pages>11--407</pages>
<marker>Daelemans, Van den Bosch, Weijters, 1997</marker>
<rawString>W. Daelemans, A. Van den Bosch, and A. Weijters. 1997b. IGTree: using trees for compression and classification in lazy learning algorithms. Artificial Intelligence Review, 11:407-423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A Van den Bosch</author>
<author>J Zavrel</author>
</authors>
<title>Forgetting exceptions is harmful in language learning.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<marker>Daelemans, Van den Bosch, Zavrel, 1999</marker>
<rawString>W. Daelemans, A. Van den Bosch, and J. Zavrel. 1999. Forgetting exceptions is harmful in language learning. Machine Learning, 34(1-3):11-43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>K Van der Sloot</author>
<author>A Van den Bosch</author>
</authors>
<title>TiMBL: Tilburg Memory Based Learner, version 3.0, reference manual.</title>
<date>2000</date>
<tech>Technical Report ILK-0001,</tech>
<institution>ILK, Tilburg University.</institution>
<marker>Daelemans, Zavrel, Van der Sloot, Van den Bosch, 2000</marker>
<rawString>W. Daelemans, J. Zavrel, K. Van der Sloot, and A. Van den Bosch. 2000. TiMBL: Tilburg Memory Based Learner, version 3.0, reference manual. Technical Report ILK-0001, ILK, Tilburg University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Domingos</author>
</authors>
<title>The rise 2.0 system: A case study in multistrategy learning.</title>
<date>1995</date>
<tech>Technical Report 95-2,</tech>
<institution>University of California at Irvine, Department of Information and Computer Science,</institution>
<location>Irvine, CA.</location>
<contexts>
<context position="16165" citStr="Domingos, 1995" startWordPosition="2816" endWordPosition="2817">e 4: Average generalisation accuracies of IB1-IG and Isl-IG + RBM, and the percentage of error reduction, on five language learning tasks. `,1,&apos; denotes significantly better accuracy of mlIG+RBm over ml-IG with p &lt; 0.05. GPSM causes a slight drop in performance - and a slowdown. 4 Discussion Representing instances by complex features that have been induced by a rule induction algorithm appears, in view of the measured accuracies, a viable alternative approach to using rules, as compared to standard rule induction. This result is in line with results reported by Domingos on the RISE algorithm (Domingos, 1995; Domingos, 1996). A marked difference is that in RISE, the rules are the instances in kNN classification (and due to the careful generalisation strategy of RISE, they can be very instance-specific), while in RBM, the rules are the features by which the original instances are indexed. When a nearest neighbor is found to a query instance in RBM, it is because the two instances share one or more matching rules. The actual classification that is transferred from the memory instance to the new instance is just the classification that this memory item is stored with - it may well be another class t</context>
</contexts>
<marker>Domingos, 1995</marker>
<rawString>P. Domingos. 1995. The rise 2.0 system: A case study in multistrategy learning. Technical Report 95-2, University of California at Irvine, Department of Information and Computer Science, Irvine, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Domingos</author>
</authors>
<title>Unifying instance-based and rule-based induction.</title>
<date>1996</date>
<booktitle>Machine Learning,</booktitle>
<pages>24--141</pages>
<contexts>
<context position="2442" citStr="Domingos, 1996" startWordPosition="391" endWordPosition="392">lgorithms that attempt to minimise search through the space of possible rule sets and orderings. Although rules appear quite different from instances as used in memory-based or instancebased learning (Aha et al., 1991; Daelemans and Van den Bosch, 1992; Daelemans et al., 1997b) there is a continuum between them. Rules can be seen as generalised instances; they represent the set of training instances with the same class that match on the conditions on the left-hand side of the rule. Therefore, classification strategies from memory-based learning can naturally be applied to rules. For example, (Domingos, 1996) describes the RISE system, in which rules are (carefully) generalised from instances, and in which the k-NN classification rule searches for nearest neighbours within these rules when classifying new instances. Often, the sets of instances covered by rules overlap. In other words, seen from the instance perspective, a single instance can match more than one rule. Consider the schematic example displayed in Figure 1. Three instances with three multi-valued features match individually with one or two of the four rules; for example, the first instance matches with rule 1 (if fl = A then c = Z) a</context>
<context position="16182" citStr="Domingos, 1996" startWordPosition="2818" endWordPosition="2819">eralisation accuracies of IB1-IG and Isl-IG + RBM, and the percentage of error reduction, on five language learning tasks. `,1,&apos; denotes significantly better accuracy of mlIG+RBm over ml-IG with p &lt; 0.05. GPSM causes a slight drop in performance - and a slowdown. 4 Discussion Representing instances by complex features that have been induced by a rule induction algorithm appears, in view of the measured accuracies, a viable alternative approach to using rules, as compared to standard rule induction. This result is in line with results reported by Domingos on the RISE algorithm (Domingos, 1995; Domingos, 1996). A marked difference is that in RISE, the rules are the instances in kNN classification (and due to the careful generalisation strategy of RISE, they can be very instance-specific), while in RBM, the rules are the features by which the original instances are indexed. When a nearest neighbor is found to a query instance in RBM, it is because the two instances share one or more matching rules. The actual classification that is transferred from the memory instance to the new instance is just the classification that this memory item is stored with - it may well be another class than any of its ma</context>
</contexts>
<marker>Domingos, 1996</marker>
<rawString>P. Domingos. 1996. Unifying instance-based and rule-based induction. Machine Learning, 24:141-168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>c4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="1252" citStr="Quinlan, 1993" startWordPosition="190" endWordPosition="191">tance is measured by taking the sum of the weights of the matching rules both instances share. We report on experiments that indicate that (i) the method works equally well or better than RIPPER on various language learning and other benchmark datasets; (ii) the method does not necessarily perform better than default memory-based learning, but (iii) when multivalued features are combined with the rulebased features, some slight to significant improvements are observed. 1 Rules as features A common machine-learning solution to classification problems is rule induction (Clark and Niblett, 1989; Quinlan, 1993; Cohen, 1995). The goal of rule induction is generally to induce a set of rules from data, that captures all generalisable knowledge within that data, and that is as small as possible at the same time. Classification in rule-induction classifiers is based on the firing of rules on a new instance, triggered by matching feature values to the left-hand side of the rule. Rules can be of various normal forms, and can furthermore be ordered. The appropriate content and ordering of rules can be hard to find, and at the heart of most rule induction systems are strong search algorithms that attempt to</context>
<context position="18799" citStr="Quinlan, 1993" startWordPosition="3250" endWordPosition="3251">specially of ml-IG+RBm are promising; the latter algorithm is consistently better than ml-IG. More research and comparisons are needed to arrive at a broader picture. An immediate point of further research lies in the external rule induction algorithm. First, RIPPER has options that have not been used here, but that may be relevant for the current issue, e.g. RIPPER&apos;S ability to represent sets of values at left-hand side conditions, and its flexibility in producing larger or smaller numbers of rules. Second, other rule induction algorithms exist that may play RIPPER&apos;S role, such as c4.5RuLEs (Quinlan, 1993). More generally, further research should focus on the scaling properties of the approach (including the scaling of the external ruleinduction algorithm), should investigate more and larger language data sets, and should seek comparisons with other existing methods that claim to handle complex features efficiently (Brill, 1993; Ratnaparkhi, 1997; Roth, 1998; Brants, 2000). % Correct test instances Task IB1-IG RBM IM-IG-ERBM CAR 93.9 ± 2.1 98.9 ± 0.8 97.2 ± 1.3 NURSERY 94.6 ± 0.6 98.6 ± 0.5 98.7 ± 0.2 SPLICE 91.7 ± 1.1 89.0 ± 2.1 92.7 ± 1.7 77 Acknowledgements The author thanks the members of t</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J.R. Quinlan. 1993. c4.5: Programs for Machine Learning. Morgan Kaufmann, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L A Ramshaw</author>
<author>M P Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proc. of Third Workshop on Very Large Corpora,</booktitle>
<pages>82--94</pages>
<contexts>
<context position="9282" citStr="Ramshaw and Marcus, 1995" startWordPosition="1564" endWordPosition="1567">e: je, tje, pje, kje, and etje, on the basis of phonemic word transcriptions, segmented at the level of syllable onset, nucei and coda of the final three syllables of the word. The data stems from a study described in (Daelemans et al., 1997a). Grapheme-phoneme conversion (oPsm): the conversion of a window of nine letters to the phonemic transcription of the middle letter. From the original data set described in (Van den Bosch, 1997) a 10% subset was drawn. Base-NP chunking (NPSM): the segmentation of sentences into non-recursive NPs. (Veenstra, 1998) used the Base-NP tag set as presented in (Ramshaw and Marcus, 1995): I for inside a Base-NP, 0 for outside a Base-NP, and B for the first word in a Base-NP following another Base-NP. See (Veenstra, 1998) for more details, and (Daelemans et al., 1999) for a series of experiments on the original data set from which we have used a randomly-extracted 10%. Part-of-speech tagging (Possm): the disambiguation of syntactic classes of words in particular contexts. We assume a tagger architecture that processes a sentence from a disambiguated left to an ambiguous right context , as described in (Daelemans et al., 1996). The original data set for the partof-speech taggin</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>L.A. Ramshaw and M.P. Marcus. 1995. Text chunking using transformation-based learning. In Proc. of Third Workshop on Very Large Corpora, pages 82-94, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
<author>J Reynar</author>
<author>S Roukos</author>
</authors>
<title>A maximum entropy model for prepositional phrase attachment.</title>
<date>1994</date>
<booktitle>In Workshop on Human Language Technology,</booktitle>
<publisher>ARPA.</publisher>
<location>Plainsboro, NJ,</location>
<contexts>
<context position="10289" citStr="Ratnaparkhi et al., 1994" startWordPosition="1738" endWordPosition="1741">contexts. We assume a tagger architecture that processes a sentence from a disambiguated left to an ambiguous right context , as described in (Daelemans et al., 1996). The original data set for the partof-speech tagging task, extracted from the LOB corpus, contains 1,046,151 instances; we have used a randomly-extracted 10% of this data. PP attachment (PP): the attachment of a PP in the sequence VP NP PP (VP = verb phrase, NP = noun phrase, PP = prepositional phrase). The data consists of fourtuples of words, extracted from the Wall Street Journal Treebank. From the original data set, used by (Ratnaparkhi et al., 1994), (Collins and Brooks, 1995), and (Zavrel et al., 1997), (Daelemans et al., 1999) took the train and test set together to form the particular data also used here. Table 2 lists the average (10-fold crossvalidation) accuracies, measured in percentages of correctly classified test instances, of IB1-IG, RIPPER, and RBM on these five tasks. The clearest overall pattern in this table is the high accuracy of iBl-Io, surpassed only twice by RBM on the DIM and NPSM tasks (significantly, according to one-tailed t-tests, with p &lt; 0.05). On the other three tasks, IB1-Io outperforms RBM. RIPPER performs s</context>
</contexts>
<marker>Ratnaparkhi, Reynar, Roukos, 1994</marker>
<rawString>A. Ratnaparkhi, J. Reynar, and S. Roukos. 1994. A maximum entropy model for prepositional phrase attachment. In Workshop on Human Language Technology, Plainsboro, NJ, March. ARPA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A linear observed time statistical parser based on maximum entropy models.</title>
<date>1997</date>
<booktitle>Technical Report cmp-lg/9706014, Computation and Language,</booktitle>
<location>http://xxx.lanl.gov/list/cmp-lg/,</location>
<marker>Ratnaparkhi, 1997</marker>
<rawString>A. Ratnaparkhi. 1997. A linear observed time statistical parser based on maximum entropy models. Technical Report cmp-lg/9706014, Computation and Language, http://xxx.lanl.gov/list/cmp-lg/, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
</authors>
<title>Learning to resolve natural language ambiguities: A unified approach.</title>
<date>1998</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence,</booktitle>
<pages>898--904</pages>
<marker>Roth, 1998</marker>
<rawString>D. Roth. 1998. Learning to resolve natural language ambiguities: A unified approach. In Proceedings of the National Conference on Artificial Intelligence, pages 898-904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Van den Bosch</author>
</authors>
<title>Learning to pronounce written words: A study in inductive language learning.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>Universiteit Maastricht.</institution>
<marker>Van den Bosch, 1997</marker>
<rawString>A. Van den Bosch. 1997. Learning to pronounce written words: A study in inductive language learning. Ph.D. thesis, Universiteit Maastricht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Van den Bosch</author>
</authors>
<title>Careful abstraction from instance families in memory-based language learning.</title>
<date>1999</date>
<journal>Journal for Experimental and Theoretical Artificial Intelligence,</journal>
<pages>11--3</pages>
<marker>Van den Bosch, 1999</marker>
<rawString>A. Van den Bosch. 1999a. Careful abstraction from instance families in memory-based language learning. Journal for Experimental and Theoretical Artificial Intelligence, 11(3):339-368.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Van den Bosch</author>
</authors>
<title>Instance-family abstraction in memory-based language learning.</title>
<date>1999</date>
<booktitle>Machine Learning: Proceedings of the Sixteenth International Conference,</booktitle>
<pages>39--48</pages>
<editor>In I. Bratko and S. Dzeroski, editors,</editor>
<location>Bled, Slovenia.</location>
<marker>Van den Bosch, 1999</marker>
<rawString>A. Van den Bosch. 1999b. Instance-family abstraction in memory-based language learning. In I. Bratko and S. Dzeroski, editors, Machine Learning: Proceedings of the Sixteenth International Conference, pages 39-48, Bled, Slovenia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Veenstra</author>
</authors>
<title>Fast NP chunking using memorybased learning techniques.</title>
<date>1998</date>
<booktitle>In Proceedings of BENELEARN&apos;98,</booktitle>
<location>Wageningen, The Netherlands.</location>
<contexts>
<context position="9214" citStr="Veenstra, 1998" startWordPosition="1554" endWordPosition="1555">t diminutive inflection to Dutch nouns out of five possible: je, tje, pje, kje, and etje, on the basis of phonemic word transcriptions, segmented at the level of syllable onset, nucei and coda of the final three syllables of the word. The data stems from a study described in (Daelemans et al., 1997a). Grapheme-phoneme conversion (oPsm): the conversion of a window of nine letters to the phonemic transcription of the middle letter. From the original data set described in (Van den Bosch, 1997) a 10% subset was drawn. Base-NP chunking (NPSM): the segmentation of sentences into non-recursive NPs. (Veenstra, 1998) used the Base-NP tag set as presented in (Ramshaw and Marcus, 1995): I for inside a Base-NP, 0 for outside a Base-NP, and B for the first word in a Base-NP following another Base-NP. See (Veenstra, 1998) for more details, and (Daelemans et al., 1999) for a series of experiments on the original data set from which we have used a randomly-extracted 10%. Part-of-speech tagging (Possm): the disambiguation of syntactic classes of words in particular contexts. We assume a tagger architecture that processes a sentence from a disambiguated left to an ambiguous right context , as described in (Daelema</context>
</contexts>
<marker>Veenstra, 1998</marker>
<rawString>J. Veenstra. 1998. Fast NP chunking using memorybased learning techniques. In Proceedings of BENELEARN&apos;98, Wageningen, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Weiss</author>
<author>C Kulikowski</author>
</authors>
<title>Computer systems that learn.</title>
<date>1991</date>
<publisher>Morgan Kaufmann.</publisher>
<location>San Mateo, CA:</location>
<contexts>
<context position="7955" citStr="Weiss and Kulikowski, 1991" startWordPosition="1349" endWordPosition="1352">e rule set. In our experiments, we let RIPPER order the rules from highfrequent to low-frequent, the idea being that this method would yield more complex features. Then, the rule set was taken as the basis for recoding both the training and test set, as schematically visualised in Figure 1. As with the training material, each test set was recoded in batch, but this could have been done on74 line during classification without much computational overhead. For each language task we experimented on, we performed 10-fold cross validation tests, so ten different train-test partitions were produced (Weiss and Kulikowski, 1991) that were recoded, and then tested on. Tests were performed with the TiMBL software package (Daelemans et al., 2000), using the software&apos;s dedicated routines for handling binary features. The default ml-Io algorithm was used (for details, consult (Aha et al., 1991; Daelemans and Van den Bosch, 1992; Daelemans et al., 1997b), with gain ratio selected as feature weighting metric. 3 Results We performed experiments on the following five language data sets — More details on numbers of features, values per features, number of classes and number of instances are displayed in Table 1: Diminutive for</context>
</contexts>
<marker>Weiss, Kulikowski, 1991</marker>
<rawString>S. Weiss and C. Kulikowski. 1991. Computer systems that learn. San Mateo, CA: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zavrel</author>
<author>W Daelemans</author>
<author>J Veenstra</author>
</authors>
<title>Resolving PP attachment ambiguities with memorybased learning.</title>
<date>1997</date>
<booktitle>Proc. of the Workshop on Computational Language Learning (CoNLL&apos;.97), ACL,</booktitle>
<editor>In M. Ellison, editor,</editor>
<location>Madrid.</location>
<contexts>
<context position="10344" citStr="Zavrel et al., 1997" startWordPosition="1747" endWordPosition="1751">entence from a disambiguated left to an ambiguous right context , as described in (Daelemans et al., 1996). The original data set for the partof-speech tagging task, extracted from the LOB corpus, contains 1,046,151 instances; we have used a randomly-extracted 10% of this data. PP attachment (PP): the attachment of a PP in the sequence VP NP PP (VP = verb phrase, NP = noun phrase, PP = prepositional phrase). The data consists of fourtuples of words, extracted from the Wall Street Journal Treebank. From the original data set, used by (Ratnaparkhi et al., 1994), (Collins and Brooks, 1995), and (Zavrel et al., 1997), (Daelemans et al., 1999) took the train and test set together to form the particular data also used here. Table 2 lists the average (10-fold crossvalidation) accuracies, measured in percentages of correctly classified test instances, of IB1-IG, RIPPER, and RBM on these five tasks. The clearest overall pattern in this table is the high accuracy of iBl-Io, surpassed only twice by RBM on the DIM and NPSM tasks (significantly, according to one-tailed t-tests, with p &lt; 0.05). On the other three tasks, IB1-Io outperforms RBM. RIPPER performs significantly more accurately than ml-Io only on the DIM</context>
</contexts>
<marker>Zavrel, Daelemans, Veenstra, 1997</marker>
<rawString>J. Zavrel, W. Daelemans, and J. Veenstra. 1997. Resolving PP attachment ambiguities with memorybased learning. In M. Ellison, editor, Proc. of the Workshop on Computational Language Learning (CoNLL&apos;.97), ACL, Madrid.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>