<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004128">
<title confidence="0.989194">
Syntagmatic Kernels:
a Word Sense Disambiguation Case Study
</title>
<author confidence="0.793136">
Claudio Giuliano and Alfio Gliozzo and Carlo Strapparava
</author>
<affiliation confidence="0.526906">
ITC-irst, Istituto per la Ricerca Scientifica e Tecnologica
</affiliation>
<address confidence="0.651126">
I-38050, Trento, ITALY
</address>
<email confidence="0.987041">
{giuliano,gliozzo,strappa}@itc.it
</email>
<sectionHeader confidence="0.995489" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997942384615385">
In this paper we present a family of ker-
nel functions, named Syntagmatic Ker-
nels, which can be used to model syn-
tagmatic relations. Syntagmatic relations
hold among words that are typically collo-
cated in a sequential order, and thus they
can be acquired by analyzing word se-
quences. In particular, Syntagmatic Ker-
nels are defined by applying a Word Se-
quence Kernel to the local contexts of the
words to be analyzed. In addition, this
approach allows us to define a semi su-
pervised learning schema where external
lexical knowledge is plugged into the su-
pervised learning process. Lexical knowl-
edge is acquired from both unlabeled data
and hand-made lexical resources, such as
WordNet. We evaluated the syntagmatic
kernel on two standard Word Sense Dis-
ambiguation tasks (i.e. English and Ital-
ian lexical-sample tasks of Senseval-3),
where the syntagmatic information plays
a crucial role. We compared the Syntag-
matic Kernel with the standard approach,
showing promising improvements in per-
formance.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999957756756757">
In computational linguistics, it is usual to deal with
sequences: words are sequences of letters and syn-
tagmatic relations are established by sequences of
words. Sequences are analyzed to measure morpho-
logical similarity, to detect multiwords, to represent
syntagmatic relations, and so on. Hence modeling
syntagmatic relations is crucial for a wide variety
of NLP tasks, such as Named Entity Recognition
(Gliozzo et al., 2005a) and Word Sense Disambigua-
tion (WSD) (Strapparava et al., 2004).
In general, the strategy adopted to model syntag-
matic relations is to provide bigrams and trigrams of
collocated words as features to describe local con-
texts (Yarowsky, 1994), and each word is regarded
as a different instance to classify. For instance, oc-
currences of a given class of named entities (such
as names of persons) can be discriminated in texts
by recognizing word patterns in their local contexts.
For example the token Rossi, whenever is preceded
by the token Prof., often represents the name of a
person. Another task that can benefit from modeling
this kind of relations is WSD. To solve ambiguity it
is necessary to analyze syntagmatic relations in the
local context of the word to be disambiguated. In
this paper we propose a kernel function that can be
used to model such relations, the Syntagmatic Ker-
nel, and we apply it to two (English and Italian)
lexical-sample WSD tasks of the Senseval-3 com-
petition (Mihalcea and Edmonds, 2004).
In a lexical-sample WSD task, training data are
provided as a set of texts, in which for each text
a given target word is manually annotated with a
sense from a predetermined set of possibilities. To
model syntagmatic relations, the typical supervised
learning framework adopts as features bigrams and
trigrams in a local context. The main drawback of
this approach is that non contiguous or shifted col-
</bodyText>
<page confidence="0.997246">
57
</page>
<bodyText confidence="0.997858703125">
locations cannot be identified, decreasing the gener-
alization power of the learning algorithm. For ex-
ample, suppose that the verb to score has to be dis-
ambiguated into the sentence “Ronaldo scored the
goal”, and that the sense tagged example “the foot-
ball player scores#1 the first goal” is provided for
training. A traditional feature mapping would ex-
tract the bigram w+1 w+2:the goal to represent the
former, and the bigram w+1 w+2:the first to index
the latter. Evidently such features will not match,
leading the algorithm to a misclassification.
In the present paper we propose the Syntagmatic
Kernel as an attempt to solve this problem. The
Syntagmatic Kernel is based on a Gap-Weighted
Subsequences Kernel (Shawe-Taylor and Cristian-
ini, 2004). In the spirit of Kernel Methods, this
kernel is able to compare sequences directly in the
input space, avoiding any explicit feature mapping.
To perform this operation, it counts how many times
a (non-contiguous) subsequence of symbols u of
length n occurs in the input string s, and penalizes
non-contiguous occurrences according to the num-
ber of the contained gaps. To define our Syntag-
matic Kernel, we adapted the generic definition of
the Sequence Kernels to the problem of recognizing
collocations in local word contexts.
In the above definition of Syntagmatic Kernel,
only exact word-matches contribute to the similar-
ity. One shortcoming of this approach is that (near-
)synonyms will never be considered similar, lead-
ing to a very low generalization power of the learn-
ing algorithm, that requires a huge amount of data
to converge to an accurate prediction. To solve this
problem we provided external lexical knowledge to
the supervised learning algorithm, in order to define
a “soft-matching” schema for the kernel function.
For example, if we consider as equivalent the terms
Ronaldo and football player, the proposition “The
football player scored the first goal” is equivalent to
the sentence “Ronaldo scored the first goal”, pro-
viding a strong evidence to disambiguate the latter
occurrence of the verb.
We propose two alternative soft-matching criteria
exploiting two different knowledge sources: (i) hand
made resources and (ii) unsupervised term similar-
ity measures. The first approach performs a soft-
matching among all those synonyms words in Word-
Net, while the second exploits domain relations, ac-
quired from unlabeled data, for the same purpose.
Our experiments, performed on two standard
WSD benchmarks, show the superiority of the Syn-
tagmatic Kernel with respect to a classical flat vector
representation of bigrams and trigrams.
The paper is structured as follows. Section 2 in-
troduces the Sequence Kernels. In Section 3 the
Syntagmatic Kernel is defined. Section 4 explains
how soft-matching can be exploited by the Collo-
cation Kernel, describing two alternative criteria:
WordNet Synonymy and Domain Proximity. Sec-
tion 5 gives a brief sketch of the complete WSD
system, composed by the combination of different
kernels, dealing with syntagmatic and paradigmatic
aspects. Section 6 evaluates the Syntagmatic Kernel,
and finally Section 7 concludes the paper.
</bodyText>
<sectionHeader confidence="0.955689" genericHeader="method">
2 Sequence Kernels
</sectionHeader>
<bodyText confidence="0.999867103448276">
The basic idea behind kernel methods is to embed
the data into a suitable feature space F via a map-
ping function 0 : X → F, and then use a linear al-
gorithm for discovering nonlinear patterns. Instead
of using the explicit mapping 0, we can use a kernel
function K : X x X → R, that corresponds to the
inner product in a feature space which is, in general,
different from the input space.
Kernel methods allow us to build a modular sys-
tem, as the kernel function acts as an interface be-
tween the data and the learning algorithm. Thus
the kernel function becomes the only domain spe-
cific module of the system, while the learning algo-
rithm is a general purpose component. Potentially
any kernel function can work with any kernel-based
algorithm. In our system we use Support Vector Ma-
chines (Cristianini and Shawe-Taylor, 2000).
Sequence Kernels (or String Kernels) are a fam-
ily of kernel functions developed to compute the
inner product among images of strings in high-
dimensional feature space using dynamic program-
ming techniques (Shawe-Taylor and Cristianini,
2004). The Gap-Weighted Subsequences Kernel is
the most general Sequence Kernel. Roughly speak-
ing, it compares two strings by means of the num-
ber of contiguous and non-contiguous substrings of
a given length they have in common. Non contigu-
ous occurrences are penalized according to the num-
ber of gaps they contain.
</bodyText>
<page confidence="0.988945">
58
</page>
<bodyText confidence="0.999363384615384">
Formally, let E be an alphabet of |E |symbols,
and s = s1s2 ... s|s |a finite sequence over E (i.e.
si E E,1 &lt; i &lt; |s|). Let i = [i1, i2, ... , in], with
1 S i1 &lt; i2 &lt; ... &lt; in S |s|, be a subset of the
indices in s: we will denote as s[i] E En the sub-
sequence si1si2 ... sin. Note that s[i] does not nec-
essarily form a contiguous subsequence of s. For
example, if s is the sequence “Ronaldo scored the
goal” and i = [2, 4], then s[i] is “scored goal”. The
length spanned by s[i] in s is l(i) = in − i1 + 1.
The feature space associated with the Gap-Weighted
Subsequences Kernel of length n is indexed by I =
En, with the embedding given by
</bodyText>
<equation confidence="0.991832">
φnu(s) = E λl(i), u E Σn, (1)
i:u=s[i]
</equation>
<bodyText confidence="0.999173333333333">
where λ E]0,1] is the decay factor used to penalize
non-contiguous subsequences&apos;. The associate ker-
nel is defined as
</bodyText>
<equation confidence="0.9660965">
Kn(s,t) = (φn(s),φn(t)) = E φnu(s)φnu(t). (2)
uErn
</equation>
<bodyText confidence="0.999749142857143">
An explicit computation of Equation 2 is unfea-
sible even for small values of n. To evaluate more
efficiently Kn, we use the recursive formulation pro-
posed in (Lodhi et al., 2002; Saunders et al., 2002;
Cancedda et al., 2003) based on a dynamic program-
ming implementation. It is reported in the following
equations:
</bodyText>
<equation confidence="0.999475076923077">
Kl0(s, t) = 1, `ds, t, (3)
Kli(s,t) = 0, if min(|s|, |t|) &lt; i, (4)
Kill (s, t) = 0, if min(|s|, |t|) &lt; i, (5)
Kill (sx, ty) = (λKill(sx,t), if x # y;
λKll
i (sx, t) + λ2Kli_1(s, t), otherwise.
(6)
Kli(sx, t) = λKli(s, t) + Kll
i (sx, t), (7)
Kn(s,t) = 0, if min(|s|, |t|) &lt; n, (8)
Kn(sx, t) = Kn(s, t) + E λ2Kln_1(s,t[1 : j − 1]),
j:tj =x
(9)
</equation>
<bodyText confidence="0.997642666666667">
Kn and Kn are auxiliary functions with a sim-
ilar definition as Kn used to facilitate the compu-
tation. Based on all definitions above, Kn can be
&apos;Notice that by choosing λ = 1 sparse subsequences are
not penalized. On the other hand, the kernel does not take into
account sparse subsequences with λ --+ 0.
computed in O(n|s||t|). Using the above recursive
definition, it turns out that computing all kernel val-
ues for subsequences of lengths up to n is not signif-
icantly more costly than computing the kernel for n
only.
In the rest of the paper we will use the normalised
version of the kernel (Equation 10) to keep the val-
ues comparable for different values of n and to be
independent from the length of the sequences.
</bodyText>
<equation confidence="0.9549475">
ˆK(s, t) = ✓ K(S, S)K(t, t) (10)
K(s, t)
</equation>
<sectionHeader confidence="0.734708" genericHeader="method">
3 The Syntagmatic Kernel
</sectionHeader>
<bodyText confidence="0.999812">
As stated in Section 1, syntagmatic relations hold
among words arranged in a particular temporal or-
der, hence they can be modeled by Sequence Ker-
nels. The Syntagmatic Kernel is defined as a linear
combination of Gap-Weighted Subsequences Ker-
nels that operate at word and PoS tag level. In partic-
ular, following the approach proposed by Cancedda
et al. (2003), it is possible to adapt sequence kernels
to operate at word level by instancing the alphabet E
with the vocabulary V = {w1, w2,... , wk}. More-
over, we restricted the generic definition of the Gap-
Weighted Subsequences Kernel to recognize collo-
cations in the local context of a specified word. The
resulting kernel, called n-gram Collocation Kernel
(KnColl), operates on sequences of lemmata around a
specified word l0 (i.e. l−3, l−2, l−1, l0, l+1, l+2, l+3).
This formulation allows us to estimate the number of
common (sparse) subsequences of lemmata (i.e. col-
locations) between two examples, in order to capture
syntagmatic similarity.
Analogously, we defined the PoS Kernel (KnPoS)
to operate on sequences of PoS tags p−3, p−2, p−1,
p0, p+1, p+2, p+3, where p0 is the PoS tag of l0.
The Collocation Kernel and the PoS Kernel are
defined by Equations 11 and 12, respectively.
</bodyText>
<equation confidence="0.957922857142857">
n
KColl(s, t) = KlColl(s, t) (11)
l=1
and
n
KPoS(s, t) = KlPoS(s, t). (12)
l=1
</equation>
<bodyText confidence="0.997923">
Both kernels depend on the parameter n, the length
of the non-contiguous subsequences, and λ, the de-
</bodyText>
<page confidence="0.994731">
59
</page>
<bodyText confidence="0.9848415">
cay factor. For example, K2Coll allows us to repre-
sent all (sparse) bi-grams in the local context of a
word.
Finally, the Syntagmatic Kernel is defined as
</bodyText>
<equation confidence="0.841778">
KSynt(s, t) = KColl(s, t) + KPoS(s, t). (13)
</equation>
<bodyText confidence="0.999882">
We will show that in WSD, the Syntagmatic Ker-
nel is more effective than standard bigrams and tri-
grams of lemmata and PoS tags typically used as
features.
</bodyText>
<sectionHeader confidence="0.996102" genericHeader="method">
4 Soft-Matching Criteria
</sectionHeader>
<bodyText confidence="0.999987380952381">
In the definition of the Syntagmatic Kernel only ex-
act word matches contribute to the similarity. To
overcome this problem, we further extended the def-
inition of the Gap-Weigthed Subsequences Kernel
given in Section 2 to allow soft-matching between
words. In order to develop soft-matching criteria,
we follow the idea that two words can be substi-
tuted preserving the meaning of the whole sentence
if they are paradigmatically related (e.g. synomyns,
hyponyms or domain related words). If the meaning
of the proposition as a whole is preserved, the mean-
ing of the lexical constituents of the sentence will
necessarily remain unchanged too, providing a vi-
able criterion to define a soft-matching schema. This
can be implemented by “plugging” external paradig-
matic information into the Collocation kernel.
Following the approach proposed by (Shawe-
Taylor and Cristianini, 2004), the soft-matching
Gap-Weighted Subsequences Kernel is now calcu-
lated recursively using Equations 3 to 5, 7 and 8,
replacing Equation 6 by the equation:
</bodyText>
<equation confidence="0.929126571428571">
K00
i (sx, ty) = AK00
i (sx, t) + A2axyK0i−1(s, t), ∀x, y, (14)
and modifying Equation 9 to:
Kn(sx, t) = Kn(s, t) + X |t |A2axtj K0n−1(s,t[1 : j − 1]).
j
(15)
</equation>
<bodyText confidence="0.999963555555556">
where axy are entries in a similarity matrix A be-
tween symbols (words). In order to ensure that the
resulting kernel is valid, A must be positive semi-
definite.
In the following subsections, we describe two al-
ternative soft-matching criteria based on WordNet
Synonymy and Domain Proximity. In both cases, to
show that the similarity matrices are a positive semi-
definite we use the following result:
</bodyText>
<construct confidence="0.7696835">
Proposition 1 A matrix A is positive semi-definite
if and only if A = BTB for some real matrix B.
</construct>
<bodyText confidence="0.9898765">
The proof is given in (Shawe-Taylor and Cristianini,
2004).
</bodyText>
<subsectionHeader confidence="0.997776">
4.1 WordNet Synonymy
</subsectionHeader>
<bodyText confidence="0.999664045454546">
The first solution we have experimented exploits a
lexical resource representing paradigmatic relations
among terms, i.e. WordNet. In particular, we used
WordNet-1.7.1 for English and the Italian part of
MultiWordNet2.
In order to find a similarity matrix between terms,
we defined a vector space where terms are repre-
sented by the WordNet synsets in which such terms
appear. Hence, we can view a term as vector in
which each dimension is associated with one synset.
The term-by-synset matrix S is then the matrix
whose rows are indexed by the synsets. The en-
try xij of S is 1 if the synset sj contains the term
wi, and 0 otherwise. The term-by-synset matrix S
gives rise to the similarity matrix A = SST be-
tween terms. Since A can be rewritten as A =
(ST )T ST = BTB, it follows directly by Proposi-
tion 1 that it is positive semi-definite.
It is straightforward to extend the soft-matching
criterion to include hyponym relation, but we
achieved worse results. In the evaluation section we
will not report such results.
</bodyText>
<subsectionHeader confidence="0.975803">
4.2 Domain Proximity
</subsectionHeader>
<bodyText confidence="0.999661769230769">
The approach described above requires a large scale
lexical resource. Unfortunately, for many languages,
such a resource is not available. Another possibility
for implementing soft-matching is introducing the
notion of Semantic Domains.
Semantic Domains are groups of strongly
paradigmatically related words, and can be acquired
automatically from corpora in a totally unsuper-
vised way (Gliozzo, 2005). Our proposal is to ex-
ploit a Domain Proximity relation to define a soft-
matching criterion on the basis of an unsupervised
similarity metric defined in a Domain Space. The
Domain Space can be determined once a Domain
</bodyText>
<footnote confidence="0.956216">
2http://multiwordnet.itc.it
</footnote>
<page confidence="0.998665">
60
</page>
<bodyText confidence="0.990434666666667">
Model (DM) is available. This solution is evidently
cheaper, because large collections of unlabeled texts
can be easily found for every language.
A DM is represented by a k x k&apos; rectangular ma-
trix D, containing the domain relevance for each
term with respect to each domain, as illustrated in
</bodyText>
<tableCaption confidence="0.868175">
Table 1. DMs can be acquired from texts by exploit-
</tableCaption>
<table confidence="0.9954392">
MEDICINE COMPUTER SCIENCE
HIV 1 0
AIDS 1 0
virus 0.5 0.5
laptop 0 1
</table>
<tableCaption confidence="0.999935">
Table 1: Example of Domain Model.
</tableCaption>
<bodyText confidence="0.98990078125">
ing a lexical coherence assumption (Gliozzo, 2005).
To this aim, Term Clustering algorithms can be used:
a different domain is defined for each cluster, and
the degree of association between terms and clusters,
estimated by the unsupervised learning algorithm,
provides a domain relevance function. As a clus-
tering technique we exploit Latent Semantic Analy-
sis (LSA), following the methodology described in
(Gliozzo et al., 2005b). This operation is done off-
line, and can be efficiently performed on large cor-
pora.
LSA is performed by means of SVD of the term-
by-document matrix T representing the corpus. The
SVD algorithm can be exploited to acquire a domain
matrix D from a large corpus in a totally unsuper-
vised way. SVD decomposes the term-by-document
matrix T into three matrices T = VEkUT where
Ek is the diagonal k x k matrix containing the k
singular values of T. D = VEk&apos; where k&apos; « k.
Once a DM has been defined by the matrix D, the
Domain Space is a k&apos; dimensional space, in which
both texts and terms are represented by means of
Domain Vectors (DVs), i.e. vectors representing the
domain relevances among the linguistic object and
�
each domain. The DV w&apos;i for the term wi E V is the
ith row of D, where V = {w1, w2, ... , wk} is the
vocabulary of the corpus.
The term-by-domain matrix D gives rise to the
term-by-term similarity matrix A = DDT among
terms. It follows from Proposition 1 that A is posi-
tive semi-definite.
</bodyText>
<sectionHeader confidence="0.954958" genericHeader="method">
5 Kernel Combination for WSD
</sectionHeader>
<bodyText confidence="0.99996703125">
To improve the performance of a WSD system, it
is possible to combine different kernels. Indeed,
we followed this approach in the participation to
Senseval-3 competition, reaching the state-of-the-
art in many lexical-sample tasks (Strapparava et al.,
2004). While this paper is focused on Syntagmatic
Kernels, in this section we would like to spend some
words on another important component for a com-
plete WSD system: the Domain Kernel, used to
model domain relations.
Syntagmatic information alone is not sufficient to
define a full kernel for WSD. In fact, in (Magnini
et al., 2002), it has been claimed that knowing the
domain of the text in which the word is located is a
crucial information for WSD. For example the (do-
main) polysemy among the COMPUTER SCIENCE
and the MEDICINE senses of the word virus can
be solved by simply considering the domain of the
context in which it is located.
This fundamental aspect of lexical polysemy can
be modeled by defining a kernel function to esti-
mate the domain similarity among the contexts of
the words to be disambiguated, namely the Domain
Kernel. The Domain Kernel measures the similarity
among the topics (domains) of two texts, so to cap-
ture domain aspects of sense distinction. It is a vari-
ation of the Latent Semantic Kernel (Shawe-Taylor
and Cristianini, 2004), in which a DM is exploited
to define an explicit mapping D : Rk → Rk&apos; from
the Vector Space Model (Salton and McGill, 1983)
into the Domain Space (see Section 4), defined by
the following mapping:
</bodyText>
<equation confidence="0.907805">
D(tj) = tj(IIDFD) = �t&apos; (16)
j
</equation>
<bodyText confidence="0.99511725">
where IIDF is a k x k diagonal matrix such that
izDF i,i= IDF(wi), t� is represented as a row vector,
and IDF(wi) is the Inverse Document Frequency of
wi. The Domain Kernel is then defined by:
</bodyText>
<equation confidence="0.936678">
KD(ti, tj) = 1/(D(tj), D(tj))(D(ti), D(ti))
(D(ti),D(tj))
(17)
</equation>
<bodyText confidence="0.999939">
The final system for WSD results from a com-
bination of kernels that deal with syntagmatic and
paradigmatic aspects (i.e. PoS, collocations, bag of
words, domains), according to the following kernel
</bodyText>
<page confidence="0.998054">
61
</page>
<table confidence="0.992020333333333">
combination schema: Standard approach
English Italian
Bigrams and trigrams 67.3 51.0
Syntagmatic Kernel
Hard matching 67.7 51.9
Soft matching (WordNet) 67.3 51.3
Soft matching (Domain proximity) 68.5 54.0
KC(xi, xj) = n Kl(xi, xj)
l=1 �Kl(xj, xj)Kl(xi, xi) (18)
</table>
<sectionHeader confidence="0.91747" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999894">
In this section we evaluate the Syntagmatic Kernel,
showing that it improves over the standard feature
extraction technique based on bigrams and trigrams
of words and PoS tags.
</bodyText>
<subsectionHeader confidence="0.989109">
6.1 Experimental settings
</subsectionHeader>
<bodyText confidence="0.999672235294118">
We conducted the experiments on two lexical sam-
ple tasks (English and Italian) of the Senseval-3
competition (Mihalcea and Edmonds, 2004). In
lexical-sample WSD, after selecting some target
words, training data is provided as a set of texts.
For each text a given target word is manually anno-
tated with a sense from a predetermined set of pos-
sibilities. Table 2 describes the tasks by reporting
the number of words to be disambiguated, the mean
polysemy, and the dimension of training, test and
unlabeled corpora. Note that the organizers of the
English task did not provide any unlabeled material.
So for English we used a domain model built from
the training partition of the task (obviously skipping
the sense annotation), while for Italian we acquired
the DM from the unlabeled corpus made available
by the organizers.
</bodyText>
<table confidence="0.837254666666667">
#w pol # train # test # unlab
English 57 6.47 7860 3944 7860
Italian 45 6.30 5145 2439 74788
</table>
<tableCaption confidence="0.997965">
Table 2: Dataset descriptions.
</tableCaption>
<subsectionHeader confidence="0.9991">
6.2 Performance of the Syntagmatic Kernel
</subsectionHeader>
<bodyText confidence="0.967011629629629">
Table 3 shows the performance of the Syntagmatic
Kernel on both data sets. As baseline, we report
the result of a standard approach consisting on ex-
plicit bigrams and trigrams of words and PoS tags
around the words to be disambiguated (Yarowsky,
1994). The results show that the Syntagmatic Ker-
nel outperforms the baseline in any configuration
(hard/soft-matching). The soft-matching criteria
further improve the classification performance. It
is interesting to note that the Domain Proximity
methodology obtained better results than WordNet
Table 3: Performance (F1) of the Syntagmatic Ker-
nel.
Synonymy. The different results observed between
Italian and English using the Domain Proximity
soft-matching criterion are probably due to the small
size of the unlabeled English corpus.
In these experiments, the parameters n and A are
optimized by cross-validation. For KnColl, we ob-
tained the best results with n = 2 and A = 0.5. For
KnPoS, n = 3 and A → 0. The domain cardinality k&apos;
was set to 50.
Finally, the global performance (F1) of the full
WSD system (see Section 5) on English and Italian
lexical sample tasks is 73.3 for English and 61.3 for
Italian. To our knowledge, these figures represent
the current state-of-the-art on these tasks.
</bodyText>
<sectionHeader confidence="0.988419" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.997325">
In this paper we presented the Syntagmatic Kernels,
i.e. a set of kernel functions that can be used to
model syntagmatic relations for a wide variety of
Natural Language Processing tasks. In addition, we
proposed two soft-matching criteria for the sequence
analysis, which can be easily modeled by relax-
ing the constraints in a Gap-Weighted Subsequences
Kernel applied to local contexts of the word to be
analyzed. Experiments, performed on two lexical
sample Word Sense Disambiguation benchmarks,
show that our approach further improves the stan-
dard techniques usually adopted to deal with syntag-
matic relations. In addition, the Domain Proximity
soft-matching criterion allows us to define a semi-
supervised learning schema, improving the overall
results.
For the future, we plan to exploit the Syntagmatic
Kernel for a wide variety of Natural Language Pro-
cessing tasks, such as Entity Recognition and Re-
lation Extraction. In addition we are applying the
soft matching criteria here defined to Tree Kernels,
</bodyText>
<page confidence="0.997044">
62
</page>
<bodyText confidence="0.99995975">
in order to take into account lexical variability in
parse trees. Finally, we are going to further improve
the soft-matching criteria here proposed by explor-
ing the use of entailment criteria for substitutability.
</bodyText>
<sectionHeader confidence="0.996557" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999945666666667">
The authors were partially supported by the Onto-
Text Project, funded by the Autonomous Province
of Trento under the FUP-2004 research program.
</bodyText>
<sectionHeader confidence="0.999186" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999939428571429">
N. Cancedda, E. Gaussier, C. Goutte, and J.M. Renders.
2003. Word-sequence kernels. Journal of Machine
Learning Research, 32(6):1059–1082.
N. Cristianini and J. Shawe-Taylor. 2000. An introduc-
tion to Support Vector Machines. Cambridge Univer-
sity Press.
A. Gliozzo, C. Giuliano, and R. Rinaldi. 2005a. Instance
filtering for entity recognition. ACM SIGKDD Explo-
rations, special Issue on Natural Language Processing
and Text Mining, 7(1):11–18, June.
A. Gliozzo, C. Giuliano, and C. Strapparava. 2005b. Do-
main kernels for word sense disambiguation. In Pro-
ceedings of the 43rd annual meeting of the Association
for Computational Linguistics (ACL-05), pages 403–
410, Ann Arbor, Michigan, June.
A. Gliozzo. 2005. Semantic Domains in Computa-
tional Linguistics. Ph.D. thesis, ITC-irst/University of
Trento.
H. Lodhi, J. Shawe-Taylor, N. Cristianini, and
C. Watkins. 2002. Text classification using string
kernels. Journal of Machine Learning Research,
2(3):419–444.
B. Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo.
2002. The role of domain information in word
sense disambiguation. Natural Language Engineer-
ing, 8(4):359–373.
R. Mihalcea and P. Edmonds, editors. 2004. Proceed-
ings of SENSEVAL-3: Third International Workshop
on the Evaluation of Systems for the Semantic Analy-
sis of Text, Barcelona, Spain, July.
G. Salton and M.H. McGill. 1983. Introduction to mod-
ern information retrieval. McGraw-Hill, New York.
C. Saunders, H. Tschach, and J. Shawe-Taylor. 2002.
Syllables and other string kernel extensions. In Pro-
ceedings of 19th International Conference on Machine
Learning (ICML02).
J. Shawe-Taylor and N. Cristianini. 2004. Kernel Meth-
ods for Pattern Analysis. Cambridge University Press.
C. Strapparava, C. Giuliano, and A. Gliozzo. 2004. Pat-
tern abstraction and term similarity for word sense dis-
ambiguation: IRST at Senseval-3. In Proceedings of
SENSEVAL-3: Third International Workshop on the
Evaluation of Systems for the Semantic Analysis of
Text, Barcelona, Spain, July.
D. Yarowsky. 1994. Decision lists for lexical ambiguity
resolution: Application to accent restoration in span-
ish and french. In Proceedings of the 32nd Annual
Meeting of the ACL, pages 88–95, Las Cruces, New
Mexico.
</reference>
<page confidence="0.999463">
63
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.491452">
<title confidence="0.998707">Syntagmatic Kernels:</title>
<author confidence="0.8171755">a Word Sense Disambiguation Case Study Giuliano Gliozzo</author>
<affiliation confidence="0.987963">ITC-irst, Istituto per la Ricerca Scientifica e</affiliation>
<address confidence="0.974119">I-38050, Trento,</address>
<abstract confidence="0.992488777777778">In this paper we present a family of kernel functions, named Syntagmatic Kernels, which can be used to model syntagmatic relations. Syntagmatic relations hold among words that are typically collocated in a sequential order, and thus they can be acquired by analyzing word sequences. In particular, Syntagmatic Kernels are defined by applying a Word Sequence Kernel to the local contexts of the words to be analyzed. In addition, this approach allows us to define a semi supervised learning schema where external lexical knowledge is plugged into the supervised learning process. Lexical knowledge is acquired from both unlabeled data and hand-made lexical resources, such as WordNet. We evaluated the syntagmatic kernel on two standard Word Sense Disambiguation tasks (i.e. English and Italian lexical-sample tasks of Senseval-3), where the syntagmatic information plays a crucial role. We compared the Syntagmatic Kernel with the standard approach, showing promising improvements in performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Cancedda</author>
<author>E Gaussier</author>
<author>C Goutte</author>
<author>J M Renders</author>
</authors>
<title>Word-sequence kernels.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>32</volume>
<issue>6</issue>
<contexts>
<context position="8710" citStr="Cancedda et al., 2003" startWordPosition="1442" endWordPosition="1445">”. The length spanned by s[i] in s is l(i) = in − i1 + 1. The feature space associated with the Gap-Weighted Subsequences Kernel of length n is indexed by I = En, with the embedding given by φnu(s) = E λl(i), u E Σn, (1) i:u=s[i] where λ E]0,1] is the decay factor used to penalize non-contiguous subsequences&apos;. The associate kernel is defined as Kn(s,t) = (φn(s),φn(t)) = E φnu(s)φnu(t). (2) uErn An explicit computation of Equation 2 is unfeasible even for small values of n. To evaluate more efficiently Kn, we use the recursive formulation proposed in (Lodhi et al., 2002; Saunders et al., 2002; Cancedda et al., 2003) based on a dynamic programming implementation. It is reported in the following equations: Kl0(s, t) = 1, `ds, t, (3) Kli(s,t) = 0, if min(|s|, |t|) &lt; i, (4) Kill (s, t) = 0, if min(|s|, |t|) &lt; i, (5) Kill (sx, ty) = (λKill(sx,t), if x # y; λKll i (sx, t) + λ2Kli_1(s, t), otherwise. (6) Kli(sx, t) = λKli(s, t) + Kll i (sx, t), (7) Kn(s,t) = 0, if min(|s|, |t|) &lt; n, (8) Kn(sx, t) = Kn(s, t) + E λ2Kln_1(s,t[1 : j − 1]), j:tj =x (9) Kn and Kn are auxiliary functions with a similar definition as Kn used to facilitate the computation. Based on all definitions above, Kn can be &apos;Notice that by choosi</context>
<context position="10288" citStr="Cancedda et al. (2003)" startWordPosition="1741" endWordPosition="1744"> the rest of the paper we will use the normalised version of the kernel (Equation 10) to keep the values comparable for different values of n and to be independent from the length of the sequences. ˆK(s, t) = ✓ K(S, S)K(t, t) (10) K(s, t) 3 The Syntagmatic Kernel As stated in Section 1, syntagmatic relations hold among words arranged in a particular temporal order, hence they can be modeled by Sequence Kernels. The Syntagmatic Kernel is defined as a linear combination of Gap-Weighted Subsequences Kernels that operate at word and PoS tag level. In particular, following the approach proposed by Cancedda et al. (2003), it is possible to adapt sequence kernels to operate at word level by instancing the alphabet E with the vocabulary V = {w1, w2,... , wk}. Moreover, we restricted the generic definition of the GapWeighted Subsequences Kernel to recognize collocations in the local context of a specified word. The resulting kernel, called n-gram Collocation Kernel (KnColl), operates on sequences of lemmata around a specified word l0 (i.e. l−3, l−2, l−1, l0, l+1, l+2, l+3). This formulation allows us to estimate the number of common (sparse) subsequences of lemmata (i.e. collocations) between two examples, in or</context>
</contexts>
<marker>Cancedda, Gaussier, Goutte, Renders, 2003</marker>
<rawString>N. Cancedda, E. Gaussier, C. Goutte, and J.M. Renders. 2003. Word-sequence kernels. Journal of Machine Learning Research, 32(6):1059–1082.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Cristianini</author>
<author>J Shawe-Taylor</author>
</authors>
<title>An introduction to Support Vector Machines.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="7083" citStr="Cristianini and Shawe-Taylor, 2000" startWordPosition="1137" endWordPosition="1140">s. Instead of using the explicit mapping 0, we can use a kernel function K : X x X → R, that corresponds to the inner product in a feature space which is, in general, different from the input space. Kernel methods allow us to build a modular system, as the kernel function acts as an interface between the data and the learning algorithm. Thus the kernel function becomes the only domain specific module of the system, while the learning algorithm is a general purpose component. Potentially any kernel function can work with any kernel-based algorithm. In our system we use Support Vector Machines (Cristianini and Shawe-Taylor, 2000). Sequence Kernels (or String Kernels) are a family of kernel functions developed to compute the inner product among images of strings in highdimensional feature space using dynamic programming techniques (Shawe-Taylor and Cristianini, 2004). The Gap-Weighted Subsequences Kernel is the most general Sequence Kernel. Roughly speaking, it compares two strings by means of the number of contiguous and non-contiguous substrings of a given length they have in common. Non contiguous occurrences are penalized according to the number of gaps they contain. 58 Formally, let E be an alphabet of |E |symbols</context>
</contexts>
<marker>Cristianini, Shawe-Taylor, 2000</marker>
<rawString>N. Cristianini and J. Shawe-Taylor. 2000. An introduction to Support Vector Machines. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gliozzo</author>
<author>C Giuliano</author>
<author>R Rinaldi</author>
</authors>
<title>Instance filtering for entity recognition.</title>
<date>2005</date>
<journal>ACM SIGKDD Explorations, special Issue on Natural Language Processing and Text Mining,</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="1681" citStr="Gliozzo et al., 2005" startWordPosition="253" endWordPosition="256">Senseval-3), where the syntagmatic information plays a crucial role. We compared the Syntagmatic Kernel with the standard approach, showing promising improvements in performance. 1 Introduction In computational linguistics, it is usual to deal with sequences: words are sequences of letters and syntagmatic relations are established by sequences of words. Sequences are analyzed to measure morphological similarity, to detect multiwords, to represent syntagmatic relations, and so on. Hence modeling syntagmatic relations is crucial for a wide variety of NLP tasks, such as Named Entity Recognition (Gliozzo et al., 2005a) and Word Sense Disambiguation (WSD) (Strapparava et al., 2004). In general, the strategy adopted to model syntagmatic relations is to provide bigrams and trigrams of collocated words as features to describe local contexts (Yarowsky, 1994), and each word is regarded as a different instance to classify. For instance, occurrences of a given class of named entities (such as names of persons) can be discriminated in texts by recognizing word patterns in their local contexts. For example the token Rossi, whenever is preceded by the token Prof., often represents the name of a person. Another task </context>
<context position="16056" citStr="Gliozzo et al., 2005" startWordPosition="2706" endWordPosition="2709">spect to each domain, as illustrated in Table 1. DMs can be acquired from texts by exploitMEDICINE COMPUTER SCIENCE HIV 1 0 AIDS 1 0 virus 0.5 0.5 laptop 0 1 Table 1: Example of Domain Model. ing a lexical coherence assumption (Gliozzo, 2005). To this aim, Term Clustering algorithms can be used: a different domain is defined for each cluster, and the degree of association between terms and clusters, estimated by the unsupervised learning algorithm, provides a domain relevance function. As a clustering technique we exploit Latent Semantic Analysis (LSA), following the methodology described in (Gliozzo et al., 2005b). This operation is done offline, and can be efficiently performed on large corpora. LSA is performed by means of SVD of the termby-document matrix T representing the corpus. The SVD algorithm can be exploited to acquire a domain matrix D from a large corpus in a totally unsupervised way. SVD decomposes the term-by-document matrix T into three matrices T = VEkUT where Ek is the diagonal k x k matrix containing the k singular values of T. D = VEk&apos; where k&apos; « k. Once a DM has been defined by the matrix D, the Domain Space is a k&apos; dimensional space, in which both texts and terms are represented</context>
</contexts>
<marker>Gliozzo, Giuliano, Rinaldi, 2005</marker>
<rawString>A. Gliozzo, C. Giuliano, and R. Rinaldi. 2005a. Instance filtering for entity recognition. ACM SIGKDD Explorations, special Issue on Natural Language Processing and Text Mining, 7(1):11–18, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gliozzo</author>
<author>C Giuliano</author>
<author>C Strapparava</author>
</authors>
<title>Domain kernels for word sense disambiguation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd annual meeting of the Association for Computational Linguistics (ACL-05),</booktitle>
<pages>403--410</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1681" citStr="Gliozzo et al., 2005" startWordPosition="253" endWordPosition="256">Senseval-3), where the syntagmatic information plays a crucial role. We compared the Syntagmatic Kernel with the standard approach, showing promising improvements in performance. 1 Introduction In computational linguistics, it is usual to deal with sequences: words are sequences of letters and syntagmatic relations are established by sequences of words. Sequences are analyzed to measure morphological similarity, to detect multiwords, to represent syntagmatic relations, and so on. Hence modeling syntagmatic relations is crucial for a wide variety of NLP tasks, such as Named Entity Recognition (Gliozzo et al., 2005a) and Word Sense Disambiguation (WSD) (Strapparava et al., 2004). In general, the strategy adopted to model syntagmatic relations is to provide bigrams and trigrams of collocated words as features to describe local contexts (Yarowsky, 1994), and each word is regarded as a different instance to classify. For instance, occurrences of a given class of named entities (such as names of persons) can be discriminated in texts by recognizing word patterns in their local contexts. For example the token Rossi, whenever is preceded by the token Prof., often represents the name of a person. Another task </context>
<context position="16056" citStr="Gliozzo et al., 2005" startWordPosition="2706" endWordPosition="2709">spect to each domain, as illustrated in Table 1. DMs can be acquired from texts by exploitMEDICINE COMPUTER SCIENCE HIV 1 0 AIDS 1 0 virus 0.5 0.5 laptop 0 1 Table 1: Example of Domain Model. ing a lexical coherence assumption (Gliozzo, 2005). To this aim, Term Clustering algorithms can be used: a different domain is defined for each cluster, and the degree of association between terms and clusters, estimated by the unsupervised learning algorithm, provides a domain relevance function. As a clustering technique we exploit Latent Semantic Analysis (LSA), following the methodology described in (Gliozzo et al., 2005b). This operation is done offline, and can be efficiently performed on large corpora. LSA is performed by means of SVD of the termby-document matrix T representing the corpus. The SVD algorithm can be exploited to acquire a domain matrix D from a large corpus in a totally unsupervised way. SVD decomposes the term-by-document matrix T into three matrices T = VEkUT where Ek is the diagonal k x k matrix containing the k singular values of T. D = VEk&apos; where k&apos; « k. Once a DM has been defined by the matrix D, the Domain Space is a k&apos; dimensional space, in which both texts and terms are represented</context>
</contexts>
<marker>Gliozzo, Giuliano, Strapparava, 2005</marker>
<rawString>A. Gliozzo, C. Giuliano, and C. Strapparava. 2005b. Domain kernels for word sense disambiguation. In Proceedings of the 43rd annual meeting of the Association for Computational Linguistics (ACL-05), pages 403– 410, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gliozzo</author>
</authors>
<date>2005</date>
<booktitle>Semantic Domains in Computational Linguistics. Ph.D. thesis, ITC-irst/University of Trento.</booktitle>
<contexts>
<context position="14934" citStr="Gliozzo, 2005" startWordPosition="2522" endWordPosition="2523">mi-definite. It is straightforward to extend the soft-matching criterion to include hyponym relation, but we achieved worse results. In the evaluation section we will not report such results. 4.2 Domain Proximity The approach described above requires a large scale lexical resource. Unfortunately, for many languages, such a resource is not available. Another possibility for implementing soft-matching is introducing the notion of Semantic Domains. Semantic Domains are groups of strongly paradigmatically related words, and can be acquired automatically from corpora in a totally unsupervised way (Gliozzo, 2005). Our proposal is to exploit a Domain Proximity relation to define a softmatching criterion on the basis of an unsupervised similarity metric defined in a Domain Space. The Domain Space can be determined once a Domain 2http://multiwordnet.itc.it 60 Model (DM) is available. This solution is evidently cheaper, because large collections of unlabeled texts can be easily found for every language. A DM is represented by a k x k&apos; rectangular matrix D, containing the domain relevance for each term with respect to each domain, as illustrated in Table 1. DMs can be acquired from texts by exploitMEDICINE</context>
</contexts>
<marker>Gliozzo, 2005</marker>
<rawString>A. Gliozzo. 2005. Semantic Domains in Computational Linguistics. Ph.D. thesis, ITC-irst/University of Trento.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Lodhi</author>
<author>J Shawe-Taylor</author>
<author>N Cristianini</author>
<author>C Watkins</author>
</authors>
<title>Text classification using string kernels.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="8663" citStr="Lodhi et al., 2002" startWordPosition="1434" endWordPosition="1437">” and i = [2, 4], then s[i] is “scored goal”. The length spanned by s[i] in s is l(i) = in − i1 + 1. The feature space associated with the Gap-Weighted Subsequences Kernel of length n is indexed by I = En, with the embedding given by φnu(s) = E λl(i), u E Σn, (1) i:u=s[i] where λ E]0,1] is the decay factor used to penalize non-contiguous subsequences&apos;. The associate kernel is defined as Kn(s,t) = (φn(s),φn(t)) = E φnu(s)φnu(t). (2) uErn An explicit computation of Equation 2 is unfeasible even for small values of n. To evaluate more efficiently Kn, we use the recursive formulation proposed in (Lodhi et al., 2002; Saunders et al., 2002; Cancedda et al., 2003) based on a dynamic programming implementation. It is reported in the following equations: Kl0(s, t) = 1, `ds, t, (3) Kli(s,t) = 0, if min(|s|, |t|) &lt; i, (4) Kill (s, t) = 0, if min(|s|, |t|) &lt; i, (5) Kill (sx, ty) = (λKill(sx,t), if x # y; λKll i (sx, t) + λ2Kli_1(s, t), otherwise. (6) Kli(sx, t) = λKli(s, t) + Kll i (sx, t), (7) Kn(s,t) = 0, if min(|s|, |t|) &lt; n, (8) Kn(sx, t) = Kn(s, t) + E λ2Kln_1(s,t[1 : j − 1]), j:tj =x (9) Kn and Kn are auxiliary functions with a similar definition as Kn used to facilitate the computation. Based on all defi</context>
</contexts>
<marker>Lodhi, Shawe-Taylor, Cristianini, Watkins, 2002</marker>
<rawString>H. Lodhi, J. Shawe-Taylor, N. Cristianini, and C. Watkins. 2002. Text classification using string kernels. Journal of Machine Learning Research, 2(3):419–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Magnini</author>
<author>C Strapparava</author>
<author>G Pezzulo</author>
<author>A Gliozzo</author>
</authors>
<title>The role of domain information in word sense disambiguation.</title>
<date>2002</date>
<journal>Natural Language Engineering,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="17674" citStr="Magnini et al., 2002" startWordPosition="2995" endWordPosition="2998">efinite. 5 Kernel Combination for WSD To improve the performance of a WSD system, it is possible to combine different kernels. Indeed, we followed this approach in the participation to Senseval-3 competition, reaching the state-of-theart in many lexical-sample tasks (Strapparava et al., 2004). While this paper is focused on Syntagmatic Kernels, in this section we would like to spend some words on another important component for a complete WSD system: the Domain Kernel, used to model domain relations. Syntagmatic information alone is not sufficient to define a full kernel for WSD. In fact, in (Magnini et al., 2002), it has been claimed that knowing the domain of the text in which the word is located is a crucial information for WSD. For example the (domain) polysemy among the COMPUTER SCIENCE and the MEDICINE senses of the word virus can be solved by simply considering the domain of the context in which it is located. This fundamental aspect of lexical polysemy can be modeled by defining a kernel function to estimate the domain similarity among the contexts of the words to be disambiguated, namely the Domain Kernel. The Domain Kernel measures the similarity among the topics (domains) of two texts, so to</context>
</contexts>
<marker>Magnini, Strapparava, Pezzulo, Gliozzo, 2002</marker>
<rawString>B. Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo. 2002. The role of domain information in word sense disambiguation. Natural Language Engineering, 8(4):359–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>P Edmonds</author>
<author>editors</author>
</authors>
<date>2004</date>
<booktitle>Proceedings of SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<location>Barcelona, Spain,</location>
<marker>Mihalcea, Edmonds, editors, 2004</marker>
<rawString>R. Mihalcea and P. Edmonds, editors. 2004. Proceedings of SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M H McGill</author>
</authors>
<title>Introduction to modern information retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill,</publisher>
<location>New York.</location>
<contexts>
<context position="18529" citStr="Salton and McGill, 1983" startWordPosition="3146" endWordPosition="3149">solved by simply considering the domain of the context in which it is located. This fundamental aspect of lexical polysemy can be modeled by defining a kernel function to estimate the domain similarity among the contexts of the words to be disambiguated, namely the Domain Kernel. The Domain Kernel measures the similarity among the topics (domains) of two texts, so to capture domain aspects of sense distinction. It is a variation of the Latent Semantic Kernel (Shawe-Taylor and Cristianini, 2004), in which a DM is exploited to define an explicit mapping D : Rk → Rk&apos; from the Vector Space Model (Salton and McGill, 1983) into the Domain Space (see Section 4), defined by the following mapping: D(tj) = tj(IIDFD) = �t&apos; (16) j where IIDF is a k x k diagonal matrix such that izDF i,i= IDF(wi), t� is represented as a row vector, and IDF(wi) is the Inverse Document Frequency of wi. The Domain Kernel is then defined by: KD(ti, tj) = 1/(D(tj), D(tj))(D(ti), D(ti)) (D(ti),D(tj)) (17) The final system for WSD results from a combination of kernels that deal with syntagmatic and paradigmatic aspects (i.e. PoS, collocations, bag of words, domains), according to the following kernel 61 combination schema: Standard approach </context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>G. Salton and M.H. McGill. 1983. Introduction to modern information retrieval. McGraw-Hill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Saunders</author>
<author>H Tschach</author>
<author>J Shawe-Taylor</author>
</authors>
<title>Syllables and other string kernel extensions.</title>
<date>2002</date>
<booktitle>In Proceedings of 19th International Conference on Machine Learning (ICML02).</booktitle>
<contexts>
<context position="8686" citStr="Saunders et al., 2002" startWordPosition="1438" endWordPosition="1441">en s[i] is “scored goal”. The length spanned by s[i] in s is l(i) = in − i1 + 1. The feature space associated with the Gap-Weighted Subsequences Kernel of length n is indexed by I = En, with the embedding given by φnu(s) = E λl(i), u E Σn, (1) i:u=s[i] where λ E]0,1] is the decay factor used to penalize non-contiguous subsequences&apos;. The associate kernel is defined as Kn(s,t) = (φn(s),φn(t)) = E φnu(s)φnu(t). (2) uErn An explicit computation of Equation 2 is unfeasible even for small values of n. To evaluate more efficiently Kn, we use the recursive formulation proposed in (Lodhi et al., 2002; Saunders et al., 2002; Cancedda et al., 2003) based on a dynamic programming implementation. It is reported in the following equations: Kl0(s, t) = 1, `ds, t, (3) Kli(s,t) = 0, if min(|s|, |t|) &lt; i, (4) Kill (s, t) = 0, if min(|s|, |t|) &lt; i, (5) Kill (sx, ty) = (λKill(sx,t), if x # y; λKll i (sx, t) + λ2Kli_1(s, t), otherwise. (6) Kli(sx, t) = λKli(s, t) + Kll i (sx, t), (7) Kn(s,t) = 0, if min(|s|, |t|) &lt; n, (8) Kn(sx, t) = Kn(s, t) + E λ2Kln_1(s,t[1 : j − 1]), j:tj =x (9) Kn and Kn are auxiliary functions with a similar definition as Kn used to facilitate the computation. Based on all definitions above, Kn can b</context>
</contexts>
<marker>Saunders, Tschach, Shawe-Taylor, 2002</marker>
<rawString>C. Saunders, H. Tschach, and J. Shawe-Taylor. 2002. Syllables and other string kernel extensions. In Proceedings of 19th International Conference on Machine Learning (ICML02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Shawe-Taylor</author>
<author>N Cristianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2004</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="3859" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="612" endWordPosition="616">e that the verb to score has to be disambiguated into the sentence “Ronaldo scored the goal”, and that the sense tagged example “the football player scores#1 the first goal” is provided for training. A traditional feature mapping would extract the bigram w+1 w+2:the goal to represent the former, and the bigram w+1 w+2:the first to index the latter. Evidently such features will not match, leading the algorithm to a misclassification. In the present paper we propose the Syntagmatic Kernel as an attempt to solve this problem. The Syntagmatic Kernel is based on a Gap-Weighted Subsequences Kernel (Shawe-Taylor and Cristianini, 2004). In the spirit of Kernel Methods, this kernel is able to compare sequences directly in the input space, avoiding any explicit feature mapping. To perform this operation, it counts how many times a (non-contiguous) subsequence of symbols u of length n occurs in the input string s, and penalizes non-contiguous occurrences according to the number of the contained gaps. To define our Syntagmatic Kernel, we adapted the generic definition of the Sequence Kernels to the problem of recognizing collocations in local word contexts. In the above definition of Syntagmatic Kernel, only exact word-matches </context>
<context position="7324" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="1173" endWordPosition="1176"> system, as the kernel function acts as an interface between the data and the learning algorithm. Thus the kernel function becomes the only domain specific module of the system, while the learning algorithm is a general purpose component. Potentially any kernel function can work with any kernel-based algorithm. In our system we use Support Vector Machines (Cristianini and Shawe-Taylor, 2000). Sequence Kernels (or String Kernels) are a family of kernel functions developed to compute the inner product among images of strings in highdimensional feature space using dynamic programming techniques (Shawe-Taylor and Cristianini, 2004). The Gap-Weighted Subsequences Kernel is the most general Sequence Kernel. Roughly speaking, it compares two strings by means of the number of contiguous and non-contiguous substrings of a given length they have in common. Non contiguous occurrences are penalized according to the number of gaps they contain. 58 Formally, let E be an alphabet of |E |symbols, and s = s1s2 ... s|s |a finite sequence over E (i.e. si E E,1 &lt; i &lt; |s|). Let i = [i1, i2, ... , in], with 1 S i1 &lt; i2 &lt; ... &lt; in S |s|, be a subset of the indices in s: we will denote as s[i] E En the subsequence si1si2 ... sin. Note that</context>
<context position="13469" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="2281" endWordPosition="2284">Equation 9 to: Kn(sx, t) = Kn(s, t) + X |t |A2axtj K0n−1(s,t[1 : j − 1]). j (15) where axy are entries in a similarity matrix A between symbols (words). In order to ensure that the resulting kernel is valid, A must be positive semidefinite. In the following subsections, we describe two alternative soft-matching criteria based on WordNet Synonymy and Domain Proximity. In both cases, to show that the similarity matrices are a positive semidefinite we use the following result: Proposition 1 A matrix A is positive semi-definite if and only if A = BTB for some real matrix B. The proof is given in (Shawe-Taylor and Cristianini, 2004). 4.1 WordNet Synonymy The first solution we have experimented exploits a lexical resource representing paradigmatic relations among terms, i.e. WordNet. In particular, we used WordNet-1.7.1 for English and the Italian part of MultiWordNet2. In order to find a similarity matrix between terms, we defined a vector space where terms are represented by the WordNet synsets in which such terms appear. Hence, we can view a term as vector in which each dimension is associated with one synset. The term-by-synset matrix S is then the matrix whose rows are indexed by the synsets. The entry xij of S is 1 </context>
<context position="18404" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="3121" endWordPosition="3124">cial information for WSD. For example the (domain) polysemy among the COMPUTER SCIENCE and the MEDICINE senses of the word virus can be solved by simply considering the domain of the context in which it is located. This fundamental aspect of lexical polysemy can be modeled by defining a kernel function to estimate the domain similarity among the contexts of the words to be disambiguated, namely the Domain Kernel. The Domain Kernel measures the similarity among the topics (domains) of two texts, so to capture domain aspects of sense distinction. It is a variation of the Latent Semantic Kernel (Shawe-Taylor and Cristianini, 2004), in which a DM is exploited to define an explicit mapping D : Rk → Rk&apos; from the Vector Space Model (Salton and McGill, 1983) into the Domain Space (see Section 4), defined by the following mapping: D(tj) = tj(IIDFD) = �t&apos; (16) j where IIDF is a k x k diagonal matrix such that izDF i,i= IDF(wi), t� is represented as a row vector, and IDF(wi) is the Inverse Document Frequency of wi. The Domain Kernel is then defined by: KD(ti, tj) = 1/(D(tj), D(tj))(D(ti), D(ti)) (D(ti),D(tj)) (17) The final system for WSD results from a combination of kernels that deal with syntagmatic and paradigmatic aspects</context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>J. Shawe-Taylor and N. Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Strapparava</author>
<author>C Giuliano</author>
<author>A Gliozzo</author>
</authors>
<title>Pattern abstraction and term similarity for word sense disambiguation: IRST at Senseval-3. In</title>
<date>2004</date>
<booktitle>Proceedings of SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<location>Barcelona, Spain,</location>
<contexts>
<context position="1746" citStr="Strapparava et al., 2004" startWordPosition="263" endWordPosition="266">al role. We compared the Syntagmatic Kernel with the standard approach, showing promising improvements in performance. 1 Introduction In computational linguistics, it is usual to deal with sequences: words are sequences of letters and syntagmatic relations are established by sequences of words. Sequences are analyzed to measure morphological similarity, to detect multiwords, to represent syntagmatic relations, and so on. Hence modeling syntagmatic relations is crucial for a wide variety of NLP tasks, such as Named Entity Recognition (Gliozzo et al., 2005a) and Word Sense Disambiguation (WSD) (Strapparava et al., 2004). In general, the strategy adopted to model syntagmatic relations is to provide bigrams and trigrams of collocated words as features to describe local contexts (Yarowsky, 1994), and each word is regarded as a different instance to classify. For instance, occurrences of a given class of named entities (such as names of persons) can be discriminated in texts by recognizing word patterns in their local contexts. For example the token Rossi, whenever is preceded by the token Prof., often represents the name of a person. Another task that can benefit from modeling this kind of relations is WSD. To </context>
<context position="17346" citStr="Strapparava et al., 2004" startWordPosition="2939" endWordPosition="2942">domain relevances among the linguistic object and � each domain. The DV w&apos;i for the term wi E V is the ith row of D, where V = {w1, w2, ... , wk} is the vocabulary of the corpus. The term-by-domain matrix D gives rise to the term-by-term similarity matrix A = DDT among terms. It follows from Proposition 1 that A is positive semi-definite. 5 Kernel Combination for WSD To improve the performance of a WSD system, it is possible to combine different kernels. Indeed, we followed this approach in the participation to Senseval-3 competition, reaching the state-of-theart in many lexical-sample tasks (Strapparava et al., 2004). While this paper is focused on Syntagmatic Kernels, in this section we would like to spend some words on another important component for a complete WSD system: the Domain Kernel, used to model domain relations. Syntagmatic information alone is not sufficient to define a full kernel for WSD. In fact, in (Magnini et al., 2002), it has been claimed that knowing the domain of the text in which the word is located is a crucial information for WSD. For example the (domain) polysemy among the COMPUTER SCIENCE and the MEDICINE senses of the word virus can be solved by simply considering the domain o</context>
</contexts>
<marker>Strapparava, Giuliano, Gliozzo, 2004</marker>
<rawString>C. Strapparava, C. Giuliano, and A. Gliozzo. 2004. Pattern abstraction and term similarity for word sense disambiguation: IRST at Senseval-3. In Proceedings of SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Decision lists for lexical ambiguity resolution: Application to accent restoration in spanish and french.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the ACL,</booktitle>
<pages>88--95</pages>
<location>Las Cruces, New Mexico.</location>
<contexts>
<context position="1922" citStr="Yarowsky, 1994" startWordPosition="293" endWordPosition="294">th sequences: words are sequences of letters and syntagmatic relations are established by sequences of words. Sequences are analyzed to measure morphological similarity, to detect multiwords, to represent syntagmatic relations, and so on. Hence modeling syntagmatic relations is crucial for a wide variety of NLP tasks, such as Named Entity Recognition (Gliozzo et al., 2005a) and Word Sense Disambiguation (WSD) (Strapparava et al., 2004). In general, the strategy adopted to model syntagmatic relations is to provide bigrams and trigrams of collocated words as features to describe local contexts (Yarowsky, 1994), and each word is regarded as a different instance to classify. For instance, occurrences of a given class of named entities (such as names of persons) can be discriminated in texts by recognizing word patterns in their local contexts. For example the token Rossi, whenever is preceded by the token Prof., often represents the name of a person. Another task that can benefit from modeling this kind of relations is WSD. To solve ambiguity it is necessary to analyze syntagmatic relations in the local context of the word to be disambiguated. In this paper we propose a kernel function that can be us</context>
<context position="20809" citStr="Yarowsky, 1994" startWordPosition="3525" endWordPosition="3526">omain model built from the training partition of the task (obviously skipping the sense annotation), while for Italian we acquired the DM from the unlabeled corpus made available by the organizers. #w pol # train # test # unlab English 57 6.47 7860 3944 7860 Italian 45 6.30 5145 2439 74788 Table 2: Dataset descriptions. 6.2 Performance of the Syntagmatic Kernel Table 3 shows the performance of the Syntagmatic Kernel on both data sets. As baseline, we report the result of a standard approach consisting on explicit bigrams and trigrams of words and PoS tags around the words to be disambiguated (Yarowsky, 1994). The results show that the Syntagmatic Kernel outperforms the baseline in any configuration (hard/soft-matching). The soft-matching criteria further improve the classification performance. It is interesting to note that the Domain Proximity methodology obtained better results than WordNet Table 3: Performance (F1) of the Syntagmatic Kernel. Synonymy. The different results observed between Italian and English using the Domain Proximity soft-matching criterion are probably due to the small size of the unlabeled English corpus. In these experiments, the parameters n and A are optimized by cross-</context>
</contexts>
<marker>Yarowsky, 1994</marker>
<rawString>D. Yarowsky. 1994. Decision lists for lexical ambiguity resolution: Application to accent restoration in spanish and french. In Proceedings of the 32nd Annual Meeting of the ACL, pages 88–95, Las Cruces, New Mexico.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>