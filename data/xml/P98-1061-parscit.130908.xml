<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.998182">
A structure-sharing parser for lexicalized grammars
</title>
<author confidence="0.996946">
Roger Evans
</author>
<affiliation confidence="0.9972175">
Information Technology Research Institute
University of Brighton
</affiliation>
<address confidence="0.995621">
Brighton, BN2 4GJ, UK
</address>
<email confidence="0.997493">
Roger.Evans@itri.brighton.ac.uk
</email>
<author confidence="0.977472">
David Weir
</author>
<affiliation confidence="0.9770295">
Cognitive and Computing Sciences
University of Sussex
</affiliation>
<address confidence="0.994647">
Brighton, BN1 9QH, UK
</address>
<email confidence="0.997834">
David.Weir@cogs.susx.ac.uk
</email>
<sectionHeader confidence="0.994259" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99983205882353">
In wide-coverage lexicalized grammars many of
the elementary structures have substructures in
common. This means that in conventional pars-
ing algorithms some of the computation associ-
ated with different structures is duplicated. In
this paper we describe a precompilation tech-
nique for such grammars which allows some of
this computation to be shared. In our approach
the elementary structures of the grammar are
transformed into finite state automata which
can be merged and minimised using standard al-
gorithms, and then parsed using an automaton-
based parser. We present algorithms for con-
structing automata from elementary structures,
merging and minimising them, and string recog-
nition and parse recovery with the resulting
grammar.
</bodyText>
<sectionHeader confidence="0.998785" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99863848">
It is well-known that fully lexicalised grammar
formalisms such as LTAG (Joshi and Schabes,
1991) are difficult to parse with efficiently. Each
word in the parser&apos;s input string introduces an
elementary tree into the parse table for each
of its possible readings, and there is often a
substantial overlap in structure between these
trees. A conventional parsing algorithm (Vijay-
Shanker and Joshi, 1985) views the trees as in-
dependent, and so is likely to duplicate the pro-
cessing of this common structure. Parsing could
be made more efficient (empirically if not for-
mally), if the shared structure could be identi-
fied and processed only once.
Recent work by Evans and Weir (1997) and
Chen and Vijay-Shanker (1997) addresses this
problem from two different perspectives. Evans
and Weir (1997) outline a technique for com-
piling LTAG grammars into automata which are
then merged to introduce some sharing of struc-
ture. Chen and Vijay-Shanker (1997) use un-
derspecified tree descriptions to represent sets
of trees during parsing. The present paper takes
the former approach, but extends our previous
work by:
</bodyText>
<listItem confidence="0.893385444444444">
• showing how merged automata can be min-
imised, so that they share as much struc-
ture as possible;
• showing that by precompiling additional
information, parsing can be broken down
into recognition followed by parse recovery;
• providing a formal treatment of the algo-
rithms for transforming and minimising the
grammar, recognition and parse recovery.
</listItem>
<bodyText confidence="0.999390166666667">
In the following sections we outline the basic
approach, and describe informally our improve-
ments to the previous account. We then give a
formal account of the optimisation process and
a possible parsing algorithm that makes use of
itl.
</bodyText>
<sectionHeader confidence="0.987191" genericHeader="method">
2 Automaton-based parsing
</sectionHeader>
<bodyText confidence="0.958106866666667">
Conventional LTAG parsers (Vijay-Shanker and
Joshi, 1985; Schabes and Joshi, 1988; Vijay-
Shanker and Weir, 1993) maintain a parse ta-
ble, a set of items corresponding to complete
and partial constituents. Parsing proceeds by
first seeding the table with items anchored on
the input string, and then repeatedly scanning
the table for parser actions. Parser actions
introduce new items into the table licensed by
one or more items already in the table. The
main types of parser actions are:
1. extending a constituent by incorporating
a complete subconstituent (on the left or
&apos;However, due to lack of space, no proofs and only
minimal informal descriptions are given in this paper.
</bodyText>
<page confidence="0.99471">
372
</page>
<bodyText confidence="0.474687">
right);
</bodyText>
<listItem confidence="0.99440475">
2. extending a constituent by adjoining a sur-
rounding complete auxiliary constituent;
3. predicting the span of the foot node of an
auxiliary constituent (to the left or right).
</listItem>
<bodyText confidence="0.996583153846154">
Parsing is complete when all possible parser ac-
tions have been executed.
In a completed parse table it is possible to
trace the sequence of items corresponding to the
recognition of an elementary tree from its lexi-
cal anchor upwards. Each item in the sequence
corresponds to a node in the tree (with the se-
quence as a whole corresponding to a complete
traversal of the tree), and each step corresponds
to the parser action that licensed the next item,
given the current one. From this perspective,
parser actions can be restated relative to the
items in such a sequence as:
</bodyText>
<listItem confidence="0.982479666666667">
1. substitute a complete subconstituent (on
the left or right);
2. adjoin a surrounding complete auxiliary
constituent;
3. predict the span of the tree&apos;s foot node (to
the left or right).
</listItem>
<bodyText confidence="0.999879291666666">
The recognition of the tree can thus be viewed
as the computation of a finite state automaton,
whose states correspond to a traversal of the
tree and whose input symbols are these rela-
tivised parser actions.
This perspective suggests a re-casting of the
conventional LTAG parser in terms of such au-
tomata2. For this automaton-based parser, the
grammar structures are not trees, but automata
corresponding to tree traversals whose inputs
are strings of relativised parser actions. Items
in the parse table reference automaton states
instead of tree addresses, and if the automa-
ton state is final, the item represents a complete
constituent. Parser actions arise as before, but
are executed by relativising them with respect
to the incomplete item participating in the ac-
tion, and passing this relativised parser action
as the next input symbol for the automaton ref-
erenced by that item. The resulting state of
that automaton is then used as the referent of
the newly licensed item.
On a first pass, this re-casting is exactly that: it
does nothing new or different from the original
</bodyText>
<footnote confidence="0.6805805">
2Evans and Weir (1997) provides a longer informal
introduction to this approach.
</footnote>
<bodyText confidence="0.73289">
parser on the original grammar. However there
are a number of subtle differences3:
</bodyText>
<listItem confidence="0.947762454545454">
• the automata are more abstract than the
trees: the only grammatical information
they contain are the input symbols and the
root node labels, indicating the category of
the constituent the automaton recognises;
• automata for several trees can be merged
together and optimised using standard
well-studied techniques, resulting in a sin-
gle automaton that recognises many trees
at once, sharing as many of the common
parser actions as possible.
</listItem>
<bodyText confidence="0.998969615384615">
It is this final point which is the focus of this
paper. By representing trees as automata, we
can merge trees together and apply standard
optimisation techniques to share their common
structure. The parser will remain unchanged,
but will operate more efficiently where struc-
ture has been shared. Additionally, because
the automata are more abstract than the trees,
capturing precisely the parser&apos;s view of the
trees, sharing may occur between trees which
are structurally quite different, but which hap-
pen to have common parser actions associated
with them.
</bodyText>
<sectionHeader confidence="0.916709" genericHeader="method">
3 Merging and minimising automata
</sectionHeader>
<bodyText confidence="0.886676608695652">
Combining the automata for several trees can
be achieved using a variety of standard algo-
rithms (Huffman, 1954; Moore, 1956). How-
ever any transformations must respect one im-
portant feature: once the parser reaches a fi-
nal state it needs to know what tree it has just
recognised4. When automata for trees with dif-
ferent root categories are merged, the resulting
automaton needs to somehow indicate to the
parser what trees are associated with its final
states.
In Evans and Weir (1997), we combined au-
tomata by introducing a new initial state with
&amp;transitions to each of the original initial states,
3A further difference is that the traversal encoded
in the automaton captures part of the parser&apos;s control
strategy. However for simplicity we assume here a fixed
parser control strategy (bottom-up, anchor-out) and do
not pursue this point further — Evans and Weir (1997)
offers some discussion.
4For recognition alone it only needs to know the root
category of the tree, but to recover the parse it needs to
identify the tree itself.
</bodyText>
<page confidence="0.99698">
373
</page>
<bodyText confidence="0.999915923076923">
and then determinising the resulting automa-
ton to induce some sharing of structure. To
recover trees, final automaton states were an-
notated with the number of the tree the final
state is associated with, which the parser can
then readily access.
However, the drawback of this approach is that
differently annotated final states can never be
merged, which restricts the scope for structure
sharing (minimisation, for example, is not pos-
sible since all the final states are distinct). To
overcome this, we propose an alternative ap-
proach as follows:
</bodyText>
<listItem confidence="0.9974305">
• each automaton transition is annotated
with the set of trees which pass through
it: when transitions are merged in au-
tomaton optimisation, their annotations
are unioned;
• the parser maintains for each item in the
</listItem>
<bodyText confidence="0.896776352941177">
table the set of trees that are valid for the
item: initially this is all the valid trees for
the automaton, but gets intersected with
the annotation of any transition followed;
also if two paths through the automaton
meet (i.e., an item is about to be added
for a second time), their annotations get
unioned.
This approach supports arbitrary merging of
states, including merging all the final states into
one. The parser maintains a dynamic record of
which trees are valid for states (in particular fi-
nal states) in the parse table. This means that
we can minimise our automata as well as deter-
minising them, and so share more structure (for
example, common processing at the end of the
recognition process as well as the beginning).
</bodyText>
<sectionHeader confidence="0.921402" genericHeader="method">
4 Recognition and parse recovery
</sectionHeader>
<bodyText confidence="0.999988763157895">
We noted above that a parsing algorithm
needs to be able to access the tree that
an automaton has recognised. The algo-
rithm we describe below actually needs rather
more information than this, because it uses a
two-phase recognition/parse-recovery approach.
The recognition phase only needs to know, for
each complete item, what the root label of the
tree recognised is. This can be recovered from
the &apos;valid tree&apos; annotation of the complete item
itself (there may be more than one valid tree,
corresponding to a phrase which has more than
one parse which happen to have been merged to-
gether). Parse recovery, however, involves run-
ning the recogniser &apos;backwards&apos; over the com-
pleted parse table, identifying for each item, the
items and actions which licensed it.
A complication arises because the automata, es-
pecially the merged automata, do not directly
correspond to tree structure. The recogniser re-
turns the tree recognised, and a search of the
parse table reveals the parser action which com-
pleted its recognition, but that information in
itself may not be enough to locate exactly where
in the tree the action took place. However, the
additional information required is static, and
so can be pre-compiled as the automata them-
selves are built up. For each action transition
(the action, plus the start and finish states)
we record the tree address that the transition
reaches (we call this the action-site, or just
a-site for short). During parse recovery, when
the parse table indicates an action that licensed
an item, we look up the relevant transition to
discover where in the tree (or trees, if we are
traversing several simultaneously) the present
item must be, so that we can correctly construct
a derivation tree.
</bodyText>
<sectionHeader confidence="0.998671" genericHeader="method">
5 Technical details
</sectionHeader>
<subsectionHeader confidence="0.999861">
5.1 Constructing the automata
</subsectionHeader>
<bodyText confidence="0.9999751">
We identify each node in an elementary tree 7
with an elementary address 7/i. The root
of 7 has the address 7/6 where E is the empty
string. Given a node -y/i, its n children are ad-
dressed from left to right with the addresses
7/il, ... 7/in, respectively. For convenience,
let anchor (-y) and foot (-y) denote the elemen-
tary address of the node that is the anchor and
footnode (if it has one) of -y, respectively; and
label (-y/i) and parent (7/i) denote the label of
-y/i and the address of the parent of -y/i, respec-
tively.
In this paper we make the following assumup-
tions about elementary trees. Each tree has a
single anchor node and therefore a single spine&apos;.
In the algorithms below we assume that nodes
not on the spine have no children. In practice,
not all elementary LTAG trees meet these con-
ditions, and we discuss how the approach de-
scribed here might be extended to the more gen-
</bodyText>
<footnote confidence="0.994459">
5The path from the root to the anchor node.
</footnote>
<page confidence="0.994557">
374
</page>
<bodyText confidence="0.940244">
eral case in Section 6.
Let 7/i be an elementary address of a
node on the spine of -y with n children
7/il, ,-y lin for n 1, where k is
such that -y I ik dominates anchor (y).
</bodyText>
<equation confidence="0.92376075">
-ylik+1 if j=18cn&gt;k
next (71ij) = -ylij — 1 if 2 &lt; j k
-ylij+ 1 if k &lt;j &lt;n
otherwise
</equation>
<bodyText confidence="0.999789074074074">
next defines a function that traverses a spine,
starting at the anchor. Traversal of an elemen-
tary tree during recognition yields a sequence of
parser actions, which we annotate as follows:
the two actions 4 and 4 indicate a substitu-
tion of a tree rooted with A to the left or right,
respectively; _A_, and 4_ indicate the presence
of the foot node, a node labelled A, to the left
or right, respectively; Finally 4 indicates an
adjunction of a tree with root and foot labelled
A. These actions constitute the input language
of the automaton that traverses the tree. This
automaton is defined as follows (note that we
use &amp;transitions between nodes to ease the con-
struction — we assume these are removed using
a standard algorithm).
Let 7 be an elementary tree with terminal and
nonterminal alphabets VT and VN, respectively.
Each state of the following automaton specifies
the elementary address -y/i being visited. When
the node is first visited we use the state 1[7/i];
when ready to move on we use the state T [7/i].
Define as follows the finite state automaton
M = (Q, E, l[anchor (-y)], 5, F). Q is the set
of states, E is the input alphabet, qo is the ini-
tial state, 5 is the transition relation, and F is
the set of final states.
</bodyText>
<equation confidence="0.8024576">
Q = { T[7/i],1[7/i] 17/i is an address in -y 1;
F= {Tfry }; and
S includes the following transitions:
(1[foot (7)], T[foot (-O]) if foot (7) is to the right
of anchor (-y)
(1[foot (&apos;)], +A_ , T[foot (-y)]), if foot (-y) is to the left
of anchor (-y)
(T[-y/i], e, l[next (yli)]) I y/i is an address in -y
i0e1
(1[-y/i], 4, T[7//) I 7/i substitution node,
label (yli) = A,
-y 1 i to right of anchor (7) }
ic-LE- 4,T[Wi]) -y/i substitution node,
label (-y/i) = A,
-y 1 i to left of anchor (-y) }
{ (J_[-y/i], 4, T[-yli]) y/i adjunction node
label (-y/i) = Al
{ T[-y/i]) I -y/i adjunction node }
(1- 11Y/i1) -y/i adjunction node,
label (7/i) = Al
</equation>
<bodyText confidence="0.846616714285714">
In order to recover derivation trees, we also
define the partial function a-site(q, a, q&apos;) for
(q, a, q&apos;) E (5 which provides information about
the site within the elementary tree of actions
occurring in the automaton.
if a e &amp; q&apos; = T[7/i]
undefined otherwise
</bodyText>
<subsectionHeader confidence="0.999402">
5.2 Combining Automata
</subsectionHeader>
<bodyText confidence="0.967357933333333">
Suppose we have a set of trees F =
{ 7772}. Let M71, , M7„ be the &amp;free
automata that are built from members of the
set r using the above construction, where for
1 &lt; k &lt;n, Mk = (QklEklqkl8k, FO•
Construction of a single automaton for F is a
two step process. First we build an automa-
ton that accepts all elementary computations
for trees in r; then we apply the standard au-
tomaton determinization and minimization al-
gorithms to produce an equivalent, compact au-
tomaton. The first step is achieved simply by
introducing a new initial state with &amp;transitions
to each of the qk:
Let M = (Q , E, qo, (5, F) where
</bodyText>
<equation confidence="0.987403">
Q = qo } U Ul&lt;k&lt;n Qi;
E= Ul&lt;k&lt;n Ek
F = Ul&lt;k&lt;n Fk
= Ul&lt;k&lt;n(q07 €7%) U Ul&lt;k&lt;n (5k•
</equation>
<bodyText confidence="0.998339230769231">
We determinize and then minimize M using
the standard set-of-states constructions to pro-
duce Mr = (Q&apos;, E, Qo, 5&apos; F&apos;). Whenever two
states are merged in either the determinizing
or minimizing algorithms the resulting state is
named by the union of the states from which it
is formed.
For each transition (Qi, a, Q2) E 5&apos; we define
the function a-sites(Qi, a, Q2) to be a set of el-
ementary nodes as follows:
a-sites(Qi , a, Q2) = jqi EQ, ,0EQ2 a-site(qi , a, q2)
Given a transition in Mr, this function returns
all the nodes in all merged trees which that tran-
</bodyText>
<equation confidence="0.863335">
a-site(q, a, q&apos; ) =
</equation>
<page confidence="0.992923">
375
</page>
<bodyText confidence="0.9895305">
sition reaches.
Finally, we define:
cross(Q1, a, Q2) = {-y I-y li E a-sites(Q1, a, Q2) }
This gives that subset of those trees whose el-
ementary computations take the Mr through
state Qi to Q2. These are the transition an-
notations referred to above, used to constrain
the parser&apos;s set of valid trees.
</bodyText>
<subsectionHeader confidence="0.998294">
5.3 The Recognition Phase
</subsectionHeader>
<bodyText confidence="0.993266116666667">
This section illustrates a simple bottom-up
parsing algorithm that makes use of minimized
automata produced from sets of trees that an-
chor the same input symbol.
The input to the parser takes the form of a se-
quence of minimized automata, one for each of
the symbols in the input. Let the input string
be w = al ... an and the associated automata
be MI, ... Mn where Mk = (Qk,Ek,qk, (5k, Fk)
for 1 &lt; k &lt; n. Let treesof(Mk) = rk where rk
is a set of the names of those elementary trees
that were used to construct the automata Mk.
During the recognition phase of the algorithm,
a set I of items are created. An item has
the form (T,q,[1,r,11,71) where T is a set of
elementary tree names, q is a automata state
and 1,r,1&apos;,r&apos; E { 0,... , n, — } such that either
/ &lt; /&apos; &lt; r&apos; &lt; r or / &lt; r and /&apos; .--- r&apos; = —. The in-
dices 1.1&apos;,r&apos;,r are positions between input sym-
bols (position 0 is before the first input symbols
and position n is after the final input symbol)
and we use wpm, to denote that substring of the
input w between positions p and p&apos;. I can be
viewed as a four dimensional array, each entry
of which contains a set of pairs comprising of a
set of nonterminals and an automata state.
Roughly speaking, an item (T,q,[1,r,l&apos; ,7-]) is in-
cluded in I when for every -y E T, anchored
by some ak (where 1 &lt; k &lt; r and if l&apos; 0 —
then k &lt; 1&apos; or r&apos; &lt; k); q is a state in Qk, such
that some elementary subcomputation reaching
q from the initial state, qk, of Mk is an ini-
tial substring of the elementary computation for
1, that reaches the elementary address -y/i, the
subtree rooted at -yli spans wi,r, and if -yli dom-
inates a foot node then that foot node spans
wi,,r,, otherwise l&apos; 0 r&apos; = —.
The input is accepted if an item
(T, q, [0, n, —, —]) is added to / where T
contains some initial tree rooted in the start
symbol S and qf E Fk for some k.
When adding items to I we use the procedure
add(T, q,[1,r,l&apos; , r&apos;]) which is defined such that
if there is already an entry (T&apos;,q,[1,r,r,r1) E
I for some T&apos; then replace this with the entry
(T U T&apos;,q,[1,r,1&apos;,71)6; otherwise add the new
entry (T,q,[1,r,1&apos;,7-1) to I.
I is initialized as follows. For each k E
{1,... , n } call add(T, qk,[k —1, k, —, —1) where
T 0 treesof(Mk) and qk is the initial state of
the automata Mk.
We now present the rules with which the com-
plete set I is built. These rules correspond
closely to the familiar steps in existing bottom-
up LTAG parser, in particular, the way that
we use the four indices is exactly the same as
in other approaches (Vijay-Shanker and Joshi,
1985). As a result a standard control strategy
can be used to control the order in which these
rules are applied to existing entries of I.
</bodyText>
<listItem confidence="0.904374157894737">
1. If (T,q,[1,r,1&apos;,71),(T&apos; ,qf,[r,r&amp;quot;, —, —]) E I,
qf E Fk for some k, (q,4,q&apos;) E bk, for
some k&apos;, label (-OE) = A from some -y&apos; E
T&apos; &amp; T&amp;quot; = T n cross(q, 4, q&apos;) then call
add(T&amp;quot;, q&apos; ,[1, r&amp;quot;, 1,, r&apos;]).
2. If (T,q,[1,r,1&apos;,71),(T&apos;,q1,[1&amp;quot;,1,—,--]) E I,
qf E Fk for some k, (q,4,q&apos;) E bk, for
some k&apos;, label (-y7E) 0 A from some -y&apos; E
T&apos; &amp; T&amp;quot; 0 T n cross(q, 4, q&apos;) then call
add (7&apos;&apos;&apos;, q&apos;,[1&amp;quot;,r,1&apos;, r&apos;]).
3. If (T, q, [1, r, —, —]) E I, (q, 2_4, q&apos;) E Ok for
some k &amp; T&apos; 0 T n cross(q, 4, q&apos;) then
for each r&apos; such that r &lt; r&apos; &lt; n call
add(T&apos;, q&apos;, [1,r&apos;, r, 71).
4. If (T,q,[1,r,—,—]) E I, (q,41L,V) E (5k
for some k &amp; T&apos; = T n cross(q, 4/1_ , q&apos;)
then for each l&apos; such that 0 &lt; l&apos; &lt; 1 call
add(T&apos;, q&apos;,[1&apos;,r,1&apos;,1]).
5. If (T,q,[1,r,1&apos;,71),(T&apos;,qf,[1&amp;quot;,r&amp;quot;,1,7-]) E I,
</listItem>
<bodyText confidence="0.96078625">
qf E Fk for some k, (q,4,q&apos;) E 8k/ for
some k&apos;, label (7&apos; le) = A from some ry&apos; E
T&apos; &amp; T&amp;quot; = T n cross(q, 4, q&apos;) then call
add(T&amp;quot;,q&apos;,[1&amp;quot;,r&amp;quot;,1&apos;,r1).
</bodyText>
<footnote confidence="0.997014">
6This replacement is treated as a new entry in the
table. If the old entry has already licenced other entries,
this may result in some duplicate processing. This could
be eliminated by a more sophisticated treatment of tree
sets.
</footnote>
<page confidence="0.998153">
376
</page>
<bodyText confidence="0.999955">
The running time of this algorithm is 0(n6)
since the last rule must be embedded within six
loops each of which varies with n. Note that
although the third and fourth rules both take
0(n) steps, they need only be embedded within
the 1 and r loops.
</bodyText>
<subsectionHeader confidence="0.999488">
5.4 Recovering Parse Trees
</subsectionHeader>
<bodyText confidence="0.95231194047619">
Once the set of items I has been completed, the
final task of the parser is to a recover derivation
tree7. This involves retracing the steps of the
recognition process in reverse. At each point,
we look for a rule that would have caused the
inclusion of item in I. Each of these rules in-
volves some transition (q, a, q&apos;) E bk for some k
where a is one of the parser actions, and from
this transition we consult the set of elementary
addresses in a-sites(q, a, q&apos;) to establish how to
build the derivation tree. We eventually reach
items added during the initialization phase and
the process ends. Given the way our parser has
been designed, some search will be needed to
find the items we need. As usual, the need for
such search can be reduced through the inclu-
sion of pointers in items, though this is at the
cost of increasing parsing time. There are var-
ious points in the following description where
nondeterminism exists. By exploring all possi-
ble paths, it would be straightforward to pro-
duce an AND/OR derivation tree that encodes
all derivation trees for the input string.
We use the procedure der((T, q, [1, r, 11 , 71) ,
which completes the partial derivation tree T by
backing up through the moves of the automata
in which q is a state.
A derivation tree for the input is returned
by the call der((T, qf, [0, n, -, -]), r) where
(T, q1, [0, n, -]) E I such that T contains
some initial tree -y rooted with the start non-
terminal S and q f is the final state of some au-
tomata Mk, 1 &lt; k &lt; n. r is a derivation tree
containing just one node labelled with name 7.
In general, on a call to der((T, q, [1, r, , r&apos;]) , T)
we examine I to find a rule that has caused this
item to be included in I. There are six rules
to consider, corresponding to the five recogniser
rules, plus lexical introduction, as follows:
1. If (T&apos; , q&apos;, [1, r&amp;quot; ,1&apos; , 71), (2&apos;&amp;quot;, qf , [r&amp;quot; ,r, -, -]) E
&apos;Derivation trees are labelled with tree names and
edges are labelled with tree addresses.
I, qf E Fk for some k, E Ski for
some k&apos;, -y is the label of the root of T,
7 E , label (77E) = A from some E T&amp;quot;
St 7/i E a-sites(q1, 4, q), then let T1 be the
derivation tree containing a single node
labelled 7i, and let Til be the result of at-
taching der( (T&amp;quot; , q1, [7.&amp;quot; ,r, -, -]), 7&apos;) under
the root of T with an edge labelled the tree
address i. We then complete the derivation
tree by calling der((T&apos;, q&apos;, [1, r&amp;quot;, ,r&apos;]), T&amp;quot;).
2. If (D, q&apos;, {r&amp;quot;, r, , ?JD , (T&amp;quot;, q1, [1, r&amp;quot;, -, -]) E
I, qf E Fk for some k, (q&apos;,4,q) E 5k1 for
some k&apos; 7 is the label of the root of T
E , label (-y1/E) = A from some -y1 E T&amp;quot;
&amp; 7/i E a-sites(q&apos;, 4, q), then let T1 be the
derivation tree containing a single node
labelled 7&apos;, and let Tit be the result of at-
taching der((T&amp;quot;, q1, [1, r&amp;quot;, -, -]), 7-&apos;) under
the root of 7- with an edge labelled the tree
address i. We then complete the derivation
tree by calling der( (T1 , ,[r&amp;quot; ,r, ,r&apos;]), T&amp;quot;).
3. If r = r&apos;, (T&apos;, q&apos;, [1,1&apos; , -, -]) E I and
(q&apos; , _A_ , q) E bk for some k, -y is the
label of the root of T, 7 E T&apos; and
foot (7) E a-sites(q&apos;, , q) then make the
call der((T&apos;, q&apos;, [1, , -, -]),
4. If 1 = , (T&apos;, q&apos;, ,r, -]) E I and
(q&apos;, q&apos;) E Sk for some k, 7 is the
label of the root of T, 7 E T&apos; and
foot (7) E a-sites(q1, ,A_, q) then make the
call der((T&apos;, ,[r&apos; ,r, -, -]), T.).
5. If (T&apos;, q&apos;, [1&amp;quot;, r&amp;quot;,1, 71), (T&amp;quot;, qf , [1, r, 1&amp;quot;, r&amp;quot;]) E
I, qf E Fk for some k, (q&apos;, 4, q) E 5k/ for
some k&apos;, 7 is the label of the root of T,
E , label (77E) = A from some -y1 E T&amp;quot;
and 7/i E a-sites(q&apos;, 4, q), then let Ti be
the derivation tree containing a single node
labelled -y1, and let Til be the result of at-
taching der((T&amp;quot;, q f , [1, r, 1&amp;quot;, r&amp;quot;]) , T&apos;) under
the root of T with an edge labelled the tree
address i. We then complete the derivation
tree by calling der((T&apos;, q&apos;, [1&amp;quot;, r&amp;quot; ,1&apos; , r&apos;]), T&amp;quot;) .
</bodyText>
<listItem confidence="0.708931666666667">
6. If / +1 = r, = - q is the initial state
of Mr, -y is the label of the root of -r, -y E T,
then return the final derivation tree T.
</listItem>
<sectionHeader confidence="0.999203" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.99973">
The approach described here offers empirical
rather than formal improvements in perfor-
mance. In the worst case, none of the trees
</bodyText>
<page confidence="0.993145">
377
</page>
<table confidence="0.948533285714286">
word no. of trees automaton no. of states no. of transitions trees per state
come 133 merged 898 1130 1
minimised 50 130 11.86
break 177 merged 1240 1587 1
minimised 68 182 12.13
give 337 merged 2494 3177 1
minimised 83 233 20.25
</table>
<tableCaption confidence="0.99979">
Table 1: DTG compaction results (from Carroll et al. (1998)).
</tableCaption>
<bodyText confidence="0.999993288888889">
in the grammar share any structure so no op-
timisation is possible. However, in the typi-
cal case, there is scope for substantial structure
sharing among closely related trees. Carroll et
al. (1998) report preliminary results using this
technique on a wide-coverage DTG (a variant
of LTAG) grammar. Table 1 gives statistics for
three common verbs in the grammar: the total
number of trees, the size of the merged automa-
ton (before any optimisation has occurred) and
the size of the minimised automaton. The fi-
nal column gives the average of the number of
trees that share each state in the automaton.
These figures show substantial optimisation is
possible, both in the space requirements of the
grammar and in the sharing of processing state
between trees during parsing.
As mentioned earlier, the algorithms we have
presented assume that elementary trees have
one anchor and one spine. Some trees, how-
ever, have secondary anchors (for example, a
subcategorised preposition). One possible way
of including such cases would be to construct
automata from secondary anchors up the sec-
ondary spine to the main spine. The automata
for both the primary and secondary anchors
associated with a lexical item could then be
merged, minimized and used for parsing as
above.
Using automata for parsing has a long his-
tory dating back to transition networks (Woods,
1970). More recent uses include Alshawi (1996)
and Eisner (1997). These approaches differ from
the present paper in their use of automata as
part of the grammar formalism itself. Here,
automata are used purely as a stepping-stone
to parser optimisation: we make no linguistic
claims about them. Indeed one view of this
work is that it frees the linguistic descriptions
from overt computational considerations. This
work has perhaps more in common with the
technology of LR parsing as a parser optimi-
sation technique, and it would be interesting to
compare our approach with a direct application
of LR ideas to LTAGs.
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999169837837838">
H. Alshawi. 1996. Head automata and bilingual
tilings: Translation with minimal representations.
In ACL96, pages 167-176.
J. Carroll, N. Nicolov, 0. Shaumyan, M. Smets, and
D. Weir. 1998. Grammar compaction and computa-
tion sharing in automaton-based parsing. In Pro-
ceedings of the First Workshop on Tabulation in
Parsing and Deduction, pages 16-25.
J. Chen and K. Vijay-Shanker. 1997. Towards a
reduced-commitment D-theory style TAG parser. In
IWPT97, pages 18-29.
J. Eisner. 1997. Bilexical grammars and a cubic-
time probabilistic parser. In IWPT97, pages 54-65.
R. Evans and D. Weir. 1997. Automaton-based
parsing for lexicalized grammars. In IWPT97, pages
66-76.
D. A. Huffman. 1954. The synthesis of sequential
switching circuits. J. Franklin Institute.
A. K. Joshi and Y. Schabes. 1991. Tree-adjoining
grammars and lexicalized grammars. In Maurice Ni-
vat and Andreas Podelski, editors, Definability and
Recognizability of Sets of Trees. Elsevier.
E. F. Moore, 1956. Automata Studies, chap-
ter Gedanken experiments on sequential machines,
pages 129-153. Princeton University Press, N.J.
Y. Schabes and A. K. Joshi. 1988. An Earley-type
parsing algorithm for tree adjoining grammars. In
A CL88.
K. Vijay-Shanker and A. K. Joshi. 1985. Some com-
putational properties of tree adjoining grammars. In
ACL85, pages 82-93.
K. Vijay-Shanker and D. Weir. 1993. Parsing some
constrained grammar formalisms. Computational
Linguistics, 19(4):591-636.
W. A. Woods. 1970. Transition network gram-
mars for natural language analysis. Commun. ACM,
13:591-606.
</reference>
<page confidence="0.998298">
378
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.487130">
<title confidence="0.999437">A structure-sharing parser for lexicalized grammars</title>
<author confidence="0.999364">Roger Evans</author>
<affiliation confidence="0.999943">Information Technology Research Institute University of Brighton</affiliation>
<address confidence="0.999782">Brighton, BN2 4GJ, UK</address>
<email confidence="0.598351">Roger.Evans@itri.brighton.ac.uk</email>
<author confidence="0.999828">David Weir</author>
<affiliation confidence="0.996017">Cognitive and Computing Sciences University of Sussex</affiliation>
<address confidence="0.99978">Brighton, BN1 9QH, UK</address>
<email confidence="0.929168">David.Weir@cogs.susx.ac.uk</email>
<abstract confidence="0.993423166666667">In wide-coverage lexicalized grammars many of the elementary structures have substructures in common. This means that in conventional parsing algorithms some of the computation associated with different structures is duplicated. In this paper we describe a precompilation technique for such grammars which allows some of this computation to be shared. In our approach the elementary structures of the grammar are transformed into finite state automata which can be merged and minimised using standard algorithms, and then parsed using an automatonbased parser. We present algorithms for constructing automata from elementary structures, merging and minimising them, and string recognition and parse recovery with the resulting grammar.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Alshawi</author>
</authors>
<title>Head automata and bilingual tilings: Translation with minimal representations.</title>
<date>1996</date>
<booktitle>In ACL96,</booktitle>
<pages>167--176</pages>
<marker>Alshawi, 1996</marker>
<rawString>H. Alshawi. 1996. Head automata and bilingual tilings: Translation with minimal representations. In ACL96, pages 167-176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Smets Shaumyan</author>
<author>D Weir</author>
</authors>
<title>Grammar compaction and computation sharing in automaton-based parsing.</title>
<date>1998</date>
<booktitle>In Proceedings of the First Workshop on Tabulation in Parsing and Deduction,</booktitle>
<pages>16--25</pages>
<marker>Shaumyan, Weir, 1998</marker>
<rawString>J. Carroll, N. Nicolov, 0. Shaumyan, M. Smets, and D. Weir. 1998. Grammar compaction and computation sharing in automaton-based parsing. In Proceedings of the First Workshop on Tabulation in Parsing and Deduction, pages 16-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chen</author>
<author>K Vijay-Shanker</author>
</authors>
<title>Towards a reduced-commitment D-theory style TAG parser.</title>
<date>1997</date>
<booktitle>In IWPT97,</booktitle>
<pages>18--29</pages>
<contexts>
<context position="1770" citStr="Chen and Vijay-Shanker (1997)" startWordPosition="261" endWordPosition="264">chabes, 1991) are difficult to parse with efficiently. Each word in the parser&apos;s input string introduces an elementary tree into the parse table for each of its possible readings, and there is often a substantial overlap in structure between these trees. A conventional parsing algorithm (VijayShanker and Joshi, 1985) views the trees as independent, and so is likely to duplicate the processing of this common structure. Parsing could be made more efficient (empirically if not formally), if the shared structure could be identified and processed only once. Recent work by Evans and Weir (1997) and Chen and Vijay-Shanker (1997) addresses this problem from two different perspectives. Evans and Weir (1997) outline a technique for compiling LTAG grammars into automata which are then merged to introduce some sharing of structure. Chen and Vijay-Shanker (1997) use underspecified tree descriptions to represent sets of trees during parsing. The present paper takes the former approach, but extends our previous work by: • showing how merged automata can be minimised, so that they share as much structure as possible; • showing that by precompiling additional information, parsing can be broken down into recognition followed by</context>
</contexts>
<marker>Chen, Vijay-Shanker, 1997</marker>
<rawString>J. Chen and K. Vijay-Shanker. 1997. Towards a reduced-commitment D-theory style TAG parser. In IWPT97, pages 18-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Bilexical grammars and a cubictime probabilistic parser.</title>
<date>1997</date>
<booktitle>In IWPT97,</booktitle>
<pages>54--65</pages>
<marker>Eisner, 1997</marker>
<rawString>J. Eisner. 1997. Bilexical grammars and a cubictime probabilistic parser. In IWPT97, pages 54-65. R. Evans and D. Weir. 1997. Automaton-based parsing for lexicalized grammars. In IWPT97, pages 66-76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Huffman</author>
</authors>
<title>The synthesis of sequential switching circuits.</title>
<date>1954</date>
<journal>J. Franklin Institute.</journal>
<contexts>
<context position="6800" citStr="Huffman, 1954" startWordPosition="1079" endWordPosition="1080">, we can merge trees together and apply standard optimisation techniques to share their common structure. The parser will remain unchanged, but will operate more efficiently where structure has been shared. Additionally, because the automata are more abstract than the trees, capturing precisely the parser&apos;s view of the trees, sharing may occur between trees which are structurally quite different, but which happen to have common parser actions associated with them. 3 Merging and minimising automata Combining the automata for several trees can be achieved using a variety of standard algorithms (Huffman, 1954; Moore, 1956). However any transformations must respect one important feature: once the parser reaches a final state it needs to know what tree it has just recognised4. When automata for trees with different root categories are merged, the resulting automaton needs to somehow indicate to the parser what trees are associated with its final states. In Evans and Weir (1997), we combined automata by introducing a new initial state with &amp;transitions to each of the original initial states, 3A further difference is that the traversal encoded in the automaton captures part of the parser&apos;s control str</context>
</contexts>
<marker>Huffman, 1954</marker>
<rawString>D. A. Huffman. 1954. The synthesis of sequential switching circuits. J. Franklin Institute.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>Y Schabes</author>
</authors>
<title>Tree-adjoining grammars and lexicalized grammars.</title>
<date>1991</date>
<booktitle>In Maurice Nivat and Andreas Podelski, editors, Definability and Recognizability of Sets of Trees.</booktitle>
<publisher>Elsevier.</publisher>
<contexts>
<context position="1154" citStr="Joshi and Schabes, 1991" startWordPosition="159" endWordPosition="162">ted. In this paper we describe a precompilation technique for such grammars which allows some of this computation to be shared. In our approach the elementary structures of the grammar are transformed into finite state automata which can be merged and minimised using standard algorithms, and then parsed using an automatonbased parser. We present algorithms for constructing automata from elementary structures, merging and minimising them, and string recognition and parse recovery with the resulting grammar. 1 Introduction It is well-known that fully lexicalised grammar formalisms such as LTAG (Joshi and Schabes, 1991) are difficult to parse with efficiently. Each word in the parser&apos;s input string introduces an elementary tree into the parse table for each of its possible readings, and there is often a substantial overlap in structure between these trees. A conventional parsing algorithm (VijayShanker and Joshi, 1985) views the trees as independent, and so is likely to duplicate the processing of this common structure. Parsing could be made more efficient (empirically if not formally), if the shared structure could be identified and processed only once. Recent work by Evans and Weir (1997) and Chen and Vija</context>
</contexts>
<marker>Joshi, Schabes, 1991</marker>
<rawString>A. K. Joshi and Y. Schabes. 1991. Tree-adjoining grammars and lexicalized grammars. In Maurice Nivat and Andreas Podelski, editors, Definability and Recognizability of Sets of Trees. Elsevier.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Moore</author>
</authors>
<title>Automata Studies, chapter Gedanken experiments on sequential machines,</title>
<date>1956</date>
<pages>129--153</pages>
<publisher>Princeton University Press, N.J.</publisher>
<contexts>
<context position="6814" citStr="Moore, 1956" startWordPosition="1081" endWordPosition="1082">trees together and apply standard optimisation techniques to share their common structure. The parser will remain unchanged, but will operate more efficiently where structure has been shared. Additionally, because the automata are more abstract than the trees, capturing precisely the parser&apos;s view of the trees, sharing may occur between trees which are structurally quite different, but which happen to have common parser actions associated with them. 3 Merging and minimising automata Combining the automata for several trees can be achieved using a variety of standard algorithms (Huffman, 1954; Moore, 1956). However any transformations must respect one important feature: once the parser reaches a final state it needs to know what tree it has just recognised4. When automata for trees with different root categories are merged, the resulting automaton needs to somehow indicate to the parser what trees are associated with its final states. In Evans and Weir (1997), we combined automata by introducing a new initial state with &amp;transitions to each of the original initial states, 3A further difference is that the traversal encoded in the automaton captures part of the parser&apos;s control strategy. However</context>
</contexts>
<marker>Moore, 1956</marker>
<rawString>E. F. Moore, 1956. Automata Studies, chapter Gedanken experiments on sequential machines, pages 129-153. Princeton University Press, N.J.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Schabes</author>
<author>A K Joshi</author>
</authors>
<title>An Earley-type parsing algorithm for tree adjoining grammars.</title>
<date>1988</date>
<booktitle>In A CL88.</booktitle>
<contexts>
<context position="2857" citStr="Schabes and Joshi, 1988" startWordPosition="431" endWordPosition="434">ch structure as possible; • showing that by precompiling additional information, parsing can be broken down into recognition followed by parse recovery; • providing a formal treatment of the algorithms for transforming and minimising the grammar, recognition and parse recovery. In the following sections we outline the basic approach, and describe informally our improvements to the previous account. We then give a formal account of the optimisation process and a possible parsing algorithm that makes use of itl. 2 Automaton-based parsing Conventional LTAG parsers (Vijay-Shanker and Joshi, 1985; Schabes and Joshi, 1988; VijayShanker and Weir, 1993) maintain a parse table, a set of items corresponding to complete and partial constituents. Parsing proceeds by first seeding the table with items anchored on the input string, and then repeatedly scanning the table for parser actions. Parser actions introduce new items into the table licensed by one or more items already in the table. The main types of parser actions are: 1. extending a constituent by incorporating a complete subconstituent (on the left or &apos;However, due to lack of space, no proofs and only minimal informal descriptions are given in this paper. 37</context>
</contexts>
<marker>Schabes, Joshi, 1988</marker>
<rawString>Y. Schabes and A. K. Joshi. 1988. An Earley-type parsing algorithm for tree adjoining grammars. In A CL88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>A K Joshi</author>
</authors>
<title>Some computational properties of tree adjoining grammars.</title>
<date>1985</date>
<booktitle>In ACL85,</booktitle>
<pages>82--93</pages>
<contexts>
<context position="2832" citStr="Vijay-Shanker and Joshi, 1985" startWordPosition="427" endWordPosition="430">mised, so that they share as much structure as possible; • showing that by precompiling additional information, parsing can be broken down into recognition followed by parse recovery; • providing a formal treatment of the algorithms for transforming and minimising the grammar, recognition and parse recovery. In the following sections we outline the basic approach, and describe informally our improvements to the previous account. We then give a formal account of the optimisation process and a possible parsing algorithm that makes use of itl. 2 Automaton-based parsing Conventional LTAG parsers (Vijay-Shanker and Joshi, 1985; Schabes and Joshi, 1988; VijayShanker and Weir, 1993) maintain a parse table, a set of items corresponding to complete and partial constituents. Parsing proceeds by first seeding the table with items anchored on the input string, and then repeatedly scanning the table for parser actions. Parser actions introduce new items into the table licensed by one or more items already in the table. The main types of parser actions are: 1. extending a constituent by incorporating a complete subconstituent (on the left or &apos;However, due to lack of space, no proofs and only minimal informal descriptions ar</context>
<context position="18543" citStr="Vijay-Shanker and Joshi, 1985" startWordPosition="3232" endWordPosition="3235">ch is defined such that if there is already an entry (T&apos;,q,[1,r,r,r1) E I for some T&apos; then replace this with the entry (T U T&apos;,q,[1,r,1&apos;,71)6; otherwise add the new entry (T,q,[1,r,1&apos;,7-1) to I. I is initialized as follows. For each k E {1,... , n } call add(T, qk,[k —1, k, —, —1) where T 0 treesof(Mk) and qk is the initial state of the automata Mk. We now present the rules with which the complete set I is built. These rules correspond closely to the familiar steps in existing bottomup LTAG parser, in particular, the way that we use the four indices is exactly the same as in other approaches (Vijay-Shanker and Joshi, 1985). As a result a standard control strategy can be used to control the order in which these rules are applied to existing entries of I. 1. If (T,q,[1,r,1&apos;,71),(T&apos; ,qf,[r,r&amp;quot;, —, —]) E I, qf E Fk for some k, (q,4,q&apos;) E bk, for some k&apos;, label (-OE) = A from some -y&apos; E T&apos; &amp; T&amp;quot; = T n cross(q, 4, q&apos;) then call add(T&amp;quot;, q&apos; ,[1, r&amp;quot;, 1,, r&apos;]). 2. If (T,q,[1,r,1&apos;,71),(T&apos;,q1,[1&amp;quot;,1,—,--]) E I, qf E Fk for some k, (q,4,q&apos;) E bk, for some k&apos;, label (-y7E) 0 A from some -y&apos; E T&apos; &amp; T&amp;quot; 0 T n cross(q, 4, q&apos;) then call add (7&apos;&apos;&apos;, q&apos;,[1&amp;quot;,r,1&apos;, r&apos;]). 3. If (T, q, [1, r, —, —]) E I, (q, 2_4, q&apos;) E Ok for some k &amp; T&apos; 0</context>
</contexts>
<marker>Vijay-Shanker, Joshi, 1985</marker>
<rawString>K. Vijay-Shanker and A. K. Joshi. 1985. Some computational properties of tree adjoining grammars. In ACL85, pages 82-93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>D Weir</author>
</authors>
<title>Parsing some constrained grammar formalisms.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--4</pages>
<marker>Vijay-Shanker, Weir, 1993</marker>
<rawString>K. Vijay-Shanker and D. Weir. 1993. Parsing some constrained grammar formalisms. Computational Linguistics, 19(4):591-636.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Woods</author>
</authors>
<title>Transition network grammars for natural language analysis.</title>
<date>1970</date>
<journal>Commun. ACM,</journal>
<pages>13--591</pages>
<contexts>
<context position="25814" citStr="Woods, 1970" startWordPosition="4692" endWordPosition="4693">ween trees during parsing. As mentioned earlier, the algorithms we have presented assume that elementary trees have one anchor and one spine. Some trees, however, have secondary anchors (for example, a subcategorised preposition). One possible way of including such cases would be to construct automata from secondary anchors up the secondary spine to the main spine. The automata for both the primary and secondary anchors associated with a lexical item could then be merged, minimized and used for parsing as above. Using automata for parsing has a long history dating back to transition networks (Woods, 1970). More recent uses include Alshawi (1996) and Eisner (1997). These approaches differ from the present paper in their use of automata as part of the grammar formalism itself. Here, automata are used purely as a stepping-stone to parser optimisation: we make no linguistic claims about them. Indeed one view of this work is that it frees the linguistic descriptions from overt computational considerations. This work has perhaps more in common with the technology of LR parsing as a parser optimisation technique, and it would be interesting to compare our approach with a direct application of LR idea</context>
</contexts>
<marker>Woods, 1970</marker>
<rawString>W. A. Woods. 1970. Transition network grammars for natural language analysis. Commun. ACM, 13:591-606.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>