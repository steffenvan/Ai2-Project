<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000690">
<title confidence="0.997708">
Co-occurrence Graph Based Iterative Bilingual Lexicon Extraction From
Comparable Corpora
</title>
<author confidence="0.992489">
Diptesh Chatterjee and Sudeshna Sarkar and Arpit Mishra
</author>
<affiliation confidence="0.9994745">
Department of Computer Science and Engineering
Indian Institute of Technology Kharagpur
</affiliation>
<email confidence="0.995903">
{diptesh,sudeshna,arpit}@cse.iitkgp.ernet.in
</email>
<sectionHeader confidence="0.997348" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999798545454546">
This paper presents an iterative algorithm
for bilingual lexicon extraction from com-
parable corpora. It is based on a bag-
of-words model generated at the level of
sentences. We present our results of ex-
perimentation on corpora of multiple de-
grees of comparability derived from the
FIRE 2010 dataset. Evaluation results on
100 nouns shows that this method outper-
forms the standard context-vector based
approaches.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999970086206897">
Bilingual dictionaries play a pivotal role in a num-
ber of Natural Language Processing tasks like
Machine Translation and Cross Lingual Informa-
tion Retrieval(CLIR). Machine Translation sys-
tems often use bilingual dictionaries in order to
augment word and phrase alignment (Och and
Ney, 2003). CLIR systems use bilingual dictio-
naries in the query translation step (Grefenstette,
1998). However, high coverage electronic bilin-
gual dictionaries are not available for all language
pairs. So a major research area in Machine Trans-
lation and CLIR is bilingual dictionary extraction.
The most common approach for extracting bilin-
gual dictionary is applying some statistical align-
ment algorithm on a parallel corpus. However,
parallel corpora are not readily available for most
language pairs. Also, it takes a lot of effort to ac-
tually get the accurate translations of sentences.
Hence, constructing parallel corpora involves a lot
of effort and time. So in recent years, extract-
ing bilingual dictionaries from comparable cor-
pora has become an important area of research.
Comparable corpora consist of documents on sim-
ilar topics in different languages. Unlike parallel
corpora, they are not sentence aligned. In fact,
the sentences in one language do not have to be
the exact translations of the sentence in the other
language. However, the two corpora must be on
the same domain or topic. Comparable corpora
can be obtained more easily than parallel corpora.
For example, a collection of news articles from
the same time period but in different languages
can form a comparable corpora. But after care-
ful study of news articles in English and Hindi
published on same days at the same city, we have
observed that along with articles on similar top-
ics, the corpora also contain a lot of articles which
have no topical similarity. Thus, the corpora are
quite noisy, which makes it unsuitable for lexicon
extraction. Thus another important factor in com-
parable corpora construction is the degree of sim-
ilarity of the corpora.
Approaches for lexicon extraction from compara-
ble corpora have been proposed that use the bag-
of-words model to find words that occur in similar
lexical contexts (Rapp, 1995). There have been
approaches proposed which improve upon this
model by using some linguistic information (Yuu
and Tsujii, 2009). However, these require some
linguistic tool like dependency parsers which are
not commonly obtainable for resource-poor lan-
guages. For example, in case of Indian languages
like Hindi and Bengali, we still do not have good
enough dependency parsers. In this paper, we
propose a word co-occurrence based approach for
lexicon extraction from comparable corpora using
English and Hindi as the source and target lan-
guages respectively. We do not use any language-
</bodyText>
<page confidence="0.991237">
35
</page>
<note confidence="0.5595005">
Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 35–42,
Beijing, August 2010
</note>
<bodyText confidence="0.998709846153846">
specific resource in our approach.
We did experiments with 100 words in En-
glish,and show that our approach performs signif-
icantly better than the the Context Heterogeneity
approach (Fung, 1995). We show the results over
corpora with varying degrees of comparability.
The outline of the paper is as follows. In section
2, we analyze the different approaches for lexicon
extraction from comparable corpora. In section 3,
we present our algorithm and the experimental re-
sults. In section 4, we present an analysis of the
results followed by the conclusion and future re-
search directions in section 5.
</bodyText>
<sectionHeader confidence="0.997677" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999986448979592">
One of the first works in the area of comparable
corpora mining was based on word co-occurrence
based approach (Rapp, 1995). The basic assump-
tion behind this approach was two words are likely
to occur together in the same context if their joint
probability of occurrence in a corpus exceeds the
probability that the words occur randomly. In his
paper, Rapp made use of a similarity matrix and
using a joint probability estimate determined the
word maps. However this approach did not yield
significantly good results.
The “Context Heterogeneity” approach was one
of the pioneering works in this area. It uses a 2-
dimensional context vector for each word based
on the right and left context. The context vector
depended on how many distinct words occur in the
particular context and also the unigram frequency
of the word to be translated. Euclidean distance
between context vectors was used as a similarity
measure.
Another approach used Distributed Clustering of
Translational Equivalents for word sense acqui-
sition from bilingual comparable corpora (Kaji,
2003). However, the major drawback of this paper
is the assumption that translation equivalents usu-
ally represent only one sense of the target word.
This may not be the case for languages having
similar origin, for example, Hindi and Bengali.
Approaches using context information for extract-
ing lexical translations from comparable corpora
have also been proposed (Fung and Yee, 1998;
Rapp, 1999). But they resulted in very poor cov-
erage. These approaches were improved upon
by extracting phrasal alignments from comparable
corpora using joint probability SMT model (Ku-
mano et al., 2007) .
Another proposed method uses dependency pars-
ing and Dependency Heterogeneity for extracting
bilingual lexicon (Yuu and Tsujii, 2009) . This
approach was similar to that of Fung, except they
used a dependency parser to get the tags for each
word and depending on the frequency of each tag
they defined a vector to represent each word in
question. Here too, Euclidean similarity was used
to compute the similarity between two words us-
ing their context vectors. However, this method is
dependent on availability of a dependency parser
for the languages and is not feasible for languages
for which resources are scarce.
</bodyText>
<sectionHeader confidence="0.9996755" genericHeader="method">
3 Bilingual Dictionary Extraction Using
Co-occurrence Information
</sectionHeader>
<subsectionHeader confidence="0.999803">
3.1 Motivation
</subsectionHeader>
<bodyText confidence="0.999917777777778">
The Context Heterogeneity and Dependency Het-
erogeneity approaches suffer from one major
drawback. They do not use any kind of infor-
mation about how individual words combine in a
particular context to form a meaningful sentence.
They only use some statistics about the number of
words that co-occur in a particular context or the
number of times a word receives a particular tag
in dependency parsing. So, we wished to study if
the quality of dictionary extracted would improve
if we consider how individual words co-occur in
text and store that information in the form of a
vector, with one dimension representing one word
in the corpus. One important point to note here
is that the function words in a language are usu-
ally very small in number. If we need to construct
a dictionary of function words in two languages,
that can be done without much effort manually.
Also, the function words do not play an impor-
tant role in CLIR applications, as they are usually
stripped off.
Our algorithm is based on the intuition that words
having similar semantic connotations occur to-
gether. For example, the words “bread” is more
likely to occur with “eat” than with “play”. Our
algorithm uses this distribution of co-occurrence
frequency along with a small initial seed dictio-
</bodyText>
<page confidence="0.989568">
36
</page>
<bodyText confidence="0.994308761904762">
nary to extract words that are translations of one
another. We define a co-occurrence vector of
words in both the languages, and also record the
number of times two words co-occur. To find
the translation for word W,,, we check for the
words co-occurring with W,, such that this word
already has a map in the other language, and com-
pute a scoring function using all such words co-
occurring with W,,. In short, we use the already
existing information to find new translations and
add them to the existing lexicon to grow it. Be-
low is a snapshot of a part of the data from one
of our experiments using the FIRE 20101 cor-
pus. For each word in English and Hindi, the co-
occurrence data is expressed as a list of tuples.
Each tuple has the form (word, co-occurrence
frequency). For the Hindi words, the English
meaning has been provided in parenthesis. For
the seed lexicon and final lexicon, the format is
(source word, target word, strength).
English:
</bodyText>
<listItem confidence="0.99476025">
1. teacher:{(training,49),(colleges,138),
(man,22)}
2. car:{(drive,238),(place,21)}
3. drive:{(car,238),(steer,125),(city,12),
(road,123)}
Hindi:
1. ghar(home):{(khidki(window),133),(makAn
(house),172), (rAstA(road),6)}
2. gAdi(car):{(rAsta,92),(chAlak(driver),121),
(signal,17)}
3. shikshaka(teacher):{(vidyalaya(school),312),
(makAn(house),6)}
Seed lexicon:
1. (colleges,vidyalaya,0.4)
2. (colleges,mahavidyalaya(college),0.6)
3. (car,gAdi,1.0)
</listItem>
<bodyText confidence="0.98611">
The following is a snapshot from the final results
given by the algorithm:
</bodyText>
<footnote confidence="0.676808">
1Forum For Information Retrieval
http://www.isical.ac.in/∼clia/index.html
</footnote>
<listItem confidence="0.99696375">
1. (car,gAdi,1.0)
2. (teacher,shikshak,0.62)
3. (teacher, vidyalaya,0.19)
4. (road, rAsta, 0.55)
</listItem>
<subsectionHeader confidence="0.997322">
3.2 The Algorithm
</subsectionHeader>
<bodyText confidence="0.999147625">
For extracting bilingual lexicon, we have not con-
sidered the function words of the two languages.
In order to filter out the function words, we have
made use of the assumption that content words
usually have low frequency in the corpus, whereas
function words have very high frequency. First,
we define some quantities:
Let the languages be E and H.
</bodyText>
<equation confidence="0.9567138125">
We = Set of words in E = {e1, e2, ...., eN}
Wh = Set of words in H = {h1, h2, ...., hM}
|We |= N
|Wh |= M
MAP = Initial map given
= {(ei, hj, wij)|wij = wt(ei, hj), ei ∈ We, hj ∈ Wh}
EM = Set of words in E which are included in
entries of MAP
HM = Set of words in H which are included in
entries of MAP
Co occ(x) = Set of words which co-occur with word x
Co oc(
c Co occ(x) ∩ EM if x ∈ We
(x) Co occ(x) ∩ HM if x ∈ Wh
Wte(x) = {Wey|y ∈ We and y ∈ Co occ(x)}
Wth(x) = {Why|y ∈ Wh and y ∈ Co occ(x)}
</equation>
<bodyText confidence="0.999424">
Given a comparable corpus, we follow the fol-
lowing steps of processing:
</bodyText>
<listItem confidence="0.9987554">
1. A sentence segmentation code is run to seg-
ment the corpus into sentences.
2. The sentence-segmented corpus is cleaned of
all punctuation marks and special symbols by
replacing them with spaces.
</listItem>
<page confidence="0.995935">
37
</page>
<table confidence="0.213333666666667">
Algorithm 1 Algorithm to Extract Bilingual Dictionary by using word Co-occurrence Information
repeat
for ei E We do
for hj E Wh do
if (ei, hj, 0) E MAP then
E E
e∈Co occ0(ei) h∈Co occ0(hj)(WijWeeiWhhj )
wt(ei, hj) =
end if
end for
end for
Select the pair with highest value of wt(ei, bj) and add it to the existing map and normalize
until termination
E E
e∈Co occ0(ei) h∈Co occ0(hj)(WeeiWhhj)
</table>
<listItem confidence="0.9547895625">
3. The collection frequency of all the terms are
computed and based on a threshold, the func-
tion words are filtered out.
4. The co-occurrence information is computed
at sentence-level for the remaining terms. In
a sentence, if words wi and wj both occur,
then wi E Co occ(wj) and vice versa.
5. Since we can visualize the co-occurrence in-
formation in the form of a graph, we next
cluster the graph into C clusters.
6. From each cluster Ci, we choose some fixed
number number of words and manually find
out their translation in the target language.
This constitutes the initial map.
7. Next we apply Algorithm 1 to compute the
word maps.
</listItem>
<bodyText confidence="0.999755333333333">
The time complexity of the algorithm is
O(IM2N2), where I is the number of itera-
tions of the algorithm.
</bodyText>
<subsectionHeader confidence="0.997939">
3.3 Corpus Construction
</subsectionHeader>
<bodyText confidence="0.999447846153846">
The corpora used for evaluating our algorithm
were derived from the FIRE 2010 English and
Hindi corpora for the ad-hoc retrieval task. These
corpora contained news articles spanning over a
time period of three years from two Indian news-
papers, “The Dainik Jagaran” in Hindi and “The
Telegraph” in English. However, due to the ex-
treme level of variation of the topics in these cor-
pora, we applied a filtering algorithm to select a
subset of the corpora.
Our approach to make the text similar involved
reducing the corora based on matching Named
Entities. Named Entities of English and Hindi
corpus were listed using LingPipe2 and a Hindi
NER system built at IIT Kharagpur(Saha et al.,
1999). The listed Named Entities of the two cor-
pora were compared to find the matching Named
Entities. Named Entities in Hindi Unicode were
converted to iTRANS3 format and matched with
English Named Entities using edit distance. Unit
cost was defined for each insert and delete opera-
tion. Similar sounding characters like ‘s’, ‘c’,‘a’,
‘e’ etc were assigned a replacement cost of 1 and
other characters were assigned a replacement cost
of 2. Two Named Entities were adjudged match-
ing if:
</bodyText>
<equation confidence="0.889982">
(2 * Cost)/(WLh + WLe) &lt; 0.5
</equation>
<bodyText confidence="0.9867748125">
where,
W Lh = Length of Hindi word
WLe = Length of English word
Using this matching scheme, accuracy of match-
ing of Hindi and English Named Entities was
found to be &gt; 95%. It was observed that there
are large number of Named Entities with small
frequency and few Named Entities with large fre-
quency. So a matching list was prepared which
contained only those Named Entities which had
frequency larger than a \IMaxFreq . This en-
sured that matching list had words with high fre-
quency in both corpus.So English words with fre-
quency larger than 368 and Hindi words with
frequency larger than 223 were considered for
matching. Based on this matching list, the two
</bodyText>
<footnote confidence="0.999841">
2http://alias-i.com/lingpipe/
3http://www.aczoom.com/itrans/
</footnote>
<page confidence="0.995855">
38
</page>
<table confidence="0.99973025">
Language Total NE Unique NE with freq NE Total No % of NE covered
NE larger than Matched of docs
.\/MaxFreq
According In the
to Zipf’s actual
Law corpus
Hindi 1195474 37606 686 360 54271 63.0% 74.3%
English 5723292 137252 2258 360 87387 65.2% 71.0%
</table>
<tableCaption confidence="0.997528">
Table 1: Statistics of the main corpora used for extraction
</tableCaption>
<table confidence="0.99048125">
Corpus Max Freq Max �MaxFreq
Word Freq
Hindi bharat 50072 223
English calcutta 135780 368
</table>
<tableCaption confidence="0.9667425">
Table 2: Criteria used for thresholding in the two
corpora
</tableCaption>
<table confidence="0.999938">
Matching Total documents in
% of corpora
NE per
document
Hindi English
&gt; 10% 34694 16950
&gt; 20% 14872 4927
&gt; 30% 2938 1650
</table>
<tableCaption confidence="0.999866">
Table 3: Statistics of extracted corpora
</tableCaption>
<bodyText confidence="0.999933066666667">
corpora were reduced by including only those files
each of which contained more than a certain fixed
percentage of total matching Named Entities. The
corpus statistics are provided in tables 1, 2 and 3.
We assume that distribution of Named Entities
follows Zipf’s law (Zipf, 1949). And analysis
shows that Named Entities with frequency greater
than the chosen threshold lead to high cover-
age both theoretically and in practice (Table 1).
Hence, the threshold was chosen as -\/MaxFreq.
The differences in the theoretical and actual val-
ues can be attributed to the poor performance of
the NER systems, especially the Hindi NER sys-
tem, whose output contained a number of false
positives.
</bodyText>
<subsectionHeader confidence="0.930132">
3.4 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999900421052632">
The languages we used for our experiments were
English and Hindi. English was the source lan-
guage and Hindi was chosen as the target. For
our experiments, we used a collection frequency
threshold of 400 to filter out the function words.
The words having a collection frequency more
than 400 were discarded. This threshold was ob-
tained manually by “Trial and Error” method in
order to perform an effective function word fil-
tering. For each corpora, we extracted the co-
occurrence information and then clustered the co-
occurrence graph into 20 clusters. From each
cluster we chose 15 words, thus giving us an over-
all initial seed dictionary size of 300. We ran the
algorithm for 3000 iterations.
For graph clustering, we used the Graclus system
(Dhillon et al., 2007) which uses a weighted ker-
nel k-means clustering algorithm at various levels
of coarseness of the input graph.
</bodyText>
<subsectionHeader confidence="0.985186">
3.5 Evaluation Method and Results
</subsectionHeader>
<bodyText confidence="0.999960333333333">
For evaluation, we have used the Accuracy and
MMR measure (Voorhees, 1999). The measures
are defined as follows:
</bodyText>
<equation confidence="0.955449444444444">
Accuracy = N EN 1 ti
where, ti =
1 if correct translation in top n
0 otherwise
MMR = 1 [&apos;�N 1
N Lei=1 ranks
ri if ri � n
where, ranki =
0 otherwise
</equation>
<bodyText confidence="0.906733">
n means top n evaluation
ri means rank of correct translation in top n ranking
N means total number of words used for evaluation
For our experiments, we have used:
</bodyText>
<page confidence="0.998722">
39
</page>
<table confidence="0.997889666666667">
Corpus Context Het- Co-
erogeneity occurrence
Acc MMR Acc MMR
&gt; 10% 0.14 0.112 0.16 0.135
&gt; 20% 0.21 0.205 0.27 0.265
&gt; 30% 0.31 0.285 0.35 0.333
</table>
<tableCaption confidence="0.927156333333333">
Table 4: Comparison of performance between
Context Heterogeneity and Co-occurrence Ap-
proach for manual evaluation
</tableCaption>
<equation confidence="0.9979435">
n = 5
N = 100
</equation>
<bodyText confidence="0.999801461538461">
The 100 words used for evaluation were chosen
randomly from the source language.
Two evaluation methods were followed - manual
and automated. In the manual evaluation, a
person who knows both English and Hindi was
asked to find the candidate translation in the target
language for the words in the source language.
Using this gold standard map, the Accuracy and
MMR values were computed.
In the second phase (automated), lexicon ex-
tracted is evaluated against English to Hindi
wordnet4. The evaluation process proceeds as
follows:
</bodyText>
<listItem confidence="0.998832111111111">
1. Hashmap is created with English words as
keys and Hindi meanings as values.
2. English words in the extracted lexicon are
crudely stemmed so that inflected words
match the root words in the dictionary. Stem-
ming is done by removing the last 4 charac-
ters, one at a time and checking if word found
in dictionary.
3. Accuracy and MMR are computed.
</listItem>
<bodyText confidence="0.999530714285714">
As a reference measure, we have used Fung’s
method of Context Heterogeneity with a context
window size of 4. The results are tabulated in
Tables 4 and 6. We can see that our proposed
algorithm shows a significant improvement over
the Context Heterogeneity method. The degree
of improvement over the Context Heterogeneity
</bodyText>
<footnote confidence="0.925341">
4Downloadable from
http://sanskritdocuments.org/hindi/dict/eng-hin-itrans.html
</footnote>
<table confidence="0.9981565">
Corpus Accuracy MMR
&gt; 10% ↑ 14.28% ↑ 20.53%
&gt; 20% ↑ 28.57% ↑ 29.27%
&gt; 30% ↑ 12.9% ↑ 16.84%
</table>
<tableCaption confidence="0.847142333333333">
Table 5: Degree of improvement shown by Co-
occurrence approach over Context Heterogeneity
for manual evaluation
</tableCaption>
<table confidence="0.999189666666667">
Corpus Context Het- Co-
erogeneity occurrence
Acc MMR Acc MMR
&gt; 10% 0.05 0.08 0.05 0.08
&gt; 20% 0.06 0.06 0.11 0.10
&gt; 30% 0.13 0.11 0.15 0.13
</table>
<tableCaption confidence="0.975780666666667">
Table 6: Comparison of performance between
Context Heterogeneity and Co-occurrence Ap-
proach for auto-evaluation
</tableCaption>
<bodyText confidence="0.9995312">
is summarized in Tables 5 and 7. For auto
evaluation, We see that the proposed approach
shows the maximum improvement (83.33% in
Accuracy and 66.67% in MMR) in performance
when the corpus size is medium. For very large
(too general) corpora, both the approaches give
identical result while for very small (too specific)
corpora, the proposed approach gives slightly
better results than the reference.
The trends are similar for manual evaluation.
Once again, the maximum improvement is
observed for the medium sized corpus (&gt; 20%).
However, in this evaluation system, the proposed
approach performs much better than the reference
even for the large (more general) corpora.
</bodyText>
<table confidence="0.992751">
Corpus Accuracy MMR
&gt; 10% 0.0% 0.0%
&gt; 20% ↑ 83.33% ↑ 66.67%
&gt; 30% ↑ 15.38% ↑ 18.18%
</table>
<tableCaption confidence="0.822173666666667">
Table 7: Degree of improvement shown by Co-
occurrence approach over Context Heterogeneity
for auto-evaluation
</tableCaption>
<page confidence="0.998603">
40
</page>
<sectionHeader confidence="0.999244" genericHeader="evaluation">
4 Discussion
</sectionHeader>
<bodyText confidence="0.99998823655914">
The co-occurrence based approach used in this
paper is quite a simple approach in the sense that
it does not make use of any kind of linguistic
information. From the aforementioned results
we can see that a model based on simple word
co-occurrence highly outperforms the “Context
Heterogeneity” model in almost all the cases.
One possible reason behind this is the amount of
information captured by our model is more than
that captured by the “Context Heterogeneity”
model. “Context Heterogeneity” does not model
actual word-word interactions. Each word is
represented by a function of the number of
different contexts it can occur in. However, we
represent the word by a co-occurrence vector.
This captures all possible contexts of the word.
Also, we can actually determine which are the
words which co-occur with any other word. So
our model captures more semantics of the word in
question than the “Context Heterogeneity” model,
thereby leading to better results. Another possible
factor is the nature in which we compute the
translation scores. Due to the iterative nature of
the algorithm and since we normalize after each
iteration, some of the word pairs that received
unduly high score in an earlier iteration end up
having a substantially low score. However, since
the “Context Heterogeneity” does only a single
pass over the set of words, it fails to tackle this
problem.
The seed dictionary plays an important role in
our algorithm. A good seed dictionary gives us
some initial information to work with. However,
since “Context Heterogeneity” does not use a
seed dictionary, it loses out on the amount of
information initially available to it. Since the seed
dictionary size for our approach is quite small,
it can be easily constructed manually. However,
how the seed dictionary size varies with corpus
size is an issue that remains to be seen.
Another important factor in our algorithm is the
way in which we have defined the co-occurrence
vectors. This is not the same as the context vector
that we define in case of Context Heterogeneity.
In a windowed context vector, we fail to capture a
lot of dependencies that might be captured using
a sentence-level co-occurrence. This problem is
especially more visible in case of free-word-order
languages like the Indo-European group of lan-
guages. For these languages, a windowed context
vector is also likely to introduce many spurious
dependencies. Since Hindi is a language of this
family, our algorithm captures many more correct
semantic dependencies than Context Heterogene-
ity algorithm, resulting in better preformance.
Another strong point of our proposed approach
is the closeness of the values of Accuracy and
MMR. This shows that the translation candidates
extracted by our algorithm are not only correct,
but also the best translation candidate gets the
highest score with high probability. This is a very
important factor in Machine Translation systems,
where a more accurate dictionary would give us
an improved performance.
A noticeable point about the evaluation scores is
the difference in scores given by the automated
system and the manual system. This can be
attributed to synonymy and spelling errors. In
the target language Hindi, synonymy plays a
very important part. It is not expected that all
synonyms of a particular word may be present
in an online dictionary. In such cases, the
manual evaluator marks a translation pair as
True, whereas the automated system marks it as
False. Instances of spelling errors have also been
found. For example, for the word ”neighbors”,
the top translation provided by the system was
”paDosana”(female neighbor). If we consider
root form of words, this is correct. But the actual
translation should be ”paDosiyAn”(neighbors,
may refer to both male and female). Thus the
auto evaluation system tags it as False, whereas
the manual evaluator tags it as True. There are
many more such occurrences throughout.
Apart from that, the manual evaluation process
has been quite relaxed. Even if the properties like
tense, number of words does not match, as long
as the root forms match the manual evaluator has
marked it as True. But this is not the case for
the automated evaluator. Although stemming has
been done, but problems still persist which can be
only solved by lemmatization, because Hindi is a
highly inflected language.
</bodyText>
<page confidence="0.999216">
41
</page>
<sectionHeader confidence="0.997714" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999892380952381">
In this paper we present a completely new ap-
proach for extracting bilingual lexicon from com-
parable corpora. We show the results of experi-
mentation on corpora of different levels of com-
parability. The basic feature of this approach is
that it is language independent and needs no ad-
ditional resource. We could not compare its per-
formance with the Dependency Heterogeneity al-
gorithm due to the lack of resources for Hindi.
So this can be taken up as a future work. Also,
the algorithm is quite inefficient. Another direc-
tion of research can be in trying to explore ways
to reduce the complexity of this algorithm. We
can also try to incorporate more linguistic infor-
mation into this model instead of just word co-
occurrence. It remains to be seen how these fac-
tors affect the performance of the algorithm. An-
other important question is what should be the size
of the seed dictionary for optimum performance
of the algorithm. This too can be taken up as a
future research direction.
</bodyText>
<sectionHeader confidence="0.999454" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999898137254902">
Dhillon, I., Y. Guan, and B. Kulis. 2007. Weighted
graph cuts without eigenvectors: A multilevel ap-
proach. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence (PAMI), 29:11:1944–
1957, November.
Fung, Pascale and Lo Yuen Yee. 1998. An ir ap-
proach for translating new words from nonparallel,
comparable texts. In Proceedings of the 36th An-
nual Meeting of the Association for Computational
Linguistics / the 17th International Conference on
Computational Linguistics, pages 414–420.
Fung, Pascale. 1995. Compiling bilingual lexicon
entries from a non-parallel english-chinese corpus.
In Third Annual Workshop on Very Large Corpora,
Boston, Massachusetts, June.
Grefenstette, G. 1998. The problem of cross-language
information retrieval. Cross-language Information
Retrieval.
Kaji, H. 2003. Word sense acquisition from bilingual
comparable corpora. In Proc. of HLT-NAACL 2003
Main papers, pages 32–39.
Kumano, T., H. Takana, and T. Tokunaga. 2007. Ex-
tracting phrasal alignments from comparable cor-
pora by using joint probability smt model. In Proc.
of TMI.
Och, F. and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19–51, March.
Rapp, Reinhard. 1995. Identifying word translations
in non-parallel texts. In Proc. of TMI.
Rapp, Reinhard. 1999. Automatic identification of
word translations from unrelated english and ger-
man corpora. In Proceedings of the 37th Annual
Meeting of the Association for Computational Lin-
guistics, pages 519–526.
Saha, Sujan Kumar, Sudeshna Sarkar, and Pabitra Mi-
tra. 1999. A hybrid feature set based maximum
entropy hindi named entity recognition. In Proceed-
ings of the Third International Joint Conference on
Natural Language Processing, pages 343–349, Hy-
derabad, India, January.
Voorhees, E.M. 1999. The trec-8 question answer-
ing track report. In Proceedings of the 81h Text Re-
trieval Conference.
Yuu, K. and J. Tsujii. 2009. Extracting bilingual dic-
tionary from comparable corpora with dependency
heterogeneity. In Proc. of NAACL-HLT, short pa-
pers, pages 121–124.
Zipf, George Kingsley. 1949. Human Behaviour and
the Principle of Least Effort: an Introduction to Hu-
man Ecology. Addison-Wesley.
</reference>
<page confidence="0.999297">
42
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.906237">
<title confidence="0.993814">Co-occurrence Graph Based Iterative Bilingual Lexicon Extraction Comparable Corpora</title>
<author confidence="0.998526">Chatterjee Sarkar</author>
<affiliation confidence="0.99316">Department of Computer Science and Indian Institute of Technology Kharagpur</affiliation>
<abstract confidence="0.99403475">This paper presents an iterative algorithm for bilingual lexicon extraction from comparable corpora. It is based on a bagof-words model generated at the level of sentences. We present our results of experimentation on corpora of multiple degrees of comparability derived from the FIRE 2010 dataset. Evaluation results on 100 nouns shows that this method outperforms the standard context-vector based approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>I Dhillon</author>
<author>Y Guan</author>
<author>B Kulis</author>
</authors>
<title>Weighted graph cuts without eigenvectors: A multilevel approach.</title>
<date>2007</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI),</journal>
<volume>29</volume>
<contexts>
<context position="15805" citStr="Dhillon et al., 2007" startWordPosition="2594" endWordPosition="2597">experiments, we used a collection frequency threshold of 400 to filter out the function words. The words having a collection frequency more than 400 were discarded. This threshold was obtained manually by “Trial and Error” method in order to perform an effective function word filtering. For each corpora, we extracted the cooccurrence information and then clustered the cooccurrence graph into 20 clusters. From each cluster we chose 15 words, thus giving us an overall initial seed dictionary size of 300. We ran the algorithm for 3000 iterations. For graph clustering, we used the Graclus system (Dhillon et al., 2007) which uses a weighted kernel k-means clustering algorithm at various levels of coarseness of the input graph. 3.5 Evaluation Method and Results For evaluation, we have used the Accuracy and MMR measure (Voorhees, 1999). The measures are defined as follows: Accuracy = N EN 1 ti where, ti = 1 if correct translation in top n 0 otherwise MMR = 1 [&apos;�N 1 N Lei=1 ranks ri if ri � n where, ranki = 0 otherwise n means top n evaluation ri means rank of correct translation in top n ranking N means total number of words used for evaluation For our experiments, we have used: 39 Corpus Context Het- Coeroge</context>
</contexts>
<marker>Dhillon, Guan, Kulis, 2007</marker>
<rawString>Dhillon, I., Y. Guan, and B. Kulis. 2007. Weighted graph cuts without eigenvectors: A multilevel approach. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 29:11:1944– 1957, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Lo Yuen Yee</author>
</authors>
<title>An ir approach for translating new words from nonparallel, comparable texts.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics / the 17th International Conference on Computational Linguistics,</booktitle>
<pages>414--420</pages>
<contexts>
<context position="5679" citStr="Fung and Yee, 1998" startWordPosition="886" endWordPosition="889"> be translated. Euclidean distance between context vectors was used as a similarity measure. Another approach used Distributed Clustering of Translational Equivalents for word sense acquisition from bilingual comparable corpora (Kaji, 2003). However, the major drawback of this paper is the assumption that translation equivalents usually represent only one sense of the target word. This may not be the case for languages having similar origin, for example, Hindi and Bengali. Approaches using context information for extracting lexical translations from comparable corpora have also been proposed (Fung and Yee, 1998; Rapp, 1999). But they resulted in very poor coverage. These approaches were improved upon by extracting phrasal alignments from comparable corpora using joint probability SMT model (Kumano et al., 2007) . Another proposed method uses dependency parsing and Dependency Heterogeneity for extracting bilingual lexicon (Yuu and Tsujii, 2009) . This approach was similar to that of Fung, except they used a dependency parser to get the tags for each word and depending on the frequency of each tag they defined a vector to represent each word in question. Here too, Euclidean similarity was used to comp</context>
</contexts>
<marker>Fung, Yee, 1998</marker>
<rawString>Fung, Pascale and Lo Yuen Yee. 1998. An ir approach for translating new words from nonparallel, comparable texts. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics / the 17th International Conference on Computational Linguistics, pages 414–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
</authors>
<title>Compiling bilingual lexicon entries from a non-parallel english-chinese corpus.</title>
<date>1995</date>
<booktitle>In Third Annual Workshop on Very Large Corpora,</booktitle>
<location>Boston, Massachusetts,</location>
<contexts>
<context position="3816" citStr="Fung, 1995" startWordPosition="588" endWordPosition="589">ke Hindi and Bengali, we still do not have good enough dependency parsers. In this paper, we propose a word co-occurrence based approach for lexicon extraction from comparable corpora using English and Hindi as the source and target languages respectively. We do not use any language35 Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 35–42, Beijing, August 2010 specific resource in our approach. We did experiments with 100 words in English,and show that our approach performs significantly better than the the Context Heterogeneity approach (Fung, 1995). We show the results over corpora with varying degrees of comparability. The outline of the paper is as follows. In section 2, we analyze the different approaches for lexicon extraction from comparable corpora. In section 3, we present our algorithm and the experimental results. In section 4, we present an analysis of the results followed by the conclusion and future research directions in section 5. 2 Previous Work One of the first works in the area of comparable corpora mining was based on word co-occurrence based approach (Rapp, 1995). The basic assumption behind this approach was two word</context>
</contexts>
<marker>Fung, 1995</marker>
<rawString>Fung, Pascale. 1995. Compiling bilingual lexicon entries from a non-parallel english-chinese corpus. In Third Annual Workshop on Very Large Corpora, Boston, Massachusetts, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>The problem of cross-language information retrieval. Cross-language Information Retrieval.</title>
<date>1998</date>
<contexts>
<context position="1096" citStr="Grefenstette, 1998" startWordPosition="153" endWordPosition="154">sults of experimentation on corpora of multiple degrees of comparability derived from the FIRE 2010 dataset. Evaluation results on 100 nouns shows that this method outperforms the standard context-vector based approaches. 1 Introduction Bilingual dictionaries play a pivotal role in a number of Natural Language Processing tasks like Machine Translation and Cross Lingual Information Retrieval(CLIR). Machine Translation systems often use bilingual dictionaries in order to augment word and phrase alignment (Och and Ney, 2003). CLIR systems use bilingual dictionaries in the query translation step (Grefenstette, 1998). However, high coverage electronic bilingual dictionaries are not available for all language pairs. So a major research area in Machine Translation and CLIR is bilingual dictionary extraction. The most common approach for extracting bilingual dictionary is applying some statistical alignment algorithm on a parallel corpus. However, parallel corpora are not readily available for most language pairs. Also, it takes a lot of effort to actually get the accurate translations of sentences. Hence, constructing parallel corpora involves a lot of effort and time. So in recent years, extracting bilingu</context>
</contexts>
<marker>Grefenstette, 1998</marker>
<rawString>Grefenstette, G. 1998. The problem of cross-language information retrieval. Cross-language Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kaji</author>
</authors>
<title>Word sense acquisition from bilingual comparable corpora.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL 2003 Main papers,</booktitle>
<pages>32--39</pages>
<contexts>
<context position="5301" citStr="Kaji, 2003" startWordPosition="829" endWordPosition="830"> However this approach did not yield significantly good results. The “Context Heterogeneity” approach was one of the pioneering works in this area. It uses a 2- dimensional context vector for each word based on the right and left context. The context vector depended on how many distinct words occur in the particular context and also the unigram frequency of the word to be translated. Euclidean distance between context vectors was used as a similarity measure. Another approach used Distributed Clustering of Translational Equivalents for word sense acquisition from bilingual comparable corpora (Kaji, 2003). However, the major drawback of this paper is the assumption that translation equivalents usually represent only one sense of the target word. This may not be the case for languages having similar origin, for example, Hindi and Bengali. Approaches using context information for extracting lexical translations from comparable corpora have also been proposed (Fung and Yee, 1998; Rapp, 1999). But they resulted in very poor coverage. These approaches were improved upon by extracting phrasal alignments from comparable corpora using joint probability SMT model (Kumano et al., 2007) . Another propose</context>
</contexts>
<marker>Kaji, 2003</marker>
<rawString>Kaji, H. 2003. Word sense acquisition from bilingual comparable corpora. In Proc. of HLT-NAACL 2003 Main papers, pages 32–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kumano</author>
<author>H Takana</author>
<author>T Tokunaga</author>
</authors>
<title>Extracting phrasal alignments from comparable corpora by using joint probability smt model.</title>
<date>2007</date>
<booktitle>In Proc. of TMI.</booktitle>
<contexts>
<context position="5883" citStr="Kumano et al., 2007" startWordPosition="917" endWordPosition="921">ingual comparable corpora (Kaji, 2003). However, the major drawback of this paper is the assumption that translation equivalents usually represent only one sense of the target word. This may not be the case for languages having similar origin, for example, Hindi and Bengali. Approaches using context information for extracting lexical translations from comparable corpora have also been proposed (Fung and Yee, 1998; Rapp, 1999). But they resulted in very poor coverage. These approaches were improved upon by extracting phrasal alignments from comparable corpora using joint probability SMT model (Kumano et al., 2007) . Another proposed method uses dependency parsing and Dependency Heterogeneity for extracting bilingual lexicon (Yuu and Tsujii, 2009) . This approach was similar to that of Fung, except they used a dependency parser to get the tags for each word and depending on the frequency of each tag they defined a vector to represent each word in question. Here too, Euclidean similarity was used to compute the similarity between two words using their context vectors. However, this method is dependent on availability of a dependency parser for the languages and is not feasible for languages for which res</context>
</contexts>
<marker>Kumano, Takana, Tokunaga, 2007</marker>
<rawString>Kumano, T., H. Takana, and T. Tokunaga. 2007. Extracting phrasal alignments from comparable corpora by using joint probability smt model. In Proc. of TMI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="1004" citStr="Och and Ney, 2003" startWordPosition="138" endWordPosition="141">. It is based on a bagof-words model generated at the level of sentences. We present our results of experimentation on corpora of multiple degrees of comparability derived from the FIRE 2010 dataset. Evaluation results on 100 nouns shows that this method outperforms the standard context-vector based approaches. 1 Introduction Bilingual dictionaries play a pivotal role in a number of Natural Language Processing tasks like Machine Translation and Cross Lingual Information Retrieval(CLIR). Machine Translation systems often use bilingual dictionaries in order to augment word and phrase alignment (Och and Ney, 2003). CLIR systems use bilingual dictionaries in the query translation step (Grefenstette, 1998). However, high coverage electronic bilingual dictionaries are not available for all language pairs. So a major research area in Machine Translation and CLIR is bilingual dictionary extraction. The most common approach for extracting bilingual dictionary is applying some statistical alignment algorithm on a parallel corpus. However, parallel corpora are not readily available for most language pairs. Also, it takes a lot of effort to actually get the accurate translations of sentences. Hence, constructin</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Och, F. and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Identifying word translations in non-parallel texts.</title>
<date>1995</date>
<booktitle>In Proc. of TMI.</booktitle>
<contexts>
<context position="2902" citStr="Rapp, 1995" startWordPosition="447" endWordPosition="448">a. But after careful study of news articles in English and Hindi published on same days at the same city, we have observed that along with articles on similar topics, the corpora also contain a lot of articles which have no topical similarity. Thus, the corpora are quite noisy, which makes it unsuitable for lexicon extraction. Thus another important factor in comparable corpora construction is the degree of similarity of the corpora. Approaches for lexicon extraction from comparable corpora have been proposed that use the bagof-words model to find words that occur in similar lexical contexts (Rapp, 1995). There have been approaches proposed which improve upon this model by using some linguistic information (Yuu and Tsujii, 2009). However, these require some linguistic tool like dependency parsers which are not commonly obtainable for resource-poor languages. For example, in case of Indian languages like Hindi and Bengali, we still do not have good enough dependency parsers. In this paper, we propose a word co-occurrence based approach for lexicon extraction from comparable corpora using English and Hindi as the source and target languages respectively. We do not use any language35 Proceedings</context>
<context position="4360" citStr="Rapp, 1995" startWordPosition="679" endWordPosition="680">tly better than the the Context Heterogeneity approach (Fung, 1995). We show the results over corpora with varying degrees of comparability. The outline of the paper is as follows. In section 2, we analyze the different approaches for lexicon extraction from comparable corpora. In section 3, we present our algorithm and the experimental results. In section 4, we present an analysis of the results followed by the conclusion and future research directions in section 5. 2 Previous Work One of the first works in the area of comparable corpora mining was based on word co-occurrence based approach (Rapp, 1995). The basic assumption behind this approach was two words are likely to occur together in the same context if their joint probability of occurrence in a corpus exceeds the probability that the words occur randomly. In his paper, Rapp made use of a similarity matrix and using a joint probability estimate determined the word maps. However this approach did not yield significantly good results. The “Context Heterogeneity” approach was one of the pioneering works in this area. It uses a 2- dimensional context vector for each word based on the right and left context. The context vector depended on </context>
</contexts>
<marker>Rapp, 1995</marker>
<rawString>Rapp, Reinhard. 1995. Identifying word translations in non-parallel texts. In Proc. of TMI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Automatic identification of word translations from unrelated english and german corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>519--526</pages>
<contexts>
<context position="5692" citStr="Rapp, 1999" startWordPosition="890" endWordPosition="891">idean distance between context vectors was used as a similarity measure. Another approach used Distributed Clustering of Translational Equivalents for word sense acquisition from bilingual comparable corpora (Kaji, 2003). However, the major drawback of this paper is the assumption that translation equivalents usually represent only one sense of the target word. This may not be the case for languages having similar origin, for example, Hindi and Bengali. Approaches using context information for extracting lexical translations from comparable corpora have also been proposed (Fung and Yee, 1998; Rapp, 1999). But they resulted in very poor coverage. These approaches were improved upon by extracting phrasal alignments from comparable corpora using joint probability SMT model (Kumano et al., 2007) . Another proposed method uses dependency parsing and Dependency Heterogeneity for extracting bilingual lexicon (Yuu and Tsujii, 2009) . This approach was similar to that of Fung, except they used a dependency parser to get the tags for each word and depending on the frequency of each tag they defined a vector to represent each word in question. Here too, Euclidean similarity was used to compute the simil</context>
</contexts>
<marker>Rapp, 1999</marker>
<rawString>Rapp, Reinhard. 1999. Automatic identification of word translations from unrelated english and german corpora. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 519–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujan Kumar Saha</author>
<author>Sudeshna Sarkar</author>
<author>Pabitra Mitra</author>
</authors>
<title>A hybrid feature set based maximum entropy hindi named entity recognition.</title>
<date>1999</date>
<booktitle>In Proceedings of the Third International Joint Conference on Natural Language Processing,</booktitle>
<pages>343--349</pages>
<location>Hyderabad, India,</location>
<contexts>
<context position="12478" citStr="Saha et al., 1999" startWordPosition="2031" endWordPosition="2034">E 2010 English and Hindi corpora for the ad-hoc retrieval task. These corpora contained news articles spanning over a time period of three years from two Indian newspapers, “The Dainik Jagaran” in Hindi and “The Telegraph” in English. However, due to the extreme level of variation of the topics in these corpora, we applied a filtering algorithm to select a subset of the corpora. Our approach to make the text similar involved reducing the corora based on matching Named Entities. Named Entities of English and Hindi corpus were listed using LingPipe2 and a Hindi NER system built at IIT Kharagpur(Saha et al., 1999). The listed Named Entities of the two corpora were compared to find the matching Named Entities. Named Entities in Hindi Unicode were converted to iTRANS3 format and matched with English Named Entities using edit distance. Unit cost was defined for each insert and delete operation. Similar sounding characters like ‘s’, ‘c’,‘a’, ‘e’ etc were assigned a replacement cost of 1 and other characters were assigned a replacement cost of 2. Two Named Entities were adjudged matching if: (2 * Cost)/(WLh + WLe) &lt; 0.5 where, W Lh = Length of Hindi word WLe = Length of English word Using this matching sche</context>
</contexts>
<marker>Saha, Sarkar, Mitra, 1999</marker>
<rawString>Saha, Sujan Kumar, Sudeshna Sarkar, and Pabitra Mitra. 1999. A hybrid feature set based maximum entropy hindi named entity recognition. In Proceedings of the Third International Joint Conference on Natural Language Processing, pages 343–349, Hyderabad, India, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>The trec-8 question answering track report.</title>
<date>1999</date>
<booktitle>In Proceedings of the 81h Text Retrieval Conference.</booktitle>
<contexts>
<context position="16024" citStr="Voorhees, 1999" startWordPosition="2631" endWordPosition="2632">thod in order to perform an effective function word filtering. For each corpora, we extracted the cooccurrence information and then clustered the cooccurrence graph into 20 clusters. From each cluster we chose 15 words, thus giving us an overall initial seed dictionary size of 300. We ran the algorithm for 3000 iterations. For graph clustering, we used the Graclus system (Dhillon et al., 2007) which uses a weighted kernel k-means clustering algorithm at various levels of coarseness of the input graph. 3.5 Evaluation Method and Results For evaluation, we have used the Accuracy and MMR measure (Voorhees, 1999). The measures are defined as follows: Accuracy = N EN 1 ti where, ti = 1 if correct translation in top n 0 otherwise MMR = 1 [&apos;�N 1 N Lei=1 ranks ri if ri � n where, ranki = 0 otherwise n means top n evaluation ri means rank of correct translation in top n ranking N means total number of words used for evaluation For our experiments, we have used: 39 Corpus Context Het- Coerogeneity occurrence Acc MMR Acc MMR &gt; 10% 0.14 0.112 0.16 0.135 &gt; 20% 0.21 0.205 0.27 0.265 &gt; 30% 0.31 0.285 0.35 0.333 Table 4: Comparison of performance between Context Heterogeneity and Co-occurrence Approach for manual</context>
</contexts>
<marker>Voorhees, 1999</marker>
<rawString>Voorhees, E.M. 1999. The trec-8 question answering track report. In Proceedings of the 81h Text Retrieval Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yuu</author>
<author>J Tsujii</author>
</authors>
<title>Extracting bilingual dictionary from comparable corpora with dependency heterogeneity.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL-HLT, short papers,</booktitle>
<pages>121--124</pages>
<contexts>
<context position="3029" citStr="Yuu and Tsujii, 2009" startWordPosition="464" endWordPosition="467">erved that along with articles on similar topics, the corpora also contain a lot of articles which have no topical similarity. Thus, the corpora are quite noisy, which makes it unsuitable for lexicon extraction. Thus another important factor in comparable corpora construction is the degree of similarity of the corpora. Approaches for lexicon extraction from comparable corpora have been proposed that use the bagof-words model to find words that occur in similar lexical contexts (Rapp, 1995). There have been approaches proposed which improve upon this model by using some linguistic information (Yuu and Tsujii, 2009). However, these require some linguistic tool like dependency parsers which are not commonly obtainable for resource-poor languages. For example, in case of Indian languages like Hindi and Bengali, we still do not have good enough dependency parsers. In this paper, we propose a word co-occurrence based approach for lexicon extraction from comparable corpora using English and Hindi as the source and target languages respectively. We do not use any language35 Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 35–42, Beijing, August 2010 specif</context>
<context position="6018" citStr="Yuu and Tsujii, 2009" startWordPosition="937" endWordPosition="940">ly represent only one sense of the target word. This may not be the case for languages having similar origin, for example, Hindi and Bengali. Approaches using context information for extracting lexical translations from comparable corpora have also been proposed (Fung and Yee, 1998; Rapp, 1999). But they resulted in very poor coverage. These approaches were improved upon by extracting phrasal alignments from comparable corpora using joint probability SMT model (Kumano et al., 2007) . Another proposed method uses dependency parsing and Dependency Heterogeneity for extracting bilingual lexicon (Yuu and Tsujii, 2009) . This approach was similar to that of Fung, except they used a dependency parser to get the tags for each word and depending on the frequency of each tag they defined a vector to represent each word in question. Here too, Euclidean similarity was used to compute the similarity between two words using their context vectors. However, this method is dependent on availability of a dependency parser for the languages and is not feasible for languages for which resources are scarce. 3 Bilingual Dictionary Extraction Using Co-occurrence Information 3.1 Motivation The Context Heterogeneity and Depen</context>
</contexts>
<marker>Yuu, Tsujii, 2009</marker>
<rawString>Yuu, K. and J. Tsujii. 2009. Extracting bilingual dictionary from comparable corpora with dependency heterogeneity. In Proc. of NAACL-HLT, short papers, pages 121–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Kingsley Zipf</author>
</authors>
<title>Human Behaviour and the Principle of Least Effort: an Introduction to Human Ecology.</title>
<date>1949</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="14614" citStr="Zipf, 1949" startWordPosition="2397" endWordPosition="2398">sed for extraction Corpus Max Freq Max �MaxFreq Word Freq Hindi bharat 50072 223 English calcutta 135780 368 Table 2: Criteria used for thresholding in the two corpora Matching Total documents in % of corpora NE per document Hindi English &gt; 10% 34694 16950 &gt; 20% 14872 4927 &gt; 30% 2938 1650 Table 3: Statistics of extracted corpora corpora were reduced by including only those files each of which contained more than a certain fixed percentage of total matching Named Entities. The corpus statistics are provided in tables 1, 2 and 3. We assume that distribution of Named Entities follows Zipf’s law (Zipf, 1949). And analysis shows that Named Entities with frequency greater than the chosen threshold lead to high coverage both theoretically and in practice (Table 1). Hence, the threshold was chosen as -\/MaxFreq. The differences in the theoretical and actual values can be attributed to the poor performance of the NER systems, especially the Hindi NER system, whose output contained a number of false positives. 3.4 Experimental Setup The languages we used for our experiments were English and Hindi. English was the source language and Hindi was chosen as the target. For our experiments, we used a collect</context>
</contexts>
<marker>Zipf, 1949</marker>
<rawString>Zipf, George Kingsley. 1949. Human Behaviour and the Principle of Least Effort: an Introduction to Human Ecology. Addison-Wesley.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>