<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.999034">
Encoding Lexicalized Tree Adjoining Grammars with a
Nonmonotonic Inheritance Hierarchy
</title>
<author confidence="0.956965">
Roger Evans
</author>
<affiliation confidence="0.918496333333333">
Information Technology
Research Institute
University of Brighton
</affiliation>
<email confidence="0.816357">
rpeOitri.bton.ac.uk
</email>
<author confidence="0.985005">
Gerald Gazdar
</author>
<affiliation confidence="0.971425333333333">
School of Cognitive &amp;
Computing Sciences
University of Sussex
</affiliation>
<email confidence="0.80123">
geraldgecogs.susx.ac.uk
</email>
<author confidence="0.992783">
David Weir
</author>
<affiliation confidence="0.974633666666667">
School of Cognitive &amp;
Computing Sciences
University of Sussex
</affiliation>
<email confidence="0.837012">
davidwOcogs.susx.ac.uk
</email>
<sectionHeader confidence="0.968281" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999677909090909">
This paper shows how DATR, a widely used
formal language for lexical knowledge re-
presentation, can be used to define an LTAG
lexicon as an inheritance hierarchy with in-
ternal lexical rules. A bottom-up featu-
ral encoding is used for LTAG trees and
this allows lexical rules to be implemen-
ted as covariation constraints within fea-
ture structures. Such an approach elimina-
tes the considerable redundancy otherwise
associated with an LTAG lexicon.
</bodyText>
<sectionHeader confidence="0.995236" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.971394833333333">
The Tree Adjoining Grammar (TAG) formalism was
first introduced two decades ago (Joshi et al., 1975),
and since then there has been a steady stream of
theoretical work using the formalism. But it is
only more recently that grammars of non-trivial size
have been developed: Abeille, Bishop, Cote &amp; Scha-
bes (1990) describe a feature-based Lexicalized Tree
Adjoining Grammar (LTAG) for English which sub-
sequently became the basis for the grammar used in
the XTAG system, a wide-coverage LTAG parser (Do-
ran et al., 1994b; Doran et al., 1994a; XTAG Rese-
arch Group, 1995). The advent of such large gram-
mars gives rise to questions of efficient representa-
tion, and the fully lexicalized character of the LTAG
formalism suggests that recent research into lexical
representation might be a place to look for answers
(see for example Briscoe et a/.(1993); Daelemans &amp;
Gazdar(1992)). In this paper we explore this sugge-
stion by showing how the lexical knowledge repre-
sentation language (LKRL) DATR (Evans &amp; Gazdar,
1989a; Evans &amp; Gazdar, 1989b) can be used to for-
mulate a compact, hierarchical encoding of an LTAG.
The issue of efficient representation for LTAG&apos; is
discussed by Vijay-Shanker &amp; Schabes (1992), who
</bodyText>
<footnote confidence="0.78161575">
lAs with all fully lexicalized grammar formalisms,
there is really no conceptual distinction to be drawn in
LTAG between the lexicon and the grammar: the gram-
matical rules are just lexical properties.
</footnote>
<bodyText confidence="0.999217">
draw attention to the considerable redundancy in-
herent in LTAG lexicons that are expressed in a flat
manner with no sharing of structure or properties
across the elementary trees. For example, XTAG cur-
rently includes over 100,000 lexemes, each of which
is associated with a family of trees (typically around
20) drawn from a set of over 500 elementary trees.
Many of these trees have structure in common, many
of the lexemes have the same tree families, and many
of the trees within families are systematically rela-
ted in ways which other formalisms capture using
transformations or metarules. However, the LTAG
formalism itself does not provide any direct support
for capturing such regularities.
Vijay-Shanker &amp; Schabes address this problem by
introducing a hierarchical lexicon structure with mo-
notonic inheritance and lexical rules, using an ap-
proach loosely based on that of Flickinger (1987)
but tailored for LTAG trees rather than HPSG sub-
categorization lists. Becker (1993; 1994) proposes a
slightly different solution, combining an inheritance
component and a set of metarules2. We share their
perception of the problem and agree that adopting
a hierarchical approach provides the best available
solution to it. However, rather than creating a hier-
archical lexical formalism that is specific to the LTAG
problem, we have used DATR, an LKRL that is al-
ready quite widely known and used. From an LTAG
perspective, it makes sense to use an already availa-
ble LKRL that was specifically designed to address
these kinds of representational issues. From a DATR
perspective, LTAG presents interesting problems ari-
sing from its radically lexicalist character: all gram-
matical relations, including unbounded dependency
constructions, are represented lexically and are thus
open to lexical generalization.
There are also several further benefits to be gai-
ned from using an established general purpose LKRL
such as DATR. First, it makes it easier to compare
the resulting LTAG lexicon with those associated with
other types of lexical syntax: there are existing DATR
</bodyText>
<note confidence="0.395798">
2 See Section 6 for further discussion of these
approaches.
</note>
<page confidence="0.998374">
77
</page>
<bodyText confidence="0.999932875">
lexicon fragments for FIPSG, PATR and Word Gram-
mar, among others. Second, DATR is not restricted
to syntactic description, so one can take advantage
of existing analyses of other levels of lexical descrip-
tion, such as phonology, prosody, morphology, com-
positional semantics and lexical semantics&apos;. Third,
one can exploit existing formal and implementation
work on the language4.
</bodyText>
<sectionHeader confidence="0.967667" genericHeader="method">
2 Representing LTAG trees
</sectionHeader>
<figure confidence="0.844028333333333">
NPj VP
Vo N131 PP
Po NPI
</figure>
<figureCaption confidence="0.999665">
Figure 1: An example LTAG tree for give
</figureCaption>
<bodyText confidence="0.999764384615385">
The principal unit of (syntactic) information asso-
ciated with an LTAG entry is a tree structure in which
the tree nodes are labeled with syntactic categories
and feature information and there is at least one
leaf node labeled with a lexical category (such lexi-
cal leaf nodes are known as anchors). For example,
the canonical tree for a ditransitive verb such as give
is shown in figure 1. Following LTAG conventions
(for the time being), the node labels here are gross
syntactic category specifications to which additional
featural information may be addeds, and are anno-
tated to indicate node type: o indicates an anchor
node, and j indicates a substitution node (where a
</bodyText>
<footnote confidence="0.8383799375">
3See, for example, Bleiching (1992; 1994), Brown 84
Hippisley (1994), Corbett 84 Fraser (1993), Cahill (1990;
1993), Cahill 84 Evans (1990), Fraser 8,4 Corbett (in
press), Gibbon (1992), Kilgarriff (1993), Kilgarriff 84
Gazdar (1995), Reinhard 84 Gibbon (1991).
&apos;See, for example, Andry et al. (1992) on compila-
tion, Kilbury et al. (1991) on coding DAGs, Duda 84 Geb-
hardi (1994) on dynamic querying, Langer (1994) on re-
verse querying, and Barg (1994), Light (1994), Light et
al. (1993) and Kilbury et al. (1994) on automatic ac-
quisition. And there are at least a dozen different DATR
implementations available, on various platforms and pro-
gramming languages.
51n fact, LTAG commonly distinguishes two sets of
features at each node (top and bottom), but for simpli-
city we shall assume just one set in this paper.
</footnote>
<bodyText confidence="0.994870444444445">
fully specified tree with a compatible root label may
be attachedr
In representing such a tree in DATR, we do two
things. First, in keeping with the radically lexica-
list character of LTAG, we describe the tree structure
from its (lexical) anchor upwards7, using a variant
of Kilbury&apos;s (1990) bottom-up encoding of trees. In
this encoding, a tree is described relative to a parti-
cular distinguished leaf node (here the anchor node),
using binary relations parent, left and right, re-
lating the node to the subtrees associated with its
parent, and immediate-left and -right sisters, enco-
ded in the same way. Second, we embed the resulting
tree structure (i.e., the node relations and type in-
formation) in the feature structure, so that the tree
relations (left, right and parent) become features.
The obvious analogy here is the use of first/rest
features to encode subcategorisation lists in frame-
works like HPSG.
Thus the syntactic feature information directly as-
sociated with the entry for give relates to the label
for the v node (for example, the value of its cat fea-
ture is v, the value of type is anchor), while speci-
fications of subfeatures of parent relate to the label
of the VP node. A simple bottom-up DATR represen-
tation for the whole tree (apart from the node type
information) follows:
</bodyText>
<equation confidence="0.90744975">
Give:
&lt;cat&gt; = v
&lt;parent cat&gt; = vp
&lt;parent left cat&gt; = np
</equation>
<bodyText confidence="0.93783685">
&lt;parent parent cat&gt;
&lt;right cat&gt; = np
&lt;right right cat&gt; =
&lt;right right parent
&lt;right right right cat&gt;
This says that Give is a verb, with VP as its pa-
rent, an s as its grandparent and an NP to the left
of its parent. It also has an NP to its right, and a
tree rooted in a P to the right of that, with a PP
parent and NP right sister. The implied bottom-up
tree structure is shown graphically in figure 2. Here
the nodes are laid out just as in figure 1, but rela-
ted via parent, left and right links, rather than
the more usual (implicitly ordered) daughter links.
Notice in particular that the right link from the
object noun-phrase node points to the preposition
node, not its phrasal parent — this whole subtree is
itself encoded bottom-up. Nevertheless, the full tree
structure is completely and accurately represented
by this encoding.
</bodyText>
<footnote confidence="0.980907111111111">
6 LTAG&apos;s other tree-building operation is adjunction,
which allows a tree-fragment to be spliced into the body
of a tree. However, we only need to concern ourselves
here with the representation of the trees involved, not
with the substitution/adjunction distinction.
7The tree in figure 1 has more than one anchor — in
such cases it is generally easy to decide which anchor is
the most appropriate root for the tree (here, the verb
anchor).
</footnote>
<equation confidence="0.9296465">
S
cat&gt;
PP
= np.
</equation>
<page confidence="0.985575">
78
</page>
<bodyText confidence="0.9999724375">
Once we adopt this representational strategy, wri-
ting an LTAG lexicon in DATR becomes similar to
writing any other type of lexicalist grammar&apos;s le-
xicon in an inheritance-based LKRL. In HPSG, for
example, the subcategorisation frames are coded as
lists of categories, whilst in LTAG they are coded as
trees. But, in both cases, the problem is one of con-
cisely describing feature structures associated with
lexical entries and relationships between lexical ent-
ries. The same kinds of generalization arise and the
same techniques are applicable. Of course, the pre-
sence of complete trees and the fully lexicalized ap-
proach provide scope for capturing generalizations
lexically that are not available to approaches that
only identify parent and sibling nodes, say, in the
lexical entries.
</bodyText>
<sectionHeader confidence="0.792316" genericHeader="method">
3 Encoding lexical entries
</sectionHeader>
<bodyText confidence="0.99799845">
Following conventional models of lexicon organisa-
tion, we would expect Give to have a minimal syn-
tactic specification itself, since syntactically it is a
completely regular ditransitive verb. In fact none
of the information introduced so far is specific to
Give. So rather than providing a completely expli-
cit DATR definition for Give, as we did above, a more
plausible account uses an inheritance hierarchy defi-
ning abstract intransitive, transitive and ditransitive
verbs to support Give (among others), as shown in
figure 3.
This basic organisational structure can be expres-
sed as the following DATR fragments:
8To gain the intuitive sense of this fragment, read
a line such as &lt;&gt; == VERB as &amp;quot;inherit everything from
the definition of VERB&amp;quot;, and a line such as &lt;parent&gt; ==
PPTREE:&lt;&gt; as &amp;quot;inherit the parent subtree from the de-
finition of PPTREE&amp;quot;. Inheritance in DATR is always by
default — locally defined feature specifications take prio-
rity over inherited ones.
</bodyText>
<figure confidence="0.97687225">
VERB
Die VERB+NP
Eat VERB+NP+PP VERB+NP+NP
Give Spare
</figure>
<figureCaption confidence="0.999139">
Figure 3: The principal lexical hierarchy
</figureCaption>
<equation confidence="0.67776862962963">
VERB:
&lt;&gt; == TREENODE
&lt;cat&gt; == v
&lt;type&gt; == anchor
&lt;parent&gt; == VPTREE:&lt;&gt;.
VERB+NP:
&lt;&gt; == VERB
&lt;right&gt; == NPCOMP:&lt;&gt;.
VERB+NP+PP:
&lt;&gt; == VERB+NP
&lt;right right&gt; == PTREE:&lt;&gt;
&lt;right right root&gt; == to.
VERB+NP+NP:
&lt;&gt; == VERB+NP
&lt;right right&gt; == NPCOMP:&lt;&gt;.
Die: VERB
&lt;&gt; =Zt
&lt;root&gt; == die.
Eat:
&lt;&gt; == VERB+NP
&lt;root&gt; == eat.
Give:
&lt;&gt; == VERB+NP+PP
&lt;root&gt; == give.
Spare:
&lt;&gt; == VERB+NP+NP
&lt;root&gt; == spare.
</equation>
<bodyText confidence="0.99973275">
Ignoring for the moment the references to
TREENODE, VPTREE, NPCOMP and PTREE (which we
shall define shortly), we see that VERB defines basic
features for all verb entries (and can be used directly
for intransitives such as Die), VERB+NP inherits from
VERB but adds an NP complement to the right of
the verb (for transitives), VERB+NP+PP inherits from
VERB+NP but adds a further PP complement and so
</bodyText>
<figure confidence="0.9994634">
s
\parent
vp
left
up
parent
&apos;up
right
pp
up
</figure>
<figureCaption confidence="0.983206">
Figure 2: Bottom-up encoding for Give
</figureCaption>
<page confidence="0.98892">
79
</page>
<bodyText confidence="0.9998479">
on. Entries for regular verb lexemes are then mi-
nimal — syntactically they just inherit everything
from the abstract definitions.
This DATR fragment is incomplete, because it neg-
lects to define the internal structure of the TREENODE
and the various subtree nodes in the lexical hierar-
chy. Each such node is a description of an LTAG tree
at some degree of abstraction9. The following DATR
statements complete the fragment, by providing de-
finitions for this internal structure:
</bodyText>
<figure confidence="0.981859434782609">
TREENODE:
&lt;&gt; == undef
&lt;type&gt; == internal.
STREE:
&lt;&gt; == TREENODE
&lt;cat&gt; == s.
VPTREE:
&lt;&gt; == TREENODE
&lt;cat&gt; == vp
&lt;parent&gt; == STREE:&lt;&gt;
&lt;left&gt; == NPCOMP:&lt;&gt;.
NPCOMP:
&lt;&gt; == TREENODE
&lt;cat&gt; == np
&lt;type&gt; == substitution.
PPTREE:
&lt;&gt; == TREENODE
&lt;cat&gt; ma pp.
PTREE:
&lt;&gt; == TREENODE
&lt;cat&gt; == p
&lt;type&gt; == anchor
&lt;parent&gt; == PPTREE:&lt;&gt;
</figure>
<bodyText confidence="0.967776928571429">
Here, TREENODE represents an abstract node in an
LTAG tree and provides a (default) type of internal.
Notice that VERB is itself a TREENODE (but with the
nondefault type anchor), and the other definitions
here define the remaining tree nodes that arise in
our small lexicon: VPTREE is the node for VERB&apos;s pa-
rent, STREE for VERB&apos;s grandparent, NPCOMP defines
the structure needed for NP complement substitution
nodes, etc.19
Taken together, these definitions provide a speci-
fication for Give just as we had it before, but with
the addition of type and root features. They also
support some other verbs too, and it should be clear
that the basic technique extends readily to a wide
range of other verbs and other parts of speech. Also,
although the trees we have described are all initial
9Even the lexeme nodes are abstract — individual
word forms might be represented by further more specific
nodes attached below the lexemes in the hierarchy.
19 Our example makes much use of multiple inheritance
(thus, for example, VPTREE inherits from TREENODE,
STREE and NPCOMP) but all such multiple inheritance is
orthogonal in DATR: no path can inherit from more than
one node.
trees (in LTAG terminology), we can describe auxi-
liary trees, which include a leaf node of type foot
just as easily. A simple example is provided by the
following definition for auxiliary verbs:
</bodyText>
<table confidence="0.523994428571429">
AUXVERB:
&lt;&gt; == TREENODE
&lt;cat&gt; == v
&lt;type&gt; == anchor
&lt;parent cat&gt; == vp
&lt;right cat&gt; == vp
&lt;right type&gt; == foot.
</table>
<sectionHeader confidence="0.792143" genericHeader="method">
4 Lexical rules
</sectionHeader>
<bodyText confidence="0.999885108695652">
Having established a basic structure for our LTAG
lexicon, we now turn our attention towards captu-
ring other kinds of relationship among trees. We
noted above that lexical entries are actually associa-
ted with tree families, and that these group to-
gether trees that are related to each other. Thus in
the same family as a standard ditransitive verb, we
might find the full passive, the agentless passive, the
dative alternation, the various relative clauses, and
so forth. It is clear that these families correspond
closely to the outputs of transformations or metaru-
les in other frameworks, but the XTAG system cur-
rently has no formal component for describing the
relationships among families nor mechanisms for ge-
nerating them. And so far we have said nothing
about them either — we have only characterized sin-
gle trees.
However, LTAG &apos;s large domain of locality means
that all such relationships can be viewed as directly
lexical, and ihus expressible by lexical rules. In fact
we can go further than this: because we have em-
bedded the domain of these lexical rules, namely the
LTAG tree structures, within the feature structures,
we can view such lexical rules as covariation cons-
traints within feature structures, in much the same
way that the covariation of, say, syntactic and mor-
phological form is treated. In particular, we can use
the mechanisms that DATR already provides for fea-
ture covariation, rather than having to invoke in ad-
dition some special purpose lexical rule machinery.
We consider six construction types found in the
XTAG grammar: passive, dative, subject-auxiliary
inversion, wh-questions, relative clauses and topica-
lisation. Our basic approach to each of these is the
same. Lexical rules are specified by defining a deri-
ved output tree structure in terms of an input tree
structure, where each of these structures is a set of
feature specifications of the sort defined above. Each
lexical rule has a name, and the input and output
tree structures for rule foo are referenced by pre-
fixing feature paths of the sort given above with
&lt;input foo ..&gt; or &lt;output foo ..&gt;. So for ex-
ample, the category of the parent tree node of the
output of the passive rule might be referenced as
&lt;output passive parent cat&gt;. We define a very
general default, stating that the output is the same
</bodyText>
<page confidence="0.985508">
80
</page>
<bodyText confidence="0.995728157894737">
as the input, so that lexical relationships need only
concern themselves with components they modify.
This approach to formulating lexical rules in DATR
is quite general and in no way restricted to LTAG: it
can be readily adapted for application in the context
of any feature-based lexicalist grammar formalism.
Using this approach, the dative lexical rule can be
given a minimalist implementation by the addition
of the following single line to VERB+NP+PP, defined
above.
VERB+NP+PP:
&lt;output dative right right&gt; == NPCOMP:&lt;&gt;.
This causes the second complement to a ditran-
sitive verb in the dative alternation to be an NP,
rather than a PP as in the unmodified case. Subject-
auxiliary inversion can be achieved similarly by just
specifying the output tree structure without refe-
rence to the input structure (note the addition here
of a form feature specifying verb form):
</bodyText>
<sectionHeader confidence="0.546535" genericHeader="method">
AUXVERB:
</sectionHeader>
<bodyText confidence="0.930393644067797">
&lt;output auxinv form&gt; == finite-iv
&lt;output auxinv parent cat&gt; == s
&lt;output auxinv right cat&gt; == s.
Passive is slightly more complex, in that it has to
modify the given input tree structure rather than
simply overwriting part of it. The definitions for pas-
sive occur at the VERB+NP node, since by default, any
transitive or subclass of transitive has a passive form.
Individual transitive verbs, or whole subclasses, can
override this default, leaving their passive tree struc-
ture undefined if required. For agentless passives,
the necessary additions to the VERB+NP node are as
follows11:
VERB+NP:
&lt;output passive form&gt; == passive
&lt;output passive right&gt; ==
&amp;quot;&lt;input passive right right&gt;&amp;quot;.
Here, the first line stipulates the form of the verb
in the output tree to be passive, while the second line
redefines the complement structure: the output of
passive has as its first complement the second com-
plement of its input, thereby discarding the first
complement of its input. Since complements are
daisy-chained, all the others move up too.
Wh-questions, relative clauses and topicalisation
are slightly different, in that the application of the
lexical rule causes structure to be added to the top
of the tree (above the s node). Although these con-
structions involve unbounded dependencies, the un-
boundedness is taken care of by the LTAG adjunction
mechanism: for lexical purposes the dependency is
local. Since the relevant lexical rules can apply to
sentences that contain any kind of verb, they need
to be stated at the VERB node. Thus, for exam-
ple, topicalisation and wh-questions can be defined
as follows:
&amp;quot;Oversimplifying slightly, the double quotes in
&amp;quot;&lt;input passive right right&gt;&amp;quot; mean that that DATR
path will not be evaluated locally (i.e., at the VERB+NP
node), but rather at the relevant lexeme node (e.g., Eat
or Give).
VERB:
&lt;output topic parent parent parent cat&gt;
==
&lt;output topic parent parent left cat&gt; == np
&lt;output topic parent parent left form)
== normal
(output whq&gt; == &amp;quot;&lt;output topic&gt;&amp;quot;
&lt;output whq parent parent left form&gt; == wh.
Here an additional NP and s are attached above
the original s node to create a topicalised struc-
ture. The wh-rule inherits from the topicalisation
rule, changing just one thing: the form of the new
NP is marked as wh, rather than as normal. In the
full fragment&apos;&apos;, the NP added by these rules is also
syntactically cross-referenced to a specific NP mar-
ked as null in the input tree. However, space does
not permit presentation or discussion of the DATR
code that achieves this here.
</bodyText>
<sectionHeader confidence="0.968281" genericHeader="method">
5 Applying lexical rules
</sectionHeader>
<bodyText confidence="0.99994">
As explained above, each lexical rule is defined to
operate on its own notion of an input and produce
its own output. In order for the rules to have an ef-
fect, the various input and output paths have to be
linked together using inheritance, creating a chain of
inheritances between the base, that is, the canonical
definitions we introduced in section 3, and surface
tree structures of the lexical entry. For example, to
&apos;apply&apos; the dative rule to our Give definition, we
could construct a definition such as this:
</bodyText>
<equation confidence="0.95668975">
Give-dat:
&lt;&gt; == Give
&lt;input dative&gt; == &lt;&gt;
&lt;surface&gt; == &lt;output dative&gt;.
</equation>
<bodyText confidence="0.999780045454545">
Values for paths prefixed with surface inherit
from the output of the dative rule. The input of
the dative rule inherits from the base (unprefixed)
case, which inherits from Give. The dative rule de-
finition (just the one line introduced above, plus the
default that output inherits from input) thus media-
tes between &apos;live and the surface of Give-dat. This
chain can be extended by inserting additional in-
heritance specifications (such as passive). Note that
surf ace defaults to the base case, so all entries have
a surface defined.
However, in our full fragment, additional support
is provided to achieve and constrain this rule chai-
ning. Word definitions include boolean features in-
dicating which rules to apply, and the presence of
these features trigger inheritance between appro-
priate input and output paths and the base and
surface specifications at the ends of the chain. For
example, Wordi is an alternative way of specifying
the dative alternant of Give, but results in inhe-
ritance linking equivalent to that found in Give-dat
above:
</bodyText>
<footnote confidence="0.889965">
12The full version of this DATR fragment includes all
the components discussed above in a single coherent, but
slightly more complex account. It is available on request
from the authors.
</footnote>
<page confidence="0.994196">
81
</page>
<table confidence="0.625733833333333">
Wordi:
&lt;&gt; == Give
&lt;alt dative&gt; == true.
More interestingly, Word2 properly describes a wh-
question based on the agentless passive of the dative
of Give.
Word2:
&lt;&gt; == Give
&lt;alt whq&gt; == true
&lt;alt dative&gt; == true
&lt;alt passive&gt; == true.
&lt;parent left form&gt; == null
</table>
<bodyText confidence="0.9997092">
Notice here the final line of Word2 which specifies
the location of the &apos;extracted&apos; NP (the subject, in this
case), by marking it as null. As noted above, the full
version of the whq lexical rule uses this to specify a
cross-reference relationship between the wh-NP and
the null NP.
We can, if we wish, encode constraints on the app-
licability of rules in the mapping from boolean flags
to actual inheritance specifications. Thus, for exam-
ple, whq, rel, and topic are mutually exclusive.
If such constraints are violated, then no value for
surface gets defined. Thus Word3 improperly att-
empts topicalisation in addition to wh-question for-
mation, and, as a result, will fail to define a surface
tree structure at all:
</bodyText>
<figure confidence="0.514611428571429">
Word3:
&lt;&gt; == Give
&lt;alt whq&gt; == true
&lt;alt topic&gt; == true
&lt;alt dative&gt; == true
&lt;alt passive&gt; == true
&lt;parent left form&gt; == null.
</figure>
<bodyText confidence="0.9998891875">
This approach to lexical rules allows them to be
specified at the appropriate point in the lexical hier-
archy, but overridden or modified in subclasses or
lexemes as appropriate. It also allows default gene-
ralisation over the lexical rules themselves, and con-
trol over their application. The last section showed
how the whq lexical rule could be built by a single mi-
nor addition to that for topicalisation. However, it is
worth noting that, in common with other DATR spe-
cifications, the lexical rules presented here are rule
instances which can only be applied once to any
given lexeme — multiple application could be sup-
ported, by making multiple instances inherit from
some common rule specification, but in our current
treatment such instances would require different rule
names.
</bodyText>
<sectionHeader confidence="0.883839" genericHeader="method">
6 Comparison with related work
</sectionHeader>
<bodyText confidence="0.999941233333333">
As noted above, Vijay-Shanker &amp; Schabes (1992)
have also proposed an inheritance-based approach
to this problem. They use monotonic inheritance to
build up partial descriptions of trees: each descrip-
tion is a finite set of dominance, immediate domi-
nance and linear precedence statements about tree
nodes in a tree description language developed by
Rogers &amp; Vijay-Shanker (1992), and category infor-
mation is located in the node labels.
This differs from our approach in a number of
ways. First, our use of nonmonotonic inheritance
allows us to manipulate total instead of partial de-
scriptions of trees. The abstract verb class in the
Vijay-Shanker &amp; Schabes account subsumes both in-
transitive and transitive verb classes but is not iden-
tical to either — a minimal-satisfying-model step is
required to map partial tree descriptions into actual
trees. In our analysis, VERB is the intransitive verb
class, with complements specifically marked as un-
defined: thus VERB: &lt;right&gt; == undef is inherited
from TREENODE and VERB+NP just overrides this com-
plement specification to add an NP complement. Se-
cond, we describe trees using only local tree relations
(between adjacent nodes in the tree), while Vijay-
Shanker &amp; Schabes also use a nonlocal dominance
relation.
Both these properties are crucial to our embed-
ding of the tree structure in the feature structure.
We want the category information at each tree node
to be partial in the conventional sense, so that in
actual use such categories can be extended (by uni-
fication or whatever). So the feature structures that
we associate with lexical entries must be viewed as
partial. But we do not want the tree structure to
be extendible in the same way: we do not want an
intransitive verb to be applicable in a transitive con-
text, by unifying in a complement NP. So the tree
structures we define must be total descriptions13.
And of course, our use of only local relations al-
lows a direct mapping from tree structure to feature
path, which would not be possible at all if nonlocal
relations were present.
So while these differences may seem small, they al-
low us to take this significant representational step —
significant because it is the tree structure embedding
that allows us to view lexical rules as feature cova-
riation constraints. The result is that while Vijay-
Shanker &amp; Schabes use a tree description language,
a category description language and a further for-
malism for lexical rules, we can capture everything
in one framework all of whose components (non-
monotonicity, covariation constraint handling, etc.)
have already been independently motivated for other
aspects of lexical description&amp;quot;.
Becker&apos;s recent work (1993; 1994) is also directed
at exactly tne problem we address in the present
paper. Like him, we have employed an inheritance
hierarchy. And, like him, we have employed a set of
lexical rules (corresponding to his metarules). The
key differences between our account and his are (1)
</bodyText>
<footnote confidence="0.967331666666667">
13Note that simplified fragment presented here does
not get this right. It makes all feature specifications total
descriptions. To correct this we would need to change
TftEENODE so that only the values of &lt;right&gt;, &lt;left&gt; and
&lt;parent&gt; default to undef.
14 As in the work cited in footnote 3, above.
</footnote>
<page confidence="0.998598">
82
</page>
<bodyText confidence="0.999940467741935">
that we have been able to use an existing lexical
knowledge representation language, rather than de-
signing a formal system that is specific to LTAG, and
(ii) that we have expressed our lexical rules in ex-
actly the same language as that we have used to
define the hierarchy, rather than invoking two quite
different formal systems.
Becker&apos;s sharp distinction between his metarules
and his hierarchy gives rise to some problems that
our approach avoids. Firstly, he notes that his meta-
rules are subject to lexical exceptions and proposes
to deal with these by stating &amp;quot;for each entry in the
(syntactic) lexicon .. which metarules are applica-
ble for this entry&amp;quot; (1993,126). We have no need to
carry over this use of (meta)rule features since, in
our account, lexical rules are not distinct from any
other kind of property in the inheritance hierarchy.
They can be stated at the most inclusive relevant
node and can then be overridden at the exceptional
descendant nodes. Nothing specific needs to be said
about the nonexceptional nodes.
Secondly, his metarules may themselves be more
or less similar to each other and he suggests
(1994,11) that these similarities could be captured
if the metarules were also to be organized in a hier-
archy. However, our approach allows us to deal with
any such similarities in the main lexical hierarchy
itself15 rather than by setting up a separate hierar-
chical component just for metarules (which appears
to be what Becker has in mind).
Thirdly, as he himself notes (1993,128), because
his metarules map from elementary trees that are in
the inheritance hierarchy to elementary trees that
are outside it, most of the elementary trees actually
used are not directly connected to the hierarchy (alt-
hough their derived status with respect to it can be
reconstructed). Our approach keeps all elementary
trees, whether or not they have been partly defined
by a lexical rule, entirely within the lexical hierarchy.
In fact, Becker himself considers the possibility
of capturing all the significant generalizations by
using just one of the two mechanisms that he pro-
poses: &amp;quot;one might want to reconsider the usage of
one mechanism for phenomena in both dimensions&amp;quot;
(1993,135). But, as he goes on to point out, his exi-
sting type of inheritance network is not up to taking
on the task performed by his metarules because the
former is monotonic whilst his metarules are not.
However, he does suggest a way in which the hierar-
chy could be completely replaced by metarules but
argues against adopting it (1993,136).
As will be apparent from the earlier sections of
this paper, we believe that Becker&apos;s insights about
the organization of an LTAG lexicon can be better
expressed if the metarule component is replaced by
&amp;quot;As illustrated by the way in which the whq lexical
rule inherits from that for topicalisation in the example
given above.
an encoding of (largely equivalent) lexical rules that
are an integral part of a nonmonotonic inheritance
hierarchy that stands as a description of all the ele-
mentary trees.
</bodyText>
<sectionHeader confidence="0.970395" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999960555555556">
A precursor of this paper was presented at the Sep-
tember 1994 TAG+ Workshop in Paris. We thank
the referees for that event and the ACL-95 referees
for a number of helpful comments. We are also gra-
teful to Aravind Joshi, Bill Keller, Owen Rambow
K. Vijay-Shanker and The XTAG Group. This rese-
arch was partly supported by grants to Evans from
SERC/EPSRC (UK) and to Gazdar from ESRC
(UK).
</bodyText>
<sectionHeader confidence="0.997751" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999749333333333">
Anne Abeille, Kathleen Bishop, Sharon Cote, &amp; Yves
Schabes. 1990. A lexicalized tree adjoining grammar
for english. Technical Report MS-CIS-90-24, Depart-
ment of Computer &amp; Information Science, Univ. of
Pennsylvania.
Francois Andry, Norman Fraser, Scott McGlashan, Si-
mon Thornton, &amp; Nick Youd. 1992. Making DATR
work for speech: lexicon compilation in SUNDIA.
Comput. Ling., 18(3):245-267.
Petra Barg. 1994. Automatic acquisition of datr theo-
ries from observations. Theories des lexicons: Arbei-
ten des sonderforschungsbereichs 282, Heinrich-Heine
Univ. of Duesseldorf, Duesseldorf.
Tilman Becker. 1993. HyTAG: A new type of Tree Ad-
joining Grammar for hybrid syntactic representation
of free word order languages. Ph.D. thesis, Univ. des
Saarlandes.
Tilman Becker. 1994. Patterns in metarules. In Pro-
ceedings of the Third International Workshop on Tree
Adjoining Grammars, 9-11.
Doris Bleiching. 1992. Prosodisches wissen in lexicon. In
G. Goerz, ed., KONVENS-92, 59-68. Springer-Verlag.
Doris Bleiching. 1994. Integration von morphophono-
logie und prosodie in em n hierarchisches lexicon. In
H. Trost, ed., Proceedings of KONVENS-94, 32-41.
Ted Briscoe, Valeria de Paiva, &amp; Ann Copestake. 1993.
Inheritance, Defaults, eg the Lexicon. CUP.
Dunstan Brown &amp; Andrew Hippisley. 1994. Conflict in
russian genitive plural assignment: A solution repre-
sented in DATR. J. of Slavic Linguistics, 2(448-76.
Lynne Cahill &amp; Roger Evans. 1990. An application of
DATR: the TIC lexicon. In ECAI-90, 120-125.
Lynne Cahill. 1990. Syllable-based morphology. In
COLING-90, volume 3,48-53.
Lynne Cahill. 1993. Morphonology in the lexicon. In
EACL-93, 37-96.
Greville Corbett &amp; Norman Fraser. 1993. Network mor-
phology: a DATR account of Russian nominal inflec-
tion. J. of Linguistics, 29:113-142.
</reference>
<page confidence="0.986036">
83
</page>
<reference confidence="0.999876191176471">
Walter Daelemans &amp; Gerald Gazdar, eds. 1992. Special
issues on inheritance. Comput. Ling., 18(2 &amp; 3).
Christy Doran, Dania Egedi, Beth Ann Hockey, Az B. Sri-
nivas. 1994a. Status of the XTAG system. In Pro-
ceedings of the Third International Workshop on Tree
Adjoining Grammars, 20-23.
Christy Doran, Dania Egedi, Beth Ann Hockey, B. Sri-
nivas, &amp; Martin Zaidel. 19946. XTAG system - a
wide coverage grammar for english. In COLING-94,
922-928.
Markus Duda &amp; Gunter Gebhardi. 1994. DUTR - a
DATR-PATR interface formalism. In H. Trost, ed.,
Proceedings of KONVENS-94, 411-414.
Roger Evans &amp; Gerald Gazdar. 1989a. Infernce in
DATR. In EACL-89, 66-71.
Roger Evans &amp; Gerald Gazdar. 1989b. The semantics
of DATR. In AISB-89, 79-87.
Daniel P. Flickinger. 1987. Lexical Rules in the Hierar-
chical Lexicon. Ph.D. thesis, Stanford Univ.
Norman Fraser &amp; Greville Corbett. in press. Gender,
animacy, &amp; declensional class assignment: a unified
account for russian. In Geert Booij &amp; Jaap van Marle,
ed., Yearbook of Morphology 1994. Kluwer, Dordrecht.
Dafydd Gibbon. 1992. ILEX: a linguistic approach to
computational lexica. In Ursula Klenk, ed., Corn-
putatio Linguae: Aufsa(&amp;quot;tze zur algorithmischen und
quantitativen Analyse der Sprache (Zeitschrift fu(&amp;quot;r
Dialektologie und Linguistik, Beiheft 73), 32-53. Franz
Steiner Verlag, Stuttgart.
A. K. Joshi, L. S. Levy, &amp; M. Takahashi. 1975. Tree
adjunct grammars. J. Comput. Syst. Sci., 1O(1):136-
163.
James Kilbury, Petra [Barg] Naerger, &amp; Ingrid Renz.
1991. DATR as a lexical component for PATR. In
EACL-91, 137-142.
James Kilbury, Petra Barg, &amp; Ingrid Renz. 1994. Simu-
lation lexicalischen erwerbs. In Christopher Babel 8z
Gert Rickheit Sascha W. Felix, ed, Kognitive Lingui-
stik: Repraesentation und Prozesse, 251-271. West-
deutscher Verlag, Opladen.
James Kilbury. 1990. Encoding constituent structure in
feature structures. Unpublished manuscript, Univ. of
Duesseldorf, Duesseldorf.
Adam Kilgarriff Sz Gerald Gazdar. 1995. Polysemous
relations. In Frank Palmer, ed., Grammar &amp; meaning:
essays in honour of Sir John Lyons, 1-25. CUP.
Adam Kilgarriff. 1993. Inheriting verb alternations. In
EACL-93, 213-221.
Hagen Langer. 1994. Reverse queries in DATR. In
COLING-94, 1089-1095.
Marc Light, Sabine Reinhard, Sz Marie Boyle-Hinrichs.
1993. INSYST: an automatic inserter system for hier-
archical lexica. In EACL-93, page 471.
Marc Light. 1994. Classification in feature-based default
inheritance hierarchies. In H. Trost, ed., Proceedings
of KONVENS-94, 220-229.
Sabine Reinhard 8z Dafydd Gibbon. 1991. Prosodic in-
heritance &amp; morphological generalisations. In EA CL-
91, 131-136.
James Rogers &amp; K. Vijay-Shanker. 1992. Reasoning
.with descriptions of trees. In ACL-92, 72-80.
K. Vijay-Shanker &amp; Yves Schabes. 1992. Structure
sharing in lexicalized tree-adjoining grammar. In
COLING-92, 205-211.
The XTAG Research Group. 1995. A lexicalized tree ad-
joining grammar for English. Technical Report IRCS
Report 95-03, The Institute for Research in Cognitive
Science, Univ. of Pennsylvania.
</reference>
<page confidence="0.999244">
84
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.921961">
<title confidence="0.998812">Encoding Lexicalized Tree Adjoining Grammars with a Nonmonotonic Inheritance Hierarchy</title>
<author confidence="0.999105">Roger Evans</author>
<affiliation confidence="0.999303666666667">Information Technology Research Institute University of Brighton</affiliation>
<email confidence="0.993467">rpeOitri.bton.ac.uk</email>
<author confidence="0.999592">Gerald Gazdar</author>
<affiliation confidence="0.999613">School of Cognitive &amp; Computing Sciences University of Sussex</affiliation>
<email confidence="0.993838">geraldgecogs.susx.ac.uk</email>
<author confidence="0.99996">David Weir</author>
<affiliation confidence="0.998904">School of Cognitive &amp; Computing Sciences University of Sussex</affiliation>
<email confidence="0.999121">davidwOcogs.susx.ac.uk</email>
<abstract confidence="0.995206583333333">This paper shows how DATR, a widely used formal language for lexical knowledge representation, can be used to define an LTAG lexicon as an inheritance hierarchy with internal lexical rules. A bottom-up featural encoding is used for LTAG trees and this allows lexical rules to be implemented as covariation constraints within feature structures. Such an approach eliminates the considerable redundancy otherwise associated with an LTAG lexicon.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anne Abeille</author>
<author>Kathleen Bishop</author>
<author>Sharon Cote</author>
<author>Yves Schabes</author>
</authors>
<title>A lexicalized tree adjoining grammar for english.</title>
<date>1990</date>
<tech>Technical Report MS-CIS-90-24,</tech>
<institution>Department of Computer &amp; Information Science, Univ. of Pennsylvania.</institution>
<marker>Abeille, Bishop, Cote, Schabes, 1990</marker>
<rawString>Anne Abeille, Kathleen Bishop, Sharon Cote, &amp; Yves Schabes. 1990. A lexicalized tree adjoining grammar for english. Technical Report MS-CIS-90-24, Department of Computer &amp; Information Science, Univ. of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francois Andry</author>
<author>Norman Fraser</author>
<author>Scott McGlashan</author>
<author>Simon Thornton</author>
<author>Nick Youd</author>
</authors>
<title>Making DATR work for speech: lexicon compilation in SUNDIA.</title>
<date>1992</date>
<journal>Comput. Ling.,</journal>
<pages>18--3</pages>
<contexts>
<context position="5822" citStr="Andry et al. (1992)" startWordPosition="914" endWordPosition="917">such as give is shown in figure 1. Following LTAG conventions (for the time being), the node labels here are gross syntactic category specifications to which additional featural information may be addeds, and are annotated to indicate node type: o indicates an anchor node, and j indicates a substitution node (where a 3See, for example, Bleiching (1992; 1994), Brown 84 Hippisley (1994), Corbett 84 Fraser (1993), Cahill (1990; 1993), Cahill 84 Evans (1990), Fraser 8,4 Corbett (in press), Gibbon (1992), Kilgarriff (1993), Kilgarriff 84 Gazdar (1995), Reinhard 84 Gibbon (1991). &apos;See, for example, Andry et al. (1992) on compilation, Kilbury et al. (1991) on coding DAGs, Duda 84 Gebhardi (1994) on dynamic querying, Langer (1994) on reverse querying, and Barg (1994), Light (1994), Light et al. (1993) and Kilbury et al. (1994) on automatic acquisition. And there are at least a dozen different DATR implementations available, on various platforms and programming languages. 51n fact, LTAG commonly distinguishes two sets of features at each node (top and bottom), but for simplicity we shall assume just one set in this paper. fully specified tree with a compatible root label may be attachedr In representing such </context>
</contexts>
<marker>Andry, Fraser, McGlashan, Thornton, Youd, 1992</marker>
<rawString>Francois Andry, Norman Fraser, Scott McGlashan, Simon Thornton, &amp; Nick Youd. 1992. Making DATR work for speech: lexicon compilation in SUNDIA. Comput. Ling., 18(3):245-267.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petra Barg</author>
</authors>
<title>Automatic acquisition of datr theories from observations. Theories des lexicons: Arbeiten des sonderforschungsbereichs 282, Heinrich-Heine Univ. of Duesseldorf,</title>
<date>1994</date>
<location>Duesseldorf.</location>
<contexts>
<context position="5972" citStr="Barg (1994)" startWordPosition="943" endWordPosition="944">additional featural information may be addeds, and are annotated to indicate node type: o indicates an anchor node, and j indicates a substitution node (where a 3See, for example, Bleiching (1992; 1994), Brown 84 Hippisley (1994), Corbett 84 Fraser (1993), Cahill (1990; 1993), Cahill 84 Evans (1990), Fraser 8,4 Corbett (in press), Gibbon (1992), Kilgarriff (1993), Kilgarriff 84 Gazdar (1995), Reinhard 84 Gibbon (1991). &apos;See, for example, Andry et al. (1992) on compilation, Kilbury et al. (1991) on coding DAGs, Duda 84 Gebhardi (1994) on dynamic querying, Langer (1994) on reverse querying, and Barg (1994), Light (1994), Light et al. (1993) and Kilbury et al. (1994) on automatic acquisition. And there are at least a dozen different DATR implementations available, on various platforms and programming languages. 51n fact, LTAG commonly distinguishes two sets of features at each node (top and bottom), but for simplicity we shall assume just one set in this paper. fully specified tree with a compatible root label may be attachedr In representing such a tree in DATR, we do two things. First, in keeping with the radically lexicalist character of LTAG, we describe the tree structure from its (lexical)</context>
</contexts>
<marker>Barg, 1994</marker>
<rawString>Petra Barg. 1994. Automatic acquisition of datr theories from observations. Theories des lexicons: Arbeiten des sonderforschungsbereichs 282, Heinrich-Heine Univ. of Duesseldorf, Duesseldorf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tilman Becker</author>
</authors>
<title>HyTAG: A new type of Tree Adjoining Grammar for hybrid syntactic representation of free word order languages.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>Univ. des Saarlandes.</institution>
<contexts>
<context position="3233" citStr="Becker (1993" startWordPosition="502" endWordPosition="503">hese trees have structure in common, many of the lexemes have the same tree families, and many of the trees within families are systematically related in ways which other formalisms capture using transformations or metarules. However, the LTAG formalism itself does not provide any direct support for capturing such regularities. Vijay-Shanker &amp; Schabes address this problem by introducing a hierarchical lexicon structure with monotonic inheritance and lexical rules, using an approach loosely based on that of Flickinger (1987) but tailored for LTAG trees rather than HPSG subcategorization lists. Becker (1993; 1994) proposes a slightly different solution, combining an inheritance component and a set of metarules2. We share their perception of the problem and agree that adopting a hierarchical approach provides the best available solution to it. However, rather than creating a hierarchical lexical formalism that is specific to the LTAG problem, we have used DATR, an LKRL that is already quite widely known and used. From an LTAG perspective, it makes sense to use an already available LKRL that was specifically designed to address these kinds of representational issues. From a DATR perspective, LTAG </context>
</contexts>
<marker>Becker, 1993</marker>
<rawString>Tilman Becker. 1993. HyTAG: A new type of Tree Adjoining Grammar for hybrid syntactic representation of free word order languages. Ph.D. thesis, Univ. des Saarlandes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tilman Becker</author>
</authors>
<title>Patterns in metarules.</title>
<date>1994</date>
<booktitle>In Proceedings of the Third International Workshop on Tree Adjoining Grammars,</booktitle>
<pages>9--11</pages>
<marker>Becker, 1994</marker>
<rawString>Tilman Becker. 1994. Patterns in metarules. In Proceedings of the Third International Workshop on Tree Adjoining Grammars, 9-11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doris Bleiching</author>
</authors>
<title>Prosodisches wissen in lexicon.</title>
<date>1992</date>
<pages>92--59</pages>
<editor>In G. Goerz, ed.,</editor>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="5556" citStr="Bleiching (1992" startWordPosition="876" endWordPosition="877">ure in which the tree nodes are labeled with syntactic categories and feature information and there is at least one leaf node labeled with a lexical category (such lexical leaf nodes are known as anchors). For example, the canonical tree for a ditransitive verb such as give is shown in figure 1. Following LTAG conventions (for the time being), the node labels here are gross syntactic category specifications to which additional featural information may be addeds, and are annotated to indicate node type: o indicates an anchor node, and j indicates a substitution node (where a 3See, for example, Bleiching (1992; 1994), Brown 84 Hippisley (1994), Corbett 84 Fraser (1993), Cahill (1990; 1993), Cahill 84 Evans (1990), Fraser 8,4 Corbett (in press), Gibbon (1992), Kilgarriff (1993), Kilgarriff 84 Gazdar (1995), Reinhard 84 Gibbon (1991). &apos;See, for example, Andry et al. (1992) on compilation, Kilbury et al. (1991) on coding DAGs, Duda 84 Gebhardi (1994) on dynamic querying, Langer (1994) on reverse querying, and Barg (1994), Light (1994), Light et al. (1993) and Kilbury et al. (1994) on automatic acquisition. And there are at least a dozen different DATR implementations available, on various platforms an</context>
</contexts>
<marker>Bleiching, 1992</marker>
<rawString>Doris Bleiching. 1992. Prosodisches wissen in lexicon. In G. Goerz, ed., KONVENS-92, 59-68. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doris Bleiching</author>
</authors>
<title>Integration von morphophonologie und prosodie in em n hierarchisches lexicon.</title>
<date>1994</date>
<booktitle>Proceedings of KONVENS-94,</booktitle>
<pages>32--41</pages>
<editor>In H. Trost, ed.,</editor>
<marker>Bleiching, 1994</marker>
<rawString>Doris Bleiching. 1994. Integration von morphophonologie und prosodie in em n hierarchisches lexicon. In H. Trost, ed., Proceedings of KONVENS-94, 32-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>Valeria de Paiva</author>
<author>Ann Copestake</author>
</authors>
<title>Inheritance, Defaults, eg the Lexicon.</title>
<date>1993</date>
<publisher>CUP.</publisher>
<marker>Briscoe, de Paiva, Copestake, 1993</marker>
<rawString>Ted Briscoe, Valeria de Paiva, &amp; Ann Copestake. 1993. Inheritance, Defaults, eg the Lexicon. CUP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dunstan Brown</author>
<author>Andrew Hippisley</author>
</authors>
<title>Conflict in russian genitive plural assignment: A solution represented in DATR.</title>
<date>1994</date>
<journal>J. of Slavic Linguistics,</journal>
<pages>2--448</pages>
<marker>Brown, Hippisley, 1994</marker>
<rawString>Dunstan Brown &amp; Andrew Hippisley. 1994. Conflict in russian genitive plural assignment: A solution represented in DATR. J. of Slavic Linguistics, 2(448-76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynne Cahill</author>
<author>Roger Evans</author>
</authors>
<title>An application of DATR: the TIC lexicon.</title>
<date>1990</date>
<booktitle>In ECAI-90,</booktitle>
<pages>120--125</pages>
<marker>Cahill, Evans, 1990</marker>
<rawString>Lynne Cahill &amp; Roger Evans. 1990. An application of DATR: the TIC lexicon. In ECAI-90, 120-125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynne Cahill</author>
</authors>
<title>Syllable-based morphology.</title>
<date>1990</date>
<booktitle>In COLING-90,</booktitle>
<volume>volume</volume>
<pages>3--48</pages>
<contexts>
<context position="5630" citStr="Cahill (1990" startWordPosition="887" endWordPosition="888"> information and there is at least one leaf node labeled with a lexical category (such lexical leaf nodes are known as anchors). For example, the canonical tree for a ditransitive verb such as give is shown in figure 1. Following LTAG conventions (for the time being), the node labels here are gross syntactic category specifications to which additional featural information may be addeds, and are annotated to indicate node type: o indicates an anchor node, and j indicates a substitution node (where a 3See, for example, Bleiching (1992; 1994), Brown 84 Hippisley (1994), Corbett 84 Fraser (1993), Cahill (1990; 1993), Cahill 84 Evans (1990), Fraser 8,4 Corbett (in press), Gibbon (1992), Kilgarriff (1993), Kilgarriff 84 Gazdar (1995), Reinhard 84 Gibbon (1991). &apos;See, for example, Andry et al. (1992) on compilation, Kilbury et al. (1991) on coding DAGs, Duda 84 Gebhardi (1994) on dynamic querying, Langer (1994) on reverse querying, and Barg (1994), Light (1994), Light et al. (1993) and Kilbury et al. (1994) on automatic acquisition. And there are at least a dozen different DATR implementations available, on various platforms and programming languages. 51n fact, LTAG commonly distinguishes two sets of</context>
</contexts>
<marker>Cahill, 1990</marker>
<rawString>Lynne Cahill. 1990. Syllable-based morphology. In COLING-90, volume 3,48-53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynne Cahill</author>
</authors>
<title>Morphonology in the lexicon.</title>
<date>1993</date>
<booktitle>In EACL-93,</booktitle>
<pages>37--96</pages>
<marker>Cahill, 1993</marker>
<rawString>Lynne Cahill. 1993. Morphonology in the lexicon. In EACL-93, 37-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greville Corbett</author>
<author>Norman Fraser</author>
</authors>
<title>Network morphology: a DATR account of Russian nominal inflection.</title>
<date>1993</date>
<journal>J. of Linguistics,</journal>
<pages>29--113</pages>
<marker>Corbett, Fraser, 1993</marker>
<rawString>Greville Corbett &amp; Norman Fraser. 1993. Network morphology: a DATR account of Russian nominal inflection. J. of Linguistics, 29:113-142.</rawString>
</citation>
<citation valid="true">
<date>1992</date>
<booktitle>Special issues on inheritance. Comput. Ling., 18(2 &amp; 3).</booktitle>
<editor>Walter Daelemans &amp; Gerald Gazdar, eds.</editor>
<contexts>
<context position="1718" citStr="(1992)" startWordPosition="260" endWordPosition="260">veloped: Abeille, Bishop, Cote &amp; Schabes (1990) describe a feature-based Lexicalized Tree Adjoining Grammar (LTAG) for English which subsequently became the basis for the grammar used in the XTAG system, a wide-coverage LTAG parser (Doran et al., 1994b; Doran et al., 1994a; XTAG Research Group, 1995). The advent of such large grammars gives rise to questions of efficient representation, and the fully lexicalized character of the LTAG formalism suggests that recent research into lexical representation might be a place to look for answers (see for example Briscoe et a/.(1993); Daelemans &amp; Gazdar(1992)). In this paper we explore this suggestion by showing how the lexical knowledge representation language (LKRL) DATR (Evans &amp; Gazdar, 1989a; Evans &amp; Gazdar, 1989b) can be used to formulate a compact, hierarchical encoding of an LTAG. The issue of efficient representation for LTAG&apos; is discussed by Vijay-Shanker &amp; Schabes (1992), who lAs with all fully lexicalized grammar formalisms, there is really no conceptual distinction to be drawn in LTAG between the lexicon and the grammar: the grammatical rules are just lexical properties. draw attention to the considerable redundancy inherent in LTAG le</context>
<context position="5563" citStr="(1992; 1994)" startWordPosition="877" endWordPosition="878">ch the tree nodes are labeled with syntactic categories and feature information and there is at least one leaf node labeled with a lexical category (such lexical leaf nodes are known as anchors). For example, the canonical tree for a ditransitive verb such as give is shown in figure 1. Following LTAG conventions (for the time being), the node labels here are gross syntactic category specifications to which additional featural information may be addeds, and are annotated to indicate node type: o indicates an anchor node, and j indicates a substitution node (where a 3See, for example, Bleiching (1992; 1994), Brown 84 Hippisley (1994), Corbett 84 Fraser (1993), Cahill (1990; 1993), Cahill 84 Evans (1990), Fraser 8,4 Corbett (in press), Gibbon (1992), Kilgarriff (1993), Kilgarriff 84 Gazdar (1995), Reinhard 84 Gibbon (1991). &apos;See, for example, Andry et al. (1992) on compilation, Kilbury et al. (1991) on coding DAGs, Duda 84 Gebhardi (1994) on dynamic querying, Langer (1994) on reverse querying, and Barg (1994), Light (1994), Light et al. (1993) and Kilbury et al. (1994) on automatic acquisition. And there are at least a dozen different DATR implementations available, on various platforms and progr</context>
<context position="23549" citStr="(1992)" startWordPosition="3905" endWordPosition="3905">d control over their application. The last section showed how the whq lexical rule could be built by a single minor addition to that for topicalisation. However, it is worth noting that, in common with other DATR specifications, the lexical rules presented here are rule instances which can only be applied once to any given lexeme — multiple application could be supported, by making multiple instances inherit from some common rule specification, but in our current treatment such instances would require different rule names. 6 Comparison with related work As noted above, Vijay-Shanker &amp; Schabes (1992) have also proposed an inheritance-based approach to this problem. They use monotonic inheritance to build up partial descriptions of trees: each description is a finite set of dominance, immediate dominance and linear precedence statements about tree nodes in a tree description language developed by Rogers &amp; Vijay-Shanker (1992), and category information is located in the node labels. This differs from our approach in a number of ways. First, our use of nonmonotonic inheritance allows us to manipulate total instead of partial descriptions of trees. The abstract verb class in the Vijay-Shanker</context>
</contexts>
<marker>1992</marker>
<rawString>Walter Daelemans &amp; Gerald Gazdar, eds. 1992. Special issues on inheritance. Comput. Ling., 18(2 &amp; 3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christy Doran</author>
<author>Dania Egedi</author>
<author>Beth Ann Hockey</author>
<author>Az B Srinivas</author>
</authors>
<title>Status of the XTAG system.</title>
<date>1994</date>
<booktitle>In Proceedings of the Third International Workshop on Tree Adjoining Grammars,</booktitle>
<contexts>
<context position="1363" citStr="Doran et al., 1994" startWordPosition="199" endWordPosition="203">roach eliminates the considerable redundancy otherwise associated with an LTAG lexicon. 1 Introduction The Tree Adjoining Grammar (TAG) formalism was first introduced two decades ago (Joshi et al., 1975), and since then there has been a steady stream of theoretical work using the formalism. But it is only more recently that grammars of non-trivial size have been developed: Abeille, Bishop, Cote &amp; Schabes (1990) describe a feature-based Lexicalized Tree Adjoining Grammar (LTAG) for English which subsequently became the basis for the grammar used in the XTAG system, a wide-coverage LTAG parser (Doran et al., 1994b; Doran et al., 1994a; XTAG Research Group, 1995). The advent of such large grammars gives rise to questions of efficient representation, and the fully lexicalized character of the LTAG formalism suggests that recent research into lexical representation might be a place to look for answers (see for example Briscoe et a/.(1993); Daelemans &amp; Gazdar(1992)). In this paper we explore this suggestion by showing how the lexical knowledge representation language (LKRL) DATR (Evans &amp; Gazdar, 1989a; Evans &amp; Gazdar, 1989b) can be used to formulate a compact, hierarchical encoding of an LTAG. The issue o</context>
</contexts>
<marker>Doran, Egedi, Hockey, Srinivas, 1994</marker>
<rawString>Christy Doran, Dania Egedi, Beth Ann Hockey, Az B. Srinivas. 1994a. Status of the XTAG system. In Proceedings of the Third International Workshop on Tree Adjoining Grammars, 20-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christy Doran</author>
<author>Dania Egedi</author>
<author>Beth Ann Hockey</author>
<author>B Srinivas</author>
<author>Martin Zaidel</author>
</authors>
<title>XTAG system - a wide coverage grammar for english.</title>
<date>1994</date>
<booktitle>In COLING-94,</booktitle>
<pages>922--928</pages>
<contexts>
<context position="1363" citStr="Doran et al., 1994" startWordPosition="199" endWordPosition="203">roach eliminates the considerable redundancy otherwise associated with an LTAG lexicon. 1 Introduction The Tree Adjoining Grammar (TAG) formalism was first introduced two decades ago (Joshi et al., 1975), and since then there has been a steady stream of theoretical work using the formalism. But it is only more recently that grammars of non-trivial size have been developed: Abeille, Bishop, Cote &amp; Schabes (1990) describe a feature-based Lexicalized Tree Adjoining Grammar (LTAG) for English which subsequently became the basis for the grammar used in the XTAG system, a wide-coverage LTAG parser (Doran et al., 1994b; Doran et al., 1994a; XTAG Research Group, 1995). The advent of such large grammars gives rise to questions of efficient representation, and the fully lexicalized character of the LTAG formalism suggests that recent research into lexical representation might be a place to look for answers (see for example Briscoe et a/.(1993); Daelemans &amp; Gazdar(1992)). In this paper we explore this suggestion by showing how the lexical knowledge representation language (LKRL) DATR (Evans &amp; Gazdar, 1989a; Evans &amp; Gazdar, 1989b) can be used to formulate a compact, hierarchical encoding of an LTAG. The issue o</context>
</contexts>
<marker>Doran, Egedi, Hockey, Srinivas, Zaidel, 1994</marker>
<rawString>Christy Doran, Dania Egedi, Beth Ann Hockey, B. Srinivas, &amp; Martin Zaidel. 19946. XTAG system - a wide coverage grammar for english. In COLING-94, 922-928.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Duda</author>
<author>Gunter Gebhardi</author>
</authors>
<title>DUTR - a DATR-PATR interface formalism.</title>
<date>1994</date>
<booktitle>Proceedings of KONVENS-94,</booktitle>
<pages>411--414</pages>
<editor>In H. Trost, ed.,</editor>
<marker>Duda, Gebhardi, 1994</marker>
<rawString>Markus Duda &amp; Gunter Gebhardi. 1994. DUTR - a DATR-PATR interface formalism. In H. Trost, ed., Proceedings of KONVENS-94, 411-414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Evans</author>
<author>Gerald Gazdar</author>
</authors>
<date>1989</date>
<booktitle>Infernce in DATR. In EACL-89,</booktitle>
<pages>66--71</pages>
<contexts>
<context position="1856" citStr="Evans &amp; Gazdar, 1989" startWordPosition="280" endWordPosition="283">sh which subsequently became the basis for the grammar used in the XTAG system, a wide-coverage LTAG parser (Doran et al., 1994b; Doran et al., 1994a; XTAG Research Group, 1995). The advent of such large grammars gives rise to questions of efficient representation, and the fully lexicalized character of the LTAG formalism suggests that recent research into lexical representation might be a place to look for answers (see for example Briscoe et a/.(1993); Daelemans &amp; Gazdar(1992)). In this paper we explore this suggestion by showing how the lexical knowledge representation language (LKRL) DATR (Evans &amp; Gazdar, 1989a; Evans &amp; Gazdar, 1989b) can be used to formulate a compact, hierarchical encoding of an LTAG. The issue of efficient representation for LTAG&apos; is discussed by Vijay-Shanker &amp; Schabes (1992), who lAs with all fully lexicalized grammar formalisms, there is really no conceptual distinction to be drawn in LTAG between the lexicon and the grammar: the grammatical rules are just lexical properties. draw attention to the considerable redundancy inherent in LTAG lexicons that are expressed in a flat manner with no sharing of structure or properties across the elementary trees. For example, XTAG curre</context>
</contexts>
<marker>Evans, Gazdar, 1989</marker>
<rawString>Roger Evans &amp; Gerald Gazdar. 1989a. Infernce in DATR. In EACL-89, 66-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Evans</author>
<author>Gerald Gazdar</author>
</authors>
<title>The semantics of DATR.</title>
<date>1989</date>
<booktitle>In AISB-89,</booktitle>
<pages>79--87</pages>
<contexts>
<context position="1856" citStr="Evans &amp; Gazdar, 1989" startWordPosition="280" endWordPosition="283">sh which subsequently became the basis for the grammar used in the XTAG system, a wide-coverage LTAG parser (Doran et al., 1994b; Doran et al., 1994a; XTAG Research Group, 1995). The advent of such large grammars gives rise to questions of efficient representation, and the fully lexicalized character of the LTAG formalism suggests that recent research into lexical representation might be a place to look for answers (see for example Briscoe et a/.(1993); Daelemans &amp; Gazdar(1992)). In this paper we explore this suggestion by showing how the lexical knowledge representation language (LKRL) DATR (Evans &amp; Gazdar, 1989a; Evans &amp; Gazdar, 1989b) can be used to formulate a compact, hierarchical encoding of an LTAG. The issue of efficient representation for LTAG&apos; is discussed by Vijay-Shanker &amp; Schabes (1992), who lAs with all fully lexicalized grammar formalisms, there is really no conceptual distinction to be drawn in LTAG between the lexicon and the grammar: the grammatical rules are just lexical properties. draw attention to the considerable redundancy inherent in LTAG lexicons that are expressed in a flat manner with no sharing of structure or properties across the elementary trees. For example, XTAG curre</context>
</contexts>
<marker>Evans, Gazdar, 1989</marker>
<rawString>Roger Evans &amp; Gerald Gazdar. 1989b. The semantics of DATR. In AISB-89, 79-87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel P Flickinger</author>
</authors>
<date>1987</date>
<booktitle>Lexical Rules in the Hierarchical Lexicon. Ph.D. thesis,</booktitle>
<institution>Stanford Univ.</institution>
<contexts>
<context position="3150" citStr="Flickinger (1987)" startWordPosition="489" endWordPosition="490"> of trees (typically around 20) drawn from a set of over 500 elementary trees. Many of these trees have structure in common, many of the lexemes have the same tree families, and many of the trees within families are systematically related in ways which other formalisms capture using transformations or metarules. However, the LTAG formalism itself does not provide any direct support for capturing such regularities. Vijay-Shanker &amp; Schabes address this problem by introducing a hierarchical lexicon structure with monotonic inheritance and lexical rules, using an approach loosely based on that of Flickinger (1987) but tailored for LTAG trees rather than HPSG subcategorization lists. Becker (1993; 1994) proposes a slightly different solution, combining an inheritance component and a set of metarules2. We share their perception of the problem and agree that adopting a hierarchical approach provides the best available solution to it. However, rather than creating a hierarchical lexical formalism that is specific to the LTAG problem, we have used DATR, an LKRL that is already quite widely known and used. From an LTAG perspective, it makes sense to use an already available LKRL that was specifically designe</context>
</contexts>
<marker>Flickinger, 1987</marker>
<rawString>Daniel P. Flickinger. 1987. Lexical Rules in the Hierarchical Lexicon. Ph.D. thesis, Stanford Univ.</rawString>
</citation>
<citation valid="true">
<title>class assignment: a unified account for russian.</title>
<date>1994</date>
<booktitle>In Geert Booij &amp; Jaap van Marle, ed., Yearbook of Morphology</booktitle>
<editor>Norman Fraser &amp; Greville Corbett. in press. Gender, animacy, &amp; declensional</editor>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="3240" citStr="(1993; 1994)" startWordPosition="503" endWordPosition="504">ees have structure in common, many of the lexemes have the same tree families, and many of the trees within families are systematically related in ways which other formalisms capture using transformations or metarules. However, the LTAG formalism itself does not provide any direct support for capturing such regularities. Vijay-Shanker &amp; Schabes address this problem by introducing a hierarchical lexicon structure with monotonic inheritance and lexical rules, using an approach loosely based on that of Flickinger (1987) but tailored for LTAG trees rather than HPSG subcategorization lists. Becker (1993; 1994) proposes a slightly different solution, combining an inheritance component and a set of metarules2. We share their perception of the problem and agree that adopting a hierarchical approach provides the best available solution to it. However, rather than creating a hierarchical lexical formalism that is specific to the LTAG problem, we have used DATR, an LKRL that is already quite widely known and used. From an LTAG perspective, it makes sense to use an already available LKRL that was specifically designed to address these kinds of representational issues. From a DATR perspective, LTAG present</context>
<context position="5563" citStr="(1992; 1994)" startWordPosition="877" endWordPosition="878">ch the tree nodes are labeled with syntactic categories and feature information and there is at least one leaf node labeled with a lexical category (such lexical leaf nodes are known as anchors). For example, the canonical tree for a ditransitive verb such as give is shown in figure 1. Following LTAG conventions (for the time being), the node labels here are gross syntactic category specifications to which additional featural information may be addeds, and are annotated to indicate node type: o indicates an anchor node, and j indicates a substitution node (where a 3See, for example, Bleiching (1992; 1994), Brown 84 Hippisley (1994), Corbett 84 Fraser (1993), Cahill (1990; 1993), Cahill 84 Evans (1990), Fraser 8,4 Corbett (in press), Gibbon (1992), Kilgarriff (1993), Kilgarriff 84 Gazdar (1995), Reinhard 84 Gibbon (1991). &apos;See, for example, Andry et al. (1992) on compilation, Kilbury et al. (1991) on coding DAGs, Duda 84 Gebhardi (1994) on dynamic querying, Langer (1994) on reverse querying, and Barg (1994), Light (1994), Light et al. (1993) and Kilbury et al. (1994) on automatic acquisition. And there are at least a dozen different DATR implementations available, on various platforms and progr</context>
<context position="26182" citStr="(1993; 1994)" startWordPosition="4334" endWordPosition="4335">rences may seem small, they allow us to take this significant representational step — significant because it is the tree structure embedding that allows us to view lexical rules as feature covariation constraints. The result is that while VijayShanker &amp; Schabes use a tree description language, a category description language and a further formalism for lexical rules, we can capture everything in one framework all of whose components (nonmonotonicity, covariation constraint handling, etc.) have already been independently motivated for other aspects of lexical description&amp;quot;. Becker&apos;s recent work (1993; 1994) is also directed at exactly tne problem we address in the present paper. Like him, we have employed an inheritance hierarchy. And, like him, we have employed a set of lexical rules (corresponding to his metarules). The key differences between our account and his are (1) 13Note that simplified fragment presented here does not get this right. It makes all feature specifications total descriptions. To correct this we would need to change TftEENODE so that only the values of &lt;right&gt;, &lt;left&gt; and &lt;parent&gt; default to undef. 14 As in the work cited in footnote 3, above. 82 that we have been able to u</context>
<context position="27891" citStr="(1994,11)" startWordPosition="4622" endWordPosition="4622">proposes to deal with these by stating &amp;quot;for each entry in the (syntactic) lexicon .. which metarules are applicable for this entry&amp;quot; (1993,126). We have no need to carry over this use of (meta)rule features since, in our account, lexical rules are not distinct from any other kind of property in the inheritance hierarchy. They can be stated at the most inclusive relevant node and can then be overridden at the exceptional descendant nodes. Nothing specific needs to be said about the nonexceptional nodes. Secondly, his metarules may themselves be more or less similar to each other and he suggests (1994,11) that these similarities could be captured if the metarules were also to be organized in a hierarchy. However, our approach allows us to deal with any such similarities in the main lexical hierarchy itself15 rather than by setting up a separate hierarchical component just for metarules (which appears to be what Becker has in mind). Thirdly, as he himself notes (1993,128), because his metarules map from elementary trees that are in the inheritance hierarchy to elementary trees that are outside it, most of the elementary trees actually used are not directly connected to the hierarchy (although t</context>
</contexts>
<marker>1994</marker>
<rawString>Norman Fraser &amp; Greville Corbett. in press. Gender, animacy, &amp; declensional class assignment: a unified account for russian. In Geert Booij &amp; Jaap van Marle, ed., Yearbook of Morphology 1994. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dafydd Gibbon</author>
</authors>
<title>ILEX: a linguistic approach to computational lexica.</title>
<date>1992</date>
<booktitle>Cornputatio Linguae: Aufsa(&amp;quot;tze zur algorithmischen und quantitativen Analyse der Sprache (Zeitschrift fu(&amp;quot;r Dialektologie und Linguistik, Beiheft</booktitle>
<volume>73</volume>
<pages>32--53</pages>
<editor>In Ursula Klenk, ed.,</editor>
<publisher>Franz Steiner Verlag,</publisher>
<location>Stuttgart.</location>
<contexts>
<context position="5707" citStr="Gibbon (1992)" startWordPosition="899" endWordPosition="900">gory (such lexical leaf nodes are known as anchors). For example, the canonical tree for a ditransitive verb such as give is shown in figure 1. Following LTAG conventions (for the time being), the node labels here are gross syntactic category specifications to which additional featural information may be addeds, and are annotated to indicate node type: o indicates an anchor node, and j indicates a substitution node (where a 3See, for example, Bleiching (1992; 1994), Brown 84 Hippisley (1994), Corbett 84 Fraser (1993), Cahill (1990; 1993), Cahill 84 Evans (1990), Fraser 8,4 Corbett (in press), Gibbon (1992), Kilgarriff (1993), Kilgarriff 84 Gazdar (1995), Reinhard 84 Gibbon (1991). &apos;See, for example, Andry et al. (1992) on compilation, Kilbury et al. (1991) on coding DAGs, Duda 84 Gebhardi (1994) on dynamic querying, Langer (1994) on reverse querying, and Barg (1994), Light (1994), Light et al. (1993) and Kilbury et al. (1994) on automatic acquisition. And there are at least a dozen different DATR implementations available, on various platforms and programming languages. 51n fact, LTAG commonly distinguishes two sets of features at each node (top and bottom), but for simplicity we shall assume j</context>
</contexts>
<marker>Gibbon, 1992</marker>
<rawString>Dafydd Gibbon. 1992. ILEX: a linguistic approach to computational lexica. In Ursula Klenk, ed., Cornputatio Linguae: Aufsa(&amp;quot;tze zur algorithmischen und quantitativen Analyse der Sprache (Zeitschrift fu(&amp;quot;r Dialektologie und Linguistik, Beiheft 73), 32-53. Franz Steiner Verlag, Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>L S Levy</author>
<author>M Takahashi</author>
</authors>
<title>Tree adjunct grammars.</title>
<date>1975</date>
<journal>J. Comput. Syst. Sci.,</journal>
<pages>1--1</pages>
<contexts>
<context position="948" citStr="Joshi et al., 1975" startWordPosition="131" endWordPosition="134">iences University of Sussex davidwOcogs.susx.ac.uk Abstract This paper shows how DATR, a widely used formal language for lexical knowledge representation, can be used to define an LTAG lexicon as an inheritance hierarchy with internal lexical rules. A bottom-up featural encoding is used for LTAG trees and this allows lexical rules to be implemented as covariation constraints within feature structures. Such an approach eliminates the considerable redundancy otherwise associated with an LTAG lexicon. 1 Introduction The Tree Adjoining Grammar (TAG) formalism was first introduced two decades ago (Joshi et al., 1975), and since then there has been a steady stream of theoretical work using the formalism. But it is only more recently that grammars of non-trivial size have been developed: Abeille, Bishop, Cote &amp; Schabes (1990) describe a feature-based Lexicalized Tree Adjoining Grammar (LTAG) for English which subsequently became the basis for the grammar used in the XTAG system, a wide-coverage LTAG parser (Doran et al., 1994b; Doran et al., 1994a; XTAG Research Group, 1995). The advent of such large grammars gives rise to questions of efficient representation, and the fully lexicalized character of the LTA</context>
</contexts>
<marker>Joshi, Levy, Takahashi, 1975</marker>
<rawString>A. K. Joshi, L. S. Levy, &amp; M. Takahashi. 1975. Tree adjunct grammars. J. Comput. Syst. Sci., 1O(1):136-163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Kilbury</author>
<author>Petra Naerger</author>
<author>Ingrid Renz</author>
</authors>
<title>DATR as a lexical component for PATR.</title>
<date>1991</date>
<booktitle>In EACL-91,</booktitle>
<pages>137--142</pages>
<contexts>
<context position="5860" citStr="Kilbury et al. (1991)" startWordPosition="921" endWordPosition="924">ollowing LTAG conventions (for the time being), the node labels here are gross syntactic category specifications to which additional featural information may be addeds, and are annotated to indicate node type: o indicates an anchor node, and j indicates a substitution node (where a 3See, for example, Bleiching (1992; 1994), Brown 84 Hippisley (1994), Corbett 84 Fraser (1993), Cahill (1990; 1993), Cahill 84 Evans (1990), Fraser 8,4 Corbett (in press), Gibbon (1992), Kilgarriff (1993), Kilgarriff 84 Gazdar (1995), Reinhard 84 Gibbon (1991). &apos;See, for example, Andry et al. (1992) on compilation, Kilbury et al. (1991) on coding DAGs, Duda 84 Gebhardi (1994) on dynamic querying, Langer (1994) on reverse querying, and Barg (1994), Light (1994), Light et al. (1993) and Kilbury et al. (1994) on automatic acquisition. And there are at least a dozen different DATR implementations available, on various platforms and programming languages. 51n fact, LTAG commonly distinguishes two sets of features at each node (top and bottom), but for simplicity we shall assume just one set in this paper. fully specified tree with a compatible root label may be attachedr In representing such a tree in DATR, we do two things. Firs</context>
</contexts>
<marker>Kilbury, Naerger, Renz, 1991</marker>
<rawString>James Kilbury, Petra [Barg] Naerger, &amp; Ingrid Renz. 1991. DATR as a lexical component for PATR. In EACL-91, 137-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Kilbury</author>
<author>Petra Barg</author>
<author>Ingrid Renz</author>
</authors>
<title>Simulation lexicalischen erwerbs.</title>
<date>1994</date>
<booktitle>In Christopher Babel 8z Gert Rickheit Sascha W. Felix, ed, Kognitive Linguistik: Repraesentation und Prozesse,</booktitle>
<pages>251--271</pages>
<publisher>Westdeutscher Verlag, Opladen.</publisher>
<contexts>
<context position="6033" citStr="Kilbury et al. (1994)" startWordPosition="952" endWordPosition="955">are annotated to indicate node type: o indicates an anchor node, and j indicates a substitution node (where a 3See, for example, Bleiching (1992; 1994), Brown 84 Hippisley (1994), Corbett 84 Fraser (1993), Cahill (1990; 1993), Cahill 84 Evans (1990), Fraser 8,4 Corbett (in press), Gibbon (1992), Kilgarriff (1993), Kilgarriff 84 Gazdar (1995), Reinhard 84 Gibbon (1991). &apos;See, for example, Andry et al. (1992) on compilation, Kilbury et al. (1991) on coding DAGs, Duda 84 Gebhardi (1994) on dynamic querying, Langer (1994) on reverse querying, and Barg (1994), Light (1994), Light et al. (1993) and Kilbury et al. (1994) on automatic acquisition. And there are at least a dozen different DATR implementations available, on various platforms and programming languages. 51n fact, LTAG commonly distinguishes two sets of features at each node (top and bottom), but for simplicity we shall assume just one set in this paper. fully specified tree with a compatible root label may be attachedr In representing such a tree in DATR, we do two things. First, in keeping with the radically lexicalist character of LTAG, we describe the tree structure from its (lexical) anchor upwards7, using a variant of Kilbury&apos;s (1990) bottom-</context>
</contexts>
<marker>Kilbury, Barg, Renz, 1994</marker>
<rawString>James Kilbury, Petra Barg, &amp; Ingrid Renz. 1994. Simulation lexicalischen erwerbs. In Christopher Babel 8z Gert Rickheit Sascha W. Felix, ed, Kognitive Linguistik: Repraesentation und Prozesse, 251-271. Westdeutscher Verlag, Opladen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Kilbury</author>
</authors>
<title>Encoding constituent structure in feature structures.</title>
<date>1990</date>
<institution>Univ. of Duesseldorf, Duesseldorf.</institution>
<note>Unpublished manuscript,</note>
<marker>Kilbury, 1990</marker>
<rawString>James Kilbury. 1990. Encoding constituent structure in feature structures. Unpublished manuscript, Univ. of Duesseldorf, Duesseldorf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff Sz Gerald Gazdar</author>
</authors>
<title>Polysemous relations.</title>
<date>1995</date>
<booktitle>Grammar &amp; meaning: essays in honour of Sir John Lyons,</booktitle>
<pages>1--25</pages>
<editor>In Frank Palmer, ed.,</editor>
<publisher>CUP.</publisher>
<contexts>
<context position="5755" citStr="Gazdar (1995)" startWordPosition="905" endWordPosition="906">rs). For example, the canonical tree for a ditransitive verb such as give is shown in figure 1. Following LTAG conventions (for the time being), the node labels here are gross syntactic category specifications to which additional featural information may be addeds, and are annotated to indicate node type: o indicates an anchor node, and j indicates a substitution node (where a 3See, for example, Bleiching (1992; 1994), Brown 84 Hippisley (1994), Corbett 84 Fraser (1993), Cahill (1990; 1993), Cahill 84 Evans (1990), Fraser 8,4 Corbett (in press), Gibbon (1992), Kilgarriff (1993), Kilgarriff 84 Gazdar (1995), Reinhard 84 Gibbon (1991). &apos;See, for example, Andry et al. (1992) on compilation, Kilbury et al. (1991) on coding DAGs, Duda 84 Gebhardi (1994) on dynamic querying, Langer (1994) on reverse querying, and Barg (1994), Light (1994), Light et al. (1993) and Kilbury et al. (1994) on automatic acquisition. And there are at least a dozen different DATR implementations available, on various platforms and programming languages. 51n fact, LTAG commonly distinguishes two sets of features at each node (top and bottom), but for simplicity we shall assume just one set in this paper. fully specified tree </context>
</contexts>
<marker>Gazdar, 1995</marker>
<rawString>Adam Kilgarriff Sz Gerald Gazdar. 1995. Polysemous relations. In Frank Palmer, ed., Grammar &amp; meaning: essays in honour of Sir John Lyons, 1-25. CUP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>Inheriting verb alternations.</title>
<date>1993</date>
<booktitle>In EACL-93,</booktitle>
<pages>213--221</pages>
<contexts>
<context position="5726" citStr="Kilgarriff (1993)" startWordPosition="901" endWordPosition="902">cal leaf nodes are known as anchors). For example, the canonical tree for a ditransitive verb such as give is shown in figure 1. Following LTAG conventions (for the time being), the node labels here are gross syntactic category specifications to which additional featural information may be addeds, and are annotated to indicate node type: o indicates an anchor node, and j indicates a substitution node (where a 3See, for example, Bleiching (1992; 1994), Brown 84 Hippisley (1994), Corbett 84 Fraser (1993), Cahill (1990; 1993), Cahill 84 Evans (1990), Fraser 8,4 Corbett (in press), Gibbon (1992), Kilgarriff (1993), Kilgarriff 84 Gazdar (1995), Reinhard 84 Gibbon (1991). &apos;See, for example, Andry et al. (1992) on compilation, Kilbury et al. (1991) on coding DAGs, Duda 84 Gebhardi (1994) on dynamic querying, Langer (1994) on reverse querying, and Barg (1994), Light (1994), Light et al. (1993) and Kilbury et al. (1994) on automatic acquisition. And there are at least a dozen different DATR implementations available, on various platforms and programming languages. 51n fact, LTAG commonly distinguishes two sets of features at each node (top and bottom), but for simplicity we shall assume just one set in this</context>
</contexts>
<marker>Kilgarriff, 1993</marker>
<rawString>Adam Kilgarriff. 1993. Inheriting verb alternations. In EACL-93, 213-221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hagen Langer</author>
</authors>
<title>Reverse queries in DATR.</title>
<date>1994</date>
<booktitle>In COLING-94,</booktitle>
<pages>1089--1095</pages>
<contexts>
<context position="5935" citStr="Langer (1994)" startWordPosition="936" endWordPosition="937">actic category specifications to which additional featural information may be addeds, and are annotated to indicate node type: o indicates an anchor node, and j indicates a substitution node (where a 3See, for example, Bleiching (1992; 1994), Brown 84 Hippisley (1994), Corbett 84 Fraser (1993), Cahill (1990; 1993), Cahill 84 Evans (1990), Fraser 8,4 Corbett (in press), Gibbon (1992), Kilgarriff (1993), Kilgarriff 84 Gazdar (1995), Reinhard 84 Gibbon (1991). &apos;See, for example, Andry et al. (1992) on compilation, Kilbury et al. (1991) on coding DAGs, Duda 84 Gebhardi (1994) on dynamic querying, Langer (1994) on reverse querying, and Barg (1994), Light (1994), Light et al. (1993) and Kilbury et al. (1994) on automatic acquisition. And there are at least a dozen different DATR implementations available, on various platforms and programming languages. 51n fact, LTAG commonly distinguishes two sets of features at each node (top and bottom), but for simplicity we shall assume just one set in this paper. fully specified tree with a compatible root label may be attachedr In representing such a tree in DATR, we do two things. First, in keeping with the radically lexicalist character of LTAG, we describe </context>
</contexts>
<marker>Langer, 1994</marker>
<rawString>Hagen Langer. 1994. Reverse queries in DATR. In COLING-94, 1089-1095.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Light</author>
</authors>
<title>Sabine Reinhard, Sz Marie Boyle-Hinrichs.</title>
<date>1993</date>
<booktitle>In EACL-93,</booktitle>
<pages>471</pages>
<marker>Light, 1993</marker>
<rawString>Marc Light, Sabine Reinhard, Sz Marie Boyle-Hinrichs. 1993. INSYST: an automatic inserter system for hierarchical lexica. In EACL-93, page 471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Light</author>
</authors>
<title>Classification in feature-based default inheritance hierarchies.</title>
<date>1994</date>
<booktitle>Proceedings of KONVENS-94,</booktitle>
<pages>220--229</pages>
<editor>In H. Trost, ed.,</editor>
<contexts>
<context position="5986" citStr="Light (1994)" startWordPosition="945" endWordPosition="946">atural information may be addeds, and are annotated to indicate node type: o indicates an anchor node, and j indicates a substitution node (where a 3See, for example, Bleiching (1992; 1994), Brown 84 Hippisley (1994), Corbett 84 Fraser (1993), Cahill (1990; 1993), Cahill 84 Evans (1990), Fraser 8,4 Corbett (in press), Gibbon (1992), Kilgarriff (1993), Kilgarriff 84 Gazdar (1995), Reinhard 84 Gibbon (1991). &apos;See, for example, Andry et al. (1992) on compilation, Kilbury et al. (1991) on coding DAGs, Duda 84 Gebhardi (1994) on dynamic querying, Langer (1994) on reverse querying, and Barg (1994), Light (1994), Light et al. (1993) and Kilbury et al. (1994) on automatic acquisition. And there are at least a dozen different DATR implementations available, on various platforms and programming languages. 51n fact, LTAG commonly distinguishes two sets of features at each node (top and bottom), but for simplicity we shall assume just one set in this paper. fully specified tree with a compatible root label may be attachedr In representing such a tree in DATR, we do two things. First, in keeping with the radically lexicalist character of LTAG, we describe the tree structure from its (lexical) anchor upward</context>
</contexts>
<marker>Light, 1994</marker>
<rawString>Marc Light. 1994. Classification in feature-based default inheritance hierarchies. In H. Trost, ed., Proceedings of KONVENS-94, 220-229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Reinhard</author>
</authors>
<title>8z Dafydd Gibbon.</title>
<date>1991</date>
<booktitle>Prosodic inheritance &amp; morphological generalisations. In EA CL91,</booktitle>
<pages>131--136</pages>
<marker>Reinhard, 1991</marker>
<rawString>Sabine Reinhard 8z Dafydd Gibbon. 1991. Prosodic inheritance &amp; morphological generalisations. In EA CL91, 131-136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Rogers</author>
<author>K Vijay-Shanker</author>
</authors>
<title>Reasoning .with descriptions of trees.</title>
<date>1992</date>
<booktitle>In ACL-92,</booktitle>
<pages>72--80</pages>
<contexts>
<context position="23880" citStr="Rogers &amp; Vijay-Shanker (1992)" startWordPosition="3952" endWordPosition="3955">nce to any given lexeme — multiple application could be supported, by making multiple instances inherit from some common rule specification, but in our current treatment such instances would require different rule names. 6 Comparison with related work As noted above, Vijay-Shanker &amp; Schabes (1992) have also proposed an inheritance-based approach to this problem. They use monotonic inheritance to build up partial descriptions of trees: each description is a finite set of dominance, immediate dominance and linear precedence statements about tree nodes in a tree description language developed by Rogers &amp; Vijay-Shanker (1992), and category information is located in the node labels. This differs from our approach in a number of ways. First, our use of nonmonotonic inheritance allows us to manipulate total instead of partial descriptions of trees. The abstract verb class in the Vijay-Shanker &amp; Schabes account subsumes both intransitive and transitive verb classes but is not identical to either — a minimal-satisfying-model step is required to map partial tree descriptions into actual trees. In our analysis, VERB is the intransitive verb class, with complements specifically marked as undefined: thus VERB: &lt;right&gt; == u</context>
</contexts>
<marker>Rogers, Vijay-Shanker, 1992</marker>
<rawString>James Rogers &amp; K. Vijay-Shanker. 1992. Reasoning .with descriptions of trees. In ACL-92, 72-80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>Yves Schabes</author>
</authors>
<title>Structure sharing in lexicalized tree-adjoining grammar.</title>
<date>1992</date>
<booktitle>In COLING-92,</booktitle>
<pages>205--211</pages>
<contexts>
<context position="2046" citStr="Vijay-Shanker &amp; Schabes (1992)" startWordPosition="311" endWordPosition="314">he advent of such large grammars gives rise to questions of efficient representation, and the fully lexicalized character of the LTAG formalism suggests that recent research into lexical representation might be a place to look for answers (see for example Briscoe et a/.(1993); Daelemans &amp; Gazdar(1992)). In this paper we explore this suggestion by showing how the lexical knowledge representation language (LKRL) DATR (Evans &amp; Gazdar, 1989a; Evans &amp; Gazdar, 1989b) can be used to formulate a compact, hierarchical encoding of an LTAG. The issue of efficient representation for LTAG&apos; is discussed by Vijay-Shanker &amp; Schabes (1992), who lAs with all fully lexicalized grammar formalisms, there is really no conceptual distinction to be drawn in LTAG between the lexicon and the grammar: the grammatical rules are just lexical properties. draw attention to the considerable redundancy inherent in LTAG lexicons that are expressed in a flat manner with no sharing of structure or properties across the elementary trees. For example, XTAG currently includes over 100,000 lexemes, each of which is associated with a family of trees (typically around 20) drawn from a set of over 500 elementary trees. Many of these trees have structure</context>
<context position="23549" citStr="Vijay-Shanker &amp; Schabes (1992)" startWordPosition="3902" endWordPosition="3905">cal rules themselves, and control over their application. The last section showed how the whq lexical rule could be built by a single minor addition to that for topicalisation. However, it is worth noting that, in common with other DATR specifications, the lexical rules presented here are rule instances which can only be applied once to any given lexeme — multiple application could be supported, by making multiple instances inherit from some common rule specification, but in our current treatment such instances would require different rule names. 6 Comparison with related work As noted above, Vijay-Shanker &amp; Schabes (1992) have also proposed an inheritance-based approach to this problem. They use monotonic inheritance to build up partial descriptions of trees: each description is a finite set of dominance, immediate dominance and linear precedence statements about tree nodes in a tree description language developed by Rogers &amp; Vijay-Shanker (1992), and category information is located in the node labels. This differs from our approach in a number of ways. First, our use of nonmonotonic inheritance allows us to manipulate total instead of partial descriptions of trees. The abstract verb class in the Vijay-Shanker</context>
</contexts>
<marker>Vijay-Shanker, Schabes, 1992</marker>
<rawString>K. Vijay-Shanker &amp; Yves Schabes. 1992. Structure sharing in lexicalized tree-adjoining grammar. In COLING-92, 205-211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>The XTAG Research Group</author>
</authors>
<title>A lexicalized tree adjoining grammar for English.</title>
<date>1995</date>
<tech>Technical Report IRCS Report 95-03,</tech>
<institution>The Institute for Research in Cognitive Science, Univ. of Pennsylvania.</institution>
<contexts>
<context position="1413" citStr="Group, 1995" startWordPosition="211" endWordPosition="212">ssociated with an LTAG lexicon. 1 Introduction The Tree Adjoining Grammar (TAG) formalism was first introduced two decades ago (Joshi et al., 1975), and since then there has been a steady stream of theoretical work using the formalism. But it is only more recently that grammars of non-trivial size have been developed: Abeille, Bishop, Cote &amp; Schabes (1990) describe a feature-based Lexicalized Tree Adjoining Grammar (LTAG) for English which subsequently became the basis for the grammar used in the XTAG system, a wide-coverage LTAG parser (Doran et al., 1994b; Doran et al., 1994a; XTAG Research Group, 1995). The advent of such large grammars gives rise to questions of efficient representation, and the fully lexicalized character of the LTAG formalism suggests that recent research into lexical representation might be a place to look for answers (see for example Briscoe et a/.(1993); Daelemans &amp; Gazdar(1992)). In this paper we explore this suggestion by showing how the lexical knowledge representation language (LKRL) DATR (Evans &amp; Gazdar, 1989a; Evans &amp; Gazdar, 1989b) can be used to formulate a compact, hierarchical encoding of an LTAG. The issue of efficient representation for LTAG&apos; is discussed </context>
</contexts>
<marker>Group, 1995</marker>
<rawString>The XTAG Research Group. 1995. A lexicalized tree adjoining grammar for English. Technical Report IRCS Report 95-03, The Institute for Research in Cognitive Science, Univ. of Pennsylvania.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>