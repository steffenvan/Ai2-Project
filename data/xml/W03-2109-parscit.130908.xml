<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000300">
<title confidence="0.94276">
Interpreter for Highly Portable Spoken Dialogue System
</title>
<author confidence="0.852376">
Masamitsu Umeda Satoru Kogure Seiichi Nakagawa
</author>
<affiliation confidence="0.997529666666667">
Department of Information and Faculty of Education, Department of Information and
Computer Science, Aichi University of Education Computer Science,
Toyohashi University of Technology Aichi, JAPAN, 448-8542 Toyohashi University of Technology
</affiliation>
<address confidence="0.530454">
Aichi, JAPAN, 441-8580 skogure@auecc.aichi-edu.ac.jp Aichi, JAPAN, 441-8580
</address>
<email confidence="0.986478">
umeda@slp.ics.tut.ac.jp nakagawa@slp.ics.tut.ac.jp
</email>
<sectionHeader confidence="0.995321" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999901190476191">
Recently the technology for speech recog-
nition and language processing for spoken
dialogue systems has been improved, and
speech recognition systems and dialogue
systems have been developed to the ex-
tent of practical usage. In order to become
more practical, not only those fundamen-
tal techniques but also the techniques of
portability and expansibility should be de-
veloped. In our previous research, we
demonstrated the portability of the speech
recognition module to a developed portal
spoken dialogue system. And we con-
structed a dialogue strategy design tool
of dialogue script for controlling the dia-
logue strategy.
In this paper, we report a highly portable
interpreter using a commercial electronic
dictionary. We apply this to three do-
mains/tasks and confirm the validity of the
interpreter for each domain/task.
</bodyText>
<keyword confidence="0.5025795">
Keywords: spoken dialogue system, robust inter-
preter, portability, dialogue script
</keyword>
<sectionHeader confidence="0.999593" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9995282">
Recently, much research has been done on the ro-
bustness and reliability of spoken dialogue sys-
tems. We developed a “Mt.Fuji sightseeing guid-
ance” system which uses touch screen input, speech
input/output and graphical output, and have im-
proved the sub-modules of a speech recognizer, nat-
ural language interpreter, response generator and
multi-modal interface (Kai and Nakagawa, 1995;
Itoh et al., 1995; Denda et al., 1996; Kogure et al.,
1999; Nakagawa et al., 2000). General speaking, all
of these modules except for the speech recognition
module depended on a given task or domain.
As speech recognition systems are increasingly
being used in practical applications, spoken dialogue
systems will also become more widespread. How-
ever, the cost of developing a new spoken dialogue
system is enormous. The systems that have been de-
veloped so far can not be transferred to other do-
mains easily, and a highly-portable system that can
be easily adapted to another domain or task urgently
should be developed. There are several examples of
researches that focused on high portability and ex-
pansibility (Kaspar and Hoffmannn, 1998; Brond-
sted et al., 1998; Sutton et al., 1998; Sasajima et al.,
1999; Abella and Gorin, 1999; Levin et al., 2000).
In (Kaspar and Hoffmannn, 1998), a prototype
could be simply constructed even in a complicated
speech dialogue system using the PIA system, which
was implemented using Visual Basic. This sys-
tem placed priority on achieving high robustness of
speech recognition and high naturalness of gener-
ated dialogue. However, the system limited the task
to the domain of knowledge search. E. Levin et
al. reported the design and implementation of the
AT&amp;T Communicator mixed-initiative spoken dia-
logue system (Levin et al., 2000). The communica-
tor project sponsored by DARPA launched in 1999.
On the other hand, we have also considered the
portability of spoken dialogue system (Kogure and
Nakagawa, 2000). We showed from experience that
</bodyText>
<figureCaption confidence="0.999863">
Figure 1: System overview.
</figureCaption>
<bodyText confidence="0.991725075471698">
it was difficult to transfer from the Mt. Fuji sight-
seeing guidance existing system to the East Mikawa
sightseeing guidance.
Therefore,we built portable spoken dialogue sys-
tem developing tools based on GUI (Kogure and
Nakagawa, 2000). Especially, we focused on the
developing tools of spoken dialogue system for
database retrieval.
Some spoken language systems focused on ro-
bust matching to handle ungrammatical utterances
and illegal sentences. The Template Matcher (TM)
at the Stanford Research Institute (Moore and Pod-
lozny, 1991) instantiates competing templates, each
of which seeks to fill its slots with appropriate words
and phrases from the utterance. The template with
the highest score yields the semantic representation.
R.Kuhn and R.De Mori suggest a method where the
system’s rules for semantic interpretation are learnt
automatically from training data (Kuhn and Mori,
1995). The rules are encoded in forest of special-
ized decision trees called as Semantic Classifica-
tion Trees (SCTs). The learned rules are robust to
ungrammatical sentences and misrecognition in the
input. Minker compared a rule-based case frame
grammar analysis and a stochastics based case frame
grammar analysis for understanding(Minker, 1998).
Globally speaking, the performances of the stochas-
tic and rule-based parsers were comparable for ATIS
(Air Travel Information Service). Y. Wang sug-
gests a robust chart parser which is the major Spo-
ken Language Understanding (SLU) engine compo-
nent behind MiPad(Wang, 2001). The robustness to
ungrammaticality and noise can be attributed to its
ability of skipping minimum unparsable segments in
the input. The robust parsing algorithm is an exten-
sion of the bottom-up chart-parsing algorithm.
In (Nakagawa et al., 2000), the dialogue system
understands spontaneous speech which has many
ambiguous phenomena such as interjections, el-
lipses, inversions, repairs, unknown words and so
on, and responds to the user’s utterance. But the sys-
tem fails to analyze some utterances. “Incomplete-
ness” with the interpreter mainly causes the analy-
sis failure, and leads to domain/task dependent de-
velopment. Therefore, in this paper we focused on
improving the interpreter in order to make it a ro-
bust/portable one.
For this purpose, we used the EDR electronic dic-
tionary as a semantic dictionary and improved the
interpreter so that it had some new functions. Fi-
nally, we applied the system to three domains/tasks;
hotel retrieval, Mt.Fuji sightseeing guidance and lit-
erature retrieval.
</bodyText>
<figureCaption confidence="0.9995525">
Figure 2: An example of processing of correction
utterance.
Figure 3: An example of removing recognition error
caused by logging error.
</figureCaption>
<sectionHeader confidence="0.7573555" genericHeader="introduction">
2 Highly-portable System for Information
Retrieval from Database
</sectionHeader>
<bodyText confidence="0.9933945">
The proposed system in the dialogue processing
parts consists of semantics interpreter, retrieval
module, response generator and dialogue man-
ager which integrates three modules. The system
overview is shown in Figure 1.
Each module has the following roles:
</bodyText>
<listItem confidence="0.997628058823529">
• Speech Recognizer: This module recognizes
a user utterance via speech input and generates
a recognized sentence. We used SPOJUS (Kai
and Nakagawa, 1995) for this.
• Semantic Interpreter: This module under-
stands a user utterance via recognized sentence
and generates semantic representation.
• Retrieval Module: This module that extracts
the word as retrieval key word/phrase from the
semantic representation.
• Response Generator: This module is acti-
vated by a dialogue manager, selects a kind
of response strategies and generates a response
sentence.
• Text-to-speech Module: This module gener-
ates a response sentence, synthesizes speech
signals and playbacks this audio files to user.
</listItem>
<bodyText confidence="0.998348875">
In this paper, we focused on three modules: Se-
mantic Interpreter, Retrieval Module and Re-
sponse Generator. We believe that the Speech
Recognizer except for the language models and the
Text-to-speech Module are domain and task inde-
pendent. A domain/task adaptation technique of the
language models for the speech recognizer was pre-
liminarily evaluated (Kogure and Nakagawa, 2000).
</bodyText>
<sectionHeader confidence="0.95051" genericHeader="method">
3 Improving the Semantic Interpreter
</sectionHeader>
<subsectionHeader confidence="0.998295">
3.1 Adding New Functionality
</subsectionHeader>
<bodyText confidence="0.999971">
Our purpose is to improve our previous inter-
preter (Kogure and Nakagawa, 2000) to make it
more robust and portable. In this study, we imple-
mented new functions into the interpreter as follows:
</bodyText>
<listItem confidence="0.974892">
• Processing of Correction Utterance: The sys-
tem omits the word sequence as restatement
</listItem>
<figureCaption confidence="0.770198928571428">
and word preceding this sequence. The omitted
word sequence is the word or word sequence
which the user uses when he/she tells the sys-
tem to repair the wrong word or word sequence.
Figure 2 shows this process.
• Removing Recognition Error by Logging
Error: The system ignores non-semantic
words at the head or the tail of the sentence
caused by detection errors of beginning/ending
speech and adopts the grammaticaly longest
candidate from plural candidates as the valid
Figure 4: An example of improving keyword re-
trieval.
Figure 5: An example of concept classification.
</figureCaption>
<bodyText confidence="0.708696333333333">
sentence. Speech segmentation errors in back-
ground noise environments yield non-semantic
words. Figure 3 shows this process.
• Improving Keyword Retrieval: If an input
sentence is inadequate for parsing, the system
creates a retrieval condition from the semantic
labels which are attached to the word used as
the keyword in a sentence. Figure 4 shows this
process.
</bodyText>
<subsectionHeader confidence="0.9669905">
3.2 EDR Electronic Dictionary for Semantic
Interpreter
</subsectionHeader>
<bodyText confidence="0.999979363636364">
In the study, we clearly separated domain/task in-
dependent parts and dependent parts. When an ap-
plication developer wants to change a domain or
task, he/she modifies or prepares only domain/task
dependent parts, such as semantic features, proper
nouns, and so on. In this section, we describe the do-
main/task dependent information obtained from the
EDR electronic dictionary, which is used to analyze
the utterance in the interpreter.
The EDR electronic dictionary1 has widely been
used for research and application development of
</bodyText>
<footnote confidence="0.51837">
1http://www.jsa.co.jp/EDR/
</footnote>
<equation confidence="0.9825674">
r ✏
search[3cec34] :::
agent 30f6b0;30f746 =GA,
object 3f97b1 =WO
✒ ✑
</equation>
<figureCaption confidence="0.981762">
Figure 6: An example of case frame.
</figureCaption>
<bodyText confidence="0.999631">
Japanese language processing. We developed a high
portable semantic dictionary for the interpreter using
the EDR electronic dictionary.
</bodyText>
<subsectionHeader confidence="0.855935">
3.2.1 Concept Dictionary
</subsectionHeader>
<bodyText confidence="0.999981583333333">
The EDR electronic dictionary is constructed by a
hierarchical tree structure with classification records
which are composed of 410,000 concepts.
Figure 5 shows a part of concept classification
from “location.” “30f751” denotes the concept clas-
sification for “location.”
We assign the concept for domain/task dependent
words to the existing hierarchical structure. In Fig-
ure 5, “G-PLACE” is a keyword which is used for
referencing to a place. Since the morphological
analysis assigns the concept to the word, the inter-
preter can use the concept information.
</bodyText>
<subsectionHeader confidence="0.9828125">
3.2.2 Dictionary of Selectional Restriction for
Japanese Verbs
</subsectionHeader>
<bodyText confidence="0.99736625">
The interpreter analyzes an input sentence us-
ing the dictionary of selectional restrictions or case
frames for Japanese verbs, which are attached as a
co-occurrence dictionary for the EDR electronic dic-
tionary. This dictionary contains information related
to the surface case and the deep case about main
verbs. The interpreter converts the input sentence
to the case frame format information using this dic-
tionary, which is formalized from the information on
a deep case and the conceptual classification infor-
mation for every verb using the technique of a selec-
tional restriction.
Figure 6 shows the case frame of the verb
“search”. It shows that “3cec34” is the concept
classification of “search.” When the deep case is
“agent” and the surface case is “GA”, “30f6b0” or
“30f746” is selected, and when the deep case is “ob-
ject” and the surface case is “WO”, “3f97b1” is se-
lected. Where “GA” and “WO” are particle peculiars
(postpositions) in Japanese.
</bodyText>
<figureCaption confidence="0.995771">
Figure 7: An example of semantic interpreter.
</figureCaption>
<subsectionHeader confidence="0.896226">
3.3 An Example
</subsectionHeader>
<figureCaption confidence="0.773123">
Figure 7 shows an example of the interpreter flow.
</figureCaption>
<bodyText confidence="0.999886">
In Figure 7 , the user inputs a Japanese utterance
that means ”What hotels are there along Yamanote
Line?” in English. The interpreter executes the pro-
cesses of a morphological analysis and parsing, and
generates a parsing result that is represented by a
tree-structure in accordance to the prepared gram-
mar. This sentence’s verb is “ARU”(There are, in
English), and the concept classification of “ARU” is
“0e5a74”. The interpreter generates a semantic anal-
ysis result from the parsing result using this concept
classification information, that is, the case frame of
“ARU”. The system transfers the semantic analysis
result into a retrieval condition. This condition is
used to generate SQL language for the retrieval.
</bodyText>
<sectionHeader confidence="0.9987575" genericHeader="method">
4 Domain/Task Independency and
Dependency
</sectionHeader>
<subsectionHeader confidence="0.999156">
4.1 Domain/Task Independency/Dependency
</subsectionHeader>
<bodyText confidence="0.999956866666667">
In this section, we describe how to realize the high
portability which has the benefit of efficiency when
application developers design a new dialogue sys-
tem. That is, while the system performance is kept at
a certain level, we will shorten the period of develop-
ing the dialogue system as possible. Therefore, we
define the domain/task independency/dependency.
Domain independency means that application devel-
opers can use common data for all domains 2. Do-
main dependency means that applications are not do-
main independent. Task independency means that
application developers can use these common data
for all tasks 3 in a certain domain. Task dependency
means that is not task independent. Hereafter, four
data types are abbreviated as DI, DD, TI, TD, respec-
tively.
We clearly separated semantics, retrieval and re-
sponse modules into DI/DI, DD/TI, TI/TD, and TI/TI
parts, respectively. The system core was built whilst
keeping it completely domain and task independent
(DI/TI). We can divide data sets three types4 as
shown in Table 1; DI/TI, DD/TI and DD/TD. DI/TI
data sets served as information for a morphological
analysis except for nouns or verbs with respect to
the domain/task and so on. DD/TI data sets are used
as information for morphological analysis of nouns
or verbs with respect to the domain/task, informa-
tion for semantics analysis and so on. DD/TD data
sets are used as information for format that the sys-
tem responds the retrieval results and so on. In the
</bodyText>
<footnote confidence="0.939062166666667">
2Domain is a field or area of dialogue object.
3Task is the problem or process that the user wants to realize
in a particular domain.
4Since we define a domain and a task as the above, all do-
main independent data sets are surely task independent data
sets, therefore DI/TD data sets are none.
</footnote>
<tableCaption confidence="0.998635">
Table 1: Separation of domain/task dependent data sets and domain/task independent data sets.
</tableCaption>
<table confidence="0.976443545454545">
types Data
DI/TI syllable HMMs, morphological dictionary except
for noun and verb, syntactic grammar, noun and verb
semantic dictionary for dialogue processing ,seman-
tic dictionary, case frame
DD/TI morphological dictionary for noun and verb field in-
formation of database, database
DD/TD convert rule from semantic representation to re-
trieval pattern, display format of retrieval result
DI: domain independent DD: domain dependent
TI: task independent TD: task dependent
</table>
<figureCaption confidence="0.999556">
Figure 8: Task adaptation.
</figureCaption>
<bodyText confidence="0.983936">
next section, we describe how application develop-
ers prepare DD/TI and DD/TD data sets.
</bodyText>
<subsectionHeader confidence="0.98351">
4.2 System Core
</subsectionHeader>
<bodyText confidence="0.999818714285714">
We used Chasen 5 as Japanese morphological analy-
sis and PostgreSQL 6 as a database retrieval man-
agement system. We have constructed Semantics
Modules based on the Mt. Fuji sightseeing guidance
system (Nakagawa et al., 2000) with separating task
dependent and independent parts. All parts of the
system core are clearly DI/TI.
</bodyText>
<footnote confidence="0.9999105">
5http://chasen.aist-nara.ac.jp/
6http://www.pgsql.com/
</footnote>
<subsectionHeader confidence="0.998415">
4.3 Data Sets
</subsectionHeader>
<bodyText confidence="0.999366">
Data sets consist of DI/TI, DD/TI and DD/TD data
sets. The separated results are shown in Table 1.
</bodyText>
<sectionHeader confidence="0.99346" genericHeader="method">
5 Task Adaptation -Hotel Retrieval
System-
</sectionHeader>
<bodyText confidence="0.9811216">
Figure 8 shows the flow chart of the task adaptation.
We consider what kind of data may be prepared as
task-dependent knowledge when a new task is ap-
plied. In the proposed framework, the application
developer prepares the following:
</bodyText>
<listItem confidence="0.86773">
• A generally usable database (machine read-
able)
• The format information of each field of the
database
</listItem>
<figure confidence="0.990044466666667">
A generally usable database
&lt;name&gt;GrandCentralHotel&lt;/name&gt;
&lt;address&gt;2-2 Kandatsukasa-machi Chiyoda Tokyo&lt;/address&gt;
&lt;access&gt;On foot-from JR line and subway line Kanda station 3 minutes&lt;/access&gt;
&lt;land&gt;Tokyo Tower,TOKYO DOME&lt;/land&gt;
&lt;single&gt;8900,7120&lt;/single&gt;
:
The format information of each field of the database
Database format information
S|&lt;name&gt;|&lt;/name&gt;|name
S|&lt;address&gt;|&lt;/address&gt;|address
R|&lt;access&gt;|:|,|&lt;/access&gt;|access
R|&lt;land&gt;|:|,|&lt;/land&gt;|land
R|&lt;single&gt;|:|,|&lt;/single&gt;|single
:
Transfer format information
DATABASE:::
hotel: $hotel id{ int] , $name{ varchar (200) ] , $address{ varchar (200) ] ,
$checkin{ varchar (10) T-, $ checkout{ varchar (10) ] , $s cale{ varchar (2 0) ] ,
$room{varchar(20)],$smoking{varchar(20)]
acc: $acc id{ int] , $hotel id{ int] , $access[ ] { varchar (200)]
land: $land id{ int] , $hotel id{ int] , $land[ ] { varchar (200)]
single: $single id{ int] , $hotel id{ int] , $single[ ] { varchar (200)]
double: $double id{ int] , $hotel id{ int] , $double[ ] { varchar (200)]
twin: $twin id{ int] , $hotel id{ int] , $twin[ ] { varchar (200)]
inside hotel: $inside hotel id{ int] , $hotel id{ int] , $inside hotel[ ] { varchar (200)]
room: $room id{ int] , $Hotel id{ int] , $room[ ] Fvarchar (200)] _
service: $service id{ int] , 5hotel id{ int] , $service[ ] { varchar (200) ]
card: $card id{ ink , $hotel id{ ink , $card[ ] { varchar (200)]
DATABASE:::
</figure>
<figureCaption confidence="0.998985">
Figure 9: An example of hotel information and the definition given by a developer.
</figureCaption>
<listItem confidence="0.978143">
• A corpus of user utterances (dialogue exam-
ples)
</listItem>
<bodyText confidence="0.970071210526316">
The database information retrieval system first re-
quires a database (generally, one that is open to the
public) as a retrieval target. The format information
of each field in the database should be given in order
to access the database. In addition, since the dictio-
nary and language model are adapted to the database
information retrieval system in the task, a corpus of
the user utterances is required.
For examples, Figure 9 shows a generally usable
database and the format information of each field
of the database in a hotel retrieval system.
We used the hotel data of Hornet7 of the hotel ref-
erence site in Japan. The web page of this site can
be automatically translated into a database. The data
obtained from the HTML source is translated into a
data format like the generally usable database in
Figure 9.
This is translated according to the rules of Fig-
ure 9. In Figure 9, hotel information consists of
</bodyText>
<footnote confidence="0.853279">
7http://www.inn-info.co.jp/
</footnote>
<bodyText confidence="0.99989275">
the name of the hotel, address of the hotel, access
method to the hotel, and so on. Transfer format in-
formation describes how each item of hotel informa-
tion is processed.
In Figure 9, the landmarks, ’Tokyo Tower’ and
’Tokyo Dome’ are substituted into an array variable
’landmark’ which represents a landmark.
From the description of this file, the construction
of a database and creation of the table for every cat-
egory can be performed automatically. And word
registration to an analysis dictionary can be auto-
matically performed using a morphological-analysis
dictionary tool and a semantic dictionary registra-
tion tool. In this way, the hotel retrieval system was
constructed in about 10 hours.
The sample of a dialogue is shown in Figure 10.
</bodyText>
<sectionHeader confidence="0.994002" genericHeader="method">
6 Other Task Adaptation
</sectionHeader>
<subsectionHeader confidence="0.9985935">
6.1 Application to Mt. Fuji Sightseeing
Guidance System
</subsectionHeader>
<bodyText confidence="0.9997335">
We applied the portable system to Mt. Fuji sight-
seeing guidance system as well as the hotel retrieval
</bodyText>
<table confidence="0.926446776470588">
===== Speech Input =====
Input :What hotels are there around Kanda Station?
&lt;=== input utterance
:What hotels are there near in Kanda Station?
&lt;=== recognized sentence
===== System Output =====
5 facilities were found.
===== Retrieval Result =====
No.1 Olympic in Kanda, single room:11000yen
No.2 Grand Central Hotel,single room:8900yen,double room:12200yen, twin room:142000yen
No.3 Sun Hotel Kanda,single room:8400yen,twin room:13600yen
No.4 Central Hotel,single room:6500yen,double room:9800yen,twin room:9300yen
No.5 New Central Hotel,single room:7000yen,double room:7500yen,twin room:9600yen
===== System Output =====
The retrieval result is the above.
===== Speech Input =====
Input :Please display the data of No.2.
&lt;=== input utterance
:Please display the data of No.2.
&lt;=== recognized sentence
===== System Output =====
The details of the data of No.2 are displayed.
===== Retrieval Result = ====
*** Grand Central Hotel ***
2-2 Kandatsukasa-machi Chiyoda Tokyo
On foot-from JR line and subway line Kanda station 3 minutes
Landmark:
Room charge :: single:8900yen double:12200yen twin:14200yen
check in time is 15:00,check out time is 11:00
11 stories,157 rooms
No smoking room is Nothing
Inside-a-hotel equipment:conference room,vending
machine,laundromat,restaurant,banquet hall
indoor equipment:bath,toilet,air conditioning,lighting
desk,refrigerator,Japanese tea,drier,amenity
set,television,video broadcast,satellite broadcasting,telephone
service:Room service,copy,facsimile,word processor loan,iron
loan,trouser press loan,parcel delivery service
receptionist,cleaning receptionist,massage,mail servic
Usable credit card:DC,VISA,JCB,Master,UC,AMEX,Million,Dainers,NICOS
Figure 10: Dialogue example (hotel retrieval system).
===== Speech Input =====
Input :What is there in Kawaguchi Lake? &lt;=== input utterance-1
===== System Output =====
29 facilities were found.
The reference conditions which can be used by addition are shown below.
action : reading(9) lodging(7) appreciation(2) resting(2) camping(2)
kind : hotel(4) museum(3) concert hall(2) pension(2) campsite(2)
===== Speech Input =====
Input :Campsite. &lt;=== input utterance-2
===== System Output =====
(Facility-kind campsite) &lt;=== retrieval condition
2 facilities were found.
===== Retrieval Result =====
Kitagishi no Myoukosan(campsite), Kawaguchi Lake, 0yen
Tozawa center(campsite), Kawaguchi Lake, 4000yen
===== Speech Input =====
The retrieval result is the above.
Figure 11: Dialogue example (Mt. Fuji sightseeing guidance system).
= ==== System Output =====
This is a document retrieval system. Please input retrieval conditions.
= ==== Speech Input =====
input: Is there a paper on the multi modal? &lt;=== input utterance
Yet uh uh.. is there a paper on the multi modal such?
&lt;=== recognized sentence
= ==== System Output =====
23 papers were found.
Please input additional retrieval conditions.
= ==== Speech Input =====
input: This is related to Internet. &lt;=== input utterance
This is related to Internet. &lt;=== recognized sentence
===== System Output =====
3 paper were found.
= ==== Retrieval Result =====
No.1 A. Nakashima, et al.:&amp;quot;Intelligent network for personal move
communication&amp;quot;,Institute of Electronics, Information and
Communication Engineers Journal of Japan, 1995)
No.2 K. Ono, et al.:&amp;quot;Development of new generation communication
network&amp;quot;,Institute of Electronics, Information and Communication
Engineers Journal of Japan, 1995)
No.3 H. Aiso, et al.:&amp;quot;Future prospects of information highway&amp;quot;,
Institute of Electronics, Information and Communication Engineers
Journal of Japan, 1995)
===== System Output =====
The retrieval result is the above.
</table>
<figureCaption confidence="0.988682666666667">
Figure 12: Dialogue example (literature retrieval system).
system. An example of using the system is shown in
Figure 11.
</figureCaption>
<bodyText confidence="0.977636333333333">
The dialogue manager uses the dialogue script in
order to decide a dialogue strategy. This scripts have
the functions that if retrieval results are too numer-
ous, the system does not display all retrieval results
but inquires the user for additional retrieval condi-
tions. Of course, application developers can easily
change this dialogue strategy with the GUI tools.
For example, since input utterance-2 in
Figure 11 is not a sentence, the interpreter may fail
at the step of the semantic analysis. The interpreter
adapts Improved Keyword Retrieval described in
Section 3. The concept classification of campsite
is ”G-KIND”, so the interpreter directly changes
this word (see retrieval condition) to ”G-
KIND.”
</bodyText>
<subsectionHeader confidence="0.999573">
6.2 Application to Literature Retrieval System
</subsectionHeader>
<bodyText confidence="0.999561">
We also applied the system to a literature retrieval
system as well as the hotel retrieval system.
Figure 12 shows an example of using the system.
In this example the recognized sentence has
misrecognition and interjections. So the CFG gram-
mar cannot accept this sentence. Therefore, Re-
moving Recognition Error by Lodging Error
in Section 3 is used for this sentence. So yet,
uh, uh and such are omitted in the recognized
utterance.
If there are some related papers catalogues as
retrieved results and the user inputs an utterance
“Please display detailed information of the second
paper.”, the system displays detailed information
like the abstract of the second paper in the paper cat-
alogue.
</bodyText>
<subsectionHeader confidence="0.999347">
6.3 Evaluation of Portability
</subsectionHeader>
<bodyText confidence="0.999972571428571">
We evaluated the portability of our dialogue sys-
tem and robustness of the interpreter through the
Mt.Fuji sightseeing system and literature retrieval
system, and we confirmed that the developing period
was shortened to the time of the hotel’s one, which
means a reduction to 10 hours from 15 hours in the
previous system (Kogure and Nakagawa, 2000).
</bodyText>
<sectionHeader confidence="0.999061" genericHeader="conclusions">
7 Summary
</sectionHeader>
<bodyText confidence="0.9999615">
We improved the portable semantic interpreter for
spoken dialogue systems. The highly portable se-
mantic interpreter was constructed using the EDR
electronic dictionary. By the example of the hotel
retrieval system, we explained the structure of a high
portable system.
We also constructed the Mt.Fuji sightseeing sys-
tem and literature retrieval system, and we con-
firmed that the developing effort/period was reduced
to the same as the hotel’s one, which means a re-
duction to 10 hours from 15 hours in the previous
system.
In the near future, we will construct a system that
can change a task during a dialogue.
</bodyText>
<sectionHeader confidence="0.999193" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999967491525424">
A. Abella and A. L. Gorin. 1999. Construct algebra:
Analytical dialog management. In Proc. of ACL ’99,
pages 191–199.
T. Brondsted, B. Bai, and J. Olsen. 1998. The reward
service creation environment. an overview. In Proc. of
ICSLP ’98, pages 1175–1178.
A. Denda, T. Itoh, and S. Nakagawa. 1996. A robust
dialogue system with spontaneous speech and touch
screen. In Proc. ofICMI ’96, pages 144–151.
T. Itoh, M. Hidano, M. Yamamoto, and S. Nakagawa.
1995. Spontaneous speech understanding for a robust
dialogue system. In Proc. of NLPRS ’95, volume 2,
pages 538–543.
A. Kai and S. Nakagawa. 1995. Investigation on un-
known word processing and strategies for spontaneous
speech understanding. In Proc. ofEUROSPEECH ’95,
pages 2095–2098.
S. Kaspar and A. Hoffmannn. 1998. Semi-automated
incremental prototyping of spoken dialog systems. In
Proc. ofICSLP ’98, pages 859–862.
S. Kogure and S. Nakagawa. 2000. A portable devel-
opment tool for spoken dialogue systems. In Proc. of
ICSLP ’2000, pages 238–241.
S. Kogure, T. Itoh, and S. Nakagawa. 1999. A seman-
tic interpreter for a robust spoken dialogue system. In
Proc. ICMI ’99, pages II 61–66.
R. Kuhn and R. De Mori. 1995. The application of se-
mantic classification trees to natural language under-
standing. In IEEE Trans. Pattern Analysis and Ma-
chine Intelligence, volume 17, pages 449–460.
E. Levin, S. Narayanan, R. Pieraccini, K. Biatov, E. Bia-
tov, E. Bocchieri, G. Di Fabbrizio, W. Eckert, S. Lee,
A. Pokrovsky, M. Rahim, P. Ruscitti, and M. Walker.
2000. The at&amp;t-darpa communicator mixed-initiative
spoken dialog system. In Proc. of ICSLP ’2000, vol-
ume 2, pages 122–125.
W. Minker. 1998. Stochastic versus rule-based speech
understanding for information retrieval. In Speech
Communication, pages 223–147.
R. Moore and A. Podlozny. 1991. A template matcher
for robust nl interpretation. In Proc. Speech and
Natural Language Workshop, Morgan Kaufmann Inc.,
pages 190–194.
S. Nakagawa, S. Kogure, and T. Itoh. 2000. A semantic
interpreter and a cooperative response generator for a
robust spoken dialogue system. IJPRAI, 14(5):553–
569.
M. Sasajima, T. Yano, and Y. Kono. 1999. Europa:
generic framework for developing spoken dialogue
systes. In Proc. of EUROSPEECH ’99, pages 1163–
1166.
S. Sutton, R. Cole, J. de Villiers, J. Schalkwyk, P. Ver-
meulen, M. Macon, Y.Yan, E. Kaiser, B. Rundle,
K. Shobaki, P. Hosom, A. Kain, J. Wouters, D. Mas-
saro, and M. Cohen. 1998. Universal speech tools:the
cslu toolkit. In Proc. ofICSLP ’98, pages 3221–3224.
Y. Wang. 2001. Robust language understanding in mi-
pad. In Proc. of EUROSPEECH ’2001, pages 1555–
1558.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.418284">
<title confidence="0.998852">Interpreter for Highly Portable Spoken Dialogue System</title>
<author confidence="0.998454">Masamitsu Umeda Satoru Kogure Seiichi Nakagawa</author>
<affiliation confidence="0.999140666666667">Department of Information Faculty of Department of Information Computer Aichi University of Computer Toyohashi University of Aichi, JAPAN, Toyohashi University of</affiliation>
<address confidence="0.753802">Aichi, JAPAN, skogure@auecc.aichi-edu.ac.jp Aichi, JAPAN,</address>
<email confidence="0.857617">umeda@slp.ics.tut.ac.jpnakagawa@slp.ics.tut.ac.jp</email>
<abstract confidence="0.997318782608696">Recently the technology for speech recognition and language processing for spoken dialogue systems has been improved, and speech recognition systems and dialogue systems have been developed to the extent of practical usage. In order to become more practical, not only those fundamental techniques but also the techniques of portability and expansibility should be developed. In our previous research, we demonstrated the portability of the speech recognition module to a developed portal spoken dialogue system. And we constructed a dialogue strategy design tool of dialogue script for controlling the dialogue strategy. In this paper, we report a highly portable interpreter using a commercial electronic dictionary. We apply this to three domains/tasks and confirm the validity of the interpreter for each domain/task. dialogue system, robust inter-</abstract>
<intro confidence="0.658779">preter, portability, dialogue script</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Abella</author>
<author>A L Gorin</author>
</authors>
<title>Construct algebra: Analytical dialog management.</title>
<date>1999</date>
<booktitle>In Proc. of ACL ’99,</booktitle>
<pages>191--199</pages>
<contexts>
<context position="2614" citStr="Abella and Gorin, 1999" startWordPosition="384" endWordPosition="387">recognition systems are increasingly being used in practical applications, spoken dialogue systems will also become more widespread. However, the cost of developing a new spoken dialogue system is enormous. The systems that have been developed so far can not be transferred to other domains easily, and a highly-portable system that can be easily adapted to another domain or task urgently should be developed. There are several examples of researches that focused on high portability and expansibility (Kaspar and Hoffmannn, 1998; Brondsted et al., 1998; Sutton et al., 1998; Sasajima et al., 1999; Abella and Gorin, 1999; Levin et al., 2000). In (Kaspar and Hoffmannn, 1998), a prototype could be simply constructed even in a complicated speech dialogue system using the PIA system, which was implemented using Visual Basic. This system placed priority on achieving high robustness of speech recognition and high naturalness of generated dialogue. However, the system limited the task to the domain of knowledge search. E. Levin et al. reported the design and implementation of the AT&amp;T Communicator mixed-initiative spoken dialogue system (Levin et al., 2000). The communicator project sponsored by DARPA launched in 19</context>
</contexts>
<marker>Abella, Gorin, 1999</marker>
<rawString>A. Abella and A. L. Gorin. 1999. Construct algebra: Analytical dialog management. In Proc. of ACL ’99, pages 191–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brondsted</author>
<author>B Bai</author>
<author>J Olsen</author>
</authors>
<title>The reward service creation environment. an overview.</title>
<date>1998</date>
<booktitle>In Proc. of ICSLP ’98,</booktitle>
<pages>1175--1178</pages>
<contexts>
<context position="2546" citStr="Brondsted et al., 1998" startWordPosition="371" endWordPosition="375">ch recognition module depended on a given task or domain. As speech recognition systems are increasingly being used in practical applications, spoken dialogue systems will also become more widespread. However, the cost of developing a new spoken dialogue system is enormous. The systems that have been developed so far can not be transferred to other domains easily, and a highly-portable system that can be easily adapted to another domain or task urgently should be developed. There are several examples of researches that focused on high portability and expansibility (Kaspar and Hoffmannn, 1998; Brondsted et al., 1998; Sutton et al., 1998; Sasajima et al., 1999; Abella and Gorin, 1999; Levin et al., 2000). In (Kaspar and Hoffmannn, 1998), a prototype could be simply constructed even in a complicated speech dialogue system using the PIA system, which was implemented using Visual Basic. This system placed priority on achieving high robustness of speech recognition and high naturalness of generated dialogue. However, the system limited the task to the domain of knowledge search. E. Levin et al. reported the design and implementation of the AT&amp;T Communicator mixed-initiative spoken dialogue system (Levin et al</context>
</contexts>
<marker>Brondsted, Bai, Olsen, 1998</marker>
<rawString>T. Brondsted, B. Bai, and J. Olsen. 1998. The reward service creation environment. an overview. In Proc. of ICSLP ’98, pages 1175–1178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Denda</author>
<author>T Itoh</author>
<author>S Nakagawa</author>
</authors>
<title>A robust dialogue system with spontaneous speech and touch screen.</title>
<date>1996</date>
<booktitle>In Proc. ofICMI ’96,</booktitle>
<pages>144--151</pages>
<contexts>
<context position="1819" citStr="Denda et al., 1996" startWordPosition="254" endWordPosition="257">pply this to three domains/tasks and confirm the validity of the interpreter for each domain/task. Keywords: spoken dialogue system, robust interpreter, portability, dialogue script 1 Introduction Recently, much research has been done on the robustness and reliability of spoken dialogue systems. We developed a “Mt.Fuji sightseeing guidance” system which uses touch screen input, speech input/output and graphical output, and have improved the sub-modules of a speech recognizer, natural language interpreter, response generator and multi-modal interface (Kai and Nakagawa, 1995; Itoh et al., 1995; Denda et al., 1996; Kogure et al., 1999; Nakagawa et al., 2000). General speaking, all of these modules except for the speech recognition module depended on a given task or domain. As speech recognition systems are increasingly being used in practical applications, spoken dialogue systems will also become more widespread. However, the cost of developing a new spoken dialogue system is enormous. The systems that have been developed so far can not be transferred to other domains easily, and a highly-portable system that can be easily adapted to another domain or task urgently should be developed. There are severa</context>
</contexts>
<marker>Denda, Itoh, Nakagawa, 1996</marker>
<rawString>A. Denda, T. Itoh, and S. Nakagawa. 1996. A robust dialogue system with spontaneous speech and touch screen. In Proc. ofICMI ’96, pages 144–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Itoh</author>
<author>M Hidano</author>
<author>M Yamamoto</author>
<author>S Nakagawa</author>
</authors>
<title>Spontaneous speech understanding for a robust dialogue system.</title>
<date>1995</date>
<booktitle>In Proc. of NLPRS ’95,</booktitle>
<volume>2</volume>
<pages>538--543</pages>
<contexts>
<context position="1799" citStr="Itoh et al., 1995" startWordPosition="250" endWordPosition="253">ic dictionary. We apply this to three domains/tasks and confirm the validity of the interpreter for each domain/task. Keywords: spoken dialogue system, robust interpreter, portability, dialogue script 1 Introduction Recently, much research has been done on the robustness and reliability of spoken dialogue systems. We developed a “Mt.Fuji sightseeing guidance” system which uses touch screen input, speech input/output and graphical output, and have improved the sub-modules of a speech recognizer, natural language interpreter, response generator and multi-modal interface (Kai and Nakagawa, 1995; Itoh et al., 1995; Denda et al., 1996; Kogure et al., 1999; Nakagawa et al., 2000). General speaking, all of these modules except for the speech recognition module depended on a given task or domain. As speech recognition systems are increasingly being used in practical applications, spoken dialogue systems will also become more widespread. However, the cost of developing a new spoken dialogue system is enormous. The systems that have been developed so far can not be transferred to other domains easily, and a highly-portable system that can be easily adapted to another domain or task urgently should be develop</context>
</contexts>
<marker>Itoh, Hidano, Yamamoto, Nakagawa, 1995</marker>
<rawString>T. Itoh, M. Hidano, M. Yamamoto, and S. Nakagawa. 1995. Spontaneous speech understanding for a robust dialogue system. In Proc. of NLPRS ’95, volume 2, pages 538–543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kai</author>
<author>S Nakagawa</author>
</authors>
<title>Investigation on unknown word processing and strategies for spontaneous speech understanding.</title>
<date>1995</date>
<booktitle>In Proc. ofEUROSPEECH ’95,</booktitle>
<pages>2095--2098</pages>
<contexts>
<context position="1780" citStr="Kai and Nakagawa, 1995" startWordPosition="246" endWordPosition="249">ng a commercial electronic dictionary. We apply this to three domains/tasks and confirm the validity of the interpreter for each domain/task. Keywords: spoken dialogue system, robust interpreter, portability, dialogue script 1 Introduction Recently, much research has been done on the robustness and reliability of spoken dialogue systems. We developed a “Mt.Fuji sightseeing guidance” system which uses touch screen input, speech input/output and graphical output, and have improved the sub-modules of a speech recognizer, natural language interpreter, response generator and multi-modal interface (Kai and Nakagawa, 1995; Itoh et al., 1995; Denda et al., 1996; Kogure et al., 1999; Nakagawa et al., 2000). General speaking, all of these modules except for the speech recognition module depended on a given task or domain. As speech recognition systems are increasingly being used in practical applications, spoken dialogue systems will also become more widespread. However, the cost of developing a new spoken dialogue system is enormous. The systems that have been developed so far can not be transferred to other domains easily, and a highly-portable system that can be easily adapted to another domain or task urgentl</context>
<context position="6508" citStr="Kai and Nakagawa, 1995" startWordPosition="978" endWordPosition="981">retrieval. Figure 2: An example of processing of correction utterance. Figure 3: An example of removing recognition error caused by logging error. 2 Highly-portable System for Information Retrieval from Database The proposed system in the dialogue processing parts consists of semantics interpreter, retrieval module, response generator and dialogue manager which integrates three modules. The system overview is shown in Figure 1. Each module has the following roles: • Speech Recognizer: This module recognizes a user utterance via speech input and generates a recognized sentence. We used SPOJUS (Kai and Nakagawa, 1995) for this. • Semantic Interpreter: This module understands a user utterance via recognized sentence and generates semantic representation. • Retrieval Module: This module that extracts the word as retrieval key word/phrase from the semantic representation. • Response Generator: This module is activated by a dialogue manager, selects a kind of response strategies and generates a response sentence. • Text-to-speech Module: This module generates a response sentence, synthesizes speech signals and playbacks this audio files to user. In this paper, we focused on three modules: Semantic Interpreter,</context>
</contexts>
<marker>Kai, Nakagawa, 1995</marker>
<rawString>A. Kai and S. Nakagawa. 1995. Investigation on unknown word processing and strategies for spontaneous speech understanding. In Proc. ofEUROSPEECH ’95, pages 2095–2098.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kaspar</author>
<author>A Hoffmannn</author>
</authors>
<title>Semi-automated incremental prototyping of spoken dialog systems.</title>
<date>1998</date>
<booktitle>In Proc. ofICSLP ’98,</booktitle>
<pages>859--862</pages>
<contexts>
<context position="2522" citStr="Kaspar and Hoffmannn, 1998" startWordPosition="367" endWordPosition="370"> modules except for the speech recognition module depended on a given task or domain. As speech recognition systems are increasingly being used in practical applications, spoken dialogue systems will also become more widespread. However, the cost of developing a new spoken dialogue system is enormous. The systems that have been developed so far can not be transferred to other domains easily, and a highly-portable system that can be easily adapted to another domain or task urgently should be developed. There are several examples of researches that focused on high portability and expansibility (Kaspar and Hoffmannn, 1998; Brondsted et al., 1998; Sutton et al., 1998; Sasajima et al., 1999; Abella and Gorin, 1999; Levin et al., 2000). In (Kaspar and Hoffmannn, 1998), a prototype could be simply constructed even in a complicated speech dialogue system using the PIA system, which was implemented using Visual Basic. This system placed priority on achieving high robustness of speech recognition and high naturalness of generated dialogue. However, the system limited the task to the domain of knowledge search. E. Levin et al. reported the design and implementation of the AT&amp;T Communicator mixed-initiative spoken dial</context>
</contexts>
<marker>Kaspar, Hoffmannn, 1998</marker>
<rawString>S. Kaspar and A. Hoffmannn. 1998. Semi-automated incremental prototyping of spoken dialog systems. In Proc. ofICSLP ’98, pages 859–862.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kogure</author>
<author>S Nakagawa</author>
</authors>
<title>A portable development tool for spoken dialogue systems.</title>
<date>2000</date>
<booktitle>In Proc. of ICSLP ’2000,</booktitle>
<pages>238--241</pages>
<contexts>
<context position="3330" citStr="Kogure and Nakagawa, 2000" startWordPosition="497" endWordPosition="500">ucted even in a complicated speech dialogue system using the PIA system, which was implemented using Visual Basic. This system placed priority on achieving high robustness of speech recognition and high naturalness of generated dialogue. However, the system limited the task to the domain of knowledge search. E. Levin et al. reported the design and implementation of the AT&amp;T Communicator mixed-initiative spoken dialogue system (Levin et al., 2000). The communicator project sponsored by DARPA launched in 1999. On the other hand, we have also considered the portability of spoken dialogue system (Kogure and Nakagawa, 2000). We showed from experience that Figure 1: System overview. it was difficult to transfer from the Mt. Fuji sightseeing guidance existing system to the East Mikawa sightseeing guidance. Therefore,we built portable spoken dialogue system developing tools based on GUI (Kogure and Nakagawa, 2000). Especially, we focused on the developing tools of spoken dialogue system for database retrieval. Some spoken language systems focused on robust matching to handle ungrammatical utterances and illegal sentences. The Template Matcher (TM) at the Stanford Research Institute (Moore and Podlozny, 1991) instan</context>
<context position="7421" citStr="Kogure and Nakagawa, 2000" startWordPosition="1115" endWordPosition="1118">ule is activated by a dialogue manager, selects a kind of response strategies and generates a response sentence. • Text-to-speech Module: This module generates a response sentence, synthesizes speech signals and playbacks this audio files to user. In this paper, we focused on three modules: Semantic Interpreter, Retrieval Module and Response Generator. We believe that the Speech Recognizer except for the language models and the Text-to-speech Module are domain and task independent. A domain/task adaptation technique of the language models for the speech recognizer was preliminarily evaluated (Kogure and Nakagawa, 2000). 3 Improving the Semantic Interpreter 3.1 Adding New Functionality Our purpose is to improve our previous interpreter (Kogure and Nakagawa, 2000) to make it more robust and portable. In this study, we implemented new functions into the interpreter as follows: • Processing of Correction Utterance: The system omits the word sequence as restatement and word preceding this sequence. The omitted word sequence is the word or word sequence which the user uses when he/she tells the system to repair the wrong word or word sequence. Figure 2 shows this process. • Removing Recognition Error by Logging E</context>
<context position="24387" citStr="Kogure and Nakagawa, 2000" startWordPosition="3706" endWordPosition="3709">some related papers catalogues as retrieved results and the user inputs an utterance “Please display detailed information of the second paper.”, the system displays detailed information like the abstract of the second paper in the paper catalogue. 6.3 Evaluation of Portability We evaluated the portability of our dialogue system and robustness of the interpreter through the Mt.Fuji sightseeing system and literature retrieval system, and we confirmed that the developing period was shortened to the time of the hotel’s one, which means a reduction to 10 hours from 15 hours in the previous system (Kogure and Nakagawa, 2000). 7 Summary We improved the portable semantic interpreter for spoken dialogue systems. The highly portable semantic interpreter was constructed using the EDR electronic dictionary. By the example of the hotel retrieval system, we explained the structure of a high portable system. We also constructed the Mt.Fuji sightseeing system and literature retrieval system, and we confirmed that the developing effort/period was reduced to the same as the hotel’s one, which means a reduction to 10 hours from 15 hours in the previous system. In the near future, we will construct a system that can change a t</context>
</contexts>
<marker>Kogure, Nakagawa, 2000</marker>
<rawString>S. Kogure and S. Nakagawa. 2000. A portable development tool for spoken dialogue systems. In Proc. of ICSLP ’2000, pages 238–241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kogure</author>
<author>T Itoh</author>
<author>S Nakagawa</author>
</authors>
<title>A semantic interpreter for a robust spoken dialogue system.</title>
<date>1999</date>
<booktitle>In Proc. ICMI ’99,</booktitle>
<pages>61--66</pages>
<contexts>
<context position="1840" citStr="Kogure et al., 1999" startWordPosition="258" endWordPosition="261">omains/tasks and confirm the validity of the interpreter for each domain/task. Keywords: spoken dialogue system, robust interpreter, portability, dialogue script 1 Introduction Recently, much research has been done on the robustness and reliability of spoken dialogue systems. We developed a “Mt.Fuji sightseeing guidance” system which uses touch screen input, speech input/output and graphical output, and have improved the sub-modules of a speech recognizer, natural language interpreter, response generator and multi-modal interface (Kai and Nakagawa, 1995; Itoh et al., 1995; Denda et al., 1996; Kogure et al., 1999; Nakagawa et al., 2000). General speaking, all of these modules except for the speech recognition module depended on a given task or domain. As speech recognition systems are increasingly being used in practical applications, spoken dialogue systems will also become more widespread. However, the cost of developing a new spoken dialogue system is enormous. The systems that have been developed so far can not be transferred to other domains easily, and a highly-portable system that can be easily adapted to another domain or task urgently should be developed. There are several examples of researc</context>
</contexts>
<marker>Kogure, Itoh, Nakagawa, 1999</marker>
<rawString>S. Kogure, T. Itoh, and S. Nakagawa. 1999. A semantic interpreter for a robust spoken dialogue system. In Proc. ICMI ’99, pages II 61–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kuhn</author>
<author>R De Mori</author>
</authors>
<title>The application of semantic classification trees to natural language understanding.</title>
<date>1995</date>
<booktitle>In IEEE Trans. Pattern Analysis and Machine Intelligence,</booktitle>
<volume>17</volume>
<pages>449--460</pages>
<marker>Kuhn, De Mori, 1995</marker>
<rawString>R. Kuhn and R. De Mori. 1995. The application of semantic classification trees to natural language understanding. In IEEE Trans. Pattern Analysis and Machine Intelligence, volume 17, pages 449–460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Levin</author>
<author>S Narayanan</author>
<author>R Pieraccini</author>
<author>K Biatov</author>
<author>E Biatov</author>
<author>E Bocchieri</author>
<author>G Di Fabbrizio</author>
<author>W Eckert</author>
<author>S Lee</author>
<author>A Pokrovsky</author>
<author>M Rahim</author>
<author>P Ruscitti</author>
<author>M Walker</author>
</authors>
<title>The at&amp;t-darpa communicator mixed-initiative spoken dialog system.</title>
<date>2000</date>
<booktitle>In Proc. of ICSLP ’2000,</booktitle>
<volume>2</volume>
<pages>122--125</pages>
<marker>Levin, Narayanan, Pieraccini, Biatov, Biatov, Bocchieri, Di Fabbrizio, Eckert, Lee, Pokrovsky, Rahim, Ruscitti, Walker, 2000</marker>
<rawString>E. Levin, S. Narayanan, R. Pieraccini, K. Biatov, E. Biatov, E. Bocchieri, G. Di Fabbrizio, W. Eckert, S. Lee, A. Pokrovsky, M. Rahim, P. Ruscitti, and M. Walker. 2000. The at&amp;t-darpa communicator mixed-initiative spoken dialog system. In Proc. of ICSLP ’2000, volume 2, pages 122–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Minker</author>
</authors>
<title>Stochastic versus rule-based speech understanding for information retrieval.</title>
<date>1998</date>
<booktitle>In Speech Communication,</booktitle>
<pages>223--147</pages>
<contexts>
<context position="4620" citStr="Minker, 1998" startWordPosition="691" endWordPosition="692">riate words and phrases from the utterance. The template with the highest score yields the semantic representation. R.Kuhn and R.De Mori suggest a method where the system’s rules for semantic interpretation are learnt automatically from training data (Kuhn and Mori, 1995). The rules are encoded in forest of specialized decision trees called as Semantic Classification Trees (SCTs). The learned rules are robust to ungrammatical sentences and misrecognition in the input. Minker compared a rule-based case frame grammar analysis and a stochastics based case frame grammar analysis for understanding(Minker, 1998). Globally speaking, the performances of the stochastic and rule-based parsers were comparable for ATIS (Air Travel Information Service). Y. Wang suggests a robust chart parser which is the major Spoken Language Understanding (SLU) engine component behind MiPad(Wang, 2001). The robustness to ungrammaticality and noise can be attributed to its ability of skipping minimum unparsable segments in the input. The robust parsing algorithm is an extension of the bottom-up chart-parsing algorithm. In (Nakagawa et al., 2000), the dialogue system understands spontaneous speech which has many ambiguous ph</context>
</contexts>
<marker>Minker, 1998</marker>
<rawString>W. Minker. 1998. Stochastic versus rule-based speech understanding for information retrieval. In Speech Communication, pages 223–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Moore</author>
<author>A Podlozny</author>
</authors>
<title>A template matcher for robust nl interpretation.</title>
<date>1991</date>
<booktitle>In Proc. Speech and Natural Language Workshop,</booktitle>
<pages>190--194</pages>
<publisher>Morgan Kaufmann Inc.,</publisher>
<contexts>
<context position="3923" citStr="Moore and Podlozny, 1991" startWordPosition="585" endWordPosition="589">stem (Kogure and Nakagawa, 2000). We showed from experience that Figure 1: System overview. it was difficult to transfer from the Mt. Fuji sightseeing guidance existing system to the East Mikawa sightseeing guidance. Therefore,we built portable spoken dialogue system developing tools based on GUI (Kogure and Nakagawa, 2000). Especially, we focused on the developing tools of spoken dialogue system for database retrieval. Some spoken language systems focused on robust matching to handle ungrammatical utterances and illegal sentences. The Template Matcher (TM) at the Stanford Research Institute (Moore and Podlozny, 1991) instantiates competing templates, each of which seeks to fill its slots with appropriate words and phrases from the utterance. The template with the highest score yields the semantic representation. R.Kuhn and R.De Mori suggest a method where the system’s rules for semantic interpretation are learnt automatically from training data (Kuhn and Mori, 1995). The rules are encoded in forest of specialized decision trees called as Semantic Classification Trees (SCTs). The learned rules are robust to ungrammatical sentences and misrecognition in the input. Minker compared a rule-based case frame gra</context>
</contexts>
<marker>Moore, Podlozny, 1991</marker>
<rawString>R. Moore and A. Podlozny. 1991. A template matcher for robust nl interpretation. In Proc. Speech and Natural Language Workshop, Morgan Kaufmann Inc., pages 190–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Nakagawa</author>
<author>S Kogure</author>
<author>T Itoh</author>
</authors>
<title>A semantic interpreter and a cooperative response generator for a robust spoken dialogue system.</title>
<date>2000</date>
<journal>IJPRAI,</journal>
<volume>14</volume>
<issue>5</issue>
<pages>569</pages>
<contexts>
<context position="1864" citStr="Nakagawa et al., 2000" startWordPosition="262" endWordPosition="265">irm the validity of the interpreter for each domain/task. Keywords: spoken dialogue system, robust interpreter, portability, dialogue script 1 Introduction Recently, much research has been done on the robustness and reliability of spoken dialogue systems. We developed a “Mt.Fuji sightseeing guidance” system which uses touch screen input, speech input/output and graphical output, and have improved the sub-modules of a speech recognizer, natural language interpreter, response generator and multi-modal interface (Kai and Nakagawa, 1995; Itoh et al., 1995; Denda et al., 1996; Kogure et al., 1999; Nakagawa et al., 2000). General speaking, all of these modules except for the speech recognition module depended on a given task or domain. As speech recognition systems are increasingly being used in practical applications, spoken dialogue systems will also become more widespread. However, the cost of developing a new spoken dialogue system is enormous. The systems that have been developed so far can not be transferred to other domains easily, and a highly-portable system that can be easily adapted to another domain or task urgently should be developed. There are several examples of researches that focused on high</context>
<context position="5140" citStr="Nakagawa et al., 2000" startWordPosition="769" endWordPosition="772">e grammar analysis and a stochastics based case frame grammar analysis for understanding(Minker, 1998). Globally speaking, the performances of the stochastic and rule-based parsers were comparable for ATIS (Air Travel Information Service). Y. Wang suggests a robust chart parser which is the major Spoken Language Understanding (SLU) engine component behind MiPad(Wang, 2001). The robustness to ungrammaticality and noise can be attributed to its ability of skipping minimum unparsable segments in the input. The robust parsing algorithm is an extension of the bottom-up chart-parsing algorithm. In (Nakagawa et al., 2000), the dialogue system understands spontaneous speech which has many ambiguous phenomena such as interjections, ellipses, inversions, repairs, unknown words and so on, and responds to the user’s utterance. But the system fails to analyze some utterances. “Incompleteness” with the interpreter mainly causes the analysis failure, and leads to domain/task dependent development. Therefore, in this paper we focused on improving the interpreter in order to make it a robust/portable one. For this purpose, we used the EDR electronic dictionary as a semantic dictionary and improved the interpreter so tha</context>
<context position="14764" citStr="Nakagawa et al., 2000" startWordPosition="2274" endWordPosition="2277">hological dictionary for noun and verb field information of database, database DD/TD convert rule from semantic representation to retrieval pattern, display format of retrieval result DI: domain independent DD: domain dependent TI: task independent TD: task dependent Figure 8: Task adaptation. next section, we describe how application developers prepare DD/TI and DD/TD data sets. 4.2 System Core We used Chasen 5 as Japanese morphological analysis and PostgreSQL 6 as a database retrieval management system. We have constructed Semantics Modules based on the Mt. Fuji sightseeing guidance system (Nakagawa et al., 2000) with separating task dependent and independent parts. All parts of the system core are clearly DI/TI. 5http://chasen.aist-nara.ac.jp/ 6http://www.pgsql.com/ 4.3 Data Sets Data sets consist of DI/TI, DD/TI and DD/TD data sets. The separated results are shown in Table 1. 5 Task Adaptation -Hotel Retrieval SystemFigure 8 shows the flow chart of the task adaptation. We consider what kind of data may be prepared as task-dependent knowledge when a new task is applied. In the proposed framework, the application developer prepares the following: • A generally usable database (machine readable) • The </context>
</contexts>
<marker>Nakagawa, Kogure, Itoh, 2000</marker>
<rawString>S. Nakagawa, S. Kogure, and T. Itoh. 2000. A semantic interpreter and a cooperative response generator for a robust spoken dialogue system. IJPRAI, 14(5):553– 569.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sasajima</author>
<author>T Yano</author>
<author>Y Kono</author>
</authors>
<title>Europa: generic framework for developing spoken dialogue systes.</title>
<date>1999</date>
<booktitle>In Proc. of EUROSPEECH ’99,</booktitle>
<pages>1163--1166</pages>
<contexts>
<context position="2590" citStr="Sasajima et al., 1999" startWordPosition="380" endWordPosition="383">k or domain. As speech recognition systems are increasingly being used in practical applications, spoken dialogue systems will also become more widespread. However, the cost of developing a new spoken dialogue system is enormous. The systems that have been developed so far can not be transferred to other domains easily, and a highly-portable system that can be easily adapted to another domain or task urgently should be developed. There are several examples of researches that focused on high portability and expansibility (Kaspar and Hoffmannn, 1998; Brondsted et al., 1998; Sutton et al., 1998; Sasajima et al., 1999; Abella and Gorin, 1999; Levin et al., 2000). In (Kaspar and Hoffmannn, 1998), a prototype could be simply constructed even in a complicated speech dialogue system using the PIA system, which was implemented using Visual Basic. This system placed priority on achieving high robustness of speech recognition and high naturalness of generated dialogue. However, the system limited the task to the domain of knowledge search. E. Levin et al. reported the design and implementation of the AT&amp;T Communicator mixed-initiative spoken dialogue system (Levin et al., 2000). The communicator project sponsored</context>
</contexts>
<marker>Sasajima, Yano, Kono, 1999</marker>
<rawString>M. Sasajima, T. Yano, and Y. Kono. 1999. Europa: generic framework for developing spoken dialogue systes. In Proc. of EUROSPEECH ’99, pages 1163– 1166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sutton</author>
<author>R Cole</author>
<author>J de Villiers</author>
<author>J Schalkwyk</author>
<author>P Vermeulen</author>
<author>M Macon</author>
<author>E Kaiser Y Yan</author>
<author>B Rundle</author>
<author>K Shobaki</author>
<author>P Hosom</author>
<author>A Kain</author>
<author>J Wouters</author>
<author>D Massaro</author>
<author>M Cohen</author>
</authors>
<title>Universal speech tools:the cslu toolkit.</title>
<date>1998</date>
<booktitle>In Proc. ofICSLP ’98,</booktitle>
<pages>3221--3224</pages>
<marker>Sutton, Cole, de Villiers, Schalkwyk, Vermeulen, Macon, Yan, Rundle, Shobaki, Hosom, Kain, Wouters, Massaro, Cohen, 1998</marker>
<rawString>S. Sutton, R. Cole, J. de Villiers, J. Schalkwyk, P. Vermeulen, M. Macon, Y.Yan, E. Kaiser, B. Rundle, K. Shobaki, P. Hosom, A. Kain, J. Wouters, D. Massaro, and M. Cohen. 1998. Universal speech tools:the cslu toolkit. In Proc. ofICSLP ’98, pages 3221–3224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wang</author>
</authors>
<title>Robust language understanding in mipad.</title>
<date>2001</date>
<booktitle>In Proc. of EUROSPEECH ’2001,</booktitle>
<pages>1555--1558</pages>
<contexts>
<context position="4893" citStr="Wang, 2001" startWordPosition="733" endWordPosition="734">he rules are encoded in forest of specialized decision trees called as Semantic Classification Trees (SCTs). The learned rules are robust to ungrammatical sentences and misrecognition in the input. Minker compared a rule-based case frame grammar analysis and a stochastics based case frame grammar analysis for understanding(Minker, 1998). Globally speaking, the performances of the stochastic and rule-based parsers were comparable for ATIS (Air Travel Information Service). Y. Wang suggests a robust chart parser which is the major Spoken Language Understanding (SLU) engine component behind MiPad(Wang, 2001). The robustness to ungrammaticality and noise can be attributed to its ability of skipping minimum unparsable segments in the input. The robust parsing algorithm is an extension of the bottom-up chart-parsing algorithm. In (Nakagawa et al., 2000), the dialogue system understands spontaneous speech which has many ambiguous phenomena such as interjections, ellipses, inversions, repairs, unknown words and so on, and responds to the user’s utterance. But the system fails to analyze some utterances. “Incompleteness” with the interpreter mainly causes the analysis failure, and leads to domain/task </context>
</contexts>
<marker>Wang, 2001</marker>
<rawString>Y. Wang. 2001. Robust language understanding in mipad. In Proc. of EUROSPEECH ’2001, pages 1555– 1558.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>