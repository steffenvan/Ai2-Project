<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.996893">
Annotating Preferences in Negotiation Dialogues
</title>
<author confidence="0.953273">
Ana¨ıs Cadilhac, Nicholas Asher and Farah Benamara
</author>
<affiliation confidence="0.925567">
IRIT, CNRS and University of Toulouse
</affiliation>
<address confidence="0.7275705">
118, route de Narbonne
31062 Toulouse, France
</address>
<email confidence="0.984906">
{cadilhac, asher, benamara}@irit.fr
</email>
<sectionHeader confidence="0.998571" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999019">
Modeling user preferences is crucial in many
real-life problems, ranging from individual
and collective decision-making to strategic in-
teractions between agents and game theory.
Since agents do not come with their prefer-
ences transparently given in advance, we have
only two means to determine what they are if
we wish to exploit them in reasoning: we can
infer them from what an agent says or from
his nonlinguistic actions. In this paper, we an-
alyze how to infer preferences from dialogue
moves in actual conversations that involve bar-
gaining or negotiation. To this end, we pro-
pose a new annotation scheme to study how
preferences are linguistically expressed in two
different corpus genres. This paper describes
the annotation methodology and details the
inter-annotator agreement study on each cor-
pus genre. Our results show that preferences
can be easily annotated by humans.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973369565217">
Modeling user preferences is crucial in many real-
life problems, ranging from individual and collec-
tive decision-making (Arora and Allenby, 1999)
to strategic interactions between agents (Brainov,
2000) and game theory (Hausman, 2000). A web-
based recommender system can, for example, help
a user to identify (among an optimal ranking) the
product item that best fits his preferences (Burke,
2000). Modeling preferences can also help to find
some compromise or consensus between two or
more agents having different goals during a nego-
tiation (Meyer and Foo, 2004).
Working with preferences involves three subtasks
(Brafman and Domshlak, 2009): preference acquisi-
tion, which extracts preferences from users, prefer-
ence modeling where a model of users’ preferences
is built using a preference representation language
and preference reasoning which aims at computing
the set of optimal outcomes. We focus in this paper
on the first task.
Handling preferences is not easy. First, specifying
an ordering over acceptable outcomes is not trivial
especially when multiple aspects of an outcome mat-
ter. For instance, choosing a new camera to buy may
depend on several criteria (e.g. battery life, weight,
etc.), hence, ordering even two outcomes (cameras)
can be cognitively difficult because of the need to
consider trade-offs and dependencies between the
criteria. Second, users often lack complete infor-
mation about preferences initially. They build a
partial description of agents’ preferences that typi-
cally changes over time. Indeed, users often learn
about the domain, each others’ preferences and even
their own preferences during a decision-making pro-
cess. Since agents don’t come with their preferences
transparently given in advance, we have only two
means to determine what they are if we wish to ex-
ploit them in reasoning: we can infer them from
what an agent says or from his nonlinguistic actions.
In this paper, we analyze how to infer preferences
from dialogue moves in actual conversations that in-
volve bargaining or negotiation.
Within the Artificial Intelligence community,
preference acquisition from nonlinguistic actions
has been performed using a variety of specific
tasks, including preference learning (F¨urnkranz and
</bodyText>
<page confidence="0.988651">
105
</page>
<note confidence="0.978766">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 105–113,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.942779">
H¨ullermeier, 2011) and preference elicitation meth-
ods (Chen and Pu, 2004) (such as query learning
(Blum et al., 2004), collaborative filtering (Su and
Khoshgoftaar, 2009) and qualitative graphical rep-
resentation of preferences (Boutilier et al., 1997)).
However, these tasks don’t occur in actual conver-
sations about negotiation. We are interested in how
agents learn about preferences from actual conver-
sational turns in real dialogue (Edwards and Barron,
1994), using NLP techniques.
To this end, we propose a new annotation scheme
to study how preferences are linguistically expressed
in dialogues. The annotation study is performed
on two different corpus genres: the Verbmobil cor-
pus (Wahlster, 2000) and a booking corpus, built
by ourselves. This paper describes the annotation
methodology and details the inter-annotator agree-
ment study on each corpus genre. Our results show
that preferences can be easily annotated by humans.
</bodyText>
<sectionHeader confidence="0.998887" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.99775">
2.1 What are preferences?
</subsectionHeader>
<bodyText confidence="0.976694216216216">
A preference is commonly understood as an order-
ing by an agent over outcomes, which are under-
stood as actions that the agent can perform or goal
states that are the direct result of an action of the
agent. For instance, an agent’s preferences may be
defined over actions like buy a new car or by its end
result like have a new car. The outcomes over which
a preference is defined will depend on the domain or
task.
Among these outcomes, some are acceptable for
the agent, i.e. the agent is ready to act in such a
way as to realize them, and some outcomes are not.
Among the acceptable outcomes, the agent will typ-
ically prefer some to others. Our aim is not to de-
termine the most preferred outcome of an agent but
follows rather the evolution of their commitments to
certain preferences as the dialogue proceeds. To give
an example, if an agent proposes to meet on a certain
day X and at a certain time Y, we learn that among
the agent’s acceptable outcomes is a meeting on X
at Y, even if this is not his most preferred outcome.
We are interested in an ordinal definition of prefer-
ences, which consists in imposing a ranking over all
(relevant) possible outcomes and not a cardinal defi-
nition which is based on numerical values that allow
comparisons.
More formally, let Q be a set of possible
outcomes. A preference relation, written r, is a
reflexive and transitive binary relation over elements
of Q. The preference orderings are not necessarily
complete, since some candidates may not be com-
parable by a given agent. Given the two outcomes
o1 and o2, o1 r o2 means that outcome o1 is equally
or more preferred to the decision maker than o2.
Strict preference o1 &gt;- o2 holds iff o1 r o2 and not
o2 r o1. The associated indifference relation is
o1 o2 if o1 r o2 and o2 r o1.
</bodyText>
<subsectionHeader confidence="0.997692">
2.2 Preferences vs. opinions
</subsectionHeader>
<bodyText confidence="0.999927090909091">
It is important to distinguish preferences from opin-
ions. While opinions are defined as a point of view, a
belief, a sentiment or a judgment that an agent may
have about an object or a person, preferences, as
we have defined them, involve an ordering on be-
half of an agent and thus are relational and com-
parative. Hence, opinions concern absolute judg-
ments towards objects or persons (positive, negative
or neutral), while preferences concern relative judg-
ments towards actions (preferring them or not over
others). The following examples illustrate this:
</bodyText>
<figure confidence="0.770049">
(a) The movie is not bad.
</figure>
<figureCaption confidence="0.48499525">
(b) The scenario of the first season is better than the
second one.
(c) I would like to go to the cinema. Let’s go and see
Madagascar 2.
</figureCaption>
<bodyText confidence="0.9845848125">
(a) expresses a direct positive opinion towards the
movie but we do not know if this movie is the most
preferred. (b) expresses a comparative opinion be-
tween two movies with respect to their shared fea-
tures (scenarios) (Ganapathibhotla and Liu, 2008).
If actions involving these movies (e.g. seeing them)
are clear in the context, such a comparative opin-
ion will imply a preference, ordering the first season
scenario over the second. Finally, (c) expresses two
preferences, one depending on the other. The first
is that the speaker prefers to go to the cinema over
other alternative actions; the second is, given that
preference, that he wants to see Madagascar 2 over
other possible movies.
Reasoning about preferences is also distinct from
reasoning about opinions. An agent’s preferences
</bodyText>
<page confidence="0.997345">
106
</page>
<bodyText confidence="0.997934166666667">
determine an order over outcomes that predicts how
the agent, if he is rational, will act. This is not true
for opinions. Opinions have at best an indirect link
to action: I may hate what I’m doing, but do it any-
way because I prefer that outcome to any of the al-
ternatives.
</bodyText>
<sectionHeader confidence="0.996558" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.9960245">
Our data come from two corpora: one already-
existing, Verbmobil (CV ), and one that we cre-
ated, Booking (CB).
The first corpus is composed of 35 dialogues ran-
domly chosen from the existing corpus Verbmobil
(Wahlster, 2000), where two agents discuss on when
and where to set up a meeting. Here is a typical frag-
ment:
</bodyText>
<listItem confidence="0.96656">
7r1 A: Shall we meet sometime in the next week?
7r2 A: What days are good for you?
7r3 B: I have some free time on almost every day
except Fridays.
7r4 B: Fridays are bad.
7r5 B: In fact, I’m busy on Thursday too.
7r6 A: Next week I am out of town Tuesday, Wednes-
day and Thursday.
7r7 A: So perhaps Monday?
</listItem>
<bodyText confidence="0.999906857142857">
The second corpus was built from various En-
glish language learning resources, available on the
Web (e.g., www.bbc.co.uk/worldservice/
learningenglish). It contains 21 randomly se-
lected dialogues, in which one agent (the customer)
calls a service to book a room, a flight, a taxi, etc.
Here is a typical fragment:
</bodyText>
<listItem confidence="0.993126142857143">
7r1 A: Northwind Airways, good morning. May I
help you?
7r2 B: Yes, do you have any flights to Sydney next
Tuesday?
7r3 A: Yes, there’s a flight at 16:45 and one at 18:00.
7r4 A: Economy, business class or first class ticket?
7r5 B: Economy, please.
</listItem>
<bodyText confidence="0.999967956521739">
Our approach to preference acquisition exploits
discourse structure and aims to study the impact
of discourse for extracting and reasoning on prefer-
ences. Cadilhac et al. (2011) show how to compute
automatically preference representations for a whole
stretch of dialogue from the preference representa-
tions for elementary discourse units. Our annota-
tion here concentrates on the commitments to pref-
erences expressed in elementary discourse units or
EDUs. We analyze how the outcomes and the depen-
dencies between them are linguistically expressed
by performing, on each corpus, a two-level anno-
tation. First, we perform a segmentation of the di-
alogue into EDUs. Second, we annotate preferences
expressed by the EDUs.
The examples above show the effects of segmen-
tation. Each EDU is associated with a label 7rz.
For Verbmobil, we rely on the already avail-
able discourse annotation of Baldridge and Las-
carides (2005). For Booking, the segmentation
was made by consensus.
We detail, in the next section, our preference an-
notation scheme.
</bodyText>
<sectionHeader confidence="0.985054" genericHeader="method">
4 Preference annotation scheme
</sectionHeader>
<bodyText confidence="0.999994928571429">
To analyze how preferences are linguistically ex-
pressed in each EDU, we must: (1) identify the set
Q of outcomes, on which the agent’s preferences
are expressed, and (2) identify the dependencies be-
tween the elements of Q by using a set of specific
operators, i.e. identifying the agent’s preferences on
the stated outcomes. Consider the segment “Let’s
meet Thursday or Friday”. We have Q = {meet
Thursday, meet Friday} where outcomes are linked
by a disjunction that means the agent is ready to act
for one of these outcomes, preferring them equally.
Within an EDU, preferences can be expressed in
different ways. They can be atomic preference state-
ments or complex preference statements.
</bodyText>
<subsectionHeader confidence="0.999236">
4.1 Atomic preferences
</subsectionHeader>
<bodyText confidence="0.999737214285714">
Atomic preference statements are of the form “I pre-
fer X”, “Let’s X”, or “We need X”, where X de-
scribes an outcome. X may be a definite noun phrase
(“Monday”, “next week”, “almost every day”), a
prepositional phrase (“at my office”) or a verb
phrase (“to meet”). They can be expressed within
comparatives and/or superlatives (“a cheaper room”
or “the cheapest flight”).
Preferences can also be expressed in an indirect
way using questions. Although not all questions
entail that their author commits to a preference, in
many cases they do. That is, if A asks “can we meet
next week?” he implicates a preference for meeting.
For negative and wh-interrogatives, the implication
</bodyText>
<page confidence="0.99412">
107
</page>
<bodyText confidence="0.999651518518518">
is even stronger. Expressions of sentiment or polite-
ness can also be used to indirectly introduce prefer-
ences. In Booking, the segment “economy please”
indicates the agent’s preference to be in an economy
class.
EDUs can also express preferences via free-choice
modalities; “I am free on Thursday” or “I can meet
on Thursday” tells us that Thursday is a possible day
to meet, it is an acceptable outcome.
A negative preference expresses an unacceptable
outcome, i.e. what the agent does not prefer. Neg-
ative preference can be expressed explicitly with
negation words (“I don’t want to meet on Friday”)
or inferred from the context (“I am busy on Mon-
day”).
While the logical form of an atomic preference
statement is something of the form Pref(X), we
abbreviate this in the annotation language, using just
the outcome expression X to denote that the agent
prefers X to the alternatives, i.e. X &gt;- X. If X is
an unacceptable outcome, we use the non-boolean
operator not to denote that the agent prefers not X to
other alternatives, i.e. X &gt;- X. In our Verbmobil
annotation, X is typically an NP denoting a time or
place; X as an outcome is thus shorthand for meet
on X or meet at X. For Booking, X is short for
reserve or book X.
</bodyText>
<subsectionHeader confidence="0.997735">
4.2 Complex preferences
</subsectionHeader>
<bodyText confidence="0.988377511111111">
Preference statements can also be complex, express-
ing dependencies between outcomes. Borrowing
from the language of conditional preference net-
works or CP-nets (Boutilier et al., 2004), we rec-
ognize that some preferences may depend on an-
other action. For instance, given that I have cho-
sen to eat fish, I will prefer to have white wine
over red wine—something which we express as
eat fish : drink white wine &gt;- drink red wine.
Among the possible combinations, we find con-
junctions, disjunctions and conditionals. We exam-
ine these conjunctive, disjunctive and conditional
operations over outcomes and suppose a language
with non-boolean operators &amp;, V and &gt; taking out-
come expressions as arguments.
With conjunctions of preferences, as in “Could
I have a breakfast and a vegetarian meal?” or in
“Mondays and Fridays are not good?”, the agent ex-
presses two preferences (respectively over the ac-
ceptable outcomes breakfast and vegetarian meal
and the non acceptable outcomes not Mondays and
not Fridays) that he wants to satisfy and he prefers
to have one of them if he can not have both. Hence
o1 &amp; o2 means o1 &gt;- o1 and o2 &gt;- o2.
The semantics of a disjunctive preference is a free
choice one. For example in “either Monday or Tues-
day is fine for me” or in “I am free Monday and
Tuesday”, the agent states that either Monday or
Tuesday is an acceptable outcome and he is indif-
ferent between the choice of the outcomes. Hence
o1 V o2 means o2 : o1 — o1, o2 : o1 &gt;- o1 and
o1 : o2 — o2, o1 : o2 &gt;- o2.
Finally, some EDUs express conditional among
preferences. For example, in the sentence “What
about Monday, in the afternoon?”, there are two
preferences: one for the day Monday, and, given the
Monday preference, one for the time afternoon (of
Monday), at least for one syntactic reading of the
utterance. Hence o1 &gt; o2 means o1 &gt;- o1 and
o1 : o2 &gt;- o2.
For each EDU, annotators identify how outcomes
are expressed and then indicate if the outcomes are
acceptable, or not, using the operator not and how
the preferences on these outcomes are linked using
the operators &amp;, V and &gt;.
</bodyText>
<subsectionHeader confidence="0.991445">
4.3 Example
</subsectionHeader>
<bodyText confidence="0.9998016">
We give below an example of how some EDUs are
annotated. &lt;o&gt; i indicates that o is the outcome
number i in the EDU, the symbol // is used to sepa-
rate the two annotation levels and brackets indicate
how outcomes are attached.
</bodyText>
<equation confidence="0.871909">
71 : &lt;Tuesday the sixteenth&gt; 1 I got class &lt;from nine
to twelve&gt; 2? // 1 &gt; not 2
72 : What about &lt;Friday afternoon&gt; 1, &lt;at two
thirty&gt; 2 or &lt;three&gt; 3, // 1 &gt; (2 V 3)
</equation>
<bodyText confidence="0.798441818181818">
73 : &lt;The room with balcony&gt; 1 should be equipped
&lt;with a queen size bed&gt; 2, &lt;the other one&gt; 3
&lt;with twin beds&gt; 4, please. // (1 &gt; 2) &amp; (3 &gt;
4)
In 71, the annotation tells us that we have two out-
comes and that the agent prefers outcome 1 over any
other alternatives and given that, he does not pre-
fer outcome 2. In 72, the annotation tells us that
the agent prefers to have one of outcome 2 and out-
come 3 satisfied given that he prefers outcome 1. In
this example, the free choice between outcome 2 and
</bodyText>
<page confidence="0.996461">
108
</page>
<bodyText confidence="0.999778666666667">
outcome 3 is lexicalized by the coordinating con-
junction “or”. On the contrary, π3 is a more complex
example where there is no discursive marker to find
that the preference operator between the couples of
outcomes 1 and 2 on one hand, and 3 and 4 on the
other hand, is the conjunctive operator &amp;.
</bodyText>
<sectionHeader confidence="0.999665" genericHeader="method">
5 Inter-annotator agreements
</sectionHeader>
<bodyText confidence="0.998926071428572">
Our two corpora (Verbmobil and Booking)
were annotated by two annotators using the pre-
viously described annotation scheme. We per-
formed an intermediate analysis of agreement and
disagreement between the two annotators on two
Verbmobil dialogues. Annotators were thus
trained only for Verbmobil. The aim is to study to
what extent our annotation scheme is genre depen-
dent. The training allowed each annotator to under-
stand the reason of some annotation choices. After
this step, the dialogues of our corpora have been an-
notated separately, discarding those two dialogues.
Table 1 presents some statistics about the annotated
data in the gold standard.
</bodyText>
<table confidence="0.999129727272727">
CV CB
No. of dialogues 35 21
No. of outcomes 1081 275
No. of EDUs with outcomes 776 182
% with 1 outcome 71% 70%
% with 2 outcomes 22% 19%
% with 3 or more outcomes 8% 11%
No. of unacceptable outcomes (not) 266 9
No. of conjunctions (&amp;) 56 31
No. of disjunctions (V) 75 29
No. of conditionals (H) 184 37
</table>
<tableCaption confidence="0.998714">
Table 1: Statistics for the two corpora.
</tableCaption>
<bodyText confidence="0.999412">
We compute four inter-annotator agreements: on
outcome identification, on outcome acceptance, on
outcome attachment and finally on operator identifi-
cation. Table 2 summarizes our results.
</bodyText>
<subsectionHeader confidence="0.996403">
5.1 Agreements on outcome identification
</subsectionHeader>
<bodyText confidence="0.99982625">
Two inter-annotator agreements were computed us-
ing Cohen’s Kappa. One based on an exact matching
between two outcome annotations (i.e. their corre-
sponding text spans), and the other based on a le-
</bodyText>
<table confidence="0.998256666666667">
CV CB
Outcome identification (Kappa) exact : 0.66
lenient: 0.85
Outcome acceptance (Kappa) 0.90 0.95
Outcome attachment (F-measure) 93% 82%
Operator identification (Kappa) 0.93 0.75
</table>
<tableCaption confidence="0.999509">
Table 2: Inter-annotator agreements for the two corpora.
</tableCaption>
<bodyText confidence="0.999449">
nient match between annotations (i.e. there is an
overlap between their text spans as in “2p.m” and
“around 2p.m”). This approach is similar to the one
used by Wiebe, Wilson and Cardie (2005) to com-
pute agreement when annotating opinions in news
corpora. We obtained an exact agreement of 0.66
and a lenient agreement of 0.85 for both corpus gen-
res.
We made the gold standard after discussing cases
of disagreement. We observed four cases. The first
one concerns redundant preferences which we de-
cided not to keep in the gold standard. In such cases,
the second EDU π2 does not introduce a new prefer-
ence, neither does it correct the preferences stated in
π1; rather, the agent just wants to insist by repeat-
ing already stated preferences, as in the following
example:
</bodyText>
<equation confidence="0.6004235">
π1 A: Thursday, Friday, and Saturday I am out.
π2 A: So those days are all out for me,
</equation>
<bodyText confidence="0.9999658">
The second case of disagreement comes from
anaphora which are often used to introduce new, to
make more precise or to accept preferences. Hence,
we decided to annotate them in the gold standard.
Here is an example:
</bodyText>
<equation confidence="0.900131">
π1 A: One p.m. on the seventeenth?
π2 B: That sounds fantastic.
</equation>
<bodyText confidence="0.999928857142857">
The third case of disagreement concerns prefer-
ence explanation. We chose not to annotate these
expressions in the gold standard because they are
used to explain already stated preferences. In the
following example, one judge annotated “from nine
to twelve” to be expressions of preferences while the
other did not:
</bodyText>
<page confidence="0.834033333333333">
π1 A: Monday is really not good,
π2 A: I have got class from nine to twelve.
109
</page>
<bodyText confidence="0.9999875">
Finally, the last case of disagreement comes from
preferences that are not directly related to the action
of fixing a date to meet but to other actions, such as
having lunch, choosing a place to meet, etc. Even
though those preferences were often missed by an-
notators, we decided to keep them, when relevant.
</bodyText>
<subsectionHeader confidence="0.999497">
5.2 Agreements on outcome acceptance
</subsectionHeader>
<bodyText confidence="0.9994464">
The aim here is to compute the agreement on the not
operator, that is if an outcome is acceptable, as in
“&lt;Mondays&gt; 1 are good // 1”, or unacceptable, as
in “&lt;Mondays&gt; 1 are not good // not 1”. We get a
Cohen’s Kappa of 0.9 for Verbmobil and 0.95 for
Booking. The main case of disagreement concerns
anaphoric negations that are inferred from the con-
text, as in 72 below where annotators sometimes fail
to consider “in the morning” as unacceptable out-
comes:
</bodyText>
<page confidence="0.5165055">
71 A: Tuesday is kind of out,
72 A: Same reason in the morning
</page>
<bodyText confidence="0.755204">
Same case of disagreement in this example where
“Monday” is an unacceptable outcome:
</bodyText>
<equation confidence="0.570438">
71 well, I am, busy &lt;in the afternoon of the twenty
sixth&gt; 1, // not 1
72 that is &lt;Monday&gt; 1 // not 1
</equation>
<subsectionHeader confidence="0.985456">
5.3 Agreements on outcome attachment
</subsectionHeader>
<bodyText confidence="0.988846866666667">
Since this task involves structure building, we com-
pute the agreement using the F-score measure. The
agreement was computed on the previously built
gold standard once annotators discussed cases of
outcome identification disagreements. We compare
how each outcome is attached to the others within
the same EDU. This agreement concerns EDUs
that contain at least three outcomes, that is 8% of
EDUs from Verbmobil and 11% of EDUs from
Booking. When comparing annotations for the ex-
ample 71 below, there is three errors, one for out-
come 2, one for 3 and one for 4.
71 &lt;for the next week&gt; 1 the only days I have
open are &lt;Monday&gt; 2 or &lt;Tuesday&gt; 3 &lt;in the
morning&gt; 4.
</bodyText>
<listItem confidence="0.9998385">
• Annotation 1 : 1 7→ (2 5 (3 7→ 4))
• Annotation 2 : 1 7→ ((2 5 3) 7→ 4)
</listItem>
<bodyText confidence="0.992031">
We obtain an agreement of 93% for Verbmobil
and 82% for Booking.
</bodyText>
<subsectionHeader confidence="0.993351">
5.4 Agreements on outcome dependencies
</subsectionHeader>
<bodyText confidence="0.999980304347826">
Finally, we compute the agreements for each couple
of outcomes on which annotators agreed about how
they are attached.
In Verbmobil, the most frequently used binary
operator is 7→. Because the main purpose of the
agents in this corpus is to schedule an appointment,
the preferences expressed by the agents are mainly
focused on concepts of time and there are many con-
ditional preferences since it is common that prefer-
ences on specific concepts depend on more broad
temporal concepts. For example, preferences on
hours are generally conditional on preferences on
days. In Booking, there are almost as many &amp; as
7→ because independent and dependent preferences
are more balanced in this corpus. The agents dis-
cuss preferences about various criteria that are in-
dependent. For example, to book a hotel, the agent
express his preferences towards the size of the bed
(single or double), the quality of the room (smoker
or nonsmoker), the presence of certain conveniences
(TV, bathtub), the possibility to have breakfast in
his room, etc. Within an EDU, such preferences are
often expressed in different sentences (compared to
Verbmobil where segments’ lengths are smaller)
which lead annotators to link those preferences with
the operator &amp;. Conditionals between preferences
hold when decision criteria are dependent. For ex-
ample, the preference for having a vegetarian meal
is conditional on the preference for having lunch.
There also are conditionals between temporal con-
cepts, for example, to choose the time of a flight.
Table 3 shows the Kappa for each operator on
each corpus genre. The Cohen’s Kappa, averaged
over all the operators, is 0.93 for Verbmobil and
0.75 for Booking. We observe two main cases of
disagreement: between 5 and &amp;, and between &amp;
and 7→. These cases are more frequent for Booking
mainly because annotators were not trained on this
corpus. This is why the Kappa was lower than for
Verbmobil. We discuss below the main two cases
of disagreement.
Confusion between 5 and &amp;. The same lin-
guistic realizations do not always lead to the same
operator. For instance, in “&lt;Monday&gt; 1 and
&lt;Wednesday&gt; 2 are good” we have 1 5 2 whereas
in “&lt;Monday&gt; 1 and &lt;Wednesday&gt; 2 are not
</bodyText>
<page confidence="0.992957">
110
</page>
<table confidence="0.9988815">
CV CB
&amp; 0.90 0.66
p 0.97 0.89
H 0.92 0.71
</table>
<tableCaption confidence="0.999834">
Table 3: Agreements on binary operators.
</tableCaption>
<bodyText confidence="0.996371">
good” or in “I would like a &lt;single room&gt; 1 and
a &lt;taxi&gt; 2” we have respectively not 1 &amp; not 2
and 1 &amp; 2.
The coordinating conjunction “or” is a strong pre-
dictor for recognizing a disjunction of preferences,
at least when the “or” is clearly outside of the scope
of a negation1, as in the examples below (in 71, the
negation is part of the wh-question, and not boolean
over the preference):
</bodyText>
<figure confidence="0.2438965">
71 Why don’t we &lt;meet, either Thursday the first&gt; 1,
or &lt;Thursday the eighth&gt; 2 // 1 p 2
72 Would you like &lt;a single&gt; 1 or &lt;a double&gt; 2? //
1 p 2
</figure>
<bodyText confidence="0.955061729166667">
The coordinating conjunction “and” is also a
strong indication, especially when it is used to link
two acceptable outcomes that are both of a single
type (e.g., day of the week, time of day, place,
type of room, etc.) between which an agent wants
to choose a single realization. For example, in
Verbmobil, agents want to fix a single appoint-
ment so if there is a conjunction “and” between two
temporal concepts of the same level, it is a disjunc-
tion of preference (see 73 below). It is also the case
in Booking when an agent wants to book a single
plane flight (see 74).
73 &lt;Monday&gt; 1 and &lt;Tuesday&gt; 2 are good for me
// 1 p 2
74 You could &lt;travel at 10am.&gt; 1, &lt;noon&gt; 2 and
&lt;2pm&gt; 3 // 1 p (2 p 3)
The acceptability modality distributes across
the conjoined NPs to deliver something like
✸(meet Monday) n ✸(meet Tuesday) in modal
logic (clearly acceptability is an existential
rather than universal modality), and as is
known from studies of free choice modality
1When there is a propositional negation over the disjunction
as in “I don’t want sheep or wheat”, which occurs frequently
in a corpus in preparation, we no longer have a disjunction of
preferences.
(Schulz, 2007), such a conjunction translates to
✸(meet Monday V meet Tuesday), which ex-
presses our free choice disjunction of preferences,
o1 p o2.
On the other hand, when the conjunction “and”
links two outcomes referring to a single concept
that are not acceptable, it gives a conjunction of
preferences, as in 75. Once again thinking in
terms of modality is helpful. The “not accept-
able” modality distributes across the conjunction,
this gives something like ✷`o1 n ✷`o2 (where `
is truth conditional negation) which is equivalent to
✷(`o1 n `o2), i.e. not o1 &amp; not o2 and not equiv-
alent to ✷(`o1 V `o2), i.e. not o1 p not o2.
The connector “and” also involves a conjunction
of preferences when it links two independent out-
comes that the agent wants to satisfy simultaneously.
For example, in 76, the agent wants to book two ho-
tel rooms, and so the outcomes are independent. In
77, the agent expresses his preferences on two differ-
ent features he wants for the hotel room he is book-
ing.
</bodyText>
<figure confidence="0.62245">
75 &lt;Thursday the thirtieth&gt; 1, and &lt;Wednesday the
twenty ninth&gt; 2 are, booked up // not 1 &amp; not 2
76 Can I have one room&lt; with balcony&gt; 1 and &lt;one
without balcony&gt; 2? // 1 &amp; 2
</figure>
<figureCaption confidence="0.291927">
77 &lt;Queen&gt; 1 and &lt;nonsmoking&gt; 2 // 1 &amp; 2
</figureCaption>
<bodyText confidence="0.978169285714286">
Confusion between &amp; and H. In this case, dis-
agreements are mainly due to the difficulty for an-
notators to decide if preferences are dependent, or
not. For example, in “I have a meeting &lt;starting
at three&gt; 1, but I could meet &lt;at one o’clock&gt; 2”,
one annotator put not 1 H 2 meaning that the
agent is ready to meet at one o’clock because he
can not meet at three, while the other annotated
not 1 &amp; 2 meaning that the agent is ready to meet
at one o’clock independently of what it will do at
three.
Some connectors introduce contrast between the
preferences expressed in a segment as “but”,
“although” and “unless”. In the annotation, we can
model it thanks to the operator H. When it is used
between two conflicting values, it represents a cor-
rection. Thus, the annotation o1 H not o1 means we
need to replace in our model of preferences o1 &gt;- o1
by o1 &gt;- o1. And vice versa for not o1 H o1.
78 I have class &lt;on Monday&gt; 1, but, &lt;any time, after
one or two&gt; 2 I am free. // not 1 �--&gt; (1 �--&gt; 2)
</bodyText>
<page confidence="0.995388">
111
</page>
<bodyText confidence="0.75097625">
7r9 &lt;Friday&gt; 1 is a little full, although there is some
possibility, &lt;before lunch&gt; 2 // not 1 7→ (1 7→ 2)
7r10 we’re full &lt;on the 22nd&gt; 1, unless you want &lt;a
smoking room&gt; 2 // not 1 7→ (1 7→ 2)
However, it is important to note that the coordi-
nating conjunction “but” does not always introduce
contrast, as in the example below, where it intro-
duces a conjunction of preferences.
</bodyText>
<figureCaption confidence="0.346898">
7r11 I am busy &lt;on Monday&gt; 1, but &lt;Tuesday
afternoon&gt; 2, sounds good // not 1 &amp; 2
</figureCaption>
<bodyText confidence="0.997232833333333">
The subordinating conjunctions “if”, “because”
and “so” are indications for detecting conditional
preferences. The preferences in the main clause de-
pend on the preferences in the subordinate clause
(if-clause, because-clause, so-clause), as in the ex-
amples below.
</bodyText>
<construct confidence="0.886073666666667">
7r12 so if we are going to be able to meet &lt;that, last
week in January&gt; 1, it is going have to be &lt;the,
twenty fifth&gt; 2 // 1 7→ 2
7r13 &lt;the twenty eighth&gt; 1 I am free, &lt;all day&gt; 2, if
you want to go for &lt;a Sunday meeting&gt; 3 // 3 7→
(2 7→ 1)
7r14 it is going to have to be &lt;Wednesday the third&gt; 1
because, I am busy &lt;Tuesday&gt; 2 // not 2 7→ 1
7r15 I have a meeting &lt;from eleven to one&gt; 1, so
we could, meet &lt;in the morning from nine to
eleven&gt; 2, or, &lt;in the afternoon after one&gt; 3 // not
1 7→ (2 5 3)
</construct>
<bodyText confidence="0.999983631578947">
Whether or not there are some discursive markers
between two outcomes, to find the appropriate oper-
ator, we need to answer some questions : does the
agent want to satisfy the two outcomes at the same
time ? Are the preferences on the outcomes depen-
dent or independent ?
We have shown in this section that it is difficult to
answer the second question and there is quite some
ambiguity between the operators &amp; et 7→. This am-
biguity can be explained by the fact that both opera-
tors model the same optimal preference. Indeed, we
saw in section 4.2 that for two outcomes o1 and o2
linked by a conjunction of preferences (o1 &amp; o2), we
have o1 &gt;- o1 and o2 &gt;- o2. For two outcomes o1 and
o2 where o2 is linked to o1 by a conditional prefer-
ence (o1 7→ o2), we have o1 &gt;- o1 and o1 : o2 &gt;- o2.
In both cases, the best possible world for the agent
is the one where o1 and o2 are both satisfied at the
same time.
</bodyText>
<sectionHeader confidence="0.995638" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999984166666667">
In this paper, we proposed a linguistic approach
to preference aquisition that aims to infer prefer-
ences from dialogue moves in actual conversations
that involve bargaining or negotiation. We stud-
ied how preferences are linguistically expressed in
elementary discourse units on two different cor-
pus genres: one already available, the Verbmobil
corpus and the Booking corpus purposely built
for this project. Annotators were trained only for
Verbmobil. The aim is to study to what extent
our annotation scheme is genre dependent.
Our preference annotation scheme requires two
steps: identify the set of acceptable and non accept-
able outcomes on which the agents preferences are
expressed, and then identify the dependencies be-
tween these outcomes by using a set of specific non-
boolean operators expressing conjunctions, disjunc-
tions and conditionals. The inter-annotator agree-
ment study shows good results on each corpus genre
for outcome identification, outcome acceptance and
outcome attachment. The results for outcome de-
pendencies are also good but they are better for
Verbmobil. The difficulties concern the confu-
sion between disjunctions and conjunctions mainly
because the same linguistic realizations do not al-
ways lead to the same operator. In addition, anno-
tators often fail to decide if the preferences on the
outcomes are dependent or independent.
This work shows that preference acquisition from
linguistic actions is feasible for humans. The next
step is to automate the process of preference extrac-
tion using NLP methods. We plan to do it using an
hybrid approach combining both machine learning
techniques (for outcome extraction and outcome ac-
ceptance) and rule-based approaches (for outcome
attachment and outcome dependencies).
</bodyText>
<sectionHeader confidence="0.999352" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.971901875">
Neeraj Arora and Greg M. Allenby. 1999. Measur-
ing the influence of individual preference structures
in group decision making. Journal of Marketing Re-
search, 36:476–487.
Jason Baldridge and Alex Lascarides. 2005. Annotating
discourse structures for robust semantic interpretation.
In Proceedings of the 6th IWCS.
Avrim Blum, Jeffrey Jackson, Tuomas Sandholm, and
</reference>
<page confidence="0.98272">
112
</page>
<reference confidence="0.999937483333333">
Martin Zinkevich. 2004. Preference elicitation and
query learning. Journal of Machine Learning Re-
search, 5:649–667.
Craig Boutilier, Ronen Brafman, Chris Geib, and David
Poole. 1997. A constraint-based approach to prefer-
ence elicitation and decision making. In AAAI Spring
Symposium on Qualitative Decision Theory, pages 19–
28.
Craig Boutilier, Craig Brafman, Carmel Domshlak, Hol-
ger H. Hoos, and David Poole. 2004. Cp-nets: A tool
for representing and reasoning with conditional ceteris
paribus preference statements. Journal ofArtificial In-
telligence Research, 21:135–191.
Ronen I. Brafman and Carmel Domshlak. 2009. Prefer-
ence handling - an introductory tutorial. AI Magazine,
30(1):58–86.
Sviatoslav Brainov. 2000. The role and the impact of
preferences on multiagent interaction. In Proceedings
of ATAL, pages 349–363. Springer-Verlag.
Robin Burke. 2000. Knowledge-based recommender
systems. In Encyclopedia of Library and Information
Science, volume 69, pages 180–200. Marcel Dekker.
Ana¨ıs Cadilhac, Nicholas Asher, Farah Benamara, and
Alex Lascarides. 2011. Commitments to preferences
in dialogue. In Proceedings of SIGDIAL, pages 204–
215. ACL.
Li Chen and Pearl Pu. 2004. Survey of preference elici-
tation methods. Technical report.
Ward Edwards and F. Hutton Barron. 1994. Smarts
and smarter: Improved simple methods for multiat-
tribute utility measurement. Organizational Behavior
and Human Decision Processes, 60(3):306–325.
Johannes Furnkranz and Eyke Hullermeier, editors.
2011. Preference Learning. Springer.
Murthy Ganapathibhotla and Bing Liu. 2008. Mining
opinions in comparative sentences. In Proceedings of
the 22nd International Conference on Computational
Linguistics - Volume 1, COLING ’08, pages 241–248,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Daniel M. Hausman. 2000. Revealed preference, be-
lief, and game theory. Economics and Philosophy,
16(01):99–115.
Thomas Meyer and Norman Foo. 2004. Logical founda-
tions of negotiation: Strategies and preferences. In In
Proceedings of the Ninth International Conference on
Principles of Knowledge Representation and Reason-
ing (KR04, pages 311–318.
Katrin Schulz. 2007. Minimal Models in Semantics and
Pragmatics: Free Choice, Exhaustivity, and Condi-
tionals. PhD thesis, ILLC.
Xiaoyuan Su and Taghi M. Khoshgoftaar. 2009. A sur-
vey of collaborative filtering techniques. Advances in
Artificial Intelligence, 2009:1–20.
Wolfgang Wahlster, editor. 2000. Verbmobil: Founda-
tions of Speech-to-Speech Translation. Springer.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-
3):165–210.
</reference>
<page confidence="0.99932">
113
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.784801">
<title confidence="0.999381">Annotating Preferences in Negotiation Dialogues</title>
<author confidence="0.904692">Ana¨ıs Cadilhac</author>
<author confidence="0.904692">Nicholas Asher</author>
<author confidence="0.904692">Farah</author>
<affiliation confidence="0.939705">IRIT, CNRS and University of</affiliation>
<address confidence="0.9719025">118, route de 31062 Toulouse,</address>
<email confidence="0.914023">asher,</email>
<abstract confidence="0.999553095238095">Modeling user preferences is crucial in many real-life problems, ranging from individual and collective decision-making to strategic interactions between agents and game theory. Since agents do not come with their preferences transparently given in advance, we have only two means to determine what they are if we wish to exploit them in reasoning: we can infer them from what an agent says or from his nonlinguistic actions. In this paper, we analyze how to infer preferences from dialogue moves in actual conversations that involve bargaining or negotiation. To this end, we propose a new annotation scheme to study how preferences are linguistically expressed in two different corpus genres. This paper describes the annotation methodology and details the inter-annotator agreement study on each corpus genre. Our results show that preferences can be easily annotated by humans.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Neeraj Arora</author>
<author>Greg M Allenby</author>
</authors>
<title>Measuring the influence of individual preference structures in group decision making.</title>
<date>1999</date>
<journal>Journal of Marketing Research,</journal>
<pages>36--476</pages>
<contexts>
<context position="1269" citStr="Arora and Allenby, 1999" startWordPosition="189" endWordPosition="192">actions. In this paper, we analyze how to infer preferences from dialogue moves in actual conversations that involve bargaining or negotiation. To this end, we propose a new annotation scheme to study how preferences are linguistically expressed in two different corpus genres. This paper describes the annotation methodology and details the inter-annotator agreement study on each corpus genre. Our results show that preferences can be easily annotated by humans. 1 Introduction Modeling user preferences is crucial in many reallife problems, ranging from individual and collective decision-making (Arora and Allenby, 1999) to strategic interactions between agents (Brainov, 2000) and game theory (Hausman, 2000). A webbased recommender system can, for example, help a user to identify (among an optimal ranking) the product item that best fits his preferences (Burke, 2000). Modeling preferences can also help to find some compromise or consensus between two or more agents having different goals during a negotiation (Meyer and Foo, 2004). Working with preferences involves three subtasks (Brafman and Domshlak, 2009): preference acquisition, which extracts preferences from users, preference modeling where a model of us</context>
</contexts>
<marker>Arora, Allenby, 1999</marker>
<rawString>Neeraj Arora and Greg M. Allenby. 1999. Measuring the influence of individual preference structures in group decision making. Journal of Marketing Research, 36:476–487.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
<author>Alex Lascarides</author>
</authors>
<title>Annotating discourse structures for robust semantic interpretation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 6th IWCS.</booktitle>
<contexts>
<context position="10207" citStr="Baldridge and Lascarides (2005)" startWordPosition="1678" endWordPosition="1682">eference representations for elementary discourse units. Our annotation here concentrates on the commitments to preferences expressed in elementary discourse units or EDUs. We analyze how the outcomes and the dependencies between them are linguistically expressed by performing, on each corpus, a two-level annotation. First, we perform a segmentation of the dialogue into EDUs. Second, we annotate preferences expressed by the EDUs. The examples above show the effects of segmentation. Each EDU is associated with a label 7rz. For Verbmobil, we rely on the already available discourse annotation of Baldridge and Lascarides (2005). For Booking, the segmentation was made by consensus. We detail, in the next section, our preference annotation scheme. 4 Preference annotation scheme To analyze how preferences are linguistically expressed in each EDU, we must: (1) identify the set Q of outcomes, on which the agent’s preferences are expressed, and (2) identify the dependencies between the elements of Q by using a set of specific operators, i.e. identifying the agent’s preferences on the stated outcomes. Consider the segment “Let’s meet Thursday or Friday”. We have Q = {meet Thursday, meet Friday} where outcomes are linked by</context>
</contexts>
<marker>Baldridge, Lascarides, 2005</marker>
<rawString>Jason Baldridge and Alex Lascarides. 2005. Annotating discourse structures for robust semantic interpretation. In Proceedings of the 6th IWCS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Jeffrey Jackson</author>
<author>Tuomas Sandholm</author>
<author>Martin Zinkevich</author>
</authors>
<title>Preference elicitation and query learning.</title>
<date>2004</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>5--649</pages>
<contexts>
<context position="3649" citStr="Blum et al., 2004" startWordPosition="548" endWordPosition="551">his paper, we analyze how to infer preferences from dialogue moves in actual conversations that involve bargaining or negotiation. Within the Artificial Intelligence community, preference acquisition from nonlinguistic actions has been performed using a variety of specific tasks, including preference learning (F¨urnkranz and 105 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 105–113, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics H¨ullermeier, 2011) and preference elicitation methods (Chen and Pu, 2004) (such as query learning (Blum et al., 2004), collaborative filtering (Su and Khoshgoftaar, 2009) and qualitative graphical representation of preferences (Boutilier et al., 1997)). However, these tasks don’t occur in actual conversations about negotiation. We are interested in how agents learn about preferences from actual conversational turns in real dialogue (Edwards and Barron, 1994), using NLP techniques. To this end, we propose a new annotation scheme to study how preferences are linguistically expressed in dialogues. The annotation study is performed on two different corpus genres: the Verbmobil corpus (Wahlster, 2000) and a booki</context>
</contexts>
<marker>Blum, Jackson, Sandholm, Zinkevich, 2004</marker>
<rawString>Avrim Blum, Jeffrey Jackson, Tuomas Sandholm, and Martin Zinkevich. 2004. Preference elicitation and query learning. Journal of Machine Learning Research, 5:649–667.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Craig Boutilier</author>
<author>Ronen Brafman</author>
<author>Chris Geib</author>
<author>David Poole</author>
</authors>
<title>A constraint-based approach to preference elicitation and decision making.</title>
<date>1997</date>
<booktitle>In AAAI Spring Symposium on Qualitative Decision Theory,</booktitle>
<pages>19--28</pages>
<contexts>
<context position="3783" citStr="Boutilier et al., 1997" startWordPosition="565" endWordPosition="568">. Within the Artificial Intelligence community, preference acquisition from nonlinguistic actions has been performed using a variety of specific tasks, including preference learning (F¨urnkranz and 105 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 105–113, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics H¨ullermeier, 2011) and preference elicitation methods (Chen and Pu, 2004) (such as query learning (Blum et al., 2004), collaborative filtering (Su and Khoshgoftaar, 2009) and qualitative graphical representation of preferences (Boutilier et al., 1997)). However, these tasks don’t occur in actual conversations about negotiation. We are interested in how agents learn about preferences from actual conversational turns in real dialogue (Edwards and Barron, 1994), using NLP techniques. To this end, we propose a new annotation scheme to study how preferences are linguistically expressed in dialogues. The annotation study is performed on two different corpus genres: the Verbmobil corpus (Wahlster, 2000) and a booking corpus, built by ourselves. This paper describes the annotation methodology and details the inter-annotator agreement study on each</context>
</contexts>
<marker>Boutilier, Brafman, Geib, Poole, 1997</marker>
<rawString>Craig Boutilier, Ronen Brafman, Chris Geib, and David Poole. 1997. A constraint-based approach to preference elicitation and decision making. In AAAI Spring Symposium on Qualitative Decision Theory, pages 19– 28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Craig Boutilier</author>
<author>Craig Brafman</author>
<author>Carmel Domshlak</author>
<author>Holger H Hoos</author>
<author>David Poole</author>
</authors>
<title>Cp-nets: A tool for representing and reasoning with conditional ceteris paribus preference statements.</title>
<date>2004</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<pages>21--135</pages>
<contexts>
<context position="13187" citStr="Boutilier et al., 2004" startWordPosition="2180" endWordPosition="2183">come expression X to denote that the agent prefers X to the alternatives, i.e. X &gt;- X. If X is an unacceptable outcome, we use the non-boolean operator not to denote that the agent prefers not X to other alternatives, i.e. X &gt;- X. In our Verbmobil annotation, X is typically an NP denoting a time or place; X as an outcome is thus shorthand for meet on X or meet at X. For Booking, X is short for reserve or book X. 4.2 Complex preferences Preference statements can also be complex, expressing dependencies between outcomes. Borrowing from the language of conditional preference networks or CP-nets (Boutilier et al., 2004), we recognize that some preferences may depend on another action. For instance, given that I have chosen to eat fish, I will prefer to have white wine over red wine—something which we express as eat fish : drink white wine &gt;- drink red wine. Among the possible combinations, we find conjunctions, disjunctions and conditionals. We examine these conjunctive, disjunctive and conditional operations over outcomes and suppose a language with non-boolean operators &amp;, V and &gt; taking outcome expressions as arguments. With conjunctions of preferences, as in “Could I have a breakfast and a vegetarian mea</context>
</contexts>
<marker>Boutilier, Brafman, Domshlak, Hoos, Poole, 2004</marker>
<rawString>Craig Boutilier, Craig Brafman, Carmel Domshlak, Holger H. Hoos, and David Poole. 2004. Cp-nets: A tool for representing and reasoning with conditional ceteris paribus preference statements. Journal ofArtificial Intelligence Research, 21:135–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronen I Brafman</author>
<author>Carmel Domshlak</author>
</authors>
<title>Preference handling - an introductory tutorial.</title>
<date>2009</date>
<journal>AI Magazine,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="1765" citStr="Brafman and Domshlak, 2009" startWordPosition="265" endWordPosition="268"> preferences is crucial in many reallife problems, ranging from individual and collective decision-making (Arora and Allenby, 1999) to strategic interactions between agents (Brainov, 2000) and game theory (Hausman, 2000). A webbased recommender system can, for example, help a user to identify (among an optimal ranking) the product item that best fits his preferences (Burke, 2000). Modeling preferences can also help to find some compromise or consensus between two or more agents having different goals during a negotiation (Meyer and Foo, 2004). Working with preferences involves three subtasks (Brafman and Domshlak, 2009): preference acquisition, which extracts preferences from users, preference modeling where a model of users’ preferences is built using a preference representation language and preference reasoning which aims at computing the set of optimal outcomes. We focus in this paper on the first task. Handling preferences is not easy. First, specifying an ordering over acceptable outcomes is not trivial especially when multiple aspects of an outcome matter. For instance, choosing a new camera to buy may depend on several criteria (e.g. battery life, weight, etc.), hence, ordering even two outcomes (came</context>
</contexts>
<marker>Brafman, Domshlak, 2009</marker>
<rawString>Ronen I. Brafman and Carmel Domshlak. 2009. Preference handling - an introductory tutorial. AI Magazine, 30(1):58–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sviatoslav Brainov</author>
</authors>
<title>The role and the impact of preferences on multiagent interaction.</title>
<date>2000</date>
<booktitle>In Proceedings of ATAL,</booktitle>
<pages>349--363</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="1326" citStr="Brainov, 2000" startWordPosition="198" endWordPosition="199">alogue moves in actual conversations that involve bargaining or negotiation. To this end, we propose a new annotation scheme to study how preferences are linguistically expressed in two different corpus genres. This paper describes the annotation methodology and details the inter-annotator agreement study on each corpus genre. Our results show that preferences can be easily annotated by humans. 1 Introduction Modeling user preferences is crucial in many reallife problems, ranging from individual and collective decision-making (Arora and Allenby, 1999) to strategic interactions between agents (Brainov, 2000) and game theory (Hausman, 2000). A webbased recommender system can, for example, help a user to identify (among an optimal ranking) the product item that best fits his preferences (Burke, 2000). Modeling preferences can also help to find some compromise or consensus between two or more agents having different goals during a negotiation (Meyer and Foo, 2004). Working with preferences involves three subtasks (Brafman and Domshlak, 2009): preference acquisition, which extracts preferences from users, preference modeling where a model of users’ preferences is built using a preference representati</context>
</contexts>
<marker>Brainov, 2000</marker>
<rawString>Sviatoslav Brainov. 2000. The role and the impact of preferences on multiagent interaction. In Proceedings of ATAL, pages 349–363. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robin Burke</author>
</authors>
<title>Knowledge-based recommender systems.</title>
<date>2000</date>
<journal>In Encyclopedia of Library and Information Science,</journal>
<volume>69</volume>
<pages>180--200</pages>
<publisher>Marcel Dekker.</publisher>
<contexts>
<context position="1520" citStr="Burke, 2000" startWordPosition="230" endWordPosition="231">corpus genres. This paper describes the annotation methodology and details the inter-annotator agreement study on each corpus genre. Our results show that preferences can be easily annotated by humans. 1 Introduction Modeling user preferences is crucial in many reallife problems, ranging from individual and collective decision-making (Arora and Allenby, 1999) to strategic interactions between agents (Brainov, 2000) and game theory (Hausman, 2000). A webbased recommender system can, for example, help a user to identify (among an optimal ranking) the product item that best fits his preferences (Burke, 2000). Modeling preferences can also help to find some compromise or consensus between two or more agents having different goals during a negotiation (Meyer and Foo, 2004). Working with preferences involves three subtasks (Brafman and Domshlak, 2009): preference acquisition, which extracts preferences from users, preference modeling where a model of users’ preferences is built using a preference representation language and preference reasoning which aims at computing the set of optimal outcomes. We focus in this paper on the first task. Handling preferences is not easy. First, specifying an orderin</context>
</contexts>
<marker>Burke, 2000</marker>
<rawString>Robin Burke. 2000. Knowledge-based recommender systems. In Encyclopedia of Library and Information Science, volume 69, pages 180–200. Marcel Dekker.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana¨ıs Cadilhac</author>
<author>Nicholas Asher</author>
<author>Farah Benamara</author>
<author>Alex Lascarides</author>
</authors>
<title>Commitments to preferences in dialogue.</title>
<date>2011</date>
<booktitle>In Proceedings of SIGDIAL,</booktitle>
<pages>204--215</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="9471" citStr="Cadilhac et al. (2011)" startWordPosition="1563" endWordPosition="1566">k/worldservice/ learningenglish). It contains 21 randomly selected dialogues, in which one agent (the customer) calls a service to book a room, a flight, a taxi, etc. Here is a typical fragment: 7r1 A: Northwind Airways, good morning. May I help you? 7r2 B: Yes, do you have any flights to Sydney next Tuesday? 7r3 A: Yes, there’s a flight at 16:45 and one at 18:00. 7r4 A: Economy, business class or first class ticket? 7r5 B: Economy, please. Our approach to preference acquisition exploits discourse structure and aims to study the impact of discourse for extracting and reasoning on preferences. Cadilhac et al. (2011) show how to compute automatically preference representations for a whole stretch of dialogue from the preference representations for elementary discourse units. Our annotation here concentrates on the commitments to preferences expressed in elementary discourse units or EDUs. We analyze how the outcomes and the dependencies between them are linguistically expressed by performing, on each corpus, a two-level annotation. First, we perform a segmentation of the dialogue into EDUs. Second, we annotate preferences expressed by the EDUs. The examples above show the effects of segmentation. Each EDU</context>
</contexts>
<marker>Cadilhac, Asher, Benamara, Lascarides, 2011</marker>
<rawString>Ana¨ıs Cadilhac, Nicholas Asher, Farah Benamara, and Alex Lascarides. 2011. Commitments to preferences in dialogue. In Proceedings of SIGDIAL, pages 204– 215. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Chen</author>
<author>Pearl Pu</author>
</authors>
<title>Survey of preference elicitation methods.</title>
<date>2004</date>
<tech>Technical report.</tech>
<contexts>
<context position="3605" citStr="Chen and Pu, 2004" startWordPosition="540" endWordPosition="543">says or from his nonlinguistic actions. In this paper, we analyze how to infer preferences from dialogue moves in actual conversations that involve bargaining or negotiation. Within the Artificial Intelligence community, preference acquisition from nonlinguistic actions has been performed using a variety of specific tasks, including preference learning (F¨urnkranz and 105 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 105–113, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics H¨ullermeier, 2011) and preference elicitation methods (Chen and Pu, 2004) (such as query learning (Blum et al., 2004), collaborative filtering (Su and Khoshgoftaar, 2009) and qualitative graphical representation of preferences (Boutilier et al., 1997)). However, these tasks don’t occur in actual conversations about negotiation. We are interested in how agents learn about preferences from actual conversational turns in real dialogue (Edwards and Barron, 1994), using NLP techniques. To this end, we propose a new annotation scheme to study how preferences are linguistically expressed in dialogues. The annotation study is performed on two different corpus genres: the V</context>
</contexts>
<marker>Chen, Pu, 2004</marker>
<rawString>Li Chen and Pearl Pu. 2004. Survey of preference elicitation methods. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ward Edwards</author>
<author>F Hutton Barron</author>
</authors>
<title>Smarts and smarter: Improved simple methods for multiattribute utility measurement. Organizational Behavior and Human Decision Processes,</title>
<date>1994</date>
<pages>60--3</pages>
<contexts>
<context position="3994" citStr="Edwards and Barron, 1994" startWordPosition="597" endWordPosition="600">oint Conference on Lexical and Computational Semantics (*SEM), pages 105–113, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics H¨ullermeier, 2011) and preference elicitation methods (Chen and Pu, 2004) (such as query learning (Blum et al., 2004), collaborative filtering (Su and Khoshgoftaar, 2009) and qualitative graphical representation of preferences (Boutilier et al., 1997)). However, these tasks don’t occur in actual conversations about negotiation. We are interested in how agents learn about preferences from actual conversational turns in real dialogue (Edwards and Barron, 1994), using NLP techniques. To this end, we propose a new annotation scheme to study how preferences are linguistically expressed in dialogues. The annotation study is performed on two different corpus genres: the Verbmobil corpus (Wahlster, 2000) and a booking corpus, built by ourselves. This paper describes the annotation methodology and details the inter-annotator agreement study on each corpus genre. Our results show that preferences can be easily annotated by humans. 2 Background 2.1 What are preferences? A preference is commonly understood as an ordering by an agent over outcomes, which are </context>
</contexts>
<marker>Edwards, Barron, 1994</marker>
<rawString>Ward Edwards and F. Hutton Barron. 1994. Smarts and smarter: Improved simple methods for multiattribute utility measurement. Organizational Behavior and Human Decision Processes, 60(3):306–325.</rawString>
</citation>
<citation valid="false">
<date>2011</date>
<editor>Johannes Furnkranz and Eyke Hullermeier, editors.</editor>
<publisher>Preference Learning. Springer.</publisher>
<contexts>
<context position="9471" citStr="(2011)" startWordPosition="1566" endWordPosition="1566">learningenglish). It contains 21 randomly selected dialogues, in which one agent (the customer) calls a service to book a room, a flight, a taxi, etc. Here is a typical fragment: 7r1 A: Northwind Airways, good morning. May I help you? 7r2 B: Yes, do you have any flights to Sydney next Tuesday? 7r3 A: Yes, there’s a flight at 16:45 and one at 18:00. 7r4 A: Economy, business class or first class ticket? 7r5 B: Economy, please. Our approach to preference acquisition exploits discourse structure and aims to study the impact of discourse for extracting and reasoning on preferences. Cadilhac et al. (2011) show how to compute automatically preference representations for a whole stretch of dialogue from the preference representations for elementary discourse units. Our annotation here concentrates on the commitments to preferences expressed in elementary discourse units or EDUs. We analyze how the outcomes and the dependencies between them are linguistically expressed by performing, on each corpus, a two-level annotation. First, we perform a segmentation of the dialogue into EDUs. Second, we annotate preferences expressed by the EDUs. The examples above show the effects of segmentation. Each EDU</context>
</contexts>
<marker>2011</marker>
<rawString>Johannes Furnkranz and Eyke Hullermeier, editors. 2011. Preference Learning. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Murthy Ganapathibhotla</author>
<author>Bing Liu</author>
</authors>
<title>Mining opinions in comparative sentences.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics - Volume 1, COLING ’08,</booktitle>
<pages>241--248</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7282" citStr="Ganapathibhotla and Liu, 2008" startWordPosition="1177" endWordPosition="1180">bsolute judgments towards objects or persons (positive, negative or neutral), while preferences concern relative judgments towards actions (preferring them or not over others). The following examples illustrate this: (a) The movie is not bad. (b) The scenario of the first season is better than the second one. (c) I would like to go to the cinema. Let’s go and see Madagascar 2. (a) expresses a direct positive opinion towards the movie but we do not know if this movie is the most preferred. (b) expresses a comparative opinion between two movies with respect to their shared features (scenarios) (Ganapathibhotla and Liu, 2008). If actions involving these movies (e.g. seeing them) are clear in the context, such a comparative opinion will imply a preference, ordering the first season scenario over the second. Finally, (c) expresses two preferences, one depending on the other. The first is that the speaker prefers to go to the cinema over other alternative actions; the second is, given that preference, that he wants to see Madagascar 2 over other possible movies. Reasoning about preferences is also distinct from reasoning about opinions. An agent’s preferences 106 determine an order over outcomes that predicts how the</context>
</contexts>
<marker>Ganapathibhotla, Liu, 2008</marker>
<rawString>Murthy Ganapathibhotla and Bing Liu. 2008. Mining opinions in comparative sentences. In Proceedings of the 22nd International Conference on Computational Linguistics - Volume 1, COLING ’08, pages 241–248, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Hausman</author>
</authors>
<title>Revealed preference, belief, and game theory.</title>
<date>2000</date>
<journal>Economics and Philosophy,</journal>
<volume>16</volume>
<issue>01</issue>
<contexts>
<context position="1358" citStr="Hausman, 2000" startWordPosition="203" endWordPosition="204">ions that involve bargaining or negotiation. To this end, we propose a new annotation scheme to study how preferences are linguistically expressed in two different corpus genres. This paper describes the annotation methodology and details the inter-annotator agreement study on each corpus genre. Our results show that preferences can be easily annotated by humans. 1 Introduction Modeling user preferences is crucial in many reallife problems, ranging from individual and collective decision-making (Arora and Allenby, 1999) to strategic interactions between agents (Brainov, 2000) and game theory (Hausman, 2000). A webbased recommender system can, for example, help a user to identify (among an optimal ranking) the product item that best fits his preferences (Burke, 2000). Modeling preferences can also help to find some compromise or consensus between two or more agents having different goals during a negotiation (Meyer and Foo, 2004). Working with preferences involves three subtasks (Brafman and Domshlak, 2009): preference acquisition, which extracts preferences from users, preference modeling where a model of users’ preferences is built using a preference representation language and preference reaso</context>
</contexts>
<marker>Hausman, 2000</marker>
<rawString>Daniel M. Hausman. 2000. Revealed preference, belief, and game theory. Economics and Philosophy, 16(01):99–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Meyer</author>
<author>Norman Foo</author>
</authors>
<title>Logical foundations of negotiation: Strategies and preferences. In</title>
<date>2004</date>
<booktitle>In Proceedings of the Ninth International Conference on Principles of Knowledge Representation and Reasoning (KR04,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="1686" citStr="Meyer and Foo, 2004" startWordPosition="255" endWordPosition="258">ferences can be easily annotated by humans. 1 Introduction Modeling user preferences is crucial in many reallife problems, ranging from individual and collective decision-making (Arora and Allenby, 1999) to strategic interactions between agents (Brainov, 2000) and game theory (Hausman, 2000). A webbased recommender system can, for example, help a user to identify (among an optimal ranking) the product item that best fits his preferences (Burke, 2000). Modeling preferences can also help to find some compromise or consensus between two or more agents having different goals during a negotiation (Meyer and Foo, 2004). Working with preferences involves three subtasks (Brafman and Domshlak, 2009): preference acquisition, which extracts preferences from users, preference modeling where a model of users’ preferences is built using a preference representation language and preference reasoning which aims at computing the set of optimal outcomes. We focus in this paper on the first task. Handling preferences is not easy. First, specifying an ordering over acceptable outcomes is not trivial especially when multiple aspects of an outcome matter. For instance, choosing a new camera to buy may depend on several crit</context>
</contexts>
<marker>Meyer, Foo, 2004</marker>
<rawString>Thomas Meyer and Norman Foo. 2004. Logical foundations of negotiation: Strategies and preferences. In In Proceedings of the Ninth International Conference on Principles of Knowledge Representation and Reasoning (KR04, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Schulz</author>
</authors>
<title>Minimal Models in Semantics and Pragmatics: Free Choice, Exhaustivity, and Conditionals.</title>
<date>2007</date>
<tech>PhD thesis, ILLC.</tech>
<contexts>
<context position="25425" citStr="Schulz, 2007" startWordPosition="4369" endWordPosition="4370"> (see 74). 73 &lt;Monday&gt; 1 and &lt;Tuesday&gt; 2 are good for me // 1 p 2 74 You could &lt;travel at 10am.&gt; 1, &lt;noon&gt; 2 and &lt;2pm&gt; 3 // 1 p (2 p 3) The acceptability modality distributes across the conjoined NPs to deliver something like ✸(meet Monday) n ✸(meet Tuesday) in modal logic (clearly acceptability is an existential rather than universal modality), and as is known from studies of free choice modality 1When there is a propositional negation over the disjunction as in “I don’t want sheep or wheat”, which occurs frequently in a corpus in preparation, we no longer have a disjunction of preferences. (Schulz, 2007), such a conjunction translates to ✸(meet Monday V meet Tuesday), which expresses our free choice disjunction of preferences, o1 p o2. On the other hand, when the conjunction “and” links two outcomes referring to a single concept that are not acceptable, it gives a conjunction of preferences, as in 75. Once again thinking in terms of modality is helpful. The “not acceptable” modality distributes across the conjunction, this gives something like ✷`o1 n ✷`o2 (where ` is truth conditional negation) which is equivalent to ✷(`o1 n `o2), i.e. not o1 &amp; not o2 and not equivalent to ✷(`o1 V `o2), i.e. </context>
</contexts>
<marker>Schulz, 2007</marker>
<rawString>Katrin Schulz. 2007. Minimal Models in Semantics and Pragmatics: Free Choice, Exhaustivity, and Conditionals. PhD thesis, ILLC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoyuan Su</author>
<author>Taghi M Khoshgoftaar</author>
</authors>
<title>A survey of collaborative filtering techniques.</title>
<date>2009</date>
<booktitle>Advances in Artificial Intelligence,</booktitle>
<pages>2009--1</pages>
<contexts>
<context position="3702" citStr="Su and Khoshgoftaar, 2009" startWordPosition="554" endWordPosition="557">s from dialogue moves in actual conversations that involve bargaining or negotiation. Within the Artificial Intelligence community, preference acquisition from nonlinguistic actions has been performed using a variety of specific tasks, including preference learning (F¨urnkranz and 105 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 105–113, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics H¨ullermeier, 2011) and preference elicitation methods (Chen and Pu, 2004) (such as query learning (Blum et al., 2004), collaborative filtering (Su and Khoshgoftaar, 2009) and qualitative graphical representation of preferences (Boutilier et al., 1997)). However, these tasks don’t occur in actual conversations about negotiation. We are interested in how agents learn about preferences from actual conversational turns in real dialogue (Edwards and Barron, 1994), using NLP techniques. To this end, we propose a new annotation scheme to study how preferences are linguistically expressed in dialogues. The annotation study is performed on two different corpus genres: the Verbmobil corpus (Wahlster, 2000) and a booking corpus, built by ourselves. This paper describes t</context>
</contexts>
<marker>Su, Khoshgoftaar, 2009</marker>
<rawString>Xiaoyuan Su and Taghi M. Khoshgoftaar. 2009. A survey of collaborative filtering techniques. Advances in Artificial Intelligence, 2009:1–20.</rawString>
</citation>
<citation valid="true">
<title>Verbmobil: Foundations of Speech-to-Speech Translation.</title>
<date>2000</date>
<editor>Wolfgang Wahlster, editor.</editor>
<publisher>Springer.</publisher>
<marker>2000</marker>
<rawString>Wolfgang Wahlster, editor. 2000. Verbmobil: Foundations of Speech-to-Speech Translation. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language Resources and Evaluation,</title>
<date>2005</date>
<pages>39--2</pages>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2-3):165–210.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>