<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003170">
<title confidence="0.985499">
Named Entity Recognition as a House of Cards: Classifier Stacking
</title>
<author confidence="0.991548">
Radu Florian
</author>
<affiliation confidence="0.951127333333333">
Department of Computer Science and Center for Language and Speech Processing
Johns Hopkins University
3400 N. Charles St., Baltimore, MD 21218, USA
</affiliation>
<email confidence="0.997097">
rflorian@cs.jhu.edu
</email>
<sectionHeader confidence="0.999823" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999868083333333">
This paper presents a classifier stacking-based ap-
proach to the named entity recognition task (NER
henceforth). Transformation-based learning (Brill,
1995), Snow (sparse network of winnows (Muñoz
et al., 1999)) and a forward-backward algorithm are
stacked (the output of one classifier is passed as in-
put to the next classifier), yielding considerable im-
provement in performance. In addition, in agree-
ment with other studies on the same problem, the
enhancement of the feature space (in the form of
capitalization information) is shown to be especially
beneficial to this task.
</bodyText>
<sectionHeader confidence="0.995166" genericHeader="categories and subject descriptors">
2 Computational Approaches
</sectionHeader>
<bodyText confidence="0.99989975">
All approaches to the NER task presented in this
paper, except the one presented in Section 3, use the
IOB chunk tagging method (Tjong Kim Sang and
Veenstra, 1999) for identifying the named entities.
</bodyText>
<subsectionHeader confidence="0.996891">
2.1 Feature Space and Baselines
</subsectionHeader>
<bodyText confidence="0.948670458333333">
A careful selection of the feature space is a very
important part of classifier design. The algorithms
presented in this paper are using only informa-
tion that can be extracted directly from the train-
ing data: the words, their capitalization informa-
tion and the chunk tags. While they can defi-
nitely incorporate additional information (such as
lists of countries/cities/regions, organizations, peo-
ple names, etc.), due to the short exposition space,
we decided to restrict them to this feature space.
Table 2 presents the results obtained by running
off-the-shelf part-of-speech/text chunking classi-
fiers; all of them use just word information, albeit
in different ways. The leader of the pack is the MX-
POST tagger (Ratnaparkhi, 1996). The measure of
choice for the NER task is F-measure, the harmonic
mean of precision and recall: F� = 2R, usu-
ally computed with , = 1.
As observed by participants in the MUC-6 and -7
tasks (Bikel et al., 1997; Borthwick, 1999; Miller et
1: Capitalization information 2: Presence in
dictionary
first_cap, all_caps, all_lower, upper, lower,
number, punct, other both, none
</bodyText>
<tableCaption confidence="0.999023">
Table 1: Capitalization information
</tableCaption>
<bodyText confidence="0.999617133333333">
al., 1998), an important feature for the NER task is
information relative to word capitalization. In an
approach similar to Zhou and Su (2002), we ex-
tracted for each word a 2-byte code, as summarized
in Table 1. The first byte specifies the capitaliza-
tion of the word (first letter capital, etc), while the
second specifies whether the word is present in the
dictionary in lower case, upper case, both or neither
forms. These two codes are extracted in order to of-
fer both a way of backing-off in sparse data cases
(unknown words) and a way of encouraging gen-
eralization. Table 2 shows the performance of the
fnTBL (Ngai and Florian, 2001) and Snow systems
when using the capitalization information, both sys-
tems displaying considerably better performance.
</bodyText>
<subsectionHeader confidence="0.994352">
2.2 Transformation-Based Learning
</subsectionHeader>
<bodyText confidence="0.993762333333333">
Transformation-based learning (TBL henceforth) is
an error-driven machine learning technique which
works by first assigning an initial classification to
the data, and then automatically proposing, evalu-
ating and selecting the transformations that max-
imally decrease the number of errors. Each such
transformation, or rule, consists of a predicate and
a target. In our implementation of TBL – fnTBL –
predicates consist of a conjunction of atomic pred-
icates, such as feature identity (e.g. wordo =
Barcelona), membership in a set (e.g. B — ORG E
{chunk-3 ... chunk_1}), etc.
TBL has some attractive qualities that make it
suitable for the language-related tasks: it can au-
tomatically integrate heterogenous types of knowl-
edge, without the need for explicit modeling (simi-
lar to Snow, Maximum Entropy, decision trees, etc);
it is error–driven, therefore directly minimizes the
</bodyText>
<figure confidence="0.990736272727273">
73.5
73
72.5
F−measure
72
71.5
71
70.5
70
Accuracy
Method
without capitalization information
TnT 94.78% 66.72
MXPOST 95.02% 69.04
Snow 94.27% 65.94
fnTBL 94.92% 68.06
with capitalization information
Snow (extended templates) 95.15% 71.36
fnTBL 95.57% 71.88
fnTBL+Snow 95.36% 73.49
0 2 4 68 10
Iteration Number
</figure>
<tableCaption confidence="0.931003">
Table 2: Comparative results for different methods on the
Spanish development data
</tableCaption>
<bodyText confidence="0.999972470588235">
ultimate evaluation measure: the error rate; and it
has an inherently dynamic behavior1. TBL has been
previously applied to the English NER task (Ab-
erdeen et al., 1995), with good results.
The fnTBL-based NER system is designed in the
same way as Brill’s POS tagger (Brill, 1995), con-
sisting of a morphological stage, where unknown
words’ chunks are guessed based on their morpho-
logical and capitalization representation, followed
by a contextual stage, in which the full interaction
between the words’ features is leveraged for learn-
ing. The feature templates used are based on a com-
bination of word, chunk and capitalization informa-
tion of words in a 7-word window around the target
word. The entire template list (133 templates) will
be made available from the author’s web page after
the conclusion of the shared task.
</bodyText>
<subsectionHeader confidence="0.996677">
2.3 Snow
</subsectionHeader>
<bodyText confidence="0.999913307692308">
Snow – Sparse Network of Winnows – is an archi-
tecture for error-driven machine learning, consisting
of a sparse network of linear separator units over
a common predefined or incrementally learned fea-
ture space. The system assigns weights to each fea-
ture, and iteratively updates these weights in such
a way that the misclassification error is minimized.
For more details on Snow’s architecture, please re-
fer to Muñoz et al. (1999).
Table 2 presents the results obtained by Snow on
the NER task, when using the same methodology
from Muñoz et al. (1999), with the their templates2
and with the same templates as fnTBL.
</bodyText>
<footnote confidence="0.999318">
1The quality of chunk tags evolves as the algorithm pro-
gresses; there is no mismatch between the quality of the sur-
rounding chunks during training and testing.
2In this experiment, we used the feature patterns described
in Muñoz et al. (1999): a combination of up to 2 words in a
3-word window around the target word and a combination of
up to 4 chunks in a 7-word window around the target word. All
throughout the paper, Snow’s default parameters were used.
</footnote>
<figureCaption confidence="0.9969385">
Figure 1: Performance of applying Snow to TBL’s out-
put, plotted against iteration number
</figureCaption>
<subsectionHeader confidence="0.998362">
2.4 Stacking Classifiers
</subsectionHeader>
<bodyText confidence="0.936676105263158">
Both the fnTBL and the Snow methods have
strengths and weaknesses:
. fnTBL’s strength is represented by its dynamic
modeling of chunk tags – by starting in a sim-
ple state and using complex feature interac-
tions, it is able to reach a reasonable end-state.
Its weakness consists in its acute myopia: the
optimization is done greedily for the local con-
text, and the feature interaction is observed
only in the order in which the rules are se-
lected.
• Snow’s strength consists in its ability to model
interactions between the all features associated
with a sample. However, in order to obtain
good results, the system needs reliable contex-
tual information. Since the approach is not dy-
namic by nature, good initial chunk classifica-
tions are needed.
One way to address both weaknesses is to com-
bine the two approaches through stacking, by ap-
plying Snow on fnTBL’s output. This allows Snow
to have access to reasonably reliable contextual in-
formation, and also allows the output of fnTBL
to be corrected for multiple feature interaction.
This stacking approach has an intuitive interpreta-
tion: first, the corpus is dynamically labeled us-
ing the most important features through fnTBL
rules (coarse-grained optimization), and then is fine-
grained tuned through a few full-feature-interaction
iterations of Snow.
Table 2 contrasts stacking Snow and fnTBL with
running either fnTBL or Snow in isolation - an im-
provement of 1.6 F-measure points is obtained when
stacking is applied. Interestingly, as shown in Fig-
ure 1, the relation between performance and Snow-
iteration number is not linear: the system initially
takes a hit as it moves out of the local fnTBL maxi-
mum, but then proceeds to increase its performance,
</bodyText>
<table confidence="0.999014">
Method Accuracy FO-1
Spanish 98.42% 90.26
Dutch 98.54% 88.03
</table>
<tableCaption confidence="0.9762955">
Table 3: Unlabeled chunking results obtained by fnTBL
on the development sets
</tableCaption>
<bodyText confidence="0.994886">
finally converging after 10 iterations to a F-measure
value of 73.49.
</bodyText>
<sectionHeader confidence="0.98412" genericHeader="general terms">
3 Breaking-Up the Task
</sectionHeader>
<bodyText confidence="0.999072222222222">
Muñoz et al. (1999) examine a different method of
chunking, called Open/Close (O/C) method: 2 clas-
sifiers are used, one predicting open brackets and
one predicting closed brackets. A final optimiza-
tion stage pairs open and closed brackets through a
global search.
We propose here a method that is similar in
spirit to the O/C method, and also to Carreras and
Màrquez (2001), Arévalo et al. (2002):
</bodyText>
<listItem confidence="0.915292055555555">
1. In the first stage, detect only the entity bound-
aries, without identifying their type, using the
fnTBL system3;
2. Using a forward-backward type algorithm (FB
henceforth), determine the most probable type
of each entity detected in the first step.
This method has some enticing properties:
• Detecting only the entity boundaries is a sim-
pler problem, as different entity types share
common features; Table 3 shows the perfor-
mance obtained by the fnTBL system – the per-
formance is sensibly higher than the one shown
in Table 2;
• The FB algorithm allows for a global search
for the optimum, which is beneficial since both
fnTBL and Snow perform only local optimiza-
tions;
• The FB algorithm has access to both entity-
</listItem>
<bodyText confidence="0.94598">
internal and external contextual features (as
first described in McDonald (1996)); further-
more, since the chunks are collapsed, the local
area is also larger in span.
The input to the FB algorithm consists of a series
of chunks C1, ... , C,,,, each spanning a sequence of
words
</bodyText>
<equation confidence="0.755175333333333">
wl ... wbl-1 . w~~
� w~~ .. �� �
C1
</equation>
<footnote confidence="0.960528">
3For this task, Snow does not bring any improvement to the
fnTBL’s output.
</footnote>
<table confidence="0.985781666666667">
Method Spanish Dutch
FB performance 76.49 73.30
FB on perfect chunk breaks 83.52 81.30
</table>
<tableCaption confidence="0.992698">
Table 4: Forward-Backward results (F-measure) on the
development sets
</tableCaption>
<bodyText confidence="0.9598635">
For each marked entity C,, the goal is to determine
its most likely type:4
</bodyText>
<equation confidence="0.9601599">
E� = arg maxE E
��-�
� ��� +1
P�w��-�
� E~ . . . Enw -
en+1)
(1)
P (len(wb&apos;) �E�) • P (wb&apos; jEj)
where P�w�� -�
� E, . . . Enw&apos;+1) represents the
</equation>
<bodyText confidence="0.967906148148148">
entity-external/contextual probability, and
P (len (wb&apos;) �E�) P (wb&apos; jEj) is the entity-internal
probability. These probabilities are computed
using the standard Markov assumption of inde-
pendence, and the forward-backward algorithm5.
Both internal and external models are using 5-gram
language models, smoothed using the modified
discount method of Chen and Goodman (1998).
In the case of unseen words, backoff to the cap-
italization tag is performed: if wk is unknown,
P (wk IEj) = P (capit (wk) IES). Finally, the
probability P (len (wb&apos;) jEj) is assumed to be
exponentially distributed.
Table 4 shows the results obtained by stacking
the FB algorithm on top of fnTBL. Comparing
the results with the ones in Table 2, one can ob-
serve that the global search does improve the perfor-
mance by 3 F-measure points when compared with
fnTBL+Snow and 5 points when compared with the
fnTBL system. Also presented in Table 4 is the per-
formance of the algorithm on perfect boundaries;
more than 6 F-measure points can be gained by
improving the boundary detection alone. Table 5
presents the detailed performance of the FB algo-
rithm on all four data sets, broken by entity type.
A quick analysis of the results revealed that most
errors were made on the unknown words, both in
</bodyText>
<footnote confidence="0.99718225">
4We use the notation wl&amp;quot; = w, . . . w,,,,.
5It is notable here that the best entity type for a chunk is
computed by selecting the best entity in all combinations of
the other entity assignments in the sentence. This choice is
made because it reflects better the scoring method, and makes
the algorithm more similar to the HMM’s forward-backward
algorithm (Jelinek, 1997, chapter 13) rather than the Viterbi
algorithm.
</footnote>
<table confidence="0.659828285714286">
. .. . w~~ . .. wn,
� w~~ .. �� �
Cn
P (Ei lwl&apos;) =
arg maxE E
��-�
� ��� +1
</table>
<bodyText confidence="0.9980874">
Spanish and Dutch: the accuracy on known words is
97.4%/98.9% (Spanish/Dutch), while the accuracy
on unknown words is 83.4%/85.1%. This suggests
that lists of entities have the potential of being ex-
tremely beneficial for the algorithm.
</bodyText>
<sectionHeader confidence="0.999461" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999743272727273">
In conclusion, we have presented a classifier stack-
ing method which uses transformation-based learn-
ing to obtain a course-grained initial entity anno-
tation, then applies Snow to improve the classi-
fication on samples where there is strong feature
interaction and, finally, uses a forward-backward
algorithm to compute a global-best entity type
assignment. By using the pipelined processing,
this method improves the performance substan-
tially when compared with the original algorithms
(fnTBL, Snow+fnTBL).
</bodyText>
<sectionHeader confidence="0.99919" genericHeader="acknowledgments">
5 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999895666666667">
The author would like to thank Richard Wicen-
towski for providing additional language resources
(such as lemmatization information), even if they
were ultimately not used in the research, David
Yarowsky for his support and advice during this
research, and Cristina Nita-Rotaru for useful com-
ments. This work was supported by NSF grant IIS-
9985033 and ONR/MURI contract N00014-01-1-
0685.
</bodyText>
<sectionHeader confidence="0.998949" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996502095238095">
J. Aberdeen, D. Day, L. Hirschman, P. Robinson, and
M. Vilain. 1995. Mitre: Description of the Alembic
system used for MUC-6. In Proceedings of MUC-6,
pages 141–155.
M. Arévalo, X. Carreras, L. Màrquez, M. A. Martí,
L. Padró, and M. J. Simón. 2002. A proposal
for wide-coverage Spanish named entity recognition.
Technical Report LSI-02-30-R, Universitat Politèc-
nica de Catalunya.
D. M. Bikel, S. Miller, R. Schwartz, and R. Weischedel.
1997. Nymble: a high-performance learning name-
finder. In Proceedings ofANLP-97, pages 194–201.
A. Borthwick. 1999. A Maximum Entropy Approach to
Named Entity Recognition. Ph.D. thesis, New York
University.
E. Brill. 1995. Transformation-based error-driven learn-
ing and natural language processing: A case study
in part of speech tagging. Computational Linguistics,
21(4):543–565.
X. Carreras and L. Màrquez. 2001. Boosting trees for
clause splitting. In Proceedings of CoNNL’01.
</reference>
<table confidence="0.999881333333334">
Spanish devel precision recall F13-1
LOC 70.44% 83.45% 76.39
MISC 53.20% 63.60% 57.93
ORG 78.35% 73.00% 75.58
PER 86.28% 84.37% 85.31
overall 75.41% 77.60% 76.49
Spanish test precision recall F13-1
LOC 82.06% 79.34% 80.68
MISC 59.71% 61.47% 60.58
ORG 78.51% 78.29% 78.40
PER 82.94% 89.93% 86.29
overall 78.70% 79.40% 79.05
Dutch deve precision recall F13-1
LOC 81.15% 74.16% 77.50
MISC 72.02% 74.53% 73.25
ORG 79.92% 60.97% 69.17
PER 66.18% 84.04% 74.05
overall 73.09% 73.51% 73.30
Dutch test precision recall F13-1
LOC 86.69% 77.69% 81.94
MISC 75.21% 68.80% 71.86
ORG 74.68% 66.59% 70.40
PER 69.39% 86.05% 76.83
overall 75.10% 74.89% 74.99
</table>
<tableCaption confidence="0.990106">
Table 5: Results on the development and test sets in
</tableCaption>
<reference confidence="0.985290111111111">
Spanish and Dutch
S. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Tech-
nical Report TR-10-98, Harvard University.
F. Jelinek, 1997. Information Extraction From Speech
And Text. MIT Press.
D. McDonald, 1996. Corpus Processing for Lexical
Aquisition, chapter Internal and External Evidence
in the Identification and Semantic Categorization of
Proper Names, pages 21–39. MIT Press.
S. Miller, M. Crystal, H. Fox, L. Ramshaw, R. Schwarz,
R. Stone, and R. Weischedel. 1998. Bbn: Description
of the SIFT system as used for MUC-7. In MUC-7.
M. Muñoz, V. Punyakanok, D. Roth, and D. Zimak.
1999. A learning approach to shallow parsing. Tech-
nical Report 2087, Urbana, Illinois.
G. Ngai and R. Florian. 2001. Transformation-based
learning in the fast lane. In Proceedings ofNAACL’01,
pages 40–47.
A. Ratnaparkhi. 1996. A maximum entropy model for
part of speech tagging. In Proceedings EMNLP’96,
Philadelphia.
E. F. Tjong Kim Sang and J. Veenstra. 1999. Represent-
ing text chunks. In Proceedings ofEACL’99.
G. D. Zhou and J. Su. 2002. Named entity recognition
using a HMM-based chunk tagger. In Proceedings of
ACL’02.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.553870">
<title confidence="0.999182">Named Entity Recognition as a House of Cards: Classifier Stacking</title>
<author confidence="0.760632">Radu</author>
<affiliation confidence="0.994359">Department of Computer Science and Center for Language and Speech</affiliation>
<address confidence="0.866769">Johns Hopkins 3400 N. Charles St., Baltimore, MD 21218,</address>
<email confidence="0.988364">rflorian@cs.jhu.edu</email>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Aberdeen</author>
<author>D Day</author>
<author>L Hirschman</author>
<author>P Robinson</author>
<author>M Vilain</author>
</authors>
<title>Mitre: Description of the Alembic system used for MUC-6.</title>
<date>1995</date>
<booktitle>In Proceedings of MUC-6,</booktitle>
<tech>Technical Report LSI-02-30-R, Universitat Politècnica de Catalunya.</tech>
<pages>141--155</pages>
<contexts>
<context position="4467" citStr="Aberdeen et al., 1995" startWordPosition="695" endWordPosition="699">, etc); it is error–driven, therefore directly minimizes the 73.5 73 72.5 F−measure 72 71.5 71 70.5 70 Accuracy Method without capitalization information TnT 94.78% 66.72 MXPOST 95.02% 69.04 Snow 94.27% 65.94 fnTBL 94.92% 68.06 with capitalization information Snow (extended templates) 95.15% 71.36 fnTBL 95.57% 71.88 fnTBL+Snow 95.36% 73.49 0 2 4 68 10 Iteration Number Table 2: Comparative results for different methods on the Spanish development data ultimate evaluation measure: the error rate; and it has an inherently dynamic behavior1. TBL has been previously applied to the English NER task (Aberdeen et al., 1995), with good results. The fnTBL-based NER system is designed in the same way as Brill’s POS tagger (Brill, 1995), consisting of a morphological stage, where unknown words’ chunks are guessed based on their morphological and capitalization representation, followed by a contextual stage, in which the full interaction between the words’ features is leveraged for learning. The feature templates used are based on a combination of word, chunk and capitalization information of words in a 7-word window around the target word. The entire template list (133 templates) will be made available from the auth</context>
</contexts>
<marker>Aberdeen, Day, Hirschman, Robinson, Vilain, 1995</marker>
<rawString>J. Aberdeen, D. Day, L. Hirschman, P. Robinson, and M. Vilain. 1995. Mitre: Description of the Alembic system used for MUC-6. In Proceedings of MUC-6, pages 141–155. M. Arévalo, X. Carreras, L. Màrquez, M. A. Martí, L. Padró, and M. J. Simón. 2002. A proposal for wide-coverage Spanish named entity recognition. Technical Report LSI-02-30-R, Universitat Politècnica de Catalunya.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Bikel</author>
<author>S Miller</author>
<author>R Schwartz</author>
<author>R Weischedel</author>
</authors>
<title>Nymble: a high-performance learning namefinder.</title>
<date>1997</date>
<booktitle>In Proceedings ofANLP-97,</booktitle>
<pages>194--201</pages>
<contexts>
<context position="2041" citStr="Bikel et al., 1997" startWordPosition="319" endWordPosition="322">formation (such as lists of countries/cities/regions, organizations, people names, etc.), due to the short exposition space, we decided to restrict them to this feature space. Table 2 presents the results obtained by running off-the-shelf part-of-speech/text chunking classifiers; all of them use just word information, albeit in different ways. The leader of the pack is the MXPOST tagger (Ratnaparkhi, 1996). The measure of choice for the NER task is F-measure, the harmonic mean of precision and recall: F� = 2R, usually computed with , = 1. As observed by participants in the MUC-6 and -7 tasks (Bikel et al., 1997; Borthwick, 1999; Miller et 1: Capitalization information 2: Presence in dictionary first_cap, all_caps, all_lower, upper, lower, number, punct, other both, none Table 1: Capitalization information al., 1998), an important feature for the NER task is information relative to word capitalization. In an approach similar to Zhou and Su (2002), we extracted for each word a 2-byte code, as summarized in Table 1. The first byte specifies the capitalization of the word (first letter capital, etc), while the second specifies whether the word is present in the dictionary in lower case, upper case, both</context>
</contexts>
<marker>Bikel, Miller, Schwartz, Weischedel, 1997</marker>
<rawString>D. M. Bikel, S. Miller, R. Schwartz, and R. Weischedel. 1997. Nymble: a high-performance learning namefinder. In Proceedings ofANLP-97, pages 194–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Borthwick</author>
</authors>
<title>A Maximum Entropy Approach to Named Entity Recognition.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>New York University.</institution>
<contexts>
<context position="2058" citStr="Borthwick, 1999" startWordPosition="323" endWordPosition="324">ists of countries/cities/regions, organizations, people names, etc.), due to the short exposition space, we decided to restrict them to this feature space. Table 2 presents the results obtained by running off-the-shelf part-of-speech/text chunking classifiers; all of them use just word information, albeit in different ways. The leader of the pack is the MXPOST tagger (Ratnaparkhi, 1996). The measure of choice for the NER task is F-measure, the harmonic mean of precision and recall: F� = 2R, usually computed with , = 1. As observed by participants in the MUC-6 and -7 tasks (Bikel et al., 1997; Borthwick, 1999; Miller et 1: Capitalization information 2: Presence in dictionary first_cap, all_caps, all_lower, upper, lower, number, punct, other both, none Table 1: Capitalization information al., 1998), an important feature for the NER task is information relative to word capitalization. In an approach similar to Zhou and Su (2002), we extracted for each word a 2-byte code, as summarized in Table 1. The first byte specifies the capitalization of the word (first letter capital, etc), while the second specifies whether the word is present in the dictionary in lower case, upper case, both or neither forms</context>
</contexts>
<marker>Borthwick, 1999</marker>
<rawString>A. Borthwick. 1999. A Maximum Entropy Approach to Named Entity Recognition. Ph.D. thesis, New York University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="4578" citStr="Brill, 1995" startWordPosition="717" endWordPosition="718">thout capitalization information TnT 94.78% 66.72 MXPOST 95.02% 69.04 Snow 94.27% 65.94 fnTBL 94.92% 68.06 with capitalization information Snow (extended templates) 95.15% 71.36 fnTBL 95.57% 71.88 fnTBL+Snow 95.36% 73.49 0 2 4 68 10 Iteration Number Table 2: Comparative results for different methods on the Spanish development data ultimate evaluation measure: the error rate; and it has an inherently dynamic behavior1. TBL has been previously applied to the English NER task (Aberdeen et al., 1995), with good results. The fnTBL-based NER system is designed in the same way as Brill’s POS tagger (Brill, 1995), consisting of a morphological stage, where unknown words’ chunks are guessed based on their morphological and capitalization representation, followed by a contextual stage, in which the full interaction between the words’ features is leveraged for learning. The feature templates used are based on a combination of word, chunk and capitalization information of words in a 7-word window around the target word. The entire template list (133 templates) will be made available from the author’s web page after the conclusion of the shared task. 2.3 Snow Snow – Sparse Network of Winnows – is an archit</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>E. Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging. Computational Linguistics, 21(4):543–565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L Màrquez</author>
</authors>
<title>Boosting trees for clause splitting.</title>
<date>2001</date>
<booktitle>In Proceedings of CoNNL’01.</booktitle>
<contexts>
<context position="8631" citStr="Carreras and Màrquez (2001)" startWordPosition="1389" endWordPosition="1392">ease its performance, Method Accuracy FO-1 Spanish 98.42% 90.26 Dutch 98.54% 88.03 Table 3: Unlabeled chunking results obtained by fnTBL on the development sets finally converging after 10 iterations to a F-measure value of 73.49. 3 Breaking-Up the Task Muñoz et al. (1999) examine a different method of chunking, called Open/Close (O/C) method: 2 classifiers are used, one predicting open brackets and one predicting closed brackets. A final optimization stage pairs open and closed brackets through a global search. We propose here a method that is similar in spirit to the O/C method, and also to Carreras and Màrquez (2001), Arévalo et al. (2002): 1. In the first stage, detect only the entity boundaries, without identifying their type, using the fnTBL system3; 2. Using a forward-backward type algorithm (FB henceforth), determine the most probable type of each entity detected in the first step. This method has some enticing properties: • Detecting only the entity boundaries is a simpler problem, as different entity types share common features; Table 3 shows the performance obtained by the fnTBL system – the performance is sensibly higher than the one shown in Table 2; • The FB algorithm allows for a global search</context>
</contexts>
<marker>Carreras, Màrquez, 2001</marker>
<rawString>X. Carreras and L. Màrquez. 2001. Boosting trees for clause splitting. In Proceedings of CoNNL’01.</rawString>
</citation>
<citation valid="false">
<institution>Spanish and Dutch</institution>
<marker></marker>
<rawString>Spanish and Dutch</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Harvard University.</institution>
<contexts>
<context position="10511" citStr="Chen and Goodman (1998)" startWordPosition="1713" endWordPosition="1716">asure) on the development sets For each marked entity C,, the goal is to determine its most likely type:4 E� = arg maxE E ��-� � ��� +1 P�w��-� � E~ . . . Enw - en+1) (1) P (len(wb&apos;) �E�) • P (wb&apos; jEj) where P�w�� -� � E, . . . Enw&apos;+1) represents the entity-external/contextual probability, and P (len (wb&apos;) �E�) P (wb&apos; jEj) is the entity-internal probability. These probabilities are computed using the standard Markov assumption of independence, and the forward-backward algorithm5. Both internal and external models are using 5-gram language models, smoothed using the modified discount method of Chen and Goodman (1998). In the case of unseen words, backoff to the capitalization tag is performed: if wk is unknown, P (wk IEj) = P (capit (wk) IES). Finally, the probability P (len (wb&apos;) jEj) is assumed to be exponentially distributed. Table 4 shows the results obtained by stacking the FB algorithm on top of fnTBL. Comparing the results with the ones in Table 2, one can observe that the global search does improve the performance by 3 F-measure points when compared with fnTBL+Snow and 5 points when compared with the fnTBL system. Also presented in Table 4 is the performance of the algorithm on perfect boundaries;</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>S. Chen and J. Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Information Extraction From Speech And Text.</title>
<date>1997</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="11773" citStr="Jelinek, 1997" startWordPosition="1940" endWordPosition="1941">proving the boundary detection alone. Table 5 presents the detailed performance of the FB algorithm on all four data sets, broken by entity type. A quick analysis of the results revealed that most errors were made on the unknown words, both in 4We use the notation wl&amp;quot; = w, . . . w,,,,. 5It is notable here that the best entity type for a chunk is computed by selecting the best entity in all combinations of the other entity assignments in the sentence. This choice is made because it reflects better the scoring method, and makes the algorithm more similar to the HMM’s forward-backward algorithm (Jelinek, 1997, chapter 13) rather than the Viterbi algorithm. . .. . w~~ . .. wn, � w~~ .. �� � Cn P (Ei lwl&apos;) = arg maxE E ��-� � ��� +1 Spanish and Dutch: the accuracy on known words is 97.4%/98.9% (Spanish/Dutch), while the accuracy on unknown words is 83.4%/85.1%. This suggests that lists of entities have the potential of being extremely beneficial for the algorithm. 4 Conclusion In conclusion, we have presented a classifier stacking method which uses transformation-based learning to obtain a course-grained initial entity annotation, then applies Snow to improve the classification on samples where ther</context>
</contexts>
<marker>Jelinek, 1997</marker>
<rawString>F. Jelinek, 1997. Information Extraction From Speech And Text. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McDonald</author>
</authors>
<title>Corpus Processing for Lexical Aquisition,</title>
<date>1996</date>
<booktitle>chapter Internal and External Evidence in the Identification and Semantic Categorization of Proper Names,</booktitle>
<pages>21--39</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="9453" citStr="McDonald (1996)" startWordPosition="1529" endWordPosition="1530">ne the most probable type of each entity detected in the first step. This method has some enticing properties: • Detecting only the entity boundaries is a simpler problem, as different entity types share common features; Table 3 shows the performance obtained by the fnTBL system – the performance is sensibly higher than the one shown in Table 2; • The FB algorithm allows for a global search for the optimum, which is beneficial since both fnTBL and Snow perform only local optimizations; • The FB algorithm has access to both entityinternal and external contextual features (as first described in McDonald (1996)); furthermore, since the chunks are collapsed, the local area is also larger in span. The input to the FB algorithm consists of a series of chunks C1, ... , C,,,, each spanning a sequence of words wl ... wbl-1 . w~~ � w~~ .. �� � C1 3For this task, Snow does not bring any improvement to the fnTBL’s output. Method Spanish Dutch FB performance 76.49 73.30 FB on perfect chunk breaks 83.52 81.30 Table 4: Forward-Backward results (F-measure) on the development sets For each marked entity C,, the goal is to determine its most likely type:4 E� = arg maxE E ��-� � ��� +1 P�w��-� � E~ . . . Enw - en+1</context>
</contexts>
<marker>McDonald, 1996</marker>
<rawString>D. McDonald, 1996. Corpus Processing for Lexical Aquisition, chapter Internal and External Evidence in the Identification and Semantic Categorization of Proper Names, pages 21–39. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miller</author>
<author>M Crystal</author>
<author>H Fox</author>
<author>L Ramshaw</author>
<author>R Schwarz</author>
<author>R Stone</author>
<author>R Weischedel</author>
</authors>
<title>Bbn: Description of the SIFT system as used for MUC-7.</title>
<date>1998</date>
<booktitle>In MUC-7.</booktitle>
<marker>Miller, Crystal, Fox, Ramshaw, Schwarz, Stone, Weischedel, 1998</marker>
<rawString>S. Miller, M. Crystal, H. Fox, L. Ramshaw, R. Schwarz, R. Stone, and R. Weischedel. 1998. Bbn: Description of the SIFT system as used for MUC-7. In MUC-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Muñoz</author>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>D Zimak</author>
</authors>
<title>A learning approach to shallow parsing.</title>
<date>1999</date>
<tech>Technical Report</tech>
<location>Urbana, Illinois.</location>
<contexts>
<context position="5561" citStr="Muñoz et al. (1999)" startWordPosition="876" endWordPosition="879">ords in a 7-word window around the target word. The entire template list (133 templates) will be made available from the author’s web page after the conclusion of the shared task. 2.3 Snow Snow – Sparse Network of Winnows – is an architecture for error-driven machine learning, consisting of a sparse network of linear separator units over a common predefined or incrementally learned feature space. The system assigns weights to each feature, and iteratively updates these weights in such a way that the misclassification error is minimized. For more details on Snow’s architecture, please refer to Muñoz et al. (1999). Table 2 presents the results obtained by Snow on the NER task, when using the same methodology from Muñoz et al. (1999), with the their templates2 and with the same templates as fnTBL. 1The quality of chunk tags evolves as the algorithm progresses; there is no mismatch between the quality of the surrounding chunks during training and testing. 2In this experiment, we used the feature patterns described in Muñoz et al. (1999): a combination of up to 2 words in a 3-word window around the target word and a combination of up to 4 chunks in a 7-word window around the target word. All throughout th</context>
<context position="8277" citStr="Muñoz et al. (1999)" startWordPosition="1330" endWordPosition="1333">d fnTBL with running either fnTBL or Snow in isolation - an improvement of 1.6 F-measure points is obtained when stacking is applied. Interestingly, as shown in Figure 1, the relation between performance and Snowiteration number is not linear: the system initially takes a hit as it moves out of the local fnTBL maximum, but then proceeds to increase its performance, Method Accuracy FO-1 Spanish 98.42% 90.26 Dutch 98.54% 88.03 Table 3: Unlabeled chunking results obtained by fnTBL on the development sets finally converging after 10 iterations to a F-measure value of 73.49. 3 Breaking-Up the Task Muñoz et al. (1999) examine a different method of chunking, called Open/Close (O/C) method: 2 classifiers are used, one predicting open brackets and one predicting closed brackets. A final optimization stage pairs open and closed brackets through a global search. We propose here a method that is similar in spirit to the O/C method, and also to Carreras and Màrquez (2001), Arévalo et al. (2002): 1. In the first stage, detect only the entity boundaries, without identifying their type, using the fnTBL system3; 2. Using a forward-backward type algorithm (FB henceforth), determine the most probable type of each entit</context>
</contexts>
<marker>Muñoz, Punyakanok, Roth, Zimak, 1999</marker>
<rawString>M. Muñoz, V. Punyakanok, D. Roth, and D. Zimak. 1999. A learning approach to shallow parsing. Technical Report 2087, Urbana, Illinois.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ngai</author>
<author>R Florian</author>
</authors>
<title>Transformation-based learning in the fast lane.</title>
<date>2001</date>
<booktitle>In Proceedings ofNAACL’01,</booktitle>
<pages>40--47</pages>
<contexts>
<context position="2879" citStr="Ngai and Florian, 2001" startWordPosition="457" endWordPosition="460">mportant feature for the NER task is information relative to word capitalization. In an approach similar to Zhou and Su (2002), we extracted for each word a 2-byte code, as summarized in Table 1. The first byte specifies the capitalization of the word (first letter capital, etc), while the second specifies whether the word is present in the dictionary in lower case, upper case, both or neither forms. These two codes are extracted in order to offer both a way of backing-off in sparse data cases (unknown words) and a way of encouraging generalization. Table 2 shows the performance of the fnTBL (Ngai and Florian, 2001) and Snow systems when using the capitalization information, both systems displaying considerably better performance. 2.2 Transformation-Based Learning Transformation-based learning (TBL henceforth) is an error-driven machine learning technique which works by first assigning an initial classification to the data, and then automatically proposing, evaluating and selecting the transformations that maximally decrease the number of errors. Each such transformation, or rule, consists of a predicate and a target. In our implementation of TBL – fnTBL – predicates consist of a conjunction of atomic pr</context>
</contexts>
<marker>Ngai, Florian, 2001</marker>
<rawString>G. Ngai and R. Florian. 2001. Transformation-based learning in the fast lane. In Proceedings ofNAACL’01, pages 40–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part of speech tagging.</title>
<date>1996</date>
<booktitle>In Proceedings EMNLP’96,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="1832" citStr="Ratnaparkhi, 1996" startWordPosition="280" endWordPosition="281"> this paper are using only information that can be extracted directly from the training data: the words, their capitalization information and the chunk tags. While they can definitely incorporate additional information (such as lists of countries/cities/regions, organizations, people names, etc.), due to the short exposition space, we decided to restrict them to this feature space. Table 2 presents the results obtained by running off-the-shelf part-of-speech/text chunking classifiers; all of them use just word information, albeit in different ways. The leader of the pack is the MXPOST tagger (Ratnaparkhi, 1996). The measure of choice for the NER task is F-measure, the harmonic mean of precision and recall: F� = 2R, usually computed with , = 1. As observed by participants in the MUC-6 and -7 tasks (Bikel et al., 1997; Borthwick, 1999; Miller et 1: Capitalization information 2: Presence in dictionary first_cap, all_caps, all_lower, upper, lower, number, punct, other both, none Table 1: Capitalization information al., 1998), an important feature for the NER task is information relative to word capitalization. In an approach similar to Zhou and Su (2002), we extracted for each word a 2-byte code, as sum</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A maximum entropy model for part of speech tagging. In Proceedings EMNLP’96, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Tjong Kim Sang</author>
<author>J Veenstra</author>
</authors>
<title>Representing text chunks.</title>
<date>1999</date>
<booktitle>In Proceedings ofEACL’99.</booktitle>
<contexts>
<context position="1030" citStr="Sang and Veenstra, 1999" startWordPosition="153" endWordPosition="156">rill, 1995), Snow (sparse network of winnows (Muñoz et al., 1999)) and a forward-backward algorithm are stacked (the output of one classifier is passed as input to the next classifier), yielding considerable improvement in performance. In addition, in agreement with other studies on the same problem, the enhancement of the feature space (in the form of capitalization information) is shown to be especially beneficial to this task. 2 Computational Approaches All approaches to the NER task presented in this paper, except the one presented in Section 3, use the IOB chunk tagging method (Tjong Kim Sang and Veenstra, 1999) for identifying the named entities. 2.1 Feature Space and Baselines A careful selection of the feature space is a very important part of classifier design. The algorithms presented in this paper are using only information that can be extracted directly from the training data: the words, their capitalization information and the chunk tags. While they can definitely incorporate additional information (such as lists of countries/cities/regions, organizations, people names, etc.), due to the short exposition space, we decided to restrict them to this feature space. Table 2 presents the results ob</context>
</contexts>
<marker>Sang, Veenstra, 1999</marker>
<rawString>E. F. Tjong Kim Sang and J. Veenstra. 1999. Representing text chunks. In Proceedings ofEACL’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G D Zhou</author>
<author>J Su</author>
</authors>
<title>Named entity recognition using a HMM-based chunk tagger.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL’02.</booktitle>
<contexts>
<context position="2382" citStr="Zhou and Su (2002)" startWordPosition="368" endWordPosition="371">ays. The leader of the pack is the MXPOST tagger (Ratnaparkhi, 1996). The measure of choice for the NER task is F-measure, the harmonic mean of precision and recall: F� = 2R, usually computed with , = 1. As observed by participants in the MUC-6 and -7 tasks (Bikel et al., 1997; Borthwick, 1999; Miller et 1: Capitalization information 2: Presence in dictionary first_cap, all_caps, all_lower, upper, lower, number, punct, other both, none Table 1: Capitalization information al., 1998), an important feature for the NER task is information relative to word capitalization. In an approach similar to Zhou and Su (2002), we extracted for each word a 2-byte code, as summarized in Table 1. The first byte specifies the capitalization of the word (first letter capital, etc), while the second specifies whether the word is present in the dictionary in lower case, upper case, both or neither forms. These two codes are extracted in order to offer both a way of backing-off in sparse data cases (unknown words) and a way of encouraging generalization. Table 2 shows the performance of the fnTBL (Ngai and Florian, 2001) and Snow systems when using the capitalization information, both systems displaying considerably bette</context>
</contexts>
<marker>Zhou, Su, 2002</marker>
<rawString>G. D. Zhou and J. Su. 2002. Named entity recognition using a HMM-based chunk tagger. In Proceedings of ACL’02.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>