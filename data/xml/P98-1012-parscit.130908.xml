<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.712994333333333">
Entity-Based Cross-Document Coreferencing Using the Vector
Space Model
Amit Bagga
</title>
<author confidence="0.679489">
Box 90129
</author>
<affiliation confidence="0.9945015">
Dept. of Computer Science
Duke University
</affiliation>
<address confidence="0.83352">
Durham, NC 27708-0129
</address>
<email confidence="0.997555">
amit@cs.duke.edu
</email>
<author confidence="0.981703">
Breck Baldwin
</author>
<affiliation confidence="0.995046">
Institute for Research in Cognitive Sciences
University of Pennsylvania
</affiliation>
<address confidence="0.883384">
3401 Walnut St. 400C
Philadelphia, PA 19104
</address>
<email confidence="0.999042">
breck@unagi.cis.upenn.edu
</email>
<sectionHeader confidence="0.9948" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999195466666667">
Cross-document coreference occurs when the same
person, place, event, or concept is discussed in more
than one text source. Computer recognition of this
phenomenon is important because it helps break
&amp;quot;the document boundary&amp;quot; by allowing a user to ex-
amine information about a particular entity from
multiple text sources at the same time. In this paper
we describe a cross-document coreference resolution
algorithm which uses the Vector Space Model to re-
solve ambiguities between people having the same
name. In addition, we also describe a scoring algo-
rithm for evaluating the cross-document coreference
chains produced by our system and we compare our
algorithm to the scoring algorithm used in the MUC-
6 (within document) coreference task.
</bodyText>
<sectionHeader confidence="0.998748" genericHeader="categories and subject descriptors">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999646285714285">
Cross-document coreference occurs when the same
person, place, event, or concept is discussed in more
than one text source. Computer recognition of this
phenomenon is important because it helps break
&amp;quot;the document boundary&amp;quot; by allowing a user to ex-
amine information about a particular entity from
multiple text sources at the same time. In partic-
ular, resolving cross-document coreferences allows
a user to identify trends and dependencies across
documents. Cross-document coreference can also be
used as the central tool for producing summaries
from multiple documents, and for information fu-
sion, both of which have been identified as advanced
areas of research by the TIPSTER Phase III pro-
gram. Cross-document coreference was also identi-
fied as one of the potential tasks for the Sixth Mes-
sage Understanding Conference (MUC-6) but was
not included as a formal task because it was consid-
ered too ambitious (Grishman 94).
In this paper we describe a highly successful cross-
document coreference resolution algorithm which
uses the Vector Space Model to resolve ambiguities
between people having the same name. In addition,
we also describe a scoring algorithm for evaluating
the cross-document coreference chains produced by
our system and we compare our algorithm to the
scoring algorithm used in the MUC-6 (within docu-
ment) coreference task.
</bodyText>
<sectionHeader confidence="0.9744525" genericHeader="general terms">
2 Cross-Document Coreference: The
Problem
</sectionHeader>
<bodyText confidence="0.9999532">
Cross-document coreference is a distinct technology
from Named Entity recognizers like IsoQuest&apos;s Ne-
tOwl and IBM&apos;s Textract because it attempts to
determine whether name matches are actually the
same individual (not all John Smiths are the same).
Neither NetOwl or Textract have mechanisms which
try to keep same-named individuals distinct if they
are different people.
Cross-document coreference also differs in sub-
stantial ways from within-document coreference.
Within a document there is a certain amount of
consistency which cannot be expected across doc-
uments. In addition, the problems encountered dur-
ing within document coreference are compounded
when looking for coreferences across documents be-
cause the underlying principles of linguistics and
discourse context no longer apply across docu-
ments. Because the underlying assumptions in cross-
document coreference are so distinct, they require
novel approaches.
</bodyText>
<sectionHeader confidence="0.947863" genericHeader="introduction">
3 Architecture and the Methodology
</sectionHeader>
<bodyText confidence="0.999340125">
Figure 1 shows the architecture of the cross-
document system developed. The system is built
upon the University of Pennsylvania&apos;s within docu-
ment coreference system, CAMP, which participated
in the Seventh Message Understanding Conference
(MUC-7) within document coreference task (MUC-
7 1998).
Our system takes as input the coreference pro-
cessed documents output by CAMP. It then passes
these documents through the SentenceExtractor
module which extracts, for each document, all the
sentences relevant to a particular entity of interest.
The VSM-Disambiguate module then uses a vector
space model algorithm to compute similarities be-
tween the sentences extracted for each pair of docu-
ments.
</bodyText>
<page confidence="0.994196">
79
</page>
<figure confidence="0.997593684210526">
Coreference Chains for doc.01
doc.02
University of Pennsylvania&apos;s
Pennlight Coreference System
Coreference Chains for doc.02
C;) CP
&apos; C:D
doc.nn
Coreference Chains for doc
SentenceExtractor
Cross-Document Coreference Chains
doc.01 doc.36 doc.zz
VSM-
doc.19 doc.38
Disambiguate
doc.20
surnmary.01
surnmary.02
surtunaty.nn
</figure>
<figureCaption confidence="0.998880777777778">
Figure 1: Architecture of the Cross-Document Coreference System
John Perry, of Weston Golf Club, an-
nounced his resignation yesterday. He was
the President of the Massachusetts Golf
Association. During his two years in of-
fice, Perry guided the MGA into a closer
relationship with the Women&apos;s Golf Asso-
ciation of Massachusetts.
Figure 2: Extract from doc.36
</figureCaption>
<bodyText confidence="0.570277428571429">
Oliver &amp;quot;Biff&amp;quot; Kelly of Weymouth suc-
ceeds John Perry as president of the Mas-
sachusetts Golf Association. &amp;quot;We will have
continued growth in the future,&amp;quot; said Kelly,
who will serve for two years. &amp;quot;There&apos;s been
a lot of changes and there will be continued
changes as we head into the year 2000.&amp;quot;
</bodyText>
<figureCaption confidence="0.999062">
Figure 4: Extract from doc.38
</figureCaption>
<figure confidence="0.9990685">
CWomen&apos;;&apos; \
Golf
Associatioy
-7-
Clohn, Perry) (&amp;quot; West COliver 73iff. Clohn PenD Massachusetts
Golf ClubD Golf Association
</figure>
<figureCaption confidence="0.999817">
Figure 3: Coreference Chains for doc.36
</figureCaption>
<bodyText confidence="0.977583">
Details about each of the main steps of the cross-
document coreference algorithm are given below.
</bodyText>
<listItem confidence="0.665166">
• First, for each article, CAMP is run on the ar-
ticle. It produces coreference chains for all the
entities mentioned in the article. For example,
</listItem>
<bodyText confidence="0.783823333333333">
consider the two extracts in Figures 2 and 4.
The coreference chains output by CAMP for the
two extracts are shown in Figures 3 and 5.
</bodyText>
<figureCaption confidence="0.81498">
Figure 5: Coreference Chains for doc.38
</figureCaption>
<listItem confidence="0.903121666666667">
• Next, for the coreference chain of interest within
each article (for example, the coreference chain
that contains &amp;quot;John Perry&amp;quot;), the Sentence Ex-
tractor module extracts all the sentences that
contain the noun phrases which form the coref-
erence chain. In other words, the SentenceEx-
</listItem>
<bodyText confidence="0.79786075">
tractor module produces a &amp;quot;summary&amp;quot; of the ar-
ticle with respect to the entity of interest. These
summaries are a special case of the query sensi-
tive techniques being developed at Penn using
</bodyText>
<page confidence="0.994497">
80
</page>
<bodyText confidence="0.999035">
CAMP. Therefore, for doc.36 (Figure 2), since
at least one of the three noun phrases (&amp;quot;John
Perry,&amp;quot; &amp;quot;he,&amp;quot; and &amp;quot;Perry&amp;quot;) in the coreference
chain of interest appears in each of the three
sentences in the extract, the summary produced
by SentenceExtractor is the extract itself. On
the other hand, the summary produced by Sen-
tenceExtractor for the coreference chain of in-
terest in doc.38 is only the first sentence of the
extract because the only element of the corefer-
ence chain appears in this sentence.
</bodyText>
<listItem confidence="0.78006">
• For each article, the VSM-Disambiguate mod-
ule uses the summary extracted by the Sen-
tenceExtractor and computes its similarity with
the summaries extracted from each of the other
articles. Summaries having similarity above a
certain threshold are considered to be regard-
ing the same entity.
</listItem>
<sectionHeader confidence="0.8485965" genericHeader="method">
4 University of Pennsylvania&apos;s
CAMP System
</sectionHeader>
<bodyText confidence="0.99982195">
The University of Pennsylvania&apos;s CAMP system re-
solves within document coreferences for several dif-
ferent classes including pronouns, and proper names
(Baldwin 95). It ranked among the top systems
in the coreference task during the MUC-6 and the
MUC-7 evaluations.
The coreference chains output by CAMP enable
us to gather all the information about the entity of
interest in an article. This information about the
entity is gathered by the SentenceExtractor mod-
ule and is used by the VSM-Disambiguate module
for disambiguation purposes. Consider the extract
for doc.36 shown in Figure 2. We are able to in-
clude the fact that the John Perry mentioned in this
article was the president of the Massachusetts Golf
Association only because CAMP recognized that the
&amp;quot;he&amp;quot; in the second sentence is coreferent with &amp;quot;John
Perry&amp;quot; in the first. And it is this fact which actually
helps VSM-Disambiguate decide that the two John
Perrys in doc.36 and doc.38 are the same person.
</bodyText>
<sectionHeader confidence="0.944308" genericHeader="method">
5 The Vector Space Model
</sectionHeader>
<bodyText confidence="0.999786909090909">
The vector space model used for disambiguating en-
tities across documents is the standard vector space
model used widely in information retrieval (Salton
89). In this model, each summary extracted by the
SentenceExtractor module is stored as a vector of
terms. The terms in the vector are in their mor-
phological root form and are filtered for stop-words
(words that have no information content like a, the,
of, an, ...). If Si and S2 are the vectors for the two
summaries extracted from documents D1 and D21
then their similarity is computed as:
</bodyText>
<equation confidence="0.747748">
Sim(Si,S2) = X W2j
common terms ti
</equation>
<bodyText confidence="0.9999234">
where tj is a term present in both Si and S2, W1j is
the weight of the term t3 in S1 and w23 is the weight
of ti in S2.
The weight of a term tj in the vector St for a
summary is given by:
</bodyText>
<equation confidence="0.996691">
Wij =
+ 42 + ...+
</equation>
<bodyText confidence="0.999740769230769">
where t f is the frequency of the term t3 in the sum-
mary, N is the total number of documents in the
collection being examined, and df is the number of
documents in the collection that the term tj occurs
in. \MI + 42 + + 4n is the cosine normaliza-
tion factor and is equal to the Euclidean length of
the vector Si.
The VSM-Disambiguate module, for each sum-
mary Si, computes the similarity of that summary
with each of the other summaries. If the similarity
computed is above a pre-defined threshold, then the
entity of interest in the two summaries are consid-
ered to be coreferent.
</bodyText>
<sectionHeader confidence="0.998867" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999795928571429">
The cross-document coreference system was tested
on a highly ambiguous test set which consisted of
197 articles from 1996 and 1997 editions of the
New York Times. The sole criteria for including
an article in the test set was the presence or the
absence of a string in the article which matched
the &amp;quot;/John.*?Smithr regular expression. In other
words, all of the articles either contained the name
John Smith or contained some variation with a mid-
dle initial/name. The system did not use any New
York Times data for training purposes. The answer
keys regarding the cross-document chains were man-
ually created, but the scoring was completely auto-
mated.
</bodyText>
<subsectionHeader confidence="0.999926">
6.1 Analysis of the Data
</subsectionHeader>
<bodyText confidence="0.999301727272727">
There were 35 different John Smiths mentioned in
the articles. Of these, 24 of them only had one ar-
ticle which mentioned them. The other 173 articles
were regarding the 11 remaining John Smiths. The
background of these John Smiths , and the number
of articles pertaining to each, varied greatly. De-
scriptions of a few of the John Smiths are: Chairman
and CEO of General Motors, assistant track coach at
UCLA, the legendary explorer, and the main charac-
ter in Disney&apos;s Pocahontas, former president of the
Labor Party of Britain.
</bodyText>
<sectionHeader confidence="0.972793" genericHeader="method">
7 Scoring the Output
</sectionHeader>
<bodyText confidence="0.99995375">
In order to score the cross-document coreference
chains output by the system, we had to map the
cross-document coreference scoring problem to a
within-document coreference scoring problem. This
</bodyText>
<equation confidence="0.940418">
t f x log icsiv
</equation>
<page confidence="0.981005">
81
</page>
<bodyText confidence="0.999695058823529">
was done by creating a meta document consisting
Of the file names of each of the documents that the
system was run on. Assuming that each of the docu-
ments in the data set was about a single John Smith,
the cross-document coreference chains produced by
the system could now be evaluated by scoring the
corresponding within-document coreference chains
in the meta document.
We used two different scoring algorithms for scor-
ing the output. The first was the standard algorithm
for within-document coreference chains which was
used for the evaluation of the systems participating
in the MUC-6 and the MUC-7 coreference tasks.
The shortcomings of the MUC scoring algorithm
when used for the cross-document coreference task
forced us to develop a second algorithm.
Details about both these algorithms follow.
</bodyText>
<sectionHeader confidence="0.809264" genericHeader="method">
7.1 The MUC Coreference Scoring
Algorithml
</sectionHeader>
<bodyText confidence="0.999664866666667">
The MUC algorithm computes precision and recall
statistics by looking at the number of links identi-
fied by a system compared to the links in an answer
key. In the model-theoretic description of the al-
gorithm that follows, the term &amp;quot;key&amp;quot; refers to the
manually annotated coreference chains (the truth)
while the term &amp;quot;response&amp;quot; refers to the coreference
chains output by a system. An equivalence set is the
transitive closure of a coreference chain. The algo-
rithm, developed by (Vilain 95), computes recall in
the following way.
First, let S be an equivalence set generated by the
key, and let R1 ... R,„. be equivalence classes gener-
ated by the response. Then we define the following
functions over S:
</bodyText>
<listItem confidence="0.995277777777778">
• p(S) is a partition of S relative to the response.
Each subset of S in the partition is formed by
intersecting S and those response sets R, that
overlap S. Note that the equivalence classes de-
fined by the response may include implicit sin-
gleton sets - these correspond to elements that
are mentioned in the key but not in the re-
sponse. For example, say the key generates the
equivalence class S = {A B C D}, and the re-
sponse is simply &lt;A-B&gt;. The relative partition
p(S) is then {A B} {C} and {D}.
• c(S) is the minimal number of &amp;quot;correct&amp;quot; links
necessary to generate the equivalence class S. It
is clear that c(S) is one less than the cardinality
of S, i.e., c(S) = (ISI — 1) .
• m(S) is the number of &amp;quot;missing&amp;quot; links in the
response relative to the key set S. As noted
above, this is the number of links necessary to
</listItem>
<footnote confidence="0.5120605">
1The exposition of this scorer has been taken nearly en-
tirely from (Vilain 95).
</footnote>
<equation confidence="0.4233885">
t r I 5
PckkPAP
</equation>
<figureCaption confidence="0.986612">
Figure 6: Truth
</figureCaption>
<figure confidence="0.954033">
11. I34c 4.4 I 5
</figure>
<figureCaption confidence="0.99998">
Figure 7: Response: Example 1
</figureCaption>
<bodyText confidence="0.97860775">
fully reunite any components of the p(S) parti-
tion. We note that this is simply one fewer than
the number of elements in the partition, that is,
m(S) = (Ip(S)I — 1) .
Looking in isolation at a single equivalence class
in the key, the recall error for that class is just the
number of missing links divided by the number of
correct links, i.e.,
</bodyText>
<equation confidence="0.9787625">
Recall
c(s) •
Recall in turn is c(s)—m(s) which equals
c(s)
(ISI — 1) — (Ip(S)I — 1)
•
IS I — 1
The whole expression can now be simplified to
ISI — IP(S)I
ISI — •
</equation>
<bodyText confidence="0.9760055">
Precision is computed by switching the roles of the
key and response in the above formulation.
</bodyText>
<subsectionHeader confidence="0.8703775">
7.2 Shortcomings of the MUC Scoring
Algorithm
</subsectionHeader>
<bodyText confidence="0.719880888888889">
While the (Vilain 95) provides intuitive results for
coreference scoring, it however does not work as well
in the context of evaluating cross document corefer-
ence. There are two main reasons.
1. The algorithm does not give any credit for sep-
arating out singletons (entities that occur in
chains consisting only of one element, the en-
tity itself) from other chains which have been
identified. This follows from the convention in
</bodyText>
<page confidence="0.998236">
82
</page>
<figureCaption confidence="0.999873">
Figure 8: Response: Example 2
</figureCaption>
<bodyText confidence="0.9833993">
coreference annotation of not identifying those
entities that are markable as possibly coreferent
with other entities in the text. Rather, entities
are only marked as being coreferent if they ac-
tually are coreferent with other entities in the
text. This shortcoming could be easily enough
overcome with different annotation conventions
and with minor changes to the algorithm, but
it is worth noting.
2. All errors are considered to be equal. The MUC
scoring algorithm penalizes the precision num-
bers equally for all types of errors. It is our po-
sition that, for certain tasks, some coreference
errors do more damage than others.
Consider the following examples: suppose the
truth contains two large coreference chains and
one small one (Figure 6), and suppose Figures 7
and 8 show two different responses. We will ex-
plore two different precision errors. The first
error will connect one of the large coreference
chains with the small one (Figure 7). The sec-
ond error occurs when the two large coreference
chains are related by the errant coreferent link
(Figure 8). It is our position that the second er-
ror is more damaging because, compared to the
first error, the second error makes more entities
coreferent that should not be. This distinction
is not reflected in the (Vilain 95) scorer which
scores both responses as having a precision score
of 90% (Figure 9).
</bodyText>
<subsectionHeader confidence="0.997594">
7.3 Our B-CUBED Scoring Algorithm2
</subsectionHeader>
<bodyText confidence="0.9999283">
Imagine a scenario where a user recalls a collection
of articles about John Smith, finds a single arti-
cle about the particular John Smith of interest and
wants to see all the other articles about that indi-
vidual. In commercial systems with News data, pre-
cision is typically the desired goal in such settings.
As a result we wanted to model the accuracy of the
system on a per-document basis and then build a
more global score based on the sum of the user&apos;s
experiences.
</bodyText>
<footnote confidence="0.6702425">
2The main idea of this algorithm was initially put forth by
Alan W. Biermann of Duke University.
</footnote>
<bodyText confidence="0.99934147826087">
Consider the case where the user selects document
6 in Figure 8. This a good outcome with all the
relevant documents being found by the system and
no extraneous documents. If the user selected doc-
ument 1, then there are 5 irrelevant documents in
the systems output — precision is quite low then.
The goal of our scoring algorithm then is to model
the precision and recall on average when looking for
more documents about the same person based on
selecting a single document.
Instead of looking at the links produced by a sys-
tem, our algorithm looks at the presence/absence
of entities from the chains produced. Therefore, we
compute the precision and recall numbers for each
entity in the document. The numbers computed
with respect to each entity in the document are then
combined to produce final precision and recall num-
bers for the entire output.
For an entity, i, we define the precision and recall
with respect to that entity in Figure 10.
The final precision and recall numbers are com-
puted by the following two formulae:
Final Precision = E wi Precisioni
Final Recall = E wi * Recalls
where N is the number of entities in the document,
and wi is the weight assigned to entity i in the doc-
ument. For all the examples and the experiments in
this paper we assign equal weights to each entity i.e.
wi = 1/N. We have also looked at the possibilities
of using other weighting schemes. Further details
about the B-CUBED algorithm including a model
theoretic version of the algorithm can be found in
(Bagga 98a).
Consider the response shown in Figure 7. Using
the B-CUBED algorithm, the precision for entity-6
in the document equals 2/7 because the chain out-
put for the entity contains 7 elements, 2 of which are
correct, namely {6,7}. The recall for entity-6, how-
ever, is 2/2 because the chain output for the entity
has 2 correct elements in it and the &amp;quot;truth&amp;quot; chain for
the entity only contains those 2 elements. Figure 9
shows the final precision and recall numbers com-
puted by the B-CUBED algorithm for the examples
shown in Figures 7 and 8. The figure also shows the
precision and recall numbers for each entity (ordered
by entity-numbers).
</bodyText>
<subsectionHeader confidence="0.996155">
7.4 Overcoming the Shortcomings of the
MUC Algorithm
</subsectionHeader>
<bodyText confidence="0.996146">
The B-CUBED algorithm does overcome the the two
main shortcomings of the MUC scoring algorithm
discussed earlier. It implicitly overcomes the first
</bodyText>
<page confidence="0.998104">
83
</page>
<table confidence="0.999068571428571">
Output MUC Algorithm B-CUBED Algorithm (equal weights for every entity)
Example 1 P. I (90%) P:h*R- -÷-g-A-t-F-i+?-+ # + # + # + # + # + 7J
R: g (100%) R: 12*[t+5+5+1+.2.+1+ii=l00%
P: P-3 (90%) P: h * [1+A1+i+i+A+-&amp;-+-&amp;-+*,+Al= 58%
Example 2
R: g (100%) R:-11-2
-*[1 +g--Ft+t+t+?-±i+- 1 -Ft+ 1 +t+-11= 100%
</table>
<figureCaption confidence="0.99857">
Figure 9: Scores of Both Algorithms on the Examples
</figureCaption>
<bodyText confidence="0.84504">
number of correct elements in the output chain containing entityi
</bodyText>
<equation confidence="0.532445">
Precision; =
</equation>
<bodyText confidence="0.86573575">
number of elements in the output chain containing entity,
number of correct elements in the output chain containing entity,
number of elements in the truth chain containing entity,
Figure 10: Definitions for Precision and Recall for an Entity i
</bodyText>
<equation confidence="0.969171">
Recall; =
</equation>
<bodyText confidence="0.999984">
shortcoming of the MUC-6 algorithm by calculating
the precision and recall numbers for each entity in
the document (irrespective of whether an entity is
part of a coreference chain). Consider the responses
shown in Figures 7 and 8. We had mentioned earlier
that the error of linking the the two large chains in
the second response is more damaging than the error
of linking one of the large chains with the smaller
chain in the first response. Our scoring algorithm
takes this into account and computes a final preci-
sion of 58% and 76% for the two responses respec-
tively. In comparison, the MUC algorithm computes
a precision of 90% for both the responses (Figure 9).
</bodyText>
<sectionHeader confidence="0.999523" genericHeader="evaluation">
8 Results
</sectionHeader>
<bodyText confidence="0.948242029411765">
Figure 11 shows the precision, recall, and F-Measure
(with equal weights for both precision and recall)
using the B-CUBED scoring algorithm. The Vector
Space Model in this case constructed the space of
terms only from the summaries extracted by Sen-
tenceExtractor. In comparison, Figure 12 shows
the results (using the B-CUBED scoring algorithm)
when the vector space model constructed the space
of terms from the articles input to the system (it
still used the summaries when computing the simi-
larity). The importance of using CAMP to extract
summaries is verified by comparing the highest F-
Measures achieved by the system for the two cases.
The highest F-Measure for the former case is 84.6%
while the highest F-Measure for the latter case is
78.0%. In comparison, for this task, named-entity
tools like NetOwl and Textract would mark all the
John Smiths the same. Their performance using our
Threshold
Figure 11: Precision, Recall, and F-Measure Us-
ing the B-CUBED Algorithm With Training On the
Summaries
scoring algorithm is 23% precision, and 100% recall.
Figures 13 and 14 show the precision, recall, and
F-Measure calculated using the MUC scoring algo-
rithm. Also, the baseline case when all the John
Smiths are considered to be the same person achieves
83% precision and 100% recall. The high initial pre-
cision is mainly due to the fact that the MUC algo-
rithm assumes that all errors are equal.
We have also tested our system on other classes of
cross-document coreference like names of companies,
and events. Details about these experiments can be
found in (Bagga 98b).
</bodyText>
<figure confidence="0.944548238095238">
Precision/Recall vs Threshold
Of 0,090,090f f 0
Our Alg: Precision -e-- _
Our Alg: Recall
Our Alg: F-Measure -0-- _
-0 - 0- - - ----
-
-+- -+- -+- -+- -+- -+
0.1 0.2 0.3 OA 0.5 0.6 0.7 0.8 0.9 1
100
90
80
70
60
50
40
30
20
10
84
Precision/Recall vs Threshold
</figure>
<figureCaption confidence="0.758203">
Figure 12: Precision, Recall, and F-Measure Using
the B-CUBED Algorithm With Training On Entire
Articles
</figureCaption>
<figure confidence="0.963919333333333">
Precision/Recall vs Threshold
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Threshold
</figure>
<figureCaption confidence="0.912913666666667">
Figure 13: Precision, Recall, and F-Measure Using
the MUC Algorithm With Training On the Sum-
maries
</figureCaption>
<sectionHeader confidence="0.993026" genericHeader="conclusions">
9 Conclusions
</sectionHeader>
<bodyText confidence="0.9999615">
As a novel research problem, cross document coref-
erence provides an different perspective from re-
lated phenomenon like named entity recognition and
within document coreference. Our system takes
summaries about an entity of interest and uses vari-
ous information retrieval metrics to rank the similar-
ity of the summaries. We found it quite challenging
to arrive at a scoring metric that satisfied our intu-
itions about what was good system output v.s. bad,
but we have developed a scoring algorithm that is an
improvement for this class of data over other within
document coreference scoring algorithms. Our re-
sults are quite encouraging with potential perfor-
mance being as good as 84.6% (F-Measure).
</bodyText>
<figure confidence="0.931797333333333">
Precision/Recall vs Threshold
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Threshold
</figure>
<figureCaption confidence="0.974155333333333">
Figure 14: Precision, Recall, and F-Measure Using
the MUC Algorithm With Training On Entire Arti-
cles
</figureCaption>
<sectionHeader confidence="0.969837" genericHeader="acknowledgments">
10 Acknowledgments
</sectionHeader>
<bodyText confidence="0.99135675">
The first author was supported in part by a Fel-
lowship from IBM Corporation, and in part by the
Institute for Research in Cognitive Science at the
University of Pennsylvania.
</bodyText>
<sectionHeader confidence="0.99527" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9993409">
Bagga, Amit, and Breck Baldwin. Algorithms for
Scoring Coreference Chains. To appear at The
First International Conference on Language Re-
sources and Evaluation Workshop on Linguistics
Coreference, May 1998.
Bagga, Amit, and Breck Baldwin. How Much Pro-
cessing Is Required for Cross-Document Corefer-
ence? To appear at The First International Con-
ference on Language Resources and Evaluation on
Linguistics Coreference, May 1998.
Baldwin, Breck, et al. University of Pennsylva-
nia: Description of the University of Pennsylva-
nia System Used for MUC-6, Proceedings of the
Sixth Message Understanding Conference (MUC-
6), pp. 177-191, November 1995.
Grishman, Ralph. Whither Written Language Eval-
uation?, Proceedings of the Human Language
Technology Workshop, pp. 120-125, March 1994,
San Francisco: Morgan Kaufmann.
Proceedings of the Seventh Message Understanding
Conference (MUC-7), April 1998.
Salton, Gerard. Automatic Text Processing: The
Transformation, Analysis, and Retrieval of In-
formation by Computer, 1989, Reading, MA:
Addison-Wesley.
Vilain, Marc, et al. A Model-Theoretic Coreference
Scoring Scheme, Proceedings of the Sixth Message
Understanding Conference (MUC-6), pp. 45-52,
November 1995, San Francisco: Morgan Kauf-
mann.
</reference>
<figure confidence="0.977237270833333">
900989009
Our Alg: Precision -e-
Our Alg: Recall
Our Alg: F-Measure -9-- _
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Threshold
100
90
80
70
60
50
40 0
30
20
10
0
0
MUC Alg: Precision -e-
MUC Alg: Recall -
MUC Alg: F-Measure -0- ,
--S
-
0.0,090,0,0f
MUC Alg: Precision -e- _
MUC Alg: Recall -4--
MUC Alg: F-Measure -o--
100
90
80
70
60
50
40
30
20
10
100
90
80
70
60
50
40
30
20
10
0 o
</figure>
<page confidence="0.986008">
85
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.209581">
<title confidence="0.99159">Entity-Based Cross-Document Coreferencing Using the Vector</title>
<author confidence="0.512131">Space Model</author>
<affiliation confidence="0.532933">Amit Bagga</affiliation>
<address confidence="0.991881">Box 90129</address>
<affiliation confidence="0.843596666666667">Dept. of Computer Science Duke University Durham, NC 27708-0129</affiliation>
<email confidence="0.998694">amit@cs.duke.edu</email>
<author confidence="0.984253">Breck Baldwin</author>
<affiliation confidence="0.9993015">Institute for Research in Cognitive Sciences University of Pennsylvania</affiliation>
<address confidence="0.9979">3401 Walnut St. 400C Philadelphia, PA 19104</address>
<email confidence="0.99956">breck@unagi.cis.upenn.edu</email>
<abstract confidence="0.9926505">Cross-document coreference occurs when the same person, place, event, or concept is discussed in more than one text source. Computer recognition of this phenomenon is important because it helps break &amp;quot;the document boundary&amp;quot; by allowing a user to examine information about a particular entity from multiple text sources at the same time. In this paper we describe a cross-document coreference resolution algorithm which uses the Vector Space Model to resolve ambiguities between people having the same name. In addition, we also describe a scoring algorithm for evaluating the cross-document coreference chains produced by our system and we compare our algorithm to the scoring algorithm used in the MUC- 6 (within document) coreference task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for Scoring Coreference Chains. To appear at</title>
<date>1998</date>
<booktitle>The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference,</booktitle>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Bagga, Amit, and Breck Baldwin. Algorithms for Scoring Coreference Chains. To appear at The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference, May 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>How Much Processing Is Required for Cross-Document Coreference? To appear at</title>
<date>1998</date>
<booktitle>The First International Conference on Language Resources and Evaluation on Linguistics Coreference,</booktitle>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Bagga, Amit, and Breck Baldwin. How Much Processing Is Required for Cross-Document Coreference? To appear at The First International Conference on Language Resources and Evaluation on Linguistics Coreference, May 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Breck Baldwin</author>
</authors>
<title>University of Pennsylvania: Description of the University of Pennsylvania System Used for MUC-6,</title>
<date>1995</date>
<booktitle>Proceedings of the Sixth Message Understanding Conference (MUC6),</booktitle>
<pages>177--191</pages>
<marker>Baldwin, 1995</marker>
<rawString>Baldwin, Breck, et al. University of Pennsylvania: Description of the University of Pennsylvania System Used for MUC-6, Proceedings of the Sixth Message Understanding Conference (MUC6), pp. 177-191, November 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
</authors>
<title>Whither Written Language Evaluation?,</title>
<date>1994</date>
<booktitle>Proceedings of the Human Language Technology Workshop,</booktitle>
<pages>120--125</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco:</location>
<marker>Grishman, 1994</marker>
<rawString>Grishman, Ralph. Whither Written Language Evaluation?, Proceedings of the Human Language Technology Workshop, pp. 120-125, March 1994, San Francisco: Morgan Kaufmann. Proceedings of the Seventh Message Understanding Conference (MUC-7), April 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
</authors>
<title>Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer,</title>
<date>1989</date>
<publisher>Addison-Wesley.</publisher>
<location>Reading, MA:</location>
<marker>Salton, 1989</marker>
<rawString>Salton, Gerard. Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer, 1989, Reading, MA: Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
</authors>
<title>A Model-Theoretic Coreference Scoring Scheme,</title>
<date>1995</date>
<booktitle>Proceedings of the Sixth Message Understanding Conference (MUC-6),</booktitle>
<pages>45--52</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco:</location>
<marker>Vilain, 1995</marker>
<rawString>Vilain, Marc, et al. A Model-Theoretic Coreference Scoring Scheme, Proceedings of the Sixth Message Understanding Conference (MUC-6), pp. 45-52, November 1995, San Francisco: Morgan Kaufmann.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>