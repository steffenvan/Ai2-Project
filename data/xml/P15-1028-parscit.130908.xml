<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000030">
<title confidence="0.999731">
A Generalisation of Lexical Functions
for Composition in Distributional Semantics
</title>
<author confidence="0.843591">
Antoine Bride Tim Van de Cruys Nicholas Asher
</author>
<affiliation confidence="0.697535">
IRIT &amp; Universit´e de Toulouse IRIT &amp; CNRS, Toulouse IRIT &amp; CNRS, Toulouse
</affiliation>
<email confidence="0.922234">
antoine.bride@irit.fr tim.vandecruys@irit.fr nicholas.asher@irit.fr
</email>
<sectionHeader confidence="0.993033" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999964304347826">
Over the last two decades, numerous algo-
rithms have been developed that success-
fully capture something of the semantics
of single words by looking at their distri-
bution in text and comparing these distri-
butions in a vector space model. How-
ever, it is not straightforward to construct
meaning representations beyond the level
of individual words – i.e. the combina-
tion of words into larger units – using dis-
tributional methods. Our contribution is
twofold. First of all, we carry out a large-
scale evaluation, comparing different com-
position methods within the distributional
framework for the cases of both adjective-
noun and noun-noun composition, making
use of a newly developed dataset. Sec-
ondly, we propose a novel method for
composition, which generalises the ap-
proach by Baroni and Zamparelli (2010).
The performance of our novel method is
also evaluated on our new dataset and
proves competitive with the best methods.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999969769230769">
In the course of the last two decades, there has
been a growing interest in distributional meth-
ods for lexical semantics (Landauer and Dumais,
1997; Lin, 1998; Turney and Pantel, 2010). These
methods are based on the distributional hypothe-
sis (Harris, 1954), according to which words that
appear in the same contexts tend to be similar in
meaning. Inspired by Harris’ hypothesis, numer-
ous researchers have developed algorithms that try
to capture the semantics of individual words by
looking at their distribution in a large corpus.
Compared to manual studies common to formal
semantics, distributional semantics offers substan-
tially larger coverage since it is able to analyze
massive amounts of empirical data. However, it is
not trivial to combine the algebraic objects created
by distributional semantics to get a sensible distri-
butional representation for more complex expres-
sions, consisting of several words. On the other
hand, the formalism of the λ-calculus provides us
with general, advanced and efficient methods for
composition that can model meaning composition
not only of simple phrases, but also more com-
plex phenomena such as coercion or composition
with fine-grained types (Asher, 2011; Luo, 2010;
Bassac et al., 2010). Despite continued efforts to
find a general method for composition and various
approaches for the composition of specific syntac-
tic structures (e.g. adjective-noun composition, or
the composition of transitive verbs and direct ob-
jects (Mitchell and Lapata, 2008; Coecke et al.,
2010; Baroni and Zamparelli, 2010)), the model-
ing of compositionality is still an important chal-
lenge for distributional semantics. Moreover, the
validation of proposed methods for composition
has used relatively small datasets of human sim-
ilarity judgements (Mitchell and Lapata, 2008).1
Although such studies comparing similarity judge-
ments have their merits, it would be interesting to
have studies that evaluate methods for composi-
tion on a larger scale, using a larger test set of dif-
ferent specific compositions. Such an evaluation
would allow us to evaluate more thoroughly the
different methods of composition that have been
proposed. This is one of the goals of this paper.
To achieve this goal, we make use of two dif-
ferent resources. We have constructed a dataset
for French containing a large number of pairs
of a compositional expression (adjective-noun)
and a single noun that is semantically close or
identical to the composed expression. These
pairs have been extracted semi-automatically from
</bodyText>
<footnote confidence="0.988160666666667">
1A notable exception is (Marelli et al., 2014), who pro-
pose a large-scale evaluation dataset for composition at the
sentence level.
</footnote>
<page confidence="0.911428">
281
</page>
<note confidence="0.975588">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 281–291,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.998670975609757">
the French Wiktionary. We have also used
the Semeval 2013 dataset of phrasal similarity
judgements for English with similar pairs ex-
tracted semi-automatically from the English Wik-
tionary to construct a dataset for English for both
adjective-noun and noun-noun composition. This
affords us a cross-linguistic comparison of the
methods.
These data sets provide a substantial evalua-
tion of the performance of different compositional
methods. We have tested three different methods
of composition proposed in the literature, viz. the
additive and multiplicative model (Mitchell and
Lapata, 2008), as well as the lexical function ap-
proach (Baroni and Zamparelli, 2010).
The two first methods are entirely general, and
take as input automatically constructed vectors for
adjectives and nouns. The method by Baroni and
Zamparelli, on the other hand, requires the acqui-
sition of a particular function for each adjective,
represented by a matrix. The second goal of our
paper is to generalise the functional approach in
order to eliminate the need for an individual func-
tion for each adjective. To this goal, we automat-
ically learn a generalised lexical function, based
on Baroni and Zamparelli’s approach. This gener-
alised function combines with an adjective vector
and a noun vector in a generalised way. The per-
formance of our novel generalised lexical function
approach is evaluated on our test sets and proves
competitive with the best, extant methods.
Our paper is organized as follows. First, we dis-
cuss the different compositional models that we
have evaluated in our study, briefly revisiting the
different existing methods for composition, fol-
lowed by a description of our generalisation of the
lexical function approach. Next, we report on our
evaluation method and its results. The results sec-
tion is followed by a section that discusses work
related to ours. Lastly, we draw conclusions and
lay out some avenues for future work.
</bodyText>
<sectionHeader confidence="0.999877" genericHeader="method">
2 Composition methods
</sectionHeader>
<subsectionHeader confidence="0.998837">
2.1 Simple Models of Composition
</subsectionHeader>
<bodyText confidence="0.985352043478261">
In this section, we describe the composition mod-
els for the adjective-noun case. The extension of
these models to the noun-noun case is straight-
forward; one just needs to replace the adjective
by the subordinate noun. Admittedly, choosing
which noun is subordinate in noun-noun compo-
sition may be an interesting problem but it is out-
side the scope of this paper. We tested three sim-
ple models of composition: a baseline method that
discounts the contribution of the adjective com-
pletely, and the additive and multiplicative models
of composition. The baseline method is defined as
follows:
Compbaseline(adj, noun) = noun
The additive model adds the point-wise values
of the adjective vector adj and noun vector noun
using independent coefficients to provide a result
for the composition:
Compadditive(adj, noun) = α noun + b adj
The multiplicative model consists in a point-
wise multiplication of the vectors adj and noun:
Compmultiplicative(adj, noun) = noun ® adj
with (noun ®adj)i = nouni x adji
</bodyText>
<subsectionHeader confidence="0.998813">
2.2 The lexical function model
</subsectionHeader>
<bodyText confidence="0.999933857142857">
Baroni and Zamparelli’s (2010) lexical func-
tion model (LF) is somewhat more complex.
Adjective-noun composition is modeled as the
functional application of an adjective meaning
(represented as a matrix) to a noun meaning (rep-
resented as a vector). Thus, the combination of
an adjective and noun is the product of the matrix
ADJ and the vector noun as shown in Figure 1.
Baroni and Zamparelli propose learning an ad-
jective’s matrix from examples of the vectors
for adj noun obtained directly from the corpus.
These vectors adj noun are obtained in the same
way as vectors representing a single word: when
the adjective-noun combination occurs, we ob-
serve its context and construct the vector from
those observations. As an illustration, consider
the example in 2. The word name appears three
times modified by an adjective in the following
excerpt from Oscar Wilde’s The Importance of
Being Earnest. This informs us about the co-
occurrence frequencies of three vectors: one for
divine name, another for nice name, and one for
charming name.
Once the adj noun vectors have been created
for a given adjective, we are able to calculate the
ADJ matrix using a least squares regression that
minimizes the equation ADJxadj noun − noun.
More formally, the problem is the following:
</bodyText>
<equation confidence="0.438747333333333">
Find ADJ s.t.
∑noun(ADJ x noun−adj noun)2
is minimal
</equation>
<page confidence="0.988567">
282
</page>
<figure confidence="0.997402">
CompositionLF(adjective, noun) ADJECTIVE n
o
×
u
n
</figure>
<figureCaption confidence="0.733860571428571">
Figure 1: Lexical Function Composition
Jack: Personally, darling, to speak quite candidly, I don’t much care about the name of Ernest ... I don’t think the
name suits me at all.
Gwendolen: It suits you perfectly. It is a divine [name]. It has a music of its own. It produces vibrations.
Jack: Well, really, Gwendolen, I must say that I think there are lots of other much nicer [names]. I think Jack, for
instance, is a charming [name].
Figure 2: Excerpt from Oscar Wilde’s The Importance of Being Earnest
</figureCaption>
<bodyText confidence="0.99981292">
For our example, we would minimize, among oth-
ers DIVINE×divine name − name to get the ma-
trix for DIVINE.
LF requires a large corpus, because we have
to observe a sufficient number of examples of the
adjective and noun combined, which are perforce
less exemplified than the presence of the noun or
adjective in isolation. In Figure 2, each of the oc-
currences of ‘name’ can contribute to the informa-
tion in the vector name but none can contribute to
the vector evanescent name.
Baroni and Zamparelli (2010) offer an expla-
nation of how to cope with the potential sparse
data problem for learning matrices for adjectives.
Moreover, recent evaluations of LF show that ex-
istent corpora have enough data for it to provide a
semantics for the most frequent adjectives and ob-
tain better results than other methods (Dinu et al.,
2013b).
Nevertheless, LF has limitations in treating rel-
atively rare adjectives. For example, the adjective
‘evanescent’ appears 359 times in the UKWaC cor-
pus (Baroni et al., 2009). This is enough to gen-
erate a vector for evanescent, but may not be suf-
ficient to generate a sufficient number of vectors
evanescent noun to build the matrix EVANES-
CENT. More importantly, for noun-noun combi-
nations, one may need to have a LF for a com-
bination. To get the meaning of blood dona-
tion campaign in the LF approach, the matrix
BLOOD DONATION must be combined to the vec-
tor campaign. Learning this matrix would require
to build vectors blood donation noun for many
nouns. Even if it were possible, the issue would
arise again for blood donation campaign plan,
then for blood donation campaign plan meeting
and so forth.
In addition, LF’s approach to adjectival mean-
ing and composition has a theoretical drawback.
Like Montague Grammar, it supposes that the ef-
fect of an adjective on a noun meaning is specific
to the adjective (Kamp, 1975). However, recent
studies suggest that the Montague approach over-
generalises from the worst case, and that the vast
majority of adjectives in the world’s languages
are subsective, suggesting that the modification of
nominal meaning that results from their compo-
sition with a noun follows general principles (Par-
tee, 2010; Asher, 2011) that are independent of the
presence or absence of examples of association.
</bodyText>
<subsectionHeader confidence="0.999309">
2.3 Generalised LF
</subsectionHeader>
<bodyText confidence="0.999932631578948">
To solve these problems, we generalise LF and re-
place individual matrices for adjectival meanings
by a single lexical function: a tensor for adjectival
composition �9/.2 Our proposal is that adjective-
noun composition is carried out by multiplying the
tensor 9/ with the vector for the adjective adj, fol-
lowed by a multiplication with the vector noun,
c.f. Figure 3.
The product of the tensor 9/ and the vector adj
yields a matrix dependent of the adjective that is
multiplied with the vector noun. This matrix cor-
responds to the LF matrix ADJ. As indicated in
Figure 4, we obtain 9/ with the help of matrices
obtained from the LF approach, and from vectors
for single words easily obtained in distributional
semantics; we perform a least square regression
minimizing the norm of the matrices generated by
the equations in Figure 4. Formally, the problem
is
</bodyText>
<footnote confidence="0.716857333333333">
2A tensor generalises a matrix to several dimensions. We
use a tensor in three modes. For an introduction to tensors,
see (Kolda and Bader, 2009).
</footnote>
<page confidence="0.995083">
283
</page>
<figure confidence="0.997915272727273">
x
a
dj
n
o
u
n
=
A djective
)x
V adjective, noun CompositionGLF(adjective, noun)
</figure>
<figureCaption confidence="0.999239">
Figure 3: Composition in the generalised lexical function model
</figureCaption>
<subsectionHeader confidence="0.355698">
Find A s.t.
</subsectionHeader>
<bodyText confidence="0.972605192307693">
∑adj(A x adj − ADJ)2
is minimal
Note that our tensor is not just the compilation of
the information found in the LF matrices: the ad-
jective mode of our tensor has a limited number
of dimensions, whereas the LF approach creates a
separate matrix for each individual adjective. This
reduction forces the model to generalise, and we
hypothesise that this generalisation allows us to
make proper noun modifications even in the light
of sparse data.
Our approach requires learning a significant
number of matrices ADJ. This is not a problem,
since FRWaC and UKWaC provide sufficient data
for the LF approach to generate matrices for a sig-
nificant number of adjectives. For example, the
2000th most frequent adjective in FRWaC (‘fas-
ciste’) has more than 4000 occurrences.
To return to our example of blood donation
campaign, once the tensor N for noun-noun
composition is learned, our approach requires
only the knowledge of the vectors blood, dona-
tion and campaign. We would then perform the
following computations:
blood donation = (N x blood) x donation
blood donation campaign =
</bodyText>
<equation confidence="0.791888">
(N x blood donation) x campaign
</equation>
<bodyText confidence="0.991863125">
and this allows us to avoid the sparse data prob-
lem for the LF approach in generating the matrix
BLOOD DONATION.
Once we have obtained the tensor A , we verify
experimentally its relevance to composition, in or-
der to check whether a tensor optimising the equa-
tions in Figure 4 would be semantically interest-
ing.
</bodyText>
<sectionHeader confidence="0.999662" genericHeader="method">
3 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999867">
3.1 Tasks description
</subsectionHeader>
<bodyText confidence="0.991913255813954">
In order to evaluate the different composition
methods, we constructed test sets for French and
English, inspired by the work of Zanzotto et al.
(2010) and the SEMEVAL-2013 task evaluating
phrasal semantics (Korkontzelos et al., 2013). The
task is to make a judgement about the semantic
similarity of a short word sequence (an adjective-
noun combination) and a single noun. This is im-
portant, as composition models need to be able to
treat word sequences of arbitrary length. Formally,
the task is presented as:
With comp = composition(adj, noun1)
Evaluate similarity(comp, noun2)
where the ‘composition’ function is carried out
by the different composition models. ‘Similarity’
needs to be a binary function, with return val-
ues ‘similar’ and ‘non-similar’. Note, however,
that the distributional approach yields a continu-
ous similarity value (such as the cosine similar-
ity between two vectors). In order to determine
which cosine values correspond to ‘similar’ and
which cosine values correspond to ‘non-similar’,
we looked at a number of examples from a de-
velopment set. More precisely, we carried out a
logistic regression on 50 positive and 50 negative
examples (separate from our test set) in order to
automatically learn the threshold at which a pair
is considered to be similar. Finally, we decided to
use balanced test sets containing as many positive
instances as negative ones.
The test set is constructed in a semi-automatic
way, making use of the canonical phrasing of dic-
tionary definitions. Take for example the defini-
tion of bassoon in the English Wiktionary3, pre-
sented in Figure 5. It is quite straightforward
to extract the pair (musical instrument,bassoon)
from this definition. Using a large dictionary
(such as Wiktionary), it is then possible to ex-
tract a large number of positive – i.e. similar –
(adjective noun,noun) pairs.
For the construction of our test set for French,
we downloaded all entries of the French Wik-
tionary (Wiktionnaire) and annotated them with
</bodyText>
<footnote confidence="0.9757105">
3http://en.wiktionary.org/wiki/bassoon, ac-
cessed on 26 February 2015.
</footnote>
<page confidence="0.993934">
284
</page>
<figure confidence="0.988612916666667">
Find tensor 9/ by minimizing:
.9/djective
X r −
e
d
RED ,
. 9/djective
X s − SLOW
l
o
w
. . .
</figure>
<figureCaption confidence="0.994362">
Figure 4: Learning the . 9/djective tensor
</figureCaption>
<figure confidence="0.909295333333333">
bassoon /ba&amp;quot;su:n/ (plural bassoons)
1. A musical instrument in the woodwind family, having a double reed and, playing in the tenor and
bass ranges.
</figure>
<figureCaption confidence="0.999938">
Figure 5: Definition of bassoon, extracted from the English Wiktionary
</figureCaption>
<bodyText confidence="0.98784252173913">
part of speech tags, using the French part of speech
tagger MElt (Denis et al., 2010). Next, we ex-
tracted all definitions that start with an adjective-
noun combination. As a final step, we filtered all
instances containing words that appear too infre-
quently in our FRWaC corpus.4
The automatically extracted instances were then
checked manually, and all instances that were con-
sidered incorrect were rejected. This gave us a fi-
nal test set of 714 positive examples.
We also created an initial set of negative ex-
amples, where we combined an existing combi-
nation of adjective noun1 (extracted from the
French Wiktionary), with a randomly selected
noun noun2. Again, we verified manually that the
resulting (adjective noun1, noun2) pairs con-
stituted actual negative examples. We then cre-
ated a second set of negative examples by ran-
domly selecting two nouns (noun1,noun2) and
one adjective adjective. The resulting pairs
(adjective noun1, noun2) were verified man-
ually.
In addition to our new test set for French, we
also experimented with the original test set of the
SEMEVAL-2013 task evaluation phrasal semantics
for English. However, the original test set lacked
human oversight as ‘manly behavior’ was consid-
ered similar to ‘testosterone’ for example. We thus
hand-checked the test set ourselves and extracted
652 positive pairs.
The negative pairs from the original SEMEVAL-
2013 are a combination of a random noun and a
4i.e. less than 200 times for adjectives and less than 1500
times for nouns
random adjective-noun compositon found in the
English Wiktionary. We used it as our first set
of English negative examples as it is similar in
construction to our first set of negative examples
in French. In addition, we created a completely
random negative test set for English in the same
fashion we did for the second negative test set for
French.
Finally, the original test set also contains noun-
noun compounds so we also created a test set for
that. This gave us 226 positive and negative pairs
for the noun-noun composition.
</bodyText>
<subsectionHeader confidence="0.999695">
3.2 Semantic space construction
</subsectionHeader>
<bodyText confidence="0.999962055555556">
In this section, we describe the construction of our
semantic space. Our semantic space for French
was built using the FRWaC corpus (Baroni et al.,
2009) – about 1,6 billion words of web texts –
which has been tagged with MElt tagger (Denis et
al., 2010) and parsed with MaltParser (Nivre et al.,
2006a), trained on a dependency-based version of
the French treebank (Candito et al., 2010). Our
semantic space for English has been built using
the UKWaC corpus (Baroni et al., 2009), which
consists of about 2 billion words extracted from
the web. The corpus has been part of speech
tagged and lemmatized with Stanford Part-Of-
Speech Tagger (Toutanova and Manning, 2000;
Toutanova et al., 2003), and parsed with Malt-
Parser (Nivre et al., 2006b) trained on sections
2-21 of the Wall Street Journal section of the
Penn Treebank extended with about 4000 ques-
</bodyText>
<page confidence="0.99197">
285
</page>
<table confidence="0.845498285714286">
positive examples random negative examples Wiktionary-based negative examples
(mot court, abr´eviation) (importance fortuit, gamme) (jugement favorable, discorde)
‘short word’, ‘abbreviation’ ‘accidental importance’, ‘range’ ‘favorable judgement’, ‘discord’
(ouvrage litt´eraire, essai) (penchant autoritaire, ile) (circonscription administratif, fumier)
‘literary work’, ‘essay’ ‘authoritarian slope’, isle’ ‘administrative district’, ‘manure’
(compagnie honorifique, ordre) (auspice aviaire, ponton) (mention honorable, renne)
‘honorary company’, ‘order’ ‘avian omen’, ‘pontoon’ ‘honorable mention’, ‘reindeer’
</table>
<tableCaption confidence="0.99565">
Table 1: A number of examples from our test set for French
</tableCaption>
<bodyText confidence="0.997233291666667">
tions from the QuestionBank5.
For both corpora, we extracted the lemmas of
all nouns, adjectives and (bag of words) context
words. We only kept those lemmas that consist of
alphabetic characters.6 We then selected the 10K
most frequent lemmas for each category (nouns,
adjectives, context words), making sure to include
all the words from the test set. As a final step,
we created our semantic space vectors using ad-
jectives and nouns as instances, and bag of words
context words as features. The resulting vectors
were weighted using positive point-wise mutual
information (ppmi, (Church and Hanks, 1990)),
and all vectors were normalized to unit length.
We then compared the different composition
methods on different versions of the same seman-
tic space (both for French and English): the full
semantic space, a reduced version of the space to
300 dimensions using singular value decomposi-
tion (svd, (Golub and Van Loan, 1996)), and a re-
duced version of the space to 300 dimensions us-
ing non-negative matrix factorization (nmf, (Lee
and Seung, 2000)). We did so in order to test each
method in its optimal conditions. In fact:
</bodyText>
<listItem confidence="0.931741">
• A non-reduced space contains more informa-
</listItem>
<bodyText confidence="0.7547255">
tion. This might be beneficial for methods
that are able to take advantage of the full se-
mantic space (viz. the additive et multiplica-
tive model). On the other hand, to be able
to use the non-reduced space for the lexical
function approach, one would have to learn
matrices of size 10K x10K for each adjec-
tive. This would be problematic in terms of
computing time and data sparseness, as we
previously noted. The same goes for our gen-
</bodyText>
<footnote confidence="0.9919036">
5http://maltparser.org/mco/english_parser/
engmalt.html
6This step generally filters out dates, numbers and punc-
tuation, which have little interest for the distributional ap-
proach.
</footnote>
<bodyText confidence="0.331487">
eralised approach.
</bodyText>
<listItem confidence="0.945256125">
• Previous research has indicated that the lexi-
cal function approach is able to achieve bet-
ter results using a reduced space with svd. On
the other hand, the negative values that result
from svd are detrimental for the multiplica-
tive approach.
• An nmf-reduced semantic space is not detri-
mental for the multiplicative approach.
</listItem>
<bodyText confidence="0.997815">
In order to determine the best parameters for the
additive model, we tested this model for different
values of a and P where a +P = 17 on a develop-
ment set and kept the values with the best results:
a = 0.4, P = 0.6.
</bodyText>
<subsectionHeader confidence="0.998768">
3.3 Data used for regression
</subsectionHeader>
<bodyText confidence="0.999980285714286">
The LF approach and its generalisation need data
in order to perform the least square regression. We
thus created a semantic space for adjective noun
and noun noun vectors using the most frequent
ones in a similar way to how we created them
in 3.2. Then we solved the equations in 2.2 and
forth. Even though the regression data were dis-
joint from the test sets, for each pair, we removed
some of the data that may cause overfitting.
For the lexical function tests, we remove the
adjective noun vector corresponding to the test
pair from the regression data. For example, we
do not use short word to learn SHORT for the
(short word, abbrevation) pair.
For the generalised lexical function tests, we use
the full regression data to learn the lexical func-
tions used to train the tensor. However, we re-
move the ADJECTIVE matrix corresponding to the
test pair from the (tensor) regression data. For ex-
ample, we do not use SHORT to learn 9/ for the
(short word, abbreviation) pair.
</bodyText>
<footnote confidence="0.9838525">
7Since the vectors are normalized (cf. 3.2), this condition
does not affect the generality of our test.
</footnote>
<page confidence="0.99836">
286
</page>
<tableCaption confidence="0.9732135">
Table 2: Percentage of correctly classified pairs for (adjective noun1,noun2) for both French and English
spaces.
</tableCaption>
<table confidence="0.990213333333333">
baseline multiplicative additive LF generalised LF
fr en fr en fr en fr en fr en
non-reduced 0.83 0.81 0.86 0.86 0.88 0.86 N/A N/A
svd 0.79 0.79 0.55 0.59 0.84 0.78 0.93 0.92 0.91 0.88
nmf 0.78 0.78 0.83 0.77 0.79 0.84 0.90 0.86 0.88 0.85
(a) Negative examples are created randomly.
baseline multiplicative additive LF generalised LF
fr en fr en fr en fr en fr en
non-reduced 0.80 0.79 0.83 0.81 0.85 0.80 N/A N/A
svd 0.78 0.77 0.54 0.48 0.83 0.78 0.84 0.79 0.81 0.77
nmf 0.78 0.78 0.79 0.78 0.83 0.82 0.82 0.82 0.81 0.80
(b) Negative examples are created from existing pairs.
</table>
<tableCaption confidence="0.9610825">
Table 3: Percentage of correctly classified pairs for (noun2 noun1,noun3) with negative examples from
existing pairs. Only the English space is tested.
</tableCaption>
<bodyText confidence="0.70246325">
English space baseline multiplicative additive LF generalised LF
non-reduced 0.77 0.80 0.84 N/A N/A
svd 0.78 0.49 0.86 0.83 0.82
nmf 0.79 0.82 0.86 0.85 0.83
</bodyText>
<sectionHeader confidence="0.68152" genericHeader="method">
3.4 Results
</sectionHeader>
<bodyText confidence="0.9998375">
In this section, we present how the various models
perform on our test sets.
</bodyText>
<subsectionHeader confidence="0.823353">
3.4.1 General results
</subsectionHeader>
<bodyText confidence="0.999942">
Tables 2 &amp; 3 give an overview of the results. Note
first that the baseline approach, which compares
only the two nouns and ignores the subordinate
adjective or noun, does relatively well on the task
(∼ 80% accuracy). This reflects the fact that the
head noun in our pairs extracted from definitions
is close to (and usually a super type of) the noun
to be defined.
In addition, we observe that the multiplicative
method performs badly, as expected, on the se-
mantic space reduced with svd. This confirms the
incompatibility of this method with the negative
values generated by svd. Indeed, multiplying two
vectors with negative values term by term may
yield a third vector very far away from the other
two. Such a combination does not support the sub-
sectivity of most our test pairs. Apart from that,
svd and nmf reductions do not affect the methods
much.
Moreover, we observe that the multiplicative
model performs better than the baseline but is
bested by the additive model. We also see that
additive and lexical functions often yield similar
performance.
Finally, the generalised lexical function is
slightly less accurate than the lexical functions.
This is an expected consequence of generalisa-
tion. Nevertheless, the generalised lexical function
yields sound results confirming our intuition that
we can represent adjective-noun (or noun-noun)
combinations by one function.
</bodyText>
<subsectionHeader confidence="0.982938">
3.4.2 Adjective-noun
</subsectionHeader>
<bodyText confidence="0.9998707">
With random negative pairs (Table 2a), we ob-
serve that the lexical function model obtains the
best results for the svd space. This result is sig-
nificantly better than any other method on any
of the spaces—e.g.,for French space, χ2 = 33.49,
p &lt; 0.01 when compared to the additive model for
the non-reduced space which performs second.
However, with non-random negative pairs (Ta-
ble 2b), LF and the additive model obtain scores
that are globally equivalent for their best respec-
</bodyText>
<page confidence="0.990555">
287
</page>
<bodyText confidence="0.999872368421053">
tive conditions — in French 0.85 for the additive
non-reduced model vs. 0.84 for the LF svd model,
a difference that is not significant (χ2 = 0.20,
p &lt; 0.05).
This seems to indicate that LF is especially ef-
ficient at separating out nonsense combinations.
This may be caused by the fact that lexical func-
tions learn from actual pairs. Thus, when an
adjective noun combination is bizarre, the ADJEC-
TIVE matrix has not been optimized to interact
with the noun vector and may lead to complete
non-sense — Which is a good thing because hu-
mans would analyze the combination as such.
Finally, similar results in French and English
confirm the intuition that distributional methods
(and its composition models) are independent of
the idiosyncrasies of a particular language; in par-
ticular they are as efficient for French as for En-
glish.
</bodyText>
<subsectionHeader confidence="0.909082">
3.4.3 Noun-noun
</subsectionHeader>
<bodyText confidence="0.999967">
The noun-noun tests (Table 3) yields similar re-
sults to the adjective-noun tests. This is not so
surprising since noun noun compounds in English
also obey a roughly subsective property: a base-
ball field is still a field (though a cricket pitch is
perhaps not so obviously a pitch). We can see that
the accuracy increase from the baseline is higher
compared to adjective-noun test on the same exact
spaces (Table 2b, right values). This may be due
to the fact that the subordinate noun in noun-noun
combinations is more important than the adjective
subordinate in adjective-noun combination.
</bodyText>
<sectionHeader confidence="0.999875" genericHeader="method">
4 Related work
</sectionHeader>
<bodyText confidence="0.999946537313433">
Many researchers have already studied and evalu-
ated different composition models within a distri-
butional approach. One of the first studies eval-
uating compositional phenomena in a systematic
way is Mitchell and Lapata’s (2008) approach.
They explore a number of different models for
vector composition, of which vector addition (the
sum of each feature) and vector multiplication (the
element-wise multiplication of each feature) are
the most important. They evaluate their models
on a noun-verb phrase similarity task. Human an-
notators were asked to judge the similarity of two
composed pairs (by attributing a certain score).
The model’s task is then to reproduce the human
judgements. Their results show that the multi-
plicative model yields the best results, along with
a weighted combination of the additive and multi-
plicative model. The authors redid their study us-
ing a larger test set in Mitchell and Lapata (2010)
(adjective-noun composition was also included),
and they confirmed their initial results.
Baroni and Zamparelli (2010) evaluate their
lexical function model within a somewhat dif-
ferent context. They evaluated their model
by looking at its capacity of reconstructing the
adjective noun vectors that have not been seen
during training. Their results show that their lexi-
cal function model obtains the best results for the
reconstruction of the original co-occurrence vec-
tors, followed by the additive model. We observe
the same tendency in our evaluation results for
French, although our results for English show a
different picture. We would like to explore this
discordance further in future work.
Grefenstette et al. (2013) equally propose a gen-
eralisation of the lexical function model that uses
tensors. Their goal is to model transitive verbs,
and the way we acquire our tensor is similar to
theirs. In fact, they use the LF approach in or-
der to learn VERB OBJECT matrices that may
be multiplied by a subject vector to obtain the
subject verb object vector. In a second step, they
learn a tensor for each individual verb, which is
similar to how we learn our adjective tensor 9 /.
Coecke et al. (2010) present an abstract theo-
retical framework in which a sentence vector is a
function of the Kronecker product of its word vec-
tors, which allows for greater interaction between
the different word features. A number of instan-
tiations of the framework – where the key idea
is that relational words (e.g. adjectives or verbs)
have a rich (multi-dimensional) structure that acts
as a filter on their arguments – are tested exper-
imentally in Grefenstette and Sadrzadeh (2011a)
and Grefenstette and Sadrzadeh (2011b). The au-
thors evaluated their models using a similarity task
that is similar to the one used by Mitchell &amp; La-
pata. However, they use more complex compo-
sitional expressions: rather than using composi-
tions of two words (such as a verb and an object),
they use simple transitive phrases (subject-verb-
object). They show that their instantiations of the
categorical model reach better results than the ad-
ditive and multiplicative models on their transitive
similarity task.
Socher et al. (2012) present a compositional
model based on a recursive neural network. Each
</bodyText>
<page confidence="0.993691">
288
</page>
<bodyText confidence="0.99994356">
node in a syntactic tree is assigned both a vector
and a matrix; the vector captures the actual mean-
ing of the constituent, while the matrix models
the way it changes the meaning of neighbouring
words and phrases. They use an extrinsic evalu-
ation, using the model for a sentiment prediction
task. They show that their model gets better re-
sults than the additive, multiplicative, and lexical
function approach. Other researchers, however,
have published different results. Blacoe and La-
pata (2012) evaluated the additive and multiplica-
tive model, as well as Socher et al.’s (2012) ap-
proach on two different tasks: Mitchell &amp; Lapata’s
(2010) similarity task and a paraphrase detection
task. They find that the additive and multiplica-
tive models reach better scores than Socher et al.’s
model.
Tensors have been used before to model differ-
ent aspects of natural language. Giesbrecht (2010)
describes a tensor factorization model for the con-
struction of a distributional model that is sensitive
to word order. And Van de Cruys (2010) uses a
tensor factorization model in order to construct a
three-way selectional preference model of verbs,
subjects, and objects.
</bodyText>
<sectionHeader confidence="0.998779" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999996416666667">
We have developed a new method of composition
and tested it in comparison with different com-
position methods assuming a distributional ap-
proach. We developed a test set for French pair-
ing nouns with adjective noun combinations very
similar in meaning from the French Wiktionary.
We also used an existing SEMEVAL-2013 set to
create a similar test set for English both for ad-
jective noun combination and noun noun combi-
nation. Our tests confirm that the lexical func-
tion approach by Baroni and Zamparelli performs
well compared to other methods of composition,
but only when the negative examples are con-
structed randomly. Our generalised lexical func-
tion approach fares almost equally well. It also
has the advantage of being constructed from au-
tomatically acquired adjectival and noun vectors,
and offers the additional advantage of countering
data sparseness. However, the lexical function
approach claims to perform well on more subtle
cases — e.g. non-subsective combinations such
as stone lion. Our test sets does not contain such
cases, and so we cannot draw any conclusion on
this claim.
In future work, we would like to test differ-
ent sizes of dimensionality reduction, in order to
optimize our generalised lexical function model.
Moreover, it is possible that better results may be
obtained by proposing multiple generalised lexi-
cal functions, rather than a single one. We could,
e.g., try to separate the intersective adjectives from
non-intersective adjectives. And finally, we would
like to further explore the performance of the lex-
ical function model and generalised lexical func-
tion model on different datasets, which involve
more complex compositional phenomena.
</bodyText>
<sectionHeader confidence="0.998948" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999671">
We thank Dinu et al. (2013a) for their work on the
DisSeCT toolkit8, which provides plenty of help-
ful functions for composition in distributional se-
mantics. We also thank the OSIRIM platform9 for
allowing us to do the computations we needed. Fi-
nally, we thank the reviewers of this paper for their
insightful comments.
This work is supported by a grant overseen
by the French National Research Agency ANR
(ANR-14-CE24-0014).
</bodyText>
<sectionHeader confidence="0.997179" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99851788">
Nicholas Asher. 2011. Lexical Meaning in Context: A
Web of Words. Cambridge University Press.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183–1193, Cambridge, MA, October. Association
for Computational Linguistics.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The wacky wide web:
A collection of very large linguistically processed
web-crawled corpora. Language Resources and
Evaluation, 43(3):209–226.
Christian Bassac, Bruno Mery, and Christian Retor´e.
2010. Towards a Type-theoretical account of lexical
semantics. Journal of Logic, Language and Infor-
mation, 19(2):229–245.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 546–556, Jeju Island, Korea,
July. Association for Computational Linguistics.
</reference>
<footnote confidence="0.999589">
8http://clic.cimec.unitn.it/composes/toolkit/
9http://osirim.irit.fr/site/en
</footnote>
<page confidence="0.995309">
289
</page>
<reference confidence="0.999718189189189">
Marie Candito, Benoit Crabb´e, Pascal Denis, et al.
2010. Statistical french dependency parsing: tree-
bank conversion and first results. In Proceed-
ings of the Seventh International Conference on
Language Resources and Evaluation (LREC 2010),
pages 1840–1847.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information &amp; lexicogra-
phy. Computational Linguistics, 16(1):22–29.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributed model of meaning. Lambek
Festschrift, Linguistic Analysis, vol. 36, 36.
Pascal Denis, Benoit Sagot, et al. 2010. Exploita-
tion d’une ressource lexicale pour la construction
d’un ´etiqueteur morphosyntaxique ´etat-de-l’art du
franc¸ais. In Traitement Automatique des Langues
Naturelles: TALN 2010.
Georgiana Dinu, Nghia The Pham, and Marco Ba-
roni. 2013a. Dissect - distributional semantics com-
position toolkit. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics: System Demonstrations, pages 31–36,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013b. General estimation and evaluation of compo-
sitional distributional semantic models. In Proceed-
ings of the Workshop on Continuous Vector Space
Models and their Compositionality, pages 50–58,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Eugenie Giesbrecht. 2010. Towards a matrix-based
distributional model of meaning. In Proceedings
of the NAACL HLT 2010 Student Research Work-
shop, pages 23–28. Association for Computational
Linguistics.
Gene H. Golub and Charles F. Van Loan. 1996. Matrix
Computations (3rd Ed.). Johns Hopkins University
Press, Baltimore, MD, USA.
Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011a. Experimental support for a categorical com-
positional distributional model of meaning. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1394–
1404, Edinburgh, Scotland, UK., July. Association
for Computational Linguistics.
Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011b. Experimenting with transitive verbs in a dis-
cocat. In Proceedings of the GEMS 2011 Workshop
on GEometrical Models of Natural Language Se-
mantics, pages 62–66, Edinburgh, UK, July. Asso-
ciation for Computational Linguistics.
E. Grefenstette, G. Dinu, Y.-Z. Zhang, M. Sadrzadeh,
and Baroni M. 2013. Multi-step regression learn-
ing for compositional distributional semantics. In
Proceedings of the 10th International Conference on
Computational Semantics (IWCS), pages 131–142,
East Stroudsburg PA. Association for Computational
Linguistics.
Zellig S. Harris. 1954. Distributional structure. Word,
10(23):146–162.
Hans Kamp. 1975. Two theories about adjectives.
Formal semantics of natural language, pages 123–
155.
Tamara G. Kolda and Brett W. Bader. 2009. Ten-
sor decompositions and applications. SIAM Review,
51(3):455–500, September.
Ioannis Korkontzelos, Torsten Zesch, Fabio Massimo
Zanzotto, and Chris Biemann. 2013. Semeval-2013
task 5: Evaluating phrasal semantics. In Second
Joint Conference on Lexical and Computational Se-
mantics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 39–47, Atlanta, Geor-
gia, USA, June. Association for Computational Lin-
guistics.
Thomas Landauer and Susan Dumais. 1997. A so-
lution to Plato’s problem: The Latent Semantic
Analysis theory of the acquisition, induction, and
representation of knowledge. Psychology Review,
104:211–240.
Daniel D. Lee and H. Sebastian Seung. 2000. Al-
gorithms for non-negative matrix factorization. In
Advances in Neural Information Processing Systems
13, pages 556–562.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics (COLING-ACL98), Volume 2,
pages 768–774, Montreal, Quebec, Canada.
Zhaohui Luo. 2010. Type-theoretical semantics with
coercive subtyping. SALT20, Vancouver.
Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, S Menini, and Roberto Zamparelli.
2014. Semeval-2014 task 1: Evaluation of compo-
sitional distributional semantic models on full sen-
tences through semantic relatedness and textual en-
tailment. In Proceedings of SemEval 2014: Interna-
tional Workshop on Semantic Evaluation.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. proceedings of
ACL-08: HLT, pages 236–244.
J. Mitchell and M. Lapata. 2010. Composition in dis-
tributional models of semantics. Cognitive Science,
34(8):1388–1429.
J. Nivre, J. Hall, and J. Nilsson. 2006a. Maltparser:
A data-driven parser-generator for dependency pars-
ing. In Proceedings of LREC-2006, pages 2216–
2219, Genoa, Italy.
</reference>
<page confidence="0.95174">
290
</page>
<reference confidence="0.999841720930233">
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006b.
Maltparser: A data-driven parser-generator for de-
pendency parsing. In Proceedings of LREC-2006,
pages 2216–2219.
Barbara H Partee. 2010. Privative adjectives: sub-
sective plus coercion. B ¨AUERLE, R. et ZIM-
MERMANN, TE, ´editeurs: Presuppositions and Dis-
course: Essays Offered to Hans Kamp, pages 273–
285.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201–1211, Jeju Island, Korea, July. Association for
Computational Linguistics.
Kristina Toutanova and Christopher D. Manning.
2000. Enriching the knowledge sources used in a
maximum entropy part-of-speech tagger. In Pro-
ceedings of the Joint SIGDAT Conference on Empir-
ical Methods in Natural Language Processing and
Very Large Corpora (EMNLP/VLC-2000), pages
63–70.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of HLT-NAACL 2003, pages 252–
259.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37(1):141–188.
Tim Van de Cruys. 2010. A non-negative tensor factor-
ization model for selectional preference induction.
Natural Language Engineering, 16(4):417–437.
Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional distribu-
tional semantics. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics
(Coling 2010), pages 1263–1271, Beijing, China,
August. Coling 2010 Organizing Committee.
</reference>
<page confidence="0.997853">
291
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.675473">
<title confidence="0.997867">A Generalisation of Lexical for Composition in Distributional Semantics</title>
<author confidence="0.999556">Antoine Bride Tim Van_de_Cruys Nicholas Asher</author>
<affiliation confidence="0.983084">IRIT &amp; Universit´e de Toulouse IRIT &amp; CNRS, Toulouse IRIT &amp; CNRS,</affiliation>
<email confidence="0.726146">antoine.bride@irit.frtim.vandecruys@irit.frnicholas.asher@irit.fr</email>
<abstract confidence="0.99731375">Over the last two decades, numerous algorithms have been developed that successfully capture something of the semantics of single words by looking at their distribution in text and comparing these distributions in a vector space model. However, it is not straightforward to construct meaning representations beyond the level of individual words – i.e. the combination of words into larger units – using distributional methods. Our contribution is twofold. First of all, we carry out a largescale evaluation, comparing different composition methods within the distributional framework for the cases of both adjectivenoun and noun-noun composition, making use of a newly developed dataset. Secondly, we propose a novel method for composition, which generalises the approach by Baroni and Zamparelli (2010). The performance of our novel method is also evaluated on our new dataset and proves competitive with the best methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nicholas Asher</author>
</authors>
<title>Lexical Meaning in Context: A Web of Words.</title>
<date>2011</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2422" citStr="Asher, 2011" startWordPosition="372" endWordPosition="373">ributional semantics offers substantially larger coverage since it is able to analyze massive amounts of empirical data. However, it is not trivial to combine the algebraic objects created by distributional semantics to get a sensible distributional representation for more complex expressions, consisting of several words. On the other hand, the formalism of the λ-calculus provides us with general, advanced and efficient methods for composition that can model meaning composition not only of simple phrases, but also more complex phenomena such as coercion or composition with fine-grained types (Asher, 2011; Luo, 2010; Bassac et al., 2010). Despite continued efforts to find a general method for composition and various approaches for the composition of specific syntactic structures (e.g. adjective-noun composition, or the composition of transitive verbs and direct objects (Mitchell and Lapata, 2008; Coecke et al., 2010; Baroni and Zamparelli, 2010)), the modeling of compositionality is still an important challenge for distributional semantics. Moreover, the validation of proposed methods for composition has used relatively small datasets of human similarity judgements (Mitchell and Lapata, 2008).</context>
<context position="11228" citStr="Asher, 2011" startWordPosition="1800" endWordPosition="1801">hen for blood donation campaign plan meeting and so forth. In addition, LF’s approach to adjectival meaning and composition has a theoretical drawback. Like Montague Grammar, it supposes that the effect of an adjective on a noun meaning is specific to the adjective (Kamp, 1975). However, recent studies suggest that the Montague approach overgeneralises from the worst case, and that the vast majority of adjectives in the world’s languages are subsective, suggesting that the modification of nominal meaning that results from their composition with a noun follows general principles (Partee, 2010; Asher, 2011) that are independent of the presence or absence of examples of association. 2.3 Generalised LF To solve these problems, we generalise LF and replace individual matrices for adjectival meanings by a single lexical function: a tensor for adjectival composition �9/.2 Our proposal is that adjectivenoun composition is carried out by multiplying the tensor 9/ with the vector for the adjective adj, followed by a multiplication with the vector noun, c.f. Figure 3. The product of the tensor 9/ and the vector adj yields a matrix dependent of the adjective that is multiplied with the vector noun. This m</context>
</contexts>
<marker>Asher, 2011</marker>
<rawString>Nicholas Asher. 2011. Lexical Meaning in Context: A Web of Words. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1183--1193</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="1083" citStr="Baroni and Zamparelli (2010)" startWordPosition="161" endWordPosition="164">t and comparing these distributions in a vector space model. However, it is not straightforward to construct meaning representations beyond the level of individual words – i.e. the combination of words into larger units – using distributional methods. Our contribution is twofold. First of all, we carry out a largescale evaluation, comparing different composition methods within the distributional framework for the cases of both adjectivenoun and noun-noun composition, making use of a newly developed dataset. Secondly, we propose a novel method for composition, which generalises the approach by Baroni and Zamparelli (2010). The performance of our novel method is also evaluated on our new dataset and proves competitive with the best methods. 1 Introduction In the course of the last two decades, there has been a growing interest in distributional methods for lexical semantics (Landauer and Dumais, 1997; Lin, 1998; Turney and Pantel, 2010). These methods are based on the distributional hypothesis (Harris, 1954), according to which words that appear in the same contexts tend to be similar in meaning. Inspired by Harris’ hypothesis, numerous researchers have developed algorithms that try to capture the semantics of </context>
<context position="2769" citStr="Baroni and Zamparelli, 2010" startWordPosition="422" endWordPosition="425"> other hand, the formalism of the λ-calculus provides us with general, advanced and efficient methods for composition that can model meaning composition not only of simple phrases, but also more complex phenomena such as coercion or composition with fine-grained types (Asher, 2011; Luo, 2010; Bassac et al., 2010). Despite continued efforts to find a general method for composition and various approaches for the composition of specific syntactic structures (e.g. adjective-noun composition, or the composition of transitive verbs and direct objects (Mitchell and Lapata, 2008; Coecke et al., 2010; Baroni and Zamparelli, 2010)), the modeling of compositionality is still an important challenge for distributional semantics. Moreover, the validation of proposed methods for composition has used relatively small datasets of human similarity judgements (Mitchell and Lapata, 2008).1 Although such studies comparing similarity judgements have their merits, it would be interesting to have studies that evaluate methods for composition on a larger scale, using a larger test set of different specific compositions. Such an evaluation would allow us to evaluate more thoroughly the different methods of composition that have been p</context>
<context position="4805" citStr="Baroni and Zamparelli, 2010" startWordPosition="728" endWordPosition="731"> used the Semeval 2013 dataset of phrasal similarity judgements for English with similar pairs extracted semi-automatically from the English Wiktionary to construct a dataset for English for both adjective-noun and noun-noun composition. This affords us a cross-linguistic comparison of the methods. These data sets provide a substantial evaluation of the performance of different compositional methods. We have tested three different methods of composition proposed in the literature, viz. the additive and multiplicative model (Mitchell and Lapata, 2008), as well as the lexical function approach (Baroni and Zamparelli, 2010). The two first methods are entirely general, and take as input automatically constructed vectors for adjectives and nouns. The method by Baroni and Zamparelli, on the other hand, requires the acquisition of a particular function for each adjective, represented by a matrix. The second goal of our paper is to generalise the functional approach in order to eliminate the need for an individual function for each adjective. To this goal, we automatically learn a generalised lexical function, based on Baroni and Zamparelli’s approach. This generalised function combines with an adjective vector and a</context>
<context position="9549" citStr="Baroni and Zamparelli (2010)" startWordPosition="1516" endWordPosition="1519">es]. I think Jack, for instance, is a charming [name]. Figure 2: Excerpt from Oscar Wilde’s The Importance of Being Earnest For our example, we would minimize, among others DIVINE×divine name − name to get the matrix for DIVINE. LF requires a large corpus, because we have to observe a sufficient number of examples of the adjective and noun combined, which are perforce less exemplified than the presence of the noun or adjective in isolation. In Figure 2, each of the occurrences of ‘name’ can contribute to the information in the vector name but none can contribute to the vector evanescent name. Baroni and Zamparelli (2010) offer an explanation of how to cope with the potential sparse data problem for learning matrices for adjectives. Moreover, recent evaluations of LF show that existent corpora have enough data for it to provide a semantics for the most frequent adjectives and obtain better results than other methods (Dinu et al., 2013b). Nevertheless, LF has limitations in treating relatively rare adjectives. For example, the adjective ‘evanescent’ appears 359 times in the UKWaC corpus (Baroni et al., 2009). This is enough to generate a vector for evanescent, but may not be sufficient to generate a sufficient </context>
<context position="28899" citStr="Baroni and Zamparelli (2010)" startWordPosition="4704" endWordPosition="4707"> of each feature) are the most important. They evaluate their models on a noun-verb phrase similarity task. Human annotators were asked to judge the similarity of two composed pairs (by attributing a certain score). The model’s task is then to reproduce the human judgements. Their results show that the multiplicative model yields the best results, along with a weighted combination of the additive and multiplicative model. The authors redid their study using a larger test set in Mitchell and Lapata (2010) (adjective-noun composition was also included), and they confirmed their initial results. Baroni and Zamparelli (2010) evaluate their lexical function model within a somewhat different context. They evaluated their model by looking at its capacity of reconstructing the adjective noun vectors that have not been seen during training. Their results show that their lexical function model obtains the best results for the reconstruction of the original co-occurrence vectors, followed by the additive model. We observe the same tendency in our evaluation results for French, although our results for English show a different picture. We would like to explore this discordance further in future work. Grefenstette et al. </context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1183–1193, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The wacky wide web: A collection of very large linguistically processed web-crawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="10044" citStr="Baroni et al., 2009" startWordPosition="1598" endWordPosition="1601">ute to the information in the vector name but none can contribute to the vector evanescent name. Baroni and Zamparelli (2010) offer an explanation of how to cope with the potential sparse data problem for learning matrices for adjectives. Moreover, recent evaluations of LF show that existent corpora have enough data for it to provide a semantics for the most frequent adjectives and obtain better results than other methods (Dinu et al., 2013b). Nevertheless, LF has limitations in treating relatively rare adjectives. For example, the adjective ‘evanescent’ appears 359 times in the UKWaC corpus (Baroni et al., 2009). This is enough to generate a vector for evanescent, but may not be sufficient to generate a sufficient number of vectors evanescent noun to build the matrix EVANESCENT. More importantly, for noun-noun combinations, one may need to have a LF for a combination. To get the meaning of blood donation campaign in the LF approach, the matrix BLOOD DONATION must be combined to the vector campaign. Learning this matrix would require to build vectors blood donation noun for many nouns. Even if it were possible, the issue would arise again for blood donation campaign plan, then for blood donation campa</context>
<context position="18575" citStr="Baroni et al., 2009" startWordPosition="3015" endWordPosition="3018"> English negative examples as it is similar in construction to our first set of negative examples in French. In addition, we created a completely random negative test set for English in the same fashion we did for the second negative test set for French. Finally, the original test set also contains nounnoun compounds so we also created a test set for that. This gave us 226 positive and negative pairs for the noun-noun composition. 3.2 Semantic space construction In this section, we describe the construction of our semantic space. Our semantic space for French was built using the FRWaC corpus (Baroni et al., 2009) – about 1,6 billion words of web texts – which has been tagged with MElt tagger (Denis et al., 2010) and parsed with MaltParser (Nivre et al., 2006a), trained on a dependency-based version of the French treebank (Candito et al., 2010). Our semantic space for English has been built using the UKWaC corpus (Baroni et al., 2009), which consists of about 2 billion words extracted from the web. The corpus has been part of speech tagged and lemmatized with Stanford Part-OfSpeech Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003), and parsed with MaltParser (Nivre et al., 2006b) trained on </context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: A collection of very large linguistically processed web-crawled corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Bassac</author>
<author>Bruno Mery</author>
<author>Christian Retor´e</author>
</authors>
<title>Towards a Type-theoretical account of lexical semantics.</title>
<date>2010</date>
<journal>Journal of Logic, Language and Information,</journal>
<volume>19</volume>
<issue>2</issue>
<marker>Bassac, Mery, Retor´e, 2010</marker>
<rawString>Christian Bassac, Bruno Mery, and Christian Retor´e. 2010. Towards a Type-theoretical account of lexical semantics. Journal of Logic, Language and Information, 19(2):229–245.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Blacoe</author>
<author>Mirella Lapata</author>
</authors>
<title>A comparison of vector-based representations for semantic composition.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>546--556</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="31556" citStr="Blacoe and Lapata (2012)" startWordPosition="5142" endWordPosition="5146">ir transitive similarity task. Socher et al. (2012) present a compositional model based on a recursive neural network. Each 288 node in a syntactic tree is assigned both a vector and a matrix; the vector captures the actual meaning of the constituent, while the matrix models the way it changes the meaning of neighbouring words and phrases. They use an extrinsic evaluation, using the model for a sentiment prediction task. They show that their model gets better results than the additive, multiplicative, and lexical function approach. Other researchers, however, have published different results. Blacoe and Lapata (2012) evaluated the additive and multiplicative model, as well as Socher et al.’s (2012) approach on two different tasks: Mitchell &amp; Lapata’s (2010) similarity task and a paraphrase detection task. They find that the additive and multiplicative models reach better scores than Socher et al.’s model. Tensors have been used before to model different aspects of natural language. Giesbrecht (2010) describes a tensor factorization model for the construction of a distributional model that is sensitive to word order. And Van de Cruys (2010) uses a tensor factorization model in order to construct a three-wa</context>
</contexts>
<marker>Blacoe, Lapata, 2012</marker>
<rawString>William Blacoe and Mirella Lapata. 2012. A comparison of vector-based representations for semantic composition. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 546–556, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie Candito</author>
<author>Benoit Crabb´e</author>
<author>Pascal Denis</author>
</authors>
<title>Statistical french dependency parsing: treebank conversion and first results.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC</booktitle>
<pages>1840--1847</pages>
<marker>Candito, Crabb´e, Denis, 2010</marker>
<rawString>Marie Candito, Benoit Crabb´e, Pascal Denis, et al. 2010. Statistical french dependency parsing: treebank conversion and first results. In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC 2010), pages 1840–1847.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information &amp; lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="20555" citStr="Church and Hanks, 1990" startWordPosition="3306" endWordPosition="3309">st set for French tions from the QuestionBank5. For both corpora, we extracted the lemmas of all nouns, adjectives and (bag of words) context words. We only kept those lemmas that consist of alphabetic characters.6 We then selected the 10K most frequent lemmas for each category (nouns, adjectives, context words), making sure to include all the words from the test set. As a final step, we created our semantic space vectors using adjectives and nouns as instances, and bag of words context words as features. The resulting vectors were weighted using positive point-wise mutual information (ppmi, (Church and Hanks, 1990)), and all vectors were normalized to unit length. We then compared the different composition methods on different versions of the same semantic space (both for French and English): the full semantic space, a reduced version of the space to 300 dimensions using singular value decomposition (svd, (Golub and Van Loan, 1996)), and a reduced version of the space to 300 dimensions using non-negative matrix factorization (nmf, (Lee and Seung, 2000)). We did so in order to test each method in its optimal conditions. In fact: • A non-reduced space contains more information. This might be beneficial fo</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth W. Church and Patrick Hanks. 1990. Word association norms, mutual information &amp; lexicography. Computational Linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coecke</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Clark</author>
</authors>
<title>Mathematical foundations for a compositional distributed model of meaning. Lambek Festschrift, Linguistic Analysis,</title>
<date>2010</date>
<volume>36</volume>
<pages>36</pages>
<contexts>
<context position="2739" citStr="Coecke et al., 2010" startWordPosition="418" endWordPosition="421">several words. On the other hand, the formalism of the λ-calculus provides us with general, advanced and efficient methods for composition that can model meaning composition not only of simple phrases, but also more complex phenomena such as coercion or composition with fine-grained types (Asher, 2011; Luo, 2010; Bassac et al., 2010). Despite continued efforts to find a general method for composition and various approaches for the composition of specific syntactic structures (e.g. adjective-noun composition, or the composition of transitive verbs and direct objects (Mitchell and Lapata, 2008; Coecke et al., 2010; Baroni and Zamparelli, 2010)), the modeling of compositionality is still an important challenge for distributional semantics. Moreover, the validation of proposed methods for composition has used relatively small datasets of human similarity judgements (Mitchell and Lapata, 2008).1 Although such studies comparing similarity judgements have their merits, it would be interesting to have studies that evaluate methods for composition on a larger scale, using a larger test set of different specific compositions. Such an evaluation would allow us to evaluate more thoroughly the different methods o</context>
<context position="29986" citStr="Coecke et al. (2010)" startWordPosition="4888" endWordPosition="4891"> results for English show a different picture. We would like to explore this discordance further in future work. Grefenstette et al. (2013) equally propose a generalisation of the lexical function model that uses tensors. Their goal is to model transitive verbs, and the way we acquire our tensor is similar to theirs. In fact, they use the LF approach in order to learn VERB OBJECT matrices that may be multiplied by a subject vector to obtain the subject verb object vector. In a second step, they learn a tensor for each individual verb, which is similar to how we learn our adjective tensor 9 /. Coecke et al. (2010) present an abstract theoretical framework in which a sentence vector is a function of the Kronecker product of its word vectors, which allows for greater interaction between the different word features. A number of instantiations of the framework – where the key idea is that relational words (e.g. adjectives or verbs) have a rich (multi-dimensional) structure that acts as a filter on their arguments – are tested experimentally in Grefenstette and Sadrzadeh (2011a) and Grefenstette and Sadrzadeh (2011b). The authors evaluated their models using a similarity task that is similar to the one used</context>
</contexts>
<marker>Coecke, Sadrzadeh, Clark, 2010</marker>
<rawString>Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. 2010. Mathematical foundations for a compositional distributed model of meaning. Lambek Festschrift, Linguistic Analysis, vol. 36, 36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Benoit Sagot</author>
</authors>
<title>Exploitation d’une ressource lexicale pour la construction d’un ´etiqueteur morphosyntaxique ´etat-de-l’art du franc¸ais.</title>
<date>2010</date>
<booktitle>In Traitement Automatique des Langues Naturelles: TALN</booktitle>
<marker>Denis, Sagot, 2010</marker>
<rawString>Pascal Denis, Benoit Sagot, et al. 2010. Exploitation d’une ressource lexicale pour la construction d’un ´etiqueteur morphosyntaxique ´etat-de-l’art du franc¸ais. In Traitement Automatique des Langues Naturelles: TALN 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Nghia The Pham</author>
<author>Marco Baroni</author>
</authors>
<title>Dissect - distributional semantics composition toolkit.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>31--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="9868" citStr="Dinu et al., 2013" startWordPosition="1571" endWordPosition="1574">ve and noun combined, which are perforce less exemplified than the presence of the noun or adjective in isolation. In Figure 2, each of the occurrences of ‘name’ can contribute to the information in the vector name but none can contribute to the vector evanescent name. Baroni and Zamparelli (2010) offer an explanation of how to cope with the potential sparse data problem for learning matrices for adjectives. Moreover, recent evaluations of LF show that existent corpora have enough data for it to provide a semantics for the most frequent adjectives and obtain better results than other methods (Dinu et al., 2013b). Nevertheless, LF has limitations in treating relatively rare adjectives. For example, the adjective ‘evanescent’ appears 359 times in the UKWaC corpus (Baroni et al., 2009). This is enough to generate a vector for evanescent, but may not be sufficient to generate a sufficient number of vectors evanescent noun to build the matrix EVANESCENT. More importantly, for noun-noun combinations, one may need to have a LF for a combination. To get the meaning of blood donation campaign in the LF approach, the matrix BLOOD DONATION must be combined to the vector campaign. Learning this matrix would re</context>
</contexts>
<marker>Dinu, Pham, Baroni, 2013</marker>
<rawString>Georgiana Dinu, Nghia The Pham, and Marco Baroni. 2013a. Dissect - distributional semantics composition toolkit. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 31–36, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Nghia The Pham</author>
<author>Marco Baroni</author>
</authors>
<title>General estimation and evaluation of compositional distributional semantic models.</title>
<date>2013</date>
<booktitle>In Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality,</booktitle>
<pages>50--58</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="9868" citStr="Dinu et al., 2013" startWordPosition="1571" endWordPosition="1574">ve and noun combined, which are perforce less exemplified than the presence of the noun or adjective in isolation. In Figure 2, each of the occurrences of ‘name’ can contribute to the information in the vector name but none can contribute to the vector evanescent name. Baroni and Zamparelli (2010) offer an explanation of how to cope with the potential sparse data problem for learning matrices for adjectives. Moreover, recent evaluations of LF show that existent corpora have enough data for it to provide a semantics for the most frequent adjectives and obtain better results than other methods (Dinu et al., 2013b). Nevertheless, LF has limitations in treating relatively rare adjectives. For example, the adjective ‘evanescent’ appears 359 times in the UKWaC corpus (Baroni et al., 2009). This is enough to generate a vector for evanescent, but may not be sufficient to generate a sufficient number of vectors evanescent noun to build the matrix EVANESCENT. More importantly, for noun-noun combinations, one may need to have a LF for a combination. To get the meaning of blood donation campaign in the LF approach, the matrix BLOOD DONATION must be combined to the vector campaign. Learning this matrix would re</context>
</contexts>
<marker>Dinu, Pham, Baroni, 2013</marker>
<rawString>Georgiana Dinu, Nghia The Pham, and Marco Baroni. 2013b. General estimation and evaluation of compositional distributional semantic models. In Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 50–58, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugenie Giesbrecht</author>
</authors>
<title>Towards a matrix-based distributional model of meaning.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Student Research Workshop,</booktitle>
<pages>23--28</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="31946" citStr="Giesbrecht (2010)" startWordPosition="5208" endWordPosition="5209"> sentiment prediction task. They show that their model gets better results than the additive, multiplicative, and lexical function approach. Other researchers, however, have published different results. Blacoe and Lapata (2012) evaluated the additive and multiplicative model, as well as Socher et al.’s (2012) approach on two different tasks: Mitchell &amp; Lapata’s (2010) similarity task and a paraphrase detection task. They find that the additive and multiplicative models reach better scores than Socher et al.’s model. Tensors have been used before to model different aspects of natural language. Giesbrecht (2010) describes a tensor factorization model for the construction of a distributional model that is sensitive to word order. And Van de Cruys (2010) uses a tensor factorization model in order to construct a three-way selectional preference model of verbs, subjects, and objects. 5 Conclusion We have developed a new method of composition and tested it in comparison with different composition methods assuming a distributional approach. We developed a test set for French pairing nouns with adjective noun combinations very similar in meaning from the French Wiktionary. We also used an existing SEMEVAL-2</context>
</contexts>
<marker>Giesbrecht, 2010</marker>
<rawString>Eugenie Giesbrecht. 2010. Towards a matrix-based distributional model of meaning. In Proceedings of the NAACL HLT 2010 Student Research Workshop, pages 23–28. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gene H Golub</author>
<author>Charles F Van Loan</author>
</authors>
<date>1996</date>
<booktitle>Matrix Computations (3rd Ed.). Johns Hopkins</booktitle>
<publisher>University Press,</publisher>
<location>Baltimore, MD, USA.</location>
<marker>Golub, Van Loan, 1996</marker>
<rawString>Gene H. Golub and Charles F. Van Loan. 1996. Matrix Computations (3rd Ed.). Johns Hopkins University Press, Baltimore, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimental support for a categorical compositional distributional model of meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1394--1404</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="30453" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="4965" endWordPosition="4968">t verb object vector. In a second step, they learn a tensor for each individual verb, which is similar to how we learn our adjective tensor 9 /. Coecke et al. (2010) present an abstract theoretical framework in which a sentence vector is a function of the Kronecker product of its word vectors, which allows for greater interaction between the different word features. A number of instantiations of the framework – where the key idea is that relational words (e.g. adjectives or verbs) have a rich (multi-dimensional) structure that acts as a filter on their arguments – are tested experimentally in Grefenstette and Sadrzadeh (2011a) and Grefenstette and Sadrzadeh (2011b). The authors evaluated their models using a similarity task that is similar to the one used by Mitchell &amp; Lapata. However, they use more complex compositional expressions: rather than using compositions of two words (such as a verb and an object), they use simple transitive phrases (subject-verbobject). They show that their instantiations of the categorical model reach better results than the additive and multiplicative models on their transitive similarity task. Socher et al. (2012) present a compositional model based on a recursive neural network. Ea</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011a. Experimental support for a categorical compositional distributional model of meaning. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1394– 1404, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimenting with transitive verbs in a discocat.</title>
<date>2011</date>
<booktitle>In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics,</booktitle>
<pages>62--66</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, UK,</location>
<contexts>
<context position="30453" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="4965" endWordPosition="4968">t verb object vector. In a second step, they learn a tensor for each individual verb, which is similar to how we learn our adjective tensor 9 /. Coecke et al. (2010) present an abstract theoretical framework in which a sentence vector is a function of the Kronecker product of its word vectors, which allows for greater interaction between the different word features. A number of instantiations of the framework – where the key idea is that relational words (e.g. adjectives or verbs) have a rich (multi-dimensional) structure that acts as a filter on their arguments – are tested experimentally in Grefenstette and Sadrzadeh (2011a) and Grefenstette and Sadrzadeh (2011b). The authors evaluated their models using a similarity task that is similar to the one used by Mitchell &amp; Lapata. However, they use more complex compositional expressions: rather than using compositions of two words (such as a verb and an object), they use simple transitive phrases (subject-verbobject). They show that their instantiations of the categorical model reach better results than the additive and multiplicative models on their transitive similarity task. Socher et al. (2012) present a compositional model based on a recursive neural network. Ea</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011b. Experimenting with transitive verbs in a discocat. In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics, pages 62–66, Edinburgh, UK, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Grefenstette</author>
<author>G Dinu</author>
<author>Y-Z Zhang</author>
<author>M Sadrzadeh</author>
<author>M Baroni</author>
</authors>
<title>Multi-step regression learning for compositional distributional semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Semantics (IWCS),</booktitle>
<pages>131--142</pages>
<institution>East Stroudsburg PA. Association for Computational Linguistics.</institution>
<contexts>
<context position="29505" citStr="Grefenstette et al. (2013)" startWordPosition="4799" endWordPosition="4802">nd Zamparelli (2010) evaluate their lexical function model within a somewhat different context. They evaluated their model by looking at its capacity of reconstructing the adjective noun vectors that have not been seen during training. Their results show that their lexical function model obtains the best results for the reconstruction of the original co-occurrence vectors, followed by the additive model. We observe the same tendency in our evaluation results for French, although our results for English show a different picture. We would like to explore this discordance further in future work. Grefenstette et al. (2013) equally propose a generalisation of the lexical function model that uses tensors. Their goal is to model transitive verbs, and the way we acquire our tensor is similar to theirs. In fact, they use the LF approach in order to learn VERB OBJECT matrices that may be multiplied by a subject vector to obtain the subject verb object vector. In a second step, they learn a tensor for each individual verb, which is similar to how we learn our adjective tensor 9 /. Coecke et al. (2010) present an abstract theoretical framework in which a sentence vector is a function of the Kronecker product of its wor</context>
</contexts>
<marker>Grefenstette, Dinu, Zhang, Sadrzadeh, Baroni, 2013</marker>
<rawString>E. Grefenstette, G. Dinu, Y.-Z. Zhang, M. Sadrzadeh, and Baroni M. 2013. Multi-step regression learning for compositional distributional semantics. In Proceedings of the 10th International Conference on Computational Semantics (IWCS), pages 131–142, East Stroudsburg PA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig S Harris</author>
</authors>
<date>1954</date>
<booktitle>Distributional structure. Word,</booktitle>
<volume>10</volume>
<issue>23</issue>
<contexts>
<context position="1476" citStr="Harris, 1954" startWordPosition="227" endWordPosition="228"> for the cases of both adjectivenoun and noun-noun composition, making use of a newly developed dataset. Secondly, we propose a novel method for composition, which generalises the approach by Baroni and Zamparelli (2010). The performance of our novel method is also evaluated on our new dataset and proves competitive with the best methods. 1 Introduction In the course of the last two decades, there has been a growing interest in distributional methods for lexical semantics (Landauer and Dumais, 1997; Lin, 1998; Turney and Pantel, 2010). These methods are based on the distributional hypothesis (Harris, 1954), according to which words that appear in the same contexts tend to be similar in meaning. Inspired by Harris’ hypothesis, numerous researchers have developed algorithms that try to capture the semantics of individual words by looking at their distribution in a large corpus. Compared to manual studies common to formal semantics, distributional semantics offers substantially larger coverage since it is able to analyze massive amounts of empirical data. However, it is not trivial to combine the algebraic objects created by distributional semantics to get a sensible distributional representation </context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig S. Harris. 1954. Distributional structure. Word, 10(23):146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Kamp</author>
</authors>
<title>Two theories about adjectives. Formal semantics of natural language,</title>
<date>1975</date>
<pages>123--155</pages>
<contexts>
<context position="10894" citStr="Kamp, 1975" startWordPosition="1749" endWordPosition="1750"> LF for a combination. To get the meaning of blood donation campaign in the LF approach, the matrix BLOOD DONATION must be combined to the vector campaign. Learning this matrix would require to build vectors blood donation noun for many nouns. Even if it were possible, the issue would arise again for blood donation campaign plan, then for blood donation campaign plan meeting and so forth. In addition, LF’s approach to adjectival meaning and composition has a theoretical drawback. Like Montague Grammar, it supposes that the effect of an adjective on a noun meaning is specific to the adjective (Kamp, 1975). However, recent studies suggest that the Montague approach overgeneralises from the worst case, and that the vast majority of adjectives in the world’s languages are subsective, suggesting that the modification of nominal meaning that results from their composition with a noun follows general principles (Partee, 2010; Asher, 2011) that are independent of the presence or absence of examples of association. 2.3 Generalised LF To solve these problems, we generalise LF and replace individual matrices for adjectival meanings by a single lexical function: a tensor for adjectival composition �9/.2 </context>
</contexts>
<marker>Kamp, 1975</marker>
<rawString>Hans Kamp. 1975. Two theories about adjectives. Formal semantics of natural language, pages 123– 155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tamara G Kolda</author>
<author>Brett W Bader</author>
</authors>
<title>Tensor decompositions and applications.</title>
<date>2009</date>
<journal>SIAM Review,</journal>
<volume>51</volume>
<issue>3</issue>
<contexts>
<context position="12326" citStr="Kolda and Bader, 2009" startWordPosition="1983" endWordPosition="1986"> the tensor 9/ and the vector adj yields a matrix dependent of the adjective that is multiplied with the vector noun. This matrix corresponds to the LF matrix ADJ. As indicated in Figure 4, we obtain 9/ with the help of matrices obtained from the LF approach, and from vectors for single words easily obtained in distributional semantics; we perform a least square regression minimizing the norm of the matrices generated by the equations in Figure 4. Formally, the problem is 2A tensor generalises a matrix to several dimensions. We use a tensor in three modes. For an introduction to tensors, see (Kolda and Bader, 2009). 283 x a dj n o u n = A djective )x V adjective, noun CompositionGLF(adjective, noun) Figure 3: Composition in the generalised lexical function model Find A s.t. ∑adj(A x adj − ADJ)2 is minimal Note that our tensor is not just the compilation of the information found in the LF matrices: the adjective mode of our tensor has a limited number of dimensions, whereas the LF approach creates a separate matrix for each individual adjective. This reduction forces the model to generalise, and we hypothesise that this generalisation allows us to make proper noun modifications even in the light of spars</context>
</contexts>
<marker>Kolda, Bader, 2009</marker>
<rawString>Tamara G. Kolda and Brett W. Bader. 2009. Tensor decompositions and applications. SIAM Review, 51(3):455–500, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Korkontzelos</author>
<author>Torsten Zesch</author>
<author>Fabio Massimo Zanzotto</author>
<author>Chris Biemann</author>
</authors>
<title>Semeval-2013 task 5: Evaluating phrasal semantics.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>39--47</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="14179" citStr="Korkontzelos et al., 2013" startWordPosition="2294" endWordPosition="2297">n campaign = (N x blood donation) x campaign and this allows us to avoid the sparse data problem for the LF approach in generating the matrix BLOOD DONATION. Once we have obtained the tensor A , we verify experimentally its relevance to composition, in order to check whether a tensor optimising the equations in Figure 4 would be semantically interesting. 3 Evaluation 3.1 Tasks description In order to evaluate the different composition methods, we constructed test sets for French and English, inspired by the work of Zanzotto et al. (2010) and the SEMEVAL-2013 task evaluating phrasal semantics (Korkontzelos et al., 2013). The task is to make a judgement about the semantic similarity of a short word sequence (an adjectivenoun combination) and a single noun. This is important, as composition models need to be able to treat word sequences of arbitrary length. Formally, the task is presented as: With comp = composition(adj, noun1) Evaluate similarity(comp, noun2) where the ‘composition’ function is carried out by the different composition models. ‘Similarity’ needs to be a binary function, with return values ‘similar’ and ‘non-similar’. Note, however, that the distributional approach yields a continuous similarit</context>
</contexts>
<marker>Korkontzelos, Zesch, Zanzotto, Biemann, 2013</marker>
<rawString>Ioannis Korkontzelos, Torsten Zesch, Fabio Massimo Zanzotto, and Chris Biemann. 2013. Semeval-2013 task 5: Evaluating phrasal semantics. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 39–47, Atlanta, Georgia, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Landauer</author>
<author>Susan Dumais</author>
</authors>
<title>A solution to Plato’s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychology Review,</journal>
<pages>104--211</pages>
<contexts>
<context position="1366" citStr="Landauer and Dumais, 1997" startWordPosition="208" endWordPosition="211"> of all, we carry out a largescale evaluation, comparing different composition methods within the distributional framework for the cases of both adjectivenoun and noun-noun composition, making use of a newly developed dataset. Secondly, we propose a novel method for composition, which generalises the approach by Baroni and Zamparelli (2010). The performance of our novel method is also evaluated on our new dataset and proves competitive with the best methods. 1 Introduction In the course of the last two decades, there has been a growing interest in distributional methods for lexical semantics (Landauer and Dumais, 1997; Lin, 1998; Turney and Pantel, 2010). These methods are based on the distributional hypothesis (Harris, 1954), according to which words that appear in the same contexts tend to be similar in meaning. Inspired by Harris’ hypothesis, numerous researchers have developed algorithms that try to capture the semantics of individual words by looking at their distribution in a large corpus. Compared to manual studies common to formal semantics, distributional semantics offers substantially larger coverage since it is able to analyze massive amounts of empirical data. However, it is not trivial to comb</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas Landauer and Susan Dumais. 1997. A solution to Plato’s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge. Psychology Review, 104:211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel D Lee</author>
<author>H Sebastian Seung</author>
</authors>
<title>Algorithms for non-negative matrix factorization.</title>
<date>2000</date>
<booktitle>In Advances in Neural Information Processing Systems 13,</booktitle>
<pages>556--562</pages>
<contexts>
<context position="21001" citStr="Lee and Seung, 2000" startWordPosition="3380" endWordPosition="3383">uns as instances, and bag of words context words as features. The resulting vectors were weighted using positive point-wise mutual information (ppmi, (Church and Hanks, 1990)), and all vectors were normalized to unit length. We then compared the different composition methods on different versions of the same semantic space (both for French and English): the full semantic space, a reduced version of the space to 300 dimensions using singular value decomposition (svd, (Golub and Van Loan, 1996)), and a reduced version of the space to 300 dimensions using non-negative matrix factorization (nmf, (Lee and Seung, 2000)). We did so in order to test each method in its optimal conditions. In fact: • A non-reduced space contains more information. This might be beneficial for methods that are able to take advantage of the full semantic space (viz. the additive et multiplicative model). On the other hand, to be able to use the non-reduced space for the lexical function approach, one would have to learn matrices of size 10K x10K for each adjective. This would be problematic in terms of computing time and data sparseness, as we previously noted. The same goes for our gen5http://maltparser.org/mco/english_parser/ en</context>
</contexts>
<marker>Lee, Seung, 2000</marker>
<rawString>Daniel D. Lee and H. Sebastian Seung. 2000. Algorithms for non-negative matrix factorization. In Advances in Neural Information Processing Systems 13, pages 556–562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics (COLING-ACL98),</booktitle>
<volume>2</volume>
<pages>768--774</pages>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="1377" citStr="Lin, 1998" startWordPosition="212" endWordPosition="213">gescale evaluation, comparing different composition methods within the distributional framework for the cases of both adjectivenoun and noun-noun composition, making use of a newly developed dataset. Secondly, we propose a novel method for composition, which generalises the approach by Baroni and Zamparelli (2010). The performance of our novel method is also evaluated on our new dataset and proves competitive with the best methods. 1 Introduction In the course of the last two decades, there has been a growing interest in distributional methods for lexical semantics (Landauer and Dumais, 1997; Lin, 1998; Turney and Pantel, 2010). These methods are based on the distributional hypothesis (Harris, 1954), according to which words that appear in the same contexts tend to be similar in meaning. Inspired by Harris’ hypothesis, numerous researchers have developed algorithms that try to capture the semantics of individual words by looking at their distribution in a large corpus. Compared to manual studies common to formal semantics, distributional semantics offers substantially larger coverage since it is able to analyze massive amounts of empirical data. However, it is not trivial to combine the alg</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics (COLING-ACL98), Volume 2, pages 768–774, Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhaohui Luo</author>
</authors>
<title>Type-theoretical semantics with coercive subtyping.</title>
<date>2010</date>
<booktitle>SALT20,</booktitle>
<location>Vancouver.</location>
<contexts>
<context position="2433" citStr="Luo, 2010" startWordPosition="374" endWordPosition="375">mantics offers substantially larger coverage since it is able to analyze massive amounts of empirical data. However, it is not trivial to combine the algebraic objects created by distributional semantics to get a sensible distributional representation for more complex expressions, consisting of several words. On the other hand, the formalism of the λ-calculus provides us with general, advanced and efficient methods for composition that can model meaning composition not only of simple phrases, but also more complex phenomena such as coercion or composition with fine-grained types (Asher, 2011; Luo, 2010; Bassac et al., 2010). Despite continued efforts to find a general method for composition and various approaches for the composition of specific syntactic structures (e.g. adjective-noun composition, or the composition of transitive verbs and direct objects (Mitchell and Lapata, 2008; Coecke et al., 2010; Baroni and Zamparelli, 2010)), the modeling of compositionality is still an important challenge for distributional semantics. Moreover, the validation of proposed methods for composition has used relatively small datasets of human similarity judgements (Mitchell and Lapata, 2008).1 Although </context>
</contexts>
<marker>Luo, 2010</marker>
<rawString>Zhaohui Luo. 2010. Type-theoretical semantics with coercive subtyping. SALT20, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Marelli</author>
<author>Luisa Bentivogli</author>
<author>Marco Baroni</author>
<author>Raffaella Bernardi</author>
<author>S Menini</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment.</title>
<date>2014</date>
<booktitle>In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="3791" citStr="Marelli et al., 2014" startWordPosition="582" endWordPosition="585">tion on a larger scale, using a larger test set of different specific compositions. Such an evaluation would allow us to evaluate more thoroughly the different methods of composition that have been proposed. This is one of the goals of this paper. To achieve this goal, we make use of two different resources. We have constructed a dataset for French containing a large number of pairs of a compositional expression (adjective-noun) and a single noun that is semantically close or identical to the composed expression. These pairs have been extracted semi-automatically from 1A notable exception is (Marelli et al., 2014), who propose a large-scale evaluation dataset for composition at the sentence level. 281 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 281–291, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics the French Wiktionary. We have also used the Semeval 2013 dataset of phrasal similarity judgements for English with similar pairs extracted semi-automatically from the English Wiktionary to construct a dataset for English for both adjective-noun and</context>
</contexts>
<marker>Marelli, Bentivogli, Baroni, Bernardi, Menini, Zamparelli, 2014</marker>
<rawString>Marco Marelli, Luisa Bentivogli, Marco Baroni, Raffaella Bernardi, S Menini, and Roberto Zamparelli. 2014. Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition. proceedings of ACL-08: HLT,</title>
<date>2008</date>
<pages>236--244</pages>
<contexts>
<context position="2718" citStr="Mitchell and Lapata, 2008" startWordPosition="414" endWordPosition="417">expressions, consisting of several words. On the other hand, the formalism of the λ-calculus provides us with general, advanced and efficient methods for composition that can model meaning composition not only of simple phrases, but also more complex phenomena such as coercion or composition with fine-grained types (Asher, 2011; Luo, 2010; Bassac et al., 2010). Despite continued efforts to find a general method for composition and various approaches for the composition of specific syntactic structures (e.g. adjective-noun composition, or the composition of transitive verbs and direct objects (Mitchell and Lapata, 2008; Coecke et al., 2010; Baroni and Zamparelli, 2010)), the modeling of compositionality is still an important challenge for distributional semantics. Moreover, the validation of proposed methods for composition has used relatively small datasets of human similarity judgements (Mitchell and Lapata, 2008).1 Although such studies comparing similarity judgements have their merits, it would be interesting to have studies that evaluate methods for composition on a larger scale, using a larger test set of different specific compositions. Such an evaluation would allow us to evaluate more thoroughly th</context>
<context position="4733" citStr="Mitchell and Lapata, 2008" startWordPosition="716" endWordPosition="719">tion for Computational Linguistics the French Wiktionary. We have also used the Semeval 2013 dataset of phrasal similarity judgements for English with similar pairs extracted semi-automatically from the English Wiktionary to construct a dataset for English for both adjective-noun and noun-noun composition. This affords us a cross-linguistic comparison of the methods. These data sets provide a substantial evaluation of the performance of different compositional methods. We have tested three different methods of composition proposed in the literature, viz. the additive and multiplicative model (Mitchell and Lapata, 2008), as well as the lexical function approach (Baroni and Zamparelli, 2010). The two first methods are entirely general, and take as input automatically constructed vectors for adjectives and nouns. The method by Baroni and Zamparelli, on the other hand, requires the acquisition of a particular function for each adjective, represented by a matrix. The second goal of our paper is to generalise the functional approach in order to eliminate the need for an individual function for each adjective. To this goal, we automatically learn a generalised lexical function, based on Baroni and Zamparelli’s app</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. proceedings of ACL-08: HLT, pages 236–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mitchell</author>
<author>M Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="28780" citStr="Mitchell and Lapata (2010)" startWordPosition="4689" endWordPosition="4692">sition, of which vector addition (the sum of each feature) and vector multiplication (the element-wise multiplication of each feature) are the most important. They evaluate their models on a noun-verb phrase similarity task. Human annotators were asked to judge the similarity of two composed pairs (by attributing a certain score). The model’s task is then to reproduce the human judgements. Their results show that the multiplicative model yields the best results, along with a weighted combination of the additive and multiplicative model. The authors redid their study using a larger test set in Mitchell and Lapata (2010) (adjective-noun composition was also included), and they confirmed their initial results. Baroni and Zamparelli (2010) evaluate their lexical function model within a somewhat different context. They evaluated their model by looking at its capacity of reconstructing the adjective noun vectors that have not been seen during training. Their results show that their lexical function model obtains the best results for the reconstruction of the original co-occurrence vectors, followed by the additive model. We observe the same tendency in our evaluation results for French, although our results for E</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>J. Mitchell and M. Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
</authors>
<title>Maltparser: A data-driven parser-generator for dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC-2006,</booktitle>
<pages>2216--2219</pages>
<location>Genoa, Italy.</location>
<contexts>
<context position="18723" citStr="Nivre et al., 2006" startWordPosition="3043" endWordPosition="3046">om negative test set for English in the same fashion we did for the second negative test set for French. Finally, the original test set also contains nounnoun compounds so we also created a test set for that. This gave us 226 positive and negative pairs for the noun-noun composition. 3.2 Semantic space construction In this section, we describe the construction of our semantic space. Our semantic space for French was built using the FRWaC corpus (Baroni et al., 2009) – about 1,6 billion words of web texts – which has been tagged with MElt tagger (Denis et al., 2010) and parsed with MaltParser (Nivre et al., 2006a), trained on a dependency-based version of the French treebank (Candito et al., 2010). Our semantic space for English has been built using the UKWaC corpus (Baroni et al., 2009), which consists of about 2 billion words extracted from the web. The corpus has been part of speech tagged and lemmatized with Stanford Part-OfSpeech Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003), and parsed with MaltParser (Nivre et al., 2006b) trained on sections 2-21 of the Wall Street Journal section of the Penn Treebank extended with about 4000 ques285 positive examples random negative examples Wi</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2006</marker>
<rawString>J. Nivre, J. Hall, and J. Nilsson. 2006a. Maltparser: A data-driven parser-generator for dependency parsing. In Proceedings of LREC-2006, pages 2216– 2219, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Maltparser: A data-driven parser-generator for dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC-2006,</booktitle>
<pages>2216--2219</pages>
<contexts>
<context position="18723" citStr="Nivre et al., 2006" startWordPosition="3043" endWordPosition="3046">om negative test set for English in the same fashion we did for the second negative test set for French. Finally, the original test set also contains nounnoun compounds so we also created a test set for that. This gave us 226 positive and negative pairs for the noun-noun composition. 3.2 Semantic space construction In this section, we describe the construction of our semantic space. Our semantic space for French was built using the FRWaC corpus (Baroni et al., 2009) – about 1,6 billion words of web texts – which has been tagged with MElt tagger (Denis et al., 2010) and parsed with MaltParser (Nivre et al., 2006a), trained on a dependency-based version of the French treebank (Candito et al., 2010). Our semantic space for English has been built using the UKWaC corpus (Baroni et al., 2009), which consists of about 2 billion words extracted from the web. The corpus has been part of speech tagged and lemmatized with Stanford Part-OfSpeech Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003), and parsed with MaltParser (Nivre et al., 2006b) trained on sections 2-21 of the Wall Street Journal section of the Penn Treebank extended with about 4000 ques285 positive examples random negative examples Wi</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2006b. Maltparser: A data-driven parser-generator for dependency parsing. In Proceedings of LREC-2006, pages 2216–2219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara H Partee</author>
</authors>
<title>Privative adjectives: subsective plus coercion.</title>
<date>2010</date>
<journal>B</journal>
<pages>273--285</pages>
<contexts>
<context position="11214" citStr="Partee, 2010" startWordPosition="1797" endWordPosition="1799">mpaign plan, then for blood donation campaign plan meeting and so forth. In addition, LF’s approach to adjectival meaning and composition has a theoretical drawback. Like Montague Grammar, it supposes that the effect of an adjective on a noun meaning is specific to the adjective (Kamp, 1975). However, recent studies suggest that the Montague approach overgeneralises from the worst case, and that the vast majority of adjectives in the world’s languages are subsective, suggesting that the modification of nominal meaning that results from their composition with a noun follows general principles (Partee, 2010; Asher, 2011) that are independent of the presence or absence of examples of association. 2.3 Generalised LF To solve these problems, we generalise LF and replace individual matrices for adjectival meanings by a single lexical function: a tensor for adjectival composition �9/.2 Our proposal is that adjectivenoun composition is carried out by multiplying the tensor 9/ with the vector for the adjective adj, followed by a multiplication with the vector noun, c.f. Figure 3. The product of the tensor 9/ and the vector adj yields a matrix dependent of the adjective that is multiplied with the vecto</context>
</contexts>
<marker>Partee, 2010</marker>
<rawString>Barbara H Partee. 2010. Privative adjectives: subsective plus coercion. B ¨AUERLE, R. et ZIMMERMANN, TE, ´editeurs: Presuppositions and Discourse: Essays Offered to Hans Kamp, pages 273– 285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1201--1211</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="30983" citStr="Socher et al. (2012)" startWordPosition="5050" endWordPosition="5053"> filter on their arguments – are tested experimentally in Grefenstette and Sadrzadeh (2011a) and Grefenstette and Sadrzadeh (2011b). The authors evaluated their models using a similarity task that is similar to the one used by Mitchell &amp; Lapata. However, they use more complex compositional expressions: rather than using compositions of two words (such as a verb and an object), they use simple transitive phrases (subject-verbobject). They show that their instantiations of the categorical model reach better results than the additive and multiplicative models on their transitive similarity task. Socher et al. (2012) present a compositional model based on a recursive neural network. Each 288 node in a syntactic tree is assigned both a vector and a matrix; the vector captures the actual meaning of the constituent, while the matrix models the way it changes the meaning of neighbouring words and phrases. They use an extrinsic evaluation, using the model for a sentiment prediction task. They show that their model gets better results than the additive, multiplicative, and lexical function approach. Other researchers, however, have published different results. Blacoe and Lapata (2012) evaluated the additive and</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1201–1211, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
</authors>
<title>Enriching the knowledge sources used in a maximum entropy part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-2000),</booktitle>
<pages>63--70</pages>
<contexts>
<context position="19088" citStr="Toutanova and Manning, 2000" startWordPosition="3102" endWordPosition="3105">uction of our semantic space. Our semantic space for French was built using the FRWaC corpus (Baroni et al., 2009) – about 1,6 billion words of web texts – which has been tagged with MElt tagger (Denis et al., 2010) and parsed with MaltParser (Nivre et al., 2006a), trained on a dependency-based version of the French treebank (Candito et al., 2010). Our semantic space for English has been built using the UKWaC corpus (Baroni et al., 2009), which consists of about 2 billion words extracted from the web. The corpus has been part of speech tagged and lemmatized with Stanford Part-OfSpeech Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003), and parsed with MaltParser (Nivre et al., 2006b) trained on sections 2-21 of the Wall Street Journal section of the Penn Treebank extended with about 4000 ques285 positive examples random negative examples Wiktionary-based negative examples (mot court, abr´eviation) (importance fortuit, gamme) (jugement favorable, discorde) ‘short word’, ‘abbreviation’ ‘accidental importance’, ‘range’ ‘favorable judgement’, ‘discord’ (ouvrage litt´eraire, essai) (penchant autoritaire, ile) (circonscription administratif, fumier) ‘literary work’, ‘essay’ ‘authoritarian slope’, isle’ ‘</context>
</contexts>
<marker>Toutanova, Manning, 2000</marker>
<rawString>Kristina Toutanova and Christopher D. Manning. 2000. Enriching the knowledge sources used in a maximum entropy part-of-speech tagger. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-2000), pages 63–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL 2003,</booktitle>
<pages>252--259</pages>
<contexts>
<context position="19113" citStr="Toutanova et al., 2003" startWordPosition="3106" endWordPosition="3109"> Our semantic space for French was built using the FRWaC corpus (Baroni et al., 2009) – about 1,6 billion words of web texts – which has been tagged with MElt tagger (Denis et al., 2010) and parsed with MaltParser (Nivre et al., 2006a), trained on a dependency-based version of the French treebank (Candito et al., 2010). Our semantic space for English has been built using the UKWaC corpus (Baroni et al., 2009), which consists of about 2 billion words extracted from the web. The corpus has been part of speech tagged and lemmatized with Stanford Part-OfSpeech Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003), and parsed with MaltParser (Nivre et al., 2006b) trained on sections 2-21 of the Wall Street Journal section of the Penn Treebank extended with about 4000 ques285 positive examples random negative examples Wiktionary-based negative examples (mot court, abr´eviation) (importance fortuit, gamme) (jugement favorable, discorde) ‘short word’, ‘abbreviation’ ‘accidental importance’, ‘range’ ‘favorable judgement’, ‘discord’ (ouvrage litt´eraire, essai) (penchant autoritaire, ile) (circonscription administratif, fumier) ‘literary work’, ‘essay’ ‘authoritarian slope’, isle’ ‘administrative district’,</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of HLT-NAACL 2003, pages 252– 259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of artificial intelligence research,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="1403" citStr="Turney and Pantel, 2010" startWordPosition="214" endWordPosition="217">luation, comparing different composition methods within the distributional framework for the cases of both adjectivenoun and noun-noun composition, making use of a newly developed dataset. Secondly, we propose a novel method for composition, which generalises the approach by Baroni and Zamparelli (2010). The performance of our novel method is also evaluated on our new dataset and proves competitive with the best methods. 1 Introduction In the course of the last two decades, there has been a growing interest in distributional methods for lexical semantics (Landauer and Dumais, 1997; Lin, 1998; Turney and Pantel, 2010). These methods are based on the distributional hypothesis (Harris, 1954), according to which words that appear in the same contexts tend to be similar in meaning. Inspired by Harris’ hypothesis, numerous researchers have developed algorithms that try to capture the semantics of individual words by looking at their distribution in a large corpus. Compared to manual studies common to formal semantics, distributional semantics offers substantially larger coverage since it is able to analyze massive amounts of empirical data. However, it is not trivial to combine the algebraic objects created by </context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of artificial intelligence research, 37(1):141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Van de Cruys</author>
</authors>
<title>A non-negative tensor factorization model for selectional preference induction.</title>
<date>2010</date>
<journal>Natural Language Engineering,</journal>
<volume>16</volume>
<issue>4</issue>
<marker>Van de Cruys, 2010</marker>
<rawString>Tim Van de Cruys. 2010. A non-negative tensor factorization model for selectional preference induction. Natural Language Engineering, 16(4):417–437.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Ioannis Korkontzelos</author>
<author>Francesca Fallucchi</author>
<author>Suresh Manandhar</author>
</authors>
<title>Estimating linear models for compositional distributional semantics.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>1263--1271</pages>
<location>Beijing, China,</location>
<contexts>
<context position="14096" citStr="Zanzotto et al. (2010)" startWordPosition="2283" endWordPosition="2286">e following computations: blood donation = (N x blood) x donation blood donation campaign = (N x blood donation) x campaign and this allows us to avoid the sparse data problem for the LF approach in generating the matrix BLOOD DONATION. Once we have obtained the tensor A , we verify experimentally its relevance to composition, in order to check whether a tensor optimising the equations in Figure 4 would be semantically interesting. 3 Evaluation 3.1 Tasks description In order to evaluate the different composition methods, we constructed test sets for French and English, inspired by the work of Zanzotto et al. (2010) and the SEMEVAL-2013 task evaluating phrasal semantics (Korkontzelos et al., 2013). The task is to make a judgement about the semantic similarity of a short word sequence (an adjectivenoun combination) and a single noun. This is important, as composition models need to be able to treat word sequences of arbitrary length. Formally, the task is presented as: With comp = composition(adj, noun1) Evaluate similarity(comp, noun2) where the ‘composition’ function is carried out by the different composition models. ‘Similarity’ needs to be a binary function, with return values ‘similar’ and ‘non-simi</context>
</contexts>
<marker>Zanzotto, Korkontzelos, Fallucchi, Manandhar, 2010</marker>
<rawString>Fabio Massimo Zanzotto, Ioannis Korkontzelos, Francesca Fallucchi, and Suresh Manandhar. 2010. Estimating linear models for compositional distributional semantics. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1263–1271, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>