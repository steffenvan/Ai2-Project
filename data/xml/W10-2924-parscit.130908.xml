<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000793">
<title confidence="0.96141">
Joint Entity and Relation Extraction using Card-Pyramid Parsing
</title>
<author confidence="0.933326">
Rohit J. Kate and Raymond J. Mooney
</author>
<affiliation confidence="0.995047666666667">
Department of Computer Science
The University of Texas at Austin
1 University Station C0500
</affiliation>
<address confidence="0.643179">
Austin, TX 78712-0233, USA
</address>
<email confidence="0.999617">
{rjkate,mooney}@cs.utexas.edu
</email>
<sectionHeader confidence="0.9974" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999883933333333">
Both entity and relation extraction can
benefit from being performed jointly, al-
lowing each task to correct the errors of
the other. We present a new method for
joint entity and relation extraction using
a graph we call a “card-pyramid.” This
graph compactly encodes all possible en-
tities and relations in a sentence, reducing
the task of their joint extraction to jointly
labeling its nodes. We give an efficient la-
beling algorithm that is analogous to pars-
ing using dynamic programming. Exper-
imental results show improved results for
our joint extraction method compared to a
pipelined approach.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999980619047619">
Information extraction (IE) is the task of extract-
ing structured information from text. The two
most common sub-tasks of IE are extracting enti-
ties (like Person, Location and Organization) and
extracting relations between them (like Work For
which relates a Person and an Organization, Org-
Based In which relates an Organization and a Lo-
cation etc.). Figure 1 shows a sample sentence an-
notated with entities and relations. The applica-
tion domain and requirements of the downstream
tasks usually dictate the type of entities and rela-
tions that an IE system needs to extract.
Most work in IE has concentrated on entity ex-
traction alone (Tjong Kim Sang, 2002; Sang and
Meulder, 2003) or on relation extraction assum-
ing entities are either given or previously extracted
(Bunescu et al., 2005; Zhang et al., 2006; Giuliano
et al., 2007; Qian et al., 2008). However, these
tasks are very closely inter-related. While iden-
tifying correct entities is essential for identifying
relations between them, identifying correct rela-
tions can in turn improve identification of entities.
For example, if the relation Work For is identified
with high confidence by a relation extractor, then
it can enforce identifying its arguments as Person
and Organization, about which the entity extractor
might not have been confident.
A brute force algorithm for finding the most
probable joint extraction will soon become in-
tractable as the number of entities in a sentence
grows. If there are n entities in a sentence, then
there are O(n2) possible relations between them
and if each relation can take l labels then there are
O(ln2) total possibilities, which is intractable even
for small l and n. Hence, an efficient inference
mechanism is needed for joint entity and relation
extraction.
The only work we are aware of for jointly ex-
tracting entities and relations is by Roth &amp; Yih
(2004; 2007). Their method first identifies the pos-
sible entities and relations in a sentence using sep-
arate classifiers which are applied independently
and then computes a most probable consistent
global set of entities and relations using linear pro-
gramming. In this paper, we present a different ap-
proach to joint extraction using a “card-pyramid”
graph. The labeled nodes in this graph compactly
encode the possible entities and relations in a sen-
tence. The task of joint extraction then reduces
to finding the most probable joint assignment to
the nodes in the card-pyramid. We give an ef-
ficient dynamic-programming algorithm for this
task which resembles CYK parsing for context-
free grammars (Jurafsky and Martin, 2008). The
algorithm does a beam search and gives an approx-
imate solution for a finite beam size. A natural
advantage of this approach is that extraction from
a part of the sentence is influenced by extraction
from its subparts and vice-versa, thus leading to a
joint extraction. During extraction from a part of
the sentence it also allows use of features based on
the extraction from its sub-parts, thus leading to a
more integrated extraction. We use Roth &amp; Yih’s
</bodyText>
<page confidence="0.98791">
203
</page>
<note confidence="0.9707005">
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 203–212,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<figure confidence="0.989121">
Work_For
OrgBased_In
Live_In
OrgBased_In
Located_In
Other Organization
John lives in Los Angeles , California and works there for an American company called ABC Inc .
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17
</figure>
<figureCaption confidence="0.995017">
Figure 1: A sentence shown with entities and relations.
</figureCaption>
<figure confidence="0.9544735">
Person Location Location
Live_In
</figure>
<figureCaption confidence="0.999552">
Figure 2: A pyramid built out of playing-cards.
</figureCaption>
<bodyText confidence="0.972871">
(2004; 2007) dataset in our experiments and show
that card-pyramid parsing improves accuracy over
both their approach and a pipelined extractor.
</bodyText>
<sectionHeader confidence="0.9950535" genericHeader="introduction">
2 Card-Pyramid Parsing for Joint
Extraction
</sectionHeader>
<bodyText confidence="0.9998558">
In this section, we first introduce the card-pyramid
structure and describe how it represents entities
and their relations in a sentence. We then describe
an efficient algorithm for doing joint extraction us-
ing this structure.
</bodyText>
<subsectionHeader confidence="0.997639">
2.1 Card-Pyramid Structure
</subsectionHeader>
<bodyText confidence="0.9948183">
We define a binary directed graph we call a card-
pyramid because it looks like a pyramid built out
of playing-cards as shown in Figure 2. A card-
pyramid is a “tree-like” graph with one root, in-
ternal nodes, and leaves, such that if there are
n leaves, then there are exactly n levels with a
decreasing number of nodes from bottom to top,
leaves are at the lowest level (0) and the root is
at the highest level (n − 1) (see Figure 3 for an
example). In addition, every non-leaf node at po-
</bodyText>
<figureCaption confidence="0.999005333333333">
Figure 3: The card-pyramid for the sentence shown in Fig-
ure 1. Levels are shown by the horizontal lines under which
the positions of its nodes are indicated.
</figureCaption>
<bodyText confidence="0.99986775">
sition i in level l is the parent of exactly two nodes
at positions i and i + 1 at level l − 1. Note that
a card-pyramid is not a tree because many of its
nodes have two parents. A useful property of a
card-pyramid is that a non-leaf node at position i
in level l is always the lowest common ancestor of
the leaves at positions i and l + i.
We now describe how entities and relations in a
sentence are easily represented in a card-pyramid.
We assume that in addition to the given entity
types, there is an extra type, Other, indicating that
the entity is of none of the given types. Similarly,
there is an extra relation type, NR, indicating that
its two entity arguments are not related.
Figure 3 shows the card-pyramid corresponding
to the annotated sentence shown in figure 1. To ob-
tain it, first, all entities present in the sentence are
made leaves of the card-pyramid in the same order
as they appear in the sentence. The label of a leaf
is the type of the corresponding entity. The leaf
also stores the range of the indices of its entity’s
words in the sentence. Note that although there is
no overlap between entities in the given example
(nor in the dataset we used for our experiments),
</bodyText>
<figure confidence="0.986989428571429">
Level 4
Work_For
0
Level 3
NR OrgBased_In
1
0
Live_In NR OrgBased_In
0 1
Level 2
2
Live_In
Located_In NR NR
Level 1
1 2
0 3
Location Location Other Organization
0 1 2 3 4
(0−0) (3−4) (6−6) (12−12) (15−16)
L-l 0
Person
</figure>
<page confidence="0.998026">
204
</page>
<bodyText confidence="0.999970111111111">
overlapping entities do not pose a problem. Over-
lapping entities can still be ordered and supplied as
the leaves of the card-pyramid. Next, the relation
between every two entities (leaves) is encoded as
the label of their lowest common ancestor. If two
entities are not related, then the label of their low-
est common ancestor is NR. This way, every non-
leaf node relates exactly two entities: the left-most
and right-most leaves beneath it.
</bodyText>
<subsectionHeader confidence="0.996973">
2.2 Card-Pyramid Parsing
</subsectionHeader>
<bodyText confidence="0.999810604395605">
The task of jointly extracting entities and rela-
tions from a sentence reduces to jointly label-
ing the nodes of a card-pyramid which has all
the candidate entities (i.e. entity boundaries) of
the sentence as its leaves. We call this process
card-pyramid parsing. We assume that all candi-
date entities are given up-front. If needed, candi-
date entities can be either obtained automatically
(Punyakanok and Roth, 2001) or generated using
a simple heuristic, like including all noun-phrase
chunks as candidate entities. Or, in the worst case,
each substring of words in the sentence can be
given as a candidate entity. Liberally including
candidate entities is possible since they can sim-
ply be given the label Other if they are none of the
given types.
In this section we describe our card-pyramid
parsing algorithm whose pseudo-code is shown
in Figure 4. While the process is analogous to
context-free grammar (CFG) parsing, particularly
CYK bottom-up parsing, there are several major
differences. Firstly, in card-pyramid parsing the
structure is already known and the only task is
labeling the nodes, whereas in CFG parsing the
structure is not known in advance. This fact sim-
plifies some aspects of card-pyramid parsing. Sec-
ondly, in CFG parsing the subtrees under a node
do not overlap which simplifies parsing. How-
ever, in card-pyramid parsing there is significant
overlap between the two sub-card-pyramids under
a given node and this overlap needs to be consis-
tently labeled. This could have potentially com-
plicated parsing, but there turns out to be a simple
constant-time method for checking consistency of
the overlap. Thirdly, while CFG parsing parses the
words in a sentence, here we are parsing candi-
date entities. Finally, as described below, in card-
pyramid parsing, a production at a non-leaf node
relates the left-most and right-most leaves beneath
it, while in CFG parsing a production at a non-leaf
node relates its immediate children which could be
other non-leaf nodes.
Parsing requires specifying a grammar for the
card-pyramid. The productions in this grammar
are of two types. For leaf nodes, the produc-
tions are of the form entityLabel —* ce where ce,
which stands for candidate entity, is the only ter-
minal symbol in the grammar. We call these pro-
ductions entity productions. For non-leaf nodes,
the productions are of the form relationLabel —*
entityLabel1 entityLabel2. We call these produc-
tions relation productions. Note that their right-
hand-side (RHS) non-terminals are entity labels
and not other relation labels. From a training
set of labeled sentences, the corresponding card-
pyramids can be constructed using the procedure
described in the previous section. From these
card-pyramids, the entity productions are obtained
by simply reading off the labels of the leaves. A
relation productions is obtained from each non-
leaf node by making the node’s label the produc-
tion’s left-hand-side (LHS) non-terminal and mak-
ing the labels of its left-most and right-most leaves
the production’s RHS non-terminals. For the ex-
ample shown in Figure 3, some of the productions
are Work For —* Person Organization, NR —* Per-
son Other, OrgBased In —* Loc Org, Person —* ce,
Location —* ce etc. Note that there could be two
separate productions like Work For —* Person Or-
ganization and Work For —* Organization Person
based on the order in which the entities are found
in a sentence. For the relations which take argu-
ments of the same type, like Kill(Person,Person),
two productions are used with different LHS non-
terminals (Kill and Kill reverse) to distinguish be-
tween the argument order of the entities.
The parsing algorithm needs a classifier for ev-
ery entity production which gives the probabil-
ity of a candidate entity being of the type given
in the production’s LHS. In the pseudo-code,
this classifier is given by the function: entity-
classifier(production, sentence, range). The func-
tion range(r) represents the boundaries or the
range of the word indices for the rth candidate
entity. Similarly, we assume that a classifier is
given for every relation production which gives
the probability that its two RHS entities are re-
lated by its LHS relation. In the pseudo-code it is
the function: relation-classifier(production, sen-
tence, range1, range2, sub-card-pyramid1, sub-
card-pyramid2), where range1 and range2 are the
</bodyText>
<page confidence="0.992268">
205
</page>
<bodyText confidence="0.999987577464789">
ranges of the word indices of the two entities
and sub-card-pyramid1 and sub-card-pyramid2
are the sub-card-pyramids rooted at its two chil-
dren. Thus, along with the two entities and the
words in the sentence, information from these sub-
card-pyramids is also used in deciding the relation
at a node. In the next section, we further spec-
ify these entity and relation classifiers and explain
how they are trained. We note that this use of
multiple classifiers to determine the most probable
parse is similar to the method used in the KRISP
semantic parser (Kate and Mooney, 2006).
Given the candidate entities in a sentence, the
grammar, and the entity and relation classifiers,
the card-pyramid parsing algorithm tries to find
the most probable joint-labeling of all of its nodes,
and thus jointly extracts entities and their rela-
tions. The parsing algorithm does a beam search
and maintains a beam at each node of the card-
pyramid. A node is represented by l[i][j] in the
pseudo-code which stands for the node in the jth
position in the ith level. Note that at level i, the
nodes range from l[i][0] to l[i][n − i − 1], where n
is the number of leaves. The beam at each node is
a queue of items we call beam elements. At leaf
nodes, a beam element simply stores a possible
entity label with its corresponding probability. At
non-leaf nodes, a beam element contains a possi-
ble joint assignment of labels to all the nodes in
the sub-card-pyramid rooted at that node with its
probability. This is efficiently maintained through
indices to the beam elements of its children nodes.
The parsing proceeds as follows. First, the en-
tity classifiers are used to fill the beams at the leaf
nodes. The add(beam, beam-element) function
adds the beam element to the beam while main-
taining its maximum beam-width size and sorted
order based on the probabilities. Next, the beams
of the non-leaf nodes are filled in a bottom-up
manner. At any node, the beams of its children
nodes are considered and every combination of
their beam elements are tried. To be considered
further, the two possible sub-card-pyramids en-
coded by the two beam elements must have a con-
sistent overlap. This is easily enforced by check-
ing that its left child’s right child’s beam element
is same as its right child’s left child’s beam ele-
ment. If this condition is satisfied, then those re-
lation productions are considered which have the
left-most leaf of the left child and right-most leaf
of the right child as its RHS non-terminals.&apos; For
every such production in the grammar, 2 the prob-
ability of the relation is determined using the re-
lation classifier. This probability is then multi-
plied by the probabilities of the children sub-card-
pyramids. But, because of the overlap between the
two children, a probability mass gets multiplied
twice. Hence the probability of the overlap sub-
card-pyramid is then suitably divided. Finally, the
estimated most-probable labeling is obtained from
the top beam element of the root node.
We note that this algorithm may not find the
optimal solution but only an approximate solu-
tion owing to a limited beam size, this is unlike
probabilistic CFG parsing algorithms in which the
optimal solution is found. A limitless beam size
will find the optimal solution but will reduce the
algorithm to a computationally intractable brute
force search. The parsing algorithm with a fi-
nite beam size keeps the search computationally
tractable while allowing a joint labelling.
</bodyText>
<sectionHeader confidence="0.9937525" genericHeader="method">
3 Classifiers for Entity and Relation
Extraction
</sectionHeader>
<bodyText confidence="0.8950535">
The card-pyramid parsing described in the previ-
ous section requires classifiers for each of the en-
tity and relation productions. In this section, we
describe the classifiers we used in our experiments
and how they were trained.
We use a support vector machine (SVM) (Cris-
tianini and Shawe-Taylor, 2000) classifier for each
of the entity productions in the grammar. An entity
classifier gets as input a sentence and a candidate
entity indicated by the range of the indices of its
words. It outputs the probability that the candi-
date entity is of the respective entity type. Prob-
abilities for the SVM outputs are computed using
the method by Platt (1999). We use all possible
word subsequences of the candidate entity words
as implicit features using a word-subsequence ker-
nel (Lodhi et al., 2002). In addition, we use
the following standard entity extraction features:
the part-of-speech (POS) tag sequence of the can-
didate entity words, two words before and after
the candidate entity and their POS tags, whether
any or all candidate entity words are capitalized,
&apos;These are stored in the beam elements.
2Note that this step enforces the consistency constraint of
Roth and Yih (Roth and Yih, 2004; Roth and Yih, 2007) that
a relation can only be between the entities of specific types.
The grammar in our approach inherently enforces this con-
straint.
</bodyText>
<page confidence="0.982069">
206
</page>
<table confidence="0.661442925925926">
function Card-Pyramid-Parsing(Sentence,Grammar,entity-classifiers,relation-classifiers)
n = number of candidate entities in S
// Let range(r) represent the range of the indices of the words for the rth candidate entity.
// Let l[i][j] represent the jth node at ith level in the card-pyramid.
// For leaves
// A beam element at a leaf node is (label,probability).
for j = 0 to n // for every leaf
for each entityLabel — candidate entity E Grammar
prob = entity-classifier(entityLabel — candidate entity, S, range(j))
add(l[0][j].beam, (entityLabel,prob))
// For non-leaf nodes
// A beam element at a non-leaf node is (label,probability,leftIndex,rightIndex,leftMostLeaf,rightMostLeaf)
// where leftIndex and rightIndex are the indices in the beams of the left and right children respectively.
for i = 1 to n // for every level above the leaves
for j = 0 to n — i — 1 // for every position at a level
// for each combination of beam elements of the two children
for each f E l[i — 1][j].beam and g E l[i — 1][j + 1].beam
// the overlapped part must be same (overlap happens for i &gt; 1)
if (i == 1||f.rightIndex == g.leftIndex)
for each relationLabel — f.leftMostLeaf g.rightMostLeaf EGrammar
// probability of that relation between the left-most and right-most leaf under the node
prob = relation-classifier(relationLabel — f.leftMostLeaf g.rightMostLeaf, S, range(i), range(i + j), f, g);
prob *= f.probability * g.probability // multiply probabilities of the children sub-card-pyramids
// divide by the common probability that got multiplied twice
if (i &gt; 1) prob/=l[i — 2][j + 1].beam[f.rightIndex].probability
add(l[i][j].beam, (relationLabel, prob, index of f, index of g, f.leftMostLeaf,g.rightMostLeaf)
return the labels starting from the first beam element of the root i.e. l[n][0].beam[0]
</table>
<figureCaption confidence="0.99939">
Figure 4: Card-Pyramid Parsing Algorithm.
</figureCaption>
<bodyText confidence="0.99992105">
whether any or all words are found in a list of en-
tity names, whether any word has sufffix “ment”
or “ing”, and finally the alphanumeric pattern of
characters (Collins, 2002) of the last candidate
entity word obtained by replacing each charac-
ter by its character type (lowercase, uppercase or
numeric) and collapsing any consecutive repeti-
tion (for example, the alphanumeric pattern for
CoNLL2010 will be AaA0). The full kernel is
computed by adding the word-subsequence kernel
and the dot-product of all these features, exploit-
ing the convolution property of kernels.
We also use an SVM classifier for each of the
relation productions in the grammar which out-
puts the probability that the relation holds between
the two entities. A relation classifier is applied
at an internal node of a card-pyramid. It takes
the input in two parts. The first part is the sen-
tence and the range of the word indices of its two
entities l and r which are the left-most and the
right-most leaves under it respectively. The sec-
ond part consists of the sub-card-pyramids rooted
at the node’s two children which represent a pos-
sible entity and relation labeling for all the nodes
underneath. In general, any information from the
sub-card-pyramids could be used in the classifier.
We use the following information: pairs of rela-
tions that exist between l and b and between b and
r for every entity (leaf) b that exists between the
two entities l and r. For example, in figure 3,
the relation classifier at the root node which re-
lates Person(0-0) and Organization (15-16) will
take three pairs of relations as the information
from the two sub-card-pyramids of its children:
“Live In—OrgBased In” (with Location(3-4) as
the in-between entity), “Live In—OrgBased In”
(with Location(6-6) as the in-between entity) and
“NR—NR” (with Other(12-12) as the in-between
entity). This information tells how the two enti-
ties are related to the entities present in between
them. This can affect the relation between the two
entities, for example, if the sentence mentions that
a person lives at a location and also mentions that
an organization is based at that location then that
person is likely to work at that organization. Note
that this information can not be incorporated in a
pipelined approach in which each relation is de-
termined independently. It is also not possible to
incorporate this in the linear programming method
presented in (Roth and Yih, 2004; Roth and Yih,
2007) because that method computes the probabil-
ities of all the relations independently before find-
ing the optimal solution through linear program-
ming. It would also not help to add hard con-
straints to their linear program relating the rela-
tions because they need not always hold.
We add the kernels for each part of the input to
compute the final kernel for the SVM classifiers.
The kernel for the second part of the input is com-
puted by simply counting the number of common
</bodyText>
<page confidence="0.994532">
207
</page>
<bodyText confidence="0.991505375">
pairs of relations between two examples thus im-
plicitly considering every pair of relation (as de-
scribed in the last paragraph) as a feature. For the
first part of the input, we use word-subsequence
kernels which have shown to be effective for re-
lation extraction (Bunescu and Mooney, 2005b).
We compute the kernel as the sum of the word-
subsequence kernels between: the words between
the two entities (between pattern), k (a parame-
ter) words before the first entity (before pattern),
k words after the second entity (after pattern) and
the words from the beginning of the first entity to
the end of the second entity (between-and-entity
pattern). The before, between and after patterns
have been found useful in previous work (Bunescu
and Mooney, 2005b; Giuliano et al., 2007). Some-
times the words of the entities can indicate the re-
lations they are in, hence we also use the between-
and-entity pattern. When a relation classifier is
used at a node, the labels of the leaves beneath it
are already known, so we replace candidate entity
words that are in the between and between-and-
entity3 patterns by their entity labels. This pro-
vides useful information to the relation classifier
and also makes these patterns less sparse for train-
ing.
Given training data of sentences annotated with
entities and relations, the positive and negative ex-
amples for training the entity and relation clas-
sifiers are collected in the following way. First,
the corresponding card-pyramids are obtained for
each of the training sentences as described in sec-
tion 2.1. For every entity production in a card-
pyramid, a positive example is collected for its
corresponding classifier as the sentence and the
range of the entity’s word indices. Similarly, for
every relation production in a card-pyramid, a pos-
itive example is collected for its corresponding
classifier as the sentence, the ranges of the two
entities’ word indices and the sub-card-pyramids
rooted at its two children. The positive examples
of a production become the negative examples for
all those productions which have the same right-
hand-sides but different left-hand-sides. We found
that for NR productions, training separate classi-
fiers is harmful because it has the unwanted side-
effect of preferring one label assignment of enti-
ties over another due to the fact that these pro-
ductions gave different probabilities for the “not-
related” relation between the entities. To avoid
3Except for the two entities at the ends
this, we found that it suffices if all these classi-
fiers for NR productions always return 0.5 as the
probability. This ensures that a real relation will
be preferred over NR if and only if its probability
is greater than 0.5, otherwise nothing will change.
</bodyText>
<sectionHeader confidence="0.999569" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999955666666667">
We conducted experiments to compare our card-
pyramid parsing approach for joint entity and re-
lation extraction to a pipelined approach.
</bodyText>
<subsectionHeader confidence="0.967474">
4.1 Methodology
</subsectionHeader>
<bodyText confidence="0.999990027777778">
We used the dataset4 created by Roth &amp; Yih (2004;
2007) that was also used by Giuliano et el. (2007).
The sentences in this data were taken from the
TREC corpus and annotated with entities and re-
lations. As in the previous work with this dataset,
in order to observe the interaction between enti-
ties and relations, our experiments used only the
1437 sentences that include at least one relation.
The boundaries of the entities are already supplied
by this dataset. There are three types of entities:
Person (1685), Location (1968) and Organization
(978), in addition there is a fourth type Other
(705), which indicates that the candidate entity is
none of the three types. There are five types of re-
lations: Located In (406) indicates that one Loca-
tion is located inside another Location, Work For
(394) indicates that a Person works for an Orga-
nization, OrgBased In (451) indicates that an Or-
ganization is based in a Location, Live In (521)
indicates that a Person lives at a Location and Kill
(268) indicates that a Person killed another Per-
son. There are 17007 pairs of entities that are not
related by any of the five relations and hence have
the NR relation between them which thus signifi-
cantly outnumbers other relations.
Our implementation uses the LIBSVM5 soft-
ware for SVM classifiers. We kept the noise
penalty parameter of SVM very high (100) as-
suming there is little noise in our data. For the
word-subsequence kernel, we set 5 as the max-
imum length of a subsequence and 0.25 as the
penalty parameter for subsequence gaps (Lodhi et
al., 2002). We used k = 5 words for before and
after patterns for the relation classifiers. These pa-
rameter values were determined through pilot ex-
periments on a subset of the data. We used a beam
</bodyText>
<footnote confidence="0.9988965">
4Available at: http://l2r.cs.uiuc.edu/
-cogcomp/Data/ER/conll04.corp
5http://www.csie.ntu.edu.tw/-cjlin/
libsvm/
</footnote>
<page confidence="0.98709">
208
</page>
<table confidence="0.999914166666667">
Entity Person Location Organization
Approach Rec Pre F Rec Pre F Rec Pre F
Pipeline 93.6 92.0 92.8 94.0 90.3 92.1 87.9 90.6 89.2
Card-pyramid 94.2 92.1 93.2 94.2 90.8 92.4† 88.7 90.5 89.5
RY07 Pipeline 89.1 88.7 88.6 88.1 89.8 88.9 71.4 89.3 78.7
RY07 Joint 89.5 89.1 89.0 88.7 89.7 89.1 72.0 89.5 79.2
Relation Located In Work For OrgBased In Live In Kill
Approach Rec Pre F Rec Pre F Rec Pre F Rec Pre F Rec Pre F
Pipeline 57.0 71.5 62.3 66.0 74.1 69.7 60.2 70.6 64.6 56.6 68.1 61.7 61.2 91.1 73.1
Card-pyramid 56.7 67.5 58.3 68.3 73.5 70.7 64.1 66.2 64.7 60.1 66.4 62.9† 64.1 91.6 75.2
RY07 Pipeline 56.4 52.5 50.7 44.4 60.8 51.2 42.1 77.8 54.3 50.0 58.9 53.5 81.5 73.0 76.5
RY07 Joint 55.7 53.9 51.3 42.3 72.0 53.1 41.6 79.8 54.3 49.0 59.1 53.0 81.5 77.5 79.0†
</table>
<tableCaption confidence="0.85729175">
Table 1: Results of five-fold cross-validation for entity and relation extraction using pipelined and joint extraction. Boldface
indicates statistical significance (p &lt; 0.1 using paired t-test) when compared to the corresponding value in the other row
grouped with it. Symbol † indicates statistical significance with p &lt; 0.05. Only statistical significance for F-measures are
indicated. RY07 stands for the “E +-+ R” model in (Roth and Yih, 2007).
</tableCaption>
<bodyText confidence="0.999701384615385">
size of 5 in our card-pyramid parsing algorithm at
which the performance plateaus.
We note that by using a beam size of 1 and by
not using the second part of input for relation clas-
sifiers as described in section 3 (i.e. by ignoring
the relations at the lower levels), the card-parsing
algorithm reduces to the traditional pipelined ap-
proach because then only the best entity label for
each candidate entity is considered for relation ex-
traction. Hence, in our experiments we simply use
this setting as our pipelined approach.
We performed a 5-fold cross-validation to com-
pare with the previous work with this dataset by
Roth &amp; Yih (2007), however, our folds are not
same as their folds which were not available. We
also note that our entity and relation classifiers are
different from theirs. They experimented with sev-
eral models to see the effect of joint inference on
them, we compare with the results they obtained
with their most sophisticated model which they
denote by “E H R”. For every entity type and
relation type, we measured Precision (percentage
of output labels correct), Recall (percentage of
gold-standard labels correctly identified) and F-
measure (the harmonic mean of Precision and Re-
call).
</bodyText>
<subsectionHeader confidence="0.927219">
4.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.964976888888889">
Table 1 shows the results of entity and relation ex-
traction. The statistical significance is shown only
for F-measures. We first note that except for the
Kill relation, all the results of our pipelined ap-
proach are far better than the pipelined approach
of (Roth and Yih, 2007), for both entities and rela-
tions. This shows that the entity and relation clas-
sifiers we used are better that the ones they used.
These strong baselines also set a higher ceiling for
our joint extraction method to improve upon.
The entity extraction results show that on all
the entities the card-pyramid parsing approach for
joint extraction obtains a better performance than
the pipelined approach. This shows that entity
extraction benefits when it is jointly done with
relation extraction. Joint extraction using card-
pyramid parsing also gave improvement in perfor-
mance on all the relations except the Located In
relation.6
The results thus show that entity and relation ex-
traction correct some of each other’s errors when
jointly performed. Roth &amp; Yih (2004; 2007) re-
port that 5% to 25% of the relation predictions
of their pipeline models were incoherent, meaning
that the types of the entities related by the relations
are not of the required types. Their joint inference
method corrects these mistakes, hence a part of the
improvement their joint model obtains over their
pipeline model is due to the fact that their pipeline
model can output incoherent relations. Since the
types of the entities a relation’s arguments should
6Through error analysis we found that the drop in the
performance for this relation was mainly because of an un-
usual sentence in the data which had twenty Location entities
in it separated by commas. After incorrectly extracting Lo-
cated In relation between the Location entities at the lower
levels, these erroneous extractions would be taken into ac-
count at higher levels in the card-pyramid, leading to extract-
ing many more incorrect instances of this relation while do-
ing joint extraction. Since this is the only such sentence in the
data, when it is present in the test set during cross-validation,
the joint method never gets a chance to learn not to make
these mistakes. The drop occurs in only that one fold and
hence the overall drop is not found as statistically significant
despite being relatively large.
</bodyText>
<page confidence="0.997066">
209
</page>
<bodyText confidence="0.9998978">
take are known, we believe that filtering out the
incoherent relation predictions of their pipeline
model can improve its precision without hurting
the recall. On the other hand our pipelined ap-
proach never outputs incoherent relations because
the grammar of relation productions enforce that
the relations are always between entities of the re-
quired types. Thus the improvement obtained by
our joint extraction method over our pipelined ap-
proach is always non-trivial.
</bodyText>
<sectionHeader confidence="0.999915" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999972509433963">
To our knowledge, Roth &amp; Yih (2004; 2007) have
done the only other work on joint entity and re-
lation extraction. Their method employs inde-
pendent entity and relation classifiers whose out-
puts are used to compute a most probable consis-
tent global set of entities and relations using lin-
ear programming. One key advantage of our card-
pyramid method over their method is that the clas-
sifiers can take the output of other classifiers under
its node as input features during parsing. This is
not possible in their approach because all classi-
fier outputs are determined before they are passed
to the linear program solver. Thus our approach
is more integrated and allows greater interaction
between dependent extraction decisions.
Miller et al. (2000) adapt a probabilistic
context-free parser for information extraction by
augmenting syntactic labels with entity and rela-
tion labels. They thus do a joint syntactic parsing
and information extraction using a fixed template.
However, as designed, such a CFG approach can-
not handle the cases when an entity is involved
in multiple relations and when the relations criss-
cross each other in the sentence, as in Figure 1.
These cases occur frequently in the dataset we
used in our experiments and many other relation-
extraction tasks.
Giuliano et al. (2007) thoroughly evaluate the
effect of entity extraction on relation extraction us-
ing the dataset used in our experiments. However,
they employ a pipeline architecture and did not in-
vestigate joint relation and entity extraction. Carl-
son et al. (2009) present a method to simultane-
ously do semi-supervised training of entity and re-
lation classifiers. However, their coupling method
is meant to take advantage of the available unsu-
pervised data and does not do joint inference.
Riedel et al. (2009) present an approach for ex-
tracting bio-molecular events and their arguments
using Markov Logic. Such an approach could
also be adapted for jointly extracting entities and
their relations, however, this would restrict entity
and relation extraction to the same machine learn-
ing method that is used with Markov Logic. For
example, one would not be able to use kernel-
based SVM for relation extraction, which has been
very successful at this task, because Markov Logic
does not support kernel-based machine learning.
In contrast, our joint approach is independent of
the individual machine learning methods for en-
tity and relation extraction, and hence allows use
of the best machine learning methods available for
each of them.
</bodyText>
<sectionHeader confidence="0.999901" genericHeader="method">
6 Future Work
</sectionHeader>
<bodyText confidence="0.999973628571428">
There are several possible directions for extend-
ing the current approach. The card-pyramid struc-
ture could be used to perform other language-
processing tasks jointly with entity and rela-
tion extraction. For example, co-reference res-
olution between two entities within a sentence
can be easily incorporated in card-pyramid pars-
ing by introducing a production like coref —*
Person Person, indicating that the two person
entities are the same.
In this work, and in most previous work, re-
lations are always considered between two enti-
ties. However, there could be relations between
more than two entities. In that case, it should
be possible to binarize those relations and then
use card-pyramid parsing. If the relations are be-
tween relations instead of between entities, then
card-pyramid parsing can handle it by considering
the labels of the immediate children as RHS non-
terminals instead of the labels of the left-most and
the right-most leaves beneath it. Thus, it would
be interesting to apply card-pyramid parsing to ex-
tract higher-order relations (such as causal or tem-
poral relations).
Given the regular graph structure of the card-
pyramid, it would be interesting to investigate
whether it can be modeled using a probabilistic
graphical model (Koller and Friedman, 2009). In
that case, instead of using multiple probabilis-
tic classifiers, one could employ a single jointly-
trained probabilistic model, which is theoretically
more appealing and might give better results.
Finally, we note that a better relation classifier
could be used in the current approach which makes
more use of linguistic information. For example,
</bodyText>
<page confidence="0.99333">
210
</page>
<bodyText confidence="0.98715075">
by using dependency-based kernels (Bunescu and
Mooney, 2005a; Kate, 2008) or syntactic kernels
(Qian et al., 2008; Moschitti, 2009) or by includ-
ing the word categories and their POS tags in the
subsequences. Also, it will be interesting to see if
a kernel that computes the similarity between sub-
card-pyramids could be developed and used for re-
lation classification.
</bodyText>
<sectionHeader confidence="0.999205" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999949545454545">
We introduced a card-pyramid graph structure and
presented a new method for jointly extracting enti-
ties and their relations from a sentence using it. A
card-pyramid compactly encodes the entities and
relations in a sentence thus reducing the joint ex-
traction task to jointly labeling its nodes. We pre-
sented an efficient parsing algorithm for jointly
labeling a card-pyramid using dynamic program-
ming and beam search. The experiments demon-
strated the benefit of our joint extraction method
over a pipelined approach.
</bodyText>
<sectionHeader confidence="0.998386" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996738333333333">
This research was funded by Air Force Contract
FA8750-09-C-0172 under the DARPA Machine
Reading Program.
</bodyText>
<sectionHeader confidence="0.999068" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999777176470588">
Razvan C. Bunescu and Raymond J. Mooney. 2005a. A
shortest path dependency kernel for relation extraction. In
Proc. of the Human Language Technology Conf. and Conf.
on Empirical Methods in Natural Language Processing
(HLT/EMNLP-05), pages 724–731, Vancouver, BC, Oc-
tober.
Razvan C. Bunescu and Raymond J. Mooney. 2005b. Sub-
sequence kernels for relation extraction. In Y. Weiss,
B. Sch¨olkopf, and J. Platt, editors, Advances in Neural In-
formation Processing Systems 18, Vancouver, BC.
Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Edward M. Mar-
cotte, Raymond J. Mooney, Arun Kumar Ramani, and
Yuk Wah Wong. 2005. Comparative experiments on
learning information extractors for proteins and their inter-
actions. Artificial Intelligence in Medicine (special issue
on Summarization and Information Extraction from Med-
ical Documents), 33(2):139–155.
Andrew Carlson, Justin Betteridge, Estevam R. Hruschka,
and Tom M. Mitchell. 2009. Coupling semi-supervised
learning of categories and relations. In SemiSupLearn
’09: Proceedings of the NAACL HLT 2009 Workshop on
Semi-Supervised Learning for Natural Language Process-
ing, pages 1–9, Boulder, Colorado.
Michael Collins. 2002. Ranking algorithms for named-entity
extraction: Boosting and the voted perceptron. In Proc. of
the 40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2002), pages 489–496, Philadel-
phia, PA.
Nello Cristianini and John Shawe-Taylor. 2000. An Introduc-
tion to Support Vector Machines and Other Kernel-based
Learning Methods. Cambridge University Press.
Claudio Giuliano, Alberto Lavelli, and Lorenza Romano.
2007. Relation extraction and the influence of automatic
named-entity recognition. ACM Trans. Speech Lang. Pro-
cess., 5(1):1–26.
D. Jurafsky and J. H. Martin. 2008. Speech and Language
Processing: An Introduction to Natural Lan guage Pro-
cessing, Computational Linguistics, and Speech Recogni-
tion. Prentice Hall, Upper Saddle River, NJ.
Rohit J. Kate and Raymond J. Mooney. 2006. Using string-
kernels for learning semantic parsers. In Proc. of the 21st
Intl. Conf. on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguistics
(COLING/ACL-06), pages 913–920, Sydney, Australia,
July.
Rohit J. Kate. 2008. A dependency-based word subsequence
kernel. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP 2008),
pages 400–409, Waikiki,Honolulu,Hawaii, October.
Daphne Koller and Nir Friedman. 2009. Probabilistic
Graphical Models: Principles and Techniques. The MIT
Press, Cambridge, MA.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Chris Watkins. 2002. Text classification
using string kernels. Journal of Machine Learning Re-
search, 2:419–444.
Scott Miller, Heidi Fox, Lance A. Ramshaw, and Ralph M.
Weischedel. 2000. A novel use of statistical parsing to
extract information from text. In Proc. of the Meeting of
the N. American Association for Computational Linguis-
tics, pages 226–233, Seattle, Washington.
Alessandro Moschitti. 2009. Syntactic and semantic ker-
nels for short text pair categorization. In Proceedings of
the 12th Conference of the European Chapter of the ACL
(EACL 2009), pages 576–584, Athens,Greece, March.
John C. Platt. 1999. Probabilistic outputs for support vec-
tor machines and comparisons to regularized likelihood
methods. In Alexander J. Smola, Peter Bartlett, Bern-
hard Sch¨olkopf, and Dale Schuurmans, editors, Advances
in Large Margin Classifiers, pages 185–208. MIT Press.
Vasin Punyakanok and Dan Roth. 2001. The use of classi-
fiers in sequential inference. In Advances in Neural Infor-
mation Processing Systems 13.
Longhua Qian, Guodong Zhou, Fang Kong, Qiaoming Zhu,
and Peide Qian. 2008. Exploiting constituent depen-
dencies for tree kernel-based semantic relation extraction.
In Proceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008), pages 697–704,
Manchester, UK, August.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi, and
Jun’ichi Tsujii. 2009. A Markov logic approach to
bio-molecular event extraction. In Proceedings of the
BioNLP 2009 Workshop Companion Volume for Shared
Task, pages 41–49, Boulder, Colorado, June. Association
for Computational Linguistics.
</reference>
<page confidence="0.977606">
211
</page>
<reference confidence="0.999845125">
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Proc. of 8th Conf. on Computational Natural Language
Learning (CoNLL-2004), pages 1–8, Boston, MA.
D. Roth and W. Yih. 2007. Global inference for entity and
relation identification via a linear programming formula-
tion. In L. Getoor and B. Taskar, editors, Introduction to
Statistical Relational Learning, pages 553–580. The MIT
Press, Cambridge, MA.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Proc. of
7th Conf. on Computational Natural Language Learning
(CoNLL-2003), Edmonton, Canada.
Erik F. Tjong Kim Sang. 2002. Introduction to the CoNLL-
2002 shared task: Language-independent named entity
recognition. In Proceedings of CoNLL-2002, pages 155–
158. Taipei, Taiwan.
Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou. 2006.
A composite kernel to extract relations between entities
with both flat and structured features. In Proc. of the 21st
Intl. Conf. on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguistics
(COLING/ACL-06), Sydney, Australia, July.
</reference>
<page confidence="0.998493">
212
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.793747">
<title confidence="0.999256">Joint Entity and Relation Extraction using Card-Pyramid Parsing</title>
<author confidence="0.999832">Rohit J Kate</author>
<author confidence="0.999832">J Raymond</author>
<affiliation confidence="0.999232666666667">Department of Computer The University of Texas at 1 University Station</affiliation>
<address confidence="0.838498">Austin, TX 78712-0233,</address>
<abstract confidence="0.996794625">Both entity and relation extraction can benefit from being performed jointly, allowing each task to correct the errors of the other. We present a new method for joint entity and relation extraction using a graph we call a “card-pyramid.” This graph compactly encodes all possible entities and relations in a sentence, reducing the task of their joint extraction to jointly labeling its nodes. We give an efficient labeling algorithm that is analogous to parsing using dynamic programming. Experimental results show improved results for our joint extraction method compared to a pipelined approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction.</title>
<date>2005</date>
<booktitle>In Proc. of the Human Language Technology Conf. and Conf. on Empirical Methods in Natural Language Processing (HLT/EMNLP-05),</booktitle>
<pages>724--731</pages>
<location>Vancouver, BC,</location>
<contexts>
<context position="21674" citStr="Bunescu and Mooney, 2005" startWordPosition="3593" endWordPosition="3596">mming. It would also not help to add hard constraints to their linear program relating the relations because they need not always hold. We add the kernels for each part of the input to compute the final kernel for the SVM classifiers. The kernel for the second part of the input is computed by simply counting the number of common 207 pairs of relations between two examples thus implicitly considering every pair of relation (as described in the last paragraph) as a feature. For the first part of the input, we use word-subsequence kernels which have shown to be effective for relation extraction (Bunescu and Mooney, 2005b). We compute the kernel as the sum of the wordsubsequence kernels between: the words between the two entities (between pattern), k (a parameter) words before the first entity (before pattern), k words after the second entity (after pattern) and the words from the beginning of the first entity to the end of the second entity (between-and-entity pattern). The before, between and after patterns have been found useful in previous work (Bunescu and Mooney, 2005b; Giuliano et al., 2007). Sometimes the words of the entities can indicate the relations they are in, hence we also use the betweenand-en</context>
<context position="35612" citStr="Bunescu and Mooney, 2005" startWordPosition="5891" endWordPosition="5894">al or temporal relations). Given the regular graph structure of the cardpyramid, it would be interesting to investigate whether it can be modeled using a probabilistic graphical model (Koller and Friedman, 2009). In that case, instead of using multiple probabilistic classifiers, one could employ a single jointlytrained probabilistic model, which is theoretically more appealing and might give better results. Finally, we note that a better relation classifier could be used in the current approach which makes more use of linguistic information. For example, 210 by using dependency-based kernels (Bunescu and Mooney, 2005a; Kate, 2008) or syntactic kernels (Qian et al., 2008; Moschitti, 2009) or by including the word categories and their POS tags in the subsequences. Also, it will be interesting to see if a kernel that computes the similarity between subcard-pyramids could be developed and used for relation classification. 7 Conclusions We introduced a card-pyramid graph structure and presented a new method for jointly extracting entities and their relations from a sentence using it. A card-pyramid compactly encodes the entities and relations in a sentence thus reducing the joint extraction task to jointly lab</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2005a. A shortest path dependency kernel for relation extraction. In Proc. of the Human Language Technology Conf. and Conf. on Empirical Methods in Natural Language Processing (HLT/EMNLP-05), pages 724–731, Vancouver, BC, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>Subsequence kernels for relation extraction.</title>
<date>2005</date>
<booktitle>Advances in Neural Information Processing Systems 18,</booktitle>
<editor>In Y. Weiss, B. Sch¨olkopf, and J. Platt, editors,</editor>
<location>Vancouver, BC.</location>
<contexts>
<context position="21674" citStr="Bunescu and Mooney, 2005" startWordPosition="3593" endWordPosition="3596">mming. It would also not help to add hard constraints to their linear program relating the relations because they need not always hold. We add the kernels for each part of the input to compute the final kernel for the SVM classifiers. The kernel for the second part of the input is computed by simply counting the number of common 207 pairs of relations between two examples thus implicitly considering every pair of relation (as described in the last paragraph) as a feature. For the first part of the input, we use word-subsequence kernels which have shown to be effective for relation extraction (Bunescu and Mooney, 2005b). We compute the kernel as the sum of the wordsubsequence kernels between: the words between the two entities (between pattern), k (a parameter) words before the first entity (before pattern), k words after the second entity (after pattern) and the words from the beginning of the first entity to the end of the second entity (between-and-entity pattern). The before, between and after patterns have been found useful in previous work (Bunescu and Mooney, 2005b; Giuliano et al., 2007). Sometimes the words of the entities can indicate the relations they are in, hence we also use the betweenand-en</context>
<context position="35612" citStr="Bunescu and Mooney, 2005" startWordPosition="5891" endWordPosition="5894">al or temporal relations). Given the regular graph structure of the cardpyramid, it would be interesting to investigate whether it can be modeled using a probabilistic graphical model (Koller and Friedman, 2009). In that case, instead of using multiple probabilistic classifiers, one could employ a single jointlytrained probabilistic model, which is theoretically more appealing and might give better results. Finally, we note that a better relation classifier could be used in the current approach which makes more use of linguistic information. For example, 210 by using dependency-based kernels (Bunescu and Mooney, 2005a; Kate, 2008) or syntactic kernels (Qian et al., 2008; Moschitti, 2009) or by including the word categories and their POS tags in the subsequences. Also, it will be interesting to see if a kernel that computes the similarity between subcard-pyramids could be developed and used for relation classification. 7 Conclusions We introduced a card-pyramid graph structure and presented a new method for jointly extracting entities and their relations from a sentence using it. A card-pyramid compactly encodes the entities and relations in a sentence thus reducing the joint extraction task to jointly lab</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2005b. Subsequence kernels for relation extraction. In Y. Weiss, B. Sch¨olkopf, and J. Platt, editors, Advances in Neural Information Processing Systems 18, Vancouver, BC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Ruifang Ge</author>
<author>Rohit J Kate</author>
<author>Edward M Marcotte</author>
<author>Raymond J Mooney</author>
<author>Arun Kumar Ramani</author>
<author>Yuk Wah Wong</author>
</authors>
<title>Comparative experiments on learning information extractors for proteins and their interactions.</title>
<date>2005</date>
<booktitle>Artificial Intelligence in Medicine (special issue on Summarization and Information Extraction from Medical Documents),</booktitle>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1657" citStr="Bunescu et al., 2005" startWordPosition="260" endWordPosition="263">tion and Organization) and extracting relations between them (like Work For which relates a Person and an Organization, OrgBased In which relates an Organization and a Location etc.). Figure 1 shows a sample sentence annotated with entities and relations. The application domain and requirements of the downstream tasks usually dictate the type of entities and relations that an IE system needs to extract. Most work in IE has concentrated on entity extraction alone (Tjong Kim Sang, 2002; Sang and Meulder, 2003) or on relation extraction assuming entities are either given or previously extracted (Bunescu et al., 2005; Zhang et al., 2006; Giuliano et al., 2007; Qian et al., 2008). However, these tasks are very closely inter-related. While identifying correct entities is essential for identifying relations between them, identifying correct relations can in turn improve identification of entities. For example, if the relation Work For is identified with high confidence by a relation extractor, then it can enforce identifying its arguments as Person and Organization, about which the entity extractor might not have been confident. A brute force algorithm for finding the most probable joint extraction will soon</context>
</contexts>
<marker>Bunescu, Ge, Kate, Marcotte, Mooney, Ramani, Wong, 2005</marker>
<rawString>Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Edward M. Marcotte, Raymond J. Mooney, Arun Kumar Ramani, and Yuk Wah Wong. 2005. Comparative experiments on learning information extractors for proteins and their interactions. Artificial Intelligence in Medicine (special issue on Summarization and Information Extraction from Medical Documents), 33(2):139–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Estevam R Hruschka</author>
<author>Tom M Mitchell</author>
</authors>
<title>Coupling semi-supervised learning of categories and relations.</title>
<date>2009</date>
<booktitle>In SemiSupLearn ’09: Proceedings of the NAACL HLT 2009 Workshop on Semi-Supervised Learning for Natural Language Processing,</booktitle>
<pages>1--9</pages>
<location>Boulder, Colorado.</location>
<contexts>
<context position="32938" citStr="Carlson et al. (2009)" startWordPosition="5467" endWordPosition="5471">information extraction using a fixed template. However, as designed, such a CFG approach cannot handle the cases when an entity is involved in multiple relations and when the relations crisscross each other in the sentence, as in Figure 1. These cases occur frequently in the dataset we used in our experiments and many other relationextraction tasks. Giuliano et al. (2007) thoroughly evaluate the effect of entity extraction on relation extraction using the dataset used in our experiments. However, they employ a pipeline architecture and did not investigate joint relation and entity extraction. Carlson et al. (2009) present a method to simultaneously do semi-supervised training of entity and relation classifiers. However, their coupling method is meant to take advantage of the available unsupervised data and does not do joint inference. Riedel et al. (2009) present an approach for extracting bio-molecular events and their arguments using Markov Logic. Such an approach could also be adapted for jointly extracting entities and their relations, however, this would restrict entity and relation extraction to the same machine learning method that is used with Markov Logic. For example, one would not be able to</context>
</contexts>
<marker>Carlson, Betteridge, Hruschka, Mitchell, 2009</marker>
<rawString>Andrew Carlson, Justin Betteridge, Estevam R. Hruschka, and Tom M. Mitchell. 2009. Coupling semi-supervised learning of categories and relations. In SemiSupLearn ’09: Proceedings of the NAACL HLT 2009 Workshop on Semi-Supervised Learning for Natural Language Processing, pages 1–9, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Ranking algorithms for named-entity extraction: Boosting and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-2002),</booktitle>
<pages>489--496</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="18634" citStr="Collins, 2002" startWordPosition="3083" endWordPosition="3084">probability * g.probability // multiply probabilities of the children sub-card-pyramids // divide by the common probability that got multiplied twice if (i &gt; 1) prob/=l[i — 2][j + 1].beam[f.rightIndex].probability add(l[i][j].beam, (relationLabel, prob, index of f, index of g, f.leftMostLeaf,g.rightMostLeaf) return the labels starting from the first beam element of the root i.e. l[n][0].beam[0] Figure 4: Card-Pyramid Parsing Algorithm. whether any or all words are found in a list of entity names, whether any word has sufffix “ment” or “ing”, and finally the alphanumeric pattern of characters (Collins, 2002) of the last candidate entity word obtained by replacing each character by its character type (lowercase, uppercase or numeric) and collapsing any consecutive repetition (for example, the alphanumeric pattern for CoNLL2010 will be AaA0). The full kernel is computed by adding the word-subsequence kernel and the dot-product of all these features, exploiting the convolution property of kernels. We also use an SVM classifier for each of the relation productions in the grammar which outputs the probability that the relation holds between the two entities. A relation classifier is applied at an inte</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Ranking algorithms for named-entity extraction: Boosting and the voted perceptron. In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-2002), pages 489–496, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nello Cristianini</author>
<author>John Shawe-Taylor</author>
</authors>
<title>An Introduction to Support Vector Machines and Other Kernel-based Learning Methods.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="15570" citStr="Cristianini and Shawe-Taylor, 2000" startWordPosition="2591" endWordPosition="2595">timal solution is found. A limitless beam size will find the optimal solution but will reduce the algorithm to a computationally intractable brute force search. The parsing algorithm with a finite beam size keeps the search computationally tractable while allowing a joint labelling. 3 Classifiers for Entity and Relation Extraction The card-pyramid parsing described in the previous section requires classifiers for each of the entity and relation productions. In this section, we describe the classifiers we used in our experiments and how they were trained. We use a support vector machine (SVM) (Cristianini and Shawe-Taylor, 2000) classifier for each of the entity productions in the grammar. An entity classifier gets as input a sentence and a candidate entity indicated by the range of the indices of its words. It outputs the probability that the candidate entity is of the respective entity type. Probabilities for the SVM outputs are computed using the method by Platt (1999). We use all possible word subsequences of the candidate entity words as implicit features using a word-subsequence kernel (Lodhi et al., 2002). In addition, we use the following standard entity extraction features: the part-of-speech (POS) tag seque</context>
</contexts>
<marker>Cristianini, Shawe-Taylor, 2000</marker>
<rawString>Nello Cristianini and John Shawe-Taylor. 2000. An Introduction to Support Vector Machines and Other Kernel-based Learning Methods. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudio Giuliano</author>
<author>Alberto Lavelli</author>
<author>Lorenza Romano</author>
</authors>
<title>Relation extraction and the influence of automatic named-entity recognition.</title>
<date>2007</date>
<journal>ACM Trans. Speech Lang. Process.,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="1700" citStr="Giuliano et al., 2007" startWordPosition="268" endWordPosition="271">tions between them (like Work For which relates a Person and an Organization, OrgBased In which relates an Organization and a Location etc.). Figure 1 shows a sample sentence annotated with entities and relations. The application domain and requirements of the downstream tasks usually dictate the type of entities and relations that an IE system needs to extract. Most work in IE has concentrated on entity extraction alone (Tjong Kim Sang, 2002; Sang and Meulder, 2003) or on relation extraction assuming entities are either given or previously extracted (Bunescu et al., 2005; Zhang et al., 2006; Giuliano et al., 2007; Qian et al., 2008). However, these tasks are very closely inter-related. While identifying correct entities is essential for identifying relations between them, identifying correct relations can in turn improve identification of entities. For example, if the relation Work For is identified with high confidence by a relation extractor, then it can enforce identifying its arguments as Person and Organization, about which the entity extractor might not have been confident. A brute force algorithm for finding the most probable joint extraction will soon become intractable as the number of entiti</context>
<context position="22161" citStr="Giuliano et al., 2007" startWordPosition="3673" endWordPosition="3676">t part of the input, we use word-subsequence kernels which have shown to be effective for relation extraction (Bunescu and Mooney, 2005b). We compute the kernel as the sum of the wordsubsequence kernels between: the words between the two entities (between pattern), k (a parameter) words before the first entity (before pattern), k words after the second entity (after pattern) and the words from the beginning of the first entity to the end of the second entity (between-and-entity pattern). The before, between and after patterns have been found useful in previous work (Bunescu and Mooney, 2005b; Giuliano et al., 2007). Sometimes the words of the entities can indicate the relations they are in, hence we also use the betweenand-entity pattern. When a relation classifier is used at a node, the labels of the leaves beneath it are already known, so we replace candidate entity words that are in the between and between-andentity3 patterns by their entity labels. This provides useful information to the relation classifier and also makes these patterns less sparse for training. Given training data of sentences annotated with entities and relations, the positive and negative examples for training the entity and rela</context>
<context position="32691" citStr="Giuliano et al. (2007)" startWordPosition="5429" endWordPosition="5432">nteraction between dependent extraction decisions. Miller et al. (2000) adapt a probabilistic context-free parser for information extraction by augmenting syntactic labels with entity and relation labels. They thus do a joint syntactic parsing and information extraction using a fixed template. However, as designed, such a CFG approach cannot handle the cases when an entity is involved in multiple relations and when the relations crisscross each other in the sentence, as in Figure 1. These cases occur frequently in the dataset we used in our experiments and many other relationextraction tasks. Giuliano et al. (2007) thoroughly evaluate the effect of entity extraction on relation extraction using the dataset used in our experiments. However, they employ a pipeline architecture and did not investigate joint relation and entity extraction. Carlson et al. (2009) present a method to simultaneously do semi-supervised training of entity and relation classifiers. However, their coupling method is meant to take advantage of the available unsupervised data and does not do joint inference. Riedel et al. (2009) present an approach for extracting bio-molecular events and their arguments using Markov Logic. Such an ap</context>
</contexts>
<marker>Giuliano, Lavelli, Romano, 2007</marker>
<rawString>Claudio Giuliano, Alberto Lavelli, and Lorenza Romano. 2007. Relation extraction and the influence of automatic named-entity recognition. ACM Trans. Speech Lang. Process., 5(1):1–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>J H Martin</author>
</authors>
<title>Speech and Language Processing: An Introduction to Natural Lan guage Processing, Computational Linguistics, and Speech Recognition. Prentice Hall, Upper Saddle River,</title>
<date>2008</date>
<location>NJ.</location>
<contexts>
<context position="3453" citStr="Jurafsky and Martin, 2008" startWordPosition="550" endWordPosition="553"> separate classifiers which are applied independently and then computes a most probable consistent global set of entities and relations using linear programming. In this paper, we present a different approach to joint extraction using a “card-pyramid” graph. The labeled nodes in this graph compactly encode the possible entities and relations in a sentence. The task of joint extraction then reduces to finding the most probable joint assignment to the nodes in the card-pyramid. We give an efficient dynamic-programming algorithm for this task which resembles CYK parsing for contextfree grammars (Jurafsky and Martin, 2008). The algorithm does a beam search and gives an approximate solution for a finite beam size. A natural advantage of this approach is that extraction from a part of the sentence is influenced by extraction from its subparts and vice-versa, thus leading to a joint extraction. During extraction from a part of the sentence it also allows use of features based on the extraction from its sub-parts, thus leading to a more integrated extraction. We use Roth &amp; Yih’s 203 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 203–212, Uppsala, Sweden, 15-16 July 2010. </context>
</contexts>
<marker>Jurafsky, Martin, 2008</marker>
<rawString>D. Jurafsky and J. H. Martin. 2008. Speech and Language Processing: An Introduction to Natural Lan guage Processing, Computational Linguistics, and Speech Recognition. Prentice Hall, Upper Saddle River, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Raymond J Mooney</author>
</authors>
<title>Using stringkernels for learning semantic parsers.</title>
<date>2006</date>
<booktitle>In Proc. of the 21st Intl. Conf. on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL-06),</booktitle>
<pages>913--920</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="12337" citStr="Kate and Mooney, 2006" startWordPosition="2049" endWordPosition="2052">yramid2), where range1 and range2 are the 205 ranges of the word indices of the two entities and sub-card-pyramid1 and sub-card-pyramid2 are the sub-card-pyramids rooted at its two children. Thus, along with the two entities and the words in the sentence, information from these subcard-pyramids is also used in deciding the relation at a node. In the next section, we further specify these entity and relation classifiers and explain how they are trained. We note that this use of multiple classifiers to determine the most probable parse is similar to the method used in the KRISP semantic parser (Kate and Mooney, 2006). Given the candidate entities in a sentence, the grammar, and the entity and relation classifiers, the card-pyramid parsing algorithm tries to find the most probable joint-labeling of all of its nodes, and thus jointly extracts entities and their relations. The parsing algorithm does a beam search and maintains a beam at each node of the cardpyramid. A node is represented by l[i][j] in the pseudo-code which stands for the node in the jth position in the ith level. Note that at level i, the nodes range from l[i][0] to l[i][n − i − 1], where n is the number of leaves. The beam at each node is a</context>
</contexts>
<marker>Kate, Mooney, 2006</marker>
<rawString>Rohit J. Kate and Raymond J. Mooney. 2006. Using stringkernels for learning semantic parsers. In Proc. of the 21st Intl. Conf. on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL-06), pages 913–920, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
</authors>
<title>A dependency-based word subsequence kernel.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>400--409</pages>
<location>Waikiki,Honolulu,Hawaii,</location>
<contexts>
<context position="35626" citStr="Kate, 2008" startWordPosition="5895" endWordPosition="5896">Given the regular graph structure of the cardpyramid, it would be interesting to investigate whether it can be modeled using a probabilistic graphical model (Koller and Friedman, 2009). In that case, instead of using multiple probabilistic classifiers, one could employ a single jointlytrained probabilistic model, which is theoretically more appealing and might give better results. Finally, we note that a better relation classifier could be used in the current approach which makes more use of linguistic information. For example, 210 by using dependency-based kernels (Bunescu and Mooney, 2005a; Kate, 2008) or syntactic kernels (Qian et al., 2008; Moschitti, 2009) or by including the word categories and their POS tags in the subsequences. Also, it will be interesting to see if a kernel that computes the similarity between subcard-pyramids could be developed and used for relation classification. 7 Conclusions We introduced a card-pyramid graph structure and presented a new method for jointly extracting entities and their relations from a sentence using it. A card-pyramid compactly encodes the entities and relations in a sentence thus reducing the joint extraction task to jointly labeling its node</context>
</contexts>
<marker>Kate, 2008</marker>
<rawString>Rohit J. Kate. 2008. A dependency-based word subsequence kernel. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2008), pages 400–409, Waikiki,Honolulu,Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daphne Koller</author>
<author>Nir Friedman</author>
</authors>
<title>Probabilistic Graphical Models: Principles and Techniques.</title>
<date>2009</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="35199" citStr="Koller and Friedman, 2009" startWordPosition="5829" endWordPosition="5832">those relations and then use card-pyramid parsing. If the relations are between relations instead of between entities, then card-pyramid parsing can handle it by considering the labels of the immediate children as RHS nonterminals instead of the labels of the left-most and the right-most leaves beneath it. Thus, it would be interesting to apply card-pyramid parsing to extract higher-order relations (such as causal or temporal relations). Given the regular graph structure of the cardpyramid, it would be interesting to investigate whether it can be modeled using a probabilistic graphical model (Koller and Friedman, 2009). In that case, instead of using multiple probabilistic classifiers, one could employ a single jointlytrained probabilistic model, which is theoretically more appealing and might give better results. Finally, we note that a better relation classifier could be used in the current approach which makes more use of linguistic information. For example, 210 by using dependency-based kernels (Bunescu and Mooney, 2005a; Kate, 2008) or syntactic kernels (Qian et al., 2008; Moschitti, 2009) or by including the word categories and their POS tags in the subsequences. Also, it will be interesting to see if</context>
</contexts>
<marker>Koller, Friedman, 2009</marker>
<rawString>Daphne Koller and Nir Friedman. 2009. Probabilistic Graphical Models: Principles and Techniques. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huma Lodhi</author>
<author>Craig Saunders</author>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
<author>Chris Watkins</author>
</authors>
<title>Text classification using string kernels.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>2--419</pages>
<contexts>
<context position="16063" citStr="Lodhi et al., 2002" startWordPosition="2676" endWordPosition="2679"> used in our experiments and how they were trained. We use a support vector machine (SVM) (Cristianini and Shawe-Taylor, 2000) classifier for each of the entity productions in the grammar. An entity classifier gets as input a sentence and a candidate entity indicated by the range of the indices of its words. It outputs the probability that the candidate entity is of the respective entity type. Probabilities for the SVM outputs are computed using the method by Platt (1999). We use all possible word subsequences of the candidate entity words as implicit features using a word-subsequence kernel (Lodhi et al., 2002). In addition, we use the following standard entity extraction features: the part-of-speech (POS) tag sequence of the candidate entity words, two words before and after the candidate entity and their POS tags, whether any or all candidate entity words are capitalized, &apos;These are stored in the beam elements. 2Note that this step enforces the consistency constraint of Roth and Yih (Roth and Yih, 2004; Roth and Yih, 2007) that a relation can only be between the entities of specific types. The grammar in our approach inherently enforces this constraint. 206 function Card-Pyramid-Parsing(Sentence,G</context>
<context position="25822" citStr="Lodhi et al., 2002" startWordPosition="4288" endWordPosition="4291"> (521) indicates that a Person lives at a Location and Kill (268) indicates that a Person killed another Person. There are 17007 pairs of entities that are not related by any of the five relations and hence have the NR relation between them which thus significantly outnumbers other relations. Our implementation uses the LIBSVM5 software for SVM classifiers. We kept the noise penalty parameter of SVM very high (100) assuming there is little noise in our data. For the word-subsequence kernel, we set 5 as the maximum length of a subsequence and 0.25 as the penalty parameter for subsequence gaps (Lodhi et al., 2002). We used k = 5 words for before and after patterns for the relation classifiers. These parameter values were determined through pilot experiments on a subset of the data. We used a beam 4Available at: http://l2r.cs.uiuc.edu/ -cogcomp/Data/ER/conll04.corp 5http://www.csie.ntu.edu.tw/-cjlin/ libsvm/ 208 Entity Person Location Organization Approach Rec Pre F Rec Pre F Rec Pre F Pipeline 93.6 92.0 92.8 94.0 90.3 92.1 87.9 90.6 89.2 Card-pyramid 94.2 92.1 93.2 94.2 90.8 92.4† 88.7 90.5 89.5 RY07 Pipeline 89.1 88.7 88.6 88.1 89.8 88.9 71.4 89.3 78.7 RY07 Joint 89.5 89.1 89.0 88.7 89.7 89.1 72.0 89.</context>
</contexts>
<marker>Lodhi, Saunders, Shawe-Taylor, Cristianini, Watkins, 2002</marker>
<rawString>Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and Chris Watkins. 2002. Text classification using string kernels. Journal of Machine Learning Research, 2:419–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Heidi Fox</author>
<author>Lance A Ramshaw</author>
<author>Ralph M Weischedel</author>
</authors>
<title>A novel use of statistical parsing to extract information from text.</title>
<date>2000</date>
<booktitle>In Proc. of the Meeting of the N. American Association for Computational Linguistics,</booktitle>
<pages>226--233</pages>
<location>Seattle, Washington.</location>
<contexts>
<context position="32140" citStr="Miller et al. (2000)" startWordPosition="5340" endWordPosition="5343">d employs independent entity and relation classifiers whose outputs are used to compute a most probable consistent global set of entities and relations using linear programming. One key advantage of our cardpyramid method over their method is that the classifiers can take the output of other classifiers under its node as input features during parsing. This is not possible in their approach because all classifier outputs are determined before they are passed to the linear program solver. Thus our approach is more integrated and allows greater interaction between dependent extraction decisions. Miller et al. (2000) adapt a probabilistic context-free parser for information extraction by augmenting syntactic labels with entity and relation labels. They thus do a joint syntactic parsing and information extraction using a fixed template. However, as designed, such a CFG approach cannot handle the cases when an entity is involved in multiple relations and when the relations crisscross each other in the sentence, as in Figure 1. These cases occur frequently in the dataset we used in our experiments and many other relationextraction tasks. Giuliano et al. (2007) thoroughly evaluate the effect of entity extract</context>
</contexts>
<marker>Miller, Fox, Ramshaw, Weischedel, 2000</marker>
<rawString>Scott Miller, Heidi Fox, Lance A. Ramshaw, and Ralph M. Weischedel. 2000. A novel use of statistical parsing to extract information from text. In Proc. of the Meeting of the N. American Association for Computational Linguistics, pages 226–233, Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Syntactic and semantic kernels for short text pair categorization.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>576--584</pages>
<location>Athens,Greece,</location>
<contexts>
<context position="35684" citStr="Moschitti, 2009" startWordPosition="5904" endWordPosition="5905"> it would be interesting to investigate whether it can be modeled using a probabilistic graphical model (Koller and Friedman, 2009). In that case, instead of using multiple probabilistic classifiers, one could employ a single jointlytrained probabilistic model, which is theoretically more appealing and might give better results. Finally, we note that a better relation classifier could be used in the current approach which makes more use of linguistic information. For example, 210 by using dependency-based kernels (Bunescu and Mooney, 2005a; Kate, 2008) or syntactic kernels (Qian et al., 2008; Moschitti, 2009) or by including the word categories and their POS tags in the subsequences. Also, it will be interesting to see if a kernel that computes the similarity between subcard-pyramids could be developed and used for relation classification. 7 Conclusions We introduced a card-pyramid graph structure and presented a new method for jointly extracting entities and their relations from a sentence using it. A card-pyramid compactly encodes the entities and relations in a sentence thus reducing the joint extraction task to jointly labeling its nodes. We presented an efficient parsing algorithm for jointly</context>
</contexts>
<marker>Moschitti, 2009</marker>
<rawString>Alessandro Moschitti. 2009. Syntactic and semantic kernels for short text pair categorization. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 576–584, Athens,Greece, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Platt</author>
</authors>
<title>Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.</title>
<date>1999</date>
<booktitle>Advances in Large Margin Classifiers,</booktitle>
<pages>185--208</pages>
<editor>In Alexander J. Smola, Peter Bartlett, Bernhard Sch¨olkopf, and Dale Schuurmans, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="15920" citStr="Platt (1999)" startWordPosition="2655" endWordPosition="2656">e previous section requires classifiers for each of the entity and relation productions. In this section, we describe the classifiers we used in our experiments and how they were trained. We use a support vector machine (SVM) (Cristianini and Shawe-Taylor, 2000) classifier for each of the entity productions in the grammar. An entity classifier gets as input a sentence and a candidate entity indicated by the range of the indices of its words. It outputs the probability that the candidate entity is of the respective entity type. Probabilities for the SVM outputs are computed using the method by Platt (1999). We use all possible word subsequences of the candidate entity words as implicit features using a word-subsequence kernel (Lodhi et al., 2002). In addition, we use the following standard entity extraction features: the part-of-speech (POS) tag sequence of the candidate entity words, two words before and after the candidate entity and their POS tags, whether any or all candidate entity words are capitalized, &apos;These are stored in the beam elements. 2Note that this step enforces the consistency constraint of Roth and Yih (Roth and Yih, 2004; Roth and Yih, 2007) that a relation can only be betwee</context>
</contexts>
<marker>Platt, 1999</marker>
<rawString>John C. Platt. 1999. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In Alexander J. Smola, Peter Bartlett, Bernhard Sch¨olkopf, and Dale Schuurmans, editors, Advances in Large Margin Classifiers, pages 185–208. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
</authors>
<title>The use of classifiers in sequential inference.</title>
<date>2001</date>
<booktitle>In Advances in Neural Information Processing Systems 13.</booktitle>
<contexts>
<context position="7834" citStr="Punyakanok and Roth, 2001" startWordPosition="1319" endWordPosition="1322">entities are not related, then the label of their lowest common ancestor is NR. This way, every nonleaf node relates exactly two entities: the left-most and right-most leaves beneath it. 2.2 Card-Pyramid Parsing The task of jointly extracting entities and relations from a sentence reduces to jointly labeling the nodes of a card-pyramid which has all the candidate entities (i.e. entity boundaries) of the sentence as its leaves. We call this process card-pyramid parsing. We assume that all candidate entities are given up-front. If needed, candidate entities can be either obtained automatically (Punyakanok and Roth, 2001) or generated using a simple heuristic, like including all noun-phrase chunks as candidate entities. Or, in the worst case, each substring of words in the sentence can be given as a candidate entity. Liberally including candidate entities is possible since they can simply be given the label Other if they are none of the given types. In this section we describe our card-pyramid parsing algorithm whose pseudo-code is shown in Figure 4. While the process is analogous to context-free grammar (CFG) parsing, particularly CYK bottom-up parsing, there are several major differences. Firstly, in card-py</context>
</contexts>
<marker>Punyakanok, Roth, 2001</marker>
<rawString>Vasin Punyakanok and Dan Roth. 2001. The use of classifiers in sequential inference. In Advances in Neural Information Processing Systems 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Longhua Qian</author>
<author>Guodong Zhou</author>
<author>Fang Kong</author>
<author>Qiaoming Zhu</author>
<author>Peide Qian</author>
</authors>
<title>Exploiting constituent dependencies for tree kernel-based semantic relation extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>697--704</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="1720" citStr="Qian et al., 2008" startWordPosition="272" endWordPosition="275">e Work For which relates a Person and an Organization, OrgBased In which relates an Organization and a Location etc.). Figure 1 shows a sample sentence annotated with entities and relations. The application domain and requirements of the downstream tasks usually dictate the type of entities and relations that an IE system needs to extract. Most work in IE has concentrated on entity extraction alone (Tjong Kim Sang, 2002; Sang and Meulder, 2003) or on relation extraction assuming entities are either given or previously extracted (Bunescu et al., 2005; Zhang et al., 2006; Giuliano et al., 2007; Qian et al., 2008). However, these tasks are very closely inter-related. While identifying correct entities is essential for identifying relations between them, identifying correct relations can in turn improve identification of entities. For example, if the relation Work For is identified with high confidence by a relation extractor, then it can enforce identifying its arguments as Person and Organization, about which the entity extractor might not have been confident. A brute force algorithm for finding the most probable joint extraction will soon become intractable as the number of entities in a sentence gro</context>
<context position="35666" citStr="Qian et al., 2008" startWordPosition="5900" endWordPosition="5903">of the cardpyramid, it would be interesting to investigate whether it can be modeled using a probabilistic graphical model (Koller and Friedman, 2009). In that case, instead of using multiple probabilistic classifiers, one could employ a single jointlytrained probabilistic model, which is theoretically more appealing and might give better results. Finally, we note that a better relation classifier could be used in the current approach which makes more use of linguistic information. For example, 210 by using dependency-based kernels (Bunescu and Mooney, 2005a; Kate, 2008) or syntactic kernels (Qian et al., 2008; Moschitti, 2009) or by including the word categories and their POS tags in the subsequences. Also, it will be interesting to see if a kernel that computes the similarity between subcard-pyramids could be developed and used for relation classification. 7 Conclusions We introduced a card-pyramid graph structure and presented a new method for jointly extracting entities and their relations from a sentence using it. A card-pyramid compactly encodes the entities and relations in a sentence thus reducing the joint extraction task to jointly labeling its nodes. We presented an efficient parsing alg</context>
</contexts>
<marker>Qian, Zhou, Kong, Zhu, Qian, 2008</marker>
<rawString>Longhua Qian, Guodong Zhou, Fang Kong, Qiaoming Zhu, and Peide Qian. 2008. Exploiting constituent dependencies for tree kernel-based semantic relation extraction. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 697–704, Manchester, UK, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Hong-Woo Chun</author>
<author>Toshihisa Takagi</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A Markov logic approach to bio-molecular event extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task,</booktitle>
<pages>41--49</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="33184" citStr="Riedel et al. (2009)" startWordPosition="5508" endWordPosition="5511">ases occur frequently in the dataset we used in our experiments and many other relationextraction tasks. Giuliano et al. (2007) thoroughly evaluate the effect of entity extraction on relation extraction using the dataset used in our experiments. However, they employ a pipeline architecture and did not investigate joint relation and entity extraction. Carlson et al. (2009) present a method to simultaneously do semi-supervised training of entity and relation classifiers. However, their coupling method is meant to take advantage of the available unsupervised data and does not do joint inference. Riedel et al. (2009) present an approach for extracting bio-molecular events and their arguments using Markov Logic. Such an approach could also be adapted for jointly extracting entities and their relations, however, this would restrict entity and relation extraction to the same machine learning method that is used with Markov Logic. For example, one would not be able to use kernelbased SVM for relation extraction, which has been very successful at this task, because Markov Logic does not support kernel-based machine learning. In contrast, our joint approach is independent of the individual machine learning meth</context>
</contexts>
<marker>Riedel, Chun, Takagi, Tsujii, 2009</marker>
<rawString>Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi, and Jun’ichi Tsujii. 2009. A Markov logic approach to bio-molecular event extraction. In Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task, pages 41–49, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks.</title>
<date>2004</date>
<booktitle>In Proc. of 8th Conf. on Computational Natural Language Learning (CoNLL-2004),</booktitle>
<pages>1--8</pages>
<location>Boston, MA.</location>
<contexts>
<context position="16464" citStr="Roth and Yih, 2004" startWordPosition="2741" endWordPosition="2744">bilities for the SVM outputs are computed using the method by Platt (1999). We use all possible word subsequences of the candidate entity words as implicit features using a word-subsequence kernel (Lodhi et al., 2002). In addition, we use the following standard entity extraction features: the part-of-speech (POS) tag sequence of the candidate entity words, two words before and after the candidate entity and their POS tags, whether any or all candidate entity words are capitalized, &apos;These are stored in the beam elements. 2Note that this step enforces the consistency constraint of Roth and Yih (Roth and Yih, 2004; Roth and Yih, 2007) that a relation can only be between the entities of specific types. The grammar in our approach inherently enforces this constraint. 206 function Card-Pyramid-Parsing(Sentence,Grammar,entity-classifiers,relation-classifiers) n = number of candidate entities in S // Let range(r) represent the range of the indices of the words for the rth candidate entity. // Let l[i][j] represent the jth node at ith level in the card-pyramid. // For leaves // A beam element at a leaf node is (label,probability). for j = 0 to n // for every leaf for each entityLabel — candidate entity E Gra</context>
<context position="20889" citStr="Roth and Yih, 2004" startWordPosition="3455" endWordPosition="3458">r(12-12) as the in-between entity). This information tells how the two entities are related to the entities present in between them. This can affect the relation between the two entities, for example, if the sentence mentions that a person lives at a location and also mentions that an organization is based at that location then that person is likely to work at that organization. Note that this information can not be incorporated in a pipelined approach in which each relation is determined independently. It is also not possible to incorporate this in the linear programming method presented in (Roth and Yih, 2004; Roth and Yih, 2007) because that method computes the probabilities of all the relations independently before finding the optimal solution through linear programming. It would also not help to add hard constraints to their linear program relating the relations because they need not always hold. We add the kernels for each part of the input to compute the final kernel for the SVM classifiers. The kernel for the second part of the input is computed by simply counting the number of common 207 pairs of relations between two examples thus implicitly considering every pair of relation (as described</context>
<context position="2733" citStr="Roth &amp; Yih (2004" startWordPosition="436" endWordPosition="439">t which the entity extractor might not have been confident. A brute force algorithm for finding the most probable joint extraction will soon become intractable as the number of entities in a sentence grows. If there are n entities in a sentence, then there are O(n2) possible relations between them and if each relation can take l labels then there are O(ln2) total possibilities, which is intractable even for small l and n. Hence, an efficient inference mechanism is needed for joint entity and relation extraction. The only work we are aware of for jointly extracting entities and relations is by Roth &amp; Yih (2004; 2007). Their method first identifies the possible entities and relations in a sentence using separate classifiers which are applied independently and then computes a most probable consistent global set of entities and relations using linear programming. In this paper, we present a different approach to joint extraction using a “card-pyramid” graph. The labeled nodes in this graph compactly encode the possible entities and relations in a sentence. The task of joint extraction then reduces to finding the most probable joint assignment to the nodes in the card-pyramid. We give an efficient dyna</context>
<context position="24316" citStr="Roth &amp; Yih (2004" startWordPosition="4027" endWordPosition="4030">se productions gave different probabilities for the “notrelated” relation between the entities. To avoid 3Except for the two entities at the ends this, we found that it suffices if all these classifiers for NR productions always return 0.5 as the probability. This ensures that a real relation will be preferred over NR if and only if its probability is greater than 0.5, otherwise nothing will change. 4 Experiments We conducted experiments to compare our cardpyramid parsing approach for joint entity and relation extraction to a pipelined approach. 4.1 Methodology We used the dataset4 created by Roth &amp; Yih (2004; 2007) that was also used by Giuliano et el. (2007). The sentences in this data were taken from the TREC corpus and annotated with entities and relations. As in the previous work with this dataset, in order to observe the interaction between entities and relations, our experiments used only the 1437 sentences that include at least one relation. The boundaries of the entities are already supplied by this dataset. There are three types of entities: Person (1685), Location (1968) and Organization (978), in addition there is a fourth type Other (705), which indicates that the candidate entity is </context>
<context position="29618" citStr="Roth &amp; Yih (2004" startWordPosition="4924" endWordPosition="4927">elines also set a higher ceiling for our joint extraction method to improve upon. The entity extraction results show that on all the entities the card-pyramid parsing approach for joint extraction obtains a better performance than the pipelined approach. This shows that entity extraction benefits when it is jointly done with relation extraction. Joint extraction using cardpyramid parsing also gave improvement in performance on all the relations except the Located In relation.6 The results thus show that entity and relation extraction correct some of each other’s errors when jointly performed. Roth &amp; Yih (2004; 2007) report that 5% to 25% of the relation predictions of their pipeline models were incoherent, meaning that the types of the entities related by the relations are not of the required types. Their joint inference method corrects these mistakes, hence a part of the improvement their joint model obtains over their pipeline model is due to the fact that their pipeline model can output incoherent relations. Since the types of the entities a relation’s arguments should 6Through error analysis we found that the drop in the performance for this relation was mainly because of an unusual sentence i</context>
<context position="31430" citStr="Roth &amp; Yih (2004" startWordPosition="5223" endWordPosition="5226">e the overall drop is not found as statistically significant despite being relatively large. 209 take are known, we believe that filtering out the incoherent relation predictions of their pipeline model can improve its precision without hurting the recall. On the other hand our pipelined approach never outputs incoherent relations because the grammar of relation productions enforce that the relations are always between entities of the required types. Thus the improvement obtained by our joint extraction method over our pipelined approach is always non-trivial. 5 Related Work To our knowledge, Roth &amp; Yih (2004; 2007) have done the only other work on joint entity and relation extraction. Their method employs independent entity and relation classifiers whose outputs are used to compute a most probable consistent global set of entities and relations using linear programming. One key advantage of our cardpyramid method over their method is that the classifiers can take the output of other classifiers under its node as input features during parsing. This is not possible in their approach because all classifier outputs are determined before they are passed to the linear program solver. Thus our approach </context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>D. Roth and W. Yih. 2004. A linear programming formulation for global inference in natural language tasks. In Proc. of 8th Conf. on Computational Natural Language Learning (CoNLL-2004), pages 1–8, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>Global inference for entity and relation identification via a linear programming formulation.</title>
<date>2007</date>
<booktitle>Introduction to Statistical Relational Learning,</booktitle>
<pages>553--580</pages>
<editor>In L. Getoor and B. Taskar, editors,</editor>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="16485" citStr="Roth and Yih, 2007" startWordPosition="2745" endWordPosition="2748"> outputs are computed using the method by Platt (1999). We use all possible word subsequences of the candidate entity words as implicit features using a word-subsequence kernel (Lodhi et al., 2002). In addition, we use the following standard entity extraction features: the part-of-speech (POS) tag sequence of the candidate entity words, two words before and after the candidate entity and their POS tags, whether any or all candidate entity words are capitalized, &apos;These are stored in the beam elements. 2Note that this step enforces the consistency constraint of Roth and Yih (Roth and Yih, 2004; Roth and Yih, 2007) that a relation can only be between the entities of specific types. The grammar in our approach inherently enforces this constraint. 206 function Card-Pyramid-Parsing(Sentence,Grammar,entity-classifiers,relation-classifiers) n = number of candidate entities in S // Let range(r) represent the range of the indices of the words for the rth candidate entity. // Let l[i][j] represent the jth node at ith level in the card-pyramid. // For leaves // A beam element at a leaf node is (label,probability). for j = 0 to n // for every leaf for each entityLabel — candidate entity E Grammar prob = entity-cl</context>
<context position="20910" citStr="Roth and Yih, 2007" startWordPosition="3459" endWordPosition="3462">etween entity). This information tells how the two entities are related to the entities present in between them. This can affect the relation between the two entities, for example, if the sentence mentions that a person lives at a location and also mentions that an organization is based at that location then that person is likely to work at that organization. Note that this information can not be incorporated in a pipelined approach in which each relation is determined independently. It is also not possible to incorporate this in the linear programming method presented in (Roth and Yih, 2004; Roth and Yih, 2007) because that method computes the probabilities of all the relations independently before finding the optimal solution through linear programming. It would also not help to add hard constraints to their linear program relating the relations because they need not always hold. We add the kernels for each part of the input to compute the final kernel for the SVM classifiers. The kernel for the second part of the input is computed by simply counting the number of common 207 pairs of relations between two examples thus implicitly considering every pair of relation (as described in the last paragrap</context>
<context position="27338" citStr="Roth and Yih, 2007" startWordPosition="4546" endWordPosition="4549"> Pipeline 56.4 52.5 50.7 44.4 60.8 51.2 42.1 77.8 54.3 50.0 58.9 53.5 81.5 73.0 76.5 RY07 Joint 55.7 53.9 51.3 42.3 72.0 53.1 41.6 79.8 54.3 49.0 59.1 53.0 81.5 77.5 79.0† Table 1: Results of five-fold cross-validation for entity and relation extraction using pipelined and joint extraction. Boldface indicates statistical significance (p &lt; 0.1 using paired t-test) when compared to the corresponding value in the other row grouped with it. Symbol † indicates statistical significance with p &lt; 0.05. Only statistical significance for F-measures are indicated. RY07 stands for the “E +-+ R” model in (Roth and Yih, 2007). size of 5 in our card-pyramid parsing algorithm at which the performance plateaus. We note that by using a beam size of 1 and by not using the second part of input for relation classifiers as described in section 3 (i.e. by ignoring the relations at the lower levels), the card-parsing algorithm reduces to the traditional pipelined approach because then only the best entity label for each candidate entity is considered for relation extraction. Hence, in our experiments we simply use this setting as our pipelined approach. We performed a 5-fold cross-validation to compare with the previous wor</context>
<context position="28855" citStr="Roth and Yih, 2007" startWordPosition="4801" endWordPosition="4804">e results they obtained with their most sophisticated model which they denote by “E H R”. For every entity type and relation type, we measured Precision (percentage of output labels correct), Recall (percentage of gold-standard labels correctly identified) and Fmeasure (the harmonic mean of Precision and Recall). 4.2 Results and Discussion Table 1 shows the results of entity and relation extraction. The statistical significance is shown only for F-measures. We first note that except for the Kill relation, all the results of our pipelined approach are far better than the pipelined approach of (Roth and Yih, 2007), for both entities and relations. This shows that the entity and relation classifiers we used are better that the ones they used. These strong baselines also set a higher ceiling for our joint extraction method to improve upon. The entity extraction results show that on all the entities the card-pyramid parsing approach for joint extraction obtains a better performance than the pipelined approach. This shows that entity extraction benefits when it is jointly done with relation extraction. Joint extraction using cardpyramid parsing also gave improvement in performance on all the relations exce</context>
<context position="27978" citStr="Roth &amp; Yih (2007)" startWordPosition="4656" endWordPosition="4659">-pyramid parsing algorithm at which the performance plateaus. We note that by using a beam size of 1 and by not using the second part of input for relation classifiers as described in section 3 (i.e. by ignoring the relations at the lower levels), the card-parsing algorithm reduces to the traditional pipelined approach because then only the best entity label for each candidate entity is considered for relation extraction. Hence, in our experiments we simply use this setting as our pipelined approach. We performed a 5-fold cross-validation to compare with the previous work with this dataset by Roth &amp; Yih (2007), however, our folds are not same as their folds which were not available. We also note that our entity and relation classifiers are different from theirs. They experimented with several models to see the effect of joint inference on them, we compare with the results they obtained with their most sophisticated model which they denote by “E H R”. For every entity type and relation type, we measured Precision (percentage of output labels correct), Recall (percentage of gold-standard labels correctly identified) and Fmeasure (the harmonic mean of Precision and Recall). 4.2 Results and Discussion </context>
</contexts>
<marker>Roth, Yih, 2007</marker>
<rawString>D. Roth and W. Yih. 2007. Global inference for entity and relation identification via a linear programming formulation. In L. Getoor and B. Taskar, editors, Introduction to Statistical Relational Learning, pages 553–580. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In Proc. of 7th Conf. on Computational Natural Language Learning (CoNLL-2003),</booktitle>
<location>Edmonton, Canada.</location>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Languageindependent named entity recognition. In Proc. of 7th Conf. on Computational Natural Language Learning (CoNLL-2003), Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
</authors>
<title>Introduction to the CoNLL2002 shared task: Language-independent named entity recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002,</booktitle>
<pages>155--158</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="1525" citStr="Sang, 2002" startWordPosition="241" endWordPosition="242">xtracting structured information from text. The two most common sub-tasks of IE are extracting entities (like Person, Location and Organization) and extracting relations between them (like Work For which relates a Person and an Organization, OrgBased In which relates an Organization and a Location etc.). Figure 1 shows a sample sentence annotated with entities and relations. The application domain and requirements of the downstream tasks usually dictate the type of entities and relations that an IE system needs to extract. Most work in IE has concentrated on entity extraction alone (Tjong Kim Sang, 2002; Sang and Meulder, 2003) or on relation extraction assuming entities are either given or previously extracted (Bunescu et al., 2005; Zhang et al., 2006; Giuliano et al., 2007; Qian et al., 2008). However, these tasks are very closely inter-related. While identifying correct entities is essential for identifying relations between them, identifying correct relations can in turn improve identification of entities. For example, if the relation Work For is identified with high confidence by a relation extractor, then it can enforce identifying its arguments as Person and Organization, about which </context>
</contexts>
<marker>Sang, 2002</marker>
<rawString>Erik F. Tjong Kim Sang. 2002. Introduction to the CoNLL2002 shared task: Language-independent named entity recognition. In Proceedings of CoNLL-2002, pages 155– 158. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
<author>Guodong Zhou</author>
</authors>
<title>A composite kernel to extract relations between entities with both flat and structured features.</title>
<date>2006</date>
<booktitle>In Proc. of the 21st Intl. Conf. on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL-06),</booktitle>
<location>Sydney, Australia,</location>
<contexts>
<context position="1677" citStr="Zhang et al., 2006" startWordPosition="264" endWordPosition="267"> and extracting relations between them (like Work For which relates a Person and an Organization, OrgBased In which relates an Organization and a Location etc.). Figure 1 shows a sample sentence annotated with entities and relations. The application domain and requirements of the downstream tasks usually dictate the type of entities and relations that an IE system needs to extract. Most work in IE has concentrated on entity extraction alone (Tjong Kim Sang, 2002; Sang and Meulder, 2003) or on relation extraction assuming entities are either given or previously extracted (Bunescu et al., 2005; Zhang et al., 2006; Giuliano et al., 2007; Qian et al., 2008). However, these tasks are very closely inter-related. While identifying correct entities is essential for identifying relations between them, identifying correct relations can in turn improve identification of entities. For example, if the relation Work For is identified with high confidence by a relation extractor, then it can enforce identifying its arguments as Person and Organization, about which the entity extractor might not have been confident. A brute force algorithm for finding the most probable joint extraction will soon become intractable </context>
</contexts>
<marker>Zhang, Zhang, Su, Zhou, 2006</marker>
<rawString>Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou. 2006. A composite kernel to extract relations between entities with both flat and structured features. In Proc. of the 21st Intl. Conf. on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL-06), Sydney, Australia, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>