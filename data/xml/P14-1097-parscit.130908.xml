<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000263">
<title confidence="0.999732">
A Step-wise Usage-based Method for Inducing
Polysemy-aware Verb Classes
</title>
<author confidence="0.99545">
Daisuke Kawahara† Daniel W. Peterson$ Martha Palmer$
</author>
<affiliation confidence="0.996347">
†Kyoto University, Kyoto, Japan
$University of Colorado at Boulder, Boulder, CO, USA
</affiliation>
<email confidence="0.98492">
dk@i.kyoto-u.ac.jp, {Daniel.W.Peterson, Martha.Palmer}@colorado.edu
</email>
<sectionHeader confidence="0.993806" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999968">
We present an unsupervised method for in-
ducing verb classes from verb uses in giga-
word corpora. Our method consists of
two clustering steps: verb-specific seman-
tic frames are first induced by clustering
verb uses in a corpus and then verb classes
are induced by clustering these frames.
By taking this step-wise approach, we can
not only generate verb classes based on a
massive amount of verb uses in a scalable
manner, but also deal with verb polysemy,
which is bypassed by most of the previous
studies on verb clustering. In our exper-
iments, we acquire semantic frames and
verb classes from two giga-word corpora,
the larger comprising 20 billion words.
The effectiveness of our approach is veri-
fied through quantitative evaluations based
on polysemy-aware gold-standard data.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997608">
A verb plays a primary role in conveying the
meaning of a sentence. Capturing the sense of a
verb is essential for natural language processing
(NLP), and thus lexical resources for verbs play
an important role in NLP.
Verb classes are one such lexical resource.
Manually-crafted verb classes have been devel-
oped, such as Levin’s classes (Levin, 1993) and
their extension, VerbNet (Kipper-Schuler, 2005),
in which verbs are organized into classes on the
basis of their syntactic and semantic behavior.
Such verb classes have been used in many NLP ap-
plications that need to consider semantics in par-
ticular, such as word sense disambiguation (Dang,
2004), semantic parsing (Swier and Stevenson,
2005; Shi and Mihalcea, 2005) and discourse pars-
ing (Subba and Di Eugenio, 2009).
There have also been many attempts to auto-
matically acquire verb classes with the goal of ei-
ther adding frequency information to an existing
resource or of inducing similar verb classes for
other languages. Most of these approaches assume
that all target verbs are monosemous (Stevenson
and Joanis, 2003; Schulte im Walde, 2006; Joa-
nis et al., 2008; Li and Brew, 2008; Sun et al.,
2008; Sun and Korhonen, 2009; Vlachos et al.,
2009; Parisien and Stevenson, 2010; Parisien and
Stevenson, 2011; Falk et al., 2012; Lippincott et
al., 2012; Reichart and Korhonen, 2013; Sun et al.,
2013). This monosemous assumption, however, is
not realistic because many frequent verbs actually
have multiple senses. Moreover, to the best of our
knowledge, none of the following approaches at-
tempt to quantitatively evaluate soft clusterings of
verb classes induced by polysemy-aware unsuper-
vised approaches (Korhonen et al., 2003; Lapata
and Brew, 2004; Li and Brew, 2007; Schulte im
Walde et al., 2008).
In this paper, we propose an unsupervised
method for inducing verb classes that is aware
of verb polysemy. Our method consists of two
clustering steps: verb-specific semantic frames are
first induced by clustering verb uses in a cor-
pus and then verb classes are induced by clus-
tering these frames. By taking this step-wise ap-
proach, we can not only induce verb classes with
frequency information from a massive amount of
verb uses in a scalable manner, but also deal with
verb polysemy.
Our novel contributions are summarized as fol-
lows:
</bodyText>
<listItem confidence="0.99839725">
• induce both semantic frames and verb classes
from a massive amount of verb uses by a scal-
able method,
• explicitly deal with verb polysemy,
• discover effective features for each of the
clustering steps, and
• quantitatively evaluate a soft clustering of
verbs.
</listItem>
<page confidence="0.935414">
1030
</page>
<note confidence="0.8611564">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1030–1040,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
Verb classes:
Semantic frames:
Verb uses:
</note>
<figureCaption confidence="0.984158">
Figure 1: Overview of our two-step approach. Verb-specific semantic frames are first induced from verb
uses (lower part) and then verb classes are induced from the semantic frames (upper part). The labels of
verb classes are manually assigned here for better understanding.
</figureCaption>
<sectionHeader confidence="0.999278" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999985043478261">
As stated in Section 1, most of the previous studies
on verb clustering assume that verbs are monose-
mous. A typical method in these studies is to rep-
resent each verb as a single data point and apply
classification (e.g., Joanis et al. (2008)) or clus-
tering (e.g., Sun and Korhonen (2009)) to these
data points. As a representation for a data point,
distributions of subcategorization frames are often
used, and other semantic features (e.g., selectional
preferences) are sometimes added to improve the
performance.
Among these studies on monosemous verb clus-
tering (i.e., predominant class induction), there
have been several Bayesian methods. Vlachos
et al. (2009) proposed a Dirichlet process mix-
ture model (DPMM; Neal (2000)) to cluster verbs
based on subcategorization frame distributions.
They evaluated their result with a gold-standard
test set, where a single class is assigned to a verb.
Parisien and Stevenson (2010) proposed a hierar-
chical Dirichlet process (HDP; Teh et al. (2006))
model to jointly learn argument structures (sub-
categorization frames) and verb classes by using
syntactic features. Parisien and Stevenson (2011)
extended their model by adding semantic features.
They tried to account for verb learning by children
and did not evaluate the resultant verb classes.
Modi et al. (2012) extended the model of Titov
and Klementiev (2012), which is an unsupervised
model for inducing semantic roles, to jointly in-
duce semantic roles and frames across verbs using
the Chinese Restaurant Process (Aldous, 1985).
All of the above methods considered verbs to be
monosemous and did not deal with verb polysemy.
Our approach also uses Bayesian methods, but is
designed to capture verb polysemy.
We summarize a few studies that consider poly-
semy of verbs in the rest of this section.
Miyao and Tsujii (2009) proposed a supervised
method that can handle verb polysemy. Their
method represents a verb’s syntactic and seman-
tic features, and learns a log-linear model from
the SemLink corpus (Loper et al., 2007). Boleda
et al. (2007) also proposed a supervised method
for Catalan adjectives considering the polysemy of
adjectives.
The most closely related work to our polysemy-
aware task of unsupervised verb class induction is
the work of Korhonen et al. (2003), who used dis-
tributions of subcategorization frames to cluster
verbs. They adopted the Nearest Neighbor (NN)
and Information Bottleneck (IB) methods for clus-
tering. In particular, they tried to consider verb
polysemy by using the IB method, which is a soft
clustering method (Tishby et al., 1999). However,
the verb itself is still represented as a single data
point. After performing soft clustering, they noted
that most verbs fell into a single class, and they
decided to assign a single class to each verb by
hardening the clustering. They considered multi-
ple classes only in the gold-standard data used for
their evaluations. We also evaluate our induced
verb classes on this gold-standard data, which was
created on the basis of Levin’s classes (Levin,
1993).
Lapata and Brew (2004) and Li and Brew
(2007) proposed probabilistic models for calculat-
ing prior probabilities of verb classes for a verb.
These models are approximated to condition not
</bodyText>
<page confidence="0.989969">
1031
</page>
<bodyText confidence="0.999960294117647">
on verbs but on subcategorization frames. As
mentioned in Li and Brew (2007), it is desirable
to extend the model to depend on verbs to fur-
ther improve accuracy. They conducted several
evaluations including predominant class induction
and token-level verb sense disambiguation, but did
not evaluate multiple classes output by their mod-
els. Schulte im Walde et al. (2008) also applied
probabilistic soft clustering to verbs by incorporat-
ing subcategorization frames and selectional pref-
erences based on WordNet. This model is based
on the Expectation-Maximization algorithm and
the Minimum Description Length principle. Since
they focused on the incorporation of selectional
preferences, they did not evaluate verb classes but
evaluated only selectional preferences using a lan-
guage model-based measure.
Materna proposed LDA-frames, which are de-
fined across verbs and can be considered to be
a kind of verb class (Materna, 2012; Materna,
2013). LDA-frames are probabilistic semantic
frames automatically induced from a raw corpus.
He used a model based on latent Dirichlet allo-
cation (LDA; Blei et al. (2003)) and the Dirichlet
process to cluster verb instances of a triple (sub-
ject, verb, object) to produce semantic frames and
roles. Both of these are represented as a proba-
bilistic distribution of words across verbs. He ap-
plied this method to the BNC and acquired 1,200
frames and 400 roles (Materna, 2012). He did not
evaluate the resulting frames as verb classes.
In sum, there have been no studies that quantita-
tively evaluate polysemous verb classes automati-
cally induced by unsupervised methods.
</bodyText>
<sectionHeader confidence="0.952606" genericHeader="method">
3 Our Approach
</sectionHeader>
<subsectionHeader confidence="0.989248">
3.1 Overview
</subsectionHeader>
<bodyText confidence="0.999608194444444">
Our objective is to automatically learn semantic
frames and verb classes from a massive amount
of verb uses following usage-based approaches.
Although Bayesian approaches are a possible so-
lution to simultaneously induce frames and verb
classes from a corpus as used in previous stud-
ies, it has prohibitive computational cost. For in-
stance, Parisien and Stevenson applied HDP only
to a small-scale child speech corpus that contains
170K verb uses to jointly induce subcategoriza-
tion frames and verb classes (Parisien and Steven-
son, 2010; Parisien and Stevenson, 2011). Ma-
terna applied an LDA-based method to the BNC,
which contains 1.4M verb uses, to induce seman-
tic frames across verbs that can be considered to
be verb classes (Materna, 2012; Materna, 2013).
However, it would take three months for this ex-
periment using this 100 million word corpus.&apos; Al-
though it is best to use the largest possible cor-
pus for this kind of knowledge acquisition tasks
(Sasano et al., 2009), it is infeasible to scale to
giga-word corpora using such joint models.
In this paper, we propose a two-step approach
for inducing semantic frames and verb classes.
First, we make multiple data points for each verb
to deal with verb polysemy (cf. polysemy-aware
previous studies still represented a verb as one
data point (Korhonen et al., 2003; Miyao and Tsu-
jii, 2009)). To do that, we induce verb-specific
semantic frames by clustering verb uses. Then,
we induce verb classes by clustering these verb-
specific semantic frames across verbs. An interest-
ing point here is that we can use exactly the same
method for these two clustering steps.
Our procedure to automatically induce verb
classes from verb uses is summarized as follows:
</bodyText>
<listItem confidence="0.891100857142857">
1. induce verb-specific semantic frames by clus-
tering predicate-argument structures for each
verb extracted from automatic parses as
shown in the lower part of Figure 1, and
2. induce verb classes by clustering the induced
semantic frames across verbs as shown in the
upper part of Figure 1.
</listItem>
<bodyText confidence="0.671568">
Each of these two steps is described in the follow-
ing sections in detail.
</bodyText>
<subsectionHeader confidence="0.999631">
3.2 Inducing Verb-specific Semantic Frames
</subsectionHeader>
<bodyText confidence="0.9977115">
We induce verb-specific semantic frames from
verb uses based on the method of Kawahara et al.
(2014). Our semantic frames consist of case slots,
each of which consists of word instances that can
be filled. The procedure for inducing these seman-
tic frames is as follows:
</bodyText>
<listItem confidence="0.922068">
1. apply dependency parsing to a raw corpus
and extract predicate-argument structures for
each verb from the automatic parses,
2. merge the predicate-argument structures that
have presumably the same meaning based
on the assumption of one sense per colloca-
tion (Yarowsky, 1993) to get a set of initial
frames, and
</listItem>
<footnote confidence="0.681668">
&apos;In our replication experiment, it took a week to perform
70 iterations using Materna’s code and an Intel Xeon E5-2680
(2.7GHz) CPU. To reach 1,000 iterations, which are reported
to be optimum, it would take three months.
</footnote>
<page confidence="0.994905">
1032
</page>
<bodyText confidence="0.9466626">
3. apply clustering to the initial frames based
on the Chinese Restaurant Process (Al-
dous, 1985) to produce verb-specific seman-
tic frames.
These three steps are briefly described below.
</bodyText>
<subsectionHeader confidence="0.9914485">
3.2.1 Extracting Predicate-argument
Structures from a Raw Corpus
</subsectionHeader>
<bodyText confidence="0.995698833333333">
We apply dependency parsing to a large raw cor-
pus. We use the Stanford parser with Stanford
dependencies (de Marneffe et al., 2006).2 Col-
lapsed dependencies are adopted to directly extract
prepositional phrases.
Then, we extract predicate-argument structures
from the dependency parses. Dependents that have
the following dependency relations to a verb are
extracted as arguments:
nsubj, xsubj, dobj, iobj, ccomp, xcomp,
prep ∗
In this process, the verb and arguments are lem-
matized, and only the head of an argument is pre-
served for compound nouns.
Predicate-argument structures are collected for
each verb and the subsequent processes are ap-
plied to the predicate-argument structures of each
verb.
</bodyText>
<subsectionHeader confidence="0.9971845">
3.2.2 Constructing Initial Frames from
Predicate-argument Structures
</subsectionHeader>
<bodyText confidence="0.999540277777778">
To make the computation feasible, we merge the
predicate-argument structures that have the same
or similar meaning to get initial frames. These ini-
tial frames are the input of the subsequent cluster-
ing process. For this merge, we assume one sense
per collocation (Yarowsky, 1993) for predicate-
argument structures.
For each predicate-argument structure of a verb,
we couple the verb and an argument to make a unit
for sense disambiguation. We select an argument
in the following order by considering the degree of
effect on the verb sense:3
dobj, ccomp, nsubj, prep ∗, iobj.
Then, the predicate-argument structures that
have the same verb and argument pair (slot and
word, e.g., “dobj:effect”) are merged into an ini-
tial frame. After this process, we discard minor
initial frames that occur fewer than 10 times.
</bodyText>
<footnote confidence="0.984605333333333">
2http://nlp.stanford.edu/software/lex-parser.shtml
3If a predicate-argument structure has multiple preposi-
tional phrases, one of them is randomly selected.
</footnote>
<subsectionHeader confidence="0.944968">
3.2.3 Clustering Method
</subsectionHeader>
<bodyText confidence="0.999992833333333">
We cluster initial frames for each verb to pro-
duce semantic frames using the Chinese Restau-
rant Process (Aldous, 1985), regarding each initial
frame as an instance.
We calculate the posterior probability of a clus-
ter cj given an initial frame fi as follows:
</bodyText>
<equation confidence="0.9877728">
�n(c�)
N+α · P(fi|cj) cj ≠ new
P(cj|fi) ∝ (1)
N+α · P(fi|cj) cj = new,
α
</equation>
<bodyText confidence="0.958377222222222">
where N is the number of initial frames for the tar-
get verb and n(cj) is the current number of initial
frames assigned to the cluster cj. α is a hyper-
parameter that determines how likely it is for a
new cluster to be created. In this equation, the first
term is the Dirichlet process prior and the second
term is the likelihood of fi.
P(fi|cj) is defined based on the Dirichlet-
Multinomial distribution as follows:
</bodyText>
<equation confidence="0.994913">
P(fi|cj) = � P(w|cj)count(fz,w), (2)
wEV
</equation>
<bodyText confidence="0.999980888888889">
where V is the vocabulary in all case slots cooc-
curring with the verb and count(fi, w) is the num-
ber of w in the initial frame fi. The original
method in Kawahara et al. (2014) defined w as
pairs of slots and words, e.g., “nsubj:child” and
“dobj:bird,” but does not consider slot-only fea-
tures, e.g., “nsubj” and “dobj,” which ignore lex-
ical information. Here we experiment with both
representations and compare the results.
</bodyText>
<equation confidence="0.98192525">
P(w|cj) is defined as follows:
count(cj, w) + β
P(w|cj) = &amp;EV count(cj, t) + |V
 |· β, (3)
</equation>
<bodyText confidence="0.9999602">
where count(cj, w) is the current number of w
in the cluster cj, and β is a hyper-parameter of
Dirichlet distribution. For a new cluster, this prob-
ability is uniform (1/|V |).
We regard each output cluster as a semantic
frame, by merging the initial frames in a clus-
ter into a semantic frame. In this way, semantic
frames for each verb are acquired.
We use Gibbs sampling to realize this cluster-
ing.
</bodyText>
<subsectionHeader confidence="0.998422">
3.3 Inducing Verb Classes from Semantic
Frames
</subsectionHeader>
<bodyText confidence="0.999916">
To induce verb classes across verbs, we apply
clustering to the induced verb-specific semantic
</bodyText>
<page confidence="0.94928">
1033
</page>
<bodyText confidence="0.99990147368421">
frames. We can use exactly the same clustering
method as described in Section 3.2.3 by using se-
mantic frames for multiple verbs as an input in-
stead of initial frames for a single verb. This is
because an initial frame has the same structure as
a semantic frame, which is produced by merging
initial frames. We regard each output cluster as a
verb class this time.
For the features, w, in equation (2), we try the
two representations again: slot-only features and
slot-word pair features. The representation using
only slots corresponds to the consideration of only
syntactic argument patterns. The other representa-
tion using the slot-word pairs means that semantic
similarity based on word overlap is naturally con-
sidered by looking at lexical information. We will
compare in our experiments four possible combi-
nations: two feature representations for each of the
two clustering steps.
</bodyText>
<sectionHeader confidence="0.996948" genericHeader="evaluation">
4 Experiments and Evaluations
</sectionHeader>
<bodyText confidence="0.999993888888889">
We first describe our experimental settings and de-
fine evaluation metrics to evaluate induced soft
clusterings of verb classes. Then, we con-
duct type-level multi-class evaluations, type-level
single-class evaluations and token-level multi-
class evaluations. These two levels of evaluations
are performed by considering the work of Reichart
et al. (2010) on clustering evaluation. Finally, we
discuss the results of our full experiments.
</bodyText>
<subsectionHeader confidence="0.9939">
4.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.999991392857143">
We use two kinds of large-scale corpora: a web
corpus and the English Gigaword corpus.
To prepare a web corpus, we extracted sen-
tences from crawled web pages that are judged to
be written in English based on the encoding infor-
mation. Then, we selected sentences that consist
of at most 40 words, and removed duplicated sen-
tences. From this process, we obtained a corpus of
one billion sentences, totaling approximately 20
billion words. We focused on verbs whose fre-
quency in the web corpus was more than 1,000.
There were 19,649 verbs, including phrasal verbs,
and separating passive and active constructions.
We extracted 2,032,774,982 predicate-argument
structures.
We also used the English Gigaword corpus
(LDC2011T07; English Gigaword Fifth Edition).
This corpus consists of approximately 180 mil-
lion sentences, which totaling four billion words.
There were 7,356 verbs after applying the same
frequency threshold as the web corpus. We ex-
tracted 423,778,278 predicate-argument structures
from this corpus.
We set the hyper-parameters α in (1) and Q in
(3) to 1.0. The cluster assignments for all the com-
ponents were initialized randomly. We took 100
samples for each input frame and selected the clus-
ter assignment that has the highest probability.
</bodyText>
<subsectionHeader confidence="0.973538">
4.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.9999963125">
To measure the precision and recall of a cluster-
ing, modified purity and inverse purity (also called
collocation or weighted class accuracy) are com-
monly used in previous studies on verb clustering
(e.g., Sun and Korhonen (2009)). However, since
these measures are only applicable to a hard clus-
tering, it is necessary to extend them to be applica-
ble to a soft clustering, because in our task a verb
can belong to multiple clusters or classes.4 We
propose a normalized version of modified purity
and inverse purity. This kind of normalization for
soft clusterings was performed for other evalua-
tion metrics as in Springorum et al. (2013).
To measure the precision of a clustering, a nor-
malized version of modified purity is defined as
follows. Suppose K is the set of automatically in-
duced clusters and G is the set of gold classes. Let
Ki be the verb vector of the i-th cluster and Gj be
the verb vector of the j-th gold class. Each com-
ponent of these vectors is a normalized frequency,
which equals a cluster/class attribute probability
given a verb. Where there is no frequency in-
formation available for class distribution, such as
the gold-standard data described in Section 4.3,
we use a uniform distribution across the verb’s
classes. The core idea of purity is that each clus-
ter Ki is associated with its most prevalent gold
class. In addition, to penalize clusters that consist
of only one verb, such singleton clusters in K are
considered as errors, as is usual with modified pu-
rity. The normalized modified purity (nmPU) can
then be written as follows:
</bodyText>
<equation confidence="0.9635015">
1 � nmPU = �
i s.t. �Ki&gt;1
JKi(Ki n Gj) = � civ, (5)
vEKinGj
</equation>
<footnote confidence="0.966611">
4Korhonen et al. (2003) evaluated hard clusterings based
on a gold standard with multiple classes per verb. They re-
ported only precision measures including modified purity,
and avoided extending the evaluation metrics for soft clus-
terings.
</footnote>
<table confidence="0.940447307692308">
JKi(Ki n Gj), (4)
max
j
1034
verb classes verb classes
place 9 drop 9, 45, 004, 47,
dye 24, 21, 41 51, A54, A30
focus 31, 45 bake 26, 45
stare 30 persuade 002
lay 9 sparkle 43
build 26, 45 pour 9, 43, 26, 57,
force 002, 11 13, 31
glow 43 invent 26, 27
</table>
<tableCaption confidence="0.999848">
Table 1: An excerpt of the gold-standard verb
</tableCaption>
<bodyText confidence="0.977816352941176">
classes for several verbs from Korhonen et al.
(2003). The classes starting with ‘0’ were de-
rived from the LCS database, those starting with
‘A’ were defined by Korhonen et al., and the other
classes were from Levin’s classes. A bolded class
is the predominant class for each verb.
where N denotes the total number of verbs, |Ki|
denotes the number of positive components in
Ki, and civ denotes the v-th component of Ki.
SKi(Ki ∩ Gj) means the total mass of the set of
verbs in Ki ∩ Gj, given by summing up the values
in Ki. In case of evaluating a hard clustering, this
is equal to |Ki ∩ Gj |because all the values of civ
are equal to 1.
As usual, the following normalized inverse pu-
rity (niPU) is used to measure the recall of a clus-
tering:
</bodyText>
<equation confidence="0.905355">
SGS(Ki ∩ Gj). (6)
</equation>
<bodyText confidence="0.999981">
Finally, we use the harmonic mean (Fi) of nmPU
and niPU as a single measure of clustering quality.
</bodyText>
<subsectionHeader confidence="0.990784">
4.3 Type-level Multi-class Evaluations
</subsectionHeader>
<bodyText confidence="0.9998891875">
We first evaluate our induced verb classes on the
test set created by Korhonen et al. (2003) (Table 1
of their paper) which was created by considering
verb polysemy on the basis of Levin’s classes and
the LCS database (Dorr, 1997). It consists of 62
classes and 110 verbs, out of which 35 verbs are
monosemous and 75 verbs are polysemous. The
average number of verb classes per verb is 2.24.
An excerpt from this data is shown in Table 1.
As our baselines, we adopt two previously pro-
posed methods. We first implemented a soft clus-
tering method for verb class induction proposed by
Korhonen et al. (2003). They used the information
bottleneck (IB) method for assigning probabilities
of classes to each verb. Note that Korhonen et al.
(2003) actually hardened the clusterings and left
</bodyText>
<table confidence="0.999813238095238">
method K nmPU niPU Fi
IB (k=35, t=0.10) 35.0 53.59 51.44 52.44
IB (k=35, t=0.05) 35.0 53.67 52.62 53.10
IB (k=35, t=0.02) 35.0 54.42 54.43 54.40
IB (k=35, t=0.01) 35.0 54.60 55.54 55.04
IB (k=42, t=0.10) 41.6 55.42 49.46 52.24
IB (k=42, t=0.05) 41.8 55.55 49.97 52.59
IB (k=42, t=0.02) 42.0 56.19 51.24 53.58
IB (k=42, t=0.01) 42.0 56.80 51.92 54.24
LDA-frames (t=0.10) 100 47.52 56.83 51.76
LDA-frames (t=0.05) 165 50.46 67.94 57.91
LDA-frames (t=0.02) 306 49.98 75.50 60.14
LDA-frames (t=0.01) 458 49.55 82.71 61.97
Gigaword/S-S 272.8 63.46 67.66 65.49
Gigaword/S-SW 36.4 31.49 95.70 47.38
Gigaword/SW-S 186.2 63.52 64.18 63.84
Gigaword/SW-SW 30.0 36.27 94.66 52.40
web/S-S 363.6 61.32 78.64 68.90
web/S-SW 52.2 35.80 99.30 52.62
web/SW-S 212.2 66.26 77.38 71.39
web/SW-SW 55.0 36.70 96.25 53.13
</table>
<tableCaption confidence="0.973815">
Table 2: Type-level multi-class evaluations. K rep-
</tableCaption>
<bodyText confidence="0.998676962962963">
resents the (average) number of induced classes.
“S” denotes the use of slot-only features and “SW”
denotes the use of slot-word pair features. For ex-
ample, “SW-S” means that slot-word pair features
are used for semantic frame induction and slot-
only features are used for verb class induction.
the evaluations of soft clusterings for their future
work. For input data, we employ VALEX (Ko-
rhonen et al., 2006), which is a publicly-available
large-scale subcategorization lexicon.5 By follow-
ing the method of Korhonen et al. (2003), preposi-
tional phrases (pp) are parameterized for two fre-
quent subcategorization frames (NP and NP PP),
and the unfiltered raw frequencies of subcatego-
rization frames are used as features to represent
a verb. It is necessary to specify the number of
clusters, k, for the IB method beforehand, and
we adopt 35 and 42 clusters according to their re-
ported high accuracies. To output multiple classes
for each verb, we set a threshold, t, for class at-
tribute probabilities. That is, classes that have a
higher class attribute probability than the thresh-
old are output for each verb. We report the results
of the following threshold values: 0.01, 0.02, 0.05
and 0.10.
The other baseline is LDA-frames (Materna,
2012). We use the induced LDA-frames that are
</bodyText>
<equation confidence="0.881879666666667">
5http://ilexir.co.uk/applications/valex/
1 �
niPU = N
j
Max
i
</equation>
<page confidence="0.978635">
1035
</page>
<table confidence="0.999007727272727">
method K predominant iPU class multiple class eval
mPU eval mPU niPU F1
F1
NN 24 46.36 52.73 49.34 52.73 46.85 49.62
IB (k=35) 34.8 42.73 51.82 46.82 51.64 46.83 49.09
IB (k=42) 41.0 47.45 50.91 49.11 55.27 45.45 49.87
LDA-frames 53 30.00 47.27 36.71 41.82 44.28 43.01
Gigaword/S 9.6 25.64 71.27 37.70 32.91 64.71 43.62
Gigaword/SW 10.6 30.36 71.09 42.25 39.82 66.92 49.70
web/S 20.4 42.73 61.46 50.31 54.91 57.12 55.86
web/SW 11.8 34.36 71.82 46.40 49.09 67.01 56.50
</table>
<tableCaption confidence="0.996363">
Table 3: Type-level single-class evaluations against predominant/multiple classes. K represents the (av-
erage) number of induced classes.
</tableCaption>
<bodyText confidence="0.997700151515151">
available on the web site.6 This frame data was in-
duced from the BNC and consists of 1,200 frames
and 400 semantic roles. Again, we set a threshold
for frame attribute probabilities.
We report results using our methods with four
feature combinations (slot-only (S) and slot-word
pair (SW) features each used for both the frame-
generation and verb-class clustering steps) for
both the Gigaword and web corpora. Table 2 lists
evaluation results for the baseline methods and our
methods.7 The results of the IB baseline and our
methods are obtained by averaging five runs.
We can see that “web/SW-S” achieved the best
performance and obtained a higher F1 than the
baselines by more than nine points. “Web/SW-
S” uses the combination of slot-word pair fea-
tures for clustering verb-specific frames and slot-
only features for clustering across verbs. Inter-
estingly, this result indicates that slot distributions
are more effective than lexical information in slot-
word pairs for inducing verb classes similar to the
gold standard. This result is consistent with ex-
pectations, given a gold standard based on Levin’s
verb classes, which are organized according to the
syntactic behavior of verbs. The use of slot-word
pairs for verb class induction generally merged too
many frames into each class, apparently due to ac-
cidental word overlaps across verbs.
The verb classes induced from the web corpus
achieved a higher F1 than those from the Gigaword
corpus. This can be attributed to the larger size of
the web corpus. The employment of this kind of
huge corpus is enabled by our scalable method.
</bodyText>
<footnote confidence="0.7961315">
6http://nlp.fi.muni.cz/projekty/lda-frames/
7Although we do not think that the classes with very small
</footnote>
<bodyText confidence="0.693235333333333">
attribute probabilities are meaningful, the Fl scores for lower
thresholds than 0.01 converged to about 66 in the case of
LDA-frames.
</bodyText>
<subsectionHeader confidence="0.9741805">
4.4 Type-level Single-class Evaluations
against Predominant/Multiple Classes
</subsectionHeader>
<bodyText confidence="0.999973111111111">
Since we focus on the handling of verb polysemy,
predominant class induction for each verb is not
our main objective. However, we wish to compare
our method with previous work on the induction of
a predominant (monosemous) class for each verb.
To output a single class for each verb by us-
ing our proposed method, we skip the induction
of verb-specific semantic frames and instead cre-
ate a single frame for each verb by merging all
predicate-argument structures of the verb. Then,
we apply clustering to these frames across verbs.
For clustering features, we again compare two rep-
resentations: slot-only features (S) and slot-word
pair features (SW).
We evaluate the single-class output for each
verb based on the predominant gold-standard
classes, which are defined for each verb in the
test set of Korhonen et al. (2003). This data con-
tains 110 verbs and 33 classes. We evaluate these
single-class outputs in the same manner as Korho-
nen et al. (2003), using the gold standard with mul-
tiple classes, which we also use for our multi-class
evaluations.
As we did with the multi-class evaluations, we
adopt modified purity (mPU), inverse purity (iPU)
and their harmonic mean (F1) as the metrics for the
evaluation with predominant classes. It is not nec-
essary to normalize these metrics when we treat
verbs as monosemous, and evaluate against the
predominant sense. When we evaluate against the
multiple classes in the gold standard, we do nor-
malize the inverse purity.
For baselines, we once more adopt the Nearest
Neighbor (NN) and Information Bottleneck (IB)
methods proposed by Korhonen et al. (2003), and
LDA-frames proposed by Materna (2012). The
</bodyText>
<page confidence="0.98872">
1036
</page>
<bodyText confidence="0.9997832">
clusterings with the NN and IB methods are ob-
tained by using the VALEX subcategorization lex-
icon. To harden the clusterings of the IB method
and the LDA-frames, the class with the highest
probability is selected for each verb. This hard-
ening process is exactly the same as Korhonen et
al. (2003). Note that our results of the NN and IB
methods are different from those reported in their
paper since the data source is different.8
Table 3 lists accuracies of baseline methods and
our methods. Our proposed method using the web
corpus achieved comparable performance with the
baseline methods on the predominant class evalu-
ation and outperformed them on the multiple class
evaluation. More sophisticated methods for pre-
dominant class induction, such as the method of
Sun and Korhonen (2009) using selectional pref-
erences, could produce better single-class outputs,
but have difficulty in producing polysemy-aware
verb classes.
From the result, we can see that the induced
verb classes based on slot-only features did not
achieve a higher F1 than those based on slot-word
pair features in many cases. This result is differ-
ent from that of multi-class evaluations in Section
4.3. We speculate that slot distributions are not so
different among verbs when all uses of a verb are
merged into one frame, and thus their discrimina-
tion power is lower than that in the intermediate
construction of semantic frames.
</bodyText>
<subsectionHeader confidence="0.979354">
4.5 Token-level Multi-class Evaluations
</subsectionHeader>
<bodyText confidence="0.999960529411765">
We conduct token-level multi-class evaluations us-
ing 119 verbs, which appear 100 or more times in
sections 02-21 of the SemLink WSJ corpus. These
119 verbs cover 102 VerbNet classes, and 48 of
them are polysemous in the sense of being in more
than one VerbNet class. Each instance of these 119
verbs in this corpus belongs to one of 102 Verb-
Net classes. We first add these instances to the
instances from a raw corpus and apply the two-
step clustering to these merged instances. Then,
we compare the induced verb classes of the Sem-
Link instances with their gold-standard VerbNet
classes. We report the values of modified purity
(mPU), inverse purity (iPU) and their harmonic
mean (F1). It is not necessary to normalize these
metrics because the clustering of these instances is
hard.
</bodyText>
<footnote confidence="0.980418333333333">
8Korhonen et al. (2003) reported that the highest modified
purity was 49% against predominant classes and 60% against
multiple classes.
</footnote>
<table confidence="0.999425666666667">
method K mPU iPU F1
Gigaword/S-NIL – 93.43 20.06 33.03
Gigaword/SW-NIL – 94.45 41.07 57.25
Gigaword/S-S 512.2 75.06 45.26 56.47
Gigaword/SW-S 260.6 73.98 56.45 64.04
web/S-NIL – 93.70 32.96 48.76
web/SW-NIL – 94.51 44.95 60.92
web/S-S 500.0 72.25 52.48 60.79
web/SW-S 255.2 72.65 61.00 66.31
</table>
<tableCaption confidence="0.792059333333333">
Table 4: Token-level evaluations against VerbNet
classes. K represents the average number of in-
duced classes.
</tableCaption>
<bodyText confidence="0.9996215">
For clustering features, we compare two fea-
ture combinations: “S-S” and “SW-S,” which
achieved high performance in the type-level multi-
class evaluations (Section 4.3). The results of
these methods are obtained by averaging five runs.
For a baseline, we use verb-specific semantic
frames without clustering across verbs (“S-NIL”
and “SW-NIL”), where these frames are consid-
ered to be verb classes but not shared across verbs.
Table 4 lists accuracies of these methods for the
two corpora. We can see that “SW-S” achieved
a higher F1 than “S-S” and the baselines without
verb class induction (“S-NIL” and “SW-NIL”).
Modi et al. (2012) induced semantic frames
across verbs using the monosemous assumption
and reported an F1 of 44.7% (77.9% PU and
31.4% iPU) for the assignment of FrameNet
frames to the FrameNet corpus. We also con-
ducted the above evaluation against FrameNet
frames for 75 verbs.9 We achieved an F1 of
62.79% (66.97% mPU and 59.09% iPU) for
“web/SW-S,” and an F1 of 60.06% (65.58% mPU
and 55.39% iPU) for “Gigaword/SW-S.” It is dif-
ficult to directly compare these results with Modi
et al. (2012), but our induced verb classes seem to
have higher F1 accuracy.
</bodyText>
<subsectionHeader confidence="0.999652">
4.6 Full Experiments and Discussions
</subsectionHeader>
<bodyText confidence="0.999977125">
We finally induce verb classes from the semantic
frames of 1,667 verbs, which appear at least once
in sections 02-21 of the WSJ corpus. Based on
the best results in the above evaluations, we in-
duced semantic frames using slot-word pair fea-
tures, and then induced verb classes using slot-
only features. We ended with 38,481 semantic
frames and 699 verb classes from the Gigaword
</bodyText>
<footnote confidence="0.971449666666667">
9Since FrameNet frames are not assigned to all verbs of
SemLink, the number of verbs is different from the evalua-
tions against VerbNet classes.
</footnote>
<page confidence="0.9971">
1037
</page>
<tableCaption confidence="0.9725545">
Table 5: Examples of induced verb classes. Un-
derlined semantic frames are shown in Table 6.
</tableCaption>
<bodyText confidence="0.997050941176471">
corpus, and 61,903 semantic frames and 840 verb
classes from the web corpus. It took two days to
induce verb classes from the Gigaword corpus and
three days from the web corpus.
Examples of verb classes and semantic frames
induced from the web corpus are shown in Table
5 and Table 6. While there are many classes with
consistent meanings, such as “Class 4” and “Class
16,” some classes have mixed meanings. For in-
stance, “Class 2” consists of the semantic frames
“need:2” and “say:2.” These frames were merged
due to the high syntactic similarity of constituting
slot distributions, which are comprised of a sub-
ject and a sentential complement. To improve the
quality of verb classes, it is necessary to develop
a clustering model that can consider syntactic and
lexical similarity in a balanced way.
</bodyText>
<sectionHeader confidence="0.993859" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999949727272727">
We presented a step-wise unsupervised method
for inducing verb classes from instances in giga-
word corpora. This method first clusters predicate-
argument structures to induce verb-specific se-
mantic frames and then clusters these semantic
frames across verbs to induce verb classes. Both
clustering steps are performed with exactly the
same method, which is based on the Chinese
Restaurant Process. The resulting semantic frames
and verb classes are open to the public and also can
be searched via our web interface.lo
</bodyText>
<footnote confidence="0.814944">
10http://nlp.ist.i.kyoto-u.ac.jp/member/kawahara/cf/crp.en/
</footnote>
<table confidence="0.999707">
slot instance words
need:2 nsubj you:2150273, i:7678, we:4599, ...
ccomp (s):2193321
say:2 nsubj she:1705781, he:20693, i:9422, ...
ccomp (s):1829616
inform:1 nsubj i:11100, he:10323, we:6373, ...
dobj me:30646, you:27678, us:21642, ...
prep of decision:846, this:759, situation:688, ...
...
notify:2 nsubj we:7505, you:3439, i:1035, ...
dobj you:18604, us:7281, them:3649, ...
prep of change:1540, problem:496, status:386, ...
...
</table>
<tableCaption confidence="0.974035">
Table 6: Examples of induced semantic frames.
</tableCaption>
<bodyText confidence="0.992838423076923">
The number following an instance word denotes
its frequency and ⟨s⟩ denotes a sentential comple-
ment.
From the results, we can see that the combi-
nation of the slot-word pair features for cluster-
ing verb-specific frames and the slot-only features
for clustering across verbs is the most effective
and outperforms the baselines by approximately
10 points. This indicates that slot distributions
are more effective than lexical information in slot-
word pairs for the induction of verb classes, when
Levin-style classes are used for evaluation. This
is consistent with Levin’s principle of organizing
verb classes according to the syntactic behavior of
verbs.
As applications of the resulting semantic frames
and verb classes, we plan to integrate them into
syntactic parsing, semantic role labeling and verb
sense disambiguation. For instance, Kawahara
and Kurohashi (2006) improved accuracy of de-
pendency parsing based on Japanese semantic
frames automatically induced from a raw corpus.
It is also valuable and promising to apply the in-
duced verb classes to NLP applications as used in
metaphor identification (Shutova et al., 2010) and
argumentative zoning (Guo et al., 2011).
</bodyText>
<sectionHeader confidence="0.997488" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9984505">
This work was supported by Kyoto University
John Mung Program and JST CREST. We also
gratefully acknowledge the support of the National
Science Foundation Grant NSF-IIS-1116782, A
Bayesian Approach to Dynamic Lexical Re-
sources for Flexible Language Processing. Any
opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the
authors and do not necessarily reflect the views of
the National Science Foundation.
</bodyText>
<figure confidence="0.96317836">
class
semantic frames
Class 1
Class 2
Class 3
Class 4
Class 5
Class 6
Class 7
Class 8
Class 9
Class 10
Class 11
Class 12
Class 13
Class 14
Class 15
Class 16
Class 17
Class 18
Class 19
Class 20
rave:1, talk:1
need:2,say:2
smell:1, sound:1
</figure>
<table confidence="0.970652294117647">
concentrate:1, focus:1
express:2, inquire:62, voice:1
revolve:1, snake:2, wrap:2
hand:1, hand:3, hand:4
depend:1, rely:1, rely:3
collaborate:1, compete:2, work:1
coach:3, teach:3, teach:4
dance:1, react:1, stick:1
advise:8, express:4, quiz:10, voice:2
give:18, grant:6, offer:11, offer:12
keep:14, keep:18, stay:4, stay:488
cuff:5, fasten:2, tie:1, tie:4
arrange:3, book:4, make:27, reserve:5
deport:6, differ:1, fluctuate:1, vary:1
peek:1, peek:3, peer:1, peer:7, ...
groan:1, growl:1, hiss:1, moan:1, purr:1
inform:1, notify:2, remind:1, beware:1, ...
</table>
<page confidence="0.982545">
1038
</page>
<sectionHeader confidence="0.989918" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.990183819047619">
David Aldous. 1985. Exchangeability and related top-
ics. ´Ecole d’ ´Et´e de Probabilit´es de Saint-Flour XIII
―1983, pages 1–198.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. the Journal of
Machine Learning Research, 3:993–1022.
Gemma Boleda, Sabine Schulte im Walde, and Toni
Badia. 2007. Modelling polysemy in adjective
classes by multi-label classification. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 171–180.
Hoa Trang Dang. 2004. Investigations into the role
of lexical semantics in word sense disambiguation.
Ph.D. thesis, University of Pennsylvania.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation, pages 449–
454.
Bonnie J. Dorr. 1997. Large-scale dictionary con-
struction for foreign language tutoring and inter-
lingual machine translation. Machine Translation,
12(4):271–322.
Ingrid Falk, Claire Gardent, and Jean-Charles Lamirel.
2012. Classifying French verbs using French and
English lexical resources. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics, pages 854–863.
Yufan Guo, Anna Korhonen, and Thierry Poibeau.
2011. A weakly-supervised approach to argumen-
tative zoning of scientific documents. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing, pages 273–283.
Eric Joanis, Suzanne Stevenson, and David James.
2008. A general feature space for automatic
verb classification. Natural Language Engineering,
14(3):337–367.
Daisuke Kawahara and Sadao Kurohashi. 2006. A
fully-lexicalized probabilistic model for Japanese
syntactic and case structure analysis. In Proceedings
of the Human Language Technology Conference of
the NAACL, pages 176–183.
Daisuke Kawahara, Daniel W. Peterson, Octavian
Popescu, and Martha Palmer. 2014. Inducing
example-based semantic frames from a massive
amount of verb uses. In Proceedings of the 14th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics.
Karin Kipper-Schuler. 2005. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. the-
sis, University of Pennsylvania.
Anna Korhonen, Yuval Krymolowski, and Zvika
Marx. 2003. Clustering polysemic subcategoriza-
tion frame distributions semantically. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 64–71.
Anna Korhonen, Yuval Krymolowski, and Ted Briscoe.
2006. A large subcategorization lexicon for natural
language processing applications. In Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation, pages 345–352.
Mirella Lapata and Chris Brew. 2004. Verb class
disambiguation using informative priors. Computa-
tional Linguistics, 30(1):45–73.
Beth Levin. 1993. English verb classes and alterna-
tions: A preliminary investigation. The University
of Chicago Press.
Jianguo Li and Chris Brew. 2007. Disambiguating
Levin verbs using untagged data. In Proceedings
of the International Conference Recent Advances in
Natural Language Processing.
Jianguo Li and Chris Brew. 2008. Which are the best
features for automatic verb classification. In Pro-
ceedings of ACL-08: HLT, pages 434–442.
Thomas Lippincott, Anna Korhonen, and Diarmuid
O´ S´eaghdha. 2012. Learning syntactic verb frames
using graphical models. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 420–429.
Edward Loper, Szu-Ting Yi, and Martha Palmer. 2007.
Combining lexical resources: mapping between
PropBank and VerbNet. In Proceedings of the 7th
International Workshop on Computational Linguis-
tics.
Jiˇr´ı Materna. 2012. LDA-frames: An unsupervised ap-
proach to generating semantic frames. In Proceed-
ings of the 13th International Conference CICLing
2012, Part I, pages 376–387.
Jiˇr´ı Materna. 2013. Parameter estimation for LDA-
frames. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 482–486.
Yusuke Miyao and Jun’ichi Tsujii. 2009. Supervised
learning of a probabilistic lexicon of verb semantic
classes. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1328–1337.
Ashutosh Modi, Ivan Titov, and Alexandre Klementiev.
2012. Unsupervised induction of frame-semantic
representations. In Proceedings of the NAACL-HLT
Workshop on the Induction of Linguistic Structure,
pages 1–7.
</reference>
<page confidence="0.87253">
1039
</page>
<reference confidence="0.999915009259259">
Radford M. Neal. 2000. Markov chain sampling meth-
ods for Dirichlet process mixture models. Journal
of computational and graphical statistics, 9(2):249–
265.
Christopher Parisien and Suzanne Stevenson. 2010.
Learning verb alternations in a usage-based
Bayesian model. In Proceedings of the 32nd Annual
Meeting of the Cognitive Science Society.
Christopher Parisien and Suzanne Stevenson. 2011.
Generalizing between form and meaning using
learned verb classes. In Proceedings of the 33rd An-
nual Meeting of the Cognitive Science Society.
Roi Reichart and Anna Korhonen. 2013. Improved
lexical acquisition through DPP-based verb cluster-
ing. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics,
pages 862–872.
Roi Reichart, Omri Abend, and Ari Rappoport. 2010.
Type level clustering evaluation: New measures and
a POS induction case study. In Proceedings of the
14th Conference on Computational Natural Lan-
guage Learning, pages 77–87.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2009. The effect of corpus size on case frame
acquisition for discourse analysis. In Proceedings of
Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, pages
521–529.
Sabine Schulte im Walde, Christian Hying, Christian
Scheible, and Helmut Schmid. 2008. Combining
EM training and the MDL principle for an automatic
verb classification incorporating selectional prefer-
ences. In Proceedings of ACL-08: HLT, pages 496–
504.
Sabine Schulte im Walde. 2006. Experiments on
the automatic induction of German semantic verb
classes. Computational Linguistics, 32(2):159–194.
Lei Shi and Rada Mihalcea. 2005. Putting pieces to-
gether: Combining FrameNet, VerbNet and Word-
Net for robust semantic parsing. In Computational
Linguistics and Intelligent Text Processing, pages
100–111. Springer.
Ekaterina Shutova, Lin Sun, and Anna Korhonen.
2010. Metaphor identification using verb and noun
clustering. In Proceedings of the 23rd International
Conference on Computational Linguistics, pages
1002–1010.
Sylvia Springorum, Sabine Schulte im Walde, and Ja-
son Utt. 2013. Detecting polysemy in hard and
soft cluster analyses of German preposition vector
spaces. In Proceedings of the 6th International Joint
Conference on Natural Language Processing, pages
632–640.
Suzanne Stevenson and Eric Joanis. 2003. Semi-
supervised verb class discovery using noisy features.
In Proceedings of the 7th Conference on Natural
Language Learning, pages 71–78.
Rajen Subba and Barbara Di Eugenio. 2009. An effec-
tive discourse parser that uses rich linguistic infor-
mation. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 566–574.
Lin Sun and Anna Korhonen. 2009. Improving verb
clustering with automatically acquired selectional
preferences. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 638–647.
Lin Sun, Anna Korhonen, and Yuval Krymolowski.
2008. Automatic classification of English verbs us-
ing rich syntactic features. In Proceedings of the
3rd International Joint Conference on Natural Lan-
guage Processing, pages 769–774.
Lin Sun, Diana McCarthy, and Anna Korhonen. 2013.
Diathesis alternation approximation for verb clus-
tering. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, Short Papers, pages 736–741.
Robert Swier and Suzanne Stevenson. 2005. Exploit-
ing a verb lexicon in automatic semantic role la-
belling. In Proceedings of Human Language Tech-
nology Conference and Conference on Empirical
Methods in Natural Language Processing, pages
883–890.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal,
and David M. Blei. 2006. Hierarchical Dirichlet
processes. Journal of the American Statistical Asso-
ciation, 101(476).
Naftali Tishby, Fernando C. Pereira, and William
Bialek. 1999. The information bottleneck method.
In Proceedings of the 37th Annual Allerton Confer-
ence on Communication, Control and Computing,
pages 368–377.
Ivan Titov and Alexandre Klementiev. 2012. A
Bayesian approach to unsupervised semantic role in-
duction. In Proceedings of the 13th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 12–22.
Andreas Vlachos, Anna Korhonen, and Zoubin
Ghahramani. 2009. Unsupervised and constrained
Dirichlet process mixture models for verb cluster-
ing. In Proceedings of the Workshop on Geometri-
cal Models of Natural Language Semantics, pages
74–82.
David Yarowsky. 1993. One sense per collocation. In
Proceedings of the Workshop on Human Language
Technology, pages 266–271.
</reference>
<page confidence="0.986937">
1040
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.505304">
<title confidence="0.985548">A Step-wise Usage-based Method for Polysemy-aware Verb Classes</title>
<author confidence="0.523469">W</author>
<affiliation confidence="0.911581">University, Kyoto,</affiliation>
<address confidence="0.996863">of Colorado at Boulder, Boulder, CO, USA</address>
<abstract confidence="0.99941955">We present an unsupervised method for inducing verb classes from verb uses in gigaword corpora. Our method consists of two clustering steps: verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames. By taking this step-wise approach, we can not only generate verb classes based on a massive amount of verb uses in a scalable manner, but also deal with verb polysemy, which is bypassed by most of the previous studies on verb clustering. In our experiments, we acquire semantic frames and verb classes from two giga-word corpora, the larger comprising 20 billion words. The effectiveness of our approach is verified through quantitative evaluations based on polysemy-aware gold-standard data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Aldous</author>
</authors>
<title>Exchangeability and related topics.</title>
<date>1985</date>
<booktitle>Ecole d’ ´Et´e de Probabilit´es de Saint-Flour XIII ―1983,</booktitle>
<pages>1--198</pages>
<contexts>
<context position="5709" citStr="Aldous, 1985" startWordPosition="895" endWordPosition="896">son (2010) proposed a hierarchical Dirichlet process (HDP; Teh et al. (2006)) model to jointly learn argument structures (subcategorization frames) and verb classes by using syntactic features. Parisien and Stevenson (2011) extended their model by adding semantic features. They tried to account for verb learning by children and did not evaluate the resultant verb classes. Modi et al. (2012) extended the model of Titov and Klementiev (2012), which is an unsupervised model for inducing semantic roles, to jointly induce semantic roles and frames across verbs using the Chinese Restaurant Process (Aldous, 1985). All of the above methods considered verbs to be monosemous and did not deal with verb polysemy. Our approach also uses Bayesian methods, but is designed to capture verb polysemy. We summarize a few studies that consider polysemy of verbs in the rest of this section. Miyao and Tsujii (2009) proposed a supervised method that can handle verb polysemy. Their method represents a verb’s syntactic and semantic features, and learns a log-linear model from the SemLink corpus (Loper et al., 2007). Boleda et al. (2007) also proposed a supervised method for Catalan adjectives considering the polysemy of</context>
<context position="12069" citStr="Aldous, 1985" startWordPosition="1925" endWordPosition="1927">ing to a raw corpus and extract predicate-argument structures for each verb from the automatic parses, 2. merge the predicate-argument structures that have presumably the same meaning based on the assumption of one sense per collocation (Yarowsky, 1993) to get a set of initial frames, and &apos;In our replication experiment, it took a week to perform 70 iterations using Materna’s code and an Intel Xeon E5-2680 (2.7GHz) CPU. To reach 1,000 iterations, which are reported to be optimum, it would take three months. 1032 3. apply clustering to the initial frames based on the Chinese Restaurant Process (Aldous, 1985) to produce verb-specific semantic frames. These three steps are briefly described below. 3.2.1 Extracting Predicate-argument Structures from a Raw Corpus We apply dependency parsing to a large raw corpus. We use the Stanford parser with Stanford dependencies (de Marneffe et al., 2006).2 Collapsed dependencies are adopted to directly extract prepositional phrases. Then, we extract predicate-argument structures from the dependency parses. Dependents that have the following dependency relations to a verb are extracted as arguments: nsubj, xsubj, dobj, iobj, ccomp, xcomp, prep ∗ In this process, </context>
<context position="14102" citStr="Aldous, 1985" startWordPosition="2233" endWordPosition="2234">ng the degree of effect on the verb sense:3 dobj, ccomp, nsubj, prep ∗, iobj. Then, the predicate-argument structures that have the same verb and argument pair (slot and word, e.g., “dobj:effect”) are merged into an initial frame. After this process, we discard minor initial frames that occur fewer than 10 times. 2http://nlp.stanford.edu/software/lex-parser.shtml 3If a predicate-argument structure has multiple prepositional phrases, one of them is randomly selected. 3.2.3 Clustering Method We cluster initial frames for each verb to produce semantic frames using the Chinese Restaurant Process (Aldous, 1985), regarding each initial frame as an instance. We calculate the posterior probability of a cluster cj given an initial frame fi as follows: �n(c�) N+α · P(fi|cj) cj ≠ new P(cj|fi) ∝ (1) N+α · P(fi|cj) cj = new, α where N is the number of initial frames for the target verb and n(cj) is the current number of initial frames assigned to the cluster cj. α is a hyperparameter that determines how likely it is for a new cluster to be created. In this equation, the first term is the Dirichlet process prior and the second term is the likelihood of fi. P(fi|cj) is defined based on the DirichletMultinomi</context>
</contexts>
<marker>Aldous, 1985</marker>
<rawString>David Aldous. 1985. Exchangeability and related topics. ´Ecole d’ ´Et´e de Probabilit´es de Saint-Flour XIII ―1983, pages 1–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>the Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="8518" citStr="Blei et al. (2003)" startWordPosition="1339" endWordPosition="1342">ces based on WordNet. This model is based on the Expectation-Maximization algorithm and the Minimum Description Length principle. Since they focused on the incorporation of selectional preferences, they did not evaluate verb classes but evaluated only selectional preferences using a language model-based measure. Materna proposed LDA-frames, which are defined across verbs and can be considered to be a kind of verb class (Materna, 2012; Materna, 2013). LDA-frames are probabilistic semantic frames automatically induced from a raw corpus. He used a model based on latent Dirichlet allocation (LDA; Blei et al. (2003)) and the Dirichlet process to cluster verb instances of a triple (subject, verb, object) to produce semantic frames and roles. Both of these are represented as a probabilistic distribution of words across verbs. He applied this method to the BNC and acquired 1,200 frames and 400 roles (Materna, 2012). He did not evaluate the resulting frames as verb classes. In sum, there have been no studies that quantitatively evaluate polysemous verb classes automatically induced by unsupervised methods. 3 Our Approach 3.1 Overview Our objective is to automatically learn semantic frames and verb classes fr</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. the Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gemma Boleda</author>
<author>Sabine Schulte im Walde</author>
<author>Toni Badia</author>
</authors>
<title>Modelling polysemy in adjective classes by multi-label classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>171--180</pages>
<contexts>
<context position="6224" citStr="Boleda et al. (2007)" startWordPosition="980" endWordPosition="983">jointly induce semantic roles and frames across verbs using the Chinese Restaurant Process (Aldous, 1985). All of the above methods considered verbs to be monosemous and did not deal with verb polysemy. Our approach also uses Bayesian methods, but is designed to capture verb polysemy. We summarize a few studies that consider polysemy of verbs in the rest of this section. Miyao and Tsujii (2009) proposed a supervised method that can handle verb polysemy. Their method represents a verb’s syntactic and semantic features, and learns a log-linear model from the SemLink corpus (Loper et al., 2007). Boleda et al. (2007) also proposed a supervised method for Catalan adjectives considering the polysemy of adjectives. The most closely related work to our polysemyaware task of unsupervised verb class induction is the work of Korhonen et al. (2003), who used distributions of subcategorization frames to cluster verbs. They adopted the Nearest Neighbor (NN) and Information Bottleneck (IB) methods for clustering. In particular, they tried to consider verb polysemy by using the IB method, which is a soft clustering method (Tishby et al., 1999). However, the verb itself is still represented as a single data point. Aft</context>
</contexts>
<marker>Boleda, Walde, Badia, 2007</marker>
<rawString>Gemma Boleda, Sabine Schulte im Walde, and Toni Badia. 2007. Modelling polysemy in adjective classes by multi-label classification. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 171–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa Trang Dang</author>
</authors>
<title>Investigations into the role of lexical semantics in word sense disambiguation.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1734" citStr="Dang, 2004" startWordPosition="265" endWordPosition="266">the meaning of a sentence. Capturing the sense of a verb is essential for natural language processing (NLP), and thus lexical resources for verbs play an important role in NLP. Verb classes are one such lexical resource. Manually-crafted verb classes have been developed, such as Levin’s classes (Levin, 1993) and their extension, VerbNet (Kipper-Schuler, 2005), in which verbs are organized into classes on the basis of their syntactic and semantic behavior. Such verb classes have been used in many NLP applications that need to consider semantics in particular, such as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and S</context>
</contexts>
<marker>Dang, 2004</marker>
<rawString>Hoa Trang Dang. 2004. Investigations into the role of lexical semantics in word sense disambiguation. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation,</booktitle>
<pages>449--454</pages>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the 5th International Conference on Language Resources and Evaluation, pages 449– 454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie J Dorr</author>
</authors>
<title>Large-scale dictionary construction for foreign language tutoring and interlingual machine translation.</title>
<date>1997</date>
<journal>Machine Translation,</journal>
<volume>12</volume>
<issue>4</issue>
<contexts>
<context position="21778" citStr="Dorr, 1997" startWordPosition="3553" endWordPosition="3554">in Ki. In case of evaluating a hard clustering, this is equal to |Ki ∩ Gj |because all the values of civ are equal to 1. As usual, the following normalized inverse purity (niPU) is used to measure the recall of a clustering: SGS(Ki ∩ Gj). (6) Finally, we use the harmonic mean (Fi) of nmPU and niPU as a single measure of clustering quality. 4.3 Type-level Multi-class Evaluations We first evaluate our induced verb classes on the test set created by Korhonen et al. (2003) (Table 1 of their paper) which was created by considering verb polysemy on the basis of Levin’s classes and the LCS database (Dorr, 1997). It consists of 62 classes and 110 verbs, out of which 35 verbs are monosemous and 75 verbs are polysemous. The average number of verb classes per verb is 2.24. An excerpt from this data is shown in Table 1. As our baselines, we adopt two previously proposed methods. We first implemented a soft clustering method for verb class induction proposed by Korhonen et al. (2003). They used the information bottleneck (IB) method for assigning probabilities of classes to each verb. Note that Korhonen et al. (2003) actually hardened the clusterings and left method K nmPU niPU Fi IB (k=35, t=0.10) 35.0 5</context>
</contexts>
<marker>Dorr, 1997</marker>
<rawString>Bonnie J. Dorr. 1997. Large-scale dictionary construction for foreign language tutoring and interlingual machine translation. Machine Translation, 12(4):271–322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ingrid Falk</author>
<author>Claire Gardent</author>
<author>Jean-Charles Lamirel</author>
</authors>
<title>Classifying French verbs using French and English lexical resources.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>854--863</pages>
<contexts>
<context position="2367" citStr="Falk et al., 2012" startWordPosition="368" endWordPosition="371">ng (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008). In this paper, we propose an unsupervised method for inducing verb classes that is aware of verb polysemy. Our method consi</context>
</contexts>
<marker>Falk, Gardent, Lamirel, 2012</marker>
<rawString>Ingrid Falk, Claire Gardent, and Jean-Charles Lamirel. 2012. Classifying French verbs using French and English lexical resources. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 854–863.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yufan Guo</author>
<author>Anna Korhonen</author>
<author>Thierry Poibeau</author>
</authors>
<title>A weakly-supervised approach to argumentative zoning of scientific documents.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>273--283</pages>
<contexts>
<context position="36324" citStr="Guo et al., 2011" startWordPosition="5888" endWordPosition="5891">t with Levin’s principle of organizing verb classes according to the syntactic behavior of verbs. As applications of the resulting semantic frames and verb classes, we plan to integrate them into syntactic parsing, semantic role labeling and verb sense disambiguation. For instance, Kawahara and Kurohashi (2006) improved accuracy of dependency parsing based on Japanese semantic frames automatically induced from a raw corpus. It is also valuable and promising to apply the induced verb classes to NLP applications as used in metaphor identification (Shutova et al., 2010) and argumentative zoning (Guo et al., 2011). Acknowledgments This work was supported by Kyoto University John Mung Program and JST CREST. We also gratefully acknowledge the support of the National Science Foundation Grant NSF-IIS-1116782, A Bayesian Approach to Dynamic Lexical Resources for Flexible Language Processing. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. class semantic frames Class 1 Class 2 Class 3 Class 4 Class 5 Class 6 Class 7 Class 8 Class 9 Class 10 Class 11 Class 12 Class 13 Cla</context>
</contexts>
<marker>Guo, Korhonen, Poibeau, 2011</marker>
<rawString>Yufan Guo, Anna Korhonen, and Thierry Poibeau. 2011. A weakly-supervised approach to argumentative zoning of scientific documents. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 273–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Joanis</author>
<author>Suzanne Stevenson</author>
<author>David James</author>
</authors>
<title>A general feature space for automatic verb classification.</title>
<date>2008</date>
<journal>Natural Language Engineering,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="2205" citStr="Joanis et al., 2008" startWordPosition="339" endWordPosition="343">h verb classes have been used in many NLP applications that need to consider semantics in particular, such as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew,</context>
<context position="4421" citStr="Joanis et al. (2008)" startWordPosition="698" endWordPosition="701">�2014 Association for Computational Linguistics Verb classes: Semantic frames: Verb uses: Figure 1: Overview of our two-step approach. Verb-specific semantic frames are first induced from verb uses (lower part) and then verb classes are induced from the semantic frames (upper part). The labels of verb classes are manually assigned here for better understanding. 2 Related Work As stated in Section 1, most of the previous studies on verb clustering assume that verbs are monosemous. A typical method in these studies is to represent each verb as a single data point and apply classification (e.g., Joanis et al. (2008)) or clustering (e.g., Sun and Korhonen (2009)) to these data points. As a representation for a data point, distributions of subcategorization frames are often used, and other semantic features (e.g., selectional preferences) are sometimes added to improve the performance. Among these studies on monosemous verb clustering (i.e., predominant class induction), there have been several Bayesian methods. Vlachos et al. (2009) proposed a Dirichlet process mixture model (DPMM; Neal (2000)) to cluster verbs based on subcategorization frame distributions. They evaluated their result with a gold-standar</context>
</contexts>
<marker>Joanis, Stevenson, James, 2008</marker>
<rawString>Eric Joanis, Suzanne Stevenson, and David James. 2008. A general feature space for automatic verb classification. Natural Language Engineering, 14(3):337–367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>A fully-lexicalized probabilistic model for Japanese syntactic and case structure analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL,</booktitle>
<pages>176--183</pages>
<contexts>
<context position="36019" citStr="Kawahara and Kurohashi (2006)" startWordPosition="5839" endWordPosition="5842">tures for clustering across verbs is the most effective and outperforms the baselines by approximately 10 points. This indicates that slot distributions are more effective than lexical information in slotword pairs for the induction of verb classes, when Levin-style classes are used for evaluation. This is consistent with Levin’s principle of organizing verb classes according to the syntactic behavior of verbs. As applications of the resulting semantic frames and verb classes, we plan to integrate them into syntactic parsing, semantic role labeling and verb sense disambiguation. For instance, Kawahara and Kurohashi (2006) improved accuracy of dependency parsing based on Japanese semantic frames automatically induced from a raw corpus. It is also valuable and promising to apply the induced verb classes to NLP applications as used in metaphor identification (Shutova et al., 2010) and argumentative zoning (Guo et al., 2011). Acknowledgments This work was supported by Kyoto University John Mung Program and JST CREST. We also gratefully acknowledge the support of the National Science Foundation Grant NSF-IIS-1116782, A Bayesian Approach to Dynamic Lexical Resources for Flexible Language Processing. Any opinions, fi</context>
</contexts>
<marker>Kawahara, Kurohashi, 2006</marker>
<rawString>Daisuke Kawahara and Sadao Kurohashi. 2006. A fully-lexicalized probabilistic model for Japanese syntactic and case structure analysis. In Proceedings of the Human Language Technology Conference of the NAACL, pages 176–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Daniel W Peterson</author>
<author>Octavian Popescu</author>
<author>Martha Palmer</author>
</authors>
<title>Inducing example-based semantic frames from a massive amount of verb uses.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="11262" citStr="Kawahara et al. (2014)" startWordPosition="1791" endWordPosition="1794">wo clustering steps. Our procedure to automatically induce verb classes from verb uses is summarized as follows: 1. induce verb-specific semantic frames by clustering predicate-argument structures for each verb extracted from automatic parses as shown in the lower part of Figure 1, and 2. induce verb classes by clustering the induced semantic frames across verbs as shown in the upper part of Figure 1. Each of these two steps is described in the following sections in detail. 3.2 Inducing Verb-specific Semantic Frames We induce verb-specific semantic frames from verb uses based on the method of Kawahara et al. (2014). Our semantic frames consist of case slots, each of which consists of word instances that can be filled. The procedure for inducing these semantic frames is as follows: 1. apply dependency parsing to a raw corpus and extract predicate-argument structures for each verb from the automatic parses, 2. merge the predicate-argument structures that have presumably the same meaning based on the assumption of one sense per collocation (Yarowsky, 1993) to get a set of initial frames, and &apos;In our replication experiment, it took a week to perform 70 iterations using Materna’s code and an Intel Xeon E5-26</context>
<context position="14947" citStr="Kawahara et al. (2014)" startWordPosition="2390" endWordPosition="2393">s the number of initial frames for the target verb and n(cj) is the current number of initial frames assigned to the cluster cj. α is a hyperparameter that determines how likely it is for a new cluster to be created. In this equation, the first term is the Dirichlet process prior and the second term is the likelihood of fi. P(fi|cj) is defined based on the DirichletMultinomial distribution as follows: P(fi|cj) = � P(w|cj)count(fz,w), (2) wEV where V is the vocabulary in all case slots cooccurring with the verb and count(fi, w) is the number of w in the initial frame fi. The original method in Kawahara et al. (2014) defined w as pairs of slots and words, e.g., “nsubj:child” and “dobj:bird,” but does not consider slot-only features, e.g., “nsubj” and “dobj,” which ignore lexical information. Here we experiment with both representations and compare the results. P(w|cj) is defined as follows: count(cj, w) + β P(w|cj) = &amp;EV count(cj, t) + |V |· β, (3) where count(cj, w) is the current number of w in the cluster cj, and β is a hyper-parameter of Dirichlet distribution. For a new cluster, this probability is uniform (1/|V |). We regard each output cluster as a semantic frame, by merging the initial frames in a</context>
</contexts>
<marker>Kawahara, Peterson, Popescu, Palmer, 2014</marker>
<rawString>Daisuke Kawahara, Daniel W. Peterson, Octavian Popescu, and Martha Palmer. 2014. Inducing example-based semantic frames from a massive amount of verb uses. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper-Schuler</author>
</authors>
<title>VerbNet: A BroadCoverage, Comprehensive Verb Lexicon.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1484" citStr="Kipper-Schuler, 2005" startWordPosition="223" endWordPosition="224">verb classes from two giga-word corpora, the larger comprising 20 billion words. The effectiveness of our approach is verified through quantitative evaluations based on polysemy-aware gold-standard data. 1 Introduction A verb plays a primary role in conveying the meaning of a sentence. Capturing the sense of a verb is essential for natural language processing (NLP), and thus lexical resources for verbs play an important role in NLP. Verb classes are one such lexical resource. Manually-crafted verb classes have been developed, such as Levin’s classes (Levin, 1993) and their extension, VerbNet (Kipper-Schuler, 2005), in which verbs are organized into classes on the basis of their syntactic and semantic behavior. Such verb classes have been used in many NLP applications that need to consider semantics in particular, such as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approa</context>
</contexts>
<marker>Kipper-Schuler, 2005</marker>
<rawString>Karin Kipper-Schuler. 2005. VerbNet: A BroadCoverage, Comprehensive Verb Lexicon. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Korhonen</author>
<author>Yuval Krymolowski</author>
<author>Zvika Marx</author>
</authors>
<title>Clustering polysemic subcategorization frame distributions semantically.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>64--71</pages>
<contexts>
<context position="2768" citStr="Korhonen et al., 2003" startWordPosition="427" endWordPosition="430"> Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008). In this paper, we propose an unsupervised method for inducing verb classes that is aware of verb polysemy. Our method consists of two clustering steps: verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames. By taking this step-wise approach, we can not only induce verb classes with frequency information from a massive amount of verb uses in a scalable manner, but also deal with verb polysemy. Our novel contributions are summarize</context>
<context position="6452" citStr="Korhonen et al. (2003)" startWordPosition="1016" endWordPosition="1019">Bayesian methods, but is designed to capture verb polysemy. We summarize a few studies that consider polysemy of verbs in the rest of this section. Miyao and Tsujii (2009) proposed a supervised method that can handle verb polysemy. Their method represents a verb’s syntactic and semantic features, and learns a log-linear model from the SemLink corpus (Loper et al., 2007). Boleda et al. (2007) also proposed a supervised method for Catalan adjectives considering the polysemy of adjectives. The most closely related work to our polysemyaware task of unsupervised verb class induction is the work of Korhonen et al. (2003), who used distributions of subcategorization frames to cluster verbs. They adopted the Nearest Neighbor (NN) and Information Bottleneck (IB) methods for clustering. In particular, they tried to consider verb polysemy by using the IB method, which is a soft clustering method (Tishby et al., 1999). However, the verb itself is still represented as a single data point. After performing soft clustering, they noted that most verbs fell into a single class, and they decided to assign a single class to each verb by hardening the clustering. They considered multiple classes only in the gold-standard d</context>
<context position="10363" citStr="Korhonen et al., 2003" startWordPosition="1643" endWordPosition="1646">onsidered to be verb classes (Materna, 2012; Materna, 2013). However, it would take three months for this experiment using this 100 million word corpus.&apos; Although it is best to use the largest possible corpus for this kind of knowledge acquisition tasks (Sasano et al., 2009), it is infeasible to scale to giga-word corpora using such joint models. In this paper, we propose a two-step approach for inducing semantic frames and verb classes. First, we make multiple data points for each verb to deal with verb polysemy (cf. polysemy-aware previous studies still represented a verb as one data point (Korhonen et al., 2003; Miyao and Tsujii, 2009)). To do that, we induce verb-specific semantic frames by clustering verb uses. Then, we induce verb classes by clustering these verbspecific semantic frames across verbs. An interesting point here is that we can use exactly the same method for these two clustering steps. Our procedure to automatically induce verb classes from verb uses is summarized as follows: 1. induce verb-specific semantic frames by clustering predicate-argument structures for each verb extracted from automatic parses as shown in the lower part of Figure 1, and 2. induce verb classes by clustering</context>
<context position="20134" citStr="Korhonen et al. (2003)" startWordPosition="3246" endWordPosition="3249">te probability given a verb. Where there is no frequency information available for class distribution, such as the gold-standard data described in Section 4.3, we use a uniform distribution across the verb’s classes. The core idea of purity is that each cluster Ki is associated with its most prevalent gold class. In addition, to penalize clusters that consist of only one verb, such singleton clusters in K are considered as errors, as is usual with modified purity. The normalized modified purity (nmPU) can then be written as follows: 1 � nmPU = � i s.t. �Ki&gt;1 JKi(Ki n Gj) = � civ, (5) vEKinGj 4Korhonen et al. (2003) evaluated hard clusterings based on a gold standard with multiple classes per verb. They reported only precision measures including modified purity, and avoided extending the evaluation metrics for soft clusterings. JKi(Ki n Gj), (4) max j 1034 verb classes verb classes place 9 drop 9, 45, 004, 47, dye 24, 21, 41 51, A54, A30 focus 31, 45 bake 26, 45 stare 30 persuade 002 lay 9 sparkle 43 build 26, 45 pour 9, 43, 26, 57, force 002, 11 13, 31 glow 43 invent 26, 27 Table 1: An excerpt of the gold-standard verb classes for several verbs from Korhonen et al. (2003). The classes starting with ‘0’ </context>
<context position="21640" citStr="Korhonen et al. (2003)" startWordPosition="3527" endWordPosition="3530">ts in Ki, and civ denotes the v-th component of Ki. SKi(Ki ∩ Gj) means the total mass of the set of verbs in Ki ∩ Gj, given by summing up the values in Ki. In case of evaluating a hard clustering, this is equal to |Ki ∩ Gj |because all the values of civ are equal to 1. As usual, the following normalized inverse purity (niPU) is used to measure the recall of a clustering: SGS(Ki ∩ Gj). (6) Finally, we use the harmonic mean (Fi) of nmPU and niPU as a single measure of clustering quality. 4.3 Type-level Multi-class Evaluations We first evaluate our induced verb classes on the test set created by Korhonen et al. (2003) (Table 1 of their paper) which was created by considering verb polysemy on the basis of Levin’s classes and the LCS database (Dorr, 1997). It consists of 62 classes and 110 verbs, out of which 35 verbs are monosemous and 75 verbs are polysemous. The average number of verb classes per verb is 2.24. An excerpt from this data is shown in Table 1. As our baselines, we adopt two previously proposed methods. We first implemented a soft clustering method for verb class induction proposed by Korhonen et al. (2003). They used the information bottleneck (IB) method for assigning probabilities of classe</context>
<context position="23709" citStr="Korhonen et al. (2003)" startWordPosition="3863" endWordPosition="3866">.38 71.39 web/SW-SW 55.0 36.70 96.25 53.13 Table 2: Type-level multi-class evaluations. K represents the (average) number of induced classes. “S” denotes the use of slot-only features and “SW” denotes the use of slot-word pair features. For example, “SW-S” means that slot-word pair features are used for semantic frame induction and slotonly features are used for verb class induction. the evaluations of soft clusterings for their future work. For input data, we employ VALEX (Korhonen et al., 2006), which is a publicly-available large-scale subcategorization lexicon.5 By following the method of Korhonen et al. (2003), prepositional phrases (pp) are parameterized for two frequent subcategorization frames (NP and NP PP), and the unfiltered raw frequencies of subcategorization frames are used as features to represent a verb. It is necessary to specify the number of clusters, k, for the IB method beforehand, and we adopt 35 and 42 clusters according to their reported high accuracies. To output multiple classes for each verb, we set a threshold, t, for class attribute probabilities. That is, classes that have a higher class attribute probability than the threshold are output for each verb. We report the result</context>
<context position="27854" citStr="Korhonen et al. (2003)" startWordPosition="4533" endWordPosition="4536">redominant (monosemous) class for each verb. To output a single class for each verb by using our proposed method, we skip the induction of verb-specific semantic frames and instead create a single frame for each verb by merging all predicate-argument structures of the verb. Then, we apply clustering to these frames across verbs. For clustering features, we again compare two representations: slot-only features (S) and slot-word pair features (SW). We evaluate the single-class output for each verb based on the predominant gold-standard classes, which are defined for each verb in the test set of Korhonen et al. (2003). This data contains 110 verbs and 33 classes. We evaluate these single-class outputs in the same manner as Korhonen et al. (2003), using the gold standard with multiple classes, which we also use for our multi-class evaluations. As we did with the multi-class evaluations, we adopt modified purity (mPU), inverse purity (iPU) and their harmonic mean (F1) as the metrics for the evaluation with predominant classes. It is not necessary to normalize these metrics when we treat verbs as monosemous, and evaluate against the predominant sense. When we evaluate against the multiple classes in the gold </context>
<context position="30939" citStr="Korhonen et al. (2003)" startWordPosition="5044" endWordPosition="5047">nd 48 of them are polysemous in the sense of being in more than one VerbNet class. Each instance of these 119 verbs in this corpus belongs to one of 102 VerbNet classes. We first add these instances to the instances from a raw corpus and apply the twostep clustering to these merged instances. Then, we compare the induced verb classes of the SemLink instances with their gold-standard VerbNet classes. We report the values of modified purity (mPU), inverse purity (iPU) and their harmonic mean (F1). It is not necessary to normalize these metrics because the clustering of these instances is hard. 8Korhonen et al. (2003) reported that the highest modified purity was 49% against predominant classes and 60% against multiple classes. method K mPU iPU F1 Gigaword/S-NIL – 93.43 20.06 33.03 Gigaword/SW-NIL – 94.45 41.07 57.25 Gigaword/S-S 512.2 75.06 45.26 56.47 Gigaword/SW-S 260.6 73.98 56.45 64.04 web/S-NIL – 93.70 32.96 48.76 web/SW-NIL – 94.51 44.95 60.92 web/S-S 500.0 72.25 52.48 60.79 web/SW-S 255.2 72.65 61.00 66.31 Table 4: Token-level evaluations against VerbNet classes. K represents the average number of induced classes. For clustering features, we compare two feature combinations: “S-S” and “SW-S,” which</context>
</contexts>
<marker>Korhonen, Krymolowski, Marx, 2003</marker>
<rawString>Anna Korhonen, Yuval Krymolowski, and Zvika Marx. 2003. Clustering polysemic subcategorization frame distributions semantically. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 64–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Korhonen</author>
<author>Yuval Krymolowski</author>
<author>Ted Briscoe</author>
</authors>
<title>A large subcategorization lexicon for natural language processing applications.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation,</booktitle>
<pages>345--352</pages>
<contexts>
<context position="23588" citStr="Korhonen et al., 2006" startWordPosition="3845" endWordPosition="3849">word/SW-SW 30.0 36.27 94.66 52.40 web/S-S 363.6 61.32 78.64 68.90 web/S-SW 52.2 35.80 99.30 52.62 web/SW-S 212.2 66.26 77.38 71.39 web/SW-SW 55.0 36.70 96.25 53.13 Table 2: Type-level multi-class evaluations. K represents the (average) number of induced classes. “S” denotes the use of slot-only features and “SW” denotes the use of slot-word pair features. For example, “SW-S” means that slot-word pair features are used for semantic frame induction and slotonly features are used for verb class induction. the evaluations of soft clusterings for their future work. For input data, we employ VALEX (Korhonen et al., 2006), which is a publicly-available large-scale subcategorization lexicon.5 By following the method of Korhonen et al. (2003), prepositional phrases (pp) are parameterized for two frequent subcategorization frames (NP and NP PP), and the unfiltered raw frequencies of subcategorization frames are used as features to represent a verb. It is necessary to specify the number of clusters, k, for the IB method beforehand, and we adopt 35 and 42 clusters according to their reported high accuracies. To output multiple classes for each verb, we set a threshold, t, for class attribute probabilities. That is,</context>
</contexts>
<marker>Korhonen, Krymolowski, Briscoe, 2006</marker>
<rawString>Anna Korhonen, Yuval Krymolowski, and Ted Briscoe. 2006. A large subcategorization lexicon for natural language processing applications. In Proceedings of the 5th International Conference on Language Resources and Evaluation, pages 345–352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Chris Brew</author>
</authors>
<title>Verb class disambiguation using informative priors.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="2791" citStr="Lapata and Brew, 2004" startWordPosition="431" endWordPosition="434">im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008). In this paper, we propose an unsupervised method for inducing verb classes that is aware of verb polysemy. Our method consists of two clustering steps: verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames. By taking this step-wise approach, we can not only induce verb classes with frequency information from a massive amount of verb uses in a scalable manner, but also deal with verb polysemy. Our novel contributions are summarized as follows: • induce </context>
<context position="7241" citStr="Lapata and Brew (2004)" startWordPosition="1144" endWordPosition="1147">n particular, they tried to consider verb polysemy by using the IB method, which is a soft clustering method (Tishby et al., 1999). However, the verb itself is still represented as a single data point. After performing soft clustering, they noted that most verbs fell into a single class, and they decided to assign a single class to each verb by hardening the clustering. They considered multiple classes only in the gold-standard data used for their evaluations. We also evaluate our induced verb classes on this gold-standard data, which was created on the basis of Levin’s classes (Levin, 1993). Lapata and Brew (2004) and Li and Brew (2007) proposed probabilistic models for calculating prior probabilities of verb classes for a verb. These models are approximated to condition not 1031 on verbs but on subcategorization frames. As mentioned in Li and Brew (2007), it is desirable to extend the model to depend on verbs to further improve accuracy. They conducted several evaluations including predominant class induction and token-level verb sense disambiguation, but did not evaluate multiple classes output by their models. Schulte im Walde et al. (2008) also applied probabilistic soft clustering to verbs by inco</context>
</contexts>
<marker>Lapata, Brew, 2004</marker>
<rawString>Mirella Lapata and Chris Brew. 2004. Verb class disambiguation using informative priors. Computational Linguistics, 30(1):45–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English verb classes and alternations: A preliminary investigation.</title>
<date>1993</date>
<publisher>The University of Chicago Press.</publisher>
<contexts>
<context position="1432" citStr="Levin, 1993" startWordPosition="217" endWordPosition="218">xperiments, we acquire semantic frames and verb classes from two giga-word corpora, the larger comprising 20 billion words. The effectiveness of our approach is verified through quantitative evaluations based on polysemy-aware gold-standard data. 1 Introduction A verb plays a primary role in conveying the meaning of a sentence. Capturing the sense of a verb is essential for natural language processing (NLP), and thus lexical resources for verbs play an important role in NLP. Verb classes are one such lexical resource. Manually-crafted verb classes have been developed, such as Levin’s classes (Levin, 1993) and their extension, VerbNet (Kipper-Schuler, 2005), in which verbs are organized into classes on the basis of their syntactic and semantic behavior. Such verb classes have been used in many NLP applications that need to consider semantics in particular, such as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar ve</context>
<context position="7217" citStr="Levin, 1993" startWordPosition="1142" endWordPosition="1143"> clustering. In particular, they tried to consider verb polysemy by using the IB method, which is a soft clustering method (Tishby et al., 1999). However, the verb itself is still represented as a single data point. After performing soft clustering, they noted that most verbs fell into a single class, and they decided to assign a single class to each verb by hardening the clustering. They considered multiple classes only in the gold-standard data used for their evaluations. We also evaluate our induced verb classes on this gold-standard data, which was created on the basis of Levin’s classes (Levin, 1993). Lapata and Brew (2004) and Li and Brew (2007) proposed probabilistic models for calculating prior probabilities of verb classes for a verb. These models are approximated to condition not 1031 on verbs but on subcategorization frames. As mentioned in Li and Brew (2007), it is desirable to extend the model to depend on verbs to further improve accuracy. They conducted several evaluations including predominant class induction and token-level verb sense disambiguation, but did not evaluate multiple classes output by their models. Schulte im Walde et al. (2008) also applied probabilistic soft clu</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English verb classes and alternations: A preliminary investigation. The University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianguo Li</author>
<author>Chris Brew</author>
</authors>
<title>Disambiguating Levin verbs using untagged data.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference Recent Advances in Natural Language Processing.</booktitle>
<contexts>
<context position="2810" citStr="Li and Brew, 2007" startWordPosition="435" endWordPosition="438">et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008). In this paper, we propose an unsupervised method for inducing verb classes that is aware of verb polysemy. Our method consists of two clustering steps: verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames. By taking this step-wise approach, we can not only induce verb classes with frequency information from a massive amount of verb uses in a scalable manner, but also deal with verb polysemy. Our novel contributions are summarized as follows: • induce both semantic frame</context>
<context position="7264" citStr="Li and Brew (2007)" startWordPosition="1149" endWordPosition="1152"> consider verb polysemy by using the IB method, which is a soft clustering method (Tishby et al., 1999). However, the verb itself is still represented as a single data point. After performing soft clustering, they noted that most verbs fell into a single class, and they decided to assign a single class to each verb by hardening the clustering. They considered multiple classes only in the gold-standard data used for their evaluations. We also evaluate our induced verb classes on this gold-standard data, which was created on the basis of Levin’s classes (Levin, 1993). Lapata and Brew (2004) and Li and Brew (2007) proposed probabilistic models for calculating prior probabilities of verb classes for a verb. These models are approximated to condition not 1031 on verbs but on subcategorization frames. As mentioned in Li and Brew (2007), it is desirable to extend the model to depend on verbs to further improve accuracy. They conducted several evaluations including predominant class induction and token-level verb sense disambiguation, but did not evaluate multiple classes output by their models. Schulte im Walde et al. (2008) also applied probabilistic soft clustering to verbs by incorporating subcategoriza</context>
</contexts>
<marker>Li, Brew, 2007</marker>
<rawString>Jianguo Li and Chris Brew. 2007. Disambiguating Levin verbs using untagged data. In Proceedings of the International Conference Recent Advances in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianguo Li</author>
<author>Chris Brew</author>
</authors>
<title>Which are the best features for automatic verb classification.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>434--442</pages>
<contexts>
<context position="2224" citStr="Li and Brew, 2008" startWordPosition="344" endWordPosition="347">een used in many NLP applications that need to consider semantics in particular, such as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im W</context>
</contexts>
<marker>Li, Brew, 2008</marker>
<rawString>Jianguo Li and Chris Brew. 2008. Which are the best features for automatic verb classification. In Proceedings of ACL-08: HLT, pages 434–442.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Lippincott</author>
<author>Anna Korhonen</author>
<author>Diarmuid O´ S´eaghdha</author>
</authors>
<title>Learning syntactic verb frames using graphical models.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>420--429</pages>
<marker>Lippincott, Korhonen, S´eaghdha, 2012</marker>
<rawString>Thomas Lippincott, Anna Korhonen, and Diarmuid O´ S´eaghdha. 2012. Learning syntactic verb frames using graphical models. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 420–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Loper</author>
<author>Szu-Ting Yi</author>
<author>Martha Palmer</author>
</authors>
<title>Combining lexical resources: mapping between PropBank and VerbNet.</title>
<date>2007</date>
<booktitle>In Proceedings of the 7th International Workshop on Computational Linguistics.</booktitle>
<contexts>
<context position="6202" citStr="Loper et al., 2007" startWordPosition="976" endWordPosition="979">g semantic roles, to jointly induce semantic roles and frames across verbs using the Chinese Restaurant Process (Aldous, 1985). All of the above methods considered verbs to be monosemous and did not deal with verb polysemy. Our approach also uses Bayesian methods, but is designed to capture verb polysemy. We summarize a few studies that consider polysemy of verbs in the rest of this section. Miyao and Tsujii (2009) proposed a supervised method that can handle verb polysemy. Their method represents a verb’s syntactic and semantic features, and learns a log-linear model from the SemLink corpus (Loper et al., 2007). Boleda et al. (2007) also proposed a supervised method for Catalan adjectives considering the polysemy of adjectives. The most closely related work to our polysemyaware task of unsupervised verb class induction is the work of Korhonen et al. (2003), who used distributions of subcategorization frames to cluster verbs. They adopted the Nearest Neighbor (NN) and Information Bottleneck (IB) methods for clustering. In particular, they tried to consider verb polysemy by using the IB method, which is a soft clustering method (Tishby et al., 1999). However, the verb itself is still represented as a </context>
</contexts>
<marker>Loper, Yi, Palmer, 2007</marker>
<rawString>Edward Loper, Szu-Ting Yi, and Martha Palmer. 2007. Combining lexical resources: mapping between PropBank and VerbNet. In Proceedings of the 7th International Workshop on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiˇr´ı Materna</author>
</authors>
<title>LDA-frames: An unsupervised approach to generating semantic frames.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th International Conference CICLing 2012, Part I,</booktitle>
<pages>376--387</pages>
<contexts>
<context position="8337" citStr="Materna, 2012" startWordPosition="1313" endWordPosition="1314">es output by their models. Schulte im Walde et al. (2008) also applied probabilistic soft clustering to verbs by incorporating subcategorization frames and selectional preferences based on WordNet. This model is based on the Expectation-Maximization algorithm and the Minimum Description Length principle. Since they focused on the incorporation of selectional preferences, they did not evaluate verb classes but evaluated only selectional preferences using a language model-based measure. Materna proposed LDA-frames, which are defined across verbs and can be considered to be a kind of verb class (Materna, 2012; Materna, 2013). LDA-frames are probabilistic semantic frames automatically induced from a raw corpus. He used a model based on latent Dirichlet allocation (LDA; Blei et al. (2003)) and the Dirichlet process to cluster verb instances of a triple (subject, verb, object) to produce semantic frames and roles. Both of these are represented as a probabilistic distribution of words across verbs. He applied this method to the BNC and acquired 1,200 frames and 400 roles (Materna, 2012). He did not evaluate the resulting frames as verb classes. In sum, there have been no studies that quantitatively ev</context>
<context position="9785" citStr="Materna, 2012" startWordPosition="1547" endWordPosition="1548"> approaches. Although Bayesian approaches are a possible solution to simultaneously induce frames and verb classes from a corpus as used in previous studies, it has prohibitive computational cost. For instance, Parisien and Stevenson applied HDP only to a small-scale child speech corpus that contains 170K verb uses to jointly induce subcategorization frames and verb classes (Parisien and Stevenson, 2010; Parisien and Stevenson, 2011). Materna applied an LDA-based method to the BNC, which contains 1.4M verb uses, to induce semantic frames across verbs that can be considered to be verb classes (Materna, 2012; Materna, 2013). However, it would take three months for this experiment using this 100 million word corpus.&apos; Although it is best to use the largest possible corpus for this kind of knowledge acquisition tasks (Sasano et al., 2009), it is infeasible to scale to giga-word corpora using such joint models. In this paper, we propose a two-step approach for inducing semantic frames and verb classes. First, we make multiple data points for each verb to deal with verb polysemy (cf. polysemy-aware previous studies still represented a verb as one data point (Korhonen et al., 2003; Miyao and Tsujii, 20</context>
<context position="24421" citStr="Materna, 2012" startWordPosition="3984" endWordPosition="3985">P PP), and the unfiltered raw frequencies of subcategorization frames are used as features to represent a verb. It is necessary to specify the number of clusters, k, for the IB method beforehand, and we adopt 35 and 42 clusters according to their reported high accuracies. To output multiple classes for each verb, we set a threshold, t, for class attribute probabilities. That is, classes that have a higher class attribute probability than the threshold are output for each verb. We report the results of the following threshold values: 0.01, 0.02, 0.05 and 0.10. The other baseline is LDA-frames (Materna, 2012). We use the induced LDA-frames that are 5http://ilexir.co.uk/applications/valex/ 1 � niPU = N j Max i 1035 method K predominant iPU class multiple class eval mPU eval mPU niPU F1 F1 NN 24 46.36 52.73 49.34 52.73 46.85 49.62 IB (k=35) 34.8 42.73 51.82 46.82 51.64 46.83 49.09 IB (k=42) 41.0 47.45 50.91 49.11 55.27 45.45 49.87 LDA-frames 53 30.00 47.27 36.71 41.82 44.28 43.01 Gigaword/S 9.6 25.64 71.27 37.70 32.91 64.71 43.62 Gigaword/SW 10.6 30.36 71.09 42.25 39.82 66.92 49.70 web/S 20.4 42.73 61.46 50.31 54.91 57.12 55.86 web/SW 11.8 34.36 71.82 46.40 49.09 67.01 56.50 Table 3: Type-level sing</context>
<context position="28677" citStr="Materna (2012)" startWordPosition="4670" endWordPosition="4671">i-class evaluations. As we did with the multi-class evaluations, we adopt modified purity (mPU), inverse purity (iPU) and their harmonic mean (F1) as the metrics for the evaluation with predominant classes. It is not necessary to normalize these metrics when we treat verbs as monosemous, and evaluate against the predominant sense. When we evaluate against the multiple classes in the gold standard, we do normalize the inverse purity. For baselines, we once more adopt the Nearest Neighbor (NN) and Information Bottleneck (IB) methods proposed by Korhonen et al. (2003), and LDA-frames proposed by Materna (2012). The 1036 clusterings with the NN and IB methods are obtained by using the VALEX subcategorization lexicon. To harden the clusterings of the IB method and the LDA-frames, the class with the highest probability is selected for each verb. This hardening process is exactly the same as Korhonen et al. (2003). Note that our results of the NN and IB methods are different from those reported in their paper since the data source is different.8 Table 3 lists accuracies of baseline methods and our methods. Our proposed method using the web corpus achieved comparable performance with the baseline method</context>
</contexts>
<marker>Materna, 2012</marker>
<rawString>Jiˇr´ı Materna. 2012. LDA-frames: An unsupervised approach to generating semantic frames. In Proceedings of the 13th International Conference CICLing 2012, Part I, pages 376–387.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiˇr´ı Materna</author>
</authors>
<title>Parameter estimation for LDAframes.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>482--486</pages>
<contexts>
<context position="8353" citStr="Materna, 2013" startWordPosition="1315" endWordPosition="1316">eir models. Schulte im Walde et al. (2008) also applied probabilistic soft clustering to verbs by incorporating subcategorization frames and selectional preferences based on WordNet. This model is based on the Expectation-Maximization algorithm and the Minimum Description Length principle. Since they focused on the incorporation of selectional preferences, they did not evaluate verb classes but evaluated only selectional preferences using a language model-based measure. Materna proposed LDA-frames, which are defined across verbs and can be considered to be a kind of verb class (Materna, 2012; Materna, 2013). LDA-frames are probabilistic semantic frames automatically induced from a raw corpus. He used a model based on latent Dirichlet allocation (LDA; Blei et al. (2003)) and the Dirichlet process to cluster verb instances of a triple (subject, verb, object) to produce semantic frames and roles. Both of these are represented as a probabilistic distribution of words across verbs. He applied this method to the BNC and acquired 1,200 frames and 400 roles (Materna, 2012). He did not evaluate the resulting frames as verb classes. In sum, there have been no studies that quantitatively evaluate polysemou</context>
<context position="9801" citStr="Materna, 2013" startWordPosition="1549" endWordPosition="1550">though Bayesian approaches are a possible solution to simultaneously induce frames and verb classes from a corpus as used in previous studies, it has prohibitive computational cost. For instance, Parisien and Stevenson applied HDP only to a small-scale child speech corpus that contains 170K verb uses to jointly induce subcategorization frames and verb classes (Parisien and Stevenson, 2010; Parisien and Stevenson, 2011). Materna applied an LDA-based method to the BNC, which contains 1.4M verb uses, to induce semantic frames across verbs that can be considered to be verb classes (Materna, 2012; Materna, 2013). However, it would take three months for this experiment using this 100 million word corpus.&apos; Although it is best to use the largest possible corpus for this kind of knowledge acquisition tasks (Sasano et al., 2009), it is infeasible to scale to giga-word corpora using such joint models. In this paper, we propose a two-step approach for inducing semantic frames and verb classes. First, we make multiple data points for each verb to deal with verb polysemy (cf. polysemy-aware previous studies still represented a verb as one data point (Korhonen et al., 2003; Miyao and Tsujii, 2009)). To do that</context>
</contexts>
<marker>Materna, 2013</marker>
<rawString>Jiˇr´ı Materna. 2013. Parameter estimation for LDAframes. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 482–486.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Supervised learning of a probabilistic lexicon of verb semantic classes.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1328--1337</pages>
<contexts>
<context position="6001" citStr="Miyao and Tsujii (2009)" startWordPosition="944" endWordPosition="947">ed to account for verb learning by children and did not evaluate the resultant verb classes. Modi et al. (2012) extended the model of Titov and Klementiev (2012), which is an unsupervised model for inducing semantic roles, to jointly induce semantic roles and frames across verbs using the Chinese Restaurant Process (Aldous, 1985). All of the above methods considered verbs to be monosemous and did not deal with verb polysemy. Our approach also uses Bayesian methods, but is designed to capture verb polysemy. We summarize a few studies that consider polysemy of verbs in the rest of this section. Miyao and Tsujii (2009) proposed a supervised method that can handle verb polysemy. Their method represents a verb’s syntactic and semantic features, and learns a log-linear model from the SemLink corpus (Loper et al., 2007). Boleda et al. (2007) also proposed a supervised method for Catalan adjectives considering the polysemy of adjectives. The most closely related work to our polysemyaware task of unsupervised verb class induction is the work of Korhonen et al. (2003), who used distributions of subcategorization frames to cluster verbs. They adopted the Nearest Neighbor (NN) and Information Bottleneck (IB) methods</context>
<context position="10388" citStr="Miyao and Tsujii, 2009" startWordPosition="1647" endWordPosition="1651">asses (Materna, 2012; Materna, 2013). However, it would take three months for this experiment using this 100 million word corpus.&apos; Although it is best to use the largest possible corpus for this kind of knowledge acquisition tasks (Sasano et al., 2009), it is infeasible to scale to giga-word corpora using such joint models. In this paper, we propose a two-step approach for inducing semantic frames and verb classes. First, we make multiple data points for each verb to deal with verb polysemy (cf. polysemy-aware previous studies still represented a verb as one data point (Korhonen et al., 2003; Miyao and Tsujii, 2009)). To do that, we induce verb-specific semantic frames by clustering verb uses. Then, we induce verb classes by clustering these verbspecific semantic frames across verbs. An interesting point here is that we can use exactly the same method for these two clustering steps. Our procedure to automatically induce verb classes from verb uses is summarized as follows: 1. induce verb-specific semantic frames by clustering predicate-argument structures for each verb extracted from automatic parses as shown in the lower part of Figure 1, and 2. induce verb classes by clustering the induced semantic fra</context>
</contexts>
<marker>Miyao, Tsujii, 2009</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2009. Supervised learning of a probabilistic lexicon of verb semantic classes. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1328–1337.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashutosh Modi</author>
<author>Ivan Titov</author>
<author>Alexandre Klementiev</author>
</authors>
<title>Unsupervised induction of frame-semantic representations.</title>
<date>2012</date>
<booktitle>In Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="5489" citStr="Modi et al. (2012)" startWordPosition="859" endWordPosition="862">ess mixture model (DPMM; Neal (2000)) to cluster verbs based on subcategorization frame distributions. They evaluated their result with a gold-standard test set, where a single class is assigned to a verb. Parisien and Stevenson (2010) proposed a hierarchical Dirichlet process (HDP; Teh et al. (2006)) model to jointly learn argument structures (subcategorization frames) and verb classes by using syntactic features. Parisien and Stevenson (2011) extended their model by adding semantic features. They tried to account for verb learning by children and did not evaluate the resultant verb classes. Modi et al. (2012) extended the model of Titov and Klementiev (2012), which is an unsupervised model for inducing semantic roles, to jointly induce semantic roles and frames across verbs using the Chinese Restaurant Process (Aldous, 1985). All of the above methods considered verbs to be monosemous and did not deal with verb polysemy. Our approach also uses Bayesian methods, but is designed to capture verb polysemy. We summarize a few studies that consider polysemy of verbs in the rest of this section. Miyao and Tsujii (2009) proposed a supervised method that can handle verb polysemy. Their method represents a v</context>
<context position="32086" citStr="Modi et al. (2012)" startWordPosition="5222" endWordPosition="5225"> features, we compare two feature combinations: “S-S” and “SW-S,” which achieved high performance in the type-level multiclass evaluations (Section 4.3). The results of these methods are obtained by averaging five runs. For a baseline, we use verb-specific semantic frames without clustering across verbs (“S-NIL” and “SW-NIL”), where these frames are considered to be verb classes but not shared across verbs. Table 4 lists accuracies of these methods for the two corpora. We can see that “SW-S” achieved a higher F1 than “S-S” and the baselines without verb class induction (“S-NIL” and “SW-NIL”). Modi et al. (2012) induced semantic frames across verbs using the monosemous assumption and reported an F1 of 44.7% (77.9% PU and 31.4% iPU) for the assignment of FrameNet frames to the FrameNet corpus. We also conducted the above evaluation against FrameNet frames for 75 verbs.9 We achieved an F1 of 62.79% (66.97% mPU and 59.09% iPU) for “web/SW-S,” and an F1 of 60.06% (65.58% mPU and 55.39% iPU) for “Gigaword/SW-S.” It is difficult to directly compare these results with Modi et al. (2012), but our induced verb classes seem to have higher F1 accuracy. 4.6 Full Experiments and Discussions We finally induce verb</context>
</contexts>
<marker>Modi, Titov, Klementiev, 2012</marker>
<rawString>Ashutosh Modi, Ivan Titov, and Alexandre Klementiev. 2012. Unsupervised induction of frame-semantic representations. In Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
</authors>
<title>Markov chain sampling methods for Dirichlet process mixture models.</title>
<date>2000</date>
<journal>Journal of computational and graphical statistics,</journal>
<volume>9</volume>
<issue>2</issue>
<pages>265</pages>
<contexts>
<context position="4907" citStr="Neal (2000)" startWordPosition="772" endWordPosition="773">method in these studies is to represent each verb as a single data point and apply classification (e.g., Joanis et al. (2008)) or clustering (e.g., Sun and Korhonen (2009)) to these data points. As a representation for a data point, distributions of subcategorization frames are often used, and other semantic features (e.g., selectional preferences) are sometimes added to improve the performance. Among these studies on monosemous verb clustering (i.e., predominant class induction), there have been several Bayesian methods. Vlachos et al. (2009) proposed a Dirichlet process mixture model (DPMM; Neal (2000)) to cluster verbs based on subcategorization frame distributions. They evaluated their result with a gold-standard test set, where a single class is assigned to a verb. Parisien and Stevenson (2010) proposed a hierarchical Dirichlet process (HDP; Teh et al. (2006)) model to jointly learn argument structures (subcategorization frames) and verb classes by using syntactic features. Parisien and Stevenson (2011) extended their model by adding semantic features. They tried to account for verb learning by children and did not evaluate the resultant verb classes. Modi et al. (2012) extended the mode</context>
</contexts>
<marker>Neal, 2000</marker>
<rawString>Radford M. Neal. 2000. Markov chain sampling methods for Dirichlet process mixture models. Journal of computational and graphical statistics, 9(2):249– 265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Parisien</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Learning verb alternations in a usage-based Bayesian model.</title>
<date>2010</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Cognitive Science Society.</booktitle>
<contexts>
<context position="2318" citStr="Parisien and Stevenson, 2010" startWordPosition="360" endWordPosition="363">ch as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008). In this paper, we propose an unsupervised method for inducing verb classes</context>
<context position="5106" citStr="Parisien and Stevenson (2010)" startWordPosition="800" endWordPosition="803"> data points. As a representation for a data point, distributions of subcategorization frames are often used, and other semantic features (e.g., selectional preferences) are sometimes added to improve the performance. Among these studies on monosemous verb clustering (i.e., predominant class induction), there have been several Bayesian methods. Vlachos et al. (2009) proposed a Dirichlet process mixture model (DPMM; Neal (2000)) to cluster verbs based on subcategorization frame distributions. They evaluated their result with a gold-standard test set, where a single class is assigned to a verb. Parisien and Stevenson (2010) proposed a hierarchical Dirichlet process (HDP; Teh et al. (2006)) model to jointly learn argument structures (subcategorization frames) and verb classes by using syntactic features. Parisien and Stevenson (2011) extended their model by adding semantic features. They tried to account for verb learning by children and did not evaluate the resultant verb classes. Modi et al. (2012) extended the model of Titov and Klementiev (2012), which is an unsupervised model for inducing semantic roles, to jointly induce semantic roles and frames across verbs using the Chinese Restaurant Process (Aldous, 19</context>
<context position="9578" citStr="Parisien and Stevenson, 2010" startWordPosition="1509" endWordPosition="1513">mous verb classes automatically induced by unsupervised methods. 3 Our Approach 3.1 Overview Our objective is to automatically learn semantic frames and verb classes from a massive amount of verb uses following usage-based approaches. Although Bayesian approaches are a possible solution to simultaneously induce frames and verb classes from a corpus as used in previous studies, it has prohibitive computational cost. For instance, Parisien and Stevenson applied HDP only to a small-scale child speech corpus that contains 170K verb uses to jointly induce subcategorization frames and verb classes (Parisien and Stevenson, 2010; Parisien and Stevenson, 2011). Materna applied an LDA-based method to the BNC, which contains 1.4M verb uses, to induce semantic frames across verbs that can be considered to be verb classes (Materna, 2012; Materna, 2013). However, it would take three months for this experiment using this 100 million word corpus.&apos; Although it is best to use the largest possible corpus for this kind of knowledge acquisition tasks (Sasano et al., 2009), it is infeasible to scale to giga-word corpora using such joint models. In this paper, we propose a two-step approach for inducing semantic frames and verb cla</context>
</contexts>
<marker>Parisien, Stevenson, 2010</marker>
<rawString>Christopher Parisien and Suzanne Stevenson. 2010. Learning verb alternations in a usage-based Bayesian model. In Proceedings of the 32nd Annual Meeting of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Parisien</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Generalizing between form and meaning using learned verb classes.</title>
<date>2011</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Cognitive Science Society.</booktitle>
<contexts>
<context position="2348" citStr="Parisien and Stevenson, 2011" startWordPosition="364" endWordPosition="367">n (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008). In this paper, we propose an unsupervised method for inducing verb classes that is aware of verb polysem</context>
<context position="5319" citStr="Parisien and Stevenson (2011)" startWordPosition="831" endWordPosition="834">nce. Among these studies on monosemous verb clustering (i.e., predominant class induction), there have been several Bayesian methods. Vlachos et al. (2009) proposed a Dirichlet process mixture model (DPMM; Neal (2000)) to cluster verbs based on subcategorization frame distributions. They evaluated their result with a gold-standard test set, where a single class is assigned to a verb. Parisien and Stevenson (2010) proposed a hierarchical Dirichlet process (HDP; Teh et al. (2006)) model to jointly learn argument structures (subcategorization frames) and verb classes by using syntactic features. Parisien and Stevenson (2011) extended their model by adding semantic features. They tried to account for verb learning by children and did not evaluate the resultant verb classes. Modi et al. (2012) extended the model of Titov and Klementiev (2012), which is an unsupervised model for inducing semantic roles, to jointly induce semantic roles and frames across verbs using the Chinese Restaurant Process (Aldous, 1985). All of the above methods considered verbs to be monosemous and did not deal with verb polysemy. Our approach also uses Bayesian methods, but is designed to capture verb polysemy. We summarize a few studies th</context>
<context position="9609" citStr="Parisien and Stevenson, 2011" startWordPosition="1514" endWordPosition="1517">y induced by unsupervised methods. 3 Our Approach 3.1 Overview Our objective is to automatically learn semantic frames and verb classes from a massive amount of verb uses following usage-based approaches. Although Bayesian approaches are a possible solution to simultaneously induce frames and verb classes from a corpus as used in previous studies, it has prohibitive computational cost. For instance, Parisien and Stevenson applied HDP only to a small-scale child speech corpus that contains 170K verb uses to jointly induce subcategorization frames and verb classes (Parisien and Stevenson, 2010; Parisien and Stevenson, 2011). Materna applied an LDA-based method to the BNC, which contains 1.4M verb uses, to induce semantic frames across verbs that can be considered to be verb classes (Materna, 2012; Materna, 2013). However, it would take three months for this experiment using this 100 million word corpus.&apos; Although it is best to use the largest possible corpus for this kind of knowledge acquisition tasks (Sasano et al., 2009), it is infeasible to scale to giga-word corpora using such joint models. In this paper, we propose a two-step approach for inducing semantic frames and verb classes. First, we make multiple d</context>
</contexts>
<marker>Parisien, Stevenson, 2011</marker>
<rawString>Christopher Parisien and Suzanne Stevenson. 2011. Generalizing between form and meaning using learned verb classes. In Proceedings of the 33rd Annual Meeting of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Reichart</author>
<author>Anna Korhonen</author>
</authors>
<title>Improved lexical acquisition through DPP-based verb clustering.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>862--872</pages>
<contexts>
<context position="2421" citStr="Reichart and Korhonen, 2013" startWordPosition="376" endWordPosition="379">cea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008). In this paper, we propose an unsupervised method for inducing verb classes that is aware of verb polysemy. Our method consists of two clustering steps: verb-specific semantic fr</context>
</contexts>
<marker>Reichart, Korhonen, 2013</marker>
<rawString>Roi Reichart and Anna Korhonen. 2013. Improved lexical acquisition through DPP-based verb clustering. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 862–872.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Reichart</author>
<author>Omri Abend</author>
<author>Ari Rappoport</author>
</authors>
<title>Type level clustering evaluation: New measures and a POS induction case study.</title>
<date>2010</date>
<booktitle>In Proceedings of the 14th Conference on Computational Natural Language Learning,</booktitle>
<pages>77--87</pages>
<contexts>
<context position="17101" citStr="Reichart et al. (2010)" startWordPosition="2740" endWordPosition="2743">eans that semantic similarity based on word overlap is naturally considered by looking at lexical information. We will compare in our experiments four possible combinations: two feature representations for each of the two clustering steps. 4 Experiments and Evaluations We first describe our experimental settings and define evaluation metrics to evaluate induced soft clusterings of verb classes. Then, we conduct type-level multi-class evaluations, type-level single-class evaluations and token-level multiclass evaluations. These two levels of evaluations are performed by considering the work of Reichart et al. (2010) on clustering evaluation. Finally, we discuss the results of our full experiments. 4.1 Experimental Settings We use two kinds of large-scale corpora: a web corpus and the English Gigaword corpus. To prepare a web corpus, we extracted sentences from crawled web pages that are judged to be written in English based on the encoding information. Then, we selected sentences that consist of at most 40 words, and removed duplicated sentences. From this process, we obtained a corpus of one billion sentences, totaling approximately 20 billion words. We focused on verbs whose frequency in the web corpus</context>
</contexts>
<marker>Reichart, Abend, Rappoport, 2010</marker>
<rawString>Roi Reichart, Omri Abend, and Ari Rappoport. 2010. Type level clustering evaluation: New measures and a POS induction case study. In Proceedings of the 14th Conference on Computational Natural Language Learning, pages 77–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryohei Sasano</author>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>The effect of corpus size on case frame acquisition for discourse analysis.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>521--529</pages>
<contexts>
<context position="10017" citStr="Sasano et al., 2009" startWordPosition="1586" endWordPosition="1589">tevenson applied HDP only to a small-scale child speech corpus that contains 170K verb uses to jointly induce subcategorization frames and verb classes (Parisien and Stevenson, 2010; Parisien and Stevenson, 2011). Materna applied an LDA-based method to the BNC, which contains 1.4M verb uses, to induce semantic frames across verbs that can be considered to be verb classes (Materna, 2012; Materna, 2013). However, it would take three months for this experiment using this 100 million word corpus.&apos; Although it is best to use the largest possible corpus for this kind of knowledge acquisition tasks (Sasano et al., 2009), it is infeasible to scale to giga-word corpora using such joint models. In this paper, we propose a two-step approach for inducing semantic frames and verb classes. First, we make multiple data points for each verb to deal with verb polysemy (cf. polysemy-aware previous studies still represented a verb as one data point (Korhonen et al., 2003; Miyao and Tsujii, 2009)). To do that, we induce verb-specific semantic frames by clustering verb uses. Then, we induce verb classes by clustering these verbspecific semantic frames across verbs. An interesting point here is that we can use exactly the </context>
</contexts>
<marker>Sasano, Kawahara, Kurohashi, 2009</marker>
<rawString>Ryohei Sasano, Daisuke Kawahara, and Sadao Kurohashi. 2009. The effect of corpus size on case frame acquisition for discourse analysis. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 521–529.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
<author>Christian Hying</author>
<author>Christian Scheible</author>
<author>Helmut Schmid</author>
</authors>
<title>Combining EM training and the MDL principle for an automatic verb classification incorporating selectional preferences.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>496--504</pages>
<contexts>
<context position="2842" citStr="Walde et al., 2008" startWordPosition="441" endWordPosition="444">8; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008). In this paper, we propose an unsupervised method for inducing verb classes that is aware of verb polysemy. Our method consists of two clustering steps: verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames. By taking this step-wise approach, we can not only induce verb classes with frequency information from a massive amount of verb uses in a scalable manner, but also deal with verb polysemy. Our novel contributions are summarized as follows: • induce both semantic frames and verb classes from a massiv</context>
<context position="7781" citStr="Walde et al. (2008)" startWordPosition="1230" endWordPosition="1233">s created on the basis of Levin’s classes (Levin, 1993). Lapata and Brew (2004) and Li and Brew (2007) proposed probabilistic models for calculating prior probabilities of verb classes for a verb. These models are approximated to condition not 1031 on verbs but on subcategorization frames. As mentioned in Li and Brew (2007), it is desirable to extend the model to depend on verbs to further improve accuracy. They conducted several evaluations including predominant class induction and token-level verb sense disambiguation, but did not evaluate multiple classes output by their models. Schulte im Walde et al. (2008) also applied probabilistic soft clustering to verbs by incorporating subcategorization frames and selectional preferences based on WordNet. This model is based on the Expectation-Maximization algorithm and the Minimum Description Length principle. Since they focused on the incorporation of selectional preferences, they did not evaluate verb classes but evaluated only selectional preferences using a language model-based measure. Materna proposed LDA-frames, which are defined across verbs and can be considered to be a kind of verb class (Materna, 2012; Materna, 2013). LDA-frames are probabilist</context>
</contexts>
<marker>Walde, Hying, Scheible, Schmid, 2008</marker>
<rawString>Sabine Schulte im Walde, Christian Hying, Christian Scheible, and Helmut Schmid. 2008. Combining EM training and the MDL principle for an automatic verb classification incorporating selectional preferences. In Proceedings of ACL-08: HLT, pages 496– 504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
</authors>
<title>Experiments on the automatic induction of German semantic verb classes.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>2</issue>
<contexts>
<context position="2184" citStr="Walde, 2006" startWordPosition="337" endWordPosition="338">behavior. Such verb classes have been used in many NLP applications that need to consider semantics in particular, such as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Bre</context>
</contexts>
<marker>Walde, 2006</marker>
<rawString>Sabine Schulte im Walde. 2006. Experiments on the automatic induction of German semantic verb classes. Computational Linguistics, 32(2):159–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Shi</author>
<author>Rada Mihalcea</author>
</authors>
<title>Putting pieces together: Combining FrameNet, VerbNet and WordNet for robust semantic parsing.</title>
<date>2005</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>100--111</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1804" citStr="Shi and Mihalcea, 2005" startWordPosition="273" endWordPosition="276">s essential for natural language processing (NLP), and thus lexical resources for verbs play an important role in NLP. Verb classes are one such lexical resource. Manually-crafted verb classes have been developed, such as Levin’s classes (Levin, 1993) and their extension, VerbNet (Kipper-Schuler, 2005), in which verbs are organized into classes on the basis of their syntactic and semantic behavior. Such verb classes have been used in many NLP applications that need to consider semantics in particular, such as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart a</context>
</contexts>
<marker>Shi, Mihalcea, 2005</marker>
<rawString>Lei Shi and Rada Mihalcea. 2005. Putting pieces together: Combining FrameNet, VerbNet and WordNet for robust semantic parsing. In Computational Linguistics and Intelligent Text Processing, pages 100–111. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Shutova</author>
<author>Lin Sun</author>
<author>Anna Korhonen</author>
</authors>
<title>Metaphor identification using verb and noun clustering.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>1002--1010</pages>
<contexts>
<context position="36280" citStr="Shutova et al., 2010" startWordPosition="5881" endWordPosition="5884">asses are used for evaluation. This is consistent with Levin’s principle of organizing verb classes according to the syntactic behavior of verbs. As applications of the resulting semantic frames and verb classes, we plan to integrate them into syntactic parsing, semantic role labeling and verb sense disambiguation. For instance, Kawahara and Kurohashi (2006) improved accuracy of dependency parsing based on Japanese semantic frames automatically induced from a raw corpus. It is also valuable and promising to apply the induced verb classes to NLP applications as used in metaphor identification (Shutova et al., 2010) and argumentative zoning (Guo et al., 2011). Acknowledgments This work was supported by Kyoto University John Mung Program and JST CREST. We also gratefully acknowledge the support of the National Science Foundation Grant NSF-IIS-1116782, A Bayesian Approach to Dynamic Lexical Resources for Flexible Language Processing. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. class semantic frames Class 1 Class 2 Class 3 Class 4 Class 5 Class 6 Class 7 Class 8 Cla</context>
</contexts>
<marker>Shutova, Sun, Korhonen, 2010</marker>
<rawString>Ekaterina Shutova, Lin Sun, and Anna Korhonen. 2010. Metaphor identification using verb and noun clustering. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 1002–1010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvia Springorum</author>
<author>Sabine Schulte im Walde</author>
<author>Jason Utt</author>
</authors>
<title>Detecting polysemy in hard and soft cluster analyses of German preposition vector spaces.</title>
<date>2013</date>
<booktitle>In Proceedings of the 6th International Joint Conference on Natural Language Processing,</booktitle>
<pages>632--640</pages>
<contexts>
<context position="19125" citStr="Springorum et al. (2013)" startWordPosition="3064" endWordPosition="3067">e precision and recall of a clustering, modified purity and inverse purity (also called collocation or weighted class accuracy) are commonly used in previous studies on verb clustering (e.g., Sun and Korhonen (2009)). However, since these measures are only applicable to a hard clustering, it is necessary to extend them to be applicable to a soft clustering, because in our task a verb can belong to multiple clusters or classes.4 We propose a normalized version of modified purity and inverse purity. This kind of normalization for soft clusterings was performed for other evaluation metrics as in Springorum et al. (2013). To measure the precision of a clustering, a normalized version of modified purity is defined as follows. Suppose K is the set of automatically induced clusters and G is the set of gold classes. Let Ki be the verb vector of the i-th cluster and Gj be the verb vector of the j-th gold class. Each component of these vectors is a normalized frequency, which equals a cluster/class attribute probability given a verb. Where there is no frequency information available for class distribution, such as the gold-standard data described in Section 4.3, we use a uniform distribution across the verb’s class</context>
</contexts>
<marker>Springorum, Walde, Utt, 2013</marker>
<rawString>Sylvia Springorum, Sabine Schulte im Walde, and Jason Utt. 2013. Detecting polysemy in hard and soft cluster analyses of German preposition vector spaces. In Proceedings of the 6th International Joint Conference on Natural Language Processing, pages 632–640.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suzanne Stevenson</author>
<author>Eric Joanis</author>
</authors>
<title>Semisupervised verb class discovery using noisy features.</title>
<date>2003</date>
<booktitle>In Proceedings of the 7th Conference on Natural Language Learning,</booktitle>
<pages>71--78</pages>
<contexts>
<context position="2160" citStr="Stevenson and Joanis, 2003" startWordPosition="331" endWordPosition="334"> basis of their syntactic and semantic behavior. Such verb classes have been used in many NLP applications that need to consider semantics in particular, such as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et a</context>
</contexts>
<marker>Stevenson, Joanis, 2003</marker>
<rawString>Suzanne Stevenson and Eric Joanis. 2003. Semisupervised verb class discovery using noisy features. In Proceedings of the 7th Conference on Natural Language Learning, pages 71–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajen Subba</author>
<author>Barbara Di Eugenio</author>
</authors>
<title>An effective discourse parser that uses rich linguistic information.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>566--574</pages>
<marker>Subba, Di Eugenio, 2009</marker>
<rawString>Rajen Subba and Barbara Di Eugenio. 2009. An effective discourse parser that uses rich linguistic information. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 566–574.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Sun</author>
<author>Anna Korhonen</author>
</authors>
<title>Improving verb clustering with automatically acquired selectional preferences.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>638--647</pages>
<contexts>
<context position="2266" citStr="Sun and Korhonen, 2009" startWordPosition="352" endWordPosition="355">t need to consider semantics in particular, such as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008). In this paper, we prop</context>
<context position="4467" citStr="Sun and Korhonen (2009)" startWordPosition="706" endWordPosition="709">tics Verb classes: Semantic frames: Verb uses: Figure 1: Overview of our two-step approach. Verb-specific semantic frames are first induced from verb uses (lower part) and then verb classes are induced from the semantic frames (upper part). The labels of verb classes are manually assigned here for better understanding. 2 Related Work As stated in Section 1, most of the previous studies on verb clustering assume that verbs are monosemous. A typical method in these studies is to represent each verb as a single data point and apply classification (e.g., Joanis et al. (2008)) or clustering (e.g., Sun and Korhonen (2009)) to these data points. As a representation for a data point, distributions of subcategorization frames are often used, and other semantic features (e.g., selectional preferences) are sometimes added to improve the performance. Among these studies on monosemous verb clustering (i.e., predominant class induction), there have been several Bayesian methods. Vlachos et al. (2009) proposed a Dirichlet process mixture model (DPMM; Neal (2000)) to cluster verbs based on subcategorization frame distributions. They evaluated their result with a gold-standard test set, where a single class is assigned t</context>
<context position="18716" citStr="Sun and Korhonen (2009)" startWordPosition="2994" endWordPosition="2997">pplying the same frequency threshold as the web corpus. We extracted 423,778,278 predicate-argument structures from this corpus. We set the hyper-parameters α in (1) and Q in (3) to 1.0. The cluster assignments for all the components were initialized randomly. We took 100 samples for each input frame and selected the cluster assignment that has the highest probability. 4.2 Evaluation Metrics To measure the precision and recall of a clustering, modified purity and inverse purity (also called collocation or weighted class accuracy) are commonly used in previous studies on verb clustering (e.g., Sun and Korhonen (2009)). However, since these measures are only applicable to a hard clustering, it is necessary to extend them to be applicable to a soft clustering, because in our task a verb can belong to multiple clusters or classes.4 We propose a normalized version of modified purity and inverse purity. This kind of normalization for soft clusterings was performed for other evaluation metrics as in Springorum et al. (2013). To measure the precision of a clustering, a normalized version of modified purity is defined as follows. Suppose K is the set of automatically induced clusters and G is the set of gold clas</context>
<context position="29476" citStr="Sun and Korhonen (2009)" startWordPosition="4801" endWordPosition="4804">class with the highest probability is selected for each verb. This hardening process is exactly the same as Korhonen et al. (2003). Note that our results of the NN and IB methods are different from those reported in their paper since the data source is different.8 Table 3 lists accuracies of baseline methods and our methods. Our proposed method using the web corpus achieved comparable performance with the baseline methods on the predominant class evaluation and outperformed them on the multiple class evaluation. More sophisticated methods for predominant class induction, such as the method of Sun and Korhonen (2009) using selectional preferences, could produce better single-class outputs, but have difficulty in producing polysemy-aware verb classes. From the result, we can see that the induced verb classes based on slot-only features did not achieve a higher F1 than those based on slot-word pair features in many cases. This result is different from that of multi-class evaluations in Section 4.3. We speculate that slot distributions are not so different among verbs when all uses of a verb are merged into one frame, and thus their discrimination power is lower than that in the intermediate construction of </context>
</contexts>
<marker>Sun, Korhonen, 2009</marker>
<rawString>Lin Sun and Anna Korhonen. 2009. Improving verb clustering with automatically acquired selectional preferences. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 638–647.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Sun</author>
<author>Anna Korhonen</author>
<author>Yuval Krymolowski</author>
</authors>
<title>Automatic classification of English verbs using rich syntactic features.</title>
<date>2008</date>
<booktitle>In Proceedings of the 3rd International Joint Conference on Natural Language Processing,</booktitle>
<pages>769--774</pages>
<contexts>
<context position="2242" citStr="Sun et al., 2008" startWordPosition="348" endWordPosition="351">P applications that need to consider semantics in particular, such as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008)</context>
</contexts>
<marker>Sun, Korhonen, Krymolowski, 2008</marker>
<rawString>Lin Sun, Anna Korhonen, and Yuval Krymolowski. 2008. Automatic classification of English verbs using rich syntactic features. In Proceedings of the 3rd International Joint Conference on Natural Language Processing, pages 769–774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Sun</author>
<author>Diana McCarthy</author>
<author>Anna Korhonen</author>
</authors>
<title>Diathesis alternation approximation for verb clustering.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, Short Papers,</booktitle>
<pages>736--741</pages>
<contexts>
<context position="2440" citStr="Sun et al., 2013" startWordPosition="380" endWordPosition="383">ing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008). In this paper, we propose an unsupervised method for inducing verb classes that is aware of verb polysemy. Our method consists of two clustering steps: verb-specific semantic frames are first indu</context>
</contexts>
<marker>Sun, McCarthy, Korhonen, 2013</marker>
<rawString>Lin Sun, Diana McCarthy, and Anna Korhonen. 2013. Diathesis alternation approximation for verb clustering. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, Short Papers, pages 736–741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Swier</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Exploiting a verb lexicon in automatic semantic role labelling.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>883--890</pages>
<contexts>
<context position="1779" citStr="Swier and Stevenson, 2005" startWordPosition="269" endWordPosition="272">uring the sense of a verb is essential for natural language processing (NLP), and thus lexical resources for verbs play an important role in NLP. Verb classes are one such lexical resource. Manually-crafted verb classes have been developed, such as Levin’s classes (Levin, 1993) and their extension, VerbNet (Kipper-Schuler, 2005), in which verbs are organized into classes on the basis of their syntactic and semantic behavior. Such verb classes have been used in many NLP applications that need to consider semantics in particular, such as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott</context>
</contexts>
<marker>Swier, Stevenson, 2005</marker>
<rawString>Robert Swier and Suzanne Stevenson. 2005. Exploiting a verb lexicon in automatic semantic role labelling. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 883–890.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Michael I Jordan</author>
<author>Matthew J Beal</author>
<author>David M Blei</author>
</authors>
<title>Hierarchical Dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>101</volume>
<issue>476</issue>
<contexts>
<context position="5172" citStr="Teh et al. (2006)" startWordPosition="811" endWordPosition="814">rization frames are often used, and other semantic features (e.g., selectional preferences) are sometimes added to improve the performance. Among these studies on monosemous verb clustering (i.e., predominant class induction), there have been several Bayesian methods. Vlachos et al. (2009) proposed a Dirichlet process mixture model (DPMM; Neal (2000)) to cluster verbs based on subcategorization frame distributions. They evaluated their result with a gold-standard test set, where a single class is assigned to a verb. Parisien and Stevenson (2010) proposed a hierarchical Dirichlet process (HDP; Teh et al. (2006)) model to jointly learn argument structures (subcategorization frames) and verb classes by using syntactic features. Parisien and Stevenson (2011) extended their model by adding semantic features. They tried to account for verb learning by children and did not evaluate the resultant verb classes. Modi et al. (2012) extended the model of Titov and Klementiev (2012), which is an unsupervised model for inducing semantic roles, to jointly induce semantic roles and frames across verbs using the Chinese Restaurant Process (Aldous, 1985). All of the above methods considered verbs to be monosemous an</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and David M. Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101(476).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naftali Tishby</author>
<author>Fernando C Pereira</author>
<author>William Bialek</author>
</authors>
<title>The information bottleneck method.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Allerton Conference on Communication, Control and Computing,</booktitle>
<pages>368--377</pages>
<contexts>
<context position="6749" citStr="Tishby et al., 1999" startWordPosition="1063" endWordPosition="1066">nd learns a log-linear model from the SemLink corpus (Loper et al., 2007). Boleda et al. (2007) also proposed a supervised method for Catalan adjectives considering the polysemy of adjectives. The most closely related work to our polysemyaware task of unsupervised verb class induction is the work of Korhonen et al. (2003), who used distributions of subcategorization frames to cluster verbs. They adopted the Nearest Neighbor (NN) and Information Bottleneck (IB) methods for clustering. In particular, they tried to consider verb polysemy by using the IB method, which is a soft clustering method (Tishby et al., 1999). However, the verb itself is still represented as a single data point. After performing soft clustering, they noted that most verbs fell into a single class, and they decided to assign a single class to each verb by hardening the clustering. They considered multiple classes only in the gold-standard data used for their evaluations. We also evaluate our induced verb classes on this gold-standard data, which was created on the basis of Levin’s classes (Levin, 1993). Lapata and Brew (2004) and Li and Brew (2007) proposed probabilistic models for calculating prior probabilities of verb classes fo</context>
</contexts>
<marker>Tishby, Pereira, Bialek, 1999</marker>
<rawString>Naftali Tishby, Fernando C. Pereira, and William Bialek. 1999. The information bottleneck method. In Proceedings of the 37th Annual Allerton Conference on Communication, Control and Computing, pages 368–377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Alexandre Klementiev</author>
</authors>
<title>A Bayesian approach to unsupervised semantic role induction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>12--22</pages>
<contexts>
<context position="5539" citStr="Titov and Klementiev (2012)" startWordPosition="867" endWordPosition="870">cluster verbs based on subcategorization frame distributions. They evaluated their result with a gold-standard test set, where a single class is assigned to a verb. Parisien and Stevenson (2010) proposed a hierarchical Dirichlet process (HDP; Teh et al. (2006)) model to jointly learn argument structures (subcategorization frames) and verb classes by using syntactic features. Parisien and Stevenson (2011) extended their model by adding semantic features. They tried to account for verb learning by children and did not evaluate the resultant verb classes. Modi et al. (2012) extended the model of Titov and Klementiev (2012), which is an unsupervised model for inducing semantic roles, to jointly induce semantic roles and frames across verbs using the Chinese Restaurant Process (Aldous, 1985). All of the above methods considered verbs to be monosemous and did not deal with verb polysemy. Our approach also uses Bayesian methods, but is designed to capture verb polysemy. We summarize a few studies that consider polysemy of verbs in the rest of this section. Miyao and Tsujii (2009) proposed a supervised method that can handle verb polysemy. Their method represents a verb’s syntactic and semantic features, and learns </context>
</contexts>
<marker>Titov, Klementiev, 2012</marker>
<rawString>Ivan Titov and Alexandre Klementiev. 2012. A Bayesian approach to unsupervised semantic role induction. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 12–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Vlachos</author>
<author>Anna Korhonen</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Unsupervised and constrained Dirichlet process mixture models for verb clustering.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics,</booktitle>
<pages>74--82</pages>
<contexts>
<context position="2288" citStr="Vlachos et al., 2009" startWordPosition="356" endWordPosition="359">tics in particular, such as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008). In this paper, we propose an unsupervised me</context>
<context position="4845" citStr="Vlachos et al. (2009)" startWordPosition="760" endWordPosition="763"> studies on verb clustering assume that verbs are monosemous. A typical method in these studies is to represent each verb as a single data point and apply classification (e.g., Joanis et al. (2008)) or clustering (e.g., Sun and Korhonen (2009)) to these data points. As a representation for a data point, distributions of subcategorization frames are often used, and other semantic features (e.g., selectional preferences) are sometimes added to improve the performance. Among these studies on monosemous verb clustering (i.e., predominant class induction), there have been several Bayesian methods. Vlachos et al. (2009) proposed a Dirichlet process mixture model (DPMM; Neal (2000)) to cluster verbs based on subcategorization frame distributions. They evaluated their result with a gold-standard test set, where a single class is assigned to a verb. Parisien and Stevenson (2010) proposed a hierarchical Dirichlet process (HDP; Teh et al. (2006)) model to jointly learn argument structures (subcategorization frames) and verb classes by using syntactic features. Parisien and Stevenson (2011) extended their model by adding semantic features. They tried to account for verb learning by children and did not evaluate th</context>
</contexts>
<marker>Vlachos, Korhonen, Ghahramani, 2009</marker>
<rawString>Andreas Vlachos, Anna Korhonen, and Zoubin Ghahramani. 2009. Unsupervised and constrained Dirichlet process mixture models for verb clustering. In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, pages 74–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>One sense per collocation.</title>
<date>1993</date>
<booktitle>In Proceedings of the Workshop on Human Language Technology,</booktitle>
<pages>266--271</pages>
<contexts>
<context position="11709" citStr="Yarowsky, 1993" startWordPosition="1864" endWordPosition="1865">wing sections in detail. 3.2 Inducing Verb-specific Semantic Frames We induce verb-specific semantic frames from verb uses based on the method of Kawahara et al. (2014). Our semantic frames consist of case slots, each of which consists of word instances that can be filled. The procedure for inducing these semantic frames is as follows: 1. apply dependency parsing to a raw corpus and extract predicate-argument structures for each verb from the automatic parses, 2. merge the predicate-argument structures that have presumably the same meaning based on the assumption of one sense per collocation (Yarowsky, 1993) to get a set of initial frames, and &apos;In our replication experiment, it took a week to perform 70 iterations using Materna’s code and an Intel Xeon E5-2680 (2.7GHz) CPU. To reach 1,000 iterations, which are reported to be optimum, it would take three months. 1032 3. apply clustering to the initial frames based on the Chinese Restaurant Process (Aldous, 1985) to produce verb-specific semantic frames. These three steps are briefly described below. 3.2.1 Extracting Predicate-argument Structures from a Raw Corpus We apply dependency parsing to a large raw corpus. We use the Stanford parser with St</context>
<context position="13272" citStr="Yarowsky, 1993" startWordPosition="2107" endWordPosition="2108">is process, the verb and arguments are lemmatized, and only the head of an argument is preserved for compound nouns. Predicate-argument structures are collected for each verb and the subsequent processes are applied to the predicate-argument structures of each verb. 3.2.2 Constructing Initial Frames from Predicate-argument Structures To make the computation feasible, we merge the predicate-argument structures that have the same or similar meaning to get initial frames. These initial frames are the input of the subsequent clustering process. For this merge, we assume one sense per collocation (Yarowsky, 1993) for predicateargument structures. For each predicate-argument structure of a verb, we couple the verb and an argument to make a unit for sense disambiguation. We select an argument in the following order by considering the degree of effect on the verb sense:3 dobj, ccomp, nsubj, prep ∗, iobj. Then, the predicate-argument structures that have the same verb and argument pair (slot and word, e.g., “dobj:effect”) are merged into an initial frame. After this process, we discard minor initial frames that occur fewer than 10 times. 2http://nlp.stanford.edu/software/lex-parser.shtml 3If a predicate-a</context>
</contexts>
<marker>Yarowsky, 1993</marker>
<rawString>David Yarowsky. 1993. One sense per collocation. In Proceedings of the Workshop on Human Language Technology, pages 266–271.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>