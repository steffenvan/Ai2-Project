<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000018">
<title confidence="0.8067915">
A STOCHASTIC PROCESS FOR WORD FREQUENCY
DISTRIBUTIONS
</title>
<author confidence="0.648955">
Harald Baayen*
</author>
<affiliation confidence="0.420066">
Max-Planck-Institut fur Psycholinguistik
</affiliation>
<address confidence="0.366276">
Wundtlaan 1, NL-6525 XD Nijmegen
</address>
<email confidence="0.293673">
Internet: baayenOmpi.n1
</email>
<sectionHeader confidence="0.898435" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999564857142857">
A stochastic model based on insights of Man-
delbrot (1953) and Simon (1955) is discussed
against the background of new criteria of ade-
quacy that have become available recently as a
result of studies of the similarity relations be-
tween words as found in large computerized text
corpora.
</bodyText>
<sectionHeader confidence="0.998134" genericHeader="method">
FREQUENCY DISTRIBUTIONS
</sectionHeader>
<bodyText confidence="0.9970488">
Various models for word frequency distributions
have been developed since Zipf (1935) applied
the zeta distribution to describe a wide range of
lexical data. Mandelbrot (1953, 1962) extended
Zipf&apos;s distribution &apos;law&apos;
</bodyText>
<equation confidence="0.602324">
(1)
</equation>
<bodyText confidence="0.999926">
where A is the sample frequency of the ith type
in a ranking according to decreasing frequency,
with the parameter B,
</bodyText>
<equation confidence="0.876486">
=
.tetB+i4,(2)
</equation>
<bodyText confidence="0.984078272727273">
by means of which fits are obtained that are more
accurate with respect to the higher frequency
words. Simon (1955, 1960) developed a stochas-
tic process which has the Yule distribution
= AB(i, p 1), (3)
with the parameter A and B(i, p 1) the Beta
function in (i, p 1), as its stationary solutions.
For i —4 oo, (3) can be written as
r(p
in other words, (3) approximates Zipf&apos;s law with
respect to the lower frequency words, the tail of
</bodyText>
<note confidence="0.933186666666667">
*I am indebted to Klaas van Ham, Richard Gill, Bert
Hocks and Erik Schils for stimulating discussions on the
statistical analysis of lexical similarity relations.
</note>
<bodyText confidence="0.999882952380952">
the distribution. Other models, such as Good
(1953), Waring-Herdan (Herdan 1960, Muller
1979) and Sichel (1975), have been put forward,
all of which have Zipf&apos;s law as some special or
limiting form. Unrelated to Zipf&apos;s law is the
lognormal hypothesis, advanced for word fre-
quency distributions by Carroll (1967, 1969),
which gives rise to reasonable fits and is widely
used in psycholinguistic research on word fre-
quency effects in mental processing.
A problem that immediately arises in the con-
text of the study of word frequency distribu-
tions concerns the fact that these distributions
have two important characteristics which they
share with other so-called large number of rare
events (LNRE) distributions (Orlov and Chi-
tashvili 1983, Chitashvili and Khmaladze 1989),
namely that on the one hand a huge number of
different word types appears, and that on the
other hand it is observed that while some events
have reasonably stable frequencies, others occur
only once, twice, etc. Crucially, these rare events
occupy a significant portion of the list of all
types observed. The presence of such large num-
bers of very low frequency types effects a signifi-
cant bias between the rank-probability distribu-
tion and the rank-frequency distribution, lead-
ing to the contradiction of the common mean
of the law of large numbers, so that expressions
concerning frequencies cannot be taken to ap-
proximate expressions concerning probabilities.
The fact that for LNRE distributions the rank-
probability distributions cannot be reliably esti-
mated on the basis of rank-frequency distribu-
tions is one source of the lack of goodness-of-fit
often observed when various distribution &apos;laws&apos;
are applied to empirical data. Better results are
obtained with Zipfian models when Orlov and
Chitashvili&apos;s (1983) extended generalized Zipf&apos;s
law is used.
A second problem which arises when the ap-
propriateness of the various lexical models is
</bodyText>
<page confidence="0.994557">
271
</page>
<bodyText confidence="0.999865">
considered, the central issue of the present dis-
cussion, concerns the similarity relations among
words in lexical distributions. These empirical
similarity relations, as observed for large corpora
of words, impose additional criteria on the ad-
equacy of models for word frequency distribu-
tions.
</bodyText>
<sectionHeader confidence="0.996396" genericHeader="method">
SIMILARITY RELATIONS
</sectionHeader>
<bodyText confidence="0.976528980392157">
There is a growing consensus in psycholinguis-
tic research that word recognition depends not
only on properties of the target word (e.g. its
length and frequency), but also upon the number
and nature of its lexical competitors or neigh-
bors. The first to study similarity relations
among lexical competitors in the lexicon in re-
lation to lexical frequency were Landauer and
Streeter (1973). Let a neighbor be a word that
differs in exactly one phoneme (or letter) from
a given target string, and let the neighborhood
be the set of all neighbors, i.e. the set of all
words at Hamming distance 1 from the target.
Landauer and Streeter observed that (1) high-
frequency words have more neighbors than low-
frequency words (the neighborhood density ef-
fect), and that (2) high-frequency words have
higher-frequency neighbors than low-frequency
words (the neighborhood frequency effect). In
order to facilitate statistical analysis, it is con-
venient to restate the neighborhood frequency
effect as a correlation between the target&apos;s num-
ber of neighbors and the frequencies of these
neighbors, rather than as a relation between
the target&apos;s frequency and the frequencies of its
neighbors — targets with many neighbors having
higher frequency neighbors, and hence a higher
mean neighborhood frequency than targets
with few neighbors. In fact, both the neighbor-
hood density and the neighborhood frequency
effect are descriptions of a single property of
lexical space, namely that its dense similarity
regions are populated by the higher frequency
types. A crucial property of word frequency dis-
tributions is that the lexical similarity effects oc-
cur not only across but also within word lengths.
Figure 1A displays the rank-frequency distri-
bution of Dutch monomorpheznic phonologically
represented stems, function words excluded, and
charts the lexical similarity effects of the subset
of words with length 4 by means of boxplots.
These show the mean (dotted line), the median,
the upper and lower quartiles, the most extreme
data points within 1.5 times the interquartile
range, and remaining outliers for the number of
neighbors (#n) against target frequency (neigh-
borhood density), and for the mean frequency of
the neighbors of a target (A) against the num-
Table 1: Spearman rank correlation analysis of
the neighborhood density and frequency effects
for empirical and theoretical words of length 4.
</bodyText>
<table confidence="0.997731">
Dutch Mand. Mand.-Simon
dens. r. 0.24 0.65 0.31
r? 0.06 0.42 0.10
t 9.16 68.58 11.97
df 1340 6423 1348
freq. r. 0.51 0.62 0.61
II 0.26 0.38 0.37
t 21.65 63.02 28.22
df 1340 6423 1348
</table>
<bodyText confidence="0.999519733333333">
ber of neighbors of the target (neighborhood fre-
quency), for targets grouped into frequency and
density classes respectively. Observe that the
rank-frequency distribution of monomorphemic
Dutch words does not show up as a straight
line in a double logarithmic plot, that there is
a small neighborhood density effect and a some-
what more pronounced neighborhood frequency
effect. A Spearman rank correlation analysis
reveals that the lexical similarity effects of fig-
ure 1A are statistically highly significant trends
(p &lt; 0.001), even though the correlations them-
selves are quite weak (see table 1, column 1): in
the case of lexical density only 6% of the variance
is explained.1
</bodyText>
<sectionHeader confidence="0.858627" genericHeader="method">
STOCHASTIC MODELLING
</sectionHeader>
<bodyText confidence="0.999931714285714">
By themselves, models of the kind proposed
by Zipf, Herdan and Muller or Sichel, even
though they may yield reasonable fits to partic-
ular word frequency distributions, have no bear-
ing on the similarity relations in the lexicon.
The only model that is promising in this respect
is that of Mandelbrot (1953, 1962). Mandel-
brot derived his modification of Zipf&apos;s law (2)
on the basis of a Markovian model for generat-
ing words as strings of letters, in combination
with some assumptions concerning the cost of
transmitting the words generated in some op-
timal code, giving a precise interpretation to
Zipf&apos;s &apos;law of abbreviation&apos;. Miller (1957), wish-
ing to avoid a teleological explanation, showed
that the Zipf-Mandelbrot law can also be de-
rived under slightly different assumptions. Inter-
estingly, Nusbaum (1985), on the basis of sim-
ulation results with a slightly different neighbor
definition, reports that the neighborhood density
and neighborhood frequency effects occur within
</bodyText>
<footnote confidence="0.773823">
1Note that the larger value of r; for the neighborhood
frequency effect is a direct consequence of the fact that
the frequencies of the neighbors of each target are av-
eraged before they enter into the calculations, masking
much of the variance.
</footnote>
<page confidence="0.993573">
272
</page>
<figure confidence="0.96145104">
O•••
TT
1 2 3
In
• • 2000 -
1000 -
500-
100 -
50 -
1••••
10 -
FC 1 - DC
4 5 6 7 1 2 3 4 5 6
226 164 96 03 # items 311 360 291 179 70 14 # items
fs #71
104 20
103 - 16 -
102 - 12
101 8 -
10°
4 -
0 -
10° 101 102 103 103 226 293 276
A: Dutch monomorphemic stems in the CELEX database, standardized at 1,000,000. For the total
distribution, N = 224567, V = 4455. For strings of length 4, N = 64854, V = 1342.
</figure>
<bodyText confidence="0.9030706">
B: Simulated Dutch monomorphemic stems, as generated by a Markov process. For the total distribu-
tion, N = 224567, V = 58300. For strings of length 4, N = 74618, V = 6425.
C: Simulated Dutch monomorphemic stems, as generated by the Mandelbrot-Simon model (a = 0.01,
Vc = 2000). For the total distribution, N = 291944, V = 4848. For strings of length 4, N = 123317,
V = 1350.
</bodyText>
<figureCaption confidence="0.848716">
Figure 1: Rank-frequency and lexical similarity characteristics of the empirical and two simulated
</figureCaption>
<bodyText confidence="0.7584056">
distributions of Dutch phonological stems. From left to right: double logarithmic plot of rank i versus
frequency L, boxplot of frequency class FC (1:1;2:2-4;3:5-12;4:13-33;5:34-90;6:91-244;7:245+) versus
number of neighbors #n (length 4), and boxplot of density class DC ( 1:1-3;2:4--6;3:7-9;4:10-12;5:13-
15;6:16-19;7:20+) versus mean frequency of neighbors J, (length 4). (Note that not all axes are scaled
equally across the three distributions). N: number of tokens, V: number of types.
</bodyText>
<figure confidence="0.991866">
2
288916441002877267 119 37 # items 264 447
DC
iiii
3 4 5 6 7
466 621 664 2463001# iterns
Is
In
1000
500
100 -
50 -
10 -
FC 1
1
#n
55 -
11
1 2 3 4 5 6 7
0
104 -
103
102 -
101 -
10°
100 101 102 103 104 105
44 -
33 -
22 -
11 -
FC
I I I
1 2 3 4 5 6 7
373 263 208 188 130 oil 133 4s iterna
Ii
104
103 -
102 -
101
100
10 101 102 io3 104
1000
500
100 -
50 -
10 7
;
llll DC
1 2 3 4 5 6 7
192 210 223 233 207 177 63 # items
1
In
</figure>
<page confidence="0.995394">
273
</page>
<bodyText confidence="0.9996938">
a given word length when the transition proba-
bilities are not uniformly distributed. Unfortu-
nately, he leaves unexplained why these effects
occur, and to what extent his simulation is a
realistic model of lexical items as used in real
speech.
In order to come to a more precise understand-
ing of the source and nature of the lexical simi-
larity effects in natural language we studied two
stochastic models by means of computer simu-
lations. We first discuss the Markovian model
figuring in Mandelbrot&apos;s derivation of (2).
Consider a first-order Markov process. Let
A = {0, 1, , ic} be the set of phonemes of
the language, with 0 representing the terminat-
ing character space, and let P = EA with
Poo = 0. If X, is the letter in the nth position of
a string, we define P(X0 = i) = poj,i E A. Let
y be a finite string (io, • • • , ins—i) form E N
and define X(m) := (X0, XI, ..., X,n_i), then
</bodyText>
<equation confidence="0.5860435">
Py P(X(m) = y) = POloPioil • • •
(4)
</equation>
<bodyText confidence="0.999267">
The string types of varying length m, terminat-
ing with the space and without any intervening
space characters, constitute the words of the the-
oretical vocabulary
</bodyText>
<equation confidence="0.988222">
Sm := {(io, ii, • • • im-2, 0)
E A \ 0, j = 0, 1, , m — 2, m E N}.
</equation>
<bodyText confidence="0.9820745">
With Ny the token frequency of type y and
V the number of different types, the vec-
tor (N1, Ny2, , Ny,„) is multinomially dis-
tributed. Focussing on the neighborhood den-
sity effect, and defining the neighborhood of a
target string yt for fixed length m as
Ct := {I/ E Sm: 3!i E {0,1, ,m — 2}
such that yi
we have that the expected number of neighbors
of yt equals
</bodyText>
<equation confidence="0.7674095">
E[V (C)] = E {1 _ (1- p)n, (5)
vECe
</equation>
<bodyText confidence="0.999972985507247">
with N denoting the number of trials (i.e. the
number of tokens sampled). Note that when the
transition matrix P defines a uniform distribu-
tion (all p, equal), we immediately have that
the expected neighborhood density for length m1
is identical for all targets y, while for length
m2 &gt; m1 the expected density will be less than
that at length ml, since Pu(m3) &lt; plirnt) given
(4). With E[N] = Npy, we find that the neigh-
borhood density effect does occur across word
lengths, even though the transition probabilities
are uniformly distributed.
In order to obtain a realistic, non-trivial the-
oretical word distribution comparable with the
empirical data of figure 1A, the transition matrix
P was constructed such that it generated a sub-
set of phonotactically legal (possible) monomor-
phematic strings of Dutch by conditioning con-
sonant Ck in the string X,X3Ck on X) and the
segmental nature (C or V) of Xi, while vowels
were conditioned on the preceding segment only.
This procedure allowed us to differentiate be-
tween e.g. phonotactically legal word initial kn
and illegal word final kn sequences, at the same
time avoiding full conditioning on two preced-
ing segments, which, for four-letter words, would
come uncomfortably close to building the prob-
abilities of the individual words in the database
into the model.
The rank-frequency distribution of 58300
types and 224567 tokens (disregarding strings of
length 1) obtained by means of this (second or-
der) Markov process shows up in a double log-
arithmic plot as roughly linear (figure 1B). Al-
though the curve has the general Zipfian shape,
the deviations at head and tail are present by ne-
cessity in the light of Rouault (1978). A compar-
ison with figure 1A reveals that the large surplus
of very low frequency types is highly unsatisfac-
tory. The model (given the present transition
matrix) fails to replicate the high rate of use of
the relatively limited set of words of natural lan-
guage.
The lexical similarity effects as they emerge
for the simulated strings of length 4 are displayed
in the boxplots of figure 1B. A very pronounced
neighborhood density effect is found, in combi-
nation with a subdued neighborhood frequency
effect (see table 1, column 2).
The appearance of the neighborhood density
effect within a fixed string length in the Marko-
vian scheme with non-uniformly distributed Ai
can be readily understood in the simple case
of the first order Markov model outlined above.
Since neighbors are obtained by substitution of
a single element of the phoneme inventory A,
two consecutive transitional probabilities of (4)
have to be replaced. For increasing target prob-
ability he, the constituting transition probabil-
ities pia must increase, so that, especially for
non-trivial m, the neighbors y E Ci will gen-
erally be protected against low probabilities py.
Consequently, by (5), for fixed length m, higher
frequency words will have more neighbors than
lower frequency words for non-uniformly dis-
tributed transition probabilities.
The fact that the lexical similarity effects
emerge for target strings of the same length is
a strong point in favour of a Markovian source
</bodyText>
<page confidence="0.988847">
274
</page>
<bodyText confidence="0.999011666666667">
for word frequency distributions. Unfortunately,
comparing the results of figure 1B with those
of figure 1A, it appears that the effects are of
the wrong order of magnitude: the neighborhood
density effect is far too strong, the neighborhood
frequency effect somewhat too weak. The source
of this distortion can be traced to the extremely
large number of types generated (6425) for a
number of tokens (74618) for which the empirical
data (64854 tokens) allow only 1342 types. This
large surplus of types gives rise to an inflated
neighborhood density effect, with the concomi-
tant effect that neighborhood frequency is scaled
down. Rather than attempting to address this
issue by changing the transition matrix by using
a more constrained but less realistic data set,
another option is explored here, namely the idea
to supplement the Markovian stochastic process
with a second stochastic process developed by
Simon (1955), by means of which the intensive
use can be modelled to which the word types of
natural language are put.
Consider the frequency distribution of e.g. a
corpus that is being compiled, and assume that
at some stage of compilation N word tokens have
been observed. Let te) be the number of word
types that have occurred exactly r times in these
first N words. If we allow for the possibilities
that both new types can be sampled, and old
types can be re-used, Simon&apos;s model in its sim-
plest form is obtained under the three assump-
tions that (1) the probability that the (N +1)-st
word is a type that has appeared exactly r times
is proportional to rn41), the summed token fre-
quencies of all types with token frequency r at
stage N, that (2) there is a constant probability
a that the (N+1)-st word represents a new type,
and that (3) all frequencies grow proportionaly
with N, so that
</bodyText>
<equation confidence="0.9767748">
n(N-1-1.) N+1
(N)
for all r,N.
N
TIT
</equation>
<bodyText confidence="0.998644671875">
Simon (1955) shows that the Yule-distribution
(3) follows from these assumptions. When the
third assumption is replaced by the assumptions
that word types are dropped with a probabil-
ity proportional to their token frequency, and
that old words are dropped at the same rate at
which new word types are introduced so that
the total number of tokens in the distribution is
a constant, the Yule-distribution is again found
to follow (Simon 1960).
By itself, this stochastic process has no ex-
planatory value with respect to the similarity
relations between words. It specifies use and re-
use of word types, without any reference to seg-
mental constituency or length. However, when a
Markovian process is fitted as a front end to Si-
mon&apos;s stochastic process, a hybrid model results
that has the desired properties, since the latter
process can be used to force the required high
intensity of use on the types of its input distri-
bution. The Markovian front end of the model
can be thought of as defining a probability dis-
tribution that reflects the ease with which words
can be pronounced by the human vocal tract,
an implementation of phonotaxis. The second
component of the model can be viewed as simu-
lating interfering factors pertaining to language
use. Extralinguistic factors codetermine the ex-
tent to which words are put to use, indepen-
dently of the slot occupied by these words in the
network of similarity relations,2 and may effect
a substantial reduction of the lexical similarity
effects.
Qualitatively satisfying results were obtained
with this `Mandelbrot-Simon&apos; stochastic model,
using the transition matrix of figure 1B for the
Markovian front end and fixing Simon&apos;s birth
rate a at 0.01.3 An additional parameter, Vc,
the critical number of types for which the switch
from the front end to what we will refer to as
the component of use is made, was fixed at 2000.
Figure 1C shows that both the general shape of
the rank-frequency curve in a double logarith-
mic grid, as well as the lexical similarity effects
(table 1, column 3) are highly similar to the em-
pirical observations (figure 1A). Moreover, the
overall number of types (4848) and the number
of types of length 4 (1350) closely approximate
the empirical numbers of types (4455 and 1342
respectively), and the same holds for the overall
numbers of tokens (291944 and 224567) respec-
tively. Only the number of tokens of length 4
is overestimated by a factor 2. Nevertheless, the
type-token ratio is far more balanced than in the
original Markovian scheme. Given that the tran-
sition matrix models only part of the phonotaxis
of Dutch, a perfect match between the theoret-
ical and empirical distributions is not to be ex-
pected.
The present results were obtained by imple-
menting Simon&apos;s stochastic model in a slightly
modified form, however. Simon&apos;s derivation of
the Yule-distribution builds on the assumption
that each r grows proportionaly with N, an as-
</bodyText>
<footnote confidence="0.918828555555556">
2For instance, the Dutch word kuip, &apos;barrel&apos;, is a low-
frequency type in the present-day language, due to the
fact that its denotatum has almost completely dropped
out of use. Nevertheless, it was a high-frequency word
in earlier centuries, to which the high frequency of the
surname kuiper bears witness.
3The new types entering the distribution at rate a
were generated by means of the transition matrix of figure
1B.
</footnote>
<page confidence="0.997523">
275
</page>
<bodyText confidence="0.999152545454546">
sumption that does not lend itself to implemen-
tation in a stochastic process. Without this as-
sumption, rank-frequency distributions are gen-
erated that depart significantly from the empir-
ical rank-frequency curve, the highest frequency
words attracting a very large proportion of all
tokens. By replacing Simon&apos;s assumptions 1 and
3 by the &apos;rule of usage&apos; that
the probability that the (N+1)-st word
is a type that has appeared exactly r
times is proportional to
</bodyText>
<equation confidence="0.852026333333333">
H,.:= rn,. rn,
log , (6)
Er rn„. Er rn,.
</equation>
<bodyText confidence="0.9406425">
theoretical rank-frequency distributions of the
desired form can be obtained. Writing
</bodyText>
<equation confidence="0.916607">
p(r) rn,
</equation>
<bodyText confidence="0.998674666666667">
for the probability of re-using any type that has
been used r times before, H,. can be interpreted
as the contribution of all types with frequency
r to the total entropy H of the distribution of
ranks r, i.e. to the average amount of informa-
tion
</bodyText>
<equation confidence="0.993737">
H = E —p(r) log p(r).
</equation>
<bodyText confidence="0.999877036363636">
Selection of ranks according to (6) rather than
proportional to rn,. (Simon&apos;s assumption 1) en-
sures that the highest ranks r have lowered prob-
abilities of being sampled, at the same time
slightly raising the probabilities of the inter-
mediate ranks r. For instance, the 58 highest
ranks of the distribution of figure 1C have some-
what raised, the complementary 212 ranks some-
what lowered probability of being sampled. The
advantage of using (6) is that unnatural rank-
frequency distributions in which a small number
of types assume exceedingly high token frequen-
cies are avoided.
The proposed rule of usage can be viewed as a
means to obtain a better trade-off in the distri-
bution between maximalization of information
transmission and optimalization of the cost of
coding the information. To see this, consider
an individual word type y. In order to mini-
malize the cost of coding C(y) = — log(Pr(y)),
high-frequency words should be re-used. Unfor-
tunately, these high-frequency words have the
lowest information content. However, it can be
shown that maximalization of information trans-
mission requires the re-use of the lowest fre-
quency types (H,. is maximal for uniformly dis-
tributed p(r)). Thus we have two opposing re-
quirements, which balance out in favor of a more
intensive use of the lower and intermediate fre-
quency ranges when selection of ranks is propor-
tional to (6).
The &apos;rule of usage&apos; (6) implies that higher
frequency words contribute less to the average
amount of information than might be expected
on the basis of their relative sample frequen-
cies. Interestingly, there is independent evidence
for this prediction. It is well known that the
higher-frequency types have more (shades of)
meaning(s) than lower-frequency words (see e.g.
Reder, Anderson and Bjork 1974, Paivio, Yuille
and Madigan 1968). A larger number of mean-
ings is correlated with increased contextual de-
pendency for interpretation. Hence the amount
of information contributed by such types out of
context (under conditions of statistical indepen-
dence) is less than what their relative sample
frequencies suggest, exactly as modelled by our
rule of usage.
Note that this semantic motivation for se-
lection proportional to H,. makes it possible
to avoid invoking external principles such as
&apos;least effort&apos; or &apos;optimal coding&apos; in the mathe-
matical definition of the model, principles that
have been criticized as straining one&apos;s credulity
(Miller 1957).4
</bodyText>
<sectionHeader confidence="0.967469" genericHeader="method">
FUNCTION WORDS
</sectionHeader>
<bodyText confidence="0.953461214285714">
Up till now, we have focused on the modelling
of monomorphemic Dutch words, to the exclu-
sion of function words and morphologically com-
plex words. One of the reasons for this ap-
proach concerns the way in which the shape of
the rank-frequency curves differs substantially
depending on which kinds of words are included
in the distribution. As shown in figure 2, the
curve of monomorphemic words without func-
tion words is highly convex. When function
words are added, the head of the tail is straight-
ened out, while the addition of complex words
brings the tail of the distribution (more or less)
in line with Zipf&apos;s law. Depending on what kind
of distribution is being modelled, different crite-
ria of adequacy have to be met.
Interestingly, function words, — articles, pro-
nouns, conjunctions and prepositions, the so-
called closed classes, among which we have also
reckoned the auxiliary verbs — typically show up
as the shortest and most frequent (Zipf) words in
frequency distributions. In fact, they are found
with raised frequencies in the the empirical rank-
frequency distribution when compared with the
curve of content words only, as shown in the first
4In this respect, Miller&apos;s (1957) alternative derivation
of (2) in terms of random spacing is unconvincing in the
light of the phonotactic constraints on word structure.
</bodyText>
<page confidence="0.992979">
276
</page>
<figure confidence="0.794231">
100 101 102 103 104 105 100 101 102 103 104 103 100 101 io 103 10*
</figure>
<figureCaption confidence="0.9898945">
Figure 2: Rank-frequency plots for Dutch phonological stems. From left to right: monomorphemic
words without function words, monomorphemic words and function words, complete distribution.
</figureCaption>
<bodyText confidence="0.99952571875">
two graphs of figure 2. Miller, Newman &amp; Fried-
man (1958), discussing the finding that the fre-
quential characteristics of function words differ
markedly from those of content words, argued
that (1958:385)
Inasmuch as the division into two
classes of words was independent of the
frequencies of the words, we might have
expected it to simply divide the sam-
ple in half, each half retaining the sta-
tistical properties of the whole. Since
this is clearly not the case, it is ob-
vious that Mandelbrot&apos;s approach is
incomplete. The general trends for
all words combined seem to follow a
stochastic pattern, but when we look
at syntactic patterns, differences begin
to appear which will require linguistic,
rather than mere statistical, explana-
tions.
In the Mandelbrot-Simon model developed here,
neither the Markovian front end nor the pro-
posed rule of usage are able to model the ex-
tremely high intensity of use of these function
words correctly without unwished-for side effects
on the distribution of content words. However,
given that the semantics of function words are
not subject to the loss of specificity that char-
acterizes high-frequency content words, function
words are not subject to selection proportional
to H,.. Instead, some form of selection propor-
tional to rn,. probably is more appropriate here.
</bodyText>
<sectionHeader confidence="0.981081" genericHeader="method">
MORPHOLOGY
</sectionHeader>
<bodyText confidence="0.992258205128205">
The Mandelbrot-Simon model has a single pa-
rameter a that allows new words to enter the dis-
tribution. Since the present theory is of a phono-
logical rather than a morphological nature, this
parameter models the (occasional) appearance
of new simplex words in the language only, and
cannot be used to model the influx of morpho-
logically complex words.
First, morphological word formation processes
may give rise to consonant clusters that are per-
mitted when they span morpheme boundaries,
but that are inadmissible within single mor-
phemes. This difference in phonotactic pattern-
ing within and across morphemes already re-
veales that morphologically complex words have
a different source than monomorphemic words.
Second, each word formation process, whether
compounding or affixation of suffixes like -mess
and -ity, is characterized by its own degree of
productivity. Quantitatively, differences in the
degree of productivity amount to differences in
the birth rates at which complex words appear
in the vocabulary. Typically, such birth rates,
which can be expressed as N„ where n&apos;1 and
N&apos; denote the number of types occurring once
only and the number of tokens of the frequency
distributions of the corresponding morphologi-
cal categories (Baayen 1989), assume values that
are significantly higher that the birth rate a of
monomorphemic words. Hence it is impossible
to model the complete lexical distribution with-
out a worked-out morphological component that
specifies the word formation processes of the lan-
guage and their degrees of productivity.
While actual modelling of the complete distri-
bution is beyond the scope of the present paper,
we may note that the addition of birth rates for
word formation processes to the model, neces-
sitated by the additional large numbers of rare
</bodyText>
<page confidence="0.985617">
277
</page>
<bodyText confidence="0.999783">
words that appear in the complete distribution,
ties in nicely with the fact that the frequency
distributions of productive morphological cate-
gories are prototypical LNRE distributions, for
which the large values for the numbers of types
occurring once or twice only are characteristic.
With respect to the effect of morphological
structure on the lexical similarity effects, we fi-
nally note that in the empirical data the longer
word lengths show up with sharply diminished
neighborhood density. However, it appears that
those longer words which do have neighbors are
morphologically complex. Morphological struc-
ture raises lexical density where the phonotaxis
fails to do so: for long monomorphemic words
the huge space of possible word types is sampled
too sparcely for the lexical similarity effects to
emerge.
</bodyText>
<sectionHeader confidence="0.983279" genericHeader="method">
RtFERENCES
</sectionHeader>
<reference confidence="0.9997127625">
Baayen, R.H. 1989. A Corpus-Based Approach
to Morphological Productivity. Statistical Anal-
ysis and Psycholinguistic Interpretation. Diss.
Vrije Universiteit, Amsterdam.
Carroll, J.B. 1967. On Sampling from a Log-
normal Model of Word Frequency Distribution.
In: H.Kuaera W.N.Francis 1967, 406-424.
Carroll, J.B. 1969. A Rationale for an Asymp-
totic Lognormal Form of Word Frequency Distri-
butions. Research Bulletin - Educational Test-
ing Service, Princeton, November 1969.
Chitaivili, R.I. &amp; Khmaladze, E.V. 1989. Sta-
tistical Analysis of Large Number of Rare Events
and Related Problems. Transactions of the Tbil-
isi Mathematical Institute.
Good, 11 . 1953. The population frequencies of
species and the estimation of population param-
eters, Biometrika 43, 45-63.
Herdan, G. 1960. Type-token Mathematics,
The Hague, Mouton.
Kaera, H. &amp; Francis, W.N. 1967. Compu-
tational Analysis of Present-Day American En-
glish. Providence: Brown University Press.
Landauer, T.K. &amp; Streeter, L.A. 1973. Struc-
tural differences between common and rare
words: failure of equivalence assumptions for
theories of word recognition, Journal of Verbal
Learning and Verbal Behavior 12, 119-131.
Mandelbrot, B. 1953. An informational the-
ory of the statistical structure of language, in:
W.Jackson (ed.), Communication Theory, But-
terworths.
Mandelbrot, B. 1962. On the theory of word
frequencies and on related Markovian models
of discourse, in: R.Jakobson, Structure of Lan-
guage and its Mathematical Aspects. Proceedings
of Symposia in Applied Mathematics Vol XII,
Providence, Rhode Island, Americal Mathemat-
ical Society, 190-219.
Miller, G.A. 1954. Communication, Annual
Review of Psychology 5, 401-420.
Miller, G.A. 1957. Some effects of intermittent
silence, The American Journal of Psychology 52,
311-314.
Miller, G.A., Newman, E.B. &amp; Friedman, E.A.
1958. Length-Frequency Statistics for Written
English, Information and control 1, 370-389.
Muller, Ch. 1979. Du nouveau sur les distri-
butions lexicales: la formule de Waring-Herdan.
In: Ch. Muller, Langue Frangaise et Linguis-
tique Quantitative. Geneve: Slatkine, 177-195.
Nusbaum, H.C. 1985. A stochastic account
of the relationship between lexical density and
word frequency, Research on Speech Perception
Report # 11, Indiana University.
Orlov, J.K. &amp; Chitashvili, R.Y. 1983. Gener-
alized Z-distribution generating the well-known
&apos;rank-distributions&apos;, Bulletin of the Academy of
Sciences, Georgia 110.2, 269-272.
Paivio, A., Yuille, J.C. &amp; Madigan, S. 1968.
Concreteness, Imagery and Meaningfulness Val-
ues for 925 Nouns. Journal of Experimental Psy-
chology Monograph 76, I, Pt. 2.
Reder, L.M., Anderson, J.R. &amp; Bjork, R.A.
1974. A Semantic Interpretation of Encoding
Specificity. Journal of Experimental Psychology
102: 648-656.
Rouault, A. 1978. Loi de Zipf et sources
markoviennes, Ann. Inst. H.Poincare 14, 169-
188.
Sichel, H.S. 1975. On a Distribution Law for
Word Frequencies. Journal of the American Sta-
tistical Association 70, 542-547.
Simon, H.A. 1955. On a class of skew distri-
bution functions, Biometrika 42, 435-440.
Simon, H.A. 1960. Some further notes on a
class of skew distribution functions, Information
and Control 3, 80-88.
Zipf, G.K. 1935. The Psycho-Biology of Lan-
guage, Boston, Houghton Mifflin.
</reference>
<page confidence="0.996934">
278
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998231">A STOCHASTIC PROCESS FOR WORD FREQUENCY DISTRIBUTIONS</title>
<author confidence="0.992098">Harald Baayen</author>
<affiliation confidence="0.769185">fur Psycholinguistik</affiliation>
<address confidence="0.8608365">Wundtlaan 1, NL-6525 XD Nijmegen Internet: baayenOmpi.n1</address>
<abstract confidence="0.977821882051282">A stochastic model based on insights of Mandelbrot (1953) and Simon (1955) is discussed against the background of new criteria of adequacy that have become available recently as a result of studies of the similarity relations between words as found in large computerized text corpora. FREQUENCY DISTRIBUTIONS Various models for word frequency distributions have been developed since Zipf (1935) applied the zeta distribution to describe a wide range of lexical data. Mandelbrot (1953, 1962) extended Zipf&apos;s distribution &apos;law&apos; the sample frequency of the type in a ranking according to decreasing frequency, the parameter = by means of which fits are obtained that are more accurate with respect to the higher frequency words. Simon (1955, 1960) developed a stochastic process which has the Yule distribution AB(i, p 1), the parameter A and p the Beta in p 1), its stationary solutions. For i —4 oo, (3) can be written as r(p in other words, (3) approximates Zipf&apos;s law with respect to the lower frequency words, the tail of *I am indebted to Klaas van Ham, Richard Gill, Bert Hocks and Erik Schils for stimulating discussions on the statistical analysis of lexical similarity relations. distribution. Other models, such (1953), Waring-Herdan (Herdan 1960, Muller 1979) and Sichel (1975), have been put forward, all of which have Zipf&apos;s law as some special or limiting form. Unrelated to Zipf&apos;s law is the lognormal hypothesis, advanced for word frequency distributions by Carroll (1967, 1969), which gives rise to reasonable fits and is widely used in psycholinguistic research on word frequency effects in mental processing. A problem that immediately arises in the context of the study of word frequency distributions concerns the fact that these distributions have two important characteristics which they share with other so-called large number of rare events (LNRE) distributions (Orlov and Chitashvili 1983, Chitashvili and Khmaladze 1989), namely that on the one hand a huge number of different word types appears, and that on the other hand it is observed that while some events have reasonably stable frequencies, others occur only once, twice, etc. Crucially, these rare events occupy a significant portion of the list of all types observed. The presence of such large numbers of very low frequency types effects a significant bias between the rank-probability distribution and the rank-frequency distribution, leading to the contradiction of the common mean of the law of large numbers, so that expressions concerning frequencies cannot be taken to approximate expressions concerning probabilities. The fact that for LNRE distributions the rankprobability distributions cannot be reliably estimated on the basis of rank-frequency distributions is one source of the lack of goodness-of-fit often observed when various distribution &apos;laws&apos; are applied to empirical data. Better results are obtained with Zipfian models when Orlov and Chitashvili&apos;s (1983) extended generalized Zipf&apos;s law is used. A second problem which arises when the appropriateness of the various lexical models is 271 considered, the central issue of the present discussion, concerns the similarity relations among words in lexical distributions. These empirical similarity relations, as observed for large corpora of words, impose additional criteria on the adequacy of models for word frequency distributions. SIMILARITY RELATIONS There is a growing consensus in psycholinguistic research that word recognition depends not only on properties of the target word (e.g. its length and frequency), but also upon the number and nature of its lexical competitors or neighbors. The first to study similarity relations among lexical competitors in the lexicon in relation to lexical frequency were Landauer and (1973). Let a a word that differs in exactly one phoneme (or letter) from a given target string, and let the neighborhood be the set of all neighbors, i.e. the set of all words at Hamming distance 1 from the target. Landauer and Streeter observed that (1) highfrequency words have more neighbors than lowfrequency words (the neighborhood density effect), and that (2) high-frequency words have higher-frequency neighbors than low-frequency words (the neighborhood frequency effect). In order to facilitate statistical analysis, it is convenient to restate the neighborhood frequency effect as a correlation between the target&apos;s number of neighbors and the frequencies of these neighbors, rather than as a relation between the target&apos;s frequency and the frequencies of its neighbors — targets with many neighbors having higher frequency neighbors, and hence a higher mean neighborhood frequency than targets with few neighbors. In fact, both the neighborhood density and the neighborhood frequency effect are descriptions of a single property of lexical space, namely that its dense similarity regions are populated by the higher frequency types. A crucial property of word frequency distributions is that the lexical similarity effects occur not only across but also within word lengths. Figure 1A displays the rank-frequency distribution of Dutch monomorpheznic phonologically represented stems, function words excluded, and charts the lexical similarity effects of the subset of words with length 4 by means of boxplots. These show the mean (dotted line), the median, the upper and lower quartiles, the most extreme data points within 1.5 times the interquartile range, and remaining outliers for the number of neighbors (#n) against target frequency (neighborhood density), and for the mean frequency of neighbors of a target the num- Table 1: Spearman rank correlation analysis of the neighborhood density and frequency effects for empirical and theoretical words of length 4. Dutch Mand. Mand.-Simon dens. r. 0.24 0.65 0.31 r? 0.06 0.42 0.10 t 9.16 68.58 11.97 df 1340 6423 1348 freq. r. 0.51 0.62 0.61 II 0.26 0.38 0.37 t 21.65 63.02 28.22 df 1340 6423 1348 ber of neighbors of the target (neighborhood frequency), for targets grouped into frequency and density classes respectively. Observe that the rank-frequency distribution of monomorphemic Dutch words does not show up as a straight line in a double logarithmic plot, that there is a small neighborhood density effect and a somewhat more pronounced neighborhood frequency effect. A Spearman rank correlation analysis reveals that the lexical similarity effects of figure 1A are statistically highly significant trends (p &lt; 0.001), even though the correlations themare quite weak (see table 1, column the case of lexical density only 6% of the variance STOCHASTIC MODELLING By themselves, models of the kind proposed by Zipf, Herdan and Muller or Sichel, even though they may yield reasonable fits to particular word frequency distributions, have no bearing on the similarity relations in the lexicon. The only model that is promising in this respect is that of Mandelbrot (1953, 1962). Mandelbrot derived his modification of Zipf&apos;s law (2) on the basis of a Markovian model for generating words as strings of letters, in combination with some assumptions concerning the cost of transmitting the words generated in some optimal code, giving a precise interpretation to Zipf&apos;s &apos;law of abbreviation&apos;. Miller (1957), wishing to avoid a teleological explanation, showed that the Zipf-Mandelbrot law can also be derived under slightly different assumptions. Interestingly, Nusbaum (1985), on the basis of simulation results with a slightly different neighbor definition, reports that the neighborhood density and neighborhood frequency effects occur within that the larger value of r; for the neighborhood frequency effect is a direct consequence of the fact that frequencies of the neighbors of each target are avthey enter into the calculations, masking much of the variance. 272 O••• TT 1 2 3 • • 2000 - 1000 - 500- 100 - 50 - 1•••• FC 1 - DC</abstract>
<phone confidence="0.772158666666667">4 5 6 7 1 2 3 4 5 6 164 96 03 360 291 179 70 14 fs #71 20</phone>
<note confidence="0.8218078">16 - - 12 10° 8 - 4 - 0 -</note>
<phone confidence="0.839132">103 293 276</phone>
<abstract confidence="0.927030882352941">A: Dutch monomorphemic stems in the CELEX database, standardized at 1,000,000. For the total = = For strings length = = B: Simulated Dutch monomorphemic stems, as generated by a Markov process. For the total distribu- = = For of length 4, = = C: Simulated Dutch monomorphemic stems, as generated by the Mandelbrot-Simon model (a = 0.01, 2000). For the total distribution, = = For strings of length 4, = = Figure 1: Rank-frequency and lexical similarity characteristics of the empirical and two simulated distributions of Dutch phonological stems. From left to right: double logarithmic plot of rank i versus of frequency class FC (1:1;2:2-4;3:5-12;4:13-33;5:34-90;6:91-244;7:245+) versus number of neighbors #n (length 4), and boxplot of density class DC ( 1:1-3;2:4--6;3:7-9;4:10-12;5:13versus mean frequency of neighbors J,(length 4). (Note that not all axes are scaled across the three distributions). of tokens, of types. 2 119 37 # 447 DC iiii</abstract>
<phone confidence="0.822734">3 4 5 6 7 466 621 664</phone>
<affiliation confidence="0.5797365">Is In</affiliation>
<address confidence="0.8347925">1000 500</address>
<abstract confidence="0.404077631578947">100 - 50 - 10 - 1 1 55 - 11 1 2 3 4 5 6 7 0 - - - 10° 44 - 33 - 22 - 11 - FC I I I</abstract>
<phone confidence="0.76879">1 2 3 4 5 6 7 263 208 188 130</phone>
<affiliation confidence="0.731912">Ii</affiliation>
<address confidence="0.769384">104</address>
<email confidence="0.52722">-</email>
<affiliation confidence="0.857668"></affiliation>
<address confidence="0.8015695">1000 500 100 - 50 -</address>
<phone confidence="0.806656">10 7</phone>
<email confidence="0.623122">;</email>
<phone confidence="0.710686">1 2 3 4 5 6 7 210 223 233 207 177 63</phone>
<abstract confidence="0.997944266821347">1 273 a given word length when the transition probabilities are not uniformly distributed. Unfortunately, he leaves unexplained why these effects occur, and to what extent his simulation is a realistic model of lexical items as used in real speech. In order to come to a more precise understanding of the source and nature of the lexical similarity effects in natural language we studied two stochastic models by means of computer simulations. We first discuss the Markovian model figuring in Mandelbrot&apos;s derivation of (2). Consider a first-order Markov process. Let = 1, , ic} be the set of phonemes of language, with 0 representing the terminatcharacter space, and let P = with = 0. If the letter in the position of string, we define i) = poj,i Let be a finite string (io, • • • , N define ..., then y) = POloPioil • • • (4) The string types of varying length m, terminating with the space and without any intervening space characters, constitute the words of the theoretical vocabulary := {(io, ii, • • • \ 0, j = 0, 1, , m — m E N}. token frequency of type y and number of different types, the vec- , is multinomially tributed. Focussing on the neighborhood density effect, and defining the neighborhood of a target string yt for fixed length m as := {I/ E {0,1, ,m — that we have that the expected number of neighbors equals (C)] = _ (1p)n, vECe the number of trials (i.e. the number of tokens sampled). Note that when the transition matrix P defines a uniform distribu- (all immediately have that the expected neighborhood density for length m1 identical for all for length &gt; the expected density will be less than at length ml, since given With E[N] = find that the neighborhood density effect does occur across word lengths, even though the transition probabilities are uniformly distributed. In order to obtain a realistic, non-trivial theoretical word distribution comparable with the empirical data of figure 1A, the transition matrix P was constructed such that it generated a subset of phonotactically legal (possible) monomorphematic strings of Dutch by conditioning conthe string and the nature (C or V) of vowels were conditioned on the preceding segment only. This procedure allowed us to differentiate bee.g. phonotactically legal word initial illegal word final at the same time avoiding full conditioning on two preceding segments, which, for four-letter words, would come uncomfortably close to building the probabilities of the individual words in the database into the model. The rank-frequency distribution of 58300 types and 224567 tokens (disregarding strings of length 1) obtained by means of this (second order) Markov process shows up in a double logarithmic plot as roughly linear (figure 1B). Although the curve has the general Zipfian shape, the deviations at head and tail are present by necessity in the light of Rouault (1978). A comparison with figure 1A reveals that the large surplus of very low frequency types is highly unsatisfactory. The model (given the present transition matrix) fails to replicate the high rate of use of the relatively limited set of words of natural language. The lexical similarity effects as they emerge for the simulated strings of length 4 are displayed in the boxplots of figure 1B. A very pronounced neighborhood density effect is found, in combination with a subdued neighborhood frequency 1, column 2). The appearance of the neighborhood density effect within a fixed string length in the Markovian scheme with non-uniformly distributed Ai can be readily understood in the simple case of the first order Markov model outlined above. Since neighbors are obtained by substitution of a single element of the phoneme inventory A, two consecutive transitional probabilities of (4) have to be replaced. For increasing target probability he, the constituting transition probabilmust increase, so that, especially for m, the neighbors Ci genbe protected against low probabilities Consequently, by (5), for fixed length m, higher frequency words will have more neighbors than lower frequency words for non-uniformly distributed transition probabilities. The fact that the lexical similarity effects emerge for target strings of the same length is a strong point in favour of a Markovian source 274 for word frequency distributions. Unfortunately, comparing the results of figure 1B with those of figure 1A, it appears that the effects are of the wrong order of magnitude: the neighborhood density effect is far too strong, the neighborhood frequency effect somewhat too weak. The source of this distortion can be traced to the extremely large number of types generated (6425) for a number of tokens (74618) for which the empirical data (64854 tokens) allow only 1342 types. This large surplus of types gives rise to an inflated neighborhood density effect, with the concomitant effect that neighborhood frequency is scaled down. Rather than attempting to address this issue by changing the transition matrix by using a more constrained but less realistic data set, another option is explored here, namely the idea to supplement the Markovian stochastic process with a second stochastic process developed by Simon (1955), by means of which the intensive use can be modelled to which the word types of natural language are put. Consider the frequency distribution of e.g. a corpus that is being compiled, and assume that some stage of compilation tokens have observed. Let the number of word types that have occurred exactly r times in these If we allow for the possibilities that both new types can be sampled, and old types can be re-used, Simon&apos;s model in its simplest form is obtained under the three assumpthat (1) the probability that the +1)-st word is a type that has appeared exactly r times proportional to the summed token frequencies of all types with token frequency r at (2) there is a constant probability that the represents a new type, and that (3) all frequencies grow proportionaly that (N) all N TIT Simon (1955) shows that the Yule-distribution (3) follows from these assumptions. When the third assumption is replaced by the assumptions that word types are dropped with a probability proportional to their token frequency, and that old words are dropped at the same rate at which new word types are introduced so that the total number of tokens in the distribution is a constant, the Yule-distribution is again found to follow (Simon 1960). By itself, this stochastic process has no explanatory value with respect to the similarity relations between words. It specifies use and reword types, without any reference to segor length. However, when a Markovian process is fitted as a front end to Simon&apos;s stochastic process, a hybrid model results that has the desired properties, since the latter process can be used to force the required high intensity of use on the types of its input distribution. The Markovian front end of the model can be thought of as defining a probability distribution that reflects the ease with which words can be pronounced by the human vocal tract, an implementation of phonotaxis. The second component of the model can be viewed as simulating interfering factors pertaining to language use. Extralinguistic factors codetermine the extent to which words are put to use, independently of the slot occupied by these words in the of similarity and may effect a substantial reduction of the lexical similarity effects. Qualitatively satisfying results were obtained with this `Mandelbrot-Simon&apos; stochastic model, using the transition matrix of figure 1B for the Markovian front end and fixing Simon&apos;s birth a at An additional parameter, Vc, the critical number of types for which the switch from the front end to what we will refer to as the component of use is made, was fixed at 2000. Figure 1C shows that both the general shape of the rank-frequency curve in a double logarithmic grid, as well as the lexical similarity effects (table 1, column 3) are highly similar to the empirical observations (figure 1A). Moreover, the overall number of types (4848) and the number of types of length 4 (1350) closely approximate the empirical numbers of types (4455 and 1342 respectively), and the same holds for the overall numbers of tokens (291944 and 224567) respectively. Only the number of tokens of length 4 is overestimated by a factor 2. Nevertheless, the type-token ratio is far more balanced than in the original Markovian scheme. Given that the transition matrix models only part of the phonotaxis of Dutch, a perfect match between the theoretical and empirical distributions is not to be expected. The present results were obtained by implementing Simon&apos;s stochastic model in a slightly form, however. Simon&apos;s derivation the Yule-distribution builds on the assumption each r grows proportionaly with asinstance, the Dutch word is a lowfrequency type in the present-day language, due to the fact that its denotatum has almost completely dropped out of use. Nevertheless, it was a high-frequency word in earlier centuries, to which the high frequency of the surname kuiper bears witness. new types entering the distribution at were generated by means of the transition matrix of figure 1B. 275 sumption that does not lend itself to implementation in a stochastic process. Without this assumption, rank-frequency distributions are generated that depart significantly from the empirical rank-frequency curve, the highest frequency words attracting a very large proportion of all tokens. By replacing Simon&apos;s assumptions 1 and 3 by the &apos;rule of usage&apos; that the probability that the (N+1)-st word is a type that has appeared exactly r times is proportional to rn,. log , (6) rn„. rn,. theoretical rank-frequency distributions of the desired form can be obtained. Writing p(r)rn, for the probability of re-using any type that has used r times before, be interpreted as the contribution of all types with frequency to the total entropy the distribution of ranks r, i.e. to the average amount of information = log p(r). Selection of ranks according to (6) rather than proportional to rn,. (Simon&apos;s assumption 1) ensures that the highest ranks r have lowered probabilities of being sampled, at the same time slightly raising the probabilities of the intermediate ranks r. For instance, the 58 highest ranks of the distribution of figure 1C have somewhat raised, the complementary 212 ranks somewhat lowered probability of being sampled. The advantage of using (6) is that unnatural rankfrequency distributions in which a small number of types assume exceedingly high token frequencies are avoided. The proposed rule of usage can be viewed as a means to obtain a better trade-off in the distribution between maximalization of information transmission and optimalization of the cost of coding the information. To see this, consider an individual word type y. In order to minimalize the cost of coding C(y) = — log(Pr(y)), high-frequency words should be re-used. Unfortunately, these high-frequency words have the lowest information content. However, it can be shown that maximalization of information transmission requires the re-use of the lowest fretypes maximal for uniformly distributed p(r)). Thus we have two opposing requirements, which balance out in favor of a more intensive use of the lower and intermediate frequency ranges when selection of ranks is proportional to (6). The &apos;rule of usage&apos; (6) implies that higher frequency words contribute less to the average amount of information than might be expected on the basis of their relative sample frequencies. Interestingly, there is independent evidence for this prediction. It is well known that the higher-frequency types have more (shades of) meaning(s) than lower-frequency words (see e.g. Reder, Anderson and Bjork 1974, Paivio, Yuille and Madigan 1968). A larger number of meanings is correlated with increased contextual dependency for interpretation. Hence the amount of information contributed by such types out of context (under conditions of statistical independence) is less than what their relative sample frequencies suggest, exactly as modelled by our rule of usage. Note that this semantic motivation for seproportional to it possible to avoid invoking external principles such as &apos;least effort&apos; or &apos;optimal coding&apos; in the mathematical definition of the model, principles that been criticized one&apos;s credulity FUNCTION WORDS Up till now, we have focused on the modelling of monomorphemic Dutch words, to the exclusion of function words and morphologically complex words. One of the reasons for this approach concerns the way in which the shape of the rank-frequency curves differs substantially depending on which kinds of words are included in the distribution. As shown in figure 2, the curve of monomorphemic words without function words is highly convex. When function words are added, the head of the tail is straightened out, while the addition of complex words brings the tail of the distribution (more or less) in line with Zipf&apos;s law. Depending on what kind of distribution is being modelled, different criteria of adequacy have to be met. Interestingly, function words, — articles, pronouns, conjunctions and prepositions, the socalled closed classes, among which we have also reckoned the auxiliary verbs — typically show up as the shortest and most frequent (Zipf) words in frequency distributions. In fact, they are found with raised frequencies in the the empirical rankfrequency distribution when compared with the curve of content words only, as shown in the first this respect, Miller&apos;s (1957) alternative derivation of (2) in terms of random spacing is unconvincing in the light of the phonotactic constraints on word structure. 276 io Figure 2: Rank-frequency plots for Dutch phonological stems. From left to right: monomorphemic words without function words, monomorphemic words and function words, complete distribution. two graphs of figure 2. Miller, Newman &amp; Friedman (1958), discussing the finding that the frequential characteristics of function words differ markedly from those of content words, argued that (1958:385) Inasmuch as the division into two of words of the frequencies of the words, we might have expected it to simply divide the sample in half, each half retaining the statistical properties of the whole. Since this is clearly not the case, it is obvious that Mandelbrot&apos;s approach is incomplete. The general trends for all words combined seem to follow a stochastic pattern, but when we look at syntactic patterns, differences begin to appear which will require linguistic, rather than mere statistical, explanations. In the Mandelbrot-Simon model developed here, neither the Markovian front end nor the proposed rule of usage are able to model the extremely high intensity of use of these function words correctly without unwished-for side effects on the distribution of content words. However, given that the semantics of function words are not subject to the loss of specificity that characterizes high-frequency content words, function words are not subject to selection proportional some form of selection proportional to rn,. probably is more appropriate here. MORPHOLOGY The Mandelbrot-Simon model has a single parameter a that allows new words to enter the distribution. Since the present theory is of a phonological rather than a morphological nature, this parameter models the (occasional) appearance of new simplex words in the language only, and cannot be used to model the influx of morphologically complex words. First, morphological word formation processes may give rise to consonant clusters that are permitted when they span morpheme boundaries, but that are inadmissible within single morphemes. This difference in phonotactic patterning within and across morphemes already reveales that morphologically complex words have a different source than monomorphemic words. Second, each word formation process, whether or affixation of suffixes like and -ity, is characterized by its own degree of productivity. Quantitatively, differences in the degree of productivity amount to differences in the birth rates at which complex words appear in the vocabulary. Typically, such birth rates, can be expressed as where and the number of types occurring once only and the number of tokens of the frequency distributions of the corresponding morphological categories (Baayen 1989), assume values that are significantly higher that the birth rate a of monomorphemic words. Hence it is impossible to model the complete lexical distribution without a worked-out morphological component that specifies the word formation processes of the language and their degrees of productivity. While actual modelling of the complete distribution is beyond the scope of the present paper, we may note that the addition of birth rates for word formation processes to the model, necessitated by the additional large numbers of rare 277 words that appear in the complete distribution, ties in nicely with the fact that the frequency distributions of productive morphological categories are prototypical LNRE distributions, for which the large values for the numbers of types occurring once or twice only are characteristic. With respect to the effect of morphological structure on the lexical similarity effects, we finally note that in the empirical data the longer word lengths show up with sharply diminished neighborhood density. However, it appears that those longer words which do have neighbors are morphologically complex. Morphological structure raises lexical density where the phonotaxis fails to do so: for long monomorphemic words the huge space of possible word types is sampled too sparcely for the lexical similarity effects to emerge.</abstract>
<note confidence="0.816207913043478">RtFERENCES R.H. 1989. Corpus-Based Approach to Morphological Productivity. Statistical Analand Psycholinguistic Interpretation. Vrije Universiteit, Amsterdam. Carroll, J.B. 1967. On Sampling from a Lognormal Model of Word Frequency Distribution. W.N.Francis 1967, Carroll, J.B. 1969. A Rationale for an Asymptotic Lognormal Form of Word Frequency Distri- Bulletin - Educational Test- Service, November 1969. Chitaivili, R.I. &amp; Khmaladze, E.V. 1989. Statistical Analysis of Large Number of Rare Events Related Problems. of the Tbilisi Mathematical Institute. . The population frequencies of species and the estimation of population param- 45-63. G. 1960. Mathematics, The Hague, Mouton. H. &amp; Francis, W.N. 1967. Computational Analysis of Present-Day American En-</note>
<affiliation confidence="0.954724">Brown University Press.</affiliation>
<address confidence="0.78641">Landauer, T.K. &amp; Streeter, L.A. 1973. Struc-</address>
<abstract confidence="0.917693916666667">tural differences between common and rare words: failure of equivalence assumptions for of word recognition, of Verbal and Verbal Behavior 119-131. Mandelbrot, B. 1953. An informational theory of the statistical structure of language, in: (ed.), Theory, Butterworths. Mandelbrot, B. 1962. On the theory of word frequencies and on related Markovian models discourse, in: R.Jakobson, of Language and its Mathematical Aspects. Proceedings</abstract>
<note confidence="0.930125976190476">Symposia in Applied Mathematics XII, Providence, Rhode Island, Americal Mathematical Society, 190-219. G.A. 1954. Communication, of Psychology 401-420. Miller, G.A. 1957. Some effects of intermittent American Journal of Psychology 311-314. Miller, G.A., Newman, E.B. &amp; Friedman, E.A. 1958. Length-Frequency Statistics for Written and control 370-389. Muller, Ch. 1979. Du nouveau sur les distributions lexicales: la formule de Waring-Herdan. Ch. Muller, Frangaise et Linguis- Quantitative. Slatkine, 177-195. Nusbaum, H.C. 1985. A stochastic account of the relationship between lexical density and frequency, on Speech Perception Orlov, J.K. &amp; Chitashvili, R.Y. 1983. Generalized Z-distribution generating the well-known of the Academy of Georgia 269-272. Paivio, A., Yuille, J.C. &amp; Madigan, S. 1968. Concreteness, Imagery and Meaningfulness Valfor 925 Nouns. of Experimental Psy- Monograph I, Pt. 2. Reder, L.M., Anderson, J.R. &amp; Bjork, R.A. 1974. A Semantic Interpretation of Encoding of Experimental Psychology 102: 648-656. Rouault, A. 1978. Loi de Zipf et sources Inst. H.Poincare 169- 188. Sichel, H.S. 1975. On a Distribution Law for Frequencies. of the American Sta- Association 542-547. Simon, H.A. 1955. On a class of skew distrifunctions, 435-440. Simon, H.A. 1960. Some further notes on a of skew distribution functions, 80-88. G.K. 1935. Psycho-Biology of Lan-</note>
<affiliation confidence="0.616535">Houghton Mifflin.</affiliation>
<address confidence="0.545844">278</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R H Baayen</author>
</authors>
<title>A Corpus-Based Approach to Morphological Productivity. Statistical Analysis and Psycholinguistic Interpretation. Diss. Vrije Universiteit,</title>
<date>1989</date>
<location>Amsterdam.</location>
<contexts>
<context position="27298" citStr="Baayen 1989" startWordPosition="4600" endWordPosition="4601">ically complex words have a different source than monomorphemic words. Second, each word formation process, whether compounding or affixation of suffixes like -mess and -ity, is characterized by its own degree of productivity. Quantitatively, differences in the degree of productivity amount to differences in the birth rates at which complex words appear in the vocabulary. Typically, such birth rates, which can be expressed as N„ where n&apos;1 and N&apos; denote the number of types occurring once only and the number of tokens of the frequency distributions of the corresponding morphological categories (Baayen 1989), assume values that are significantly higher that the birth rate a of monomorphemic words. Hence it is impossible to model the complete lexical distribution without a worked-out morphological component that specifies the word formation processes of the language and their degrees of productivity. While actual modelling of the complete distribution is beyond the scope of the present paper, we may note that the addition of birth rates for word formation processes to the model, necessitated by the additional large numbers of rare 277 words that appear in the complete distribution, ties in nicely </context>
</contexts>
<marker>Baayen, 1989</marker>
<rawString>Baayen, R.H. 1989. A Corpus-Based Approach to Morphological Productivity. Statistical Analysis and Psycholinguistic Interpretation. Diss. Vrije Universiteit, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Carroll</author>
</authors>
<title>On Sampling from a Lognormal Model of Word Frequency Distribution. In: H.Kuaera W.N.Francis</title>
<date>1967</date>
<pages>406--424</pages>
<contexts>
<context position="1748" citStr="Carroll (1967" startWordPosition="279" endWordPosition="280">. For i —4 oo, (3) can be written as r(p in other words, (3) approximates Zipf&apos;s law with respect to the lower frequency words, the tail of *I am indebted to Klaas van Ham, Richard Gill, Bert Hocks and Erik Schils for stimulating discussions on the statistical analysis of lexical similarity relations. the distribution. Other models, such as Good (1953), Waring-Herdan (Herdan 1960, Muller 1979) and Sichel (1975), have been put forward, all of which have Zipf&apos;s law as some special or limiting form. Unrelated to Zipf&apos;s law is the lognormal hypothesis, advanced for word frequency distributions by Carroll (1967, 1969), which gives rise to reasonable fits and is widely used in psycholinguistic research on word frequency effects in mental processing. A problem that immediately arises in the context of the study of word frequency distributions concerns the fact that these distributions have two important characteristics which they share with other so-called large number of rare events (LNRE) distributions (Orlov and Chitashvili 1983, Chitashvili and Khmaladze 1989), namely that on the one hand a huge number of different word types appears, and that on the other hand it is observed that while some event</context>
</contexts>
<marker>Carroll, 1967</marker>
<rawString>Carroll, J.B. 1967. On Sampling from a Lognormal Model of Word Frequency Distribution. In: H.Kuaera W.N.Francis 1967, 406-424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Carroll</author>
</authors>
<title>A Rationale for an Asymptotic Lognormal Form of Word Frequency Distributions. Research Bulletin - Educational Testing Service,</title>
<date>1969</date>
<location>Princeton,</location>
<marker>Carroll, 1969</marker>
<rawString>Carroll, J.B. 1969. A Rationale for an Asymptotic Lognormal Form of Word Frequency Distributions. Research Bulletin - Educational Testing Service, Princeton, November 1969.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R I Chitaivili</author>
<author>E V Khmaladze</author>
</authors>
<title>Statistical Analysis of Large Number of Rare Events and Related Problems.</title>
<date>1989</date>
<journal>Transactions of the Tbilisi Mathematical Institute.</journal>
<marker>Chitaivili, Khmaladze, 1989</marker>
<rawString>Chitaivili, R.I. &amp; Khmaladze, E.V. 1989. Statistical Analysis of Large Number of Rare Events and Related Problems. Transactions of the Tbilisi Mathematical Institute.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Good</author>
</authors>
<title>The population frequencies of species and the estimation of population parameters,</title>
<date>1953</date>
<journal>Biometrika</journal>
<volume>43</volume>
<pages>45--63</pages>
<contexts>
<context position="1489" citStr="Good (1953)" startWordPosition="238" endWordPosition="239">t are more accurate with respect to the higher frequency words. Simon (1955, 1960) developed a stochastic process which has the Yule distribution = AB(i, p 1), (3) with the parameter A and B(i, p 1) the Beta function in (i, p 1), as its stationary solutions. For i —4 oo, (3) can be written as r(p in other words, (3) approximates Zipf&apos;s law with respect to the lower frequency words, the tail of *I am indebted to Klaas van Ham, Richard Gill, Bert Hocks and Erik Schils for stimulating discussions on the statistical analysis of lexical similarity relations. the distribution. Other models, such as Good (1953), Waring-Herdan (Herdan 1960, Muller 1979) and Sichel (1975), have been put forward, all of which have Zipf&apos;s law as some special or limiting form. Unrelated to Zipf&apos;s law is the lognormal hypothesis, advanced for word frequency distributions by Carroll (1967, 1969), which gives rise to reasonable fits and is widely used in psycholinguistic research on word frequency effects in mental processing. A problem that immediately arises in the context of the study of word frequency distributions concerns the fact that these distributions have two important characteristics which they share with other </context>
</contexts>
<marker>Good, 1953</marker>
<rawString>Good, 11 . 1953. The population frequencies of species and the estimation of population parameters, Biometrika 43, 45-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Herdan</author>
</authors>
<title>Type-token Mathematics,</title>
<date>1960</date>
<location>The Hague, Mouton.</location>
<contexts>
<context position="1517" citStr="Herdan 1960" startWordPosition="241" endWordPosition="242">pect to the higher frequency words. Simon (1955, 1960) developed a stochastic process which has the Yule distribution = AB(i, p 1), (3) with the parameter A and B(i, p 1) the Beta function in (i, p 1), as its stationary solutions. For i —4 oo, (3) can be written as r(p in other words, (3) approximates Zipf&apos;s law with respect to the lower frequency words, the tail of *I am indebted to Klaas van Ham, Richard Gill, Bert Hocks and Erik Schils for stimulating discussions on the statistical analysis of lexical similarity relations. the distribution. Other models, such as Good (1953), Waring-Herdan (Herdan 1960, Muller 1979) and Sichel (1975), have been put forward, all of which have Zipf&apos;s law as some special or limiting form. Unrelated to Zipf&apos;s law is the lognormal hypothesis, advanced for word frequency distributions by Carroll (1967, 1969), which gives rise to reasonable fits and is widely used in psycholinguistic research on word frequency effects in mental processing. A problem that immediately arises in the context of the study of word frequency distributions concerns the fact that these distributions have two important characteristics which they share with other so-called large number of ra</context>
</contexts>
<marker>Herdan, 1960</marker>
<rawString>Herdan, G. 1960. Type-token Mathematics, The Hague, Mouton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kaera</author>
<author>W N Francis</author>
</authors>
<title>Computational Analysis of Present-Day American English.</title>
<date>1967</date>
<publisher>Brown University Press.</publisher>
<location>Providence:</location>
<marker>Kaera, Francis, 1967</marker>
<rawString>Kaera, H. &amp; Francis, W.N. 1967. Computational Analysis of Present-Day American English. Providence: Brown University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>L A Streeter</author>
</authors>
<title>Structural differences between common and rare words: failure of equivalence assumptions for theories of word recognition,</title>
<date>1973</date>
<journal>Journal of Verbal Learning and Verbal Behavior</journal>
<volume>12</volume>
<pages>119--131</pages>
<contexts>
<context position="4061" citStr="Landauer and Streeter (1973)" startWordPosition="638" endWordPosition="641">larity relations among words in lexical distributions. These empirical similarity relations, as observed for large corpora of words, impose additional criteria on the adequacy of models for word frequency distributions. SIMILARITY RELATIONS There is a growing consensus in psycholinguistic research that word recognition depends not only on properties of the target word (e.g. its length and frequency), but also upon the number and nature of its lexical competitors or neighbors. The first to study similarity relations among lexical competitors in the lexicon in relation to lexical frequency were Landauer and Streeter (1973). Let a neighbor be a word that differs in exactly one phoneme (or letter) from a given target string, and let the neighborhood be the set of all neighbors, i.e. the set of all words at Hamming distance 1 from the target. Landauer and Streeter observed that (1) highfrequency words have more neighbors than lowfrequency words (the neighborhood density effect), and that (2) high-frequency words have higher-frequency neighbors than low-frequency words (the neighborhood frequency effect). In order to facilitate statistical analysis, it is convenient to restate the neighborhood frequency effect as a</context>
</contexts>
<marker>Landauer, Streeter, 1973</marker>
<rawString>Landauer, T.K. &amp; Streeter, L.A. 1973. Structural differences between common and rare words: failure of equivalence assumptions for theories of word recognition, Journal of Verbal Learning and Verbal Behavior 12, 119-131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Mandelbrot</author>
</authors>
<title>An informational theory of the statistical structure of language,</title>
<date>1953</date>
<booktitle>Communication Theory,</booktitle>
<editor>in: W.Jackson (ed.),</editor>
<location>Butterworths.</location>
<contexts>
<context position="659" citStr="Mandelbrot (1953" startWordPosition="94" endWordPosition="95">TRIBUTIONS Harald Baayen* Max-Planck-Institut fur Psycholinguistik Wundtlaan 1, NL-6525 XD Nijmegen Internet: baayenOmpi.n1 ABSTRACT A stochastic model based on insights of Mandelbrot (1953) and Simon (1955) is discussed against the background of new criteria of adequacy that have become available recently as a result of studies of the similarity relations between words as found in large computerized text corpora. FREQUENCY DISTRIBUTIONS Various models for word frequency distributions have been developed since Zipf (1935) applied the zeta distribution to describe a wide range of lexical data. Mandelbrot (1953, 1962) extended Zipf&apos;s distribution &apos;law&apos; (1) where A is the sample frequency of the ith type in a ranking according to decreasing frequency, with the parameter B, = .tetB+i4,(2) by means of which fits are obtained that are more accurate with respect to the higher frequency words. Simon (1955, 1960) developed a stochastic process which has the Yule distribution = AB(i, p 1), (3) with the parameter A and B(i, p 1) the Beta function in (i, p 1), as its stationary solutions. For i —4 oo, (3) can be written as r(p in other words, (3) approximates Zipf&apos;s law with respect to the lower frequency wor</context>
<context position="7249" citStr="Mandelbrot (1953" startWordPosition="1151" endWordPosition="1152"> rank correlation analysis reveals that the lexical similarity effects of figure 1A are statistically highly significant trends (p &lt; 0.001), even though the correlations themselves are quite weak (see table 1, column 1): in the case of lexical density only 6% of the variance is explained.1 STOCHASTIC MODELLING By themselves, models of the kind proposed by Zipf, Herdan and Muller or Sichel, even though they may yield reasonable fits to particular word frequency distributions, have no bearing on the similarity relations in the lexicon. The only model that is promising in this respect is that of Mandelbrot (1953, 1962). Mandelbrot derived his modification of Zipf&apos;s law (2) on the basis of a Markovian model for generating words as strings of letters, in combination with some assumptions concerning the cost of transmitting the words generated in some optimal code, giving a precise interpretation to Zipf&apos;s &apos;law of abbreviation&apos;. Miller (1957), wishing to avoid a teleological explanation, showed that the Zipf-Mandelbrot law can also be derived under slightly different assumptions. Interestingly, Nusbaum (1985), on the basis of simulation results with a slightly different neighbor definition, reports that</context>
</contexts>
<marker>Mandelbrot, 1953</marker>
<rawString>Mandelbrot, B. 1953. An informational theory of the statistical structure of language, in: W.Jackson (ed.), Communication Theory, Butterworths.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Mandelbrot</author>
</authors>
<title>On the theory of word frequencies and on related Markovian models of discourse, in: R.Jakobson, Structure of Language and its Mathematical Aspects.</title>
<date>1962</date>
<booktitle>Proceedings of Symposia in Applied Mathematics Vol XII,</booktitle>
<marker>Mandelbrot, 1962</marker>
<rawString>Mandelbrot, B. 1962. On the theory of word frequencies and on related Markovian models of discourse, in: R.Jakobson, Structure of Language and its Mathematical Aspects. Proceedings of Symposia in Applied Mathematics Vol XII,</rawString>
</citation>
<citation valid="false">
<authors>
<author>Rhode Island Providence</author>
</authors>
<pages>190--219</pages>
<publisher>Americal Mathematical Society,</publisher>
<marker>Providence, </marker>
<rawString>Providence, Rhode Island, Americal Mathematical Society, 190-219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<date>1954</date>
<journal>Communication, Annual Review of Psychology</journal>
<volume>5</volume>
<pages>401--420</pages>
<marker>Miller, 1954</marker>
<rawString>Miller, G.A. 1954. Communication, Annual Review of Psychology 5, 401-420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>Some effects of intermittent silence,</title>
<date>1957</date>
<journal>The American Journal of Psychology</journal>
<volume>52</volume>
<pages>311--314</pages>
<contexts>
<context position="7583" citStr="Miller (1957)" startWordPosition="1205" endWordPosition="1206">the kind proposed by Zipf, Herdan and Muller or Sichel, even though they may yield reasonable fits to particular word frequency distributions, have no bearing on the similarity relations in the lexicon. The only model that is promising in this respect is that of Mandelbrot (1953, 1962). Mandelbrot derived his modification of Zipf&apos;s law (2) on the basis of a Markovian model for generating words as strings of letters, in combination with some assumptions concerning the cost of transmitting the words generated in some optimal code, giving a precise interpretation to Zipf&apos;s &apos;law of abbreviation&apos;. Miller (1957), wishing to avoid a teleological explanation, showed that the Zipf-Mandelbrot law can also be derived under slightly different assumptions. Interestingly, Nusbaum (1985), on the basis of simulation results with a slightly different neighbor definition, reports that the neighborhood density and neighborhood frequency effects occur within 1Note that the larger value of r; for the neighborhood frequency effect is a direct consequence of the fact that the frequencies of the neighbors of each target are averaged before they enter into the calculations, masking much of the variance. 272 O••• TT 1 2</context>
<context position="23126" citStr="Miller 1957" startWordPosition="3928" endWordPosition="3929">larger number of meanings is correlated with increased contextual dependency for interpretation. Hence the amount of information contributed by such types out of context (under conditions of statistical independence) is less than what their relative sample frequencies suggest, exactly as modelled by our rule of usage. Note that this semantic motivation for selection proportional to H,. makes it possible to avoid invoking external principles such as &apos;least effort&apos; or &apos;optimal coding&apos; in the mathematical definition of the model, principles that have been criticized as straining one&apos;s credulity (Miller 1957).4 FUNCTION WORDS Up till now, we have focused on the modelling of monomorphemic Dutch words, to the exclusion of function words and morphologically complex words. One of the reasons for this approach concerns the way in which the shape of the rank-frequency curves differs substantially depending on which kinds of words are included in the distribution. As shown in figure 2, the curve of monomorphemic words without function words is highly convex. When function words are added, the head of the tail is straightened out, while the addition of complex words brings the tail of the distribution (mo</context>
</contexts>
<marker>Miller, 1957</marker>
<rawString>Miller, G.A. 1957. Some effects of intermittent silence, The American Journal of Psychology 52, 311-314.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>E B Newman</author>
<author>E A Friedman</author>
</authors>
<title>Length-Frequency Statistics for Written English,</title>
<date>1958</date>
<journal>Information and</journal>
<volume>1</volume>
<pages>370--389</pages>
<marker>Miller, Newman, Friedman, 1958</marker>
<rawString>Miller, G.A., Newman, E.B. &amp; Friedman, E.A. 1958. Length-Frequency Statistics for Written English, Information and control 1, 370-389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ch Muller</author>
</authors>
<title>Du nouveau sur les distributions lexicales: la formule de Waring-Herdan.</title>
<date>1979</date>
<journal>In: Ch. Muller, Langue Frangaise et Linguistique Quantitative. Geneve: Slatkine,</journal>
<pages>177--195</pages>
<contexts>
<context position="1531" citStr="Muller 1979" startWordPosition="243" endWordPosition="244">igher frequency words. Simon (1955, 1960) developed a stochastic process which has the Yule distribution = AB(i, p 1), (3) with the parameter A and B(i, p 1) the Beta function in (i, p 1), as its stationary solutions. For i —4 oo, (3) can be written as r(p in other words, (3) approximates Zipf&apos;s law with respect to the lower frequency words, the tail of *I am indebted to Klaas van Ham, Richard Gill, Bert Hocks and Erik Schils for stimulating discussions on the statistical analysis of lexical similarity relations. the distribution. Other models, such as Good (1953), Waring-Herdan (Herdan 1960, Muller 1979) and Sichel (1975), have been put forward, all of which have Zipf&apos;s law as some special or limiting form. Unrelated to Zipf&apos;s law is the lognormal hypothesis, advanced for word frequency distributions by Carroll (1967, 1969), which gives rise to reasonable fits and is widely used in psycholinguistic research on word frequency effects in mental processing. A problem that immediately arises in the context of the study of word frequency distributions concerns the fact that these distributions have two important characteristics which they share with other so-called large number of rare events (LNR</context>
</contexts>
<marker>Muller, 1979</marker>
<rawString>Muller, Ch. 1979. Du nouveau sur les distributions lexicales: la formule de Waring-Herdan. In: Ch. Muller, Langue Frangaise et Linguistique Quantitative. Geneve: Slatkine, 177-195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H C Nusbaum</author>
</authors>
<title>A stochastic account of the relationship between lexical density and word frequency, Research on Speech Perception</title>
<date>1985</date>
<tech>Report # 11,</tech>
<institution>Indiana University.</institution>
<contexts>
<context position="7753" citStr="Nusbaum (1985)" startWordPosition="1230" endWordPosition="1231">ilarity relations in the lexicon. The only model that is promising in this respect is that of Mandelbrot (1953, 1962). Mandelbrot derived his modification of Zipf&apos;s law (2) on the basis of a Markovian model for generating words as strings of letters, in combination with some assumptions concerning the cost of transmitting the words generated in some optimal code, giving a precise interpretation to Zipf&apos;s &apos;law of abbreviation&apos;. Miller (1957), wishing to avoid a teleological explanation, showed that the Zipf-Mandelbrot law can also be derived under slightly different assumptions. Interestingly, Nusbaum (1985), on the basis of simulation results with a slightly different neighbor definition, reports that the neighborhood density and neighborhood frequency effects occur within 1Note that the larger value of r; for the neighborhood frequency effect is a direct consequence of the fact that the frequencies of the neighbors of each target are averaged before they enter into the calculations, masking much of the variance. 272 O••• TT 1 2 3 In • • 2000 - 1000 - 500- 100 - 50 - 1•••• 10 - FC 1 - DC 4 5 6 7 1 2 3 4 5 6 226 164 96 03 # items 311 360 291 179 70 14 # items fs #71 104 20 103 - 16 - 102 - 12 101</context>
</contexts>
<marker>Nusbaum, 1985</marker>
<rawString>Nusbaum, H.C. 1985. A stochastic account of the relationship between lexical density and word frequency, Research on Speech Perception Report # 11, Indiana University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J K Orlov</author>
<author>R Y Chitashvili</author>
</authors>
<title>Generalized Z-distribution generating the well-known &apos;rank-distributions&apos;,</title>
<date>1983</date>
<journal>Bulletin of the Academy of Sciences, Georgia</journal>
<volume>110</volume>
<pages>269--272</pages>
<contexts>
<context position="2175" citStr="Orlov and Chitashvili 1983" startWordPosition="343" endWordPosition="347">, have been put forward, all of which have Zipf&apos;s law as some special or limiting form. Unrelated to Zipf&apos;s law is the lognormal hypothesis, advanced for word frequency distributions by Carroll (1967, 1969), which gives rise to reasonable fits and is widely used in psycholinguistic research on word frequency effects in mental processing. A problem that immediately arises in the context of the study of word frequency distributions concerns the fact that these distributions have two important characteristics which they share with other so-called large number of rare events (LNRE) distributions (Orlov and Chitashvili 1983, Chitashvili and Khmaladze 1989), namely that on the one hand a huge number of different word types appears, and that on the other hand it is observed that while some events have reasonably stable frequencies, others occur only once, twice, etc. Crucially, these rare events occupy a significant portion of the list of all types observed. The presence of such large numbers of very low frequency types effects a significant bias between the rank-probability distribution and the rank-frequency distribution, leading to the contradiction of the common mean of the law of large numbers, so that expres</context>
</contexts>
<marker>Orlov, Chitashvili, 1983</marker>
<rawString>Orlov, J.K. &amp; Chitashvili, R.Y. 1983. Generalized Z-distribution generating the well-known &apos;rank-distributions&apos;, Bulletin of the Academy of Sciences, Georgia 110.2, 269-272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Paivio</author>
<author>J C Yuille</author>
<author>S Madigan</author>
</authors>
<title>Concreteness, Imagery and Meaningfulness Values for 925 Nouns.</title>
<date>1968</date>
<journal>Journal of Experimental Psychology Monograph 76, I, Pt.</journal>
<volume>2</volume>
<marker>Paivio, Yuille, Madigan, 1968</marker>
<rawString>Paivio, A., Yuille, J.C. &amp; Madigan, S. 1968. Concreteness, Imagery and Meaningfulness Values for 925 Nouns. Journal of Experimental Psychology Monograph 76, I, Pt. 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L M Reder</author>
<author>J R Anderson</author>
<author>R A Bjork</author>
</authors>
<title>A Semantic Interpretation of Encoding Specificity.</title>
<date>1974</date>
<journal>Journal of Experimental Psychology</journal>
<volume>102</volume>
<pages>648--656</pages>
<marker>Reder, Anderson, Bjork, 1974</marker>
<rawString>Reder, L.M., Anderson, J.R. &amp; Bjork, R.A. 1974. A Semantic Interpretation of Encoding Specificity. Journal of Experimental Psychology 102: 648-656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rouault</author>
</authors>
<title>Loi de Zipf et sources markoviennes,</title>
<date>1978</date>
<journal>Ann. Inst. H.Poincare</journal>
<volume>14</volume>
<pages>169--188</pages>
<contexts>
<context position="13195" citStr="Rouault (1978)" startWordPosition="2278" endWordPosition="2279"> illegal word final kn sequences, at the same time avoiding full conditioning on two preceding segments, which, for four-letter words, would come uncomfortably close to building the probabilities of the individual words in the database into the model. The rank-frequency distribution of 58300 types and 224567 tokens (disregarding strings of length 1) obtained by means of this (second order) Markov process shows up in a double logarithmic plot as roughly linear (figure 1B). Although the curve has the general Zipfian shape, the deviations at head and tail are present by necessity in the light of Rouault (1978). A comparison with figure 1A reveals that the large surplus of very low frequency types is highly unsatisfactory. The model (given the present transition matrix) fails to replicate the high rate of use of the relatively limited set of words of natural language. The lexical similarity effects as they emerge for the simulated strings of length 4 are displayed in the boxplots of figure 1B. A very pronounced neighborhood density effect is found, in combination with a subdued neighborhood frequency effect (see table 1, column 2). The appearance of the neighborhood density effect within a fixed str</context>
</contexts>
<marker>Rouault, 1978</marker>
<rawString>Rouault, A. 1978. Loi de Zipf et sources markoviennes, Ann. Inst. H.Poincare 14, 169-188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H S Sichel</author>
</authors>
<title>On a Distribution Law for Word Frequencies.</title>
<date>1975</date>
<journal>Journal of the American Statistical Association</journal>
<volume>70</volume>
<pages>542--547</pages>
<contexts>
<context position="1549" citStr="Sichel (1975)" startWordPosition="246" endWordPosition="247">ords. Simon (1955, 1960) developed a stochastic process which has the Yule distribution = AB(i, p 1), (3) with the parameter A and B(i, p 1) the Beta function in (i, p 1), as its stationary solutions. For i —4 oo, (3) can be written as r(p in other words, (3) approximates Zipf&apos;s law with respect to the lower frequency words, the tail of *I am indebted to Klaas van Ham, Richard Gill, Bert Hocks and Erik Schils for stimulating discussions on the statistical analysis of lexical similarity relations. the distribution. Other models, such as Good (1953), Waring-Herdan (Herdan 1960, Muller 1979) and Sichel (1975), have been put forward, all of which have Zipf&apos;s law as some special or limiting form. Unrelated to Zipf&apos;s law is the lognormal hypothesis, advanced for word frequency distributions by Carroll (1967, 1969), which gives rise to reasonable fits and is widely used in psycholinguistic research on word frequency effects in mental processing. A problem that immediately arises in the context of the study of word frequency distributions concerns the fact that these distributions have two important characteristics which they share with other so-called large number of rare events (LNRE) distributions (</context>
</contexts>
<marker>Sichel, 1975</marker>
<rawString>Sichel, H.S. 1975. On a Distribution Law for Word Frequencies. Journal of the American Statistical Association 70, 542-547.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H A Simon</author>
</authors>
<title>On a class of skew distribution functions,</title>
<date>1955</date>
<journal>Biometrika</journal>
<volume>42</volume>
<pages>435--440</pages>
<contexts>
<context position="953" citStr="Simon (1955" startWordPosition="143" endWordPosition="144">recently as a result of studies of the similarity relations between words as found in large computerized text corpora. FREQUENCY DISTRIBUTIONS Various models for word frequency distributions have been developed since Zipf (1935) applied the zeta distribution to describe a wide range of lexical data. Mandelbrot (1953, 1962) extended Zipf&apos;s distribution &apos;law&apos; (1) where A is the sample frequency of the ith type in a ranking according to decreasing frequency, with the parameter B, = .tetB+i4,(2) by means of which fits are obtained that are more accurate with respect to the higher frequency words. Simon (1955, 1960) developed a stochastic process which has the Yule distribution = AB(i, p 1), (3) with the parameter A and B(i, p 1) the Beta function in (i, p 1), as its stationary solutions. For i —4 oo, (3) can be written as r(p in other words, (3) approximates Zipf&apos;s law with respect to the lower frequency words, the tail of *I am indebted to Klaas van Ham, Richard Gill, Bert Hocks and Erik Schils for stimulating discussions on the statistical analysis of lexical similarity relations. the distribution. Other models, such as Good (1953), Waring-Herdan (Herdan 1960, Muller 1979) and Sichel (1975), ha</context>
<context position="15576" citStr="Simon (1955)" startWordPosition="2660" endWordPosition="2661">an be traced to the extremely large number of types generated (6425) for a number of tokens (74618) for which the empirical data (64854 tokens) allow only 1342 types. This large surplus of types gives rise to an inflated neighborhood density effect, with the concomitant effect that neighborhood frequency is scaled down. Rather than attempting to address this issue by changing the transition matrix by using a more constrained but less realistic data set, another option is explored here, namely the idea to supplement the Markovian stochastic process with a second stochastic process developed by Simon (1955), by means of which the intensive use can be modelled to which the word types of natural language are put. Consider the frequency distribution of e.g. a corpus that is being compiled, and assume that at some stage of compilation N word tokens have been observed. Let te) be the number of word types that have occurred exactly r times in these first N words. If we allow for the possibilities that both new types can be sampled, and old types can be re-used, Simon&apos;s model in its simplest form is obtained under the three assumptions that (1) the probability that the (N +1)-st word is a type that has</context>
</contexts>
<marker>Simon, 1955</marker>
<rawString>Simon, H.A. 1955. On a class of skew distribution functions, Biometrika 42, 435-440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H A Simon</author>
</authors>
<title>Some further notes on a class of skew distribution functions,</title>
<date>1960</date>
<journal>Information and Control</journal>
<volume>3</volume>
<pages>80--88</pages>
<contexts>
<context position="16936" citStr="Simon 1960" startWordPosition="2903" endWordPosition="2904">constant probability a that the (N+1)-st word represents a new type, and that (3) all frequencies grow proportionaly with N, so that n(N-1-1.) N+1 (N) for all r,N. N TIT Simon (1955) shows that the Yule-distribution (3) follows from these assumptions. When the third assumption is replaced by the assumptions that word types are dropped with a probability proportional to their token frequency, and that old words are dropped at the same rate at which new word types are introduced so that the total number of tokens in the distribution is a constant, the Yule-distribution is again found to follow (Simon 1960). By itself, this stochastic process has no explanatory value with respect to the similarity relations between words. It specifies use and reuse of word types, without any reference to segmental constituency or length. However, when a Markovian process is fitted as a front end to Simon&apos;s stochastic process, a hybrid model results that has the desired properties, since the latter process can be used to force the required high intensity of use on the types of its input distribution. The Markovian front end of the model can be thought of as defining a probability distribution that reflects the ea</context>
</contexts>
<marker>Simon, 1960</marker>
<rawString>Simon, H.A. 1960. Some further notes on a class of skew distribution functions, Information and Control 3, 80-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G K Zipf</author>
</authors>
<title>The Psycho-Biology of Language,</title>
<date>1935</date>
<location>Boston, Houghton Mifflin.</location>
<marker>Zipf, 1935</marker>
<rawString>Zipf, G.K. 1935. The Psycho-Biology of Language, Boston, Houghton Mifflin.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>