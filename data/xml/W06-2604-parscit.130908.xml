<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000619">
<title confidence="0.9991455">
A Multiclassifier based Document Categorization System: profiting from
the Singular Value Decomposition Dimensionality Reduction Technique
</title>
<author confidence="0.59305">
Ana Zelaia I˜naki Alegria Olatz Arregi Basilio Sierra
</author>
<affiliation confidence="0.330799">
UPV-EHU UPV-EHU UPV-EHU UPV-EHU
</affiliation>
<address confidence="0.447187">
Basque Country Basque Country Basque Country Basque Country
</address>
<email confidence="0.930411">
ccpzejaa@si.ehu.es acpalloi@si.ehu.es acparuro@si.ehu.es ccpsiarb@si.ehu.es
</email>
<sectionHeader confidence="0.993774" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996910764705882">
In this paper we present a multiclassifier
approach for multilabel document classifi-
cation problems, where a set of k-NN clas-
sifiers is used to predict the category of
text documents based on different training
subsampling databases. These databases
are obtained from the original training
database by random subsampling. In or-
der to combine the predictions generated
by the multiclassifier, Bayesian voting is
applied. Through all the classification pro-
cess, a reduced dimension vector represen-
tation obtained by Singular Value Decom-
position (SVD) is used for training and
testing documents. The good results of our
experiments give an indication of the po-
tentiality of the proposed approach.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9994428">
Document Categorization, the assignment of nat-
ural language texts to one or more predefined
categories based on their content, is an impor-
tant component in many information organization
and management tasks. Researchers have con-
centrated their efforts in finding the appropriate
way to represent documents, index them and con-
struct classifiers to assign the correct categories to
each document. Both, document representation
and classification method are crucial steps in the
categorization process.
In this paper we concentrate on both issues. On
the one hand, we use Latent Semantic Indexing
(LSI) (Deerwester et al., 1990), which is a vari-
ant of the vector space model (VSM) (Salton and
McGill, 1983), in order to obtain the vector rep-
resentation of documents. This technique com-
presses vectors representing documents into vec-
tors of a lower-dimensional space. LSI, which
is based on Singular Value Decomposition (SVD)
of matrices, has showed to have the ability to ex-
tract the relations among words and documents by
means of their context of use, and has been suc-
cessfully applied to Information Retrieval tasks.
On the other hand, we construct a multiclassi-
fier (Ho et al., 1994) which uses different train-
ing databases. These databases are obtained from
the original training set by random subsampling.
We implement this approach by bagging, and use
the k-NN classification algorithm to make the cat-
egory predictions for testing documents. Finally,
we combine all predictions made for a given doc-
ument by Bayesian voting.
The experiment we present has been evaluated
for Reuters-21578 standard document collection.
Reuters-21578 is a multilabel document collec-
tion, which means that categories are not mutu-
ally exclusive because the same document may be
relevant to more than one category. Being aware
of the results published in the most recent litera-
ture, and having obtained good results in our ex-
periments, we consider the categorization method
presented in this paper an interesting contribution
for text categorization tasks.
The remainder of this paper is organized as fol-
lows: Section 2, discusses related work on docu-
ment categorization for Reuters-21578 collection.
In Section 3, we present our approach to deal with
the multilabel text categorization task. In Section
4 the experimental setup is introduced, and details
about the Reuters database, the preprocessing ap-
plied and some parameter setting are provided. In
Section 5, experimental results are presented and
discussed. Finally, Section 6 contains some con-
clusions and comments on future work.
</bodyText>
<page confidence="0.998239">
25
</page>
<sectionHeader confidence="0.999432" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999940545454546">
As previously mentioned in the introduction, text
categorization consists in assigning predefined
categories to text documents. In the past two
decades, document categorization has received
much attention and a considerable number of ma-
chine learning based approaches have been pro-
posed. A good tutorial on the state-of-the-art of
document categorization techniques can be found
in (Sebastiani, 2002).
In the document categorization task we can find
two cases; (1) the multilabel case, which means
that categories are not mutually exclusive, because
the same document may be relevant to more than
one category (1 to m category labels may be as-
signed to the same document, being m the to-
tal number of predefined categories), and (2) the
single-label case, where exactly one category is
assigned to each document. While most machine
learning systems are designated to handle multi-
class data l, much less common are systems that
can handle multilabel data.
For experimentation purposes, there are stan-
dard document collections available in the pub-
lic domain that can be used for document catego-
rization. The most widely used is Reuters-21578
collection, which is a multiclass (135 categories)
and multilabel (the mean number of categories as-
signed to a document is 1.2) dataset. Many ex-
periments have been carried out for the Reuters
collection. However, they have been performed in
different experimental conditions. This makes re-
sults difficult to compare among them. In fact, ef-
fectiveness results can only be compared between
studies that use the same training and testing sets.
In order to lead researchers to use the same train-
ing/testing divisions, the Reuters documents have
been specifically tagged, and researchers are en-
couraged to use one of those divisions. In our
experiment we use the “ModApte” split (Lewis,
2004).
In this section, we analize the category sub-
sets, evaluation measures and results obtained in
the past and in the recent years for Reuters-21578
ModApte split.
</bodyText>
<subsectionHeader confidence="0.987199">
2.1 Category subsets
</subsectionHeader>
<bodyText confidence="0.9991227">
Concerning the evaluation of the classification
system, we restrict our attention to the TOPICS
group of categories that labels Reuters dataset,
which contains 135 categories. However, many
categories appear in no document and conse-
quently, and because inductive based learning
classifiers learn from training examples, these cat-
egories are not usually considered at evaluation
time. The most widely used subsets are the fol-
lowing:
</bodyText>
<listItem confidence="0.987958">
• Top-10: It is the set of the 10 categories
which have the highest number of documents
in the training set.
• R(90): It is the set of 90 categories which
have at least one document in the training set
and one in the testing set.
• R(115): It is the set of 115 categories which
have at least one document in the training set.
</listItem>
<bodyText confidence="0.9996865">
In order to analyze the relative hardness of the
three category subsets, a very recent paper has
been published by Debole and Sebastiani (Debole
and Sebastiani, 2005) where a systematic, compar-
ative experimental study has been carried out.
The results of the classification system we pro-
pose are evaluated according to these three cate-
gory subsets.
</bodyText>
<subsectionHeader confidence="0.996111">
2.2 Evaluation measures
</subsectionHeader>
<bodyText confidence="0.999905136363636">
The evaluation of a text categorization system is
usually done experimentally, by measuring the ef-
fectiveness, i.e. average correctness of the catego-
rization. In binary text categorization, two known
statistics are widely used to measure this effective-
ness: precision and recall. Precision (Prec) is the
percentage of documents correctly classified into a
given category, and recall (Rec) is the percentage
of documents belonging to a given category that
are indeed classified into it.
In general, there is a trade-off between preci-
sion and recall. Thus, a classifier is usually evalu-
ated by means of a measure which combines pre-
cision and recall. Various such measures have
been proposed. The breakeven point, the value at
which precision equals recall, has been frequently
used during the past decade. However, it has
been recently criticized by its proposer ((Sebas-
tiani, 2002) footnote 19). Nowadays, the F1 score
is more frequently used. The F1 score combines
recall and precision with an equal weight in the
following way:
</bodyText>
<footnote confidence="0.949629666666667">
&apos;Categorization problems where there are more than two F1 = 2 · Prec · Rec
possible categories.
Prec + Rec
</footnote>
<page confidence="0.995566">
26
</page>
<bodyText confidence="0.999568">
Since precision and recall are defined only for
binary classification tasks, for multiclass problems
results need to be averaged to get a single perfor-
mance value. This will be done using microav-
eraging and macroaveraging. In microaveraging,
which is calculated by globally summing over all
individual cases, categories count proportionally
to the number of their positive testing examples.
In macroaveraging, which is calculated by aver-
aging over the results of the different categories,
all categories count the same. See (Debole and
Sebastiani, 2005; Yang, 1999) for more detailed
explanation of the evaluation measures mentioned
above.
</bodyText>
<subsectionHeader confidence="0.996958">
2.3 Comparative Results
</subsectionHeader>
<bodyText confidence="0.999839777777778">
Sebastiani (Sebastiani, 2002) presents a table
where lists results of experiments for various train-
ing/testing divisions of Reuters. Although we are
aware that the results listed are microaveraged
breakeven point measures, and consequently, are
not directly comparable to the ones we present in
this paper, F1, we want to remark some of them.
In Table 1 we summarize the best results reported
for the ModApte split listed by Sebastiani.
</bodyText>
<table confidence="0.99290575">
Results reported by R(90) Top-10
(Joachims, 1998) 86.4
(Dumais et al., 1998) 87.0 92.0
(Weiss et.al., 1999) 87.8
</table>
<tableCaption confidence="0.999635">
Table 1: Microaveraged breakeven point results
</tableCaption>
<bodyText confidence="0.896872333333333">
reported by Sebastiani for the Reuters-21578
ModApte split.
In Table 2 we include some more recent re-
sults, evaluated according to the microaveraged
F1 score. For R(115) there is also a good result,
F1 = 87.2, obtained by (Zhang and Oles, 2001)2.
</bodyText>
<sectionHeader confidence="0.981394" genericHeader="method">
3 Proposed Approach
</sectionHeader>
<bodyText confidence="0.980090111111111">
In this paper we propose a multiclassifier based
document categorization system. Documents in
the training and testing sets are represented in a
reduced dimensional vector space. Different train-
ing databases are generated from the original train-
2Actually, this result is obtained for 118 categories which
correspond to the 115 mentioned before and three more cat-
egories which have testing documents but no training docu-
ment assigned.
</bodyText>
<table confidence="0.98635425">
Results reported by R(90) Top-10
(Gao et al., 2003) 88.42 93.07
(Kim et al., 2005) 87.11 92.21
(Gliozzo and Strapparava, 2005) 92.80
</table>
<tableCaption confidence="0.905162">
Table 2: F1 results reported for the Reuters-21578
ModApte split.
</tableCaption>
<bodyText confidence="0.9983196">
ing dataset in order to construct the multiclassifier.
We use the k-NN classification algorithm, which
according to each training database makes a pre-
diction for testing documents. Finally, a Bayesian
voting scheme is used in order to definitively as-
sign category labels to testing documents.
In the rest of this section we make a brief re-
view of the SVD dimensionality reduction tech-
nique, the k-NN algorithm and the combination of
classifiers used.
</bodyText>
<subsectionHeader confidence="0.961567">
3.1 The SVD Dimensionality Reduction
Technique
</subsectionHeader>
<bodyText confidence="0.999896923076923">
The classical Vector Space Model (VSM) has been
successfully employed to represent documents in
text categorization tasks. The newer method of
Latent Semantic Indexing (LSI) 3 (Deerwester et
al., 1990) is a variant of the VSM in which doc-
uments are represented in a lower dimensional
space created from the input training dataset. It
is based on the assumption that there is some
underlying latent semantic structure in the term-
document matrix that is corrupted by the wide va-
riety of words used in documents. This is referred
to as the problem of polysemy and synonymy. The
basic idea is that if two document vectors represent
two very similar topics, many words will co-occur
on them, and they will have very close semantic
structures after dimension reduction.
The SVD technique used by LSI consists in fac-
toring term-document matrix M into the product
of three matrices, M = UEVT where E is a di-
agonal matrix of singular values in non-increasing
order, and U and V are orthogonal matrices of sin-
gular vectors (term and document vectors, respec-
tively). Matrix M can be approximated by a lower
rank Mp which is calculated by using the p largest
singular values of M. This operation is called
dimensionality reduction, and the p-dimensional
</bodyText>
<footnote confidence="0.968082">
3http://lsi.research.telcordia.com,
http://www.cs.utk.edu/∼lsi
</footnote>
<page confidence="0.998182">
27
</page>
<bodyText confidence="0.979764888888889">
space to which document vectors are projected is
called the reduced space. Choosing the right di-
mension p is required for successful application
of the LSI/SVD technique. However, since there
is no theoretical optimum value for it, potentially
expensive experimentation may be required to de-
termine it (Berry and Browne, 1999).
For document categorization purposes (Dumais,
2004), the testing document q is also projected to
the p-dimensional space, qp = qT UpΣ−1
p , and the
cosine is usually calculated to measure the seman-
tic similarity between training and testing docu-
ment vectors.
In Figure 1 we can see an ilustration of the doc-
ument vector projection. Documents in the train-
ing collection are represented by using the term-
document matrix M, and each one of the docu-
ments is represented by a vector in the R m vec-
tor space like in the traditional vector space model
(VSM) scheme. Afterwards, the dimension p is se-
lected, and by applying SVD vectors are projected
to the reduced space. Documents in the testing
collection will also be projected to the same re-
duced space.
Figure 1: Vectors in the VSM are projected to the
reduced space by using SVD.
</bodyText>
<subsectionHeader confidence="0.9449295">
3.2 The k nearest neighbor classification
algorithm (k-NN)
</subsectionHeader>
<bodyText confidence="0.99956735">
k-NN is a distance based classification approach.
According to this approach, given an arbitrary test-
ing document, the k-NN classifier ranks its near-
est neighbors among the training documents, and
uses the categories of the k top-ranking neighbors
to predict the categories of the testing document
(Dasarathy, 1991). In this paper, the training and
testing documents are represented as reduced di-
mensional vectors in the lower dimensional space,
and in order to find the nearest neighbors of a
given document, we calculate the cosine similar-
ity measure.
In Figure 2 an ilustration of this phase can be
seen, where some training documents and a test-
ing document q are projected in the R p reduced
space. The nearest to the qp testing document are
considered to be the vectors which have the small-
est angle with qp. According to the category labels
of the nearest documents, a category label predic-
tion, c, will be made for testing document q.
</bodyText>
<figureCaption confidence="0.8792845">
Figure 2: The k-NN classifier is applied to qp test-
ing document and c category label is predicted.
</figureCaption>
<bodyText confidence="0.9999751">
We have decided to use the k-NN classifier be-
cause it has been found that on the Reuters-21578
database it performs best among the conventional
methods (Joachims, 1998; Yang, 1999) and be-
cause we have obtained good results in our pre-
vious work on text categorization for documents
written in Basque, a highly inflected language (Ze-
laia et al., 2005). Besides, the k-NN classification
algorithm can be easily adapted to multilabel cat-
egorization problems such as Reuters.
</bodyText>
<subsectionHeader confidence="0.999935">
3.3 Combination of classifiers
</subsectionHeader>
<bodyText confidence="0.999900866666667">
The combination of multiple classifiers has been
intensively studied with the aim of improving the
accuracy of individual components (Ho et al.,
1994). Two widely used techniques to implement
this approach are bagging (Breiman, 1996), that
uses more than one model of the same paradigm;
and boosting (Freund and Schapire, 1999), in
which a different weight is given to different train-
ing examples looking for a better accuracy.
In our experiment we have decided to construct
a multiclassifier via bagging. In bagging, a set of
training databases TDi is generated by selecting n
training examples drawn randomly with replace-
ment from the original training database TD of n
examples. When a set of n1 training examples,
</bodyText>
<figure confidence="0.998090885714286">
Rp
Reuters-21578, ModApte, Training
d7
R m
d9603 d1
d9603
d7
.
.
.
..
.
d6 d5
d1 d2 d9603
d6
d1 d2
d5
d4
d3
d2
d3
d4
M
Mp = UpΣpVpT
SVD
VSM
d509
qp
d135
c
k−NN
d23
d61
d34
Rp
</figure>
<page confidence="0.995601">
28
</page>
<bodyText confidence="0.999976290322581">
ni &lt; n, is chosen from the original training col-
lection, the bagging is said to be applied by ran-
dom subsampling. This is the approach used in our
work. The ni parameter has been selected via tun-
ing. In Section 4.3 the selection will be explained
in a more extended way.
According to the random subsampling, given a
testing document q, the classifier will make a la-
bel prediction cZ based on each one of the train-
ing databases TDZ. One way to combine the pre-
dictions is by Bayesian voting (Dietterich, 1998),
where a confidence value cvZcj is calculated for
each training database TDZ and category cj to be
predicted. These confidence values have been cal-
culated based on the original training collection.
Confidence values are summed by category. The
category cj that gets the highest value is finally
proposed as a prediction for the testing document.
In Figure 3 an ilustration of the whole ex-
periment can be seen. First, vectors in the
VSM are projected to the reduced space by using
SVD. Next, random subsampling is applied to the
training database TD to obtain different training
databases TDZ. Afterwards the k-NN classifier is
applied for each TDZ to make category label pre-
dictions. Finally, Bayesian voting is used to com-
bine predictions, and cj, and in some cases ck as
well, will be the final category label prediction of
the categorization system for testing document q.
In Section 4.3 the cases when a second category
label prediction ck is given are explained.
</bodyText>
<figureCaption confidence="0.9754455">
Figure 3: Proposed approach for multilabel docu-
ment categorization tasks.
</figureCaption>
<sectionHeader confidence="0.99857" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999778">
The aim of this section is to describe the document
collection used in our experiment and to give an
account of the preprocessing techniques and pa-
rameter settings we have applied.
When machine learning and other approaches
are applied to text categorization problems, a com-
mon technique has been to decompose the mul-
ticlass problem into multiple, independent binary
classification problems. In this paper, we adopt a
different approach. We will be primarily interested
in a classifier which produces a ranking of possi-
ble labels for a given document, with the hope that
the appropriate labels will appear at the top of the
ranking.
</bodyText>
<subsectionHeader confidence="0.985413">
4.1 Document Collection
</subsectionHeader>
<bodyText confidence="0.9999595">
As previously mentioned, the experiment reported
in this paper has been carried out for the Reuters-
21578 dataset4 compiled by David Lewis and orig-
inally collected by the Carnegie group from the
Reuters newswire in 1987. We use one of the
most widely used training/testing divisions, the
“ModApte” split, in which 75 % of the documents
(9,603 documents) are selected for training and the
remaining 25 % (3299 documents) to test the ac-
curacy of the classifier.
Document distribution over categories in both
the training and the testing sets is very unbalanced:
the 10 most frequent categories, top-10, account
75% of the training documents; the rest is dis-
tributed among the other 108 categories.
According to the number of labels assigned to
each document, many of them (19% in training
and 8.48% in testing) are not assigned to any cat-
egory, and some of them are assigned to 12. We
have decided to keep the unlabeled documents in
both the training and testing collections, as it is
suggested in (Lewis, 2004)5.
</bodyText>
<subsectionHeader confidence="0.984628">
4.2 Preprocessing
</subsectionHeader>
<bodyText confidence="0.9999045">
The original format of the text documents is in
SGML. We perform some preprocessing to fil-
ter out the unused parts of a document. We pre-
served only the title and the body text, punctua-
tion and numbers have been removed and all let-
ters have been converted to lowercase. We have
</bodyText>
<footnote confidence="0.972944375">
4http://daviddlewis.com/resources/testcollections
5In the ”ModApte” Split section it is suggested as fol-
lows: “If you are using a learning algorithm that requires
each training document to have at least TOPICS category,
you can screen out the training documents with no TOPICS
categories. Please do NOT screen out any of the 3,299 docu-
ments - that will make your results incomparable with other
studies.”
</footnote>
<figure confidence="0.988591">
Random
Subsampling
R p
d7
R m R m
d1 d2
d509
Reuters−21578, ModApte, Train
d9603 d1
c1 c2 c30
cj ,(ck)
d9603
d7
..
.
qp
d135
..
.
d6 d5
d6
d1 d2 d9603
k
−
k−NN
d5
d23
d4
d3
d2
d3
d4
TD1 TD2 TD30
NN k−NN
d61
d34
R p R p R p
TD
qp
M
Mp=UpEpVpT
SVD
VSM
Bayesian voting
d256
d638
d98
d50
d778
d848
q
VSM
Reuters−21578, ModApte, Test
qp
d9
q1 q2 q3299
d4612
q
qp=qT UpE−1
p
d33
d2787
d1989
d55
</figure>
<page confidence="0.997643">
29
</page>
<bodyText confidence="0.9991102">
used the tools provided in the web6 in order to ex-
tract text and categories from each document. We
have stemmed the training and testing documents
by using the Porter stemmer (Porter, 1980)7. By
using it, case and flection information are removed
from words. Consequently, the same experiment
has been carried out for the two forms of the doc-
ument collection: word-forms and Porter stems.
According to the dimension reduction, we have
created the matrices for the two mentioned doc-
ument collection forms. The sizes of the train-
ing matrices created are 15591 × 9603 for word-
forms and 11114 × 9603 for Porter stems. Differ-
ent number of dimensions have been experimented
(p = 100, 300, 500, 700).
</bodyText>
<subsectionHeader confidence="0.989334">
4.3 Parameter setting
</subsectionHeader>
<bodyText confidence="0.9999425">
We have designed our experiment in order to op-
timize the microaveraged F1 score. Based on pre-
vious experiments (Zelaia et al., 2005), we have
set parameter k for the k-NN algorithm to k = 3.
This way, the k-NN classifier will give a category
label prediction based on the categories of the 3
nearest ones.
On the other hand, we also needed to decide
the number of training databases TDi to create. It
has to be taken into account that a high number of
training databases implies an increasing computa-
tional cost for the final classification system. We
decided to create 30 training databases. However,
this is a parameter that has not been optimized.
There are two other parameters which have been
tuned: the size of each training database and the
threshold for multilabeling. We now briefly give
some cues about the tuning performed.
</bodyText>
<subsubsectionHeader confidence="0.541491">
4.3.1 The size of the training databases
</subsubsectionHeader>
<bodyText confidence="0.999949363636363">
As we have previously mentioned, documents
have been randomly selected from the original
training database in order to construct the 30 train-
ing databases TDi used in our classification sys-
tem. There are n = 9, 603 documents in the orig-
inal Reuters training collection. We had to decide
the number of documents to select in order to con-
struct each TDi. The number of documents se-
lected from each category preserves the propor-
tion of documents in the original one. We have
experimented to select different numbers n1 &lt; n
</bodyText>
<footnote confidence="0.994239666666667">
6http://www.lins.fju.edu.tw/∼tseng/Collections/Reuters-
21578.html
7http://tartarus.org/martin/PorterStemmer/
</footnote>
<bodyText confidence="0.998108769230769">
of documents, according to the following formula:
where ti is the total number of training documents
in category i. In Figure 4 it can be seen the vari-
ation of the n1 parameter depending on the value
of parameter j. We have experimented different j
values, and evaluated the results. Based on the re-
sults obtained we decided to select j = 60, which
means that each one of the 30 training databases
will have n1 = 298 documents. As we can see,
the final classification system will be using train-
ing databases which are quite smaller that the orig-
inal one. This gives a lower computational cost,
and makes the classification system faster.
</bodyText>
<figure confidence="0.533074">
Parameter j
</figure>
<figureCaption confidence="0.8519845">
Figure 4: Random subsampling rate.
4.3.2 Threshold for multilabeling
</figureCaption>
<bodyText confidence="0.970969">
The k-NN algorithm predicts a unique cate-
gory label for each testing document, based on the
ranked list of categories obtained for each training
database TDi8. As previously mentioned, we use
Bayesian voting to combine the predictions.
The Reuters-21578 is a multilabel database, and
therefore, we had to decide in which cases to as-
sign a second category label to a testing document.
Given that cj is the category with the highest value
in Bayesian voting and ck the next one, the second
ck category label will be assigned when the fol-
lowing relation is true:
cvck &gt; cvcj × r, r = 0.1, 0.2, ... , 0.9,1
In Figure 5 we can see the mean number of cate-
gories assigned to a document for different values
8It has to be noted that unlabeled documents have been
preserved, and thus, our classification system treats unlabeled
documents as documents of a new category
</bodyText>
<figure confidence="0.990510263157895">
1100
1000
900
Parameter n1
800
700
600
500
400
300
200
10 20 30 40 50 60 70
2 +ti
j ,
j = 10, 20, ... , 70,
1 1 5
�
i=1
n1 =
</figure>
<page confidence="0.989642">
30
</page>
<bodyText confidence="0.998393333333333">
of r. Results obtained were evaluated and based
on them we decided to select r = 0.4, which cor-
responds to a ratio of 1.05 categories.
</bodyText>
<figure confidence="0.703888">
Parameter r
</figure>
<figureCaption confidence="0.994824">
Figure 5: Threshold for multilabeling.
</figureCaption>
<sectionHeader confidence="0.998687" genericHeader="method">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.999974086956522">
In Table 3 microaveraged F1 scores obtained
in our experiment are shown. As it could be
expected, a simple stemming process increases
slightly results, and it can be observed that the best
result for the three category subsets has been ob-
tained for the stemmed corpus, even though gain
is low (less than 0.6).
The evaluation for the Top-10 category subset
gives the best results, reaching up to 93.57%. In
fact, this is the expected behavior, as the number of
categories to be evaluated is small and the number
of documents in each category is high. For this
subset the best result has been obtained for 100
dimensions, although the variation is low among
results for 100, 300 and 500 dimensions. When
using higher dimensions results become poorer.
According to the R(90) and R(115) subsets, the
best results are 87.27% and 87.01% respectively.
Given that the difficulty of these subsets is quite
similar, their behavior is also analogous. As we
can see in the table, most of the best results for
these subsets have been obtained by reducing the
dimension of the space to 500.
</bodyText>
<sectionHeader confidence="0.999055" genericHeader="method">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.982356428571429">
In this paper we present an approach for multilabel
document categorization problems which consists
in a multiclassifier system based on the k-NN al-
gorithm. The documents are represented in a re-
duced dimensional space calculated by SVD. We
want to emphasize that, due to the multilabel char-
acter of the database used, we have adapted the
</bodyText>
<table confidence="0.99985475">
Corpus 100 Dimension reduction
300 500 700
Words(10) 93.06 93.17 93.44 92.00
Porter(10) 93.57 93.20 93.50 92.57
Words(90) 84.90 86.71 87.09 86.18
Porter(90) 85.34 86.64 87.27 86.30
Words(115) 84.66 86.44 86.73 85.84
Porter(115) 85.13 86.47 87.01 86.00
</table>
<tableCaption confidence="0.938919">
Table 3: Microaveraged F1 scores for Reuters-
21578 ModApte split.
</tableCaption>
<bodyText confidence="0.997946833333333">
classification system in order for it to be multilabel
too. The learning of the system has been unique
(9603 training documents) and the category label
predictions made by the classifier have been eval-
uated on the testing set according to the three cat-
egory sets: top-10, R(90) and R(115). The mi-
croaveraged F1 scores we obtain are among the
best reported for the Reuters-21578.
As future work, we want to experiment with
generating more than 30 training databases, and
in a preliminary phase select the best among them.
The predictions made using the selected training
databases will be combined to obtain the final pre-
dictions.
When there is a low number of documents avail-
able for a given category, the power of LSI gets
limited to create a space that reflects interesting
properties of the data. As future work we want
to include background text in the training col-
lection and use an expanded term-document ma-
trix that includes, besides the 9603 training doc-
uments, some other relevant texts. This may in-
crease results, specially for the categories with less
documents (Zelikovitz and Hirsh, 2001).
In order to see the consistency of our classi-
fier, we also plan to repeat the experiment for the
RCV1 (Lewis et al., 2004), a new benchmark col-
lection for text categorization tasks which consists
of 800,000 manually categorized newswire stories
recently made available by Reuters.
</bodyText>
<sectionHeader confidence="0.998221" genericHeader="method">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.991698">
This research was supported by the Univer-
sity of the Basque Country (UPV00141.226-T-
15948/2004) and Gipuzkoa Council in a European
</bodyText>
<figure confidence="0.995172333333333">
1.16
1.14
1.12
1.1
1.08
1.06
1.04
1.02
1
0.98
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Multilabeling Ratio
</figure>
<page confidence="0.999493">
31
</page>
<sectionHeader confidence="0.8357175" genericHeader="conclusions">
Union Program.
References
</sectionHeader>
<reference confidence="0.999932142857143">
Berry, M.W. and Browne, M.: Understanding Search
Engines: Mathematical Modeling and Text Re-
trieval. SIAM Society for Industrial and Applied
Mathematics, ISBN: 0-89871-437-0, Philadelphia,
(1999)
Breiman, L.: Bagging Predictors. Machine Learning,
24(2), 123–140, (1996)
Cristianini, N., Shawe-Taylor, J. and Lodhi, H.: Latent
Semantic Kernels. Proceedings of ICML’01, 18th
International Conference on Machine Learning, 66–
73, Morgan Kaufmann Publishers, (2001)
Dasarathy, B.V.: Nearest Neighbor (NN) Norms:
NN Pattern Recognition Classification Techniques.
IEEE Computer Society Press, (1991)
Debole, F. and Sebastiani, F.: An Analysis of the Rela-
tive Hardness of Reuters-21578 Subsets. Journal of
the American Society for Information Science and
Technology, 56(6),584–596, (2005)
Deerwester, S., Dumais, S.T., Furnas, G.W., Landauer,
T.K. and Harshman, R.: Indexing by Latent Seman-
tic Analysis. Journal of the American Society for
Information Science, 41, 391–407, (1990)
Dietterich, T.G.: Machine-Learning Research: Four
Current Directions. The AI Magazine, 18(4), 97–
136, (1998)
Dumais, S.T., Platt, J., Heckerman, D. and Sahami,
M.: Inductive Learning Algorithms and Repre-
sentations for Text Categorization. Proceedings of
CIKM’98: 7th International Conference on Infor-
mation and Knowledge Management, ACM Press,
148–155 (1998)
Dumais, S.: Latent Semantic Analysis. ARIST, An-
nual Review of Information Science Technology, 38,
189–230, (2004)
Freund, Y. and Schapire, R.E.: A Short Introduction to
Boosting. Journal of Japanese Society for Artificial
Intelligence, 14(5), 771-780, (1999)
Gao, S., Wu, W., Lee, C.H. and Chua, T.S.: A Maxi-
mal Figure-of-Merit Learning Approach to Text Cat-
egorization. Proceedings of SIGIR’03: 26th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
174–181, ACM Press, (2003)
Gliozzo, A. and Strapparava, C.: Domain Kernels
for Text Categorization. Proceedings of CoNLL’05:
9th Conference on Computational Natural Language
Learning, 56–63, (2005)
Ho, T.K., Hull, J.J. and Srihari, S.N.: Decision Combi-
nation in Multiple Classifier Systems. IEEE Trans-
actions on Pattern Analysis and Machine Intelli-
gence, 16(1), 66–75, (1994)
Joachims, T. Text Categorization with Support Vector
Machines: Learning with Many Relevant Features.
Proceedings of ECML’98: 10th European Confer-
ence on Machine Learning, Springer 1398, 137–142,
(1998)
Kim, H., Howland, P. and Park, H.: Dimension Re-
duction in Text Classification with Support Vector
Machines. Journal of Machine Learning Research,
6, 37–53, MIT Press, (2005)
Lewis, D.D.: Reuters-21578 Text Catego-
rization Test Collection, Distribution 1.0.
http://daviddlewis.com/resources/testcollections
README file (v 1.3), (2004)
Lewis, D.D., Yang, Y., Rose, T.G. and Li, F.: RCV1: A
New Benchmark Collection for Text Categorization
Research. Journal of Machine Learning Research,
5, 361–397, (2004)
Porter, M.F.: An Algorithm for Suffix Stripping. Pro-
gram,14(3), 130–137, (1980)
Salton, G. and McGill, M.: Introduction to Modern
Information Retrieval. McGraw-Hill, New York,
(1983)
Sebastiani, F.: Machine Learning in Automated Text
Categorization. ACM Computing Surveys, 34(1),
1–47, (2002)
Weiss, S.M., Apte, C., Damerau, F.J., Johnson, D.E.,
Oles, F.J., Goetz, T. and Hampp, T.: Maximizing
Text-Mining Performance. IEEE Intelligent Sys-
tems, 14(4),63–69, (1999)
Yang, Y. An Evaluation of Statistical Approaches to
Text Categorization. Journal of Information Re-
trieval. Kluwer Academic Publishers, 1,(1/2), 69–
90, (1999)
Zelaia, A., Alegria, I., Arregi, O. and Sierra, B.: An-
alyzing the Effect of Dimensionality Reduction in
Document Categorization for Basque. Proceedings
of L&amp;TC’05: 2nd Language &amp; Technology Confer-
ence, 72–75, (2005)
Zelikovitz, S. and Hirsh, H.: Using LSI for Text
Classification in the Presence of Background Text.
Proceedings of CIKM’01: 10th ACM International
Conference on Information and Knowledge Man-
agement, ACM Press, 113–118, (2001)
Zhang, T. and Oles, F.J.: Text Categorization Based
on Regularized Linear Classification Methods. In-
formation Retrieval, 4(1): 5–31, Kluwer Academic
Publishers, (2001)
</reference>
<page confidence="0.999298">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.179338">
<title confidence="0.9895815">A Multiclassifier based Document Categorization System: profiting from the Singular Value Decomposition Dimensionality Reduction Technique</title>
<author confidence="0.99507">Zelaia Alegria Olatz Arregi Basilio Sierra</author>
<affiliation confidence="0.971636">UPV-EHU UPV-EHU UPV-EHU UPV-EHU</affiliation>
<note confidence="0.3321085">Basque Country Basque Country Basque Country Basque Country ccpzejaa@si.ehu.es acpalloi@si.ehu.es acparuro@si.ehu.es ccpsiarb@si.ehu.es</note>
<abstract confidence="0.998666055555556">In this paper we present a multiclassifier approach for multilabel document classifiproblems, where a set of classifiers is used to predict the category of text documents based on different training subsampling databases. These databases are obtained from the original training database by random subsampling. In order to combine the predictions generated by the multiclassifier, Bayesian voting is applied. Through all the classification process, a reduced dimension vector representation obtained by Singular Value Decomposition (SVD) is used for training and testing documents. The good results of our experiments give an indication of the potentiality of the proposed approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M W Berry</author>
<author>M Browne</author>
</authors>
<title>Understanding Search Engines: Mathematical Modeling and Text Retrieval.</title>
<date>1999</date>
<journal>SIAM Society for Industrial and Applied Mathematics, ISBN:</journal>
<pages>0--89871</pages>
<location>Philadelphia,</location>
<contexts>
<context position="12239" citStr="Berry and Browne, 1999" startWordPosition="1916" endWordPosition="1919">s (term and document vectors, respectively). Matrix M can be approximated by a lower rank Mp which is calculated by using the p largest singular values of M. This operation is called dimensionality reduction, and the p-dimensional 3http://lsi.research.telcordia.com, http://www.cs.utk.edu/∼lsi 27 space to which document vectors are projected is called the reduced space. Choosing the right dimension p is required for successful application of the LSI/SVD technique. However, since there is no theoretical optimum value for it, potentially expensive experimentation may be required to determine it (Berry and Browne, 1999). For document categorization purposes (Dumais, 2004), the testing document q is also projected to the p-dimensional space, qp = qT UpΣ−1 p , and the cosine is usually calculated to measure the semantic similarity between training and testing document vectors. In Figure 1 we can see an ilustration of the document vector projection. Documents in the training collection are represented by using the termdocument matrix M, and each one of the documents is represented by a vector in the R m vector space like in the traditional vector space model (VSM) scheme. Afterwards, the dimension p is selected</context>
</contexts>
<marker>Berry, Browne, 1999</marker>
<rawString>Berry, M.W. and Browne, M.: Understanding Search Engines: Mathematical Modeling and Text Retrieval. SIAM Society for Industrial and Applied Mathematics, ISBN: 0-89871-437-0, Philadelphia, (1999)</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Breiman</author>
</authors>
<title>Bagging Predictors.</title>
<date>1996</date>
<booktitle>Machine Learning,</booktitle>
<volume>24</volume>
<issue>2</issue>
<pages>123--140</pages>
<contexts>
<context position="14904" citStr="Breiman, 1996" startWordPosition="2366" endWordPosition="2367">among the conventional methods (Joachims, 1998; Yang, 1999) and because we have obtained good results in our previous work on text categorization for documents written in Basque, a highly inflected language (Zelaia et al., 2005). Besides, the k-NN classification algorithm can be easily adapted to multilabel categorization problems such as Reuters. 3.3 Combination of classifiers The combination of multiple classifiers has been intensively studied with the aim of improving the accuracy of individual components (Ho et al., 1994). Two widely used techniques to implement this approach are bagging (Breiman, 1996), that uses more than one model of the same paradigm; and boosting (Freund and Schapire, 1999), in which a different weight is given to different training examples looking for a better accuracy. In our experiment we have decided to construct a multiclassifier via bagging. In bagging, a set of training databases TDi is generated by selecting n training examples drawn randomly with replacement from the original training database TD of n examples. When a set of n1 training examples, Rp Reuters-21578, ModApte, Training d7 R m d9603 d1 d9603 d7 . . . .. . d6 d5 d1 d2 d9603 d6 d1 d2 d5 d4 d3 d2 d3 d</context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>Breiman, L.: Bagging Predictors. Machine Learning, 24(2), 123–140, (1996)</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Cristianini</author>
<author>J Shawe-Taylor</author>
<author>H Lodhi</author>
</authors>
<title>Latent Semantic Kernels.</title>
<date>2001</date>
<booktitle>Proceedings of ICML’01, 18th International Conference on Machine Learning, 66– 73,</booktitle>
<publisher>Morgan Kaufmann Publishers,</publisher>
<marker>Cristianini, Shawe-Taylor, Lodhi, 2001</marker>
<rawString>Cristianini, N., Shawe-Taylor, J. and Lodhi, H.: Latent Semantic Kernels. Proceedings of ICML’01, 18th International Conference on Machine Learning, 66– 73, Morgan Kaufmann Publishers, (2001)</rawString>
</citation>
<citation valid="true">
<authors>
<author>B V Dasarathy</author>
</authors>
<title>Nearest Neighbor (NN) Norms: NN Pattern Recognition Classification Techniques.</title>
<date>1991</date>
<publisher>IEEE Computer Society Press,</publisher>
<contexts>
<context position="13443" citStr="Dasarathy, 1991" startWordPosition="2121" endWordPosition="2122"> is selected, and by applying SVD vectors are projected to the reduced space. Documents in the testing collection will also be projected to the same reduced space. Figure 1: Vectors in the VSM are projected to the reduced space by using SVD. 3.2 The k nearest neighbor classification algorithm (k-NN) k-NN is a distance based classification approach. According to this approach, given an arbitrary testing document, the k-NN classifier ranks its nearest neighbors among the training documents, and uses the categories of the k top-ranking neighbors to predict the categories of the testing document (Dasarathy, 1991). In this paper, the training and testing documents are represented as reduced dimensional vectors in the lower dimensional space, and in order to find the nearest neighbors of a given document, we calculate the cosine similarity measure. In Figure 2 an ilustration of this phase can be seen, where some training documents and a testing document q are projected in the R p reduced space. The nearest to the qp testing document are considered to be the vectors which have the smallest angle with qp. According to the category labels of the nearest documents, a category label prediction, c, will be ma</context>
</contexts>
<marker>Dasarathy, 1991</marker>
<rawString>Dasarathy, B.V.: Nearest Neighbor (NN) Norms: NN Pattern Recognition Classification Techniques. IEEE Computer Society Press, (1991)</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Debole</author>
<author>F Sebastiani</author>
</authors>
<title>An Analysis of the Relative Hardness of Reuters-21578 Subsets.</title>
<date>2005</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>56</volume>
<issue>6</issue>
<contexts>
<context position="6600" citStr="Debole and Sebastiani, 2005" startWordPosition="1023" endWordPosition="1026">from training examples, these categories are not usually considered at evaluation time. The most widely used subsets are the following: • Top-10: It is the set of the 10 categories which have the highest number of documents in the training set. • R(90): It is the set of 90 categories which have at least one document in the training set and one in the testing set. • R(115): It is the set of 115 categories which have at least one document in the training set. In order to analyze the relative hardness of the three category subsets, a very recent paper has been published by Debole and Sebastiani (Debole and Sebastiani, 2005) where a systematic, comparative experimental study has been carried out. The results of the classification system we propose are evaluated according to these three category subsets. 2.2 Evaluation measures The evaluation of a text categorization system is usually done experimentally, by measuring the effectiveness, i.e. average correctness of the categorization. In binary text categorization, two known statistics are widely used to measure this effectiveness: precision and recall. Precision (Prec) is the percentage of documents correctly classified into a given category, and recall (Rec) is t</context>
<context position="8498" citStr="Debole and Sebastiani, 2005" startWordPosition="1322" endWordPosition="1325">e are more than two F1 = 2 · Prec · Rec possible categories. Prec + Rec 26 Since precision and recall are defined only for binary classification tasks, for multiclass problems results need to be averaged to get a single performance value. This will be done using microaveraging and macroaveraging. In microaveraging, which is calculated by globally summing over all individual cases, categories count proportionally to the number of their positive testing examples. In macroaveraging, which is calculated by averaging over the results of the different categories, all categories count the same. See (Debole and Sebastiani, 2005; Yang, 1999) for more detailed explanation of the evaluation measures mentioned above. 2.3 Comparative Results Sebastiani (Sebastiani, 2002) presents a table where lists results of experiments for various training/testing divisions of Reuters. Although we are aware that the results listed are microaveraged breakeven point measures, and consequently, are not directly comparable to the ones we present in this paper, F1, we want to remark some of them. In Table 1 we summarize the best results reported for the ModApte split listed by Sebastiani. Results reported by R(90) Top-10 (Joachims, 1998) 8</context>
</contexts>
<marker>Debole, Sebastiani, 2005</marker>
<rawString>Debole, F. and Sebastiani, F.: An Analysis of the Relative Hardness of Reuters-21578 Subsets. Journal of the American Society for Information Science and Technology, 56(6),584–596, (2005)</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>G W Furnas</author>
<author>T K Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by Latent Semantic Analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<pages>391--407</pages>
<contexts>
<context position="1704" citStr="Deerwester et al., 1990" startWordPosition="238" endWordPosition="241">ocument Categorization, the assignment of natural language texts to one or more predefined categories based on their content, is an important component in many information organization and management tasks. Researchers have concentrated their efforts in finding the appropriate way to represent documents, index them and construct classifiers to assign the correct categories to each document. Both, document representation and classification method are crucial steps in the categorization process. In this paper we concentrate on both issues. On the one hand, we use Latent Semantic Indexing (LSI) (Deerwester et al., 1990), which is a variant of the vector space model (VSM) (Salton and McGill, 1983), in order to obtain the vector representation of documents. This technique compresses vectors representing documents into vectors of a lower-dimensional space. LSI, which is based on Singular Value Decomposition (SVD) of matrices, has showed to have the ability to extract the relations among words and documents by means of their context of use, and has been successfully applied to Information Retrieval tasks. On the other hand, we construct a multiclassifier (Ho et al., 1994) which uses different training databases.</context>
<context position="10806" citStr="Deerwester et al., 1990" startWordPosition="1682" endWordPosition="1685">sification algorithm, which according to each training database makes a prediction for testing documents. Finally, a Bayesian voting scheme is used in order to definitively assign category labels to testing documents. In the rest of this section we make a brief review of the SVD dimensionality reduction technique, the k-NN algorithm and the combination of classifiers used. 3.1 The SVD Dimensionality Reduction Technique The classical Vector Space Model (VSM) has been successfully employed to represent documents in text categorization tasks. The newer method of Latent Semantic Indexing (LSI) 3 (Deerwester et al., 1990) is a variant of the VSM in which documents are represented in a lower dimensional space created from the input training dataset. It is based on the assumption that there is some underlying latent semantic structure in the termdocument matrix that is corrupted by the wide variety of words used in documents. This is referred to as the problem of polysemy and synonymy. The basic idea is that if two document vectors represent two very similar topics, many words will co-occur on them, and they will have very close semantic structures after dimension reduction. The SVD technique used by LSI consist</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Deerwester, S., Dumais, S.T., Furnas, G.W., Landauer, T.K. and Harshman, R.: Indexing by Latent Semantic Analysis. Journal of the American Society for Information Science, 41, 391–407, (1990)</rawString>
</citation>
<citation valid="true">
<authors>
<author>T G Dietterich</author>
</authors>
<title>Machine-Learning Research: Four Current Directions.</title>
<date>1998</date>
<journal>The AI Magazine,</journal>
<volume>18</volume>
<issue>4</issue>
<pages>97--136</pages>
<contexts>
<context position="16074" citStr="Dietterich, 1998" startWordPosition="2587" endWordPosition="2588">. . d6 d5 d1 d2 d9603 d6 d1 d2 d5 d4 d3 d2 d3 d4 M Mp = UpΣpVpT SVD VSM d509 qp d135 c k−NN d23 d61 d34 Rp 28 ni &lt; n, is chosen from the original training collection, the bagging is said to be applied by random subsampling. This is the approach used in our work. The ni parameter has been selected via tuning. In Section 4.3 the selection will be explained in a more extended way. According to the random subsampling, given a testing document q, the classifier will make a label prediction cZ based on each one of the training databases TDZ. One way to combine the predictions is by Bayesian voting (Dietterich, 1998), where a confidence value cvZcj is calculated for each training database TDZ and category cj to be predicted. These confidence values have been calculated based on the original training collection. Confidence values are summed by category. The category cj that gets the highest value is finally proposed as a prediction for the testing document. In Figure 3 an ilustration of the whole experiment can be seen. First, vectors in the VSM are projected to the reduced space by using SVD. Next, random subsampling is applied to the training database TD to obtain different training databases TDZ. Afterw</context>
</contexts>
<marker>Dietterich, 1998</marker>
<rawString>Dietterich, T.G.: Machine-Learning Research: Four Current Directions. The AI Magazine, 18(4), 97– 136, (1998)</rawString>
</citation>
<citation valid="true">
<authors>
<author>S T Dumais</author>
<author>J Platt</author>
<author>D Heckerman</author>
<author>M Sahami</author>
</authors>
<title>Inductive Learning Algorithms and Representations for Text Categorization.</title>
<date>1998</date>
<booktitle>Proceedings of CIKM’98: 7th International Conference on Information and Knowledge Management,</booktitle>
<pages>148--155</pages>
<publisher>ACM Press,</publisher>
<contexts>
<context position="9123" citStr="Dumais et al., 1998" startWordPosition="1418" endWordPosition="1421">g, 1999) for more detailed explanation of the evaluation measures mentioned above. 2.3 Comparative Results Sebastiani (Sebastiani, 2002) presents a table where lists results of experiments for various training/testing divisions of Reuters. Although we are aware that the results listed are microaveraged breakeven point measures, and consequently, are not directly comparable to the ones we present in this paper, F1, we want to remark some of them. In Table 1 we summarize the best results reported for the ModApte split listed by Sebastiani. Results reported by R(90) Top-10 (Joachims, 1998) 86.4 (Dumais et al., 1998) 87.0 92.0 (Weiss et.al., 1999) 87.8 Table 1: Microaveraged breakeven point results reported by Sebastiani for the Reuters-21578 ModApte split. In Table 2 we include some more recent results, evaluated according to the microaveraged F1 score. For R(115) there is also a good result, F1 = 87.2, obtained by (Zhang and Oles, 2001)2. 3 Proposed Approach In this paper we propose a multiclassifier based document categorization system. Documents in the training and testing sets are represented in a reduced dimensional vector space. Different training databases are generated from the original train2Act</context>
</contexts>
<marker>Dumais, Platt, Heckerman, Sahami, 1998</marker>
<rawString>Dumais, S.T., Platt, J., Heckerman, D. and Sahami, M.: Inductive Learning Algorithms and Representations for Text Categorization. Proceedings of CIKM’98: 7th International Conference on Information and Knowledge Management, ACM Press, 148–155 (1998)</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dumais</author>
</authors>
<title>Latent Semantic Analysis.</title>
<date>2004</date>
<journal>ARIST, Annual Review of Information Science Technology,</journal>
<volume>38</volume>
<contexts>
<context position="12292" citStr="Dumais, 2004" startWordPosition="1924" endWordPosition="1925">proximated by a lower rank Mp which is calculated by using the p largest singular values of M. This operation is called dimensionality reduction, and the p-dimensional 3http://lsi.research.telcordia.com, http://www.cs.utk.edu/∼lsi 27 space to which document vectors are projected is called the reduced space. Choosing the right dimension p is required for successful application of the LSI/SVD technique. However, since there is no theoretical optimum value for it, potentially expensive experimentation may be required to determine it (Berry and Browne, 1999). For document categorization purposes (Dumais, 2004), the testing document q is also projected to the p-dimensional space, qp = qT UpΣ−1 p , and the cosine is usually calculated to measure the semantic similarity between training and testing document vectors. In Figure 1 we can see an ilustration of the document vector projection. Documents in the training collection are represented by using the termdocument matrix M, and each one of the documents is represented by a vector in the R m vector space like in the traditional vector space model (VSM) scheme. Afterwards, the dimension p is selected, and by applying SVD vectors are projected to the re</context>
</contexts>
<marker>Dumais, 2004</marker>
<rawString>Dumais, S.: Latent Semantic Analysis. ARIST, Annual Review of Information Science Technology, 38, 189–230, (2004)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>A Short Introduction to Boosting.</title>
<date>1999</date>
<journal>Journal of Japanese Society for Artificial Intelligence,</journal>
<volume>14</volume>
<issue>5</issue>
<pages>771--780</pages>
<contexts>
<context position="14998" citStr="Freund and Schapire, 1999" startWordPosition="2380" endWordPosition="2383">tained good results in our previous work on text categorization for documents written in Basque, a highly inflected language (Zelaia et al., 2005). Besides, the k-NN classification algorithm can be easily adapted to multilabel categorization problems such as Reuters. 3.3 Combination of classifiers The combination of multiple classifiers has been intensively studied with the aim of improving the accuracy of individual components (Ho et al., 1994). Two widely used techniques to implement this approach are bagging (Breiman, 1996), that uses more than one model of the same paradigm; and boosting (Freund and Schapire, 1999), in which a different weight is given to different training examples looking for a better accuracy. In our experiment we have decided to construct a multiclassifier via bagging. In bagging, a set of training databases TDi is generated by selecting n training examples drawn randomly with replacement from the original training database TD of n examples. When a set of n1 training examples, Rp Reuters-21578, ModApte, Training d7 R m d9603 d1 d9603 d7 . . . .. . d6 d5 d1 d2 d9603 d6 d1 d2 d5 d4 d3 d2 d3 d4 M Mp = UpΣpVpT SVD VSM d509 qp d135 c k−NN d23 d61 d34 Rp 28 ni &lt; n, is chosen from the orig</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Freund, Y. and Schapire, R.E.: A Short Introduction to Boosting. Journal of Japanese Society for Artificial Intelligence, 14(5), 771-780, (1999)</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gao</author>
<author>W Wu</author>
<author>C H Lee</author>
<author>T S Chua</author>
</authors>
<title>A Maximal Figure-of-Merit Learning Approach to Text Categorization.</title>
<date>2003</date>
<booktitle>Proceedings of SIGIR’03: 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 174–181,</booktitle>
<publisher>ACM Press,</publisher>
<contexts>
<context position="9959" citStr="Gao et al., 2003" startWordPosition="1550" endWordPosition="1553">microaveraged F1 score. For R(115) there is also a good result, F1 = 87.2, obtained by (Zhang and Oles, 2001)2. 3 Proposed Approach In this paper we propose a multiclassifier based document categorization system. Documents in the training and testing sets are represented in a reduced dimensional vector space. Different training databases are generated from the original train2Actually, this result is obtained for 118 categories which correspond to the 115 mentioned before and three more categories which have testing documents but no training document assigned. Results reported by R(90) Top-10 (Gao et al., 2003) 88.42 93.07 (Kim et al., 2005) 87.11 92.21 (Gliozzo and Strapparava, 2005) 92.80 Table 2: F1 results reported for the Reuters-21578 ModApte split. ing dataset in order to construct the multiclassifier. We use the k-NN classification algorithm, which according to each training database makes a prediction for testing documents. Finally, a Bayesian voting scheme is used in order to definitively assign category labels to testing documents. In the rest of this section we make a brief review of the SVD dimensionality reduction technique, the k-NN algorithm and the combination of classifiers used. 3</context>
</contexts>
<marker>Gao, Wu, Lee, Chua, 2003</marker>
<rawString>Gao, S., Wu, W., Lee, C.H. and Chua, T.S.: A Maximal Figure-of-Merit Learning Approach to Text Categorization. Proceedings of SIGIR’03: 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 174–181, ACM Press, (2003)</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gliozzo</author>
<author>C Strapparava</author>
</authors>
<title>Domain Kernels for Text Categorization.</title>
<date>2005</date>
<booktitle>Proceedings of CoNLL’05: 9th Conference on Computational Natural Language Learning,</booktitle>
<volume>56</volume>
<contexts>
<context position="10034" citStr="Gliozzo and Strapparava, 2005" startWordPosition="1562" endWordPosition="1565">, F1 = 87.2, obtained by (Zhang and Oles, 2001)2. 3 Proposed Approach In this paper we propose a multiclassifier based document categorization system. Documents in the training and testing sets are represented in a reduced dimensional vector space. Different training databases are generated from the original train2Actually, this result is obtained for 118 categories which correspond to the 115 mentioned before and three more categories which have testing documents but no training document assigned. Results reported by R(90) Top-10 (Gao et al., 2003) 88.42 93.07 (Kim et al., 2005) 87.11 92.21 (Gliozzo and Strapparava, 2005) 92.80 Table 2: F1 results reported for the Reuters-21578 ModApte split. ing dataset in order to construct the multiclassifier. We use the k-NN classification algorithm, which according to each training database makes a prediction for testing documents. Finally, a Bayesian voting scheme is used in order to definitively assign category labels to testing documents. In the rest of this section we make a brief review of the SVD dimensionality reduction technique, the k-NN algorithm and the combination of classifiers used. 3.1 The SVD Dimensionality Reduction Technique The classical Vector Space Mo</context>
</contexts>
<marker>Gliozzo, Strapparava, 2005</marker>
<rawString>Gliozzo, A. and Strapparava, C.: Domain Kernels for Text Categorization. Proceedings of CoNLL’05: 9th Conference on Computational Natural Language Learning, 56–63, (2005)</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Ho</author>
<author>J J Hull</author>
<author>S N Srihari</author>
</authors>
<title>Decision Combination in Multiple Classifier Systems.</title>
<date>1994</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>16</volume>
<issue>1</issue>
<pages>66--75</pages>
<contexts>
<context position="2263" citStr="Ho et al., 1994" startWordPosition="333" endWordPosition="336"> Latent Semantic Indexing (LSI) (Deerwester et al., 1990), which is a variant of the vector space model (VSM) (Salton and McGill, 1983), in order to obtain the vector representation of documents. This technique compresses vectors representing documents into vectors of a lower-dimensional space. LSI, which is based on Singular Value Decomposition (SVD) of matrices, has showed to have the ability to extract the relations among words and documents by means of their context of use, and has been successfully applied to Information Retrieval tasks. On the other hand, we construct a multiclassifier (Ho et al., 1994) which uses different training databases. These databases are obtained from the original training set by random subsampling. We implement this approach by bagging, and use the k-NN classification algorithm to make the category predictions for testing documents. Finally, we combine all predictions made for a given document by Bayesian voting. The experiment we present has been evaluated for Reuters-21578 standard document collection. Reuters-21578 is a multilabel document collection, which means that categories are not mutually exclusive because the same document may be relevant to more than on</context>
<context position="14821" citStr="Ho et al., 1994" startWordPosition="2352" endWordPosition="2355">sifier because it has been found that on the Reuters-21578 database it performs best among the conventional methods (Joachims, 1998; Yang, 1999) and because we have obtained good results in our previous work on text categorization for documents written in Basque, a highly inflected language (Zelaia et al., 2005). Besides, the k-NN classification algorithm can be easily adapted to multilabel categorization problems such as Reuters. 3.3 Combination of classifiers The combination of multiple classifiers has been intensively studied with the aim of improving the accuracy of individual components (Ho et al., 1994). Two widely used techniques to implement this approach are bagging (Breiman, 1996), that uses more than one model of the same paradigm; and boosting (Freund and Schapire, 1999), in which a different weight is given to different training examples looking for a better accuracy. In our experiment we have decided to construct a multiclassifier via bagging. In bagging, a set of training databases TDi is generated by selecting n training examples drawn randomly with replacement from the original training database TD of n examples. When a set of n1 training examples, Rp Reuters-21578, ModApte, Train</context>
</contexts>
<marker>Ho, Hull, Srihari, 1994</marker>
<rawString>Ho, T.K., Hull, J.J. and Srihari, S.N.: Decision Combination in Multiple Classifier Systems. IEEE Transactions on Pattern Analysis and Machine Intelligence, 16(1), 66–75, (1994)</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text Categorization with Support Vector Machines: Learning with Many Relevant Features.</title>
<date>1998</date>
<booktitle>Proceedings of ECML’98: 10th European Conference on Machine Learning, Springer 1398,</booktitle>
<pages>137--142</pages>
<contexts>
<context position="9096" citStr="Joachims, 1998" startWordPosition="1415" endWordPosition="1416"> Sebastiani, 2005; Yang, 1999) for more detailed explanation of the evaluation measures mentioned above. 2.3 Comparative Results Sebastiani (Sebastiani, 2002) presents a table where lists results of experiments for various training/testing divisions of Reuters. Although we are aware that the results listed are microaveraged breakeven point measures, and consequently, are not directly comparable to the ones we present in this paper, F1, we want to remark some of them. In Table 1 we summarize the best results reported for the ModApte split listed by Sebastiani. Results reported by R(90) Top-10 (Joachims, 1998) 86.4 (Dumais et al., 1998) 87.0 92.0 (Weiss et.al., 1999) 87.8 Table 1: Microaveraged breakeven point results reported by Sebastiani for the Reuters-21578 ModApte split. In Table 2 we include some more recent results, evaluated according to the microaveraged F1 score. For R(115) there is also a good result, F1 = 87.2, obtained by (Zhang and Oles, 2001)2. 3 Proposed Approach In this paper we propose a multiclassifier based document categorization system. Documents in the training and testing sets are represented in a reduced dimensional vector space. Different training databases are generated </context>
<context position="14336" citStr="Joachims, 1998" startWordPosition="2278" endWordPosition="2279"> can be seen, where some training documents and a testing document q are projected in the R p reduced space. The nearest to the qp testing document are considered to be the vectors which have the smallest angle with qp. According to the category labels of the nearest documents, a category label prediction, c, will be made for testing document q. Figure 2: The k-NN classifier is applied to qp testing document and c category label is predicted. We have decided to use the k-NN classifier because it has been found that on the Reuters-21578 database it performs best among the conventional methods (Joachims, 1998; Yang, 1999) and because we have obtained good results in our previous work on text categorization for documents written in Basque, a highly inflected language (Zelaia et al., 2005). Besides, the k-NN classification algorithm can be easily adapted to multilabel categorization problems such as Reuters. 3.3 Combination of classifiers The combination of multiple classifiers has been intensively studied with the aim of improving the accuracy of individual components (Ho et al., 1994). Two widely used techniques to implement this approach are bagging (Breiman, 1996), that uses more than one model </context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Joachims, T. Text Categorization with Support Vector Machines: Learning with Many Relevant Features. Proceedings of ECML’98: 10th European Conference on Machine Learning, Springer 1398, 137–142, (1998)</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kim</author>
<author>P Howland</author>
<author>H Park</author>
</authors>
<title>Dimension Reduction in Text Classification with Support Vector Machines.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>6</volume>
<pages>37--53</pages>
<publisher>MIT Press,</publisher>
<contexts>
<context position="9990" citStr="Kim et al., 2005" startWordPosition="1556" endWordPosition="1559">15) there is also a good result, F1 = 87.2, obtained by (Zhang and Oles, 2001)2. 3 Proposed Approach In this paper we propose a multiclassifier based document categorization system. Documents in the training and testing sets are represented in a reduced dimensional vector space. Different training databases are generated from the original train2Actually, this result is obtained for 118 categories which correspond to the 115 mentioned before and three more categories which have testing documents but no training document assigned. Results reported by R(90) Top-10 (Gao et al., 2003) 88.42 93.07 (Kim et al., 2005) 87.11 92.21 (Gliozzo and Strapparava, 2005) 92.80 Table 2: F1 results reported for the Reuters-21578 ModApte split. ing dataset in order to construct the multiclassifier. We use the k-NN classification algorithm, which according to each training database makes a prediction for testing documents. Finally, a Bayesian voting scheme is used in order to definitively assign category labels to testing documents. In the rest of this section we make a brief review of the SVD dimensionality reduction technique, the k-NN algorithm and the combination of classifiers used. 3.1 The SVD Dimensionality Reduc</context>
</contexts>
<marker>Kim, Howland, Park, 2005</marker>
<rawString>Kim, H., Howland, P. and Park, H.: Dimension Reduction in Text Classification with Support Vector Machines. Journal of Machine Learning Research, 6, 37–53, MIT Press, (2005)</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
</authors>
<title>Reuters-21578 Text Categorization Test Collection, Distribution 1.0. http://daviddlewis.com/resources/testcollections README file (v 1.3),</title>
<date>2004</date>
<contexts>
<context position="5493" citStr="Lewis, 2004" startWordPosition="840" endWordPosition="841"> number of categories assigned to a document is 1.2) dataset. Many experiments have been carried out for the Reuters collection. However, they have been performed in different experimental conditions. This makes results difficult to compare among them. In fact, effectiveness results can only be compared between studies that use the same training and testing sets. In order to lead researchers to use the same training/testing divisions, the Reuters documents have been specifically tagged, and researchers are encouraged to use one of those divisions. In our experiment we use the “ModApte” split (Lewis, 2004). In this section, we analize the category subsets, evaluation measures and results obtained in the past and in the recent years for Reuters-21578 ModApte split. 2.1 Category subsets Concerning the evaluation of the classification system, we restrict our attention to the TOPICS group of categories that labels Reuters dataset, which contains 135 categories. However, many categories appear in no document and consequently, and because inductive based learning classifiers learn from training examples, these categories are not usually considered at evaluation time. The most widely used subsets are </context>
<context position="18803" citStr="Lewis, 2004" startWordPosition="3040" endWordPosition="3041"> 25 % (3299 documents) to test the accuracy of the classifier. Document distribution over categories in both the training and the testing sets is very unbalanced: the 10 most frequent categories, top-10, account 75% of the training documents; the rest is distributed among the other 108 categories. According to the number of labels assigned to each document, many of them (19% in training and 8.48% in testing) are not assigned to any category, and some of them are assigned to 12. We have decided to keep the unlabeled documents in both the training and testing collections, as it is suggested in (Lewis, 2004)5. 4.2 Preprocessing The original format of the text documents is in SGML. We perform some preprocessing to filter out the unused parts of a document. We preserved only the title and the body text, punctuation and numbers have been removed and all letters have been converted to lowercase. We have 4http://daviddlewis.com/resources/testcollections 5In the ”ModApte” Split section it is suggested as follows: “If you are using a learning algorithm that requires each training document to have at least TOPICS category, you can screen out the training documents with no TOPICS categories. Please do NOT</context>
</contexts>
<marker>Lewis, 2004</marker>
<rawString>Lewis, D.D.: Reuters-21578 Text Categorization Test Collection, Distribution 1.0. http://daviddlewis.com/resources/testcollections README file (v 1.3), (2004)</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
<author>Y Yang</author>
<author>T G Rose</author>
<author>F Li</author>
</authors>
<title>RCV1: A New Benchmark Collection for Text Categorization Research.</title>
<date>2004</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>5</volume>
<pages>361--397</pages>
<marker>Lewis, Yang, Rose, Li, 2004</marker>
<rawString>Lewis, D.D., Yang, Y., Rose, T.G. and Li, F.: RCV1: A New Benchmark Collection for Text Categorization Research. Journal of Machine Learning Research, 5, 361–397, (2004)</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>An Algorithm for Suffix Stripping.</title>
<date>1980</date>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="20087" citStr="Porter, 1980" startWordPosition="3282" endWordPosition="3283">comparable with other studies.” Random Subsampling R p d7 R m R m d1 d2 d509 Reuters−21578, ModApte, Train d9603 d1 c1 c2 c30 cj ,(ck) d9603 d7 .. . qp d135 .. . d6 d5 d6 d1 d2 d9603 k − k−NN d5 d23 d4 d3 d2 d3 d4 TD1 TD2 TD30 NN k−NN d61 d34 R p R p R p TD qp M Mp=UpEpVpT SVD VSM Bayesian voting d256 d638 d98 d50 d778 d848 q VSM Reuters−21578, ModApte, Test qp d9 q1 q2 q3299 d4612 q qp=qT UpE−1 p d33 d2787 d1989 d55 29 used the tools provided in the web6 in order to extract text and categories from each document. We have stemmed the training and testing documents by using the Porter stemmer (Porter, 1980)7. By using it, case and flection information are removed from words. Consequently, the same experiment has been carried out for the two forms of the document collection: word-forms and Porter stems. According to the dimension reduction, we have created the matrices for the two mentioned document collection forms. The sizes of the training matrices created are 15591 × 9603 for wordforms and 11114 × 9603 for Porter stems. Different number of dimensions have been experimented (p = 100, 300, 500, 700). 4.3 Parameter setting We have designed our experiment in order to optimize the microaveraged F1</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Porter, M.F.: An Algorithm for Suffix Stripping. Program,14(3), 130–137, (1980)</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill,</publisher>
<location>New York,</location>
<contexts>
<context position="1782" citStr="Salton and McGill, 1983" startWordPosition="253" endWordPosition="256">e predefined categories based on their content, is an important component in many information organization and management tasks. Researchers have concentrated their efforts in finding the appropriate way to represent documents, index them and construct classifiers to assign the correct categories to each document. Both, document representation and classification method are crucial steps in the categorization process. In this paper we concentrate on both issues. On the one hand, we use Latent Semantic Indexing (LSI) (Deerwester et al., 1990), which is a variant of the vector space model (VSM) (Salton and McGill, 1983), in order to obtain the vector representation of documents. This technique compresses vectors representing documents into vectors of a lower-dimensional space. LSI, which is based on Singular Value Decomposition (SVD) of matrices, has showed to have the ability to extract the relations among words and documents by means of their context of use, and has been successfully applied to Information Retrieval tasks. On the other hand, we construct a multiclassifier (Ho et al., 1994) which uses different training databases. These databases are obtained from the original training set by random subsamp</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Salton, G. and McGill, M.: Introduction to Modern Information Retrieval. McGraw-Hill, New York, (1983)</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sebastiani</author>
</authors>
<title>Machine Learning in Automated Text Categorization.</title>
<date>2002</date>
<journal>ACM Computing Surveys,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="4067" citStr="Sebastiani, 2002" startWordPosition="608" endWordPosition="609">eprocessing applied and some parameter setting are provided. In Section 5, experimental results are presented and discussed. Finally, Section 6 contains some conclusions and comments on future work. 25 2 Related Work As previously mentioned in the introduction, text categorization consists in assigning predefined categories to text documents. In the past two decades, document categorization has received much attention and a considerable number of machine learning based approaches have been proposed. A good tutorial on the state-of-the-art of document categorization techniques can be found in (Sebastiani, 2002). In the document categorization task we can find two cases; (1) the multilabel case, which means that categories are not mutually exclusive, because the same document may be relevant to more than one category (1 to m category labels may be assigned to the same document, being m the total number of predefined categories), and (2) the single-label case, where exactly one category is assigned to each document. While most machine learning systems are designated to handle multiclass data l, much less common are systems that can handle multilabel data. For experimentation purposes, there are standa</context>
<context position="7687" citStr="Sebastiani, 2002" startWordPosition="1194" endWordPosition="1196">sion and recall. Precision (Prec) is the percentage of documents correctly classified into a given category, and recall (Rec) is the percentage of documents belonging to a given category that are indeed classified into it. In general, there is a trade-off between precision and recall. Thus, a classifier is usually evaluated by means of a measure which combines precision and recall. Various such measures have been proposed. The breakeven point, the value at which precision equals recall, has been frequently used during the past decade. However, it has been recently criticized by its proposer ((Sebastiani, 2002) footnote 19). Nowadays, the F1 score is more frequently used. The F1 score combines recall and precision with an equal weight in the following way: &apos;Categorization problems where there are more than two F1 = 2 · Prec · Rec possible categories. Prec + Rec 26 Since precision and recall are defined only for binary classification tasks, for multiclass problems results need to be averaged to get a single performance value. This will be done using microaveraging and macroaveraging. In microaveraging, which is calculated by globally summing over all individual cases, categories count proportionally </context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>Sebastiani, F.: Machine Learning in Automated Text Categorization. ACM Computing Surveys, 34(1), 1–47, (2002)</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Weiss</author>
<author>C Apte</author>
<author>F J Damerau</author>
<author>D E Johnson</author>
<author>F J Oles</author>
<author>T Goetz</author>
<author>T Hampp</author>
</authors>
<title>Maximizing Text-Mining Performance.</title>
<date>1999</date>
<journal>IEEE Intelligent Systems,</journal>
<volume>14</volume>
<issue>4</issue>
<marker>Weiss, Apte, Damerau, Johnson, Oles, Goetz, Hampp, 1999</marker>
<rawString>Weiss, S.M., Apte, C., Damerau, F.J., Johnson, D.E., Oles, F.J., Goetz, T. and Hampp, T.: Maximizing Text-Mining Performance. IEEE Intelligent Systems, 14(4),63–69, (1999)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
</authors>
<title>An Evaluation of Statistical Approaches to Text Categorization.</title>
<date>1999</date>
<journal>Journal of Information Retrieval. Kluwer Academic Publishers,</journal>
<volume>1</volume>
<contexts>
<context position="8511" citStr="Yang, 1999" startWordPosition="1326" endWordPosition="1327">Prec · Rec possible categories. Prec + Rec 26 Since precision and recall are defined only for binary classification tasks, for multiclass problems results need to be averaged to get a single performance value. This will be done using microaveraging and macroaveraging. In microaveraging, which is calculated by globally summing over all individual cases, categories count proportionally to the number of their positive testing examples. In macroaveraging, which is calculated by averaging over the results of the different categories, all categories count the same. See (Debole and Sebastiani, 2005; Yang, 1999) for more detailed explanation of the evaluation measures mentioned above. 2.3 Comparative Results Sebastiani (Sebastiani, 2002) presents a table where lists results of experiments for various training/testing divisions of Reuters. Although we are aware that the results listed are microaveraged breakeven point measures, and consequently, are not directly comparable to the ones we present in this paper, F1, we want to remark some of them. In Table 1 we summarize the best results reported for the ModApte split listed by Sebastiani. Results reported by R(90) Top-10 (Joachims, 1998) 86.4 (Dumais e</context>
<context position="14349" citStr="Yang, 1999" startWordPosition="2280" endWordPosition="2281">ere some training documents and a testing document q are projected in the R p reduced space. The nearest to the qp testing document are considered to be the vectors which have the smallest angle with qp. According to the category labels of the nearest documents, a category label prediction, c, will be made for testing document q. Figure 2: The k-NN classifier is applied to qp testing document and c category label is predicted. We have decided to use the k-NN classifier because it has been found that on the Reuters-21578 database it performs best among the conventional methods (Joachims, 1998; Yang, 1999) and because we have obtained good results in our previous work on text categorization for documents written in Basque, a highly inflected language (Zelaia et al., 2005). Besides, the k-NN classification algorithm can be easily adapted to multilabel categorization problems such as Reuters. 3.3 Combination of classifiers The combination of multiple classifiers has been intensively studied with the aim of improving the accuracy of individual components (Ho et al., 1994). Two widely used techniques to implement this approach are bagging (Breiman, 1996), that uses more than one model of the same p</context>
</contexts>
<marker>Yang, 1999</marker>
<rawString>Yang, Y. An Evaluation of Statistical Approaches to Text Categorization. Journal of Information Retrieval. Kluwer Academic Publishers, 1,(1/2), 69– 90, (1999)</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Zelaia</author>
<author>I Alegria</author>
<author>O Arregi</author>
<author>B Sierra</author>
</authors>
<title>Analyzing the Effect of Dimensionality Reduction in Document Categorization for Basque.</title>
<date>2005</date>
<booktitle>Proceedings of L&amp;TC’05: 2nd Language &amp; Technology Conference,</booktitle>
<pages>72--75</pages>
<contexts>
<context position="14518" citStr="Zelaia et al., 2005" startWordPosition="2307" endWordPosition="2311">tors which have the smallest angle with qp. According to the category labels of the nearest documents, a category label prediction, c, will be made for testing document q. Figure 2: The k-NN classifier is applied to qp testing document and c category label is predicted. We have decided to use the k-NN classifier because it has been found that on the Reuters-21578 database it performs best among the conventional methods (Joachims, 1998; Yang, 1999) and because we have obtained good results in our previous work on text categorization for documents written in Basque, a highly inflected language (Zelaia et al., 2005). Besides, the k-NN classification algorithm can be easily adapted to multilabel categorization problems such as Reuters. 3.3 Combination of classifiers The combination of multiple classifiers has been intensively studied with the aim of improving the accuracy of individual components (Ho et al., 1994). Two widely used techniques to implement this approach are bagging (Breiman, 1996), that uses more than one model of the same paradigm; and boosting (Freund and Schapire, 1999), in which a different weight is given to different training examples looking for a better accuracy. In our experiment w</context>
<context position="20746" citStr="Zelaia et al., 2005" startWordPosition="3392" endWordPosition="3395">mation are removed from words. Consequently, the same experiment has been carried out for the two forms of the document collection: word-forms and Porter stems. According to the dimension reduction, we have created the matrices for the two mentioned document collection forms. The sizes of the training matrices created are 15591 × 9603 for wordforms and 11114 × 9603 for Porter stems. Different number of dimensions have been experimented (p = 100, 300, 500, 700). 4.3 Parameter setting We have designed our experiment in order to optimize the microaveraged F1 score. Based on previous experiments (Zelaia et al., 2005), we have set parameter k for the k-NN algorithm to k = 3. This way, the k-NN classifier will give a category label prediction based on the categories of the 3 nearest ones. On the other hand, we also needed to decide the number of training databases TDi to create. It has to be taken into account that a high number of training databases implies an increasing computational cost for the final classification system. We decided to create 30 training databases. However, this is a parameter that has not been optimized. There are two other parameters which have been tuned: the size of each training d</context>
</contexts>
<marker>Zelaia, Alegria, Arregi, Sierra, 2005</marker>
<rawString>Zelaia, A., Alegria, I., Arregi, O. and Sierra, B.: Analyzing the Effect of Dimensionality Reduction in Document Categorization for Basque. Proceedings of L&amp;TC’05: 2nd Language &amp; Technology Conference, 72–75, (2005)</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Zelikovitz</author>
<author>H Hirsh</author>
</authors>
<title>Using LSI for Text Classification in the Presence of Background Text.</title>
<date>2001</date>
<booktitle>Proceedings of CIKM’01: 10th ACM International Conference on Information and Knowledge Management,</booktitle>
<pages>113--118</pages>
<publisher>ACM Press,</publisher>
<marker>Zelikovitz, Hirsh, 2001</marker>
<rawString>Zelikovitz, S. and Hirsh, H.: Using LSI for Text Classification in the Presence of Background Text. Proceedings of CIKM’01: 10th ACM International Conference on Information and Knowledge Management, ACM Press, 113–118, (2001)</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Zhang</author>
<author>F J Oles</author>
</authors>
<title>Text Categorization Based on Regularized Linear Classification Methods.</title>
<date>2001</date>
<journal>Information Retrieval,</journal>
<volume>4</volume>
<issue>1</issue>
<pages>5--31</pages>
<publisher>Kluwer Academic Publishers,</publisher>
<contexts>
<context position="9451" citStr="Zhang and Oles, 2001" startWordPosition="1472" endWordPosition="1475">s, and consequently, are not directly comparable to the ones we present in this paper, F1, we want to remark some of them. In Table 1 we summarize the best results reported for the ModApte split listed by Sebastiani. Results reported by R(90) Top-10 (Joachims, 1998) 86.4 (Dumais et al., 1998) 87.0 92.0 (Weiss et.al., 1999) 87.8 Table 1: Microaveraged breakeven point results reported by Sebastiani for the Reuters-21578 ModApte split. In Table 2 we include some more recent results, evaluated according to the microaveraged F1 score. For R(115) there is also a good result, F1 = 87.2, obtained by (Zhang and Oles, 2001)2. 3 Proposed Approach In this paper we propose a multiclassifier based document categorization system. Documents in the training and testing sets are represented in a reduced dimensional vector space. Different training databases are generated from the original train2Actually, this result is obtained for 118 categories which correspond to the 115 mentioned before and three more categories which have testing documents but no training document assigned. Results reported by R(90) Top-10 (Gao et al., 2003) 88.42 93.07 (Kim et al., 2005) 87.11 92.21 (Gliozzo and Strapparava, 2005) 92.80 Table 2: F</context>
</contexts>
<marker>Zhang, Oles, 2001</marker>
<rawString>Zhang, T. and Oles, F.J.: Text Categorization Based on Regularized Linear Classification Methods. Information Retrieval, 4(1): 5–31, Kluwer Academic Publishers, (2001)</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>