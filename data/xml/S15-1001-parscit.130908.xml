<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002646">
<title confidence="0.999534">
Neural Networks for Integrating Compositional and Non-compositional
Sentiment in Sentiment Composition
</title>
<author confidence="0.997793">
Xiaodan Zhu &amp; Hongyu Guo
</author>
<affiliation confidence="0.996708">
National Research Council Canada
</affiliation>
<address confidence="0.9927425">
1200 Montreal Road, M50
Ottawa, ON K1A 0R6, Canada
</address>
<email confidence="0.975685">
{xiaodan.zhu,hongyu.guo}@nrc-cnrc.gc.ca
</email>
<author confidence="0.979569">
Parinaz Sobhani
</author>
<affiliation confidence="0.997271">
EECS, University of Ottawa
</affiliation>
<address confidence="0.989259">
800 King Edward Avenue
Ottawa, ON K1N 6N5, Canada
</address>
<email confidence="0.99799">
psobh090@uottawa.ca
</email>
<sectionHeader confidence="0.998592" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999585875">
This paper proposes neural networks for inte-
grating compositional and non-compositional
sentiment in the process of sentiment compo-
sition, a type of semantic composition that op-
timizes a sentiment objective. We enable in-
dividual composition operations in a recursive
process to possess the capability of choosing
and merging information from these two types
of sources. We propose our models in neural
network frameworks with structures, in which
the merging parameters can be learned in a
principled way to optimize a well-defined ob-
jective. We conduct experiments on the Stan-
ford Sentiment Treebank and show that the
proposed models achieve better results over
the model that lacks this ability.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994575428571429">
Automatically determining the sentiment of a
phrase, a sentence, or even a longer piece of text
is still a challenging problem. Data sparseness en-
countered in such tasks often requires to factorize
the problem to consider smaller pieces of compo-
nent words or phrases, for which much research has
been performed on bag-of-words or bag-of-phrases
models (Pang and Lee, 2008; Liu and Zhang, 2012).
More recent work has started to model sentiment
composition (Moilanen and Pulman, 2007; Choi and
Cardie, 2008; Socher et al., 2012; Socher et al.,
2013), a type of semantic composition that opti-
mizes a sentiment objective. In general, the com-
position process is critical in the formation of the
1
sentiment of a span of text, which has not been well
modeled yet and there is still scope for future work.
Compositionality, or non-compositionality, of the
senses of text spans is important for language under-
standing. Sentiment, as one of the major semantic
differential categories (Osgood et al., 1957), faces
the problem as well. For example, the phrase must
see or must try in a movie or restaurant review often
indicates a positive sentiment, which, however, may
be hard to learn from the component words. More
extreme examples, e.g., slangs like bad ass, are not
rare in social media text. This particular example
can actually convey a very positive sentiment even
though its component words are very negative. In
brief, a sentiment composition framework that can
consider both compositional and non-compositional
sentiment is theoretically interesting.
From a more pragmatical viewpoint, if one is
able to reliably learn the sentiment of a text span
(e.g., an ngram) holistically, it would be desirable
that a composition model has the ability to de-
cide the sources of knowledge it trusts more: the
composition from the component words, the non-
compositional source, or a soft combination of them.
In such a situation, whether the text span is actually
composable may be blur or may not be a concern.
In general, the composition of sentiment is a
rather complicated process. As a glimpse of ev-
idence, the effect of negation words on changing
sentiment of their scopes appears to be a compli-
cated function (Zhu et al., 2014). The recently pro-
posed neural networks (Socher et al., 2013; Socher
et al., 2011) are promising, for their capability of
modeling complicated functions (Mitchell, 1997) in
</bodyText>
<note confidence="0.9318455">
Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 1–9,
Denver, Colorado, June 4–5, 2015.
</note>
<bodyText confidence="0.997914222222222">
general, handling data sparseness by learning low-
dimensional embeddings at each layer of compo-
sition, and providing a framework to optimize the
composition process in principled way.
This paper proposes neural networks for integrat-
ing compositional and non-compositional sentiment
in the process of sentiment composition. To achieve
this, we enable individual composition operations
in a recursive process to possess the capability of
choosing and merging information from these two
types of sources. We propose our models in neu-
ral network frameworks with structures (Socher et
al., 2013), in which the merging parameters can
be learned in a principled way to optimize a well-
defined objective. We conduct experiments on the
Stanford Sentiment Treebank and show that the pro-
posed models achieve better results over the model
that does not consider this property.
</bodyText>
<sectionHeader confidence="0.999854" genericHeader="method">
2 Related work
</sectionHeader>
<bodyText confidence="0.999927977777778">
Composition of sentiment Early work on modeling
sentiment does not examine semantic composition
closely (Pang and Lee, 2008; Liu and Zhang, 2012),
as mentioned above. Recent work has considered
sentiment-oriented semantic composition (Moilanen
and Pulman, 2007; Choi and Cardie, 2008; Socher et
al., 2012; Socher et al., 2013), or simply called senti-
ment composition in this paper. For example, Moila-
nen and Pulman (2007) used a collection of hand-
written compositional rules to assign sentiment val-
ues to different granularities of text spans. Choi
and Cardie (2008) proposed a learning-based frame-
work. The more recent work of (Socher et al., 2013)
proposed models based on neural networks that do
not rely on any heuristic rules. Such models work
in a bottom-up fashion over a tree to infer the sen-
timent label of a phrase or sentence as a composi-
tion of the sentiment expressed by its constituting
parts. The approach leverages a principled method,
the forward and backward propagation, to optimize
the system performance. In this paper, we follow the
neural network approach to integrate compositional
and non-compositional sentiment in sentiment com-
position.
Prior knowledge of sentiment Integrating non-
compositional sentiment into the composition pro-
cess can be viewed as introducing some prior sen-
timent knowledge, as in general the sentiment of a
word or a phrase perceived independent of its con-
text is often referred to as prior sentiment. Word-
level prior sentiment is typically annotated in man-
ual sentiment lexicons (Wilson et al., 2005; Hu
and Liu, 2004; Mohammad and Turney, 2010),
or learned in an unsupervised or semisupervised
way (Hatzivassiloglou and McKeown, 1997; Esuli
and Sebastiani, 2006; Turney and Littman, 2003;
Mohammad et al., 2009). More recently, senti-
ment indicators, such as emoticons and hashtags,
are utilized (Go et al., 2009; Davidov et al., 2010;
Kouloumpis et al., 2011; Mohammad, 2012; Mo-
hammad et al., 2013a). With enough data, such
freely available (but noisy) annotation can be used to
learn the sentiment of ngrams. In our study, we will
investigate in the proposed composition models the
effect of automatically learned sentimental ngrams.
</bodyText>
<sectionHeader confidence="0.964254" genericHeader="method">
3 Prior-enriched semantic networks
</sectionHeader>
<bodyText confidence="0.999931">
In this paper, we propose several neural networks
that enable each composition operation to pos-
sess the ability of choosing and merging senti-
ment from lower-level composition and that from
non-compositional sources. We call the networks
Prior-Enriched Semantic Networks (PESN). We
present several specific implementations based on
RNTN (Socher et al., 2013); the latter has showed
to be a state-of-the-art sentiment composition frame-
work. However, the realization of a PESN node is
not necessarily only tied with RNTN.
Figure 1 shows a piece of PESN. Each of the three
big nodes, i.e., N1, N2, and N3, corresponds to a
node in a constituency parse tree; e.g., N3 may cor-
respond to the phrase not a must try, where N1 and
N2 are not and a must try, respectively. We ex-
tend each of the nodes to possess the ability to con-
sider sentiment from lower-level composition and
non-compositional sources. In node N3, knowledge
from the lower-level composition is represented in
the hidden vector i3, which is merged with non-
compositional knowledge represented in e3, and the
merged information is saved in m3. The black box
in the center performs the actual merging, which in-
tegrates the two knowledge sources in order to min-
</bodyText>
<page confidence="0.962549">
2
</page>
<bodyText confidence="0.999396833333333">
imize an overall objective function that we will dis-
cuss in detail later. The recursive neural networks
and the forward-backward propagation over struc-
tures (Socher et al., 2013; Goller and Kchler, 1996)
provide a principled way to optimize the whole net-
work.
</bodyText>
<figureCaption confidence="0.9315535">
Figure 1: A prior-enriched semantic network (PESN) for
sentiment composition. The three nodes, N1, N2, and
</figureCaption>
<bodyText confidence="0.9655845">
N3, correspond to three nodes in a constituency parse
tree, and each of them consider sentiment from lower-
level composition (i1, i2, i3) and from non-compositional
sentiment (e1, e2, e3).
</bodyText>
<subsectionHeader confidence="0.995744">
3.1 Regular bilinear merging
</subsectionHeader>
<bodyText confidence="0.9999776">
The most straightforward way of implementing a
PESN node is probably through a regular bilinear
merging. Take node N3 in Figure 1 as an example;
the node vector m3 will be simply merged from i3
and e3 as follows:
</bodyText>
<equation confidence="0.997117">
m3 = tanh(Wm Ie3
i3J+ bm)
</equation>
<bodyText confidence="0.9997105625">
Again, vector i3 contains the knowledge from the
lower-level composition; e3 is a vector representing
non-compositional sentiment information, which
can be either from human annotation or automati-
cally learned resources. Note that in the network,
all hidden vectors m and i (including word embed-
ding vectors) have the same dimensionality d, but
the non-compositional nodes, i.e., the nodes e , do
not necessarily have to have the same number of el-
ements, and we let l be their dimensionality. The
merging matrix Wm is d-by-(d+l).
As in this paper we discuss PESN in the frame-
work of RNTN, computation outside the nodes
N1, N2, N3 follows that for the standard three-way
tensors in RNTN. That is, the hidden vector i3 is
computed with the following formula:
</bodyText>
<equation confidence="0.989772">
Im2
1Ti3 = tanh(J V[1:d] [m1]
+ Wr [m1])
m (2)
</equation>
<bodyText confidence="0.9495636">
where, Wr E Rd×(d+d) and Vr E R(d+d)×(d+d)×d
are the matrix and tensor of the composition func-
tion used in RNTN, respectively, each of which is
shared over the whole tree in computing vectors i1,
i2, and i3.
</bodyText>
<subsectionHeader confidence="0.998541">
3.2 Explicitly gated merging
</subsectionHeader>
<bodyText confidence="0.999932846153846">
Compared to the regular bilinear merging model, we
here further explicitly control the input of the com-
positional and non-compositional semantics. Ex-
plicitly gating neural network has been studied in the
literature. For example, the long short-term mem-
ory (LSTM) utilizes input gates, together with out-
put gates and forget gates, to guide memory blocks
to remember/forget history (Hochreiter and Schmid-
huber, 1997).
For our purpose here, we explore an input gate to
explicitly control the two different input sources. As
shown in Figure 2, an additional gating layer g3 is
used to control i3, e3 explicitly.
</bodyText>
<equation confidence="0.9894415">
⎡ ⎤
Wgee3
g3 = σ( ⎣ ⎦+ bg) (3)
Wgii3 1
m3 = tanh(Wm(g3 ⊗ i3J ) + bm) (4)
Ie3
</equation>
<bodyText confidence="0.99961825">
The sign ⊗ is a Hadamard product; σ is a logis-
tic sigmoid function instead of a tanh activation,
which makes the gating signal g3 to be in the range
of [0, 1] and serve as a soft switch (not a hard binary
</bodyText>
<page confidence="0.995955">
3
</page>
<figureCaption confidence="0.997241">
Figure 2: An input-gated network that explicitly controls
the compositional and non-compositional sentiment in-
put.
</figureCaption>
<bodyText confidence="0.9998036">
0/1 switch) to explicitly gate i3 and e3. Note that
elsewhere in the network, we still use tanh as our
activation function. In addition, Wge E Rd×l and
Wgi E Rl×d are the weight matrices used to calcu-
late the gate vector.
</bodyText>
<subsectionHeader confidence="0.985668">
3.3 Confined-tensor-based merging
</subsectionHeader>
<bodyText confidence="0.999991571428571">
The third approach we use for merging composi-
tional and non-compositional knowledge employs
tensors, which are able to explore multiplicative
combination among variables. Tensors have already
been successfully used in a wide range of NLP
tasks in capturing high-order interactions among
variables. The forward computation of m3 follows:
</bodyText>
<equation confidence="0.9997488">
~i3 ~T ~i3~ ~i3 �
V [1:d]
m3 = tanh( + Wm ) (5)
m
e3 e3 e3
</equation>
<bodyText confidence="0.998752315789474">
where V [1:d] mE R(d+l)×(d+l)×d is the tensor m that
defines multiple bilinear forms, and the matrix Wm
is as defined in the previous models.
As we focus on the interaction between i3 and e3,
we force each slice of tensor, e.g. V [k]
m , to have zero-
valued blocks. More specifically, the top-right d-by-
l block of the piece matrix V [k]
m (k E 11...d}) and
the bottom-left l-by-d block are non-zero parame-
ters, used to capture multiplicative, element-pair in-
teractions between i3 and e3, while the rest block are
set to be zero, to ignore interactions between those
variables within i3 and those within e3. This does
not only make the model focus on the interaction
between vector i and e, it also helps significantly re-
duce the number of parameters to estimate, which,
otherwise, could potentially lead to overfitting. We
call this model confined-tensor-based merging.
</bodyText>
<subsectionHeader confidence="0.99436">
3.4 Learning and inference
</subsectionHeader>
<bodyText confidence="0.999941357142857">
Objective The overall objective function in learning
PESN, following (Socher et al., 2013), minimizes
the cross-entropy error between the predicted dis-
tribution yseni E Rc×1 at a node i and the target
distribution ti E Rc×1 at that node, where c is the
number of sentiment categories. PESN learns the
parameters that are used to merge the compositional
and non-compositional sentiment so that the merg-
ing operations integrate the two sources in minimiz-
ing prediction loss. The neural network over struc-
tures provides a principled framework to optimize
these parameters.
More specifically, the error over an entire sen-
tence is calculated as a regularized sum:
</bodyText>
<equation confidence="0.979068">
E(0) = X X tijlogysenij + A 110112 (6)
i j
</equation>
<bodyText confidence="0.999833909090909">
where, A is the regularization parameter, j E c de-
notes the j-th element of the multinomial target dis-
tribution, 0 are model parameters that will be dis-
cussed below, and i iterates over all nodes ix (e.g.,
i1, i2, and i3) in Figure 1, where the model predicts
sentiment labels.
Backpropagation over the structures To minimize
E(0), the gradient of the objective function with
respect to each of the parameters in 0 is calcu-
lated efficiently via backpropagation through struc-
ture (Socher et al., 2013; Goller and Kchler, 1996),
after computing the prediction errors in forward
propagation with formulas described above.
Regular bilinear merging The PESN implemented
with simple bilinear merging has the following
model parameters: 0 = (Vr, Wr, Wm, Wlabel, L).
As discussed above, Vr and Wr are the tensor and
matrix in RNTN; Wm is the weight matrix for merg-
ing the compositional and non-compositional senti-
ment vectors. L denotes the vector representations
of the word dictionary, and Wlabel is sentiment clas-
sification matrix used to predict sentiment label at a
</bodyText>
<page confidence="0.987941">
4
</page>
<bodyText confidence="0.99996625">
node. Backpropagation on the regular bilinear merg-
ing node follows a standard derivative computation
in a regular feed-forward network, which we skip
here.
Explicitly gated merging In this model, in addition
to Wm, we further learn two weight matrices Wgi
and Wge, as introduced in Formula 3 and 4 above.
Consider Figure 2 and let δm3 denote the error mes-
sages passed down to node m3. The error messages
are passed back to i3 directly through the Hadamard
product and also through the gate node g3. The for-
mer, denoted as δi3,dir, is calculated with:
</bodyText>
<equation confidence="0.998148">
δi3,dir = (δm3 ⊗ g3)[1 : d] (7)
</equation>
<bodyText confidence="0.999282875">
where, g3 is calculated with Formula 3 above in the
forward process; [1 : d] means taking the first d ele-
ments of the vector yielded by the Hadamard prod-
uct; the rest [d+1 : d+l] elements of the Hadamard
production are discarded, as we do not update e3,
which is given as our prior knowledge.
The error messages passed down to gate vector g3
is computed with
</bodyText>
<equation confidence="0.989101">
δg3 = δm3 ⊗ i3] ⊗ s&apos; (g3) (8)
1e3
</equation>
<bodyText confidence="0.99704875">
where, s&apos;(.) is the element-wise derivative of logis-
tic function, which can be calculated only using s(.),
as s(.)(1 − s(.)). The derivative of Wgi can be cal-
culated with:
</bodyText>
<equation confidence="0.909523">
= (δg3[1 : d])eT (9)
3
</equation>
<bodyText confidence="0.9999432">
Similarly, partial derivatives over Wgi can be cal-
culated. These values will be summed to the to-
tal derivative of Wgi and Wge, respectively. With
these notations, the error messages passed down to
i3 through the gate can then be computed with:
</bodyText>
<equation confidence="0.637384">
δi3,gate = WTgi (δg3[d + 1 : d + l]) (10)
</equation>
<bodyText confidence="0.975258454545455">
and the total error messages to node i3 is then:
δi3,total = (δi3,dir+δi3,gate+δi3,local)⊗f&apos;(i3) (11)
where δi3,local is the local error message from the
sentiment prediction errors performed at the node i3
itself to obtain the total error message for i3, which
is in turn passed down through regular RNTN tensor
to the lower levels. f&apos;(.) is the element-wise deriva-
tive of tanh function.
Confined-tensor-based merging In confined-tensor-
based merging, the error messages passed to the two
children i3 and e3 is computed with:
</bodyText>
<equation confidence="0.97809325">
δi3,e3 = (WTm δm3) ⊗f&apos; (Le3J ) + δtns (12)
where,
δk 3 (V lk] + (V ,k] )T) 1e i3] ®f ( L ) e3
(13)
</equation>
<bodyText confidence="0.9954248125">
where the error messages to i3 are the first d num-
bers of elements of δi3,e3. The rest elements of δi3,e3
are discarded; as mentioned above, we do not update
e3 as it is given as the prior knowledge. We skip the
derivative for the Wm3. While the derivative of each
slice k(k = 1, ... , d) of the tensor V is calculated
with:
= δm3,down 1e3i3J [i3](14)k e3
Again, the full derivative for Vm and Wm is the
sum of their derivatives over the trees. After the er-
ror message passing from m3 to i3 is obtained, it can
be summed up with the local error message from the
sentiment prediction errors at the node i3 itself to
obtain the total error message for i3, which is in turn
used to calculate the error messages passed down as
well as the derivative in the lower-level tree.
</bodyText>
<sectionHeader confidence="0.999718" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.964762">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.99999125">
We use the Stanford Sentiment Treebank (Socher
et al., 2013) in our experiments. The data contain
about 11,800 sentences from the movie reviews that
were originally collected by Pang and Lee (2005).
</bodyText>
<equation confidence="0.979552222222222">
∂Eg3
Wge
d
δtns = X
k=1
∂Em3
V [k]
m
T
</equation>
<page confidence="0.929284">
5
</page>
<bodyText confidence="0.961884928571429">
The sentences were parsed with the Stanford parser between the target ngram and the pseudo-labeled
(Klein and Manning, 2003). Phrases at all the tree positive tweets as well as that between the ngram
nodes were manually annotated with sentiment val- and the negative tweets, respectively. Accordingly,
ues. We use the same split of the training and test a positive score(.) indicates association with pos-
data as in (Socher et al., 2013) to predict the sen- itive sentiment, whereas a negative score indicates
timent categories of the roots (sentences) and the association with negative sentiment.
phrases, and use the same evaluation metric, clas- We use in our experiments the bigrams and tri-
sification accuracy, to measure the performances. grams learned from the dataset with the occurrences
4.2 Obtaining non-compositional sentiment higher than 5. We assign these ngrams into one
In our experiments, we explore in sentiment com- of the 5 bins according to their sentiment scores
position the effect of two different types of non- obtained with Formula 15: (−∞, −2], (−2, −1],
compositional sentiment: (1) sentiment of ngrams (−1, 1), [1, 2), and [2, +∞). Each ngram is now
automatically learned from an external, much larger given a one-hot vector, indicating the polarity and
corpus, and (2) sentiment of ngrams assigned by hu- strength of its sentiment. For example, a bigram
man annotators. with a score of -1.5 will be assigned a 5-dimensional
Following the method proposed in (Mohammad vector [0, 1, 0, 0, 0], indicating a weak negative.
et al., 2013b), we learn sentimental ngrams from Note that PESN can also take into other forms
Tweets. The unsupervised approach utilizes hash- of sentiment embeddings, such as those learned in
tags, which can be regarded as conveying freely (Tang et al., 2014).
available (but noisy) human annotation of sentiment. In addition, the Stanford Sentiment Treebank con-
More specifically, certain words in tweets are spe- tains manually annotated sentiment for each indi-
cially marked with the hash character (#) to indi- vidual phrase in a parse tree, so we use such an-
cate the topic, sentiment polarity, or emotions such notation but not other manual lexicons, by assum-
as joy, sadness, angry, and surprised. With enough ing such annotation fits the corpus itself the best.
data, such artificial annotation can be used to learn Specifically, we use bigram and trigram annotation
the sentiment of ngrams by their likelihood of co- in the treebank. Note that even longer ngrams are
occurring with such hashtagged words. much sparser and probably less useful in general,
More specifically, a collection of 78 seed hash- one may learn sentiment for multi-word expressions
tags closely related to positive and negative such as of a larger length, which we will leave as future
#good, #excellent, #bad, and #terrible were used (32 work.
positive and 36 negative). These terms were chosen
from entries for positive and negative in the Roget’s
Thesaurus. A set of 775,000 tweets that contain at
least a positive hashtag or a negative hashtag were
used as the learning corpus. A tweet was considered
positive if it had one of the 32 positive seed hash-
tags, and negative if it had one of the 36 negative
seed hashtags. The association score for an ngram
w was calculated from these pseudo-labeled tweets
as follows:
score(w) = PMI(w, positive) − PMI(w, negative)
(15)
where PMI stands for pointwise mutual information,
and the two terms in the formula calculate the PMI
6
4.3 Results
Overall prediction performance Table 1 shows
the accuracies of different models on Stanford Sen-
timent Treebank. We evaluate the models on 5-
category sentiment prediction at both the sentence
(root) level and at all nodes (including roots).1 The
results reported in Table 1 are all based on the ver-
sion 3.3.0 of the Stanford CoreNLP2 and our imple-
mentation of PESN on it. The CoreNLP includes
a java implementation of RNTN.3 To make the re-
sults reported in the table comparable, we trained the
</bodyText>
<footnote confidence="0.61051175">
1The package only gives approximate accuracies for 2-
category sentiment, which are not included here in the table.
2http://nlp.stanford.edu/sentiment/code.html
3The matlab code used in (Socher et al., 2013) is not pub-
</footnote>
<figure confidence="0.935722777777778">
lished.
Models sentence-level (roots) all phrases (all nodes)
(1) RNTN 42.44 79.95
(2) Regular-bilinear (auto) 42.37 79.97
(3) Regular-bilinear (manu) 42.98 80.14
(4) Explicitly-gated (auto) 42.58 80.06
(5) Explicitly-gated (manu) 43.21 80.21
(6) Confined-tensor (auto) 42.99 80.49
(7) Confined-tensor (manu) 43.75† 80.66†
</figure>
<tableCaption confidence="0.990329333333333">
Table 1: Model performances (accuracies) on predicting 5-category sentiment at the sentence (root) level and phrase-
level on Stanford Sentiment Treebank. The numbers in the bold font are the best performances achieved on the two
tasks. Both results are statistically significantly better (p &lt; 0.05) than the corresponding RNTN results.
</tableCaption>
<bodyText confidence="0.999772">
RNTN models with the default parameter4 and run
the training from 5 different random initializations,
and report the best results we observed.
The rows in the table marked with auto are models
using the automatically learned ngrams, and those
marked with manu using manually annotated senti-
ment for bigrams and trigrams. Note that the non-
compositional sentiment of a node is only used to
predict the sentiment of phrases above it in the tree.
For example, in Figure 1 discussed earlier, the effect
of e1 and e2 will be used to predict the sentiment
of i3 and other node i above, but not that of i1 and
i2 themselves, avoiding the concern of using the an-
notation of a tree node to predict the sentiment of
itself.
The models in general benefit from incorporating
the non-compositional knowledge. The numbers in
the bold font are the best performance achieved on
the two tasks. While using the simple regular bi-
linear merging shows some gains, the more compli-
cated models achieve further improvement.
Above we have seen the general performance of
the models. Below, we take a closer look at the
prediction errors at different depths of the senti-
ment treebank. The depth here is defined as the
longest distance between a tree node and its descen-
dant leafs. In Figure 3, the x-axis corresponds to
different depths and y-axis is the accuracy. The fig-
ure was drawn with the RNTN and the model (7) in
Table 1, so as to study the compositional property in
the ideal situation where the lexical has a full cover-
age of bigrams and trigrams.
</bodyText>
<footnote confidence="0.977592666666667">
4java -mx8g edu.stanford.nlp.sentiment.SentimentTraining -
numHid 25 -trainPath train.txt -devPath dev.txt -train -model
model.ser.gz
</footnote>
<figureCaption confidence="0.9994745">
Figure 3: Errors made at different depths in the sentiment
tree bank.
</figureCaption>
<bodyText confidence="0.999993222222222">
The figure shows that using the confined tensor
to combine holistic sentiment information outper-
forms the original RNTN model that does not con-
sider this, starting from depth 3, showing the benefit
of using holistic bigram sentiment. The improve-
ment increases at depth 4 (indicating the benefit of
using trigram sentiment), and then was propagated
to the higher levels of the tree. As discussed above,
we only use non-compositional sentiment of a node
to predict the sentiment of the phrases above it in
the tree but not the node itself. And the system still
needs to balance which source it trusts more, by op-
timizing the overall objective.
Although the empirical improvement may depend
on the percentage of non-compositional instances in
a data set or the sentiment that need to be learned
holistically, we present here the first effort, accord-
ing to our knowledge, on studying the concern of in-
</bodyText>
<page confidence="0.997699">
7
</page>
<bodyText confidence="0.991905548387097">
tegrating compositional and non-compositional sen- opinion mining. In In Proceedings of the 5th Confer-
timent in the semantic composition process. ence on Language Resources and Evaluation, LREC
5 Conclusions and future work ’06, pages 417–422.
This paper proposes models for integrating com- Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
positional and non-compositional sentiment in the ter sentiment classification using distant supervision.
process of sentiment composition. To achieve this, Technical report, Stanford University.
we enable each composition operation to be able to Christoph Goller and Andreas Kchler. 1996. Learning
choose and merge information from these two types task-dependent distributed representations by back-
of sources. We propose to implement such mod- propagation through structure. In In Proc. of the
els within neural network frameworks with struc- ICNN-96, pages 347–352, Bochum, Germany. IEEE.
tures (Socher et al., 2013), in which the merging pa- Vasileios Hatzivassiloglou and Kathleen R. McKeown.
rameters can be optimized in a principled way, to 1997. Predicting the semantic orientation of ad-
minimize a well-defined objective. We conduct ex- jectives. In Proceedings of the 8th Conference of
periments on the Stanford Sentiment Treebank and European Chapter of the Association for Computa-
show that the proposed models achieve better results tional Linguistics, EACL ’97, pages 174–181, Madrid,
over the model that does not consider this property. Spain.
Although the empirical improvement may depend S. Hochreiter and J. Schmidhuber. 1997. Long short-
on the percentage of non-compositional instances in term memory. Neural Computation, 9(8):1735–1780.
a data set or the sentiment that need to be learned Minqing Hu and Bing Liu. 2004. Mining and summa-
holistically, we present here the first effort, accord- rizing customer reviews. In Proceedings of the 10th
ing to our knowledge, on studying the basic concern ACM SIGKDD International Conference on Knowl-
of integrating compositional and non-compositional edge Discovery and Data Mining, KDD ’04, pages
sentiment in composition. While we focus on senti- 168–177, New York, NY, USA. ACM.
ment in this paper, investigating compositional and Dan Klein and Christopher D. Manning. 2003. Accurate
non-compositional semantics for general semantic unlexicalized parsing. In Proceedings of the 41st An-
composition with neural networks is interesting to nual Meeting on Association for Computational Lin-
us as an immediate future problem, as such mod- guistics - Volume 1, ACL ’03, pages 423–430, Sap-
els provide a principled way to optimize the over- poro, Japan. Association for Computational Linguis-
all objective over the sentence structures when we tics.
consider both compositional and non-compositional Efthymios Kouloumpis, Theresa Wilson, and Johanna
semantics. Moore. 2011. Twitter sentiment analysis: The Good
</bodyText>
<table confidence="0.458921111111111">
References the Bad and the OMG! In Proceedings of the 5th In-
Yejin Choi and Claire Cardie. 2008. Learning with com- ternational AAAI Conference on Weblogs and Social
positional semantics as structural inference for subsen- Media.
tential sentiment analysis. In Proceedings of the Con- Bing Liu and Lei Zhang. 2012. A survey of opinion
ference on Empirical Methods in Natural Language mining and sentiment analysis. In Charu C. Aggar-
Processing, EMNLP ’08, pages 793–801, Honolulu, wal and ChengXiang Zhai, editors, Mining Text Data,
Hawaii. pages 415–463. Springer US.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Tom M Mitchell. 1997. Machine learning. 1997. Burr
Enhanced sentiment learning using Twitter hashtags Ridge, IL: McGraw Hill, 45.
</table>
<reference confidence="0.98655191025641">
and smileys. In Proceedings of the 23rd International Saif M. Mohammad and Peter D. Turney. 2010. Emo-
Conference on Computational Linguistics: Posters, tions evoked by common words and phrases: Using
COLING ’10, pages 241–249, Beijing, China. Mechanical Turk to create an emotion lexicon. In Pro-
Andrea Esuli and Fabrizio Sebastiani. 2006. SENTI- ceedings of the NAACL-HLT Workshop on Computa-
WORDNET: A publicly available lexical resource for tional Approaches to Analysis and Generation of Emo-
8 tion in Text, LA, California.
Saif Mohammad, Cody Dunne, and Bonnie Dorr. 2009.
Generating high-coverage semantic orientation lexi-
cons from overtly marked words and a thesaurus. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing: Volume 2, EMNLP
’09, pages 599–608, Singapore.
S. Mohammad, S. Kiritchenko, and X. Zhu. 2013a.
NRC-Canada: Building the state-of-the-art in senti-
ment analysis of tweets. In Proceedings of the Inter-
national Workshop on Semantic Evaluation, SemEval
’13, Atlanta, Georgia, USA, June.
Saif M. Mohammad, Bonnie J. Dorr, Graeme Hirst, and
Peter D. Turney. 2013b. Computing lexical contrast.
Computational Linguistics, 39(3):555–590.
Saif Mohammad. 2012. #emotional tweets. In Pro-
ceedings of the First Joint Conference on Lexical
and Computational Semantics, *SEM ’12, pages 246–
255, Montr´eal, Canada. Association for Computa-
tional Linguistics.
Karo Moilanen and Stephen Pulman. 2007. Senti-
ment composition. In Proceedings of RANLP 2007,
Borovets, Bulgaria.
Charles E Osgood, George J Suci, and Percy Tannen-
baum. 1957. The measurement of meaning. Univer-
sity of Illinois Press.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics, ACL ’05, pages 115–124.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1–2):1–135.
Richard Socher, Jeffrey Pennington, Eric Huang, An-
drew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predicting
sentiment distributions. In Conference on Empirical
Methods in Natural Language Processing.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ’12, Jeju, Korea.
Association for Computational Linguistics.
Richard Socher, Alex Perelygin, Jean Y. Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng, and
Christopher Potts. 2013. Recursive deep models for
semantic compositionality over a sentiment treebank.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ’13,
Seattle, USA. Association for Computational Linguis-
tics.
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu,
and Bing Qin. 2014. Learning sentiment-specific
word embedding for twitter sentiment classification.
In Proceedings of ACL, Baltimore, Maryland, USA,
June.
Peter Turney and Michael L Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information
Systems, 21(4).
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the Confer-
ence on Human Language Technology and Empirical
Methods in Natural Language Processing, HLT ’05,
pages 347–354, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Xiaodan Zhu, Hongyu Guo, Saif Mohammad, and Svet-
lana Kiritchenko. 2014. An empirical study on the
effect of negation words on sentiment. In Proceedings
of ACL, Baltimore, Maryland, USA, June.
</reference>
<page confidence="0.997113">
9
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.716206">
<title confidence="0.9943575">Neural Networks for Integrating Compositional and Sentiment in Sentiment Composition</title>
<author confidence="0.981119">Xiaodan Zhu</author>
<author confidence="0.981119">Hongyu</author>
<affiliation confidence="0.998726">National Research Council</affiliation>
<address confidence="0.9883475">1200 Montreal Road, Ottawa, ON K1A 0R6,</address>
<email confidence="0.793611">Parinaz</email>
<affiliation confidence="0.998384">EECS, University of</affiliation>
<address confidence="0.993724">800 King Edward Ottawa, ON K1N 6N5, Canada</address>
<email confidence="0.971658">psobh090@uottawa.ca</email>
<abstract confidence="0.998514058823529">This paper proposes neural networks for integrating compositional and non-compositional in the process of compoa type of semantic composition that optimizes a sentiment objective. We enable individual composition operations in a recursive process to possess the capability of choosing and merging information from these two types of sources. We propose our models in neural network frameworks with structures, in which the merging parameters can be learned in a principled way to optimize a well-defined objective. We conduct experiments on the Stanford Sentiment Treebank and show that the proposed models achieve better results over the model that lacks this ability.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>