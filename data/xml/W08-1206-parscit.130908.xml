<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.993416">
Polysemy in verbs: systematic relations between senses
and their effect on annotation
</title>
<author confidence="0.994035">
Anna Rumshisky
</author>
<affiliation confidence="0.862898666666667">
*Dept. of Computer Science
Brandeis University
Waltham, MA USA
</affiliation>
<email confidence="0.970985">
arum@cs.brandeis.edu
</email>
<author confidence="0.757611">
Olga Batiukova†*
</author>
<affiliation confidence="0.905888">
†Dept. of Spanish Philology
Madrid Autonomous University
</affiliation>
<address confidence="0.881714">
Madrid, Spain
</address>
<email confidence="0.998135">
volha.batsiukova@uam.es
</email>
<sectionHeader confidence="0.993859" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999834692307692">
Sense inventories for polysemous predicates
are often comprised by a number of related
senses. In this paper, we examine different
types of relations within sense inventories and
give a qualitative analysis of the effects they
have on decisions made by the annotators and
annotator error. We also discuss some common
traps and pitfalls in design of sense inventories.
We use the data set developed specifically for
the task of annotating sense distinctions depen-
dent predominantly on semantics of the argu-
ments and only to a lesser extent on syntactic
frame.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999892666666666">
Lexical ambiguity is pervasive in natural language, and
its resolution has been used to improve performance of
a number of natural language processing (NLP) appli-
cations, such as statistical machine translation (Chan
et al., 2007; Carpuat and Wu, 2007), cross-language
information retrieval and question answering (Resnik,
2006). Sense differentiation for the predicates depends
on a number of factors, including syntactic frame, se-
mantics of the arguments and adjuncts, contextual clues
from the wider context, text domain identification, etc.
Preparing sense-tagged data for training and evalua-
tion of word sense disambiguation (WSD) systems in-
volves two stages: (1) creating a sense inventory and
(2) applying it in annotation. Creating sense invento-
ries for polysemous words is a task that is notoriously
difficult to formalize. For polysemous verbs especially,
constellations of related meanings make this task even
more difficult. In lexicography, “lumping and splitting”
senses during dictionary construction – i.e. deciding
when to describe a set of usages as a separate sense
– is a well-known problem (Hanks and Pustejovsky,
</bodyText>
<note confidence="0.723207">
© 2008. Licensed under the Creative Commons
</note>
<footnote confidence="0.967772666666667">
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.999163093023256">
2005; Kilgarriff, 1997). It is often resolved on an ad-
hoc basis, resulting in numerous cases of “overlapping
senses”, i.e. instances when the same occurrence may
fall under more than one sense category simultaneously.
This problem has also been the subject of extensive
study in lexical semantics, addressing questions such
as when the context selects a distinct sense and when
it merely modulates the meaning, what is the regular
relationship between related senses, and what composi-
tional processes are involved in sense selection (Puste-
jovsky, 1995; Cruse, 1995; Apresjan, 1973). A num-
ber of syntactic and semantic tests are traditionally ap-
plied for sense identification, such as examining syn-
onym series, compatible syntactic environments, coor-
dination tests such as cross-understanding or zeugma
test (Cruse, 2000). None of these tests are conclu-
sive and normally a combination of factors is used.
At the recent Senseval competitions (Mihalcea et al.,
2004; Snyder and Palmer, 2004; Preiss and Yarowsky,
2001), the choice of sense inventories frequently pre-
sented problems, spurring the efforts to create coarser-
grained sense inventories (Hovy et al., 2006; Palmer et
al., 2007; Navigli, 2006).
Part of the reason for such difficulties in establish-
ing a set of senses available to a lexical item is that
the meaning of a polysemous verb is often determined
in composition and depends to the same extent on se-
mantics of the particular arguments as it does on the
base meaning of the verb itself. A number of system-
atic relations often holds between different senses of a
polysemous verb. Depending on the kind of ambiguity
involved in each case, some senses are easier to dis-
tinguish than others. Sense-tagged data (e.g. SemCor
(Landes et al., 1998), PropBank (Palmer et al., 2005),
OntoNotes (Hovy et al., 2006)) typically provides no
way to differentiate between sense distinctions moti-
vated by different factors. Treating different disam-
biguation factors separately would allow one to exam-
ine the contribution of each factor, as well as the success
of a given algorithm in identifying the corresponding
senses.
Within the scope of a sentence, syntactic frame and
semantics of the arguments are most prominent in sense
</bodyText>
<page confidence="0.995762">
33
</page>
<note confidence="0.8491505">
Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 33–41
Manchester, August 2008
</note>
<bodyText confidence="0.999938441176471">
disambiguation. The latter is often more subtle and
hence complex. Our goal in the present study was to tar-
get sense distinctions motivated strongly or exclusively
by differences in argument semantics. We base the
present discussion on the sense-tagged data set we de-
veloped for 20 polysemous verbs. We argue below that
cases which can not be reliably disambiguated by hu-
mans introduce noise into the data and therefore should
be kept out, a principle adhered to in the design of this
data set.
The choice of argument semantics as the target dis-
ambiguation factor was motivated by several consider-
ations. In automatic sense detection systems, argument
semantics is often represented using external resources
such as thesauri or shallow ontologies. Sense induction
systems using distributional information often do not
take into account the possible implications of induced
word clusters for sense disambiguation. Our goal was
to analyze differences in argument semantics that con-
tribute to disambiguation.
In this paper, we discuss different kinds of systematic
relations observed between senses of polysemous pred-
icates and examine the effects they have on decisions
made by the annotators. We also examine sense in-
ventories for other factors that influence inter-annotator
agreement rates and lead to annotation error. In Section
2, we discuss some of the factors that influence com-
pilation of sense inventories and the methodology in-
volved. In Section 3, we describe briefly the data set
and the annotation task. In Sections 4 and 5, we discuss
the relations observed between different senses within
sense inventories in our data set, their effect on deci-
sions made by the annotators, and the related annotation
errors.
</bodyText>
<sectionHeader confidence="0.908056" genericHeader="method">
2 Defining A Sense Inventory
</sectionHeader>
<bodyText confidence="0.999759243589744">
Several current resource-oriented projects undertake to
formalize the procedure of identifying a word sense.
FrameNet (Ruppenhofer et al., 2006) attempts to orga-
nize lexical information in terms of script-like semantic
frames, with semantic and syntactic combinatorial pos-
sibilities specified for each frame-evoking lexical unit
(word/sense pairing). Semantics of the arguments is
represented by Fillmore’s case roles (frame elements)
which are derived on ad-hoc basis for each frame.
In OntoNotes project, annotators use small-scale cor-
pus analysis to create sense inventories derived by
grouping together WordNet senses. The procedure is
restricted to maintain 90% inter-annotator agreement
(Hovy et al., 2006).
Corpus Pattern Analysis (CPA) (Hanks and Puste-
jovsky, 2005; Pustejovsky et al., 2004) attempts to cat-
alog prototypical norms of usage for individual words,
specifying them in terms of context patterns. As a cor-
pus analysis technique, CPA has its origins in the anal-
ysis of large corpora for lexicographic purposes, of the
kind that was used for compiling the Cobuild dictionary
(Sinclair and Hanks, 1987). Each pattern gives a com-
bination of surface textual clues and argument specifi-
cations. A lexicographer creates a set of patterns by
sorting a concordance for the target predicate according
to the context features. In the present study, we use a
modification of the CPA technique in the way explained
in Section 3.
In CPA, syntactic and textual clues include argu-
ment structure and minor syntactic categories such as
locatives and adjuncts; collocates from wider context;
subphrasal cues such as genitives, partitives, bare plu-
ral/determiner, infinitivals, negatives, etc. Semantics
of the arguments is represented either through a set of
shallow semantic types corresponding to basic seman-
tic features (e.g. Person, Location, PhysObj, Abstract,
Event, etc.) or extensionally through lexical sets, which
are effectively collections of lexical items.1
Several CPA patterns may correspond to a single
sense. The patterns vary in syntactic structure or the en-
coding of semantic roles relative to the described event.
For example, for the verb treat, DOCTOR treating PA-
TIENT and DOCTOR treating DISEASE both correspond
to the medical sense of treat. Knowing which seman-
tic role is expressed by a particular argument is often
useful for performing inference. For instance, treating
a disease eliminates the disease, but not the patient. In
the present annotation task, each pattern is viewed as
sense in construction and labeled as a separate sense.
In the rest of the paper, we will use the term “sense” to
refer also to such microsenses.
For the cases where sense differentiation depends
strongly on differences in semantics of the arguments,
several factors further complicate creating a sense in-
ventory. Prototypicality as a general principle of cat-
egory organization seems to play an important role in
defining both the boundaries of senses and the corre-
sponding argument groupings. The same sense of the
predicate is often activated by a number of semantically
diverse arguments. Such argument sets are frequently
organized around a core of typical members that are
a “good fit” with respect to semantic requirements of
the corresponding sense of the target. The relevant se-
mantic feature is prominent for them, while other, more
peripheral members of the argument set, merely allow
the relevant interpretation (see Rumshisky (2008) for
discussion). For example, the verb absorb has a sense
involving absorbing a substance, and the typical mem-
bers of the corresponding argument set would be actual
substances, such as oil, oxygen, water, air, salt, etc. But
goodness, dirt, flavor, moisture would also activate the
same sense.
Each decision to split a sense and make another cat-
egory is to a certain extent an arbitrary decision. For
example, for the verb absorb, one can separate absorb-
ing a substance (oil, oxygen, water, air, salt) from ab-
sorbing energy (radiation, heat, sound, energy). The
latter sense may or may not be separated from absorb-
</bodyText>
<footnote confidence="0.997213">
1See Rumshisky et al. (2006) and Pustejovsky et al. (2004)
for more detail.
</footnote>
<page confidence="0.999385">
34
</page>
<bodyText confidence="0.999842888888889">
ing impact (blow, shock, stress). But it is a marked con-
tinuum, i.e. certain points in the continuum are more
prominent, with necessity of a given concept reflected
in the frequency of use.
When several senses are postulated based on argu-
ment distinctions, there are almost always boundary
cases that can be seen to belong to both categories.
Consider, for example, two senses defined for the verb
launch and the corresponding direct objects in (1):
</bodyText>
<page confidence="0.189937">
(1) a. Physically propel an object into the air or water
</page>
<tableCaption confidence="0.4903405">
missile, rocket, torpedo, satellite, shuttle, craft
b. Begin or initiate an endeavor
campaign, initiative, investigation, expedition, drive,
competition, crusade, attack, assault, inquiry
</tableCaption>
<bodyText confidence="0.99763975">
The senses seem to be very clearly separated, yet ex-
amples like launch a ship clearly fall on the bound-
ary: while ships are physical objects propelled into wa-
ter, launching a ship can be virtually synonymous with
launching an expedition.
Similarly, for the verb conclude, two senses below
which are linked to nominal complements are clearly
separated:
</bodyText>
<equation confidence="0.21743925">
(2) a. finish
meeting, debate, investigation, visit, tour, discussion;
letter, chapter, novel
b. reach an agreement
</equation>
<bodyText confidence="0.978757347826087">
treaty, agreement, deal, contract, truce, alliance,
ceasefire, sale
However, conclude negotiations is clearly a boundary
case where both interpretations are equally possible
(negotiations may be concluded without reaching an
agreement). In fact, the two annotators chose different
senses for this example:2
(3) We were able to operate under a lease agreement until
purchase negotiations were concluded.
annoA: finish
annoB: reach an agreement
In many cases, postulating a separate sense for a co-
herent set of nominal complements is not justified, as
there are regular semantic processes that allow the com-
plements to satisfy selectional requirements of the verb.
For example, the verb conclude, in the finish sense ac-
cepts EVENT complements. Therefore, nouns such as
letter, chapter, novel in (2) must be coerced into events
corresponding to the activity that typically brings them
about, that is, re-interpreted as events of writing (their
Agentive quale, cf. Pustejovsky (1995)). Similarly, the
verb deny in the first sense (state or maintain that some-
thing is untrue) accepts PROPOSITION complements:
</bodyText>
<listItem confidence="0.997333">
(4) a. state or maintain that something is untrue
allegations, reports, rumour; significance, impor-
tance, difference; attack, assault, involvement
b. refuse to grant something
</listItem>
<bodyText confidence="0.348386">
access, visa, approval, funding, license
</bodyText>
<footnote confidence="0.737528666666667">
2All examples are taken from the annotated data set.
In some cases, sentence structure was slightly modified for
brevity.
</footnote>
<bodyText confidence="0.967566875">
Event nouns such as attack and assault are coerced into
a propositional reading, as are relational nouns such as
significance and importance.
Interestingly, as we have noted before (Rumshisky
et al., 2006), each predicate imposes its own gradation
with respect to prototypicality of elements of the ar-
gument set. As a result, even though basic semantic
types such as PHYSOBJ, ANIMATE, EVENT, are used
uniformly by many predicates, argument sets, while se-
mantically similar, typically differ between predicates.
For example, fall in the subject position and cut in the
direct object position select for things that can be de-
creased:
(5) a. cut (dobj): reduce or lessen
price, inflation, profits, cost, emission, spending,
deficit, wages overhead, production, consumption,
fees, staff
b. fall (subj): decrease
price, inflation, profits, attendance, turnover, temper-
ature, membership, import, demand, level
While there is a clear commonality between these argu-
ment sets, the overlap is only partial. To give another
example, consider INFORMATION-selecting predicates
explain (subj), grasp (dobj) and know (dobj). The nouns
book and note occur in the subject position of explain;
answer occurs both as the subject of explain and direct
object of know; however, grasp accepts neither of these
nouns as direct object. Thus, the actual selectional be-
havior of the predicates does not seem to be well de-
scribed in terms of a fixed set of types, which is what
is typically assumed by many ontologies used in auto-
matic WSD.
</bodyText>
<sectionHeader confidence="0.991594" genericHeader="method">
3 Task Description
</sectionHeader>
<bodyText confidence="0.999873428571429">
We were interested specifically in those cases where
disambiguation needs to be made without relying on
syntactic frame, and the main source of disambiguation
is semantics of the arguments. Such cases are harder
to identify formally in the development of sense inven-
tories and harder for the annotators to determine. For
example, phrasal verbs or idiomatic constructions that
help identify a particular sense were intentionally ex-
cluded from our data set. Thus, for the verb cut, one of
the senses involves cutting out a shape or a form (e.g.
cut a suit), but the sentences with the corresponding
phrasal form cut out were thrown out.
Even so, syntactic clues that contribute to disam-
biguation in some cases overrule the interpretation sug-
gested by the argument. For example, for the verb deny,
in deny the attack, the direct object strongly suggests
a propositional interpretation for deny (that the attack
didn’t happen). However, the use of ditransitive con-
struction (indicated in the example below by the past
participle) overrules this interpretation, and we get the
refuse to grant sense:
</bodyText>
<listItem confidence="0.6307635">
(6) Astorre, denied his attack, had stayed in camp, uneasily
brooding.
</listItem>
<page confidence="0.996714">
35
</page>
<bodyText confidence="0.99998525">
In fact, during the actual annotation, one of the anno-
tators did not recognize the use of past participle, and
erroneously assigned the state or maintain something to
be untrue sense to this sentence.
</bodyText>
<subsectionHeader confidence="0.999424">
3.1 Data set
</subsectionHeader>
<bodyText confidence="0.966615204081633">
The data set was developed using the British National
Corpus (BNC), which is more balanced than the more
commonly annotated Wall Street Journal data. We se-
lected 20 polysemous verbs with sense distinctions that
were judged to depend for disambiguation on seman-
tics of the argument in several argument positions, in-
cluding direct object (dobj), subject (subj), or indirect
object within a prepositional phrase governed by with
(iobj with):
dobj: absorb, acquire, admit, assume, claim, conclude,
cut, deny, dictate, drive, edit, enjoy, fire, grasp, know,
launch
subj: explain, fall, lead
iobj with: meet
We used the Sketch Engine (Kilgarriff et al., 2004)
both to select the verbs and to aid the creation of the
sense inventories. The Sketch Engine is a lexicographic
tool that lists collocates that co-occur with a given target
word in the specified grammatical relation. The collo-
cates are sorted by their association score with the tar-
get.
A set of senses was created for each verb using a
modification of the CPA technique (Pustejovsky et al.,
2004). A set of complements was examined in the
Sketch Engine. If a clear division was observed be-
tween semantically different groups of collocates in a
certain argument position, the verb was selected. For
semantically distinct groups of collocates, a separate
sense was added to the sense inventory for the target.
For example, for the verb acquire, a separate sense was
added for each of the following sets of direct objects:
(7) a. Take on certain characteristics
shape, meaning, color, form, dimension, reality, sig-
nificance, identity, appearance, characteristic, flavor
b. Purchase or become the owner ofproperty
land, stock, business, property, wealth, subsidiary, es-
tate, stake
The sense inventory for each verb was cross-checked
against several resources, including WordNet, Prop-
Bank, Merriam-Webster and Oxford English dictionar-
ies, and existing correspondences in FrameNet (Rup-
penhofer et al., 2006; Hiroaki, 2003), OntoNotes (Hovy
et al., 2006),3 and CPA patterns (Hanks and Puste-
jovsky, 2005; Rumshisky and Pustejovsky, 2006; Puste-
jovsky et al., 2004).
We performed test annotation on 100 instances, with
the sense inventory additionally modified upon exam-
ining the results of the annotation. This sense inven-
tory was provided to two annotators, along with 200
</bodyText>
<footnote confidence="0.473318">
3Sense inventories released for the 65 verbs made avail-
able for SemEval-2007.
</footnote>
<bodyText confidence="0.9990595">
sentences for each verb. Each sentence was pre-parsed
with RASP (Briscoe and Carroll, 2002), and the head
of the target argument phrase was identified. Misparses
were manually corrected in post-processing.
</bodyText>
<subsectionHeader confidence="0.998914">
3.2 Defining the task for the annotators
</subsectionHeader>
<bodyText confidence="0.999982238095238">
Data set creation for a WSD task is notoriously hard (cf.
Palmer et al. (2007)), as the annotators are frequently
forced to perform disambiguation on sentences where
no disambiguation can really be performed. This is the
case, for example, for overlapping senses, where more
than one sense is activated simultaneously (Rumshisky,
2008; Pustejovsky and Boguraev, 1993). The goal was
to create, for each target word, a set of instances where
humans had no trouble disambiguating between differ-
ent senses.
Two undergraduate linguistics majors served as an-
notators. The annotators were instructed to mark each
sentence with the most fitting sense. The annotators
were allowed to mark the sentence as “N/A” and were
instructed to do so if (i) the sense inventory was missing
the relevant sense, (ii) more than one sense seemed to
fit, or (iii) the sense was impossible to determine from
the context.
With respect to metaphoric senses, instructions were
to throw out cases of creative use where the interpreta-
tion was difficult or not immediately clear. The cases
where the target grammatical relation was actually ab-
sent from the sentence also had to be marked as “N/A”
(e.g. for fire, sentences without direct object, e.g. a
stolen car was fired upon). The annotators were also
instructed to mark idiomatic expressions and phrasal
verbs as “N/A”, e.g. for the verb fall: fall from favor,
fall through, fall in, fall back, fall silent, fall short, fall
in love.
Disagreements between the annotators were resolved
in adjudication by the co-authors. The average inter-
annotator agreement (ITA) for our data set was com-
puted as a macro-average of the percentage of instances
that were annotated with the same sense by both anno-
tators to the total number of instances retained in the
data set for each verb. The instances that were marked
as “N/A” by one of the annotators (or thrown out during
the adjudication) were not included in the computation.
The ITA value for our data set was 95%. However, as
we will see below, the ITA values do not always reflect
the actual accuracy of annotation, due to some common
problems with sense inventories.
</bodyText>
<subsectionHeader confidence="0.999719">
3.3 Glossing a sense
</subsectionHeader>
<bodyText confidence="0.99988125">
A very common problem with glossing a sense in-
volves the situation where a sense inventory includes
two senses one of which is an extension of the other.
The derived sense may be related to the primary sense
through metaphor, and this often results in the for-
mer taking on a semantically less specific interpreta-
tion. The problem with creating glosses in this situa-
tion is that the words used may have sense distinctions
</bodyText>
<page confidence="0.991646">
36
</page>
<bodyText confidence="0.942303466666667">
parallel to the ones in the target verb being described.
This leaves the annotators free to choose either sense.
This seems to be the case, for example, with OntoNotes
sense inventory for fire, where ignite or become ignited
is the gloss under which very divergent examples are
grouped: oil fired the furnace (literal, primary sense)
and curiosity fired my imagination (metaphoric exten-
sion). Clearly, annotators were having a problem with
this sense due to the fact that the verb ignite has sense
distinctions which are based on the same metaphor (fire
= inspire) and therefore are very similar to those of the
verb fire.
In case of semantic underspecification, annotators
may be left free to choose the more generic sense,
which contaminates the data set while not being re-
flected in the inter-annotator agreement values. For ex-
ample, in our sense inventory for acquire, the gloss for
acquire a new customer has to be very generic. We
used the gloss “become associated with something, of-
ten newly brought into being”. However, that led the
annotators to overuse this gloss and select this sense in
cases where a more specific gloss was more appropri-
ate:4
(8) By this treaty, Russia acquired a Black Sea coastline.
annoA: become associated with something, often newly
brought into being
annoB: become associated with something,...
correct: purchase or become the owner ofproperty
For a more detailed analysis of this phenomenon, see
Section 5.
</bodyText>
<sectionHeader confidence="0.992392" genericHeader="method">
4 Relations Between Senses
</sectionHeader>
<bodyText confidence="0.999451857142857">
In this section, we discuss linguistic processes underly-
ing relations between senses within a single sense in-
ventory. We believe that a detailed analysis of these
processes should help to account for the annotator’s
ability to perform disambiguation. Some sense distinc-
tions appear more striking to the annotators, depending
on the type of relation involved.
In line with existing approaches to sense relations,
we will look at both the linguistic structures involved
in sense modification and the productive processes act-
ing on linguistic structures. For the purposes of our
present discussion, we interpret the literal (physical, di-
rect) senses to be primary, with respect to more abstract
or metaphorical senses.
</bodyText>
<subsectionHeader confidence="0.998848">
4.1 Argument structure alternations
</subsectionHeader>
<bodyText confidence="0.963004571428571">
Some of the most striking differences between the
senses are related to the argument structure alternations:
1. Different case roles (frame elements) may be ex-
pressed in the same argument position (in this case, di-
rect object), corresponding to different perspectives on
the same event. For example, direct object position of
the verb drive may be filled by VEHICLE, DISTANCE,
</bodyText>
<footnote confidence="0.787493">
4We will refer to annotators A and B as annoA and annoB.
</footnote>
<bodyText confidence="0.9949784">
or PHYSOBJ giving rise to three distinct senses: (i) op-
erate a vehicle controlling its motion, (ii) travel in a ve-
hicle a certain distance, and (iii) transport something or
someone. Similarly, for the verb fire, PROJECTILE or
WEAPON in direct object position give rise to two re-
lated senses: (i) shoot, discharge a weapon, (ii) shoot,
propel a projectile.
2. The distinction between propositional and non-
propositional complements, as for the verbs admit and
deny in (9) and (10):
</bodyText>
<listItem confidence="0.781701333333333">
(9) a. admit defeat, inconsistency, offense
(acknowledge the truth orreality of)
b. admitpatients, students
</listItem>
<bodyText confidence="0.8309543">
(grant entry or allow into a community)
(10) a. deny reports, importance, allegations
(state or maintain to be untrue)
b. deny visa, access
(refuse to grant)
3. There is a mutual dependency between subcate-
gorization features of the complements in different ar-
gument positions. For example, the [+animate] subject
may combine with specific complements not available
for [−animate], as for the two senses of acquire: (i)
learn and (ii) take on certain characteristics. Compare
NPsubj [-animate] acquire NPdobj (language, man-
ners, knowledge, skill) vs. NPsubj [−animate] acquire
NPdobj (importance, significance). Similarly, for ab-
sorb, compare NPsubj [±animate] absorb NPdobj (sub-
stance) and NPsubj [+animate] absorb NPdobj (skill,
information). Note that, as one would expect, such de-
pendencies are inevitable even despite the fact that our
data set was developed specifically to target sense dis-
tinctions dependent on a single argument position.
</bodyText>
<subsectionHeader confidence="0.996832">
4.2 Event structure modification
</subsectionHeader>
<bodyText confidence="0.98868025">
Event structure modifications (i.e. operations affecting
aspectual properties of the predicate) are another source
of sense differentiation. Two cases appear most promi-
nent:
</bodyText>
<listItem confidence="0.985183684210526">
1. The event structure is modified along with the
characteristics of the arguments. For example, for en-
joy, compare enjoy skiing, vacation (DYNAMIC EVENT)
with enjoying a status (STATE). Similarly, for lead,
compare a person leads smb somewhere (PROCESS) vs.
a road (PATH) leads somewhere (STATE); for explain,
compare something or somebody explains smth (= clar-
ifies, describes, makes comprehensible, PROCESS) vs.
something [−inanimate, +abstract] explains something
(= is a reason for something, STATE); for fall, compare
PHYSOBJ falls (TRANSITION or ACCOMPLISHMENT)
vs. a case falls into a certain category (STATE).
2. The aspectual nature of the predicate is the only
semantically relevant feature that remains unchanged
after consecutive sense modifications. For example, the
ingressive meaning of ‘beginning something’ is pre-
served in shifting from the physical sense of the verb
launch in launch a missile to launch a campaign and
launch a product.
</listItem>
<page confidence="0.998105">
37
</page>
<subsectionHeader confidence="0.996378">
4.3 Lexical semantic features
</subsectionHeader>
<bodyText confidence="0.999884">
Sense distinctions often involve deeper semantic char-
acteristics of the verbs which could be accounted for by
means of lexical semantic features such as qualia struc-
ture roles in Generative Lexicon (Pustejovsky, 1995):5
</bodyText>
<listItem confidence="0.883436866666667">
1. Consider how the meaning component ‘manner
of motion’ (typically associated with the agentive role)
gets transformed in the different senses of drive. It is
obviously present in the physical uses of drive (such
as operate a vehicle, transport something or somebody,
etc.), but is completely lost in motivate the progress
of (as in drive the economy, drive the market forward,
etc.). The value of the agentive role of drive becomes
underspecified or semantically weak, so that the overall
meaning of drive is transformed to cause something to
move.
2. Information about semantic type contained in
qualia structure allows apparently diverse elements to
activate the same sense of the verb. For instance, the
verb absorb in the sense learn or incorporate skill or
</listItem>
<bodyText confidence="0.919973615384616">
information occurs with direct objects such as values,
atmosphere, information, idea, words, lesson, attitudes,
culture. The requisite semantic component is realized
differently for each of these words. Some of them are
complex types6 with INFORMATION as one of the con-
stituent types: words (ACOUSTIC/VISUAL ENTITY •
INFO), lesson (EVENT • INFO). Others, such as idea,
are polysemous, with one of the senses being INFOR-
MATION. Cases like culture and values are more diffi-
cult, but since they refer to knowledge, the INFORMA-
TION component is clearly present. Consequently, the
annotators are able to identify the corresponding sense
of absorb with a high degree of agreement.
</bodyText>
<subsectionHeader confidence="0.968654">
4.4 Metaphor and metonymy
</subsectionHeader>
<bodyText confidence="0.99804125">
The processes causing the mentioned meaning trans-
formations in our corpus often involve metaphor and
metonymy. Below are some of the conventionalized ex-
tensions with metaphorical flavor:
</bodyText>
<listItem confidence="0.3553292">
(11) a. grasp object vs. grasp meaning
b. launch object vs. launch an event (campaign, as-
sault) or launch a product (newspaper, collection)
c. meet with a person vs. meet with success, resistance
d. lead somebody somewhere vs. lead to a consequence
</listItem>
<bodyText confidence="0.99991475">
Note that these metaphorical extensions involve ab-
stract or continuous objects (meaning, assault, success,
consequence), which in turn cause event structure mod-
ifications (lead as a process vs. lead as a state). Thus,
the processes and structures we are dealing with are
clearly interrelated.
The metonymical process can be exemplified by edit
as make changes to the text and as supervise publica-
</bodyText>
<footnote confidence="0.997412333333333">
5We will use the terminology from Generative Lexicon
(Pustejovsky, 1995; Pustejovsky, 2007) to discuss lexical se-
mantic properties, such as qualia roles, complex and func-
tional types, and so on.
6Complex type is a term used for concepts that inherently
refer to more than one semantic type.
</footnote>
<bodyText confidence="0.999243909090909">
tion, which are in a clear contiguity relationship.
One of the effects of the metaphorization and pro-
gressive emptying of the primary (physical, concrete)
senses is the distinction between generic and specific
senses. For example, compare acquire land, business
(specific sense) to acquire an infection, a boyfriend, a
following, which refers to some extremely light generic
association. Similar process is observed for the seman-
tically weak sense offall, be associated with or get as-
signed to a person or location or for event to fall onto a
time:
</bodyText>
<listItem confidence="0.5952855">
(12) Birthdays, lunches, celebrations fall on a certain date or
time
</listItem>
<bodyText confidence="0.94650625">
Stress or emphasis fall on a given topic or a syllable
Responsibility, luck, suspicion fall on or to a person
The specificity often involves specialization within a
certain domain:
</bodyText>
<listItem confidence="0.810587333333333">
(13) a. conclude as finish vs. conclude as reach an agree-
ment (Law, Politics)
b. fire as shoot a weapon or a projectile vs. fire as kick
</listItem>
<bodyText confidence="0.948796666666667">
orpass an object ofplay in sports (Sport)
Thus, when concluding a pact or an agreement, a cer-
tain EVENT is also being finished (negotiation of that
agreement), necessarily with a positive outcome.
In the following section, we will try to show how dif-
ferent kinds of relations between senses influence dis-
ambiguation carried out by the annotators. In particular,
we look at different sources of disagreement and anno-
tator error as determined in adjudication.
</bodyText>
<subsectionHeader confidence="0.590169">
5 Analysis of Annotation Decisions
</subsectionHeader>
<bodyText confidence="0.983138375">
As we have seen above, in many cases disambigua-
tion is impossible due to the nature of compositional-
ity. Also, as there are no clear answers to a number of
questions concerning sense identification, the annota-
tors deal with sense inventories that are imperfect. Re-
sults of the disambiguation task carried out by the an-
notators reflect all these defects.
In cases when a specific meaning from the data set
is not included into the sense inventory (e.g. due to its
low frequency or extreme fine-grainedness) the annota-
tors may use a more general meaning or pick the clos-
est meaning available. For example, within the sense
inventory forfire, there was no separate gloss for fire an
engine. Annotator A in our experiment chose the clos-
est specific meaning available, and Annotator B marked
it with a more generic sense:
(14) Engineers successfully fired thrusters to boost the re-
search satellite to an altitude of 507 km.
annoA: shoot, propel a projectile
annoB: apply fire to
As mentioned in Section 3.3, even when the appropriate
specific sense is available, annotators frequently chose
the more generic sense in its place, as in (15), (16) and
(17), and also in (8).
</bodyText>
<page confidence="0.996825">
38
</page>
<bodyText confidence="0.759398777777778">
(15) Several referrals fell into this category.
annoA: be associated with or get assigned to a person
or location or for event to fall onto a time
annoB: be categorized as or fall into a range
(16) The terrible silence had fallen.
annoA: be associated with or get assigned to a person
or location or for event to fall onto a time
annoB: fora state (such as darkness orsilence) to come,
to commence
</bodyText>
<listItem confidence="0.799988">
(17) He acquired a taste for performing in public.
</listItem>
<tableCaption confidence="0.42793325">
annoA: become associated with something, often
newly brought into being
annoB: become associated with something, ...
correct: learn
</tableCaption>
<bodyText confidence="0.990859315789474">
Note that in (8) this decision was probably motivated by
the annotators’ uncertainty about the semantic ascrip-
tion of the relevant argument (coastline is not a proto-
typical owned property). The generic sense seems to be
the safest option to take for the annotators, as compared
to taking a chance with a specific meaning. Due to its
low degree of semantic specification, the generic sense
is potentially able to embrace almost every possible use.
This is not a desirable outcome because the generic
senses are introduced in the inventory to account only
for semantically underspecified cases. For instance, be-
come associated with something, often newly brought
into being is appropriate for acquire a grandchild, but
not for acquire a taste or acquire a proficiency.
Remarkable variation is also observed with respect to
non-literal uses as discussed in Section 4.4. For exam-
ple, in (18) and (19) abstract NPs panic and imbalance
offorces are equated with energy or impact by one an-
notator and with substance by the other.
</bodyText>
<listItem confidence="0.833547428571429">
(18) Her panic was absorbed by his warmth.
annoA: absorb energy or impact
annoB: absorb substance
(19) Alternatively, imbalance of forces can be absorbed into
the body.
annoA: absorb energy or impact
annoB: absorb substance
</listItem>
<bodyText confidence="0.9336855">
In some cases, the literal and the metaphoric senses
are activated simultaneously resulting in ambiguity (cf.
Cruse (2000)):
(20) For over 300 years this waterfall has provided the en-
ergy to drive the wheels of industry.
annoA: motivate the progress of
annoB: provide power for or physically move a mech-
anism
</bodyText>
<listItem confidence="0.931166333333333">
(21) But fashion changed and the short skirtfell – literally –
from favour and started skimming the ankles.
annoA: lose power or suffer a defeat
annoB: N/A
(22) She was delighted when the story of Hank fell into her
lap.
</listItem>
<bodyText confidence="0.939550852459017">
annoA: be associated with or get assigned to a person
or location or for event to fall onto a time
annoB: physically drop; move or extend downward
Impact of subcategorization features on disam-
biguation (cf. Section 4.1 para 3) is illustrated in (23).
(23) The reggae tourist can easily absorb the current reggae
vibe.
annoA: absorb energy or impact
annoB: learn or incorporate skill or information
Both interpretations chosen here (absorb energy or im-
pact and learn or incorporate skill or information) were
possible due to the animacy of the subject, which acti-
vates two different subcategorization frames and subse-
quently two different senses.
Typically, cases where semantic type of the relevant
arguments (cf. Section 4.3 para 2) is not clear result in
annotator disagreement:
(24) The AAA launched education programs.
annoA: begin or initiate an endeavor (EVENT)
annoB: begin to produce or distribute; start a company
(PRODUCT)
(25) France plans to launch a remote-sensing vehicle called
Spot.
annoA: physically propel into the air, water or space
(PHYSOBJ)
annoB: begin to produce or distribute; start a company
(PRODUCT)
The two cases above are interesting in that both pro-
gram and vehicle are ambiguous and can be analyzed
semantically as members of different semantic classes.
This is what the annotators in fact do, and as a result,
ascribe them to different senses. Program can be cate-
gorized as EVENT (‘series of steps’) or as INTELLEC-
TUAL ACTIVITY PRODUCT (‘document or system of
projects’). It is a complex type, i.e. it is an inherently
polysemous word that represents at least two different
semantic types. Vehicle, in turn, is a functional type:
on the one hand, it represents an entity with certain for-
mal properties (PHYSOBJ interpretation), on the other
hand, it is an artifact, with a prominent practical pur-
pose (PRODUCT interpretation).
In fact, most problems the annotators had with the
task are due to the inherent semantic complexity of
words such as vehicle and program in (24) and (25) and
to the existence of boundary cases, where the relevant
noun does not properly belong to one or another seman-
tic category. This is the case with panic, imbalance or
reggae vibe in (18), (19), and (23), and also with taste
and coastline in (17) and (7).
In some of these cases, other contextual clues may
come into play and tip the balance in favor of one or an-
other sense. Note that disambiguation was influenced
by a wider context even despite the intentionally re-
strictive task design (targeting a particular syntactic re-
lation for each verb). For instance, in (26), domain-
specific clues referring to war or military conflict (such
as rebel control) could have motivated Annotator B’s
decision to ascribe it to the sense lose power or suffer
a defeat (even though a road is not typically an entity
that can lose power), while the other annotator chose a
more generic meaning:
</bodyText>
<page confidence="0.997884">
39
</page>
<bodyText confidence="0.9653415">
(26) The roadfell into rebel control.
annoA: be associated with or get assigned to a person
or location or for event to fall onto a time
annoB: lose power or suffer a defeat
Other pragmatic and discourse-oriented clues played
a role, in particular, positive and negative connotation
of the senses and the relevant arguments, as well as
the temporal organization of discourse. For example, in
(27) and (28), positive or neutral interpretation of wave
of immigrants and change could have led to the choice
of take in or assimilate and learn or incorporate skill or
information senses, while the negatively-colored inter-
pretation might explain the choice of the bear the cost
of sense.
(27) ..help absorb the latest wave of immigrants.
annoA: bear the cost of; take on an expense
annoB: take in or assimilate, making part ofa whole or
a group
(28) For senior management an important lesson was the
trade unions’ capacity to absorb change and to become
its agents.
annoA: learn or incorporate skill or information
annoB: bear the cost of; take on an expense
Temporal organization of a broader discourse is an-
other important factor. For example, for the verb claim,
the senses claim the truth of and claim property you are
entitled to have different presuppositions with respect
to preexistence of the thing claimed. In (28), due to the
absence of a broader context, the annotators chose two
different temporal reference interpretations. For Anno-
tator B, success was something that has happened al-
ready, while for A this was not clear (success might
have been achieved or not):
(29) One area where the government can claim some success
involves debt repayment.
annoA: come in possession ofor claim property you are
entitled to
annoB: claim the truth of
</bodyText>
<sectionHeader confidence="0.99921" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999977527777778">
We have given a brief overview of different types of
sense relations commonly found in polysemous predi-
cates and analyzed their effect on different aspects of
the annotation task, including sense inventory design
and execution of the WSD annotation.
The present analysis suggests that theoretical tools
must be refined and further developed in order to give
an adequate account to the sense modifications found in
real corpus data. To this end, broader contextual clues
and discourse-oriented clues need to be included in the
analysis.
Semantically annotated corpora are routinely devel-
oped for the training and testing of automatic sense
detection and induction algorithms. But they do not
typically provide a way to distinguish between differ-
ent kinds of ambiguities. Consequently, it is difficult
to perform adequate error analysis for different sense
detection systems. Appropriate semantic annotation
that would allow one to determine which sense dis-
tinctions can be detected better by automatic systems
does not need to be highly specific and unnecessarily
complex, but requires development of robust general-
izations about sense relations.
One obvious conclusion is that data sets need to be
explicitly restricted to the instances where humans have
no trouble disambiguating between different senses.
Thus, prototypical cases can be accounted for reliably,
ensuring the clarity of annotated sense distinctions. At
face value, imposing such restrictions may appear to
negatively influence usability of the resulting data set
in particular applications requiring WSD, such as ma-
chine translation or information retrieval. However, this
decision impacts most strongly those boundary cases
which are not reliably disambiguated by human anno-
tators, and which rather introduce noise into the data
set.
</bodyText>
<sectionHeader confidence="0.998929" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99939975">
This work was supported in part by NSF CRI grant to
Brandeis University. The work of O. Batiukova is sup-
ported by postdoctoral grant of the Ministry of Educa-
tion of Spain and Madrid Autonomous University.
</bodyText>
<sectionHeader confidence="0.999035" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999323896551724">
Apresjan, Ju. 1973. Regular polysemy. Linguistics,
142(5):5–32.
Briscoe, T. and J. Carroll. 2002. Robust accurate sta-
tistical annotation of general text. Proceedings of
the Third International Conference on Language Re-
sources and Evaluation (LREC 2002), Las Palmas,
Canary Islands, May 2002, pages 1499–1504.
Carpuat, M. and D. Wu. 2007. Improving statistical
machine translation using word sense disambigua-
tion. In Proc. of EMNLP-CoNLL, pages 61–72.
Chan, Y. S., H. T. Ng, and D. Chiang. 2007. Word
sense disambiguation improves statistical machine
translation. In Proc. of ACL, pages 33–40, Prague,
Czech Republic, June.
Cruse, D. A. 1995. Polysemy and related phenom-
ena from a cognitive linguistic viewpoint. In Dizier,
Patrick St. and Evelyne Viegas, editors, Computa-
tional Lexical Semantics, pages 33–49. Cambridge
University Press, Cambridge, England.
Cruse, D. A. 2000. Meaning in Language, an Intro-
duction to Semantics and Pragmatics. Oxford Uni-
versity Press, Oxford, United Kingdom.
Hanks, P. and J. Pustejovsky. 2005. A pattern
dictionary for natural language processing. Revue
Franc¸aise de Linguistique Appliqu´ee.
Hiroaki, S. 2003. FrameSQL: A software tool for
FrameNet. In Proceedigns of ASIALEX ’03, pages
251–258, Tokyo, Japan. Asian Association of Lexi-
cography.
</reference>
<page confidence="0.966882">
40
</page>
<reference confidence="0.999787108433735">
Hovy, E., M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. OntoNotes: The 90% solu-
tion. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Companion Vol-
ume: Short Papers, pages 57–60, New York City,
USA, June. Association for Computational Linguis-
tics.
Kilgarriff, A., P. Rychly, P. Smrz, and D. Tugwell.
2004. The Sketch Engine. Proceedings of Euralex,
Lorient, France, pages 105–116.
Kilgarriff, A. 1997. I don’t believe in word senses.
Computers and the Humanities, 31:91–113.
Landes, S., C. Leacock, and R.I. Tengi. 1998. Build-
ing semantic concordances. In Fellbaum, C., editor,
Wordnet: an electronic lexical database. MIT Press,
Cambridge (Mass.).
Mihalcea, R., T. Chklovski, and A. Kilgarriff. 2004.
The Senseval-3 English lexical sample task. In Mi-
halcea, Rada and Phil Edmonds, editors, Senseval-3:
Third International Workshop on the Evaluation of
Systems for the Semantic Analysis of Text, pages 25–
28, Barcelona, Spain, July. Association for Compu-
tational Linguistics.
Navigli, R. 2006. Meaningful clustering of senses
helps boost word sense disambiguation performance.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 105–112, Sydney, Australia, July. Association
for Computational Linguistics.
Palmer, M., D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71–106.
Palmer, M., H. Dang, and C. Fellbaum. 2007. Making
fine-grained and coarse-grained sense distinctions,
both manually and automatically. Journal of Natu-
ral Language Engineering.
Preiss, J and D. Yarowsky, editors. 2001. Proceedings
of the Second Int. Workshop on Evaluating WSD Sys-
tems (Senseval2). ACL2002/EACL2001.
Pustejovsky, J. and B. Boguraev. 1993. Lexical knowl-
edge representation and natural language processing.
Artif. Intell., 63(1-2):193–223.
Pustejovsky, J., P. Hanks, and A. Rumshisky. 2004.
Automated Induction of Sense in Context. In COL-
ING 2004, Geneva, Switzerland, pages 924–931.
Pustejovsky, J. 1995. Generative Lexicon. Cambridge
(Mass.): MIT Press.
Pustejovsky, J. 2007. Type Theory and Lexical De-
composition. In Bouillon, P. and C. Lee, editors,
Trends in Generative Lexicon Theory. Kluwer Pub-
lishers (in press).
Resnik, P. 2006. Word sense disambiguation in NLP
applications. In Agirre, E. and P. Edmonds, editors,
Word Sense Disambiguation: Algorithms and Appli-
cations. Springer.
Rumshisky, A. and J. Pustejovsky. 2006. Induc-
ing sense-discriminating context patterns from sense-
tagged corpora. In LREC 2006, Genoa, Italy.
Rumshisky, A., P. Hanks, C. Havasi, and J. Pustejovsky.
2006. Constructing a corpus-based ontology using
model bias. In The 19th International FLAIRS Con-
ference, FLAIRS 2006, Melbourne Beach, Florida,
USA.
Rumshisky, A. 2008. Resolving polysemy in verbs:
Contextualized distributional approach to argument
semantics. Distributional Models of the Lexicon in
Linguistics and Cognitive Science, special issue of
Italian Journal ofLinguistics / Rivista di Linguistica.
forthcoming.
Ruppenhofer, J., M. Ellsworth, M. Petruck, C. Johnson,
and J. Scheffczyk. 2006. FrameNet II: Extended
Theory and Practice.
Sinclair, J. and P. Hanks. 1987. The Collins Cobuild
English Language Dictionary. HarperCollins, 4th
edition (2003) edition. Published as Collins Cobuild
Advanced Learner’s English Dictionary.
Snyder, B. and M. Palmer. 2004. The english all-words
task. In Mihalcea, Rada and Phil Edmonds, edi-
tors, Senseval-3: Third International Workshop on
the Evaluation of Systems for the Semantic Analysis
of Text, pages 41–43, Barcelona, Spain, July. Associ-
ation for Computational Linguistics.
</reference>
<page confidence="0.999447">
41
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.373136">
<title confidence="0.985698">Polysemy in verbs: systematic relations between and their effect on annotation</title>
<author confidence="0.987316">Anna</author>
<affiliation confidence="0.824987">of Computer Brandeis</affiliation>
<address confidence="0.886166">Waltham, MA</address>
<email confidence="0.990027">arum@cs.brandeis.edu</email>
<affiliation confidence="0.866494">of Spanish Madrid Autonomous</affiliation>
<address confidence="0.905304">Madrid,</address>
<email confidence="0.978862">volha.batsiukova@uam.es</email>
<abstract confidence="0.995371428571429">Sense inventories for polysemous predicates are often comprised by a number of related senses. In this paper, we examine different types of relations within sense inventories and give a qualitative analysis of the effects they have on decisions made by the annotators and annotator error. We also discuss some common traps and pitfalls in design of sense inventories. We use the data set developed specifically for the task of annotating sense distinctions dependent predominantly on semantics of the arguments and only to a lesser extent on syntactic frame.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ju Apresjan</author>
</authors>
<title>Regular polysemy.</title>
<date>1973</date>
<journal>Linguistics,</journal>
<volume>142</volume>
<issue>5</issue>
<contexts>
<context position="2774" citStr="Apresjan, 1973" startWordPosition="402" endWordPosition="403">c-sa/3.0/). Some rights reserved. 2005; Kilgarriff, 1997). It is often resolved on an adhoc basis, resulting in numerous cases of “overlapping senses”, i.e. instances when the same occurrence may fall under more than one sense category simultaneously. This problem has also been the subject of extensive study in lexical semantics, addressing questions such as when the context selects a distinct sense and when it merely modulates the meaning, what is the regular relationship between related senses, and what compositional processes are involved in sense selection (Pustejovsky, 1995; Cruse, 1995; Apresjan, 1973). A number of syntactic and semantic tests are traditionally applied for sense identification, such as examining synonym series, compatible syntactic environments, coordination tests such as cross-understanding or zeugma test (Cruse, 2000). None of these tests are conclusive and normally a combination of factors is used. At the recent Senseval competitions (Mihalcea et al., 2004; Snyder and Palmer, 2004; Preiss and Yarowsky, 2001), the choice of sense inventories frequently presented problems, spurring the efforts to create coarsergrained sense inventories (Hovy et al., 2006; Palmer et al., 20</context>
</contexts>
<marker>Apresjan, 1973</marker>
<rawString>Apresjan, Ju. 1973. Regular polysemy. Linguistics, 142(5):5–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Robust accurate statistical annotation of general text.</title>
<date>2002</date>
<booktitle>Proceedings of the Third International Conference on Language Resources and Evaluation (LREC 2002), Las Palmas, Canary Islands,</booktitle>
<pages>1499--1504</pages>
<contexts>
<context position="18458" citStr="Briscoe and Carroll, 2002" startWordPosition="2861" endWordPosition="2864">ord English dictionaries, and existing correspondences in FrameNet (Ruppenhofer et al., 2006; Hiroaki, 2003), OntoNotes (Hovy et al., 2006),3 and CPA patterns (Hanks and Pustejovsky, 2005; Rumshisky and Pustejovsky, 2006; Pustejovsky et al., 2004). We performed test annotation on 100 instances, with the sense inventory additionally modified upon examining the results of the annotation. This sense inventory was provided to two annotators, along with 200 3Sense inventories released for the 65 verbs made available for SemEval-2007. sentences for each verb. Each sentence was pre-parsed with RASP (Briscoe and Carroll, 2002), and the head of the target argument phrase was identified. Misparses were manually corrected in post-processing. 3.2 Defining the task for the annotators Data set creation for a WSD task is notoriously hard (cf. Palmer et al. (2007)), as the annotators are frequently forced to perform disambiguation on sentences where no disambiguation can really be performed. This is the case, for example, for overlapping senses, where more than one sense is activated simultaneously (Rumshisky, 2008; Pustejovsky and Boguraev, 1993). The goal was to create, for each target word, a set of instances where huma</context>
</contexts>
<marker>Briscoe, Carroll, 2002</marker>
<rawString>Briscoe, T. and J. Carroll. 2002. Robust accurate statistical annotation of general text. Proceedings of the Third International Conference on Language Resources and Evaluation (LREC 2002), Las Palmas, Canary Islands, May 2002, pages 1499–1504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Carpuat</author>
<author>D Wu</author>
</authors>
<title>Improving statistical machine translation using word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL,</booktitle>
<pages>61--72</pages>
<contexts>
<context position="1132" citStr="Carpuat and Wu, 2007" startWordPosition="164" endWordPosition="167">s of the effects they have on decisions made by the annotators and annotator error. We also discuss some common traps and pitfalls in design of sense inventories. We use the data set developed specifically for the task of annotating sense distinctions dependent predominantly on semantics of the arguments and only to a lesser extent on syntactic frame. 1 Introduction Lexical ambiguity is pervasive in natural language, and its resolution has been used to improve performance of a number of natural language processing (NLP) applications, such as statistical machine translation (Chan et al., 2007; Carpuat and Wu, 2007), cross-language information retrieval and question answering (Resnik, 2006). Sense differentiation for the predicates depends on a number of factors, including syntactic frame, semantics of the arguments and adjuncts, contextual clues from the wider context, text domain identification, etc. Preparing sense-tagged data for training and evaluation of word sense disambiguation (WSD) systems involves two stages: (1) creating a sense inventory and (2) applying it in annotation. Creating sense inventories for polysemous words is a task that is notoriously difficult to formalize. For polysemous verb</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Carpuat, M. and D. Wu. 2007. Improving statistical machine translation using word sense disambiguation. In Proc. of EMNLP-CoNLL, pages 61–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y S Chan</author>
<author>H T Ng</author>
<author>D Chiang</author>
</authors>
<title>Word sense disambiguation improves statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>33--40</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1109" citStr="Chan et al., 2007" startWordPosition="160" endWordPosition="163">qualitative analysis of the effects they have on decisions made by the annotators and annotator error. We also discuss some common traps and pitfalls in design of sense inventories. We use the data set developed specifically for the task of annotating sense distinctions dependent predominantly on semantics of the arguments and only to a lesser extent on syntactic frame. 1 Introduction Lexical ambiguity is pervasive in natural language, and its resolution has been used to improve performance of a number of natural language processing (NLP) applications, such as statistical machine translation (Chan et al., 2007; Carpuat and Wu, 2007), cross-language information retrieval and question answering (Resnik, 2006). Sense differentiation for the predicates depends on a number of factors, including syntactic frame, semantics of the arguments and adjuncts, contextual clues from the wider context, text domain identification, etc. Preparing sense-tagged data for training and evaluation of word sense disambiguation (WSD) systems involves two stages: (1) creating a sense inventory and (2) applying it in annotation. Creating sense inventories for polysemous words is a task that is notoriously difficult to formali</context>
</contexts>
<marker>Chan, Ng, Chiang, 2007</marker>
<rawString>Chan, Y. S., H. T. Ng, and D. Chiang. 2007. Word sense disambiguation improves statistical machine translation. In Proc. of ACL, pages 33–40, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Cruse</author>
</authors>
<title>Polysemy and related phenomena from a cognitive linguistic viewpoint.</title>
<date>1995</date>
<booktitle>Computational Lexical Semantics,</booktitle>
<pages>33--49</pages>
<editor>In Dizier, Patrick St. and Evelyne Viegas, editors,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England.</location>
<contexts>
<context position="2757" citStr="Cruse, 1995" startWordPosition="400" endWordPosition="401">licenses/by-nc-sa/3.0/). Some rights reserved. 2005; Kilgarriff, 1997). It is often resolved on an adhoc basis, resulting in numerous cases of “overlapping senses”, i.e. instances when the same occurrence may fall under more than one sense category simultaneously. This problem has also been the subject of extensive study in lexical semantics, addressing questions such as when the context selects a distinct sense and when it merely modulates the meaning, what is the regular relationship between related senses, and what compositional processes are involved in sense selection (Pustejovsky, 1995; Cruse, 1995; Apresjan, 1973). A number of syntactic and semantic tests are traditionally applied for sense identification, such as examining synonym series, compatible syntactic environments, coordination tests such as cross-understanding or zeugma test (Cruse, 2000). None of these tests are conclusive and normally a combination of factors is used. At the recent Senseval competitions (Mihalcea et al., 2004; Snyder and Palmer, 2004; Preiss and Yarowsky, 2001), the choice of sense inventories frequently presented problems, spurring the efforts to create coarsergrained sense inventories (Hovy et al., 2006; </context>
</contexts>
<marker>Cruse, 1995</marker>
<rawString>Cruse, D. A. 1995. Polysemy and related phenomena from a cognitive linguistic viewpoint. In Dizier, Patrick St. and Evelyne Viegas, editors, Computational Lexical Semantics, pages 33–49. Cambridge University Press, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Cruse</author>
</authors>
<title>Meaning in Language, an Introduction to Semantics and Pragmatics.</title>
<date>2000</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford, United Kingdom.</location>
<contexts>
<context position="3013" citStr="Cruse, 2000" startWordPosition="437" endWordPosition="438">aneously. This problem has also been the subject of extensive study in lexical semantics, addressing questions such as when the context selects a distinct sense and when it merely modulates the meaning, what is the regular relationship between related senses, and what compositional processes are involved in sense selection (Pustejovsky, 1995; Cruse, 1995; Apresjan, 1973). A number of syntactic and semantic tests are traditionally applied for sense identification, such as examining synonym series, compatible syntactic environments, coordination tests such as cross-understanding or zeugma test (Cruse, 2000). None of these tests are conclusive and normally a combination of factors is used. At the recent Senseval competitions (Mihalcea et al., 2004; Snyder and Palmer, 2004; Preiss and Yarowsky, 2001), the choice of sense inventories frequently presented problems, spurring the efforts to create coarsergrained sense inventories (Hovy et al., 2006; Palmer et al., 2007; Navigli, 2006). Part of the reason for such difficulties in establishing a set of senses available to a lexical item is that the meaning of a polysemous verb is often determined in composition and depends to the same extent on semantic</context>
<context position="33940" citStr="Cruse (2000)" startWordPosition="5356" endWordPosition="5357">ficiency. Remarkable variation is also observed with respect to non-literal uses as discussed in Section 4.4. For example, in (18) and (19) abstract NPs panic and imbalance offorces are equated with energy or impact by one annotator and with substance by the other. (18) Her panic was absorbed by his warmth. annoA: absorb energy or impact annoB: absorb substance (19) Alternatively, imbalance of forces can be absorbed into the body. annoA: absorb energy or impact annoB: absorb substance In some cases, the literal and the metaphoric senses are activated simultaneously resulting in ambiguity (cf. Cruse (2000)): (20) For over 300 years this waterfall has provided the energy to drive the wheels of industry. annoA: motivate the progress of annoB: provide power for or physically move a mechanism (21) But fashion changed and the short skirtfell – literally – from favour and started skimming the ankles. annoA: lose power or suffer a defeat annoB: N/A (22) She was delighted when the story of Hank fell into her lap. annoA: be associated with or get assigned to a person or location or for event to fall onto a time annoB: physically drop; move or extend downward Impact of subcategorization features on disam</context>
</contexts>
<marker>Cruse, 2000</marker>
<rawString>Cruse, D. A. 2000. Meaning in Language, an Introduction to Semantics and Pragmatics. Oxford University Press, Oxford, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hanks</author>
<author>J Pustejovsky</author>
</authors>
<title>A pattern dictionary for natural language processing. Revue Franc¸aise de Linguistique Appliqu´ee.</title>
<date>2005</date>
<contexts>
<context position="7073" citStr="Hanks and Pustejovsky, 2005" startWordPosition="1068" endWordPosition="1072">attempts to organize lexical information in terms of script-like semantic frames, with semantic and syntactic combinatorial possibilities specified for each frame-evoking lexical unit (word/sense pairing). Semantics of the arguments is represented by Fillmore’s case roles (frame elements) which are derived on ad-hoc basis for each frame. In OntoNotes project, annotators use small-scale corpus analysis to create sense inventories derived by grouping together WordNet senses. The procedure is restricted to maintain 90% inter-annotator agreement (Hovy et al., 2006). Corpus Pattern Analysis (CPA) (Hanks and Pustejovsky, 2005; Pustejovsky et al., 2004) attempts to catalog prototypical norms of usage for individual words, specifying them in terms of context patterns. As a corpus analysis technique, CPA has its origins in the analysis of large corpora for lexicographic purposes, of the kind that was used for compiling the Cobuild dictionary (Sinclair and Hanks, 1987). Each pattern gives a combination of surface textual clues and argument specifications. A lexicographer creates a set of patterns by sorting a concordance for the target predicate according to the context features. In the present study, we use a modific</context>
<context position="18019" citStr="Hanks and Pustejovsky, 2005" startWordPosition="2792" endWordPosition="2796">d for each of the following sets of direct objects: (7) a. Take on certain characteristics shape, meaning, color, form, dimension, reality, significance, identity, appearance, characteristic, flavor b. Purchase or become the owner ofproperty land, stock, business, property, wealth, subsidiary, estate, stake The sense inventory for each verb was cross-checked against several resources, including WordNet, PropBank, Merriam-Webster and Oxford English dictionaries, and existing correspondences in FrameNet (Ruppenhofer et al., 2006; Hiroaki, 2003), OntoNotes (Hovy et al., 2006),3 and CPA patterns (Hanks and Pustejovsky, 2005; Rumshisky and Pustejovsky, 2006; Pustejovsky et al., 2004). We performed test annotation on 100 instances, with the sense inventory additionally modified upon examining the results of the annotation. This sense inventory was provided to two annotators, along with 200 3Sense inventories released for the 65 verbs made available for SemEval-2007. sentences for each verb. Each sentence was pre-parsed with RASP (Briscoe and Carroll, 2002), and the head of the target argument phrase was identified. Misparses were manually corrected in post-processing. 3.2 Defining the task for the annotators Data </context>
</contexts>
<marker>Hanks, Pustejovsky, 2005</marker>
<rawString>Hanks, P. and J. Pustejovsky. 2005. A pattern dictionary for natural language processing. Revue Franc¸aise de Linguistique Appliqu´ee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hiroaki</author>
</authors>
<title>FrameSQL: A software tool for FrameNet.</title>
<date>2003</date>
<journal>Asian Association of Lexicography.</journal>
<booktitle>In Proceedigns of ASIALEX ’03,</booktitle>
<pages>251--258</pages>
<location>Tokyo,</location>
<contexts>
<context position="17940" citStr="Hiroaki, 2003" startWordPosition="2782" endWordPosition="2783">rget. For example, for the verb acquire, a separate sense was added for each of the following sets of direct objects: (7) a. Take on certain characteristics shape, meaning, color, form, dimension, reality, significance, identity, appearance, characteristic, flavor b. Purchase or become the owner ofproperty land, stock, business, property, wealth, subsidiary, estate, stake The sense inventory for each verb was cross-checked against several resources, including WordNet, PropBank, Merriam-Webster and Oxford English dictionaries, and existing correspondences in FrameNet (Ruppenhofer et al., 2006; Hiroaki, 2003), OntoNotes (Hovy et al., 2006),3 and CPA patterns (Hanks and Pustejovsky, 2005; Rumshisky and Pustejovsky, 2006; Pustejovsky et al., 2004). We performed test annotation on 100 instances, with the sense inventory additionally modified upon examining the results of the annotation. This sense inventory was provided to two annotators, along with 200 3Sense inventories released for the 65 verbs made available for SemEval-2007. sentences for each verb. Each sentence was pre-parsed with RASP (Briscoe and Carroll, 2002), and the head of the target argument phrase was identified. Misparses were manual</context>
</contexts>
<marker>Hiroaki, 2003</marker>
<rawString>Hiroaki, S. 2003. FrameSQL: A software tool for FrameNet. In Proceedigns of ASIALEX ’03, pages 251–258, Tokyo, Japan. Asian Association of Lexicography.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>M Marcus</author>
<author>M Palmer</author>
<author>L Ramshaw</author>
<author>R Weischedel</author>
</authors>
<title>OntoNotes: The 90% solution.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers,</booktitle>
<pages>57--60</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="3355" citStr="Hovy et al., 2006" startWordPosition="489" endWordPosition="492">, 1995; Cruse, 1995; Apresjan, 1973). A number of syntactic and semantic tests are traditionally applied for sense identification, such as examining synonym series, compatible syntactic environments, coordination tests such as cross-understanding or zeugma test (Cruse, 2000). None of these tests are conclusive and normally a combination of factors is used. At the recent Senseval competitions (Mihalcea et al., 2004; Snyder and Palmer, 2004; Preiss and Yarowsky, 2001), the choice of sense inventories frequently presented problems, spurring the efforts to create coarsergrained sense inventories (Hovy et al., 2006; Palmer et al., 2007; Navigli, 2006). Part of the reason for such difficulties in establishing a set of senses available to a lexical item is that the meaning of a polysemous verb is often determined in composition and depends to the same extent on semantics of the particular arguments as it does on the base meaning of the verb itself. A number of systematic relations often holds between different senses of a polysemous verb. Depending on the kind of ambiguity involved in each case, some senses are easier to distinguish than others. Sense-tagged data (e.g. SemCor (Landes et al., 1998), PropBa</context>
<context position="7013" citStr="Hovy et al., 2006" startWordPosition="1060" endWordPosition="1063"> a word sense. FrameNet (Ruppenhofer et al., 2006) attempts to organize lexical information in terms of script-like semantic frames, with semantic and syntactic combinatorial possibilities specified for each frame-evoking lexical unit (word/sense pairing). Semantics of the arguments is represented by Fillmore’s case roles (frame elements) which are derived on ad-hoc basis for each frame. In OntoNotes project, annotators use small-scale corpus analysis to create sense inventories derived by grouping together WordNet senses. The procedure is restricted to maintain 90% inter-annotator agreement (Hovy et al., 2006). Corpus Pattern Analysis (CPA) (Hanks and Pustejovsky, 2005; Pustejovsky et al., 2004) attempts to catalog prototypical norms of usage for individual words, specifying them in terms of context patterns. As a corpus analysis technique, CPA has its origins in the analysis of large corpora for lexicographic purposes, of the kind that was used for compiling the Cobuild dictionary (Sinclair and Hanks, 1987). Each pattern gives a combination of surface textual clues and argument specifications. A lexicographer creates a set of patterns by sorting a concordance for the target predicate according to </context>
<context position="17971" citStr="Hovy et al., 2006" startWordPosition="2785" endWordPosition="2788">verb acquire, a separate sense was added for each of the following sets of direct objects: (7) a. Take on certain characteristics shape, meaning, color, form, dimension, reality, significance, identity, appearance, characteristic, flavor b. Purchase or become the owner ofproperty land, stock, business, property, wealth, subsidiary, estate, stake The sense inventory for each verb was cross-checked against several resources, including WordNet, PropBank, Merriam-Webster and Oxford English dictionaries, and existing correspondences in FrameNet (Ruppenhofer et al., 2006; Hiroaki, 2003), OntoNotes (Hovy et al., 2006),3 and CPA patterns (Hanks and Pustejovsky, 2005; Rumshisky and Pustejovsky, 2006; Pustejovsky et al., 2004). We performed test annotation on 100 instances, with the sense inventory additionally modified upon examining the results of the annotation. This sense inventory was provided to two annotators, along with 200 3Sense inventories released for the 65 verbs made available for SemEval-2007. sentences for each verb. Each sentence was pre-parsed with RASP (Briscoe and Carroll, 2002), and the head of the target argument phrase was identified. Misparses were manually corrected in post-processing</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Hovy, E., M. Marcus, M. Palmer, L. Ramshaw, and R. Weischedel. 2006. OntoNotes: The 90% solution. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, pages 57–60, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
<author>P Rychly</author>
<author>P Smrz</author>
<author>D Tugwell</author>
</authors>
<title>The Sketch Engine.</title>
<date>2004</date>
<booktitle>Proceedings of Euralex,</booktitle>
<pages>105--116</pages>
<location>Lorient, France,</location>
<contexts>
<context position="16622" citStr="Kilgarriff et al., 2004" startWordPosition="2574" endWordPosition="2577">ing the British National Corpus (BNC), which is more balanced than the more commonly annotated Wall Street Journal data. We selected 20 polysemous verbs with sense distinctions that were judged to depend for disambiguation on semantics of the argument in several argument positions, including direct object (dobj), subject (subj), or indirect object within a prepositional phrase governed by with (iobj with): dobj: absorb, acquire, admit, assume, claim, conclude, cut, deny, dictate, drive, edit, enjoy, fire, grasp, know, launch subj: explain, fall, lead iobj with: meet We used the Sketch Engine (Kilgarriff et al., 2004) both to select the verbs and to aid the creation of the sense inventories. The Sketch Engine is a lexicographic tool that lists collocates that co-occur with a given target word in the specified grammatical relation. The collocates are sorted by their association score with the target. A set of senses was created for each verb using a modification of the CPA technique (Pustejovsky et al., 2004). A set of complements was examined in the Sketch Engine. If a clear division was observed between semantically different groups of collocates in a certain argument position, the verb was selected. For </context>
</contexts>
<marker>Kilgarriff, Rychly, Smrz, Tugwell, 2004</marker>
<rawString>Kilgarriff, A., P. Rychly, P. Smrz, and D. Tugwell. 2004. The Sketch Engine. Proceedings of Euralex, Lorient, France, pages 105–116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
</authors>
<title>I don’t believe in word senses.</title>
<date>1997</date>
<booktitle>Computers and the Humanities,</booktitle>
<pages>31--91</pages>
<contexts>
<context position="2216" citStr="Kilgarriff, 1997" startWordPosition="315" endWordPosition="316">annotation. Creating sense inventories for polysemous words is a task that is notoriously difficult to formalize. For polysemous verbs especially, constellations of related meanings make this task even more difficult. In lexicography, “lumping and splitting” senses during dictionary construction – i.e. deciding when to describe a set of usages as a separate sense – is a well-known problem (Hanks and Pustejovsky, © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 2005; Kilgarriff, 1997). It is often resolved on an adhoc basis, resulting in numerous cases of “overlapping senses”, i.e. instances when the same occurrence may fall under more than one sense category simultaneously. This problem has also been the subject of extensive study in lexical semantics, addressing questions such as when the context selects a distinct sense and when it merely modulates the meaning, what is the regular relationship between related senses, and what compositional processes are involved in sense selection (Pustejovsky, 1995; Cruse, 1995; Apresjan, 1973). A number of syntactic and semantic tests</context>
</contexts>
<marker>Kilgarriff, 1997</marker>
<rawString>Kilgarriff, A. 1997. I don’t believe in word senses. Computers and the Humanities, 31:91–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Landes</author>
<author>C Leacock</author>
<author>R I Tengi</author>
</authors>
<title>Building semantic concordances.</title>
<date>1998</date>
<editor>In Fellbaum, C., editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge (Mass.).</location>
<contexts>
<context position="3947" citStr="Landes et al., 1998" startWordPosition="593" endWordPosition="596">ventories (Hovy et al., 2006; Palmer et al., 2007; Navigli, 2006). Part of the reason for such difficulties in establishing a set of senses available to a lexical item is that the meaning of a polysemous verb is often determined in composition and depends to the same extent on semantics of the particular arguments as it does on the base meaning of the verb itself. A number of systematic relations often holds between different senses of a polysemous verb. Depending on the kind of ambiguity involved in each case, some senses are easier to distinguish than others. Sense-tagged data (e.g. SemCor (Landes et al., 1998), PropBank (Palmer et al., 2005), OntoNotes (Hovy et al., 2006)) typically provides no way to differentiate between sense distinctions motivated by different factors. Treating different disambiguation factors separately would allow one to examine the contribution of each factor, as well as the success of a given algorithm in identifying the corresponding senses. Within the scope of a sentence, syntactic frame and semantics of the arguments are most prominent in sense 33 Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 33–41 Manchester, August 200</context>
</contexts>
<marker>Landes, Leacock, Tengi, 1998</marker>
<rawString>Landes, S., C. Leacock, and R.I. Tengi. 1998. Building semantic concordances. In Fellbaum, C., editor, Wordnet: an electronic lexical database. MIT Press, Cambridge (Mass.).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>T Chklovski</author>
<author>A Kilgarriff</author>
</authors>
<title>The Senseval-3 English lexical sample task.</title>
<date>2004</date>
<booktitle>Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>25--28</pages>
<editor>In Mihalcea, Rada and Phil Edmonds, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="3155" citStr="Mihalcea et al., 2004" startWordPosition="459" endWordPosition="462">xt selects a distinct sense and when it merely modulates the meaning, what is the regular relationship between related senses, and what compositional processes are involved in sense selection (Pustejovsky, 1995; Cruse, 1995; Apresjan, 1973). A number of syntactic and semantic tests are traditionally applied for sense identification, such as examining synonym series, compatible syntactic environments, coordination tests such as cross-understanding or zeugma test (Cruse, 2000). None of these tests are conclusive and normally a combination of factors is used. At the recent Senseval competitions (Mihalcea et al., 2004; Snyder and Palmer, 2004; Preiss and Yarowsky, 2001), the choice of sense inventories frequently presented problems, spurring the efforts to create coarsergrained sense inventories (Hovy et al., 2006; Palmer et al., 2007; Navigli, 2006). Part of the reason for such difficulties in establishing a set of senses available to a lexical item is that the meaning of a polysemous verb is often determined in composition and depends to the same extent on semantics of the particular arguments as it does on the base meaning of the verb itself. A number of systematic relations often holds between differen</context>
</contexts>
<marker>Mihalcea, Chklovski, Kilgarriff, 2004</marker>
<rawString>Mihalcea, R., T. Chklovski, and A. Kilgarriff. 2004. The Senseval-3 English lexical sample task. In Mihalcea, Rada and Phil Edmonds, editors, Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pages 25– 28, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
</authors>
<title>Meaningful clustering of senses helps boost word sense disambiguation performance.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>105--112</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="3392" citStr="Navigli, 2006" startWordPosition="497" endWordPosition="498">number of syntactic and semantic tests are traditionally applied for sense identification, such as examining synonym series, compatible syntactic environments, coordination tests such as cross-understanding or zeugma test (Cruse, 2000). None of these tests are conclusive and normally a combination of factors is used. At the recent Senseval competitions (Mihalcea et al., 2004; Snyder and Palmer, 2004; Preiss and Yarowsky, 2001), the choice of sense inventories frequently presented problems, spurring the efforts to create coarsergrained sense inventories (Hovy et al., 2006; Palmer et al., 2007; Navigli, 2006). Part of the reason for such difficulties in establishing a set of senses available to a lexical item is that the meaning of a polysemous verb is often determined in composition and depends to the same extent on semantics of the particular arguments as it does on the base meaning of the verb itself. A number of systematic relations often holds between different senses of a polysemous verb. Depending on the kind of ambiguity involved in each case, some senses are easier to distinguish than others. Sense-tagged data (e.g. SemCor (Landes et al., 1998), PropBank (Palmer et al., 2005), OntoNotes (</context>
</contexts>
<marker>Navigli, 2006</marker>
<rawString>Navigli, R. 2006. Meaningful clustering of senses helps boost word sense disambiguation performance. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 105–112, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The Proposition Bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="3979" citStr="Palmer et al., 2005" startWordPosition="598" endWordPosition="601">lmer et al., 2007; Navigli, 2006). Part of the reason for such difficulties in establishing a set of senses available to a lexical item is that the meaning of a polysemous verb is often determined in composition and depends to the same extent on semantics of the particular arguments as it does on the base meaning of the verb itself. A number of systematic relations often holds between different senses of a polysemous verb. Depending on the kind of ambiguity involved in each case, some senses are easier to distinguish than others. Sense-tagged data (e.g. SemCor (Landes et al., 1998), PropBank (Palmer et al., 2005), OntoNotes (Hovy et al., 2006)) typically provides no way to differentiate between sense distinctions motivated by different factors. Treating different disambiguation factors separately would allow one to examine the contribution of each factor, as well as the success of a given algorithm in identifying the corresponding senses. Within the scope of a sentence, syntactic frame and semantics of the arguments are most prominent in sense 33 Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 33–41 Manchester, August 2008 disambiguation. The latter is </context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Palmer, M., D. Gildea, and P. Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>H Dang</author>
<author>C Fellbaum</author>
</authors>
<title>Making fine-grained and coarse-grained sense distinctions, both manually and automatically.</title>
<date>2007</date>
<journal>Journal of Natural Language Engineering.</journal>
<contexts>
<context position="3376" citStr="Palmer et al., 2007" startWordPosition="493" endWordPosition="496">; Apresjan, 1973). A number of syntactic and semantic tests are traditionally applied for sense identification, such as examining synonym series, compatible syntactic environments, coordination tests such as cross-understanding or zeugma test (Cruse, 2000). None of these tests are conclusive and normally a combination of factors is used. At the recent Senseval competitions (Mihalcea et al., 2004; Snyder and Palmer, 2004; Preiss and Yarowsky, 2001), the choice of sense inventories frequently presented problems, spurring the efforts to create coarsergrained sense inventories (Hovy et al., 2006; Palmer et al., 2007; Navigli, 2006). Part of the reason for such difficulties in establishing a set of senses available to a lexical item is that the meaning of a polysemous verb is often determined in composition and depends to the same extent on semantics of the particular arguments as it does on the base meaning of the verb itself. A number of systematic relations often holds between different senses of a polysemous verb. Depending on the kind of ambiguity involved in each case, some senses are easier to distinguish than others. Sense-tagged data (e.g. SemCor (Landes et al., 1998), PropBank (Palmer et al., 20</context>
<context position="18692" citStr="Palmer et al. (2007)" startWordPosition="2899" endWordPosition="2902">, 2004). We performed test annotation on 100 instances, with the sense inventory additionally modified upon examining the results of the annotation. This sense inventory was provided to two annotators, along with 200 3Sense inventories released for the 65 verbs made available for SemEval-2007. sentences for each verb. Each sentence was pre-parsed with RASP (Briscoe and Carroll, 2002), and the head of the target argument phrase was identified. Misparses were manually corrected in post-processing. 3.2 Defining the task for the annotators Data set creation for a WSD task is notoriously hard (cf. Palmer et al. (2007)), as the annotators are frequently forced to perform disambiguation on sentences where no disambiguation can really be performed. This is the case, for example, for overlapping senses, where more than one sense is activated simultaneously (Rumshisky, 2008; Pustejovsky and Boguraev, 1993). The goal was to create, for each target word, a set of instances where humans had no trouble disambiguating between different senses. Two undergraduate linguistics majors served as annotators. The annotators were instructed to mark each sentence with the most fitting sense. The annotators were allowed to mar</context>
</contexts>
<marker>Palmer, Dang, Fellbaum, 2007</marker>
<rawString>Palmer, M., H. Dang, and C. Fellbaum. 2007. Making fine-grained and coarse-grained sense distinctions, both manually and automatically. Journal of Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Preiss</author>
<author>D Yarowsky</author>
<author>editors</author>
</authors>
<date>2001</date>
<booktitle>Proceedings of the Second Int. Workshop on Evaluating WSD Systems (Senseval2). ACL2002/EACL2001.</booktitle>
<marker>Preiss, Yarowsky, editors, 2001</marker>
<rawString>Preiss, J and D. Yarowsky, editors. 2001. Proceedings of the Second Int. Workshop on Evaluating WSD Systems (Senseval2). ACL2002/EACL2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>B Boguraev</author>
</authors>
<title>Lexical knowledge representation and natural language processing.</title>
<date>1993</date>
<journal>Artif. Intell.,</journal>
<pages>63--1</pages>
<contexts>
<context position="18981" citStr="Pustejovsky and Boguraev, 1993" startWordPosition="2940" endWordPosition="2943">or SemEval-2007. sentences for each verb. Each sentence was pre-parsed with RASP (Briscoe and Carroll, 2002), and the head of the target argument phrase was identified. Misparses were manually corrected in post-processing. 3.2 Defining the task for the annotators Data set creation for a WSD task is notoriously hard (cf. Palmer et al. (2007)), as the annotators are frequently forced to perform disambiguation on sentences where no disambiguation can really be performed. This is the case, for example, for overlapping senses, where more than one sense is activated simultaneously (Rumshisky, 2008; Pustejovsky and Boguraev, 1993). The goal was to create, for each target word, a set of instances where humans had no trouble disambiguating between different senses. Two undergraduate linguistics majors served as annotators. The annotators were instructed to mark each sentence with the most fitting sense. The annotators were allowed to mark the sentence as “N/A” and were instructed to do so if (i) the sense inventory was missing the relevant sense, (ii) more than one sense seemed to fit, or (iii) the sense was impossible to determine from the context. With respect to metaphoric senses, instructions were to throw out cases </context>
</contexts>
<marker>Pustejovsky, Boguraev, 1993</marker>
<rawString>Pustejovsky, J. and B. Boguraev. 1993. Lexical knowledge representation and natural language processing. Artif. Intell., 63(1-2):193–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>P Hanks</author>
<author>A Rumshisky</author>
</authors>
<title>Automated Induction of Sense in Context.</title>
<date>2004</date>
<booktitle>In COLING 2004,</booktitle>
<pages>924--931</pages>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="7100" citStr="Pustejovsky et al., 2004" startWordPosition="1073" endWordPosition="1076">information in terms of script-like semantic frames, with semantic and syntactic combinatorial possibilities specified for each frame-evoking lexical unit (word/sense pairing). Semantics of the arguments is represented by Fillmore’s case roles (frame elements) which are derived on ad-hoc basis for each frame. In OntoNotes project, annotators use small-scale corpus analysis to create sense inventories derived by grouping together WordNet senses. The procedure is restricted to maintain 90% inter-annotator agreement (Hovy et al., 2006). Corpus Pattern Analysis (CPA) (Hanks and Pustejovsky, 2005; Pustejovsky et al., 2004) attempts to catalog prototypical norms of usage for individual words, specifying them in terms of context patterns. As a corpus analysis technique, CPA has its origins in the analysis of large corpora for lexicographic purposes, of the kind that was used for compiling the Cobuild dictionary (Sinclair and Hanks, 1987). Each pattern gives a combination of surface textual clues and argument specifications. A lexicographer creates a set of patterns by sorting a concordance for the target predicate according to the context features. In the present study, we use a modification of the CPA technique </context>
<context position="10426" citStr="Pustejovsky et al. (2004)" startWordPosition="1604" endWordPosition="1607">as a sense involving absorbing a substance, and the typical members of the corresponding argument set would be actual substances, such as oil, oxygen, water, air, salt, etc. But goodness, dirt, flavor, moisture would also activate the same sense. Each decision to split a sense and make another category is to a certain extent an arbitrary decision. For example, for the verb absorb, one can separate absorbing a substance (oil, oxygen, water, air, salt) from absorbing energy (radiation, heat, sound, energy). The latter sense may or may not be separated from absorb1See Rumshisky et al. (2006) and Pustejovsky et al. (2004) for more detail. 34 ing impact (blow, shock, stress). But it is a marked continuum, i.e. certain points in the continuum are more prominent, with necessity of a given concept reflected in the frequency of use. When several senses are postulated based on argument distinctions, there are almost always boundary cases that can be seen to belong to both categories. Consider, for example, two senses defined for the verb launch and the corresponding direct objects in (1): (1) a. Physically propel an object into the air or water missile, rocket, torpedo, satellite, shuttle, craft b. Begin or initiate</context>
<context position="17020" citStr="Pustejovsky et al., 2004" startWordPosition="2643" endWordPosition="2646">(iobj with): dobj: absorb, acquire, admit, assume, claim, conclude, cut, deny, dictate, drive, edit, enjoy, fire, grasp, know, launch subj: explain, fall, lead iobj with: meet We used the Sketch Engine (Kilgarriff et al., 2004) both to select the verbs and to aid the creation of the sense inventories. The Sketch Engine is a lexicographic tool that lists collocates that co-occur with a given target word in the specified grammatical relation. The collocates are sorted by their association score with the target. A set of senses was created for each verb using a modification of the CPA technique (Pustejovsky et al., 2004). A set of complements was examined in the Sketch Engine. If a clear division was observed between semantically different groups of collocates in a certain argument position, the verb was selected. For semantically distinct groups of collocates, a separate sense was added to the sense inventory for the target. For example, for the verb acquire, a separate sense was added for each of the following sets of direct objects: (7) a. Take on certain characteristics shape, meaning, color, form, dimension, reality, significance, identity, appearance, characteristic, flavor b. Purchase or become the own</context>
</contexts>
<marker>Pustejovsky, Hanks, Rumshisky, 2004</marker>
<rawString>Pustejovsky, J., P. Hanks, and A. Rumshisky. 2004. Automated Induction of Sense in Context. In COLING 2004, Geneva, Switzerland, pages 924–931.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
</authors>
<title>Generative Lexicon.</title>
<date>1995</date>
<publisher>MIT Press.</publisher>
<location>Cambridge (Mass.):</location>
<contexts>
<context position="2744" citStr="Pustejovsky, 1995" startWordPosition="397" endWordPosition="399">reativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 2005; Kilgarriff, 1997). It is often resolved on an adhoc basis, resulting in numerous cases of “overlapping senses”, i.e. instances when the same occurrence may fall under more than one sense category simultaneously. This problem has also been the subject of extensive study in lexical semantics, addressing questions such as when the context selects a distinct sense and when it merely modulates the meaning, what is the regular relationship between related senses, and what compositional processes are involved in sense selection (Pustejovsky, 1995; Cruse, 1995; Apresjan, 1973). A number of syntactic and semantic tests are traditionally applied for sense identification, such as examining synonym series, compatible syntactic environments, coordination tests such as cross-understanding or zeugma test (Cruse, 2000). None of these tests are conclusive and normally a combination of factors is used. At the recent Senseval competitions (Mihalcea et al., 2004; Snyder and Palmer, 2004; Preiss and Yarowsky, 2001), the choice of sense inventories frequently presented problems, spurring the efforts to create coarsergrained sense inventories (Hovy e</context>
<context position="12589" citStr="Pustejovsky (1995)" startWordPosition="1939" endWordPosition="1940">til purchase negotiations were concluded. annoA: finish annoB: reach an agreement In many cases, postulating a separate sense for a coherent set of nominal complements is not justified, as there are regular semantic processes that allow the complements to satisfy selectional requirements of the verb. For example, the verb conclude, in the finish sense accepts EVENT complements. Therefore, nouns such as letter, chapter, novel in (2) must be coerced into events corresponding to the activity that typically brings them about, that is, re-interpreted as events of writing (their Agentive quale, cf. Pustejovsky (1995)). Similarly, the verb deny in the first sense (state or maintain that something is untrue) accepts PROPOSITION complements: (4) a. state or maintain that something is untrue allegations, reports, rumour; significance, importance, difference; attack, assault, involvement b. refuse to grant something access, visa, approval, funding, license 2All examples are taken from the annotated data set. In some cases, sentence structure was slightly modified for brevity. Event nouns such as attack and assault are coerced into a propositional reading, as are relational nouns such as significance and import</context>
<context position="26800" citStr="Pustejovsky, 1995" startWordPosition="4191" endWordPosition="4192">to a certain category (STATE). 2. The aspectual nature of the predicate is the only semantically relevant feature that remains unchanged after consecutive sense modifications. For example, the ingressive meaning of ‘beginning something’ is preserved in shifting from the physical sense of the verb launch in launch a missile to launch a campaign and launch a product. 37 4.3 Lexical semantic features Sense distinctions often involve deeper semantic characteristics of the verbs which could be accounted for by means of lexical semantic features such as qualia structure roles in Generative Lexicon (Pustejovsky, 1995):5 1. Consider how the meaning component ‘manner of motion’ (typically associated with the agentive role) gets transformed in the different senses of drive. It is obviously present in the physical uses of drive (such as operate a vehicle, transport something or somebody, etc.), but is completely lost in motivate the progress of (as in drive the economy, drive the market forward, etc.). The value of the agentive role of drive becomes underspecified or semantically weak, so that the overall meaning of drive is transformed to cause something to move. 2. Information about semantic type contained i</context>
<context position="29171" citStr="Pustejovsky, 1995" startWordPosition="4564" endWordPosition="4565">launch a product (newspaper, collection) c. meet with a person vs. meet with success, resistance d. lead somebody somewhere vs. lead to a consequence Note that these metaphorical extensions involve abstract or continuous objects (meaning, assault, success, consequence), which in turn cause event structure modifications (lead as a process vs. lead as a state). Thus, the processes and structures we are dealing with are clearly interrelated. The metonymical process can be exemplified by edit as make changes to the text and as supervise publica5We will use the terminology from Generative Lexicon (Pustejovsky, 1995; Pustejovsky, 2007) to discuss lexical semantic properties, such as qualia roles, complex and functional types, and so on. 6Complex type is a term used for concepts that inherently refer to more than one semantic type. tion, which are in a clear contiguity relationship. One of the effects of the metaphorization and progressive emptying of the primary (physical, concrete) senses is the distinction between generic and specific senses. For example, compare acquire land, business (specific sense) to acquire an infection, a boyfriend, a following, which refers to some extremely light generic assoc</context>
</contexts>
<marker>Pustejovsky, 1995</marker>
<rawString>Pustejovsky, J. 1995. Generative Lexicon. Cambridge (Mass.): MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
</authors>
<title>Type Theory and Lexical Decomposition.</title>
<date>2007</date>
<booktitle>Trends in Generative Lexicon Theory.</booktitle>
<editor>In Bouillon, P. and C. Lee, editors,</editor>
<publisher>Kluwer Publishers (in press).</publisher>
<contexts>
<context position="29191" citStr="Pustejovsky, 2007" startWordPosition="4566" endWordPosition="4567">ewspaper, collection) c. meet with a person vs. meet with success, resistance d. lead somebody somewhere vs. lead to a consequence Note that these metaphorical extensions involve abstract or continuous objects (meaning, assault, success, consequence), which in turn cause event structure modifications (lead as a process vs. lead as a state). Thus, the processes and structures we are dealing with are clearly interrelated. The metonymical process can be exemplified by edit as make changes to the text and as supervise publica5We will use the terminology from Generative Lexicon (Pustejovsky, 1995; Pustejovsky, 2007) to discuss lexical semantic properties, such as qualia roles, complex and functional types, and so on. 6Complex type is a term used for concepts that inherently refer to more than one semantic type. tion, which are in a clear contiguity relationship. One of the effects of the metaphorization and progressive emptying of the primary (physical, concrete) senses is the distinction between generic and specific senses. For example, compare acquire land, business (specific sense) to acquire an infection, a boyfriend, a following, which refers to some extremely light generic association. Similar proc</context>
</contexts>
<marker>Pustejovsky, 2007</marker>
<rawString>Pustejovsky, J. 2007. Type Theory and Lexical Decomposition. In Bouillon, P. and C. Lee, editors, Trends in Generative Lexicon Theory. Kluwer Publishers (in press).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Word sense disambiguation in NLP applications.</title>
<date>2006</date>
<editor>In Agirre, E. and P. Edmonds, editors, Word</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="1208" citStr="Resnik, 2006" startWordPosition="174" endWordPosition="175">We also discuss some common traps and pitfalls in design of sense inventories. We use the data set developed specifically for the task of annotating sense distinctions dependent predominantly on semantics of the arguments and only to a lesser extent on syntactic frame. 1 Introduction Lexical ambiguity is pervasive in natural language, and its resolution has been used to improve performance of a number of natural language processing (NLP) applications, such as statistical machine translation (Chan et al., 2007; Carpuat and Wu, 2007), cross-language information retrieval and question answering (Resnik, 2006). Sense differentiation for the predicates depends on a number of factors, including syntactic frame, semantics of the arguments and adjuncts, contextual clues from the wider context, text domain identification, etc. Preparing sense-tagged data for training and evaluation of word sense disambiguation (WSD) systems involves two stages: (1) creating a sense inventory and (2) applying it in annotation. Creating sense inventories for polysemous words is a task that is notoriously difficult to formalize. For polysemous verbs especially, constellations of related meanings make this task even more di</context>
</contexts>
<marker>Resnik, 2006</marker>
<rawString>Resnik, P. 2006. Word sense disambiguation in NLP applications. In Agirre, E. and P. Edmonds, editors, Word Sense Disambiguation: Algorithms and Applications. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rumshisky</author>
<author>J Pustejovsky</author>
</authors>
<title>Inducing sense-discriminating context patterns from sensetagged corpora.</title>
<date>2006</date>
<booktitle>In LREC 2006,</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="18052" citStr="Rumshisky and Pustejovsky, 2006" startWordPosition="2797" endWordPosition="2800">ets of direct objects: (7) a. Take on certain characteristics shape, meaning, color, form, dimension, reality, significance, identity, appearance, characteristic, flavor b. Purchase or become the owner ofproperty land, stock, business, property, wealth, subsidiary, estate, stake The sense inventory for each verb was cross-checked against several resources, including WordNet, PropBank, Merriam-Webster and Oxford English dictionaries, and existing correspondences in FrameNet (Ruppenhofer et al., 2006; Hiroaki, 2003), OntoNotes (Hovy et al., 2006),3 and CPA patterns (Hanks and Pustejovsky, 2005; Rumshisky and Pustejovsky, 2006; Pustejovsky et al., 2004). We performed test annotation on 100 instances, with the sense inventory additionally modified upon examining the results of the annotation. This sense inventory was provided to two annotators, along with 200 3Sense inventories released for the 65 verbs made available for SemEval-2007. sentences for each verb. Each sentence was pre-parsed with RASP (Briscoe and Carroll, 2002), and the head of the target argument phrase was identified. Misparses were manually corrected in post-processing. 3.2 Defining the task for the annotators Data set creation for a WSD task is no</context>
</contexts>
<marker>Rumshisky, Pustejovsky, 2006</marker>
<rawString>Rumshisky, A. and J. Pustejovsky. 2006. Inducing sense-discriminating context patterns from sensetagged corpora. In LREC 2006, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rumshisky</author>
<author>P Hanks</author>
<author>C Havasi</author>
<author>J Pustejovsky</author>
</authors>
<title>Constructing a corpus-based ontology using model bias.</title>
<date>2006</date>
<booktitle>In The 19th International FLAIRS Conference, FLAIRS 2006,</booktitle>
<location>Melbourne Beach, Florida, USA.</location>
<contexts>
<context position="10396" citStr="Rumshisky et al. (2006)" startWordPosition="1599" endWordPosition="1602">r example, the verb absorb has a sense involving absorbing a substance, and the typical members of the corresponding argument set would be actual substances, such as oil, oxygen, water, air, salt, etc. But goodness, dirt, flavor, moisture would also activate the same sense. Each decision to split a sense and make another category is to a certain extent an arbitrary decision. For example, for the verb absorb, one can separate absorbing a substance (oil, oxygen, water, air, salt) from absorbing energy (radiation, heat, sound, energy). The latter sense may or may not be separated from absorb1See Rumshisky et al. (2006) and Pustejovsky et al. (2004) for more detail. 34 ing impact (blow, shock, stress). But it is a marked continuum, i.e. certain points in the continuum are more prominent, with necessity of a given concept reflected in the frequency of use. When several senses are postulated based on argument distinctions, there are almost always boundary cases that can be seen to belong to both categories. Consider, for example, two senses defined for the verb launch and the corresponding direct objects in (1): (1) a. Physically propel an object into the air or water missile, rocket, torpedo, satellite, shutt</context>
<context position="13258" citStr="Rumshisky et al., 2006" startWordPosition="2036" endWordPosition="2039"> (state or maintain that something is untrue) accepts PROPOSITION complements: (4) a. state or maintain that something is untrue allegations, reports, rumour; significance, importance, difference; attack, assault, involvement b. refuse to grant something access, visa, approval, funding, license 2All examples are taken from the annotated data set. In some cases, sentence structure was slightly modified for brevity. Event nouns such as attack and assault are coerced into a propositional reading, as are relational nouns such as significance and importance. Interestingly, as we have noted before (Rumshisky et al., 2006), each predicate imposes its own gradation with respect to prototypicality of elements of the argument set. As a result, even though basic semantic types such as PHYSOBJ, ANIMATE, EVENT, are used uniformly by many predicates, argument sets, while semantically similar, typically differ between predicates. For example, fall in the subject position and cut in the direct object position select for things that can be decreased: (5) a. cut (dobj): reduce or lessen price, inflation, profits, cost, emission, spending, deficit, wages overhead, production, consumption, fees, staff b. fall (subj): decrea</context>
</contexts>
<marker>Rumshisky, Hanks, Havasi, Pustejovsky, 2006</marker>
<rawString>Rumshisky, A., P. Hanks, C. Havasi, and J. Pustejovsky. 2006. Constructing a corpus-based ontology using model bias. In The 19th International FLAIRS Conference, FLAIRS 2006, Melbourne Beach, Florida, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rumshisky</author>
</authors>
<title>Resolving polysemy in verbs: Contextualized distributional approach to argument semantics.</title>
<date>2008</date>
<journal>Distributional Models of the Lexicon in Linguistics and Cognitive Science, special issue of Italian Journal ofLinguistics / Rivista di Linguistica. forthcoming.</journal>
<contexts>
<context position="9753" citStr="Rumshisky (2008)" startWordPosition="1493" endWordPosition="1494">y as a general principle of category organization seems to play an important role in defining both the boundaries of senses and the corresponding argument groupings. The same sense of the predicate is often activated by a number of semantically diverse arguments. Such argument sets are frequently organized around a core of typical members that are a “good fit” with respect to semantic requirements of the corresponding sense of the target. The relevant semantic feature is prominent for them, while other, more peripheral members of the argument set, merely allow the relevant interpretation (see Rumshisky (2008) for discussion). For example, the verb absorb has a sense involving absorbing a substance, and the typical members of the corresponding argument set would be actual substances, such as oil, oxygen, water, air, salt, etc. But goodness, dirt, flavor, moisture would also activate the same sense. Each decision to split a sense and make another category is to a certain extent an arbitrary decision. For example, for the verb absorb, one can separate absorbing a substance (oil, oxygen, water, air, salt) from absorbing energy (radiation, heat, sound, energy). The latter sense may or may not be separa</context>
<context position="18948" citStr="Rumshisky, 2008" startWordPosition="2938" endWordPosition="2939"> made available for SemEval-2007. sentences for each verb. Each sentence was pre-parsed with RASP (Briscoe and Carroll, 2002), and the head of the target argument phrase was identified. Misparses were manually corrected in post-processing. 3.2 Defining the task for the annotators Data set creation for a WSD task is notoriously hard (cf. Palmer et al. (2007)), as the annotators are frequently forced to perform disambiguation on sentences where no disambiguation can really be performed. This is the case, for example, for overlapping senses, where more than one sense is activated simultaneously (Rumshisky, 2008; Pustejovsky and Boguraev, 1993). The goal was to create, for each target word, a set of instances where humans had no trouble disambiguating between different senses. Two undergraduate linguistics majors served as annotators. The annotators were instructed to mark each sentence with the most fitting sense. The annotators were allowed to mark the sentence as “N/A” and were instructed to do so if (i) the sense inventory was missing the relevant sense, (ii) more than one sense seemed to fit, or (iii) the sense was impossible to determine from the context. With respect to metaphoric senses, inst</context>
</contexts>
<marker>Rumshisky, 2008</marker>
<rawString>Rumshisky, A. 2008. Resolving polysemy in verbs: Contextualized distributional approach to argument semantics. Distributional Models of the Lexicon in Linguistics and Cognitive Science, special issue of Italian Journal ofLinguistics / Rivista di Linguistica. forthcoming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ruppenhofer</author>
<author>M Ellsworth</author>
<author>M Petruck</author>
<author>C Johnson</author>
<author>J Scheffczyk</author>
</authors>
<title>FrameNet II: Extended Theory and Practice.</title>
<date>2006</date>
<contexts>
<context position="6445" citStr="Ruppenhofer et al., 2006" startWordPosition="980" endWordPosition="983">tor agreement rates and lead to annotation error. In Section 2, we discuss some of the factors that influence compilation of sense inventories and the methodology involved. In Section 3, we describe briefly the data set and the annotation task. In Sections 4 and 5, we discuss the relations observed between different senses within sense inventories in our data set, their effect on decisions made by the annotators, and the related annotation errors. 2 Defining A Sense Inventory Several current resource-oriented projects undertake to formalize the procedure of identifying a word sense. FrameNet (Ruppenhofer et al., 2006) attempts to organize lexical information in terms of script-like semantic frames, with semantic and syntactic combinatorial possibilities specified for each frame-evoking lexical unit (word/sense pairing). Semantics of the arguments is represented by Fillmore’s case roles (frame elements) which are derived on ad-hoc basis for each frame. In OntoNotes project, annotators use small-scale corpus analysis to create sense inventories derived by grouping together WordNet senses. The procedure is restricted to maintain 90% inter-annotator agreement (Hovy et al., 2006). Corpus Pattern Analysis (CPA) </context>
<context position="17924" citStr="Ruppenhofer et al., 2006" startWordPosition="2777" endWordPosition="2781">sense inventory for the target. For example, for the verb acquire, a separate sense was added for each of the following sets of direct objects: (7) a. Take on certain characteristics shape, meaning, color, form, dimension, reality, significance, identity, appearance, characteristic, flavor b. Purchase or become the owner ofproperty land, stock, business, property, wealth, subsidiary, estate, stake The sense inventory for each verb was cross-checked against several resources, including WordNet, PropBank, Merriam-Webster and Oxford English dictionaries, and existing correspondences in FrameNet (Ruppenhofer et al., 2006; Hiroaki, 2003), OntoNotes (Hovy et al., 2006),3 and CPA patterns (Hanks and Pustejovsky, 2005; Rumshisky and Pustejovsky, 2006; Pustejovsky et al., 2004). We performed test annotation on 100 instances, with the sense inventory additionally modified upon examining the results of the annotation. This sense inventory was provided to two annotators, along with 200 3Sense inventories released for the 65 verbs made available for SemEval-2007. sentences for each verb. Each sentence was pre-parsed with RASP (Briscoe and Carroll, 2002), and the head of the target argument phrase was identified. Mispa</context>
</contexts>
<marker>Ruppenhofer, Ellsworth, Petruck, Johnson, Scheffczyk, 2006</marker>
<rawString>Ruppenhofer, J., M. Ellsworth, M. Petruck, C. Johnson, and J. Scheffczyk. 2006. FrameNet II: Extended Theory and Practice.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sinclair</author>
<author>P Hanks</author>
</authors>
<title>The Collins Cobuild English Language Dictionary. HarperCollins, 4th edition (2003) edition. Published as Collins Cobuild Advanced Learner’s English Dictionary.</title>
<date>1987</date>
<contexts>
<context position="7419" citStr="Sinclair and Hanks, 1987" startWordPosition="1126" endWordPosition="1129">otes project, annotators use small-scale corpus analysis to create sense inventories derived by grouping together WordNet senses. The procedure is restricted to maintain 90% inter-annotator agreement (Hovy et al., 2006). Corpus Pattern Analysis (CPA) (Hanks and Pustejovsky, 2005; Pustejovsky et al., 2004) attempts to catalog prototypical norms of usage for individual words, specifying them in terms of context patterns. As a corpus analysis technique, CPA has its origins in the analysis of large corpora for lexicographic purposes, of the kind that was used for compiling the Cobuild dictionary (Sinclair and Hanks, 1987). Each pattern gives a combination of surface textual clues and argument specifications. A lexicographer creates a set of patterns by sorting a concordance for the target predicate according to the context features. In the present study, we use a modification of the CPA technique in the way explained in Section 3. In CPA, syntactic and textual clues include argument structure and minor syntactic categories such as locatives and adjuncts; collocates from wider context; subphrasal cues such as genitives, partitives, bare plural/determiner, infinitivals, negatives, etc. Semantics of the arguments</context>
</contexts>
<marker>Sinclair, Hanks, 1987</marker>
<rawString>Sinclair, J. and P. Hanks. 1987. The Collins Cobuild English Language Dictionary. HarperCollins, 4th edition (2003) edition. Published as Collins Cobuild Advanced Learner’s English Dictionary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Snyder</author>
<author>M Palmer</author>
</authors>
<title>The english all-words task.</title>
<date>2004</date>
<booktitle>Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>41--43</pages>
<editor>In Mihalcea, Rada and Phil Edmonds, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="3180" citStr="Snyder and Palmer, 2004" startWordPosition="463" endWordPosition="466">ense and when it merely modulates the meaning, what is the regular relationship between related senses, and what compositional processes are involved in sense selection (Pustejovsky, 1995; Cruse, 1995; Apresjan, 1973). A number of syntactic and semantic tests are traditionally applied for sense identification, such as examining synonym series, compatible syntactic environments, coordination tests such as cross-understanding or zeugma test (Cruse, 2000). None of these tests are conclusive and normally a combination of factors is used. At the recent Senseval competitions (Mihalcea et al., 2004; Snyder and Palmer, 2004; Preiss and Yarowsky, 2001), the choice of sense inventories frequently presented problems, spurring the efforts to create coarsergrained sense inventories (Hovy et al., 2006; Palmer et al., 2007; Navigli, 2006). Part of the reason for such difficulties in establishing a set of senses available to a lexical item is that the meaning of a polysemous verb is often determined in composition and depends to the same extent on semantics of the particular arguments as it does on the base meaning of the verb itself. A number of systematic relations often holds between different senses of a polysemous </context>
</contexts>
<marker>Snyder, Palmer, 2004</marker>
<rawString>Snyder, B. and M. Palmer. 2004. The english all-words task. In Mihalcea, Rada and Phil Edmonds, editors, Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pages 41–43, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>