<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000068">
<title confidence="0.9063915">
SemEval-2013 Task 13:
Word Sense Induction for Graded and Non-Graded Senses
</title>
<author confidence="0.833917">
David Jurgens
</author>
<affiliation confidence="0.5248">
Dipartimento di Informatica
</affiliation>
<address confidence="0.33282">
Sapienza Universit`a di Roma
</address>
<email confidence="0.977625">
jurgens@di.uniroma1.it
</email>
<title confidence="0.321979">
Ioannis Klapaftis
</title>
<author confidence="0.518955">
Search Technology Center Europe
</author>
<affiliation confidence="0.636901">
Microsoft
</affiliation>
<email confidence="0.995787">
ioannisk@microsoft.com
</email>
<sectionHeader confidence="0.995579" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999617454545455">
Most work on word sense disambiguation has
assumed that word usages are best labeled
with a single sense. However, contextual am-
biguity or fine-grained senses can potentially
enable multiple sense interpretations of a us-
age. We present a new SemEval task for evalu-
ating Word Sense Induction and Disambigua-
tion systems in a setting where instances may
be labeled with multiple senses, weighted by
their applicability. Four teams submitted nine
systems, which were evaluated in two settings.
</bodyText>
<sectionHeader confidence="0.998906" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998980957446809">
Word Sense Disambiguation (WSD) attempts to
identify which of a word’s meanings applies in a
given context. A long-standing task, WSD is fun-
damental to many NLP applications (Navigli, 2009).
Typically, each usage of a word is treated as express-
ing only a single sense. However, contextual ambi-
guity as well as the relatedness of certain meanings
can potentially elicit multiple sense interpretations.
Recent work has shown that annotators find multi-
ple applicable senses in a given target word context
when using fine-grained sense inventories such as
WordNet (V´eronis, 1998; Murray and Green, 2004;
Erk et al., 2009; Passonneau et al., 2012b; Jurgens,
2013; Navigli et al., 2013). Such contexts would be
better annotated with multiple sense labels, weight-
ing each sense according to its applicability (Erk et
al., 2009; Jurgens, 2013), in effect allowing ambigu-
ity or multiple interpretations to be explicitly mod-
eled. Accordingly, the first goal of this task is to
evaluate WSD systems in a setting where instances
may be labeled with one or more senses, weighted
by their applicability.
WSD methods are ultimately defined and poten-
tially restricted by their choice in sense inventory;
for example, a sense inventory may have insufficient
sense-annotated data to build WSD systems for spe-
cific types of text (e.g., social media), or the inven-
tory may lack domain-specific senses. Word Sense
Induction (WSI) has been proposed as a method for
overcoming such limitations by learning the senses
automatically from text. In essence, a WSI algo-
rithm acts as a lexicographer by grouping word us-
ages according to their shared meaning. The sec-
ond goal of this task is to assess the performance of
WSI algorithms when they are able to model multi-
ple meanings of a usage with graded senses.
Task 12 focuses on disambiguating senses for 50
target lemmas: 20 nouns, 20 verbs, and 10 adjectives
(Sec. 2). Since the Task evaluates only unsupervised
systems, no training data was provided; however, to
enable more comparison, Unsupervised WSD sys-
tems were also allowed to participate. Participat-
ing systems were evaluated in two settings (Sec. 3),
depending on whether they used induced senses or
WordNet 3.1 senses for their annotations. The re-
sults (Sec. 5) demonstrate a substantial improvement
over the competitive most frequent sense baseline.
</bodyText>
<sectionHeader confidence="0.992797" genericHeader="method">
2 Task Description
</sectionHeader>
<bodyText confidence="0.999542">
This task required participating systems to annotate
instances of nouns, verb, and adjectives using Word-
Net 3.1 (Fellbaum, 1998), which was selected due
to its fine-grained senses. Participants could label
each instance with one or more senses, weighting
</bodyText>
<page confidence="0.933406">
290
</page>
<bodyText confidence="0.6406586">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 290–299, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
We all are relieved to lay aside our fight-or-flight reflexes and to commemorate our births from out of the dark
centers of the women, to feel the complexity of our love and frustration with each other, to stretch our cognition to
encompass the thoughts of every entity we know.
</bodyText>
<note confidence="0.9386048">
dark%3:00:01:: – devoid of or deficient in light or brightness; shadowed or black
dark%3:00:00:: – secret
I ask because my practice has always been to allow about five minutes grace, then remove it.
ask%2:32:02:: – direct or put; seek an answer to
ask%2:32:04:: – address a question to and expect an answer from
</note>
<tableCaption confidence="0.983872">
Table 1: Example instances with multiple senses due to intended double meanings (top) or contextual am-
biguity (bottom). Senses are specified using their WordNet 3.1 sense keys.
</tableCaption>
<bodyText confidence="0.995557090909091">
each by their applicability. Table 1 highlights two
example contexts where multiple senses apply. The
first example shows a case of an intentional dou-
ble meaning that evokes both the physical aspect of
dark.a as being devoid of light and the causal re-
sult of being secret. In contrast, the second example
shows a case of multiple interpretations from ambi-
guity; a different preceding context could generate
the alternate interpretations “I ask [you] because”
(sense ask%2:32:04::) or “I ask [the question]
because” (sense ask%2:32:02::).
</bodyText>
<subsectionHeader confidence="0.903236">
2.1 Data
</subsectionHeader>
<bodyText confidence="0.999831666666667">
Three datasets were provided with the task. The trial
dataset provided weighted word sense annotations
using the data gathered by Erk et al. (2009). The
trial dataset consisted of 50 contexts for eight words,
where each context was labeled with WordNet 3.0
sense ratings from three untrained lexicographers.
Due to the unsupervised nature of the task, partic-
ipants were not provided with sense-labeled training
data. However, WSI systems were provided with the
ukWaC corpus (Baroni et al., 2009) to use in induc-
ing senses. Previous SemEval WSI tasks had pro-
vided participants with corpora specific to the task’s
target terms; in contrast, this task opted to use a large
corpus to enable WSI methods that require corpus-
wide statistics, e.g., statistical associations.
Test data was drawn from the Open American
National Corpus (Ide and Suderman, 2004, OANC)
across a variety of genres and from both the spoken
and written portions of the corpus, summarized in
Table 2. All contexts were manually inspected to en-
sure that the lemma being disambiguated was of the
correct part of speech and had an interpretation that
matched at least one WordNet 3.1 sense. This filter-
ing also removed instances that were in a colloca-
tion, or had an idiomatic meaning. Ultimately, 4664
contexts were used as test data, with a minimum of
22 and a maximum of 100 contexts per word.
</bodyText>
<subsectionHeader confidence="0.998984">
2.2 Sense Annotation
</subsectionHeader>
<bodyText confidence="0.999973">
Recent work proposes to gather sense annotations
using crowdsourcing in order to reduce the time
and cost of acquiring sense-annotated corpora (Bie-
mann and Nygaard, 2010; Passonneau et al., 2012b;
Rumshisky et al., 2012; Jurgens, 2013). There-
fore, we initially annotated the Task’s data using the
method of Jurgens (2013), where workers on Ama-
zon Mechanical Turk (AMT) rated all senses of a
word on a Likert scale from one to five, indicat-
ing the sense does not apply at all or completely
applies, respectively. Twenty annotators were as-
signed per instance, with their ratings combined by
selecting the most frequent rating. However, we
found that while the annotators achieved moderate
inter-annotator agreement (IAA), the resulting an-
notations were not of high enough quality to use in
the Task’s evaluations. Specifically, for some senses
and contexts, AMT annotators required more infor-
mation about sense distinctions than was feasible to
integrate into the AMT setting, which led to consis-
tent but incorrect sense assignments.
Therefore, the test data was annotated by the two
authors, with the first author annotating all instances
and the second author annotating a 10% sample of
each lemma’s instances in order to calculate IAA.
IAA was calculated using Krippendorff’s α (Krip-
pendorff, 1980; Artstein and Poesio, 2008), which is
an agreement measurement that adjusts for chance,
</bodyText>
<page confidence="0.996381">
291
</page>
<table confidence="0.9992426">
Spoken Written
Genre Face-to-face Telephone Fiction Journal Letters Non-fiction Technical Travel Guides All
Instances 52 699 127 2403 103 477 611 192 4664
Tokens 1742 30,700 3438 69,479 2238 11,780 17,337 4490 141,204
Mean senses/inst. 1.17 1.08 1.15 1.13 1.31 1.10 1.11 1.11 1.12
</table>
<tableCaption confidence="0.999415">
Table 2: Test data used in Task 12, divided according to source type
</tableCaption>
<bodyText confidence="0.999852333333333">
ranging in (−1, 1] for interval data, where 1 indi-
cates perfect agreement and -1 indicates systematic
disagreement; two random annotations have an ex-
pected α of zero. We treat each sense and instance
combination as a separate item to rate. The total IAA
for the dataset was 0.504, and on individual words,
ranged from 0.903 for number.n to 0.00 for win.v.
While this IAA is less than the 0.8 recommended by
Krippendorff (2004), it is consistent with the IAA
distribution for the sense annotations of MASC on
other parts of the OANC corpus: Passonneau et al.
(2012a) reports an α of 0.88 to -0.02 with the MASI
statistic (Passonneau et al., 2006).
Table 2 summarizes the annotation statistics for
the Task’s data. The annotation process resulted in
far fewer senses per instance in the trial data, which
we attribute to using trained annotators. An analysis
across the corpora genres showed that the multiple-
sense annotation rates were similar. Due to the vari-
ety of contextual sources, all lemmas were observed
with at least two distinct senses.
</bodyText>
<sectionHeader confidence="0.998403" genericHeader="method">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999896111111111">
We adopt a two-part evaluation setting used in pre-
vious SemEval WSI and WSD tasks (Agirre and
Soroa, 2007; Manandhar et al., 2010). The first eval-
uation uses a traditional WSD task that directly com-
pares WordNet sense labels. For WSI systems, their
induced sense labels are converted to WordNet 3.1
labels via a mapping procedure. The second evalu-
ation performs a direct comparison of the two sense
inventories using clustering comparisons.
</bodyText>
<subsectionHeader confidence="0.997751">
3.1 WSD Task
</subsectionHeader>
<bodyText confidence="0.98536475">
In the first evaluation, we adopt a WSD task with
three objectives: (1) detecting which senses are ap-
plicable, (2) ranking senses by their applicability,
and (3) measuring agreement in applicability rat-
ings with human annotators. Each objectives uses
a specific measurement: (1) the Jaccard Index, (2)
positionally-weighted Kendall’s T similarity, and
(3) a weighted variant of Normalized Discounted
Cumulative Gain, respectively. Each measure is
bounded in [0, 1], where 1 indicates complete agree-
ment with the gold standard. We generalize the tra-
ditional definition of WSD Recall such that it mea-
sures the average score for each measure across all
instances, including those not labeled by the system.
Systems are ultimately scored using the F1 measure
between each objective’s measure and Recall.
</bodyText>
<subsectionHeader confidence="0.931795">
3.1.1 Transforming Induced Sense Labels
</subsectionHeader>
<bodyText confidence="0.999987">
In the WSD setting, induced sense labels may be
transformed into a reference inventory (e.g., Word-
Net 3.1) using a sense mapping procedure. We fol-
low the 80/20 setup of Manandhar et al. (2010),
where the corpus is randomly divided into five par-
titions, four of which are used to learn the sense
mapping; the sense labels for the held-out partition
are then converted and compared with the gold stan-
dard. This process is repeated so that each partition
is tested once. For learning the sense mapping func-
tion, we use the distribution mapping technique of
Jurgens (2012), which takes into account the sense
applicability weights in both labelings.
</bodyText>
<subsectionHeader confidence="0.843016">
3.1.2 Jaccard Index
</subsectionHeader>
<bodyText confidence="0.965308666666667">
Given two sets of sense labels for an instance,
X and Y , the Jaccard Index is used to measure the
agreement: �XnY �
�XuY �. The Jaccard Index is maximized
when X and Y use identical labels, and is mini-
mized when the sets of sense labels are disjoint.
</bodyText>
<subsectionHeader confidence="0.48256">
3.1.3 Positionally-Weighted Kendall’s T
</subsectionHeader>
<bodyText confidence="0.9997465">
Rank correlations have been proposed for evalu-
ating a system’s ability to order senses by applicabil-
ity; in previous work, both Erk and McCarthy (2009)
and Jurgens (2012) propose rank correlation coeffi-
cients that assume all positions in the ranking are
equally important. However, in the case of graded
</bodyText>
<page confidence="0.981548">
292
</page>
<bodyText confidence="0.999863291666667">
sense evaluation, often only a few senses are appli-
cable, with the applicability ratings of the remain-
ing senses being relatively inconsequential. There-
fore, we consider an alternate rank scoring based on
Kumar and Vassilvitskii (2010), which weights the
penalty of reordering the lower positions less than
the penalty of reordering the first ranks.
Kendall’s T distance, K, is a measure of the
number of item position swaps required to make
two sequences identical. Kumar and Vassilvitskii
(2010) extend this distance definition using a vari-
able penalty function S for the cost of swapping two
positions, which we denote Kδ. By using an appro-
priate S, Kδ can be biased towards the correctness
of higher ranks by assigning a smaller S to lower
ranks. Because Kδ is a distance measure, its value
range will be different depending on the number of
ranks used. Therefore, to convert the measure to a
similarity we normalize the distance to [0, 1] by di-
viding by the maximum Kδ distance and then sub-
tracting the distance from one. Given two rankings
x and y where x is the reference by which y is to be
measured, we may compute the normalized similar-
ity using
</bodyText>
<equation confidence="0.998385333333333">
Ksimδ = 1 − Kδ(x, y) (1)
Kmax
δ (x)�
</equation>
<bodyText confidence="0.987505384615385">
Equation 1 has its maximal value of one when rank-
ing y is identical to ranking x, and its minimal value
of zero when y is in the reverse order as x. We refer
to this value as the positionally-weighted Kendall’s
T similarity, Ksim
δ . As defined, Ksim
δ does not ac-
count for ties. Therefore, we arbitrarily break ties in
a deterministic fashion for both rankings. Second,
we define S to assign higher cost to the first ranks:
the cost to move an item into position i, Si, is de-
fined as n−(i+1), where n is the number of senses.
n
</bodyText>
<subsectionHeader confidence="0.801444">
3.1.4 Weighted NDCG
</subsectionHeader>
<bodyText confidence="0.997977">
To compare the applicability ratings for sense an-
notations, we recast the annotation process in an In-
formation Retrieval setting: Given an example con-
text acting as a query over a word’s senses, the task
is to retrieve all applicable senses, ranking and scor-
ing them by their applicability. Moffat and Zobel
(2008) propose using Discounted Cumulative Gain
(DCG) as a method to compare a ranking against a
baseline. Given (1) a gold standard weighting of the
k senses applicable to a context, where wi denotes
the applicability for sense i in the gold standard, and
(2) a ranking of the k senses by some method, the
</bodyText>
<equation confidence="0.85276">
DCG may be calculated as k 2wi+1—1 DCG is
Y �i=1 log2(i+1)
</equation>
<bodyText confidence="0.988590142857143">
commonly normalized to [0, 1] so that the value is
comparable when computed on rankings with dif-
ferent k and weight values. To normalize, the maxi-
mum value is calculated by first computing the DCG
on the ranking when the k items are sorted by their
weights, referred as the Ideal DCG (IDCG), and then
normalizing as NDCG = DCG
IDCG.
The DCG only considers the weights assigned
in the gold standard, which potentially masks im-
portance differences in the weights assigned to the
senses. Therefore, we propose weighting the DCG
by the relative difference in the two weights. Given
an alternate weighting of the k items, denoted as wi,
</bodyText>
<equation confidence="0.998282">
min(wi, ibi) (2wi+1 − 1
max(wi, wi) k-
lo92 (i)
</equation>
<bodyText confidence="0.999660111111111">
The key impact in Equation 2 comes from weight-
ing an item’s contribution to the score by its rela-
tive deviation in absolute weight. A set of weights
that achieves an equivalent ranking may have a low
WDCG if the weights are significantly higher or
lower than the reference. Equation 2 may be nor-
malized in the same way as the DCG. We refer to
this final normalized measure as the Weighted Nor-
malized Discounted Cumulative Gain (WNDCG).
</bodyText>
<subsectionHeader confidence="0.999717">
3.2 Sense Cluster Comparisons
</subsectionHeader>
<bodyText confidence="0.9999311875">
Sense induction can be viewed as an unsupervised
clustering task where usages of a word are grouped
into clusters, each representing uses of the same
meaning. In previous SemEval tasks on sense in-
duction, instances were labeled with a single sense,
which yields a partition over the instances into dis-
joint sets. The proposed partition can then be com-
pared with a gold-standard partition using many ex-
isting clustering comparison methods, such as the
V-Measure (Rosenberg and Hirschberg, 2007) or
paired FScore (Artiles et al., 2009). Such cluster
comparison methods measure the degree of similar-
ity between the sense boundaries created by lexicog-
raphers and those created by WSI methods.
In the present task, instances are potentially la-
beled both with multiple senses and with weights
</bodyText>
<equation confidence="0.9973175">
k
WDCG =
i=1
� (2)
</equation>
<page confidence="0.99128">
293
</page>
<bodyText confidence="0.999983421052632">
reflecting the applicability. This type of sense label-
ing produces a fuzzy clustering: An instance may
belong to one or more sense clusters with its clus-
ter membership relative to its weight for that sense.
Formally, we refer to (1) a solution where the sets
of instances overlap as a cover and (2) a solution
where the sets overlap and instances may have par-
tial memberships in a set as fuzzy cover.
We propose two new fuzzy measures for com-
paring fuzzy sense assignments: Fuzzy B-Cubed
and Fuzzy Normalized Mutual Information. The
two measures provide complementary information.
B-Cubed summarizes the performance per instance
and therefore provides an estimate of how well a sys-
tem would perform on a new corpus with a similar
sense distribution. In contrast, Fuzzy NMI is mea-
sured based on the clusters rather than the instances,
thereby providing a performance analysis that is in-
dependent of the corpus sense distribution.
</bodyText>
<subsectionHeader confidence="0.588364">
3.2.1 Fuzzy B-Cubed
</subsectionHeader>
<bodyText confidence="0.999808913043478">
Bagga and Baldwin (1998) proposed a clustering
evaluation known as B-Cubed, which compares two
partitions on a per-item basis. Amig´o et al. (2009)
later extended the definition of B-Cubed to compare
overlapping clusters (i.e., covers). We generalize B-
Cubed further to handle the case of fuzzy covers.
B-Cubed is based on precision and recall, which es-
timate the fit between two clusterings, X and Y at
the item level. For an item i, precision reflects how
many items sharing a cluster with i in X appear in its
cluster in Y ; conversely, recall measures how many
items sharing a cluster in Y with i also appear in its
cluster in X. The final B-Cubed value is the har-
monic mean of the two scores.
To generalize B-Cubed to fuzzy covers, we adopt
the formalization of Amig´o et al. (2009), who define
item-based precision and recall functions, P and R,
in terms of a correctness function, C —* 10, 11. For
notational brevity, let avg be a function that returns
the mean value of a series, and µx(i) denote the set
of clusters in clustering X of which item i is a mem-
ber. B-Cubed precision and recall may therefore cal-
culated over all n items:
</bodyText>
<equation confidence="0.89254925">
B-Cubed Precision = avg [ avg P(i, j)] (3)
i j6=i∈∪µy(i)
B-Cubed Recall = avg [ avg R(i, j)]. (4)
i j6=i∈∪µ.(i)
</equation>
<bodyText confidence="0.999617">
When comparing partitions, P and R are defined as
1 if two items cluster labels are identical. To gen-
eralize B-Cubed for fuzzy covers, we redefine P
and R to account for differences in the partial clus-
ter membership of items. Let EX(i) denote the set
of clusters of which i is a member, and wk(i) de-
note the membership weight of item i in cluster k in
X. We therefore define C with respect to X of two
items as
</bodyText>
<equation confidence="0.995386">
C(i, j, X) = � 1−|wk(i)−wk(j)|. (5)
k∈`X(i)∪`X(j)
</equation>
<bodyText confidence="0.995966428571429">
Equation 5 is maximized when i and j have
identical membership weights in the clusters of
which they are members. Importantly, Equation
5 generalizes to the correctness operations both
when comparing partitions and covers, as defined
by Amig´o et al. (2009). Item-based Precision
and Recall are then defined using Equation 5 as
</bodyText>
<equation confidence="0.9690906">
P(i, j, X) = Min(C(i,j,X),C(i,j,Y ))
and R(i, j, X) =
C(i,j,X)
Min(C(i,j,X),C(i,j,Y )) , respectively. These fuzzy gen-
C(i,j,Y )
</equation>
<bodyText confidence="0.656411">
eralizations are used in Equations 3 and 4.
</bodyText>
<subsectionHeader confidence="0.661255">
3.2.2 Fuzzy Normalized Mutual Information
</subsectionHeader>
<bodyText confidence="0.999981625">
Mutual information measures the dependence be-
tween two random variables. In the context of
clustering evaluation, mutual information treats the
sense labels as random variables and measures the
level of agreement in which instances are labeled
with the same senses (Danon et al., 2005). For-
mally, mutual information is defined as I(X; Y ) =
H(X)−(H(X|Y ) where H(X) denotes the entropy
of the random variable X that represents a parti-
tion, i.e., the sets of instances assigned to each sense.
Typically, mutual information is normalized to [0, 1]
in order to facilitate comparisons between multiple
clustering solutions on the same scale (Luo et al.,
2009), with Max(H(X), H(Y )) being the recom-
mended normalizing factor (Vinh et al., 2010).
In its original formulation Mutual information
is defined only to compare non-overlapping cluster
partitions. Therefore, we propose a new definition of
mutual information between fuzzy covers using ex-
tension of Lancichinetti et al. (2009) for calculating
the normalized mutual information between covers.
In the case of partitions, a clustering is represented
as a discrete random variable whose states denote
the probability of being assigned to each cluster. In
</bodyText>
<page confidence="0.995131">
294
</page>
<bodyText confidence="0.999991472222222">
the fuzzy cover setting, each item may be assigned
to multiple clusters and no longer has a binary as-
signment to a cluster, but takes on a value in [0, 1].
Therefore, each cluster Xi can be represented sepa-
rately as a continuous random variable, with the en-
tire fuzzy cover denoted as the variable X1...k, where
the ith entry of X is the continuous random vari-
able for cluster i. However, by modeling clusters us-
ing continuous domain, differential entropy must be
used for the continuous variables; importantly, dif-
ferential entropy does not obey the same properties
as discrete entropy and may be negative.
To avoid calculating entropy in the continuous do-
main, we therefore propose an alternative method of
computing mutual information based on discretiz-
ing the continuous values of Xi in the fuzzy set-
ting. For the continuous random variable Xi, we
discretize the value by dividing up probability mass
into discrete bins. That is, the support of Xi is parti-
tioned into disjoint ranges, each of which represents
a discrete outcome of Xi. As a result, Xi becomes a
categorical distribution over a set of weights ranges
{w1, ... , wn} that denote the strength of member-
ship in the fuzzy set. With respect to sense annota-
tion, this discretization process is analogous to hav-
ing an annotator rate the applicability of a sense for
an instance using a Likert scale instead of using a
rational number within a fixed bound.
Discretizing the continuous cluster membership
ratings into bins allows us to avoid the problematic
interpretation of entropy in the continuous domain
while still expanding the definition of mutual infor-
mation from a binary cluster membership to one of
degrees. Using the definition of Xi and Yj as a cate-
gorical variables over discrete ratings, we may then
estimate the entropy and joint entropy as follows.
</bodyText>
<equation confidence="0.988977666666667">
n
H(Xi) = p(wi)log2p(wi) (6)
i=1
</equation>
<bodyText confidence="0.999909666666667">
where p(wi) is the probability of an instance being
labeled with rating wi Similarly, we may define the
joint entropy of two fuzzy clusters as
</bodyText>
<equation confidence="0.887201">
p(wi, wj)log2p(wi, wj) (7)
</equation>
<bodyText confidence="0.999858">
where p(wi, wj) is the probability of an instance be-
ing labeled with rating wi in cluster Xk and wj in
cluster Yl, and m denotes the number of bins for Yl.
The conditional entropy between two clusters may
then be calculated as
</bodyText>
<equation confidence="0.992772">
H(XkJYl) = H(Xk,Yl) − H(Yl).
</equation>
<bodyText confidence="0.999917285714286">
Together, Equations 6 and 7 may be used to define
I(X, Y ) as in the original definition. We then nor-
malize using the method of McDaid et al. (2011).
Based on the limited range of fuzzy memberships
in [0, 1], we selected uniformly distributed bins in
[0, 1] at 0.1 intervals when discretizing the member-
ship weights for sense labelings.
</bodyText>
<subsectionHeader confidence="0.997889">
3.3 Baselines
</subsectionHeader>
<bodyText confidence="0.9999746875">
Task 12 included multiple baselines based on mod-
eling different types of WSI and WSD systems.
Due to space constraints, we include only the four
most descriptive here: (1) Semcor MFS which la-
bels each instance with the most frequent sense of
that lemma in SemCor, (2) Semcor Ranked Senses
baseline, which labels each instance with all of the
target lemma’s senses, ranked according to their fre-
quency in SemCor, using weights n�i�1
n , where n is
the number of senses and i is the rank, (3) 1c1inst
which labels each instance with its own induced
sense and (4) All-instances, One sense which la-
bels all instances with the same induced sense. The
first two baselines directly use WordNet 3.1 senses,
while the last two use induced senses.
</bodyText>
<sectionHeader confidence="0.96472" genericHeader="method">
4 Participating Systems
</sectionHeader>
<bodyText confidence="0.999968117647059">
Four teams submitted nine systems, seven of which
used induced sense inventories. AI-KU submitted
three WSI systems based on a lexical substitution
method; a language model is built from the target
word’s contexts in the test data and the ukWaC cor-
pus and then Fastsubs (Yuret, 2012) is used to iden-
tify lexical substitutes for the target. Together, the
contexts of the target and substitutes are used to
build a distributional model using the S-CODE al-
gorithm (Maron et al., 2010). The resulting contex-
tual distributions are then clustered using K-means
to identify word senses. The University of Mel-
bourne (Unimelb) team submitted two WSI systems
based on the approach of Lau et al. (2012). Their
systems use a Hierarchical Dirichlet Process (Teh
et al., 2006) to automatically infer the number of
senses from contextual and positional features. Un-
</bodyText>
<equation confidence="0.9729878">
H(Xk, Yl) =
n
i=1
�m
j=1
</equation>
<page confidence="0.997042">
295
</page>
<table confidence="0.9997530625">
Team System WSD F1 Cluster Comparison #Cl #S
Jac. Ind. Ksim WNDCG Fuzzy NMI Fuzzy B-Cubed
�
AI-KU Base 0.197 0.620 0.387 0.065 0.390 7.76 6.61
AI-KU add1000 0.197 0.606 0.215 0.035 0.320 7.76 6.61
AI-KU remove5-add1000 0.244 0.642 0.332 0.039 0.451 3.12 5.33
Unimelb 5p 0.218 0.614 0.365 0.056 0.459 2.37 5.97
Unimelb 50k 0.213 0.620 0.371 0.060 0.483 2.48 6.08
UoS #WN Senses 0.192 0.596 0.315 0.047 0.201 8.08 6.77
UoS top-3 0.232 0.625 0.374 0.045 0.448 3.00 5.44
La Sapienza system-1 0.149 0.507 0.311 - - - 8.69
La Sapienza system-2 0.149 0.510 0.383 - - - 8.67
All-instances, One sense 0.192 0.609 0.288 0.0 0.623 1.00 6.62
1c1inst 0.0 0.0 0.0 0.071 0.0 1.00 0.0
Semcor MFS 0.455 0.465 0.339 - - - 1.00
Semcor Ranked Senses 0.149 0.559 0.489 - - - 8.66
</table>
<tableCaption confidence="0.9880145">
Table 3: Performance on the five evaluation measures for all system and selected baselines. Top system
performances are marked in bold.
</tableCaption>
<bodyText confidence="0.999667571428572">
like other teams, the Unimelb systems were trained
on a Wikipedia corpus instead of the ukWaC cor-
pus. The University of Sussex (UoS) team submit-
ted two WSI systems that use dependency-parsed
features from the corpus, which are then clustered
into senses using the MaxMax algorithm (Hope and
Keller, 2013); the resulting fine-grained clusters are
then combined based on their degree of separabil-
ity. The La Sapienza team submitted two Unsu-
pervised WSD systems based applying Personal-
ized Page Rank (Agirre and Soroa, 2009) over a
WordNet-based network to compare the similarity of
each sense with the similarity of the context, ranking
each sense according to its similarity.
</bodyText>
<sectionHeader confidence="0.998675" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999252285714286">
Table 3 shows the main results for all instances. Ad-
ditionally, we report the number of induced clusters
used to label each sense as #Cl and the number of
resulting WordNet 3.1 senses for each sense with
#S. As in previous WSD tasks, the MFS baseline
was quite competitive, outperforming all systems on
detecting which senses were applicable, measured
using the Jaccard Index. However, most systems
were able to outperform the MFS baseline on rank-
ing senses and quantifying their applicability.
Previous cluster comparison evaluations often
faced issues with the measures being biased either
towards the 1c1inst baseline or labeling all instances
with the same sense. However, Table 3 shows that
</bodyText>
<table confidence="0.999938384615385">
Team System F1 NMI B-Cubed
AI-KU Base 0.641 0.045 0.351
AI-KU add1000 0.601 0.023 0.288
AI-KU remove5-add1000 0.628 0.026 0.421
Unimelb 5p 0.596 0.035 0.421
Unimelb 50k 0.605 0.039 0.441
UoS #WN Senses 0.574 0.031 0.180
UoS top-3 0.600 0.028 0.414
La Sapienza System-1 0.204 - -
La Sapienza System-2 0.217 - -
All-instances, One sense 0.569 0.0 0.570
1c1inst 0.0 0.018 0.0
Semcor MFS 0.477 0.0 0.570
</table>
<tableCaption confidence="0.8289725">
Table 4: System performance in the single-sense set-
ting. Top system performances are marked in bold.
</tableCaption>
<bodyText confidence="0.998300333333333">
systems are capable of performing well in both the
Fuzzy NMI and Fuzzy B-Cubed measures, thereby
avoiding the extreme performance of either baseline.
An analysis of the systems’ results showed that
many systems labeled instances with a high num-
ber of senses, which could have been influenced by
the trial data having significantly more instances la-
beled with multiple senses than the test data. There-
fore, we performed a second analysis that parti-
tioned the test set into two sets: those labeled with
a single sense and those with multiple senses. For
single-sense set, we modified the test setting to have
systems also label instances with a single sense:
(1) the sense mapping function for WSI systems
(Sec. 3.1.1) was modified so that after the mapping,
</bodyText>
<page confidence="0.994941">
296
</page>
<table confidence="0.999700875">
Team System WSD F1 Cluster Comparison
Jac. Ind. Ksim WNDCG Fuzzy NMI Fuzzy B-Cubed
�
AI-KU Base 0.394 0.617 0.317 0.029 0.078
AI-KU add1000 0.394 0.620 0.214 0.014 0.061
AI-KU remove5-add1000 0.434 0.585 0.290 0.004 0.116
Unimelb 5p 0.436 0.585 0.286 0.019 0.130
Unimelb 5000k 0.414 0.602 0.298 0.021 0.134
UoS #WN Senses 0.367 0.627 0.313 0.036 0.037
UoS top-3 0.421 0.574 0.302 0.006 0.113
La Sapienza system-1 0.263 0.660 0.447 - -
La Sapienza system-2 0.412 0.694 0.536 - -
All-instances, One sense 0.387 0.635 0.254 0.0 0.130
1c1inst 0.0 0.0 0.0 0.300 0.0
Semcor MFS 0.283 0.373 0.197
Semcor Ranked Senses 0.263 0.593 0.395
</table>
<tableCaption confidence="0.886824">
Table 5: System performance on all instances labeled with multiple senses. Top system performances are
marked in bold.
</tableCaption>
<bodyText confidence="0.999601">
only the highest-weighted WordNet 3.1 sense was
used, and (2) the La Sapienza system output was
modified to retain only the highest weighted sense.
In this single-sense setting, systems were evaluated
using the standard WSD Precision and Recall mea-
sures; we report the F1 measure of Precision and Re-
call. The remaining subset of instances annotated
with multiple senses were evaluated separately.
Table 4 shows the systems’ performance on
single-sense instances, revealing substantially in-
creased performance and improvement over the
MFS baseline for WSI systems. Notably, the per-
formance of the best sense-remapped WSI systems
surpasses the performance of many supervised WSD
systems in previous WSD evaluations (Kilgarriff,
2002; Mihalcea et al., 2004; Pradhan et al., 2007;
Agirre et al., 2010). This performance suggests that
WSI systems using graded labels provide a way to
leverage huge amounts of unannotated corpus data
for finding sense-related features in order to train
semi-supervised WSD systems.
Table 5 shows the performance on the subset of
instances that were annotated with multiple senses.
We note that in this setting, the mapping proce-
dure transforms the All-Instances One Sense base-
line into the average applicability rating for each
sense in the test corpus. Notably, the La Sapienza
systems sees a significant performance increase in
this setting; their systems label each instance with
all of the lemma’s senses, which significantly de-
grades performance in the most common case where
only a single sense applies. However, when multi-
ple senses are known to be present, their method for
quantifying sense applicability appears closest to the
gold standard judgments. Furthermore, the majority
of WSI systems are able to surpass all four baselines
on identifying which senses are present and quanti-
fying their applicability.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999977357142857">
We have introduced a new evaluation setting for
WSI and WSD systems where systems are measured
by their ability to detect and weight multiple appli-
cable senses for a single context. Four teams submit-
ted nine systems, annotating a total of 4664 contexts
for 50 words from the OANC. Many systems were
able to surpass the competitive MFS baseline. Fur-
thermore, when WSI systems were trained to pro-
duce only a single sense label, the performance of
resulting semi-supervised WSD systems surpassed
that of many supervised systems in previous WSD
evaluations. Future work may assess the impact of
graded sense annotations in a task-based setting. All
materials have been released on the task website.1
</bodyText>
<sectionHeader confidence="0.998299" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.993422">
We thank Rebecca Passonneau for her feedback and
suggestions for target lemmas used in this task.
</bodyText>
<footnote confidence="0.99077">
1http://www.cs.york.ac.uk/semeval-2013/task13/
</footnote>
<page confidence="0.994404">
297
</page>
<sectionHeader confidence="0.989419" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998977058252427">
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task
2: Evaluating word sense induction and discrimination
systems. In Proceedings of the Fourth International
Workshop on Semantic Evaluations, pages 7–12. ACL.
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for Word Sense Disambiguation. In Pro-
ceedings of EACL, pages 33–41. ACL.
Eneko Agirre, Oier L´opez De Lacalle, Christine Fell-
baum, Andrea Marchetti, Antonio Toral, and Piek
Vossen. 2010. SemEval-2010 task 17: All-words
word sense disambiguation on specific domains. In
Proceedings of SemEval-2010. ACL.
Enrique Amig´o, Julio Gonzalo, Javier Artiles, and Felisa
Verdejo. 2009. A comparison of extrinsic clustering
evaluation metrics based on formal constraints. Infor-
mation Retrieval, 12(4):461–486.
Javier Artiles, Enrique Amig´o, and Julio Gonzalo. 2009.
The role of named entities in web people search. In
Proceedings of EMNLP, pages 534–542. Association
for Computational Linguistics.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555–596.
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In Proceedings of the Lin-
guistic Coreference Workshop at LREC, pages 563–
566.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209–226.
Chris Biemann and Valerie Nygaard. 2010. Crowdsourc-
ing wordnet. In The 5th International Conference of
the Global WordNet Association (GWC-2010).
Leon Danon, Albert Diaz-Guilera, Jordi Duch, and Alex
Arenas. 2005. Comparing community structure iden-
tification. Journal of Statistical Mechanics: Theory
and Experiment, 2005(09):P09008.
Katrin Erk and Diana McCarthy. 2009. Graded word
sense assignment. In Proceedings of Empirical Meth-
ods in Natural Language Processing (EMNLP), pages
440–449. ACL.
Katrin Erk, Diana McCarthy, and Nicholas Gaylord.
2009. Investigations on word senses and word us-
ages. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 10–18. ACL.
Christine Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press.
David Hope and Bill Keller. 2013. MaxMax: A Graph-
Based Soft Clustering Algorithm Applied to Word
Sense Induction. In Proceedings of CICLing, pages
368–381.
Nancy Ide and Keith Suderman. 2004. The american
national corpus first release. In Proceedings of the
Fourth Language Resources and Evaluation Confer-
ence, pages 1681–1684.
David Jurgens. 2012. An Evaluation of Graded Sense
Disambiguation using Word Sense Induction. In Pro-
ceedings of *SEM, the First Joint Conference on Lexi-
cal and Computational Semantics. ACL.
David Jurgens. 2013. Embracing Ambiguity: A Com-
parison of Annotation Methodologies for Crowdsourc-
ing Word Sense Labels. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL). ACL.
Adam Kilgarriff. 2002. English lexical sample
task description. In Proceedings of ACL-SIGLEX
SENSEVAL-2 Workshop.
Klaus Krippendorff. 1980. Content Analysis: An Intro-
duction to Its Methodology. Sage, Beverly Hills, CA.
Klaus Krippendorff. 2004. Content Analysis: An In-
troduction to Its Methodology. Sage, Thousand Oaks,
CA, second edition.
Ravi Kumar and Sergei Vassilvitskii. 2010. General-
ized distances between rankings. In Proceedings of
the 19th International Conference on World Wide Web
(WWW), pages 571–580. ACM.
Andrea Lancichinetti, Santo Fortunato, and J´anos
Kert´esz. 2009. Detecting the overlapping and hierar-
chical community structure in complex networks. New
Journal of Physics, 11(3):033015.
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012. Word sense induc-
tion for novel sense detection. In Proceedings of the
13th Conference of the European Chapter of the Asso-
ciation for computational Linguistics (EACL 2012).
Ping Luo, Hui Xiong, Guoxing Zhan, Junjie Wu, and
Zhongzhi Shi. 2009. Information-theoretic distance
measures for clustering validation: Generalization and
normalization. IEEE Transactions on Knowledge and
Data Engineering, 21(9):1249–1262.
Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-
gach, and Sameer S. Pradhan. 2010. SemEval-2010
task 14: Word sense induction &amp; disambiguation. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 63–68. ACL.
Yariv Maron, Michael Lamar, and Elie Bienenstock.
2010. Sphere embedding: An application to part-of-
speech induction. In Proceedings of Advances in Neu-
ral Information Processing Systems (NIPS).
</reference>
<page confidence="0.968846">
298
</page>
<reference confidence="0.999486681818182">
Aaron F. McDaid, Derek Greene, and Neil Hurley. 2011.
Normalized mutual information to evaluate overlap-
ping community finding algorithms. arXiv:1110.2515.
Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-
riff. 2004. The Senseval-3 English lexical sample
task. In Senseval-3: Third International Workshop on
the Evaluation of Systems for the Semantic Analysis of
Text, pages 25–28. ACL.
Alistair Moffat and Justin Zobel. 2008. Rank-biased
precision for measurement of retrieval effectiveness.
ACM Transactions on Information Systems (TOIS),
27(1):2.
G. Craig Murray and Rebecca Green. 2004. Lexical
knowledge and human disagreement on a wsd task.
Computer Speech &amp; Language, 18(3):209–222.
Roberto Navigli, David Jurgens, and Daniele Vanilla.
2013. Semeval-2013 task 12: Multilingual word sense
disambiguation. In Proceedings of the 7th Interna-
tional Workshop on Semantic Evaluation.
Roberto Navigli. 2009. Word Sense Disambiguation: A
Survey. ACM Computing Surveys, 41(2):1–69.
Rebecca Passonneau, Nizar Habash, and Owen Rambow.
2006. Inter-annotator agreement on a multilingual se-
mantic annotation task. In Proceedings of the Fifth
International Conference on Language Resources and
Evaluation (LREC), pages 1951–1956.
Rebecca J Passonneau, Collin Baker, Christiane Fell-
baum, and Nancy Ide. 2012a. The MASC word sense
sentence corpus. In Proceedings of LREC.
Rebecca J. Passonneau, Vikas Bhardwaj, Ansaf Salleb-
Aouissi, and Nancy Ide. 2012b. Multiplicity and
word sense: evaluating and learning from multiply la-
beled word sense annotations. Language Resources
and Evaluation, pages 1–34.
Sameer S. Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. SemEval-2007 task 17: English
lexical sample, SRL, and all-words. In Proceedings of
the 4th International Workshop on Semantic Evalua-
tions. ACL.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external cluster
evaluation measure. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL). ACL.
Anna Rumshisky, Nick Botchan, Sophie Kushkuley, and
James Pustejovsky. 2012. Word sense inventories by
non-experts. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(LREC).
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566–1581.
Jean V´eronis. 1998. A study of polysemy judgments and
inter-annotator agreement. In Program and advanced
papers of the Senseval workshop.
Nguyen Xuan Vinh, Julien Epps, and James Bailey.
2010. Information theoretic measures for clusterings
comparison: Variants, properties, normalization and
correction for chance. The Journal of Machine Learn-
ing Research, 11:2837–2854.
Deniz Yuret. 2012. FASTSUBS: An Efcient Admissible
Algorithm for Finding the Most Likely Lexical Substi-
tutes Using a Statistical Language Model. Computing
Research Repository (CoRR).
</reference>
<page confidence="0.998618">
299
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.296808">
<title confidence="0.9543745">SemEval-2013 Task 13: Word Sense Induction for Graded and Non-Graded Senses</title>
<author confidence="0.957196">David</author>
<affiliation confidence="0.702">Dipartimento di</affiliation>
<author confidence="0.671387">Sapienza Universit`a di</author>
<email confidence="0.935124">jurgens@di.uniroma1.it</email>
<affiliation confidence="0.9403955">Ioannis Search Technology Center</affiliation>
<email confidence="0.999576">ioannisk@microsoft.com</email>
<abstract confidence="0.999305666666667">Most work on word sense disambiguation has assumed that word usages are best labeled with a single sense. However, contextual ambiguity or fine-grained senses can potentially enable multiple sense interpretations of a usage. We present a new SemEval task for evaluating Word Sense Induction and Disambiguation systems in a setting where instances may be labeled with multiple senses, weighted by their applicability. Four teams submitted nine systems, which were evaluated in two settings.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Semeval-2007 task 2: Evaluating word sense induction and discrimination systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations,</booktitle>
<pages>7--12</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="9235" citStr="Agirre and Soroa, 2007" startWordPosition="1474" endWordPosition="1477">ssonneau et al. (2012a) reports an α of 0.88 to -0.02 with the MASI statistic (Passonneau et al., 2006). Table 2 summarizes the annotation statistics for the Task’s data. The annotation process resulted in far fewer senses per instance in the trial data, which we attribute to using trained annotators. An analysis across the corpora genres showed that the multiplesense annotation rates were similar. Due to the variety of contextual sources, all lemmas were observed with at least two distinct senses. 3 Evaluation We adopt a two-part evaluation setting used in previous SemEval WSI and WSD tasks (Agirre and Soroa, 2007; Manandhar et al., 2010). The first evaluation uses a traditional WSD task that directly compares WordNet sense labels. For WSI systems, their induced sense labels are converted to WordNet 3.1 labels via a mapping procedure. The second evaluation performs a direct comparison of the two sense inventories using clustering comparisons. 3.1 WSD Task In the first evaluation, we adopt a WSD task with three objectives: (1) detecting which senses are applicable, (2) ranking senses by their applicability, and (3) measuring agreement in applicability ratings with human annotators. Each objectives uses </context>
</contexts>
<marker>Agirre, Soroa, 2007</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task 2: Evaluating word sense induction and discrimination systems. In Proceedings of the Fourth International Workshop on Semantic Evaluations, pages 7–12. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Personalizing PageRank for Word Sense Disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>33--41</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="26168" citStr="Agirre and Soroa, 2009" startWordPosition="4371" endWordPosition="4374">valuation measures for all system and selected baselines. Top system performances are marked in bold. like other teams, the Unimelb systems were trained on a Wikipedia corpus instead of the ukWaC corpus. The University of Sussex (UoS) team submitted two WSI systems that use dependency-parsed features from the corpus, which are then clustered into senses using the MaxMax algorithm (Hope and Keller, 2013); the resulting fine-grained clusters are then combined based on their degree of separability. The La Sapienza team submitted two Unsupervised WSD systems based applying Personalized Page Rank (Agirre and Soroa, 2009) over a WordNet-based network to compare the similarity of each sense with the similarity of the context, ranking each sense according to its similarity. 5 Results and Discussion Table 3 shows the main results for all instances. Additionally, we report the number of induced clusters used to label each sense as #Cl and the number of resulting WordNet 3.1 senses for each sense with #S. As in previous WSD tasks, the MFS baseline was quite competitive, outperforming all systems on detecting which senses were applicable, measured using the Jaccard Index. However, most systems were able to outperfor</context>
</contexts>
<marker>Agirre, Soroa, 2009</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2009. Personalizing PageRank for Word Sense Disambiguation. In Proceedings of EACL, pages 33–41. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Oier L´opez De Lacalle</author>
<author>Christine Fellbaum</author>
<author>Andrea Marchetti</author>
<author>Antonio Toral</author>
<author>Piek Vossen</author>
</authors>
<title>SemEval-2010 task 17: All-words word sense disambiguation on specific domains.</title>
<date>2010</date>
<booktitle>In Proceedings of SemEval-2010. ACL.</booktitle>
<marker>Agirre, De Lacalle, Fellbaum, Marchetti, Toral, Vossen, 2010</marker>
<rawString>Eneko Agirre, Oier L´opez De Lacalle, Christine Fellbaum, Andrea Marchetti, Antonio Toral, and Piek Vossen. 2010. SemEval-2010 task 17: All-words word sense disambiguation on specific domains. In Proceedings of SemEval-2010. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrique Amig´o</author>
<author>Julio Gonzalo</author>
<author>Javier Artiles</author>
<author>Felisa Verdejo</author>
</authors>
<title>A comparison of extrinsic clustering evaluation metrics based on formal constraints.</title>
<date>2009</date>
<journal>Information Retrieval,</journal>
<volume>12</volume>
<issue>4</issue>
<marker>Amig´o, Gonzalo, Artiles, Verdejo, 2009</marker>
<rawString>Enrique Amig´o, Julio Gonzalo, Javier Artiles, and Felisa Verdejo. 2009. A comparison of extrinsic clustering evaluation metrics based on formal constraints. Information Retrieval, 12(4):461–486.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Javier Artiles</author>
<author>Enrique Amig´o</author>
<author>Julio Gonzalo</author>
</authors>
<title>The role of named entities in web people search.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>534--542</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Artiles, Amig´o, Gonzalo, 2009</marker>
<rawString>Javier Artiles, Enrique Amig´o, and Julio Gonzalo. 2009. The role of named entities in web people search. In Proceedings of EMNLP, pages 534–542. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Inter-coder agreement for computational linguistics.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="7656" citStr="Artstein and Poesio, 2008" startWordPosition="1209" endWordPosition="1212">or agreement (IAA), the resulting annotations were not of high enough quality to use in the Task’s evaluations. Specifically, for some senses and contexts, AMT annotators required more information about sense distinctions than was feasible to integrate into the AMT setting, which led to consistent but incorrect sense assignments. Therefore, the test data was annotated by the two authors, with the first author annotating all instances and the second author annotating a 10% sample of each lemma’s instances in order to calculate IAA. IAA was calculated using Krippendorff’s α (Krippendorff, 1980; Artstein and Poesio, 2008), which is an agreement measurement that adjusts for chance, 291 Spoken Written Genre Face-to-face Telephone Fiction Journal Letters Non-fiction Technical Travel Guides All Instances 52 699 127 2403 103 477 611 192 4664 Tokens 1742 30,700 3438 69,479 2238 11,780 17,337 4490 141,204 Mean senses/inst. 1.17 1.08 1.15 1.13 1.31 1.10 1.11 1.11 1.12 Table 2: Test data used in Task 12, divided according to source type ranging in (−1, 1] for interval data, where 1 indicates perfect agreement and -1 indicates systematic disagreement; two random annotations have an expected α of zero. We treat each sens</context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4):555–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<booktitle>In Proceedings of the Linguistic Coreference Workshop at LREC,</booktitle>
<pages>563--566</pages>
<contexts>
<context position="17033" citStr="Bagga and Baldwin (1998)" startWordPosition="2805" endWordPosition="2808">y have partial memberships in a set as fuzzy cover. We propose two new fuzzy measures for comparing fuzzy sense assignments: Fuzzy B-Cubed and Fuzzy Normalized Mutual Information. The two measures provide complementary information. B-Cubed summarizes the performance per instance and therefore provides an estimate of how well a system would perform on a new corpus with a similar sense distribution. In contrast, Fuzzy NMI is measured based on the clusters rather than the instances, thereby providing a performance analysis that is independent of the corpus sense distribution. 3.2.1 Fuzzy B-Cubed Bagga and Baldwin (1998) proposed a clustering evaluation known as B-Cubed, which compares two partitions on a per-item basis. Amig´o et al. (2009) later extended the definition of B-Cubed to compare overlapping clusters (i.e., covers). We generalize BCubed further to handle the case of fuzzy covers. B-Cubed is based on precision and recall, which estimate the fit between two clusterings, X and Y at the item level. For an item i, precision reflects how many items sharing a cluster with i in X appear in its cluster in Y ; conversely, recall measures how many items sharing a cluster in Y with i also appear in its clust</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Amit Bagga and Breck Baldwin. 1998. Algorithms for scoring coreference chains. In Proceedings of the Linguistic Coreference Workshop at LREC, pages 563– 566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The WaCky wide web: A collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="5443" citStr="Baroni et al., 2009" startWordPosition="847" endWordPosition="850">nterpretations “I ask [you] because” (sense ask%2:32:04::) or “I ask [the question] because” (sense ask%2:32:02::). 2.1 Data Three datasets were provided with the task. The trial dataset provided weighted word sense annotations using the data gathered by Erk et al. (2009). The trial dataset consisted of 50 contexts for eight words, where each context was labeled with WordNet 3.0 sense ratings from three untrained lexicographers. Due to the unsupervised nature of the task, participants were not provided with sense-labeled training data. However, WSI systems were provided with the ukWaC corpus (Baroni et al., 2009) to use in inducing senses. Previous SemEval WSI tasks had provided participants with corpora specific to the task’s target terms; in contrast, this task opted to use a large corpus to enable WSI methods that require corpuswide statistics, e.g., statistical associations. Test data was drawn from the Open American National Corpus (Ide and Suderman, 2004, OANC) across a variety of genres and from both the spoken and written portions of the corpus, summarized in Table 2. All contexts were manually inspected to ensure that the lemma being disambiguated was of the correct part of speech and had an </context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The WaCky wide web: A collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
<author>Valerie Nygaard</author>
</authors>
<title>Crowdsourcing wordnet.</title>
<date>2010</date>
<booktitle>In The 5th International Conference of the Global WordNet Association (GWC-2010).</booktitle>
<contexts>
<context position="6499" citStr="Biemann and Nygaard, 2010" startWordPosition="1024" endWordPosition="1028">ons of the corpus, summarized in Table 2. All contexts were manually inspected to ensure that the lemma being disambiguated was of the correct part of speech and had an interpretation that matched at least one WordNet 3.1 sense. This filtering also removed instances that were in a collocation, or had an idiomatic meaning. Ultimately, 4664 contexts were used as test data, with a minimum of 22 and a maximum of 100 contexts per word. 2.2 Sense Annotation Recent work proposes to gather sense annotations using crowdsourcing in order to reduce the time and cost of acquiring sense-annotated corpora (Biemann and Nygaard, 2010; Passonneau et al., 2012b; Rumshisky et al., 2012; Jurgens, 2013). Therefore, we initially annotated the Task’s data using the method of Jurgens (2013), where workers on Amazon Mechanical Turk (AMT) rated all senses of a word on a Likert scale from one to five, indicating the sense does not apply at all or completely applies, respectively. Twenty annotators were assigned per instance, with their ratings combined by selecting the most frequent rating. However, we found that while the annotators achieved moderate inter-annotator agreement (IAA), the resulting annotations were not of high enough</context>
</contexts>
<marker>Biemann, Nygaard, 2010</marker>
<rawString>Chris Biemann and Valerie Nygaard. 2010. Crowdsourcing wordnet. In The 5th International Conference of the Global WordNet Association (GWC-2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leon Danon</author>
<author>Albert Diaz-Guilera</author>
<author>Jordi Duch</author>
<author>Alex Arenas</author>
</authors>
<title>Comparing community structure identification.</title>
<date>2005</date>
<journal>Journal of Statistical Mechanics: Theory and Experiment,</journal>
<volume>2005</volume>
<issue>09</issue>
<contexts>
<context position="19550" citStr="Danon et al., 2005" startWordPosition="3248" endWordPosition="3251">ns and covers, as defined by Amig´o et al. (2009). Item-based Precision and Recall are then defined using Equation 5 as P(i, j, X) = Min(C(i,j,X),C(i,j,Y )) and R(i, j, X) = C(i,j,X) Min(C(i,j,X),C(i,j,Y )) , respectively. These fuzzy genC(i,j,Y ) eralizations are used in Equations 3 and 4. 3.2.2 Fuzzy Normalized Mutual Information Mutual information measures the dependence between two random variables. In the context of clustering evaluation, mutual information treats the sense labels as random variables and measures the level of agreement in which instances are labeled with the same senses (Danon et al., 2005). Formally, mutual information is defined as I(X; Y ) = H(X)−(H(X|Y ) where H(X) denotes the entropy of the random variable X that represents a partition, i.e., the sets of instances assigned to each sense. Typically, mutual information is normalized to [0, 1] in order to facilitate comparisons between multiple clustering solutions on the same scale (Luo et al., 2009), with Max(H(X), H(Y )) being the recommended normalizing factor (Vinh et al., 2010). In its original formulation Mutual information is defined only to compare non-overlapping cluster partitions. Therefore, we propose a new defini</context>
</contexts>
<marker>Danon, Diaz-Guilera, Duch, Arenas, 2005</marker>
<rawString>Leon Danon, Albert Diaz-Guilera, Jordi Duch, and Alex Arenas. 2005. Comparing community structure identification. Journal of Statistical Mechanics: Theory and Experiment, 2005(09):P09008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Diana McCarthy</author>
</authors>
<title>Graded word sense assignment.</title>
<date>2009</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>440--449</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="11533" citStr="Erk and McCarthy (2009)" startWordPosition="1849" endWordPosition="1852">ense mapping function, we use the distribution mapping technique of Jurgens (2012), which takes into account the sense applicability weights in both labelings. 3.1.2 Jaccard Index Given two sets of sense labels for an instance, X and Y , the Jaccard Index is used to measure the agreement: �XnY � �XuY �. The Jaccard Index is maximized when X and Y use identical labels, and is minimized when the sets of sense labels are disjoint. 3.1.3 Positionally-Weighted Kendall’s T Rank correlations have been proposed for evaluating a system’s ability to order senses by applicability; in previous work, both Erk and McCarthy (2009) and Jurgens (2012) propose rank correlation coefficients that assume all positions in the ranking are equally important. However, in the case of graded 292 sense evaluation, often only a few senses are applicable, with the applicability ratings of the remaining senses being relatively inconsequential. Therefore, we consider an alternate rank scoring based on Kumar and Vassilvitskii (2010), which weights the penalty of reordering the lower positions less than the penalty of reordering the first ranks. Kendall’s T distance, K, is a measure of the number of item position swaps required to make t</context>
</contexts>
<marker>Erk, McCarthy, 2009</marker>
<rawString>Katrin Erk and Diana McCarthy. 2009. Graded word sense assignment. In Proceedings of Empirical Methods in Natural Language Processing (EMNLP), pages 440–449. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Diana McCarthy</author>
<author>Nicholas Gaylord</author>
</authors>
<title>Investigations on word senses and word usages.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>10--18</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="1384" citStr="Erk et al., 2009" startWordPosition="203" endWordPosition="206">ord Sense Disambiguation (WSD) attempts to identify which of a word’s meanings applies in a given context. A long-standing task, WSD is fundamental to many NLP applications (Navigli, 2009). Typically, each usage of a word is treated as expressing only a single sense. However, contextual ambiguity as well as the relatedness of certain meanings can potentially elicit multiple sense interpretations. Recent work has shown that annotators find multiple applicable senses in a given target word context when using fine-grained sense inventories such as WordNet (V´eronis, 1998; Murray and Green, 2004; Erk et al., 2009; Passonneau et al., 2012b; Jurgens, 2013; Navigli et al., 2013). Such contexts would be better annotated with multiple sense labels, weighting each sense according to its applicability (Erk et al., 2009; Jurgens, 2013), in effect allowing ambiguity or multiple interpretations to be explicitly modeled. Accordingly, the first goal of this task is to evaluate WSD systems in a setting where instances may be labeled with one or more senses, weighted by their applicability. WSD methods are ultimately defined and potentially restricted by their choice in sense inventory; for example, a sense invento</context>
<context position="5095" citStr="Erk et al. (2009)" startWordPosition="793" endWordPosition="796">ltiple senses apply. The first example shows a case of an intentional double meaning that evokes both the physical aspect of dark.a as being devoid of light and the causal result of being secret. In contrast, the second example shows a case of multiple interpretations from ambiguity; a different preceding context could generate the alternate interpretations “I ask [you] because” (sense ask%2:32:04::) or “I ask [the question] because” (sense ask%2:32:02::). 2.1 Data Three datasets were provided with the task. The trial dataset provided weighted word sense annotations using the data gathered by Erk et al. (2009). The trial dataset consisted of 50 contexts for eight words, where each context was labeled with WordNet 3.0 sense ratings from three untrained lexicographers. Due to the unsupervised nature of the task, participants were not provided with sense-labeled training data. However, WSI systems were provided with the ukWaC corpus (Baroni et al., 2009) to use in inducing senses. Previous SemEval WSI tasks had provided participants with corpora specific to the task’s target terms; in contrast, this task opted to use a large corpus to enable WSI methods that require corpuswide statistics, e.g., statis</context>
</contexts>
<marker>Erk, McCarthy, Gaylord, 2009</marker>
<rawString>Katrin Erk, Diana McCarthy, and Nicholas Gaylord. 2009. Investigations on word senses and word usages. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 10–18. ACL.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christine Fellbaum, editor.</editor>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="17033" citStr="(1998)" startWordPosition="2808" endWordPosition="2808">berships in a set as fuzzy cover. We propose two new fuzzy measures for comparing fuzzy sense assignments: Fuzzy B-Cubed and Fuzzy Normalized Mutual Information. The two measures provide complementary information. B-Cubed summarizes the performance per instance and therefore provides an estimate of how well a system would perform on a new corpus with a similar sense distribution. In contrast, Fuzzy NMI is measured based on the clusters rather than the instances, thereby providing a performance analysis that is independent of the corpus sense distribution. 3.2.1 Fuzzy B-Cubed Bagga and Baldwin (1998) proposed a clustering evaluation known as B-Cubed, which compares two partitions on a per-item basis. Amig´o et al. (2009) later extended the definition of B-Cubed to compare overlapping clusters (i.e., covers). We generalize BCubed further to handle the case of fuzzy covers. B-Cubed is based on precision and recall, which estimate the fit between two clusterings, X and Y at the item level. For an item i, precision reflects how many items sharing a cluster with i in X appear in its cluster in Y ; conversely, recall measures how many items sharing a cluster in Y with i also appear in its clust</context>
</contexts>
<marker>1998</marker>
<rawString>Christine Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Hope</author>
<author>Bill Keller</author>
</authors>
<title>MaxMax: A GraphBased Soft Clustering Algorithm Applied to Word Sense Induction.</title>
<date>2013</date>
<booktitle>In Proceedings of CICLing,</booktitle>
<pages>368--381</pages>
<contexts>
<context position="25951" citStr="Hope and Keller, 2013" startWordPosition="4337" endWordPosition="4340">ces, One sense 0.192 0.609 0.288 0.0 0.623 1.00 6.62 1c1inst 0.0 0.0 0.0 0.071 0.0 1.00 0.0 Semcor MFS 0.455 0.465 0.339 - - - 1.00 Semcor Ranked Senses 0.149 0.559 0.489 - - - 8.66 Table 3: Performance on the five evaluation measures for all system and selected baselines. Top system performances are marked in bold. like other teams, the Unimelb systems were trained on a Wikipedia corpus instead of the ukWaC corpus. The University of Sussex (UoS) team submitted two WSI systems that use dependency-parsed features from the corpus, which are then clustered into senses using the MaxMax algorithm (Hope and Keller, 2013); the resulting fine-grained clusters are then combined based on their degree of separability. The La Sapienza team submitted two Unsupervised WSD systems based applying Personalized Page Rank (Agirre and Soroa, 2009) over a WordNet-based network to compare the similarity of each sense with the similarity of the context, ranking each sense according to its similarity. 5 Results and Discussion Table 3 shows the main results for all instances. Additionally, we report the number of induced clusters used to label each sense as #Cl and the number of resulting WordNet 3.1 senses for each sense with </context>
</contexts>
<marker>Hope, Keller, 2013</marker>
<rawString>David Hope and Bill Keller. 2013. MaxMax: A GraphBased Soft Clustering Algorithm Applied to Word Sense Induction. In Proceedings of CICLing, pages 368–381.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Keith Suderman</author>
</authors>
<title>The american national corpus first release.</title>
<date>2004</date>
<booktitle>In Proceedings of the Fourth Language Resources and Evaluation Conference,</booktitle>
<pages>1681--1684</pages>
<contexts>
<context position="5797" citStr="Ide and Suderman, 2004" startWordPosition="905" endWordPosition="908"> was labeled with WordNet 3.0 sense ratings from three untrained lexicographers. Due to the unsupervised nature of the task, participants were not provided with sense-labeled training data. However, WSI systems were provided with the ukWaC corpus (Baroni et al., 2009) to use in inducing senses. Previous SemEval WSI tasks had provided participants with corpora specific to the task’s target terms; in contrast, this task opted to use a large corpus to enable WSI methods that require corpuswide statistics, e.g., statistical associations. Test data was drawn from the Open American National Corpus (Ide and Suderman, 2004, OANC) across a variety of genres and from both the spoken and written portions of the corpus, summarized in Table 2. All contexts were manually inspected to ensure that the lemma being disambiguated was of the correct part of speech and had an interpretation that matched at least one WordNet 3.1 sense. This filtering also removed instances that were in a collocation, or had an idiomatic meaning. Ultimately, 4664 contexts were used as test data, with a minimum of 22 and a maximum of 100 contexts per word. 2.2 Sense Annotation Recent work proposes to gather sense annotations using crowdsourcin</context>
</contexts>
<marker>Ide, Suderman, 2004</marker>
<rawString>Nancy Ide and Keith Suderman. 2004. The american national corpus first release. In Proceedings of the Fourth Language Resources and Evaluation Conference, pages 1681–1684.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
</authors>
<title>An Evaluation of Graded Sense Disambiguation using Word Sense Induction.</title>
<date>2012</date>
<booktitle>In Proceedings of *SEM, the First Joint Conference on Lexical and Computational Semantics. ACL.</booktitle>
<contexts>
<context position="10992" citStr="Jurgens (2012)" startWordPosition="1758" endWordPosition="1759">call. 3.1.1 Transforming Induced Sense Labels In the WSD setting, induced sense labels may be transformed into a reference inventory (e.g., WordNet 3.1) using a sense mapping procedure. We follow the 80/20 setup of Manandhar et al. (2010), where the corpus is randomly divided into five partitions, four of which are used to learn the sense mapping; the sense labels for the held-out partition are then converted and compared with the gold standard. This process is repeated so that each partition is tested once. For learning the sense mapping function, we use the distribution mapping technique of Jurgens (2012), which takes into account the sense applicability weights in both labelings. 3.1.2 Jaccard Index Given two sets of sense labels for an instance, X and Y , the Jaccard Index is used to measure the agreement: �XnY � �XuY �. The Jaccard Index is maximized when X and Y use identical labels, and is minimized when the sets of sense labels are disjoint. 3.1.3 Positionally-Weighted Kendall’s T Rank correlations have been proposed for evaluating a system’s ability to order senses by applicability; in previous work, both Erk and McCarthy (2009) and Jurgens (2012) propose rank correlation coefficients t</context>
</contexts>
<marker>Jurgens, 2012</marker>
<rawString>David Jurgens. 2012. An Evaluation of Graded Sense Disambiguation using Word Sense Induction. In Proceedings of *SEM, the First Joint Conference on Lexical and Computational Semantics. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
</authors>
<title>Embracing Ambiguity: A Comparison of Annotation Methodologies for Crowdsourcing Word Sense Labels.</title>
<date>2013</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL).</booktitle>
<publisher>ACL.</publisher>
<contexts>
<context position="1425" citStr="Jurgens, 2013" startWordPosition="211" endWordPosition="212">dentify which of a word’s meanings applies in a given context. A long-standing task, WSD is fundamental to many NLP applications (Navigli, 2009). Typically, each usage of a word is treated as expressing only a single sense. However, contextual ambiguity as well as the relatedness of certain meanings can potentially elicit multiple sense interpretations. Recent work has shown that annotators find multiple applicable senses in a given target word context when using fine-grained sense inventories such as WordNet (V´eronis, 1998; Murray and Green, 2004; Erk et al., 2009; Passonneau et al., 2012b; Jurgens, 2013; Navigli et al., 2013). Such contexts would be better annotated with multiple sense labels, weighting each sense according to its applicability (Erk et al., 2009; Jurgens, 2013), in effect allowing ambiguity or multiple interpretations to be explicitly modeled. Accordingly, the first goal of this task is to evaluate WSD systems in a setting where instances may be labeled with one or more senses, weighted by their applicability. WSD methods are ultimately defined and potentially restricted by their choice in sense inventory; for example, a sense inventory may have insufficient sense-annotated </context>
<context position="6565" citStr="Jurgens, 2013" startWordPosition="1037" endWordPosition="1038">d to ensure that the lemma being disambiguated was of the correct part of speech and had an interpretation that matched at least one WordNet 3.1 sense. This filtering also removed instances that were in a collocation, or had an idiomatic meaning. Ultimately, 4664 contexts were used as test data, with a minimum of 22 and a maximum of 100 contexts per word. 2.2 Sense Annotation Recent work proposes to gather sense annotations using crowdsourcing in order to reduce the time and cost of acquiring sense-annotated corpora (Biemann and Nygaard, 2010; Passonneau et al., 2012b; Rumshisky et al., 2012; Jurgens, 2013). Therefore, we initially annotated the Task’s data using the method of Jurgens (2013), where workers on Amazon Mechanical Turk (AMT) rated all senses of a word on a Likert scale from one to five, indicating the sense does not apply at all or completely applies, respectively. Twenty annotators were assigned per instance, with their ratings combined by selecting the most frequent rating. However, we found that while the annotators achieved moderate inter-annotator agreement (IAA), the resulting annotations were not of high enough quality to use in the Task’s evaluations. Specifically, for some </context>
</contexts>
<marker>Jurgens, 2013</marker>
<rawString>David Jurgens. 2013. Embracing Ambiguity: A Comparison of Annotation Methodologies for Crowdsourcing Word Sense Labels. In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL). ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>English lexical sample task description.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL-SIGLEX SENSEVAL-2 Workshop.</booktitle>
<contexts>
<context position="29783" citStr="Kilgarriff, 2002" startWordPosition="4956" endWordPosition="4957"> only the highest weighted sense. In this single-sense setting, systems were evaluated using the standard WSD Precision and Recall measures; we report the F1 measure of Precision and Recall. The remaining subset of instances annotated with multiple senses were evaluated separately. Table 4 shows the systems’ performance on single-sense instances, revealing substantially increased performance and improvement over the MFS baseline for WSI systems. Notably, the performance of the best sense-remapped WSI systems surpasses the performance of many supervised WSD systems in previous WSD evaluations (Kilgarriff, 2002; Mihalcea et al., 2004; Pradhan et al., 2007; Agirre et al., 2010). This performance suggests that WSI systems using graded labels provide a way to leverage huge amounts of unannotated corpus data for finding sense-related features in order to train semi-supervised WSD systems. Table 5 shows the performance on the subset of instances that were annotated with multiple senses. We note that in this setting, the mapping procedure transforms the All-Instances One Sense baseline into the average applicability rating for each sense in the test corpus. Notably, the La Sapienza systems sees a signific</context>
</contexts>
<marker>Kilgarriff, 2002</marker>
<rawString>Adam Kilgarriff. 2002. English lexical sample task description. In Proceedings of ACL-SIGLEX SENSEVAL-2 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology. Sage,</title>
<date>1980</date>
<location>Beverly Hills, CA.</location>
<contexts>
<context position="7628" citStr="Krippendorff, 1980" startWordPosition="1206" endWordPosition="1208">derate inter-annotator agreement (IAA), the resulting annotations were not of high enough quality to use in the Task’s evaluations. Specifically, for some senses and contexts, AMT annotators required more information about sense distinctions than was feasible to integrate into the AMT setting, which led to consistent but incorrect sense assignments. Therefore, the test data was annotated by the two authors, with the first author annotating all instances and the second author annotating a 10% sample of each lemma’s instances in order to calculate IAA. IAA was calculated using Krippendorff’s α (Krippendorff, 1980; Artstein and Poesio, 2008), which is an agreement measurement that adjusts for chance, 291 Spoken Written Genre Face-to-face Telephone Fiction Journal Letters Non-fiction Technical Travel Guides All Instances 52 699 127 2403 103 477 611 192 4664 Tokens 1742 30,700 3438 69,479 2238 11,780 17,337 4490 141,204 Mean senses/inst. 1.17 1.08 1.15 1.13 1.31 1.10 1.11 1.11 1.12 Table 2: Test data used in Task 12, divided according to source type ranging in (−1, 1] for interval data, where 1 indicates perfect agreement and -1 indicates systematic disagreement; two random annotations have an expected α</context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>Klaus Krippendorff. 1980. Content Analysis: An Introduction to Its Methodology. Sage, Beverly Hills, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology. Sage,</title>
<date>2004</date>
<location>Thousand Oaks, CA,</location>
<note>second edition.</note>
<contexts>
<context position="8497" citStr="Krippendorff (2004)" startWordPosition="1352" endWordPosition="1353">kens 1742 30,700 3438 69,479 2238 11,780 17,337 4490 141,204 Mean senses/inst. 1.17 1.08 1.15 1.13 1.31 1.10 1.11 1.11 1.12 Table 2: Test data used in Task 12, divided according to source type ranging in (−1, 1] for interval data, where 1 indicates perfect agreement and -1 indicates systematic disagreement; two random annotations have an expected α of zero. We treat each sense and instance combination as a separate item to rate. The total IAA for the dataset was 0.504, and on individual words, ranged from 0.903 for number.n to 0.00 for win.v. While this IAA is less than the 0.8 recommended by Krippendorff (2004), it is consistent with the IAA distribution for the sense annotations of MASC on other parts of the OANC corpus: Passonneau et al. (2012a) reports an α of 0.88 to -0.02 with the MASI statistic (Passonneau et al., 2006). Table 2 summarizes the annotation statistics for the Task’s data. The annotation process resulted in far fewer senses per instance in the trial data, which we attribute to using trained annotators. An analysis across the corpora genres showed that the multiplesense annotation rates were similar. Due to the variety of contextual sources, all lemmas were observed with at least t</context>
</contexts>
<marker>Krippendorff, 2004</marker>
<rawString>Klaus Krippendorff. 2004. Content Analysis: An Introduction to Its Methodology. Sage, Thousand Oaks, CA, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ravi Kumar</author>
<author>Sergei Vassilvitskii</author>
</authors>
<title>Generalized distances between rankings.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th International Conference on World Wide Web (WWW),</booktitle>
<pages>571--580</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="11925" citStr="Kumar and Vassilvitskii (2010)" startWordPosition="1910" endWordPosition="1913">ed when the sets of sense labels are disjoint. 3.1.3 Positionally-Weighted Kendall’s T Rank correlations have been proposed for evaluating a system’s ability to order senses by applicability; in previous work, both Erk and McCarthy (2009) and Jurgens (2012) propose rank correlation coefficients that assume all positions in the ranking are equally important. However, in the case of graded 292 sense evaluation, often only a few senses are applicable, with the applicability ratings of the remaining senses being relatively inconsequential. Therefore, we consider an alternate rank scoring based on Kumar and Vassilvitskii (2010), which weights the penalty of reordering the lower positions less than the penalty of reordering the first ranks. Kendall’s T distance, K, is a measure of the number of item position swaps required to make two sequences identical. Kumar and Vassilvitskii (2010) extend this distance definition using a variable penalty function S for the cost of swapping two positions, which we denote Kδ. By using an appropriate S, Kδ can be biased towards the correctness of higher ranks by assigning a smaller S to lower ranks. Because Kδ is a distance measure, its value range will be different depending on the</context>
</contexts>
<marker>Kumar, Vassilvitskii, 2010</marker>
<rawString>Ravi Kumar and Sergei Vassilvitskii. 2010. Generalized distances between rankings. In Proceedings of the 19th International Conference on World Wide Web (WWW), pages 571–580. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Lancichinetti</author>
<author>Santo Fortunato</author>
<author>J´anos Kert´esz</author>
</authors>
<title>Detecting the overlapping and hierarchical community structure in complex networks.</title>
<date>2009</date>
<journal>New Journal of Physics,</journal>
<volume>11</volume>
<issue>3</issue>
<marker>Lancichinetti, Fortunato, Kert´esz, 2009</marker>
<rawString>Andrea Lancichinetti, Santo Fortunato, and J´anos Kert´esz. 2009. Detecting the overlapping and hierarchical community structure in complex networks. New Journal of Physics, 11(3):033015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jey Han Lau</author>
<author>Paul Cook</author>
<author>Diana McCarthy</author>
<author>David Newman</author>
<author>Timothy Baldwin</author>
</authors>
<title>Word sense induction for novel sense detection.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for computational Linguistics (EACL</booktitle>
<contexts>
<context position="24564" citStr="Lau et al. (2012)" startWordPosition="4096" endWordPosition="4099">tories. AI-KU submitted three WSI systems based on a lexical substitution method; a language model is built from the target word’s contexts in the test data and the ukWaC corpus and then Fastsubs (Yuret, 2012) is used to identify lexical substitutes for the target. Together, the contexts of the target and substitutes are used to build a distributional model using the S-CODE algorithm (Maron et al., 2010). The resulting contextual distributions are then clustered using K-means to identify word senses. The University of Melbourne (Unimelb) team submitted two WSI systems based on the approach of Lau et al. (2012). Their systems use a Hierarchical Dirichlet Process (Teh et al., 2006) to automatically infer the number of senses from contextual and positional features. UnH(Xk, Yl) = n i=1 �m j=1 295 Team System WSD F1 Cluster Comparison #Cl #S Jac. Ind. Ksim WNDCG Fuzzy NMI Fuzzy B-Cubed � AI-KU Base 0.197 0.620 0.387 0.065 0.390 7.76 6.61 AI-KU add1000 0.197 0.606 0.215 0.035 0.320 7.76 6.61 AI-KU remove5-add1000 0.244 0.642 0.332 0.039 0.451 3.12 5.33 Unimelb 5p 0.218 0.614 0.365 0.056 0.459 2.37 5.97 Unimelb 50k 0.213 0.620 0.371 0.060 0.483 2.48 6.08 UoS #WN Senses 0.192 0.596 0.315 0.047 0.201 8.08 </context>
</contexts>
<marker>Lau, Cook, McCarthy, Newman, Baldwin, 2012</marker>
<rawString>Jey Han Lau, Paul Cook, Diana McCarthy, David Newman, and Timothy Baldwin. 2012. Word sense induction for novel sense detection. In Proceedings of the 13th Conference of the European Chapter of the Association for computational Linguistics (EACL 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ping Luo</author>
<author>Hui Xiong</author>
<author>Guoxing Zhan</author>
<author>Junjie Wu</author>
<author>Zhongzhi Shi</author>
</authors>
<title>Information-theoretic distance measures for clustering validation: Generalization and normalization.</title>
<date>2009</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>21</volume>
<issue>9</issue>
<contexts>
<context position="19920" citStr="Luo et al., 2009" startWordPosition="3310" endWordPosition="3313">ence between two random variables. In the context of clustering evaluation, mutual information treats the sense labels as random variables and measures the level of agreement in which instances are labeled with the same senses (Danon et al., 2005). Formally, mutual information is defined as I(X; Y ) = H(X)−(H(X|Y ) where H(X) denotes the entropy of the random variable X that represents a partition, i.e., the sets of instances assigned to each sense. Typically, mutual information is normalized to [0, 1] in order to facilitate comparisons between multiple clustering solutions on the same scale (Luo et al., 2009), with Max(H(X), H(Y )) being the recommended normalizing factor (Vinh et al., 2010). In its original formulation Mutual information is defined only to compare non-overlapping cluster partitions. Therefore, we propose a new definition of mutual information between fuzzy covers using extension of Lancichinetti et al. (2009) for calculating the normalized mutual information between covers. In the case of partitions, a clustering is represented as a discrete random variable whose states denote the probability of being assigned to each cluster. In 294 the fuzzy cover setting, each item may be assi</context>
</contexts>
<marker>Luo, Xiong, Zhan, Wu, Shi, 2009</marker>
<rawString>Ping Luo, Hui Xiong, Guoxing Zhan, Junjie Wu, and Zhongzhi Shi. 2009. Information-theoretic distance measures for clustering validation: Generalization and normalization. IEEE Transactions on Knowledge and Data Engineering, 21(9):1249–1262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suresh Manandhar</author>
<author>Ioannis P Klapaftis</author>
<author>Dmitriy Dligach</author>
<author>Sameer S Pradhan</author>
</authors>
<title>SemEval-2010 task 14: Word sense induction &amp; disambiguation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>63--68</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="9260" citStr="Manandhar et al., 2010" startWordPosition="1478" endWordPosition="1481">reports an α of 0.88 to -0.02 with the MASI statistic (Passonneau et al., 2006). Table 2 summarizes the annotation statistics for the Task’s data. The annotation process resulted in far fewer senses per instance in the trial data, which we attribute to using trained annotators. An analysis across the corpora genres showed that the multiplesense annotation rates were similar. Due to the variety of contextual sources, all lemmas were observed with at least two distinct senses. 3 Evaluation We adopt a two-part evaluation setting used in previous SemEval WSI and WSD tasks (Agirre and Soroa, 2007; Manandhar et al., 2010). The first evaluation uses a traditional WSD task that directly compares WordNet sense labels. For WSI systems, their induced sense labels are converted to WordNet 3.1 labels via a mapping procedure. The second evaluation performs a direct comparison of the two sense inventories using clustering comparisons. 3.1 WSD Task In the first evaluation, we adopt a WSD task with three objectives: (1) detecting which senses are applicable, (2) ranking senses by their applicability, and (3) measuring agreement in applicability ratings with human annotators. Each objectives uses a specific measurement: (</context>
<context position="10616" citStr="Manandhar et al. (2010)" startWordPosition="1692" endWordPosition="1695">e Gain, respectively. Each measure is bounded in [0, 1], where 1 indicates complete agreement with the gold standard. We generalize the traditional definition of WSD Recall such that it measures the average score for each measure across all instances, including those not labeled by the system. Systems are ultimately scored using the F1 measure between each objective’s measure and Recall. 3.1.1 Transforming Induced Sense Labels In the WSD setting, induced sense labels may be transformed into a reference inventory (e.g., WordNet 3.1) using a sense mapping procedure. We follow the 80/20 setup of Manandhar et al. (2010), where the corpus is randomly divided into five partitions, four of which are used to learn the sense mapping; the sense labels for the held-out partition are then converted and compared with the gold standard. This process is repeated so that each partition is tested once. For learning the sense mapping function, we use the distribution mapping technique of Jurgens (2012), which takes into account the sense applicability weights in both labelings. 3.1.2 Jaccard Index Given two sets of sense labels for an instance, X and Y , the Jaccard Index is used to measure the agreement: �XnY � �XuY �. T</context>
</contexts>
<marker>Manandhar, Klapaftis, Dligach, Pradhan, 2010</marker>
<rawString>Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dligach, and Sameer S. Pradhan. 2010. SemEval-2010 task 14: Word sense induction &amp; disambiguation. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 63–68. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yariv Maron</author>
<author>Michael Lamar</author>
<author>Elie Bienenstock</author>
</authors>
<title>Sphere embedding: An application to part-ofspeech induction.</title>
<date>2010</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="24354" citStr="Maron et al., 2010" startWordPosition="4062" endWordPosition="4065">me induced sense. The first two baselines directly use WordNet 3.1 senses, while the last two use induced senses. 4 Participating Systems Four teams submitted nine systems, seven of which used induced sense inventories. AI-KU submitted three WSI systems based on a lexical substitution method; a language model is built from the target word’s contexts in the test data and the ukWaC corpus and then Fastsubs (Yuret, 2012) is used to identify lexical substitutes for the target. Together, the contexts of the target and substitutes are used to build a distributional model using the S-CODE algorithm (Maron et al., 2010). The resulting contextual distributions are then clustered using K-means to identify word senses. The University of Melbourne (Unimelb) team submitted two WSI systems based on the approach of Lau et al. (2012). Their systems use a Hierarchical Dirichlet Process (Teh et al., 2006) to automatically infer the number of senses from contextual and positional features. UnH(Xk, Yl) = n i=1 �m j=1 295 Team System WSD F1 Cluster Comparison #Cl #S Jac. Ind. Ksim WNDCG Fuzzy NMI Fuzzy B-Cubed � AI-KU Base 0.197 0.620 0.387 0.065 0.390 7.76 6.61 AI-KU add1000 0.197 0.606 0.215 0.035 0.320 7.76 6.61 AI-KU</context>
</contexts>
<marker>Maron, Lamar, Bienenstock, 2010</marker>
<rawString>Yariv Maron, Michael Lamar, and Elie Bienenstock. 2010. Sphere embedding: An application to part-ofspeech induction. In Proceedings of Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron F McDaid</author>
<author>Derek Greene</author>
<author>Neil Hurley</author>
</authors>
<title>Normalized mutual information to evaluate overlapping community finding algorithms.</title>
<date>2011</date>
<pages>1110--2515</pages>
<contexts>
<context position="22908" citStr="McDaid et al. (2011)" startWordPosition="3816" endWordPosition="3819"> as follows. n H(Xi) = p(wi)log2p(wi) (6) i=1 where p(wi) is the probability of an instance being labeled with rating wi Similarly, we may define the joint entropy of two fuzzy clusters as p(wi, wj)log2p(wi, wj) (7) where p(wi, wj) is the probability of an instance being labeled with rating wi in cluster Xk and wj in cluster Yl, and m denotes the number of bins for Yl. The conditional entropy between two clusters may then be calculated as H(XkJYl) = H(Xk,Yl) − H(Yl). Together, Equations 6 and 7 may be used to define I(X, Y ) as in the original definition. We then normalize using the method of McDaid et al. (2011). Based on the limited range of fuzzy memberships in [0, 1], we selected uniformly distributed bins in [0, 1] at 0.1 intervals when discretizing the membership weights for sense labelings. 3.3 Baselines Task 12 included multiple baselines based on modeling different types of WSI and WSD systems. Due to space constraints, we include only the four most descriptive here: (1) Semcor MFS which labels each instance with the most frequent sense of that lemma in SemCor, (2) Semcor Ranked Senses baseline, which labels each instance with all of the target lemma’s senses, ranked according to their freque</context>
</contexts>
<marker>McDaid, Greene, Hurley, 2011</marker>
<rawString>Aaron F. McDaid, Derek Greene, and Neil Hurley. 2011. Normalized mutual information to evaluate overlapping community finding algorithms. arXiv:1110.2515.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Timothy Chklovski</author>
<author>Adam Kilgarriff</author>
</authors>
<title>The Senseval-3 English lexical sample task. In</title>
<date>2004</date>
<booktitle>Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>25--28</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="29806" citStr="Mihalcea et al., 2004" startWordPosition="4958" endWordPosition="4961">weighted sense. In this single-sense setting, systems were evaluated using the standard WSD Precision and Recall measures; we report the F1 measure of Precision and Recall. The remaining subset of instances annotated with multiple senses were evaluated separately. Table 4 shows the systems’ performance on single-sense instances, revealing substantially increased performance and improvement over the MFS baseline for WSI systems. Notably, the performance of the best sense-remapped WSI systems surpasses the performance of many supervised WSD systems in previous WSD evaluations (Kilgarriff, 2002; Mihalcea et al., 2004; Pradhan et al., 2007; Agirre et al., 2010). This performance suggests that WSI systems using graded labels provide a way to leverage huge amounts of unannotated corpus data for finding sense-related features in order to train semi-supervised WSD systems. Table 5 shows the performance on the subset of instances that were annotated with multiple senses. We note that in this setting, the mapping procedure transforms the All-Instances One Sense baseline into the average applicability rating for each sense in the test corpus. Notably, the La Sapienza systems sees a significant performance increas</context>
</contexts>
<marker>Mihalcea, Chklovski, Kilgarriff, 2004</marker>
<rawString>Rada Mihalcea, Timothy Chklovski, and Adam Kilgarriff. 2004. The Senseval-3 English lexical sample task. In Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pages 25–28. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Moffat</author>
<author>Justin Zobel</author>
</authors>
<title>Rank-biased precision for measurement of retrieval effectiveness.</title>
<date>2008</date>
<journal>ACM Transactions on Information Systems (TOIS),</journal>
<volume>27</volume>
<issue>1</issue>
<contexts>
<context position="13746" citStr="Moffat and Zobel (2008)" startWordPosition="2243" endWordPosition="2246">ined, Ksim δ does not account for ties. Therefore, we arbitrarily break ties in a deterministic fashion for both rankings. Second, we define S to assign higher cost to the first ranks: the cost to move an item into position i, Si, is defined as n−(i+1), where n is the number of senses. n 3.1.4 Weighted NDCG To compare the applicability ratings for sense annotations, we recast the annotation process in an Information Retrieval setting: Given an example context acting as a query over a word’s senses, the task is to retrieve all applicable senses, ranking and scoring them by their applicability. Moffat and Zobel (2008) propose using Discounted Cumulative Gain (DCG) as a method to compare a ranking against a baseline. Given (1) a gold standard weighting of the k senses applicable to a context, where wi denotes the applicability for sense i in the gold standard, and (2) a ranking of the k senses by some method, the DCG may be calculated as k 2wi+1—1 DCG is Y �i=1 log2(i+1) commonly normalized to [0, 1] so that the value is comparable when computed on rankings with different k and weight values. To normalize, the maximum value is calculated by first computing the DCG on the ranking when the k items are sorted </context>
</contexts>
<marker>Moffat, Zobel, 2008</marker>
<rawString>Alistair Moffat and Justin Zobel. 2008. Rank-biased precision for measurement of retrieval effectiveness. ACM Transactions on Information Systems (TOIS), 27(1):2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Craig Murray</author>
<author>Rebecca Green</author>
</authors>
<title>Lexical knowledge and human disagreement on a wsd task.</title>
<date>2004</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>18</volume>
<issue>3</issue>
<contexts>
<context position="1366" citStr="Murray and Green, 2004" startWordPosition="199" endWordPosition="202">ttings. 1 Introduction Word Sense Disambiguation (WSD) attempts to identify which of a word’s meanings applies in a given context. A long-standing task, WSD is fundamental to many NLP applications (Navigli, 2009). Typically, each usage of a word is treated as expressing only a single sense. However, contextual ambiguity as well as the relatedness of certain meanings can potentially elicit multiple sense interpretations. Recent work has shown that annotators find multiple applicable senses in a given target word context when using fine-grained sense inventories such as WordNet (V´eronis, 1998; Murray and Green, 2004; Erk et al., 2009; Passonneau et al., 2012b; Jurgens, 2013; Navigli et al., 2013). Such contexts would be better annotated with multiple sense labels, weighting each sense according to its applicability (Erk et al., 2009; Jurgens, 2013), in effect allowing ambiguity or multiple interpretations to be explicitly modeled. Accordingly, the first goal of this task is to evaluate WSD systems in a setting where instances may be labeled with one or more senses, weighted by their applicability. WSD methods are ultimately defined and potentially restricted by their choice in sense inventory; for exampl</context>
</contexts>
<marker>Murray, Green, 2004</marker>
<rawString>G. Craig Murray and Rebecca Green. 2004. Lexical knowledge and human disagreement on a wsd task. Computer Speech &amp; Language, 18(3):209–222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>David Jurgens</author>
<author>Daniele Vanilla</author>
</authors>
<title>Semeval-2013 task 12: Multilingual word sense disambiguation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="1448" citStr="Navigli et al., 2013" startWordPosition="213" endWordPosition="216">f a word’s meanings applies in a given context. A long-standing task, WSD is fundamental to many NLP applications (Navigli, 2009). Typically, each usage of a word is treated as expressing only a single sense. However, contextual ambiguity as well as the relatedness of certain meanings can potentially elicit multiple sense interpretations. Recent work has shown that annotators find multiple applicable senses in a given target word context when using fine-grained sense inventories such as WordNet (V´eronis, 1998; Murray and Green, 2004; Erk et al., 2009; Passonneau et al., 2012b; Jurgens, 2013; Navigli et al., 2013). Such contexts would be better annotated with multiple sense labels, weighting each sense according to its applicability (Erk et al., 2009; Jurgens, 2013), in effect allowing ambiguity or multiple interpretations to be explicitly modeled. Accordingly, the first goal of this task is to evaluate WSD systems in a setting where instances may be labeled with one or more senses, weighted by their applicability. WSD methods are ultimately defined and potentially restricted by their choice in sense inventory; for example, a sense inventory may have insufficient sense-annotated data to build WSD syste</context>
</contexts>
<marker>Navigli, Jurgens, Vanilla, 2013</marker>
<rawString>Roberto Navigli, David Jurgens, and Daniele Vanilla. 2013. Semeval-2013 task 12: Multilingual word sense disambiguation. In Proceedings of the 7th International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word Sense Disambiguation: A Survey.</title>
<date>2009</date>
<journal>ACM Computing Surveys,</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="956" citStr="Navigli, 2009" startWordPosition="137" endWordPosition="138">eled with a single sense. However, contextual ambiguity or fine-grained senses can potentially enable multiple sense interpretations of a usage. We present a new SemEval task for evaluating Word Sense Induction and Disambiguation systems in a setting where instances may be labeled with multiple senses, weighted by their applicability. Four teams submitted nine systems, which were evaluated in two settings. 1 Introduction Word Sense Disambiguation (WSD) attempts to identify which of a word’s meanings applies in a given context. A long-standing task, WSD is fundamental to many NLP applications (Navigli, 2009). Typically, each usage of a word is treated as expressing only a single sense. However, contextual ambiguity as well as the relatedness of certain meanings can potentially elicit multiple sense interpretations. Recent work has shown that annotators find multiple applicable senses in a given target word context when using fine-grained sense inventories such as WordNet (V´eronis, 1998; Murray and Green, 2004; Erk et al., 2009; Passonneau et al., 2012b; Jurgens, 2013; Navigli et al., 2013). Such contexts would be better annotated with multiple sense labels, weighting each sense according to its </context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Roberto Navigli. 2009. Word Sense Disambiguation: A Survey. ACM Computing Surveys, 41(2):1–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Passonneau</author>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>Inter-annotator agreement on a multilingual semantic annotation task.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>1951--1956</pages>
<contexts>
<context position="8716" citStr="Passonneau et al., 2006" startWordPosition="1389" endWordPosition="1392">r interval data, where 1 indicates perfect agreement and -1 indicates systematic disagreement; two random annotations have an expected α of zero. We treat each sense and instance combination as a separate item to rate. The total IAA for the dataset was 0.504, and on individual words, ranged from 0.903 for number.n to 0.00 for win.v. While this IAA is less than the 0.8 recommended by Krippendorff (2004), it is consistent with the IAA distribution for the sense annotations of MASC on other parts of the OANC corpus: Passonneau et al. (2012a) reports an α of 0.88 to -0.02 with the MASI statistic (Passonneau et al., 2006). Table 2 summarizes the annotation statistics for the Task’s data. The annotation process resulted in far fewer senses per instance in the trial data, which we attribute to using trained annotators. An analysis across the corpora genres showed that the multiplesense annotation rates were similar. Due to the variety of contextual sources, all lemmas were observed with at least two distinct senses. 3 Evaluation We adopt a two-part evaluation setting used in previous SemEval WSI and WSD tasks (Agirre and Soroa, 2007; Manandhar et al., 2010). The first evaluation uses a traditional WSD task that </context>
</contexts>
<marker>Passonneau, Habash, Rambow, 2006</marker>
<rawString>Rebecca Passonneau, Nizar Habash, and Owen Rambow. 2006. Inter-annotator agreement on a multilingual semantic annotation task. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC), pages 1951–1956.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Collin Baker</author>
<author>Christiane Fellbaum</author>
<author>Nancy Ide</author>
</authors>
<title>The MASC word sense sentence corpus.</title>
<date>2012</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="1409" citStr="Passonneau et al., 2012" startWordPosition="207" endWordPosition="210">uation (WSD) attempts to identify which of a word’s meanings applies in a given context. A long-standing task, WSD is fundamental to many NLP applications (Navigli, 2009). Typically, each usage of a word is treated as expressing only a single sense. However, contextual ambiguity as well as the relatedness of certain meanings can potentially elicit multiple sense interpretations. Recent work has shown that annotators find multiple applicable senses in a given target word context when using fine-grained sense inventories such as WordNet (V´eronis, 1998; Murray and Green, 2004; Erk et al., 2009; Passonneau et al., 2012b; Jurgens, 2013; Navigli et al., 2013). Such contexts would be better annotated with multiple sense labels, weighting each sense according to its applicability (Erk et al., 2009; Jurgens, 2013), in effect allowing ambiguity or multiple interpretations to be explicitly modeled. Accordingly, the first goal of this task is to evaluate WSD systems in a setting where instances may be labeled with one or more senses, weighted by their applicability. WSD methods are ultimately defined and potentially restricted by their choice in sense inventory; for example, a sense inventory may have insufficient </context>
<context position="6524" citStr="Passonneau et al., 2012" startWordPosition="1029" endWordPosition="1032">ed in Table 2. All contexts were manually inspected to ensure that the lemma being disambiguated was of the correct part of speech and had an interpretation that matched at least one WordNet 3.1 sense. This filtering also removed instances that were in a collocation, or had an idiomatic meaning. Ultimately, 4664 contexts were used as test data, with a minimum of 22 and a maximum of 100 contexts per word. 2.2 Sense Annotation Recent work proposes to gather sense annotations using crowdsourcing in order to reduce the time and cost of acquiring sense-annotated corpora (Biemann and Nygaard, 2010; Passonneau et al., 2012b; Rumshisky et al., 2012; Jurgens, 2013). Therefore, we initially annotated the Task’s data using the method of Jurgens (2013), where workers on Amazon Mechanical Turk (AMT) rated all senses of a word on a Likert scale from one to five, indicating the sense does not apply at all or completely applies, respectively. Twenty annotators were assigned per instance, with their ratings combined by selecting the most frequent rating. However, we found that while the annotators achieved moderate inter-annotator agreement (IAA), the resulting annotations were not of high enough quality to use in the Ta</context>
<context position="8634" citStr="Passonneau et al. (2012" startWordPosition="1374" endWordPosition="1377">Test data used in Task 12, divided according to source type ranging in (−1, 1] for interval data, where 1 indicates perfect agreement and -1 indicates systematic disagreement; two random annotations have an expected α of zero. We treat each sense and instance combination as a separate item to rate. The total IAA for the dataset was 0.504, and on individual words, ranged from 0.903 for number.n to 0.00 for win.v. While this IAA is less than the 0.8 recommended by Krippendorff (2004), it is consistent with the IAA distribution for the sense annotations of MASC on other parts of the OANC corpus: Passonneau et al. (2012a) reports an α of 0.88 to -0.02 with the MASI statistic (Passonneau et al., 2006). Table 2 summarizes the annotation statistics for the Task’s data. The annotation process resulted in far fewer senses per instance in the trial data, which we attribute to using trained annotators. An analysis across the corpora genres showed that the multiplesense annotation rates were similar. Due to the variety of contextual sources, all lemmas were observed with at least two distinct senses. 3 Evaluation We adopt a two-part evaluation setting used in previous SemEval WSI and WSD tasks (Agirre and Soroa, 200</context>
</contexts>
<marker>Passonneau, Baker, Fellbaum, Ide, 2012</marker>
<rawString>Rebecca J Passonneau, Collin Baker, Christiane Fellbaum, and Nancy Ide. 2012a. The MASC word sense sentence corpus. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Vikas Bhardwaj</author>
<author>Ansaf SallebAouissi</author>
<author>Nancy Ide</author>
</authors>
<title>Multiplicity and word sense: evaluating and learning from multiply labeled word sense annotations. Language Resources and Evaluation,</title>
<date>2012</date>
<pages>1--34</pages>
<contexts>
<context position="1409" citStr="Passonneau et al., 2012" startWordPosition="207" endWordPosition="210">uation (WSD) attempts to identify which of a word’s meanings applies in a given context. A long-standing task, WSD is fundamental to many NLP applications (Navigli, 2009). Typically, each usage of a word is treated as expressing only a single sense. However, contextual ambiguity as well as the relatedness of certain meanings can potentially elicit multiple sense interpretations. Recent work has shown that annotators find multiple applicable senses in a given target word context when using fine-grained sense inventories such as WordNet (V´eronis, 1998; Murray and Green, 2004; Erk et al., 2009; Passonneau et al., 2012b; Jurgens, 2013; Navigli et al., 2013). Such contexts would be better annotated with multiple sense labels, weighting each sense according to its applicability (Erk et al., 2009; Jurgens, 2013), in effect allowing ambiguity or multiple interpretations to be explicitly modeled. Accordingly, the first goal of this task is to evaluate WSD systems in a setting where instances may be labeled with one or more senses, weighted by their applicability. WSD methods are ultimately defined and potentially restricted by their choice in sense inventory; for example, a sense inventory may have insufficient </context>
<context position="6524" citStr="Passonneau et al., 2012" startWordPosition="1029" endWordPosition="1032">ed in Table 2. All contexts were manually inspected to ensure that the lemma being disambiguated was of the correct part of speech and had an interpretation that matched at least one WordNet 3.1 sense. This filtering also removed instances that were in a collocation, or had an idiomatic meaning. Ultimately, 4664 contexts were used as test data, with a minimum of 22 and a maximum of 100 contexts per word. 2.2 Sense Annotation Recent work proposes to gather sense annotations using crowdsourcing in order to reduce the time and cost of acquiring sense-annotated corpora (Biemann and Nygaard, 2010; Passonneau et al., 2012b; Rumshisky et al., 2012; Jurgens, 2013). Therefore, we initially annotated the Task’s data using the method of Jurgens (2013), where workers on Amazon Mechanical Turk (AMT) rated all senses of a word on a Likert scale from one to five, indicating the sense does not apply at all or completely applies, respectively. Twenty annotators were assigned per instance, with their ratings combined by selecting the most frequent rating. However, we found that while the annotators achieved moderate inter-annotator agreement (IAA), the resulting annotations were not of high enough quality to use in the Ta</context>
<context position="8634" citStr="Passonneau et al. (2012" startWordPosition="1374" endWordPosition="1377">Test data used in Task 12, divided according to source type ranging in (−1, 1] for interval data, where 1 indicates perfect agreement and -1 indicates systematic disagreement; two random annotations have an expected α of zero. We treat each sense and instance combination as a separate item to rate. The total IAA for the dataset was 0.504, and on individual words, ranged from 0.903 for number.n to 0.00 for win.v. While this IAA is less than the 0.8 recommended by Krippendorff (2004), it is consistent with the IAA distribution for the sense annotations of MASC on other parts of the OANC corpus: Passonneau et al. (2012a) reports an α of 0.88 to -0.02 with the MASI statistic (Passonneau et al., 2006). Table 2 summarizes the annotation statistics for the Task’s data. The annotation process resulted in far fewer senses per instance in the trial data, which we attribute to using trained annotators. An analysis across the corpora genres showed that the multiplesense annotation rates were similar. Due to the variety of contextual sources, all lemmas were observed with at least two distinct senses. 3 Evaluation We adopt a two-part evaluation setting used in previous SemEval WSI and WSD tasks (Agirre and Soroa, 200</context>
</contexts>
<marker>Passonneau, Bhardwaj, SallebAouissi, Ide, 2012</marker>
<rawString>Rebecca J. Passonneau, Vikas Bhardwaj, Ansaf SallebAouissi, and Nancy Ide. 2012b. Multiplicity and word sense: evaluating and learning from multiply labeled word sense annotations. Language Resources and Evaluation, pages 1–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer S Pradhan</author>
<author>Edward Loper</author>
<author>Dmitriy Dligach</author>
<author>Martha Palmer</author>
</authors>
<title>SemEval-2007 task 17: English lexical sample, SRL, and all-words.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations. ACL.</booktitle>
<contexts>
<context position="29828" citStr="Pradhan et al., 2007" startWordPosition="4962" endWordPosition="4965"> single-sense setting, systems were evaluated using the standard WSD Precision and Recall measures; we report the F1 measure of Precision and Recall. The remaining subset of instances annotated with multiple senses were evaluated separately. Table 4 shows the systems’ performance on single-sense instances, revealing substantially increased performance and improvement over the MFS baseline for WSI systems. Notably, the performance of the best sense-remapped WSI systems surpasses the performance of many supervised WSD systems in previous WSD evaluations (Kilgarriff, 2002; Mihalcea et al., 2004; Pradhan et al., 2007; Agirre et al., 2010). This performance suggests that WSI systems using graded labels provide a way to leverage huge amounts of unannotated corpus data for finding sense-related features in order to train semi-supervised WSD systems. Table 5 shows the performance on the subset of instances that were annotated with multiple senses. We note that in this setting, the mapping procedure transforms the All-Instances One Sense baseline into the average applicability rating for each sense in the test corpus. Notably, the La Sapienza systems sees a significant performance increase in this setting; the</context>
</contexts>
<marker>Pradhan, Loper, Dligach, Palmer, 2007</marker>
<rawString>Sameer S. Pradhan, Edward Loper, Dmitriy Dligach, and Martha Palmer. 2007. SemEval-2007 task 17: English lexical sample, SRL, and all-words. In Proceedings of the 4th International Workshop on Semantic Evaluations. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Rosenberg</author>
<author>Julia Hirschberg</author>
</authors>
<title>Vmeasure: A conditional entropy-based external cluster evaluation measure.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL). ACL.</booktitle>
<contexts>
<context position="15743" citStr="Rosenberg and Hirschberg, 2007" startWordPosition="2590" endWordPosition="2593">G. We refer to this final normalized measure as the Weighted Normalized Discounted Cumulative Gain (WNDCG). 3.2 Sense Cluster Comparisons Sense induction can be viewed as an unsupervised clustering task where usages of a word are grouped into clusters, each representing uses of the same meaning. In previous SemEval tasks on sense induction, instances were labeled with a single sense, which yields a partition over the instances into disjoint sets. The proposed partition can then be compared with a gold-standard partition using many existing clustering comparison methods, such as the V-Measure (Rosenberg and Hirschberg, 2007) or paired FScore (Artiles et al., 2009). Such cluster comparison methods measure the degree of similarity between the sense boundaries created by lexicographers and those created by WSI methods. In the present task, instances are potentially labeled both with multiple senses and with weights k WDCG = i=1 � (2) 293 reflecting the applicability. This type of sense labeling produces a fuzzy clustering: An instance may belong to one or more sense clusters with its cluster membership relative to its weight for that sense. Formally, we refer to (1) a solution where the sets of instances overlap as </context>
</contexts>
<marker>Rosenberg, Hirschberg, 2007</marker>
<rawString>Andrew Rosenberg and Julia Hirschberg. 2007. Vmeasure: A conditional entropy-based external cluster evaluation measure. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL). ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Rumshisky</author>
<author>Nick Botchan</author>
<author>Sophie Kushkuley</author>
<author>James Pustejovsky</author>
</authors>
<title>Word sense inventories by non-experts.</title>
<date>2012</date>
<booktitle>In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context position="6549" citStr="Rumshisky et al., 2012" startWordPosition="1033" endWordPosition="1036">s were manually inspected to ensure that the lemma being disambiguated was of the correct part of speech and had an interpretation that matched at least one WordNet 3.1 sense. This filtering also removed instances that were in a collocation, or had an idiomatic meaning. Ultimately, 4664 contexts were used as test data, with a minimum of 22 and a maximum of 100 contexts per word. 2.2 Sense Annotation Recent work proposes to gather sense annotations using crowdsourcing in order to reduce the time and cost of acquiring sense-annotated corpora (Biemann and Nygaard, 2010; Passonneau et al., 2012b; Rumshisky et al., 2012; Jurgens, 2013). Therefore, we initially annotated the Task’s data using the method of Jurgens (2013), where workers on Amazon Mechanical Turk (AMT) rated all senses of a word on a Likert scale from one to five, indicating the sense does not apply at all or completely applies, respectively. Twenty annotators were assigned per instance, with their ratings combined by selecting the most frequent rating. However, we found that while the annotators achieved moderate inter-annotator agreement (IAA), the resulting annotations were not of high enough quality to use in the Task’s evaluations. Specifi</context>
</contexts>
<marker>Rumshisky, Botchan, Kushkuley, Pustejovsky, 2012</marker>
<rawString>Anna Rumshisky, Nick Botchan, Sophie Kushkuley, and James Pustejovsky. 2012. Word sense inventories by non-experts. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Michael I Jordan</author>
<author>Matthew J Beal</author>
<author>David M Blei</author>
</authors>
<title>Hierarchical dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>101</volume>
<issue>476</issue>
<contexts>
<context position="24635" citStr="Teh et al., 2006" startWordPosition="4107" endWordPosition="4110">on method; a language model is built from the target word’s contexts in the test data and the ukWaC corpus and then Fastsubs (Yuret, 2012) is used to identify lexical substitutes for the target. Together, the contexts of the target and substitutes are used to build a distributional model using the S-CODE algorithm (Maron et al., 2010). The resulting contextual distributions are then clustered using K-means to identify word senses. The University of Melbourne (Unimelb) team submitted two WSI systems based on the approach of Lau et al. (2012). Their systems use a Hierarchical Dirichlet Process (Teh et al., 2006) to automatically infer the number of senses from contextual and positional features. UnH(Xk, Yl) = n i=1 �m j=1 295 Team System WSD F1 Cluster Comparison #Cl #S Jac. Ind. Ksim WNDCG Fuzzy NMI Fuzzy B-Cubed � AI-KU Base 0.197 0.620 0.387 0.065 0.390 7.76 6.61 AI-KU add1000 0.197 0.606 0.215 0.035 0.320 7.76 6.61 AI-KU remove5-add1000 0.244 0.642 0.332 0.039 0.451 3.12 5.33 Unimelb 5p 0.218 0.614 0.365 0.056 0.459 2.37 5.97 Unimelb 50k 0.213 0.620 0.371 0.060 0.483 2.48 6.08 UoS #WN Senses 0.192 0.596 0.315 0.047 0.201 8.08 6.77 UoS top-3 0.232 0.625 0.374 0.045 0.448 3.00 5.44 La Sapienza syst</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and David M. Blei. 2006. Hierarchical dirichlet processes. Journal of the American Statistical Association, 101(476):1566–1581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean V´eronis</author>
</authors>
<title>A study of polysemy judgments and inter-annotator agreement.</title>
<date>1998</date>
<booktitle>In Program and advanced papers of the Senseval workshop.</booktitle>
<marker>V´eronis, 1998</marker>
<rawString>Jean V´eronis. 1998. A study of polysemy judgments and inter-annotator agreement. In Program and advanced papers of the Senseval workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nguyen Xuan Vinh</author>
<author>Julien Epps</author>
<author>James Bailey</author>
</authors>
<title>Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance.</title>
<date>2010</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>11--2837</pages>
<contexts>
<context position="20004" citStr="Vinh et al., 2010" startWordPosition="3324" endWordPosition="3327">information treats the sense labels as random variables and measures the level of agreement in which instances are labeled with the same senses (Danon et al., 2005). Formally, mutual information is defined as I(X; Y ) = H(X)−(H(X|Y ) where H(X) denotes the entropy of the random variable X that represents a partition, i.e., the sets of instances assigned to each sense. Typically, mutual information is normalized to [0, 1] in order to facilitate comparisons between multiple clustering solutions on the same scale (Luo et al., 2009), with Max(H(X), H(Y )) being the recommended normalizing factor (Vinh et al., 2010). In its original formulation Mutual information is defined only to compare non-overlapping cluster partitions. Therefore, we propose a new definition of mutual information between fuzzy covers using extension of Lancichinetti et al. (2009) for calculating the normalized mutual information between covers. In the case of partitions, a clustering is represented as a discrete random variable whose states denote the probability of being assigned to each cluster. In 294 the fuzzy cover setting, each item may be assigned to multiple clusters and no longer has a binary assignment to a cluster, but ta</context>
</contexts>
<marker>Vinh, Epps, Bailey, 2010</marker>
<rawString>Nguyen Xuan Vinh, Julien Epps, and James Bailey. 2010. Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance. The Journal of Machine Learning Research, 11:2837–2854.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
</authors>
<title>FASTSUBS: An Efcient Admissible Algorithm for Finding the Most Likely Lexical Substitutes Using a Statistical Language Model. Computing Research Repository (CoRR).</title>
<date>2012</date>
<contexts>
<context position="24156" citStr="Yuret, 2012" startWordPosition="4030" endWordPosition="4031"> , where n is the number of senses and i is the rank, (3) 1c1inst which labels each instance with its own induced sense and (4) All-instances, One sense which labels all instances with the same induced sense. The first two baselines directly use WordNet 3.1 senses, while the last two use induced senses. 4 Participating Systems Four teams submitted nine systems, seven of which used induced sense inventories. AI-KU submitted three WSI systems based on a lexical substitution method; a language model is built from the target word’s contexts in the test data and the ukWaC corpus and then Fastsubs (Yuret, 2012) is used to identify lexical substitutes for the target. Together, the contexts of the target and substitutes are used to build a distributional model using the S-CODE algorithm (Maron et al., 2010). The resulting contextual distributions are then clustered using K-means to identify word senses. The University of Melbourne (Unimelb) team submitted two WSI systems based on the approach of Lau et al. (2012). Their systems use a Hierarchical Dirichlet Process (Teh et al., 2006) to automatically infer the number of senses from contextual and positional features. UnH(Xk, Yl) = n i=1 �m j=1 295 Team</context>
</contexts>
<marker>Yuret, 2012</marker>
<rawString>Deniz Yuret. 2012. FASTSUBS: An Efcient Admissible Algorithm for Finding the Most Likely Lexical Substitutes Using a Statistical Language Model. Computing Research Repository (CoRR).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>