<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001409">
<title confidence="0.9954255">
MindNet: acquiring and structuring semantic
information from text
</title>
<author confidence="0.988453">
Stephen D. Richardson, William B. Dolan, Lucy Vanderwende
</author>
<affiliation confidence="0.891308">
Microsoft Research
</affiliation>
<address confidence="0.7363985">
One Microsoft Way
Redmond, WA 98052
</address>
<email confidence="0.352174">
U.S.A.
</email>
<sectionHeader confidence="0.937" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999764714285714">
As a lexical knowledge base constructed
automatically from the definitions and
example sentences in two machine-readable
dictionaries (MRDs), MindNet embodies
several features that distinguish it from prior
work with MRDs. It is, however, more than
this static resource alone. MindNet represents
a general methodology for acquiring,
structuring, accessing, and exploiting semantic
information from natural language text. This
paper provides an overview of the
distinguishing characteristics of MindNet, the
steps involved in its creation, and its extension
beyond dictionary text.
</bodyText>
<sectionHeader confidence="0.999395" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9993149">
In this paper, we provide a description of the salient
characteristics and functionality of MindNet as it exists
today, together with comparisons to related work. We
conclude with a discussion on extending the MindNet
methodology to the processing of other corpora
(specifically, to the text of the Microsoft Encarta® 98
Encyclopedia) and on future plans for MindNet. For
additional details and background on the creation and
use of MindNet, readers are referred to Richardson
(1997), Vanderwende (1996), and Dolan et al. (1993).
</bodyText>
<sectionHeader confidence="0.99193" genericHeader="introduction">
2 Full automation
</sectionHeader>
<bodyText confidence="0.999903260869565">
MindNet is produced by a fully automatic process,
based on the use of a broad-coverage NL parser. A
fresh version of MindNet is built regularly as part of a
normal regression process. Problems introduced by
daily changes to the underlying system or parsing
grammar are quickly identified and fixed.
Although there has been much research on the use
of automatic methods for extracting information from
dictionary definitions (e.g., Vossen 1995, Wilks et al.
1996), hand-coded knowledge bases, e.g. WordNet
(Miller et al. 1990), continue to be the focus of ongoing
research. The Euro WordNet project (Vossen 1996),
although continuing in the WordNet tradition, includes
a focus on semi-automated procedures for acquiring
lexical content.
Outside the realm of NLP, we believe that
automatic procedures such as MindNet&apos;s provide the
only credible prospect for acquiring world knowledge
on the scale needed to support common-sense
reasoning. At the same time, we acknowledge the
potential need for the hand vetting of such information
to insure accuracy and consistency in production level
systems.
</bodyText>
<sectionHeader confidence="0.997162" genericHeader="method">
3 Broad-coverage parsing
</sectionHeader>
<bodyText confidence="0.999961269230769">
The extraction of the semantic information
contained in MindNet exploits the very same broad-
coverage parser used in the Microsoft Word 97
grammar checker. This parser produces syntactic parse
trees and deeper logical forms, to which rules are
applied that generate corresponding structures of
semantic relations. The parser has not been specially
tuned to process dictionary definitions. All
enhancements to the parser are geared to handle the
immense variety of general text, of which dictionary
definitions are simply a modest subset.
There have been many other attempts to process
dictionary definitions using heuristic pattern matching
(e.g., Chodorow et al. 1985), specially constructed
definition parsers (e.g., Wilks et al. 1996, Vossen
1995), and even general coverage syntactic parsers
(e.g., Briscoe and Carroll 1993). However, none of
these has succeeded in producing the breadth of
semantic relations across entire dictionaries that has
been produced for MindNet.
Vanderwende (1996) describes in detail the
methodology used in the extraction of the semantic
relations comprising MindNet. A truly broad-coverage
parser is an essential component of this process, and it
is the basis for extending it to other sources of
information such as encyclopedias and text corpora.
</bodyText>
<sectionHeader confidence="0.967831" genericHeader="method">
4 Labeled, semantic relations
</sectionHeader>
<bodyText confidence="0.989593">
The different types of labeled, semantic relations
extracted by parsing for inclusion in MindNet are given
in the table below:
</bodyText>
<page confidence="0.982408">
1098
</page>
<table confidence="0.9996541">
Attribute Goal Possessor
Cause Hypernym Purpose
Size
Co-Agent Location
Color Manner Source
Deep Object Material Subclass
Deep Subject Means Synonym
Time
Domain Modifier
Equivalent Part User
</table>
<tableCaption confidence="0.937248">
Table 1. Current set of semantic relation types in
MindNet
</tableCaption>
<bodyText confidence="0.999946235294118">
These relation types may be contrasted with simple
co-occurrence statistics used to create network
structures from dictionaries by researchers including
Veronis and Ide (1990), Kozima and Furugori (1993),
and Wilks et al. (1996). Labeled relations, while more
difficult to obtain, provide greater power for resolving
both structural attachment and word sense ambiguities.
While many researchers have acknowledged the
utility of labeled relations, they have been at times
either unable (e.g., for lack of a sufficiently powerful
parser) or unwilling (e.g., focused on purely statistical
methods) to make the effort to obtain them. This
deficiency limits the characterization of word pairs such
as river/bank (Wilks et al. 1996) and write/pen
(Veronis and Ide 1990) to simple relatedness, whereas
the labeled relations of MindNet specify precisely the
relations river—Part--&gt;bank and write—Means--&gt;pen.
</bodyText>
<sectionHeader confidence="0.979168" genericHeader="method">
5 Semantic relation structures
</sectionHeader>
<bodyText confidence="0.947653769230769">
The automatic extraction of semantic relations (or
semrels) from a definition or example sentence for
MindNet produces a hierarchical structure of these
relations, representing the entire definition or sentence
from which they came. Such structures are stored in
their entirety in MindNet and provide crucial context
for some of the procedures described in later sections of
this paper. The semrel structure for a definition of car
is given in the figure below.
car:
&amp;quot;a vehicle with 3 or usu. 4 wheels
and driven by a motor, esp. one
one for carrying people&amp;quot;
</bodyText>
<figure confidence="0.940475428571429">
car
111r1:0&gt; vehicle
Partr wheel
Tobi drive
------ Means,— motor
Purp› carry
-------Tobj&gt;--- people
</figure>
<figureCaption confidence="0.999934">
Figure 1. Semrel structure for a definition of car.
</figureCaption>
<bodyText confidence="0.999980076923077">
Early dictionary-based work focused on the
extraction of paradigmatic relations, in particular
Hypernym relations (e.g., car—Hypernym-4vehicle).
Almost exclusively, these relations, as well as other
syntagmatic ones, have continued to take the form of
relational triples (see Wilks et al. 1996). The larger
contexts from which these relations have been taken
have generally not been retained. For labeled relations,
only a few researchers (recently, Barriere and Popowich
1996), have appeared to be interested in entire semantic
structures extracted from dictionary definitions, though
they have not reported extracting a significant number
of them.
</bodyText>
<sectionHeader confidence="0.996091" genericHeader="method">
6 Full inversion of structures
</sectionHeader>
<bodyText confidence="0.999933285714286">
After semrel structures are created, they are fully
inverted and propagated throughout the entire MindNet
database, being linked to every word that appears in
them. Such an inverted structure, produced from a
definition for motorist and linked to the entry for car
(appearing as the root of the inverted structure), is
shown in the figure below:
</bodyText>
<figureCaption confidence="0.855193">
Figure 2. Inverted semrel structure from a definition of
motorist
</figureCaption>
<bodyText confidence="0.99802916">
Researchers who produced spreading activation
networks from MRDs, including Veronis and Ide
(1990) and Kozima and Furugori (1993), typically only
implemented forward links (from headwords to their
definition words) in those networks. Words were not
related backward to any of the headwords whose
definitions mentioned them, and words co-occurring in
the same definition were not related directly. In the
fully inverted structures stored in MindNet, however,
all words are cross-linked, no matter where they appear.
The massive network of inverted semrel structures
contained in MindNet invalidates the criticism leveled
against dictionary-based methods by Yarowsky (1992)
and Ide and Veronis (1993) that LKBs created from
MRDs provide spotty coverage of a language at best.
Experiments described elsewhere (Richardson 1997)
demonstrate the comprehensive coverage of the
information contained in MindNet.
Some statistics indicating the size (rounded to the
nearest thousand) of the current version of MindNet and
the processing time required to create it are provided in
the table below. The definitions and example sentences
are from the Longman Dictionary of Contemporary
English (LDOCE) and the American Heritage
Dictionary, ri Edition (AHD3).
</bodyText>
<figure confidence="0.993574181818182">
motorist:
&amp;quot;a person who drives, and usu. owns, a car&amp;quot;
(inverted)
car
.-------&lt;Tobj— drive
--...._
--Tsub&gt;— motorist
..:1&apos;1? &gt;—pers on
sub--owa
-,......,.....
Tobj&gt;— car
</figure>
<page confidence="0.962839">
1099
</page>
<table confidence="0.999469625">
Dictionaries used LDOCE &amp; AHD 3
Time to create (on a P2/266) 7 hours
Head words 159,000
Definitions (N, V, ADJ) 191,000
Example sentences (N, V, ADJ) 58,000
Unique semantic relations 713,000
Inverted structures 1,047,000
Linked head words 91,000
</table>
<tableCaption confidence="0.999855">
Table 2. Statistics on the current version of MindNet
</tableCaption>
<sectionHeader confidence="0.982872" genericHeader="method">
7 Weighted paths
</sectionHeader>
<bodyText confidence="0.999001774193548">
Inverted semrel structures facilitate the access to
direct and indirect relationships between the root word
of each structure, which is the headword for the
MindNet entry containing it, and every other word
contained in the structures. These relationships,
consisting of one or more semantic relations connected
together, constitute semrel paths between two words.
For example, the semrel path between car and person
in Figure 2 above is:
car(--Tobj--drive—Tsub--&gt;motorist—Hyp--&gt;person.
An extended semrel path is a path created from sub-
paths in two different inverted semrel structures. For
example, car and truck are not related directly by a
semantic relation or by a semrel path from any single
semrel structure. However, if one allows the joining of
the semantic relations car—Hyp--vehicle and
vehicle‹-Hyp—truck, each from a different semrel
structure, at the word vehicle, the semrel path
car—Hyp-4vehiclet-Hyp--truck results. Adequately
constrained, extended semrel paths have proven
invaluable in determining the relationship between
words in MindNet that would not otherwise be
connected.
Semrel paths are automatically assigned weights
that reflect their salience. The weights in MindNet are
based on the computation of averaged vertex
probability, which gives preference to semantic
relations occurring with middle frequency, and are
described in detail in Richardson (1997). Weighting
schemes with similar goals are found in work by
Braden-Harder (1993) and Bookman (1994).
</bodyText>
<sectionHeader confidence="0.968931" genericHeader="method">
8 Similarity and inference
</sectionHeader>
<bodyText confidence="0.998937142857143">
Many researchers, both in the dictionary- and
corpus-based camps, have worked extensively on
developing methods to identify similarity between
words, since similarity determination is crucial to many
word sense disambiguation and parameter-
smoothing/inference procedures. However, some
researchers have failed to distinguish between
substitutional similarity and general relatedness. The
similarity procedure of MindNet focuses on measuring
substitutional similarity, but a function is also provided
for producing clusters of generally related words.
Two general strategies have been described in the
literature for identifying substitutional similarity. One
is based on identifying direct, paradigmatic relations
between the words, such as Hypernym or Synonym.
For example, paradigmatic relations in WordNet have
been used by many to determine similarity, including Li
et al. (1995) and Agirre and Rigau (1996). The other
strategy is based on identifying syntagmatic relations
with other words that similar words have in common.
Syntagmatic strategies for determining similarity have
often been based on statistical analyses of large corpora
that yield clusters of words occurring in similar bigram
and trigram contexts (e.g., Brown et al. 1992,
Yarowsky 1992), as well as in similar predicate-
argument structure contexts (e.g., Grishman and
Sterling 1994).
There have been a number of attempts to combine
paradigmatic and syntagmatic similarity strategies (e.g.,
Hearst and Grefenstette 1992, Resnik 1995). However,
none of these has completely integrated both
syntagmatic and paradigmatic information into a single
repository, as is the case with MindNet.
The MindNet similarity procedure is based on the
top-ranked (by weight) semrel paths between words.
For example, some of the top semrel paths in MindNet
between pen and pencil, are shown below:
penf--Means--draw—Means--4pencil
penf-Means—write—Means-&gt;pencil
pen—Hyp-4instrument*-Hyp—pencil
pen—Hyp--&gt;write—Means---*pencil
pen‹-Means—write(-Hyp--pencil
</bodyText>
<tableCaption confidence="0.966988">
Table 3. Highly weighted semrel paths between pen and
</tableCaption>
<bodyText confidence="0.98826005">
pencil
In the above example, a pattern of semrel symmetry
clearly emerges in many of the paths. This observation
of symmetry led to the hypothesis that similar words
are typically connected in MindNet by semrel paths that
frequently exhibit certain patterns of relations
(exclusive of the words they actually connect), many
patterns being symmetrical, but others not.
Several experiments were performed in which word
pairs from a thesaurus and an anti-thesaurus (the latter
containing dissimilar words) were used in a training
phase to identify semrel path patterns that indicate
similarity. These path patterns were then used in a
testing phase to determine the substitutional similarity
or dissimilarity of unseen word pairs (algorithms are
described in Richardson 1997). The results,
summarized in the table below, demonstrate the
strength of this integrated approach, which uniquely
exploits both the paradigmatic and the syntagmatic
relations in MindNet.
</bodyText>
<page confidence="0.981676">
1100
</page>
<bodyText confidence="0.9738793">
Training: over 100,000 word pairs from a thesaurus
and anti-thesaurus produced 285,000 semrel paths
containing approx. 13,500 unique path patterns.
Testing: over 100,000 (different) word pairs from a
thesaurus and anti-thesaurus were evaluated using the
path patterns. Similar correct Dissimilar correct
84% 82%
Human benchmark: random sample of 200 similar
and dissimilar word pairs were evaluated by 5 humans
and by MindNet: Similar correct Dissimilar correct
</bodyText>
<table confidence="0.582179">
Humans: 83% 93%
MindNet: 82% 80%
</table>
<tableCaption confidence="0.994263">
Table 4. Results of similarity experiment
</tableCaption>
<bodyText confidence="0.997872875">
This powerful similarity procedure may also be
used to extend the coverage of the relations in MindNet.
Equivalent to the use of similarity determination in
corpus-based approaches to infer absent n-grams or
triples (e.g., Dagan et al. 1994, Grishman and Sterling
1994), an inference procedure has been developed
which allows semantic relations not presently in
MindNet to be inferred from those that are. It also
exploits the top-ranked paths between the words in the
relation to be inferred. For example, if the relation
watch—Means—gelescope were not in MindNet, it
could be inferred by first finding the semrel paths
between watch and telescope, examining those paths to
see if another word appears in a Means relation with
telescope, and then checking the similarity between that
word and watch. As it turns out, the word observe
satisfies these conditions in the path:
watch—Hyp--observe—Means--telescope
and therefore, it may be inferred that one can watch by
Means of a telescope. The seamless integration of the
inference and similarity procedures, both utilizing the
weighted, extended paths derived from inverted semrel
structures in MindNet, is a unique strength of this
approach.
</bodyText>
<sectionHeader confidence="0.862687" genericHeader="method">
9 Disambiguating MindNet
</sectionHeader>
<bodyText confidence="0.999975527777778">
An additional level of processing during the
creation of MindNet seeks to provide sense identifiers
on the words of semrel structures. Typically, word
sense disambiguation (WSD) occurs during the parsing
of definitions and example sentences, following the
construction of logical forms (see Braden-Harder,
1993). Detailed information from the parse, both
morphological and syntactic, sharply reduces the range
of senses that can be plausibly assigned to each word.
Other aspects of dictionary structure are also exploited,
including domain information associated with particular
senses (e.g., Baseball).
In processing normal input text outside of the
context of MindNet creation, WSD relies crucially on
information from MindNet about how word senses are
linked to one another. To help mitigate this
bootstrapping problem during the initial construction of
MindNet, we have experimented with a two-pass
approach to WSD.
During a first pass, a version of MindNet that does
not include WSD is constructed. The result is a
semantic network that nonetheless contains a great deal
of &amp;quot;ambient&amp;quot; information about sense assignments. For
instance, processing the definition spin 101: (of a
spider or silkworm) to produce thread.., yields a
semrel structure in which the sense node spin101 is
linked by a Deep_Subject relation to the
undisambiguated form spider. On the subsequent pass,
this information can be exploited by WSD in assigning
sense 101 to the word spin in unrelated definitions:
wolf spider 100: any of various spiders...that...do not
spin webs. This kind of bootstrapping reflects the
broader nature of our approach, as discussed in the next
section: a fully and accurately disambiguated MindNet
allows us to bootstrap senses onto words encountered in
free text outside the dictionary domain.
</bodyText>
<subsectionHeader confidence="0.531378">
&apos;10 MindNet as a methodology
</subsectionHeader>
<bodyText confidence="0.9998943125">
The creation of MindNet was never intended to be
an end unto itself. Instead, our emphasis has been on
building a broad-coverage NLP understanding system.
We consider the methodology for creating MindNet to
consist of a set of general tools for acquiring,
structuring, accessing, and exploiting semantic
information from NL text.
Our techniques for building MindNet are largely
rule-based. However we arrive at these representations,
though, the overall structure of MindNet can be
regarded as crucially dependent on statistics. We have
much more in common with traditional corpus-based
approaches than a first glance might suggest. An
advantage we have over these approaches, however, is
the rich structure imposed by the parse, logical form,
and word sense disambiguation components of our
system. The statistics we use in the context of MindNet
allow richer metrics because the data themselves are
richer.
Our first foray into the realm of processing free text
with our methods has already been accomplished; Table
2 showed that some 58,000 example sentences from
LDOCE and AHD3 were processed in the creation of
our current MindNet. To put our hypothesis to a much
more rigorous test, we have recently embarked on the
assimilation of the entire text of the Microsoft Encarta®
98 Encyclopedia. While this has presented several new
challenges in terms of volume alone, we have
nevertheless successfully completed a first pass and
have produced and added semrel structures from the
Encarta® 98 text to MindNet. Statistics on that pass
are given below:
</bodyText>
<page confidence="0.964882">
1101
</page>
<table confidence="0.995059666666667">
Processing time (on a P2/266) 34 hours
Sentences 497,000
Words 10,900,000
Average words/sentence 22
New headwords in MindNet 220,000
New inverted structures in MindNet 5,600,000
</table>
<tableCaption confidence="0.987851">
Table 5. Statistics for Microsoft Encarta® 98
</tableCaption>
<bodyText confidence="0.999877916666667">
Besides our venture into additional English data, we
fully intend to apply the same methodologies to text in
other languages as well. We are currently developing
NLP systems for 3 European and 3 Asian languages:
French, German, and Spanish; Chinese, Japanese, and
Korean. The syntactic parsers for some of these
languages are already quite advanced and have been
demonstrated publicly. As the systems for these
languages mature, we will create corresponding
MindNets, beginning, as we did in English, with the
processing of machine-readable reference materials and
then adding information gleaned from corpora.
</bodyText>
<sectionHeader confidence="0.997705" genericHeader="method">
11 References:
</sectionHeader>
<reference confidence="0.885075325842696">
Agirre, E., and G. Rigau. 1996. Word sense
disambiguation using conceptual density. In
Proceedings of COLING96, 16-22.
Barriere, C., and F. Popowich. 1996. Concept
clustering and knowledge integration from a children&apos;s
dictionary. In Proceedings of COLING96, 65-70.
Bookman, L. 1994. Trajectories through knowledge
space: A dynamic framework for machine
comprehension. Boston, MA: Kluwer Academic
Publishers.
Braden-Harder, L. 1993. Sense disambiguation
using an online dictionary. In Natural language
processing: The PLNLP approach, ed. K. Jensen, G.
Heidorn, and S. Richardson, 247-261. Boston, MA:
Kluwer Academic Publishers.
Briscoe, T., and J. Carroll. Generalized probabilistic
LR parsing of natural language (corpora) with
unification-based grammars. Computational Linguistics
19, no. 1:25-59.
Brown, P., V. Della Pietra, P. deSouza, J. Lai, and
R. Mercer. 1992. Class-based n-gram models of natural
language. Computational Linguistics 18, no. 4:467-479.
Chodorow, M., R. Byrd, and G. Heidorn. 1985.
Extracting semantic hierarchies from a large on-line
dictionary. In Proceedings of the 23rd Annual Meeting
of the ACL, 299-304.
Dagan, I., F. Pereira, and L. Lee. 1994. Similarity-
based estimation of word cooccurrence probabilities. In
Proceedings of the 32&amp;quot; Annual Meeting of the ACL,
272-278.
Dolan, W., L. Vanderwende, and S. Richardson.
1993. Automatically deriving structured knowledge
bases from on-line dictionaries. In Proceedings of the
First Conference of the Pacific Association for
Computational Linguistics (Vancouver, Canada), 5-14.
Grishman, R., and J. Sterling. 1994. Generalizing
automatically generated selectional patterns. In
Proceedings of COLING94, 742-747.
Hearst, M., and G. Grefenstette. 1992. Refining
automatically-discovered lexical relations: Combining
weak techniques for stronger results. In Statistically-
Based Natural Language Programming Techniques,
Papers from the 1992 AAAI Workshop (Menlo Park,
CA), 64-72.
Ide, N., and J. Veronis. 1993. Extracting knowledge
bases from machine-readable dictionaries: Have we
wasted our time? In Proceedings of KB&amp;KS &apos;93
(Tokyo), 257-266.
Kozima, H., and T. Furugori. 1993. Similarity
between words computed by spreading activation on an
English dictionary. In Proceedings of the 6th
Conference of the European Chapter of the ACL, 232-
239.
Li, X., S. Szpakowicz, and S. Matwin. 1995. A
WordNet-based algorithm for word sense
disambiguation. In Proceedings of IJCA195, 1368-
1374.
Miller, G., R. Beckwith, C. Fellbaum, D. Gross, and
K. Miller. 1990. Introduction to WordNet: an on-line
lexical database. In International Journal of
Lexicography 3, no. 4:235-244.
Resnik, P. 1995. Disambiguating noun groupings
with respect to WordNet senses. In Proceedings of the
Third Workshop on Very Large Corpora, 54-68.
Richardson, S. 1997. Determining similarity and
inferring relations in a lexical knowledge base. PhD.
dissertation, City University of New York.
Vanderwende, L. 1996. The analysis of noun
sequences using semantic information extracted from
on-line dictionaries. Ph.D. dissertation, Georgetown
University, Washington, DC.
Veronis, J., and N. Ide. 1990. Word sense
disambiguation with very large neural networks
extracted from machine readable dictionaries. In
Proceedings of COLING90, 289-295.
Vossen, P. 1995. Grammatical and conceptual
individuation in the lexicon. PhD. diss. University of
Amsterdam.
Vossen, P. 1996: Right or Wrong. Combining
lexical resources in the EuroWordNet project. In: M.
Gellerstam, J. Jarborg, S. Malmgren, K. Noren, L.
Rogstrom, C.R. Papmehl, Proceedings of Euralex-96,
Goetheborg, 1996, 715-728
Wilks, Y., B. Slator, and L. Guthrie. 1996. Electric
words: Dictionaries, computers, and meanings.
Cambridge, MA: The MIT Press.
Yarowsky, D. 1992. Word-sense disambiguation
using statistical models of Roget&apos;s categories trained on
large corpora. In Proceedings of COLING92, 454-460.
</reference>
<page confidence="0.99781">
1102
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.926739">
<title confidence="0.996538">MindNet: acquiring and structuring semantic information from text</title>
<author confidence="0.983933">D Richardson</author>
<author confidence="0.983933">William Lucy Vanderwende</author>
<affiliation confidence="0.998988">Microsoft Research</affiliation>
<address confidence="0.992162333333333">One Microsoft Way Redmond, WA 98052 U.S.A.</address>
<abstract confidence="0.997968266666667">As a lexical knowledge base constructed automatically from the definitions and example sentences in two machine-readable dictionaries (MRDs), MindNet embodies several features that distinguish it from prior work with MRDs. It is, however, more than this static resource alone. MindNet represents a general methodology for acquiring, structuring, accessing, and exploiting semantic information from natural language text. This paper provides an overview of the distinguishing characteristics of MindNet, the steps involved in its creation, and its extension beyond dictionary text.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>G Rigau</author>
</authors>
<title>Word sense disambiguation using conceptual density.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING96,</booktitle>
<pages>16--22</pages>
<contexts>
<context position="11048" citStr="Agirre and Rigau (1996)" startWordPosition="1618" endWordPosition="1621">searchers have failed to distinguish between substitutional similarity and general relatedness. The similarity procedure of MindNet focuses on measuring substitutional similarity, but a function is also provided for producing clusters of generally related words. Two general strategies have been described in the literature for identifying substitutional similarity. One is based on identifying direct, paradigmatic relations between the words, such as Hypernym or Synonym. For example, paradigmatic relations in WordNet have been used by many to determine similarity, including Li et al. (1995) and Agirre and Rigau (1996). The other strategy is based on identifying syntagmatic relations with other words that similar words have in common. Syntagmatic strategies for determining similarity have often been based on statistical analyses of large corpora that yield clusters of words occurring in similar bigram and trigram contexts (e.g., Brown et al. 1992, Yarowsky 1992), as well as in similar predicateargument structure contexts (e.g., Grishman and Sterling 1994). There have been a number of attempts to combine paradigmatic and syntagmatic similarity strategies (e.g., Hearst and Grefenstette 1992, Resnik 1995). How</context>
</contexts>
<marker>Agirre, Rigau, 1996</marker>
<rawString>Agirre, E., and G. Rigau. 1996. Word sense disambiguation using conceptual density. In Proceedings of COLING96, 16-22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Barriere</author>
<author>F Popowich</author>
</authors>
<title>Concept clustering and knowledge integration from a children&apos;s dictionary.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING96,</booktitle>
<pages>65--70</pages>
<contexts>
<context position="6271" citStr="Barriere and Popowich 1996" startWordPosition="926" endWordPosition="929">r 111r1:0&gt; vehicle Partr wheel Tobi drive ------ Means,— motor Purp› carry -------Tobj&gt;--- people Figure 1. Semrel structure for a definition of car. Early dictionary-based work focused on the extraction of paradigmatic relations, in particular Hypernym relations (e.g., car—Hypernym-4vehicle). Almost exclusively, these relations, as well as other syntagmatic ones, have continued to take the form of relational triples (see Wilks et al. 1996). The larger contexts from which these relations have been taken have generally not been retained. For labeled relations, only a few researchers (recently, Barriere and Popowich 1996), have appeared to be interested in entire semantic structures extracted from dictionary definitions, though they have not reported extracting a significant number of them. 6 Full inversion of structures After semrel structures are created, they are fully inverted and propagated throughout the entire MindNet database, being linked to every word that appears in them. Such an inverted structure, produced from a definition for motorist and linked to the entry for car (appearing as the root of the inverted structure), is shown in the figure below: Figure 2. Inverted semrel structure from a definit</context>
</contexts>
<marker>Barriere, Popowich, 1996</marker>
<rawString>Barriere, C., and F. Popowich. 1996. Concept clustering and knowledge integration from a children&apos;s dictionary. In Proceedings of COLING96, 65-70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Bookman</author>
</authors>
<title>Trajectories through knowledge space: A dynamic framework for machine comprehension.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers.</publisher>
<location>Boston, MA:</location>
<contexts>
<context position="10109" citStr="Bookman (1994)" startWordPosition="1493" endWordPosition="1494">vehicle, the semrel path car—Hyp-4vehiclet-Hyp--truck results. Adequately constrained, extended semrel paths have proven invaluable in determining the relationship between words in MindNet that would not otherwise be connected. Semrel paths are automatically assigned weights that reflect their salience. The weights in MindNet are based on the computation of averaged vertex probability, which gives preference to semantic relations occurring with middle frequency, and are described in detail in Richardson (1997). Weighting schemes with similar goals are found in work by Braden-Harder (1993) and Bookman (1994). 8 Similarity and inference Many researchers, both in the dictionary- and corpus-based camps, have worked extensively on developing methods to identify similarity between words, since similarity determination is crucial to many word sense disambiguation and parametersmoothing/inference procedures. However, some researchers have failed to distinguish between substitutional similarity and general relatedness. The similarity procedure of MindNet focuses on measuring substitutional similarity, but a function is also provided for producing clusters of generally related words. Two general strategie</context>
</contexts>
<marker>Bookman, 1994</marker>
<rawString>Bookman, L. 1994. Trajectories through knowledge space: A dynamic framework for machine comprehension. Boston, MA: Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Braden-Harder</author>
</authors>
<title>Sense disambiguation using an online dictionary.</title>
<date>1993</date>
<booktitle>In Natural language processing: The PLNLP</booktitle>
<pages>247--261</pages>
<editor>approach, ed. K. Jensen, G. Heidorn, and S. Richardson,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<location>Boston, MA:</location>
<contexts>
<context position="10090" citStr="Braden-Harder (1993)" startWordPosition="1490" endWordPosition="1491">l structure, at the word vehicle, the semrel path car—Hyp-4vehiclet-Hyp--truck results. Adequately constrained, extended semrel paths have proven invaluable in determining the relationship between words in MindNet that would not otherwise be connected. Semrel paths are automatically assigned weights that reflect their salience. The weights in MindNet are based on the computation of averaged vertex probability, which gives preference to semantic relations occurring with middle frequency, and are described in detail in Richardson (1997). Weighting schemes with similar goals are found in work by Braden-Harder (1993) and Bookman (1994). 8 Similarity and inference Many researchers, both in the dictionary- and corpus-based camps, have worked extensively on developing methods to identify similarity between words, since similarity determination is crucial to many word sense disambiguation and parametersmoothing/inference procedures. However, some researchers have failed to distinguish between substitutional similarity and general relatedness. The similarity procedure of MindNet focuses on measuring substitutional similarity, but a function is also provided for producing clusters of generally related words. Tw</context>
<context position="15232" citStr="Braden-Harder, 1993" startWordPosition="2229" endWordPosition="2230">e and therefore, it may be inferred that one can watch by Means of a telescope. The seamless integration of the inference and similarity procedures, both utilizing the weighted, extended paths derived from inverted semrel structures in MindNet, is a unique strength of this approach. 9 Disambiguating MindNet An additional level of processing during the creation of MindNet seeks to provide sense identifiers on the words of semrel structures. Typically, word sense disambiguation (WSD) occurs during the parsing of definitions and example sentences, following the construction of logical forms (see Braden-Harder, 1993). Detailed information from the parse, both morphological and syntactic, sharply reduces the range of senses that can be plausibly assigned to each word. Other aspects of dictionary structure are also exploited, including domain information associated with particular senses (e.g., Baseball). In processing normal input text outside of the context of MindNet creation, WSD relies crucially on information from MindNet about how word senses are linked to one another. To help mitigate this bootstrapping problem during the initial construction of MindNet, we have experimented with a two-pass approach</context>
</contexts>
<marker>Braden-Harder, 1993</marker>
<rawString>Braden-Harder, L. 1993. Sense disambiguation using an online dictionary. In Natural language processing: The PLNLP approach, ed. K. Jensen, G. Heidorn, and S. Richardson, 247-261. Boston, MA: Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="false">
<authors>
<author>T Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars.</title>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<pages>1--25</pages>
<marker>Briscoe, Carroll, </marker>
<rawString>Briscoe, T., and J. Carroll. Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars. Computational Linguistics 19, no. 1:25-59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>V Della Pietra</author>
<author>P deSouza</author>
<author>J Lai</author>
<author>R Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics</journal>
<volume>18</volume>
<pages>4--467</pages>
<contexts>
<context position="11382" citStr="Brown et al. 1992" startWordPosition="1668" endWordPosition="1671">substitutional similarity. One is based on identifying direct, paradigmatic relations between the words, such as Hypernym or Synonym. For example, paradigmatic relations in WordNet have been used by many to determine similarity, including Li et al. (1995) and Agirre and Rigau (1996). The other strategy is based on identifying syntagmatic relations with other words that similar words have in common. Syntagmatic strategies for determining similarity have often been based on statistical analyses of large corpora that yield clusters of words occurring in similar bigram and trigram contexts (e.g., Brown et al. 1992, Yarowsky 1992), as well as in similar predicateargument structure contexts (e.g., Grishman and Sterling 1994). There have been a number of attempts to combine paradigmatic and syntagmatic similarity strategies (e.g., Hearst and Grefenstette 1992, Resnik 1995). However, none of these has completely integrated both syntagmatic and paradigmatic information into a single repository, as is the case with MindNet. The MindNet similarity procedure is based on the top-ranked (by weight) semrel paths between words. For example, some of the top semrel paths in MindNet between pen and pencil, are shown </context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Brown, P., V. Della Pietra, P. deSouza, J. Lai, and R. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics 18, no. 4:467-479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chodorow</author>
<author>R Byrd</author>
<author>G Heidorn</author>
</authors>
<title>Extracting semantic hierarchies from a large on-line dictionary.</title>
<date>1985</date>
<booktitle>In Proceedings of the 23rd Annual Meeting of the ACL,</booktitle>
<pages>299--304</pages>
<contexts>
<context position="3126" citStr="Chodorow et al. 1985" startWordPosition="459" endWordPosition="462">ained in MindNet exploits the very same broadcoverage parser used in the Microsoft Word 97 grammar checker. This parser produces syntactic parse trees and deeper logical forms, to which rules are applied that generate corresponding structures of semantic relations. The parser has not been specially tuned to process dictionary definitions. All enhancements to the parser are geared to handle the immense variety of general text, of which dictionary definitions are simply a modest subset. There have been many other attempts to process dictionary definitions using heuristic pattern matching (e.g., Chodorow et al. 1985), specially constructed definition parsers (e.g., Wilks et al. 1996, Vossen 1995), and even general coverage syntactic parsers (e.g., Briscoe and Carroll 1993). However, none of these has succeeded in producing the breadth of semantic relations across entire dictionaries that has been produced for MindNet. Vanderwende (1996) describes in detail the methodology used in the extraction of the semantic relations comprising MindNet. A truly broad-coverage parser is an essential component of this process, and it is the basis for extending it to other sources of information such as encyclopedias and </context>
</contexts>
<marker>Chodorow, Byrd, Heidorn, 1985</marker>
<rawString>Chodorow, M., R. Byrd, and G. Heidorn. 1985. Extracting semantic hierarchies from a large on-line dictionary. In Proceedings of the 23rd Annual Meeting of the ACL, 299-304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>F Pereira</author>
<author>L Lee</author>
</authors>
<title>Similaritybased estimation of word cooccurrence probabilities.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32&amp;quot; Annual Meeting of the ACL,</booktitle>
<pages>272--278</pages>
<contexts>
<context position="13943" citStr="Dagan et al. 1994" startWordPosition="2033" endWordPosition="2036">r 100,000 (different) word pairs from a thesaurus and anti-thesaurus were evaluated using the path patterns. Similar correct Dissimilar correct 84% 82% Human benchmark: random sample of 200 similar and dissimilar word pairs were evaluated by 5 humans and by MindNet: Similar correct Dissimilar correct Humans: 83% 93% MindNet: 82% 80% Table 4. Results of similarity experiment This powerful similarity procedure may also be used to extend the coverage of the relations in MindNet. Equivalent to the use of similarity determination in corpus-based approaches to infer absent n-grams or triples (e.g., Dagan et al. 1994, Grishman and Sterling 1994), an inference procedure has been developed which allows semantic relations not presently in MindNet to be inferred from those that are. It also exploits the top-ranked paths between the words in the relation to be inferred. For example, if the relation watch—Means—gelescope were not in MindNet, it could be inferred by first finding the semrel paths between watch and telescope, examining those paths to see if another word appears in a Means relation with telescope, and then checking the similarity between that word and watch. As it turns out, the word observe satis</context>
</contexts>
<marker>Dagan, Pereira, Lee, 1994</marker>
<rawString>Dagan, I., F. Pereira, and L. Lee. 1994. Similaritybased estimation of word cooccurrence probabilities. In Proceedings of the 32&amp;quot; Annual Meeting of the ACL, 272-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Dolan</author>
<author>L Vanderwende</author>
<author>S Richardson</author>
</authors>
<title>Automatically deriving structured knowledge bases from on-line dictionaries.</title>
<date>1993</date>
<booktitle>In Proceedings of the First Conference of the Pacific Association for Computational Linguistics</booktitle>
<pages>5--14</pages>
<location>Vancouver,</location>
<contexts>
<context position="1319" citStr="Dolan et al. (1993)" startWordPosition="185" endWordPosition="188">e steps involved in its creation, and its extension beyond dictionary text. 1 Introduction In this paper, we provide a description of the salient characteristics and functionality of MindNet as it exists today, together with comparisons to related work. We conclude with a discussion on extending the MindNet methodology to the processing of other corpora (specifically, to the text of the Microsoft Encarta® 98 Encyclopedia) and on future plans for MindNet. For additional details and background on the creation and use of MindNet, readers are referred to Richardson (1997), Vanderwende (1996), and Dolan et al. (1993). 2 Full automation MindNet is produced by a fully automatic process, based on the use of a broad-coverage NL parser. A fresh version of MindNet is built regularly as part of a normal regression process. Problems introduced by daily changes to the underlying system or parsing grammar are quickly identified and fixed. Although there has been much research on the use of automatic methods for extracting information from dictionary definitions (e.g., Vossen 1995, Wilks et al. 1996), hand-coded knowledge bases, e.g. WordNet (Miller et al. 1990), continue to be the focus of ongoing research. The Eur</context>
</contexts>
<marker>Dolan, Vanderwende, Richardson, 1993</marker>
<rawString>Dolan, W., L. Vanderwende, and S. Richardson. 1993. Automatically deriving structured knowledge bases from on-line dictionaries. In Proceedings of the First Conference of the Pacific Association for Computational Linguistics (Vancouver, Canada), 5-14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
<author>J Sterling</author>
</authors>
<title>Generalizing automatically generated selectional patterns.</title>
<date>1994</date>
<booktitle>In Proceedings of COLING94,</booktitle>
<pages>742--747</pages>
<contexts>
<context position="11493" citStr="Grishman and Sterling 1994" startWordPosition="1684" endWordPosition="1687">ds, such as Hypernym or Synonym. For example, paradigmatic relations in WordNet have been used by many to determine similarity, including Li et al. (1995) and Agirre and Rigau (1996). The other strategy is based on identifying syntagmatic relations with other words that similar words have in common. Syntagmatic strategies for determining similarity have often been based on statistical analyses of large corpora that yield clusters of words occurring in similar bigram and trigram contexts (e.g., Brown et al. 1992, Yarowsky 1992), as well as in similar predicateargument structure contexts (e.g., Grishman and Sterling 1994). There have been a number of attempts to combine paradigmatic and syntagmatic similarity strategies (e.g., Hearst and Grefenstette 1992, Resnik 1995). However, none of these has completely integrated both syntagmatic and paradigmatic information into a single repository, as is the case with MindNet. The MindNet similarity procedure is based on the top-ranked (by weight) semrel paths between words. For example, some of the top semrel paths in MindNet between pen and pencil, are shown below: penf--Means--draw—Means--4pencil penf-Means—write—Means-&gt;pencil pen—Hyp-4instrument*-Hyp—pencil pen—Hyp-</context>
<context position="13972" citStr="Grishman and Sterling 1994" startWordPosition="2037" endWordPosition="2040">t) word pairs from a thesaurus and anti-thesaurus were evaluated using the path patterns. Similar correct Dissimilar correct 84% 82% Human benchmark: random sample of 200 similar and dissimilar word pairs were evaluated by 5 humans and by MindNet: Similar correct Dissimilar correct Humans: 83% 93% MindNet: 82% 80% Table 4. Results of similarity experiment This powerful similarity procedure may also be used to extend the coverage of the relations in MindNet. Equivalent to the use of similarity determination in corpus-based approaches to infer absent n-grams or triples (e.g., Dagan et al. 1994, Grishman and Sterling 1994), an inference procedure has been developed which allows semantic relations not presently in MindNet to be inferred from those that are. It also exploits the top-ranked paths between the words in the relation to be inferred. For example, if the relation watch—Means—gelescope were not in MindNet, it could be inferred by first finding the semrel paths between watch and telescope, examining those paths to see if another word appears in a Means relation with telescope, and then checking the similarity between that word and watch. As it turns out, the word observe satisfies these conditions in the </context>
</contexts>
<marker>Grishman, Sterling, 1994</marker>
<rawString>Grishman, R., and J. Sterling. 1994. Generalizing automatically generated selectional patterns. In Proceedings of COLING94, 742-747.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
<author>G Grefenstette</author>
</authors>
<title>Refining automatically-discovered lexical relations: Combining weak techniques for stronger results.</title>
<date>1992</date>
<booktitle>In StatisticallyBased Natural Language Programming Techniques, Papers from the 1992 AAAI Workshop (Menlo Park, CA),</booktitle>
<pages>64--72</pages>
<contexts>
<context position="11629" citStr="Hearst and Grefenstette 1992" startWordPosition="1703" endWordPosition="1706">ng Li et al. (1995) and Agirre and Rigau (1996). The other strategy is based on identifying syntagmatic relations with other words that similar words have in common. Syntagmatic strategies for determining similarity have often been based on statistical analyses of large corpora that yield clusters of words occurring in similar bigram and trigram contexts (e.g., Brown et al. 1992, Yarowsky 1992), as well as in similar predicateargument structure contexts (e.g., Grishman and Sterling 1994). There have been a number of attempts to combine paradigmatic and syntagmatic similarity strategies (e.g., Hearst and Grefenstette 1992, Resnik 1995). However, none of these has completely integrated both syntagmatic and paradigmatic information into a single repository, as is the case with MindNet. The MindNet similarity procedure is based on the top-ranked (by weight) semrel paths between words. For example, some of the top semrel paths in MindNet between pen and pencil, are shown below: penf--Means--draw—Means--4pencil penf-Means—write—Means-&gt;pencil pen—Hyp-4instrument*-Hyp—pencil pen—Hyp--&gt;write—Means---*pencil pen‹-Means—write(-Hyp--pencil Table 3. Highly weighted semrel paths between pen and pencil In the above example,</context>
</contexts>
<marker>Hearst, Grefenstette, 1992</marker>
<rawString>Hearst, M., and G. Grefenstette. 1992. Refining automatically-discovered lexical relations: Combining weak techniques for stronger results. In StatisticallyBased Natural Language Programming Techniques, Papers from the 1992 AAAI Workshop (Menlo Park, CA), 64-72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ide</author>
<author>J Veronis</author>
</authors>
<title>Extracting knowledge bases from machine-readable dictionaries: Have we wasted our time?</title>
<date>1993</date>
<booktitle>In Proceedings of KB&amp;KS &apos;93</booktitle>
<pages>257--266</pages>
<location>(Tokyo),</location>
<contexts>
<context position="7585" citStr="Ide and Veronis (1993)" startWordPosition="1121" endWordPosition="1124">Veronis and Ide (1990) and Kozima and Furugori (1993), typically only implemented forward links (from headwords to their definition words) in those networks. Words were not related backward to any of the headwords whose definitions mentioned them, and words co-occurring in the same definition were not related directly. In the fully inverted structures stored in MindNet, however, all words are cross-linked, no matter where they appear. The massive network of inverted semrel structures contained in MindNet invalidates the criticism leveled against dictionary-based methods by Yarowsky (1992) and Ide and Veronis (1993) that LKBs created from MRDs provide spotty coverage of a language at best. Experiments described elsewhere (Richardson 1997) demonstrate the comprehensive coverage of the information contained in MindNet. Some statistics indicating the size (rounded to the nearest thousand) of the current version of MindNet and the processing time required to create it are provided in the table below. The definitions and example sentences are from the Longman Dictionary of Contemporary English (LDOCE) and the American Heritage Dictionary, ri Edition (AHD3). motorist: &amp;quot;a person who drives, and usu. owns, a car</context>
</contexts>
<marker>Ide, Veronis, 1993</marker>
<rawString>Ide, N., and J. Veronis. 1993. Extracting knowledge bases from machine-readable dictionaries: Have we wasted our time? In Proceedings of KB&amp;KS &apos;93 (Tokyo), 257-266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kozima</author>
<author>T Furugori</author>
</authors>
<title>Similarity between words computed by spreading activation on an English dictionary.</title>
<date>1993</date>
<booktitle>In Proceedings of the 6th Conference of the European Chapter of the ACL,</booktitle>
<pages>232--239</pages>
<contexts>
<context position="4354" citStr="Kozima and Furugori (1993)" startWordPosition="639" endWordPosition="642">xt corpora. 4 Labeled, semantic relations The different types of labeled, semantic relations extracted by parsing for inclusion in MindNet are given in the table below: 1098 Attribute Goal Possessor Cause Hypernym Purpose Size Co-Agent Location Color Manner Source Deep Object Material Subclass Deep Subject Means Synonym Time Domain Modifier Equivalent Part User Table 1. Current set of semantic relation types in MindNet These relation types may be contrasted with simple co-occurrence statistics used to create network structures from dictionaries by researchers including Veronis and Ide (1990), Kozima and Furugori (1993), and Wilks et al. (1996). Labeled relations, while more difficult to obtain, provide greater power for resolving both structural attachment and word sense ambiguities. While many researchers have acknowledged the utility of labeled relations, they have been at times either unable (e.g., for lack of a sufficiently powerful parser) or unwilling (e.g., focused on purely statistical methods) to make the effort to obtain them. This deficiency limits the characterization of word pairs such as river/bank (Wilks et al. 1996) and write/pen (Veronis and Ide 1990) to simple relatedness, whereas the labe</context>
<context position="7016" citStr="Kozima and Furugori (1993)" startWordPosition="1039" endWordPosition="1042">not reported extracting a significant number of them. 6 Full inversion of structures After semrel structures are created, they are fully inverted and propagated throughout the entire MindNet database, being linked to every word that appears in them. Such an inverted structure, produced from a definition for motorist and linked to the entry for car (appearing as the root of the inverted structure), is shown in the figure below: Figure 2. Inverted semrel structure from a definition of motorist Researchers who produced spreading activation networks from MRDs, including Veronis and Ide (1990) and Kozima and Furugori (1993), typically only implemented forward links (from headwords to their definition words) in those networks. Words were not related backward to any of the headwords whose definitions mentioned them, and words co-occurring in the same definition were not related directly. In the fully inverted structures stored in MindNet, however, all words are cross-linked, no matter where they appear. The massive network of inverted semrel structures contained in MindNet invalidates the criticism leveled against dictionary-based methods by Yarowsky (1992) and Ide and Veronis (1993) that LKBs created from MRDs pr</context>
</contexts>
<marker>Kozima, Furugori, 1993</marker>
<rawString>Kozima, H., and T. Furugori. 1993. Similarity between words computed by spreading activation on an English dictionary. In Proceedings of the 6th Conference of the European Chapter of the ACL, 232-239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>S Szpakowicz</author>
<author>S Matwin</author>
</authors>
<title>A WordNet-based algorithm for word sense disambiguation.</title>
<date>1995</date>
<booktitle>In Proceedings of IJCA195,</booktitle>
<pages>1368--1374</pages>
<contexts>
<context position="11020" citStr="Li et al. (1995)" startWordPosition="1613" endWordPosition="1616">res. However, some researchers have failed to distinguish between substitutional similarity and general relatedness. The similarity procedure of MindNet focuses on measuring substitutional similarity, but a function is also provided for producing clusters of generally related words. Two general strategies have been described in the literature for identifying substitutional similarity. One is based on identifying direct, paradigmatic relations between the words, such as Hypernym or Synonym. For example, paradigmatic relations in WordNet have been used by many to determine similarity, including Li et al. (1995) and Agirre and Rigau (1996). The other strategy is based on identifying syntagmatic relations with other words that similar words have in common. Syntagmatic strategies for determining similarity have often been based on statistical analyses of large corpora that yield clusters of words occurring in similar bigram and trigram contexts (e.g., Brown et al. 1992, Yarowsky 1992), as well as in similar predicateargument structure contexts (e.g., Grishman and Sterling 1994). There have been a number of attempts to combine paradigmatic and syntagmatic similarity strategies (e.g., Hearst and Grefenst</context>
</contexts>
<marker>Li, Szpakowicz, Matwin, 1995</marker>
<rawString>Li, X., S. Szpakowicz, and S. Matwin. 1995. A WordNet-based algorithm for word sense disambiguation. In Proceedings of IJCA195, 1368-1374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>R Beckwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K Miller</author>
</authors>
<title>Introduction to WordNet: an on-line lexical database.</title>
<date>1990</date>
<journal>In International Journal of Lexicography</journal>
<volume>3</volume>
<pages>4--235</pages>
<contexts>
<context position="1864" citStr="Miller et al. 1990" startWordPosition="271" endWordPosition="274">referred to Richardson (1997), Vanderwende (1996), and Dolan et al. (1993). 2 Full automation MindNet is produced by a fully automatic process, based on the use of a broad-coverage NL parser. A fresh version of MindNet is built regularly as part of a normal regression process. Problems introduced by daily changes to the underlying system or parsing grammar are quickly identified and fixed. Although there has been much research on the use of automatic methods for extracting information from dictionary definitions (e.g., Vossen 1995, Wilks et al. 1996), hand-coded knowledge bases, e.g. WordNet (Miller et al. 1990), continue to be the focus of ongoing research. The Euro WordNet project (Vossen 1996), although continuing in the WordNet tradition, includes a focus on semi-automated procedures for acquiring lexical content. Outside the realm of NLP, we believe that automatic procedures such as MindNet&apos;s provide the only credible prospect for acquiring world knowledge on the scale needed to support common-sense reasoning. At the same time, we acknowledge the potential need for the hand vetting of such information to insure accuracy and consistency in production level systems. 3 Broad-coverage parsing The ex</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>Miller, G., R. Beckwith, C. Fellbaum, D. Gross, and K. Miller. 1990. Introduction to WordNet: an on-line lexical database. In International Journal of Lexicography 3, no. 4:235-244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Disambiguating noun groupings with respect to WordNet senses.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third Workshop on Very Large Corpora,</booktitle>
<pages>54--68</pages>
<contexts>
<context position="11643" citStr="Resnik 1995" startWordPosition="1707" endWordPosition="1708"> and Rigau (1996). The other strategy is based on identifying syntagmatic relations with other words that similar words have in common. Syntagmatic strategies for determining similarity have often been based on statistical analyses of large corpora that yield clusters of words occurring in similar bigram and trigram contexts (e.g., Brown et al. 1992, Yarowsky 1992), as well as in similar predicateargument structure contexts (e.g., Grishman and Sterling 1994). There have been a number of attempts to combine paradigmatic and syntagmatic similarity strategies (e.g., Hearst and Grefenstette 1992, Resnik 1995). However, none of these has completely integrated both syntagmatic and paradigmatic information into a single repository, as is the case with MindNet. The MindNet similarity procedure is based on the top-ranked (by weight) semrel paths between words. For example, some of the top semrel paths in MindNet between pen and pencil, are shown below: penf--Means--draw—Means--4pencil penf-Means—write—Means-&gt;pencil pen—Hyp-4instrument*-Hyp—pencil pen—Hyp--&gt;write—Means---*pencil pen‹-Means—write(-Hyp--pencil Table 3. Highly weighted semrel paths between pen and pencil In the above example, a pattern of </context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Resnik, P. 1995. Disambiguating noun groupings with respect to WordNet senses. In Proceedings of the Third Workshop on Very Large Corpora, 54-68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Richardson</author>
</authors>
<title>Determining similarity and inferring relations in a lexical knowledge base.</title>
<date>1997</date>
<institution>City University of New York.</institution>
<note>PhD. dissertation,</note>
<contexts>
<context position="1274" citStr="Richardson (1997)" startWordPosition="180" endWordPosition="181">stinguishing characteristics of MindNet, the steps involved in its creation, and its extension beyond dictionary text. 1 Introduction In this paper, we provide a description of the salient characteristics and functionality of MindNet as it exists today, together with comparisons to related work. We conclude with a discussion on extending the MindNet methodology to the processing of other corpora (specifically, to the text of the Microsoft Encarta® 98 Encyclopedia) and on future plans for MindNet. For additional details and background on the creation and use of MindNet, readers are referred to Richardson (1997), Vanderwende (1996), and Dolan et al. (1993). 2 Full automation MindNet is produced by a fully automatic process, based on the use of a broad-coverage NL parser. A fresh version of MindNet is built regularly as part of a normal regression process. Problems introduced by daily changes to the underlying system or parsing grammar are quickly identified and fixed. Although there has been much research on the use of automatic methods for extracting information from dictionary definitions (e.g., Vossen 1995, Wilks et al. 1996), hand-coded knowledge bases, e.g. WordNet (Miller et al. 1990), continue</context>
<context position="7710" citStr="Richardson 1997" startWordPosition="1141" endWordPosition="1142"> words) in those networks. Words were not related backward to any of the headwords whose definitions mentioned them, and words co-occurring in the same definition were not related directly. In the fully inverted structures stored in MindNet, however, all words are cross-linked, no matter where they appear. The massive network of inverted semrel structures contained in MindNet invalidates the criticism leveled against dictionary-based methods by Yarowsky (1992) and Ide and Veronis (1993) that LKBs created from MRDs provide spotty coverage of a language at best. Experiments described elsewhere (Richardson 1997) demonstrate the comprehensive coverage of the information contained in MindNet. Some statistics indicating the size (rounded to the nearest thousand) of the current version of MindNet and the processing time required to create it are provided in the table below. The definitions and example sentences are from the Longman Dictionary of Contemporary English (LDOCE) and the American Heritage Dictionary, ri Edition (AHD3). motorist: &amp;quot;a person who drives, and usu. owns, a car&amp;quot; (inverted) car .-------&lt;Tobj— drive --...._ --Tsub&gt;— motorist ..:1&apos;1? &gt;—pers on sub--owa -,......,..... Tobj&gt;— car 1099 Dic</context>
<context position="10010" citStr="Richardson (1997)" startWordPosition="1478" endWordPosition="1479">elations car—Hyp--vehicle and vehicle‹-Hyp—truck, each from a different semrel structure, at the word vehicle, the semrel path car—Hyp-4vehiclet-Hyp--truck results. Adequately constrained, extended semrel paths have proven invaluable in determining the relationship between words in MindNet that would not otherwise be connected. Semrel paths are automatically assigned weights that reflect their salience. The weights in MindNet are based on the computation of averaged vertex probability, which gives preference to semantic relations occurring with middle frequency, and are described in detail in Richardson (1997). Weighting schemes with similar goals are found in work by Braden-Harder (1993) and Bookman (1994). 8 Similarity and inference Many researchers, both in the dictionary- and corpus-based camps, have worked extensively on developing methods to identify similarity between words, since similarity determination is crucial to many word sense disambiguation and parametersmoothing/inference procedures. However, some researchers have failed to distinguish between substitutional similarity and general relatedness. The similarity procedure of MindNet focuses on measuring substitutional similarity, but a</context>
<context position="12973" citStr="Richardson 1997" startWordPosition="1892" endWordPosition="1893">lar words are typically connected in MindNet by semrel paths that frequently exhibit certain patterns of relations (exclusive of the words they actually connect), many patterns being symmetrical, but others not. Several experiments were performed in which word pairs from a thesaurus and an anti-thesaurus (the latter containing dissimilar words) were used in a training phase to identify semrel path patterns that indicate similarity. These path patterns were then used in a testing phase to determine the substitutional similarity or dissimilarity of unseen word pairs (algorithms are described in Richardson 1997). The results, summarized in the table below, demonstrate the strength of this integrated approach, which uniquely exploits both the paradigmatic and the syntagmatic relations in MindNet. 1100 Training: over 100,000 word pairs from a thesaurus and anti-thesaurus produced 285,000 semrel paths containing approx. 13,500 unique path patterns. Testing: over 100,000 (different) word pairs from a thesaurus and anti-thesaurus were evaluated using the path patterns. Similar correct Dissimilar correct 84% 82% Human benchmark: random sample of 200 similar and dissimilar word pairs were evaluated by 5 hum</context>
</contexts>
<marker>Richardson, 1997</marker>
<rawString>Richardson, S. 1997. Determining similarity and inferring relations in a lexical knowledge base. PhD. dissertation, City University of New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Vanderwende</author>
</authors>
<title>The analysis of noun sequences using semantic information extracted from on-line dictionaries.</title>
<date>1996</date>
<institution>Georgetown University,</institution>
<location>Washington, DC.</location>
<note>Ph.D. dissertation,</note>
<contexts>
<context position="1294" citStr="Vanderwende (1996)" startWordPosition="182" endWordPosition="183">teristics of MindNet, the steps involved in its creation, and its extension beyond dictionary text. 1 Introduction In this paper, we provide a description of the salient characteristics and functionality of MindNet as it exists today, together with comparisons to related work. We conclude with a discussion on extending the MindNet methodology to the processing of other corpora (specifically, to the text of the Microsoft Encarta® 98 Encyclopedia) and on future plans for MindNet. For additional details and background on the creation and use of MindNet, readers are referred to Richardson (1997), Vanderwende (1996), and Dolan et al. (1993). 2 Full automation MindNet is produced by a fully automatic process, based on the use of a broad-coverage NL parser. A fresh version of MindNet is built regularly as part of a normal regression process. Problems introduced by daily changes to the underlying system or parsing grammar are quickly identified and fixed. Although there has been much research on the use of automatic methods for extracting information from dictionary definitions (e.g., Vossen 1995, Wilks et al. 1996), hand-coded knowledge bases, e.g. WordNet (Miller et al. 1990), continue to be the focus of </context>
<context position="3452" citStr="Vanderwende (1996)" startWordPosition="507" endWordPosition="508">efinitions. All enhancements to the parser are geared to handle the immense variety of general text, of which dictionary definitions are simply a modest subset. There have been many other attempts to process dictionary definitions using heuristic pattern matching (e.g., Chodorow et al. 1985), specially constructed definition parsers (e.g., Wilks et al. 1996, Vossen 1995), and even general coverage syntactic parsers (e.g., Briscoe and Carroll 1993). However, none of these has succeeded in producing the breadth of semantic relations across entire dictionaries that has been produced for MindNet. Vanderwende (1996) describes in detail the methodology used in the extraction of the semantic relations comprising MindNet. A truly broad-coverage parser is an essential component of this process, and it is the basis for extending it to other sources of information such as encyclopedias and text corpora. 4 Labeled, semantic relations The different types of labeled, semantic relations extracted by parsing for inclusion in MindNet are given in the table below: 1098 Attribute Goal Possessor Cause Hypernym Purpose Size Co-Agent Location Color Manner Source Deep Object Material Subclass Deep Subject Means Synonym Ti</context>
</contexts>
<marker>Vanderwende, 1996</marker>
<rawString>Vanderwende, L. 1996. The analysis of noun sequences using semantic information extracted from on-line dictionaries. Ph.D. dissertation, Georgetown University, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Veronis</author>
<author>N Ide</author>
</authors>
<title>Word sense disambiguation with very large neural networks extracted from machine readable dictionaries.</title>
<date>1990</date>
<booktitle>In Proceedings of COLING90,</booktitle>
<pages>289--295</pages>
<contexts>
<context position="4326" citStr="Veronis and Ide (1990)" startWordPosition="635" endWordPosition="638"> as encyclopedias and text corpora. 4 Labeled, semantic relations The different types of labeled, semantic relations extracted by parsing for inclusion in MindNet are given in the table below: 1098 Attribute Goal Possessor Cause Hypernym Purpose Size Co-Agent Location Color Manner Source Deep Object Material Subclass Deep Subject Means Synonym Time Domain Modifier Equivalent Part User Table 1. Current set of semantic relation types in MindNet These relation types may be contrasted with simple co-occurrence statistics used to create network structures from dictionaries by researchers including Veronis and Ide (1990), Kozima and Furugori (1993), and Wilks et al. (1996). Labeled relations, while more difficult to obtain, provide greater power for resolving both structural attachment and word sense ambiguities. While many researchers have acknowledged the utility of labeled relations, they have been at times either unable (e.g., for lack of a sufficiently powerful parser) or unwilling (e.g., focused on purely statistical methods) to make the effort to obtain them. This deficiency limits the characterization of word pairs such as river/bank (Wilks et al. 1996) and write/pen (Veronis and Ide 1990) to simple r</context>
<context position="6985" citStr="Veronis and Ide (1990)" startWordPosition="1034" endWordPosition="1037">initions, though they have not reported extracting a significant number of them. 6 Full inversion of structures After semrel structures are created, they are fully inverted and propagated throughout the entire MindNet database, being linked to every word that appears in them. Such an inverted structure, produced from a definition for motorist and linked to the entry for car (appearing as the root of the inverted structure), is shown in the figure below: Figure 2. Inverted semrel structure from a definition of motorist Researchers who produced spreading activation networks from MRDs, including Veronis and Ide (1990) and Kozima and Furugori (1993), typically only implemented forward links (from headwords to their definition words) in those networks. Words were not related backward to any of the headwords whose definitions mentioned them, and words co-occurring in the same definition were not related directly. In the fully inverted structures stored in MindNet, however, all words are cross-linked, no matter where they appear. The massive network of inverted semrel structures contained in MindNet invalidates the criticism leveled against dictionary-based methods by Yarowsky (1992) and Ide and Veronis (1993)</context>
</contexts>
<marker>Veronis, Ide, 1990</marker>
<rawString>Veronis, J., and N. Ide. 1990. Word sense disambiguation with very large neural networks extracted from machine readable dictionaries. In Proceedings of COLING90, 289-295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Vossen</author>
</authors>
<title>Grammatical and conceptual individuation in the lexicon.</title>
<date>1995</date>
<institution>PhD. diss. University of Amsterdam.</institution>
<contexts>
<context position="1781" citStr="Vossen 1995" startWordPosition="260" endWordPosition="261">nal details and background on the creation and use of MindNet, readers are referred to Richardson (1997), Vanderwende (1996), and Dolan et al. (1993). 2 Full automation MindNet is produced by a fully automatic process, based on the use of a broad-coverage NL parser. A fresh version of MindNet is built regularly as part of a normal regression process. Problems introduced by daily changes to the underlying system or parsing grammar are quickly identified and fixed. Although there has been much research on the use of automatic methods for extracting information from dictionary definitions (e.g., Vossen 1995, Wilks et al. 1996), hand-coded knowledge bases, e.g. WordNet (Miller et al. 1990), continue to be the focus of ongoing research. The Euro WordNet project (Vossen 1996), although continuing in the WordNet tradition, includes a focus on semi-automated procedures for acquiring lexical content. Outside the realm of NLP, we believe that automatic procedures such as MindNet&apos;s provide the only credible prospect for acquiring world knowledge on the scale needed to support common-sense reasoning. At the same time, we acknowledge the potential need for the hand vetting of such information to insure ac</context>
<context position="3207" citStr="Vossen 1995" startWordPosition="472" endWordPosition="473"> grammar checker. This parser produces syntactic parse trees and deeper logical forms, to which rules are applied that generate corresponding structures of semantic relations. The parser has not been specially tuned to process dictionary definitions. All enhancements to the parser are geared to handle the immense variety of general text, of which dictionary definitions are simply a modest subset. There have been many other attempts to process dictionary definitions using heuristic pattern matching (e.g., Chodorow et al. 1985), specially constructed definition parsers (e.g., Wilks et al. 1996, Vossen 1995), and even general coverage syntactic parsers (e.g., Briscoe and Carroll 1993). However, none of these has succeeded in producing the breadth of semantic relations across entire dictionaries that has been produced for MindNet. Vanderwende (1996) describes in detail the methodology used in the extraction of the semantic relations comprising MindNet. A truly broad-coverage parser is an essential component of this process, and it is the basis for extending it to other sources of information such as encyclopedias and text corpora. 4 Labeled, semantic relations The different types of labeled, seman</context>
</contexts>
<marker>Vossen, 1995</marker>
<rawString>Vossen, P. 1995. Grammatical and conceptual individuation in the lexicon. PhD. diss. University of Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Vossen</author>
</authors>
<title>Right or Wrong. Combining lexical resources in the EuroWordNet project. In:</title>
<date>1996</date>
<booktitle>Proceedings of Euralex-96,</booktitle>
<pages>715--728</pages>
<location>Goetheborg,</location>
<contexts>
<context position="1950" citStr="Vossen 1996" startWordPosition="287" endWordPosition="288"> MindNet is produced by a fully automatic process, based on the use of a broad-coverage NL parser. A fresh version of MindNet is built regularly as part of a normal regression process. Problems introduced by daily changes to the underlying system or parsing grammar are quickly identified and fixed. Although there has been much research on the use of automatic methods for extracting information from dictionary definitions (e.g., Vossen 1995, Wilks et al. 1996), hand-coded knowledge bases, e.g. WordNet (Miller et al. 1990), continue to be the focus of ongoing research. The Euro WordNet project (Vossen 1996), although continuing in the WordNet tradition, includes a focus on semi-automated procedures for acquiring lexical content. Outside the realm of NLP, we believe that automatic procedures such as MindNet&apos;s provide the only credible prospect for acquiring world knowledge on the scale needed to support common-sense reasoning. At the same time, we acknowledge the potential need for the hand vetting of such information to insure accuracy and consistency in production level systems. 3 Broad-coverage parsing The extraction of the semantic information contained in MindNet exploits the very same broad</context>
</contexts>
<marker>Vossen, 1996</marker>
<rawString>Vossen, P. 1996: Right or Wrong. Combining lexical resources in the EuroWordNet project. In: M. Gellerstam, J. Jarborg, S. Malmgren, K. Noren, L. Rogstrom, C.R. Papmehl, Proceedings of Euralex-96, Goetheborg, 1996, 715-728</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wilks</author>
<author>B Slator</author>
<author>L Guthrie</author>
</authors>
<title>Electric words: Dictionaries, computers, and meanings.</title>
<date>1996</date>
<publisher>The MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="1801" citStr="Wilks et al. 1996" startWordPosition="262" endWordPosition="265">nd background on the creation and use of MindNet, readers are referred to Richardson (1997), Vanderwende (1996), and Dolan et al. (1993). 2 Full automation MindNet is produced by a fully automatic process, based on the use of a broad-coverage NL parser. A fresh version of MindNet is built regularly as part of a normal regression process. Problems introduced by daily changes to the underlying system or parsing grammar are quickly identified and fixed. Although there has been much research on the use of automatic methods for extracting information from dictionary definitions (e.g., Vossen 1995, Wilks et al. 1996), hand-coded knowledge bases, e.g. WordNet (Miller et al. 1990), continue to be the focus of ongoing research. The Euro WordNet project (Vossen 1996), although continuing in the WordNet tradition, includes a focus on semi-automated procedures for acquiring lexical content. Outside the realm of NLP, we believe that automatic procedures such as MindNet&apos;s provide the only credible prospect for acquiring world knowledge on the scale needed to support common-sense reasoning. At the same time, we acknowledge the potential need for the hand vetting of such information to insure accuracy and consisten</context>
<context position="3193" citStr="Wilks et al. 1996" startWordPosition="468" endWordPosition="471">e Microsoft Word 97 grammar checker. This parser produces syntactic parse trees and deeper logical forms, to which rules are applied that generate corresponding structures of semantic relations. The parser has not been specially tuned to process dictionary definitions. All enhancements to the parser are geared to handle the immense variety of general text, of which dictionary definitions are simply a modest subset. There have been many other attempts to process dictionary definitions using heuristic pattern matching (e.g., Chodorow et al. 1985), specially constructed definition parsers (e.g., Wilks et al. 1996, Vossen 1995), and even general coverage syntactic parsers (e.g., Briscoe and Carroll 1993). However, none of these has succeeded in producing the breadth of semantic relations across entire dictionaries that has been produced for MindNet. Vanderwende (1996) describes in detail the methodology used in the extraction of the semantic relations comprising MindNet. A truly broad-coverage parser is an essential component of this process, and it is the basis for extending it to other sources of information such as encyclopedias and text corpora. 4 Labeled, semantic relations The different types of </context>
<context position="4877" citStr="Wilks et al. 1996" startWordPosition="718" endWordPosition="721">s from dictionaries by researchers including Veronis and Ide (1990), Kozima and Furugori (1993), and Wilks et al. (1996). Labeled relations, while more difficult to obtain, provide greater power for resolving both structural attachment and word sense ambiguities. While many researchers have acknowledged the utility of labeled relations, they have been at times either unable (e.g., for lack of a sufficiently powerful parser) or unwilling (e.g., focused on purely statistical methods) to make the effort to obtain them. This deficiency limits the characterization of word pairs such as river/bank (Wilks et al. 1996) and write/pen (Veronis and Ide 1990) to simple relatedness, whereas the labeled relations of MindNet specify precisely the relations river—Part--&gt;bank and write—Means--&gt;pen. 5 Semantic relation structures The automatic extraction of semantic relations (or semrels) from a definition or example sentence for MindNet produces a hierarchical structure of these relations, representing the entire definition or sentence from which they came. Such structures are stored in their entirety in MindNet and provide crucial context for some of the procedures described in later sections of this paper. The sem</context>
</contexts>
<marker>Wilks, Slator, Guthrie, 1996</marker>
<rawString>Wilks, Y., B. Slator, and L. Guthrie. 1996. Electric words: Dictionaries, computers, and meanings. Cambridge, MA: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Word-sense disambiguation using statistical models of Roget&apos;s categories trained on large corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING92,</booktitle>
<pages>454--460</pages>
<contexts>
<context position="7558" citStr="Yarowsky (1992)" startWordPosition="1118" endWordPosition="1119">rom MRDs, including Veronis and Ide (1990) and Kozima and Furugori (1993), typically only implemented forward links (from headwords to their definition words) in those networks. Words were not related backward to any of the headwords whose definitions mentioned them, and words co-occurring in the same definition were not related directly. In the fully inverted structures stored in MindNet, however, all words are cross-linked, no matter where they appear. The massive network of inverted semrel structures contained in MindNet invalidates the criticism leveled against dictionary-based methods by Yarowsky (1992) and Ide and Veronis (1993) that LKBs created from MRDs provide spotty coverage of a language at best. Experiments described elsewhere (Richardson 1997) demonstrate the comprehensive coverage of the information contained in MindNet. Some statistics indicating the size (rounded to the nearest thousand) of the current version of MindNet and the processing time required to create it are provided in the table below. The definitions and example sentences are from the Longman Dictionary of Contemporary English (LDOCE) and the American Heritage Dictionary, ri Edition (AHD3). motorist: &amp;quot;a person who d</context>
<context position="11398" citStr="Yarowsky 1992" startWordPosition="1672" endWordPosition="1673">larity. One is based on identifying direct, paradigmatic relations between the words, such as Hypernym or Synonym. For example, paradigmatic relations in WordNet have been used by many to determine similarity, including Li et al. (1995) and Agirre and Rigau (1996). The other strategy is based on identifying syntagmatic relations with other words that similar words have in common. Syntagmatic strategies for determining similarity have often been based on statistical analyses of large corpora that yield clusters of words occurring in similar bigram and trigram contexts (e.g., Brown et al. 1992, Yarowsky 1992), as well as in similar predicateargument structure contexts (e.g., Grishman and Sterling 1994). There have been a number of attempts to combine paradigmatic and syntagmatic similarity strategies (e.g., Hearst and Grefenstette 1992, Resnik 1995). However, none of these has completely integrated both syntagmatic and paradigmatic information into a single repository, as is the case with MindNet. The MindNet similarity procedure is based on the top-ranked (by weight) semrel paths between words. For example, some of the top semrel paths in MindNet between pen and pencil, are shown below: penf--Mea</context>
</contexts>
<marker>Yarowsky, 1992</marker>
<rawString>Yarowsky, D. 1992. Word-sense disambiguation using statistical models of Roget&apos;s categories trained on large corpora. In Proceedings of COLING92, 454-460.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>