<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000190">
<note confidence="0.539154">
SENSEVAL-3: Third International Workshop on the Evaluation of Systems
for the Semantic Analysis of Text, Barcelona, Spain, July 2004
Association for Computational Linguistics
</note>
<title confidence="0.9971455">
Finding optimal parameter settings for high performance word sense
disambiguation
</title>
<author confidence="0.99858">
Cristian Grozea
</author>
<affiliation confidence="0.9970065">
Department of Computer Science
University of Bucharest
</affiliation>
<address confidence="0.799529">
Str. Academiei 14, 70109 Bucharest, Romania
</address>
<email confidence="0.991041">
chrisg@phobos.ro
</email>
<sectionHeader confidence="0.995417" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999717">
This article describes the four systems sent by the
author to the SENSEVAL-3 contest, the English
lexical sample task. The best recognition rate ob-
tained by one of these systems was 72.9% (fine
grain score).
</bodyText>
<sectionHeader confidence="0.5574695" genericHeader="method">
1 Introduction. RLSC algorithm, input
and output.
</sectionHeader>
<bodyText confidence="0.999833608695653">
This paper is not self-contained. The reader should
read first the paper of Marius Popescu (Popescu,
2004), paper that contains the full description the
base algorithm, Regularized Least Square Classifi-
cation (RLSC) applied to WSD.
Our systems used the feature extraction described
in (Popescu, 2004), with some differences.
Let us fix a word that is on the list of words we
must be able to disambiguate. Let be the number
of possible senses of this word.
Each instance of the WSD problem for this fixed
word is represented as an array of binary values
(features), divided by its Euclidian norm. The num-
ber of input features is different from one word to
another. The desired output for that array is another
binary array, having the length .
After the feature extraction, the WSD problem is
regarded as a linear regression problem. The equa-
tion of the regression is where each of the
lines of the matrix is an example and each line of
is an array of length containing zeros and
a single . The output of the trained model on
some particular input is an array of values that ide-
ally are just 0 or 1. Actually those values are never
exactly 0 and 1, so we are prepared to consider them
as an ”activation” of the sense recognizers and con-
sider that the most ”activated” (the sense with high-
est value) wins and gives the sense we decide on. In
other words, we consider the values an approxi-
mation of the true probabilities associated with each
sense.
The RLSC solution to this linear regression prob-
lem is
The first difference between our system and Mar-
ius Popescu’s RLSC-LIN is that two of the systems
(HTSA3 and HTSA4) use supplementary features,
obtained by multiplying up to three of the exist-
ing features, because they improved the accuracy on
Senseval-2.
Another difference is that the targets have values
0 and 1, while in the Marius Popescu’s RLSC-LIN
the targets have values -1 and 1. We see the output
values of the trained model as approximations of the
true probabilities of the senses.
The main difference is the postprocessing we ap-
ply after obtaining . It is explained below.
</bodyText>
<sectionHeader confidence="0.709639" genericHeader="method">
2 Adding parameters
</sectionHeader>
<bodyText confidence="0.99985255">
The obviously single parameter of the RLSC is .
Some improvement can be obtained using larger
values. After dropping the parser information from
features (when it became clear that we won’t have
those for Senseval-3) the improvements proved to
be too small. Therefore we fixed .
During the tests we performed it has been ob-
served that normalizing the models for each sense
(the columns of ) - that is dividing them by
their Euclidian norm - gives better results, at least
on Senseval-2 and don’t give too bad results on
Senseval-1 either. When you have a yes/no param-
eter like this one (that is normalizing or not the
columns of ), you don’t have too much room for
fine tuning. After some experimentation we decided
that the most promising way to convert this new dis-
crete parameter to a continuous one was to consider
that in both cases it was a division by , where
when we leave the model unchanged and
when we normalize the model columns.
</bodyText>
<sectionHeader confidence="0.903044" genericHeader="method">
3 Choosing the best value of the
parameters
</sectionHeader>
<bodyText confidence="0.999128333333333">
This is the procedure that has been employed to tune
the parameter until the recognition rate achieved
the best levels on SENSEVAL-1 and 2 data.
</bodyText>
<listItem confidence="0.974252428571429">
1. preprocess the input data - obtain the features
;
2. compute
3. for each from 0 to 1 with step 0.1
4. test the model (using in the post-
processing phase and then the scoring python
script)
</listItem>
<bodyText confidence="0.999674631578948">
At this point we were worried by the lack of any
explanation (and therefore the lack of any guaran-
tee about performance on SENSEVAL-3). After
some thinking on the strengths and weaknesses of
RLSC it became apparent that RLSC implicitly in-
corporates a Bayesian style reasoning. That is, the
senses most frequent in the training data lead to
higher norm models, having thus a higher aposte-
riori probability. Experimental evidence was ob-
tained by plotting graphs with the sense frequencies
near graphs with the norms of the model’s columns.
If you consider this, then the correction done was
more or less dividing implicitly by the empiric fre-
quency of the senses in the training data. So, we
switched to dividing the columns by the observed
frequency of the -th sense instead of the norm
. This lead to an improvement on SENSEVAL-
2, so this is our base system HTSA1:
Test procedure for HTSA1:
</bodyText>
<listItem confidence="0.9083272">
1. Postprocessing: correct for =1.. the model
by doing
For each test input do 2,3
2. Compute the output for the input
3. Find the maximum component of . Its posi-
</listItem>
<bodyText confidence="0.902237">
tion is the label returned by the algorithm for the the
input .
Please observe that, because of the linearity, the
correction can be performed on instead of , just
after the step 2 : . For this reason we call
this correction ”postprocessing”.
</bodyText>
<sectionHeader confidence="0.9706305" genericHeader="method">
4 Description of the systems.
Performance.
</sectionHeader>
<bodyText confidence="0.999948625">
Here is a very short description of our systems. It
describes what they have in common and what is
different, as well which is their performance level
(recognition rate).
There are four flavors of the same algorithm,
based on RLSC. They differ by the preprocessing
and by the postprocessing done (name and explana-
tion is under each graphic).
</bodyText>
<figure confidence="0.9947671">
0.8
0.75
0.7
0.65
0.6
0.55
0.5
0.45
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
alpha
</figure>
<bodyText confidence="0.483685">
HTSA1: implicit correction of the frequencies,
by dividing the output confidences of the senses by
the ; The graphic shows how the recog-
nition rate depends on on SENSEVAL-1.
</bodyText>
<figure confidence="0.966255545454546">
0.7
0.65
0.6
0.55
0.5
0.45
0.4
0.35
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
alpha
HTSA1 on SENSEVAL-2 - the recognition rated
depicted as a function of
0.666
0.664
0.662
0.66
0.658
0.656
0.654
0.652
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4
alpha
</figure>
<bodyText confidence="0.991872666666667">
HTSA2: explicit correction of the frequencies,
by multiplying the output confidences by a certain
decreasing function of frequency, that tries to ap-
proximate the effect of the postprocessing done by
HTSA1; here the performance on SENSEVAL-2 as
a function of .
</bodyText>
<equation confidence="0.771006666666667">
HTSA1 performance on SENSEVAL−1
HTSA1 performance on SENSEVAL−2
HTSA2 performance on SENSEVAL−2
</equation>
<bodyText confidence="0.9941905">
HTSA3: like HTSA1, with a preprocessing that
adds supplementary features by multiplying some
of the existing ones; here the performance on
SENSEVAL-2 as a function of .
The supplementary features added to HTSA3 and
HTSA4 are all products of two and three local con-
text features. This was meant to supply the linear
regression with some nonlinear terms, giving thus
the algorithm the possibility to use conjunctions.
Was our best result lucky? Here is the perfor-
mance graph of HTSA3 on SENSEVAL-3 as a func-
tion of . As we can see, any between 0.2 and 0.3
would have given accuracies between and
.
ing described above. Here the performance on
SENSEVAL-2 as a function of .
</bodyText>
<equation confidence="0.50336775">
0.72
0.718
0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6
alpha
</equation>
<bodyText confidence="0.971943216216216">
The performance of HTSA4 on SENSEVAL-3 as
a function of .
What can be seen on this graphic is that
was not such a good choice for SENSEVAL-3. In-
stead, would have achieved a recogni-
tion rate of . In other words, the best value
of on SENSEVAL-2 is not necessary the best on
SENSEVAL-3. The next section discusses alterna-
tive ways of ”guessing” the best values of the pa-
rameters, as well as why they won’t work in this
case.
5 Cross-validation. Possible explanations
of the results
The common idea of HTSA 1,2,3 and 4 is that a
slight departure from the Bayes apriori frequencies
improves the accuracy. This is done here by post-
processing and works on any method that produces
probabilities/credibilities for all word senses. The
degree of departure from the Bayes apriori frequen-
cies can be varied and has been tuned on Senseval-1
and Senseval-2 data until the optimum value
has been determined.
Of course, there was still no guarantee on how
good will be the performance on SENSEVAL-3.
The natural idea is to apply cross-validation to de-
termine the best using the current training set.
We tried that, but a very strange thing could be ob-
served. On both SENSEVAL-1 and SENSEVAL-
2 the cross-validation indicated that values of
around should have been better than .
We see this as an indication that the distribution
of frequencies on the test set does not fully match
with the one of the train set. This could be an
explanation about why it is better to depart from
the Bayesian style and to go toward the maximum
verosimility method. We think that this is exactly
what we did.
</bodyText>
<figure confidence="0.991924055555556">
0.73
0.728
0.726
0.724
0.722
0.72
0.718
0.716
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4
alpha
HTSA3 performance on SENSEVAL−3
0.675
0.67
0.665
0.66
0.655
0.65
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4
alpha
HTSA3 performance on SENSEVAL−2
0.675
0.67
HTSA4 performance on SENSEVAL−2
0.665
0.66
0.655
0.65
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4
alpha
HTSA4: like HTSA2, with the preprocess-
HTSA4 performance on SENSEVAL−3 0.732
0.73
0.728
0.726
0.724
0.722
</figure>
<bodyText confidence="0.975333125">
Initially we only had HTSA1 and HTSA3. By
looking at the graph of the correction done by divid-
ing by , reproduced below in red, we
observed that it tends to give more chances to the
weakly represented senses. To test this hypothesis
we built an explicit correction, piecewise linear, also
reproduced below on the same graphic. Thus we
have obtained HTSA2 and HTSA4. In their case,
is the position of the joining point. Those performed
close to HTSA1 and HTSA3, so we have experi-
mental evidence that increasing the apriori proba-
bilities of the lower frequency senses gives better
recognition rates.
observed frequency
Red: Implicit correction (HTSA 1, 3); Blue: Ex-
plicit correction (HTSA 2, 4)
</bodyText>
<sectionHeader confidence="0.985952" genericHeader="conclusions">
6 Conclusions. Further work.
</sectionHeader>
<bodyText confidence="0.9999825">
RLSC proved to be a very powerful learning model.
We also believe that tuning the parameters of a
model is a must, even if you have to invent parame-
ters first. We think that the way we have proceeded
here with can be applied to other models, as a
simple and direct post processing. Of course the
right value of has to be found case by case. We
would suggest everyone who participated with sys-
tems that produce Bayesian-like class probabilities
to try to apply this postprocessing to their systems.
</bodyText>
<sectionHeader confidence="0.994371" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.515168666666667">
Marius Popescu. 2004. Regularized least-squares
classification for word sense disambiguation. In
Proceedings of SENSEVAL-3, page N/A.
</reference>
<figure confidence="0.9991394">
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
correction factor
1.9
1.8
1.7
1.6
1.5
1.4
1.3
1.2
1.1
2
1
implicit correction
explicit correction
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.264559">
<note confidence="0.683621">SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July 2004 Association for Computational Linguistics</note>
<title confidence="0.9680805">Finding optimal parameter settings for high performance word sense disambiguation</title>
<author confidence="0.995906">Cristian</author>
<affiliation confidence="0.999517">Department of Computer University of</affiliation>
<address confidence="0.763276">Str. Academiei 14, 70109 Bucharest,</address>
<email confidence="0.991264">chrisg@phobos.ro</email>
<abstract confidence="0.993">This article describes the four systems sent by the author to the SENSEVAL-3 contest, the English lexical sample task. The best recognition rate obtained by one of these systems was 72.9% (fine grain score).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marius Popescu</author>
</authors>
<title>Regularized least-squares classification for word sense disambiguation.</title>
<date>2004</date>
<booktitle>In Proceedings of SENSEVAL-3,</booktitle>
<pages>page N/A.</pages>
<contexts>
<context position="762" citStr="Popescu, 2004" startWordPosition="107" endWordPosition="108"> Computational Linguistics Finding optimal parameter settings for high performance word sense disambiguation Cristian Grozea Department of Computer Science University of Bucharest Str. Academiei 14, 70109 Bucharest, Romania chrisg@phobos.ro Abstract This article describes the four systems sent by the author to the SENSEVAL-3 contest, the English lexical sample task. The best recognition rate obtained by one of these systems was 72.9% (fine grain score). 1 Introduction. RLSC algorithm, input and output. This paper is not self-contained. The reader should read first the paper of Marius Popescu (Popescu, 2004), paper that contains the full description the base algorithm, Regularized Least Square Classification (RLSC) applied to WSD. Our systems used the feature extraction described in (Popescu, 2004), with some differences. Let us fix a word that is on the list of words we must be able to disambiguate. Let be the number of possible senses of this word. Each instance of the WSD problem for this fixed word is represented as an array of binary values (features), divided by its Euclidian norm. The number of input features is different from one word to another. The desired output for that array is anoth</context>
</contexts>
<marker>Popescu, 2004</marker>
<rawString>Marius Popescu. 2004. Regularized least-squares classification for word sense disambiguation. In Proceedings of SENSEVAL-3, page N/A.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>