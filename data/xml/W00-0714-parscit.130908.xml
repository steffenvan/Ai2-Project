<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007142">
<note confidence="0.849103">
In: Proceedings of CoNLL-2000 and LLL-2000, pages 79-82, Lisbon, Portugal, 2000.
</note>
<title confidence="0.5774445">
Using Perfect Sampling in Parameter Estimation of a Whole
Sentence Maximum Entropy Language Model*
</title>
<author confidence="0.591566">
F. Amayat and .1. M. Benedi
</author>
<affiliation confidence="0.576959">
Departamento de Sistemas Informaticos y ComputaciOn
</affiliation>
<address confidence="0.4692895">
Universidad Politecnica de Valencia
Camino de vera s/n, 46022-Valencia (Spain)
</address>
<email confidence="0.982854">
Ifamaya, jbenedil@dsic.upv.es
</email>
<sectionHeader confidence="0.996103" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999647823529412">
The Maximum Entropy principle (ME) is an ap-
propriate framework for combining information
of a diverse nature from several sources into the
same language model. In order to incorporate
long-distance information into the ME frame-
work in a language model, a Whole Sentence
Maximum Entropy Language Model (WSME)
could be used. Until now MonteCarlo Markov
Chains (MCMC) sampling techniques has been
used to estimate the paramenters of the WSME
model. In this paper, we propose the applica-
tion of another sampling technique: the Perfect
Sampling (PS). The experiment has shown a re-
duction of 30% in the perplexity of the WSME
model over the trigram model and a reduc-
tion of 2% over the WSME model trained with
MCMC.
</bodyText>
<sectionHeader confidence="0.998522" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999729285714286">
The language modeling problem may be defined
as the problem of calculating the probability of
a string, p(w) = p(wi, , wn). The probability
p(w) is usually calculated via conditional prob-
abilities. The n-gram model is one of the most
widely used language models. The power of the
n-gram, model resides in its simple formulation
and the ease of training. On the other hand, n-
grams only take into account local information,
and important long-distance information con-
tained in the string w1 wn cannot be modeled
by it. In an attempt to supplement the local in-
formation with long-distance information, hy-
brid models have been proposed such us (Belle-
</bodyText>
<footnote confidence="0.903391">
* This work has been partially supported by the Spanish
CYCIT under contract (TIC98/0423-006).
t Granted by Universidad del Cauca, Popayan (Colom-
bia)
</footnote>
<bodyText confidence="0.9927834">
garda, 1998; Chelba, 1998; Benedi and Sanchez,
2000).
The Maximum Entropy principle is an ap-
propriate framework for combining information
of a diverse nature from several sources into
the same model: the Maximum Entropy model
(ME) (Rosenfeld, 1996). The information is in-
corporated as features which are submitted to
constraints. The conditional form of the ME
model is:
</bodyText>
<equation confidence="0.9985725">
P(YIX) = 1 Z eE7=1 At ft(x&apos;Y)
(X)
</equation>
<bodyText confidence="0.999797666666667">
where Ai are the parameters to be learned (one
for each feature), the fi are usually characteris-
tic functions which are associated to the fea-
tures and Z(x) = Ey exp{ali Azfi(x, y)} is
the normalization constant. The main advan-
tages of ME are its flexibility (local and global
information can be included in the model) and
its simplicity. The drawbacks are that the para-
menter&apos;s estimation is computationally expen-
sive, specially the evaluation of the normaliza-
tion constant Z(x) and that the grammatical
information contained in the sentence is poorly
encoded in the conditional framework. This is
due to the assumption of independence in the
conditional events: in the events in the state
space, only a part of the information contained
in the sentence influences de calculation of the
probability (Ristad, 1998).
</bodyText>
<sectionHeader confidence="0.9960175" genericHeader="method">
2 Whole Sentence Maximum
Entropy Language Model
</sectionHeader>
<bodyText confidence="0.9961062">
An alternative to combining local, long-distance
and structural information contained in the
sentence, within the maximum entropy frame-
work, is the Whole Sentence Maximum En-
tropy model (WSME) (Rosenfeld, 1997). The
</bodyText>
<equation confidence="0.922226">
(1)
</equation>
<page confidence="0.973981">
79
</page>
<bodyText confidence="0.9994955">
WSME is based in the calculation of unre-
stricted ME probability p(w) of a whole sen-
tence w = w1 wn. The probability distribu-
tion is the distribution p that has the maximum
entropy relative to a prior distribution po (in
other words: the distribution that minimize de
divergence D(PliP0)) (Della Pietra et al., 1995).
The distribution p is given by:
</bodyText>
<equation confidence="0.9482105">
, 1
P(w) =—Po(w)eE7,--iAL(w)
</equation>
<bodyText confidence="0.999988538461539">
where A, and L are the same as in (1). Z is
a (global) normalization constant and po is a
prior proposal distribution. The A, and Z are
unknown and must be learned.
The parameters A, may be interpreted as be-
ing weights of the features and could be learned
using some type of iterative algorithm. We have
used the Improved Iterative Scaling algorithm
(IIS) (Berger et al., 1996). In each iteration of
the ITS, we find a Si value such that adding this
value to Ai parameters, we obtain an increase
in the the log-likelihood. The 8i values are ob-
tained as the solution of the m equations:
</bodyText>
<equation confidence="0.987924">
EP(w)L(w)e&amp;quot; (w) E j3(w)f(w) = 0
(3)
</equation>
<bodyText confidence="0.99676364">
where i = 1, ,m, f# (w) =1f(w) and
Si is a training corpus. Because the domain of
WSME is not restricted to a part of the sen-
tence (context) as in the conditional case, it
allows us to combine global structural syntac-
tic information which is contained in the sen-
tence with local and other kinds of long range
information such us triggers. Furthermore, the
WSME model is easier to train than the con-
ditional one, because in the WSME model we
don&apos;t need to estimate the normalization con-
stant Z during the training time. In contrast,
for each event (x, y) in the training corpus, we
have to calculate Z(x) in each iteration of the
MEG model.
The main drawbacks of the WSME model are
its integration with other modules and the cal-
culation of the expected value in the left part of
equation (3), because the event space is huge.
Here we focus on the problem of calculating
the expected value in (3). The first sum in (3)
is the expected value of fie(52f#, and it is obvi-
ously not possible to sum over all the sentences.
However, we can estimate the mean by using
the empirical expected value:
</bodyText>
<table confidence="0.767282">
E [fie] 1 m (4)
P —m E fi(si)e6if#(sj)
j=1
</table>
<bodyText confidence="0.998979666666667">
where Si, ,SM is a random sample from p(w).
Once the parameters have been learned it is pos-
sible to estimate the value of the normalization
</bodyText>
<equation confidence="0.769106">
constant, because Z = eEzin=i Ad;(w)Po(w) =
m A f
[ea=1 &amp;quot;] , and it can be estimated by
</equation>
<bodyText confidence="0.999884285714286">
means of the sample mean with respect to po
(Chen and Rosenfeld, 1999).
In each iteration of ITS, the calculation of (4)
requires sampling from a probability distribu-
tion which is partially known (Z is unknown),
so the classical sampling techniques are not use-
ful. In the literature, there are some meth-
ods like the MonteCarlo Markov Chain meth-
ods (MCMC) that generate random samples
from p(w) (Sahu, 1997; Tierney, 1994). With
the MCMC methods, we can simulate a sample
approximately from the probability distribution
and then use the sample to estimate the desired
expected value in (4).
</bodyText>
<sectionHeader confidence="0.970644" genericHeader="method">
3 Perfect Sampling
</sectionHeader>
<bodyText confidence="0.999563956521739">
In this paper, we propose the application of an-
other sampling technique in the parameter esti-
mation process of the WSME model which was
introduced by Propp and Wilson (Propp and
Wilson, 1996): the Perfect Sampling (PS). The
PS method produces samples from the exact
limit distribution and, thus, the sampling mean
given in (4) is less biased than the one obtained
with the MCMC methods. Therefore, we can
obtain better estimations of the parameters Ai.
In PS, we obtain a sample from the limit
distribution of an ergodic Markov Chain X =
{X„; n &gt; 0}, taking values in the state space S
(in the WSME case, the state space is the set of
possible sentences). Because of the ergodicity,
if the transition law of X is P(x, A) := P(Xn E
AlXn_i = x), then it has a limit distribution it,
that is: if we start a path on the chain in any
state at time n = 0, then as n oo,
The first algorithm of the family of PS was pre-
sented by Propp and Wilson (Propp and Wil-
son, 1996) under the name Coupling From the
Past (CFP) and is as follows: start a path in
</bodyText>
<figure confidence="0.478619">
(2)
</figure>
<page confidence="0.98257">
80
</page>
<bodyText confidence="0.9995913125">
every state of S at some time (—T) in the past
such that at time n = 0, all the paths collapse
to a unique value (due to the ergodicity). This
value is a sample element. In the majority of
cases, the state space is huge, so attempting
to begin a path in every state is not practical.
Thus, we can define a partial stochastic order
in the state space and so we only need start two
paths: one in the minimum and one in the maxi-
mum. The two paths collapse at time 71 = 0 and
the value of the coalescence state is a sample
element of 7r. The CFP algorithm first deter-
mines the time T to start and then runs the two
paths from time (—T) to 0. Information about
PS methods may be consulted in (Corcoran and
Tweedie, 1998; Propp and Wilson, 1998).
</bodyText>
<sectionHeader confidence="0.999067" genericHeader="evaluation">
4 Experimental work
</sectionHeader>
<bodyText confidence="0.999912866666667">
In this work, we have made preliminary exper-
iments using PS in the estimation of the ex-
pected value (4) during the learning of the pa-
rameters of a WSME model. We have imple-
mented the Cai algorithm (Cai, 1999) to obtain
perfect samples. The Cai algorithm has the ad-
vantage that it doesn&apos;t need the definition of the
partial order.
The experiments were carried out using a
pseudonatural corpus: &amp;quot;the traveler task&amp;quot;.
The traveler task consists in dialogs between
travelers and hotel clerks. The size of the vocab-
ulary is 693 words. The training set has 490,000
sentences and 4,748,690 words. The test set has
10,000 sentences and 97,153 words.
Three kinds of features were used in the
WSME model: n-grams (1-grams, 2-grams, 3-
grams), distance 2 n-grams (d2-2-grams, d2-3-
grams) and triggers. The proposal prior distri-
bution used was a trigram model.
We trained WSME models with different sets
of features using the two sampling techniques:
MCMC and PS. We measured the perplexity
(PP) of each of the models and obtained the
percentage of improvement in the PP with re-
spect to a trigram base-line model (see table 1).
The first model used MCMC techniques (specif-
ically the Independence Metropolis-Hastings al-
gorithm (IMH) 2) and features of n-grams and
distance 2 n-grams. The second model used a
</bodyText>
<footnote confidence="0.759022333333333">
lEuTrans ESPRIT-LTR Project 20268
2IMH has been reported recently as the most useful
MCMC algorithm used in the WSME training process.
</footnote>
<table confidence="0.999269166666667">
Method PP % Improvement
IMH 3.37115 28
PS 3.46336 26
IMH-T 3.37198 28
PS-T 3.26964 30
Trigram 4.66975 -
</table>
<tableCaption confidence="0.999676">
Table 1: Test set perplexity of the WSME
</tableCaption>
<bodyText confidence="0.997536235294118">
model over the traveler task corpus: IMH with
features of n-grams and d-n-grams (IMH), PS
with n-grams and d-n-grams (PS) IMH with
triggers (IMH-T), PS with triggers (PS-T). The
base-line model is a trigram model (Trigram)
PS algorithm and features of n-grams and dis-
tance 2 n-grams. The third model used the IMH
algorithm and features of triggers. The fourth
used PS and features of triggers. Finally, in or-
der to compare with the classical methods, we
included the trigram base-line model.
In all cases, the WSME had a better perfor-
mance than the n-gram model. From the results
in Table 1, we see that the use of features of
triggers improves the performance of the model
more than the use of n-gram features, this may
be due to the correlation between the triggers
and the n-grams, the n-gram information has
been absorbed by the prior distribution and di-
minishes the effects of the feature of n-grams.
We believe this is the reason why PS-T in Ta-
ble 1 is better than PS. We also see how IMH
and IHM-T shows the same improvement, i.e.
the use of triggers does not seem improve the
perplexity of the model but, this may be due
to the sampling technique: the parameter val-
ues depends on the estimation of an expected
value, and the estimation depends on the sam-
pling. Finally, the PS-T has better perplexity
than the IMH-T. The only difference between
both of these is the sampling technique,neither
of then has the correlation influence in the fea-
tures, so we think that the improvement may
be due to the sampling technique.
</bodyText>
<sectionHeader confidence="0.979856" genericHeader="conclusions">
5 Conclusion and future works
</sectionHeader>
<bodyText confidence="0.999951">
We have presented a different approach to the
sampling step needed in the parameter estima-
tion of a WSME model. Using this technique,
we have obtained a reduction of 30% in the per-
plexity of the WSME model over the base-line
</bodyText>
<page confidence="0.995116">
81
</page>
<bodyText confidence="0.999943636363636">
trigram model and an improvement of 2% over
the model trained with MCMC techniques. We
are extending our experiments to a major cor-
pus: the Wall Street Journal corpus and using a
set of features which is more general, including
features that reflect the global structure of the
sentence.
We are working on introducing the grammat-
ical information contained into the sentence to
the model; we believe that such information im-
proves the quality of the model significantly.
</bodyText>
<sectionHeader confidence="0.996483" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999340705882353">
J. R. Bellegarda. 1998. A multispan language
modeling framework for large vocabulary speech
recognition. IEEE Transactions on Speech and
Audio Processing, 6 (5):456-467.
J.M. Benedi and J.A. Sanchez. 2000. Combination
of n-grams and stochastic context-free grammars
for language modeling. International conference
on computational linguistics (COLIN-ACL).
A.L. Berger, V.J. Della Pietra, and S.A. Della
Pietra. 1996. A Maximum Entropy approach to
natural language processing. Computational Lin-
guistics, 22(1):39-72.
H. Cai. 1999. Exact Sampling using auxiliary vari-
ables. Statistical Computing Section, ASA Pro-
ceedings.
C. Chelba. 1998. A structured Language Model.
PhD Dissertation Proposal, The Johns Hopkins
University.
S. Chen and R. Rosenfeld. 1999. Efficient sampling
and feature selection in whole sentence maximum
entropy language models. Proc. IEEE Int. Con-
ference on Acoustics, Speech and Signal Process-
ing (ICASSP).
J.N. Corcoran and R.L. Tweedie. 1998. Perfect sam-
pling for Independent Metropolis-Hastings chains.
preprint. Colorado State University.
S. Della Pietra, V. Della Pietra, and J. Lafferty.
1995. Inducing features of random fields. Tech-
nical Report CMU-CS-95-144, Carnegie Mellon
University.
J. G. Propp and D. B. Wilson. 1996. Exact sampling
with coupled markov chains and applications to
statistical mechanics. Random Structures and Al-
gorithms, 9:223-252.
J. A. Propp and D. B. Wilson. 1998. Coupling from
the Past: User&apos;s Guide. Dimacs series in discrete
Mathematics and Theoretical Computer Science,
pages 181-192.
E. S. Ristad, 1998. Maximum Entropy Modeling
Toolkit, Version 1.6 Beta.
R. Rosenfeld. 1996. A Maximum Entropy approach
to adaptive statistical language modeling. Com-
puter Speech and Language, 10:187-228.
R. Rosenfeld. 1997. A whole sentence Maximum En-
tropy language model. IEEE workshop on Speech
Recognition and Understanding.
S. Sahu. 1997. Bayesian data analysis. Technical re-
port, School of Mathematics, University of Walles.
L. Tierney. 1994. Markov chains for exploring pos-
terior distributions. The Annals of Statistics,
22:1701-1762.
</reference>
<page confidence="0.999125">
82
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.278522">
<note confidence="0.990632">of CoNLL-2000 and LLL-2000, 79-82, Lisbon, Portugal, 2000.</note>
<title confidence="0.9795285">Using Perfect Sampling in Parameter Estimation of a Whole Sentence Maximum Entropy Language Model*</title>
<author confidence="0.795736">F Amayat</author>
<affiliation confidence="0.933451">Departamento de Sistemas Informaticos y Universidad Politecnica de</affiliation>
<address confidence="0.775276">Camino de vera s/n, 46022-Valencia</address>
<email confidence="0.634034">Ifamaya,jbenedil@dsic.upv.es</email>
<abstract confidence="0.954021722222222">The Maximum Entropy principle (ME) is an appropriate framework for combining information of a diverse nature from several sources into the same language model. In order to incorporate long-distance information into the ME framework in a language model, a Whole Sentence Maximum Entropy Language Model (WSME) could be used. Until now MonteCarlo Markov Chains (MCMC) sampling techniques has been used to estimate the paramenters of the WSME model. In this paper, we propose the application of another sampling technique: the Perfect Sampling (PS). The experiment has shown a reof the perplexity of the WSME model over the trigram model and a reduction of 2% over the WSME model trained with MCMC.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J R Bellegarda</author>
</authors>
<title>A multispan language modeling framework for large vocabulary speech recognition.</title>
<date>1998</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>6</volume>
<pages>5--456</pages>
<marker>Bellegarda, 1998</marker>
<rawString>J. R. Bellegarda. 1998. A multispan language modeling framework for large vocabulary speech recognition. IEEE Transactions on Speech and Audio Processing, 6 (5):456-467.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Benedi</author>
<author>J A Sanchez</author>
</authors>
<title>Combination of n-grams and stochastic context-free grammars for language modeling.</title>
<date>2000</date>
<booktitle>International conference on computational linguistics (COLIN-ACL).</booktitle>
<contexts>
<context position="1951" citStr="Benedi and Sanchez, 2000" startWordPosition="307" endWordPosition="310">f the most widely used language models. The power of the n-gram, model resides in its simple formulation and the ease of training. On the other hand, ngrams only take into account local information, and important long-distance information contained in the string w1 wn cannot be modeled by it. In an attempt to supplement the local information with long-distance information, hybrid models have been proposed such us (Belle* This work has been partially supported by the Spanish CYCIT under contract (TIC98/0423-006). t Granted by Universidad del Cauca, Popayan (Colombia) garda, 1998; Chelba, 1998; Benedi and Sanchez, 2000). The Maximum Entropy principle is an appropriate framework for combining information of a diverse nature from several sources into the same model: the Maximum Entropy model (ME) (Rosenfeld, 1996). The information is incorporated as features which are submitted to constraints. The conditional form of the ME model is: P(YIX) = 1 Z eE7=1 At ft(x&apos;Y) (X) where Ai are the parameters to be learned (one for each feature), the fi are usually characteristic functions which are associated to the features and Z(x) = Ey exp{ali Azfi(x, y)} is the normalization constant. The main advantages of ME are its f</context>
</contexts>
<marker>Benedi, Sanchez, 2000</marker>
<rawString>J.M. Benedi and J.A. Sanchez. 2000. Combination of n-grams and stochastic context-free grammars for language modeling. International conference on computational linguistics (COLIN-ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>V J Della Pietra</author>
<author>S A Della Pietra</author>
</authors>
<title>A Maximum Entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="4144" citStr="Berger et al., 1996" startWordPosition="672" endWordPosition="675"> distribution p that has the maximum entropy relative to a prior distribution po (in other words: the distribution that minimize de divergence D(PliP0)) (Della Pietra et al., 1995). The distribution p is given by: , 1 P(w) =—Po(w)eE7,--iAL(w) where A, and L are the same as in (1). Z is a (global) normalization constant and po is a prior proposal distribution. The A, and Z are unknown and must be learned. The parameters A, may be interpreted as being weights of the features and could be learned using some type of iterative algorithm. We have used the Improved Iterative Scaling algorithm (IIS) (Berger et al., 1996). In each iteration of the ITS, we find a Si value such that adding this value to Ai parameters, we obtain an increase in the the log-likelihood. The 8i values are obtained as the solution of the m equations: EP(w)L(w)e&amp;quot; (w) E j3(w)f(w) = 0 (3) where i = 1, ,m, f# (w) =1f(w) and Si is a training corpus. Because the domain of WSME is not restricted to a part of the sentence (context) as in the conditional case, it allows us to combine global structural syntactic information which is contained in the sentence with local and other kinds of long range information such us triggers. Furthermore, the</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A.L. Berger, V.J. Della Pietra, and S.A. Della Pietra. 1996. A Maximum Entropy approach to natural language processing. Computational Linguistics, 22(1):39-72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cai</author>
</authors>
<title>Exact Sampling using auxiliary variables.</title>
<date>1999</date>
<journal>Statistical Computing Section, ASA Proceedings.</journal>
<contexts>
<context position="8376" citStr="Cai, 1999" startWordPosition="1473" endWordPosition="1474"> only need start two paths: one in the minimum and one in the maximum. The two paths collapse at time 71 = 0 and the value of the coalescence state is a sample element of 7r. The CFP algorithm first determines the time T to start and then runs the two paths from time (—T) to 0. Information about PS methods may be consulted in (Corcoran and Tweedie, 1998; Propp and Wilson, 1998). 4 Experimental work In this work, we have made preliminary experiments using PS in the estimation of the expected value (4) during the learning of the parameters of a WSME model. We have implemented the Cai algorithm (Cai, 1999) to obtain perfect samples. The Cai algorithm has the advantage that it doesn&apos;t need the definition of the partial order. The experiments were carried out using a pseudonatural corpus: &amp;quot;the traveler task&amp;quot;. The traveler task consists in dialogs between travelers and hotel clerks. The size of the vocabulary is 693 words. The training set has 490,000 sentences and 4,748,690 words. The test set has 10,000 sentences and 97,153 words. Three kinds of features were used in the WSME model: n-grams (1-grams, 2-grams, 3- grams), distance 2 n-grams (d2-2-grams, d2-3- grams) and triggers. The proposal prio</context>
</contexts>
<marker>Cai, 1999</marker>
<rawString>H. Cai. 1999. Exact Sampling using auxiliary variables. Statistical Computing Section, ASA Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chelba</author>
</authors>
<title>A structured Language Model.</title>
<date>1998</date>
<tech>PhD</tech>
<institution>Dissertation Proposal, The Johns Hopkins University.</institution>
<contexts>
<context position="1924" citStr="Chelba, 1998" startWordPosition="305" endWordPosition="306">model is one of the most widely used language models. The power of the n-gram, model resides in its simple formulation and the ease of training. On the other hand, ngrams only take into account local information, and important long-distance information contained in the string w1 wn cannot be modeled by it. In an attempt to supplement the local information with long-distance information, hybrid models have been proposed such us (Belle* This work has been partially supported by the Spanish CYCIT under contract (TIC98/0423-006). t Granted by Universidad del Cauca, Popayan (Colombia) garda, 1998; Chelba, 1998; Benedi and Sanchez, 2000). The Maximum Entropy principle is an appropriate framework for combining information of a diverse nature from several sources into the same model: the Maximum Entropy model (ME) (Rosenfeld, 1996). The information is incorporated as features which are submitted to constraints. The conditional form of the ME model is: P(YIX) = 1 Z eE7=1 At ft(x&apos;Y) (X) where Ai are the parameters to be learned (one for each feature), the fi are usually characteristic functions which are associated to the features and Z(x) = Ey exp{ali Azfi(x, y)} is the normalization constant. The main</context>
</contexts>
<marker>Chelba, 1998</marker>
<rawString>C. Chelba. 1998. A structured Language Model. PhD Dissertation Proposal, The Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
<author>R Rosenfeld</author>
</authors>
<title>Efficient sampling and feature selection in whole sentence maximum entropy language models.</title>
<date>1999</date>
<booktitle>Proc. IEEE Int. Conference on Acoustics, Speech and Signal Processing (ICASSP).</booktitle>
<contexts>
<context position="5819" citStr="Chen and Rosenfeld, 1999" startWordPosition="989" endWordPosition="992">vent space is huge. Here we focus on the problem of calculating the expected value in (3). The first sum in (3) is the expected value of fie(52f#, and it is obviously not possible to sum over all the sentences. However, we can estimate the mean by using the empirical expected value: E [fie] 1 m (4) P —m E fi(si)e6if#(sj) j=1 where Si, ,SM is a random sample from p(w). Once the parameters have been learned it is possible to estimate the value of the normalization constant, because Z = eEzin=i Ad;(w)Po(w) = m A f [ea=1 &amp;quot;] , and it can be estimated by means of the sample mean with respect to po (Chen and Rosenfeld, 1999). In each iteration of ITS, the calculation of (4) requires sampling from a probability distribution which is partially known (Z is unknown), so the classical sampling techniques are not useful. In the literature, there are some methods like the MonteCarlo Markov Chain methods (MCMC) that generate random samples from p(w) (Sahu, 1997; Tierney, 1994). With the MCMC methods, we can simulate a sample approximately from the probability distribution and then use the sample to estimate the desired expected value in (4). 3 Perfect Sampling In this paper, we propose the application of another sampling</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>S. Chen and R. Rosenfeld. 1999. Efficient sampling and feature selection in whole sentence maximum entropy language models. Proc. IEEE Int. Conference on Acoustics, Speech and Signal Processing (ICASSP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J N Corcoran</author>
<author>R L Tweedie</author>
</authors>
<title>Perfect sampling for Independent Metropolis-Hastings chains.</title>
<date>1998</date>
<tech>preprint.</tech>
<institution>Colorado State University.</institution>
<contexts>
<context position="8121" citStr="Corcoran and Tweedie, 1998" startWordPosition="1424" endWordPosition="1427">llapse to a unique value (due to the ergodicity). This value is a sample element. In the majority of cases, the state space is huge, so attempting to begin a path in every state is not practical. Thus, we can define a partial stochastic order in the state space and so we only need start two paths: one in the minimum and one in the maximum. The two paths collapse at time 71 = 0 and the value of the coalescence state is a sample element of 7r. The CFP algorithm first determines the time T to start and then runs the two paths from time (—T) to 0. Information about PS methods may be consulted in (Corcoran and Tweedie, 1998; Propp and Wilson, 1998). 4 Experimental work In this work, we have made preliminary experiments using PS in the estimation of the expected value (4) during the learning of the parameters of a WSME model. We have implemented the Cai algorithm (Cai, 1999) to obtain perfect samples. The Cai algorithm has the advantage that it doesn&apos;t need the definition of the partial order. The experiments were carried out using a pseudonatural corpus: &amp;quot;the traveler task&amp;quot;. The traveler task consists in dialogs between travelers and hotel clerks. The size of the vocabulary is 693 words. The training set has 490</context>
</contexts>
<marker>Corcoran, Tweedie, 1998</marker>
<rawString>J.N. Corcoran and R.L. Tweedie. 1998. Perfect sampling for Independent Metropolis-Hastings chains. preprint. Colorado State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>J Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1995</date>
<tech>Technical Report CMU-CS-95-144,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="3704" citStr="Pietra et al., 1995" startWordPosition="593" endWordPosition="596">of the probability (Ristad, 1998). 2 Whole Sentence Maximum Entropy Language Model An alternative to combining local, long-distance and structural information contained in the sentence, within the maximum entropy framework, is the Whole Sentence Maximum Entropy model (WSME) (Rosenfeld, 1997). The (1) 79 WSME is based in the calculation of unrestricted ME probability p(w) of a whole sentence w = w1 wn. The probability distribution is the distribution p that has the maximum entropy relative to a prior distribution po (in other words: the distribution that minimize de divergence D(PliP0)) (Della Pietra et al., 1995). The distribution p is given by: , 1 P(w) =—Po(w)eE7,--iAL(w) where A, and L are the same as in (1). Z is a (global) normalization constant and po is a prior proposal distribution. The A, and Z are unknown and must be learned. The parameters A, may be interpreted as being weights of the features and could be learned using some type of iterative algorithm. We have used the Improved Iterative Scaling algorithm (IIS) (Berger et al., 1996). In each iteration of the ITS, we find a Si value such that adding this value to Ai parameters, we obtain an increase in the the log-likelihood. The 8i values </context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1995</marker>
<rawString>S. Della Pietra, V. Della Pietra, and J. Lafferty. 1995. Inducing features of random fields. Technical Report CMU-CS-95-144, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Propp</author>
<author>D B Wilson</author>
</authors>
<title>Exact sampling with coupled markov chains and applications to statistical mechanics. Random Structures and Algorithms,</title>
<date>1996</date>
<pages>9--223</pages>
<contexts>
<context position="6549" citStr="Propp and Wilson, 1996" startWordPosition="1110" endWordPosition="1113">is partially known (Z is unknown), so the classical sampling techniques are not useful. In the literature, there are some methods like the MonteCarlo Markov Chain methods (MCMC) that generate random samples from p(w) (Sahu, 1997; Tierney, 1994). With the MCMC methods, we can simulate a sample approximately from the probability distribution and then use the sample to estimate the desired expected value in (4). 3 Perfect Sampling In this paper, we propose the application of another sampling technique in the parameter estimation process of the WSME model which was introduced by Propp and Wilson (Propp and Wilson, 1996): the Perfect Sampling (PS). The PS method produces samples from the exact limit distribution and, thus, the sampling mean given in (4) is less biased than the one obtained with the MCMC methods. Therefore, we can obtain better estimations of the parameters Ai. In PS, we obtain a sample from the limit distribution of an ergodic Markov Chain X = {X„; n &gt; 0}, taking values in the state space S (in the WSME case, the state space is the set of possible sentences). Because of the ergodicity, if the transition law of X is P(x, A) := P(Xn E AlXn_i = x), then it has a limit distribution it, that is: i</context>
</contexts>
<marker>Propp, Wilson, 1996</marker>
<rawString>J. G. Propp and D. B. Wilson. 1996. Exact sampling with coupled markov chains and applications to statistical mechanics. Random Structures and Algorithms, 9:223-252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Propp</author>
<author>D B Wilson</author>
</authors>
<title>Coupling from the Past: User&apos;s Guide. Dimacs series in discrete Mathematics and Theoretical Computer Science,</title>
<date>1998</date>
<pages>181--192</pages>
<contexts>
<context position="8146" citStr="Propp and Wilson, 1998" startWordPosition="1428" endWordPosition="1431">e to the ergodicity). This value is a sample element. In the majority of cases, the state space is huge, so attempting to begin a path in every state is not practical. Thus, we can define a partial stochastic order in the state space and so we only need start two paths: one in the minimum and one in the maximum. The two paths collapse at time 71 = 0 and the value of the coalescence state is a sample element of 7r. The CFP algorithm first determines the time T to start and then runs the two paths from time (—T) to 0. Information about PS methods may be consulted in (Corcoran and Tweedie, 1998; Propp and Wilson, 1998). 4 Experimental work In this work, we have made preliminary experiments using PS in the estimation of the expected value (4) during the learning of the parameters of a WSME model. We have implemented the Cai algorithm (Cai, 1999) to obtain perfect samples. The Cai algorithm has the advantage that it doesn&apos;t need the definition of the partial order. The experiments were carried out using a pseudonatural corpus: &amp;quot;the traveler task&amp;quot;. The traveler task consists in dialogs between travelers and hotel clerks. The size of the vocabulary is 693 words. The training set has 490,000 sentences and 4,748,</context>
</contexts>
<marker>Propp, Wilson, 1998</marker>
<rawString>J. A. Propp and D. B. Wilson. 1998. Coupling from the Past: User&apos;s Guide. Dimacs series in discrete Mathematics and Theoretical Computer Science, pages 181-192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E S Ristad</author>
</authors>
<title>Maximum Entropy Modeling Toolkit, Version 1.6 Beta.</title>
<date>1998</date>
<contexts>
<context position="3117" citStr="Ristad, 1998" startWordPosition="500" endWordPosition="501">n constant. The main advantages of ME are its flexibility (local and global information can be included in the model) and its simplicity. The drawbacks are that the paramenter&apos;s estimation is computationally expensive, specially the evaluation of the normalization constant Z(x) and that the grammatical information contained in the sentence is poorly encoded in the conditional framework. This is due to the assumption of independence in the conditional events: in the events in the state space, only a part of the information contained in the sentence influences de calculation of the probability (Ristad, 1998). 2 Whole Sentence Maximum Entropy Language Model An alternative to combining local, long-distance and structural information contained in the sentence, within the maximum entropy framework, is the Whole Sentence Maximum Entropy model (WSME) (Rosenfeld, 1997). The (1) 79 WSME is based in the calculation of unrestricted ME probability p(w) of a whole sentence w = w1 wn. The probability distribution is the distribution p that has the maximum entropy relative to a prior distribution po (in other words: the distribution that minimize de divergence D(PliP0)) (Della Pietra et al., 1995). The distrib</context>
</contexts>
<marker>Ristad, 1998</marker>
<rawString>E. S. Ristad, 1998. Maximum Entropy Modeling Toolkit, Version 1.6 Beta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
</authors>
<title>A Maximum Entropy approach to adaptive statistical language modeling.</title>
<date>1996</date>
<journal>Computer Speech and Language,</journal>
<pages>10--187</pages>
<contexts>
<context position="2147" citStr="Rosenfeld, 1996" startWordPosition="339" endWordPosition="340">portant long-distance information contained in the string w1 wn cannot be modeled by it. In an attempt to supplement the local information with long-distance information, hybrid models have been proposed such us (Belle* This work has been partially supported by the Spanish CYCIT under contract (TIC98/0423-006). t Granted by Universidad del Cauca, Popayan (Colombia) garda, 1998; Chelba, 1998; Benedi and Sanchez, 2000). The Maximum Entropy principle is an appropriate framework for combining information of a diverse nature from several sources into the same model: the Maximum Entropy model (ME) (Rosenfeld, 1996). The information is incorporated as features which are submitted to constraints. The conditional form of the ME model is: P(YIX) = 1 Z eE7=1 At ft(x&apos;Y) (X) where Ai are the parameters to be learned (one for each feature), the fi are usually characteristic functions which are associated to the features and Z(x) = Ey exp{ali Azfi(x, y)} is the normalization constant. The main advantages of ME are its flexibility (local and global information can be included in the model) and its simplicity. The drawbacks are that the paramenter&apos;s estimation is computationally expensive, specially the evaluation</context>
</contexts>
<marker>Rosenfeld, 1996</marker>
<rawString>R. Rosenfeld. 1996. A Maximum Entropy approach to adaptive statistical language modeling. Computer Speech and Language, 10:187-228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
</authors>
<title>A whole sentence Maximum Entropy language model.</title>
<date>1997</date>
<booktitle>IEEE workshop on Speech Recognition and Understanding.</booktitle>
<contexts>
<context position="3376" citStr="Rosenfeld, 1997" startWordPosition="537" endWordPosition="538">lization constant Z(x) and that the grammatical information contained in the sentence is poorly encoded in the conditional framework. This is due to the assumption of independence in the conditional events: in the events in the state space, only a part of the information contained in the sentence influences de calculation of the probability (Ristad, 1998). 2 Whole Sentence Maximum Entropy Language Model An alternative to combining local, long-distance and structural information contained in the sentence, within the maximum entropy framework, is the Whole Sentence Maximum Entropy model (WSME) (Rosenfeld, 1997). The (1) 79 WSME is based in the calculation of unrestricted ME probability p(w) of a whole sentence w = w1 wn. The probability distribution is the distribution p that has the maximum entropy relative to a prior distribution po (in other words: the distribution that minimize de divergence D(PliP0)) (Della Pietra et al., 1995). The distribution p is given by: , 1 P(w) =—Po(w)eE7,--iAL(w) where A, and L are the same as in (1). Z is a (global) normalization constant and po is a prior proposal distribution. The A, and Z are unknown and must be learned. The parameters A, may be interpreted as bein</context>
</contexts>
<marker>Rosenfeld, 1997</marker>
<rawString>R. Rosenfeld. 1997. A whole sentence Maximum Entropy language model. IEEE workshop on Speech Recognition and Understanding.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sahu</author>
</authors>
<title>Bayesian data analysis.</title>
<date>1997</date>
<tech>Technical report,</tech>
<institution>School of Mathematics, University of Walles.</institution>
<contexts>
<context position="6154" citStr="Sahu, 1997" startWordPosition="1047" endWordPosition="1048">om sample from p(w). Once the parameters have been learned it is possible to estimate the value of the normalization constant, because Z = eEzin=i Ad;(w)Po(w) = m A f [ea=1 &amp;quot;] , and it can be estimated by means of the sample mean with respect to po (Chen and Rosenfeld, 1999). In each iteration of ITS, the calculation of (4) requires sampling from a probability distribution which is partially known (Z is unknown), so the classical sampling techniques are not useful. In the literature, there are some methods like the MonteCarlo Markov Chain methods (MCMC) that generate random samples from p(w) (Sahu, 1997; Tierney, 1994). With the MCMC methods, we can simulate a sample approximately from the probability distribution and then use the sample to estimate the desired expected value in (4). 3 Perfect Sampling In this paper, we propose the application of another sampling technique in the parameter estimation process of the WSME model which was introduced by Propp and Wilson (Propp and Wilson, 1996): the Perfect Sampling (PS). The PS method produces samples from the exact limit distribution and, thus, the sampling mean given in (4) is less biased than the one obtained with the MCMC methods. Therefore</context>
</contexts>
<marker>Sahu, 1997</marker>
<rawString>S. Sahu. 1997. Bayesian data analysis. Technical report, School of Mathematics, University of Walles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Tierney</author>
</authors>
<title>Markov chains for exploring posterior distributions. The Annals of Statistics,</title>
<date>1994</date>
<pages>22--1701</pages>
<contexts>
<context position="6170" citStr="Tierney, 1994" startWordPosition="1049" endWordPosition="1050">om p(w). Once the parameters have been learned it is possible to estimate the value of the normalization constant, because Z = eEzin=i Ad;(w)Po(w) = m A f [ea=1 &amp;quot;] , and it can be estimated by means of the sample mean with respect to po (Chen and Rosenfeld, 1999). In each iteration of ITS, the calculation of (4) requires sampling from a probability distribution which is partially known (Z is unknown), so the classical sampling techniques are not useful. In the literature, there are some methods like the MonteCarlo Markov Chain methods (MCMC) that generate random samples from p(w) (Sahu, 1997; Tierney, 1994). With the MCMC methods, we can simulate a sample approximately from the probability distribution and then use the sample to estimate the desired expected value in (4). 3 Perfect Sampling In this paper, we propose the application of another sampling technique in the parameter estimation process of the WSME model which was introduced by Propp and Wilson (Propp and Wilson, 1996): the Perfect Sampling (PS). The PS method produces samples from the exact limit distribution and, thus, the sampling mean given in (4) is less biased than the one obtained with the MCMC methods. Therefore, we can obtain </context>
</contexts>
<marker>Tierney, 1994</marker>
<rawString>L. Tierney. 1994. Markov chains for exploring posterior distributions. The Annals of Statistics, 22:1701-1762.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>