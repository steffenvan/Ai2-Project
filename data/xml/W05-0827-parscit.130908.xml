<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000653">
<title confidence="0.8604765">
Improving Phrase-Based Statistical Translation by modifying
phrase extraction and including several features
</title>
<author confidence="0.313642">
Marta Ruiz Costa-jussa and Jose A. R. Fonollosa
</author>
<affiliation confidence="0.395713">
TALP Research Center
</affiliation>
<address confidence="0.568503">
Universitat Politecnica de Catalunya
</address>
<email confidence="0.998206">
{mruiz,adrian}@gps.tsc.upc.edu
</email>
<sectionHeader confidence="0.993883" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999874214285714">
Nowadays, most of the statistical translation sys-
tems are based on phrases (i.e. groups of words).
In this paper we study different improvements to
the standard phrase-based translation system. We
describe a modified method for the phrase extrac-
tion which deals with larger phrases while keeping
a reasonable number of phrases. We also propose
additional features which lead to a clear improve-
ment in the performance of the translation. We
present results with the EuroParl task in the direc-
tion Spanish to English and results from the evalu-
ation of the shared task “Exploiting Parallel Texts
for Statistical Machine Translation” (ACL Work-
shop on Parallel Texts 2005).
</bodyText>
<sectionHeader confidence="0.998976" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9986866">
Statistical Machine Translation (SMT) is based on
the assumption that every sentence e in the target
language is a possible translation of a given sen-
tence f in the source language. The main difference
between two possible translations of a given sen-
tence is a probability assigned to each, which has
to be learned from a bilingual text corpus. Thus,
the translation of a source sentence f can be for-
mulated as the search of the target sentence e that
maximizes the translation probability P(elf),
</bodyText>
<equation confidence="0.797083">
e� = argmax P(elf) (1)
e
</equation>
<bodyText confidence="0.9949795">
OThis work has been supported by the European Union
under grant FP6-506738 (TC-STAR project).
If we use Bayes rule to reformulate the transla-
tion probability, we obtain,
</bodyText>
<equation confidence="0.9049275">
e� = argmax P(f|e)P(e) (2)
e
</equation>
<bodyText confidence="0.9990782">
This translation model is known as the source-
channel approach [1] and it consists on a lan-
guage model P(e) and a separate translation model
P(fIe) [5].
In the last few years, new systems tend to use
sequences of words, commonly called phrases [8],
aiming at introducing word context in the transla-
tion model. As alternative to the source-channel
approach the decision rule can be modeled through
a log-linear maximum entropy framework.
</bodyText>
<equation confidence="0.999710666666667">
e� = argmax
e ��� �
M=1 AMhM(e,f) (3)
</equation>
<bodyText confidence="0.999992294117647">
The features functions, hM, are the system mod-
els (translation model, language model and others)
and weigths, Ai, are typically optimized to max-
imize a scoring function. It is derived from the
Maximum Entropy approach suggested by [13] [14]
for a natural language understanding task. It has
the advantatge that additional features functions
can be easily integrated in the overall system.
This paper addresses a modification of the
phrase-extraction algorythm in [11]. It also com-
bines several interesting features and it reports an
important improvement from the baseline. It is or-
ganized as follows. Section 2 introduces the base-
line; the following section explains the modification
in the phrase extraction; section 4 shows the differ-
ent features which have been taken into account;
section 5 presents the evaluation framework; and
</bodyText>
<page confidence="0.988481">
149
</page>
<note confidence="0.8361105">
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 149–154,
Ann Arbor, June 2005. c�Association for Computational Linguistics, 2005
</note>
<bodyText confidence="0.999787714285714">
the final section shows some conclusions on the ex-
periments in the paper and on the results in the
shared task.
The target language model. It is combined
with the translation probability as showed in equa-
tion (2). It gives coherence to the target text ob-
tained by the concatenated phrases.
</bodyText>
<sectionHeader confidence="0.973336" genericHeader="introduction">
2 Baseline
</sectionHeader>
<bodyText confidence="0.999688136363636">
The baseline is based on the source-channel ap-
proach, and it is composed of the following models
which later will be combined in the decoder.
The Translation Model. It is based on bilin-
gual phrases, where a bilingual phrase (BP) is
simply two monolingual phrases (MP) in which
each one is supposed to be the translation of each
other. A monolingual phrase is a sequence of words.
Therefore, the basic idea of phrase-based transla-
tion is to segment the given source sentence into
phrases, then translate each phrase and finally com-
pose the target sentence from these phrase transla-
tions [17].
During training, the system has to learn a dictio-
nary of phrases. We begin by aligning the training
corpus using GIZA++ [6], which is done in both
translation directions. We take the union of both
alignments to obtain a symmetrized word align-
ment matrix. This alignment matrix is the starting
point for the phrase based extraction.
Next, we define the criterion to extract the set of
BP of the sentence pair (fj�
</bodyText>
<equation confidence="0.873673">
j� ; ei�
</equation>
<bodyText confidence="0.948688">
i�) and the alignment
matrix A C_ J*I, which is identical to the alignment
criterion described in [11].
</bodyText>
<equation confidence="0.981956666666667">
BP(fJ1 ,eI1,A) _ {(f�l,ezl) :
d(j, i)eA : j1 &lt; j &lt; j2 H i1 &lt; i &lt; i2
nEI(j, i)eA : j1 &lt; j &lt; j2 n i1 &lt; i &lt; i2}
</equation>
<bodyText confidence="0.999923333333333">
The set of BP is consistent with the alignment
and consists of all BP pairs where all words within
the foreign language phrase are only aligned to the
words of the English language phrase and viceversa.
At least one word in the foreign language phrase has
to be aligned with at least one word of the English
language. Finally, the algorithm takes into account
possibly unaligned words at the boundaries of the
foreign or English language phrases.
</bodyText>
<sectionHeader confidence="0.967466" genericHeader="method">
3 Phrase Extraction
</sectionHeader>
<bodyText confidence="0.99975652631579">
Motivation. The length of a MP is defined as
its number of words. The length of a BP is the
greatest of the lengths of its MP.
As we are working with a huge amount of data
(see corpus statistics), it is unfeasible to build a
dictionary with all the phrases longer than length
4. Moreover, the huge increase in computational
and storage cost of including longer phrases does
not provide a significant improve in quality [8].
X-length In our system we considered two length
limits. We first extract all the phrases of length 3
or less. Then, we also add phrases up to length
5 if they cannot be generated by smaller phrases.
Empirically, we chose 5, as the probability of reap-
pearence of larger phrases decreases.
Basically, we select additional phrases with
source words that otherwise would be missed be-
cause of cross or long alignments. For example,
from the following sentence,
</bodyText>
<construct confidence="0.986569125">
Cuando el Parlamento Europeo , que tan fre-
cuentemente insiste en los derechos de los traba-
jadores y en la debida proteccion social , (...)
NULL ( ) When ( 1 ) the ( 2 ) European ( 4
) Parliament ( 3 4 ) ,( 5 ) that ( 6 ) so ( 7 )
frequently ( 8 ) insists ( 9 ) on ( 10 ) workers ( 11
15 ) &apos; ( 14 ) rights ( 12 ) and ( 16 ) proper ( 19 )
social ( 21 ) protection ( 20 ) , ( 22 ) (...)
</construct>
<bodyText confidence="0.998013">
where the number inside the clauses is the
aligned word(s). And the phrase that we are look-
ing for is the following one.
los derechos de los trabajadores # workers &apos;
rights
which only could appear in the case the maximum
length was 5.
</bodyText>
<page confidence="0.995781">
150
</page>
<sectionHeader confidence="0.989817" genericHeader="method">
4 Phrase ranking
</sectionHeader>
<subsectionHeader confidence="0.999413">
4.1 Conditional probability P(f|e)
</subsectionHeader>
<bodyText confidence="0.999962666666667">
Given the collected phrase pairs, we estimated the
phrase translation probability distribution by rela-
tive frecuency.
</bodyText>
<equation confidence="0.998215">
N(f, e)
P(f|e) = (4)
N(e)
</equation>
<bodyText confidence="0.9990291875">
where N(f,e) means the number of times the phrase
f is translated by e. If a phrase e has N &gt; 1
possible translations, then each one contributes as
1/N [17].
Note that no smoothing is performed, which may
cause an overestimation of the probability of rare
phrases. This is specially harmful given a BP
where the source part has a big frecuency of ap-
pearence but the target part appears rarely. For
example, from our database we can extract the fol-
lowing BP: ”you # la que no&amp;quot;, where the English
is the source language and the Spanish, the tar-
get language. Clearly, &amp;quot;la que no&amp;quot; is not a good
translation of &amp;quot;you&amp;quot;, so this phrase should have a
low probability. However, from our aligned training
database we obtain,
</bodyText>
<equation confidence="0.753993">
P(f|e) = P(you|la que no) = 0.23
</equation>
<bodyText confidence="0.9987644">
This BP is clearly overestimated due to sparse-
ness. On the other, note that &amp;quot;la que no&amp;quot; can-
not be considered an unusual trigram in Spanish.
Hence, the language model does not penalise this
target sequence either. So, the total probability
(P(f|e)P(e)) would be higher than desired.
In order to somehow compensate these unreili-
able probabilities we have studied the inclusion of
the posterior [12] and lexical probabilities [1] [10]
as additional features.
</bodyText>
<subsectionHeader confidence="0.757089">
4.2 Feature P(elf)
</subsectionHeader>
<bodyText confidence="0.999962">
In order to estimate the posterior phrase probabil-
ity, we compute again the relative frequency but re-
placing the count of the target phrase by the count
of the source phrase.
</bodyText>
<equation confidence="0.605758">
(5)
N(f)
</equation>
<bodyText confidence="0.997195375">
where N&apos;(f,e) means the number of times the
phrase e is translated by f. If a phrase f has N &gt; 1
possible translations, then each one contributes as
1/N.
Adding this feature function we reduce the num-
ber of cases in which the overall probability is over-
estimated. This results in an important improve-
ment in translation quality.
</bodyText>
<subsectionHeader confidence="0.790412">
4.3 IBM Model 1
</subsectionHeader>
<bodyText confidence="0.999967875">
We used IBM Model 1 to estimate the probability
of a BP. As IBM Model 1 is a word translation and
it gives the sum of all possible alignment probabil-
ities, a lexical co-ocurrence effect is expected. This
captures a sort of semantic coherence in transla-
tions.
Therefore, the probability of a sentence pair is
given by the following equation.
</bodyText>
<equation confidence="0.956779666666667">
J I
P(f |e; M1) = (I + 1 J H E p(fj  |ei) (6)
j=1 i=0
</equation>
<bodyText confidence="0.98577325">
The p(fj|ei) are the source-target IBM Model 1
word probabilities trained by GIZA++. Because
the phrases are formed from the union of source-to-
target and target-to-source alignments, there can
be words that are not in the P(fj|ei) table. In this
case, the probability was taken to be 10−40.
In addition, we have calculated the IBM−1 Model
1.
</bodyText>
<equation confidence="0.84173025">
(
+ 1)
� � p(ei|fj) (7)
I=1 j=0
</equation>
<subsectionHeader confidence="0.927297">
4.4 Language Model
</subsectionHeader>
<bodyText confidence="0.999477444444445">
The English language model plays an important
role in the source channel model, see equation (2),
and also in its modification, see equation (3). The
English language model should give an idea of the
sentence quality that is generated.
As default language model feature, we use a stan-
dard word-based trigram language model generated
with smoothing Kneser-Ney and interpolation (by
using SRILM [16]).
</bodyText>
<subsectionHeader confidence="0.99642">
4.5 Word and Phrase Penalty
</subsectionHeader>
<bodyText confidence="0.9999585">
To compensate the preference of the target lan-
guage model for shorter sentences, we added two
</bodyText>
<equation confidence="0.995299666666667">
N�(f, e)
P(e|f) =
1 I J
P(e|f; M1) =
J
I
</equation>
<page confidence="0.995583">
151
</page>
<table confidence="0.9996139">
Spanish English
Train Sentences 1223443 1223443
Words 34794006 33379333
Vocabulary 168685 104975
Dev Sentences 504 504
Words 15353 15335
OOV 25 16
Test Sentences 504 504
Words 10305 10667
OOV 36 19
</table>
<tableCaption confidence="0.999855">
Table 1: Statistics of training and test corpus
</tableCaption>
<bodyText confidence="0.981747636363636">
simple features which are widely used [17] [7]. The
word penalty provides means to ensure that the
translations do not get too long or too short. Neg-
ative values for the word penalty favor longer out-
put, positive values favor shorter output [7].
The phrase penalty is a constant cost per pro-
duced phrase. Here, a negative weight, which
means reducing the costs per phrase, results in a
preference for adding phrases. Alternatively, by us-
ing a positive scaling factors, the system will favor
less phrases.
</bodyText>
<subsectionHeader confidence="0.977002">
5.2 Experiments
</subsectionHeader>
<bodyText confidence="0.999982">
The decoder used for the presented translation sys-
tem is reported in [2]. This decoder is called
MARIE and it takes into account simultaneously
all the 7 features functions described above. It im-
plements a beam-search strategy.
As evaluation criteria we use: the Word Error
Rate (WER), the BLEU score [15] and the NIST
score [3].
As follows we report the results for several ex-
periments that show the performance of: the base-
line, adding the posterior probability, IBM Model
1 and IBM1−1, and, finally, the modification of the
phrases extraction.
Optimisation. Significant improvements can be
obtained by tuning the parameters of the features
adequately. In the complet system we have 7 pa-
rameters to tune: the relatives frecuencies P(f|e)
and P(elf), IBM Model 1 and its inverse, the word
penalty, the phrase penalty and the weight of the
language model. We applied the widely used algo-
rithm SIMPLEX to optimise [9]. In Table 2 (line
5th), we see the final results.
</bodyText>
<sectionHeader confidence="0.992554" genericHeader="evaluation">
5 Evaluation framework
</sectionHeader>
<subsectionHeader confidence="0.998897">
5.1 Corpus Statistics
</subsectionHeader>
<bodyText confidence="0.999093357142857">
Experiments were performed to study the effect
of our modifications in the phrases. The training
material covers the transcriptions from April 1996
to September 2004. This material has been dis-
tributed by the European Parlament. In our ex-
periments, we have used the distribution of RWTH
of Aachen under the project of TC-STAR 1. The
test material was used in the first evaluation of the
project in March 2005. In our case, we have used
the development divided in two sets. This mate-
rial corresponds to the transcriptions of the sessions
from October the 21st to October the 28th. It has
been distributed by ELDA2. Results are reported
for Spanish-to-English translations.
</bodyText>
<footnote confidence="0.9996915">
1http://www.tcstar.org/
2http://www.elda.org/
</footnote>
<bodyText confidence="0.986521842105263">
Baseline. We report the results of the baseline.
We use the union alignment and we extract the
BP of length 3. As default language model fea-
ture, we use the standard trigram with smoothing
Kneser-Ney and interpolation. Also we tune the
parameters (only two parameters) with the SIM-
PLEX algorithm (see Table 2).
Posterior probability. Table 2 shows the effect
of using the posterior probability: P(elf). We use
all the features but the P(elf) and we optimise the
parameters. We see the results without this feature
decrease around 1.1 points both in BLEU and WER
(see line 2rd and 5th in Table 2).
IBM Model 1. We do the same as in the para-
graph above, we do not consider the IBM Model
1 and the IBM1−1. Under these conditions, the
translation’s quality decreases around 1.3 points
both in BLEU and WER (see line 3th and 5th in
Table 2).
</bodyText>
<page confidence="0.996234">
152
</page>
<bodyText confidence="0.982969636363636">
Modification of the Phrase Extraction. Fi-
nally, we made an experiment without modification
of the phrases’ length. We can see the comparison
between: (1) the phrases of fixed maximum length
of 3; and (2) including phrases with a maximum
length of 5 which can not be generated by smaller
phrases. We can see it in Table 2 (lines 4th and
5th). We observe that there is no much difference
between the number of phrases, so this approach
does not require more resources. However, we get
slightly better scores.
</bodyText>
<subsectionHeader confidence="0.99941">
5.3 Shared Task
</subsectionHeader>
<bodyText confidence="0.974881153846154">
This section explains the participation of “Exploit-
ing Parallel Texts for Statistical Machine Transla-
tion”. We used the EuroParl data provided for this
shared task [4]. A word-to-word alignment was per-
formed in both directions as explained in section
2. The phrase-based translation system which has
been considered implements a total of 7 features
(already explained in section 4). Notice that the
language model has been trained with the training
provided in the shared task. However, the opti-
mization in the parameters has not been repeated,
and we used the parameters obtained in the sub-
section above. We have obtained the results in the
</bodyText>
<tableCaption confidence="0.744481">
Table 3.
</tableCaption>
<sectionHeader confidence="0.998992" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.998636285714286">
We reported a new method to extract longer
phrases without increasing the quantity of phrases
(less than 0.5%).
We also reported several features as P(e|f)
which in combination with the functions of the
source-channel model provides significant improve-
ment. Also, the feature IBM1 in combination with
IBM1−1 provides improved scores, too.
Finally, we have optimized the parameters, and
we provided the final results which have been pre-
sented in the Shared Task: Exploiting Parallel
Texts for Statistical Machine Translation (June 30,
2005) in conjunction with ACL 2005 in Ann Arbor,
Michigan.
</bodyText>
<sectionHeader confidence="0.995847" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.98411525">
The authors want to thank Jose B. Marino, Adria
de Gispert, Josep M. Crego, Patrik Lambert and
Rafael E. Banchs (members of the TALP Research
Center) for their contribution to this work.
</bodyText>
<sectionHeader confidence="0.998463" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999957666666667">
[1] P.F. Brown, J. Cocke, S.A. Della Pietra,
V.J. Della Pietra, F. Jelinek, J.D. Lafferty,
R.L. Mercer, and P.S. Rossin. A statistical ap-
proach to machine translation. Computational
Linguistics, 16(2):79–85, June 1990.
[2] Josep M. Crego, Jose B. Marino, and Adria
de Gispert. An Ngram-based Statistical Ma-
chine Translation Decoder. In Draft, 2005.
[3] G. Doddington. Automatic evaluation ma-
chine translation quality using n-gram co-
ocurrence statistics. In Proc. ARPA Workshop
on Human Language Technology, 2002.
[4] EuroParl: European Parliament Proceed-
ings Parallel Corpus. Available on-line at:
http://people.csail.mit.edu/koehn/publica-
tions/europarl/. 1996-2003.
[5] I. Garcia-Varea. Traducci6n Automdtica es-
tadistica: Modelos de Traducci6n basados en
Mdxima Entropia y Algoritmos de Biusqueda .
UPV, Diciembre 2003.
[6] Giza++. http://www-i6.informatik.rwth-
aachen.de/∼och/software/giza++.html/,
1999.
[7] P. Koehn. A Beam Search Decoder for Phrase-
Based Statistical Machine Translation Models.
2003.
[8] P. Koehn, F. J. Och, and D. Marcu. Statisti-
cal phrase-based translation. In Proceedings of
the Human Language Technology Conference
(HLT-NAACL), pages 127–133, May 2003.
[9] J.A. Nelder and R. Mead. A simplex method
for function minimization. The Computer
Journal, 7:308–313, 1965.
</reference>
<page confidence="0.998465">
153
</page>
<table confidence="0.999462333333333">
Phr Length ALM Ap(fle) Ap(e|f) AIBM1 AIBM1−1 APP AWP WER BLEU NIST # frases
3 0.788 0.906 0 0 0 0 0 33.98 57.44 10.11 67.7M
3+5length 0.788 0.941 0 0.771 0.200 3.227 0.448 28.97 64.71 11.07 68M
3+5length 0.788 0.824 0.820 0 0 3.430 -0.083 29.17 64.59 10.99 68M
3 0.746 0.515 0.979 0.514 0.390 1.537 -1.264 27.94 65.70 11.18 67.7M
3+5length 0.788 0.617 0.810 0.635 0.101 1.995 -0.296 27.88 65.82 11.23 68M
</table>
<tableCaption confidence="0.907022">
Table 2: Results for the different experiments with optimized parameters in the direction SPA-&gt;ENG
</tableCaption>
<table confidence="0.9957435">
Phr Length ALM Ap(f|e) Ap(e|f) AIBM1 AIBM1−1 APP AWP BLEU # frases
3+5length 0.788 0.617 0.810 0.635 0.101 1.995 -0.296 29.84 34.8M
</table>
<tableCaption confidence="0.99631">
Table 3: Results for the ACL training and ACL test (SPA-&gt;ENG)
</tableCaption>
<reference confidence="0.999952868421053">
[10] F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar,
K. Yamada, A. Fraser, S. Kumar, L. Shen,
D. Smith, K. Eng, V. Jain, Z. Jin, and
D. Radev. A Smorgasbord of Features for Sta-
tistical Machine Translation. In Proceedings of
the Human Language Technology Conference
(HLT-NAACL), May 2004.
[11] F. J. Och and H. Ney. The Alignment Tem-
plate Approach to Statistical Machine Trans-
lation. Computational linguistics, 30:417–449,
December 2004.
[12] Franz Josef Och and Hermann Ney. Discrimi-
native Training and Maximum Entropy Mod-
els for Statistical Machine Translation. In
ACL, pages pages 295–302, July 2002.
[13] Papineni, S.Roukos, and R.T. Ward. Feature-
based language understanding. In European
Conf. on Speech Communication and Technol-
ogy, pages 1435–1438, September 1997.
[14] Papineni, S.Roukos, and R.T. Ward. Maxi-
mum likelihood and discriminative training of
direct translation models. In Proc. Int. Conf.
on Acoustics, Speech, and Signal Proceedings,
pages 189–192, May 1998.
[15] K.A. Papineni, S. Roukos, T. Ward, and W.J.
Zhu. Bleu: a method for automatic evaluation
of machine translation. In Technical Report
RC22176 (W0109-022), IBM Research Divi-
sion, 2001.
[16] A. Stolcke. SRILM - An Extensible Language
Modeling Toolkit. In Proceedings Intl. Confer-
ence Spoken Language Processing, September
2002.
[17] R. Zens and H. Ney. Improvements in Phrase-
Based Statistical Machine Translation. In
Proceedings of the Human Language Technol-
ogy Conference (HLT-NAACL), pages 257–
264, May 2004.
</reference>
<page confidence="0.999771">
154
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.335893">
<title confidence="0.995722">Improving Phrase-Based Statistical Translation by phrase extraction and including several features</title>
<author confidence="0.999981">Marta Ruiz Costa-jussa</author>
<author confidence="0.999981">Jose A R Fonollosa</author>
<affiliation confidence="0.998068">TALP Research Center Universitat Politecnica de Catalunya</affiliation>
<abstract confidence="0.921208066666667">Nowadays, most of the statistical translation systems are based on phrases (i.e. groups of words). In this paper we study different improvements to the standard phrase-based translation system. We describe a modified method for the phrase extraction which deals with larger phrases while keeping a reasonable number of phrases. We also propose additional features which lead to a clear improvement in the performance of the translation. We present results with the EuroParl task in the direction Spanish to English and results from the evaluation of the shared task “Exploiting Parallel Texts for Statistical Machine Translation” (ACL Workshop on Parallel Texts 2005).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>J Cocke</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>F Jelinek</author>
<author>J D Lafferty</author>
<author>R L Mercer</author>
<author>P S Rossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="1726" citStr="[1]" startWordPosition="272" endWordPosition="272">e main difference between two possible translations of a given sentence is a probability assigned to each, which has to be learned from a bilingual text corpus. Thus, the translation of a source sentence f can be formulated as the search of the target sentence e that maximizes the translation probability P(elf), e� = argmax P(elf) (1) e OThis work has been supported by the European Union under grant FP6-506738 (TC-STAR project). If we use Bayes rule to reformulate the translation probability, we obtain, e� = argmax P(f|e)P(e) (2) e This translation model is known as the sourcechannel approach [1] and it consists on a language model P(e) and a separate translation model P(fIe) [5]. In the last few years, new systems tend to use sequences of words, commonly called phrases [8], aiming at introducing word context in the translation model. As alternative to the source-channel approach the decision rule can be modeled through a log-linear maximum entropy framework. e� = argmax e ��� � M=1 AMhM(e,f) (3) The features functions, hM, are the system models (translation model, language model and others) and weigths, Ai, are typically optimized to maximize a scoring function. It is derived from th</context>
<context position="7998" citStr="[1]" startWordPosition="1391" endWordPosition="1391">ue no&amp;quot; is not a good translation of &amp;quot;you&amp;quot;, so this phrase should have a low probability. However, from our aligned training database we obtain, P(f|e) = P(you|la que no) = 0.23 This BP is clearly overestimated due to sparseness. On the other, note that &amp;quot;la que no&amp;quot; cannot be considered an unusual trigram in Spanish. Hence, the language model does not penalise this target sequence either. So, the total probability (P(f|e)P(e)) would be higher than desired. In order to somehow compensate these unreiliable probabilities we have studied the inclusion of the posterior [12] and lexical probabilities [1] [10] as additional features. 4.2 Feature P(elf) In order to estimate the posterior phrase probability, we compute again the relative frequency but replacing the count of the target phrase by the count of the source phrase. (5) N(f) where N&apos;(f,e) means the number of times the phrase e is translated by f. If a phrase f has N &gt; 1 possible translations, then each one contributes as 1/N. Adding this feature function we reduce the number of cases in which the overall probability is overestimated. This results in an important improvement in translation quality. 4.3 IBM Model 1 We used IBM Model 1 to</context>
</contexts>
<marker>[1]</marker>
<rawString>P.F. Brown, J. Cocke, S.A. Della Pietra, V.J. Della Pietra, F. Jelinek, J.D. Lafferty, R.L. Mercer, and P.S. Rossin. A statistical approach to machine translation. Computational Linguistics, 16(2):79–85, June 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep M Crego</author>
<author>Jose B Marino</author>
<author>Adria de Gispert</author>
</authors>
<title>An Ngram-based Statistical Machine Translation Decoder. In Draft,</title>
<date>2005</date>
<contexts>
<context position="10769" citStr="[2]" startWordPosition="1878" endWordPosition="1878">st corpus simple features which are widely used [17] [7]. The word penalty provides means to ensure that the translations do not get too long or too short. Negative values for the word penalty favor longer output, positive values favor shorter output [7]. The phrase penalty is a constant cost per produced phrase. Here, a negative weight, which means reducing the costs per phrase, results in a preference for adding phrases. Alternatively, by using a positive scaling factors, the system will favor less phrases. 5.2 Experiments The decoder used for the presented translation system is reported in [2]. This decoder is called MARIE and it takes into account simultaneously all the 7 features functions described above. It implements a beam-search strategy. As evaluation criteria we use: the Word Error Rate (WER), the BLEU score [15] and the NIST score [3]. As follows we report the results for several experiments that show the performance of: the baseline, adding the posterior probability, IBM Model 1 and IBM1−1, and, finally, the modification of the phrases extraction. Optimisation. Significant improvements can be obtained by tuning the parameters of the features adequately. In the complet sy</context>
</contexts>
<marker>[2]</marker>
<rawString>Josep M. Crego, Jose B. Marino, and Adria de Gispert. An Ngram-based Statistical Machine Translation Decoder. In Draft, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Doddington</author>
</authors>
<title>Automatic evaluation machine translation quality using n-gram coocurrence statistics.</title>
<date>2002</date>
<booktitle>In Proc. ARPA Workshop on Human Language Technology,</booktitle>
<contexts>
<context position="11025" citStr="[3]" startWordPosition="1921" endWordPosition="1921">The phrase penalty is a constant cost per produced phrase. Here, a negative weight, which means reducing the costs per phrase, results in a preference for adding phrases. Alternatively, by using a positive scaling factors, the system will favor less phrases. 5.2 Experiments The decoder used for the presented translation system is reported in [2]. This decoder is called MARIE and it takes into account simultaneously all the 7 features functions described above. It implements a beam-search strategy. As evaluation criteria we use: the Word Error Rate (WER), the BLEU score [15] and the NIST score [3]. As follows we report the results for several experiments that show the performance of: the baseline, adding the posterior probability, IBM Model 1 and IBM1−1, and, finally, the modification of the phrases extraction. Optimisation. Significant improvements can be obtained by tuning the parameters of the features adequately. In the complet system we have 7 parameters to tune: the relatives frecuencies P(f|e) and P(elf), IBM Model 1 and its inverse, the word penalty, the phrase penalty and the weight of the language model. We applied the widely used algorithm SIMPLEX to optimise [9]. In Table 2</context>
</contexts>
<marker>[3]</marker>
<rawString>G. Doddington. Automatic evaluation machine translation quality using n-gram coocurrence statistics. In Proc. ARPA Workshop on Human Language Technology, 2002.</rawString>
</citation>
<citation valid="false">
<title>EuroParl: European Parliament Proceedings Parallel Corpus. Available on-line at:</title>
<pages>1996--2003</pages>
<contexts>
<context position="13958" citStr="[4]" startWordPosition="2414" endWordPosition="2414">ion of the phrases’ length. We can see the comparison between: (1) the phrases of fixed maximum length of 3; and (2) including phrases with a maximum length of 5 which can not be generated by smaller phrases. We can see it in Table 2 (lines 4th and 5th). We observe that there is no much difference between the number of phrases, so this approach does not require more resources. However, we get slightly better scores. 5.3 Shared Task This section explains the participation of “Exploiting Parallel Texts for Statistical Machine Translation”. We used the EuroParl data provided for this shared task [4]. A word-to-word alignment was performed in both directions as explained in section 2. The phrase-based translation system which has been considered implements a total of 7 features (already explained in section 4). Notice that the language model has been trained with the training provided in the shared task. However, the optimization in the parameters has not been repeated, and we used the parameters obtained in the subsection above. We have obtained the results in the Table 3. 6 Conclusions We reported a new method to extract longer phrases without increasing the quantity of phrases (less th</context>
</contexts>
<marker>[4]</marker>
<rawString>EuroParl: European Parliament Proceedings Parallel Corpus. Available on-line at: http://people.csail.mit.edu/koehn/publications/europarl/. 1996-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Garcia-Varea</author>
</authors>
<title>Traducci6n Automdtica estadistica: Modelos de Traducci6n basados en Mdxima Entropia y Algoritmos de Biusqueda . UPV,</title>
<date>2003</date>
<location>Diciembre</location>
<contexts>
<context position="1811" citStr="[5]" startWordPosition="288" endWordPosition="288">ity assigned to each, which has to be learned from a bilingual text corpus. Thus, the translation of a source sentence f can be formulated as the search of the target sentence e that maximizes the translation probability P(elf), e� = argmax P(elf) (1) e OThis work has been supported by the European Union under grant FP6-506738 (TC-STAR project). If we use Bayes rule to reformulate the translation probability, we obtain, e� = argmax P(f|e)P(e) (2) e This translation model is known as the sourcechannel approach [1] and it consists on a language model P(e) and a separate translation model P(fIe) [5]. In the last few years, new systems tend to use sequences of words, commonly called phrases [8], aiming at introducing word context in the translation model. As alternative to the source-channel approach the decision rule can be modeled through a log-linear maximum entropy framework. e� = argmax e ��� � M=1 AMhM(e,f) (3) The features functions, hM, are the system models (translation model, language model and others) and weigths, Ai, are typically optimized to maximize a scoring function. It is derived from the Maximum Entropy approach suggested by [13] [14] for a natural language understandin</context>
</contexts>
<marker>[5]</marker>
<rawString>I. Garcia-Varea. Traducci6n Automdtica estadistica: Modelos de Traducci6n basados en Mdxima Entropia y Algoritmos de Biusqueda . UPV, Diciembre 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giza http www-i6 informatik rwthaachen de∼ochsoftwaregiza html</author>
</authors>
<date>1999</date>
<contexts>
<context position="4147" citStr="[6]" startWordPosition="671" endWordPosition="671">l be combined in the decoder. The Translation Model. It is based on bilingual phrases, where a bilingual phrase (BP) is simply two monolingual phrases (MP) in which each one is supposed to be the translation of each other. A monolingual phrase is a sequence of words. Therefore, the basic idea of phrase-based translation is to segment the given source sentence into phrases, then translate each phrase and finally compose the target sentence from these phrase translations [17]. During training, the system has to learn a dictionary of phrases. We begin by aligning the training corpus using GIZA++ [6], which is done in both translation directions. We take the union of both alignments to obtain a symmetrized word alignment matrix. This alignment matrix is the starting point for the phrase based extraction. Next, we define the criterion to extract the set of BP of the sentence pair (fj� j� ; ei� i�) and the alignment matrix A C_ J*I, which is identical to the alignment criterion described in [11]. BP(fJ1 ,eI1,A) _ {(f�l,ezl) : d(j, i)eA : j1 &lt; j &lt; j2 H i1 &lt; i &lt; i2 nEI(j, i)eA : j1 &lt; j &lt; j2 n i1 &lt; i &lt; i2} The set of BP is consistent with the alignment and consists of all BP pairs where all wo</context>
</contexts>
<marker>[6]</marker>
<rawString>Giza++. http://www-i6.informatik.rwthaachen.de/∼och/software/giza++.html/, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>A Beam Search Decoder for PhraseBased Statistical Machine Translation Models.</title>
<date>2003</date>
<contexts>
<context position="10222" citStr="[7]" startWordPosition="1785" endWordPosition="1785">model feature, we use a standard word-based trigram language model generated with smoothing Kneser-Ney and interpolation (by using SRILM [16]). 4.5 Word and Phrase Penalty To compensate the preference of the target language model for shorter sentences, we added two N�(f, e) P(e|f) = 1 I J P(e|f; M1) = J I 151 Spanish English Train Sentences 1223443 1223443 Words 34794006 33379333 Vocabulary 168685 104975 Dev Sentences 504 504 Words 15353 15335 OOV 25 16 Test Sentences 504 504 Words 10305 10667 OOV 36 19 Table 1: Statistics of training and test corpus simple features which are widely used [17] [7]. The word penalty provides means to ensure that the translations do not get too long or too short. Negative values for the word penalty favor longer output, positive values favor shorter output [7]. The phrase penalty is a constant cost per produced phrase. Here, a negative weight, which means reducing the costs per phrase, results in a preference for adding phrases. Alternatively, by using a positive scaling factors, the system will favor less phrases. 5.2 Experiments The decoder used for the presented translation system is reported in [2]. This decoder is called MARIE and it takes into acco</context>
</contexts>
<marker>[7]</marker>
<rawString>P. Koehn. A Beam Search Decoder for PhraseBased Statistical Machine Translation Models. 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference (HLT-NAACL),</booktitle>
<pages>127--133</pages>
<contexts>
<context position="1907" citStr="[8]" startWordPosition="305" endWordPosition="305">n of a source sentence f can be formulated as the search of the target sentence e that maximizes the translation probability P(elf), e� = argmax P(elf) (1) e OThis work has been supported by the European Union under grant FP6-506738 (TC-STAR project). If we use Bayes rule to reformulate the translation probability, we obtain, e� = argmax P(f|e)P(e) (2) e This translation model is known as the sourcechannel approach [1] and it consists on a language model P(e) and a separate translation model P(fIe) [5]. In the last few years, new systems tend to use sequences of words, commonly called phrases [8], aiming at introducing word context in the translation model. As alternative to the source-channel approach the decision rule can be modeled through a log-linear maximum entropy framework. e� = argmax e ��� � M=1 AMhM(e,f) (3) The features functions, hM, are the system models (translation model, language model and others) and weigths, Ai, are typically optimized to maximize a scoring function. It is derived from the Maximum Entropy approach suggested by [13] [14] for a natural language understanding task. It has the advantatge that additional features functions can be easily integrated in the</context>
<context position="5548" citStr="[8]" startWordPosition="932" endWordPosition="932">e word of the English language. Finally, the algorithm takes into account possibly unaligned words at the boundaries of the foreign or English language phrases. 3 Phrase Extraction Motivation. The length of a MP is defined as its number of words. The length of a BP is the greatest of the lengths of its MP. As we are working with a huge amount of data (see corpus statistics), it is unfeasible to build a dictionary with all the phrases longer than length 4. Moreover, the huge increase in computational and storage cost of including longer phrases does not provide a significant improve in quality [8]. X-length In our system we considered two length limits. We first extract all the phrases of length 3 or less. Then, we also add phrases up to length 5 if they cannot be generated by smaller phrases. Empirically, we chose 5, as the probability of reappearence of larger phrases decreases. Basically, we select additional phrases with source words that otherwise would be missed because of cross or long alignments. For example, from the following sentence, Cuando el Parlamento Europeo , que tan frecuentemente insiste en los derechos de los trabajadores y en la debida proteccion social , (...) NUL</context>
</contexts>
<marker>[8]</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. Statistical phrase-based translation. In Proceedings of the Human Language Technology Conference (HLT-NAACL), pages 127–133, May 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Nelder</author>
<author>R Mead</author>
</authors>
<title>A simplex method for function minimization.</title>
<date>1965</date>
<journal>The Computer Journal,</journal>
<volume>7</volume>
<contexts>
<context position="11613" citStr="[9]" startWordPosition="2017" endWordPosition="2017"> NIST score [3]. As follows we report the results for several experiments that show the performance of: the baseline, adding the posterior probability, IBM Model 1 and IBM1−1, and, finally, the modification of the phrases extraction. Optimisation. Significant improvements can be obtained by tuning the parameters of the features adequately. In the complet system we have 7 parameters to tune: the relatives frecuencies P(f|e) and P(elf), IBM Model 1 and its inverse, the word penalty, the phrase penalty and the weight of the language model. We applied the widely used algorithm SIMPLEX to optimise [9]. In Table 2 (line 5th), we see the final results. 5 Evaluation framework 5.1 Corpus Statistics Experiments were performed to study the effect of our modifications in the phrases. The training material covers the transcriptions from April 1996 to September 2004. This material has been distributed by the European Parlament. In our experiments, we have used the distribution of RWTH of Aachen under the project of TC-STAR 1. The test material was used in the first evaluation of the project in March 2005. In our case, we have used the development divided in two sets. This material corresponds to th</context>
</contexts>
<marker>[9]</marker>
<rawString>J.A. Nelder and R. Mead. A simplex method for function minimization. The Computer Journal, 7:308–313, 1965.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>D Gildea</author>
<author>S Khudanpur</author>
<author>A Sarkar</author>
<author>K Yamada</author>
<author>A Fraser</author>
<author>S Kumar</author>
<author>L Shen</author>
<author>D Smith</author>
<author>K Eng</author>
<author>V Jain</author>
<author>Z Jin</author>
<author>D Radev</author>
</authors>
<title>A Smorgasbord of Features for Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference (HLT-NAACL),</booktitle>
<contexts>
<context position="8003" citStr="[10]" startWordPosition="1392" endWordPosition="1392">o&amp;quot; is not a good translation of &amp;quot;you&amp;quot;, so this phrase should have a low probability. However, from our aligned training database we obtain, P(f|e) = P(you|la que no) = 0.23 This BP is clearly overestimated due to sparseness. On the other, note that &amp;quot;la que no&amp;quot; cannot be considered an unusual trigram in Spanish. Hence, the language model does not penalise this target sequence either. So, the total probability (P(f|e)P(e)) would be higher than desired. In order to somehow compensate these unreiliable probabilities we have studied the inclusion of the posterior [12] and lexical probabilities [1] [10] as additional features. 4.2 Feature P(elf) In order to estimate the posterior phrase probability, we compute again the relative frequency but replacing the count of the target phrase by the count of the source phrase. (5) N(f) where N&apos;(f,e) means the number of times the phrase e is translated by f. If a phrase f has N &gt; 1 possible translations, then each one contributes as 1/N. Adding this feature function we reduce the number of cases in which the overall probability is overestimated. This results in an important improvement in translation quality. 4.3 IBM Model 1 We used IBM Model 1 to esti</context>
</contexts>
<marker>[10]</marker>
<rawString>F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, V. Jain, Z. Jin, and D. Radev. A Smorgasbord of Features for Statistical Machine Translation. In Proceedings of the Human Language Technology Conference (HLT-NAACL), May 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>The Alignment Template Approach to Statistical Machine Translation. Computational linguistics,</title>
<date>2004</date>
<pages>30--417</pages>
<contexts>
<context position="2602" citStr="[11]" startWordPosition="414" endWordPosition="414">-channel approach the decision rule can be modeled through a log-linear maximum entropy framework. e� = argmax e ��� � M=1 AMhM(e,f) (3) The features functions, hM, are the system models (translation model, language model and others) and weigths, Ai, are typically optimized to maximize a scoring function. It is derived from the Maximum Entropy approach suggested by [13] [14] for a natural language understanding task. It has the advantatge that additional features functions can be easily integrated in the overall system. This paper addresses a modification of the phrase-extraction algorythm in [11]. It also combines several interesting features and it reports an important improvement from the baseline. It is organized as follows. Section 2 introduces the baseline; the following section explains the modification in the phrase extraction; section 4 shows the different features which have been taken into account; section 5 presents the evaluation framework; and 149 Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 149–154, Ann Arbor, June 2005. c�Association for Computational Linguistics, 2005 the final section shows some conclusions on the experiments in the pape</context>
<context position="4548" citStr="[11]" startWordPosition="742" endWordPosition="742">se and finally compose the target sentence from these phrase translations [17]. During training, the system has to learn a dictionary of phrases. We begin by aligning the training corpus using GIZA++ [6], which is done in both translation directions. We take the union of both alignments to obtain a symmetrized word alignment matrix. This alignment matrix is the starting point for the phrase based extraction. Next, we define the criterion to extract the set of BP of the sentence pair (fj� j� ; ei� i�) and the alignment matrix A C_ J*I, which is identical to the alignment criterion described in [11]. BP(fJ1 ,eI1,A) _ {(f�l,ezl) : d(j, i)eA : j1 &lt; j &lt; j2 H i1 &lt; i &lt; i2 nEI(j, i)eA : j1 &lt; j &lt; j2 n i1 &lt; i &lt; i2} The set of BP is consistent with the alignment and consists of all BP pairs where all words within the foreign language phrase are only aligned to the words of the English language phrase and viceversa. At least one word in the foreign language phrase has to be aligned with at least one word of the English language. Finally, the algorithm takes into account possibly unaligned words at the boundaries of the foreign or English language phrases. 3 Phrase Extraction Motivation. The length</context>
</contexts>
<marker>[11]</marker>
<rawString>F. J. Och and H. Ney. The Alignment Template Approach to Statistical Machine Translation. Computational linguistics, 30:417–449, December 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative Training and Maximum Entropy Models for Statistical Machine Translation. In</title>
<date>2002</date>
<booktitle>ACL,</booktitle>
<pages>295--302</pages>
<contexts>
<context position="7968" citStr="[12]" startWordPosition="1387" endWordPosition="1387">target language. Clearly, &amp;quot;la que no&amp;quot; is not a good translation of &amp;quot;you&amp;quot;, so this phrase should have a low probability. However, from our aligned training database we obtain, P(f|e) = P(you|la que no) = 0.23 This BP is clearly overestimated due to sparseness. On the other, note that &amp;quot;la que no&amp;quot; cannot be considered an unusual trigram in Spanish. Hence, the language model does not penalise this target sequence either. So, the total probability (P(f|e)P(e)) would be higher than desired. In order to somehow compensate these unreiliable probabilities we have studied the inclusion of the posterior [12] and lexical probabilities [1] [10] as additional features. 4.2 Feature P(elf) In order to estimate the posterior phrase probability, we compute again the relative frequency but replacing the count of the target phrase by the count of the source phrase. (5) N(f) where N&apos;(f,e) means the number of times the phrase e is translated by f. If a phrase f has N &gt; 1 possible translations, then each one contributes as 1/N. Adding this feature function we reduce the number of cases in which the overall probability is overestimated. This results in an important improvement in translation quality. 4.3 IBM </context>
</contexts>
<marker>[12]</marker>
<rawString>Franz Josef Och and Hermann Ney. Discriminative Training and Maximum Entropy Models for Statistical Machine Translation. In ACL, pages pages 295–302, July 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Roukos Papineni</author>
<author>R T Ward</author>
</authors>
<title>Featurebased language understanding.</title>
<date>1997</date>
<booktitle>In European Conf. on Speech Communication and Technology,</booktitle>
<pages>1435--1438</pages>
<contexts>
<context position="2370" citStr="[13]" startWordPosition="380" endWordPosition="380">) and a separate translation model P(fIe) [5]. In the last few years, new systems tend to use sequences of words, commonly called phrases [8], aiming at introducing word context in the translation model. As alternative to the source-channel approach the decision rule can be modeled through a log-linear maximum entropy framework. e� = argmax e ��� � M=1 AMhM(e,f) (3) The features functions, hM, are the system models (translation model, language model and others) and weigths, Ai, are typically optimized to maximize a scoring function. It is derived from the Maximum Entropy approach suggested by [13] [14] for a natural language understanding task. It has the advantatge that additional features functions can be easily integrated in the overall system. This paper addresses a modification of the phrase-extraction algorythm in [11]. It also combines several interesting features and it reports an important improvement from the baseline. It is organized as follows. Section 2 introduces the baseline; the following section explains the modification in the phrase extraction; section 4 shows the different features which have been taken into account; section 5 presents the evaluation framework; and </context>
</contexts>
<marker>[13]</marker>
<rawString>Papineni, S.Roukos, and R.T. Ward. Featurebased language understanding. In European Conf. on Speech Communication and Technology, pages 1435–1438, September 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Roukos Papineni</author>
<author>R T Ward</author>
</authors>
<title>Maximum likelihood and discriminative training of direct translation models.</title>
<date>1998</date>
<booktitle>In Proc. Int. Conf. on Acoustics, Speech, and Signal Proceedings,</booktitle>
<pages>189--192</pages>
<contexts>
<context position="2375" citStr="[14]" startWordPosition="381" endWordPosition="381"> a separate translation model P(fIe) [5]. In the last few years, new systems tend to use sequences of words, commonly called phrases [8], aiming at introducing word context in the translation model. As alternative to the source-channel approach the decision rule can be modeled through a log-linear maximum entropy framework. e� = argmax e ��� � M=1 AMhM(e,f) (3) The features functions, hM, are the system models (translation model, language model and others) and weigths, Ai, are typically optimized to maximize a scoring function. It is derived from the Maximum Entropy approach suggested by [13] [14] for a natural language understanding task. It has the advantatge that additional features functions can be easily integrated in the overall system. This paper addresses a modification of the phrase-extraction algorythm in [11]. It also combines several interesting features and it reports an important improvement from the baseline. It is organized as follows. Section 2 introduces the baseline; the following section explains the modification in the phrase extraction; section 4 shows the different features which have been taken into account; section 5 presents the evaluation framework; and 149 P</context>
</contexts>
<marker>[14]</marker>
<rawString>Papineni, S.Roukos, and R.T. Ward. Maximum likelihood and discriminative training of direct translation models. In Proc. Int. Conf. on Acoustics, Speech, and Signal Proceedings, pages 189–192, May 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K A Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation. In</title>
<date>2001</date>
<tech>Technical Report RC22176 (W0109-022),</tech>
<institution>IBM Research Division,</institution>
<contexts>
<context position="11002" citStr="[15]" startWordPosition="1916" endWordPosition="1916">vor shorter output [7]. The phrase penalty is a constant cost per produced phrase. Here, a negative weight, which means reducing the costs per phrase, results in a preference for adding phrases. Alternatively, by using a positive scaling factors, the system will favor less phrases. 5.2 Experiments The decoder used for the presented translation system is reported in [2]. This decoder is called MARIE and it takes into account simultaneously all the 7 features functions described above. It implements a beam-search strategy. As evaluation criteria we use: the Word Error Rate (WER), the BLEU score [15] and the NIST score [3]. As follows we report the results for several experiments that show the performance of: the baseline, adding the posterior probability, IBM Model 1 and IBM1−1, and, finally, the modification of the phrases extraction. Optimisation. Significant improvements can be obtained by tuning the parameters of the features adequately. In the complet system we have 7 parameters to tune: the relatives frecuencies P(f|e) and P(elf), IBM Model 1 and its inverse, the word penalty, the phrase penalty and the weight of the language model. We applied the widely used algorithm SIMPLEX to o</context>
</contexts>
<marker>[15]</marker>
<rawString>K.A. Papineni, S. Roukos, T. Ward, and W.J. Zhu. Bleu: a method for automatic evaluation of machine translation. In Technical Report RC22176 (W0109-022), IBM Research Division, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM - An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings Intl. Conference Spoken Language Processing,</booktitle>
<contexts>
<context position="9760" citStr="[16]" startWordPosition="1703" endWordPosition="1703">re can be words that are not in the P(fj|ei) table. In this case, the probability was taken to be 10−40. In addition, we have calculated the IBM−1 Model 1. ( + 1) � � p(ei|fj) (7) I=1 j=0 4.4 Language Model The English language model plays an important role in the source channel model, see equation (2), and also in its modification, see equation (3). The English language model should give an idea of the sentence quality that is generated. As default language model feature, we use a standard word-based trigram language model generated with smoothing Kneser-Ney and interpolation (by using SRILM [16]). 4.5 Word and Phrase Penalty To compensate the preference of the target language model for shorter sentences, we added two N�(f, e) P(e|f) = 1 I J P(e|f; M1) = J I 151 Spanish English Train Sentences 1223443 1223443 Words 34794006 33379333 Vocabulary 168685 104975 Dev Sentences 504 504 Words 15353 15335 OOV 25 16 Test Sentences 504 504 Words 10305 10667 OOV 36 19 Table 1: Statistics of training and test corpus simple features which are widely used [17] [7]. The word penalty provides means to ensure that the translations do not get too long or too short. Negative values for the word penalty f</context>
</contexts>
<marker>[16]</marker>
<rawString>A. Stolcke. SRILM - An Extensible Language Modeling Toolkit. In Proceedings Intl. Conference Spoken Language Processing, September 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Improvements in PhraseBased Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference (HLT-NAACL),</booktitle>
<pages>257--264</pages>
<contexts>
<context position="4022" citStr="[17]" startWordPosition="649" endWordPosition="649">s. 2 Baseline The baseline is based on the source-channel approach, and it is composed of the following models which later will be combined in the decoder. The Translation Model. It is based on bilingual phrases, where a bilingual phrase (BP) is simply two monolingual phrases (MP) in which each one is supposed to be the translation of each other. A monolingual phrase is a sequence of words. Therefore, the basic idea of phrase-based translation is to segment the given source sentence into phrases, then translate each phrase and finally compose the target sentence from these phrase translations [17]. During training, the system has to learn a dictionary of phrases. We begin by aligning the training corpus using GIZA++ [6], which is done in both translation directions. We take the union of both alignments to obtain a symmetrized word alignment matrix. This alignment matrix is the starting point for the phrase based extraction. Next, we define the criterion to extract the set of BP of the sentence pair (fj� j� ; ei� i�) and the alignment matrix A C_ J*I, which is identical to the alignment criterion described in [11]. BP(fJ1 ,eI1,A) _ {(f�l,ezl) : d(j, i)eA : j1 &lt; j &lt; j2 H i1 &lt; i &lt; i2 nEI(</context>
<context position="6981" citStr="[17]" startWordPosition="1216" endWordPosition="1216">( 22 ) (...) where the number inside the clauses is the aligned word(s). And the phrase that we are looking for is the following one. los derechos de los trabajadores # workers &apos; rights which only could appear in the case the maximum length was 5. 150 4 Phrase ranking 4.1 Conditional probability P(f|e) Given the collected phrase pairs, we estimated the phrase translation probability distribution by relative frecuency. N(f, e) P(f|e) = (4) N(e) where N(f,e) means the number of times the phrase f is translated by e. If a phrase e has N &gt; 1 possible translations, then each one contributes as 1/N [17]. Note that no smoothing is performed, which may cause an overestimation of the probability of rare phrases. This is specially harmful given a BP where the source part has a big frecuency of appearence but the target part appears rarely. For example, from our database we can extract the following BP: ”you # la que no&amp;quot;, where the English is the source language and the Spanish, the target language. Clearly, &amp;quot;la que no&amp;quot; is not a good translation of &amp;quot;you&amp;quot;, so this phrase should have a low probability. However, from our aligned training database we obtain, P(f|e) = P(you|la que no) = 0.23 This BP i</context>
<context position="10218" citStr="[17]" startWordPosition="1784" endWordPosition="1784">uage model feature, we use a standard word-based trigram language model generated with smoothing Kneser-Ney and interpolation (by using SRILM [16]). 4.5 Word and Phrase Penalty To compensate the preference of the target language model for shorter sentences, we added two N�(f, e) P(e|f) = 1 I J P(e|f; M1) = J I 151 Spanish English Train Sentences 1223443 1223443 Words 34794006 33379333 Vocabulary 168685 104975 Dev Sentences 504 504 Words 15353 15335 OOV 25 16 Test Sentences 504 504 Words 10305 10667 OOV 36 19 Table 1: Statistics of training and test corpus simple features which are widely used [17] [7]. The word penalty provides means to ensure that the translations do not get too long or too short. Negative values for the word penalty favor longer output, positive values favor shorter output [7]. The phrase penalty is a constant cost per produced phrase. Here, a negative weight, which means reducing the costs per phrase, results in a preference for adding phrases. Alternatively, by using a positive scaling factors, the system will favor less phrases. 5.2 Experiments The decoder used for the presented translation system is reported in [2]. This decoder is called MARIE and it takes into </context>
</contexts>
<marker>[17]</marker>
<rawString>R. Zens and H. Ney. Improvements in PhraseBased Statistical Machine Translation. In Proceedings of the Human Language Technology Conference (HLT-NAACL), pages 257– 264, May 2004.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>