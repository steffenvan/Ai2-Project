<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.032033">
<title confidence="0.9995935">
A Multitask Objective to Inject Lexical Contrast
into Distributional Semantics
</title>
<author confidence="0.943636">
Nghia The Pham Angeliki Lazaridou Marco Baroni
</author>
<affiliation confidence="0.9804425">
Center for Mind/Brain Sciences
University of Trento
</affiliation>
<email confidence="0.992009">
{thenghia.pham|angeliki.lazaridou|marco.baroni}@unitn.it
</email>
<sectionHeader confidence="0.99372" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999898952380952">
Distributional semantic models have trou-
ble distinguishing strongly contrasting
words (such as antonyms) from highly
compatible ones (such as synonyms), be-
cause both kinds tend to occur in similar
contexts in corpora. We introduce the mul-
titask Lexical Contrast Model (mLCM),
an extension of the effective Skip-gram
method that optimizes semantic vectors
on the joint tasks of predicting corpus
contexts and making the representations
of WordNet synonyms closer than that
of matching WordNet antonyms. mLCM
outperforms Skip-gram both on general
semantic tasks and on synonym/antonym
discrimination, even when no direct lex-
ical contrast information about the test
words is provided during training. mLCM
also shows promising results on the task
of learning a compositional negation oper-
ator mapping adjectives to their antonyms.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999896818181818">
Distributional semantic models (DSMs) extract
vectors representing word meaning by relying on
the distributional hypothesis, that is, the idea that
words that are related in meaning will tend to oc-
cur in similar contexts (Turney and Pantel, 2010).
While extensive work has shown that contextual
similarity is an excellent proxy to semantic simi-
larity, a big problem for DSMs is that both words
with very compatible meanings (e.g., near syn-
onyms) and words with strongly contrasting mean-
ings (e.g., antonyms) tend to occur in the same
contexts. Indeed, Mohammad et al. (2013) have
shown that synonyms and antonyms are indistin-
guishable in terms of their average degree of dis-
tributional similarity.
This is problematic for the application of DSMs
to reasoning tasks such as entailment detection
(black is very close to both dark and white in dis-
tributional semantic space, but it implies the for-
mer while contradicting the latter). Beyond word-
level relations, the same difficulties make it chal-
lenging for compositional extensions of DSMs
to capture the fundamental phenomenon of nega-
tion at the phrasal and sentential levels (the dis-
tributional vectors for good and not good are
nearly identical) (Hermann et al., 2013; Preller
and Sadrzadeh, 2011).
Mohammad and colleagues concluded that
DSMs alone cannot detect semantic contrast, and
proposed an approach that couples them with other
resources. Pure-DSM solutions include isolating
contexts that are expected to be more discrimina-
tive of contrast, tuning the similarity measure to
make it more sensitive to contrast or training a su-
pervised contrast classifier on DSM vectors (Adel
and Sch¨utze, 2014; Santus et al., 2014; Schulte im
Walde and K¨oper, 2013; Turney, 2008). We pro-
pose instead to induce word vectors using a mul-
titask cost function combining a traditional DSM
context-prediction objective with a term forcing
words to be closer to their WordNet synonyms
than to their antonyms. In this way, we make the
model aware that contrasting words such as hot
and cold, while still semantically related, should
not be nearest neighbours in the space.
In a similar spirit, Yih et al. (2012) devise a
DSM in which the embeddings of the antonyms
of a word are pushed to be the vectors that are
farthest away from its representation. While their
model is able to correctly pick the antonym of a
target item from a list of candidates (since it is
the most dissimilar element in the list), we con-
jecture that their radical strategy produces embed-
dings with poor performance on general semantic
tasks.1 Our method has instead a beneficial global
</bodyText>
<footnote confidence="0.841048666666667">
1Indeed, by simulating their strategy, we were able to in-
ject lexical contrast into word embeddings, but performance
on a general semantic relatedness task decreased dramati-
</footnote>
<page confidence="0.964446">
21
</page>
<bodyText confidence="0.94881392">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 21–26,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
effect on semantic vectors, leading to state-of-the-
art results in a challenging similarity task, and en-
abling better learning of a compositional negation
function.
Our work is also closely related to Faruqui et al.
(2015), who propose an algorithm to adapt pre-
trained DSM representations using semantic re-
sources such as WordNet. This post-processing
approach, while extremely effective, has the dis-
advantage that changes only affect words that are
present in the resource, without propagating to
the whole lexicon. Other recent work has instead
adopted multitask objectives similar to ours in or-
der to directly plug in knowledge from structured
resources at DSM induction time (Fried and Duh,
2015; Xu et al., 2014; Yu and Dredze, 2014). Our
main novelties with respect to these proposals are
the focus on capturing semantic contrast, and ex-
plicitly testing the hypothesis that the multitask
objective is also beneficial to words that are not di-
rectly exposed to WordNet evidence during train-
ing.2
</bodyText>
<sectionHeader confidence="0.813371" genericHeader="method">
2 The multitask Lexical Contrast Model
</sectionHeader>
<bodyText confidence="0.989748428571429">
Skip-gram model The multitask Lexical Con-
trast Model (mLCM) extends the Skip-gram
model (Mikolov et al., 2013). Given an input
text corpus, Skip-gram optimizes word vectors
on the task of approximating, for each word, the
probability of other words to occur in its context.
More specifically, its objective function is:
</bodyText>
<equation confidence="0.975382">
⎝⎛⎞
X log p(wt+j|wt) ⎠(1)
−c&lt;j&lt;c,j=,40
</equation>
<bodyText confidence="0.999453375">
where w1, w2, ..., wT is the training corpus,
consisting of a list of target words wt, for which
we want to learn the vector representations (and
serving as contexts of each other), and c is the
window size determining the span of context
words to be considered. p(wt+j|wt), the proba-
bility of a context word given the target word is
computed using softmax:
</bodyText>
<equation confidence="0.99624">
T
ewt+j vwt
p(wt+j|wt) = W v v (2)
,T
Pwi=1 e wi wt
</equation>
<bodyText confidence="0.945766176470588">
cally, with a 25% drop in terms of Spearman correlation.
2After submitting this work, we became aware of Ono et
al. (2015), that implement very similar ideas. However, one
major difference between their work and ours is that their
strategy is in the same direction of (Yih et al., 2012), which
might result in poor performance on general semantic tasks.
where vw and v&apos;w are respectively the target and
context vector representations of word w, and W
is the number of words in the vocabulary. To avoid
the O(|W |) time complexity of the normalization
term in Equation (2), Mikolov et al. (2013) use
either hierarchical softmax or negative sampling.
Here, we adopt the negative sampling method.
Injecting lexical contrast information We
account for lexical contrast by implementing a
2-task strategy, combining the Skip-gram context
prediction objective with a new term:
</bodyText>
<equation confidence="0.996717">
(Jskipgram(wt) + Jlc(wt)) (3)
</equation>
<bodyText confidence="0.989989444444444">
The lexical contrast objective Jlc(wt) tries to en-
force the constraint that contrasting pairs should
have lower similarity than compatible ones within
a max-margin framework. Our formulation is in-
spired by Lazaridou et al. (2015), who use a sim-
ilar multitask strategy to induce multimodal em-
beddings. Given a target word w, with sets of
antonyms A(w) and synonyms S(w), the max-
margin objective for lexical contrast is:
</bodyText>
<equation confidence="0.998289">
X− max(0, A − cos(vw, vs)
sES(w),aEA(w)
+ cos(vw,va)) (4)
</equation>
<bodyText confidence="0.990126625">
where A is the margin and cos(x, y) stands for
cosine similarity between vectors x and y. Note
that, by equation (3), the Jlc(wt) term is evalu-
ated each time a word is encountered in the corpus.
We extract antonym and synonym sets from Word-
Net (Miller, 1995). If a word wt is not associated
to synonym/antonym information in WordNet, we
set Jlc(wt) = 0.
</bodyText>
<sectionHeader confidence="0.995465" genericHeader="method">
3 Experimental setup
</sectionHeader>
<bodyText confidence="0.9994101">
We compare the performance of mLCM against
Skip-gram. Both models’ parameters are esti-
mated by backpropagation of error via stochastic
gradient descent. Our text corpus is a Wikipedia3
2009 dump comprising approximately 800M to-
kens and 200K distinct word types.4 Other hyper-
parameters, selected without tuning, include: vec-
tor size (300), window size (5), negative sam-
ples (10), sub-sampling to disfavor frequent words
(10−3). For mLCM, we use 7500 antonym pairs
</bodyText>
<footnote confidence="0.967194666666667">
3https://en.wikipedia.org
4We only consider words that occur more than 50 times in
the corpus
</footnote>
<figure confidence="0.850041375">
1
T
XT
t=1
1
T
XT
t=1
</figure>
<page confidence="0.994064">
22
</page>
<tableCaption confidence="0.977374">
Table 1: Relatedness/similarity tasks
</tableCaption>
<table confidence="0.999392428571429">
MEN SimLex
Skip-gram 0.73 0.39
mLCM 0.74 0.52
AUC
Skip-gram 0.62
mLCM 0.78
mLCM-propagate 0.66
</table>
<tableCaption confidence="0.999102">
Table 2: Synonym vs antonym task
</tableCaption>
<bodyText confidence="0.99982575">
and 15000 synonym pairs; on average, 2.5 pairs
per word and 9000 words are covered.
Both models are evaluated in four tasks:
two lexical tasks testing the general quality of
the learned embeddings and one focusing on
antonymy, and a negation task which verifies the
positive influence of lexical contrast in a composi-
tional setting.
</bodyText>
<sectionHeader confidence="0.999232" genericHeader="method">
4 Lexical tasks
</sectionHeader>
<subsectionHeader confidence="0.990257">
4.1 Relatedness and similarity
</subsectionHeader>
<bodyText confidence="0.999765227272727">
In classic semantic relatedness/similarity tasks,
the models provide cosine scores between pairs of
word vectors that are then compared to human rat-
ings for the same pairs. Performance is evaluated
by Spearman correlation between system and hu-
man scores. For general relatedness, we use the
MEN dataset of Bruni et al. (2014), which con-
sists of 3,000 word pairs comprising 656 nouns,
57 adjectives and 38 verbs. The SimLex dataset
from Hill et al. (2014b), comprising 999 word
pairs (666 noun, 222 verb and 111 adjective pairs)
was explicitly built to test a tighter notion of strict
“semantic” similarity.
Table 1 reports model performance. On MEN,
mLCM outperforms Skip-gram by a small margin,
which shows that the new information, at the very
least, does not have any negative effect on gen-
eral semantic relatedness. On the other hand, lex-
ical contrast information has a strong positive ef-
fect on measuring strict semantic similarity, lead-
ing mLCM to achieve state-of-the-art SimLex per-
formance (Hill et al., 2014a).
</bodyText>
<subsectionHeader confidence="0.999751">
4.2 Distinguishing antonyms and synonyms
</subsectionHeader>
<bodyText confidence="0.999962913043478">
Having shown that capturing lexical contrast in-
formation results in higher-quality representations
for general purposes, we focus next on the spe-
cific task of distinguishing contrasting words from
highly compatible ones. We use the adjective part
of dataset of Santus et al. (2014), that contains 262
antonym and 364 synonym pairs. We compute co-
sine similarity of all pairs and use the area under
the ROC curve (AUC) to measure model perfor-
mance. Moreover, we directly test mLCM’s abil-
ity to propagate lexical contrast across the vocab-
ulary by retraining it without using WordNet in-
formation for any of the words in the dataset, i.e.
the words in the dataset are removed from the syn-
onym or antonym sets of all the adjectives used in
training (mLCM-propagate in the results table).
The results, in Table 2, show that mLCM can
successfully learn to distinguish contrasting words
from synonyms. The performance of the mLCM
model trained without explicit contrast informa-
tion about the dataset words proves moreover that
lexical contrast information is indeed propagated
through the lexical network.
</bodyText>
<subsectionHeader confidence="0.980791">
4.3 Vector space structure
</subsectionHeader>
<bodyText confidence="0.999924807692308">
To further investigate the effect of lexical con-
trast information, we perform a qualitative anal-
ysis of how it affects the space structure. We pick
20 scalar adjectives denoting spatial or weight-
related aspects of objects and living beings, where
10 indicate the presence of the relevant property
to a great degree (big, long, heavy... ), whereas
the remaining 10 suggest that the property is
present in little amounts (little, short, light... ).
We project the 300-dimensional vectors of these
adjectives onto a 2-dimensional plane using the
t-SNE toolkit,5 which attempts to preserve the
structure of the original high-dimensional word
neighborhoods. Figure 1 shows that, in Skip-
gram space, pairs at the extreme of the same scale
(light vs heavy, narrow vs wide, fat vs skinny) are
very close to each other compared to other words;
whereas for mLCM the extremes are farther apart
from each other, as expected. Moreover, the ad-
jectives at the two ends of the scales are grouped
together. This is a very nice property, since many
adjectives in one group will tend to characterize
the same objects. Within the two clusters, words
that are more similar (e.g., wide and broad) are
still closer to each other, just as we would expect
them to be.
</bodyText>
<footnote confidence="0.974837">
5http://lvdmaaten.github.io/tsne/
</footnote>
<page confidence="0.997068">
23
</page>
<figure confidence="0.999756684210527">
-1000
1500
1000
-500
500
0
small
tiny
miniature
giant
largehuge
petite
long
short
broad
light
heavy
littlebig
skinny
narrow
wide
fat
deepshallow
large-group
small-group
thick
thin
tal
-1000
-1500
1500
1000
-500
500
0
skinny
petite
narrow shallow
thin light thick
tiny
small
short
miniature
little
tall
big large
gianthuge long
fat
deep
heavy
large-group
small-group
widebroa
-2000 -1500 -1000 -500 0 500 1000 1500 2000
(a) Skip-gram space
-800 -600 -400 -200 0 200 400 600 800 1000
(b) mLCM space
</figure>
<figureCaption confidence="0.999976">
Figure 1: Arrangement of some scalar adjectives in Skip-gram vs mLCM spaces
</figureCaption>
<sectionHeader confidence="0.946357" genericHeader="method">
5 Learning Negation
</sectionHeader>
<bodyText confidence="0.999950555555556">
Having shown that injecting lexical contrast in-
formation into word embeddings is beneficial for
lexical tasks, we further explore if it can also
help composition. Since mLCM makes contrast-
ing and compatible words more distinguishable
from each other, we conjecture that it would be
easier for compositional DSMs to capture negation
in mLCM space. We perform a proof-of-concept
experiment where we represent not as a function
that is trained to map an adjective to its antonym
(good to bad). That is, by adopting the frame-
work of Baroni et al. (2014), we take not to be
a matrix that, when multiplied with an adjective-
representing vector, returns the vector of an adjec-
tive with the opposite meaning. We realize that
this is capturing only a tiny fraction of the linguis-
tic uses of negation, but it is at least a concrete
starting point.
First, we select a list of adjectives and antonyms
from WordNet; for each adjective, we only pick
the antonym of its first sense. This yields a to-
tal of around 4,000 antonym pairs. Then, we in-
duce the not matrix with least-squares regression
on training pairs. Finally, we assess the learned
negation function by applying it to an adjective
and computing accuracy in the task of retrieving
the correct antonym as nearest neighbour of the
not-composed vector, searching across all Word-
Net adjectives (10K items). The results in Table 3
are obtained by using 10-fold cross-validation on
the 4,000 pairs. We see that mLCM outperforms
Skip-gram by a large margin.
Figure 2 shows heatmaps of the weight matrices
learnt for not by the two models. Intriguingly, for
mLCM, the not matrix has negative values on the
diagonal, that is, it will tend to flip the values in
</bodyText>
<table confidence="0.810016">
train test
Skip-gram 0.44 0.02
mLCM 0.87 0.27
</table>
<tableCaption confidence="0.817434">
Table 3: Average accuracy in retrieving antonym
as nearest neighbour when applying the not com-
position function to 4,000 adjectives.
</tableCaption>
<figure confidence="0.996760428571429">
0.3
0.2
0.1
0
-0.1
-0.2
Skip-Gram mLCM
</figure>
<figureCaption confidence="0.99997">
Figure 2: Heatmaps of not-composition matrices.
</figureCaption>
<bodyText confidence="0.999593875">
the input vector, not unlike what arithmetic nega-
tion would do. On the other hand, the Skip-gram-
based not matrix is remarkably identity-like, with
large positive values concentrated on the diagonal.
Thus, under this approach, an adjective will be al-
most identical to its antonym, which explains why
it fails completely on the test set data: the nearest
neighbour of not-X will typically be X itself.
</bodyText>
<sectionHeader confidence="0.999249" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999304">
Given the promise shown by mLCM in the ex-
periments reported here, we plan to test it next
on a range of linguistically interesting phenomena
that are challenging for DSMs and where lexical
contrast information might help. These include
modeling a broader range of negation types (de
Swart, 2010), capturing lexical and phrasal infer-
ence (Levy et al., 2015), deriving adjectival scales
(Kim and de Marneffe, 2013) and distinguishing
semantic similarity from referential compatibility
</bodyText>
<page confidence="0.996651">
24
</page>
<note confidence="0.505907">
(Kruszewski and Baroni, 2015).
</note>
<sectionHeader confidence="0.989739" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.996022">
This research was supported by the ERC 2011
Starting Independent Research Grant n. 283554
(COMPOSES).
</bodyText>
<sectionHeader confidence="0.998124" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999357208333333">
Heike Adel and Hinrich Sch¨utze. 2014. Using mined
coreference chains as a resource for a semantic task.
In Proceedings of EMNLP, pages 1447–1452, Doha,
Qatar.
Marco Baroni, Raffaella Bernardi, and Roberto Zam-
parelli. 2014. Frege in space: A program for com-
positional distributional semantics. Linguistic Is-
sues in Language Technology, 9(6):5–110.
Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. Journal of Ar-
tificial Intelligence Research, 49:1–47.
Henriette de Swart. 2010. Expression and Interpreta-
tion of Negation: an OT Typology. Springer, Dor-
drecht, Netherlands.
Manaal Faruqui, Jesse Dodge, Sujay Jauhar, Chris
Dyer, Ed Hovy, and Noah Smith. 2015. Retrofitting
word vectors to semantic lexicons. In Proceedings
of NAACL, Denver, CO. In press.
Daniel Fried and Kevin Duh. 2015. Incorporat-
ing both distributional and relational semantics in
word representations. In Proceedings of ICLR
Workshop Track, San Diego, CA. Published on-
line: http://www.iclr.cc/doku.php?id=
iclr2015:main#accepted_papers.
Karl Moritz Hermann, Edward Grefenstette, and Phil
Blunsom. 2013. “Not not bad” is not “bad”: A dis-
tributional account of negation. In Proceedings of
ACL Workshop on Continuous Vector Space Mod-
els and their Compositionality, pages 74–82, Sofia,
Bulgaria.
Felix Hill, KyungHyun Cho, Sebastien Jean, Coline
Devin, and Yoshua Bengio. 2014a. Not all neu-
ral embeddings are born equal. arXiv preprint
arXiv:1410.0718.
Felix Hill, Roi Reichart, and Anna Korhonen. 2014b.
Simlex-999: Evaluating semantic models with
(genuine) similarity estimation. arXiv preprint
arXiv:1408.3456.
Joo-Kyung Kim and Marie-Catherine de Marneffe.
2013. Deriving adjectival scales from continu-
ous space word representations. In Proceedings of
EMNLP, pages 1625–1630, Seattle, WA.
Germ´an Kruszewski and Marco Baroni. 2015. So sim-
ilar and yet incompatible: Toward automated identi-
fication of semantically compatible words. In Pro-
ceedings of NAACL, pages 64–969, Denver, CO.
Angeliki Lazaridou, Nghia The Pham, and Marco Ba-
roni. 2015. Combining language and vision with
a multimodal skip-gram model. In Proceedings of
NAACL, pages 153–163, Denver, CO.
Omer Levy, Steffen Remus, Chris Biemann, , and Ido
Dagan. 2015. Do supervised distributional methods
really learn lexical inference relations? In Proceed-
ings of NAACL, Denver, CO. In press.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL,
pages 746–751, Atlanta, Georgia.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39–41.
Saif Mohammad, Bonnie Dorr, Graeme Hirst, and Pe-
ter Turney. 2013. Computing lexical contrast. Com-
putational Linguistics, 39(3):555–590.
Masataka Ono, Makoto Miwa, and Yutaka Sasaki.
2015. Word embedding-based antonym detection
using thesauri and distributional information. In
Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 984–989, Denver, Colorado, May–June. As-
sociation for Computational Linguistics.
Anne Preller and Mehrnoosh Sadrzadeh. 2011. Bell
states and negative sentences in the distributed
model of meaning. Electr. Notes Theor. Comput.
Sci., 270(2):141–153.
Enrico Santus, Qin Lu, Alessandro Lenci, and Chu-Ren
Huang. 2014. Taking antonymy mask off in vector
space. In Proceedings of PACLIC, pages 135–144,
Phuket,Thailand.
Sabine Schulte im Walde and Maximilian K¨oper. 2013.
Pattern-based distinction of paradigmatic relations
for German nouns, verbs, adjectives. In Proceed-
ings of GSCL, pages 184–198, Darmstadt, Germany.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141–188.
Peter Turney. 2008. A uniform approach to analogies,
synonyms, antonyms and associations. In Proceed-
ings of COLING, pages 905–912, Manchester, UK.
Chang Xu, Yalong Bai, Jiang Bian, Bin Gao, Gang
Wang, Xiaoguang Liu, and Tie-Yan Liu. 2014. RC-
NET: A general framework for incorporating knowl-
edge into word representations. In Proceedings of
CIKM, pages 1219–1228, Shanghai, China.
</reference>
<page confidence="0.964655">
25
</page>
<reference confidence="0.995132166666667">
Wen-tau Yih, Geoffrey Zweig, and John Platt. 2012.
Polarity inducing latent semantic analysis. In Pro-
ceedings of EMNLP-CONLL, pages 1212–1222.
Mo Yu and Mark Dredze. 2014. Improving lexical em-
beddings with semantic knowledge. In Proceedings
of ACL, pages 545–550, Baltimore, MD.
</reference>
<page confidence="0.998035">
26
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.353835">
<title confidence="0.875947">A Multitask Objective to Inject Lexical into Distributional Semantics</title>
<affiliation confidence="0.855264666666667">Nghia The Pham Angeliki Lazaridou Marco Center for Mind/Brain University of Trento</affiliation>
<abstract confidence="0.999307818181818">Distributional semantic models have trouble distinguishing strongly contrasting words (such as antonyms) from highly compatible ones (such as synonyms), because both kinds tend to occur in similar contexts in corpora. We introduce the multitask Lexical Contrast Model (mLCM), an extension of the effective Skip-gram method that optimizes semantic vectors on the joint tasks of predicting corpus contexts and making the representations of WordNet synonyms closer than that of matching WordNet antonyms. mLCM outperforms Skip-gram both on general semantic tasks and on synonym/antonym discrimination, even when no direct lexical contrast information about the test words is provided during training. mLCM also shows promising results on the task of learning a compositional negation operator mapping adjectives to their antonyms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Heike Adel</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Using mined coreference chains as a resource for a semantic task.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1447--1452</pages>
<location>Doha, Qatar.</location>
<marker>Adel, Sch¨utze, 2014</marker>
<rawString>Heike Adel and Hinrich Sch¨utze. 2014. Using mined coreference chains as a resource for a semantic task. In Proceedings of EMNLP, pages 1447–1452, Doha, Qatar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Raffaella Bernardi</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Frege in space: A program for compositional distributional semantics.</title>
<date>2014</date>
<journal>Linguistic Issues in Language Technology,</journal>
<volume>9</volume>
<issue>6</issue>
<contexts>
<context position="13427" citStr="Baroni et al. (2014)" startWordPosition="2156" endWordPosition="2159">some scalar adjectives in Skip-gram vs mLCM spaces 5 Learning Negation Having shown that injecting lexical contrast information into word embeddings is beneficial for lexical tasks, we further explore if it can also help composition. Since mLCM makes contrasting and compatible words more distinguishable from each other, we conjecture that it would be easier for compositional DSMs to capture negation in mLCM space. We perform a proof-of-concept experiment where we represent not as a function that is trained to map an adjective to its antonym (good to bad). That is, by adopting the framework of Baroni et al. (2014), we take not to be a matrix that, when multiplied with an adjectiverepresenting vector, returns the vector of an adjective with the opposite meaning. We realize that this is capturing only a tiny fraction of the linguistic uses of negation, but it is at least a concrete starting point. First, we select a list of adjectives and antonyms from WordNet; for each adjective, we only pick the antonym of its first sense. This yields a total of around 4,000 antonym pairs. Then, we induce the not matrix with least-squares regression on training pairs. Finally, we assess the learned negation function by</context>
</contexts>
<marker>Baroni, Bernardi, Zamparelli, 2014</marker>
<rawString>Marco Baroni, Raffaella Bernardi, and Roberto Zamparelli. 2014. Frege in space: A program for compositional distributional semantics. Linguistic Issues in Language Technology, 9(6):5–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Nam Khanh Tran</author>
<author>Marco Baroni</author>
</authors>
<title>Multimodal distributional semantics.</title>
<date>2014</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>49--1</pages>
<contexts>
<context position="9146" citStr="Bruni et al. (2014)" startWordPosition="1456" endWordPosition="1459">th models are evaluated in four tasks: two lexical tasks testing the general quality of the learned embeddings and one focusing on antonymy, and a negation task which verifies the positive influence of lexical contrast in a compositional setting. 4 Lexical tasks 4.1 Relatedness and similarity In classic semantic relatedness/similarity tasks, the models provide cosine scores between pairs of word vectors that are then compared to human ratings for the same pairs. Performance is evaluated by Spearman correlation between system and human scores. For general relatedness, we use the MEN dataset of Bruni et al. (2014), which consists of 3,000 word pairs comprising 656 nouns, 57 adjectives and 38 verbs. The SimLex dataset from Hill et al. (2014b), comprising 999 word pairs (666 noun, 222 verb and 111 adjective pairs) was explicitly built to test a tighter notion of strict “semantic” similarity. Table 1 reports model performance. On MEN, mLCM outperforms Skip-gram by a small margin, which shows that the new information, at the very least, does not have any negative effect on general semantic relatedness. On the other hand, lexical contrast information has a strong positive effect on measuring strict semantic</context>
</contexts>
<marker>Bruni, Tran, Baroni, 2014</marker>
<rawString>Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014. Multimodal distributional semantics. Journal of Artificial Intelligence Research, 49:1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henriette de Swart</author>
</authors>
<title>Expression and Interpretation of Negation: an OT Typology.</title>
<date>2010</date>
<publisher>Springer,</publisher>
<location>Dordrecht, Netherlands.</location>
<marker>de Swart, 2010</marker>
<rawString>Henriette de Swart. 2010. Expression and Interpretation of Negation: an OT Typology. Springer, Dordrecht, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manaal Faruqui</author>
<author>Jesse Dodge</author>
<author>Sujay Jauhar</author>
<author>Chris Dyer</author>
<author>Ed Hovy</author>
<author>Noah Smith</author>
</authors>
<title>Retrofitting word vectors to semantic lexicons.</title>
<date>2015</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<location>Denver, CO.</location>
<note>In press.</note>
<contexts>
<context position="4355" citStr="Faruqui et al. (2015)" startWordPosition="668" endWordPosition="671">e to inject lexical contrast into word embeddings, but performance on a general semantic relatedness task decreased dramati21 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 21–26, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics effect on semantic vectors, leading to state-of-theart results in a challenging similarity task, and enabling better learning of a compositional negation function. Our work is also closely related to Faruqui et al. (2015), who propose an algorithm to adapt pretrained DSM representations using semantic resources such as WordNet. This post-processing approach, while extremely effective, has the disadvantage that changes only affect words that are present in the resource, without propagating to the whole lexicon. Other recent work has instead adopted multitask objectives similar to ours in order to directly plug in knowledge from structured resources at DSM induction time (Fried and Duh, 2015; Xu et al., 2014; Yu and Dredze, 2014). Our main novelties with respect to these proposals are the focus on capturing sema</context>
</contexts>
<marker>Faruqui, Dodge, Jauhar, Dyer, Hovy, Smith, 2015</marker>
<rawString>Manaal Faruqui, Jesse Dodge, Sujay Jauhar, Chris Dyer, Ed Hovy, and Noah Smith. 2015. Retrofitting word vectors to semantic lexicons. In Proceedings of NAACL, Denver, CO. In press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Fried</author>
<author>Kevin Duh</author>
</authors>
<title>Incorporating both distributional and relational semantics in word representations.</title>
<date>2015</date>
<booktitle>In Proceedings of ICLR Workshop Track,</booktitle>
<location>San Diego, CA.</location>
<note>Published online: http://www.iclr.cc/doku.php?id= iclr2015:main#accepted_papers.</note>
<contexts>
<context position="4832" citStr="Fried and Duh, 2015" startWordPosition="742" endWordPosition="745"> similarity task, and enabling better learning of a compositional negation function. Our work is also closely related to Faruqui et al. (2015), who propose an algorithm to adapt pretrained DSM representations using semantic resources such as WordNet. This post-processing approach, while extremely effective, has the disadvantage that changes only affect words that are present in the resource, without propagating to the whole lexicon. Other recent work has instead adopted multitask objectives similar to ours in order to directly plug in knowledge from structured resources at DSM induction time (Fried and Duh, 2015; Xu et al., 2014; Yu and Dredze, 2014). Our main novelties with respect to these proposals are the focus on capturing semantic contrast, and explicitly testing the hypothesis that the multitask objective is also beneficial to words that are not directly exposed to WordNet evidence during training.2 2 The multitask Lexical Contrast Model Skip-gram model The multitask Lexical Contrast Model (mLCM) extends the Skip-gram model (Mikolov et al., 2013). Given an input text corpus, Skip-gram optimizes word vectors on the task of approximating, for each word, the probability of other words to occur in</context>
</contexts>
<marker>Fried, Duh, 2015</marker>
<rawString>Daniel Fried and Kevin Duh. 2015. Incorporating both distributional and relational semantics in word representations. In Proceedings of ICLR Workshop Track, San Diego, CA. Published online: http://www.iclr.cc/doku.php?id= iclr2015:main#accepted_papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>Not not bad” is not “bad”: A distributional account of negation.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL Workshop on Continuous Vector Space Models and their Compositionality,</booktitle>
<pages>74--82</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="2305" citStr="Hermann et al., 2013" startWordPosition="342" endWordPosition="345">s and antonyms are indistinguishable in terms of their average degree of distributional similarity. This is problematic for the application of DSMs to reasoning tasks such as entailment detection (black is very close to both dark and white in distributional semantic space, but it implies the former while contradicting the latter). Beyond wordlevel relations, the same difficulties make it challenging for compositional extensions of DSMs to capture the fundamental phenomenon of negation at the phrasal and sentential levels (the distributional vectors for good and not good are nearly identical) (Hermann et al., 2013; Preller and Sadrzadeh, 2011). Mohammad and colleagues concluded that DSMs alone cannot detect semantic contrast, and proposed an approach that couples them with other resources. Pure-DSM solutions include isolating contexts that are expected to be more discriminative of contrast, tuning the similarity measure to make it more sensitive to contrast or training a supervised contrast classifier on DSM vectors (Adel and Sch¨utze, 2014; Santus et al., 2014; Schulte im Walde and K¨oper, 2013; Turney, 2008). We propose instead to induce word vectors using a multitask cost function combining a tradit</context>
</contexts>
<marker>Hermann, Grefenstette, Blunsom, 2013</marker>
<rawString>Karl Moritz Hermann, Edward Grefenstette, and Phil Blunsom. 2013. “Not not bad” is not “bad”: A distributional account of negation. In Proceedings of ACL Workshop on Continuous Vector Space Models and their Compositionality, pages 74–82, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Hill</author>
<author>KyungHyun Cho</author>
<author>Sebastien Jean</author>
<author>Coline Devin</author>
<author>Yoshua Bengio</author>
</authors>
<title>Not all neural embeddings are born equal. arXiv preprint arXiv:1410.0718.</title>
<date>2014</date>
<contexts>
<context position="9274" citStr="Hill et al. (2014" startWordPosition="1479" endWordPosition="1482"> antonymy, and a negation task which verifies the positive influence of lexical contrast in a compositional setting. 4 Lexical tasks 4.1 Relatedness and similarity In classic semantic relatedness/similarity tasks, the models provide cosine scores between pairs of word vectors that are then compared to human ratings for the same pairs. Performance is evaluated by Spearman correlation between system and human scores. For general relatedness, we use the MEN dataset of Bruni et al. (2014), which consists of 3,000 word pairs comprising 656 nouns, 57 adjectives and 38 verbs. The SimLex dataset from Hill et al. (2014b), comprising 999 word pairs (666 noun, 222 verb and 111 adjective pairs) was explicitly built to test a tighter notion of strict “semantic” similarity. Table 1 reports model performance. On MEN, mLCM outperforms Skip-gram by a small margin, which shows that the new information, at the very least, does not have any negative effect on general semantic relatedness. On the other hand, lexical contrast information has a strong positive effect on measuring strict semantic similarity, leading mLCM to achieve state-of-the-art SimLex performance (Hill et al., 2014a). 4.2 Distinguishing antonyms and s</context>
</contexts>
<marker>Hill, Cho, Jean, Devin, Bengio, 2014</marker>
<rawString>Felix Hill, KyungHyun Cho, Sebastien Jean, Coline Devin, and Yoshua Bengio. 2014a. Not all neural embeddings are born equal. arXiv preprint arXiv:1410.0718.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Hill</author>
<author>Roi Reichart</author>
<author>Anna Korhonen</author>
</authors>
<title>Simlex-999: Evaluating semantic models with (genuine) similarity estimation. arXiv preprint arXiv:1408.3456.</title>
<date>2014</date>
<contexts>
<context position="9274" citStr="Hill et al. (2014" startWordPosition="1479" endWordPosition="1482"> antonymy, and a negation task which verifies the positive influence of lexical contrast in a compositional setting. 4 Lexical tasks 4.1 Relatedness and similarity In classic semantic relatedness/similarity tasks, the models provide cosine scores between pairs of word vectors that are then compared to human ratings for the same pairs. Performance is evaluated by Spearman correlation between system and human scores. For general relatedness, we use the MEN dataset of Bruni et al. (2014), which consists of 3,000 word pairs comprising 656 nouns, 57 adjectives and 38 verbs. The SimLex dataset from Hill et al. (2014b), comprising 999 word pairs (666 noun, 222 verb and 111 adjective pairs) was explicitly built to test a tighter notion of strict “semantic” similarity. Table 1 reports model performance. On MEN, mLCM outperforms Skip-gram by a small margin, which shows that the new information, at the very least, does not have any negative effect on general semantic relatedness. On the other hand, lexical contrast information has a strong positive effect on measuring strict semantic similarity, leading mLCM to achieve state-of-the-art SimLex performance (Hill et al., 2014a). 4.2 Distinguishing antonyms and s</context>
</contexts>
<marker>Hill, Reichart, Korhonen, 2014</marker>
<rawString>Felix Hill, Roi Reichart, and Anna Korhonen. 2014b. Simlex-999: Evaluating semantic models with (genuine) similarity estimation. arXiv preprint arXiv:1408.3456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joo-Kyung Kim</author>
<author>Marie-Catherine de Marneffe</author>
</authors>
<title>Deriving adjectival scales from continuous space word representations.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1625--1630</pages>
<location>Seattle, WA.</location>
<marker>Kim, de Marneffe, 2013</marker>
<rawString>Joo-Kyung Kim and Marie-Catherine de Marneffe. 2013. Deriving adjectival scales from continuous space word representations. In Proceedings of EMNLP, pages 1625–1630, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Germ´an Kruszewski</author>
<author>Marco Baroni</author>
</authors>
<title>So similar and yet incompatible: Toward automated identification of semantically compatible words.</title>
<date>2015</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>64--969</pages>
<location>Denver, CO.</location>
<marker>Kruszewski, Baroni, 2015</marker>
<rawString>Germ´an Kruszewski and Marco Baroni. 2015. So similar and yet incompatible: Toward automated identification of semantically compatible words. In Proceedings of NAACL, pages 64–969, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angeliki Lazaridou</author>
<author>Nghia The Pham</author>
<author>Marco Baroni</author>
</authors>
<title>Combining language and vision with a multimodal skip-gram model.</title>
<date>2015</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>153--163</pages>
<location>Denver, CO.</location>
<contexts>
<context position="7070" citStr="Lazaridou et al. (2015)" startWordPosition="1112" endWordPosition="1115">O(|W |) time complexity of the normalization term in Equation (2), Mikolov et al. (2013) use either hierarchical softmax or negative sampling. Here, we adopt the negative sampling method. Injecting lexical contrast information We account for lexical contrast by implementing a 2-task strategy, combining the Skip-gram context prediction objective with a new term: (Jskipgram(wt) + Jlc(wt)) (3) The lexical contrast objective Jlc(wt) tries to enforce the constraint that contrasting pairs should have lower similarity than compatible ones within a max-margin framework. Our formulation is inspired by Lazaridou et al. (2015), who use a similar multitask strategy to induce multimodal embeddings. Given a target word w, with sets of antonyms A(w) and synonyms S(w), the maxmargin objective for lexical contrast is: X− max(0, A − cos(vw, vs) sES(w),aEA(w) + cos(vw,va)) (4) where A is the margin and cos(x, y) stands for cosine similarity between vectors x and y. Note that, by equation (3), the Jlc(wt) term is evaluated each time a word is encountered in the corpus. We extract antonym and synonym sets from WordNet (Miller, 1995). If a word wt is not associated to synonym/antonym information in WordNet, we set Jlc(wt) = 0</context>
</contexts>
<marker>Lazaridou, Pham, Baroni, 2015</marker>
<rawString>Angeliki Lazaridou, Nghia The Pham, and Marco Baroni. 2015. Combining language and vision with a multimodal skip-gram model. In Proceedings of NAACL, pages 153–163, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Steffen Remus</author>
<author>Chris Biemann</author>
</authors>
<title>Do supervised distributional methods really learn lexical inference relations?</title>
<date>2015</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<location>Denver, CO.</location>
<note>In press.</note>
<marker>Levy, Remus, Biemann, 2015</marker>
<rawString>Omer Levy, Steffen Remus, Chris Biemann, , and Ido Dagan. 2015. Do supervised distributional methods really learn lexical inference relations? In Proceedings of NAACL, Denver, CO. In press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>746--751</pages>
<location>Atlanta,</location>
<contexts>
<context position="5282" citStr="Mikolov et al., 2013" startWordPosition="815" endWordPosition="818">t work has instead adopted multitask objectives similar to ours in order to directly plug in knowledge from structured resources at DSM induction time (Fried and Duh, 2015; Xu et al., 2014; Yu and Dredze, 2014). Our main novelties with respect to these proposals are the focus on capturing semantic contrast, and explicitly testing the hypothesis that the multitask objective is also beneficial to words that are not directly exposed to WordNet evidence during training.2 2 The multitask Lexical Contrast Model Skip-gram model The multitask Lexical Contrast Model (mLCM) extends the Skip-gram model (Mikolov et al., 2013). Given an input text corpus, Skip-gram optimizes word vectors on the task of approximating, for each word, the probability of other words to occur in its context. More specifically, its objective function is: ⎝⎛⎞ X log p(wt+j|wt) ⎠(1) −c&lt;j&lt;c,j=,40 where w1, w2, ..., wT is the training corpus, consisting of a list of target words wt, for which we want to learn the vector representations (and serving as contexts of each other), and c is the window size determining the span of context words to be considered. p(wt+j|wt), the probability of a context word given the target word is computed using so</context>
<context position="6535" citStr="Mikolov et al. (2013)" startWordPosition="1035" endWordPosition="1038">W v v (2) ,T Pwi=1 e wi wt cally, with a 25% drop in terms of Spearman correlation. 2After submitting this work, we became aware of Ono et al. (2015), that implement very similar ideas. However, one major difference between their work and ours is that their strategy is in the same direction of (Yih et al., 2012), which might result in poor performance on general semantic tasks. where vw and v&apos;w are respectively the target and context vector representations of word w, and W is the number of words in the vocabulary. To avoid the O(|W |) time complexity of the normalization term in Equation (2), Mikolov et al. (2013) use either hierarchical softmax or negative sampling. Here, we adopt the negative sampling method. Injecting lexical contrast information We account for lexical contrast by implementing a 2-task strategy, combining the Skip-gram context prediction objective with a new term: (Jskipgram(wt) + Jlc(wt)) (3) The lexical contrast objective Jlc(wt) tries to enforce the constraint that contrasting pairs should have lower similarity than compatible ones within a max-margin framework. Our formulation is inspired by Lazaridou et al. (2015), who use a similar multitask strategy to induce multimodal embed</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word representations. In Proceedings of NAACL, pages 746–751, Atlanta, Georgia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="7576" citStr="Miller, 1995" startWordPosition="1205" endWordPosition="1206"> than compatible ones within a max-margin framework. Our formulation is inspired by Lazaridou et al. (2015), who use a similar multitask strategy to induce multimodal embeddings. Given a target word w, with sets of antonyms A(w) and synonyms S(w), the maxmargin objective for lexical contrast is: X− max(0, A − cos(vw, vs) sES(w),aEA(w) + cos(vw,va)) (4) where A is the margin and cos(x, y) stands for cosine similarity between vectors x and y. Note that, by equation (3), the Jlc(wt) term is evaluated each time a word is encountered in the corpus. We extract antonym and synonym sets from WordNet (Miller, 1995). If a word wt is not associated to synonym/antonym information in WordNet, we set Jlc(wt) = 0. 3 Experimental setup We compare the performance of mLCM against Skip-gram. Both models’ parameters are estimated by backpropagation of error via stochastic gradient descent. Our text corpus is a Wikipedia3 2009 dump comprising approximately 800M tokens and 200K distinct word types.4 Other hyperparameters, selected without tuning, include: vector size (300), window size (5), negative samples (10), sub-sampling to disfavor frequent words (10−3). For mLCM, we use 7500 antonym pairs 3https://en.wikipedi</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Bonnie Dorr</author>
<author>Graeme Hirst</author>
<author>Peter Turney</author>
</authors>
<date>2013</date>
<booktitle>Computing lexical contrast. Computational Linguistics,</booktitle>
<pages>39--3</pages>
<contexts>
<context position="1661" citStr="Mohammad et al. (2013)" startWordPosition="238" endWordPosition="241">ng adjectives to their antonyms. 1 Introduction Distributional semantic models (DSMs) extract vectors representing word meaning by relying on the distributional hypothesis, that is, the idea that words that are related in meaning will tend to occur in similar contexts (Turney and Pantel, 2010). While extensive work has shown that contextual similarity is an excellent proxy to semantic similarity, a big problem for DSMs is that both words with very compatible meanings (e.g., near synonyms) and words with strongly contrasting meanings (e.g., antonyms) tend to occur in the same contexts. Indeed, Mohammad et al. (2013) have shown that synonyms and antonyms are indistinguishable in terms of their average degree of distributional similarity. This is problematic for the application of DSMs to reasoning tasks such as entailment detection (black is very close to both dark and white in distributional semantic space, but it implies the former while contradicting the latter). Beyond wordlevel relations, the same difficulties make it challenging for compositional extensions of DSMs to capture the fundamental phenomenon of negation at the phrasal and sentential levels (the distributional vectors for good and not good</context>
</contexts>
<marker>Mohammad, Dorr, Hirst, Turney, 2013</marker>
<rawString>Saif Mohammad, Bonnie Dorr, Graeme Hirst, and Peter Turney. 2013. Computing lexical contrast. Computational Linguistics, 39(3):555–590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masataka Ono</author>
<author>Makoto Miwa</author>
<author>Yutaka Sasaki</author>
</authors>
<title>Word embedding-based antonym detection using thesauri and distributional information.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>984--989</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Denver, Colorado, May–June.</location>
<contexts>
<context position="6063" citStr="Ono et al. (2015)" startWordPosition="954" endWordPosition="957">More specifically, its objective function is: ⎝⎛⎞ X log p(wt+j|wt) ⎠(1) −c&lt;j&lt;c,j=,40 where w1, w2, ..., wT is the training corpus, consisting of a list of target words wt, for which we want to learn the vector representations (and serving as contexts of each other), and c is the window size determining the span of context words to be considered. p(wt+j|wt), the probability of a context word given the target word is computed using softmax: T ewt+j vwt p(wt+j|wt) = W v v (2) ,T Pwi=1 e wi wt cally, with a 25% drop in terms of Spearman correlation. 2After submitting this work, we became aware of Ono et al. (2015), that implement very similar ideas. However, one major difference between their work and ours is that their strategy is in the same direction of (Yih et al., 2012), which might result in poor performance on general semantic tasks. where vw and v&apos;w are respectively the target and context vector representations of word w, and W is the number of words in the vocabulary. To avoid the O(|W |) time complexity of the normalization term in Equation (2), Mikolov et al. (2013) use either hierarchical softmax or negative sampling. Here, we adopt the negative sampling method. Injecting lexical contrast i</context>
</contexts>
<marker>Ono, Miwa, Sasaki, 2015</marker>
<rawString>Masataka Ono, Makoto Miwa, and Yutaka Sasaki. 2015. Word embedding-based antonym detection using thesauri and distributional information. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 984–989, Denver, Colorado, May–June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne Preller</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Bell states and negative sentences in the distributed model of meaning.</title>
<date>2011</date>
<journal>Electr. Notes Theor. Comput. Sci.,</journal>
<volume>270</volume>
<issue>2</issue>
<contexts>
<context position="2335" citStr="Preller and Sadrzadeh, 2011" startWordPosition="346" endWordPosition="349">istinguishable in terms of their average degree of distributional similarity. This is problematic for the application of DSMs to reasoning tasks such as entailment detection (black is very close to both dark and white in distributional semantic space, but it implies the former while contradicting the latter). Beyond wordlevel relations, the same difficulties make it challenging for compositional extensions of DSMs to capture the fundamental phenomenon of negation at the phrasal and sentential levels (the distributional vectors for good and not good are nearly identical) (Hermann et al., 2013; Preller and Sadrzadeh, 2011). Mohammad and colleagues concluded that DSMs alone cannot detect semantic contrast, and proposed an approach that couples them with other resources. Pure-DSM solutions include isolating contexts that are expected to be more discriminative of contrast, tuning the similarity measure to make it more sensitive to contrast or training a supervised contrast classifier on DSM vectors (Adel and Sch¨utze, 2014; Santus et al., 2014; Schulte im Walde and K¨oper, 2013; Turney, 2008). We propose instead to induce word vectors using a multitask cost function combining a traditional DSM context-prediction o</context>
</contexts>
<marker>Preller, Sadrzadeh, 2011</marker>
<rawString>Anne Preller and Mehrnoosh Sadrzadeh. 2011. Bell states and negative sentences in the distributed model of meaning. Electr. Notes Theor. Comput. Sci., 270(2):141–153.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrico Santus</author>
<author>Qin Lu</author>
<author>Alessandro Lenci</author>
<author>Chu-Ren Huang</author>
</authors>
<title>Taking antonymy mask off in vector space.</title>
<date>2014</date>
<booktitle>In Proceedings of PACLIC,</booktitle>
<pages>135--144</pages>
<contexts>
<context position="2761" citStr="Santus et al., 2014" startWordPosition="411" endWordPosition="414">ndamental phenomenon of negation at the phrasal and sentential levels (the distributional vectors for good and not good are nearly identical) (Hermann et al., 2013; Preller and Sadrzadeh, 2011). Mohammad and colleagues concluded that DSMs alone cannot detect semantic contrast, and proposed an approach that couples them with other resources. Pure-DSM solutions include isolating contexts that are expected to be more discriminative of contrast, tuning the similarity measure to make it more sensitive to contrast or training a supervised contrast classifier on DSM vectors (Adel and Sch¨utze, 2014; Santus et al., 2014; Schulte im Walde and K¨oper, 2013; Turney, 2008). We propose instead to induce word vectors using a multitask cost function combining a traditional DSM context-prediction objective with a term forcing words to be closer to their WordNet synonyms than to their antonyms. In this way, we make the model aware that contrasting words such as hot and cold, while still semantically related, should not be nearest neighbours in the space. In a similar spirit, Yih et al. (2012) devise a DSM in which the embeddings of the antonyms of a word are pushed to be the vectors that are farthest away from its re</context>
<context position="10163" citStr="Santus et al. (2014)" startWordPosition="1618" endWordPosition="1621">nformation, at the very least, does not have any negative effect on general semantic relatedness. On the other hand, lexical contrast information has a strong positive effect on measuring strict semantic similarity, leading mLCM to achieve state-of-the-art SimLex performance (Hill et al., 2014a). 4.2 Distinguishing antonyms and synonyms Having shown that capturing lexical contrast information results in higher-quality representations for general purposes, we focus next on the specific task of distinguishing contrasting words from highly compatible ones. We use the adjective part of dataset of Santus et al. (2014), that contains 262 antonym and 364 synonym pairs. We compute cosine similarity of all pairs and use the area under the ROC curve (AUC) to measure model performance. Moreover, we directly test mLCM’s ability to propagate lexical contrast across the vocabulary by retraining it without using WordNet information for any of the words in the dataset, i.e. the words in the dataset are removed from the synonym or antonym sets of all the adjectives used in training (mLCM-propagate in the results table). The results, in Table 2, show that mLCM can successfully learn to distinguish contrasting words fro</context>
</contexts>
<marker>Santus, Lu, Lenci, Huang, 2014</marker>
<rawString>Enrico Santus, Qin Lu, Alessandro Lenci, and Chu-Ren Huang. 2014. Taking antonymy mask off in vector space. In Proceedings of PACLIC, pages 135–144, Phuket,Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
<author>Maximilian K¨oper</author>
</authors>
<title>Pattern-based distinction of paradigmatic relations for German nouns, verbs, adjectives.</title>
<date>2013</date>
<booktitle>In Proceedings of GSCL,</booktitle>
<pages>184--198</pages>
<location>Darmstadt, Germany.</location>
<marker>Walde, K¨oper, 2013</marker>
<rawString>Sabine Schulte im Walde and Maximilian K¨oper. 2013. Pattern-based distinction of paradigmatic relations for German nouns, verbs, adjectives. In Proceedings of GSCL, pages 184–198, Darmstadt, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="1333" citStr="Turney and Pantel, 2010" startWordPosition="184" endWordPosition="187">n that of matching WordNet antonyms. mLCM outperforms Skip-gram both on general semantic tasks and on synonym/antonym discrimination, even when no direct lexical contrast information about the test words is provided during training. mLCM also shows promising results on the task of learning a compositional negation operator mapping adjectives to their antonyms. 1 Introduction Distributional semantic models (DSMs) extract vectors representing word meaning by relying on the distributional hypothesis, that is, the idea that words that are related in meaning will tend to occur in similar contexts (Turney and Pantel, 2010). While extensive work has shown that contextual similarity is an excellent proxy to semantic similarity, a big problem for DSMs is that both words with very compatible meanings (e.g., near synonyms) and words with strongly contrasting meanings (e.g., antonyms) tend to occur in the same contexts. Indeed, Mohammad et al. (2013) have shown that synonyms and antonyms are indistinguishable in terms of their average degree of distributional similarity. This is problematic for the application of DSMs to reasoning tasks such as entailment detection (black is very close to both dark and white in distr</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>A uniform approach to analogies, synonyms, antonyms and associations.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>905--912</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="2811" citStr="Turney, 2008" startWordPosition="421" endWordPosition="422">ential levels (the distributional vectors for good and not good are nearly identical) (Hermann et al., 2013; Preller and Sadrzadeh, 2011). Mohammad and colleagues concluded that DSMs alone cannot detect semantic contrast, and proposed an approach that couples them with other resources. Pure-DSM solutions include isolating contexts that are expected to be more discriminative of contrast, tuning the similarity measure to make it more sensitive to contrast or training a supervised contrast classifier on DSM vectors (Adel and Sch¨utze, 2014; Santus et al., 2014; Schulte im Walde and K¨oper, 2013; Turney, 2008). We propose instead to induce word vectors using a multitask cost function combining a traditional DSM context-prediction objective with a term forcing words to be closer to their WordNet synonyms than to their antonyms. In this way, we make the model aware that contrasting words such as hot and cold, while still semantically related, should not be nearest neighbours in the space. In a similar spirit, Yih et al. (2012) devise a DSM in which the embeddings of the antonyms of a word are pushed to be the vectors that are farthest away from its representation. While their model is able to correct</context>
</contexts>
<marker>Turney, 2008</marker>
<rawString>Peter Turney. 2008. A uniform approach to analogies, synonyms, antonyms and associations. In Proceedings of COLING, pages 905–912, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Xu</author>
<author>Yalong Bai</author>
<author>Jiang Bian</author>
<author>Bin Gao</author>
<author>Gang Wang</author>
<author>Xiaoguang Liu</author>
<author>Tie-Yan Liu</author>
</authors>
<title>RCNET: A general framework for incorporating knowledge into word representations.</title>
<date>2014</date>
<booktitle>In Proceedings of CIKM,</booktitle>
<pages>1219--1228</pages>
<location>Shanghai, China.</location>
<contexts>
<context position="4849" citStr="Xu et al., 2014" startWordPosition="746" endWordPosition="749"> enabling better learning of a compositional negation function. Our work is also closely related to Faruqui et al. (2015), who propose an algorithm to adapt pretrained DSM representations using semantic resources such as WordNet. This post-processing approach, while extremely effective, has the disadvantage that changes only affect words that are present in the resource, without propagating to the whole lexicon. Other recent work has instead adopted multitask objectives similar to ours in order to directly plug in knowledge from structured resources at DSM induction time (Fried and Duh, 2015; Xu et al., 2014; Yu and Dredze, 2014). Our main novelties with respect to these proposals are the focus on capturing semantic contrast, and explicitly testing the hypothesis that the multitask objective is also beneficial to words that are not directly exposed to WordNet evidence during training.2 2 The multitask Lexical Contrast Model Skip-gram model The multitask Lexical Contrast Model (mLCM) extends the Skip-gram model (Mikolov et al., 2013). Given an input text corpus, Skip-gram optimizes word vectors on the task of approximating, for each word, the probability of other words to occur in its context. Mor</context>
</contexts>
<marker>Xu, Bai, Bian, Gao, Wang, Liu, Liu, 2014</marker>
<rawString>Chang Xu, Yalong Bai, Jiang Bian, Bin Gao, Gang Wang, Xiaoguang Liu, and Tie-Yan Liu. 2014. RCNET: A general framework for incorporating knowledge into word representations. In Proceedings of CIKM, pages 1219–1228, Shanghai, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
<author>John Platt</author>
</authors>
<title>Polarity inducing latent semantic analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CONLL,</booktitle>
<pages>1212--1222</pages>
<contexts>
<context position="3234" citStr="Yih et al. (2012)" startWordPosition="491" endWordPosition="494"> to make it more sensitive to contrast or training a supervised contrast classifier on DSM vectors (Adel and Sch¨utze, 2014; Santus et al., 2014; Schulte im Walde and K¨oper, 2013; Turney, 2008). We propose instead to induce word vectors using a multitask cost function combining a traditional DSM context-prediction objective with a term forcing words to be closer to their WordNet synonyms than to their antonyms. In this way, we make the model aware that contrasting words such as hot and cold, while still semantically related, should not be nearest neighbours in the space. In a similar spirit, Yih et al. (2012) devise a DSM in which the embeddings of the antonyms of a word are pushed to be the vectors that are farthest away from its representation. While their model is able to correctly pick the antonym of a target item from a list of candidates (since it is the most dissimilar element in the list), we conjecture that their radical strategy produces embeddings with poor performance on general semantic tasks.1 Our method has instead a beneficial global 1Indeed, by simulating their strategy, we were able to inject lexical contrast into word embeddings, but performance on a general semantic relatedness</context>
<context position="6227" citStr="Yih et al., 2012" startWordPosition="982" endWordPosition="985">rds wt, for which we want to learn the vector representations (and serving as contexts of each other), and c is the window size determining the span of context words to be considered. p(wt+j|wt), the probability of a context word given the target word is computed using softmax: T ewt+j vwt p(wt+j|wt) = W v v (2) ,T Pwi=1 e wi wt cally, with a 25% drop in terms of Spearman correlation. 2After submitting this work, we became aware of Ono et al. (2015), that implement very similar ideas. However, one major difference between their work and ours is that their strategy is in the same direction of (Yih et al., 2012), which might result in poor performance on general semantic tasks. where vw and v&apos;w are respectively the target and context vector representations of word w, and W is the number of words in the vocabulary. To avoid the O(|W |) time complexity of the normalization term in Equation (2), Mikolov et al. (2013) use either hierarchical softmax or negative sampling. Here, we adopt the negative sampling method. Injecting lexical contrast information We account for lexical contrast by implementing a 2-task strategy, combining the Skip-gram context prediction objective with a new term: (Jskipgram(wt) +</context>
</contexts>
<marker>Yih, Zweig, Platt, 2012</marker>
<rawString>Wen-tau Yih, Geoffrey Zweig, and John Platt. 2012. Polarity inducing latent semantic analysis. In Proceedings of EMNLP-CONLL, pages 1212–1222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mo Yu</author>
<author>Mark Dredze</author>
</authors>
<title>Improving lexical embeddings with semantic knowledge.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>545--550</pages>
<location>Baltimore, MD.</location>
<contexts>
<context position="4871" citStr="Yu and Dredze, 2014" startWordPosition="750" endWordPosition="753">learning of a compositional negation function. Our work is also closely related to Faruqui et al. (2015), who propose an algorithm to adapt pretrained DSM representations using semantic resources such as WordNet. This post-processing approach, while extremely effective, has the disadvantage that changes only affect words that are present in the resource, without propagating to the whole lexicon. Other recent work has instead adopted multitask objectives similar to ours in order to directly plug in knowledge from structured resources at DSM induction time (Fried and Duh, 2015; Xu et al., 2014; Yu and Dredze, 2014). Our main novelties with respect to these proposals are the focus on capturing semantic contrast, and explicitly testing the hypothesis that the multitask objective is also beneficial to words that are not directly exposed to WordNet evidence during training.2 2 The multitask Lexical Contrast Model Skip-gram model The multitask Lexical Contrast Model (mLCM) extends the Skip-gram model (Mikolov et al., 2013). Given an input text corpus, Skip-gram optimizes word vectors on the task of approximating, for each word, the probability of other words to occur in its context. More specifically, its ob</context>
</contexts>
<marker>Yu, Dredze, 2014</marker>
<rawString>Mo Yu and Mark Dredze. 2014. Improving lexical embeddings with semantic knowledge. In Proceedings of ACL, pages 545–550, Baltimore, MD.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>