<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001776">
<title confidence="0.9978425">
Cross-lingual comparison between distributionally determined
word similarity networks
</title>
<author confidence="0.976617">
Olof G¨ornerup
</author>
<affiliation confidence="0.978563">
Swedish Institute of Computer Science
</affiliation>
<note confidence="0.501402">
(SICS)
</note>
<address confidence="0.65269">
164 29 Kista, Sweden
</address>
<email confidence="0.994836">
olofg@sics.se
</email>
<author confidence="0.973861">
Jussi Karlgren
</author>
<affiliation confidence="0.972425">
Swedish Institute of Computer Science
</affiliation>
<note confidence="0.504531">
(SICS)
</note>
<address confidence="0.652759">
164 29 Kista, Sweden
</address>
<email confidence="0.997114">
jussi@sics.se
</email>
<sectionHeader confidence="0.993839" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999871875">
As an initial effort to identify universal
and language-specific factors that influ-
ence the behavior of distributional models,
we have formulated a distributionally de-
termined word similarity network model,
implemented it for eleven different lan-
guages, and compared the resulting net-
works. In the model, vertices constitute
words and two words are linked if they oc-
cur in similar contexts. The model is found
to capture clear isomorphisms across lan-
guages in terms of syntactic and semantic
classes, as well as functional categories of
abstract discourse markers. Language spe-
cific morphology is found to be a dominat-
ing factor for the accuracy of the model.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999949282051282">
This work takes as its point of departure the fact
that most studies of the distributional character of
terms in language are language specific. A model
or technique—either geometric (Deerwester et al.,
1990; Finch and Chater, 1992; Lund and Burgess,
1996; Letsche and Berry, 1997; Kanerva et al.,
2000) or graph based (i Cancho and Sol´e, 2001;
Widdows and Dorow, 2002; Biemann, 2006)—
that works quite well for one language may not be
suitable for other languages. A general question
of interest is then: What strengths and weaknesses
of distributional models are universal and what are
language specific?
In this paper we approach this question by for-
mulating a distributionally based network model,
apply the model on eleven different languages, and
then compare the resulting networks. We com-
pare the networks both in terms of global statisti-
cal properties and local structures of word-to-word
relations of linguistic relevance. More specif-
ically, the generated networks constitute words
(vertices) that are connected with edges if they
are observed to occur in similar contexts. The
networks are derived from the Europarl corpus
(Koehn, 2005)—the annotated proceedings of the
European parliament during 1996-2006. This is
a parallel corpus that covers Danish, Dutch, En-
glish, Finnish, French, German, Greek, Italian,
Portuguese, Spanish and Swedish.
The objective of this paper is not to provide a
extensive comparison of how distributional net-
work models perform in specific applications for
specific languages, for instance in terms of bench-
mark performance, but rather to, firstly, demon-
strate the expressive strength of distributionally
based network models and, secondly, to highlight
fundamental similarities and differences between
languages that these models are capable of captur-
ing (and fail in capturing).
</bodyText>
<sectionHeader confidence="0.996834" genericHeader="introduction">
2 Methods
</sectionHeader>
<bodyText confidence="0.997871733333333">
We consider a base case where a context is defined
as the preceding and subsequent words of a focus
word. Word order matters and so a context forms
a word pair. Consider for instance the following
sentence1:
Ladies and gentlemen, once again, we
see it is essential for Members to bring
their voting cards along on a Monday.
Here the focus word essential occurs in the con-
text is * for, the word bring in the context to *
their etcetera (the asterisk * denotes an interme-
diate focus word). Since a context occurs with a
word with a certain probability, each word wi is
associated with a probability distribution of con-
texts:
</bodyText>
<equation confidence="0.916046">
Pi = {Pr[wpwiws|wi]}w,,wsEW, (1)
</equation>
<footnote confidence="0.988586">
1Quoting Nicole Fontaine, president of the European Par-
liament 1999-2001, from the first session of year 2000.
</footnote>
<page confidence="0.980254">
48
</page>
<note confidence="0.9794565">
Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing, ACL 2010, pages 48–54,
Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999868555555556">
where W denotes the set of all words and
Pr[wpwzwsjwz] is the conditional probability that
context wp ¤ ws occurrs, given that the focus word
is wz. In practice, we estimate Pz by counting
the occurring contexts of wz and then normalizing
the counts. Context counts, in turn, were derived
from trigram counts. No pre-processing, such as
stemming, was performed prior to collecting the
trigrams.
</bodyText>
<subsectionHeader confidence="0.984478">
2.1 Similarity measure
</subsectionHeader>
<bodyText confidence="0.999708">
If two words have similar context distributions,
they are assumed to have a similar function in
the language. For instance, it is reasonable to as-
sume that the word “salt” to a higher degree occurs
in similar contexts as “pepper” compared to, say,
“friendly”. One could imagine that a narrow 1+1
neighborhood only captures fundamental syntactic
agreement between words, which has also been ar-
gued in the literature (Sahlgren, 2006). However,
as we will see below, the intermediate two-word
context also captures richer word relationships.
We measure the degree of similarity by com-
paring the respective context distributions. This
can be done in a number of ways. For example,
as the Euclidian distance (also known as L2 diver-
gence), the Harmonic mean, Spearman’s rank cor-
relation coefficient and the Jensen-Shannon diver-
gence (information radius). Here we quantify the
difference between two words wz and wj, denoted
dzj, by the variational distance (or L1 divergence)
between their corresponding context distributions
Pz and Pj:
</bodyText>
<equation confidence="0.994261">
�dzj = jPz(X = c) — Pj(X = c)j, (2)
cEC
</equation>
<bodyText confidence="0.998054842105263">
where X is a stochastic variable drawn from C,
which is the set of contexts that either wz or wj
occur in. 0 &lt; dzj &lt; 2, where dzj = 0 if
the two distributions are identical and dzj = 2
if the words do not share any contexts at all. It
is not obvious that the variational distance is the
best choice of measure. However, we chose to
employ it since it is a well-established and well-
understood statistical measure; since it is straight-
forward and fast to calculate; and since it appears
to be robust. To compare, we have also tested
to employ the Jensen-Shannon divergence (a sym-
metrized and smoothed version of Kullback infor-
mation) and acquire very similar results as those
presented here. In fact, this is expected since the
two measures are found to be approximately lin-
early related in this context. However, for the two
first reasons listed above, the variational distance
is our divergence measure of choice in this study.
</bodyText>
<subsectionHeader confidence="0.996327">
2.2 Network representation
</subsectionHeader>
<bodyText confidence="0.999982625">
A set of words and their similarity relations are
naturally interpreted as a weighted and undirected
network. The vertices then constitute words and
two vertices are linked by an edge if their corre-
sponding words wz and wj have overlapping con-
text sets. The strength of the links vary depend-
ing on the respective degrees of word similarities.
Here the edge between two words wz and wj’s
is weighted with wzj = 2 — dzj (note again that
maxzj dzj = 2) since a large word difference im-
plies a weak link and vice versa.
In our experiment we consider the 3000 most
common words, excluding the 19 first ones, in
each language. To keep the data more manage-
able during analysis we employ various thresh-
olds. Firstly, we only consider context words that
occur five times or more. As formed by the re-
maining context words, we then only consider tri-
grams that occur three times or more. This allows
us to cut away a large chunk of the data. We have
tested to vary these thresholds and the resulting
networks are found to have very similar statisti-
cal properties, even though the networks differ by
a large number of very weak edges.
</bodyText>
<sectionHeader confidence="0.999986" genericHeader="method">
3 Results
</sectionHeader>
<subsectionHeader confidence="0.999828">
3.1 Degree distributions
</subsectionHeader>
<bodyText confidence="0.999701833333333">
The degree gz of a vertex i is defined as the sum
of weights of the edges of the vertex: gz = E wzj.
The degree distribution of a network may provide
valuable statistical information about the networks
structure. For the word networks, Figure 1, the de-
gree distributions are all found to be highly right-
skewed and have longer tails than expected from
random graphs (Erd˝os and R´enyi, 1959). This
characteristics is often observed in complex net-
works, which typically also are scale-free (New-
man, 2003). Interestingly, the word similarity net-
works are not scale-free as their degree distribu-
tions do no obey power-laws: Pr(g) » g−o&apos; for
some exponent α. Instead, the degree distributions
of each word network appears to lay somewhere
between a power-law distribution and an exponen-
tial distribution (Pr(g) » e−9/&amp;quot;). However, due
to quite noisy statistics it is difficult to reliably
</bodyText>
<page confidence="0.998943">
49
</page>
<bodyText confidence="0.999957636363636">
measure and characterize the tails in the word net-
works. Note that there appears to be a bump in
the distributions for some languages at around de-
gree 60, but again, this may be due to noise and
more data is required before we can draw any con-
clusions. Note also that the degree distribution of
Finnish stands out: Finnish words typically have
less or weaker links than words in the other lan-
guages. This is reasonably in view of the special
morphological character of Finnish compared to
Indo-European languages (see below).
</bodyText>
<subsectionHeader confidence="0.999885">
3.2 Community structures
</subsectionHeader>
<bodyText confidence="0.999987789473684">
The acquired networks display interesting global
structures that emerge from the local and pair-
wise word to word relations. Each network form
a single strongly connected component. In other
words, any vertex can be reached by any other ver-
tex and so there is always a path of “associations”
between any two words. Furthermore, all word
networks have significant community structures;
vertices are organized into groups, where there are
higher densities of edges within groups than be-
tween them. The strength of community structure
can be quantified as follows (Newman and Gir-
van, 2004): Let {vi}ni=1 be a partition of the set
of vertices into n groups, ri the fraction of edge
weights that are internal to vi (i.e. the sum of in-
ternal weights over the sum of all weights in the
network), and si the fraction of edge weights of
the edges starting in vi. The modularity strength is
then defined as
</bodyText>
<equation confidence="0.993697333333333">
n
Q = (ri − s�i ). (3)
i=1
</equation>
<bodyText confidence="0.999929956521739">
Q constitutes the fraction of edge weights given
by edges in the network that link vertices within
the same communities, minus the expected value
of the same quantity in a random network with the
same community assignments (i.e. the same ver-
tex set partition). There are several algorithms that
aim to find the community structure of a network
by maximizing Q. Here we use an agglomerative
clustering method by Clauset (2005), which works
as follows: Initialize by assigning each vertex to
its own cluster. Then successively merge clusters
such that the positive change of Q is maximized.
The procedure is repeated as long as Q increases.
Typically Q is close to 0 for random partitions
and indicates strong community structure when
approaching its maximum 1. In practice Q is typi-
cally within the range 0.3 to 0.7, also for highly
modular networks (Newman and Girvan, 2004).
As can be seen in Table 1, all networks are highly
modular, although the degree of modularity varies
between languages. Greek in particular stands out.
However, the reason for this remains an open ques-
tion that requires further investigations.
</bodyText>
<table confidence="0.999874666666667">
Dutch 0.43 Swedish 0.58
German 0.43 French 0.63
Spanish 0.48 Finnish 0.68
Portuguese 0.51 Italian 0.68
English 0.53 Greek 0.78
Danish 0.55
</table>
<tableCaption confidence="0.999921">
Table 1: Community modularity.
</tableCaption>
<bodyText confidence="0.9990615">
Communities become more apparent when
edges are pruned by a threshold as they crystal-
ize into isolated subgraphs. This is exemplified
for English in Figure 2.
</bodyText>
<sectionHeader confidence="0.999446" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.99997475">
We examine the resulting graphs and show in this
section through some example subgraphs how fea-
tures of human language emerge as charactersitics
of the model.
</bodyText>
<subsectionHeader confidence="0.994797">
4.1 Morphology matters
</subsectionHeader>
<bodyText confidence="0.999980833333333">
Morphology is a determining and observable char-
acteristic of several languages. For the purposes
of distributional study of linguistic items, mor-
phological variation is problematic, since it splits
one lexical item into several surface realisations,
requiring more data to perform reliable and ro-
bust statistical analysis. Of the languages stud-
ied in this experiment, Finnish stands out atypi-
cal through its morphological characteristics. In
theory, Finnish nouns can take more than 2 000
surface forms, through more than 12 cases in sin-
gular and plural as well as possessive suffixes
and clitic particles (Linden and Pirinen, 2009),
and while in practice something between six and
twelve forms suffice to cover about 80 per cent
of the variation (Kettunen, 2007) this is still an
order of magnitude more variation than in typi-
cal Indo-European languages such as the others
in this sample. This variation is evident in Fig-
ure 1—Finnish behaves differently than the Indo-
European languages in the sample: as each word
is split in several other surface forms, its links to
other forms will be weaker. Morphological anal-
ysis, transforming surface forms to base forms
</bodyText>
<page confidence="0.972161">
50
</page>
<figure confidence="0.999664384615385">
Danish
German
Greek
English
Spanish
Finnish
French
Italian
Dutch
Portuguese
Swedish
0 50 100 150 200 250 300 350
degree
</figure>
<figureCaption confidence="0.99824">
Figure 1: Degree histograms of word similarity networks.
</figureCaption>
<figure confidence="0.979915090909091">
1800
1600
1400
1200
1000
800
600
400
200
0
l&apos; application
</figure>
<bodyText confidence="0.99064803125">
would strengthen those links.
In practice, the data sparsity caused by mor-
phological variation causes semantically homoge-
nous classes to be split. Even for languages such
as English and French, with very little data varia-
tion we find examples where morphological varia-
tion causes divergence as seen in Figure 3, where
French nouns in definite form are clustered. It is
not surprising that certain nouns in definite form
assume similar roles in text, but the neatness of
the graph is a striking exposition of this fact.
These problems could have been avoided with
better preprocessing—simple such processing in
the case of English and French, and considerably
more complex but feasible in the case of Finnish—
but are retained in the present example as proxies
for the difficulties typical of processing unknown
languages. Our methodology is robust even in
face of shoddy preprocessing and no knowledge
of the morphological basis of the target language.
In general, as a typological fact, it is reasonable to
assume that morphological variation is offset for
the language user in a greater freedom in choice of
word order. This would seem to cause a great deal
of problems for an approach such as the present
one, since it relies on the sequential organisation
of symbols in the signal. However, it is observ-
able that languages with free word order have pre-
ferred unmarked arrangements for their sentence
structure, and thus we find stable relationships in
the data even for Finnish, although weaker than for
the other languages examined.
</bodyText>
<subsectionHeader confidence="0.984814">
4.2 Syntactic classes
</subsectionHeader>
<bodyText confidence="0.999941875">
Previous studies have shown that a narrow con-
text window of one neighour to the left and one
neighbour to the right such as the one used in
the present experiments retrieves syntactic rela-
tionships (Sahlgren, 2006). We find several such
examples in the graphs. In Figure 2 we can see
subgraphs with past participles, auxiliary verbs,
progressive verbs, person names.
</bodyText>
<subsectionHeader confidence="0.998792">
4.3 Semantic classes
</subsectionHeader>
<bodyText confidence="0.9999701">
Some of the subgraphs we find are models of clear
semantic family resemblance as shown in Fig-
ure 4. This provides us with a good argument for
blurring the artificial distinction between syntax
and semantics. Word classes are defined by their
meaning and usage alike; the a priori distinction
between classification by function such as auxil-
iary verbs given above and classification by mean-
ing such months and places given here is not fruit-
ful. We expect to be able to provide much more in-
</bodyText>
<figure confidence="0.982434904761905">
l ensemblel&apos; interdiction
l&apos; augmentation
l&apos;intervention l&apos;examen l&apos;harmonisation
l adhésion l adoption l application
l&apos; existence
l&apos; adoption l&apos; évaluation
l&apos; élaboration
l&apos; octroi l&apos;ouverture
l&apos; histoire
l élargissement
l&apos;adhésion
l utilisation
l&apos; amélioration
l&apos; exécution
l égalité
l&apos; égalité
l&apos;établissement
l&apos; introduction
l&apos; utilisation
# %
#%
</figure>
<figureCaption confidence="0.971194">
Figure 3: French definite nouns clustered.
</figureCaption>
<figure confidence="0.996184117647059">
l&apos; activité
51
nice
immigrants
believe
think
hope
amsterdamministers
individuals
women
feel
presidents
refugeessmesstudents
movement
children
workers
ngos
fishermen
employees
journalists
inquiry
consumers
auditors
farmers
politics
tourism
producers
origin
gmos
expression
minorities
crucial
unacceptable
subsidiarity
religion
sport
nato
globalisation
wishes
vital
emu
forests
macedonia
continues intends
employers europol japan
zimbabwe
burma
wants
cyprus
slovakia
indonesia
morocco cuba
chechnya
algeria britai
israel
serbiromania
poland
croatia
albania
afghanistan
india k sovo denmark
belarus
essential
torture
n
chin iran o
science
strasbourg brussels
greece
finland
sweden iraq dublin
russi austria luxembourg berlin copenhagen
portugal
france italy london vienna barcelona
ukraine
germany belgium
terrorism
turkey
bulgaria
corruption
tampere
poverty fraud
spain
olaf
helsinki
geneva
ireland
unemployment
racism
bosnia
openness
foremost
hence
men
asiapalestine
thirdly
unfortunate
secondly
true obvious evident
apparent
regrettable
transparency
justified
innovation
clarity
debated
conducteddiscussed
improved
regulatedmonitored resolved
solved
novemberapril
october december
flexibility
financed
coherence
funded
pursued
examined
protected respected
strengthened
completed
addressed
januarymarch
competitiveness
june
september
ground
februaryjuly
balkans
promoted
maintained
introduced
culture
courts
efficiency
established
cap
created
internet
improve
implemented
euro
guaranteed
uk
usa
encouraged
treaties
ratify
strengthen
igc
applied
ecb
bureau netherlands
s railways roads
reinforce
palestinians
americans
amend
condemned
oppose
ombudsman
enhance
facts
greens
reject
condemn
safeguard
define
regulate
address
citizen
clarify
guaranteeing
tackle
discuss
examine
assess
maintaining
implementing
resolveovercome
supporting
defining safeguarding addressing applying
achieving defending tackling
preserve
monitor
coordinate
adapt
remove
replaceeliminate
solve
incorporate
protecting
changing
treat
retain
fulfil
withdraw
pursue
completion
reiteratestress
implement
definition
appointment
adoption
maintain
emphasise
realiseacknowledge emphasizeunderline
grounds
concept promoton
possibility establishment
absence
remember
recognise
recognize
happy
continuation
troduction
creation
inclu
in event
sion
abolition
chairma
purpose face
purposes light
middle
highlight
prospect
n
glad
sure
background
afraid
raise
pleased
leader
delighted
occasion
german
sorry
aim french
two
three
seven
greek
willing
dutch
spanish swedish
four
prepared able entitled supposed
austrian finnish belgian
want wish
danish
five
portuguese italian
intend
subject
matter
going
ten
irish
will
issue
evening morning
confirmed
stressed
would
should
weeks
months
must
paragraphparagraphs
emphasised
afternoon
years
days
articles
highlighted noted
remind ask urge invite
refuse fail belong seem
appear
debating
order
relation
prodi
people&apos;
union&apos;
harmonization
liberalisation
covers
sort
type
refers
considering
examining aims
reflects involves barroso
solana
seeks
discussing
includes
santer
addition
fails
kind
parliament&apos;
harmonisation
</figure>
<figureCaption confidence="0.996656">
Figure 2: English. Network involving edges with weights w &gt; 0.85. For sake of clarity, only subgraphs
with three or more words are shown. Note that the threshold 0.85 is used only for the visualization. The
full network consists of the 3000 most common words in English, excluding the 19 most common ones.
</figureCaption>
<page confidence="0.854289">
52
</page>
<figure confidence="0.998824705882353">
enhance visserligen
förvisso
uppenbarligen
säkerligen
självfallet
givetvis
förmodligen
preserve
torture
racism
corruption fraud
poverty
terrorism
reinforce
safeguard strengthen
improve
sannerligen
</figure>
<figureCaption confidence="0.987591">
Figure 4: Examples of semantically homogenous
classes in English, French and Swedish.
</figureCaption>
<bodyText confidence="0.9982142">
formed classification schemes than the traditional
“parts of speech” if we define classes by their dis-
tributional qualities rather than by the “content”
they “represent”, schemes which will cut across
the function-topic distinction.
</bodyText>
<subsectionHeader confidence="0.8533625">
4.4 Abstract discourse markers are a
functional category
</subsectionHeader>
<bodyText confidence="0.99994125">
Further, several subgraphs have clear collections
of discourse markers of various types where the
terms are markers of informational organisation in
the text, as exemplified in Figure 5.
</bodyText>
<sectionHeader confidence="0.999568" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999844058823529">
This preliminary experiment supports future stud-
ies to build knowledge structures across lan-
guages, using distributional isomorphism between
linguistic material in translated or even compara-
ble corpora, on several levels of abstraction, from
function words, to semantic classes, to discourse
markers. The isomorphism across the languages
is clear and incontrovertible; this will allow us to
continue experiments using collections of multi-
lingual materials, even for languages with rela-
tively little technological support. Previous stud-
ies show that knowledge structures of this type
that are created in one language show consider-
able isomorphism to knowledge structures created
in another language if the corpora are comparable
(Holmlund et al., 2005). Holmlund et al show how
translation equivalences can be established using
</bodyText>
<figure confidence="0.659321">
toki
siis
</figure>
<figureCaption confidence="0.896435">
Figure 5: Examples of discourse functional classes
</figureCaption>
<bodyText confidence="0.991949827586207">
in Swedish and Finnish. The terms in the two sub-
graphs are discourse markers and correspond to
English “certainly”, “possibly”, “evidently”, “nat-
urally”, “absolutely”, “hence” and similar terms.
two semantic networks automatically created in
two languages by providing a relatively limited set
of equivalence relations in a translation lexicon.
This study supports those findings.
The results presented here display the potential
of distributionally derived network representations
of word similarities. Although geometric (vector
based) and probabilistic models have proven vi-
able in various applications, they are limited by
the fact that word or term relations are constrained
by the geometric (often Euclidian) space in which
they live. Network representations are richer in
the sense that they are not bound by the same con-
straints. For instance, a polyseme word (“may” for
example) can have strong links to two other words
(“might” and ”September” for example), where
the two other words are completely unrelated. In
an Euclidean space this relation is not possible due
to the triangle inequality. It is possible to em-
bed a network in a geometric space, but this re-
quires a very high dimensionality which makes the
representation both cumbersome and inefficient in
terms of computation and memory. This has been
addressed by coarse graining or dimension reduc-
tion, for example by means of singular value de-
</bodyText>
<figure confidence="0.998784107142857">
avril
octobre
maj
februari
november
mai
janvier
juillet
mars
février
septembre
novembre
juin
marsdecember
september oktober
april
décembre
juli
juni
januari
todellakin
kuitenkin
luonnollisesti
epäilemättä
tietenkin
tietysti
varmasti
selvästikin
</figure>
<page confidence="0.996373">
53
</page>
<bodyText confidence="0.9988665">
composition (Deerwester et al., 1990; Letsche and
Berry, 1997; Kanerva et al., 2000), which results
in information loss. This can be problematic, in
particular since distributional models often face
data sparsity due to the curse of dimensionality.
In a network representation, such dimension re-
duction is not necessary and so potentially impor-
tant information about word or term relations is
retained.
The experiments presented here also show the
potential of moving from a purely probabilistic
model of term occurrence, or a bare distributional
model such as those typically presented using a
geometric metaphor, in that it affords the possibil-
ity of abstract categories inferred from the primary
distributional data. This will give the possibility
of further utilising the results in studies, e.g. for
learning syntactic or functional categories in more
complex constructional models of linguistic form.
Automatically establishing lexically and function-
ally coherent classes in this manner will have bear-
ing on future project goals of automatically learn-
ing syntactic and semantic roles of words in lan-
guage. This target is today typically pursued rely-
ing on traditional lexical categories which are not
necessarily the most salient ones in view of actual
distributional characteristics of words.
Acknowledgments: OG was supported by Johan
and Jacob S¨oderberg’s Foundation. JK was sup-
ported by the Swedish Research Council.
</bodyText>
<sectionHeader confidence="0.998955" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99916546875">
Chris. Biemann. 2006. Chinese whispers - an efficient
graph clustering algorithm and its application to nat-
ural language processing problems. In Proceedings
of the HLT-NAACL-06 Workshop on Textgraphs-06,
New York, USA.
Aaron Clauset. 2005. Finding local community struc-
ture in networks. Physical Review E, 72:026132.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41:391–
407.
Pal Erd˝os and Alfr´ed R´enyi. 1959. On random graphs.
Publications Mathematicae, 6:290.
Steven Finch and Nick Chater. 1992. Bootstrap-
ping syntactic categories. In Proceedings of the
Fourteenth Annual Conference of the Cognitive Sci-
ence Society, pages 820–825, Bloomington, IN.
Lawrence Erlbaum.
Jon Holmlund, Magnus Sahlgren, and Jussi Karlgren.
2005. Creating bilingual lexica using reference
wordlists for alignment of monolingual semantic
vector spaces. In Proceedings of 15th Nordic Con-
ference of Computational Linguistics.
Ramon Ferrer i Cancho and Ricard V. Sol´e. 2001. The
small world of human language. Proceedings of the
Royal Society of London. Series B, Biological Sci-
ences, 268:2261–2266.
Pentti Kanerva, Jan Kristoferson, and Anders Holst.
2000. Random indexing of text samples for latent
semantic analysis. In Proceedings of the 22nd An-
nual Conference of the Cognitive Science Society,
pages 103–6.
Kimmo Kettunen. 2007. Management of keyword
variation with frequency based generation of word
forms in ir. In Proceedings of SIGIR 2007.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Todd Letsche and Michael Berry. 1997. Large-scale
information retrieval with latent semantic indexing.
Information Sciences, 100(1-4):105–137.
Krister Linden and Tommi Pirinen. 2009. Weighting
finite-state morphological analyzers using hfst tools.
In Proceedings of the Finite-State Methods and Nat-
ural Language Processing. Pretoria, South Africa.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, and Computers, 28(2):203–208.
Mark Newman and Michelle Girvan. 2004. Find-
ing and evaluating community structure in networks.
Physical Review E, 69(2).
Mark E. J. Newman. 2003. The structure and function
of complex networks. SIAM Review, 45(2):167–
256.
Magnus Sahlgren. 2006. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. PhD Dissertation, De-
partment of Linguistics, Stockholm University.
Dominic Widdows and Beate Dorow. 2002. A graph
model for unsupervised lexical acquisition. In In
19th International Conference on Computational
Linguistics, pages 1093–1099.
</reference>
<page confidence="0.99902">
54
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.272292">
<title confidence="0.9513885">Cross-lingual comparison between distributionally word similarity networks</title>
<author confidence="0.617242">Olof</author>
<affiliation confidence="0.973445">Swedish Institute of Computer</affiliation>
<address confidence="0.972462">164 29 Kista,</address>
<email confidence="0.981753">olofg@sics.se</email>
<author confidence="0.513574">Jussi</author>
<affiliation confidence="0.998978">Swedish Institute of Computer</affiliation>
<address confidence="0.985383">164 29 Kista,</address>
<email confidence="0.992774">jussi@sics.se</email>
<abstract confidence="0.996517588235294">As an initial effort to identify universal and language-specific factors that influence the behavior of distributional models, we have formulated a distributionally determined word similarity network model, implemented it for eleven different languages, and compared the resulting networks. In the model, vertices constitute words and two words are linked if they occur in similar contexts. The model is found to capture clear isomorphisms across languages in terms of syntactic and semantic classes, as well as functional categories of abstract discourse markers. Language specific morphology is found to be a dominating factor for the accuracy of the model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Biemann</author>
</authors>
<title>Chinese whispers - an efficient graph clustering algorithm and its application to natural language processing problems.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLT-NAACL-06 Workshop on Textgraphs-06,</booktitle>
<location>New York, USA.</location>
<contexts>
<context position="1345" citStr="Biemann, 2006" startWordPosition="203" endWordPosition="204"> languages in terms of syntactic and semantic classes, as well as functional categories of abstract discourse markers. Language specific morphology is found to be a dominating factor for the accuracy of the model. 1 Introduction This work takes as its point of departure the fact that most studies of the distributional character of terms in language are language specific. A model or technique—either geometric (Deerwester et al., 1990; Finch and Chater, 1992; Lund and Burgess, 1996; Letsche and Berry, 1997; Kanerva et al., 2000) or graph based (i Cancho and Sol´e, 2001; Widdows and Dorow, 2002; Biemann, 2006)— that works quite well for one language may not be suitable for other languages. A general question of interest is then: What strengths and weaknesses of distributional models are universal and what are language specific? In this paper we approach this question by formulating a distributionally based network model, apply the model on eleven different languages, and then compare the resulting networks. We compare the networks both in terms of global statistical properties and local structures of word-to-word relations of linguistic relevance. More specifically, the generated networks constitut</context>
</contexts>
<marker>Biemann, 2006</marker>
<rawString>Chris. Biemann. 2006. Chinese whispers - an efficient graph clustering algorithm and its application to natural language processing problems. In Proceedings of the HLT-NAACL-06 Workshop on Textgraphs-06, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron Clauset</author>
</authors>
<title>Finding local community structure in networks. Physical Review E,</title>
<date>2005</date>
<pages>72--026132</pages>
<contexts>
<context position="10142" citStr="Clauset (2005)" startWordPosition="1678" endWordPosition="1679"> weights over the sum of all weights in the network), and si the fraction of edge weights of the edges starting in vi. The modularity strength is then defined as n Q = (ri − s�i ). (3) i=1 Q constitutes the fraction of edge weights given by edges in the network that link vertices within the same communities, minus the expected value of the same quantity in a random network with the same community assignments (i.e. the same vertex set partition). There are several algorithms that aim to find the community structure of a network by maximizing Q. Here we use an agglomerative clustering method by Clauset (2005), which works as follows: Initialize by assigning each vertex to its own cluster. Then successively merge clusters such that the positive change of Q is maximized. The procedure is repeated as long as Q increases. Typically Q is close to 0 for random partitions and indicates strong community structure when approaching its maximum 1. In practice Q is typically within the range 0.3 to 0.7, also for highly modular networks (Newman and Girvan, 2004). As can be seen in Table 1, all networks are highly modular, although the degree of modularity varies between languages. Greek in particular stands ou</context>
</contexts>
<marker>Clauset, 2005</marker>
<rawString>Aaron Clauset. 2005. Finding local community structure in networks. Physical Review E, 72:026132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<pages>407</pages>
<contexts>
<context position="1167" citStr="Deerwester et al., 1990" startWordPosition="171" endWordPosition="174">ompared the resulting networks. In the model, vertices constitute words and two words are linked if they occur in similar contexts. The model is found to capture clear isomorphisms across languages in terms of syntactic and semantic classes, as well as functional categories of abstract discourse markers. Language specific morphology is found to be a dominating factor for the accuracy of the model. 1 Introduction This work takes as its point of departure the fact that most studies of the distributional character of terms in language are language specific. A model or technique—either geometric (Deerwester et al., 1990; Finch and Chater, 1992; Lund and Burgess, 1996; Letsche and Berry, 1997; Kanerva et al., 2000) or graph based (i Cancho and Sol´e, 2001; Widdows and Dorow, 2002; Biemann, 2006)— that works quite well for one language may not be suitable for other languages. A general question of interest is then: What strengths and weaknesses of distributional models are universal and what are language specific? In this paper we approach this question by formulating a distributionally based network model, apply the model on eleven different languages, and then compare the resulting networks. We compare the n</context>
<context position="22372" citStr="Deerwester et al., 1990" startWordPosition="3466" endWordPosition="3469">ue to the triangle inequality. It is possible to embed a network in a geometric space, but this requires a very high dimensionality which makes the representation both cumbersome and inefficient in terms of computation and memory. This has been addressed by coarse graining or dimension reduction, for example by means of singular value deavril octobre maj februari november mai janvier juillet mars février septembre novembre juin marsdecember september oktober april décembre juli juni januari todellakin kuitenkin luonnollisesti epäilemättä tietenkin tietysti varmasti selvästikin 53 composition (Deerwester et al., 1990; Letsche and Berry, 1997; Kanerva et al., 2000), which results in information loss. This can be problematic, in particular since distributional models often face data sparsity due to the curse of dimensionality. In a network representation, such dimension reduction is not necessary and so potentially important information about word or term relations is retained. The experiments presented here also show the potential of moving from a purely probabilistic model of term occurrence, or a bare distributional model such as those typically presented using a geometric metaphor, in that it affords th</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41:391– 407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pal Erd˝os</author>
<author>Alfr´ed R´enyi</author>
</authors>
<title>On random graphs.</title>
<date>1959</date>
<journal>Publications Mathematicae,</journal>
<pages>6--290</pages>
<marker>Erd˝os, R´enyi, 1959</marker>
<rawString>Pal Erd˝os and Alfr´ed R´enyi. 1959. On random graphs. Publications Mathematicae, 6:290.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Finch</author>
<author>Nick Chater</author>
</authors>
<title>Bootstrapping syntactic categories.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society,</booktitle>
<pages>820--825</pages>
<location>Bloomington, IN. Lawrence Erlbaum.</location>
<contexts>
<context position="1191" citStr="Finch and Chater, 1992" startWordPosition="175" endWordPosition="178">works. In the model, vertices constitute words and two words are linked if they occur in similar contexts. The model is found to capture clear isomorphisms across languages in terms of syntactic and semantic classes, as well as functional categories of abstract discourse markers. Language specific morphology is found to be a dominating factor for the accuracy of the model. 1 Introduction This work takes as its point of departure the fact that most studies of the distributional character of terms in language are language specific. A model or technique—either geometric (Deerwester et al., 1990; Finch and Chater, 1992; Lund and Burgess, 1996; Letsche and Berry, 1997; Kanerva et al., 2000) or graph based (i Cancho and Sol´e, 2001; Widdows and Dorow, 2002; Biemann, 2006)— that works quite well for one language may not be suitable for other languages. A general question of interest is then: What strengths and weaknesses of distributional models are universal and what are language specific? In this paper we approach this question by formulating a distributionally based network model, apply the model on eleven different languages, and then compare the resulting networks. We compare the networks both in terms of</context>
</contexts>
<marker>Finch, Chater, 1992</marker>
<rawString>Steven Finch and Nick Chater. 1992. Bootstrapping syntactic categories. In Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society, pages 820–825, Bloomington, IN. Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Holmlund</author>
<author>Magnus Sahlgren</author>
<author>Jussi Karlgren</author>
</authors>
<title>Creating bilingual lexica using reference wordlists for alignment of monolingual semantic vector spaces.</title>
<date>2005</date>
<booktitle>In Proceedings of 15th Nordic Conference of Computational Linguistics.</booktitle>
<contexts>
<context position="20539" citStr="Holmlund et al., 2005" startWordPosition="3192" endWordPosition="3195">ism between linguistic material in translated or even comparable corpora, on several levels of abstraction, from function words, to semantic classes, to discourse markers. The isomorphism across the languages is clear and incontrovertible; this will allow us to continue experiments using collections of multilingual materials, even for languages with relatively little technological support. Previous studies show that knowledge structures of this type that are created in one language show considerable isomorphism to knowledge structures created in another language if the corpora are comparable (Holmlund et al., 2005). Holmlund et al show how translation equivalences can be established using toki siis Figure 5: Examples of discourse functional classes in Swedish and Finnish. The terms in the two subgraphs are discourse markers and correspond to English “certainly”, “possibly”, “evidently”, “naturally”, “absolutely”, “hence” and similar terms. two semantic networks automatically created in two languages by providing a relatively limited set of equivalence relations in a translation lexicon. This study supports those findings. The results presented here display the potential of distributionally derived netwo</context>
</contexts>
<marker>Holmlund, Sahlgren, Karlgren, 2005</marker>
<rawString>Jon Holmlund, Magnus Sahlgren, and Jussi Karlgren. 2005. Creating bilingual lexica using reference wordlists for alignment of monolingual semantic vector spaces. In Proceedings of 15th Nordic Conference of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramon Ferrer i Cancho</author>
<author>Ricard V Sol´e</author>
</authors>
<title>The small world of human language.</title>
<date>2001</date>
<journal>Proceedings of the Royal Society of London. Series B, Biological Sciences,</journal>
<pages>268--2261</pages>
<marker>Cancho, Sol´e, 2001</marker>
<rawString>Ramon Ferrer i Cancho and Ricard V. Sol´e. 2001. The small world of human language. Proceedings of the Royal Society of London. Series B, Biological Sciences, 268:2261–2266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pentti Kanerva</author>
<author>Jan Kristoferson</author>
<author>Anders Holst</author>
</authors>
<title>Random indexing of text samples for latent semantic analysis.</title>
<date>2000</date>
<booktitle>In Proceedings of the 22nd Annual Conference of the Cognitive Science Society,</booktitle>
<pages>103--6</pages>
<contexts>
<context position="1263" citStr="Kanerva et al., 2000" startWordPosition="187" endWordPosition="190">f they occur in similar contexts. The model is found to capture clear isomorphisms across languages in terms of syntactic and semantic classes, as well as functional categories of abstract discourse markers. Language specific morphology is found to be a dominating factor for the accuracy of the model. 1 Introduction This work takes as its point of departure the fact that most studies of the distributional character of terms in language are language specific. A model or technique—either geometric (Deerwester et al., 1990; Finch and Chater, 1992; Lund and Burgess, 1996; Letsche and Berry, 1997; Kanerva et al., 2000) or graph based (i Cancho and Sol´e, 2001; Widdows and Dorow, 2002; Biemann, 2006)— that works quite well for one language may not be suitable for other languages. A general question of interest is then: What strengths and weaknesses of distributional models are universal and what are language specific? In this paper we approach this question by formulating a distributionally based network model, apply the model on eleven different languages, and then compare the resulting networks. We compare the networks both in terms of global statistical properties and local structures of word-to-word rela</context>
<context position="22420" citStr="Kanerva et al., 2000" startWordPosition="3474" endWordPosition="3477">mbed a network in a geometric space, but this requires a very high dimensionality which makes the representation both cumbersome and inefficient in terms of computation and memory. This has been addressed by coarse graining or dimension reduction, for example by means of singular value deavril octobre maj februari november mai janvier juillet mars février septembre novembre juin marsdecember september oktober april décembre juli juni januari todellakin kuitenkin luonnollisesti epäilemättä tietenkin tietysti varmasti selvästikin 53 composition (Deerwester et al., 1990; Letsche and Berry, 1997; Kanerva et al., 2000), which results in information loss. This can be problematic, in particular since distributional models often face data sparsity due to the curse of dimensionality. In a network representation, such dimension reduction is not necessary and so potentially important information about word or term relations is retained. The experiments presented here also show the potential of moving from a purely probabilistic model of term occurrence, or a bare distributional model such as those typically presented using a geometric metaphor, in that it affords the possibility of abstract categories inferred fr</context>
</contexts>
<marker>Kanerva, Kristoferson, Holst, 2000</marker>
<rawString>Pentti Kanerva, Jan Kristoferson, and Anders Holst. 2000. Random indexing of text samples for latent semantic analysis. In Proceedings of the 22nd Annual Conference of the Cognitive Science Society, pages 103–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Kettunen</author>
</authors>
<title>Management of keyword variation with frequency based generation of word forms in ir.</title>
<date>2007</date>
<booktitle>In Proceedings of SIGIR</booktitle>
<contexts>
<context position="12124" citStr="Kettunen, 2007" startWordPosition="1994" endWordPosition="1995">logical variation is problematic, since it splits one lexical item into several surface realisations, requiring more data to perform reliable and robust statistical analysis. Of the languages studied in this experiment, Finnish stands out atypical through its morphological characteristics. In theory, Finnish nouns can take more than 2 000 surface forms, through more than 12 cases in singular and plural as well as possessive suffixes and clitic particles (Linden and Pirinen, 2009), and while in practice something between six and twelve forms suffice to cover about 80 per cent of the variation (Kettunen, 2007) this is still an order of magnitude more variation than in typical Indo-European languages such as the others in this sample. This variation is evident in Figure 1—Finnish behaves differently than the IndoEuropean languages in the sample: as each word is split in several other surface forms, its links to other forms will be weaker. Morphological analysis, transforming surface forms to base forms 50 Danish German Greek English Spanish Finnish French Italian Dutch Portuguese Swedish 0 50 100 150 200 250 300 350 degree Figure 1: Degree histograms of word similarity networks. 1800 1600 1400 1200 </context>
</contexts>
<marker>Kettunen, 2007</marker>
<rawString>Kimmo Kettunen. 2007. Management of keyword variation with frequency based generation of word forms in ir. In Proceedings of SIGIR 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT Summit.</booktitle>
<contexts>
<context position="2108" citStr="Koehn, 2005" startWordPosition="321" endWordPosition="322"> of distributional models are universal and what are language specific? In this paper we approach this question by formulating a distributionally based network model, apply the model on eleven different languages, and then compare the resulting networks. We compare the networks both in terms of global statistical properties and local structures of word-to-word relations of linguistic relevance. More specifically, the generated networks constitute words (vertices) that are connected with edges if they are observed to occur in similar contexts. The networks are derived from the Europarl corpus (Koehn, 2005)—the annotated proceedings of the European parliament during 1996-2006. This is a parallel corpus that covers Danish, Dutch, English, Finnish, French, German, Greek, Italian, Portuguese, Spanish and Swedish. The objective of this paper is not to provide a extensive comparison of how distributional network models perform in specific applications for specific languages, for instance in terms of benchmark performance, but rather to, firstly, demonstrate the expressive strength of distributionally based network models and, secondly, to highlight fundamental similarities and differences between lan</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Todd Letsche</author>
<author>Michael Berry</author>
</authors>
<title>Large-scale information retrieval with latent semantic indexing.</title>
<date>1997</date>
<journal>Information Sciences,</journal>
<pages>100--1</pages>
<contexts>
<context position="1240" citStr="Letsche and Berry, 1997" startWordPosition="183" endWordPosition="186">nd two words are linked if they occur in similar contexts. The model is found to capture clear isomorphisms across languages in terms of syntactic and semantic classes, as well as functional categories of abstract discourse markers. Language specific morphology is found to be a dominating factor for the accuracy of the model. 1 Introduction This work takes as its point of departure the fact that most studies of the distributional character of terms in language are language specific. A model or technique—either geometric (Deerwester et al., 1990; Finch and Chater, 1992; Lund and Burgess, 1996; Letsche and Berry, 1997; Kanerva et al., 2000) or graph based (i Cancho and Sol´e, 2001; Widdows and Dorow, 2002; Biemann, 2006)— that works quite well for one language may not be suitable for other languages. A general question of interest is then: What strengths and weaknesses of distributional models are universal and what are language specific? In this paper we approach this question by formulating a distributionally based network model, apply the model on eleven different languages, and then compare the resulting networks. We compare the networks both in terms of global statistical properties and local structur</context>
<context position="22397" citStr="Letsche and Berry, 1997" startWordPosition="3470" endWordPosition="3473">lity. It is possible to embed a network in a geometric space, but this requires a very high dimensionality which makes the representation both cumbersome and inefficient in terms of computation and memory. This has been addressed by coarse graining or dimension reduction, for example by means of singular value deavril octobre maj februari november mai janvier juillet mars février septembre novembre juin marsdecember september oktober april décembre juli juni januari todellakin kuitenkin luonnollisesti epäilemättä tietenkin tietysti varmasti selvästikin 53 composition (Deerwester et al., 1990; Letsche and Berry, 1997; Kanerva et al., 2000), which results in information loss. This can be problematic, in particular since distributional models often face data sparsity due to the curse of dimensionality. In a network representation, such dimension reduction is not necessary and so potentially important information about word or term relations is retained. The experiments presented here also show the potential of moving from a purely probabilistic model of term occurrence, or a bare distributional model such as those typically presented using a geometric metaphor, in that it affords the possibility of abstract</context>
</contexts>
<marker>Letsche, Berry, 1997</marker>
<rawString>Todd Letsche and Michael Berry. 1997. Large-scale information retrieval with latent semantic indexing. Information Sciences, 100(1-4):105–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Krister Linden</author>
<author>Tommi Pirinen</author>
</authors>
<title>Weighting finite-state morphological analyzers using hfst tools.</title>
<date>2009</date>
<booktitle>In Proceedings of the Finite-State Methods and Natural Language Processing.</booktitle>
<location>Pretoria, South Africa.</location>
<contexts>
<context position="11993" citStr="Linden and Pirinen, 2009" startWordPosition="1970" endWordPosition="1973">ogy is a determining and observable characteristic of several languages. For the purposes of distributional study of linguistic items, morphological variation is problematic, since it splits one lexical item into several surface realisations, requiring more data to perform reliable and robust statistical analysis. Of the languages studied in this experiment, Finnish stands out atypical through its morphological characteristics. In theory, Finnish nouns can take more than 2 000 surface forms, through more than 12 cases in singular and plural as well as possessive suffixes and clitic particles (Linden and Pirinen, 2009), and while in practice something between six and twelve forms suffice to cover about 80 per cent of the variation (Kettunen, 2007) this is still an order of magnitude more variation than in typical Indo-European languages such as the others in this sample. This variation is evident in Figure 1—Finnish behaves differently than the IndoEuropean languages in the sample: as each word is split in several other surface forms, its links to other forms will be weaker. Morphological analysis, transforming surface forms to base forms 50 Danish German Greek English Spanish Finnish French Italian Dutch P</context>
</contexts>
<marker>Linden, Pirinen, 2009</marker>
<rawString>Krister Linden and Tommi Pirinen. 2009. Weighting finite-state morphological analyzers using hfst tools. In Proceedings of the Finite-State Methods and Natural Language Processing. Pretoria, South Africa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lund</author>
<author>Curt Burgess</author>
</authors>
<title>Producing high-dimensional semantic spaces from lexical cooccurrence.</title>
<date>1996</date>
<journal>Behavior Research Methods, Instruments, and Computers,</journal>
<volume>28</volume>
<issue>2</issue>
<contexts>
<context position="1215" citStr="Lund and Burgess, 1996" startWordPosition="179" endWordPosition="182">tices constitute words and two words are linked if they occur in similar contexts. The model is found to capture clear isomorphisms across languages in terms of syntactic and semantic classes, as well as functional categories of abstract discourse markers. Language specific morphology is found to be a dominating factor for the accuracy of the model. 1 Introduction This work takes as its point of departure the fact that most studies of the distributional character of terms in language are language specific. A model or technique—either geometric (Deerwester et al., 1990; Finch and Chater, 1992; Lund and Burgess, 1996; Letsche and Berry, 1997; Kanerva et al., 2000) or graph based (i Cancho and Sol´e, 2001; Widdows and Dorow, 2002; Biemann, 2006)— that works quite well for one language may not be suitable for other languages. A general question of interest is then: What strengths and weaknesses of distributional models are universal and what are language specific? In this paper we approach this question by formulating a distributionally based network model, apply the model on eleven different languages, and then compare the resulting networks. We compare the networks both in terms of global statistical prop</context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>Kevin Lund and Curt Burgess. 1996. Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods, Instruments, and Computers, 28(2):203–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Newman</author>
<author>Michelle Girvan</author>
</authors>
<title>Finding and evaluating community structure in networks. Physical Review E,</title>
<date>2004</date>
<contexts>
<context position="9379" citStr="Newman and Girvan, 2004" startWordPosition="1535" endWordPosition="1539"> (see below). 3.2 Community structures The acquired networks display interesting global structures that emerge from the local and pairwise word to word relations. Each network form a single strongly connected component. In other words, any vertex can be reached by any other vertex and so there is always a path of “associations” between any two words. Furthermore, all word networks have significant community structures; vertices are organized into groups, where there are higher densities of edges within groups than between them. The strength of community structure can be quantified as follows (Newman and Girvan, 2004): Let {vi}ni=1 be a partition of the set of vertices into n groups, ri the fraction of edge weights that are internal to vi (i.e. the sum of internal weights over the sum of all weights in the network), and si the fraction of edge weights of the edges starting in vi. The modularity strength is then defined as n Q = (ri − s�i ). (3) i=1 Q constitutes the fraction of edge weights given by edges in the network that link vertices within the same communities, minus the expected value of the same quantity in a random network with the same community assignments (i.e. the same vertex set partition). T</context>
</contexts>
<marker>Newman, Girvan, 2004</marker>
<rawString>Mark Newman and Michelle Girvan. 2004. Finding and evaluating community structure in networks. Physical Review E, 69(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark E J Newman</author>
</authors>
<title>The structure and function of complex networks.</title>
<date>2003</date>
<journal>SIAM Review,</journal>
<volume>45</volume>
<issue>2</issue>
<pages>256</pages>
<contexts>
<context position="7859" citStr="Newman, 2003" startWordPosition="1287" endWordPosition="1289">ties, even though the networks differ by a large number of very weak edges. 3 Results 3.1 Degree distributions The degree gz of a vertex i is defined as the sum of weights of the edges of the vertex: gz = E wzj. The degree distribution of a network may provide valuable statistical information about the networks structure. For the word networks, Figure 1, the degree distributions are all found to be highly rightskewed and have longer tails than expected from random graphs (Erd˝os and R´enyi, 1959). This characteristics is often observed in complex networks, which typically also are scale-free (Newman, 2003). Interestingly, the word similarity networks are not scale-free as their degree distributions do no obey power-laws: Pr(g) » g−o&apos; for some exponent α. Instead, the degree distributions of each word network appears to lay somewhere between a power-law distribution and an exponential distribution (Pr(g) » e−9/&amp;quot;). However, due to quite noisy statistics it is difficult to reliably 49 measure and characterize the tails in the word networks. Note that there appears to be a bump in the distributions for some languages at around degree 60, but again, this may be due to noise and more data is required</context>
</contexts>
<marker>Newman, 2003</marker>
<rawString>Mark E. J. Newman. 2003. The structure and function of complex networks. SIAM Review, 45(2):167– 256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in highdimensional vector spaces.</title>
<date>2006</date>
<tech>PhD Dissertation,</tech>
<institution>Department of Linguistics, Stockholm University.</institution>
<contexts>
<context position="4605" citStr="Sahlgren, 2006" startWordPosition="719" endWordPosition="720">zing the counts. Context counts, in turn, were derived from trigram counts. No pre-processing, such as stemming, was performed prior to collecting the trigrams. 2.1 Similarity measure If two words have similar context distributions, they are assumed to have a similar function in the language. For instance, it is reasonable to assume that the word “salt” to a higher degree occurs in similar contexts as “pepper” compared to, say, “friendly”. One could imagine that a narrow 1+1 neighborhood only captures fundamental syntactic agreement between words, which has also been argued in the literature (Sahlgren, 2006). However, as we will see below, the intermediate two-word context also captures richer word relationships. We measure the degree of similarity by comparing the respective context distributions. This can be done in a number of ways. For example, as the Euclidian distance (also known as L2 divergence), the Harmonic mean, Spearman’s rank correlation coefficient and the Jensen-Shannon divergence (information radius). Here we quantify the difference between two words wz and wj, denoted dzj, by the variational distance (or L1 divergence) between their corresponding context distributions Pz and Pj: </context>
<context position="14536" citStr="Sahlgren, 2006" startWordPosition="2394" endWordPosition="2395"> of problems for an approach such as the present one, since it relies on the sequential organisation of symbols in the signal. However, it is observable that languages with free word order have preferred unmarked arrangements for their sentence structure, and thus we find stable relationships in the data even for Finnish, although weaker than for the other languages examined. 4.2 Syntactic classes Previous studies have shown that a narrow context window of one neighour to the left and one neighbour to the right such as the one used in the present experiments retrieves syntactic relationships (Sahlgren, 2006). We find several such examples in the graphs. In Figure 2 we can see subgraphs with past participles, auxiliary verbs, progressive verbs, person names. 4.3 Semantic classes Some of the subgraphs we find are models of clear semantic family resemblance as shown in Figure 4. This provides us with a good argument for blurring the artificial distinction between syntax and semantics. Word classes are defined by their meaning and usage alike; the a priori distinction between classification by function such as auxiliary verbs given above and classification by meaning such months and places given here</context>
</contexts>
<marker>Sahlgren, 2006</marker>
<rawString>Magnus Sahlgren. 2006. The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in highdimensional vector spaces. PhD Dissertation, Department of Linguistics, Stockholm University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
<author>Beate Dorow</author>
</authors>
<title>A graph model for unsupervised lexical acquisition. In</title>
<date>2002</date>
<booktitle>In 19th International Conference on Computational Linguistics,</booktitle>
<pages>1093--1099</pages>
<contexts>
<context position="1329" citStr="Widdows and Dorow, 2002" startWordPosition="199" endWordPosition="202">clear isomorphisms across languages in terms of syntactic and semantic classes, as well as functional categories of abstract discourse markers. Language specific morphology is found to be a dominating factor for the accuracy of the model. 1 Introduction This work takes as its point of departure the fact that most studies of the distributional character of terms in language are language specific. A model or technique—either geometric (Deerwester et al., 1990; Finch and Chater, 1992; Lund and Burgess, 1996; Letsche and Berry, 1997; Kanerva et al., 2000) or graph based (i Cancho and Sol´e, 2001; Widdows and Dorow, 2002; Biemann, 2006)— that works quite well for one language may not be suitable for other languages. A general question of interest is then: What strengths and weaknesses of distributional models are universal and what are language specific? In this paper we approach this question by formulating a distributionally based network model, apply the model on eleven different languages, and then compare the resulting networks. We compare the networks both in terms of global statistical properties and local structures of word-to-word relations of linguistic relevance. More specifically, the generated ne</context>
</contexts>
<marker>Widdows, Dorow, 2002</marker>
<rawString>Dominic Widdows and Beate Dorow. 2002. A graph model for unsupervised lexical acquisition. In In 19th International Conference on Computational Linguistics, pages 1093–1099.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>