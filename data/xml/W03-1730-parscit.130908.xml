<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000328">
<title confidence="0.920903">
HHMM-based Chinese Lexical Analyzer ICTCLAS*
</title>
<author confidence="0.997327">
Hua-Ping ZHANG1 Hong-Kui Yu1 De-Yi Xiong1 Qun LIU1,2
</author>
<affiliation confidence="0.9961325">
1Inst. of Computing Tech., The Chinese Academy of Science, Beijing, 100080 CHINA
2Inst. of Computational Linguistics, Peking University, Beijing, 100871 CHINA
</affiliation>
<email confidence="0.998024">
Email: zhanghp@software.ict.ac.cn
</email>
<sectionHeader confidence="0.969899" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99997212">
This document presents the results from
Inst. of Computing Tech., CAS in the
ACL- SIGHAN-sponsored First Interna-
tional Chinese Word Segmentation Bake-
off. The authors introduce the unified
HHMM-based frame of our Chinese lexi-
cal analyzer ICTCLAS and explain the
operation of the six tracks. Then provide
the evaluation results and give more
analysis. Evaluation on ICTCLAS shows
that its performance is competitive. Com-
pared with other system, ICTCLAS has
ranked top both in CTB and PK closed
track. In PK open track, it ranks second
position. ICTCLAS BIG5 version was
transformed from GB version only in two
days; however, it achieved well in two
BIG5 closed tracks. Through the first
bakeoff, we could learn more about the
development in Chinese word segmenta-
tion and become more confident on our
HHMM-based approach. At the same
time, we really find our problems during
the evaluation. The bakeoff is interesting
and helpful.
</bodyText>
<sectionHeader confidence="0.986435" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.929199071428571">
ICT (Institute of Computing Technology,
Chinese Academy of Sciences) participated the
First International Chinese Word Segmentation
Bakeoff. We have taken six tracks: Academia
Sinica closed (ASc), U. Penn Chinese Tree Bank
open and closed(CTBo,c), Hong Kong CityU
closed (HKc), Peking University open and
closed(PKo,c).
The structure of this document is as follows.
The next section presents the HHMM-based
framework of ICTCLAS. Next we detail the opera-
tion of six tracks. The following section provides
evaluation result and gives further analysis.
2 HHMM-based Chinese lexical analysis
</bodyText>
<sectionHeader confidence="0.868224" genericHeader="method">
2.1 ICTCLAS Framework
</sectionHeader>
<bodyText confidence="0.9994924375">
As illustrated in Figure 1, HHMM-based Chi-
nese lexical analysis comprises five levels: atom
segmentation, simple and recursive unknown
words recognition, class-based segmentation and
POS tagging. In the whole frame, class-based seg-
mentation graph, which is a directed graph de-
signed for word segmentation, is an essential
intermediate data structure that links disambigua-
tion, unknown words recognition with word seg-
mentation and POS tagging.
Atom segmentation, the bottom level of
HHMM, is an initial step. Here, atom is defined to
be the minimal segmentation unit that cannot be
split in any stage. The atom consists of Chinese
character, punctuation, symbol string, numeric ex-
pression and other non-Chinese char string. Any
word is made up of an atom or more. Atom seg-
mentation is to segment original text into atom se-
quence and it provides pure and simple source for
its parent HMM. For instance, a sentence like
&amp;quot;2002.9,ICTCLAS (6,nEh:gfFApRtti&amp;quot; (The
free source codes of ICTCLAS was distributed in
September, 2002) would be segmented as atom
sequence &amp;quot;2002.9/,/ICTCLAS/�J/n/Eh/�/q/fF
/44M/M/�/&amp;quot;. In this HMM, the original symbol is
observation while the atom is state. We skip the
detail of operation in that it&apos;s a simple application
on the basis of HMM. POS tagging and role tag-
ging using Viterbi are also skipped because they
are classic application of HMM. Because of paper
length limit, unknown words recognition is omitted.
Our previous papers (Zhang et al. 2003) gave more
</bodyText>
<footnote confidence="0.865703">
. ICTCLAS is the abbreviation for &amp;quot;Inst. of Computing Tech., Chinese Lexical Analysis System.&amp;quot;. We published ICTCLAS as
free software. The full source code and document of ICTCLAS is available at no cost for non-commercial use. Wel-
come researchers and technical users download ICTCLAS from Open Platform of Chinese NLP (www.nlp.org.cn).
</footnote>
<figure confidence="0.942727666666667">
explanation.
Atom sequence
String
Atom
Segmentation
Srh HMM
</figure>
<bodyText confidence="0.9996393">
Given a word wi, classc i is defined in Figure 2.
Suppose ILEXI to be the lexicon size, then
the total number of word classes is
ILEXI+9.
Given the atom sequence A=(al,...an),
let W=(wl,...wm) be the words sequence,
C= (cl,...cm) be a corresponding class se-
quence of W, and W# be the choice of word
segmentation with the maximized probabil-
ity, respectively. Then, we could get:
</bodyText>
<equation confidence="0.890101666666667">
2rh HMM
W#= arg max P(WIA)= arg max P(W,A)/P(A)
W W
</equation>
<bodyText confidence="0.981536">
For a specific atom sequence A, P(A)
is a constant and P(W,A)= P(W). So,
</bodyText>
<equation confidence="0.971319">
W#= arg max P(W)
W
</equation>
<bodyText confidence="0.920271">
On the basis of Baye&apos;s Theorem, it
can be induced that:
</bodyText>
<equation confidence="0.999473111111111">
W# = arg max P(WIC)P(C)
w
W# can be found with another level of
HMM if class ci is viewed as state while
word wi is output. Therefore:
( I ) ( I 1),
�� �� � �� ���
� � ... �
1 � �
</equation>
<bodyText confidence="0.9990265">
where co is begin of sentence.
For convenience, we often use the
negative log probability instead of the
proper form. That is:
</bodyText>
<figure confidence="0.999184257142857">
Simple unknown
words recognition
Recursive unknown
words recognition
Revised
SP-based rough
segmentation
Top n sequence
results
4`hHMM
PER
LOC
LOC
ORG
3&apos;d HMM
Class-based
segmentation
Words sequence
W# arg max
M
IIP
i~1
W # = arg min Y_ [— ln p(w. I c )
t
W i=1
M
i P(ci Ii
ln
c
�
1)�
ci =
POS
Tagging
POS sequence
</figure>
<figureCaption confidence="0.999739">
Figure 1. HHMM-based Chinese lexical analysis
</figureCaption>
<sectionHeader confidence="0.5097915" genericHeader="method">
2.2 Class-based HMM for word segmenta-
tion
</sectionHeader>
<bodyText confidence="0.875692714285714">
We apply to word segmentation class-based
HMM, which is a generalized approach covering
both common words and unknown words.
wi iff wi is listed in the segmentation lexicon;
PER iff wi is unlisted� personal name;
LOC iff wi is unlisted location name;
ORG iff wi is unlisted organization name;
TIME iff wi is unlisted time expression;
NUM iff wi is unlisted numeric expression;
STR iffwi is unlisted symbol string;
BEG iff beginning of a sentence
END iff ending of a sentence
OTHER otherwise.
� &amp;quot;unlisted&amp;quot; is referred as being outside the lexicon
</bodyText>
<figureCaption confidence="0.986344">
Figure 2: Class Definition of word wi
</figureCaption>
<bodyText confidence="0.966865333333333">
According to the word class definition, if wi is
listed in lexicon, then ci is wi, and p(wiIci) is equal
to 1.0.Otherwise, p(wiIci) is probability that class
</bodyText>
<figure confidence="0.98767952">
Lexical
results
Ph HMM
-I0g P(�I-T_-) -log p(T,IM -log p(NUMI30 -Iog P(`fINUM) -Iog P(101!�f) -Iog P(1I10)
S/S
-log p(-T--IS)
-logp(PERIS)
:t/:t
:t43/PER
-log p(NUMIPER)
4/4 ter,/,ter, 1893/NUM
-logp(TIMEIPER)
-1og P(TIMEI*)
-log P(A1-I*)
1893 */TIME 01/91
-log P(AITIME)
-1og p(1A1ITIME)
/ Wig 1/1
-IogP(EI&apos;l)
E/E
Figure3. Class-based word segmentation
Note:
1. The original sentence is &amp;quot;Et43T,, 1893 *01&amp;quot; (Mao Ze-Dong was born in the year of
1893). Its atom sequence is &amp;quot; /�/.ter./1893/*/i9/1/&amp;quot; after atom segmentation;
2.The node format is &amp;quot;word/class&amp;quot; (wi / ci) and the weight on the node is —log p (wi I ci);
</figure>
<listItem confidence="0.906235">
3. Weight on the directed edge is —log p (ci I ci-1);
4. &amp;quot; %,,ter.&amp;quot; (Mao Ze-Dong) is personal name outside the lexicon.The node &amp;quot; -:t4-/T,,/PER&amp;quot; and
the related edges with dash line is inserted after unknown words recognition.
</listItem>
<bodyText confidence="0.987729214285714">
ci initially activates wi , and it could be estimated
in its child HMM for unknown words recognition.
As demonstrated in Figure 3, we provide the
process of class-based word segmentation on &amp;quot;Et
MT,, 1893&apos; 01&amp;quot; (Mao Ze-Dong was born in the
year of 1893). The significance of our method is: it
covers the possible ambiguity. Moreover, unknown
words, which are recognized in the following steps,
can be added into the segmentation graph and pro-
ceeded as any other common words.
After transformation through class-based HMM,
word segmentation becomes single-source shortest
paths problem. Hence the best choice W# of word seg-
mentation is easy to find using Djikstra&apos;s algorithm.
</bodyText>
<sectionHeader confidence="0.96578" genericHeader="evaluation">
3 Tracks
</sectionHeader>
<bodyText confidence="0.9998595">
Here, we would introduce the operation of
some different track.
</bodyText>
<subsectionHeader confidence="0.998845">
3.1 Closed Tracks
</subsectionHeader>
<bodyText confidence="0.984378">
We participate all the closed tracks. As for
each closed track, we first extracted all the com-
mon words and tokens that appear in the training
corpus. Then build the segmentation core lexicons
with the words. Those named entity words are
classified into different named entities: numeric
and time expression, personal names, location
names, and transliterated names. According to
named entities in the given corpus, we could train
both class-based segmentation HMM and role-
based HMM model for unknown word recognition.
Therefore, the whole lexical system including un-
known word detection is accomplished as shown in
Figure 1.
</bodyText>
<subsectionHeader confidence="0.998394">
3.2 Open Tracks
</subsectionHeader>
<bodyText confidence="0.999988944444445">
We only participate GB code open tracks. Ac-
tually, open track is similar to closed one. The only
difference is the size of training data set. In Peking
University open track, ICTCLAS is trained on six-
month news corpus that is 5 months more than
closed track. The entire corpus is also from Peking
University. Except for the additional corpus, we
have not employed any other special libraries or
other resources.
As for CTB open track, we find that it cannot
benefit from that 5 month PKU corpus. Actually,
PKU standard is very different from CTB one
though they seemed similar. Core lexicon extracted
from Peking corpus degraded the performance on
CTB testing data. Except for some named entity
corpus, we could not get any more sources related
to CTB standard. Therefore, CTB open track is
operated in the similar way as closed track.
</bodyText>
<subsectionHeader confidence="0.9764">
3.3 BIG5-coded Tracks
</subsectionHeader>
<bodyText confidence="0.999754111111111">
Before the bakeoff, BIG5-coded word seg-
mentation has never been researched in our insti-
tute. Besides the character code, common words
and sentence styles are greatly different in China
mainland and Taiwan or Hong Kong. Because of
time limitation, we have only spent two days on
transforming our GB-coded ICTCLAS to BIG5-
coded lexical analyzer. For each BIG5 closed, we
extracted a BIG5-coded core lexicon. Then, the
</bodyText>
<table confidence="0.947863789473684">
BIG5 version ICTCLAS could work properly. The core source code is same as GB version.
4 Evaluation result
Track ASc CTBc CTB o HKc PKc PKo
Participant Number 6 6 7 4 10 8
Corpus Size (bytes) 38,882 125,248 125,248 114,384 56,254 56,254
True Word count 11,985 39,922 39,922 34,955 17,194 17,194
Test Word count 12,360 40,460 40,426 37,274 17,582 17,563
Insertions 434 1,789 1,755 2,439 485 458
Deletions 59 1,251 1,251 120 97 89
Substitutions 506 3,281 3,262 2,291 562 539
Nchange 999 6,321 6,268 4,850 1,144 1,086
Recall (Rank) 0.953 (3) 0.886 (2) 0.887 (4) 0.931 (3) 0.962 (1) 0.963 (1)
Precision (Rank) 0.924 (5) 0.875 (1) 0.876 (4) 0.873 (3) 0.940 (3) 0.943 (2)
F measure (Rank) 0.938 (5) 0.881 (1) 0.881 (4) 0.901 (3) 0.951 (1) 0.953 (2)
OOV rate 0.022 0.181 0.181 0.071 0.069 0.069
OOV Recall (Rank) 0.178 (5) 0.705 (1) 0.707 (5) 0.243 (4) 0.724 (2) 0.743 (2)
IV Recall(Rank) 0.970 (3) 0.927 (5) 0.927 (5) 0.984 (1) 0.979 (2) 0.980 (1)
*Time Cost (s) 3.92 10.57 10.62 7.11 5.18 5.53
**Speed (bytes/s) 9,919 11,849 11,794 16,088 10,860 10,173
</table>
<tableCaption confidence="0.7006885">
Table 1. Evaluation result of ICTCLAS
in the First International Chinese Word Segmentation Bakeoff
</tableCaption>
<table confidence="0.7179895">
*Time Cost: CPU: Pentium 4, 1.6GHz; Main Memory: 192M
**Speed=Corpus Size / Time cost * 1000
</table>
<bodyText confidence="0.998278416666667">
Compared with other systems, ICTCLAS espe-
cially GB-coded version is competitive. In both
GB-coded closed tracks, ICTCLAS ranked top.
ICTCLAS also rank second position in Peking
open track. Because of the lack of resources, CTB
open track is almost as same as CTB closed track.
The final performance in BIG5 track is not very
good. As a preliminary BIG-coded system, how-
ever, we are satisfied with the result.
As is shown in Table 1, It could also be con-
cluded that class-based segmentation HMM is ef-
fective. Excepted for CTB, IV Recall is over 97%.
</bodyText>
<sectionHeader confidence="0.612948" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999950571428571">
Through the first bakeoff, we have learn more
about the development in Chinese word segmenta-
tion and become more confident on our HHMM-
based approach. At the same time, we really find
our problems during the evaluation. The bakeoff is
interesting and helpful. We look forward to par-
ticipate forthcoming bakeoff.
</bodyText>
<sectionHeader confidence="0.999484" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.998545">
The authors would like to thank Prof. Shiwen
Yu of Peking University for the Peking corpus.
And we acknowledge our debt to Gang Zou, Dr.
Bin Wang, Dr. Jian Sun, Ji-Feng Li, Hao Zhang
and other colleagues. Huaping Zhang would espe-
cially express gratitude to his graceful girl friend
Feifei and her family for their encouragement. We
also thank Richard Sproat, Qing Ma, Fei Xia and
other SIGHAN colleagues for their elaborate or-
ganization and enthusiastic help in the First Inter-
national Chinese Word Segmentation Bakeoff.
</bodyText>
<sectionHeader confidence="0.669304" genericHeader="references">
References
</sectionHeader>
<figureCaption confidence="0.739584157894737">
Lawrence. R.Rabiner.1989. A Tutorial on Hidden
Markov Models and Selected Applications in Speech
Recognition. Proceedings of IEEE 77(2): pp.257-286.
Shai Fine, Yoram Singer, and Naftali Tishby.1998. The
hierarchical Hidden Markov Model: Analysis and ap-
plications. Machine Learning, 32:41
Zhang Hua-Ping, Liu Qun. Model of Chinese Words
Rough Segmentation Based on N-Shortest-Paths
Method. Journal of Chinese information processing,
2002,16(5):1-7 (in Chinese)
ZHANG Hua-Ping, LIU Qun, Zhang Hao and Cheng
Xue-Qi. 2002. Automatic Recognition of Chinese Un-
known Words Recognition. Proc. of First SigHan at-
tached on COLING 2002
ZHANG Hua-Ping, LIU Qun, YU Hong-Kui, CHENG
Xue-Qi, BAI Shuo. Chinese Named Entity Recognition
Using Role Model. International Journal of Computa-
tional Linguistics and Chinese language processing,
2003,Vol. 8 (2)
</figureCaption>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.865126">
<title confidence="0.9985">Chinese Lexical Analyzer</title>
<author confidence="0.975983">Hong-Kui De-Yi Qun</author>
<address confidence="0.9448135">of Computing Tech., The Chinese Academy of Science, Beijing, 100080 of Computational Linguistics, Peking University, Beijing, 100871</address>
<abstract confidence="0.998922423076923">This document presents the results from Inst. of Computing Tech., CAS in the ACL- SIGHAN-sponsored First International Chinese Word Segmentation Bakeoff. The authors introduce the unified HHMM-based frame of our Chinese lexical analyzer ICTCLAS and explain the operation of the six tracks. Then provide the evaluation results and give more analysis. Evaluation on ICTCLAS shows that its performance is competitive. Compared with other system, ICTCLAS has ranked top both in CTB and PK closed track. In PK open track, it ranks second position. ICTCLAS BIG5 version was transformed from GB version only in two days; however, it achieved well in two BIG5 closed tracks. Through the first bakeoff, we could learn more about the development in Chinese word segmentation and become more confident on our HHMM-based approach. At the same time, we really find our problems during the evaluation. The bakeoff is interesting and helpful.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>