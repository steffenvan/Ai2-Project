<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007349">
<title confidence="0.999119">
TeamX: A Sentiment Analyzer with Enhanced Lexicon Mapping and
Weighting Scheme for Unbalanced Data
</title>
<author confidence="0.97452">
Yasuhide Miura
</author>
<affiliation confidence="0.952255">
Fuji Xerox Co., Ltd. / Japan
</affiliation>
<email confidence="0.976626">
yasuhide.miura@fujixerox.co.jp
</email>
<author confidence="0.957502">
Keigo Hattori
</author>
<affiliation confidence="0.934046">
Fuji Xerox Co., Ltd. / Japan
</affiliation>
<email confidence="0.995585">
keigo.hattori@fujixerox.co.jp
</email>
<sectionHeader confidence="0.993838" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999822684210526">
This paper describes the system that has
been used by TeamX in SemEval-2014
Task 9 Subtask B. The system is a senti-
ment analyzer based on a supervised text
categorization approach designed with fol-
lowing two concepts. Firstly, since lex-
icon features were shown to be effective
in SemEval-2013 Task 2, various lexicons
and pre-processors for them are introduced
to enhance lexical information. Secondly,
since a distribution of sentiment on tweets
is known to be unbalanced, an weighting
scheme is introduced to bias an output of a
machine learner. For the test run, the sys-
tem was tuned towards Twitter texts and
successfully achieved high scoring results
on Twitter data, average F1 70.96 on Twit-
ter2014 and average F1 56.50 on Twit-
ter2014Sarcasm.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999422333333333">
The growth of social media has brought a ris-
ing interest to make natural language technologies
that work with informal texts. Sentiment anal-
ysis is one such technology, and several work-
shops such as SemEval-2013 Task 2 (Nakov et
al., 2013), CLEF 2013 RepLab 2013 (Amig´o
et al., 2013), and TASS 2013 (Villena-Rom´an
and Garcia-Morera, 2013) have recently targeted
tweets or cell phone messages as analysis text.
This paper describes a system that has submit-
ted a sentiment analysis result to Subtask B of
SemEval-2014 Task9 (Rosenthal et al., 2014).
SemEval-2014 Task9 is a rerun of SemEval-2013
Task 2 with different test data, and Subtask B is a
task of message polarity classification.
</bodyText>
<footnote confidence="0.93341575">
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
</footnote>
<author confidence="0.682902">
Shigeyuki Sakaki
</author>
<affiliation confidence="0.670376">
Fuji Xerox Co., Ltd. / Japan
</affiliation>
<email confidence="0.935528">
sakaki.shigeyuki@fujixerox.co.jp
</email>
<author confidence="0.802504">
Tomoko Ohkuma
</author>
<affiliation confidence="0.728681">
Fuji Xerox Co., Ltd. / Japan
</affiliation>
<email confidence="0.972561">
ohkuma.tomoko@fujixerox.co.jp
</email>
<bodyText confidence="0.999907428571429">
The system we prepared is a sentiment ana-
lyzer based on a supervised text categorization
approach. Various features and their extraction
methods are integrated in the system following the
works presented in SemEval-2013 Task 2. Addi-
tionally to these features, we assembled following
notable functionalities to the system:
</bodyText>
<listItem confidence="0.987772909090909">
1. Processes to enhance word-to-lemma map-
ping.
(a) A spelling corrector to normalize out-of-
vocabulary words.
(b) Two Part-of-Speech (POS) taggers to
realize word-to-lemma mapping in two
perspectives.
(c) A word sense disambiguator to obtain
word senses and their confidence scores.
2. An weighting scheme to bias an output of a
machine learner.
</listItem>
<bodyText confidence="0.999730125">
Functionalities 1a to 1c are introduced to enhance
information based on lexical knowledge, since
features based on lexicons are shown to be ef-
fective in SemEval-2013 Task 2 (Mohammad et
al., 2013). Functionality 2 is introduced to make
the system adjustable to polarity unbalancedness
known to exists in Twitter data (Nakov et al.,
2013).
The accompanying sections of this papers are
organized as follows. Section 2 describes re-
sources such as labeled texts and lexicons used in
our system. Section 3 explains the details of the
system. Section 4 discusses the submission test
run and some extra test runs that we performed
after the test data release. Finally, section 5 con-
cludes the paper.
</bodyText>
<sectionHeader confidence="0.99943" genericHeader="introduction">
2 Resources
</sectionHeader>
<subsectionHeader confidence="0.999848">
2.1 Sentiment Labeled Data
</subsectionHeader>
<bodyText confidence="0.9953775">
The system is a constrained system, therefore only
the sentiment labeled data distributed by the task
</bodyText>
<page confidence="0.96218">
628
</page>
<note confidence="0.760259">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 628–632,
Dublin, Ireland, August 23-24, 2014.
</note>
<table confidence="0.9992246">
Type #Used #Full %
Twitter(train) 6949 9684 71.8
Twitter(dev) 1066 1654 64.4
Twitter(dev-test) 3269 3813 85.7
SMS(dev-test) 2094 2094 100
</table>
<tableCaption confidence="0.688963375">
Table 1: The numbers of messages for each type.
‘train’, ‘dev’, and ‘dev-test’ denote training, devel-
opment, and development-test respectively. #Used
is the number of messages that we were able to
obtain, and #Full is the maximum number of mes-
sages that were provided.
Table 2: The seven sentiment lexicons and their
criteria.
</tableCaption>
<bodyText confidence="0.9989758">
organizers were used. However, due to accessibil-
ity changes in tweets, a subset of the training, the
development, and the development-test data were
used. Table 1 shows the numbers of messages for
each type.
</bodyText>
<subsectionHeader confidence="0.999784">
2.2 Sentiment Lexicons
</subsectionHeader>
<bodyText confidence="0.999960733333333">
The system includes seven sentiment lexicons
namely: AFINN-111 (Nielsen, 2011), Bing Liu’s
Opinion Lexicon1, General Inquirer (Stone et al.,
1966), MPQA Subjectivity Lexicon (Wilson et al.,
2005), NRC Hashtag Sentiment Lexicon (Moham-
mad et al., 2013), Sentiment140 Lexicon (Moham-
mad et al., 2013), and SentiWordNet (Baccianella
et al., 2010). We categorized these seven lexi-
cons to two criteria: ‘FORMAL’ and ‘INFOR-
MAL’. Lexicons that include lemmas of erroneous
words (e.g. misspelled words) were categorized
to ‘INFORMAL’. Table 2 illustrates the criteria of
the seven lexicons. These criteria are used in the
process of word-to-lemma mapping processes and
will be explained in Section 3.1.3.
</bodyText>
<sectionHeader confidence="0.990891" genericHeader="method">
3 System Details
</sectionHeader>
<bodyText confidence="0.9986235">
The system is a modularized system consisting
of a variety of pre-processors, feature extractors,
</bodyText>
<footnote confidence="0.9508995">
1http://www.cs.uic.edu/˜liub/FBS/
sentiment-analysis.html
</footnote>
<figureCaption confidence="0.99963">
Figure 1: An overview of the system.
</figureCaption>
<bodyText confidence="0.890798">
and a machine learner. Figure 1 illustrates the
overview of the system.
</bodyText>
<subsectionHeader confidence="0.9996255">
3.1 Pre-processors
3.1.1 Text Normalizer
</subsectionHeader>
<bodyText confidence="0.999929">
The text normalizer performs following three rule-
based normalization of an input text:
</bodyText>
<listItem confidence="0.9995078">
• Unicode normalization in form NFKC2.
• All upper case letters are converted to lower
case ones (ex. ‘GooD’ to ‘good’).
• URLs are exchanged with string ‘URL’s (ex.
‘http://example.org’ to ‘URL’).
</listItem>
<subsectionHeader confidence="0.953629">
3.1.2 Spelling Corrector
</subsectionHeader>
<bodyText confidence="0.999977142857143">
A spelling corrector is included in the system to
normalize misspellings. We used Jazzy3, an open
source spell checker with US English dictionaries
provided along with Jazzy. Jazzy combines Dou-
bleMetaphone phonetic matching algorithm and a
near-miss match algorithm based on Levenshtein
distance to correct a misspelled word.
</bodyText>
<subsectionHeader confidence="0.863523">
3.1.3 POS Taggers
</subsectionHeader>
<bodyText confidence="0.9590614">
The system includes two POS taggers to realize
word-to-lemma mapping in two perspectives.
Stanford POS Tagger Stanford Log-linear Part-
of-Speech Tagger (Toutanova et al., 2003) is
one POS tagger which is used to map words
</bodyText>
<footnote confidence="0.997513">
2http://www.unicode.org/reports/tr15/
3http://jazzy.sourceforge.net/
</footnote>
<figure confidence="0.993443363636364">
Criterion
Lexicon
General Inquirer
FORMAL
MPQA Subjectivity Lexicon
SentiWordNet
AFINN-111
Bing Liu’s Opinion Lexicon
NRC Hashtag Sentiment Lexicon
Sentiment140 Lexicon
INFORMAL
Output
Input
word
senses
Word Sense
Disambiguator
FORMAL
lexicons
Text
Normalizer
INFORMAL
lexicons
Machine
Learner
Spelling
Corrector
character
ngrams
Stanford
POS Tagger
Negation
Detector
Feature Extractors
word
ngrams
Prediction
Adjuster
Pre­processors
CMU ARK
POS Tagger
Negation
Detector
clusters
</figure>
<page confidence="0.990957">
629
</page>
<bodyText confidence="0.998535454545454">
to lemmas of ‘FORMAL’ criterion lexicons,
and to extract word sense features. A finite-
state transducer based lemmatizer (Minnen et
al., 2001) included in the POS tagger is used
to obtain lemmas of tokenized words.
CMU ARK POS Tagger A POS tagger for
tweets by CMU ARK group (Owoputi et al.,
2013) is another POS tagger used to map
words to lemmas of ‘INFORMAL’ criterion
lexicons, and to extract ngram features and a
cluster feature.
</bodyText>
<subsectionHeader confidence="0.824349">
3.1.4 Word Sense Disambiguator
</subsectionHeader>
<bodyText confidence="0.998282125">
A word sense disambiguator is included in the sys-
tem to determine a sense of a word. We used
UKB4 which implements graph-based word sense
disambiguation based on Personalized PageRank
algorithm (Agirre and Soroa, 2009) on a lexical
knowledge base. As a lexical knowledge base,
WordNet 3.0 (Fellbaum, 1998) included in the
UKB package is used.
</bodyText>
<subsectionHeader confidence="0.848256">
3.1.5 Negation Detector
</subsectionHeader>
<bodyText confidence="0.999612">
The system includes a simple rule-based negation
detector. The detector is an implementation of the
algorithm on Christopher Potts’ Sentiment Sym-
posium Tutorial5. The algorithm is a simple algo-
rithm that appends a negation suffix to words that
appear within a negation scope surrounded by a
negation key (ex. ‘no’) and a certain punctuation
(ex. ‘:’).
</bodyText>
<subsectionHeader confidence="0.942517">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.99989025">
The followings are the features used in the system.
word ngrams Contiguous 1, 2, 3, and 4 grams
of words, and non-contiguous 3 and 4 grams
of words are extracted from a given words.
Non-contiguous ngram are ngrams where one
of words are replaced with a wild card word
‘*’. Example of contiguous 3 grams is
‘by the way’, and the corresponding noncon-
tiguous variation is ‘by * way’.
character ngrams Contiguous 3, 4, and 5 grams
of characters with in a word are extracted
from given words.
lexicons Words are mapped to seven lexicons of
section 2.2. For two sentiment labels (pos-
itive and negative) in each lexicon, follow-
ing four values are extracted: total matched
</bodyText>
<footnote confidence="0.977493333333333">
4http://ixa2.si.ehu.es/ukb/
5http://sentiment.christopherpotts.
net/lingstruc.html#negation
</footnote>
<figureCaption confidence="0.999456">
Figure 2: An example of word senses feature.
</figureCaption>
<bodyText confidence="0.999570615384615">
word count, total score, maximal score, and
last word score6. For lexicons without senti-
ment scores, score 1.0 is used for all entries.
Note that different POS taggers are used in
word-to-lemma mapping as described in Sec-
tion 3.1.3.
clusters Words are mapped to Twitter Word Clus-
ters of CMU ARK group7. The largest clus-
tering result consisting of 1000 clusters from
approximately 56 million tweets is used as
clusters.
word senses A result of the word sense disam-
biguator is extracted as weighted features ac-
cording to their scores. Figure 2 shows an
example of this feature.
The ngram features are introduced as basic bag-
of-words features in a supervised text categoriza-
tion approach. Lexicon features are designed to
strengthen the lexical features of Mohammad et
al. (2013) which have been shown to be effective
in the last year’s task. Cluster features are im-
plemented as an improvement for an supervised
NLP system following the work of Turian et al.
(2010). Word sense features are utilized to help
subjectivity analysis and contextual polarity anal-
ysis (Akkaya et al., 2009).
</bodyText>
<subsectionHeader confidence="0.999189">
3.3 Machine Learner
</subsectionHeader>
<bodyText confidence="0.999657142857143">
Logistic Regression is utilized as an algorithm of
a supervised machine learning method. As an
implementation of Logistic Regression, LIBLIN-
EAR (Fan et al., 2008) is used. A Logistic Regres-
sion is trained using the features of Section 3.2
with the three polarities (positive, negative, and
neutral) as labels.
</bodyText>
<footnote confidence="0.997942">
6The total number of lexical features is 7 x 2 x 4 = 56.
7http://www.ark.cs.cmu.edu/TweetNLP/
</footnote>
<figure confidence="0.99881395">
Feature Weight
01824736-v 0.442313
01777210-v 0.355679
01776952-v 0.148101
06277280-n 0.688655
...
WSD result
text
Sense ID Score
01824736-v 0.442313
01777210-v 0.355679
01776952-v 0.148101
I liked an example.org video http://example.org
...
Sense ID Score
06277280-n 0.688655
06277803-n 0.163343
04534127-n 0.103199
...
Features
</figure>
<page confidence="0.980724">
630
</page>
<table confidence="0.998839">
Parameters Sources
Parameter Selection Source LiveJournal SMS Twitter Twitter Twitter2014
C Wpos Wneg 2014 2013 2013 2014 Sarcasm
Twitter(train)+Twitter(dev) 0.07 1.7 2.6 71.23 62.33 71.28 70.40 53.32
Twitter(dev-test)* 0.03 2.4 3.3 69.44 57.36 72.12 70.96 56.50
SMS(dev-test) 0.80 1.1 1.2 72.99 68.92 65.65 66.66 48.24
SMS(dev-test)+Twitter(dev-test) 0.07 1.9 2.0 72.54 65.44 70.41 69.80 51.09
</table>
<tableCaption confidence="0.743124666666667">
Table 3: The scores for each source in the test runs. The run with asterisk (*) denotes the submission
run. The values in the ‘Sources’ columns represent scores in SemEval-2014 Task 9 metric (the average
of positive F1 and negative F1).
</tableCaption>
<subsectionHeader confidence="0.999478">
3.4 Prediction Adjuster
</subsectionHeader>
<bodyText confidence="0.99740275">
Since the labels in the tweets data are unbalanced
(Nakov et al., 2013), we prepared a prediction ad-
juster for Logistic Regression output. For each po-
larity l, an weighting factor wl that adjusts a proba-
bility output Pr(l) is introduced. An updated pre-
diction label is decided by selecting an l that max-
imizes score(l) which can be expressed as equa-
tion 1.
</bodyText>
<equation confidence="0.96146">
arg max score(l) = wlPr(l) (1)
lE{pos,neg,neu}
</equation>
<bodyText confidence="0.999970333333333">
The approach we took in this prediction adjuster
is a simple approach to bias an output of Logistic
Regression, but may not be a typical approach to
handle unbalanced data. For instance, LIBLIN-
EAR includes the weighting option ‘-wi’ which
enables a use of different cost parameter C for dif-
ferent classes. One advantage of our approach is
that the change in wl does not require a training of
Logistic Regression. Various values of wl can be
tested with very low computational cost, which is
helpful in a situation like SemEval tasks where the
time for development is limited.
</bodyText>
<sectionHeader confidence="0.965326" genericHeader="method">
4 Test Runs
</sectionHeader>
<subsectionHeader confidence="0.999819">
4.1 Submission Test Run
</subsectionHeader>
<bodyText confidence="0.999811">
The system was trained using the 8,015 tweets in-
cluded in Twitter(train) and Twitter(dev) described
in Section 2.1. Three parameters: cost parameter
C of Logistic Regression, weight wpos of the pre-
diction adjuster, and weight wneg of the predic-
tion adjuster, were considered in the submission
test run. For the wneu of the prediction adjuster, a
fixed value of 1.0 was used.
Prior to the submission test run, the following
two steps were performed to select a parameter
combination for the submission run.
Step 1 The system with all combinations of C in
range of {0.01 to 0.09 by step 0.01, 0.1 to 0.9
by step 0.1, 1 to 10 by step 1}, wpos in range
of {1.0 to 5.0 by step 0.1}, and wneg in range
of {1.0 to 5.0 by step 0.1} were prepared8.
Step 2 The performances of the system for all
these parameter combinations were calcu-
lated using Twitter(dev-test) described in
Section 2.1.
As a result, the parameter combination C = 0.03,
wpos = 2.4, and wneg = 3.3 which performed
best in Twitter(dev-test) was selected as a parame-
ter combination for the submission run.
Finally, the system with the selected parameters
was applied to the test set of SemEval-2014 Task
9. ‘Twitter(dev-test)’ in Table 3 shows the val-
ues of this submission run. The system achieved
high performances on Twitter data: 72.12, 70.96,
and 56.50 on Twitter2013, Twitter2014, and Twit-
ter2014Sarcasm respectively.
</bodyText>
<subsectionHeader confidence="0.987887">
4.2 Post-Submission Test Runs
</subsectionHeader>
<bodyText confidence="0.999988736842105">
The system performed quite well on Twitter
data but not so well on other data on the sub-
mission run. After the release of the gold
data of the 2014 test tun, we conducted sev-
eral test runs using different parameter combina-
tions. ‘Twitter(train)+Twitter(dev)’, ‘SMS(dev-
test)’, and ‘SMS(dev-test)+Twitter(dev-test)’ are
the results of test runs with different data sources
used for the parameter selection process. In ‘Twit-
ter(train)+Twitter(dev)’, the parameter combina-
tion that maximizes a micro-average score of 5-
fold cross validation was chosen since the training
data and the parameter selection are equivalent.
The parameter combination selected with ‘Twit-
ter(train)+Twitter(dev)’ showed similar result as
the submission run, which is high performances
on Twitter data. In the case of ‘SMS(dev-test)’, the
system performed well on ‘LiveJournal2014’ and
‘SMS(dev-test)’ namely 72.99 and 68.92. How-
</bodyText>
<footnote confidence="0.7820455">
8The total number of parameter combination is 29 x 51 x
51 = 75429.
</footnote>
<page confidence="0.997098">
631
</page>
<bodyText confidence="0.999926666666667">
ever, in this parameter combination the scores on
Twitter data were clearly lower than the submis-
sion run. Finally, ‘SMS(dev-test)+Twitter(dev-
test)’ resulted to a mid performing result, where
scores for each source marked in-between values
of ‘Twitter(dev-test)’ and ‘SMS(dev-test)’.
</bodyText>
<sectionHeader confidence="0.997701" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999988733333333">
We proposed a system that is designed to enhance
information based on lexical knowledge and to
be adjustable to unbalanced training data. With
parameters tuned towards Twitter data, the sys-
tem successfully achieved high scoring results on
Twitter data, average F1 70.96 on Twitter2014 and
average F1 56.50 on Twitter2014Sarcasm.
Additional test runs with different parameter
combination showed that the system can be tuned
to perform well on non-Twitter data such as blogs
or short messages. However, the limitation of our
approach to directly weight a machine learner’s
output was shown, since we could not find a
general purpose parameter combination that can
achieve high scores on any types of data.
</bodyText>
<sectionHeader confidence="0.994727" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9997155">
We would like to thank the anonymous reviewers
for their valuable comments to improve this paper.
</bodyText>
<sectionHeader confidence="0.99816" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998903256756757">
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for word sense disambiguation. In Pro-
ceedings of EACL 2009, pages 33–41.
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea.
2009. Subjectivity word sense disambiguation. In
Proceedings of EMNLP 2009, pages 190–199.
Enrique Amig´o, Jorge Carrillo de Albornoz, Irina
Chugur, Adolfo Corujo, Julio Gonzalo, Tamara
Martfn, Edgar Meij, Maarten de Rijke, and Damiano
Spina. 2013. Overview of RepLab 2013: Evaluat-
ing online reputation monitoring systems. In CLEF
2013 Evaluation Labs and Workshop, Online Work-
ing Notes.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Proceedings of LREC 2010, pages 2200–2204.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. In Journal of
Machine Learning Research, volume 9, pages 1871–
1874.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat-
ural Language Engineering, 7(3):207–223.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the state-of-the-
art in sentiment analysis of tweets. In Proceedings
of the seventh international workshop on Semantic
Evaluation Exercises (SemEval-2013), pages 321–
327.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 task 2: Sentiment analysis
in Twitter. In Proceedings of the seventh interna-
tional workshop on Semantic Evaluation Exercises
(SemEval-2013), pages 312–320.
Finn ˚Arup Nielsen. 2011. A new ANEW: Evalu-
ation of a word list for sentiment analysis in mi-
croblogs. In Proceedings of the ESWC2011 Work-
shop on ‘Making Sense of Microposts’: Big things
come in small packages, pages 93–98.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL 2013, pages 380–390.
Sara Rosenthal, Alan Ritter, Preslav Nakov, and
Veselin Stoyanov. 2014. SemEval-2014 task 9:
Sentiment analysis in Twitter. In Proceedings of the
eighth international workshop on Semantic Evalua-
tion Exercises (SemEval-2014).
Philip Stone, Dexter Dunphy, Marshall Smith, and
Daniel Ogilvie. 1966. General Inquirer: A Com-
puter Approach to Content Analysis. MIT Press.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of HLT-NAACL 2003, pages 252–
259.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of
ACL 2010, pages 384–394.
Julio Villena-Rom´an and Janine Garcfa-Morera. 2013.
TASS 2013 - Workshop on sentiment analysis at SE-
PLN 2013: An overview. In Proceedings of the
TASS workshop at SEPLN 2013.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of HLT-
EMNLP 2005, pages 347–354.
</reference>
<page confidence="0.99773">
632
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.051337">
<title confidence="0.719712666666667">TeamX: A Sentiment Analyzer with Enhanced Lexicon Mapping Weighting Scheme for Unbalanced Data Yasuhide Miura</title>
<author confidence="0.438711">Japan</author>
<email confidence="0.947411">yasuhide.miura@fujixerox.co.jp</email>
<author confidence="0.994456">Keigo Hattori</author>
<affiliation confidence="0.640215">Fuji Xerox Co., Ltd. / Japan</affiliation>
<email confidence="0.975715">keigo.hattori@fujixerox.co.jp</email>
<abstract confidence="0.994213368421053">This paper describes the system that has been used by TeamX in SemEval-2014 Task 9 Subtask B. The system is a sentiment analyzer based on a supervised text categorization approach designed with following two concepts. Firstly, since lexicon features were shown to be effective in SemEval-2013 Task 2, various lexicons and pre-processors for them are introduced to enhance lexical information. Secondly, since a distribution of sentiment on tweets is known to be unbalanced, an weighting scheme is introduced to bias an output of a machine learner. For the test run, the system was tuned towards Twitter texts and successfully achieved high scoring results Twitter data, average on Twitand average on Twit-</abstract>
<intro confidence="0.372638">ter2014Sarcasm.</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Personalizing PageRank for word sense disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL</booktitle>
<pages>33--41</pages>
<contexts>
<context position="7541" citStr="Agirre and Soroa, 2009" startWordPosition="1133" endWordPosition="1136">d sense features. A finitestate transducer based lemmatizer (Minnen et al., 2001) included in the POS tagger is used to obtain lemmas of tokenized words. CMU ARK POS Tagger A POS tagger for tweets by CMU ARK group (Owoputi et al., 2013) is another POS tagger used to map words to lemmas of ‘INFORMAL’ criterion lexicons, and to extract ngram features and a cluster feature. 3.1.4 Word Sense Disambiguator A word sense disambiguator is included in the system to determine a sense of a word. We used UKB4 which implements graph-based word sense disambiguation based on Personalized PageRank algorithm (Agirre and Soroa, 2009) on a lexical knowledge base. As a lexical knowledge base, WordNet 3.0 (Fellbaum, 1998) included in the UKB package is used. 3.1.5 Negation Detector The system includes a simple rule-based negation detector. The detector is an implementation of the algorithm on Christopher Potts’ Sentiment Symposium Tutorial5. The algorithm is a simple algorithm that appends a negation suffix to words that appear within a negation scope surrounded by a negation key (ex. ‘no’) and a certain punctuation (ex. ‘:’). 3.2 Features The followings are the features used in the system. word ngrams Contiguous 1, 2, 3, an</context>
</contexts>
<marker>Agirre, Soroa, 2009</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2009. Personalizing PageRank for word sense disambiguation. In Proceedings of EACL 2009, pages 33–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cem Akkaya</author>
<author>Janyce Wiebe</author>
<author>Rada Mihalcea</author>
</authors>
<title>Subjectivity word sense disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>190--199</pages>
<contexts>
<context position="9938" citStr="Akkaya et al., 2009" startWordPosition="1523" endWordPosition="1526">ambiguator is extracted as weighted features according to their scores. Figure 2 shows an example of this feature. The ngram features are introduced as basic bagof-words features in a supervised text categorization approach. Lexicon features are designed to strengthen the lexical features of Mohammad et al. (2013) which have been shown to be effective in the last year’s task. Cluster features are implemented as an improvement for an supervised NLP system following the work of Turian et al. (2010). Word sense features are utilized to help subjectivity analysis and contextual polarity analysis (Akkaya et al., 2009). 3.3 Machine Learner Logistic Regression is utilized as an algorithm of a supervised machine learning method. As an implementation of Logistic Regression, LIBLINEAR (Fan et al., 2008) is used. A Logistic Regression is trained using the features of Section 3.2 with the three polarities (positive, negative, and neutral) as labels. 6The total number of lexical features is 7 x 2 x 4 = 56. 7http://www.ark.cs.cmu.edu/TweetNLP/ Feature Weight 01824736-v 0.442313 01777210-v 0.355679 01776952-v 0.148101 06277280-n 0.688655 ... WSD result text Sense ID Score 01824736-v 0.442313 01777210-v 0.355679 0177</context>
</contexts>
<marker>Akkaya, Wiebe, Mihalcea, 2009</marker>
<rawString>Cem Akkaya, Janyce Wiebe, and Rada Mihalcea. 2009. Subjectivity word sense disambiguation. In Proceedings of EMNLP 2009, pages 190–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrique Amig´o</author>
<author>Jorge Carrillo de Albornoz</author>
<author>Irina Chugur</author>
<author>Adolfo Corujo</author>
<author>Julio Gonzalo</author>
<author>Tamara Martfn</author>
<author>Edgar Meij</author>
<author>Maarten de Rijke</author>
<author>Damiano Spina</author>
</authors>
<title>Overview of RepLab 2013: Evaluating online reputation monitoring systems.</title>
<date>2013</date>
<booktitle>In CLEF 2013 Evaluation Labs and Workshop, Online Working Notes.</booktitle>
<marker>Amig´o, de Albornoz, Chugur, Corujo, Gonzalo, Martfn, Meij, de Rijke, Spina, 2013</marker>
<rawString>Enrique Amig´o, Jorge Carrillo de Albornoz, Irina Chugur, Adolfo Corujo, Julio Gonzalo, Tamara Martfn, Edgar Meij, Maarten de Rijke, and Damiano Spina. 2013. Overview of RepLab 2013: Evaluating online reputation monitoring systems. In CLEF 2013 Evaluation Labs and Workshop, Online Working Notes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC</booktitle>
<pages>2200--2204</pages>
<contexts>
<context position="4753" citStr="Baccianella et al., 2010" startWordPosition="726" endWordPosition="729"> 2: The seven sentiment lexicons and their criteria. organizers were used. However, due to accessibility changes in tweets, a subset of the training, the development, and the development-test data were used. Table 1 shows the numbers of messages for each type. 2.2 Sentiment Lexicons The system includes seven sentiment lexicons namely: AFINN-111 (Nielsen, 2011), Bing Liu’s Opinion Lexicon1, General Inquirer (Stone et al., 1966), MPQA Subjectivity Lexicon (Wilson et al., 2005), NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013), Sentiment140 Lexicon (Mohammad et al., 2013), and SentiWordNet (Baccianella et al., 2010). We categorized these seven lexicons to two criteria: ‘FORMAL’ and ‘INFORMAL’. Lexicons that include lemmas of erroneous words (e.g. misspelled words) were categorized to ‘INFORMAL’. Table 2 illustrates the criteria of the seven lexicons. These criteria are used in the process of word-to-lemma mapping processes and will be explained in Section 3.1.3. 3 System Details The system is a modularized system consisting of a variety of pre-processors, feature extractors, 1http://www.cs.uic.edu/˜liub/FBS/ sentiment-analysis.html Figure 1: An overview of the system. and a machine learner. Figure 1 illu</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In Proceedings of LREC 2010, pages 2200–2204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>In Journal of Machine Learning Research,</journal>
<volume>9</volume>
<pages>1871--1874</pages>
<contexts>
<context position="10122" citStr="Fan et al., 2008" startWordPosition="1551" endWordPosition="1554">ervised text categorization approach. Lexicon features are designed to strengthen the lexical features of Mohammad et al. (2013) which have been shown to be effective in the last year’s task. Cluster features are implemented as an improvement for an supervised NLP system following the work of Turian et al. (2010). Word sense features are utilized to help subjectivity analysis and contextual polarity analysis (Akkaya et al., 2009). 3.3 Machine Learner Logistic Regression is utilized as an algorithm of a supervised machine learning method. As an implementation of Logistic Regression, LIBLINEAR (Fan et al., 2008) is used. A Logistic Regression is trained using the features of Section 3.2 with the three polarities (positive, negative, and neutral) as labels. 6The total number of lexical features is 7 x 2 x 4 = 56. 7http://www.ark.cs.cmu.edu/TweetNLP/ Feature Weight 01824736-v 0.442313 01777210-v 0.355679 01776952-v 0.148101 06277280-n 0.688655 ... WSD result text Sense ID Score 01824736-v 0.442313 01777210-v 0.355679 01776952-v 0.148101 I liked an example.org video http://example.org ... Sense ID Score 06277280-n 0.688655 06277803-n 0.163343 04534127-n 0.103199 ... Features 630 Parameters Sources Param</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. In Journal of Machine Learning Research, volume 9, pages 1871– 1874.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Minnen</author>
<author>John Carroll</author>
<author>Darren Pearce</author>
</authors>
<title>Applied morphological processing of English.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="6999" citStr="Minnen et al., 2001" startWordPosition="1041" endWordPosition="1044">rion Lexicon General Inquirer FORMAL MPQA Subjectivity Lexicon SentiWordNet AFINN-111 Bing Liu’s Opinion Lexicon NRC Hashtag Sentiment Lexicon Sentiment140 Lexicon INFORMAL Output Input word senses Word Sense Disambiguator FORMAL lexicons Text Normalizer INFORMAL lexicons Machine Learner Spelling Corrector character ngrams Stanford POS Tagger Negation Detector Feature Extractors word ngrams Prediction Adjuster Preprocessors CMU ARK POS Tagger Negation Detector clusters 629 to lemmas of ‘FORMAL’ criterion lexicons, and to extract word sense features. A finitestate transducer based lemmatizer (Minnen et al., 2001) included in the POS tagger is used to obtain lemmas of tokenized words. CMU ARK POS Tagger A POS tagger for tweets by CMU ARK group (Owoputi et al., 2013) is another POS tagger used to map words to lemmas of ‘INFORMAL’ criterion lexicons, and to extract ngram features and a cluster feature. 3.1.4 Word Sense Disambiguator A word sense disambiguator is included in the system to determine a sense of a word. We used UKB4 which implements graph-based word sense disambiguation based on Personalized PageRank algorithm (Agirre and Soroa, 2009) on a lexical knowledge base. As a lexical knowledge base,</context>
</contexts>
<marker>Minnen, Carroll, Pearce, 2001</marker>
<rawString>Guido Minnen, John Carroll, and Darren Pearce. 2001. Applied morphological processing of English. Natural Language Engineering, 7(3):207–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>NRC-Canada: Building the state-of-theart in sentiment analysis of tweets.</title>
<date>2013</date>
<booktitle>In Proceedings of the seventh international workshop on Semantic Evaluation Exercises (SemEval-2013),</booktitle>
<pages>321--327</pages>
<contexts>
<context position="2937" citStr="Mohammad et al., 2013" startWordPosition="444" endWordPosition="447">to these features, we assembled following notable functionalities to the system: 1. Processes to enhance word-to-lemma mapping. (a) A spelling corrector to normalize out-ofvocabulary words. (b) Two Part-of-Speech (POS) taggers to realize word-to-lemma mapping in two perspectives. (c) A word sense disambiguator to obtain word senses and their confidence scores. 2. An weighting scheme to bias an output of a machine learner. Functionalities 1a to 1c are introduced to enhance information based on lexical knowledge, since features based on lexicons are shown to be effective in SemEval-2013 Task 2 (Mohammad et al., 2013). Functionality 2 is introduced to make the system adjustable to polarity unbalancedness known to exists in Twitter data (Nakov et al., 2013). The accompanying sections of this papers are organized as follows. Section 2 describes resources such as labeled texts and lexicons used in our system. Section 3 explains the details of the system. Section 4 discusses the submission test run and some extra test runs that we performed after the test data release. Finally, section 5 concludes the paper. 2 Resources 2.1 Sentiment Labeled Data The system is a constrained system, therefore only the sentiment</context>
<context position="4662" citStr="Mohammad et al., 2013" startWordPosition="712" endWordPosition="716">re able to obtain, and #Full is the maximum number of messages that were provided. Table 2: The seven sentiment lexicons and their criteria. organizers were used. However, due to accessibility changes in tweets, a subset of the training, the development, and the development-test data were used. Table 1 shows the numbers of messages for each type. 2.2 Sentiment Lexicons The system includes seven sentiment lexicons namely: AFINN-111 (Nielsen, 2011), Bing Liu’s Opinion Lexicon1, General Inquirer (Stone et al., 1966), MPQA Subjectivity Lexicon (Wilson et al., 2005), NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013), Sentiment140 Lexicon (Mohammad et al., 2013), and SentiWordNet (Baccianella et al., 2010). We categorized these seven lexicons to two criteria: ‘FORMAL’ and ‘INFORMAL’. Lexicons that include lemmas of erroneous words (e.g. misspelled words) were categorized to ‘INFORMAL’. Table 2 illustrates the criteria of the seven lexicons. These criteria are used in the process of word-to-lemma mapping processes and will be explained in Section 3.1.3. 3 System Details The system is a modularized system consisting of a variety of pre-processors, feature extractors, 1http://www.cs.uic.edu/˜liub/FBS/ sentim</context>
<context position="9633" citStr="Mohammad et al. (2013)" startWordPosition="1472" endWordPosition="1475">t POS taggers are used in word-to-lemma mapping as described in Section 3.1.3. clusters Words are mapped to Twitter Word Clusters of CMU ARK group7. The largest clustering result consisting of 1000 clusters from approximately 56 million tweets is used as clusters. word senses A result of the word sense disambiguator is extracted as weighted features according to their scores. Figure 2 shows an example of this feature. The ngram features are introduced as basic bagof-words features in a supervised text categorization approach. Lexicon features are designed to strengthen the lexical features of Mohammad et al. (2013) which have been shown to be effective in the last year’s task. Cluster features are implemented as an improvement for an supervised NLP system following the work of Turian et al. (2010). Word sense features are utilized to help subjectivity analysis and contextual polarity analysis (Akkaya et al., 2009). 3.3 Machine Learner Logistic Regression is utilized as an algorithm of a supervised machine learning method. As an implementation of Logistic Regression, LIBLINEAR (Fan et al., 2008) is used. A Logistic Regression is trained using the features of Section 3.2 with the three polarities (positiv</context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. NRC-Canada: Building the state-of-theart in sentiment analysis of tweets. In Proceedings of the seventh international workshop on Semantic Evaluation Exercises (SemEval-2013), pages 321– 327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Zornitsa Kozareva</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
<author>Theresa Wilson</author>
</authors>
<title>SemEval-2013 task 2: Sentiment analysis in Twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the seventh international workshop on Semantic Evaluation Exercises (SemEval-2013),</booktitle>
<pages>312--320</pages>
<contexts>
<context position="1259" citStr="Nakov et al., 2013" startWordPosition="195" endWordPosition="198">rmation. Secondly, since a distribution of sentiment on tweets is known to be unbalanced, an weighting scheme is introduced to bias an output of a machine learner. For the test run, the system was tuned towards Twitter texts and successfully achieved high scoring results on Twitter data, average F1 70.96 on Twitter2014 and average F1 56.50 on Twitter2014Sarcasm. 1 Introduction The growth of social media has brought a rising interest to make natural language technologies that work with informal texts. Sentiment analysis is one such technology, and several workshops such as SemEval-2013 Task 2 (Nakov et al., 2013), CLEF 2013 RepLab 2013 (Amig´o et al., 2013), and TASS 2013 (Villena-Rom´an and Garcia-Morera, 2013) have recently targeted tweets or cell phone messages as analysis text. This paper describes a system that has submitted a sentiment analysis result to Subtask B of SemEval-2014 Task9 (Rosenthal et al., 2014). SemEval-2014 Task9 is a rerun of SemEval-2013 Task 2 with different test data, and Subtask B is a task of message polarity classification. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers.</context>
<context position="3078" citStr="Nakov et al., 2013" startWordPosition="466" endWordPosition="469">orrector to normalize out-ofvocabulary words. (b) Two Part-of-Speech (POS) taggers to realize word-to-lemma mapping in two perspectives. (c) A word sense disambiguator to obtain word senses and their confidence scores. 2. An weighting scheme to bias an output of a machine learner. Functionalities 1a to 1c are introduced to enhance information based on lexical knowledge, since features based on lexicons are shown to be effective in SemEval-2013 Task 2 (Mohammad et al., 2013). Functionality 2 is introduced to make the system adjustable to polarity unbalancedness known to exists in Twitter data (Nakov et al., 2013). The accompanying sections of this papers are organized as follows. Section 2 describes resources such as labeled texts and lexicons used in our system. Section 3 explains the details of the system. Section 4 discusses the submission test run and some extra test runs that we performed after the test data release. Finally, section 5 concludes the paper. 2 Resources 2.1 Sentiment Labeled Data The system is a constrained system, therefore only the sentiment labeled data distributed by the task 628 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 628–632,</context>
<context position="11425" citStr="Nakov et al., 2013" startWordPosition="1743" endWordPosition="1746">013 2013 2014 Sarcasm Twitter(train)+Twitter(dev) 0.07 1.7 2.6 71.23 62.33 71.28 70.40 53.32 Twitter(dev-test)* 0.03 2.4 3.3 69.44 57.36 72.12 70.96 56.50 SMS(dev-test) 0.80 1.1 1.2 72.99 68.92 65.65 66.66 48.24 SMS(dev-test)+Twitter(dev-test) 0.07 1.9 2.0 72.54 65.44 70.41 69.80 51.09 Table 3: The scores for each source in the test runs. The run with asterisk (*) denotes the submission run. The values in the ‘Sources’ columns represent scores in SemEval-2014 Task 9 metric (the average of positive F1 and negative F1). 3.4 Prediction Adjuster Since the labels in the tweets data are unbalanced (Nakov et al., 2013), we prepared a prediction adjuster for Logistic Regression output. For each polarity l, an weighting factor wl that adjusts a probability output Pr(l) is introduced. An updated prediction label is decided by selecting an l that maximizes score(l) which can be expressed as equation 1. arg max score(l) = wlPr(l) (1) lE{pos,neg,neu} The approach we took in this prediction adjuster is a simple approach to bias an output of Logistic Regression, but may not be a typical approach to handle unbalanced data. For instance, LIBLINEAR includes the weighting option ‘-wi’ which enables a use of different c</context>
</contexts>
<marker>Nakov, Rosenthal, Kozareva, Stoyanov, Ritter, Wilson, 2013</marker>
<rawString>Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva, Veselin Stoyanov, Alan Ritter, and Theresa Wilson. 2013. SemEval-2013 task 2: Sentiment analysis in Twitter. In Proceedings of the seventh international workshop on Semantic Evaluation Exercises (SemEval-2013), pages 312–320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Finn ˚Arup Nielsen</author>
</authors>
<title>A new ANEW: Evaluation of a word list for sentiment analysis in microblogs.</title>
<date>2011</date>
<booktitle>In Proceedings of the ESWC2011 Workshop on ‘Making Sense of Microposts’: Big things come in small packages,</booktitle>
<pages>93--98</pages>
<contexts>
<context position="4490" citStr="Nielsen, 2011" startWordPosition="689" endWordPosition="690">f messages for each type. ‘train’, ‘dev’, and ‘dev-test’ denote training, development, and development-test respectively. #Used is the number of messages that we were able to obtain, and #Full is the maximum number of messages that were provided. Table 2: The seven sentiment lexicons and their criteria. organizers were used. However, due to accessibility changes in tweets, a subset of the training, the development, and the development-test data were used. Table 1 shows the numbers of messages for each type. 2.2 Sentiment Lexicons The system includes seven sentiment lexicons namely: AFINN-111 (Nielsen, 2011), Bing Liu’s Opinion Lexicon1, General Inquirer (Stone et al., 1966), MPQA Subjectivity Lexicon (Wilson et al., 2005), NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013), Sentiment140 Lexicon (Mohammad et al., 2013), and SentiWordNet (Baccianella et al., 2010). We categorized these seven lexicons to two criteria: ‘FORMAL’ and ‘INFORMAL’. Lexicons that include lemmas of erroneous words (e.g. misspelled words) were categorized to ‘INFORMAL’. Table 2 illustrates the criteria of the seven lexicons. These criteria are used in the process of word-to-lemma mapping processes and will be explained i</context>
</contexts>
<marker>Nielsen, 2011</marker>
<rawString>Finn ˚Arup Nielsen. 2011. A new ANEW: Evaluation of a word list for sentiment analysis in microblogs. In Proceedings of the ESWC2011 Workshop on ‘Making Sense of Microposts’: Big things come in small packages, pages 93–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL 2013,</booktitle>
<pages>380--390</pages>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of NAACL 2013, pages 380–390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Alan Ritter</author>
<author>Preslav Nakov</author>
<author>Veselin Stoyanov</author>
</authors>
<title>SemEval-2014 task 9: Sentiment analysis in Twitter.</title>
<date>2014</date>
<booktitle>In Proceedings of the eighth international workshop on Semantic Evaluation Exercises (SemEval-2014).</booktitle>
<contexts>
<context position="1568" citStr="Rosenthal et al., 2014" startWordPosition="244" endWordPosition="247"> on Twitter2014 and average F1 56.50 on Twitter2014Sarcasm. 1 Introduction The growth of social media has brought a rising interest to make natural language technologies that work with informal texts. Sentiment analysis is one such technology, and several workshops such as SemEval-2013 Task 2 (Nakov et al., 2013), CLEF 2013 RepLab 2013 (Amig´o et al., 2013), and TASS 2013 (Villena-Rom´an and Garcia-Morera, 2013) have recently targeted tweets or cell phone messages as analysis text. This paper describes a system that has submitted a sentiment analysis result to Subtask B of SemEval-2014 Task9 (Rosenthal et al., 2014). SemEval-2014 Task9 is a rerun of SemEval-2013 Task 2 with different test data, and Subtask B is a task of message polarity classification. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ Shigeyuki Sakaki Fuji Xerox Co., Ltd. / Japan sakaki.shigeyuki@fujixerox.co.jp Tomoko Ohkuma Fuji Xerox Co., Ltd. / Japan ohkuma.tomoko@fujixerox.co.jp The system we prepared is a sentiment analyzer based on a supervised text categorization appro</context>
</contexts>
<marker>Rosenthal, Ritter, Nakov, Stoyanov, 2014</marker>
<rawString>Sara Rosenthal, Alan Ritter, Preslav Nakov, and Veselin Stoyanov. 2014. SemEval-2014 task 9: Sentiment analysis in Twitter. In Proceedings of the eighth international workshop on Semantic Evaluation Exercises (SemEval-2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Stone</author>
<author>Dexter Dunphy</author>
<author>Marshall Smith</author>
<author>Daniel Ogilvie</author>
</authors>
<title>General Inquirer: A Computer Approach to Content Analysis.</title>
<date>1966</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4558" citStr="Stone et al., 1966" startWordPosition="697" endWordPosition="700"> training, development, and development-test respectively. #Used is the number of messages that we were able to obtain, and #Full is the maximum number of messages that were provided. Table 2: The seven sentiment lexicons and their criteria. organizers were used. However, due to accessibility changes in tweets, a subset of the training, the development, and the development-test data were used. Table 1 shows the numbers of messages for each type. 2.2 Sentiment Lexicons The system includes seven sentiment lexicons namely: AFINN-111 (Nielsen, 2011), Bing Liu’s Opinion Lexicon1, General Inquirer (Stone et al., 1966), MPQA Subjectivity Lexicon (Wilson et al., 2005), NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013), Sentiment140 Lexicon (Mohammad et al., 2013), and SentiWordNet (Baccianella et al., 2010). We categorized these seven lexicons to two criteria: ‘FORMAL’ and ‘INFORMAL’. Lexicons that include lemmas of erroneous words (e.g. misspelled words) were categorized to ‘INFORMAL’. Table 2 illustrates the criteria of the seven lexicons. These criteria are used in the process of word-to-lemma mapping processes and will be explained in Section 3.1.3. 3 System Details The system is a modularized system</context>
</contexts>
<marker>Stone, Dunphy, Smith, Ogilvie, 1966</marker>
<rawString>Philip Stone, Dexter Dunphy, Marshall Smith, and Daniel Ogilvie. 1966. General Inquirer: A Computer Approach to Content Analysis. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL 2003,</booktitle>
<pages>252--259</pages>
<contexts>
<context position="6259" citStr="Toutanova et al., 2003" startWordPosition="947" endWordPosition="950">. • URLs are exchanged with string ‘URL’s (ex. ‘http://example.org’ to ‘URL’). 3.1.2 Spelling Corrector A spelling corrector is included in the system to normalize misspellings. We used Jazzy3, an open source spell checker with US English dictionaries provided along with Jazzy. Jazzy combines DoubleMetaphone phonetic matching algorithm and a near-miss match algorithm based on Levenshtein distance to correct a misspelled word. 3.1.3 POS Taggers The system includes two POS taggers to realize word-to-lemma mapping in two perspectives. Stanford POS Tagger Stanford Log-linear Partof-Speech Tagger (Toutanova et al., 2003) is one POS tagger which is used to map words 2http://www.unicode.org/reports/tr15/ 3http://jazzy.sourceforge.net/ Criterion Lexicon General Inquirer FORMAL MPQA Subjectivity Lexicon SentiWordNet AFINN-111 Bing Liu’s Opinion Lexicon NRC Hashtag Sentiment Lexicon Sentiment140 Lexicon INFORMAL Output Input word senses Word Sense Disambiguator FORMAL lexicons Text Normalizer INFORMAL lexicons Machine Learner Spelling Corrector character ngrams Stanford POS Tagger Negation Detector Feature Extractors word ngrams Prediction Adjuster Preprocessors CMU ARK POS Tagger Negation Detector clusters 629 t</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of HLT-NAACL 2003, pages 252– 259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>384--394</pages>
<contexts>
<context position="9819" citStr="Turian et al. (2010)" startWordPosition="1505" endWordPosition="1508">g of 1000 clusters from approximately 56 million tweets is used as clusters. word senses A result of the word sense disambiguator is extracted as weighted features according to their scores. Figure 2 shows an example of this feature. The ngram features are introduced as basic bagof-words features in a supervised text categorization approach. Lexicon features are designed to strengthen the lexical features of Mohammad et al. (2013) which have been shown to be effective in the last year’s task. Cluster features are implemented as an improvement for an supervised NLP system following the work of Turian et al. (2010). Word sense features are utilized to help subjectivity analysis and contextual polarity analysis (Akkaya et al., 2009). 3.3 Machine Learner Logistic Regression is utilized as an algorithm of a supervised machine learning method. As an implementation of Logistic Regression, LIBLINEAR (Fan et al., 2008) is used. A Logistic Regression is trained using the features of Section 3.2 with the three polarities (positive, negative, and neutral) as labels. 6The total number of lexical features is 7 x 2 x 4 = 56. 7http://www.ark.cs.cmu.edu/TweetNLP/ Feature Weight 01824736-v 0.442313 01777210-v 0.355679 </context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of ACL 2010, pages 384–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julio Villena-Rom´an</author>
<author>Janine Garcfa-Morera</author>
</authors>
<title>Workshop on sentiment analysis at SEPLN 2013: An overview.</title>
<date>2013</date>
<journal>TASS</journal>
<booktitle>In Proceedings of the TASS workshop at SEPLN</booktitle>
<marker>Villena-Rom´an, Garcfa-Morera, 2013</marker>
<rawString>Julio Villena-Rom´an and Janine Garcfa-Morera. 2013. TASS 2013 - Workshop on sentiment analysis at SEPLN 2013: An overview. In Proceedings of the TASS workshop at SEPLN 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLTEMNLP</booktitle>
<pages>347--354</pages>
<contexts>
<context position="4607" citStr="Wilson et al., 2005" startWordPosition="704" endWordPosition="707">pectively. #Used is the number of messages that we were able to obtain, and #Full is the maximum number of messages that were provided. Table 2: The seven sentiment lexicons and their criteria. organizers were used. However, due to accessibility changes in tweets, a subset of the training, the development, and the development-test data were used. Table 1 shows the numbers of messages for each type. 2.2 Sentiment Lexicons The system includes seven sentiment lexicons namely: AFINN-111 (Nielsen, 2011), Bing Liu’s Opinion Lexicon1, General Inquirer (Stone et al., 1966), MPQA Subjectivity Lexicon (Wilson et al., 2005), NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013), Sentiment140 Lexicon (Mohammad et al., 2013), and SentiWordNet (Baccianella et al., 2010). We categorized these seven lexicons to two criteria: ‘FORMAL’ and ‘INFORMAL’. Lexicons that include lemmas of erroneous words (e.g. misspelled words) were categorized to ‘INFORMAL’. Table 2 illustrates the criteria of the seven lexicons. These criteria are used in the process of word-to-lemma mapping processes and will be explained in Section 3.1.3. 3 System Details The system is a modularized system consisting of a variety of pre-processors, featu</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of HLTEMNLP 2005, pages 347–354.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>