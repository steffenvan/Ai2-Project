<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006637">
<title confidence="0.9922055">
SGRank: Combining Statistical and Graphical Methods to Improve the
State of the Art in Unsupervised Keyphrase Extraction
</title>
<author confidence="0.995833">
Soheil Danesh
</author>
<affiliation confidence="0.902227">
University of Colorado
Boulder
</affiliation>
<email confidence="0.99779">
soheildb@gmail.com
</email>
<author confidence="0.98827">
Tamara Sumner
</author>
<affiliation confidence="0.8757295">
University of Colorado
Boulder
</affiliation>
<email confidence="0.615399">
tamara.sumner@
colorado.edu
</email>
<author confidence="0.986927">
James H. Martin
</author>
<affiliation confidence="0.887793">
University of Colorado
Boulder
</affiliation>
<email confidence="0.5960665">
james.martin@
colorado.edu
</email>
<sectionHeader confidence="0.990858" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999619666666667">
Keyphrase extraction is a fundamental
technique in natural language processing. It
enables documents to be mapped to a concise
set of phrases that can be used for indexing,
clustering, ontology building, auto-tagging
and other information organization schemes.
Two major families of unsupervised
keyphrase extraction algorithms may be
characterized as statistical and graph-based.
We present a hybrid statistical-graphical
algorithm that capitalizes on the heuristics of
both families of algorithms and is able to
outperform the state of the art in
unsupervised keyphrase extraction on several
datasets.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999926423076923">
Keyphrase extraction algorithms aim to extract,
from within the document phrases and words that
best represent the document‚Äôs main topics. Being
able to accurately determine what a document is
about allows computers to cluster together
documents that share topics (Hammouda et al.,
2005), better answer search queries (Qiu et al.,
2012), and generate short document summaries
(D‚ÄôAvanzo et al., 2004). Furthermore, keyphrase
extraction can be used to facilitate the automatic
construction of concept maps (Leake et al., 2003)
or ontologies (Fortuna et al., 2006) which enable
better understanding of the interconnections and
relations between different topics. Keyphrase
extraction is also used in content-based
recommender systems which help users in
discovering information relevant to their
previously expressed interests (Lops et al., 2011).
The aforementioned techniques are all important
tools in the organization and understanding of the
ever expanding repositories of textual information
available online in the form of research papers,
news articles, blog posts, etc. and keyphrase
extraction is central to all of them. Therefore it
could be said that keyphrase extraction is a
fundamental NLP task, improvements in which
could cascade into improvements in higher-level
applications that build upon it.
In this work we have focused on unsupervised
keyphrase extraction approaches as not only they
are useful in domains where training data is hard to
procure but even in the presence of ample training
data word weights calculated using unsupervised
methods can be used as one of several features in
supervised keyphrase extraction algorithms.
Therefore increases in the accuracy of
unsupervised methods can propagate into the
results of supervised algorithms as well.
There are two prominent families of
unsupervised keyphrase extraction algorithms. The
older of these two is clustered around the tf-idf
term weighting metric where word statistics such
as frequency of occurrence in the document or
rareness in the corpus are used to distinguish
potential keyphrases. The more recently developed
of the two families has been built on the
foundation of the TextRank algorithm (Mihalcea &amp;
Tarau, 2004). In algorithms of this family a
graphical representation of the text is constructed
with words as nodes and edges reflecting co-
occurrence relations. This graph is then used to run
node ranking algorithms such as PageRank (Page
</bodyText>
<page confidence="0.968834">
117
</page>
<note confidence="0.9532935">
Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 117‚Äì126,
Denver, Colorado, June 4‚Äì5, 2015.
</note>
<bodyText confidence="0.99975975">
et al., 1999) that assign weights to the node-words
reflecting their semantic importance to the text.
Although some overlap between these two
families of algorithms has occurred in works that
incorporate statistical heuristics into graph-based
methods this overlap is small and most methods do
not utilize the full set of statistical heuristics. Our
aim has been to 1) Construct a keyphrase
extraction algorithm based on optimal statistical
features and 2) Combine it with a graph-based
algorithm for further improvements. The advantage
of graph-based methods is that they take into
account term co-occurrence patterns that are not
generally utilized by statistical methods which take
a bag of n-grams approach to document
representation.
</bodyText>
<sectionHeader confidence="0.99954" genericHeader="introduction">
2 Related Works
</sectionHeader>
<bodyText confidence="0.99989837037037">
In this section we focus mainly on related
unsupervised keyphrase extraction algorithms. One
of the most prominent of these algorithms has been
the term frequency-inverse document frequency
(tf-idf) term weighting function (Salton et al,
1975). Given a corpus of documents the tf-idf
weight of term t in document d is mathematically
expressed as tf-idf(t,d)=tf(t,d)*idf(t) where tf(t,d) is
the frequency of term t in document d and
idf(t)=log(N/df(t)) where N is the total number of
documents in the corpus and df(t) is the number of
documents in the corpus that contain term t (Jones,
1972). The term frequency heuristic is based on the
intuition that terms which occur more often in a
document are more likely to be important to its
meaning. The idf function captures the rareness
heuristic, that is, words which occur in many
documents in the corpus are unlikely to be
important to the meaning of any specific one.
Tf-idf is simple yet relatively accurate therefore
many variations of it have been used by other
algorithms. One of the most successful of these is
KP-Miner (El-Beltagy &amp; Rafea , 2010) which to
the best of our knowledge represents the state of
the art in unsupervised keyphrase extraction. KP-
Miner operates on n-grams and uses a modified
version of tf-idf where the document frequency for
n-grams with n greater than one is assumed to be
one. We will explain the intuition behind this
modification later as we have adopted it in our
algorithm as well. KP-Miner‚Äôs initial candidates
are comprised of the longest n-grams that do not
contain a stop word or punctuation mark, occur for
the first time within the first 400 words of the
document and have a term frequency above a
minimum threshold determined by document
length. KP-Miner also boosts the weights of multi-
word candidates in proportion to the ratio of the
frequencies of single word candidates to all
candidates. In a reranking step, the tf-idf of each
term is recalculated based on the number of times
it is subsumed by other candidates in the top 15
candidates list. Another tf-idf based unsupervised
system is KX-FBK (Pianta &amp; Tonelli, 2010) which
uses some of the same heuristics as KP-Miner but
with different formulations and was shown to
underperform in comparison in the Semeval 2010
keyphrase extraction task.
An approach fundamentally different from tf-idf
and its family of algorithms is TextRank. It is
based on the intuition that 1) keywords in a
document are more semantically interrelated as
they are generally about related topics and 2) that
semantic relatedness can be estimated using co-
occurrence relations. Therefore in TextRank a
graphical representation of the text is constructed
in which edges connect words co-occurring in a
window of a certain length. The PageRank
algorithm is then applied to this network of words
to distinguish the important ones which are then
reassembled into phrases wherever they occur next
to each other in the text.
TopicRank (Bougouin et al., 2013) which to the
best of our knowledge is the state of the art in
graph-based keyphrase extraction, is an
enhancement of TextRank. Here, nodes represent
topics which consist of sets of candidate terms
clustered around shared sub-terms. In (Liang et al.,
2009) Chinese search engine query logs are used to
extract candidate terms which are used as nodes in
the graph. Edges are weighted based on co-
occurrence count. Also candidate terms which are
longer or whose first occurrence is in the title or
first paragraph have boosted edge weights.
SingleRank (Wan &amp; Xiao, 2008) also uses co-
occurrence counts as edge weights. It ranks noun
phrases in the text based on the sum of their word
weights. ExpandRank (Wan &amp; Xiao, 2008) builds
upon SingleRank by incorporating neighboring
documents but without significant performance
improvements (Hasan &amp; NG, 2010).
</bodyText>
<page confidence="0.998652">
118
</page>
<sectionHeader confidence="0.991732" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.999982285714286">
Our algorithm processes an input document in four
stages. In the first stage we extract all possible n-
grams from the input text and eliminate those that
are highly unlikely to be keyphrases, for instance
n-grams containing punctuation marks. In the
second stage the remaining n-grams are ranked
based on a modified version of tf-idf. In the third
stage the top ranking candidates from stage two are
reranked based on additional statistical heuristics
such as position of first occurrence and term
length. In the fourth and final stage the ranking
produced in stage three is incorporated into a
graph-based algorithm which produces the final
ranking of keyphrase candidates.
</bodyText>
<subsectionHeader confidence="0.99959">
3.1 Eliminating Unlikely Candidates
</subsectionHeader>
<bodyText confidence="0.999996875">
In the first stage all possible n-grams in the text for
n from 1 to 6 are produced. Those n-grams
considered highly unlikely to be keyphrases are
eliminated from the candidates list. These include
n-grams containing stop words, punctuation marks
or words whose part of speech tag is anything
different than noun, adjective or verb. Furthermore
n-grams whose frequency of occurrence in the text
falls below a minimum threshold are also
eliminated. In the current work this threshold is
determined based on document length and is 0 for
short documents, 2 for medium-length documents
and 3 for long documents where short is defined as
containing less than 1500 words, medium as
between 1500 to 4000 and long as any document
with more than 4000 words.
</bodyText>
<subsectionHeader confidence="0.999216">
3.2 Initial Ranking of All Candidates
</subsectionHeader>
<bodyText confidence="0.999970535714286">
In the second stage n-grams not eliminated in the
first stage are ranked based on a modified version
of tf-idf as used in KP-Miner. The modification
involves changing the document frequency count
in idf calculation such that for n-grams with n &gt; 1
document frequency is always considered to be
one. In other words we assume that all multi-word
candidates occur in one document only. This is
because while rareness is a reliable indication of
semantic importance in the case of single words, it
does not offer the same accuracy when it comes to
multi-words. In many cases relatively common
single words can combine into rare multi-words
without much semantic importance. For example,
in the Semeval test dataset of 100 full-length
academic papers, to be described later in the
evaluation section, the n-grams control has, rule
satisfies and become known all have a document
frequency of 1. On the other hand phrases chosen
by humans as keyphrases such as Query expansion
which occurs in 9 documents and as a keyphrase in
4 or language models which occurs in 12
document and again in 4 of them as key, have
relatively high document frequency counts. These
examples demonstrate how including the actual
document frequency counts in idf calculation could
be disadvantageous for distinguishing multi-word
keyphrases.
</bodyText>
<subsectionHeader confidence="0.999657">
3.3 Reranking Top Candidates
</subsectionHeader>
<bodyText confidence="0.979720558823529">
At the end of stage two we have an initial
ranking of our candidates based on their tf-idf
scores. In the third stage we rerank the top T
candidates from stage two based on additional
heuristics. These heuristics are position of first
occurrence, term length and subsumption count. In
the current work T is set to 100 based on
experiments on a small development set of 40
documents from the Semeval trial set.
The position of first occurrence heuristic has
performed consistently well in previous keyphrase
extraction experiments. Medelyan and Witten
(2008) use a linear decay function of the position
of first occurrence as a feature in their supervised
algorithm. It has also been utilized in unsupervised
methods. In KP-Miner a constant position
threshold is used where n-grams whose first
occurrence is beyond it are eliminated from the
candidate list. KX-FBK uses the linear decay
function raised to the power of two. We introduce
a novel encoding of this heuristic in the form of a
logarithmic decay function, which as we will show
in the discussion section outperforms all
aforementioned variations. We define the Position
of First Occurrence factor (PFO) according to the
following formula:
ùëÉùêπùëÇ ùë° d = log cuto f fPosition 1
&apos; g ùëùùë°, ùëë ()
where p(t,d) is the position of term t√ïs first
occurrence in document d. In the current work
cutoffPosition is set to 3000 as it performed best in
experiments on the development set.
Regarding term length, we hypothesize that
among words with a high likelihood of being a
</bodyText>
<page confidence="0.99503">
119
</page>
<bodyText confidence="0.9999856">
keyphrase, in this case the top 100 candidates from
stage two, the addition of a word to an n-gram is
likely to construct a more semantically specific
phrase geared towards signifying a specific topic
or subject e.g. web versus semantic web. Therefore
longer n-grams are generally more likely to be
keyphrases. Accordingly, we boost term t‚Äôs weight
by its term length, TL(t) where length is the
number of space separated words in t.
Finally, we recalculate a term‚Äôs tf-idf weight by
reducing its term frequency by its subsumption
count among the top 100 candidates. Term t is said
to be subsumed by term ts when ts contains t.
The following formula shows how the statistical
weight for term t in document d is calculated:
</bodyText>
<equation confidence="0.999476666666667">
ùë§s ùë°, ùëë =
ùë°ùëì ùë°, ùëë ‚àí subSumCount ùë°, ùëë ‚àó ùëñùëëùëì ùë° ‚àó
ùëÉùêπùëÇ ùë°, ùëë ‚àó ùëáùêø ùë° (2)
</equation>
<bodyText confidence="0.956517">
Where subSumCount(t,d) is the sum of term
frequencies of all terms included in the top T list
that subsume t.
</bodyText>
<subsectionHeader confidence="0.960831">
3.4 Graph-based Ranking
</subsectionHeader>
<bodyText confidence="0.999934818181818">
In the fourth and final stage of our algorithm we
use terms with positive weights after stage three as
nodes in a graphical representation of the text. An
edge is placed between two nodes if they co-occur
within a window of width d. Whereas d is usually
small, generally less than 20 words in most graph-
based algorithms, we have chosen a large window
of 1500 and instead attenuate the edge weight
based on the average log decayed distance between
all co-occurrences of the term pair as show in
equation 3 below
</bodyText>
<equation confidence="0.936421333333333">
tf ti tf tj to winSi%e
L=1 j=1 P
numCo‚Äîoccurrences(ti,tj) (3)
</equation>
<bodyText confidence="0.999968153846154">
where winSize is the co-occurrence window size
set to 1500 in the current work based on F-measure
performance on the development set, posi and posj
are the respective positions of occurrences of terms
ti and tj and numCo-occurrences(ti,tj) is the number
of co-occurrences of the terms within the window
of 1500.
Furthermore, we incorporate term weights
calculated using statistical features in the previous
stages into the graphical representation of the text.
We hypothesize that term weights calculated using
statistical features may serve as a first estimate of a
term‚Äôs keyphraseness, i.e. likelihood of being a
keyphrase. The PageRank algorithm simulates a
random walker on the graph. Each node‚Äôs eventual
PageRank score reflects the portion of time the
walker spends on that node (Langville &amp; Meyer,
2011). To make sure terms with higher statistical
weights are visited more often we would want
higher transition probabilities between them but
lower transition probabilities between terms with
lower weights. Therefore we use the product of the
term pair‚Äôs weights as a factor into the weight of
the edge between them in the graph. The weight of
the edge between terms ti and tj is calculated using
the following equation:
</bodyText>
<equation confidence="0.960116">
1
ùë§e ùë°i, ùë°j = ùë§d ùë°i, ùë°j ‚àó ùë§s ùë°i ‚àó ùë§s (ùë°j) (4)
</equation>
<bodyText confidence="0.9997446">
where, as previously defined in equation 3, wd is
the distance based portion of the edge weight while
ws(ti)*ws(tj) takes the terms‚Äô statistical properties
into account.
For each node we normalize edge weights by
dividing each outgoing edge weight by sum of
outgoing edge weights for that node. This results in
a slightly modified formula for PageRank
compared to the one used in TextRank, where
edges are uniform weight, as shown below.
</bodyText>
<equation confidence="0.996075333333333">
We 1,1 ‚àóS(vi)
ùëÜ ùëâi = 1 ‚àí ùëë + ùëë ‚àó (5)
jeln(ÔøΩÔøΩ) keout(ÔøΩÔøΩ)ÔøΩÔøΩ ÔøΩ,ÔøΩ
</equation>
<bodyText confidence="0.9999642">
where S(Vi) is the PageRank score of node Vi, d is
the damping factor usually set to 0.15 and In(Vi)
and Out(Vi) are the sets of edges where node Vi is
the destination or the source respectively.
PageRank consists of iteratively calculating the
scores for each node until convergence where
scores do not change significantly between
iterations. The converged-on PageRank score for
each node is our algorithm‚Äôs final output and
determines the rankings of the candidate terms.
</bodyText>
<sectionHeader confidence="0.999189" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.99998125">
We evaluate our keyphrase extraction algorithm by
comparing it to two state-of-the-art algorithms,
KP-Miner and TextRank, on three datasets: The
Semeval 2010 keyphrase extraction shared task
dataset, the Inspec dataset of ACM abstracts and
the Krapivin dataset of full length papers. To
obtain results for KP-Miner we have used an
executable kindly shared with us by the system‚Äôs
author. For TextRank we have built on an existing
open source implementation. The comparisons
between the algorithms are done using the
precision and recall at k metric where the top k
</bodyText>
<equation confidence="0.912662">
ùë§d ùë°i, ùë°j =
</equation>
<page confidence="0.930248">
120
</page>
<bodyText confidence="0.999961076923077">
terms returned by each algorithm are used to
measure precision and recall. Here k ranges from 1
to 15. We also calculate the F-measure for k = 5,
10 and 15. In the following section we describe
each dataset in detail and report the results
achieved by each algorithm on each dataset.
The Semeval and Inspec datasets have also been
used by Bougouin et al. (2013) for evaluating their
implementation of TextRank along with more
advanced graph-based algorithms SingleRank and
TopicRank. We have used these results for further
comparisons between our method and advanced
graph-based algorithms as reported in section 4.4.
</bodyText>
<subsectionHeader confidence="0.981329">
4.1 Semeval Dataset
</subsectionHeader>
<bodyText confidence="0.999789473684211">
The Semeval dataset was used in the Semeval 2010
keyphrase extraction shared task (Kim et al.,
2010). To the best of our knowledge this shared
task is the largest recent comparison of keyphrase
extraction algorithms and an algorithm‚Äôs
performance on this dataset is a relatively good
indication of where it stands compared to others in
the field. The Semeval dataset consists of 284 full
length ACM articles divided into a test set of size
100, training set of size 144 and trial set of size 40
which we used as the development set for
parameter tuning. Each article has two sets of
human assigned keyphrases: the author-assigned
and reader-assigned ones. The gold standard used
in our experiments is the combined set of author
and reader assigned keyphrases which is the same
as was done in the Semeval shared task. The table
below provides a statistical overview of this
dataset‚Äôs documents.
</bodyText>
<table confidence="0.999924">
100 Document Number of Keyphrase
docs Length Keyphrases Length
Max. 14171 29 8
Avg. 7979 15.13 2.14
Min. 4060 9 1
</table>
<tableCaption confidence="0.999987">
Table 1. Semeval test set statistics.
</tableCaption>
<bodyText confidence="0.999320625">
We have compared our algorithm with KP-
Miner and TextRank using only the 100 documents
in the test set. The following diagram shows the
average precision and recall achieved by each
algorithm. As was done in the Semeval task,
comparisons are done between once stemmed
human assigned keyphrases and ranked candidates
returned by each algorithm.
</bodyText>
<figureCaption confidence="0.999175">
Figure 1. Semeval precision(y), recall(x) k &lt; 16
</figureCaption>
<bodyText confidence="0.992395142857143">
The following table shows the achieved F-
measure for each algorithm at k=5, 10 and 15. It
also contains the corresponding percentage
improvement at each k. The statistical significance
of each improvement is measured using a 2-sided
paired t-test. Improvements are in bold font where
they are statistically significant at p &lt; 0.05.
</bodyText>
<table confidence="0.978071">
K = 5 10 15
SGRank 20.25 26.07 27.20
KPMiner 19.01 24.06 25.54
Improvement 6.5% 8.3% 6.4%
Textrank 1.25 2.46 3.47
Improvement 1509% 960% 683%
</table>
<tableCaption confidence="0.999164">
Table 2. Semeval F-measures and improvements.
</tableCaption>
<bodyText confidence="0.999535642857143">
As can be seen from the above results our
method outperforms KP-Miner in both precision
and recall for all k and achieves statistically
significant improvements in the F-measure over
KP-Miner for k=10 and 15. These results are
noteworthy considering that in the Semeval
keyphrase extraction shared task KP-Miner was the
best performing unsupervised algorithm, and the
second best overall out of 19 systems,
outperforming prominent supervised algorithms
such as Maui (Medelyan et al., 2009). TextRank
seems to generally underperform on longer
documents and has performed poorly on the
Semeval dataset.
</bodyText>
<subsectionHeader confidence="0.909021">
4.2 Inspec Dataset
</subsectionHeader>
<bodyText confidence="0.99998875">
The Inspec dataset is comprised of 2000 ACM
abstracts divided into test, training and validation
sets containing 500, 1000 and 500 abstracts
respectively. We follow the same approach as
</bodyText>
<page confidence="0.99623">
121
</page>
<bodyText confidence="0.99419375">
taken by Mihalcea and Tarau (2004). We use only
the 500 documents in the test set. The following
table provides a statistical overview of this
document set.
</bodyText>
<table confidence="0.9991246">
500 Document Number of Keyphrase
docs Length Keyphrases Length
Max. 338 31 9
Avg. 121.8 9.8 2.3
Min. 23 2 1
</table>
<tableCaption confidence="0.999623">
Table 3. Inspec dataset statistics.
</tableCaption>
<bodyText confidence="0.996493333333333">
Figure 2 shows the average precision and recall
for all three algorithms for k from 1 to 15. Table 4
shows the F-measure improvements made by our
method over the two other algorithms for k=5, 10
and 15. As these results show, on this dataset of
relatively short documents, TextRank outperforms
KP-Miner for k&gt;2. Our algorithm achieves higher
precision and recall than both KP-Miner and
TextRank for all k with statistically significant
gains in the F-measure for k=5, 10 and 15.
assigned and editor-corrected keyphrases that we
use as the gold standard in our evaluation. Our
experiments are done on a 400-document subset of
this dataset. The table below provides a statistical
characterization of these 400 documents.
</bodyText>
<table confidence="0.9991884">
400 Document Number of Keyphrase
docs Length Keyphrases Length
Max. 16721 24 6
Avg. 7934 6.38 2.1
Min. 3892 1 1
</table>
<tableCaption confidence="0.999773">
Table 5. Krapivin dataset statistics.
</tableCaption>
<bodyText confidence="0.999840230769231">
On this dataset keyphrases and candidate terms
have been stemmed once before comparison.
Similar to the previously mentioned experiments
we have measured the precision and recall of all
three algorithms for k from 1 to 15 as shown in
figure 3. Table 6 contains the F-measures for all
three algorithms at k=5, 10 and 15 along with the
improvements made by our algorithm. Similar to
the Semeval dataset TextRank performs very
poorly on this dataset of longer documents. KP-
Miner performs much better but both methods are
outperformed by our method on all k with
statistical significance as shown in Table 6.
</bodyText>
<figureCaption confidence="0.998787">
Figure 2. Inspec precision(y), recall(x) k &lt; 16
</figureCaption>
<table confidence="0.993451666666667">
K = 5 10 15
SGRank 29.16 33.95 33.66
KPMiner 18.45 15.89 12.73
Improvement 59.7% 118% 175%
TextRank 25.53 30.6 29.7
Improvement 15.4% 13.3% 17.7%
</table>
<tableCaption confidence="0.9992">
Table 4. Inspec F-measures and improvements.
</tableCaption>
<subsectionHeader confidence="0.994057">
4.3 Krapivin Dataset
</subsectionHeader>
<bodyText confidence="0.996009666666667">
The Krapivin dataset consists of 2000 full length
ACM papers. This dataset has been prepared by
Krapivin et al. (2009). Each article has author-
</bodyText>
<figureCaption confidence="0.995728">
Figure 3. Krapivin precision(y), recall(x) k &lt; 16
</figureCaption>
<table confidence="0.994075166666667">
K = 5 10 15
SGRank 21.2 21.6 19.4
KPMiner 18.43 18.65 17.4
Improvement 15.% 16.1% 11.7%
Textrank 1.02 1.61 2.1
Improvement 1974% 1240% 823%
</table>
<tableCaption confidence="0.949911">
Table 6. Krapivin F-measures and improvements.
</tableCaption>
<page confidence="0.995031">
122
</page>
<subsectionHeader confidence="0.996663">
4.4 Advanced Graph-based Methods
</subsectionHeader>
<bodyText confidence="0.999892384615385">
As mentioned previously Bougouin et al. (2013)
introduce TopicRank and use F-measure at k=10 to
compare against TextRank and another advanced
graph-ranking method SingleRank. They use the
Semeval and Inspec datasets for comparison
providing us with an opportunity to compare our
performance with those of TextRank and the two
more advanced graph-based algorithms. Table 7
contains the F-measures at k=10 for our algorithm,
SGRank, and all aforementioned algorithms. Note
that the particular implementation of TextRank
used in this paper performs worse than ours on the
Inspec dataset but better for the Semeval dataset.
</bodyText>
<table confidence="0.9993412">
F at k=10 Inspec Semeval Average
SGRank 33.95 26.4 30.1
TextRank 12.7 5.6 9.1
SingleRank 35.2 3.7 19.4
TopicRank 27.9 12.1 20
</table>
<tableCaption confidence="0.9960355">
Table 7. Comparison with Advanced Graph-based
methods F-measures at K=10.
</tableCaption>
<bodyText confidence="0.998595333333333">
As seen in Table 7 our algorithm‚Äôs average
performance is considerably better than all of the
advanced graph-based algorithms.
</bodyText>
<sectionHeader confidence="0.999179" genericHeader="evaluation">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999983847222223">
As shown in the preceding results our algorithm
outperforms all other methods in all the used
datasets. The only exception is SingleRank which
marginally outperforms our method on the Inspec
dataset but performs very poorly on the Semeval
dataset, as seen in Table 7. Also worth noting is
KP-Miner‚Äôs relatively poor performance on the
shorter documents of the Inspec dataset. This could
potentially be due to the fact that KP-Miner only
considers terms as candidates which occur on their
own in the text i.e. surrounded by punctuation
marks or stop words. In shorter documents it is
more likely that fewer keyphrases would occur in
such conditions in the text, causing them to be
eliminated early on by KP-Miner. Our algorithm
however considers all n-grams without requiring
that they occur on their own. This allows us to
consider more candidates and avoid a performance
reduction in shorter documents. However, there is
an advantage to eliminating terms that never occur
on their own. Many keyphrases are multi-words. In
some cases smaller parts of keyphrases tend to
occur in high frequencies, as they are related to the
topic of the document and are sometimes used in
place of the keyphrase, and therefore achieve high
rankings. We call such frequent sub-phrases
keyphrase fragments. For example document C-1
in the Semeval test set includes two keyphrases
grid service discovery and web service leading to a
highly ranked keyphrase fragment service. High
ranking keyphrase fragments are detrimental to the
algorithm‚Äôs performance. One way to counteract
them is based on the observation that they rarely
occur on their own as they usually appear as part of
larger phrases. This is the motivation behind KP-
Miner‚Äôs elimination of candidates that do not occur
on their own. Therefore, to consider all candidates,
while countering the keyphrase fragments problem,
we calculate the subsumption count over a much
larger portion of the ranked terms compared to KP-
Miner. This larger list will include more terms
which keyphrase fragments are a part of, causing
greater reductions in the fragments‚Äô rankings. The
number of top candidates used in KP-Miner to
calculated subsumption is set equal to an input
parameter that determines the number of
keyphrases to be returned to the user. In the current
work and the Semeval shared task this parameter is
15. In other words KP-Miner calculates the
subsumption count over the top 15 terms whereas
we calculate it over the top 100 terms. To test the
effectiveness of this strategy we reduced our
subsumption threshold to 15. This change led to a
9% decrease in the F-Measure at k=15 on the
Semeval dataset, 4.7% decrease on the Inspec and
1.5% decrease in the Krapivin dataset. Note that
for the rest of the Discussion section percentage
changes are those of the F-measure at k=15. Our
algorithm‚Äôs high performance on both short and
long documents indicates the viability of
considering all n-grams as candidates and
mitigating the effect of keyphrase fragments by
counting subsumption over more top ranking
terms.
Another novel aspect of our algorithm is its
formulation of the position of first occurrence
heuristic as described by the PFO function in
equation 1. We compare our approach with two
other unsupervised algorithms that utilize this
heuristic: KP-Miner and KX-FBK. The method
used in KP-Miner is a hard cutoff threshold where
candidates whose first occurrence is beyond 400
</bodyText>
<page confidence="0.997598">
123
</page>
<bodyText confidence="0.999927535353535">
words into the document are eliminated. KX-FBK
uses the following decay function:
We tested our system with our PFO function
replaced by those of KP-Miner and KX-FBK on
the Semeval, Inspec and Krapivin datasets.
Replacement with KX-FBK‚Äôs PFO led to
respective reductions in the F-Measure at k=15 of
6.5%, 9.5% and 15.1%. Replacement with KP-
Miner‚Äôs PFO led to a 10.9% reduction in Semeval
but no reduction in Inspeq and a 2.9%
improvement in Krapivin. We also replaced our
PFO function with the linear decay function used
by Medelyan and Witten (2008). This function is
the same as equation 9 but without the exponent.
This led to a 1.4% reduction in Semeval, 0.7%
reduction in Inspec and a 2.9% reduction in
Krapivin. These results show that our encoding of
the PFO heuristic as a logarithmic decay function
leads to overall gains in accuracy although it
underperforms slightly compared to KP-Miner‚Äôs
PFO on the Krapivin dataset which points to
further room for improvement. One possible future
direction would be to design functions that adjust
the cutoffPosition in equation 1 based on document
length as some sensitivity to this was observed in
our experimentations. We also replaced our term
length factor, TL in equation 2, with KP-miner‚Äôs
boosting function for multi-words. This caused a
3.2% reduction in Semeval, 5.1% reduction in
Inspec and 2.3% reduction in Krapivin.
Our algorithm uses graph-based methods on top
of statistical features to capture keyphrases not
distinguishable using statistical heuristics. To test
the effectiveness of this addition we eliminated the
graph-based reranking stage. This caused a 1.1%
reduction in Semeval, 4% reduction in Inspec and
a 4.3% reduction in Krapivin which demonstrates
that our approach of combining statistical and
graph-based features leads to overall
improvements in performance. Our method also
introduces a novel distance-based edge weighting
formula to the graph-based family of algorithms.
Most graph-based algorithms place edges where
terms co-occur within a window of a few words.
This is equivalent to a sudden drop in the
estimation of semantic relatedness at the edge of
the window. We however choose a much larger
window of 1500 and gradually reduce the edge
weight with increasing distance between the terms
according to equation 3. To measure the
effectiveness of this approach we compared it with
a window of 100 words with no positional decay,
i.e. wd in equation 4 is set to one for terms
occurring within the window of 100 and zero
otherwise. This caused a 2.2% drop in the Semeval
dataset i.e. it performed lower than with no
graphical reranking at all. In Krapivin it caused a
2.4% drop in performance and a 0.4% drop in the
Inspec dataset. For further comparison we replaced
wd with the dist function used in TopicRank which
is the sum of inverse distances between all
occurrences of a term pair. This caused a 1.4%
reduction in Semeval, no difference in Krapivin
and a 0.3% reduction in Inspec. These results
demonstrate the effectiveness of our novel distance
based edge weighting function.
An interesting point is that both our positional
functions, PFO and wd are logarithmic decays. This
hints at a logarithmic decrease in semantic
importance or relatedness with increased distance
which is the same as how the idf function relates a
word‚Äôs semantic importance to its document
frequency. Our initial hypothesis for the success of
logarithmic decay functions is that both positional
and document frequency heuristics are governed
by the law of diminishing returns. That is, the
distinguishing power of each heuristic decreases as
the inputs increase. Taking the document
frequency heuristic (df) as an example, we know
that rareness, i.e. small dfs, indicate higher
semantic importance. Therefore, in a corpus of
1000 documents, a word with a df of 1 is much
more likely to be a keyphrase than a word with a df
of 20, as reflected in their idf scores. However as
the df increases the same difference in df‚Äôs does
not imply the same difference in probability of
being a keyphrase e.g. we intuitively know that
based on rareness alone, our estimate of the
difference in the probability of being a keyphrase
for a pair of terms with df‚Äôs 980 and 1000 would
be much less reliable compared to a pair with df‚Äôs
1 and 20, even though the difference in the df pairs
are the same. The same logic applies to the
position of first occurrence and distance based
semantic relatedness heuristics. Incorporating this
diminishing returns property into the mathematical
formulation of the heuristic calls for a function
with a decreasing absolute value slope i.e. with a
second derivative with the opposite sign of the
</bodyText>
<figure confidence="0.90616375">
documùëíùëõùë° lengt‚Ñé ‚àí ùëù ùë°, ùëë
document lengt‚Ñé
6
ùëÉùêπùëÇB ùë°, ùëë =
</figure>
<page confidence="0.99293">
124
</page>
<bodyText confidence="0.999462277777778">
first, to reflect our decreasing confidence in the
heuristic as values increase. This rules out linear
functions. Although other functions such as
reciprocals fulfill this property, judging based on
the success of idf and our positional decay
functions it seems that logarithmic decay does
better at modeling the intrinsic rate of this
diminishing distinguishing power of the heuristic,
perhaps due to its slower decline. Why this is and
whether better decay functions can be designed or
tuned to specific domains is a future direction we
plan to explore. It is also worth noting that unlike
most graph-based algorithms whose performances
are completely dependent on a POS tag filter,
SGRank suffers relatively slight reductions in F-
measure at 15 without the POS tag filter: 3.6% on
Semeval, and 2.6% in Krapivin and 24.5% on
Inspec.
</bodyText>
<table confidence="0.99958356">
Feature Datasets Datasets S. I. K.
Tested Average Average
F at 15 Change
None 26.7 0 27.2 33.6 19.4
Subsume 25.3 -5.6% 24.7 32 16.4
Top 15
kx-fbk 24.1 -10.3% 25.4 30.4 16.4
PFO
kp-miner 25.95 -2.6% 24.2 33.6 19.9
PFO
kp-miner 25.74 -3.5% 26.3 31.9 18.9
multi-
word
boosting
edge 26.3 -1.6% 26.6 33.5 18.9
weight
=1 if
d&lt;100
topicRa- 26.5 -0.56% 26.8 31.9 18.9
nk edge
weighting
no POS- 23.5 -10.2% 26.2 25.4 18.9
tagging
no graph- 25.9 -3.1% 26.9 32.2 18.5
based
</table>
<tableCaption confidence="0.947564">
Table 8. Effects of individual features on performance.
Columns S., I. and K. contain F-measures (k=15) for the
Semeval, Inspec and Krapivin datasets respectively.
</tableCaption>
<bodyText confidence="0.954558571428571">
Table 8 contains a summary of how the
elimination or replacement of different features
affects the performance of our algorithm, as
discussed previously. It contains the F-measure at
k = 15, averaged across all datasets, for the full
algorithm along with variations of it produced by
changing different features.
</bodyText>
<sectionHeader confidence="0.988888" genericHeader="conclusions">
6 Conclusion and Future Directions
</sectionHeader>
<bodyText confidence="0.9999936">
We introduce an unsupervised keyphrase
extraction algorithm that combines statistical and
graph-based heuristics and is able to improve upon
the state of the art, with statistical significance, on
several datasets. Among other features, our
algorithm uses a novel variation of the
subsumption heuristic. We also demonstrate the
suitability of log decay functions for
mathematically expressing heuristics that are based
on phrase distance such as the position of first
occurrence and the weighting of graph edges based
on the average distance of phrase occurrences.
Another way of looking at the presented algorithm
is as a term weighting scheme. Therefore an
interesting future direction would be to investigate
whether replacing traditional term weighting
schemes, e.g. tf-idf, in areas such as information
retrieval, document clustering and supervised
algorithms where tf-idf is used as a feature would
cause any improvements in performance.
</bodyText>
<sectionHeader confidence="0.999563" genericHeader="references">
7 References
</sectionHeader>
<reference confidence="0.998341476190476">
Bougouin A., F. Boudin, and B. Daille. 2013, October.
Topicrank: Graph-based topic ranking for keyphrase
extraction. In International Joint Conference on
Natural Language Processing (IJCNLP), pages 543-
551.
D‚ÄôAvanzo F., B. Magnini, and A.Vallin. 2004.
Keyphrase extraction for summarization purposes:
The LAKE system at DUC-2004. In Proceedings of
the 2004 document understanding conference.
El-Beltagy S. and A. Rafea. 2009. KP-Miner: A
keyphrase extraction system for English and Arabic
documents. In Information Systems, 34(1), pages
132-144.
Samhaa R. El-Beltagy and Ahmed Rafea. 2010. Kp-
miner: Participation in semeval-2. In Proceedings of
the 5th international workshop on semantic
evaluation, pages 190-193.
Fortuna B., M. Grobelnik, and D. Mladeniƒá. 2006.
Semi-automatic data-driven ontology construction
system. In Proceedings of the 9th international multi-
conference information society, pages 223‚Äì226.
</reference>
<page confidence="0.981443">
125
</page>
<reference confidence="0.99992498630137">
Hammouda K., D. Matute, and M. Kamel. 2005.
Corephrase: Keyphrase extraction for document
clustering. In Machine Learning and Data Mining in
Pattern Recognition, pages 265-274.
Hasan, K. S., &amp; Ng, V. 2010. Conundrums in
unsupervised keyphrase extraction: making sense of
the state-of-the-art. In Proceedings of the 23rd
International Conference on Computational
Linguistics: Posters (pp. 365-373). Association for
Computational Linguistics.
Hulth A. 2003. Improved automatic keyword extraction
given more linguistic knowledge. In Proceedings of
the 2003 conference on Empirical methods in natural
language processing, pages 216-223.
Jones K.. 1972. A statistical interpretation of term
specificity and its application in retrieval. Journal of
documentation, 28(1), Pages 11-21.
Kim S., O. Medelyan, M. Kan, and T. Baldwin. 2010.
Semeval-2010 task 5: Automatic keyphrase
extraction from scientific articles. In Proceedings of
the 5th International Workshop on Semantic
Evaluation , pages 21-26.
Krapivin M., A. Autaeu &amp; M. Marchese. 2009. Large
dataset for keyphrases extraction. Technical Report
DISI-09-055.
Langville, A. N., &amp; Meyer, C. D. 2011. Google&apos;s
PageRank and beyond: the science of search engine
rankings. Princeton University Press.
Leake D., A. Maguitman, and T. Reichherzer. 2003.
Topic Extraction and Extension to Support Concept
Mapping. In FLAIRS Conference, pages 325-329.
Liang, W., Huang, C., Li, M., &amp; Lu, B. L. 2009.
Extracting Keyphrases from Chinese News Articles
Using TextRank and Query Log Knowledge.
In PACLIC
Lops P., M. Gemmis, and G. Semeraro. 2011. Content-
based recommender systems: State of the art and
trends. In Recommender systems handbook, pages
73-105.
Medelyan, O., &amp; Witten, I. H. 2008. Domain
Independent Automatic Keyphrase Indexing with
Small Training Sets. Journal of the American Society
for Information Science and Technology, 59(7),
1026-1040.
Medelyan, O., Frank, E., &amp; Witten, I. H. (2009).
Human-competitive tagging using automatic
keyphrase extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural
Language Processing: Volume 3-Volume 3 (pp.
1318-1327). Association for Computational
Linguistics.
Mihalcea R. and P. Tarau. 2004. Textrank: Bringing
order into texts. In Proceedings of International
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 404‚Äì411
Page, L., Brin, S., Motwani, R., &amp; Winograd, T. 1999.
The PageRank citation ranking: Bringing order to the
web.
Pianta, E., &amp; Tonelli, S. 2010. KX: A flexible system
for keyphrase extraction. In Proceedings of the 5th
international workshop on semantic evaluation (pp.
170-173). Association for Computational Linguistics.
Qiu M., Y. Li, and Jing Jiang. 2012. Query-oriented
keyphrase extraction. In Information Retrieval
Technology, pages 64-75.
Salton, G., A. Wong, and C. Yang. 1975. A vector space
model for automatic indexing. In Communications of
the ACM, 18(11), pages 613-620.
Xiaojun Wan and Jianguo Xiao. 2008. Single Document
Keyphrase Extraction Using Neighborhood
Knowledge. In Proceedings of the 23rd National
Conference on Artificial Intelligence - Volume 2 ,
pages 855‚Äì860. AAAI Press.
</reference>
<page confidence="0.998529">
126
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.395694">
<title confidence="0.977336">SGRank: Combining Statistical and Graphical Methods to Improve State of the Art in Unsupervised Keyphrase Extraction</title>
<author confidence="0.869533">Soheil</author>
<affiliation confidence="0.998185">University of</affiliation>
<address confidence="0.803009">Boulder</address>
<email confidence="0.999386">soheildb@gmail.com</email>
<author confidence="0.943048">Tamara</author>
<affiliation confidence="0.998988">University of</affiliation>
<address confidence="0.95775">Boulder</address>
<email confidence="0.963222">tamara.sumner@colorado.edu</email>
<author confidence="0.999695">H James</author>
<affiliation confidence="0.998606">University of</affiliation>
<address confidence="0.712685">Boulder</address>
<email confidence="0.999631">colorado.edu</email>
<abstract confidence="0.9966819375">Keyphrase extraction is a fundamental technique in natural language processing. It enables documents to be mapped to a concise set of phrases that can be used for indexing, clustering, ontology building, auto-tagging and other information organization schemes. Two major families of unsupervised keyphrase extraction algorithms may be characterized as statistical and graph-based. We present a hybrid statistical-graphical algorithm that capitalizes on the heuristics of both families of algorithms and is able to outperform the state of the art in unsupervised keyphrase extraction on several datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Bougouin</author>
<author>F Boudin</author>
<author>B Daille</author>
</authors>
<title>Topicrank: Graph-based topic ranking for keyphrase extraction.</title>
<date>2013</date>
<booktitle>In International Joint Conference on Natural Language Processing (IJCNLP),</booktitle>
<pages>543--551</pages>
<contexts>
<context position="7275" citStr="Bougouin et al., 2013" startWordPosition="1117" endWordPosition="1120">ly of algorithms is TextRank. It is based on the intuition that 1) keywords in a document are more semantically interrelated as they are generally about related topics and 2) that semantic relatedness can be estimated using cooccurrence relations. Therefore in TextRank a graphical representation of the text is constructed in which edges connect words co-occurring in a window of a certain length. The PageRank algorithm is then applied to this network of words to distinguish the important ones which are then reassembled into phrases wherever they occur next to each other in the text. TopicRank (Bougouin et al., 2013) which to the best of our knowledge is the state of the art in graph-based keyphrase extraction, is an enhancement of TextRank. Here, nodes represent topics which consist of sets of candidate terms clustered around shared sub-terms. In (Liang et al., 2009) Chinese search engine query logs are used to extract candidate terms which are used as nodes in the graph. Edges are weighted based on cooccurrence count. Also candidate terms which are longer or whose first occurrence is in the title or first paragraph have boosted edge weights. SingleRank (Wan &amp; Xiao, 2008) also uses cooccurrence counts as</context>
<context position="17197" citStr="Bougouin et al. (2013)" startWordPosition="2786" endWordPosition="2789">ve used an executable kindly shared with us by the system‚Äôs author. For TextRank we have built on an existing open source implementation. The comparisons between the algorithms are done using the precision and recall at k metric where the top k ùë§d ùë°i, ùë°j = 120 terms returned by each algorithm are used to measure precision and recall. Here k ranges from 1 to 15. We also calculate the F-measure for k = 5, 10 and 15. In the following section we describe each dataset in detail and report the results achieved by each algorithm on each dataset. The Semeval and Inspec datasets have also been used by Bougouin et al. (2013) for evaluating their implementation of TextRank along with more advanced graph-based algorithms SingleRank and TopicRank. We have used these results for further comparisons between our method and advanced graph-based algorithms as reported in section 4.4. 4.1 Semeval Dataset The Semeval dataset was used in the Semeval 2010 keyphrase extraction shared task (Kim et al., 2010). To the best of our knowledge this shared task is the largest recent comparison of keyphrase extraction algorithms and an algorithm‚Äôs performance on this dataset is a relatively good indication of where it stands compared </context>
<context position="22739" citStr="Bougouin et al. (2013)" startWordPosition="3686" endWordPosition="3689">.89 12.73 Improvement 59.7% 118% 175% TextRank 25.53 30.6 29.7 Improvement 15.4% 13.3% 17.7% Table 4. Inspec F-measures and improvements. 4.3 Krapivin Dataset The Krapivin dataset consists of 2000 full length ACM papers. This dataset has been prepared by Krapivin et al. (2009). Each article has authorFigure 3. Krapivin precision(y), recall(x) k &lt; 16 K = 5 10 15 SGRank 21.2 21.6 19.4 KPMiner 18.43 18.65 17.4 Improvement 15.% 16.1% 11.7% Textrank 1.02 1.61 2.1 Improvement 1974% 1240% 823% Table 6. Krapivin F-measures and improvements. 122 4.4 Advanced Graph-based Methods As mentioned previously Bougouin et al. (2013) introduce TopicRank and use F-measure at k=10 to compare against TextRank and another advanced graph-ranking method SingleRank. They use the Semeval and Inspec datasets for comparison providing us with an opportunity to compare our performance with those of TextRank and the two more advanced graph-based algorithms. Table 7 contains the F-measures at k=10 for our algorithm, SGRank, and all aforementioned algorithms. Note that the particular implementation of TextRank used in this paper performs worse than ours on the Inspec dataset but better for the Semeval dataset. F at k=10 Inspec Semeval A</context>
</contexts>
<marker>Bougouin, Boudin, Daille, 2013</marker>
<rawString>Bougouin A., F. Boudin, and B. Daille. 2013, October. Topicrank: Graph-based topic ranking for keyphrase extraction. In International Joint Conference on Natural Language Processing (IJCNLP), pages 543-551.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F D‚ÄôAvanzo</author>
<author>B Magnini</author>
<author>A Vallin</author>
</authors>
<title>Keyphrase extraction for summarization purposes: The LAKE system at DUC-2004.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 document understanding conference.</booktitle>
<marker>D‚ÄôAvanzo, Magnini, Vallin, 2004</marker>
<rawString>D‚ÄôAvanzo F., B. Magnini, and A.Vallin. 2004. Keyphrase extraction for summarization purposes: The LAKE system at DUC-2004. In Proceedings of the 2004 document understanding conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S El-Beltagy</author>
<author>A Rafea</author>
</authors>
<title>KP-Miner: A keyphrase extraction system for English and Arabic documents.</title>
<date>2009</date>
<journal>In Information Systems,</journal>
<volume>34</volume>
<issue>1</issue>
<pages>132--144</pages>
<marker>El-Beltagy, Rafea, 2009</marker>
<rawString>El-Beltagy S. and A. Rafea. 2009. KP-Miner: A keyphrase extraction system for English and Arabic documents. In Information Systems, 34(1), pages 132-144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samhaa R El-Beltagy</author>
<author>Ahmed Rafea</author>
</authors>
<title>Kpminer: Participation in semeval-2.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th international workshop on semantic evaluation,</booktitle>
<pages>190--193</pages>
<marker>El-Beltagy, Rafea, 2010</marker>
<rawString>Samhaa R. El-Beltagy and Ahmed Rafea. 2010. Kpminer: Participation in semeval-2. In Proceedings of the 5th international workshop on semantic evaluation, pages 190-193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Fortuna</author>
<author>M Grobelnik</author>
<author>D Mladeniƒá</author>
</authors>
<title>Semi-automatic data-driven ontology construction system.</title>
<date>2006</date>
<booktitle>In Proceedings of the 9th international multiconference information society,</booktitle>
<pages>223--226</pages>
<contexts>
<context position="1521" citStr="Fortuna et al., 2006" startWordPosition="209" endWordPosition="212">ed keyphrase extraction on several datasets. 1 Introduction Keyphrase extraction algorithms aim to extract, from within the document phrases and words that best represent the document‚Äôs main topics. Being able to accurately determine what a document is about allows computers to cluster together documents that share topics (Hammouda et al., 2005), better answer search queries (Qiu et al., 2012), and generate short document summaries (D‚ÄôAvanzo et al., 2004). Furthermore, keyphrase extraction can be used to facilitate the automatic construction of concept maps (Leake et al., 2003) or ontologies (Fortuna et al., 2006) which enable better understanding of the interconnections and relations between different topics. Keyphrase extraction is also used in content-based recommender systems which help users in discovering information relevant to their previously expressed interests (Lops et al., 2011). The aforementioned techniques are all important tools in the organization and understanding of the ever expanding repositories of textual information available online in the form of research papers, news articles, blog posts, etc. and keyphrase extraction is central to all of them. Therefore it could be said that k</context>
</contexts>
<marker>Fortuna, Grobelnik, Mladeniƒá, 2006</marker>
<rawString>Fortuna B., M. Grobelnik, and D. Mladeniƒá. 2006. Semi-automatic data-driven ontology construction system. In Proceedings of the 9th international multiconference information society, pages 223‚Äì226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hammouda</author>
<author>D Matute</author>
<author>M Kamel</author>
</authors>
<title>Corephrase: Keyphrase extraction for document clustering.</title>
<date>2005</date>
<booktitle>In Machine Learning and Data Mining in Pattern Recognition,</booktitle>
<pages>265--274</pages>
<contexts>
<context position="1247" citStr="Hammouda et al., 2005" startWordPosition="168" endWordPosition="171">nsupervised keyphrase extraction algorithms may be characterized as statistical and graph-based. We present a hybrid statistical-graphical algorithm that capitalizes on the heuristics of both families of algorithms and is able to outperform the state of the art in unsupervised keyphrase extraction on several datasets. 1 Introduction Keyphrase extraction algorithms aim to extract, from within the document phrases and words that best represent the document‚Äôs main topics. Being able to accurately determine what a document is about allows computers to cluster together documents that share topics (Hammouda et al., 2005), better answer search queries (Qiu et al., 2012), and generate short document summaries (D‚ÄôAvanzo et al., 2004). Furthermore, keyphrase extraction can be used to facilitate the automatic construction of concept maps (Leake et al., 2003) or ontologies (Fortuna et al., 2006) which enable better understanding of the interconnections and relations between different topics. Keyphrase extraction is also used in content-based recommender systems which help users in discovering information relevant to their previously expressed interests (Lops et al., 2011). The aforementioned techniques are all impo</context>
</contexts>
<marker>Hammouda, Matute, Kamel, 2005</marker>
<rawString>Hammouda K., D. Matute, and M. Kamel. 2005. Corephrase: Keyphrase extraction for document clustering. In Machine Learning and Data Mining in Pattern Recognition, pages 265-274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K S Hasan</author>
<author>V Ng</author>
</authors>
<title>Conundrums in unsupervised keyphrase extraction: making sense of the state-of-the-art.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters</booktitle>
<pages>365--373</pages>
<institution>Association for Computational Linguistics.</institution>
<marker>Hasan, Ng, 2010</marker>
<rawString>Hasan, K. S., &amp; Ng, V. 2010. Conundrums in unsupervised keyphrase extraction: making sense of the state-of-the-art. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters (pp. 365-373). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hulth</author>
</authors>
<title>Improved automatic keyword extraction given more linguistic knowledge.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 conference on Empirical methods in natural language processing,</booktitle>
<pages>216--223</pages>
<marker>Hulth, 2003</marker>
<rawString>Hulth A. 2003. Improved automatic keyword extraction given more linguistic knowledge. In Proceedings of the 2003 conference on Empirical methods in natural language processing, pages 216-223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Jones</author>
</authors>
<title>A statistical interpretation of term specificity and its application in retrieval.</title>
<date>1972</date>
<journal>Journal of documentation,</journal>
<volume>28</volume>
<issue>1</issue>
<pages>11--21</pages>
<contexts>
<context position="4889" citStr="Jones, 1972" startWordPosition="722" endWordPosition="723">ion. 2 Related Works In this section we focus mainly on related unsupervised keyphrase extraction algorithms. One of the most prominent of these algorithms has been the term frequency-inverse document frequency (tf-idf) term weighting function (Salton et al, 1975). Given a corpus of documents the tf-idf weight of term t in document d is mathematically expressed as tf-idf(t,d)=tf(t,d)*idf(t) where tf(t,d) is the frequency of term t in document d and idf(t)=log(N/df(t)) where N is the total number of documents in the corpus and df(t) is the number of documents in the corpus that contain term t (Jones, 1972). The term frequency heuristic is based on the intuition that terms which occur more often in a document are more likely to be important to its meaning. The idf function captures the rareness heuristic, that is, words which occur in many documents in the corpus are unlikely to be important to the meaning of any specific one. Tf-idf is simple yet relatively accurate therefore many variations of it have been used by other algorithms. One of the most successful of these is KP-Miner (El-Beltagy &amp; Rafea , 2010) which to the best of our knowledge represents the state of the art in unsupervised keyph</context>
</contexts>
<marker>Jones, 1972</marker>
<rawString>Jones K.. 1972. A statistical interpretation of term specificity and its application in retrieval. Journal of documentation, 28(1), Pages 11-21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kim</author>
<author>O Medelyan</author>
<author>M Kan</author>
<author>T Baldwin</author>
</authors>
<title>Semeval-2010 task 5: Automatic keyphrase extraction from scientific articles.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation ,</booktitle>
<pages>21--26</pages>
<contexts>
<context position="17574" citStr="Kim et al., 2010" startWordPosition="2841" endWordPosition="2844">the F-measure for k = 5, 10 and 15. In the following section we describe each dataset in detail and report the results achieved by each algorithm on each dataset. The Semeval and Inspec datasets have also been used by Bougouin et al. (2013) for evaluating their implementation of TextRank along with more advanced graph-based algorithms SingleRank and TopicRank. We have used these results for further comparisons between our method and advanced graph-based algorithms as reported in section 4.4. 4.1 Semeval Dataset The Semeval dataset was used in the Semeval 2010 keyphrase extraction shared task (Kim et al., 2010). To the best of our knowledge this shared task is the largest recent comparison of keyphrase extraction algorithms and an algorithm‚Äôs performance on this dataset is a relatively good indication of where it stands compared to others in the field. The Semeval dataset consists of 284 full length ACM articles divided into a test set of size 100, training set of size 144 and trial set of size 40 which we used as the development set for parameter tuning. Each article has two sets of human assigned keyphrases: the author-assigned and reader-assigned ones. The gold standard used in our experiments is</context>
</contexts>
<marker>Kim, Medelyan, Kan, Baldwin, 2010</marker>
<rawString>Kim S., O. Medelyan, M. Kan, and T. Baldwin. 2010. Semeval-2010 task 5: Automatic keyphrase extraction from scientific articles. In Proceedings of the 5th International Workshop on Semantic Evaluation , pages 21-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Krapivin</author>
<author>A Autaeu</author>
<author>M Marchese</author>
</authors>
<title>Large dataset for keyphrases extraction.</title>
<date>2009</date>
<tech>Technical Report DISI-09-055.</tech>
<contexts>
<context position="22394" citStr="Krapivin et al. (2009)" startWordPosition="3631" endWordPosition="3634">gorithm. Similar to the Semeval dataset TextRank performs very poorly on this dataset of longer documents. KPMiner performs much better but both methods are outperformed by our method on all k with statistical significance as shown in Table 6. Figure 2. Inspec precision(y), recall(x) k &lt; 16 K = 5 10 15 SGRank 29.16 33.95 33.66 KPMiner 18.45 15.89 12.73 Improvement 59.7% 118% 175% TextRank 25.53 30.6 29.7 Improvement 15.4% 13.3% 17.7% Table 4. Inspec F-measures and improvements. 4.3 Krapivin Dataset The Krapivin dataset consists of 2000 full length ACM papers. This dataset has been prepared by Krapivin et al. (2009). Each article has authorFigure 3. Krapivin precision(y), recall(x) k &lt; 16 K = 5 10 15 SGRank 21.2 21.6 19.4 KPMiner 18.43 18.65 17.4 Improvement 15.% 16.1% 11.7% Textrank 1.02 1.61 2.1 Improvement 1974% 1240% 823% Table 6. Krapivin F-measures and improvements. 122 4.4 Advanced Graph-based Methods As mentioned previously Bougouin et al. (2013) introduce TopicRank and use F-measure at k=10 to compare against TextRank and another advanced graph-ranking method SingleRank. They use the Semeval and Inspec datasets for comparison providing us with an opportunity to compare our performance with those</context>
</contexts>
<marker>Krapivin, Autaeu, Marchese, 2009</marker>
<rawString>Krapivin M., A. Autaeu &amp; M. Marchese. 2009. Large dataset for keyphrases extraction. Technical Report DISI-09-055.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A N Langville</author>
<author>C D Meyer</author>
</authors>
<title>Google&apos;s PageRank and beyond: the science of search engine rankings.</title>
<date>2011</date>
<publisher>Princeton University Press.</publisher>
<contexts>
<context position="14803" citStr="Langville &amp; Meyer, 2011" startWordPosition="2375" endWordPosition="2378">nces of terms ti and tj and numCo-occurrences(ti,tj) is the number of co-occurrences of the terms within the window of 1500. Furthermore, we incorporate term weights calculated using statistical features in the previous stages into the graphical representation of the text. We hypothesize that term weights calculated using statistical features may serve as a first estimate of a term‚Äôs keyphraseness, i.e. likelihood of being a keyphrase. The PageRank algorithm simulates a random walker on the graph. Each node‚Äôs eventual PageRank score reflects the portion of time the walker spends on that node (Langville &amp; Meyer, 2011). To make sure terms with higher statistical weights are visited more often we would want higher transition probabilities between them but lower transition probabilities between terms with lower weights. Therefore we use the product of the term pair‚Äôs weights as a factor into the weight of the edge between them in the graph. The weight of the edge between terms ti and tj is calculated using the following equation: 1 ùë§e ùë°i, ùë°j = ùë§d ùë°i, ùë°j ‚àó ùë§s ùë°i ‚àó ùë§s (ùë°j) (4) where, as previously defined in equation 3, wd is the distance based portion of the edge weight while ws(ti)*ws(tj) takes the terms‚Äô sta</context>
</contexts>
<marker>Langville, Meyer, 2011</marker>
<rawString>Langville, A. N., &amp; Meyer, C. D. 2011. Google&apos;s PageRank and beyond: the science of search engine rankings. Princeton University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Leake</author>
<author>A Maguitman</author>
<author>T Reichherzer</author>
</authors>
<title>Topic Extraction and Extension to Support Concept Mapping.</title>
<date>2003</date>
<booktitle>In FLAIRS Conference,</booktitle>
<pages>325--329</pages>
<contexts>
<context position="1484" citStr="Leake et al., 2003" startWordPosition="203" endWordPosition="206"> the state of the art in unsupervised keyphrase extraction on several datasets. 1 Introduction Keyphrase extraction algorithms aim to extract, from within the document phrases and words that best represent the document‚Äôs main topics. Being able to accurately determine what a document is about allows computers to cluster together documents that share topics (Hammouda et al., 2005), better answer search queries (Qiu et al., 2012), and generate short document summaries (D‚ÄôAvanzo et al., 2004). Furthermore, keyphrase extraction can be used to facilitate the automatic construction of concept maps (Leake et al., 2003) or ontologies (Fortuna et al., 2006) which enable better understanding of the interconnections and relations between different topics. Keyphrase extraction is also used in content-based recommender systems which help users in discovering information relevant to their previously expressed interests (Lops et al., 2011). The aforementioned techniques are all important tools in the organization and understanding of the ever expanding repositories of textual information available online in the form of research papers, news articles, blog posts, etc. and keyphrase extraction is central to all of th</context>
</contexts>
<marker>Leake, Maguitman, Reichherzer, 2003</marker>
<rawString>Leake D., A. Maguitman, and T. Reichherzer. 2003. Topic Extraction and Extension to Support Concept Mapping. In FLAIRS Conference, pages 325-329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Liang</author>
<author>C Huang</author>
<author>M Li</author>
<author>B L Lu</author>
</authors>
<title>Extracting Keyphrases from Chinese News Articles Using TextRank and Query Log Knowledge.</title>
<date>2009</date>
<booktitle>In PACLIC</booktitle>
<contexts>
<context position="7531" citStr="Liang et al., 2009" startWordPosition="1159" endWordPosition="1162">n TextRank a graphical representation of the text is constructed in which edges connect words co-occurring in a window of a certain length. The PageRank algorithm is then applied to this network of words to distinguish the important ones which are then reassembled into phrases wherever they occur next to each other in the text. TopicRank (Bougouin et al., 2013) which to the best of our knowledge is the state of the art in graph-based keyphrase extraction, is an enhancement of TextRank. Here, nodes represent topics which consist of sets of candidate terms clustered around shared sub-terms. In (Liang et al., 2009) Chinese search engine query logs are used to extract candidate terms which are used as nodes in the graph. Edges are weighted based on cooccurrence count. Also candidate terms which are longer or whose first occurrence is in the title or first paragraph have boosted edge weights. SingleRank (Wan &amp; Xiao, 2008) also uses cooccurrence counts as edge weights. It ranks noun phrases in the text based on the sum of their word weights. ExpandRank (Wan &amp; Xiao, 2008) builds upon SingleRank by incorporating neighboring documents but without significant performance improvements (Hasan &amp; NG, 2010). 118 3 </context>
</contexts>
<marker>Liang, Huang, Li, Lu, 2009</marker>
<rawString>Liang, W., Huang, C., Li, M., &amp; Lu, B. L. 2009. Extracting Keyphrases from Chinese News Articles Using TextRank and Query Log Knowledge. In PACLIC</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Lops</author>
<author>M Gemmis</author>
<author>G Semeraro</author>
</authors>
<title>Contentbased recommender systems: State of the art and trends. In Recommender systems handbook,</title>
<date>2011</date>
<pages>73--105</pages>
<contexts>
<context position="1803" citStr="Lops et al., 2011" startWordPosition="246" endWordPosition="249">r together documents that share topics (Hammouda et al., 2005), better answer search queries (Qiu et al., 2012), and generate short document summaries (D‚ÄôAvanzo et al., 2004). Furthermore, keyphrase extraction can be used to facilitate the automatic construction of concept maps (Leake et al., 2003) or ontologies (Fortuna et al., 2006) which enable better understanding of the interconnections and relations between different topics. Keyphrase extraction is also used in content-based recommender systems which help users in discovering information relevant to their previously expressed interests (Lops et al., 2011). The aforementioned techniques are all important tools in the organization and understanding of the ever expanding repositories of textual information available online in the form of research papers, news articles, blog posts, etc. and keyphrase extraction is central to all of them. Therefore it could be said that keyphrase extraction is a fundamental NLP task, improvements in which could cascade into improvements in higher-level applications that build upon it. In this work we have focused on unsupervised keyphrase extraction approaches as not only they are useful in domains where training d</context>
</contexts>
<marker>Lops, Gemmis, Semeraro, 2011</marker>
<rawString>Lops P., M. Gemmis, and G. Semeraro. 2011. Contentbased recommender systems: State of the art and trends. In Recommender systems handbook, pages 73-105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Medelyan</author>
<author>I H Witten</author>
</authors>
<title>Domain Independent Automatic Keyphrase Indexing with Small Training Sets.</title>
<date>2008</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>59</volume>
<issue>7</issue>
<pages>1026--1040</pages>
<contexts>
<context position="11532" citStr="Medelyan and Witten (2008)" startWordPosition="1812" endWordPosition="1815">r distinguishing multi-word keyphrases. 3.3 Reranking Top Candidates At the end of stage two we have an initial ranking of our candidates based on their tf-idf scores. In the third stage we rerank the top T candidates from stage two based on additional heuristics. These heuristics are position of first occurrence, term length and subsumption count. In the current work T is set to 100 based on experiments on a small development set of 40 documents from the Semeval trial set. The position of first occurrence heuristic has performed consistently well in previous keyphrase extraction experiments. Medelyan and Witten (2008) use a linear decay function of the position of first occurrence as a feature in their supervised algorithm. It has also been utilized in unsupervised methods. In KP-Miner a constant position threshold is used where n-grams whose first occurrence is beyond it are eliminated from the candidate list. KX-FBK uses the linear decay function raised to the power of two. We introduce a novel encoding of this heuristic in the form of a logarithmic decay function, which as we will show in the discussion section outperforms all aforementioned variations. We define the Position of First Occurrence factor </context>
<context position="27634" citStr="Medelyan and Witten (2008)" startWordPosition="4474" endWordPosition="4477"> cutoff threshold where candidates whose first occurrence is beyond 400 123 words into the document are eliminated. KX-FBK uses the following decay function: We tested our system with our PFO function replaced by those of KP-Miner and KX-FBK on the Semeval, Inspec and Krapivin datasets. Replacement with KX-FBK‚Äôs PFO led to respective reductions in the F-Measure at k=15 of 6.5%, 9.5% and 15.1%. Replacement with KPMiner‚Äôs PFO led to a 10.9% reduction in Semeval but no reduction in Inspeq and a 2.9% improvement in Krapivin. We also replaced our PFO function with the linear decay function used by Medelyan and Witten (2008). This function is the same as equation 9 but without the exponent. This led to a 1.4% reduction in Semeval, 0.7% reduction in Inspec and a 2.9% reduction in Krapivin. These results show that our encoding of the PFO heuristic as a logarithmic decay function leads to overall gains in accuracy although it underperforms slightly compared to KP-Miner‚Äôs PFO on the Krapivin dataset which points to further room for improvement. One possible future direction would be to design functions that adjust the cutoffPosition in equation 1 based on document length as some sensitivity to this was observed in ou</context>
</contexts>
<marker>Medelyan, Witten, 2008</marker>
<rawString>Medelyan, O., &amp; Witten, I. H. 2008. Domain Independent Automatic Keyphrase Indexing with Small Training Sets. Journal of the American Society for Information Science and Technology, 59(7), 1026-1040.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Medelyan</author>
<author>E Frank</author>
<author>I H Witten</author>
</authors>
<title>Human-competitive tagging using automatic keyphrase extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>3</volume>
<pages>1318--1327</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="19920" citStr="Medelyan et al., 2009" startWordPosition="3222" endWordPosition="3225">6 25.54 Improvement 6.5% 8.3% 6.4% Textrank 1.25 2.46 3.47 Improvement 1509% 960% 683% Table 2. Semeval F-measures and improvements. As can be seen from the above results our method outperforms KP-Miner in both precision and recall for all k and achieves statistically significant improvements in the F-measure over KP-Miner for k=10 and 15. These results are noteworthy considering that in the Semeval keyphrase extraction shared task KP-Miner was the best performing unsupervised algorithm, and the second best overall out of 19 systems, outperforming prominent supervised algorithms such as Maui (Medelyan et al., 2009). TextRank seems to generally underperform on longer documents and has performed poorly on the Semeval dataset. 4.2 Inspec Dataset The Inspec dataset is comprised of 2000 ACM abstracts divided into test, training and validation sets containing 500, 1000 and 500 abstracts respectively. We follow the same approach as 121 taken by Mihalcea and Tarau (2004). We use only the 500 documents in the test set. The following table provides a statistical overview of this document set. 500 Document Number of Keyphrase docs Length Keyphrases Length Max. 338 31 9 Avg. 121.8 9.8 2.3 Min. 23 2 1 Table 3. Inspe</context>
</contexts>
<marker>Medelyan, Frank, Witten, 2009</marker>
<rawString>Medelyan, O., Frank, E., &amp; Witten, I. H. (2009). Human-competitive tagging using automatic keyphrase extraction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3-Volume 3 (pp. 1318-1327). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>P Tarau</author>
</authors>
<title>Textrank: Bringing order into texts.</title>
<date>2004</date>
<booktitle>In Proceedings of International Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="20275" citStr="Mihalcea and Tarau (2004)" startWordPosition="3277" endWordPosition="3280">lts are noteworthy considering that in the Semeval keyphrase extraction shared task KP-Miner was the best performing unsupervised algorithm, and the second best overall out of 19 systems, outperforming prominent supervised algorithms such as Maui (Medelyan et al., 2009). TextRank seems to generally underperform on longer documents and has performed poorly on the Semeval dataset. 4.2 Inspec Dataset The Inspec dataset is comprised of 2000 ACM abstracts divided into test, training and validation sets containing 500, 1000 and 500 abstracts respectively. We follow the same approach as 121 taken by Mihalcea and Tarau (2004). We use only the 500 documents in the test set. The following table provides a statistical overview of this document set. 500 Document Number of Keyphrase docs Length Keyphrases Length Max. 338 31 9 Avg. 121.8 9.8 2.3 Min. 23 2 1 Table 3. Inspec dataset statistics. Figure 2 shows the average precision and recall for all three algorithms for k from 1 to 15. Table 4 shows the F-measure improvements made by our method over the two other algorithms for k=5, 10 and 15. As these results show, on this dataset of relatively short documents, TextRank outperforms KP-Miner for k&gt;2. Our algorithm achieve</context>
<context position="3167" citStr="Mihalcea &amp; Tarau, 2004" startWordPosition="454" endWordPosition="457">of several features in supervised keyphrase extraction algorithms. Therefore increases in the accuracy of unsupervised methods can propagate into the results of supervised algorithms as well. There are two prominent families of unsupervised keyphrase extraction algorithms. The older of these two is clustered around the tf-idf term weighting metric where word statistics such as frequency of occurrence in the document or rareness in the corpus are used to distinguish potential keyphrases. The more recently developed of the two families has been built on the foundation of the TextRank algorithm (Mihalcea &amp; Tarau, 2004). In algorithms of this family a graphical representation of the text is constructed with words as nodes and edges reflecting cooccurrence relations. This graph is then used to run node ranking algorithms such as PageRank (Page 117 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 117‚Äì126, Denver, Colorado, June 4‚Äì5, 2015. et al., 1999) that assign weights to the node-words reflecting their semantic importance to the text. Although some overlap between these two families of algorithms has occurred in works that incorporate statistical heuristi</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Mihalcea R. and P. Tarau. 2004. Textrank: Bringing order into texts. In Proceedings of International Conference on Empirical Methods in Natural Language Processing, pages 404‚Äì411</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Page</author>
<author>S Brin</author>
<author>R Motwani</author>
<author>T Winograd</author>
</authors>
<title>The PageRank citation ranking: Bringing order to the web.</title>
<date>1999</date>
<marker>Page, Brin, Motwani, Winograd, 1999</marker>
<rawString>Page, L., Brin, S., Motwani, R., &amp; Winograd, T. 1999. The PageRank citation ranking: Bringing order to the web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Pianta</author>
<author>S Tonelli</author>
</authors>
<title>KX: A flexible system for keyphrase extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th international workshop on semantic evaluation</booktitle>
<pages>170--173</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="6418" citStr="Pianta &amp; Tonelli, 2010" startWordPosition="981" endWordPosition="984">es are comprised of the longest n-grams that do not contain a stop word or punctuation mark, occur for the first time within the first 400 words of the document and have a term frequency above a minimum threshold determined by document length. KP-Miner also boosts the weights of multiword candidates in proportion to the ratio of the frequencies of single word candidates to all candidates. In a reranking step, the tf-idf of each term is recalculated based on the number of times it is subsumed by other candidates in the top 15 candidates list. Another tf-idf based unsupervised system is KX-FBK (Pianta &amp; Tonelli, 2010) which uses some of the same heuristics as KP-Miner but with different formulations and was shown to underperform in comparison in the Semeval 2010 keyphrase extraction task. An approach fundamentally different from tf-idf and its family of algorithms is TextRank. It is based on the intuition that 1) keywords in a document are more semantically interrelated as they are generally about related topics and 2) that semantic relatedness can be estimated using cooccurrence relations. Therefore in TextRank a graphical representation of the text is constructed in which edges connect words co-occurring</context>
</contexts>
<marker>Pianta, Tonelli, 2010</marker>
<rawString>Pianta, E., &amp; Tonelli, S. 2010. KX: A flexible system for keyphrase extraction. In Proceedings of the 5th international workshop on semantic evaluation (pp. 170-173). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Qiu</author>
<author>Y Li</author>
<author>Jing Jiang</author>
</authors>
<title>Query-oriented keyphrase extraction.</title>
<date>2012</date>
<booktitle>In Information Retrieval Technology,</booktitle>
<pages>64--75</pages>
<contexts>
<context position="1296" citStr="Qiu et al., 2012" startWordPosition="176" endWordPosition="179">racterized as statistical and graph-based. We present a hybrid statistical-graphical algorithm that capitalizes on the heuristics of both families of algorithms and is able to outperform the state of the art in unsupervised keyphrase extraction on several datasets. 1 Introduction Keyphrase extraction algorithms aim to extract, from within the document phrases and words that best represent the document‚Äôs main topics. Being able to accurately determine what a document is about allows computers to cluster together documents that share topics (Hammouda et al., 2005), better answer search queries (Qiu et al., 2012), and generate short document summaries (D‚ÄôAvanzo et al., 2004). Furthermore, keyphrase extraction can be used to facilitate the automatic construction of concept maps (Leake et al., 2003) or ontologies (Fortuna et al., 2006) which enable better understanding of the interconnections and relations between different topics. Keyphrase extraction is also used in content-based recommender systems which help users in discovering information relevant to their previously expressed interests (Lops et al., 2011). The aforementioned techniques are all important tools in the organization and understanding</context>
</contexts>
<marker>Qiu, Li, Jiang, 2012</marker>
<rawString>Qiu M., Y. Li, and Jing Jiang. 2012. Query-oriented keyphrase extraction. In Information Retrieval Technology, pages 64-75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Wong</author>
<author>C Yang</author>
</authors>
<title>A vector space model for automatic indexing.</title>
<date>1975</date>
<journal>In Communications of the ACM,</journal>
<volume>18</volume>
<issue>11</issue>
<pages>613--620</pages>
<contexts>
<context position="4541" citStr="Salton et al, 1975" startWordPosition="661" endWordPosition="664">a keyphrase extraction algorithm based on optimal statistical features and 2) Combine it with a graph-based algorithm for further improvements. The advantage of graph-based methods is that they take into account term co-occurrence patterns that are not generally utilized by statistical methods which take a bag of n-grams approach to document representation. 2 Related Works In this section we focus mainly on related unsupervised keyphrase extraction algorithms. One of the most prominent of these algorithms has been the term frequency-inverse document frequency (tf-idf) term weighting function (Salton et al, 1975). Given a corpus of documents the tf-idf weight of term t in document d is mathematically expressed as tf-idf(t,d)=tf(t,d)*idf(t) where tf(t,d) is the frequency of term t in document d and idf(t)=log(N/df(t)) where N is the total number of documents in the corpus and df(t) is the number of documents in the corpus that contain term t (Jones, 1972). The term frequency heuristic is based on the intuition that terms which occur more often in a document are more likely to be important to its meaning. The idf function captures the rareness heuristic, that is, words which occur in many documents in t</context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>Salton, G., A. Wong, and C. Yang. 1975. A vector space model for automatic indexing. In Communications of the ACM, 18(11), pages 613-620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianguo Xiao</author>
</authors>
<title>Single Document Keyphrase Extraction Using Neighborhood Knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of the 23rd National Conference on Artificial Intelligence - Volume</booktitle>
<volume>2</volume>
<pages>855--860</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="7842" citStr="Wan &amp; Xiao, 2008" startWordPosition="1212" endWordPosition="1215">her in the text. TopicRank (Bougouin et al., 2013) which to the best of our knowledge is the state of the art in graph-based keyphrase extraction, is an enhancement of TextRank. Here, nodes represent topics which consist of sets of candidate terms clustered around shared sub-terms. In (Liang et al., 2009) Chinese search engine query logs are used to extract candidate terms which are used as nodes in the graph. Edges are weighted based on cooccurrence count. Also candidate terms which are longer or whose first occurrence is in the title or first paragraph have boosted edge weights. SingleRank (Wan &amp; Xiao, 2008) also uses cooccurrence counts as edge weights. It ranks noun phrases in the text based on the sum of their word weights. ExpandRank (Wan &amp; Xiao, 2008) builds upon SingleRank by incorporating neighboring documents but without significant performance improvements (Hasan &amp; NG, 2010). 118 3 Method Our algorithm processes an input document in four stages. In the first stage we extract all possible ngrams from the input text and eliminate those that are highly unlikely to be keyphrases, for instance n-grams containing punctuation marks. In the second stage the remaining n-grams are ranked based on </context>
</contexts>
<marker>Wan, Xiao, 2008</marker>
<rawString>Xiaojun Wan and Jianguo Xiao. 2008. Single Document Keyphrase Extraction Using Neighborhood Knowledge. In Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 2 , pages 855‚Äì860. AAAI Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>