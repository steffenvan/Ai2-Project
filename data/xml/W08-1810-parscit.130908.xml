<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000214">
<title confidence="0.994486">
Indexing on Semantic Roles for Question Answering
</title>
<author confidence="0.98783">
Luiz Augusto Pizzato
</author>
<affiliation confidence="0.868293">
Centre for Language Technology
Macquarie University
Sydney, Australia
</affiliation>
<email confidence="0.992853">
pizzato@ics.mq.edu.au
</email>
<author confidence="0.997086">
Diego Moll´a
</author>
<affiliation confidence="0.868307333333333">
Centre for Language Technology
Macquarie University
Sydney, Australia
</affiliation>
<email confidence="0.996876">
diego@ics.mq.edu.au
</email>
<sectionHeader confidence="0.995605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99930725">
Semantic Role Labeling (SRL) has been
used successfully in several stages of auto-
mated Question Answering (QA) systems
but its inherent slow procedures make it
difficult to use at the indexing stage of the
document retrieval component. In this pa-
per we confirm the intuition that SRL at
indexing stage improves the performance
of QA and propose a simplified technique
named the Question Prediction Language
Model (QPLM), which provides similar in-
formation with a much lower cost. The
methods were tested on four different QA
systems and the results suggest that QPLM
can be used as a good compromise be-
tween speed and accuracy.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998838266666667">
Semantic Role Labeling (SRL) has been imple-
mented or suggested as a means to aid several Nat-
ural Language Processing (NLP) tasks such as in-
formation extraction (Kogan et al., 2005), multi-
document summarization (Barzilay et al., 1999)
and machine translation (Quantz and Schmitz,
1994). Question Answering (QA) is one task that
takes advantage of SRL, and in fact much of the
research about the application of SRL to NLP is
related to QA. Thus, Narayanan and Harabagiu
(2004) apply the argument-predicate relationship
from PropBank (Palmer et al., 2005) together with
the semantic frames from FrameNet (Baker et al.,
1998) to create an inference mechanism to improve
QA. Kaisser and Webber (2007) apply semantic
</bodyText>
<note confidence="0.722497">
© 2008. Licensed under the Creative Commons
</note>
<footnote confidence="0.967297666666667">
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.999683583333333">
relational information in order to transform ques-
tions into information retrieval queries and further
analyze the results to find the answers for natural
language questions. Sun et al. (2005) use a shal-
low semantic parser to create semantic roles in or-
der to match questions and answers. Shen and La-
pata (2007) developed an answer extraction mod-
ule that incorporates FrameNet style semantic role
information. They deal with the semantic role as-
signment as a optimization problem in a bipartite
graph and the answer extraction as a graph match-
ing over the semantic relations.
Most of the studies that use SRL or similar tech-
niques to QA apply semantic relation tools on the
input or output of the Information Retrieval phase
of their system. Our paper investigates the use of
semantic information for indexing documents. Our
hypothesis is that allowing Semantic Role infor-
mation at the indexing stage the question analyzer
and subsequent stages of the QA system can obtain
higher accuracy by providing an implicit query an-
alyzer as well as more precise retrieval. Theoret-
ically, the inclusion of this information at index-
ing time can also speed up the overall QA process
since syntactic rephrasing or re-ranking of docu-
ments based on semantic roles would not be nec-
essary. However, SRL techniques are still highly
complex and they demand a computational power
that is not yet available to most research groups
when working with large corpora. In our experi-
ence the annotation of a 3GB corpus, such as the
AQUAINT (Graff, 2002), using a semantic role
labeler, for instance SwiRL from Surdeanu and
Turmo (2005) can take more than one year using
a standard PC configuration1.
In order to efficiently process a corpus with se-
</bodyText>
<footnote confidence="0.360226">
1Intel(R) Pentium(R) 4 HT 2.80GHz with 2.0 GB RAM
</footnote>
<page confidence="0.981716">
74
</page>
<note confidence="0.9945055">
Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 74–81
Manchester, UK. August 2008
</note>
<bodyText confidence="0.999927260869565">
mantic relations, we have developed an alterna-
tive annotation strategy based on word-to-word re-
lations instead of noun phrase-to-predicate rela-
tions. We define semantic triples based on syn-
tactic clues; this approach was also studied by
Litkowski (1999) but some major differences with
our work are that we use automatically learned
rules to generate the semantic relations, and that
we use different semantic labels than those de-
fined by Litkowski, some more specific and some
more general. Our annotation scheme is named the
Question Prediction Language Model (QPLM) and
represents relations between pairs of words using
labels such as Who and When, according to how
one word complements the other.
In the following section we provide an overview
of the proposed semantic annotation module. Then
in Section 3 we detail the information retrieval
framework used that allows the indexing and re-
trieval of semantic information. Section 4 de-
scribes the experimental setup and presents the re-
sults. Finally, Section 5 presents the concluding
remarks and some discussion of further work.
</bodyText>
<sectionHeader confidence="0.891872" genericHeader="method">
2 Question Prediction Language Model
</sectionHeader>
<bodyText confidence="0.998210269230769">
QPLM, as described in Pizzato and Moll´a (2007),
represents sentences by specifying the semantic re-
lationship among its components using question
words. In this way, we focus on dividing the prob-
lem of representing a large sentence into small
questions that could be asked about its compo-
nents. QPLM is expressed by triples 0(ω) → α
where 0 is a question word, ω is the word that con-
cerns the question word 0 and α is the word that
answers the relation 0 about ω. For instance the
relation Who(eat) → Jack tells us that the per-
son who eats is Jack. The representation of our se-
mantic relations as triples 0(ω) → α is important
because it allows the representation of sentences as
directed graphs of semantic relations. This repre-
sentation has the capacity of generating questions
about the sentence being analyzed. Figure 1 shows
such a representation of the sentence: “John asked
that a flag be placed in every school”.
Having the sentence of Figure 1 and remov-
ing a possible answer α from any relation triple,
it is possible to formulate a complete question
about this sentence that would require α as an
answer. For instance, we can observe that re-
moving the node John we obtain the question
“Who asked for a flag to be placed in every
</bodyText>
<figureCaption confidence="0.998561">
Figure 1: Graph Representation
</figureCaption>
<bodyText confidence="0.999862769230769">
school?” where Who was extracted from the triple
Who(ask) → John. The same is valid for other
relations, such as removing the word school to ob-
tain the question “Where did John ask for a flag
to be placed?”. The name Question Prediction
for this model is due to its capability of generat-
ing questions regarding the sentence that has been
modeled.
We have developed a process to automatically
annotate QPLM information, the process is rule
based where the rules are automatically learned
from a corpus obtained from mapping PropBank
into QPLM instances. The mapping between se-
mantic roles and QPLM is not one-to-one, which
reduces the accuracy of the training corpus. A
sample of 40 randomly selected documents was
manually evaluated showing that nearly 90% of the
QPLM triples obtained were correctly converted
from the PropBank mapping. PropBank does not
give us some relations that we wish to include such
as ownership (Whose(car) → Maria) or quan-
tity (HowMany(country) → twenty)), but it
does give us the benefits of a large training set cov-
ering a variety of different predicates.
Our QPLM annotation tool, like most SRL
tools, makes use of a syntactic parser and a named-
entity (NE) recognizer. We are currently using
Connexor2 for syntactic parsing and LingPipe3 for
named-entity recognition.
An evaluation of our QPLM annotation has
shown a reasonable precision (50%) with a low
recall (24%). Both precision and recall seem to
be connected with the choice of training corpus.
The high precision is influenced by the large train-
ing set and the different variety of predicates. The
low recall is due to the low amount of connections
that can be mapped from one sentence in Prop-
Bank to QPLM. As we will present in Section 4,
QPLM helps to improve results for QA even when
</bodyText>
<footnote confidence="0.9993635">
2http://www.connexor.com
3http://alias-i.com/lingpipe/
</footnote>
<figure confidence="0.997579545454545">
John
who
placed
asked
flag
what
what
where
school
every
which
</figure>
<page confidence="0.996835">
75
</page>
<bodyText confidence="0.999933714285714">
this training corpus is not optimal. This suggests
that if a more suitable corpus is used to create the
QPLM rules then we can improve the already pos-
itive results. An ideal training corpus would con-
tain all QPLM pairs; not only verbs and head of
noun phrases but also connections among all rele-
vant words in a sentence.
</bodyText>
<sectionHeader confidence="0.982031" genericHeader="method">
3 Indexing and Retrieving Semantic
Information
</sectionHeader>
<bodyText confidence="0.976083146341464">
A document index that contains information about
semantic relations provides a way of finding docu-
ments on the basis of meaningful relations among
words instead of simply their co-occurrence or
proximity to each other. A semantic relation index
allows the retrieval of the same piece of informa-
tion when queried using syntactic variations of the
same query such as: “Bill kicked the ball” or “The
ball was kicked by Bill”.
Several strategies can be used to build the index-
ing structure that includes relational information.
The task of IR requires fast indexing and retrieval
of information regardless of the amount of data
stored and how it is going to be retrieved. From
our experience, the use of relational databases is
acceptable only if the amount of documents and
speed of indexing and retrieval is not a concern.
When database systems are used on large IR sys-
tems there is always a trade off between the speed
of indexing and the speed of retrieval as well speed
and storage efficiency.
The best approach for IR has always been a cus-
tom built inverted file structure. In the semantic
role/QPLM case it is important to develop an in-
dexing structure that can maintain the annotation
information. Because it is important to allow dif-
ferent types of information to be indexed, we im-
plemented a framework for information retrieval
that easily incorporates different linguistic infor-
mation. The framework allows fast indexing and
retrieval and the implementation of different rank-
ing strategies.
With the inclusion of relational information, the
framework provides a way to retrieve documents
according to a query of semantically connected
words. This feature is best used when queries are
formed as sentences in natural language. A sim-
plified representation of the framework index is
shown in Figure 2 for a QPLM annotated sentence.
Figure 2 shows that the relation of words are rep-
resented by a common relation identifier and a re-
</bodyText>
<figureCaption confidence="0.9957245">
Figure 2: Simplified representation of the indexing
of QPLM relations
</figureCaption>
<figure confidence="0.9928464">
Query Returns documents that
∗(kick) → ∗ contain the word kick
Who(kick) → ∗ inform that someone kicks
Who(∗) → Bill inform that Bill does an action
Who(kick) → Bill inform that Bill kicks
</figure>
<figureCaption confidence="0.9889125">
Figure 3: QPLM Queries (asterisk symbol is used
to represent a wildcard)
</figureCaption>
<bodyText confidence="0.986108857142857">
lation type. The roles that each word plays in a
relation is also included within the same record.
The IF is optimized so that redundant information
is not represented, as illustrated by the record of
the word kick and the single document number.
The framework also provides a way to include
words that have not been explicitly related to other
words in the text just in the same way as a stan-
dard bag-of-words (BoW) approach. This feature
is important even when the text is fully semanti-
cally or syntactically parsed. Many words may not
be associated with the others in a sentence because
of different reasons such as errors in the parser.
Therefore, even if the query presented to the re-
trieval component is not a proper natural language
sentence or it fails to be analyzed, the system will
perform as a normal BoW system.
Once the retrieval query is analyzed, it is pos-
sible to perform queries that focus on retrieving
all documents where a certain relation occurs as
well as all documents where a certain word plays
a specific role. The example in Figure 3 demon-
strates some queries and what documents or sen-
tences they return.
A document containing the sentence “Bill kicked
the ball” would be retrieved for all the queries in
Figure 3. The framework also allows the formula-
tion of more complex queries such as:
</bodyText>
<figure confidence="0.976046909090909">
(Who(kick) → ∗) ∧ (What(kick) → ball)
QPLM representation for “Bill kicked the ball”:
ID Relation
11 Who(kick) → Bill
12 What(kick) → ball
Inverted file representation:
Term Document Rel. ID Rel. Type Role
Bill 1 11 Who Arg
kick 1 11 Who Pred
12 What Pred
ball 1 12 What Arg
</figure>
<page confidence="0.914516">
76
</page>
<bodyText confidence="0.997827958333334">
Each token is indexed by itself (i.e not together
with the related words) including the information
from the relations it is part of. This is done with no
overhead or redundant information being stored.
This approach makes it possible to keep the stan-
dard models for document ranking. A normal
calculation of Term Frequency (TF) and Inverted
Document Frequency (IDF) is performed when
taking the terms individually or as BoW, while
only a minimal modification of TF/IDF is required
when a more complex retrieval strategy is needed.
The ranking strategy is based on a vector space
model. Documents and queries are represented
as three different vectors: bag-of-words (BoW-V),
partial relation (PR-V) and full relation (FR-V).
The weights of the vector tokens are calculated us-
ing the weights of their individual tokens in the
context of the vector being analyzed. In BoW-V,
weights are calculated based on words; PR-V uses
individual words and their relation types; FR-V
uses the association of a specific word with another
word. Figure 4 illustrates the contents of these vec-
tors for the sentence “John loves Mary, but Mary
likes Brad” when used as a query:
</bodyText>
<figureCaption confidence="0.98394">
Figure 4: Vectors used for document ranking
</figureCaption>
<bodyText confidence="0.9999141">
The tokens of the above example would have
different weights if the same sentence appeared in
a document with additional sentences. Because of
their lower frequency, it is expected that the com-
ponents of FR-V and, in a lesser extent, of PR-V to
have a stronger impact on the calculation of simi-
larity than the components of BoW-V. With this
approach, for queries with relations that are not
indexed, the method is equivalent to a traditional
BoW approach.
</bodyText>
<sectionHeader confidence="0.998524" genericHeader="evaluation">
4 Experiments and Evaluation
</sectionHeader>
<bodyText confidence="0.995278">
We have performed a series of experiments using
the techniques described on Section 3 in order to
verify the usefulness of QPLM in comparison to
SRL based on PropBank. We compared both se-
mantic annotations by using it with IR and under
QA evaluation methods.
</bodyText>
<subsectionHeader confidence="0.998498">
4.1 Configuration of experiments
</subsectionHeader>
<bodyText confidence="0.99999347826087">
We performed experiments using data resources
from the QA track of the TREC conferences
(Voorhees and Dang, 2006) and the evaluation
scripts available at their TREC website of years
2004, 2005 and 2006. The retrieval experiments
were carried out using only a reduced set of docu-
ments from the AQUAINT corpus because the se-
mantic role labelers tested were not able to parse
the full set, unlike QPLM which parsed all docu-
ments successfully.
The SRL tool SwiRL (Surdeanu and Turmo,
2005) has a good precision and coverage, however
it is slow and quite unstable when parsing large
amounts of data. We have assembled a cluster
of computers in order to speed up the corpus an-
notation, but even when having around ten ded-
icated computers the estimated completion time
was larger than one year. The lack of semantic an-
notators that can quickly evaluate large amount of
data gave us the stimulus needed to use a simplified
and quicker technique. We used the QPLM anno-
tation tool which takes less than 3 weeks to fully
annotate the 3GB of data from the AQUAINT cor-
pus using a single machine.
Since we wanted to determinate how QPLM
compares to SRL, particularly on the basis of its
usage for IR and for QA, we performed some
tests using the available amount of data anno-
tated with semantic roles, and the same docu-
ments with QPLM. The part of the AQUAINT
corpus annotated includes the first 41,116 docu-
ments, in chronological order, from the New York
Times (NYT) newspaper. We used the 1,448 ques-
tions from the QA track of 2004, 2005 and 2006
from the TREC competition. Since these questions
are not always self contained and in some cases
(OTHER-type questions) not even a proper natu-
ral language sentence, we performed some ques-
tion modification so that the entire topic text could
be included. These modifications include substitu-
tion of key pronouns as well as the inclusion of the
whole topic text when shorter representations were
found. In some extreme cases when no substitution
was possible and the question did not mention the
topic, we added a phrase containing the topic at the
start of the question. Some examples are presented
</bodyText>
<table confidence="0.872397157894737">
BoW-V: ([John:1], [loves:1], [Mary:2],
[likes:1], [Brad:1])
([John:ARG0:1], [loves:PRED:1],
PR-V: [Mary:ARG1:1], [Mary:ARG0:1],
[likes:PRED:1], [Brad:ARG1:1])
([John:ARG0:loves:1],
[Mary:ARG1:loves:1],
[Mary:ARG0:likes:1],
[Brad:ARG1:likes:1])
FR-V:
77
Topic: Gordon Gekko
Question: What year was the movie released?
Modification: Regarding Gordon Gekko, what year
was the movie released?
Question: What was Gekko’s profession?
Modification: What was Gordon Gekko’s profession?
Question: Other
Modification: Tell me more about Gordon Gekko.
</table>
<figureCaption confidence="0.994193">
Figure 5: Modifications applied to TREC ques-
tions
</figureCaption>
<bodyText confidence="0.9871547">
in Figure 5.
Using these questions as queries for our IR
framework, we retrieved a set of 50 documents for
every question. We analyzed the impact of the se-
mantic annotation when used on document indices
by checking the presence of the answer string in
the documents returned. We also obtained a list
of 50 documents using solely the BoW approach
in order to compare what is the gain over standard
retrieval.
</bodyText>
<subsectionHeader confidence="0.998174">
4.2 Evaluation of retrieval sets
</subsectionHeader>
<bodyText confidence="0.999905642857143">
Table 1 presents the results of the retrieval set using
TREC’s QA track from 2004, 2005 and 2006 us-
ing the BoW, the SRL and the QPLM approaches.
Because we performed the evaluation of these doc-
uments automatically, we consider a document rel-
evant on the only basis of the presence of the
required answer string. We adopted the evalua-
tion metrics for QA documents sets proposed by
Roberts and Gaizauskas (2004). We used the fol-
lowing metrics: p@n as the precision at n docu-
ments or percentage of documents containing an
answer when retrieving at most n documents; c@n
as the coverage at n documents or percentage of
questions that can be answered using up to n doc-
uments for each question; and r@n as the redun-
dancy at n document or the average number of an-
swers found in the first n documents per question.
As observed in Table 1, the SRL approach gives
the best results for all question sets on all evalu-
ation metrics, with the exception of c@50 on the
2006 question set. In most other retrieval sets
the baseline performs worse than both QPLM and
SRL, however for 2004 questions it performed bet-
ter than QPLM on p@50 and r@50. It is interesting
to observe that the QPLM results for the same year
on c@50 are better than the BoW approach indi-
cating that a larger amount of questions can poten-
tially be answered by QPLM.
</bodyText>
<table confidence="0.99984375">
2004 p@50 c@50 r@50
BoW 5.85% 33.33% 2.92
SRL 6.40% 35.33% 3.20
QPLM 5.58% 34.47% 2.79
2005 p@50 c@50 r@50
BoW 10.03% 41.13% 5.02
SRL 11.00% 43.77% 5.50
QPLM 10.58% 42.08% 5.29
2006 p@50 c@50 r@50
BoW 7.30% 34.57% 3.65
SRL 8.73% 36.33% 4.37
QPLM 8.31% 38.45% 4.16
</table>
<tableCaption confidence="0.975846">
Table 1: Experimental results of index approaches
on TREC questions
</tableCaption>
<subsectionHeader confidence="0.963604">
4.3 Experiments on QA systems
</subsectionHeader>
<bodyText confidence="0.94621725">
To better understand the relation between the re-
trieved document sets and question answering we
applied the retrieval sets to four question answer-
ing systems:
</bodyText>
<listItem confidence="0.697184666666667">
• Aranea: Developed by Lin (2007), the Aranea
system utilizes the redundancy from the
World Wide Web using different Web Search
</listItem>
<bodyText confidence="0.920726714285714">
Engines. The system relies on the text snip-
pets to generate candidate answers. It applies
filtering techniques based on intuitive rules,
as well as the expected answer classes with
named-entities recognition defined by regular
expressions and a fixed list for some special
cases.
</bodyText>
<listItem confidence="0.985155">
• OpenEphyra: Developed by Schlaefer et al.
(2007), the OpenEphyra framework attempts
to be a test bench for question answering tech-
niques. The system approaches QA in a fairly
standard way. Using a three-stage QA archi-
tecture (Question Analysis, Information Re-
trieval, Answer Extraction), it performed rea-
sonably well at the QA Track at TREC 2007
by using Web Search engines on its IR stage
and mapping the answers back into the TREC
corpus.
• MetaQA System: Similar to the Aranea QA
system, MetaQA (Pizzato and Molla, 2005)
</listItem>
<bodyText confidence="0.965252">
makes heavy use of redundancy and the in-
formation provided by Web Search Engines.
However it goes a step further by combining
different classes of Web Search engines (in-
cluding Web Question Answering Systems)
and assigning different confidence scores to
each of the classes.
</bodyText>
<page confidence="0.987701">
78
</page>
<listItem confidence="0.995556">
• AnswerFinder: Developed by Moll´a and Van
</listItem>
<bodyText confidence="0.95265954">
Zaanen (2006), the AnswerFinder QA system
unique feature is the use of QA graph rules
learned automatically from a small training
corpus. These graph rules are based on
the maximum common subgraph between the
deep syntactic representation of a question
and a candidate answer sentence. The graphs
were derived from the output of the Connexor
dependency-based parser.
For most of these systems some modifications of
the standard system configuration were required.
All the systems used, with the exception of An-
swerFinder, make heavy use of web search en-
gines and the redundancy obtained to find their
answers. For our experiments we had to turn the
Web search off, causing a significant drop in per-
formance when compared to the reported results in
the literature. Because AnswerFinder’s IR compo-
nent is performed offline, the integration is seam-
less and only required providing the system with
a list of documents in the same format as TREC
distributes the ranked list of files per topic. The
OpenEphyra framework is well designed and im-
plemented, however the interaction between its
components still depended on the overall system
architecture, which makes the implementation of
new modules for the system quite difficult.
With the exception of AnswerFinder, all the QA
systems received a retrieval set as a collection of
snippets. This was based on the fact that these
systems are based on Web Retrieval and they ex-
pect to receive documents in this format. We ex-
tracted for every document the 255 character win-
dow where more question words (non-stopwords)
were found. The implementation of different rank-
ing strategies for passage retrieval such as those
described by Tellex et al. (2003) could improve the
results for individual QA systems. However, a pre-
liminary evaluation of the passage retrieval have
shown us that the 255 character window with the
current snippet construction method was enough
to achieve near optimal performance on the docu-
ment set used.
The results obtained by the QA systems were
processed using the answer regular expressions
distributed by TREC. The numbers described in
this study show the factoid score for correct an-
swers. We have not used the exact answer be-
cause it required some cleaning of the answer log
files and some modification of some QA systems.
</bodyText>
<table confidence="0.98827675">
2004 2005 2006
BoW 5.00% 2.30% 2.10%
SRL 6.10% 3.50% 2.70%
QPLM 5.00% 2.50% 3.50%
</table>
<tableCaption confidence="0.7961875">
Table 2: Factoid results for C@1 on the Aranea
system
</tableCaption>
<table confidence="0.99979325">
2004 2005 2006
BoW 2.50% 5.10% 3.00%
SRL 3.30% 7.00% 4.40%
QPLM 2.80% 6.20% 4.20%
</table>
<tableCaption confidence="0.9758865">
Table 3: Factoid results for C@1 on the OpenE-
phyra system
</tableCaption>
<bodyText confidence="0.999456552631579">
Therefore, the results shown on Tables 2, 3 and
5 are product of the same retrieval set and result
of the same evaluation procedure. Results of the
MetaQA system at Table 4 are presented as cover-
age at answer 10 (C@10) since this system has a
non standard approach for QA that is invalidated
by the methodology of this test. The results in the
other tables could be understood as either precision
or coverage at answer 1, we will refer to them as
C@1.
We observed that the results from the QA sys-
tem are consistent with the findings from the re-
sults of the retrieval system. The Aranea QA sys-
tem results on Table 2 show an average improve-
ment for the SRL approach. QPLM has similar
performance to BoW for 2004 and 2005 questions
but outperforms both techniques on 2006 ques-
tions.
The results shown by OpenEphyra in Table 3
also demonstrate that semantic annotation can help
question answering when used in the IR stages of a
QA system. The best results were observed when
SRL was applied. QPLM followed SRL and out-
performed BoW on three tests. It is important to
point out that results for the retrieval set alone in
Table 1 showed BoW outperforming QPLM for
2004 questions on both redundancy and precision
metrics. This might be an indication that OpenE-
phyra answer extraction modules are more precise
than the other QA systems and do not heavily rely
on redundancy as do the Aranea and the MetaQA
systems.
Because of the high dependency on Web
sources, the MetaQA system performed rather
poorly. As explained earlier, the results were mea-
sured using C@10 instead of C@1. The reason for
this is that the MetaQA system is meant to be an
aggregator of information sources and its ranking
</bodyText>
<page confidence="0.997443">
79
</page>
<table confidence="0.996204">
2004 2005 2006
BoW 0.87% 3.31% 1.24%
SRL 2.61% 3.87% 1.99%
QPLM 0.43% 3.31% 1.24%
</table>
<tableCaption confidence="0.8970255">
Table 4: Factoid results for C@10 on the MetaQA
system
</tableCaption>
<table confidence="0.99976525">
2004 2005 2006
BoW 1.10% 2.50% 1.20%
SRL 1.80% 2.60% 2.20%
QPLM 1.80% 2.70% 2.00%
</table>
<tableCaption confidence="0.9651555">
Table 5: Factoid results for C@1 on the An-
swerFinder system
</tableCaption>
<bodyText confidence="0.99943856">
mechanisms only work when sufficient evidence is
given for certain entities. Not only was the system
not designed for the single-source setup, but it was
not designed to provide a single answer. Neverthe-
less, even with the non-conformity of the system,
it appears to support that semantic markup can en-
hance the IR results for QA. Not surprisingly the
extra redundancy presented in the 2004 BoW re-
trieval contributed to better results in this redun-
dancy based QA system.
Results in Table 5 show that AnswerFinder cor-
rectly answered only a few questions for the given
question set. On the other hand, it provided some
consistent results such that the improvements were
due to additional correct answers and not to a
larger but different set of correct answers. The
AnswerFinder QA system showed a similar perfor-
mance for both semantic-based strategies and both
outperformed the BoW strategy.
In this section we have shown an evaluation of
different retrieval sets of documents using four dis-
tinct QA systems. We have observed that semantic
strategies not only assist the retrieval of better doc-
uments, but also help in finding answers for ques-
tions when used with QA systems.
</bodyText>
<sectionHeader confidence="0.984333" genericHeader="conclusions">
5 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999678016129033">
In this work we propose the use of semantic re-
lation in QA. We also present QPLM as an alter-
native to SRL. QPLM is a simpler approach to se-
mantic annotation based on relations between pairs
of words, which gives a large advantage in speed
performance over SRL. We show some compari-
son of retrieval sets using the questions from the
QA track of TREC and conclude that SRL and
QPLM improve the quality of the retrieval set over
a standard BoW approach. From these results we
also observe that QPLM performance does not fall
much behind SRL.
We performed an evaluation using four QA sys-
tems. These systems are conceptually different
which gives a broad perspective of the obtained re-
sults. The results once again show the effective-
ness of semantic annotation. Over QA, SRL has
performed better than the other techniques, but was
closely followed by QPLM. The results obtained
here suggest that QPLM is a cheaper and effective
method of semantic annotation that can help in tun-
ing the search component of a QA system to find
the correct answers for a question.
The results presented in this work for all QA
systems are much lower than those reported in
the literature. This is an undesirable but ex-
pected problem that occurred not only because of
the modifications carried on the QA systems but
mainly because of the reduced number of docu-
ments used for this evaluation. We are looking into
more efficient alternatives for performing the SRL
annotation of the AQUAINT corpus.
Only recently we have been able to test Koomen
et al. (2005) SRL tool. This SRL tool is the top
ranking SRL tool at the CoNLL-2005 Shared Task
Evaluation and it seems to be much faster than
SwiRL. Preliminary tests suggest that it is able
to perform the annotation of AQUAINT in almost
one full year using a single computer; however,
this tool, like SwiRL, is not very stable, crashing
several times during the experiments. As further
work, we plan to employ several computers and
attempt to parse the whole AQUAINT corpus with
this tool.
It is important to point out that although the tool
of Koomen et al. seems much faster than SwiRL,
QPLM still outperforms both of them on speed by
large. QPLM represents word relations that are
built using rules from syntactic and NE informa-
tion. This simpler representation, combined with
a smaller number of supporting NLP tools, allow
QPLM to be faster than current SRL tools. We
plan to carry out further work on the QPLM tool
to increase its performance on both speed and ac-
curacy. QPLM’s precision and recall figures are
going to be improved by using a hand annotated
corpus. QPLM’s speed suggest that it can be cur-
rently used on IR tools as a pre-processing engine.
It is understandable that any delay in the IR phases
is undesirable when dealing with large amount of
data, therefore optimizing the speed of QPLM is
one of our priorities.
</bodyText>
<page confidence="0.991113">
80
</page>
<sectionHeader confidence="0.600657" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.515695833333333">
This work was supported by an iMURS scholar-
ship from Macquarie University and the CSIRO.
Palmer, Martha, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguist., 31(1):71–
106.
</bodyText>
<sectionHeader confidence="0.990342" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999906635416667">
Baker, Collin F., Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 17th international conference on Com-
putational linguistics, pages 86–90, Morristown, NJ,
USA. Association for Computational Linguistics.
Barzilay, Regina, Kathleen R. McKeown, and Michael
Elhadad. 1999. Information fusion in the context of
multi-document summarization. In Proceedings of
the 37th annual meeting of the Association for Com-
putational Linguistics on Computational Linguistics,
pages 550–557, Morristown, NJ, USA. Association
for Computational Linguistics.
Graff, David. 2002. The AQUAINT corpus of english
news text. CDROM. ISBN: 1-58563-240-6.
Kaisser, Michael and Bonnie Webber. 2007. Question
answering based on semantic roles. In Proceedings
of the ACL 2007 Workshop on Deep Linguistic Pro-
cessing,, page 4148, Prague, Czech Republic, June.
c2007 Association for Computational Linguistics.
Kogan, Y., N. Collier, S. Pakhomov, and M. Krautham-
mer. 2005. Towards semantic role labeling &amp; ie in
the medical literature. In American Medical Infor-
matics Association Annual Symposium., Washing-
ton, DC.
Koomen, P., V. Punyakanok, D. Roth, and W. Yih.
2005. Generalized inference with multiple seman-
tic role labeling systems (shared task paper). In
Dagan, Ido and Dan Gildea, editors, Proc. of the
Annual Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 181–184.
Lin, Jimmy. 2007. An exploration of the principles un-
derlying redundancy-based factoid question answer-
ing. ACM Trans. Inf. Syst., 25(2):6.
Litkowski, K. 1999. Question-answering using seman-
tic relation triples. In In Proceedings of the 8th Text
Retrieval Conference (TREC-8, pages 349–356.
Molla, Diego and Menno van Zaanen. 2006. An-
swerfinder at TREC 2005. In The Fourteenth Text
REtrieval Conference (TREC 2005), Gaithersburg,
Maryland. National Institute of Standards and Tech-
nology.
Narayanan, Srini and Sanda Harabagiu. 2004. Ques-
tion answering based on semantic structures. In
COLING ’04: Proceedings of the 20th international
conference on Computational Linguistics, page 693,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Pizzato, Luiz Augusto and Diego Molla. 2005. Ex-
tracting exact answers using a meta question answer-
ing system. In Proceedings of the Australasian Lan-
guage Technology Workshop 2005 (ALTA-2005).,
The University of Sydney, Australia, December.
Pizzato, Luiz Augusto and Diego Moll´a. 2007. Ques-
tion prediction language model. In Proceedings
of the Australasian Language Technology Workshop
2007, pages 92–99, Melbourne, December.
Quantz, Joachim and Birte Schmitz. 1994.
Knowledge-based disambiguation for machine
translation. Minds and Machines, 4(1):39–57,
February.
Roberts, Ian and Robert J. Gaizauskas. 2004. Eval-
uating passage retrieval approaches for question an-
swering. In McDonald, Sharon and John Tait, edi-
tors, Advances in Information Retrieval, 26th Euro-
pean Conference on IR Research, ECIR 2004, Sun-
derland, UK, April 5-7, 2004, Proceedings, volume
2997 of Lecture Notes in Computer Science, pages
72–84. Springer.
Schlaefer, N., P. Gieselmann, and G. Sautter. 2007.
The ephyra qa system at trec 2006 the Ephyra QA
system at TREC 2006. In The Fifteenth Text RE-
trieval Conference (TREC 2006).
Shen, Dan and Mirella Lapata. 2007. Using semantic
roles to improve question answering. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 12–21, Prague,
June 2007. Association for Computational Linguis-
tics.
Sun, R. X., J. J. Jiang, Y. F. Tan, H. Cui, T. S. Chua, and
M. Y. Kan. 2005. Using syntactic and semantic rela-
tion analysis in question answering. In Proceedings
of the TREC.
Surdeanu, Mihai and Jordi Turmo. 2005. Semantic
role labeling using complete syntactic analysis. In
Proceedings of CoNLL 2005 Shared Task, June.
Tellex, Stefanie, Boris Katz, Jimmy Lin, Aaron Fer-
nandes, and Gregory Marton. 2003. Quantitative
evaluation of passage retrieval algorithms for ques-
tion answering. In SIGIR ’03: Proceedings of the
26th annual international ACM SIGIR conference on
Research and development in informaion retrieval,
pages 41–47, New York, NY, USA. ACM Press.
Voorhees, Ellen M. and Hoa Trang Dang. 2006.
Overview of the TREC 2005 question answering
track. In Text REtrieval Conference.
</reference>
<page confidence="0.999265">
81
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.196867">
<title confidence="0.999875">Indexing on Semantic Roles for Question Answering</title>
<author confidence="0.986264">Luiz Augusto</author>
<affiliation confidence="0.800418">Centre for Language Macquarie</affiliation>
<address confidence="0.546147">Sydney,</address>
<email confidence="0.998052">pizzato@ics.mq.edu.au</email>
<author confidence="0.815471">Diego</author>
<affiliation confidence="0.8341245">Centre for Language Macquarie</affiliation>
<address confidence="0.687713">Sydney,</address>
<email confidence="0.999263">diego@ics.mq.edu.au</email>
<abstract confidence="0.996749764705882">Semantic Role Labeling (SRL) has been used successfully in several stages of automated Question Answering (QA) systems but its inherent slow procedures make it difficult to use at the indexing stage of the document retrieval component. In this paper we confirm the intuition that SRL at indexing stage improves the performance of QA and propose a simplified technique named the Question Prediction Language Model (QPLM), which provides similar information with a much lower cost. The methods were tested on four different QA systems and the results suggest that QPLM can be used as a good compromise between speed and accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The berkeley framenet project.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguistics,</booktitle>
<pages>86--90</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1539" citStr="Baker et al., 1998" startWordPosition="233" endWordPosition="236">mantic Role Labeling (SRL) has been implemented or suggested as a means to aid several Natural Language Processing (NLP) tasks such as information extraction (Kogan et al., 2005), multidocument summarization (Barzilay et al., 1999) and machine translation (Quantz and Schmitz, 1994). Question Answering (QA) is one task that takes advantage of SRL, and in fact much of the research about the application of SRL to NLP is related to QA. Thus, Narayanan and Harabagiu (2004) apply the argument-predicate relationship from PropBank (Palmer et al., 2005) together with the semantic frames from FrameNet (Baker et al., 1998) to create an inference mechanism to improve QA. Kaisser and Webber (2007) apply semantic © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. relational information in order to transform questions into information retrieval queries and further analyze the results to find the answers for natural language questions. Sun et al. (2005) use a shallow semantic parser to create semantic roles in order to match questions and answers. Shen and Lapata (2007) developed an answer e</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Baker, Collin F., Charles J. Fillmore, and John B. Lowe. 1998. The berkeley framenet project. In Proceedings of the 17th international conference on Computational linguistics, pages 86–90, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
<author>Michael Elhadad</author>
</authors>
<title>Information fusion in the context of multi-document summarization.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics,</booktitle>
<pages>550--557</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1151" citStr="Barzilay et al., 1999" startWordPosition="171" endWordPosition="174">onfirm the intuition that SRL at indexing stage improves the performance of QA and propose a simplified technique named the Question Prediction Language Model (QPLM), which provides similar information with a much lower cost. The methods were tested on four different QA systems and the results suggest that QPLM can be used as a good compromise between speed and accuracy. 1 Introduction Semantic Role Labeling (SRL) has been implemented or suggested as a means to aid several Natural Language Processing (NLP) tasks such as information extraction (Kogan et al., 2005), multidocument summarization (Barzilay et al., 1999) and machine translation (Quantz and Schmitz, 1994). Question Answering (QA) is one task that takes advantage of SRL, and in fact much of the research about the application of SRL to NLP is related to QA. Thus, Narayanan and Harabagiu (2004) apply the argument-predicate relationship from PropBank (Palmer et al., 2005) together with the semantic frames from FrameNet (Baker et al., 1998) to create an inference mechanism to improve QA. Kaisser and Webber (2007) apply semantic © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecom</context>
</contexts>
<marker>Barzilay, McKeown, Elhadad, 1999</marker>
<rawString>Barzilay, Regina, Kathleen R. McKeown, and Michael Elhadad. 1999. Information fusion in the context of multi-document summarization. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, pages 550–557, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
</authors>
<title>The AQUAINT corpus of english news text.</title>
<date>2002</date>
<journal>CDROM. ISBN:</journal>
<pages>1--58563</pages>
<contexts>
<context position="3332" citStr="Graff, 2002" startWordPosition="520" endWordPosition="521">analyzer and subsequent stages of the QA system can obtain higher accuracy by providing an implicit query analyzer as well as more precise retrieval. Theoretically, the inclusion of this information at indexing time can also speed up the overall QA process since syntactic rephrasing or re-ranking of documents based on semantic roles would not be necessary. However, SRL techniques are still highly complex and they demand a computational power that is not yet available to most research groups when working with large corpora. In our experience the annotation of a 3GB corpus, such as the AQUAINT (Graff, 2002), using a semantic role labeler, for instance SwiRL from Surdeanu and Turmo (2005) can take more than one year using a standard PC configuration1. In order to efficiently process a corpus with se1Intel(R) Pentium(R) 4 HT 2.80GHz with 2.0 GB RAM 74 Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 74–81 Manchester, UK. August 2008 mantic relations, we have developed an alternative annotation strategy based on word-to-word relations instead of noun phrase-to-predicate relations. We define semantic triples based on syntactic clues; this ap</context>
</contexts>
<marker>Graff, 2002</marker>
<rawString>Graff, David. 2002. The AQUAINT corpus of english news text. CDROM. ISBN: 1-58563-240-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Kaisser</author>
<author>Bonnie Webber</author>
</authors>
<title>Question answering based on semantic roles.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing,,</booktitle>
<pages>4148</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1613" citStr="Kaisser and Webber (2007)" startWordPosition="245" endWordPosition="248">ans to aid several Natural Language Processing (NLP) tasks such as information extraction (Kogan et al., 2005), multidocument summarization (Barzilay et al., 1999) and machine translation (Quantz and Schmitz, 1994). Question Answering (QA) is one task that takes advantage of SRL, and in fact much of the research about the application of SRL to NLP is related to QA. Thus, Narayanan and Harabagiu (2004) apply the argument-predicate relationship from PropBank (Palmer et al., 2005) together with the semantic frames from FrameNet (Baker et al., 1998) to create an inference mechanism to improve QA. Kaisser and Webber (2007) apply semantic © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. relational information in order to transform questions into information retrieval queries and further analyze the results to find the answers for natural language questions. Sun et al. (2005) use a shallow semantic parser to create semantic roles in order to match questions and answers. Shen and Lapata (2007) developed an answer extraction module that incorporates FrameNet style semantic role informatio</context>
</contexts>
<marker>Kaisser, Webber, 2007</marker>
<rawString>Kaisser, Michael and Bonnie Webber. 2007. Question answering based on semantic roles. In Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing,, page 4148, Prague, Czech Republic, June. c2007 Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Kogan</author>
<author>N Collier</author>
<author>S Pakhomov</author>
<author>M Krauthammer</author>
</authors>
<title>Towards semantic role labeling &amp; ie in the medical literature.</title>
<date>2005</date>
<booktitle>In American Medical Informatics Association Annual Symposium.,</booktitle>
<location>Washington, DC.</location>
<contexts>
<context position="1098" citStr="Kogan et al., 2005" startWordPosition="164" endWordPosition="167">e document retrieval component. In this paper we confirm the intuition that SRL at indexing stage improves the performance of QA and propose a simplified technique named the Question Prediction Language Model (QPLM), which provides similar information with a much lower cost. The methods were tested on four different QA systems and the results suggest that QPLM can be used as a good compromise between speed and accuracy. 1 Introduction Semantic Role Labeling (SRL) has been implemented or suggested as a means to aid several Natural Language Processing (NLP) tasks such as information extraction (Kogan et al., 2005), multidocument summarization (Barzilay et al., 1999) and machine translation (Quantz and Schmitz, 1994). Question Answering (QA) is one task that takes advantage of SRL, and in fact much of the research about the application of SRL to NLP is related to QA. Thus, Narayanan and Harabagiu (2004) apply the argument-predicate relationship from PropBank (Palmer et al., 2005) together with the semantic frames from FrameNet (Baker et al., 1998) to create an inference mechanism to improve QA. Kaisser and Webber (2007) apply semantic © 2008. Licensed under the Creative Commons Attribution-Noncommercial</context>
</contexts>
<marker>Kogan, Collier, Pakhomov, Krauthammer, 2005</marker>
<rawString>Kogan, Y., N. Collier, S. Pakhomov, and M. Krauthammer. 2005. Towards semantic role labeling &amp; ie in the medical literature. In American Medical Informatics Association Annual Symposium., Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koomen</author>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>Generalized inference with multiple semantic role labeling systems (shared task paper).</title>
<date>2005</date>
<booktitle>In Dagan, Ido and Dan Gildea, editors, Proc. of the Annual Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>181--184</pages>
<contexts>
<context position="27741" citStr="Koomen et al. (2005)" startWordPosition="4647" endWordPosition="4650"> effective method of semantic annotation that can help in tuning the search component of a QA system to find the correct answers for a question. The results presented in this work for all QA systems are much lower than those reported in the literature. This is an undesirable but expected problem that occurred not only because of the modifications carried on the QA systems but mainly because of the reduced number of documents used for this evaluation. We are looking into more efficient alternatives for performing the SRL annotation of the AQUAINT corpus. Only recently we have been able to test Koomen et al. (2005) SRL tool. This SRL tool is the top ranking SRL tool at the CoNLL-2005 Shared Task Evaluation and it seems to be much faster than SwiRL. Preliminary tests suggest that it is able to perform the annotation of AQUAINT in almost one full year using a single computer; however, this tool, like SwiRL, is not very stable, crashing several times during the experiments. As further work, we plan to employ several computers and attempt to parse the whole AQUAINT corpus with this tool. It is important to point out that although the tool of Koomen et al. seems much faster than SwiRL, QPLM still outperforms</context>
</contexts>
<marker>Koomen, Punyakanok, Roth, Yih, 2005</marker>
<rawString>Koomen, P., V. Punyakanok, D. Roth, and W. Yih. 2005. Generalized inference with multiple semantic role labeling systems (shared task paper). In Dagan, Ido and Dan Gildea, editors, Proc. of the Annual Conference on Computational Natural Language Learning (CoNLL), pages 181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jimmy Lin</author>
</authors>
<title>An exploration of the principles underlying redundancy-based factoid question answering.</title>
<date>2007</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="19254" citStr="Lin (2007)" startWordPosition="3204" endWordPosition="3205">mount of questions can potentially be answered by QPLM. 2004 p@50 c@50 r@50 BoW 5.85% 33.33% 2.92 SRL 6.40% 35.33% 3.20 QPLM 5.58% 34.47% 2.79 2005 p@50 c@50 r@50 BoW 10.03% 41.13% 5.02 SRL 11.00% 43.77% 5.50 QPLM 10.58% 42.08% 5.29 2006 p@50 c@50 r@50 BoW 7.30% 34.57% 3.65 SRL 8.73% 36.33% 4.37 QPLM 8.31% 38.45% 4.16 Table 1: Experimental results of index approaches on TREC questions 4.3 Experiments on QA systems To better understand the relation between the retrieved document sets and question answering we applied the retrieval sets to four question answering systems: • Aranea: Developed by Lin (2007), the Aranea system utilizes the redundancy from the World Wide Web using different Web Search Engines. The system relies on the text snippets to generate candidate answers. It applies filtering techniques based on intuitive rules, as well as the expected answer classes with named-entities recognition defined by regular expressions and a fixed list for some special cases. • OpenEphyra: Developed by Schlaefer et al. (2007), the OpenEphyra framework attempts to be a test bench for question answering techniques. The system approaches QA in a fairly standard way. Using a three-stage QA architectur</context>
</contexts>
<marker>Lin, 2007</marker>
<rawString>Lin, Jimmy. 2007. An exploration of the principles underlying redundancy-based factoid question answering. ACM Trans. Inf. Syst., 25(2):6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Litkowski</author>
</authors>
<title>Question-answering using semantic relation triples. In</title>
<date>1999</date>
<booktitle>In Proceedings of the 8th Text Retrieval Conference (TREC-8,</booktitle>
<pages>349--356</pages>
<contexts>
<context position="3975" citStr="Litkowski (1999)" startWordPosition="621" endWordPosition="622">eler, for instance SwiRL from Surdeanu and Turmo (2005) can take more than one year using a standard PC configuration1. In order to efficiently process a corpus with se1Intel(R) Pentium(R) 4 HT 2.80GHz with 2.0 GB RAM 74 Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 74–81 Manchester, UK. August 2008 mantic relations, we have developed an alternative annotation strategy based on word-to-word relations instead of noun phrase-to-predicate relations. We define semantic triples based on syntactic clues; this approach was also studied by Litkowski (1999) but some major differences with our work are that we use automatically learned rules to generate the semantic relations, and that we use different semantic labels than those defined by Litkowski, some more specific and some more general. Our annotation scheme is named the Question Prediction Language Model (QPLM) and represents relations between pairs of words using labels such as Who and When, according to how one word complements the other. In the following section we provide an overview of the proposed semantic annotation module. Then in Section 3 we detail the information retrieval framew</context>
</contexts>
<marker>Litkowski, 1999</marker>
<rawString>Litkowski, K. 1999. Question-answering using semantic relation triples. In In Proceedings of the 8th Text Retrieval Conference (TREC-8, pages 349–356.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diego Molla</author>
<author>Menno van Zaanen</author>
</authors>
<title>Answerfinder at TREC</title>
<date>2006</date>
<booktitle>In The Fourteenth Text REtrieval Conference (TREC 2005),</booktitle>
<institution>National Institute of Standards and Technology.</institution>
<location>Gaithersburg, Maryland.</location>
<marker>Molla, van Zaanen, 2006</marker>
<rawString>Molla, Diego and Menno van Zaanen. 2006. Answerfinder at TREC 2005. In The Fourteenth Text REtrieval Conference (TREC 2005), Gaithersburg, Maryland. National Institute of Standards and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srini Narayanan</author>
<author>Sanda Harabagiu</author>
</authors>
<title>Question answering based on semantic structures.</title>
<date>2004</date>
<booktitle>In COLING ’04: Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>693</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1392" citStr="Narayanan and Harabagiu (2004)" startWordPosition="212" endWordPosition="215">hods were tested on four different QA systems and the results suggest that QPLM can be used as a good compromise between speed and accuracy. 1 Introduction Semantic Role Labeling (SRL) has been implemented or suggested as a means to aid several Natural Language Processing (NLP) tasks such as information extraction (Kogan et al., 2005), multidocument summarization (Barzilay et al., 1999) and machine translation (Quantz and Schmitz, 1994). Question Answering (QA) is one task that takes advantage of SRL, and in fact much of the research about the application of SRL to NLP is related to QA. Thus, Narayanan and Harabagiu (2004) apply the argument-predicate relationship from PropBank (Palmer et al., 2005) together with the semantic frames from FrameNet (Baker et al., 1998) to create an inference mechanism to improve QA. Kaisser and Webber (2007) apply semantic © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. relational information in order to transform questions into information retrieval queries and further analyze the results to find the answers for natural language questions. Sun et al. </context>
</contexts>
<marker>Narayanan, Harabagiu, 2004</marker>
<rawString>Narayanan, Srini and Sanda Harabagiu. 2004. Question answering based on semantic structures. In COLING ’04: Proceedings of the 20th international conference on Computational Linguistics, page 693, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luiz Augusto Pizzato</author>
<author>Diego Molla</author>
</authors>
<title>Extracting exact answers using a meta question answering system.</title>
<date>2005</date>
<booktitle>In Proceedings of the Australasian Language Technology Workshop</booktitle>
<institution>The University of Sydney,</institution>
<location>Australia,</location>
<contexts>
<context position="20154" citStr="Pizzato and Molla, 2005" startWordPosition="3348" endWordPosition="3351">named-entities recognition defined by regular expressions and a fixed list for some special cases. • OpenEphyra: Developed by Schlaefer et al. (2007), the OpenEphyra framework attempts to be a test bench for question answering techniques. The system approaches QA in a fairly standard way. Using a three-stage QA architecture (Question Analysis, Information Retrieval, Answer Extraction), it performed reasonably well at the QA Track at TREC 2007 by using Web Search engines on its IR stage and mapping the answers back into the TREC corpus. • MetaQA System: Similar to the Aranea QA system, MetaQA (Pizzato and Molla, 2005) makes heavy use of redundancy and the information provided by Web Search Engines. However it goes a step further by combining different classes of Web Search engines (including Web Question Answering Systems) and assigning different confidence scores to each of the classes. 78 • AnswerFinder: Developed by Moll´a and Van Zaanen (2006), the AnswerFinder QA system unique feature is the use of QA graph rules learned automatically from a small training corpus. These graph rules are based on the maximum common subgraph between the deep syntactic representation of a question and a candidate answer s</context>
</contexts>
<marker>Pizzato, Molla, 2005</marker>
<rawString>Pizzato, Luiz Augusto and Diego Molla. 2005. Extracting exact answers using a meta question answering system. In Proceedings of the Australasian Language Technology Workshop 2005 (ALTA-2005)., The University of Sydney, Australia, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luiz Augusto Pizzato</author>
<author>Diego Moll´a</author>
</authors>
<title>Question prediction language model.</title>
<date>2007</date>
<booktitle>In Proceedings of the Australasian Language Technology Workshop</booktitle>
<pages>92--99</pages>
<location>Melbourne,</location>
<marker>Pizzato, Moll´a, 2007</marker>
<rawString>Pizzato, Luiz Augusto and Diego Moll´a. 2007. Question prediction language model. In Proceedings of the Australasian Language Technology Workshop 2007, pages 92–99, Melbourne, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Quantz</author>
<author>Birte Schmitz</author>
</authors>
<title>Knowledge-based disambiguation for machine translation.</title>
<date>1994</date>
<journal>Minds and Machines,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="1202" citStr="Quantz and Schmitz, 1994" startWordPosition="178" endWordPosition="181">improves the performance of QA and propose a simplified technique named the Question Prediction Language Model (QPLM), which provides similar information with a much lower cost. The methods were tested on four different QA systems and the results suggest that QPLM can be used as a good compromise between speed and accuracy. 1 Introduction Semantic Role Labeling (SRL) has been implemented or suggested as a means to aid several Natural Language Processing (NLP) tasks such as information extraction (Kogan et al., 2005), multidocument summarization (Barzilay et al., 1999) and machine translation (Quantz and Schmitz, 1994). Question Answering (QA) is one task that takes advantage of SRL, and in fact much of the research about the application of SRL to NLP is related to QA. Thus, Narayanan and Harabagiu (2004) apply the argument-predicate relationship from PropBank (Palmer et al., 2005) together with the semantic frames from FrameNet (Baker et al., 1998) to create an inference mechanism to improve QA. Kaisser and Webber (2007) apply semantic © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reser</context>
</contexts>
<marker>Quantz, Schmitz, 1994</marker>
<rawString>Quantz, Joachim and Birte Schmitz. 1994. Knowledge-based disambiguation for machine translation. Minds and Machines, 4(1):39–57, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Roberts</author>
<author>Robert J Gaizauskas</author>
</authors>
<title>Evaluating passage retrieval approaches for question answering.</title>
<date>2004</date>
<booktitle>Advances in Information Retrieval, 26th European Conference on IR Research, ECIR 2004,</booktitle>
<volume>2997</volume>
<pages>72--84</pages>
<editor>In McDonald, Sharon and John Tait, editors,</editor>
<publisher>Springer.</publisher>
<location>Sunderland, UK,</location>
<contexts>
<context position="17788" citStr="Roberts and Gaizauskas (2004)" startWordPosition="2938" endWordPosition="2941">esence of the answer string in the documents returned. We also obtained a list of 50 documents using solely the BoW approach in order to compare what is the gain over standard retrieval. 4.2 Evaluation of retrieval sets Table 1 presents the results of the retrieval set using TREC’s QA track from 2004, 2005 and 2006 using the BoW, the SRL and the QPLM approaches. Because we performed the evaluation of these documents automatically, we consider a document relevant on the only basis of the presence of the required answer string. We adopted the evaluation metrics for QA documents sets proposed by Roberts and Gaizauskas (2004). We used the following metrics: p@n as the precision at n documents or percentage of documents containing an answer when retrieving at most n documents; c@n as the coverage at n documents or percentage of questions that can be answered using up to n documents for each question; and r@n as the redundancy at n document or the average number of answers found in the first n documents per question. As observed in Table 1, the SRL approach gives the best results for all question sets on all evaluation metrics, with the exception of c@50 on the 2006 question set. In most other retrieval sets the bas</context>
</contexts>
<marker>Roberts, Gaizauskas, 2004</marker>
<rawString>Roberts, Ian and Robert J. Gaizauskas. 2004. Evaluating passage retrieval approaches for question answering. In McDonald, Sharon and John Tait, editors, Advances in Information Retrieval, 26th European Conference on IR Research, ECIR 2004, Sunderland, UK, April 5-7, 2004, Proceedings, volume 2997 of Lecture Notes in Computer Science, pages 72–84. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Schlaefer</author>
<author>P Gieselmann</author>
<author>G Sautter</author>
</authors>
<title>The ephyra qa system at trec 2006 the Ephyra QA system at TREC</title>
<date>2007</date>
<booktitle>In The Fifteenth Text REtrieval Conference (TREC</booktitle>
<contexts>
<context position="19679" citStr="Schlaefer et al. (2007)" startWordPosition="3268" endWordPosition="3271">stems To better understand the relation between the retrieved document sets and question answering we applied the retrieval sets to four question answering systems: • Aranea: Developed by Lin (2007), the Aranea system utilizes the redundancy from the World Wide Web using different Web Search Engines. The system relies on the text snippets to generate candidate answers. It applies filtering techniques based on intuitive rules, as well as the expected answer classes with named-entities recognition defined by regular expressions and a fixed list for some special cases. • OpenEphyra: Developed by Schlaefer et al. (2007), the OpenEphyra framework attempts to be a test bench for question answering techniques. The system approaches QA in a fairly standard way. Using a three-stage QA architecture (Question Analysis, Information Retrieval, Answer Extraction), it performed reasonably well at the QA Track at TREC 2007 by using Web Search engines on its IR stage and mapping the answers back into the TREC corpus. • MetaQA System: Similar to the Aranea QA system, MetaQA (Pizzato and Molla, 2005) makes heavy use of redundancy and the information provided by Web Search Engines. However it goes a step further by combinin</context>
</contexts>
<marker>Schlaefer, Gieselmann, Sautter, 2007</marker>
<rawString>Schlaefer, N., P. Gieselmann, and G. Sautter. 2007. The ephyra qa system at trec 2006 the Ephyra QA system at TREC 2006. In The Fifteenth Text REtrieval Conference (TREC 2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Shen</author>
<author>Mirella Lapata</author>
</authors>
<title>Using semantic roles to improve question answering.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>12--21</pages>
<location>Prague,</location>
<contexts>
<context position="2117" citStr="Shen and Lapata (2007)" startWordPosition="315" endWordPosition="319">ic frames from FrameNet (Baker et al., 1998) to create an inference mechanism to improve QA. Kaisser and Webber (2007) apply semantic © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. relational information in order to transform questions into information retrieval queries and further analyze the results to find the answers for natural language questions. Sun et al. (2005) use a shallow semantic parser to create semantic roles in order to match questions and answers. Shen and Lapata (2007) developed an answer extraction module that incorporates FrameNet style semantic role information. They deal with the semantic role assignment as a optimization problem in a bipartite graph and the answer extraction as a graph matching over the semantic relations. Most of the studies that use SRL or similar techniques to QA apply semantic relation tools on the input or output of the Information Retrieval phase of their system. Our paper investigates the use of semantic information for indexing documents. Our hypothesis is that allowing Semantic Role information at the indexing stage the questi</context>
</contexts>
<marker>Shen, Lapata, 2007</marker>
<rawString>Shen, Dan and Mirella Lapata. 2007. Using semantic roles to improve question answering. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 12–21, Prague, June 2007. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R X Sun</author>
<author>J J Jiang</author>
<author>Y F Tan</author>
<author>H Cui</author>
<author>T S Chua</author>
<author>M Y Kan</author>
</authors>
<title>Using syntactic and semantic relation analysis in question answering.</title>
<date>2005</date>
<booktitle>In Proceedings of the TREC.</booktitle>
<contexts>
<context position="1998" citStr="Sun et al. (2005)" startWordPosition="293" endWordPosition="296">agiu (2004) apply the argument-predicate relationship from PropBank (Palmer et al., 2005) together with the semantic frames from FrameNet (Baker et al., 1998) to create an inference mechanism to improve QA. Kaisser and Webber (2007) apply semantic © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. relational information in order to transform questions into information retrieval queries and further analyze the results to find the answers for natural language questions. Sun et al. (2005) use a shallow semantic parser to create semantic roles in order to match questions and answers. Shen and Lapata (2007) developed an answer extraction module that incorporates FrameNet style semantic role information. They deal with the semantic role assignment as a optimization problem in a bipartite graph and the answer extraction as a graph matching over the semantic relations. Most of the studies that use SRL or similar techniques to QA apply semantic relation tools on the input or output of the Information Retrieval phase of their system. Our paper investigates the use of semantic informa</context>
</contexts>
<marker>Sun, Jiang, Tan, Cui, Chua, Kan, 2005</marker>
<rawString>Sun, R. X., J. J. Jiang, Y. F. Tan, H. Cui, T. S. Chua, and M. Y. Kan. 2005. Using syntactic and semantic relation analysis in question answering. In Proceedings of the TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Jordi Turmo</author>
</authors>
<title>Semantic role labeling using complete syntactic analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL 2005 Shared Task,</booktitle>
<contexts>
<context position="3414" citStr="Surdeanu and Turmo (2005)" startWordPosition="531" endWordPosition="534">uracy by providing an implicit query analyzer as well as more precise retrieval. Theoretically, the inclusion of this information at indexing time can also speed up the overall QA process since syntactic rephrasing or re-ranking of documents based on semantic roles would not be necessary. However, SRL techniques are still highly complex and they demand a computational power that is not yet available to most research groups when working with large corpora. In our experience the annotation of a 3GB corpus, such as the AQUAINT (Graff, 2002), using a semantic role labeler, for instance SwiRL from Surdeanu and Turmo (2005) can take more than one year using a standard PC configuration1. In order to efficiently process a corpus with se1Intel(R) Pentium(R) 4 HT 2.80GHz with 2.0 GB RAM 74 Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 74–81 Manchester, UK. August 2008 mantic relations, we have developed an alternative annotation strategy based on word-to-word relations instead of noun phrase-to-predicate relations. We define semantic triples based on syntactic clues; this approach was also studied by Litkowski (1999) but some major differences with our wo</context>
<context position="14700" citStr="Surdeanu and Turmo, 2005" startWordPosition="2434" endWordPosition="2437">ed on PropBank. We compared both semantic annotations by using it with IR and under QA evaluation methods. 4.1 Configuration of experiments We performed experiments using data resources from the QA track of the TREC conferences (Voorhees and Dang, 2006) and the evaluation scripts available at their TREC website of years 2004, 2005 and 2006. The retrieval experiments were carried out using only a reduced set of documents from the AQUAINT corpus because the semantic role labelers tested were not able to parse the full set, unlike QPLM which parsed all documents successfully. The SRL tool SwiRL (Surdeanu and Turmo, 2005) has a good precision and coverage, however it is slow and quite unstable when parsing large amounts of data. We have assembled a cluster of computers in order to speed up the corpus annotation, but even when having around ten dedicated computers the estimated completion time was larger than one year. The lack of semantic annotators that can quickly evaluate large amount of data gave us the stimulus needed to use a simplified and quicker technique. We used the QPLM annotation tool which takes less than 3 weeks to fully annotate the 3GB of data from the AQUAINT corpus using a single machine. Si</context>
</contexts>
<marker>Surdeanu, Turmo, 2005</marker>
<rawString>Surdeanu, Mihai and Jordi Turmo. 2005. Semantic role labeling using complete syntactic analysis. In Proceedings of CoNLL 2005 Shared Task, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefanie Tellex</author>
<author>Boris Katz</author>
<author>Jimmy Lin</author>
<author>Aaron Fernandes</author>
<author>Gregory Marton</author>
</authors>
<title>Quantitative evaluation of passage retrieval algorithms for question answering.</title>
<date>2003</date>
<booktitle>In SIGIR ’03: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval,</booktitle>
<pages>41--47</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="22167" citStr="Tellex et al. (2003)" startWordPosition="3675" endWordPosition="3678">n between its components still depended on the overall system architecture, which makes the implementation of new modules for the system quite difficult. With the exception of AnswerFinder, all the QA systems received a retrieval set as a collection of snippets. This was based on the fact that these systems are based on Web Retrieval and they expect to receive documents in this format. We extracted for every document the 255 character window where more question words (non-stopwords) were found. The implementation of different ranking strategies for passage retrieval such as those described by Tellex et al. (2003) could improve the results for individual QA systems. However, a preliminary evaluation of the passage retrieval have shown us that the 255 character window with the current snippet construction method was enough to achieve near optimal performance on the document set used. The results obtained by the QA systems were processed using the answer regular expressions distributed by TREC. The numbers described in this study show the factoid score for correct answers. We have not used the exact answer because it required some cleaning of the answer log files and some modification of some QA systems.</context>
</contexts>
<marker>Tellex, Katz, Lin, Fernandes, Marton, 2003</marker>
<rawString>Tellex, Stefanie, Boris Katz, Jimmy Lin, Aaron Fernandes, and Gregory Marton. 2003. Quantitative evaluation of passage retrieval algorithms for question answering. In SIGIR ’03: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 41–47, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
<author>Hoa Trang Dang</author>
</authors>
<title>question answering track.</title>
<date>2006</date>
<journal>Overview of the TREC</journal>
<booktitle>In Text REtrieval Conference.</booktitle>
<contexts>
<context position="14328" citStr="Voorhees and Dang, 2006" startWordPosition="2370" endWordPosition="2373">t on the calculation of similarity than the components of BoW-V. With this approach, for queries with relations that are not indexed, the method is equivalent to a traditional BoW approach. 4 Experiments and Evaluation We have performed a series of experiments using the techniques described on Section 3 in order to verify the usefulness of QPLM in comparison to SRL based on PropBank. We compared both semantic annotations by using it with IR and under QA evaluation methods. 4.1 Configuration of experiments We performed experiments using data resources from the QA track of the TREC conferences (Voorhees and Dang, 2006) and the evaluation scripts available at their TREC website of years 2004, 2005 and 2006. The retrieval experiments were carried out using only a reduced set of documents from the AQUAINT corpus because the semantic role labelers tested were not able to parse the full set, unlike QPLM which parsed all documents successfully. The SRL tool SwiRL (Surdeanu and Turmo, 2005) has a good precision and coverage, however it is slow and quite unstable when parsing large amounts of data. We have assembled a cluster of computers in order to speed up the corpus annotation, but even when having around ten d</context>
</contexts>
<marker>Voorhees, Dang, 2006</marker>
<rawString>Voorhees, Ellen M. and Hoa Trang Dang. 2006. Overview of the TREC 2005 question answering track. In Text REtrieval Conference.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>