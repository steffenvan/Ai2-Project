<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.987047">
Evaluating the pairwise string alignment of pronunciations
</title>
<author confidence="0.988238">
Martijn Wieling Jelena Proki´c John Nerbonne
</author>
<affiliation confidence="0.939747">
University of Groningen University of Groningen University of Groningen
The Netherlands The Netherlands The Netherlands
</affiliation>
<email confidence="0.984798">
m.b.wieling@rug.nl j.prokic@rug.nl j.nerbonne@rug.nl
</email>
<sectionHeader confidence="0.99358" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999814571428571">
Pairwise string alignment (PSA) is an im-
portant general technique for obtaining a
measure of similarity between two strings,
used e.g., in dialectology, historical lin-
guistics, transliteration, and in evaluating
name distinctiveness. The current study
focuses on evaluating different PSA meth-
ods at the alignment level instead of via
the distances it induces. About 3.5 million
pairwise alignments of Bulgarian phonetic
dialect data are used to compare four al-
gorithms with a manually corrected gold
standard. The algorithms evaluated in-
clude three variants of the Levenshtein al-
gorithm as well as the Pair Hidden Markov
Model. Our results show that while all
algorithms perform very well and align
around 95% of all alignments correctly,
there are specific qualitative differences in
the (mis)alignments of the different algo-
rithms.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954982456141">
Our cultural heritage is not only accessible
through museums, libraries, archives and their
digital portals, it is alive and well in the varied
cultural habits practiced today by the various peo-
ples of the world. To research and understand this
cultural heritage we require instruments which are
sensitive to its signals, and, in particular sensitive
to signals of common provenance. The present
paper focuses on speech habits which even today
bear signals of common provenance in the vari-
ous dialects of the world’s languages, and which
have also been recorded and preserved in major
archives of folk culture internationally. We present
work in a research line which seeks to develop
digital instruments capable of detecting common
provenance among pronunciation habits, focusing
in this paper on the issue of evaluating the quality
of these instruments.
Pairwise string alignment (PSA) methods, like
the popular Levenshtein algorithm (Levenshtein,
1965) which uses insertions (alignments of a seg-
ment against a gap), deletions (alignments of a gap
against a segment) and substitutions (alignments
of two segments) often form the basis of deter-
mining the distance between two strings. Since
there are many alignment algorithms and specific
settings for each algorithm influencing the dis-
tance between two strings (Nerbonne and Klei-
weg, 2007), evaluation is very important in deter-
mining the effectiveness of the distance methods.
Determining the distance (or similarity) be-
tween two phonetic strings is an important aspect
of dialectometry, and alignment quality is impor-
tant in applications in which string alignment is
a goal in itself, for example, determining if two
words are likely to be cognate (Kondrak, 2003),
detecting confusable drug names (Kondrak and
Dorr, 2003), or determining whether a string is
the transliteration of the same name from another
writing system (Pouliquen, 2008).
In this paper we evaluate string distance mea-
sures on the basis of data from dialectology. We
therefore explain a bit more of the intended use of
the pronunciation distance measure.
Dialect atlases normally contain a large num-
ber of pronunciations of the same word in various
places throughout a language area. All pairs of
pronunciations of corresponding words are com-
pared in order to obtain a measure of the aggre-
gate linguistic distance between dialectal varieties
(Heeringa, 2004). It is clear that the quality of the
measurement is of crucial importance.
Almost all evaluation methods in dialectometry
focus on the aggregate results and ignore the in-
dividual word-pair distances and individual align-
ments on which the distances are based. The fo-
cus on the aggregate distance of 100 or so word
</bodyText>
<note confidence="0.952151333333333">
Proceedings of the EACL 2009 Workshop on Language Technology and Resources for Cultural Heritage,
Social Sciences, Humanities, and Education –LaTeCH – SHELT&amp;R 2009, pages 26–34,
Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.996878">
26
</page>
<bodyText confidence="0.999988133333334">
pairs effectively hides many differences between
methods. For example, Heeringa et al. (2006) find
no significant differences in the degrees to which
several pairwise string distance measures correlate
with perceptual distances when examined at an ag-
gregate level. Wieling et al. (2007) and Wieling
and Nerbonne (2007) also report almost no differ-
ence between different PSA algorithms at the ag-
gregate level. It is important to be able to evaluate
the different techniques more sensitively, which is
why this paper examines alignment quality at the
segment level.
Kondrak (2003) applies a PSA algorithm to
align words in different languages in order to de-
tect cognates automatically. Exceptionally, he
does provide an evaluation of the string alignments
generated by different algorithms. But he restricts
his examination to a set of only 82 gold standard
pairwise alignments and he only distinguishes cor-
rect and incorrect alignments and does not look at
misaligned phones.
In the current study we introduce and evaluate
several alignment algorithms more extensively at
the alignment level. The algorithms we evaluate
include the Levenshtein algorithm (with syllabic-
ity constraint), which is one of the most popular
alignment methods and has successfully been used
in determining pronunciation differences in pho-
netic strings (Kessler, 1995; Heeringa, 2004). In
addition we look at two adaptations of the Lev-
enshtein algorithm. The first adaptation includes
the swap-operation (Wagner and Lowrance, 1975),
while the second adaptation includes phonetic seg-
ment distances, which are generated by applying
an iterative pointwise mutual information (PMI)
procedure (Church and Hanks, 1990). Finally we
include alignments generated with the Pair Hid-
den Markov Model (PHMM) as introduced to lan-
guage studies by Mackay and Kondrak (2005).
They reported that the Pair Hidden Markov Model
outperformed ALINE, the best performing algo-
rithm at the alignment level in the aforementioned
study of Kondrak (2003). The PHMM has also
successfully been used in dialectology by Wieling
et al. (2007).
</bodyText>
<sectionHeader confidence="0.99523" genericHeader="introduction">
2 Dataset
</sectionHeader>
<bodyText confidence="0.9992524">
The dataset used in this study consists of 152
words collected from 197 sites equally distributed
over Bulgaria. The transcribed word pronuncia-
tions include diacritics and suprasegmentals (e.g.,
intonation). The total number of different phonetic
types (or segments) is 98.1
The gold standard pairwise alignment was au-
tomatically generated from a manually corrected
gold standard set of N multiple alignments (see
Proki´c et al., 2009 ) in the following way:
</bodyText>
<listItem confidence="0.970013266666667">
• Every individual string (including gaps) in
the multiple alignment is aligned with ev-
ery other string of the same word. With 152
words and 197 sites and in some cases more
than one pronunciations per site for a cer-
tain word, the total number of pairwise align-
ments is about 3.5 million.
• If a resulting pairwise alignment contains a
gap in both strings at the same position (a
gap-gap alignment), these gaps are removed
from the pairwise alignment. We justify this,
reasoning that no alignment algorithm may
be expected to detect parallel deletions in a
single pair of words. There is no evidence for
this in the single pair.
</listItem>
<bodyText confidence="0.93248">
To make this clear, consider the multiple align-
ment of three Bulgarian dialectal variants of the
word ‘I’ (as in ‘I am’):
</bodyText>
<equation confidence="0.687656">
j &apos;a s
&apos;a z i
j &apos;a
</equation>
<bodyText confidence="0.6712675">
Using the procedure above, the three generated
pairwise alignments are:
j &apos;a s
&apos;a z i
</bodyText>
<sectionHeader confidence="0.991834" genericHeader="method">
3 Algorithms
</sectionHeader>
<bodyText confidence="0.9929405">
Four algorithms are evaluated with respect to the
quality of their alignments, including three vari-
ants of the Levenshtein algorithm and the Pair
Hidden Markov Model.
</bodyText>
<subsectionHeader confidence="0.999872">
3.1 The VC-sensitive Levenshtein algorithm
</subsectionHeader>
<bodyText confidence="0.9999415">
The Levenshtein algorithm is a very efficient dy-
namic programming algorithm, which was first in-
troduced by Kessler (1995) as a tool for computa-
tionally comparing dialects. The Levenshtein dis-
tance between two strings is determined by count-
ing the minimum number of edit operations (i.e.
insertions, deletions and substitutions) needed to
transform one string into the other.
</bodyText>
<footnote confidence="0.99292">
1The dataset is available online at the website
http://www.bultreebank.org/BulDialects/
</footnote>
<equation confidence="0.3660765">
j &apos;a s &apos;a z i
j &apos;a j &apos;a
</equation>
<page confidence="0.991724">
27
</page>
<bodyText confidence="0.999679">
For example, the Levenshtein distance between
[j&amp;quot;As] and [&amp;quot;Azi], two Bulgarian dialectal variants
of the word ‘I’ (as in ‘I am’), is 3:
</bodyText>
<equation confidence="0.935236222222222">
j&amp;quot;As delete j 1
&amp;quot;As subst. s/z 1
&amp;quot;Az insert i 1
&amp;quot;Azi
3
The corresponding alignment is:
j &amp;quot;A s
&amp;quot;A z i
1 1 1
</equation>
<bodyText confidence="0.998322066666667">
The Levenshtein distance has been used fre-
quently and successfully in measuring linguis-
tic distances in several languages, including Irish
(Kessler, 1995), Dutch (Heeringa, 2004) and Nor-
wegian (Heeringa, 2004). Additionally, the Lev-
enshtein distance has been shown to yield aggre-
gate results that are consistent (Cronbach’s α =
0.99) and valid when compared to dialect speak-
ers judgements of similarity (r Pt� 0.7; Heeringa et
al., 2006).
Following Heeringa (2004), we have adapted
the Levenshtein algorithm slightly, so that it does
not allow alignments of vowels with consonants.
We refer to this adapted algorithm as the VC-
sensitive Levenshtein algorithm.
</bodyText>
<subsectionHeader confidence="0.9903185">
3.2 The Levenshtein algorithm with the swap
operation
</subsectionHeader>
<bodyText confidence="0.999938111111111">
Because metathesis (i.e. transposition of sounds)
occurs relatively frequently in the Bulgarian di-
alect data (in 21 of 152 words), we extend the
VC-sensitive Levenshtein algorithm as described
in section 3.1 to include the swap-operation (Wag-
ner and Lowrance, 1975), which allows two ad-
jacent characters to be interchanged. The swap-
operation is also known as a transposition, which
was introduced with respect to detecting spelling
errors by Damerau (1964). As a consequence the
Damerau distance refers to the minimum number
of insertions, deletions, substitutions and transpo-
sitions required to transform one string into the
other. In contrast to Wagner and Lowrance (1975)
and in line with Damerau (1964) we restrict the
swap operation to be only allowed for string X
and Y when xi = yi+1 and yi = xi+1 (with xi
being the token at position i in string X):
</bodyText>
<equation confidence="0.927475666666667">
xi xi+1
yi yi+1
&gt;&lt; 1
</equation>
<bodyText confidence="0.9999785">
Note that a swap-operation in the alignment is in-
dicated by the symbol ‘&gt;&lt;’. The first number fol-
lowing this symbol indicates the cost of the swap-
operation.
Consider the alignment of [vr&amp;quot;7] and [v&amp;quot;7r],2
two Bulgarian dialectal variants of the word ‘peak’
(mountain). The alignment involves a swap and
results in a total Levenshtein distance of 1:
</bodyText>
<equation confidence="0.784215">
v r &amp;quot;7
v &amp;quot;7 r
&gt;&lt; 1
</equation>
<bodyText confidence="0.99716075">
However, the alignment of the transcription [vr&amp;quot;7]
with another dialectal transcription [v&amp;quot;ar] does not
allow a swap and yields a total Levenshtein dis-
tance of 2:
</bodyText>
<equation confidence="0.978865">
v r &amp;quot;7
v &amp;quot;a r
1 1
</equation>
<bodyText confidence="0.999864444444444">
Including just the option of swapping identical
segments in the implementation of the Leven-
shtein algorithm is relatively easy. We set the
cost of the swap operation to one3 plus twice the
cost of substituting xi with yi+1 plus twice the
cost of substituting yi with xi+1. In this way the
swap operation will be preferred when xi = yi+1
and yi = xi+1, but not when xi =� yi+1 and/or
yi =� xi+1. In the first case the cost of the swap
operation is 1, which is less than the cost of the
alternative of two substitutions. In the second case
the cost is either 3 (if xi =� yi+1 or yi =� xi+1) or
5 (if xi =� yi+1 and yi =� xi+1), which is higher
than the cost of using insertions, deletions and/or
substitutions.
Just as in the previous section, we do not allow
vowels to align with consonants (except in the case
of a swap).
</bodyText>
<subsectionHeader confidence="0.994699">
3.3 The Levenshtein algorithm with
generated segment distances
</subsectionHeader>
<bodyText confidence="0.999566166666667">
The VC-sensitive Levenshtein algorithm as de-
scribed in section 3.1 only distinguishes between
vowels and consonants. However, more sensi-
tive segment distances are also possible. Heeringa
(2004) experimented with specifying phonetic
segment distances based on phonetic features and
</bodyText>
<footnote confidence="0.924546428571429">
2We use transcriptions in which stress is marked on
stressed vowels instead of before stressed syllables. We fol-
low in this the Bulgarian convention instead of the IPA con-
vention.
3Actually the cost is set to 0.999 to prefer an alignment
involving a swap over an alternative alignment involving only
regular edit operations.
</footnote>
<page confidence="0.998557">
28
</page>
<bodyText confidence="0.999794166666667">
also based on acoustic differences derived from
spectrograms, but he did not obtain improved re-
sults at the aggregate level.
Instead of using segment distances as these are
(incompletely) suggested by phonetic or phono-
logical theory, we tried to determine the sound
distances automatically based on the available
data. We used pointwise mutual information
(PMI; Church and Hanks, 1990) to obtain these
distances. It generates segment distances by as-
sessing the degree of statistical dependence be-
tween the segments x and y:
</bodyText>
<equation confidence="0.908806333333333">
PMI(x, y) = lo92AX, y) (1)
Ux) p(y)
Where:
</equation>
<listItem confidence="0.715826">
• p(x, y): the number of times x and y occur
at the same position in two aligned strings
X and Y , divided by the total number of
aligned segments (i.e. the relative occurrence
of the aligned segments x and y in the whole
dataset). Note that either x or y can be a gap
in the case of insertion or deletion.
• p(x) and p(y): the number of times x (or y)
occurs, divided by the total number of seg-
ment occurrences (i.e. the relative occurrence
of x or y in the whole dataset). Dividing by
this term normalizes the empirical frequency
with respect to the frequency expected if x
and y are statistically independent.
</listItem>
<bodyText confidence="0.999666214285714">
The greater the PMI value, the more segments tend
to cooccur in correspondences. Negative PMI val-
ues indicate that segments do not tend to cooccur
in correspondences, while positive PMI values in-
dicate that segments tend to cooccur in correspon-
dences. The segment distances can therefore be
generated by subtracting the PMI value from 0 and
adding the maximum PMI value (i.e. lowest dis-
tance is 0). In that way corresponding segments
obtain the lowest distance.
Based on the PMI value and its conversion to
segment distances, we developed an iterative pro-
cedure to automatically obtain the segment dis-
tances:
</bodyText>
<footnote confidence="0.733569666666667">
1. The string alignments are generated using the
VC-sensitive Levenshtein algorithm (see sec-
tion 3.1).4
4We also used the Levenshtein algorithm without the
vowel-consonant restriction to generate the PMI values, but
this had a negative effect on the performance.
</footnote>
<listItem confidence="0.998733444444444">
2. The PMI value for every segment pair is cal-
culated according to (1) and subsequently
transformed to a segment distance by sub-
tracting it from zero and adding the maxi-
mum PMI value.
3. The Levenshtein algorithm using these seg-
ment distances is applied to generate a new
set of alignments.
4. Step 2 and 3 are repeated until the alignments
</listItem>
<bodyText confidence="0.903965">
of two consecutive iterations do not differ
(i.e. convergence is reached).
The potential merit of using PMI-generated seg-
ment distances can be made clear by the following
example. Consider the strings [v&amp;quot;7n] and [v&amp;quot;7ïk@],
Bulgarian dialectal variants of the word ‘outside’.
The VC-sensitive Levenshtein algorithm yields
the following (correct) alignment:
</bodyText>
<equation confidence="0.921247">
v &amp;quot;7 n
v &amp;quot;7 ï k @
1 1 1
</equation>
<bodyText confidence="0.662135">
But also the alternative (incorrect) alignment:
</bodyText>
<equation confidence="0.948956666666667">
v &amp;quot;7 n
v &amp;quot;7 ï k @
1 1 1
</equation>
<bodyText confidence="0.99979495">
The VC-sensitive Levenshtein algorithm gener-
ates the erroneous alignment because it has no way
to identify that the consonant [n] is nearer to the
consonant [ï] than to the consonant [k]. In con-
trast, the Levenshtein algorithm which uses the
PMI-generated segment distances only generates
the correct first alignment, because the [n] occurs
relatively more often aligned with [ï] than with
[k] so that the distance between [n] and [ï] will
be lower than the distance between [n] and [k].
The idea behind this procedure is similar to Ris-
tad’s suggestion to learn segment distances for edit
distance using an expectation maximization algo-
rithm (Ristad and Yianilos, 1998). Our approach
differs from their approach in that we only learn
segment distances based on the alignments gener-
ated by the VC-sensitive Levenshtein algorithm,
while Ristad and Yianilos (1998) learn segment
distances by considering all possible alignments of
two strings.
</bodyText>
<subsectionHeader confidence="0.991497">
3.4 The Pair Hidden Markov Model
</subsectionHeader>
<bodyText confidence="0.998599666666667">
The Pair Hidden Markov Model (PHMM) also
generates alignments based on automatically gen-
erated segment distances and has been used suc-
</bodyText>
<page confidence="0.997842">
29
</page>
<figureCaption confidence="0.991947">
Figure 1: Pair Hidden Markov Model. Image
courtesy of Mackay and Kondrak (2005).
</figureCaption>
<bodyText confidence="0.9682432">
cessfully in language studies (Mackay and Kon-
drak, 2005; Wieling et al., 2007).
A Hidden Markov Model (HMM) is a proba-
bilistic finite-state transducer that generates an ob-
servation sequence by starting in an initial state,
going from state to state based on transition prob-
abilities and emitting an output symbol in each
state based on the emission probabilities in that
state for that output symbol (Rabiner, 1989). The
PHMM was originally proposed by Durbin et al.
(1998) for aligning biological sequences and was
first used in linguistics by Mackay and Kondrak
(2005) to identify cognates. The PHMM differs
from the regular HMM in that it outputs two ob-
servation streams (i.e. a series of alignments of
pairs of individual segments) instead of only a se-
ries of single symbols. The PHMM displayed in
Figure 1 has three emitting states: the substitution
(‘match’) state (M) which emits two aligned sym-
bols, the insertion state (Y) which emits a symbol
and a gap, and the deletion state (X) which emits
a gap and a symbol.
The following example shows the state se-
quence for the pronunciations [j&apos;as] and [&apos;azi] (En-
glish ‘I’):
</bodyText>
<equation confidence="0.811845">
j &apos;a s
&apos;a z i
X M M Y
</equation>
<bodyText confidence="0.999973861111111">
Before generating the alignments, all probabil-
ities of the PHMM have to be estimated. These
probabilities consist of the 5 transition probabili-
ties shown in Figure 1: e, A, 6, TX,. and TM. In
addition there are 98 emission probabilities for the
insertion state and the deletion state (one for ev-
ery segment) and 9604 emission probabilities for
the substitution state. The probability of starting in
one of the three states is set equal to the probability
of going from the substitution state to that particu-
lar state. The Baum-Welch expectation maximiza-
tion algorithm (Baum et al., 1970) can be used to
iteratively reestimate these probabilities until a lo-
cal optimum is found.
To prevent order effects in training, every word
pair is considered twice (e.g., wa − wb and wb −
wa). The resulting insertion and deletion probabil-
ities are therefore the same (for each segment), and
the probability of substituting x for y is equal to
the probability of substituting y for x, effectively
yielding 4802 distinct substitution probabilities.
Wieling et al. (2007) showed that using Dutch
dialect data for training, sensible segment dis-
tances were obtained; acoustic vowel distances
on the basis of spectrograms correlated signifi-
cantly (r = −0.72) with the vowel substitution
probabilities of the PHMM. Additionally, proba-
bilities of substituting a symbol with itself were
much higher than the probabilities of substitut-
ing an arbitrary vowel with another non-identical
vowel (mutatis mutandis for consonants), which
were in turn much higher than the probabilities of
substituting a vowel for a consonant.
After training, the well known Viterbi algorithm
can be used to obtain the best alignments (Rabiner,
1989).
</bodyText>
<sectionHeader confidence="0.998873" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.996000611111111">
As described in section 2, we use the generated
pairwise alignments from a gold standard of multi-
ple alignments for evaluation. In addition, we look
at the performance of a baseline of pairwise align-
ments, which is constructed by aligning the strings
according to the Hamming distance (i.e. only al-
lowing substitutions and no insertions or deletions;
Hamming, 1950).
The evaluation procedure consists of comparing
the alignments of the previously discussed algo-
rithms including the baseline with the alignments
of the gold standard. For the comparison we use
the standard Levenshtein algorithm without any
restrictions. The evaluation proceeds as follows:
1. The pairwise alignments of the four algo-
rithms, the baseline and the gold standard are
generated and standardized (see section 4.1).
When multiple equal-scoring alignments are
</bodyText>
<page confidence="0.99153">
30
</page>
<listItem confidence="0.906089090909091">
generated by an algorithm, only one (i.e. the
final) alignment is selected.
2. In each alignment, we convert each pair of
aligned segments to a single token, so that ev-
ery alignment of two strings is converted to a
single string of segment pairs.
3. For every algorithm these transformed strings
are aligned with the transformed strings of
the gold standard using the standard Leven-
shtein algorithm.
4. The Levenshtein distances for all these
</listItem>
<bodyText confidence="0.909695291666667">
strings are summed up resulting in the total
distance between every alignment algorithm
and the gold standard. Only if individual
segments match completely the segment dis-
tance is 0, otherwise it is 1.
To illustrate this procedure, consider the following
gold standard alignment of [vl&amp;quot;7k] and [v&amp;quot;7lk], two
Bulgarian dialectal variants of the word ‘wolf’:
v l &amp;quot;7 k
v &amp;quot;7 l k
Every aligned segment pair is converted to a single
token by adding the symbol ‘/’ between the seg-
ments and using the symbol ‘-’ to indicate a gap.
This yields the following transformed string:
v/v l/&amp;quot;7 &amp;quot;7/l k/k
Suppose another algorithm generates the follow-
ing alignment (not detecting the swap):
v l &amp;quot;7 k
v &amp;quot;7 l k
The transformed string for this alignment is:
v/v l/- &amp;quot;7/&amp;quot;7 -/l k/k
To evaluate this alignment, we align this string to
the transformed string of the gold standard and ob-
tain a Levenshtein distance of 3:
</bodyText>
<equation confidence="0.846398333333333">
v/v l/&amp;quot;7 &amp;quot;7/l k/k
v/v l/- &amp;quot;7/&amp;quot;7 -/l k/k
1 1 1
</equation>
<bodyText confidence="0.999872">
By repeating this procedure for all alignments and
summing up all distances, we obtain total dis-
tances between the gold standard and every align-
ment algorithm. Algorithms which generate high-
quality alignments will have a low distance from
the gold standard, while the distance will be higher
for algorithms which generate low-quality align-
ments.
</bodyText>
<subsectionHeader confidence="0.995836">
4.1 Standardization
</subsectionHeader>
<bodyText confidence="0.999891529411765">
The gold standard contains a number of align-
ments which have alternative equivalent align-
ments, most notably an alignment containing an
insertion followed by a deletion (which is equal
to the deletion followed by the insertion), or an
alignment containing a syllabic consonant such as
[&amp;quot;ô&amp;quot;], which in fact matches both a vowel and a
neighboring r-like consonant and can therefore be
aligned with either the vowel or the consonant. In
order to prevent punishing the algorithms which
do not match the exact gold standard in these
cases, the alignments of the gold standard and all
alignment algorithms are transformed to one stan-
dard form in all relevant cases.
For example, consider the correct alignment of
[v&amp;quot;iA] and [v&amp;quot;ij], two Bulgarian dialectal variations
of the English plural pronoun ‘you’:
</bodyText>
<equation confidence="0.8828414">
v &amp;quot;i A
v &amp;quot;i j
Of course, this alignment is as reasonable as:
v &amp;quot;i A
v &amp;quot;i j
</equation>
<bodyText confidence="0.978701">
To avoid punishing the first, we transform all in-
sertions followed by deletions to deletions fol-
lowed by insertions, effectively scoring the two
alignments the same.
For the syllabic consonants we transform all
alignments to a form in which the syllabic con-
sonant is followed by a gap and not vice versa.
For instance, aligning [v&amp;quot;ô&amp;quot;x] with [v&amp;quot;Arx] (English:
‘peak’) yields:
</bodyText>
<equation confidence="0.9813906">
v &amp;quot;ô&amp;quot; x
v &amp;quot;A r x
Which is transformed to the equivalent alignment:
v&amp;quot;ô&amp;quot; x
v&amp;quot;A r x
</equation>
<sectionHeader confidence="0.999701" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.9999516">
We will report both quantitative results using the
evaluation method discussed in the previous sec-
tion, as well as the qualitative results, where we
focus on characteristic errors of the different align-
ment algorithms.
</bodyText>
<subsectionHeader confidence="0.999016">
5.1 Quantitative results
</subsectionHeader>
<bodyText confidence="0.999973">
Because there are two algorithms which use gen-
erated segment distances (or probabilities) in their
alignments, we first check if these values are sen-
sible and comparable to each other.
</bodyText>
<page confidence="0.999658">
31
</page>
<subsectionHeader confidence="0.779581">
5.1.1 Comparison of segment distances
</subsectionHeader>
<bodyText confidence="0.999966057142857">
With respect to the PMI results (convergence
was reached after 7 iterations, taking less than
5 CPU minutes), we indeed found sensible re-
sults: the average distance between identical sym-
bols was significantly lower than the distance be-
tween pairs of different vowels and consonants
(t &lt; −13, p &lt; .001). Because we did not allow
vowel-consonants alignments in the Levenshtein
algorithm, no PMI values were generated for those
segment pairs.
Just as Wieling et al. (2007), we found sen-
sible PHMM substitution probabilities (conver-
gence was reached after 1675 iterations, taking
about 7 CPU hours): the probability of matching
a symbol with itself was significantly higher than
the probability of substituting one vowel for an-
other (similarly for consonants), which in turn was
higher than the probability of substituting a vowel
with a consonant (all t’s &gt; 9, p &lt; .001).
To allow a fair comparison between the PHMM
probabilities and the PMI distances, we trans-
formed the PHMM probabilities to log-odds
scores (i.e. dividing the probability by the rela-
tive frequency of the segments and subsequently
taking the log). Because the residues after the
linear regression between the PHMM similarities
and PMI distances were not normally distributed,
we used Spearman’s rank correlation coefficient
to assess the relationship between the two vari-
ables. We found a highly significant Spearman’s
p = −.965 (p &lt; .001), which means that the re-
lationship between the PHMM similarities and the
PMI distances is very strong. When looking at the
insertions and deletions we also found a significant
relationship: Spearman’s p = −.736 (p &lt; .001).
</bodyText>
<subsubsectionHeader confidence="0.968895">
5.1.2 Evaluation against the gold standard
</subsubsectionHeader>
<bodyText confidence="0.99997476">
Using the procedure described in section 4, we cal-
culated the distances between the gold standard
and the alignment algorithms. Besides reporting
the total number of misaligned tokens, we also di-
vided this number by the total number of aligned
segments in the gold standard (about 16 million)
to get an idea of the error rate. Note that the error
rate is 0 in the perfect case, but might rise to nearly
2 in the worst case, which is an alignment consist-
ing of only insertions and deletions and therefore
up to twice as long as the alignments in the gold
standard. Finally, we also report the total number
of alignments (word pairs) which are not exactly
equal to the alignments of the gold standard.
The results are shown in Table 1. We can
clearly see that all algorithms beat the baseline
and align about 95% of all string pairs correctly.
While the Levenshtein PMI algorithm aligns most
strings perfectly, it misaligns slightly more indi-
vidual segments than the PHMM and the Leven-
shtein algorithm with the swap operation (i.e. it
makes more segment alignment errors per word
pair). The VC-sensitive Levenshtein algorithm
in general performs slightly worse than the other
three algorithms.
</bodyText>
<subsectionHeader confidence="0.999855">
5.2 Qualitative results
</subsectionHeader>
<bodyText confidence="0.999944894736842">
Let us first note that it is almost impossible for
any algorithm to achieve a perfect overlap with the
gold standard, because the gold standard was gen-
erated from multiple alignments and therefore in-
corporates other constraints. For example, while a
certain pairwise alignment could appear correct in
aligning two consonants, the multiple alignment
could show contextual support (from pronuncia-
tions in other varieties) for separating the conso-
nants. Consequently, all algorithms discussed be-
low make errors of this kind.
In general, the specific errors of the VC-
sensitive Levenshtein algorithm can be separated
into three cases. First, as we illustrated in section
3.3, the VC-sensitive Levenshtein algorithm has
no way to distinguish between aligning a conso-
nant with one of two neighboring consonants and
sometimes chooses the wrong one (this also holds
for vowels). Second, it does not allow alignments
of vowels with consonants and therefore cannot
detect correct vowel-consonant alignments such as
correspondences of [u] with [v] initially. Third,
for the same reason the VC-sensitive Levenshtein
algorithm is also not able to detect metathesis of
vowels with consonants.
The misalignments of the Levenshtein algo-
rithm with the swap-operation can also be split in
three cases. It suffers from the same two prob-
lems as the VC-sensitive Levenshtein algorithm in
choosing to align a consonant incorrectly with one
of two neighboring consonants and not being able
to align a vowel with a consonant. Third, even
though it aligns some of the metathesis cases cor-
rectly, it also makes some errors by incorrectly ap-
plying the swap-operation. For example, consider
the alignment of [s&apos;irjmi] and [s&apos;irjm], two Bul-
garian dialectal variations of the word ‘cheese’, in
which the swap-operation is applied:
</bodyText>
<page confidence="0.997048">
32
</page>
<table confidence="0.999514">
Algorithm Misaligned segments (error rate) Incorrect alignments (%)
Baseline (Hamming algorithm) 2510094 (0.1579) 726844 (20.92%)
VC-sens. Levenshtein algorithm 490703 (0.0309) 191674 (5.52%)
Levenshtein PMI algorithm 399216 (0.0251) 156440 (4.50%)
Levenshtein swap algorithm 392345 (0.0247) 161834 (4.66%)
Pair Hidden Markov Model 362423 (0.0228) 160896 (4.63%)
</table>
<tableCaption confidence="0.747368333333333">
Table 1: Comparison to gold standard alignments. All differences are significant (p &lt; 0.01).
s i rj I n i
s i rj n I
</tableCaption>
<bodyText confidence="0.89504325">
0 0 0 &gt;&lt; 1 1
However, the two I’s are not related and should not
be swapped, which is reflected in the gold standard
alignment:
</bodyText>
<equation confidence="0.859309">
s i rj I n i
s i rj n I
0 0 0 1 0 1
</equation>
<bodyText confidence="0.999861954545455">
The incorrect alignments of the Levenshtein
algorithm with the PMI-generated segment dis-
tances are mainly caused by its inability to align
vowels with consonants and therefore, just as the
VC-sensitive Levenshtein algorithm, it fails to de-
tect metathesis. On the other hand, using seg-
ment distances often solves the problem of select-
ing which of two plausible neighbors a consonant
should be aligned with.
Because the PHMM employs segment substi-
tution probabilities, it also often solves the prob-
lem of aligning a consonant to one of two neigh-
bors. In addition, the PHMM often correctly
aligns metathesis involving equal as well as sim-
ilar symbols, even realizing an improvement over
the Levenshtein swap algorithm. Unfortunately,
many wrong alignments of the PHMM are also
caused by allowing vowel-consonant alignments.
Since the PHMM does not take context into ac-
count, it also aligns vowels and consonants which
often play a role in metathesis when no metathesis
is involved.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999982138888889">
This study provides an alternative evaluation of
string distance algorithms by focusing on their ef-
fectiveness in aligning segments. We proposed,
implemented, and tested the new procedure on a
substantial body of data. This provides a new per-
spective on the quality of distance and alignment
algorithms as they have been used in dialectology,
where aggregate comparisons had been at times
frustratingly inconclusive.
In addition, we introduced the PMI weight-
ing within the Levenshtein algorithm as a sim-
ple means of obtaining segment distances, and
showed that it improves on the popular Leven-
shtein algorithm with respect to alignment accu-
racy.
While the results indicated that the PHMM mis-
aligned the fewest segments, training the PHMM
is a lengthy process lasting several hours. Con-
sidering that the Levenshtein algorithm with the
swap operation and the Levenshtein algorithm
with the PMI-generated segment distances are
much quicker to (train and) apply, and that they
have only slightly lower performance with respect
to the segment alignments, we actually prefer us-
ing those methods. Another argument in favor of
using one of these Levenshtein algorithms is that
it is a priori clearer what type of alignment errors
to expect from them, while the PHMM algorithm
is less predictable and harder to comprehend.
While our results are an indication of the good
quality of the evaluated algorithms, we only evalu-
ated the algorithms on a single dataset for which a
gold standard was available. Ideally we would like
to verify these results on other datasets, for which
gold standards consisting of multiple or pairwise
alignments are available.
</bodyText>
<sectionHeader confidence="0.99802" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999711444444444">
We are grateful to Peter Kleiweg for extending the
Levenshtein algorithm in the L04 package with the
swap-operation. We also thank Greg Kondrak for
providing the original source code of the Pair Hid-
den Markov Models. Finally, we thank Therese
Leinonen and Sebastian K¨urschner of the Univer-
sity of Groningen and Esteve Valls i Alecha of the
University of Barcelona for their useful feedback
on our ideas.
</bodyText>
<page confidence="0.998812">
33
</page>
<sectionHeader confidence="0.998333" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999826183908046">
Leonard E. Baum, Ted Petrie, George Soules, and Nor-
man Weiss. 1970. A maximization technique occur-
ring in the statistical analysis of probabilistic func-
tions of Markov Chains. The Annals of Mathemati-
cal Statistics, 41(1):164–171.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1):22–29.
Fred J. Damerau. 1964. A technique for computer de-
tection and correction of spelling errors. Communi-
cations of the ACM, 7:171–176.
Richard Durbin, Sean R. Eddy, Anders Krogh, and
Graeme Mitchison. 1998. Biological Sequence
Analysis: Probabilistic Models of Proteins and Nu-
cleic Acids. Cambridge University Press, United
Kingdom, July.
Richard Hamming. 1950. Error detecting and error
correcting codes. Bell System Technical Journal,
29:147–160.
Wilbert Heeringa, Peter Kleiweg, Charlotte Gooskens,
and John Nerbonne. 2006. Evaluation of string dis-
tance algorithms for dialectology. In John Nerbonne
and Erhard Hinrichs, editors, Linguistic Distances,
pages 51–62, Shroudsburg, PA. ACL.
Wilbert Heeringa. 2004. Measuring Dialect Pronunci-
ation Differences using Levenshtein Distance. Ph.D.
thesis, Rijksuniversiteit Groningen.
Brett Kessler. 1995. Computational dialectology in
Irish Gaelic. In Proceedings of the seventh con-
ference on European chapter of the Association for
Computational Linguistics, pages 60–66, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.
Grzegorz Kondrak and Bonnie Dorr. 2003. Identifica-
tion of Confusable Drug Names: A New Approach
and Evaluation Methodology. Artificial Intelligence
in Medicine, 36:273–291.
Grzegorz Kondrak. 2003. Phonetic Alignment and
Similarity. Computers and the Humanities, 37:273–
291.
Vladimir Levenshtein. 1965. Binary codes capable of
correcting deletions, insertions and reversals. Dok-
lady Akademii Nauk SSSR, 163:845–848.
Wesley Mackay and Grzegorz Kondrak. 2005. Com-
puting word similarity and identifying cognates with
Pair Hidden Markov Models. In Proceedings of
the 9th Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 40–47, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
John Nerbonne and Peter Kleiweg. 2007. Toward a di-
alectological yardstick. Journal of Quantitative Lin-
guistics, 14:148–167.
Bruno Pouliquen. 2008. Similarity of names across
scripts: Edit distance using learned costs of N-
Grams. In Bent Nordstr¨om and Aarne Ranta, ed-
itors, Proceedings of the 6th international Con-
ference on Natural Language Processing (Go-
Tal’2008), volume 5221, pages 405–416.
Jelena Proki´c, Martijn Wieling, and John Nerbonne.
2009. Multiple sequence alignments in linguistics.
In Piroska Lendvai and Lars Borin, editors, Proceed-
ings of the EACL 2009 Workshop on Language Tech-
nology and Resources for Cultural Heritage, Social
Sciences, Humanities, and Education.
Lawrence R. Rabiner. 1989. A tutorial on Hidden
Markov Models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2):257–
286.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learn-
ing string-edit distance. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 20:522–
532.
Robert Wagner and Roy Lowrance. 1975. An exten-
sion of the string-to-string correction problem. Jour-
nal of the ACM, 22(2):177–183.
Martijn Wieling and John Nerbonne. 2007. Dialect
pronunciation comparison and spoken word recog-
nition. In Petya Osenova, editor, Proceedings of
the RANLP Workshop on Computational Phonology,
pages 71–78.
Martijn Wieling, Therese Leinonen, and John Ner-
bonne. 2007. Inducing sound segment differences
using Pair Hidden Markov Models. In Mark Ellison
John Nerbonne and Greg Kondrak, editors, Comput-
ing and Historical Phonology: 9th Meeting of the
ACL Special Interest Group for Computational Mor-
phology and Phonology, pages 48–56.
</reference>
<page confidence="0.999308">
34
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.731500">
<title confidence="0.999557">Evaluating the pairwise string alignment of pronunciations</title>
<author confidence="0.999964">Martijn Wieling Jelena Proki´c John Nerbonne</author>
<affiliation confidence="0.999895">University of Groningen University of Groningen University of Groningen</affiliation>
<address confidence="0.904356">The Netherlands The Netherlands The Netherlands</address>
<email confidence="0.970229">m.b.wieling@rug.nlj.prokic@rug.nlj.nerbonne@rug.nl</email>
<abstract confidence="0.991867363636364">Pairwise string alignment (PSA) is an important general technique for obtaining a measure of similarity between two strings, used e.g., in dialectology, historical linguistics, transliteration, and in evaluating name distinctiveness. The current study focuses on evaluating different PSA methods at the alignment level instead of via the distances it induces. About 3.5 million pairwise alignments of Bulgarian phonetic dialect data are used to compare four algorithms with a manually corrected gold standard. The algorithms evaluated include three variants of the Levenshtein algorithm as well as the Pair Hidden Markov Model. Our results show that while all algorithms perform very well and align around 95% of all alignments correctly, there are specific qualitative differences in the (mis)alignments of the different algorithms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Leonard E Baum</author>
<author>Ted Petrie</author>
<author>George Soules</author>
<author>Norman Weiss</author>
</authors>
<title>A maximization technique occurring in the statistical analysis of probabilistic functions of Markov Chains.</title>
<date>1970</date>
<journal>The Annals of Mathematical Statistics,</journal>
<volume>41</volume>
<issue>1</issue>
<contexts>
<context position="17861" citStr="Baum et al., 1970" startWordPosition="2915" endWordPosition="2918">sh ‘I’): j &apos;a s &apos;a z i X M M Y Before generating the alignments, all probabilities of the PHMM have to be estimated. These probabilities consist of the 5 transition probabilities shown in Figure 1: e, A, 6, TX,. and TM. In addition there are 98 emission probabilities for the insertion state and the deletion state (one for every segment) and 9604 emission probabilities for the substitution state. The probability of starting in one of the three states is set equal to the probability of going from the substitution state to that particular state. The Baum-Welch expectation maximization algorithm (Baum et al., 1970) can be used to iteratively reestimate these probabilities until a local optimum is found. To prevent order effects in training, every word pair is considered twice (e.g., wa − wb and wb − wa). The resulting insertion and deletion probabilities are therefore the same (for each segment), and the probability of substituting x for y is equal to the probability of substituting y for x, effectively yielding 4802 distinct substitution probabilities. Wieling et al. (2007) showed that using Dutch dialect data for training, sensible segment distances were obtained; acoustic vowel distances on the basis</context>
</contexts>
<marker>Baum, Petrie, Soules, Weiss, 1970</marker>
<rawString>Leonard E. Baum, Ted Petrie, George Soules, and Norman Weiss. 1970. A maximization technique occurring in the statistical analysis of probabilistic functions of Markov Chains. The Annals of Mathematical Statistics, 41(1):164–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="5778" citStr="Church and Hanks, 1990" startWordPosition="870" endWordPosition="873">vely at the alignment level. The algorithms we evaluate include the Levenshtein algorithm (with syllabicity constraint), which is one of the most popular alignment methods and has successfully been used in determining pronunciation differences in phonetic strings (Kessler, 1995; Heeringa, 2004). In addition we look at two adaptations of the Levenshtein algorithm. The first adaptation includes the swap-operation (Wagner and Lowrance, 1975), while the second adaptation includes phonetic segment distances, which are generated by applying an iterative pointwise mutual information (PMI) procedure (Church and Hanks, 1990). Finally we include alignments generated with the Pair Hidden Markov Model (PHMM) as introduced to language studies by Mackay and Kondrak (2005). They reported that the Pair Hidden Markov Model outperformed ALINE, the best performing algorithm at the alignment level in the aforementioned study of Kondrak (2003). The PHMM has also successfully been used in dialectology by Wieling et al. (2007). 2 Dataset The dataset used in this study consists of 152 words collected from 197 sites equally distributed over Bulgaria. The transcribed word pronunciations include diacritics and suprasegmentals (e.g</context>
<context position="12475" citStr="Church and Hanks, 1990" startWordPosition="1999" endWordPosition="2002">ssed syllables. We follow in this the Bulgarian convention instead of the IPA convention. 3Actually the cost is set to 0.999 to prefer an alignment involving a swap over an alternative alignment involving only regular edit operations. 28 also based on acoustic differences derived from spectrograms, but he did not obtain improved results at the aggregate level. Instead of using segment distances as these are (incompletely) suggested by phonetic or phonological theory, we tried to determine the sound distances automatically based on the available data. We used pointwise mutual information (PMI; Church and Hanks, 1990) to obtain these distances. It generates segment distances by assessing the degree of statistical dependence between the segments x and y: PMI(x, y) = lo92AX, y) (1) Ux) p(y) Where: • p(x, y): the number of times x and y occur at the same position in two aligned strings X and Y , divided by the total number of aligned segments (i.e. the relative occurrence of the aligned segments x and y in the whole dataset). Note that either x or y can be a gap in the case of insertion or deletion. • p(x) and p(y): the number of times x (or y) occurs, divided by the total number of segment occurrences (i.e. </context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth W. Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred J Damerau</author>
</authors>
<title>A technique for computer detection and correction of spelling errors.</title>
<date>1964</date>
<journal>Communications of the ACM,</journal>
<pages>7--171</pages>
<contexts>
<context position="9626" citStr="Damerau (1964)" startWordPosition="1505" endWordPosition="1506">of vowels with consonants. We refer to this adapted algorithm as the VCsensitive Levenshtein algorithm. 3.2 The Levenshtein algorithm with the swap operation Because metathesis (i.e. transposition of sounds) occurs relatively frequently in the Bulgarian dialect data (in 21 of 152 words), we extend the VC-sensitive Levenshtein algorithm as described in section 3.1 to include the swap-operation (Wagner and Lowrance, 1975), which allows two adjacent characters to be interchanged. The swapoperation is also known as a transposition, which was introduced with respect to detecting spelling errors by Damerau (1964). As a consequence the Damerau distance refers to the minimum number of insertions, deletions, substitutions and transpositions required to transform one string into the other. In contrast to Wagner and Lowrance (1975) and in line with Damerau (1964) we restrict the swap operation to be only allowed for string X and Y when xi = yi+1 and yi = xi+1 (with xi being the token at position i in string X): xi xi+1 yi yi+1 &gt;&lt; 1 Note that a swap-operation in the alignment is indicated by the symbol ‘&gt;&lt;’. The first number following this symbol indicates the cost of the swapoperation. Consider the alignme</context>
</contexts>
<marker>Damerau, 1964</marker>
<rawString>Fred J. Damerau. 1964. A technique for computer detection and correction of spelling errors. Communications of the ACM, 7:171–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Durbin</author>
<author>Sean R Eddy</author>
<author>Anders Krogh</author>
<author>Graeme Mitchison</author>
</authors>
<title>Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids.</title>
<date>1998</date>
<publisher>Cambridge University Press,</publisher>
<location>United Kingdom,</location>
<contexts>
<context position="16599" citStr="Durbin et al. (1998)" startWordPosition="2694" endWordPosition="2697">automatically generated segment distances and has been used suc29 Figure 1: Pair Hidden Markov Model. Image courtesy of Mackay and Kondrak (2005). cessfully in language studies (Mackay and Kondrak, 2005; Wieling et al., 2007). A Hidden Markov Model (HMM) is a probabilistic finite-state transducer that generates an observation sequence by starting in an initial state, going from state to state based on transition probabilities and emitting an output symbol in each state based on the emission probabilities in that state for that output symbol (Rabiner, 1989). The PHMM was originally proposed by Durbin et al. (1998) for aligning biological sequences and was first used in linguistics by Mackay and Kondrak (2005) to identify cognates. The PHMM differs from the regular HMM in that it outputs two observation streams (i.e. a series of alignments of pairs of individual segments) instead of only a series of single symbols. The PHMM displayed in Figure 1 has three emitting states: the substitution (‘match’) state (M) which emits two aligned symbols, the insertion state (Y) which emits a symbol and a gap, and the deletion state (X) which emits a gap and a symbol. The following example shows the state sequence for</context>
</contexts>
<marker>Durbin, Eddy, Krogh, Mitchison, 1998</marker>
<rawString>Richard Durbin, Sean R. Eddy, Anders Krogh, and Graeme Mitchison. 1998. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids. Cambridge University Press, United Kingdom, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Hamming</author>
</authors>
<title>Error detecting and error correcting codes.</title>
<date>1950</date>
<journal>Bell System Technical Journal,</journal>
<pages>29--147</pages>
<contexts>
<context position="19359" citStr="Hamming, 1950" startWordPosition="3153" endWordPosition="3154">utatis mutandis for consonants), which were in turn much higher than the probabilities of substituting a vowel for a consonant. After training, the well known Viterbi algorithm can be used to obtain the best alignments (Rabiner, 1989). 4 Evaluation As described in section 2, we use the generated pairwise alignments from a gold standard of multiple alignments for evaluation. In addition, we look at the performance of a baseline of pairwise alignments, which is constructed by aligning the strings according to the Hamming distance (i.e. only allowing substitutions and no insertions or deletions; Hamming, 1950). The evaluation procedure consists of comparing the alignments of the previously discussed algorithms including the baseline with the alignments of the gold standard. For the comparison we use the standard Levenshtein algorithm without any restrictions. The evaluation proceeds as follows: 1. The pairwise alignments of the four algorithms, the baseline and the gold standard are generated and standardized (see section 4.1). When multiple equal-scoring alignments are 30 generated by an algorithm, only one (i.e. the final) alignment is selected. 2. In each alignment, we convert each pair of align</context>
</contexts>
<marker>Hamming, 1950</marker>
<rawString>Richard Hamming. 1950. Error detecting and error correcting codes. Bell System Technical Journal, 29:147–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wilbert Heeringa</author>
<author>Peter Kleiweg</author>
<author>Charlotte Gooskens</author>
<author>John Nerbonne</author>
</authors>
<title>Evaluation of string distance algorithms for dialectology. In</title>
<date>2006</date>
<booktitle>Linguistic Distances,</booktitle>
<pages>51--62</pages>
<editor>John Nerbonne and Erhard Hinrichs, editors,</editor>
<publisher>ACL.</publisher>
<location>Shroudsburg, PA.</location>
<contexts>
<context position="4185" citStr="Heeringa et al. (2006)" startWordPosition="630" endWordPosition="633">ent is of crucial importance. Almost all evaluation methods in dialectometry focus on the aggregate results and ignore the individual word-pair distances and individual alignments on which the distances are based. The focus on the aggregate distance of 100 or so word Proceedings of the EACL 2009 Workshop on Language Technology and Resources for Cultural Heritage, Social Sciences, Humanities, and Education –LaTeCH – SHELT&amp;R 2009, pages 26–34, Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics 26 pairs effectively hides many differences between methods. For example, Heeringa et al. (2006) find no significant differences in the degrees to which several pairwise string distance measures correlate with perceptual distances when examined at an aggregate level. Wieling et al. (2007) and Wieling and Nerbonne (2007) also report almost no difference between different PSA algorithms at the aggregate level. It is important to be able to evaluate the different techniques more sensitively, which is why this paper examines alignment quality at the segment level. Kondrak (2003) applies a PSA algorithm to align words in different languages in order to detect cognates automatically. Exception</context>
<context position="8894" citStr="Heeringa et al., 2006" startWordPosition="1392" endWordPosition="1395">&amp;quot;Azi], two Bulgarian dialectal variants of the word ‘I’ (as in ‘I am’), is 3: j&amp;quot;As delete j 1 &amp;quot;As subst. s/z 1 &amp;quot;Az insert i 1 &amp;quot;Azi 3 The corresponding alignment is: j &amp;quot;A s &amp;quot;A z i 1 1 1 The Levenshtein distance has been used frequently and successfully in measuring linguistic distances in several languages, including Irish (Kessler, 1995), Dutch (Heeringa, 2004) and Norwegian (Heeringa, 2004). Additionally, the Levenshtein distance has been shown to yield aggregate results that are consistent (Cronbach’s α = 0.99) and valid when compared to dialect speakers judgements of similarity (r Pt� 0.7; Heeringa et al., 2006). Following Heeringa (2004), we have adapted the Levenshtein algorithm slightly, so that it does not allow alignments of vowels with consonants. We refer to this adapted algorithm as the VCsensitive Levenshtein algorithm. 3.2 The Levenshtein algorithm with the swap operation Because metathesis (i.e. transposition of sounds) occurs relatively frequently in the Bulgarian dialect data (in 21 of 152 words), we extend the VC-sensitive Levenshtein algorithm as described in section 3.1 to include the swap-operation (Wagner and Lowrance, 1975), which allows two adjacent characters to be interchanged. </context>
</contexts>
<marker>Heeringa, Kleiweg, Gooskens, Nerbonne, 2006</marker>
<rawString>Wilbert Heeringa, Peter Kleiweg, Charlotte Gooskens, and John Nerbonne. 2006. Evaluation of string distance algorithms for dialectology. In John Nerbonne and Erhard Hinrichs, editors, Linguistic Distances, pages 51–62, Shroudsburg, PA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wilbert Heeringa</author>
</authors>
<title>Measuring Dialect Pronunciation Differences using Levenshtein Distance.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>Rijksuniversiteit Groningen.</institution>
<contexts>
<context position="3517" citStr="Heeringa, 2004" startWordPosition="530" endWordPosition="531">rr, 2003), or determining whether a string is the transliteration of the same name from another writing system (Pouliquen, 2008). In this paper we evaluate string distance measures on the basis of data from dialectology. We therefore explain a bit more of the intended use of the pronunciation distance measure. Dialect atlases normally contain a large number of pronunciations of the same word in various places throughout a language area. All pairs of pronunciations of corresponding words are compared in order to obtain a measure of the aggregate linguistic distance between dialectal varieties (Heeringa, 2004). It is clear that the quality of the measurement is of crucial importance. Almost all evaluation methods in dialectometry focus on the aggregate results and ignore the individual word-pair distances and individual alignments on which the distances are based. The focus on the aggregate distance of 100 or so word Proceedings of the EACL 2009 Workshop on Language Technology and Resources for Cultural Heritage, Social Sciences, Humanities, and Education –LaTeCH – SHELT&amp;R 2009, pages 26–34, Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics 26 pairs effectively hides m</context>
<context position="5450" citStr="Heeringa, 2004" startWordPosition="825" endWordPosition="826">alignments generated by different algorithms. But he restricts his examination to a set of only 82 gold standard pairwise alignments and he only distinguishes correct and incorrect alignments and does not look at misaligned phones. In the current study we introduce and evaluate several alignment algorithms more extensively at the alignment level. The algorithms we evaluate include the Levenshtein algorithm (with syllabicity constraint), which is one of the most popular alignment methods and has successfully been used in determining pronunciation differences in phonetic strings (Kessler, 1995; Heeringa, 2004). In addition we look at two adaptations of the Levenshtein algorithm. The first adaptation includes the swap-operation (Wagner and Lowrance, 1975), while the second adaptation includes phonetic segment distances, which are generated by applying an iterative pointwise mutual information (PMI) procedure (Church and Hanks, 1990). Finally we include alignments generated with the Pair Hidden Markov Model (PHMM) as introduced to language studies by Mackay and Kondrak (2005). They reported that the Pair Hidden Markov Model outperformed ALINE, the best performing algorithm at the alignment level in t</context>
<context position="8635" citStr="Heeringa, 2004" startWordPosition="1351" endWordPosition="1352">s, deletions and substitutions) needed to transform one string into the other. 1The dataset is available online at the website http://www.bultreebank.org/BulDialects/ j &apos;a s &apos;a z i j &apos;a j &apos;a 27 For example, the Levenshtein distance between [j&amp;quot;As] and [&amp;quot;Azi], two Bulgarian dialectal variants of the word ‘I’ (as in ‘I am’), is 3: j&amp;quot;As delete j 1 &amp;quot;As subst. s/z 1 &amp;quot;Az insert i 1 &amp;quot;Azi 3 The corresponding alignment is: j &amp;quot;A s &amp;quot;A z i 1 1 1 The Levenshtein distance has been used frequently and successfully in measuring linguistic distances in several languages, including Irish (Kessler, 1995), Dutch (Heeringa, 2004) and Norwegian (Heeringa, 2004). Additionally, the Levenshtein distance has been shown to yield aggregate results that are consistent (Cronbach’s α = 0.99) and valid when compared to dialect speakers judgements of similarity (r Pt� 0.7; Heeringa et al., 2006). Following Heeringa (2004), we have adapted the Levenshtein algorithm slightly, so that it does not allow alignments of vowels with consonants. We refer to this adapted algorithm as the VCsensitive Levenshtein algorithm. 3.2 The Levenshtein algorithm with the swap operation Because metathesis (i.e. transposition of sounds) occurs relative</context>
<context position="11674" citStr="Heeringa (2004)" startWordPosition="1877" endWordPosition="1878"> than the cost of the alternative of two substitutions. In the second case the cost is either 3 (if xi =� yi+1 or yi =� xi+1) or 5 (if xi =� yi+1 and yi =� xi+1), which is higher than the cost of using insertions, deletions and/or substitutions. Just as in the previous section, we do not allow vowels to align with consonants (except in the case of a swap). 3.3 The Levenshtein algorithm with generated segment distances The VC-sensitive Levenshtein algorithm as described in section 3.1 only distinguishes between vowels and consonants. However, more sensitive segment distances are also possible. Heeringa (2004) experimented with specifying phonetic segment distances based on phonetic features and 2We use transcriptions in which stress is marked on stressed vowels instead of before stressed syllables. We follow in this the Bulgarian convention instead of the IPA convention. 3Actually the cost is set to 0.999 to prefer an alignment involving a swap over an alternative alignment involving only regular edit operations. 28 also based on acoustic differences derived from spectrograms, but he did not obtain improved results at the aggregate level. Instead of using segment distances as these are (incomplete</context>
</contexts>
<marker>Heeringa, 2004</marker>
<rawString>Wilbert Heeringa. 2004. Measuring Dialect Pronunciation Differences using Levenshtein Distance. Ph.D. thesis, Rijksuniversiteit Groningen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brett Kessler</author>
</authors>
<title>Computational dialectology in Irish Gaelic.</title>
<date>1995</date>
<booktitle>In Proceedings of the seventh conference on European chapter of the Association for Computational Linguistics,</booktitle>
<pages>60--66</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="5433" citStr="Kessler, 1995" startWordPosition="823" endWordPosition="824"> of the string alignments generated by different algorithms. But he restricts his examination to a set of only 82 gold standard pairwise alignments and he only distinguishes correct and incorrect alignments and does not look at misaligned phones. In the current study we introduce and evaluate several alignment algorithms more extensively at the alignment level. The algorithms we evaluate include the Levenshtein algorithm (with syllabicity constraint), which is one of the most popular alignment methods and has successfully been used in determining pronunciation differences in phonetic strings (Kessler, 1995; Heeringa, 2004). In addition we look at two adaptations of the Levenshtein algorithm. The first adaptation includes the swap-operation (Wagner and Lowrance, 1975), while the second adaptation includes phonetic segment distances, which are generated by applying an iterative pointwise mutual information (PMI) procedure (Church and Hanks, 1990). Finally we include alignments generated with the Pair Hidden Markov Model (PHMM) as introduced to language studies by Mackay and Kondrak (2005). They reported that the Pair Hidden Markov Model outperformed ALINE, the best performing algorithm at the ali</context>
<context position="7845" citStr="Kessler (1995)" startWordPosition="1217" endWordPosition="1218">s no evidence for this in the single pair. To make this clear, consider the multiple alignment of three Bulgarian dialectal variants of the word ‘I’ (as in ‘I am’): j &apos;a s &apos;a z i j &apos;a Using the procedure above, the three generated pairwise alignments are: j &apos;a s &apos;a z i 3 Algorithms Four algorithms are evaluated with respect to the quality of their alignments, including three variants of the Levenshtein algorithm and the Pair Hidden Markov Model. 3.1 The VC-sensitive Levenshtein algorithm The Levenshtein algorithm is a very efficient dynamic programming algorithm, which was first introduced by Kessler (1995) as a tool for computationally comparing dialects. The Levenshtein distance between two strings is determined by counting the minimum number of edit operations (i.e. insertions, deletions and substitutions) needed to transform one string into the other. 1The dataset is available online at the website http://www.bultreebank.org/BulDialects/ j &apos;a s &apos;a z i j &apos;a j &apos;a 27 For example, the Levenshtein distance between [j&amp;quot;As] and [&amp;quot;Azi], two Bulgarian dialectal variants of the word ‘I’ (as in ‘I am’), is 3: j&amp;quot;As delete j 1 &amp;quot;As subst. s/z 1 &amp;quot;Az insert i 1 &amp;quot;Azi 3 The corresponding alignment is: j &amp;quot;A s &amp;quot;</context>
</contexts>
<marker>Kessler, 1995</marker>
<rawString>Brett Kessler. 1995. Computational dialectology in Irish Gaelic. In Proceedings of the seventh conference on European chapter of the Association for Computational Linguistics, pages 60–66, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Kondrak</author>
<author>Bonnie Dorr</author>
</authors>
<title>Identification of Confusable Drug Names: A New Approach and Evaluation Methodology.</title>
<date>2003</date>
<journal>Artificial Intelligence in Medicine,</journal>
<pages>36--273</pages>
<contexts>
<context position="2911" citStr="Kondrak and Dorr, 2003" startWordPosition="431" endWordPosition="434">he distance between two strings. Since there are many alignment algorithms and specific settings for each algorithm influencing the distance between two strings (Nerbonne and Kleiweg, 2007), evaluation is very important in determining the effectiveness of the distance methods. Determining the distance (or similarity) between two phonetic strings is an important aspect of dialectometry, and alignment quality is important in applications in which string alignment is a goal in itself, for example, determining if two words are likely to be cognate (Kondrak, 2003), detecting confusable drug names (Kondrak and Dorr, 2003), or determining whether a string is the transliteration of the same name from another writing system (Pouliquen, 2008). In this paper we evaluate string distance measures on the basis of data from dialectology. We therefore explain a bit more of the intended use of the pronunciation distance measure. Dialect atlases normally contain a large number of pronunciations of the same word in various places throughout a language area. All pairs of pronunciations of corresponding words are compared in order to obtain a measure of the aggregate linguistic distance between dialectal varieties (Heeringa,</context>
</contexts>
<marker>Kondrak, Dorr, 2003</marker>
<rawString>Grzegorz Kondrak and Bonnie Dorr. 2003. Identification of Confusable Drug Names: A New Approach and Evaluation Methodology. Artificial Intelligence in Medicine, 36:273–291.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Kondrak</author>
</authors>
<date>2003</date>
<booktitle>Phonetic Alignment and Similarity. Computers and the Humanities,</booktitle>
<pages>37--273</pages>
<contexts>
<context position="2853" citStr="Kondrak, 2003" startWordPosition="425" endWordPosition="426">o segments) often form the basis of determining the distance between two strings. Since there are many alignment algorithms and specific settings for each algorithm influencing the distance between two strings (Nerbonne and Kleiweg, 2007), evaluation is very important in determining the effectiveness of the distance methods. Determining the distance (or similarity) between two phonetic strings is an important aspect of dialectometry, and alignment quality is important in applications in which string alignment is a goal in itself, for example, determining if two words are likely to be cognate (Kondrak, 2003), detecting confusable drug names (Kondrak and Dorr, 2003), or determining whether a string is the transliteration of the same name from another writing system (Pouliquen, 2008). In this paper we evaluate string distance measures on the basis of data from dialectology. We therefore explain a bit more of the intended use of the pronunciation distance measure. Dialect atlases normally contain a large number of pronunciations of the same word in various places throughout a language area. All pairs of pronunciations of corresponding words are compared in order to obtain a measure of the aggregate </context>
<context position="4670" citStr="Kondrak (2003)" startWordPosition="708" endWordPosition="709">on for Computational Linguistics 26 pairs effectively hides many differences between methods. For example, Heeringa et al. (2006) find no significant differences in the degrees to which several pairwise string distance measures correlate with perceptual distances when examined at an aggregate level. Wieling et al. (2007) and Wieling and Nerbonne (2007) also report almost no difference between different PSA algorithms at the aggregate level. It is important to be able to evaluate the different techniques more sensitively, which is why this paper examines alignment quality at the segment level. Kondrak (2003) applies a PSA algorithm to align words in different languages in order to detect cognates automatically. Exceptionally, he does provide an evaluation of the string alignments generated by different algorithms. But he restricts his examination to a set of only 82 gold standard pairwise alignments and he only distinguishes correct and incorrect alignments and does not look at misaligned phones. In the current study we introduce and evaluate several alignment algorithms more extensively at the alignment level. The algorithms we evaluate include the Levenshtein algorithm (with syllabicity constra</context>
<context position="6091" citStr="Kondrak (2003)" startWordPosition="922" endWordPosition="923">o adaptations of the Levenshtein algorithm. The first adaptation includes the swap-operation (Wagner and Lowrance, 1975), while the second adaptation includes phonetic segment distances, which are generated by applying an iterative pointwise mutual information (PMI) procedure (Church and Hanks, 1990). Finally we include alignments generated with the Pair Hidden Markov Model (PHMM) as introduced to language studies by Mackay and Kondrak (2005). They reported that the Pair Hidden Markov Model outperformed ALINE, the best performing algorithm at the alignment level in the aforementioned study of Kondrak (2003). The PHMM has also successfully been used in dialectology by Wieling et al. (2007). 2 Dataset The dataset used in this study consists of 152 words collected from 197 sites equally distributed over Bulgaria. The transcribed word pronunciations include diacritics and suprasegmentals (e.g., intonation). The total number of different phonetic types (or segments) is 98.1 The gold standard pairwise alignment was automatically generated from a manually corrected gold standard set of N multiple alignments (see Proki´c et al., 2009 ) in the following way: • Every individual string (including gaps) in </context>
</contexts>
<marker>Kondrak, 2003</marker>
<rawString>Grzegorz Kondrak. 2003. Phonetic Alignment and Similarity. Computers and the Humanities, 37:273– 291.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions and reversals.</title>
<date>1965</date>
<booktitle>Doklady Akademii Nauk SSSR,</booktitle>
<pages>163--845</pages>
<contexts>
<context position="2090" citStr="Levenshtein, 1965" startWordPosition="305" endWordPosition="306">lar sensitive to signals of common provenance. The present paper focuses on speech habits which even today bear signals of common provenance in the various dialects of the world’s languages, and which have also been recorded and preserved in major archives of folk culture internationally. We present work in a research line which seeks to develop digital instruments capable of detecting common provenance among pronunciation habits, focusing in this paper on the issue of evaluating the quality of these instruments. Pairwise string alignment (PSA) methods, like the popular Levenshtein algorithm (Levenshtein, 1965) which uses insertions (alignments of a segment against a gap), deletions (alignments of a gap against a segment) and substitutions (alignments of two segments) often form the basis of determining the distance between two strings. Since there are many alignment algorithms and specific settings for each algorithm influencing the distance between two strings (Nerbonne and Kleiweg, 2007), evaluation is very important in determining the effectiveness of the distance methods. Determining the distance (or similarity) between two phonetic strings is an important aspect of dialectometry, and alignment</context>
</contexts>
<marker>Levenshtein, 1965</marker>
<rawString>Vladimir Levenshtein. 1965. Binary codes capable of correcting deletions, insertions and reversals. Doklady Akademii Nauk SSSR, 163:845–848.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wesley Mackay</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Computing word similarity and identifying cognates with Pair Hidden Markov Models.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>40--47</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="5923" citStr="Mackay and Kondrak (2005)" startWordPosition="894" endWordPosition="897">st popular alignment methods and has successfully been used in determining pronunciation differences in phonetic strings (Kessler, 1995; Heeringa, 2004). In addition we look at two adaptations of the Levenshtein algorithm. The first adaptation includes the swap-operation (Wagner and Lowrance, 1975), while the second adaptation includes phonetic segment distances, which are generated by applying an iterative pointwise mutual information (PMI) procedure (Church and Hanks, 1990). Finally we include alignments generated with the Pair Hidden Markov Model (PHMM) as introduced to language studies by Mackay and Kondrak (2005). They reported that the Pair Hidden Markov Model outperformed ALINE, the best performing algorithm at the alignment level in the aforementioned study of Kondrak (2003). The PHMM has also successfully been used in dialectology by Wieling et al. (2007). 2 Dataset The dataset used in this study consists of 152 words collected from 197 sites equally distributed over Bulgaria. The transcribed word pronunciations include diacritics and suprasegmentals (e.g., intonation). The total number of different phonetic types (or segments) is 98.1 The gold standard pairwise alignment was automatically generat</context>
<context position="16124" citStr="Mackay and Kondrak (2005)" startWordPosition="2616" endWordPosition="2619">ment distances for edit distance using an expectation maximization algorithm (Ristad and Yianilos, 1998). Our approach differs from their approach in that we only learn segment distances based on the alignments generated by the VC-sensitive Levenshtein algorithm, while Ristad and Yianilos (1998) learn segment distances by considering all possible alignments of two strings. 3.4 The Pair Hidden Markov Model The Pair Hidden Markov Model (PHMM) also generates alignments based on automatically generated segment distances and has been used suc29 Figure 1: Pair Hidden Markov Model. Image courtesy of Mackay and Kondrak (2005). cessfully in language studies (Mackay and Kondrak, 2005; Wieling et al., 2007). A Hidden Markov Model (HMM) is a probabilistic finite-state transducer that generates an observation sequence by starting in an initial state, going from state to state based on transition probabilities and emitting an output symbol in each state based on the emission probabilities in that state for that output symbol (Rabiner, 1989). The PHMM was originally proposed by Durbin et al. (1998) for aligning biological sequences and was first used in linguistics by Mackay and Kondrak (2005) to identify cognates. The P</context>
</contexts>
<marker>Mackay, Kondrak, 2005</marker>
<rawString>Wesley Mackay and Grzegorz Kondrak. 2005. Computing word similarity and identifying cognates with Pair Hidden Markov Models. In Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), pages 40–47, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Nerbonne</author>
<author>Peter Kleiweg</author>
</authors>
<title>Toward a dialectological yardstick.</title>
<date>2007</date>
<journal>Journal of Quantitative Linguistics,</journal>
<pages>14--148</pages>
<contexts>
<context position="2477" citStr="Nerbonne and Kleiweg, 2007" startWordPosition="363" endWordPosition="367"> detecting common provenance among pronunciation habits, focusing in this paper on the issue of evaluating the quality of these instruments. Pairwise string alignment (PSA) methods, like the popular Levenshtein algorithm (Levenshtein, 1965) which uses insertions (alignments of a segment against a gap), deletions (alignments of a gap against a segment) and substitutions (alignments of two segments) often form the basis of determining the distance between two strings. Since there are many alignment algorithms and specific settings for each algorithm influencing the distance between two strings (Nerbonne and Kleiweg, 2007), evaluation is very important in determining the effectiveness of the distance methods. Determining the distance (or similarity) between two phonetic strings is an important aspect of dialectometry, and alignment quality is important in applications in which string alignment is a goal in itself, for example, determining if two words are likely to be cognate (Kondrak, 2003), detecting confusable drug names (Kondrak and Dorr, 2003), or determining whether a string is the transliteration of the same name from another writing system (Pouliquen, 2008). In this paper we evaluate string distance mea</context>
</contexts>
<marker>Nerbonne, Kleiweg, 2007</marker>
<rawString>John Nerbonne and Peter Kleiweg. 2007. Toward a dialectological yardstick. Journal of Quantitative Linguistics, 14:148–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruno Pouliquen</author>
</authors>
<title>Similarity of names across scripts: Edit distance using learned costs of NGrams.</title>
<date>2008</date>
<booktitle>In Bent Nordstr¨om and Aarne Ranta, editors, Proceedings of the 6th international Conference on Natural Language Processing (GoTal’2008),</booktitle>
<volume>5221</volume>
<pages>405--416</pages>
<contexts>
<context position="3030" citStr="Pouliquen, 2008" startWordPosition="451" endWordPosition="452">g the distance between two strings (Nerbonne and Kleiweg, 2007), evaluation is very important in determining the effectiveness of the distance methods. Determining the distance (or similarity) between two phonetic strings is an important aspect of dialectometry, and alignment quality is important in applications in which string alignment is a goal in itself, for example, determining if two words are likely to be cognate (Kondrak, 2003), detecting confusable drug names (Kondrak and Dorr, 2003), or determining whether a string is the transliteration of the same name from another writing system (Pouliquen, 2008). In this paper we evaluate string distance measures on the basis of data from dialectology. We therefore explain a bit more of the intended use of the pronunciation distance measure. Dialect atlases normally contain a large number of pronunciations of the same word in various places throughout a language area. All pairs of pronunciations of corresponding words are compared in order to obtain a measure of the aggregate linguistic distance between dialectal varieties (Heeringa, 2004). It is clear that the quality of the measurement is of crucial importance. Almost all evaluation methods in dial</context>
</contexts>
<marker>Pouliquen, 2008</marker>
<rawString>Bruno Pouliquen. 2008. Similarity of names across scripts: Edit distance using learned costs of NGrams. In Bent Nordstr¨om and Aarne Ranta, editors, Proceedings of the 6th international Conference on Natural Language Processing (GoTal’2008), volume 5221, pages 405–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jelena Proki´c</author>
<author>Martijn Wieling</author>
<author>John Nerbonne</author>
</authors>
<title>Multiple sequence alignments in linguistics.</title>
<date>2009</date>
<booktitle>In Piroska Lendvai and Lars Borin, editors, Proceedings of the EACL 2009 Workshop on Language Technology and Resources for Cultural Heritage, Social Sciences, Humanities, and Education.</booktitle>
<marker>Proki´c, Wieling, Nerbonne, 2009</marker>
<rawString>Jelena Proki´c, Martijn Wieling, and John Nerbonne. 2009. Multiple sequence alignments in linguistics. In Piroska Lendvai and Lars Borin, editors, Proceedings of the EACL 2009 Workshop on Language Technology and Resources for Cultural Heritage, Social Sciences, Humanities, and Education.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence R Rabiner</author>
</authors>
<title>A tutorial on Hidden Markov Models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<volume>77</volume>
<issue>2</issue>
<pages>286</pages>
<contexts>
<context position="16541" citStr="Rabiner, 1989" startWordPosition="2686" endWordPosition="2687">kov Model (PHMM) also generates alignments based on automatically generated segment distances and has been used suc29 Figure 1: Pair Hidden Markov Model. Image courtesy of Mackay and Kondrak (2005). cessfully in language studies (Mackay and Kondrak, 2005; Wieling et al., 2007). A Hidden Markov Model (HMM) is a probabilistic finite-state transducer that generates an observation sequence by starting in an initial state, going from state to state based on transition probabilities and emitting an output symbol in each state based on the emission probabilities in that state for that output symbol (Rabiner, 1989). The PHMM was originally proposed by Durbin et al. (1998) for aligning biological sequences and was first used in linguistics by Mackay and Kondrak (2005) to identify cognates. The PHMM differs from the regular HMM in that it outputs two observation streams (i.e. a series of alignments of pairs of individual segments) instead of only a series of single symbols. The PHMM displayed in Figure 1 has three emitting states: the substitution (‘match’) state (M) which emits two aligned symbols, the insertion state (Y) which emits a symbol and a gap, and the deletion state (X) which emits a gap and a </context>
<context position="18979" citStr="Rabiner, 1989" startWordPosition="3092" endWordPosition="3093">ta for training, sensible segment distances were obtained; acoustic vowel distances on the basis of spectrograms correlated significantly (r = −0.72) with the vowel substitution probabilities of the PHMM. Additionally, probabilities of substituting a symbol with itself were much higher than the probabilities of substituting an arbitrary vowel with another non-identical vowel (mutatis mutandis for consonants), which were in turn much higher than the probabilities of substituting a vowel for a consonant. After training, the well known Viterbi algorithm can be used to obtain the best alignments (Rabiner, 1989). 4 Evaluation As described in section 2, we use the generated pairwise alignments from a gold standard of multiple alignments for evaluation. In addition, we look at the performance of a baseline of pairwise alignments, which is constructed by aligning the strings according to the Hamming distance (i.e. only allowing substitutions and no insertions or deletions; Hamming, 1950). The evaluation procedure consists of comparing the alignments of the previously discussed algorithms including the baseline with the alignments of the gold standard. For the comparison we use the standard Levenshtein a</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Lawrence R. Rabiner. 1989. A tutorial on Hidden Markov Models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257– 286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Sven Ristad</author>
<author>Peter N Yianilos</author>
</authors>
<title>Learning string-edit distance.</title>
<date>1998</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>20</volume>
<pages>532</pages>
<contexts>
<context position="15603" citStr="Ristad and Yianilos, 1998" startWordPosition="2535" endWordPosition="2538">rates the erroneous alignment because it has no way to identify that the consonant [n] is nearer to the consonant [ï] than to the consonant [k]. In contrast, the Levenshtein algorithm which uses the PMI-generated segment distances only generates the correct first alignment, because the [n] occurs relatively more often aligned with [ï] than with [k] so that the distance between [n] and [ï] will be lower than the distance between [n] and [k]. The idea behind this procedure is similar to Ristad’s suggestion to learn segment distances for edit distance using an expectation maximization algorithm (Ristad and Yianilos, 1998). Our approach differs from their approach in that we only learn segment distances based on the alignments generated by the VC-sensitive Levenshtein algorithm, while Ristad and Yianilos (1998) learn segment distances by considering all possible alignments of two strings. 3.4 The Pair Hidden Markov Model The Pair Hidden Markov Model (PHMM) also generates alignments based on automatically generated segment distances and has been used suc29 Figure 1: Pair Hidden Markov Model. Image courtesy of Mackay and Kondrak (2005). cessfully in language studies (Mackay and Kondrak, 2005; Wieling et al., 2007</context>
</contexts>
<marker>Ristad, Yianilos, 1998</marker>
<rawString>Eric Sven Ristad and Peter N. Yianilos. 1998. Learning string-edit distance. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20:522– 532.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Wagner</author>
<author>Roy Lowrance</author>
</authors>
<title>An extension of the string-to-string correction problem.</title>
<date>1975</date>
<journal>Journal of the ACM,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="5597" citStr="Wagner and Lowrance, 1975" startWordPosition="845" endWordPosition="848"> he only distinguishes correct and incorrect alignments and does not look at misaligned phones. In the current study we introduce and evaluate several alignment algorithms more extensively at the alignment level. The algorithms we evaluate include the Levenshtein algorithm (with syllabicity constraint), which is one of the most popular alignment methods and has successfully been used in determining pronunciation differences in phonetic strings (Kessler, 1995; Heeringa, 2004). In addition we look at two adaptations of the Levenshtein algorithm. The first adaptation includes the swap-operation (Wagner and Lowrance, 1975), while the second adaptation includes phonetic segment distances, which are generated by applying an iterative pointwise mutual information (PMI) procedure (Church and Hanks, 1990). Finally we include alignments generated with the Pair Hidden Markov Model (PHMM) as introduced to language studies by Mackay and Kondrak (2005). They reported that the Pair Hidden Markov Model outperformed ALINE, the best performing algorithm at the alignment level in the aforementioned study of Kondrak (2003). The PHMM has also successfully been used in dialectology by Wieling et al. (2007). 2 Dataset The dataset</context>
<context position="9435" citStr="Wagner and Lowrance, 1975" startWordPosition="1472" endWordPosition="1476">pared to dialect speakers judgements of similarity (r Pt� 0.7; Heeringa et al., 2006). Following Heeringa (2004), we have adapted the Levenshtein algorithm slightly, so that it does not allow alignments of vowels with consonants. We refer to this adapted algorithm as the VCsensitive Levenshtein algorithm. 3.2 The Levenshtein algorithm with the swap operation Because metathesis (i.e. transposition of sounds) occurs relatively frequently in the Bulgarian dialect data (in 21 of 152 words), we extend the VC-sensitive Levenshtein algorithm as described in section 3.1 to include the swap-operation (Wagner and Lowrance, 1975), which allows two adjacent characters to be interchanged. The swapoperation is also known as a transposition, which was introduced with respect to detecting spelling errors by Damerau (1964). As a consequence the Damerau distance refers to the minimum number of insertions, deletions, substitutions and transpositions required to transform one string into the other. In contrast to Wagner and Lowrance (1975) and in line with Damerau (1964) we restrict the swap operation to be only allowed for string X and Y when xi = yi+1 and yi = xi+1 (with xi being the token at position i in string X): xi xi+1</context>
</contexts>
<marker>Wagner, Lowrance, 1975</marker>
<rawString>Robert Wagner and Roy Lowrance. 1975. An extension of the string-to-string correction problem. Journal of the ACM, 22(2):177–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martijn Wieling</author>
<author>John Nerbonne</author>
</authors>
<title>Dialect pronunciation comparison and spoken word recognition.</title>
<date>2007</date>
<booktitle>In Petya Osenova, editor, Proceedings of the RANLP Workshop on Computational Phonology,</booktitle>
<pages>71--78</pages>
<contexts>
<context position="4410" citStr="Wieling and Nerbonne (2007)" startWordPosition="664" endWordPosition="667">cus on the aggregate distance of 100 or so word Proceedings of the EACL 2009 Workshop on Language Technology and Resources for Cultural Heritage, Social Sciences, Humanities, and Education –LaTeCH – SHELT&amp;R 2009, pages 26–34, Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics 26 pairs effectively hides many differences between methods. For example, Heeringa et al. (2006) find no significant differences in the degrees to which several pairwise string distance measures correlate with perceptual distances when examined at an aggregate level. Wieling et al. (2007) and Wieling and Nerbonne (2007) also report almost no difference between different PSA algorithms at the aggregate level. It is important to be able to evaluate the different techniques more sensitively, which is why this paper examines alignment quality at the segment level. Kondrak (2003) applies a PSA algorithm to align words in different languages in order to detect cognates automatically. Exceptionally, he does provide an evaluation of the string alignments generated by different algorithms. But he restricts his examination to a set of only 82 gold standard pairwise alignments and he only distinguishes correct and inco</context>
</contexts>
<marker>Wieling, Nerbonne, 2007</marker>
<rawString>Martijn Wieling and John Nerbonne. 2007. Dialect pronunciation comparison and spoken word recognition. In Petya Osenova, editor, Proceedings of the RANLP Workshop on Computational Phonology, pages 71–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martijn Wieling</author>
<author>Therese Leinonen</author>
<author>John Nerbonne</author>
</authors>
<title>Inducing sound segment differences using Pair Hidden Markov Models.</title>
<date>2007</date>
<booktitle>In Mark Ellison John Nerbonne and Greg Kondrak, editors, Computing and Historical Phonology: 9th Meeting of the ACL Special Interest Group for Computational Morphology and Phonology,</booktitle>
<pages>48--56</pages>
<contexts>
<context position="4378" citStr="Wieling et al. (2007)" startWordPosition="659" endWordPosition="662">istances are based. The focus on the aggregate distance of 100 or so word Proceedings of the EACL 2009 Workshop on Language Technology and Resources for Cultural Heritage, Social Sciences, Humanities, and Education –LaTeCH – SHELT&amp;R 2009, pages 26–34, Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics 26 pairs effectively hides many differences between methods. For example, Heeringa et al. (2006) find no significant differences in the degrees to which several pairwise string distance measures correlate with perceptual distances when examined at an aggregate level. Wieling et al. (2007) and Wieling and Nerbonne (2007) also report almost no difference between different PSA algorithms at the aggregate level. It is important to be able to evaluate the different techniques more sensitively, which is why this paper examines alignment quality at the segment level. Kondrak (2003) applies a PSA algorithm to align words in different languages in order to detect cognates automatically. Exceptionally, he does provide an evaluation of the string alignments generated by different algorithms. But he restricts his examination to a set of only 82 gold standard pairwise alignments and he onl</context>
<context position="6174" citStr="Wieling et al. (2007)" startWordPosition="934" endWordPosition="937">e swap-operation (Wagner and Lowrance, 1975), while the second adaptation includes phonetic segment distances, which are generated by applying an iterative pointwise mutual information (PMI) procedure (Church and Hanks, 1990). Finally we include alignments generated with the Pair Hidden Markov Model (PHMM) as introduced to language studies by Mackay and Kondrak (2005). They reported that the Pair Hidden Markov Model outperformed ALINE, the best performing algorithm at the alignment level in the aforementioned study of Kondrak (2003). The PHMM has also successfully been used in dialectology by Wieling et al. (2007). 2 Dataset The dataset used in this study consists of 152 words collected from 197 sites equally distributed over Bulgaria. The transcribed word pronunciations include diacritics and suprasegmentals (e.g., intonation). The total number of different phonetic types (or segments) is 98.1 The gold standard pairwise alignment was automatically generated from a manually corrected gold standard set of N multiple alignments (see Proki´c et al., 2009 ) in the following way: • Every individual string (including gaps) in the multiple alignment is aligned with every other string of the same word. With 15</context>
<context position="16204" citStr="Wieling et al., 2007" startWordPosition="2629" endWordPosition="2632"> and Yianilos, 1998). Our approach differs from their approach in that we only learn segment distances based on the alignments generated by the VC-sensitive Levenshtein algorithm, while Ristad and Yianilos (1998) learn segment distances by considering all possible alignments of two strings. 3.4 The Pair Hidden Markov Model The Pair Hidden Markov Model (PHMM) also generates alignments based on automatically generated segment distances and has been used suc29 Figure 1: Pair Hidden Markov Model. Image courtesy of Mackay and Kondrak (2005). cessfully in language studies (Mackay and Kondrak, 2005; Wieling et al., 2007). A Hidden Markov Model (HMM) is a probabilistic finite-state transducer that generates an observation sequence by starting in an initial state, going from state to state based on transition probabilities and emitting an output symbol in each state based on the emission probabilities in that state for that output symbol (Rabiner, 1989). The PHMM was originally proposed by Durbin et al. (1998) for aligning biological sequences and was first used in linguistics by Mackay and Kondrak (2005) to identify cognates. The PHMM differs from the regular HMM in that it outputs two observation streams (i.e</context>
<context position="18330" citStr="Wieling et al. (2007)" startWordPosition="2992" endWordPosition="2995"> to the probability of going from the substitution state to that particular state. The Baum-Welch expectation maximization algorithm (Baum et al., 1970) can be used to iteratively reestimate these probabilities until a local optimum is found. To prevent order effects in training, every word pair is considered twice (e.g., wa − wb and wb − wa). The resulting insertion and deletion probabilities are therefore the same (for each segment), and the probability of substituting x for y is equal to the probability of substituting y for x, effectively yielding 4802 distinct substitution probabilities. Wieling et al. (2007) showed that using Dutch dialect data for training, sensible segment distances were obtained; acoustic vowel distances on the basis of spectrograms correlated significantly (r = −0.72) with the vowel substitution probabilities of the PHMM. Additionally, probabilities of substituting a symbol with itself were much higher than the probabilities of substituting an arbitrary vowel with another non-identical vowel (mutatis mutandis for consonants), which were in turn much higher than the probabilities of substituting a vowel for a consonant. After training, the well known Viterbi algorithm can be u</context>
<context position="23860" citStr="Wieling et al. (2007)" startWordPosition="3900" endWordPosition="3903"> probabilities) in their alignments, we first check if these values are sensible and comparable to each other. 31 5.1.1 Comparison of segment distances With respect to the PMI results (convergence was reached after 7 iterations, taking less than 5 CPU minutes), we indeed found sensible results: the average distance between identical symbols was significantly lower than the distance between pairs of different vowels and consonants (t &lt; −13, p &lt; .001). Because we did not allow vowel-consonants alignments in the Levenshtein algorithm, no PMI values were generated for those segment pairs. Just as Wieling et al. (2007), we found sensible PHMM substitution probabilities (convergence was reached after 1675 iterations, taking about 7 CPU hours): the probability of matching a symbol with itself was significantly higher than the probability of substituting one vowel for another (similarly for consonants), which in turn was higher than the probability of substituting a vowel with a consonant (all t’s &gt; 9, p &lt; .001). To allow a fair comparison between the PHMM probabilities and the PMI distances, we transformed the PHMM probabilities to log-odds scores (i.e. dividing the probability by the relative frequency of th</context>
</contexts>
<marker>Wieling, Leinonen, Nerbonne, 2007</marker>
<rawString>Martijn Wieling, Therese Leinonen, and John Nerbonne. 2007. Inducing sound segment differences using Pair Hidden Markov Models. In Mark Ellison John Nerbonne and Greg Kondrak, editors, Computing and Historical Phonology: 9th Meeting of the ACL Special Interest Group for Computational Morphology and Phonology, pages 48–56.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>