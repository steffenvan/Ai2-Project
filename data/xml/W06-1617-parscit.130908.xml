<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.990104">
Semantic Role Labeling of NomBank: A Maximum Entropy Approach
</title>
<author confidence="0.994801">
Zheng Ping Jiang and Hwee Tou Ng
</author>
<affiliation confidence="0.913718333333333">
Department of Computer Science
National University of Singapore
3 Science Drive 2, Singapore 117543
</affiliation>
<email confidence="0.990154">
{jiangzp, nght}@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.982699" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999884705882353">
This paper describes our attempt at
NomBank-based automatic Semantic Role
Labeling (SRL). NomBank is a project at
New York University to annotate the ar-
gument structures for common nouns in
the Penn Treebank II corpus. We treat
the NomBank SRL task as a classifica-
tion problem and explore the possibility
of adapting features previously shown use-
ful in PropBank-based SRL systems. Var-
ious NomBank-specific features are ex-
plored. On test section 23, our best sys-
tem achieves F1 score of 72.73 (69.14)
when correct (automatic) syntactic parse
trees are used. To our knowledge, this
is the first reported automatic NomBank
SRL system.
</bodyText>
<sectionHeader confidence="0.994532" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998536">
Automatic Semantic Role Labeling (SRL) sys-
tems, made possible by the availability of Prop-
Bank (Kingsbury and Palmer, 2003; Palmer et
al., 2005), and encouraged by evaluation ef-
forts in (Carreras and Marquez, 2005; Litkowski,
2004), have been shown to accurately determine
the argument structure of verb predicates.
A successful PropBank-based SRL system
would correctly determine that “Ben Bernanke”
is the subject (labeled as ARG0 in PropBank) of
predicate “replace”, and “Greenspan” is the object
(labeled as ARG1):
</bodyText>
<listItem confidence="0.99437575">
• Ben Bernanke replaced Greenspan as Fed
chair.
• Greenspan was replaced by Ben Bernanke as
Fed chair.
</listItem>
<bodyText confidence="0.999053166666667">
The recent release of NomBank (Meyers et al.,
2004c; Meyers et al., 2004b), a databank that an-
notates argument structure for instances of com-
mon nouns in the Penn Treebank II corpus, made
it possible to develop automatic SRL systems that
analyze the argument structures of noun predi-
cates.
Given the following two noun phrases and one
sentence, a successful NomBank-based SRL sys-
tem should label “Ben Bernanke” as the subject
(ARG0) and “Greenspan” as the object (ARG1)
of the noun predicate “replacement”.
</bodyText>
<listItem confidence="0.994428">
• Greenspan’s replacement Ben Bernanke
• Ben Bernanke’s replacement of Greenspan
• Ben Bernanke was nominated as Greenspan’s
replacement.
</listItem>
<bodyText confidence="0.9999059">
The ability to automatically analyze the argu-
ment structures of verb and noun predicates would
greatly facilitate NLP tasks like question answer-
ing, information extraction, etc.
This paper focuses on our efforts at building
an accurate automatic NomBank-based SRL sys-
tem. We study how techniques used in building
PropBank SRL system can be transferred to de-
veloping NomBank SRL system. We also make
NomBank-specific enhancements to our baseline
system. Our implemented SRL system and exper-
iments are based on the September 2005 release of
NomBank (NomBank.0.8).
The rest of this paper is organized as follows:
Section 2 gives an overview of NomBank, Sec-
tion 3 introduces the Maximum Entropy classifica-
tion model, Section 4 introduces our features and
feature selection strategy, Section 5 explains the
experimental setup and presents the experimen-
tal results, Section 6 compares NomBank SRL to
</bodyText>
<page confidence="0.467514">
138
</page>
<note confidence="0.9845585">
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 138–145,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<figure confidence="0.975226866666667">
PP
   
IN NP

as  
NP
(ARG1)
Greenspan ’s
VBN
(Support)
nominated
NN
predicate
replacement

</figure>
<figureCaption confidence="0.9613365">
Figure 1: A sample sentence and its parse tree la-
beled in the style of NomBank
</figureCaption>
<note confidence="0.678699">
 
PropBank SRL and discusses possible future re-
search directions.
2 Overview of NomBank
</note>
<bodyText confidence="0.999368375">
The NomBank (Meyers et al., 2004c; Meyers
et al., 2004b) annotation project originated from
the NOMLEX (Macleod et al., 1997; Macleod et
al., 1998) nominalization lexicon developed under
the New York University Proteus Project. NOM-
LEX lists 1,000 nominalizations and the corre-
spondences between their arguments and the ar-
guments of their verb counterparts. NomBank
frames combine various lexical resources (Meyers
et al., 2004a), including an extended NOMLEX
and PropBank frames, and form the basis for anno-
tating the argument structures of common nouns.
Similar to PropBank, NomBank annotation is
made on the Penn TreeBank II (PTB II) corpus.
For each common noun in PTB II that takes argu-
ments, its core arguments are labeled with ARG0,
ARG1, etc, and modifying arguments are labeled
with ARGM-LOC to denote location, ARGM-
MNR to denote manner, etc. Annotations are
made on PTB II parse tree nodes, and argument
boundaries align with the span of parse tree nodes.
A sample sentence and its parse tree labeled
in the style of NomBank is shown in Figure 1.
For the nominal predicate “replacement”, “Ben
Bernanke” is labeled as ARG0 and “Greenspan
’s” is labeled as ARG1. There is also the special
label “Support” on “nominated” which introduces
“Ben Bernanke” as an argument of “replacement”.
The support construct will be explained in detail in
Section 4.2.3.
We are not aware of any NomBank-based auto-
matic SRL systems. The work in (Pradhan et al.,
</bodyText>
<equation confidence="0.893721">
NNP
NNP
was
Bernanke
Ben
S   
VBD
VP

   
VP

   
</equation>
<bodyText confidence="0.914259">
2004) experimented with an automatic SRL sys-
tem developed using a relatively small set of man-
ually selected nominalizations from FrameNet and
Penn Chinese TreeBank. The SRL accuracy of
their system is not directly comparable to ours.
</bodyText>
<figure confidence="0.49117825">
NP
(ARG0)
 
3 Model training and testing
</figure>
<bodyText confidence="0.999882709677419">
We treat the NomBank-based SRL task as a clas-
sification problem and divide it into two phases:
argument identification and argument classifica-
tion. During the argument identification phase,
each parse tree node is marked as either argument
or non-argument. Each node marked as argument
is then labeled with a specific class during the
argument classification phase. The identification
model is a binary classifier , while the classifica-
tion model is a multi-class classifier.
Opennlp maxent1, an implementation of Maxi-
mum Entropy (ME) modeling, is used as the clas-
sification tool. Since its introduction to the Natural
Language Processing (NLP) community (Berger
et al., 1996), ME-based classifiers have been
shown to be effective in various NLP tasks. ME
modeling is based on the insight that the best
model is consistent with the set of constraints im-
posed and otherwise as uniform as possible. ME
models the probability of label l given input x as
in Equation 1. fi(l, x) is a feature function that
maps label l and input x to either 0 or 1, while the
summation is over all n feature functions and with
Ai as the weight parameter for each feature func-
tion fi(l, x). Zx is a normalization factor. In the
identification model, label l corresponds to either
“argument” or “non-argument”, and in the classi-
fication model, label l corresponds to one of the
specific NomBank argument classes. The classifi-
cation output is the label l with the highest condi-
tional probability p(l|x).
</bodyText>
<equation confidence="0.996431333333333">
exp(�n i�1 Aifi(l, x))
p(l|x) = (1)
Zx
</equation>
<bodyText confidence="0.999898333333333">
To train the ME-based identification model,
training data is gathered by treating each parse tree
node that is an argument as a positive example and
the rest as negative examples. Classification train-
ing data is generated from argument nodes only.
During testing, the algorithm of enforcing non-
overlapping arguments by (Toutanova et al., 2005)
is used. The algorithm maximizes the log-
probability of the entire NomBank labeled parse
</bodyText>
<footnote confidence="0.741638">
1http://maxent.sourceforge.net/
</footnote>
<page confidence="0.818176">
139
</page>
<bodyText confidence="0.985799">
tree. Specifically, assuming we only have two
classes “ARG” and “NONE”, the log-probability
of a NomBank labeled parse tree is defined by
Equation 2.
</bodyText>
<equation confidence="0.9934735">
� NONE(T) + �(Max(child))
Max(T) = max
ARG(T) + E(NONETree(child))
(2)
</equation>
<bodyText confidence="0.985641117647059">
Max(T) is the maximum log-probability of a
tree T, NONE(T) and ARG(T) are respectively
the log-probability of assigning label “NONE”
and “ARG” by our argument identification model
to tree node T, child ranges through each of
T’s children, and NONETree(child) is the log-
probability of each node that is dominated by node
child being labeled as “NONE”. Details are pre-
sented in Algorithm 1.
Algorithm 1 Maximizing the probability of an
SRL tree
Input p{syntactic parse tree}
Input m{argument identification model, assigns each con-
stituent in the parse tree log likelihood of being a semantic
argument}
Output score{maximum log likelihood of the parse tree p
with arguments identified using model m}
</bodyText>
<equation confidence="0.876478384615385">
MLParse(p, m)
if parse p is a leaf node then
return max(Score(p, m, ARG), Score(p, m, NONE))
else
MLscore = 0
for each node ci in Children(p) do
MLscore += MLParse(ci, m)
end for
NONEscore = 0
for each node ci in Children(p) do
NONEscore += NONETree(ci, m)
end for
return max(Score(p, m, NONE)+MLscore,
Score(p, m, ARG)+NONEscore)
end if
NONETree(p,m)
NONEscore = Score(p, m, NONE)
if parse p is a leaf node then
return NONEscore
else
for each node ci in Children(p) do
NONEscore += NONETree(ci, m)
end for
return NONEscore
end if
Subroutine:
</equation>
<bodyText confidence="0.985962142857143">
Children(p) returns the list of children nodes of p.
Score(p, m, state) returns the log likelihood assigned by
model m, for parse p with state. state is either ARG or
NONE.
NomBank sections 02-21 are used as training
data, section 24 and 23 are used as development
and test data, respectively.
</bodyText>
<subsectionHeader confidence="0.998046">
3.1 Training data preprocessing
</subsectionHeader>
<bodyText confidence="0.9999478">
Unlike PropBank annotation which does not con-
tain overlapping arguments (in the form of parse
tree nodes domination) and does not allow pred-
icates to be dominated by arguments, NomBank
annotation in the September 2005 release contains
such cases. In NomBank sections 02-21, about
0.6% of the argument nodes dominate some other
argument nodes or the predicate. To simplify our
task, during training example generation, we ig-
nore arguments that dominate the predicate. We
also ignore arguments that are dominated by other
arguments, so that when argument domination oc-
curs, only the argument with the largest word span
is kept. We do not perform similar pruning on the
test data.
</bodyText>
<sectionHeader confidence="0.981448" genericHeader="introduction">
4 Features and feature selection
</sectionHeader>
<subsectionHeader confidence="0.98153">
4.1 Baseline NomBank SRL features
</subsectionHeader>
<bodyText confidence="0.9896635">
Table 1 lists the baseline features we adapted from
previous PropBank-based SRL systems (Pradhan
et al., 2005; Xue and Palmer, 2004). For ease
of description, related features are grouped, with
a specific individual feature given individual ref-
erence name. For example, feature b11FW in
the group b11 denotes the first word spanned by
the constituent and b13LH denotes the left sis-
ter’s head word. We also experimented with vari-
ous feature combinations, inspired by the features
used in (Xue and Palmer, 2004). These are listed
as features b31 to b34 in Table 1.
Suppose the current constituent under identifi-
cation or classification is “NP-Ben Bernanke” in
Figure 1. The instantiations of the baseline fea-
tures in Table 1 for this example are presented in
Table 2. The symbol “NULL” is used to denote
features that fail to instantiate.
</bodyText>
<subsectionHeader confidence="0.908339">
4.2 NomBank-specific features
</subsectionHeader>
<bodyText confidence="0.997806777777778">
4.2.1 NomBank predicate morphology and
class
The “NomBank-morph” dictionary provided by
the current NomBank release maps the base form
of a noun to various morphological forms. Be-
sides singular-plural noun form mapping, it also
maps base nouns to hyphenated and compound
nouns. For example, “healthcare” and “medical-
care” both map to “care”. For NomBank SRL fea-
</bodyText>
<table confidence="0.99157759375">
140
Baseline Features (Pradhan et al., 2005)
b1 predicate: stemmed noun
b2 subcat: grammar rule that expands the predicate’s
b3 parent
b4 phrase type: syntactic category of the constituent
b5 head word: syntactic head of the constituent
b6 path: syntactic path from the constituent to the
predicate
position: to the left or right of the predicate
b11 first or last word/POS spanned by the constituent
b12 (b11FW, b11LW, b11FP, b11LP)
b13 phrase type of the left or right sister (b12L, b12R)
b14 left or right sister’s head word/POS (b13LH,
b15 b13LP, b13RH, b13RP)
b16 phrase type of parent
b17 parent’s head word or its POS (b15H, b15P)
b18 head word of the constituent if its parent has phrase
b19 type PP
b20 head word or POS tag of the rightmost NP node, if
b21 the constituent is PP (b17H, b17P)
phrase type appended with the length of path
temporal keyword, e.g., ”Monday”
partial path from the constituent to the lowest com-
mon ancestor with the predicate
projected path from the constituent to the highest
NP dominating the predicate
Baseline Combined Features (Xue and Palmer, 2004)
b31 b1 &amp; b3
b32 b1 &amp; b4
b33 b1 &amp; b5
b34 b1 &amp; b6
</table>
<tableCaption confidence="0.999681">
Table 1: Baseline features for NomBank SRL
</tableCaption>
<bodyText confidence="0.999934818181818">
tures, we use this set of more specific mappings
to replace the morphological mappings based on
WordNet. Specifically, we replace feature b1 in
Table 1 with feature a1 in Table 3.
The current NomBank release also contains
the “NOMLEX-PLUS” dictionary, which con-
tains the class of nominal predicates according to
their origin and the roles they play. For exam-
ple, “employment” originates from the verb “em-
ploy” and is classified as “VERB-NOM”, while
the nouns “employer” and “employee” are classi-
fied as “SUBJECT” and “OBJECT” respectively.
Other classes include “ADJ-NOM” for nominal-
ization of adjectives and “NOM-REL” for rela-
tional nouns. The class of a nominal predicate is
very indicative of the role of its arguments. We
would expect a “VERB-NOM” predicate to take
both ARG0 and ARG1, while an “OBJECT” pred-
icate to take only ARG0. We incorporated the
class of nominal predicates as additional features
in our NomBank SRL system. We add feature a2
in Table 3 to use this information.
</bodyText>
<table confidence="0.997518130434783">
Baseline Features (Pradhan et al., 2005)
b1 replacement
b2 NP --+ NP NN
b3 NP
b4 Bernanke
b5 NPTSIVPIVPIPPINPINN
b6 left
b11 Ben, Bernanke, NNP, NNP
b12 NULL, VP
b13 NULL, NULL, was, VBD
b14 S
b15 was, VBD
b16 NULL
b17 NULL, NULL
b18 NP-7
b19 NULL
b20 NPTS
b21 NPTSIVPIVPIPPINP
Baseline Combined Features (Xue and Palmer, 2004)
b31 replacement &amp; NP
b32 replacement &amp; Bernanke
b33 replacement &amp; NPTSIVPIVPIPPINPINN
b34 replacement &amp; left
</table>
<tableCaption confidence="0.702316333333333">
Table 2: Baseline feature instantiations, assuming
the current constituent is “NP-Ben Bernanke” in
Figure 1.
</tableCaption>
<table confidence="0.980019125">
Additional Features Based on NomBank
a1 Nombank morphed noun stem
a2 Nombank nominal class
a3 identical to predicate?
a4 a DEFREL noun?
a5 whether under the noun phrase headed by the pred-
a6 icate
a7 whether the noun phrase headed by the predicate
is dominated by a VP node or has neighboring VP
nodes
whether there is a verb between the constituent and
the predicate
Additional Combined Features
a11 a1 &amp; a2
a12 a1 &amp; a3
a13 a1 &amp; a5
a14 a3 &amp; a4
a15 a1 &amp; a6
a16 a1 &amp; a7
Additional Features of Neighboring Arguments
n1 for each argument already classified, b3-b4-b5-b6-
n2 r, where r is the argument class, otherwise b3-b4-
b5-b6
backoff version of n1, b3-b6-r or b3-b6
</table>
<tableCaption confidence="0.8403465">
Table 3: Additional NomBank-specific features
for NomBank SRL
</tableCaption>
<subsectionHeader confidence="0.316919">
4.2.2 DEFREL relational noun predicate
</subsectionHeader>
<bodyText confidence="0.999338333333333">
About 14% of the argument node instances in
NomBank sections 02-21 are identical to their
nominal predicate nodes. Most of these nominal
predicates are DEFREL relational nouns (Mey-
ers et al., 2004c). Examples of DEFREL rela-
tional nouns include “employee”, “participant”,
</bodyText>
<page confidence="0.827711">
141
</page>
<bodyText confidence="0.974377444444445">
and “husband”, where the nominal predicate itself
takes part as an implied argument.
We include in our classification features an indi-
cator of whether the argument coincides with the
nominal predicate. We also include a feature test-
ing if the argument is one of the DEFREL nouns
we extracted from NomBank training sections 02-
21. These two features correspond to a3 and a4 in
Table 3.
</bodyText>
<subsectionHeader confidence="0.67688">
4.2.3 Support verb
</subsectionHeader>
<bodyText confidence="0.999980230769231">
Statistics show that almost 60% of the argu-
ments of nominal predicates occur locally inside
the noun phrase headed by the nominal pred-
icate. For the cases where an argument ap-
pears outside the local noun phrase, over half of
these arguments are introduced by support verbs.
Consider our example “Ben Bernanke was nomi-
nated as Greenspan’s replacement.”, the argument
“Ben Bernanke” is introduced by the support verb
“nominate”. The arguments introduced by sup-
port verbs can appear syntactically distant from
the nominal predicate.
To capture the location of arguments and the
existence of support verbs, we add features in-
dicating whether the argument is under the noun
phrase headed by the predicate, whether the noun
phrase headed by the predicate is dominated by
a VP phrase or has neighboring VP phrases, and
whether there is a verb between the argument and
the predicate. These are represented as features
a5, a6, and a7 in Table 3. Feature a7 was also pro-
posed by the system in (Pradhan et al., 2004).
We also experimented with various feature
combinations, inspired by the features used
in (Xue and Palmer, 2004). These are listed as
features a11 to a16 in Table 3.
</bodyText>
<subsectionHeader confidence="0.752579">
4.2.4 Neighboring arguments
</subsectionHeader>
<bodyText confidence="0.999983681818182">
The research of (Jiang et al., 2005; Toutanova et
al., 2005) has shown the importance of capturing
information of the global argument frame in order
to correctly classify the local argument.
We make use of the features {b3,b4,b5,b6} of
the neighboring arguments as defined in Table 1.
Arguments are classified from left to right in the
textual order they appear. For arguments that are
already labeled, we also add their argument class
r. Specifically, for each argument to the left of the
current argument, we have a feature b3-b4-b5-b6-
r. For each argument to the right of the current
argument, the feature is defined as b3-b4-b5-b6.
We extract features in a window of size 7, centered
at the current argument. We also add a backoff
version (b3-b6-r or b3-b6) of this specific feature.
These additional features are shown as n1 and n2
in Table 3.
Suppose the current constituent under identi-
fication or classification is “NP-Ben Bernanke”.
The instantiations of the additional features in Ta-
ble 3 are listed in Table 4.
</bodyText>
<table confidence="0.793797444444444">
Additional Features based on NomBank
a1 replacement
a2 VERB-NOM
a3 no
a4 no
a5 no
a6 yes
a7 yes
Additional Combined Features
a11 replacement &amp; VERB-NOM
a12 replacement &amp; no
a13 replacement &amp; no
a14 no &amp; no
a15 replacement &amp; yes
a16 replacement &amp; yes
Additional Features of Neighboring Arguments
n1 NP-Greenspan-NPTNPINN-left
n2 NP-left
</table>
<tableCaption confidence="0.672055333333333">
Table 4: Additional feature instantiations, assum-
ing the current constituent is “NP-Ben Bernanke”
in Figure 1.
</tableCaption>
<subsectionHeader confidence="0.994445">
4.3 Feature selection
</subsectionHeader>
<bodyText confidence="0.999794428571428">
Features used by our SRL system are automati-
cally extracted from PTB II parse trees manually
labeled in NomBank. Features from Table 1 and
Table 3 are selected empirically and incremen-
tally according to their contribution to test accu-
racy on the development section 24. The feature
selection process stops when adding any of the
remaining features fails to improve the SRL ac-
curacy on development section 24. We start the
selection process with the basic set of features
{b1,b2,b3,b4,b5,b6}. The detailed feature selec-
tion algorithm is presented in Algorithm 2.
Features for argument identification and argu-
ment classification are independently selected. To
select the features for argument classification, we
assume that all arguments have been correctly
identified.
After performing greedy feature selection, the
baseline set of features selected for identification
is {b1-b6, b11FW, b11LW, b12L, b13RH, b13RP,
b14, b15H, b18, b20, b32-b34}, and the baseline
</bodyText>
<figure confidence="0.766914166666667">
142
Algorithm 2 Greedy feature selection
Input Fcandidate{set of all candidate features}
Output Fselect{set of selected features}
Output Mselect{selected model}
Initialize:
</figure>
<equation confidence="0.900157941176471">
Fselect = {b1, b2, b3, b4, b5, b6}
Fcandidate = AllFeatures − Fselect
Mselect = Train(Fselect)
Eselect = Evaluate(Mselect, DevData)
loop
for each feature fi in Fcandidate do
Fi = Fselect U fi
Mi = Train(Fi)
Ei = Evaluate(Mi, DevData)
end for
Emax = Max(Ei)
if Emax &gt; Eselect then
Fselect = Fselect U fmax
Mselect = Mmax
Eselect = Emax
Fcandidate = Fcandidate − fmax
end if
</equation>
<construct confidence="0.643408">
if Fcandidate == Y&apos; or Emax C Eselect then
</construct>
<table confidence="0.829424125">
return Fselect, Mselect
end if
end loop
Subroutine:
Evaluate(Model, Data) returns the accuracy score by
evaluating Model on Data.
Train(FeatureSet) returns maxent model trained on the
given feature set.
</table>
<bodyText confidence="0.9225455">
set of features selected for classification is {b1-b6,
b11, b12, b13LH, b13LP, b13RP, b14, b15, b16,
b17P, b20, b31-b34}. Note that features in {b19,
b21} are not selected. For the additional features
in Table 3, greedy feature selection chose {a1, a5,
a6, a11, a12, a14} for the identification model and
{a1, a3, a6, a11, a14, a16, n1, n2} for the classifi-
cation model.
</bodyText>
<sectionHeader confidence="0.997231" genericHeader="method">
5 Experimental results
</sectionHeader>
<subsectionHeader confidence="0.999014">
5.1 Scores on development section 24
</subsectionHeader>
<bodyText confidence="0.999737230769231">
After applying the feature selection algorithm in
Section 4.3, the SRL F1 scores on development
section 24 are presented in Table 5. We sepa-
rately present the F1 score for identification-only
and classification-only model. We also apply the
classification model on the output of the identifica-
tion phase (which may contain erroneously identi-
fied arguments in general) to obtain the combined
accuracy. During the identification-only and com-
bined identification and classification testing, the
tree log-probability maximization algorithm based
on Equation 2 (and its extension to multi-classes)
is used. During the classification-only testing, we
</bodyText>
<table confidence="0.969007">
identification classification combined
baseline 80.32 84.86 69.70
additional 80.55 87.31 70.12
</table>
<tableCaption confidence="0.991325">
Table 5: Noml3ank SRL F1 scores on develop-
ment section 24, based on correct parse trees
</tableCaption>
<table confidence="0.994778666666667">
identification classification combined
baseline 82.33 85.85 72.20
additional 82.50 87.80 72.73
</table>
<tableCaption confidence="0.950336">
Table 6: Noml3ank SRL F1 scores on test section
23, based on correct parse trees
</tableCaption>
<bodyText confidence="0.993172833333333">
classify each correctly identified argument using
the classification ME model. The “baseline” row
lists the F1 scores when only the baseline fea-
tures are used, and the “additional” row lists the
F1 scores when additional features are added to
the baseline features.
</bodyText>
<subsectionHeader confidence="0.999926">
5.2 Testing on section 23
</subsectionHeader>
<bodyText confidence="0.999992666666667">
The identification and classification models based
on the chosen features in Section 4.3 are then ap-
plied to test section 23. The resulting F1 scores
are listed in Table 6. Using additional features, the
identification-only, classification-only, and com-
bined F1 scores are 82.50, 87.80, and 72.73, re-
spectively.
Performing chi-square test at the level of sig-
nificance 0.05, we found that the improvement of
the classification model using additional features
compared to using just the baseline features is sta-
tistically significant, while the corresponding im-
provements due to additional features for the iden-
tification model and the combined model are not
statistically significant.
The improved classification accuracy due to the
use of additional features does not contribute any
significant improvement to the combined identifi-
cation and classification SRL accuracy. This is due
to the noisy arguments identified by the inadequate
identification model, since the accurate determi-
nation of the additional features (such as those of
neighboring arguments) depends critically on an
accurate identification model.
</bodyText>
<subsectionHeader confidence="0.998174">
5.3 Using automatic syntactic parse trees
</subsectionHeader>
<bodyText confidence="0.9999715">
So far we have assumed the availability of cor-
rect syntactic parse trees during model training
and testing. We relax this assumption by using
the re-ranking parser presented in (Charniak and
</bodyText>
<page confidence="0.850799">
143
</page>
<bodyText confidence="0.999745076923077">
Johnson, 2005) to automatically generate the syn-
tactic parse trees for both training and test data.
The F1 scores of our best NomBank SRL sys-
tem, when applied to automatic syntactic parse
trees, are 66.77 for development section 24 and
69.14 for test section 23. These F1 scores are for
combined identification and classification, with
the use of additional features. Comparing these
scores with those in Table 5 and Table 6, the usage
of automatic parse trees lowers the F1 accuracy by
more than 3%. The decrease in accuracy is ex-
pected, due to the noise introduced by automatic
syntactic parsing.
</bodyText>
<sectionHeader confidence="0.995888" genericHeader="evaluation">
6 Discussion and future work
</sectionHeader>
<subsectionHeader confidence="0.9834775">
6.1 Comparison of the composition of
PropBank and NomBank
</subsectionHeader>
<bodyText confidence="0.999718454545455">
Counting the number of annotated predicates, the
size of the September 2005 release of NomBank
(NomBank.0.8) is about 83% of PropBank release
1. Preliminary consistency tests reported in (Mey-
ers et al., 2004c) shows that NomBank’s inter-
annotator agreement rate is about 85% for core
arguments and lower for adjunct arguments. The
inter-annotator agreement for PropBank reported
in (Palmer et al., 2005) is above 0.9 in terms of the
Kappa statistic (Sidney and Castellan Jr., 1988).
While the two agreement measures are not di-
rectly comparable, the current NomBank.0.8 re-
lease documentation indicates that only 32 of the
most frequently occurring nouns in PTB II have
been adjudicated.
We believe the smaller size of NomBank.0.8
and the potential noise contained in the current re-
lease of the NomBank data may partly explain our
lower SRL accuracy on NomBank, especially in
the argument identification phase, as compared to
the published accuracies of PropBank-based SRL
systems.
</bodyText>
<subsectionHeader confidence="0.992518">
6.2 Difficulties in NomBank SRL
</subsectionHeader>
<bodyText confidence="0.92100856">
The argument structure of nominalization phrases
is less fixed (i.e., more flexible) than the argument
structure of verbs. Consider again the example
given in the introduction, we find the following
flexibility in forming grammatical NomBank ar-
gument structures for “replacement”:
• The positions of the arguments are flexi-
ble, so that “Greenspan’s replacement Ben
Bernanke”, ”Ben Bernanke’s replacement of
Greenspan” are both grammatical.
• Arguments can be optional, so that
“Greenspan’s replacement will assume
the post soon”, “The replacement Ben
Bernanke will assume the post soon”, and
“The replacement will assume the post soon”
are all grammatical.
With the verb predicate “replace”, except for
“Greenspan was replaced”, there is no freedom of
forming phrases like “Ben Bernanke replaces” or
simply “replaces” without supplying the necessary
arguments to complete the grammatical structure.
We believe the flexible argument structure of
NomBank noun predicates contributes to the lower
automatic SRL accuracy as compared to that of the
PropBank SRL task.
</bodyText>
<subsectionHeader confidence="0.8029345">
6.3 Integrating PropBank and NomBank
SRL
</subsectionHeader>
<bodyText confidence="0.9976318">
Work in (Pustejovsky et al., 2005) discussed the
possibility of merging various Treebank annota-
tion efforts including PropBank, NomBank, and
others. Future work involves studying ways
of concurrently producing automatic PropBank
and NomBank SRL, and improving the accuracy
by exploiting the inter-relationship between verb
predicate-argument and noun predicate-argument
structures.
Besides the obvious correspondence between a
verb and its nominalizations, e.g., “replace” and
“replacement”, there is also correspondence be-
tween verb predicates in PropBank and support
verbs in NomBank. Statistics from NomBank sec-
tions 02-21 show that 86% of the support verbs in
NomBank are also predicate verbs in PropBank.
When they coincide, they share 18,250 arguments
of which 63% have the same argument class in
PropBank and NomBank.
Possible integration approaches include:
</bodyText>
<listItem confidence="0.9239236">
• Using PropBank data as augmentation to
NomBank training data.
• Using re-ranking techniques (Collins, 2000)
to jointly improve PropBank and NomBank
SRL accuracy.
</listItem>
<sectionHeader confidence="0.994575" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999858">
We have successfully developed a statistical
NomBank-based SRL system. Features that were
previously shown to be effective in PropBank SRL
are carefully selected and adapted for NomBank
SRL. We also proposed new features to address
</bodyText>
<page confidence="0.783843">
144
</page>
<bodyText confidence="0.998318666666667">
the special predicate-argument structure in Nom-
Bank data. To our knowledge, we presented the
first result in statistical NomBank SRL.
</bodyText>
<sectionHeader confidence="0.996144" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999906479452055">
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A Maximum Entropy Approach to
Natural Language Processing. Computational Lin-
guistics.
Xavier Carreras and Lluis Marquez. 2005. Intro-
duction to the CoNLL-2005 Shared Task: Semantic
Role Labeling. In Proceedings of CoNLL-2005.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings ofACL-2005.
Michael Collins. 2000. Discriminative Reranking for
Natural Language Parsing. In Proceedings ofICML
2000.
Zheng Ping Jiang, Jia Li, and Hwee Tou Ng. 2005.
Semantic Argument Classification Exploiting Argu-
ment Interdependence. In Proceedings of IJCAI
2005.
Paul Kingsbury and Martha Palmer. 2003. PropBank:
the Next Level of TreeBank. In Proceedings of Tree-
banks and Lexical Theories.
Kenneth C. Litkowski. 2004. SENSEVAL-3 Task: Au-
tomatic Labeling of Semantic Roles. In Proceedings
of Senseval-3: The Third International Workshop on
the Evaluation of Systems for the Semantic Analysis
of Text.
Catherine Macleod, Adam Meyers, Ralph Grishman,
Leslie Barrett, and Ruth Reeves. 1997. Designing a
Dictionary of Derived Nominals. In Proceedings of
Recent Advances in Natural Language Processing.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. NOMLEX:
A Lexicon of Nominalizations. In Proceedings of
EURALEX’98.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, and Brian
Young. 2004a. The Cross-Breeding of Dictionar-
ies. In Proceedings ofLREC-2004.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004b. Annotating Noun Ar-
gument Structure for NomBank. In Proceedings of
LREC-2004.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004c. The NomBank
Project: An Interim Report. In Proceedings ofHLT-
NAACL 2004 Workshop on Frontiers in Corpus An-
notation.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Corpus
of Semantic Roles. Computational Linguistics.
Sameer S. Pradhan, Honglin Sun, Wayne Ward,
James H. Martin, and Dan Jurafsky. 2004. Parsing
Arguments of Nominalizations in English and Chi-
nese. In Proceedings ofHLT/NAACL 2004.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James H. Martin, and Daniel Juraf-
sky. 2005. Support Vector Learning for Semantic
Argument Classification. Machine Learning.
James Pustejovsky, Adam Meyers, Martha Palmer, and
Massimo Poesio. 2005. Merging PropBank, Nom-
Bank, TimeBank, Penn Discourse Treebank and
Coreference. In ACL 2005 Workshop on Frontiers
in Corpus Annotations II: Pie in the Sky.
Siegel Sidney and N. John Castellan Jr. 1988. Non-
parametric Statistics for the Behavioral Sciences.
McGraw-Hill, New York.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint Learning Improves Semantic
Role Labeling. In Proceedings ofACL 2005.
Nianwen Xue and Martha Palmer. 2004. Calibrating
Features for Semantic Role Labeling. In Proceed-
ings of EMNLP-2004.
</reference>
<page confidence="0.951814">
145
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.427826">
<title confidence="0.99932">Semantic Role Labeling of NomBank: A Maximum Entropy Approach</title>
<author confidence="0.999597">Ping Jiang Tou</author>
<affiliation confidence="0.841407666666667">Department of Computer National University of 3 Science Drive 2, Singapore</affiliation>
<abstract confidence="0.988708611111111">This paper describes our attempt at NomBank-based automatic Semantic Role Labeling (SRL). NomBank is a project at New York University to annotate the argument structures for common nouns in the Penn Treebank II corpus. We treat the NomBank SRL task as a classification problem and explore the possibility of adapting features previously shown useful in PropBank-based SRL systems. Various NomBank-specific features are explored. On test section 23, our best system achieves F1 score of 72.73 (69.14) when correct (automatic) syntactic parse trees are used. To our knowledge, this is the first reported automatic NomBank SRL system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics.</title>
<date>1996</date>
<contexts>
<context position="5986" citStr="Berger et al., 1996" startWordPosition="957" endWordPosition="960">problem and divide it into two phases: argument identification and argument classification. During the argument identification phase, each parse tree node is marked as either argument or non-argument. Each node marked as argument is then labeled with a specific class during the argument classification phase. The identification model is a binary classifier , while the classification model is a multi-class classifier. Opennlp maxent1, an implementation of Maximum Entropy (ME) modeling, is used as the classification tool. Since its introduction to the Natural Language Processing (NLP) community (Berger et al., 1996), ME-based classifiers have been shown to be effective in various NLP tasks. ME modeling is based on the insight that the best model is consistent with the set of constraints imposed and otherwise as uniform as possible. ME models the probability of label l given input x as in Equation 1. fi(l, x) is a feature function that maps label l and input x to either 0 or 1, while the summation is over all n feature functions and with Ai as the weight parameter for each feature function fi(l, x). Zx is a normalization factor. In the identification model, label l corresponds to either “argument” or “non</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam Berger, Stephen Della Pietra, and Vincent Della Pietra. 1996. A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lluis Marquez</author>
</authors>
<title>Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL-2005.</booktitle>
<contexts>
<context position="1095" citStr="Carreras and Marquez, 2005" startWordPosition="167" endWordPosition="170">omBank SRL task as a classification problem and explore the possibility of adapting features previously shown useful in PropBank-based SRL systems. Various NomBank-specific features are explored. On test section 23, our best system achieves F1 score of 72.73 (69.14) when correct (automatic) syntactic parse trees are used. To our knowledge, this is the first reported automatic NomBank SRL system. 1 Introduction Automatic Semantic Role Labeling (SRL) systems, made possible by the availability of PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005), and encouraged by evaluation efforts in (Carreras and Marquez, 2005; Litkowski, 2004), have been shown to accurately determine the argument structure of verb predicates. A successful PropBank-based SRL system would correctly determine that “Ben Bernanke” is the subject (labeled as ARG0 in PropBank) of predicate “replace”, and “Greenspan” is the object (labeled as ARG1): • Ben Bernanke replaced Greenspan as Fed chair. • Greenspan was replaced by Ben Bernanke as Fed chair. The recent release of NomBank (Meyers et al., 2004c; Meyers et al., 2004b), a databank that annotates argument structure for instances of common nouns in the Penn Treebank II corpus, made it </context>
</contexts>
<marker>Carreras, Marquez, 2005</marker>
<rawString>Xavier Carreras and Lluis Marquez. 2005. Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling. In Proceedings of CoNLL-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-Best Parsing and MaxEnt Discriminative Reranking.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL-2005.</booktitle>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-Best Parsing and MaxEnt Discriminative Reranking. In Proceedings ofACL-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative Reranking for Natural Language Parsing.</title>
<date>2000</date>
<booktitle>In Proceedings ofICML</booktitle>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative Reranking for Natural Language Parsing. In Proceedings ofICML 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng Ping Jiang</author>
<author>Jia Li</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Semantic Argument Classification Exploiting Argument Interdependence.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCAI</booktitle>
<contexts>
<context position="16457" citStr="Jiang et al., 2005" startWordPosition="2704" endWordPosition="2707">s indicating whether the argument is under the noun phrase headed by the predicate, whether the noun phrase headed by the predicate is dominated by a VP phrase or has neighboring VP phrases, and whether there is a verb between the argument and the predicate. These are represented as features a5, a6, and a7 in Table 3. Feature a7 was also proposed by the system in (Pradhan et al., 2004). We also experimented with various feature combinations, inspired by the features used in (Xue and Palmer, 2004). These are listed as features a11 to a16 in Table 3. 4.2.4 Neighboring arguments The research of (Jiang et al., 2005; Toutanova et al., 2005) has shown the importance of capturing information of the global argument frame in order to correctly classify the local argument. We make use of the features {b3,b4,b5,b6} of the neighboring arguments as defined in Table 1. Arguments are classified from left to right in the textual order they appear. For arguments that are already labeled, we also add their argument class r. Specifically, for each argument to the left of the current argument, we have a feature b3-b4-b5-b6- r. For each argument to the right of the current argument, the feature is defined as b3-b4-b5-b6</context>
</contexts>
<marker>Jiang, Li, Ng, 2005</marker>
<rawString>Zheng Ping Jiang, Jia Li, and Hwee Tou Ng. 2005. Semantic Argument Classification Exploiting Argument Interdependence. In Proceedings of IJCAI 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kingsbury</author>
<author>Martha Palmer</author>
</authors>
<title>PropBank: the Next Level of TreeBank.</title>
<date>2003</date>
<booktitle>In Proceedings of Treebanks and Lexical Theories.</booktitle>
<contexts>
<context position="1004" citStr="Kingsbury and Palmer, 2003" startWordPosition="152" endWordPosition="155">ate the argument structures for common nouns in the Penn Treebank II corpus. We treat the NomBank SRL task as a classification problem and explore the possibility of adapting features previously shown useful in PropBank-based SRL systems. Various NomBank-specific features are explored. On test section 23, our best system achieves F1 score of 72.73 (69.14) when correct (automatic) syntactic parse trees are used. To our knowledge, this is the first reported automatic NomBank SRL system. 1 Introduction Automatic Semantic Role Labeling (SRL) systems, made possible by the availability of PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005), and encouraged by evaluation efforts in (Carreras and Marquez, 2005; Litkowski, 2004), have been shown to accurately determine the argument structure of verb predicates. A successful PropBank-based SRL system would correctly determine that “Ben Bernanke” is the subject (labeled as ARG0 in PropBank) of predicate “replace”, and “Greenspan” is the object (labeled as ARG1): • Ben Bernanke replaced Greenspan as Fed chair. • Greenspan was replaced by Ben Bernanke as Fed chair. The recent release of NomBank (Meyers et al., 2004c; Meyers et al., 2004b), a databank that annotate</context>
</contexts>
<marker>Kingsbury, Palmer, 2003</marker>
<rawString>Paul Kingsbury and Martha Palmer. 2003. PropBank: the Next Level of TreeBank. In Proceedings of Treebanks and Lexical Theories.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth C Litkowski</author>
</authors>
<title>SENSEVAL-3 Task: Automatic Labeling of Semantic Roles.</title>
<date>2004</date>
<booktitle>In Proceedings of Senseval-3: The Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text.</booktitle>
<contexts>
<context position="1113" citStr="Litkowski, 2004" startWordPosition="171" endWordPosition="172">ication problem and explore the possibility of adapting features previously shown useful in PropBank-based SRL systems. Various NomBank-specific features are explored. On test section 23, our best system achieves F1 score of 72.73 (69.14) when correct (automatic) syntactic parse trees are used. To our knowledge, this is the first reported automatic NomBank SRL system. 1 Introduction Automatic Semantic Role Labeling (SRL) systems, made possible by the availability of PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005), and encouraged by evaluation efforts in (Carreras and Marquez, 2005; Litkowski, 2004), have been shown to accurately determine the argument structure of verb predicates. A successful PropBank-based SRL system would correctly determine that “Ben Bernanke” is the subject (labeled as ARG0 in PropBank) of predicate “replace”, and “Greenspan” is the object (labeled as ARG1): • Ben Bernanke replaced Greenspan as Fed chair. • Greenspan was replaced by Ben Bernanke as Fed chair. The recent release of NomBank (Meyers et al., 2004c; Meyers et al., 2004b), a databank that annotates argument structure for instances of common nouns in the Penn Treebank II corpus, made it possible to develo</context>
</contexts>
<marker>Litkowski, 2004</marker>
<rawString>Kenneth C. Litkowski. 2004. SENSEVAL-3 Task: Automatic Labeling of Semantic Roles. In Proceedings of Senseval-3: The Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catherine Macleod</author>
<author>Adam Meyers</author>
<author>Ralph Grishman</author>
<author>Leslie Barrett</author>
<author>Ruth Reeves</author>
</authors>
<title>Designing a Dictionary of Derived Nominals.</title>
<date>1997</date>
<booktitle>In Proceedings of Recent Advances in Natural Language Processing.</booktitle>
<contexts>
<context position="3632" citStr="Macleod et al., 1997" startWordPosition="568" endWordPosition="571">tion 6 compares NomBank SRL to 138 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 138–145, Sydney, July 2006. c�2006 Association for Computational Linguistics PP     IN NP  as   NP (ARG1) Greenspan ’s VBN (Support) nominated NN predicate replacement  Figure 1: A sample sentence and its parse tree labeled in the style of NomBank   PropBank SRL and discusses possible future research directions. 2 Overview of NomBank The NomBank (Meyers et al., 2004c; Meyers et al., 2004b) annotation project originated from the NOMLEX (Macleod et al., 1997; Macleod et al., 1998) nominalization lexicon developed under the New York University Proteus Project. NOMLEX lists 1,000 nominalizations and the correspondences between their arguments and the arguments of their verb counterparts. NomBank frames combine various lexical resources (Meyers et al., 2004a), including an extended NOMLEX and PropBank frames, and form the basis for annotating the argument structures of common nouns. Similar to PropBank, NomBank annotation is made on the Penn TreeBank II (PTB II) corpus. For each common noun in PTB II that takes arguments, its core arguments are labe</context>
</contexts>
<marker>Macleod, Meyers, Grishman, Barrett, Reeves, 1997</marker>
<rawString>Catherine Macleod, Adam Meyers, Ralph Grishman, Leslie Barrett, and Ruth Reeves. 1997. Designing a Dictionary of Derived Nominals. In Proceedings of Recent Advances in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catherine Macleod</author>
<author>Ralph Grishman</author>
<author>Adam Meyers</author>
<author>Leslie Barrett</author>
<author>Ruth Reeves</author>
</authors>
<title>NOMLEX: A Lexicon of Nominalizations.</title>
<date>1998</date>
<booktitle>In Proceedings of EURALEX’98.</booktitle>
<contexts>
<context position="3655" citStr="Macleod et al., 1998" startWordPosition="572" endWordPosition="575">k SRL to 138 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 138–145, Sydney, July 2006. c�2006 Association for Computational Linguistics PP     IN NP  as   NP (ARG1) Greenspan ’s VBN (Support) nominated NN predicate replacement  Figure 1: A sample sentence and its parse tree labeled in the style of NomBank   PropBank SRL and discusses possible future research directions. 2 Overview of NomBank The NomBank (Meyers et al., 2004c; Meyers et al., 2004b) annotation project originated from the NOMLEX (Macleod et al., 1997; Macleod et al., 1998) nominalization lexicon developed under the New York University Proteus Project. NOMLEX lists 1,000 nominalizations and the correspondences between their arguments and the arguments of their verb counterparts. NomBank frames combine various lexical resources (Meyers et al., 2004a), including an extended NOMLEX and PropBank frames, and form the basis for annotating the argument structures of common nouns. Similar to PropBank, NomBank annotation is made on the Penn TreeBank II (PTB II) corpus. For each common noun in PTB II that takes arguments, its core arguments are labeled with ARG0, ARG1, et</context>
</contexts>
<marker>Macleod, Grishman, Meyers, Barrett, Reeves, 1998</marker>
<rawString>Catherine Macleod, Ralph Grishman, Adam Meyers, Leslie Barrett, and Ruth Reeves. 1998. NOMLEX: A Lexicon of Nominalizations. In Proceedings of EURALEX’98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
<author>Ruth Reeves</author>
<author>Catherine Macleod</author>
<author>Rachel Szekely</author>
<author>Veronika Zielinska</author>
<author>Brian Young</author>
</authors>
<title>The Cross-Breeding of Dictionaries.</title>
<date>2004</date>
<booktitle>In Proceedings ofLREC-2004.</booktitle>
<contexts>
<context position="1554" citStr="Meyers et al., 2004" startWordPosition="238" endWordPosition="241"> possible by the availability of PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005), and encouraged by evaluation efforts in (Carreras and Marquez, 2005; Litkowski, 2004), have been shown to accurately determine the argument structure of verb predicates. A successful PropBank-based SRL system would correctly determine that “Ben Bernanke” is the subject (labeled as ARG0 in PropBank) of predicate “replace”, and “Greenspan” is the object (labeled as ARG1): • Ben Bernanke replaced Greenspan as Fed chair. • Greenspan was replaced by Ben Bernanke as Fed chair. The recent release of NomBank (Meyers et al., 2004c; Meyers et al., 2004b), a databank that annotates argument structure for instances of common nouns in the Penn Treebank II corpus, made it possible to develop automatic SRL systems that analyze the argument structures of noun predicates. Given the following two noun phrases and one sentence, a successful NomBank-based SRL system should label “Ben Bernanke” as the subject (ARG0) and “Greenspan” as the object (ARG1) of the noun predicate “replacement”. • Greenspan’s replacement Ben Bernanke • Ben Bernanke’s replacement of Greenspan • Ben Bernanke was nominated as Greenspan’s replacement. The a</context>
<context position="3540" citStr="Meyers et al., 2004" startWordPosition="554" endWordPosition="557">ategy, Section 5 explains the experimental setup and presents the experimental results, Section 6 compares NomBank SRL to 138 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 138–145, Sydney, July 2006. c�2006 Association for Computational Linguistics PP     IN NP  as   NP (ARG1) Greenspan ’s VBN (Support) nominated NN predicate replacement  Figure 1: A sample sentence and its parse tree labeled in the style of NomBank   PropBank SRL and discusses possible future research directions. 2 Overview of NomBank The NomBank (Meyers et al., 2004c; Meyers et al., 2004b) annotation project originated from the NOMLEX (Macleod et al., 1997; Macleod et al., 1998) nominalization lexicon developed under the New York University Proteus Project. NOMLEX lists 1,000 nominalizations and the correspondences between their arguments and the arguments of their verb counterparts. NomBank frames combine various lexical resources (Meyers et al., 2004a), including an extended NOMLEX and PropBank frames, and form the basis for annotating the argument structures of common nouns. Similar to PropBank, NomBank annotation is made on the Penn TreeBank II (PTB </context>
<context position="14738" citStr="Meyers et al., 2004" startWordPosition="2416" endWordPosition="2420">ent and the predicate Additional Combined Features a11 a1 &amp; a2 a12 a1 &amp; a3 a13 a1 &amp; a5 a14 a3 &amp; a4 a15 a1 &amp; a6 a16 a1 &amp; a7 Additional Features of Neighboring Arguments n1 for each argument already classified, b3-b4-b5-b6- n2 r, where r is the argument class, otherwise b3-b4- b5-b6 backoff version of n1, b3-b6-r or b3-b6 Table 3: Additional NomBank-specific features for NomBank SRL 4.2.2 DEFREL relational noun predicate About 14% of the argument node instances in NomBank sections 02-21 are identical to their nominal predicate nodes. Most of these nominal predicates are DEFREL relational nouns (Meyers et al., 2004c). Examples of DEFREL relational nouns include “employee”, “participant”, 141 and “husband”, where the nominal predicate itself takes part as an implied argument. We include in our classification features an indicator of whether the argument coincides with the nominal predicate. We also include a feature testing if the argument is one of the DEFREL nouns we extracted from NomBank training sections 02- 21. These two features correspond to a3 and a4 in Table 3. 4.2.3 Support verb Statistics show that almost 60% of the arguments of nominal predicates occur locally inside the noun phrase headed b</context>
<context position="23638" citStr="Meyers et al., 2004" startWordPosition="3830" endWordPosition="3834">ores are for combined identification and classification, with the use of additional features. Comparing these scores with those in Table 5 and Table 6, the usage of automatic parse trees lowers the F1 accuracy by more than 3%. The decrease in accuracy is expected, due to the noise introduced by automatic syntactic parsing. 6 Discussion and future work 6.1 Comparison of the composition of PropBank and NomBank Counting the number of annotated predicates, the size of the September 2005 release of NomBank (NomBank.0.8) is about 83% of PropBank release 1. Preliminary consistency tests reported in (Meyers et al., 2004c) shows that NomBank’s interannotator agreement rate is about 85% for core arguments and lower for adjunct arguments. The inter-annotator agreement for PropBank reported in (Palmer et al., 2005) is above 0.9 in terms of the Kappa statistic (Sidney and Castellan Jr., 1988). While the two agreement measures are not directly comparable, the current NomBank.0.8 release documentation indicates that only 32 of the most frequently occurring nouns in PTB II have been adjudicated. We believe the smaller size of NomBank.0.8 and the potential noise contained in the current release of the NomBank data ma</context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, 2004</marker>
<rawString>Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel Szekely, Veronika Zielinska, and Brian Young. 2004a. The Cross-Breeding of Dictionaries. In Proceedings ofLREC-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
<author>Ruth Reeves</author>
<author>Catherine Macleod</author>
<author>Rachel Szekely</author>
<author>Veronika Zielinska</author>
<author>Brian Young</author>
<author>Ralph Grishman</author>
</authors>
<title>Annotating Noun Argument Structure for NomBank.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC-2004.</booktitle>
<contexts>
<context position="1554" citStr="Meyers et al., 2004" startWordPosition="238" endWordPosition="241"> possible by the availability of PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005), and encouraged by evaluation efforts in (Carreras and Marquez, 2005; Litkowski, 2004), have been shown to accurately determine the argument structure of verb predicates. A successful PropBank-based SRL system would correctly determine that “Ben Bernanke” is the subject (labeled as ARG0 in PropBank) of predicate “replace”, and “Greenspan” is the object (labeled as ARG1): • Ben Bernanke replaced Greenspan as Fed chair. • Greenspan was replaced by Ben Bernanke as Fed chair. The recent release of NomBank (Meyers et al., 2004c; Meyers et al., 2004b), a databank that annotates argument structure for instances of common nouns in the Penn Treebank II corpus, made it possible to develop automatic SRL systems that analyze the argument structures of noun predicates. Given the following two noun phrases and one sentence, a successful NomBank-based SRL system should label “Ben Bernanke” as the subject (ARG0) and “Greenspan” as the object (ARG1) of the noun predicate “replacement”. • Greenspan’s replacement Ben Bernanke • Ben Bernanke’s replacement of Greenspan • Ben Bernanke was nominated as Greenspan’s replacement. The a</context>
<context position="3540" citStr="Meyers et al., 2004" startWordPosition="554" endWordPosition="557">ategy, Section 5 explains the experimental setup and presents the experimental results, Section 6 compares NomBank SRL to 138 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 138–145, Sydney, July 2006. c�2006 Association for Computational Linguistics PP     IN NP  as   NP (ARG1) Greenspan ’s VBN (Support) nominated NN predicate replacement  Figure 1: A sample sentence and its parse tree labeled in the style of NomBank   PropBank SRL and discusses possible future research directions. 2 Overview of NomBank The NomBank (Meyers et al., 2004c; Meyers et al., 2004b) annotation project originated from the NOMLEX (Macleod et al., 1997; Macleod et al., 1998) nominalization lexicon developed under the New York University Proteus Project. NOMLEX lists 1,000 nominalizations and the correspondences between their arguments and the arguments of their verb counterparts. NomBank frames combine various lexical resources (Meyers et al., 2004a), including an extended NOMLEX and PropBank frames, and form the basis for annotating the argument structures of common nouns. Similar to PropBank, NomBank annotation is made on the Penn TreeBank II (PTB </context>
<context position="14738" citStr="Meyers et al., 2004" startWordPosition="2416" endWordPosition="2420">ent and the predicate Additional Combined Features a11 a1 &amp; a2 a12 a1 &amp; a3 a13 a1 &amp; a5 a14 a3 &amp; a4 a15 a1 &amp; a6 a16 a1 &amp; a7 Additional Features of Neighboring Arguments n1 for each argument already classified, b3-b4-b5-b6- n2 r, where r is the argument class, otherwise b3-b4- b5-b6 backoff version of n1, b3-b6-r or b3-b6 Table 3: Additional NomBank-specific features for NomBank SRL 4.2.2 DEFREL relational noun predicate About 14% of the argument node instances in NomBank sections 02-21 are identical to their nominal predicate nodes. Most of these nominal predicates are DEFREL relational nouns (Meyers et al., 2004c). Examples of DEFREL relational nouns include “employee”, “participant”, 141 and “husband”, where the nominal predicate itself takes part as an implied argument. We include in our classification features an indicator of whether the argument coincides with the nominal predicate. We also include a feature testing if the argument is one of the DEFREL nouns we extracted from NomBank training sections 02- 21. These two features correspond to a3 and a4 in Table 3. 4.2.3 Support verb Statistics show that almost 60% of the arguments of nominal predicates occur locally inside the noun phrase headed b</context>
<context position="23638" citStr="Meyers et al., 2004" startWordPosition="3830" endWordPosition="3834">ores are for combined identification and classification, with the use of additional features. Comparing these scores with those in Table 5 and Table 6, the usage of automatic parse trees lowers the F1 accuracy by more than 3%. The decrease in accuracy is expected, due to the noise introduced by automatic syntactic parsing. 6 Discussion and future work 6.1 Comparison of the composition of PropBank and NomBank Counting the number of annotated predicates, the size of the September 2005 release of NomBank (NomBank.0.8) is about 83% of PropBank release 1. Preliminary consistency tests reported in (Meyers et al., 2004c) shows that NomBank’s interannotator agreement rate is about 85% for core arguments and lower for adjunct arguments. The inter-annotator agreement for PropBank reported in (Palmer et al., 2005) is above 0.9 in terms of the Kappa statistic (Sidney and Castellan Jr., 1988). While the two agreement measures are not directly comparable, the current NomBank.0.8 release documentation indicates that only 32 of the most frequently occurring nouns in PTB II have been adjudicated. We believe the smaller size of NomBank.0.8 and the potential noise contained in the current release of the NomBank data ma</context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel Szekely, Veronika Zielinska, Brian Young, and Ralph Grishman. 2004b. Annotating Noun Argument Structure for NomBank. In Proceedings of LREC-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
<author>Ruth Reeves</author>
<author>Catherine Macleod</author>
<author>Rachel Szekely</author>
<author>Veronika Zielinska</author>
<author>Brian Young</author>
<author>Ralph Grishman</author>
</authors>
<title>The NomBank Project: An Interim Report.</title>
<date>2004</date>
<booktitle>In Proceedings ofHLTNAACL 2004 Workshop on Frontiers in Corpus Annotation.</booktitle>
<contexts>
<context position="1554" citStr="Meyers et al., 2004" startWordPosition="238" endWordPosition="241"> possible by the availability of PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005), and encouraged by evaluation efforts in (Carreras and Marquez, 2005; Litkowski, 2004), have been shown to accurately determine the argument structure of verb predicates. A successful PropBank-based SRL system would correctly determine that “Ben Bernanke” is the subject (labeled as ARG0 in PropBank) of predicate “replace”, and “Greenspan” is the object (labeled as ARG1): • Ben Bernanke replaced Greenspan as Fed chair. • Greenspan was replaced by Ben Bernanke as Fed chair. The recent release of NomBank (Meyers et al., 2004c; Meyers et al., 2004b), a databank that annotates argument structure for instances of common nouns in the Penn Treebank II corpus, made it possible to develop automatic SRL systems that analyze the argument structures of noun predicates. Given the following two noun phrases and one sentence, a successful NomBank-based SRL system should label “Ben Bernanke” as the subject (ARG0) and “Greenspan” as the object (ARG1) of the noun predicate “replacement”. • Greenspan’s replacement Ben Bernanke • Ben Bernanke’s replacement of Greenspan • Ben Bernanke was nominated as Greenspan’s replacement. The a</context>
<context position="3540" citStr="Meyers et al., 2004" startWordPosition="554" endWordPosition="557">ategy, Section 5 explains the experimental setup and presents the experimental results, Section 6 compares NomBank SRL to 138 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 138–145, Sydney, July 2006. c�2006 Association for Computational Linguistics PP     IN NP  as   NP (ARG1) Greenspan ’s VBN (Support) nominated NN predicate replacement  Figure 1: A sample sentence and its parse tree labeled in the style of NomBank   PropBank SRL and discusses possible future research directions. 2 Overview of NomBank The NomBank (Meyers et al., 2004c; Meyers et al., 2004b) annotation project originated from the NOMLEX (Macleod et al., 1997; Macleod et al., 1998) nominalization lexicon developed under the New York University Proteus Project. NOMLEX lists 1,000 nominalizations and the correspondences between their arguments and the arguments of their verb counterparts. NomBank frames combine various lexical resources (Meyers et al., 2004a), including an extended NOMLEX and PropBank frames, and form the basis for annotating the argument structures of common nouns. Similar to PropBank, NomBank annotation is made on the Penn TreeBank II (PTB </context>
<context position="14738" citStr="Meyers et al., 2004" startWordPosition="2416" endWordPosition="2420">ent and the predicate Additional Combined Features a11 a1 &amp; a2 a12 a1 &amp; a3 a13 a1 &amp; a5 a14 a3 &amp; a4 a15 a1 &amp; a6 a16 a1 &amp; a7 Additional Features of Neighboring Arguments n1 for each argument already classified, b3-b4-b5-b6- n2 r, where r is the argument class, otherwise b3-b4- b5-b6 backoff version of n1, b3-b6-r or b3-b6 Table 3: Additional NomBank-specific features for NomBank SRL 4.2.2 DEFREL relational noun predicate About 14% of the argument node instances in NomBank sections 02-21 are identical to their nominal predicate nodes. Most of these nominal predicates are DEFREL relational nouns (Meyers et al., 2004c). Examples of DEFREL relational nouns include “employee”, “participant”, 141 and “husband”, where the nominal predicate itself takes part as an implied argument. We include in our classification features an indicator of whether the argument coincides with the nominal predicate. We also include a feature testing if the argument is one of the DEFREL nouns we extracted from NomBank training sections 02- 21. These two features correspond to a3 and a4 in Table 3. 4.2.3 Support verb Statistics show that almost 60% of the arguments of nominal predicates occur locally inside the noun phrase headed b</context>
<context position="23638" citStr="Meyers et al., 2004" startWordPosition="3830" endWordPosition="3834">ores are for combined identification and classification, with the use of additional features. Comparing these scores with those in Table 5 and Table 6, the usage of automatic parse trees lowers the F1 accuracy by more than 3%. The decrease in accuracy is expected, due to the noise introduced by automatic syntactic parsing. 6 Discussion and future work 6.1 Comparison of the composition of PropBank and NomBank Counting the number of annotated predicates, the size of the September 2005 release of NomBank (NomBank.0.8) is about 83% of PropBank release 1. Preliminary consistency tests reported in (Meyers et al., 2004c) shows that NomBank’s interannotator agreement rate is about 85% for core arguments and lower for adjunct arguments. The inter-annotator agreement for PropBank reported in (Palmer et al., 2005) is above 0.9 in terms of the Kappa statistic (Sidney and Castellan Jr., 1988). While the two agreement measures are not directly comparable, the current NomBank.0.8 release documentation indicates that only 32 of the most frequently occurring nouns in PTB II have been adjudicated. We believe the smaller size of NomBank.0.8 and the potential noise contained in the current release of the NomBank data ma</context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel Szekely, Veronika Zielinska, Brian Young, and Ralph Grishman. 2004c. The NomBank Project: An Interim Report. In Proceedings ofHLTNAACL 2004 Workshop on Frontiers in Corpus Annotation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics.</title>
<date>2005</date>
<contexts>
<context position="1026" citStr="Palmer et al., 2005" startWordPosition="156" endWordPosition="159">for common nouns in the Penn Treebank II corpus. We treat the NomBank SRL task as a classification problem and explore the possibility of adapting features previously shown useful in PropBank-based SRL systems. Various NomBank-specific features are explored. On test section 23, our best system achieves F1 score of 72.73 (69.14) when correct (automatic) syntactic parse trees are used. To our knowledge, this is the first reported automatic NomBank SRL system. 1 Introduction Automatic Semantic Role Labeling (SRL) systems, made possible by the availability of PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005), and encouraged by evaluation efforts in (Carreras and Marquez, 2005; Litkowski, 2004), have been shown to accurately determine the argument structure of verb predicates. A successful PropBank-based SRL system would correctly determine that “Ben Bernanke” is the subject (labeled as ARG0 in PropBank) of predicate “replace”, and “Greenspan” is the object (labeled as ARG1): • Ben Bernanke replaced Greenspan as Fed chair. • Greenspan was replaced by Ben Bernanke as Fed chair. The recent release of NomBank (Meyers et al., 2004c; Meyers et al., 2004b), a databank that annotates argument structure f</context>
<context position="23833" citStr="Palmer et al., 2005" startWordPosition="3860" endWordPosition="3863">the F1 accuracy by more than 3%. The decrease in accuracy is expected, due to the noise introduced by automatic syntactic parsing. 6 Discussion and future work 6.1 Comparison of the composition of PropBank and NomBank Counting the number of annotated predicates, the size of the September 2005 release of NomBank (NomBank.0.8) is about 83% of PropBank release 1. Preliminary consistency tests reported in (Meyers et al., 2004c) shows that NomBank’s interannotator agreement rate is about 85% for core arguments and lower for adjunct arguments. The inter-annotator agreement for PropBank reported in (Palmer et al., 2005) is above 0.9 in terms of the Kappa statistic (Sidney and Castellan Jr., 1988). While the two agreement measures are not directly comparable, the current NomBank.0.8 release documentation indicates that only 32 of the most frequently occurring nouns in PTB II have been adjudicated. We believe the smaller size of NomBank.0.8 and the potential noise contained in the current release of the NomBank data may partly explain our lower SRL accuracy on NomBank, especially in the argument identification phase, as compared to the published accuracies of PropBank-based SRL systems. 6.2 Difficulties in Nom</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer S Pradhan</author>
<author>Honglin Sun</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
<author>Dan Jurafsky</author>
</authors>
<date>2004</date>
<booktitle>Parsing Arguments of Nominalizations in English and Chinese. In Proceedings ofHLT/NAACL</booktitle>
<contexts>
<context position="16227" citStr="Pradhan et al., 2004" startWordPosition="2666" endWordPosition="2669">introduced by the support verb “nominate”. The arguments introduced by support verbs can appear syntactically distant from the nominal predicate. To capture the location of arguments and the existence of support verbs, we add features indicating whether the argument is under the noun phrase headed by the predicate, whether the noun phrase headed by the predicate is dominated by a VP phrase or has neighboring VP phrases, and whether there is a verb between the argument and the predicate. These are represented as features a5, a6, and a7 in Table 3. Feature a7 was also proposed by the system in (Pradhan et al., 2004). We also experimented with various feature combinations, inspired by the features used in (Xue and Palmer, 2004). These are listed as features a11 to a16 in Table 3. 4.2.4 Neighboring arguments The research of (Jiang et al., 2005; Toutanova et al., 2005) has shown the importance of capturing information of the global argument frame in order to correctly classify the local argument. We make use of the features {b3,b4,b5,b6} of the neighboring arguments as defined in Table 1. Arguments are classified from left to right in the textual order they appear. For arguments that are already labeled, we</context>
</contexts>
<marker>Pradhan, Sun, Ward, Martin, Jurafsky, 2004</marker>
<rawString>Sameer S. Pradhan, Honglin Sun, Wayne Ward, James H. Martin, and Dan Jurafsky. 2004. Parsing Arguments of Nominalizations in English and Chinese. In Proceedings ofHLT/NAACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Kadri Hacioglu</author>
<author>Valerie Krugler</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Support Vector Learning for Semantic Argument Classification.</title>
<date>2005</date>
<journal>Machine Learning.</journal>
<contexts>
<context position="9938" citStr="Pradhan et al., 2005" startWordPosition="1609" endWordPosition="1612"> such cases. In NomBank sections 02-21, about 0.6% of the argument nodes dominate some other argument nodes or the predicate. To simplify our task, during training example generation, we ignore arguments that dominate the predicate. We also ignore arguments that are dominated by other arguments, so that when argument domination occurs, only the argument with the largest word span is kept. We do not perform similar pruning on the test data. 4 Features and feature selection 4.1 Baseline NomBank SRL features Table 1 lists the baseline features we adapted from previous PropBank-based SRL systems (Pradhan et al., 2005; Xue and Palmer, 2004). For ease of description, related features are grouped, with a specific individual feature given individual reference name. For example, feature b11FW in the group b11 denotes the first word spanned by the constituent and b13LH denotes the left sister’s head word. We also experimented with various feature combinations, inspired by the features used in (Xue and Palmer, 2004). These are listed as features b31 to b34 in Table 1. Suppose the current constituent under identification or classification is “NP-Ben Bernanke” in Figure 1. The instantiations of the baseline featur</context>
<context position="13268" citStr="Pradhan et al., 2005" startWordPosition="2166" endWordPosition="2169">” and is classified as “VERB-NOM”, while the nouns “employer” and “employee” are classified as “SUBJECT” and “OBJECT” respectively. Other classes include “ADJ-NOM” for nominalization of adjectives and “NOM-REL” for relational nouns. The class of a nominal predicate is very indicative of the role of its arguments. We would expect a “VERB-NOM” predicate to take both ARG0 and ARG1, while an “OBJECT” predicate to take only ARG0. We incorporated the class of nominal predicates as additional features in our NomBank SRL system. We add feature a2 in Table 3 to use this information. Baseline Features (Pradhan et al., 2005) b1 replacement b2 NP --+ NP NN b3 NP b4 Bernanke b5 NPTSIVPIVPIPPINPINN b6 left b11 Ben, Bernanke, NNP, NNP b12 NULL, VP b13 NULL, NULL, was, VBD b14 S b15 was, VBD b16 NULL b17 NULL, NULL b18 NP-7 b19 NULL b20 NPTS b21 NPTSIVPIVPIPPINP Baseline Combined Features (Xue and Palmer, 2004) b31 replacement &amp; NP b32 replacement &amp; Bernanke b33 replacement &amp; NPTSIVPIVPIPPINPINN b34 replacement &amp; left Table 2: Baseline feature instantiations, assuming the current constituent is “NP-Ben Bernanke” in Figure 1. Additional Features Based on NomBank a1 Nombank morphed noun stem a2 Nombank nominal class a3 </context>
</contexts>
<marker>Pradhan, Hacioglu, Krugler, Ward, Martin, Jurafsky, 2005</marker>
<rawString>Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne Ward, James H. Martin, and Daniel Jurafsky. 2005. Support Vector Learning for Semantic Argument Classification. Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Adam Meyers</author>
<author>Martha Palmer</author>
<author>Massimo Poesio</author>
</authors>
<title>Merging PropBank, NomBank, TimeBank, Penn Discourse Treebank and Coreference.</title>
<date>2005</date>
<booktitle>In ACL 2005 Workshop on Frontiers in Corpus Annotations II: Pie in the Sky.</booktitle>
<contexts>
<context position="25580" citStr="Pustejovsky et al., 2005" startWordPosition="4126" endWordPosition="4129"> the post soon”, “The replacement Ben Bernanke will assume the post soon”, and “The replacement will assume the post soon” are all grammatical. With the verb predicate “replace”, except for “Greenspan was replaced”, there is no freedom of forming phrases like “Ben Bernanke replaces” or simply “replaces” without supplying the necessary arguments to complete the grammatical structure. We believe the flexible argument structure of NomBank noun predicates contributes to the lower automatic SRL accuracy as compared to that of the PropBank SRL task. 6.3 Integrating PropBank and NomBank SRL Work in (Pustejovsky et al., 2005) discussed the possibility of merging various Treebank annotation efforts including PropBank, NomBank, and others. Future work involves studying ways of concurrently producing automatic PropBank and NomBank SRL, and improving the accuracy by exploiting the inter-relationship between verb predicate-argument and noun predicate-argument structures. Besides the obvious correspondence between a verb and its nominalizations, e.g., “replace” and “replacement”, there is also correspondence between verb predicates in PropBank and support verbs in NomBank. Statistics from NomBank sections 02-21 show tha</context>
</contexts>
<marker>Pustejovsky, Meyers, Palmer, Poesio, 2005</marker>
<rawString>James Pustejovsky, Adam Meyers, Martha Palmer, and Massimo Poesio. 2005. Merging PropBank, NomBank, TimeBank, Penn Discourse Treebank and Coreference. In ACL 2005 Workshop on Frontiers in Corpus Annotations II: Pie in the Sky.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siegel Sidney</author>
<author>N John Castellan Jr</author>
</authors>
<title>Nonparametric Statistics for the Behavioral Sciences.</title>
<date>1988</date>
<publisher>McGraw-Hill,</publisher>
<location>New York.</location>
<marker>Sidney, Jr, 1988</marker>
<rawString>Siegel Sidney and N. John Castellan Jr. 1988. Nonparametric Statistics for the Behavioral Sciences. McGraw-Hill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Aria Haghighi</author>
<author>Christopher D Manning</author>
</authors>
<title>Joint Learning Improves Semantic Role Labeling.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL</booktitle>
<contexts>
<context position="7172" citStr="Toutanova et al., 2005" startWordPosition="1161" endWordPosition="1164">responds to either “argument” or “non-argument”, and in the classification model, label l corresponds to one of the specific NomBank argument classes. The classification output is the label l with the highest conditional probability p(l|x). exp(�n i�1 Aifi(l, x)) p(l|x) = (1) Zx To train the ME-based identification model, training data is gathered by treating each parse tree node that is an argument as a positive example and the rest as negative examples. Classification training data is generated from argument nodes only. During testing, the algorithm of enforcing nonoverlapping arguments by (Toutanova et al., 2005) is used. The algorithm maximizes the logprobability of the entire NomBank labeled parse 1http://maxent.sourceforge.net/ 139 tree. Specifically, assuming we only have two classes “ARG” and “NONE”, the log-probability of a NomBank labeled parse tree is defined by Equation 2. � NONE(T) + �(Max(child)) Max(T) = max ARG(T) + E(NONETree(child)) (2) Max(T) is the maximum log-probability of a tree T, NONE(T) and ARG(T) are respectively the log-probability of assigning label “NONE” and “ARG” by our argument identification model to tree node T, child ranges through each of T’s children, and NONETree(ch</context>
<context position="16482" citStr="Toutanova et al., 2005" startWordPosition="2708" endWordPosition="2711"> the argument is under the noun phrase headed by the predicate, whether the noun phrase headed by the predicate is dominated by a VP phrase or has neighboring VP phrases, and whether there is a verb between the argument and the predicate. These are represented as features a5, a6, and a7 in Table 3. Feature a7 was also proposed by the system in (Pradhan et al., 2004). We also experimented with various feature combinations, inspired by the features used in (Xue and Palmer, 2004). These are listed as features a11 to a16 in Table 3. 4.2.4 Neighboring arguments The research of (Jiang et al., 2005; Toutanova et al., 2005) has shown the importance of capturing information of the global argument frame in order to correctly classify the local argument. We make use of the features {b3,b4,b5,b6} of the neighboring arguments as defined in Table 1. Arguments are classified from left to right in the textual order they appear. For arguments that are already labeled, we also add their argument class r. Specifically, for each argument to the left of the current argument, we have a feature b3-b4-b5-b6- r. For each argument to the right of the current argument, the feature is defined as b3-b4-b5-b6. We extract features in </context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2005</marker>
<rawString>Kristina Toutanova, Aria Haghighi, and Christopher D. Manning. 2005. Joint Learning Improves Semantic Role Labeling. In Proceedings ofACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Calibrating Features for Semantic Role Labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP-2004.</booktitle>
<contexts>
<context position="9961" citStr="Xue and Palmer, 2004" startWordPosition="1613" endWordPosition="1616">k sections 02-21, about 0.6% of the argument nodes dominate some other argument nodes or the predicate. To simplify our task, during training example generation, we ignore arguments that dominate the predicate. We also ignore arguments that are dominated by other arguments, so that when argument domination occurs, only the argument with the largest word span is kept. We do not perform similar pruning on the test data. 4 Features and feature selection 4.1 Baseline NomBank SRL features Table 1 lists the baseline features we adapted from previous PropBank-based SRL systems (Pradhan et al., 2005; Xue and Palmer, 2004). For ease of description, related features are grouped, with a specific individual feature given individual reference name. For example, feature b11FW in the group b11 denotes the first word spanned by the constituent and b13LH denotes the left sister’s head word. We also experimented with various feature combinations, inspired by the features used in (Xue and Palmer, 2004). These are listed as features b31 to b34 in Table 1. Suppose the current constituent under identification or classification is “NP-Ben Bernanke” in Figure 1. The instantiations of the baseline features in Table 1 for this </context>
<context position="12147" citStr="Xue and Palmer, 2004" startWordPosition="1974" endWordPosition="1977">t sister (b12L, b12R) b14 left or right sister’s head word/POS (b13LH, b15 b13LP, b13RH, b13RP) b16 phrase type of parent b17 parent’s head word or its POS (b15H, b15P) b18 head word of the constituent if its parent has phrase b19 type PP b20 head word or POS tag of the rightmost NP node, if b21 the constituent is PP (b17H, b17P) phrase type appended with the length of path temporal keyword, e.g., ”Monday” partial path from the constituent to the lowest common ancestor with the predicate projected path from the constituent to the highest NP dominating the predicate Baseline Combined Features (Xue and Palmer, 2004) b31 b1 &amp; b3 b32 b1 &amp; b4 b33 b1 &amp; b5 b34 b1 &amp; b6 Table 1: Baseline features for NomBank SRL tures, we use this set of more specific mappings to replace the morphological mappings based on WordNet. Specifically, we replace feature b1 in Table 1 with feature a1 in Table 3. The current NomBank release also contains the “NOMLEX-PLUS” dictionary, which contains the class of nominal predicates according to their origin and the roles they play. For example, “employment” originates from the verb “employ” and is classified as “VERB-NOM”, while the nouns “employer” and “employee” are classified as “SUBJ</context>
<context position="13555" citStr="Xue and Palmer, 2004" startWordPosition="2219" endWordPosition="2222"> the role of its arguments. We would expect a “VERB-NOM” predicate to take both ARG0 and ARG1, while an “OBJECT” predicate to take only ARG0. We incorporated the class of nominal predicates as additional features in our NomBank SRL system. We add feature a2 in Table 3 to use this information. Baseline Features (Pradhan et al., 2005) b1 replacement b2 NP --+ NP NN b3 NP b4 Bernanke b5 NPTSIVPIVPIPPINPINN b6 left b11 Ben, Bernanke, NNP, NNP b12 NULL, VP b13 NULL, NULL, was, VBD b14 S b15 was, VBD b16 NULL b17 NULL, NULL b18 NP-7 b19 NULL b20 NPTS b21 NPTSIVPIVPIPPINP Baseline Combined Features (Xue and Palmer, 2004) b31 replacement &amp; NP b32 replacement &amp; Bernanke b33 replacement &amp; NPTSIVPIVPIPPINPINN b34 replacement &amp; left Table 2: Baseline feature instantiations, assuming the current constituent is “NP-Ben Bernanke” in Figure 1. Additional Features Based on NomBank a1 Nombank morphed noun stem a2 Nombank nominal class a3 identical to predicate? a4 a DEFREL noun? a5 whether under the noun phrase headed by the preda6 icate a7 whether the noun phrase headed by the predicate is dominated by a VP node or has neighboring VP nodes whether there is a verb between the constituent and the predicate Additional Com</context>
<context position="16340" citStr="Xue and Palmer, 2004" startWordPosition="2683" endWordPosition="2686">tant from the nominal predicate. To capture the location of arguments and the existence of support verbs, we add features indicating whether the argument is under the noun phrase headed by the predicate, whether the noun phrase headed by the predicate is dominated by a VP phrase or has neighboring VP phrases, and whether there is a verb between the argument and the predicate. These are represented as features a5, a6, and a7 in Table 3. Feature a7 was also proposed by the system in (Pradhan et al., 2004). We also experimented with various feature combinations, inspired by the features used in (Xue and Palmer, 2004). These are listed as features a11 to a16 in Table 3. 4.2.4 Neighboring arguments The research of (Jiang et al., 2005; Toutanova et al., 2005) has shown the importance of capturing information of the global argument frame in order to correctly classify the local argument. We make use of the features {b3,b4,b5,b6} of the neighboring arguments as defined in Table 1. Arguments are classified from left to right in the textual order they appear. For arguments that are already labeled, we also add their argument class r. Specifically, for each argument to the left of the current argument, we have a </context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>Nianwen Xue and Martha Palmer. 2004. Calibrating Features for Semantic Role Labeling. In Proceedings of EMNLP-2004.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>