<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000043">
<title confidence="0.991536">
Generation of Output Style Variation in the SAMMIE Dialogue System
</title>
<author confidence="0.9409715">
Ivana Kruijff-Korbayov´a, Ciprian Gerstenberger Jan Schehl
Olga Kukina
</author>
<affiliation confidence="0.97664">
Saarland University, Germany DFKI, Germany
</affiliation>
<email confidence="0.994869">
{korbay|gerstenb|olgak}@coli.uni-sb.de jan.schehl@dfki.de
</email>
<sectionHeader confidence="0.995595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999902933333333">
A dialogue system can present itself and/or
address the user as an active agent by means
of linguistic constructions in personal style, or
suppress agentivity by using impersonal style.
We describe how we generate and control per-
sonal and impersonal style variation in the out-
put of SAMMIE, a multimodal in-car dialogue
system for an MP3 player. We carried out an
experiment to compare subjective evaluation
judgments and input style alignment behavior
of users interacting with versions of the sys-
tem generating output in personal vs. imper-
sonal style. Although our results are consis-
tent with earlier findings obtained with simu-
lated systems, the effects are weaker.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961777777778">
One of the goals in developing dialogue systems that
users find appealing and natural is to endow the sys-
tems with contextually appropriate output. This en-
compasses a broad range of research issues. Our
present contribution concerns the generation of per-
sonal and impersonal style.
We define the personal/impersonal style di-
chotomy as reflecting primarily a distinction with
respect to agentivity: personal style involves the ex-
plicit realization of an agent, whereas impersonal
style avoids it. In the simplest way this is mani-
fested by the presence of explicit reference to the di-
alogue participants (typically by means of personal
pronouns) vs. its absence, respectively. More gen-
erally, active voice and finite verb forms are typical
for personal style, whereas impersonal style often,
though not exclusively, employs passive construc-
tions or infinite verb forms:
</bodyText>
<figure confidence="0.969323125">
(1) Typical personal style constructions:
a. I found 20 albums.
b. You have 20 albums.
c. Please search for albums by The Beatles.
(2) Typical impersonal style constructions:
a. 20 albums have been found.
b. There are 20 albums.
c. The database contains 20 albums.
</figure>
<figureCaption confidence="0.62905">
d. 20 albums found.
</figureCaption>
<bodyText confidence="0.9999496875">
The dialogue system SAMMIE developed in the
TALK project uses either personal or impersonal out-
put style, employing constructions such as (1a–1c)
and (2a–2d), respectively, to manifest its own and
the user’s agentivity linguistically. We ran an ex-
periment to assess the effects of the system output
style on users’ judgments of the system’s usability
and performance and on their input formulation.
In Section 2 we review related work on system
output adaptation and previous experiments con-
cerning the effect of system output style on users’
judgments and style. We describe the SAMMIE sys-
tem and the generation of style variation in Sec-
tion 3. In Section 4 we describe our experiment and
in Section 5 present the results. In Section 6 we pro-
vide a discussion and conclusions.
</bodyText>
<sectionHeader confidence="0.994696" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.992946">
Although recently developed dialogue systems
adapt their output to the users in various ways, this
</bodyText>
<page confidence="0.997631">
129
</page>
<bodyText confidence="0.999897681818182">
usually concerns content selection rather than sur-
face realization. There is to our knowledge no sys-
tem that varies the style of its output in the in-
terpersonal dimension as we have done in SAM-
MIE. Work on animated conversational agents has
addressed various issues concerning agents display-
ing their personality, but this usually concerns emo-
tional states and personality traits, rather than the
personal/impersonal alteration. (Isard et al., 2006)
model personality and alignment in generated dia-
logues between pairs of agents using OpenCCG and
an over-generation and ranking approach, guided by
a set of language models. Their approach probably
could produce the personal/impersonal style varia-
tion as an effect of personality or a side-effect of
syntactic alignment.
The question whether a system should generate
output in personal or impersonal style has been ad-
dressed by (Nass and Brave, 2005): They observe
that agents that use “I” are generally perceived more
like a person than those that do not. However, sys-
tems tend to be more positively rated when consis-
tent with respect to such parameters as personality,
gender, ontology (human vs. machine), etc. On
the basis of an investigation of a range of user atti-
tudes to their simulated system with a synthetic vs. a
recorded voice, they conclude that a recorded voice
system is perceived as more human-like and thus en-
titled to use “I”, whereas a synthetic-voice system is
not perceived as human enough to use “I” to refer to
itself (Nass et al., 2006).
Another question is whether system output style
influences users’ input formulation, as would be ex-
pected due to the phenomenon of alignment, which
is generally considered a basic principle in natural
language dialogue (Garrod and Pickering, 2004).1
Experiments targeting human-human conversa-
tion show that speakers in spontaneous dialogues
tend to express themselves in similar ways at lexi-
cal and syntactic levels (e.g., (Hadelich et al., 2004;
Garrod and Pickering, 2004). Lexical and syntactic
alignment is present in human-computer interaction,
too. (Brennan, 1996) suggested that users adopt
system’s terms to avoid errors, expecting the sys-
</bodyText>
<footnote confidence="0.89132">
1This dialogue phenomenon goes under a variety of terms in
the literature, besides alignment, e.g., accommodation, adapta-
tion, convergence, entrainment or shaping (used, e.g., by (Bren-
nan and Ohaeri, 1994)).
</footnote>
<bodyText confidence="0.998733181818182">
tem to be inflexible. However, recent experiments
show that alignment in human-computer interaction
is also automatic and its strength is comparable to
that in human-human communication (Branigan et
al., 2003; Pearson et al., 2006).
Early results concerning users’ alignment to sys-
tem output style in the interpersonal dimension are
reported in (Brennan and Ohaeri, 1994): They dis-
tinguish three styles: anthropomorphic (the system
refers to itself using first person pronouns, like in
(1a) above, fluent (complete sentences, but no self-
reference) and telegraphic, like (2d). They found no
difference in users’ perception of the system’s in-
telligence across the different conditions. However,
they observed that the anthropomorphic group was
more than twice as likely to refer to the computer
using the second person pronoun “you” and it used
more indirect requests and conventional politeness
than the other groups. They conclude that the an-
thropomorphic style is undesirable for dialogue sys-
tems because it encourages more complex user input
which is harder to recognize and interpret.
The described experiments used either the
Wizard-of-Oz paradigm (Brennan and Ohaeri, 1994)
or preprogrammed system output (Branigan et al.,
2003; Nass and Brave, 2005) and involved written
communication. Such methods allow one to test as-
sumptions about idealized human-computer interac-
tion. Experimenting with the SAMMIE system al-
lows us to test whether similar effects arise in an in-
teraction with an actual dialogue system, which is
plagued, among other factors, by speech recognition
problems.
</bodyText>
<sectionHeader confidence="0.983734" genericHeader="method">
3 The SAMMIE System
</sectionHeader>
<bodyText confidence="0.9996005">
SAMMIE is a multimodal dialogue system developed
in the TALK project with particular emphasis on mul-
timodal turn-planning and natural language genera-
tion to support intuitive mixed-initiative interaction.
The SAMMIE system provides a multimodal in-
terface to an in-car MP3 player through speech and
haptic input with a BMW iDrive input device, a but-
ton which can be turned, pushed down and sideways
in four directions. System output is by speech and a
graphical display integrated into the car’s dashboard.
SAMMIE has a German and an English version with
the same functionality.
</bodyText>
<page confidence="0.994488">
130
</page>
<bodyText confidence="0.999594166666667">
The MP3 player application offers a wide range
of tasks: The user can control the currently playing
song, search and browse by looking for fields in the
MP3 database (song, artist, album, etc.), search and
select playlists and construct and edit them. A sam-
ple interaction is shown below (Becker et al., 2006).
</bodyText>
<listItem confidence="0.602381">
(3) U: Show me the Beatles albums.
</listItem>
<bodyText confidence="0.736706772727273">
S: I have these four Beatles albums. [shows a list
of album names]
U: Which songs are on this one? [selects the Red
Album]
S: The Red Album contains these songs [shows a
list of the songs]
U: Play the third one.
S: [song “From Me To You” plays]
The system puts the user in control of the inter-
action. Input can be given through any modality
and is not restricted to answers to system queries.
On the contrary, the user can provide new tasks as
well as any information relevant to the current task
at any time. This is achieved through modeling the
interaction as a collaborative problem solving (CPS)
process, modeling the tasks and their progression as
recipes and a multimodal interpretation that fits any
user input into the context of the current task (Blay-
lock and Allen, 2005). To support dialogue flexibil-
ity, we model discourse context, the CPS state and
the driver’s attention state by an enriched informa-
tion state (Kruijff-Korbayov´a et al., 2006a).
</bodyText>
<subsectionHeader confidence="0.999667">
3.1 System Architecture
</subsectionHeader>
<bodyText confidence="0.9999238">
The SAMMIE system architecture follows the classi-
cal approach of a pipelined architecture with mul-
timodal fusion and fission modules encapsulating
the dialogue manager (Bunt et al., 2005). Figure 1
shows the modules and their interaction: Modality-
specific recognizers and analysers provide seman-
tically interpreted input to the multimodal fusion
module (interpretation manager in Figure 1), that in-
terprets them in the context of the other modalities
and the current dialog context. The dialogue man-
ager decides on the next system move, based on its
CPS encoded task model, on the current context and
also on the results from calls to the MP3 database.
The multimodal fission component then generates
the system reaction on a modality-dependent level
</bodyText>
<figureCaption confidence="0.999629">
Figure 1: SAMMIE system architecture.
</figureCaption>
<bodyText confidence="0.9988778">
by selecting the content to present, distributing it ap-
propriately over the available output modalities and
finally co-ordinating and synchronizing the output.
Modality-specific output modules generate spoken
output and an update of the graphical display. All
modules interact with the extended information state
in which all context information is stored.
Many tasks in the SAMMIE system are modeled by
a rule-based approach. Discourse modeling, inter-
pretation management, dialogue management, turn
planning and linguistic planning are all based on
the production rule system PATE (Pfleger, 2004;
Kempe, 2004). For speech recognition, we use Nu-
ance. The spoken output is synthesized with the
Mary TTS (Schr¨oder and Trouvain, 2003).2
</bodyText>
<subsectionHeader confidence="0.992277">
3.2 Generation of Natural Language Output
with Variation
</subsectionHeader>
<bodyText confidence="0.999881538461539">
To generate natural language output in SAMMIE, we
developed a template-based generator. It is imple-
mented by a set of sentence planning rules in PATE
to build the templates, and a set of XSLT transforma-
tions for sentence realization, which yield the out-
put strings. German and English output is produced
by accessing different dictionaries in a uniform way.
The output is either plain text, if it is to be displayed
in the graphical user interface (e.g., captions in ta-
bles, written messages to the user) or it is text with
mark-up for speech synthesis using the MaryXML
format (Schr¨oder and Trouvain, 2003), if it is to be
spoken by a speech synthesizer.
</bodyText>
<footnote confidence="0.967707">
2http://mary.dfki.de/
</footnote>
<page confidence="0.996866">
131
</page>
<bodyText confidence="0.999104777777778">
The SAMMIE generator can produce alternative
realizations for a given content that it receives as in-
put from the turn planner. The implemented range
of system output variation involves the following as-
pects, which have been determined by an analysis
of a corpus of dialogues collected in a Wizard-of-
Oz experiment using several wizards who were free
to formulate their responses to the users (Kruijff-
Korbayov´a et al., 2006b):
</bodyText>
<listItem confidence="0.989574333333333">
1. Personal vs. impersonal style: Ich habe 3 Liederge-
funden (I’ve found three songs) vs. 3 Lieder wurden
gefunden (Three songs have been found);
2. Telegraphic vs. non-telegraphic style: 23 Alben ge-
funden (23 albums found) vs. Ich habe 23 Alben
gefunden (Ifound 23 albums)
3. Reduced vs. non-reduced referring expressions: der
Song “Kinder An Die Macht” (the song “Kinder An
Die Macht”) vs. der Song (the song) vs. “Kinder
An Die Macht” (“Kinder An Die Macht”);
4. Lexical choice for (quasi-)synonyms: Song vs. Lied
vs. Titel (song vs. track)
5. Presence vs. absence of adverbs/adverbials: Ich
spiele jetzt den Song (I’ll now play the song) vs. Ich
spiele den Song (I’ll play the song).
</listItem>
<bodyText confidence="0.999574210526316">
The generation of alternatives is achieved by con-
ditioning the sentence planning and realization de-
cisions. The system can be set either to use one
style consistently throughout a dialogue, or to align
to the user, i.e., mimic the user’s style on a turn-
by-turn basis. For the purpose of experimenting
with system output variation, the generator supports
three sources of control for the available choices:
(a) global (default) parameter settings (resulting in
no variation); (b) random selection (resulting in ran-
dom variation); (c) contextual information (resulting
in variation based on the dialogue context).
The contextual information used by the genera-
tor to control realization includes (i) the grounding
status of the content to be communicated (e.g., to
decide for vs. against reducing a referring expres-
sion); and (ii) linguistic features extracted from the
recognized user input (e.g., to make the correspond-
ing syntactic and lexical choices in the output).
</bodyText>
<subsectionHeader confidence="0.999093">
3.3 Personal/Impersonal Style Variation
</subsectionHeader>
<bodyText confidence="0.999389142857143">
The style variation in SAMMIE amounts to varying
between active voice for personal style and passive
voice or the “es-gibt” (“there is”) construction for
impersonal style whenever applicable, as illustrated
for several typical dialogue moves below (where (i)
always shows the impersonal, and (ii) the personal
version).
</bodyText>
<table confidence="0.7957029375">
(4) Search result:3
i. Es gibt 20 Alben.
There are 20 albums.
ii. Ich habe 20 Alben gefunden.
Ifound 20 albums.
Sie haben 20 Alben. / Du hast 20 Alben.
You have 20 albums
Wir haben 20 Alben.
We have 20 albums.
(5) Song addition:
i. Der Titel Bittersweet Symphony wurde zu
der Playliste 2 hinzugef¨ugt.
The track Bittersweet Symphony has been
added to Playlist 2.
ii. Ich habe den Titel Bittersweet Symphony zu
der Playliste 2 hinzugef¨ugt.
</table>
<figure confidence="0.790391">
I added the track Bittersweer Symphony to
Playlist 2.
(6) Song playback:
i. Der Titel M¨anner von Herbert Gr¨onemeyer
wird gespielt.
The track M¨anner by Herbert Gr¨onemeyer is
playing.
ii. Ich spiele den Titel M¨anner von Herbert
Gr¨onemeyer.
I am playing the track M¨anner by Herbert
Gr¨onemeyer.
</figure>
<listItem confidence="0.9480635">
(7) Non-understanding:
i. Das wurde leider nicht verstanden.
That has unfortunately not been understood.
ii. Das habe ich leider nicht verstanden.
I have unfortunately not understood that.
(8) Clarification request:
i. Welches von diesen acht Liedern?/Welches
von diesen acht Liedern wird gew¨unscht?
Which of these eight songs? / Which of these
eight songs is desired?
ii. Welches von diesen acht Liedern m¨ochtest du
/ m¨ochten Sie h¨oren?
</listItem>
<footnote confidence="0.7585708">
Which of these eight songs would you like to
hear?
3When referring to the user, personal style has several vari-
ants which differ in formality (formal and informal address) and
first vs. second person reference.
</footnote>
<page confidence="0.984654">
132
</page>
<figureCaption confidence="0.991895">
Figure 2: Experiment setup
</figureCaption>
<bodyText confidence="0.758101">
The personal/impersonal style variation is not ap-
plicable for some dialogue moves, e.g., (9), and for
output in telegraphic style.
</bodyText>
<table confidence="0.578673333333333">
(9) Song interpreter:
Der Titel Bongo Girl ist von Nena.
The track Bongo Girl is by Nena.
</table>
<sectionHeader confidence="0.989372" genericHeader="method">
4 Experiment
</sectionHeader>
<bodyText confidence="0.997226045454546">
In order to assess the effects of style manipulation in
the SAMMIE system, we ran an experiment in simu-
lated driving conditions, comparing two versions of
the system: one consistently using personal and the
other impersonal style output.4 The experiment em-
ployed the German version of SAMMIE. The setup
(see Figure 2), participants, procedure and collected
data are described in detail in (Kruijff-Korbayov´a
and Kukina, 2008), and summarized below.
There were 28 participants, all native speakers
of German. We balanced gender and background
when assigning them to the style conditions. The
experiment followed a fixed script for each partici-
pant: welcome, instruction, warm-up driving, 2 trial
and 11 experimental tasks, evaluation questionnaire,
payment and farewell. The participants were in-
structed to use mainly spoken input, although they
could also use the iDrive button. It took them about
40 minutes to complete all the tasks. The tasks in-
volved exploring the contents of a database of about
25 music albums and were of four types: (1) find-
ing some specified title(s); (2) selecting some title(s)
</bodyText>
<footnote confidence="0.9033165">
4For the time being we have not evaluated the version of the
system aligning to the user’s style.
</footnote>
<bodyText confidence="0.998073333333333">
satisfying certain constraints; (3) manipulating the
playlists by adding or removing songs and (4) free-
use of the system.
The experimental tasks were presented to each
participant in randomized order apart from the free
use of the system, which was always the last task.
The experimenter (E) repeated each task assignment
twice to the participant, once in personal and once
in impersonal style, as shown in the example below.
</bodyText>
<table confidence="0.488659833333333">
(10) E: Bitte frage das System nach den Liedern von
“Pur”. Du willst also wissen welche Lieder von
“Pur” es gibt.
E: Please ask the the system about the songs by
“Pur”. You would like to know which songs by
“Pur” there are.
</table>
<bodyText confidence="0.993963384615385">
The questionnaire was based on (Nass and Brave,
2005) and (Mutschler et al., 2007). It contained
questions with a 6-point scale ranging from 1 (low
grade) to 6 (high grade), such as How do you assess
the system in general: technical (1) – human-like
(6); Communication with the system seemed to you:
boring (1) – exciting (6); In terms of usability, the
system is: inefficient (1) —efficient(6).
The recorded dialogues have been transcribed, the
questionnaire responses tabulated. We manually an-
notated the participants’ utterances (on average 95
per session) with the following features for further
analysis:
</bodyText>
<listItem confidence="0.980866">
• Construction type:
</listItem>
<bodyText confidence="0.97276075">
Personal (+/-) Is the utterance a complete sen-
tence in active voice or imperative form
Impersonal (+/-) Is the utterance expressed
by passive voice, infinite verb form (e.g.,
“Lied abspielen” (lit. “song play”)), or ex-
pletive “es-gibt” (“there-is”) construction
Telegraphic (+/-) Is the utterance expressed
by a phrase, e.g., “weiter” (“next”)
</bodyText>
<listItem confidence="0.9648665">
• Personal pronouns: (+/-) Does the utterance
contain a first or second person pronoun
• Politeness marking: (+/-) Does the utterance
contain a politeness marker, such as “bitte”
(“please”), “danke” (“thanks”) and verbs in
subjunctive mood (eg. “ich h¨atte gerne”)
</listItem>
<page confidence="0.999396">
133
</page>
<sectionHeader confidence="0.999864" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999966">
The results concerning users’ attitudes and align-
ment are presented in detail in (Kruijff-Korbayov´a
and Kukina, 2008). Here we summarize the signif-
icant findings and provide an additional analysis of
the influence of speech recognition problems.
</bodyText>
<subsectionHeader confidence="0.994131">
5.1 Style and Users’ Attitudes
</subsectionHeader>
<bodyText confidence="0.952020333333333">
The first issue addressed in the experiment was
whether the users have different judgments of the
personal vs. impersonal version of the system. Since
the system used a synthetic voice, the judgments
were expected to be more positive in the impersonal
style condition (Nass and Brave, 2005). Based on
factor analysis performed on attitudinal data from
the user questionnaires we created the six indices
listed below. All indices were meaningful and reli-
able. (A detailed description of the indices including
the contributing factors from the questionnaires can
be found in (Kruijff-Korbayov´a and Kukina, 2008).)
</bodyText>
<listItem confidence="0.999668888888889">
1. General satisfaction with the communication
with the system (Cronbach’s α=0.86)
2. Easiness of communication with the system
(α=0.83)
3. Usability of the system (α=0.76)
4. Clarity of the system’s speech (α=0.88)
5. Perceived “humanness” of the system (α=0.69)
6. System’s perceived flexibility and creativity
(α=0.78)
</listItem>
<bodyText confidence="0.999893111111111">
We did not find any significant influence of sys-
tem output style on users’ attitudes. Only for per-
ceived humanness of the system we found a weak
tendency in the predicted direction (independent
samples test: t(25)=1.64, p=0.06 (one-tailed)), in
line with the earlier observation that an interface that
refers to itself by a personal pronoun is perceived to
be more human-like than one that does not (Nass and
Brave, 2005).
</bodyText>
<subsectionHeader confidence="0.996846">
5.2 Style and Alignment
</subsectionHeader>
<bodyText confidence="0.99997378125">
The next issue we investigated was whether the users
formulated their input differently in the personal vs.
impersonal system version. For each dialogue ses-
sion, we calculated the percentage of utterances con-
taining the feature of interest relative to the total
number of user utterances in the session.
In accordance with the expectation based on style
alignment in terms of agentivity, we observed a sig-
nificant difference in the number of personal con-
structions across style conditions (t(19)=1.8, p=0.05
(one-tailed)). But we did not find a significant dif-
ference in the distribution of impersonal construc-
tions. Not surprisingly, there was also no signifi-
cant difference in the distribution of telegraphic con-
structions. An unexpected finding was the higher
proportion of telegraphic constructions than verb-
containing ones within the impersonal style condi-
tion (t(13)=3.5, p&lt;0.001 (one-tailed)). However, no
such difference was found in the personal style con-
dition. Contrary to expectations, we also did not find
any significant effect of style-manipulation on the
number of personal pronouns, nor on the number of
politeness markers.
Since alignment can also be seen as a process
of gradual adjustment among dialogue participants
over time we compared the proportion of personal,
impersonal and telegraphic constructions in the first
and second halves of the conversations for both style
conditions. The only significant effect we found was
a decrease in the number of personal constructions
in the second halves of the impersonal style interac-
tions (t(13)=2.5, p=0.02 (one-tailed)).
</bodyText>
<subsectionHeader confidence="0.998164">
5.3 Influence of Speech Recognition Problems
</subsectionHeader>
<bodyText confidence="0.999974333333334">
Unlike an interaction in a Wizard-of-Oz simulation
or similar, an interaction with a real system is bound
to suffer from speech recognition problems. There-
fore, we made a post-hoc analysis with respect to
how much speech recognition difficulty the partici-
pants experienced, in terms of the proportion of par-
ticipant utterances not recognized by the system rel-
ative to the total number of participant utterances in
a session.
On average, around 33% of participant utterances
were not understood by the system.5 We classi-
fied the participants into three groups according to
the performance of speech recognition they expe-
rienced: the good group with less than 27% of in-
put not understood (7 participants); the poor group
</bodyText>
<footnote confidence="0.9640784">
5This is admittedly rather bad performance, nevertheless it
mostly does not prevent the participants from getting their tasks
successfully completed within a reasonable time, as was shown
in an rigorous usability evaluation of the system in normal driv-
ing conditions (Mutschler et al., 2007).
</footnote>
<page confidence="0.994787">
134
</page>
<figureCaption confidence="0.999871">
Figure 3: Judgments of the system by the “good” and “poor” speech recognition group
</figureCaption>
<bodyText confidence="0.982840314285715">
with more than 37% of input not uderstood (7 par-
ticipants); the average group (the remaining 14 par-
ticipants).
Speech Recognition and Attitudinal Data We
suspected that speech recognition problems might
be neutralizing a potential influence of style. There-
fore we contrasted the judgments on all six factors
between the good and the poor speech recognition
group (see Figure 3). The “good” speech recognition
group showed higher satisfaction with the communi-
cation (t(16)=1.9, p=0.04 (one-tailed)) and evaluated
the clarity of the system’s speech better (t(16)= 2.0,
p=0.03 (one-tailed)). The good speech recognition
group also showed a tendency to assess the usabil-
ity and flexibility of the system higher than the poor
speech recognition group (t(16)=1.71, p=0.05 and
t(16)=1.61, p=0.06, respectively (marginally signif-
icant results)). The two groups did not differ with
respect to their judgments of the ease of commu-
nication and perceived humanness of the system
(t(16)=0.45, p=0.66 and t(16)=0.90, p=0.38). These
results are not surprising. They confirm that speech
recognition does have an effect on the user’s percep-
tion of the system.
Speech Recognition and Style Alignment We
also checked post-hoc whether differences in the ex-
perienced speech recognition performance had an
influence on the style employed by the participants,
again in terms of the proportion of utterances with
personal, impersonal and telegraphic constructions,
personal pronouns and politeness marking. How-
ever, we found no significant effect on the linguistic
structure of the participant input across the groups
(politeness marking: F(2)=1.5, p=0.24; all other
Fs&lt;1 (ANOVA)).
</bodyText>
<sectionHeader confidence="0.990691" genericHeader="conclusions">
6 Discussion and Conclusions
</sectionHeader>
<bodyText confidence="0.99994245945946">
We presented the generation of personal/impersonal
style variation in the SAMMIE multimodal dialogue
system, and the results of an experiment evaluating
the influence of the system output style on the users’
subjective judgments and their formulation of input.
Although our results are not conclusive, they point
at a range of issues for further research.
Regarding users’ attitudes to the system, we
found no significant difference among the styles.
This is similar to (Brennan and Ohaeri, 1994) who
found no difference in intelligence attributed to the
system by the users, but it is at odds with the earlier
finding that a synthetic voice interface was judged
to be more useful when avoiding self-reference by
personal pronouns (Nass and Brave, 2005).
Whereas (Brennan and Ohaeri, 1994) used a flight
reservation dialogue system, (Nass and Brave, 2005)
used a phone-based auction system which read out
an introduction and five object descriptions. There
are two points to note: First, the subjects heard
system output that was a read out continuous text
rather than turns in an interaction. This may have
reinforced the activation of particular style features.
Second, the auction task may have sensibilized the
subjects to the distinction between subjective (the
system’s) vs. objective information presentation,
and thus make them more sensitive to whether the
system presents itself as an active agent or not.
Regarding the question whether users align their
style to that of the system, where previous experi-
ments showed strong effects of alignment (Brennan
and Ohaeri, 1994), our experiment shows some ef-
fects, but some of the results are conflicting. On
the one hand, subjects interacting with the personal
style version of the system used more personal con-
structions than those interacting with the impersonal
style version. However, subjects in either condi-
</bodyText>
<page confidence="0.996495">
135
</page>
<bodyText confidence="0.9999929">
tion did not show any significant difference with re-
spect to the use of impersonal constructions or tele-
graphic forms. We also found a higher proportion of
telegraphic constructions than verb-containing ones
within the impersonal style condition, but no such
difference in the personal style. Finally, when we
considered alignment over time, we found no change
in construction use in the personal style, whereas we
found a decrease in the use of personal constructions
in the impersonal style. It is possible that divid-
ing the interactions into three parts and comparing
alignment in the first and the last part might lead to
stronger results.
That there is no difference in the use of tele-
graphic constructions across conditions is not sur-
prising. Being just phrasal sentence fragments, these
constructions are neutral with respect to style. But
why does there seem to be an alignment effect for
personal constructions and not for others? One way
of explaining this is that (some of) the construc-
tions that we counted as impersonal are common in
both styles. Besides their deliberate use as means
to avoid explicit reference to oneself, they also have
their normal, neutral usage, and therefore, some of
the utterances that we classified as impersonal style
may just be neutral formulations, rather than cases
of distancing or “de-agentivization”. However, we
could not test this hypothesis, because we have not
found a way to reliably distinguish between neutral
and marked, truly impersonal utterances. This is an
issue for future work.
The difference between our results concerning
alignment and those of (Brennan and Ohaeri, 1994)
is not likely to be due to a difference in the degree
of interactivity (as with (Nass and Brave, 2005)).
We now comment on other differences between our
systems, which might have contributed to the differ-
ences in results.
One aspect where we differ concerns our distinc-
tion between personal and impersonal style, both in
the implementation of the SAMMIE system and in
the experiment: We include the presence/absence
of agentivity not only in the system’s reference to
itself (akin to (Nass and Brave, 2005) and (Bren-
nan and Ohaeri, 1994)), but also in addressing the
user. This concept of the personal/impersonal dis-
tinction was inspired by such differences observed
in a study of instructional texts in several languages
(Kruijff et al., 1999), where the latter dimension is
predominant. The present experiment results make
it pertinent that more research into the motives be-
hind expressing or suppressing agentivity in both di-
mensions is needed.
Apart from the linguistic design of the system’s
output, other factors influence users’ behavior and
perception of the system, and thus might confound
experiment results, e.g., functionality, design, er-
gonomics, speech synthesis and speech recognition.
A system with synthesized speech should be more
positively rated when it does not refer to itself as an
active agent by personal pronouns (Nass and Brave,
2005). (Brennan and Ohaeri, 1994) used a sys-
tem with written interaction, the SAMMIE system
employs the MARY text-to-speech synthesis system
(Schr¨oder and Trouvain, 2003) with an MBROLA
diphone synthesiser, which produces an acceptable
though not outstanding output quality. Our post-hoc
analysis showed a tendency towards better judge-
ments of the system by the participants experienc-
ing less speech recognition problems. This is as
expected. We did not find any statistically signif-
icant effect regarding the style-related features we
analyzed. A future experiment should address the
possibility of an interaction between system style
and speech recognition performance as both factors
might be influencing the user simultaneously.
One radical difference between our experiment
and the earlier ones is that the users of the SAMMIE
system are occupied by the driving task, and thus
only have a limited cognitive capacity left for the
interaction with the system. This may make them
less susceptible to the subtleties of style manipula-
tion than would be the case if they were free of other
tasks. A possible future experiment could address
this issue by including a non-driving condition.
Finally, the SAMMIE system has also the style-
alignment mode, where it mimics the user’s style on
turn-to-turn basis. We plan to present experimental
results comparing the alignment-mode with the fixed
personal/impersonal style in a future publication.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999631">
This work was carried out in the TALK project
(www.talk-project.org) funded by the EU as project
No. IST-507802 within the 6th Framework Program.
</bodyText>
<page confidence="0.998435">
136
</page>
<sectionHeader confidence="0.995892" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999837817204301">
T. Becker, N. Blaylock, C. Gerstenberger, I. Kruijff-
Korbayov´a, A. Korthauer, M. Pinkal, M. Pitz, P. Poller,
and J. Schehl. 2006. Natural and intuitive multimodal
dialogue for in-car applications: The SAMMIE system.
In Proceedings of ECAI, PAIS Special section.
N. Blaylock and J. Allen. 2005. A collaborative
problem-solving model of dialogue. In L. Dybkjær
and W. Minker, editors, Proceedings of the 6th SIGdial
Workshop on Discourse and Dialogue, pages 200–211,
Lisbon, September 2–3.
H. Branigan, M. Pickering, J. Pearson, J. F. McLean, and
C. Nass. 2003. Syntactic alignment between com-
puter and people: the role of belief about mental states.
In Proceedings of the Annual Conference of the Cog-
nitive Science Society.
S. Brennan and J.O. Ohaeri. 1994. Effects of mes-
sage style on user’s attribution toward agents. In Pro-
ceedings of CHI’94 Conference Companion Human
Factors in Computing Systems, pages 281–282. ACM
Press.
S. Brennan. 1996. Lexical entrainment in spontaneous
dialogue. In Proceedings of the International Sympo-
sium on Spoken Dialogue (ISSD-96), pages 41–44.
H. Bunt, M. Kipp, M. Maybury, and W. Wahlster. 2005.
Fusion and coordination for multimodal interactive in-
formation presentation: Roadmap, architecture, tools,
semantics. In O. Stock and M. Zancanaro, editors,
Multimodal Intelligent Information Presentation, vol-
ume 27 of Text, Speech and Language Technology,
pages 325–340. Kluwer Academic.
S. Garrod and M. Pickering. 2004. Why is conversation
so easy? TRENDS in Cognitive Sciences, 8.
K. Hadelich, H. Branigan, M. Pickering, and M. Crocker.
2004. Alignment in dialogue: Effects of feedback
on lexical overlap within and between participants.
In Proceedings of the AMLaP Conference. Aix en
Provence, France.
Amy Isard, Carsten Brockmann, and Jon Oberlander.
2006. Individuality and alignment in generated di-
alogues. In Proceedings of the 4th International
Natural Language Generation Conference (INLG-06),
pages 22–29, Sydney, Australia.
Benjamin Kempe. 2004. PATE a production rule sys-
tem based on activation and typed feature structure ele-
ments. Bachelor Thesis, Saarland University, August.
G.J.M. Kruijff, I. Kruijff-Korbayov´a, J. Bateman,
D. Dochev, N. Gromova, T. Hartley, E. Teich,
S. Sharoff, L. Sokolova, and K. Staykova. 1999.
Deliverable TEXS2: Specification of elaborated text
structures. Technical report, AGILE Project, EU
INCO COPERNICUS PL961104.
I. Kruijff-Korbayov´a and O. Kukina. 2008. The effect of
dialogue system output style variation on users’ eval-
uation judgements and input style. In Proceedings of
SigDial’08, Columbus, Ohio.
I. Kruijff-Korbayov´a, G. Amores, N. Blaylock, S. Eric-
sson, G. P´erez, K. Georgila, M. Kaisser, S. Larsson,
O. Lemon, P. Manch´on, and J. Schehl. 2006a. De-
liverable D3.1: Extended information state modeling.
Technical report, TALK Project, EU FP6, IST-507802.
Ivana Kruijff-Korbayov´a, Tilman Becker, Nate Blaylock,
Ciprian Gerstenberger, Michael Kaisser, Peter Poller,
Verena Rieser, and Jan Schehl. 2006b. The SAMMIE
corpus of multimodal dialogues with an MP3 player.
In Proceedings ofLREC, Genova, Italy.
H. Mutschler, F. Steffens, and A. Korthauer. 2007. De-
liverable D6.4: Final report on multimodal experi-
ments Part I: Evaluation of the SAMMIE system. Tech-
nical report, TALK Project, EU FP6, IST-507802.
C. Nass and S. Brave, 2005. Should voice interfaces say
”I”? Recorded and synthetic voice interfaces’ claims
to humanity, chapter 10, pages 113–124. The MIT
Press, Cambridge.
C. Nass, S. Brave, and L. Takayama. 2006. Socializing
consistency: from technical homogeneity to human
epitome. In P. Zhang &amp; D. Galletta (Eds.), Human-
computer interaction in management information sys-
tems: Foundations, pages 373–390. Armonk, NY: M.
E. Sharpe.
J. Pearson, J. Hu, H. Branigan, M. J. Pickering, and C. I.
Nass. 2006. Adaptive language behavior in HCI: how
expectations and beliefs about a system affect users’
word choice. In CHI ’06: Proceedings of the SIGCHI
conference on Human Factors in computing systems,
pages 1177–1180, New York, NY, USA. ACM.
N. Pfleger. 2004. Context based multimodal fusion. In
ICMI ’04: Proceedings of the 6th international confer-
ence on Multimodal interfaces, pages 265–272, New
York, NY, USA. ACM Press.
M. Schr¨oder and J. Trouvain. 2003. The German text-to-
speech synthesis system MARY: A tool for research,
development and teaching. International Journal of
Speech Technology, 6:365–377.
</reference>
<page confidence="0.997827">
137
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.512871">
<title confidence="0.999839">of Output Style Variation in the System</title>
<author confidence="0.999002">Ivana Kruijff-Korbayov´a</author>
<author confidence="0.999002">Ciprian Gerstenberger Jan Schehl</author>
<affiliation confidence="0.779425">Olga Kukina Saarland University, Germany DFKI,</affiliation>
<email confidence="0.989281">jan.schehl@dfki.de</email>
<abstract confidence="0.995228">A dialogue system can present itself and/or address the user as an active agent by means of linguistic constructions in personal style, or suppress agentivity by using impersonal style. We describe how we generate and control personal and impersonal style variation in the outof a multimodal in-car dialogue system for an MP3 player. We carried out an experiment to compare subjective evaluation judgments and input style alignment behavior of users interacting with versions of the system generating output in personal vs. impersonal style. Although our results are consistent with earlier findings obtained with simulated systems, the effects are weaker.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Becker</author>
<author>N Blaylock</author>
<author>C Gerstenberger</author>
<author>I KruijffKorbayov´a</author>
<author>A Korthauer</author>
<author>M Pinkal</author>
<author>M Pitz</author>
<author>P Poller</author>
<author>J Schehl</author>
</authors>
<title>Natural and intuitive multimodal dialogue for in-car applications: The SAMMIE system.</title>
<date>2006</date>
<booktitle>In Proceedings of ECAI, PAIS Special section.</booktitle>
<marker>Becker, Blaylock, Gerstenberger, KruijffKorbayov´a, Korthauer, Pinkal, Pitz, Poller, Schehl, 2006</marker>
<rawString>T. Becker, N. Blaylock, C. Gerstenberger, I. KruijffKorbayov´a, A. Korthauer, M. Pinkal, M. Pitz, P. Poller, and J. Schehl. 2006. Natural and intuitive multimodal dialogue for in-car applications: The SAMMIE system. In Proceedings of ECAI, PAIS Special section.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Blaylock</author>
<author>J Allen</author>
</authors>
<title>A collaborative problem-solving model of dialogue.</title>
<date>2005</date>
<booktitle>Proceedings of the 6th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>200--211</pages>
<editor>In L. Dybkjær and W. Minker, editors,</editor>
<contexts>
<context position="8665" citStr="Blaylock and Allen, 2005" startWordPosition="1370" endWordPosition="1374">ngs [shows a list of the songs] U: Play the third one. S: [song “From Me To You” plays] The system puts the user in control of the interaction. Input can be given through any modality and is not restricted to answers to system queries. On the contrary, the user can provide new tasks as well as any information relevant to the current task at any time. This is achieved through modeling the interaction as a collaborative problem solving (CPS) process, modeling the tasks and their progression as recipes and a multimodal interpretation that fits any user input into the context of the current task (Blaylock and Allen, 2005). To support dialogue flexibility, we model discourse context, the CPS state and the driver’s attention state by an enriched information state (Kruijff-Korbayov´a et al., 2006a). 3.1 System Architecture The SAMMIE system architecture follows the classical approach of a pipelined architecture with multimodal fusion and fission modules encapsulating the dialogue manager (Bunt et al., 2005). Figure 1 shows the modules and their interaction: Modalityspecific recognizers and analysers provide semantically interpreted input to the multimodal fusion module (interpretation manager in Figure 1), that i</context>
</contexts>
<marker>Blaylock, Allen, 2005</marker>
<rawString>N. Blaylock and J. Allen. 2005. A collaborative problem-solving model of dialogue. In L. Dybkjær and W. Minker, editors, Proceedings of the 6th SIGdial Workshop on Discourse and Dialogue, pages 200–211,</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Branigan</author>
<author>M Pickering</author>
<author>J Pearson</author>
<author>J F McLean</author>
<author>C Nass</author>
</authors>
<title>Syntactic alignment between computer and people: the role of belief about mental states.</title>
<date>2003</date>
<booktitle>In Proceedings of the Annual Conference of the Cognitive Science Society.</booktitle>
<contexts>
<context position="5558" citStr="Branigan et al., 2003" startWordPosition="864" endWordPosition="867">004; Garrod and Pickering, 2004). Lexical and syntactic alignment is present in human-computer interaction, too. (Brennan, 1996) suggested that users adopt system’s terms to avoid errors, expecting the sys1This dialogue phenomenon goes under a variety of terms in the literature, besides alignment, e.g., accommodation, adaptation, convergence, entrainment or shaping (used, e.g., by (Brennan and Ohaeri, 1994)). tem to be inflexible. However, recent experiments show that alignment in human-computer interaction is also automatic and its strength is comparable to that in human-human communication (Branigan et al., 2003; Pearson et al., 2006). Early results concerning users’ alignment to system output style in the interpersonal dimension are reported in (Brennan and Ohaeri, 1994): They distinguish three styles: anthropomorphic (the system refers to itself using first person pronouns, like in (1a) above, fluent (complete sentences, but no selfreference) and telegraphic, like (2d). They found no difference in users’ perception of the system’s intelligence across the different conditions. However, they observed that the anthropomorphic group was more than twice as likely to refer to the computer using the secon</context>
</contexts>
<marker>Branigan, Pickering, Pearson, McLean, Nass, 2003</marker>
<rawString>Lisbon, September 2–3. H. Branigan, M. Pickering, J. Pearson, J. F. McLean, and C. Nass. 2003. Syntactic alignment between computer and people: the role of belief about mental states. In Proceedings of the Annual Conference of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brennan</author>
<author>J O Ohaeri</author>
</authors>
<title>Effects of message style on user’s attribution toward agents.</title>
<date>1994</date>
<booktitle>In Proceedings of CHI’94 Conference Companion Human Factors in Computing Systems,</booktitle>
<pages>281--282</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="5347" citStr="Brennan and Ohaeri, 1994" startWordPosition="833" endWordPosition="837">and Pickering, 2004).1 Experiments targeting human-human conversation show that speakers in spontaneous dialogues tend to express themselves in similar ways at lexical and syntactic levels (e.g., (Hadelich et al., 2004; Garrod and Pickering, 2004). Lexical and syntactic alignment is present in human-computer interaction, too. (Brennan, 1996) suggested that users adopt system’s terms to avoid errors, expecting the sys1This dialogue phenomenon goes under a variety of terms in the literature, besides alignment, e.g., accommodation, adaptation, convergence, entrainment or shaping (used, e.g., by (Brennan and Ohaeri, 1994)). tem to be inflexible. However, recent experiments show that alignment in human-computer interaction is also automatic and its strength is comparable to that in human-human communication (Branigan et al., 2003; Pearson et al., 2006). Early results concerning users’ alignment to system output style in the interpersonal dimension are reported in (Brennan and Ohaeri, 1994): They distinguish three styles: anthropomorphic (the system refers to itself using first person pronouns, like in (1a) above, fluent (complete sentences, but no selfreference) and telegraphic, like (2d). They found no differe</context>
<context position="24918" citStr="Brennan and Ohaeri, 1994" startWordPosition="3910" endWordPosition="3913">participant input across the groups (politeness marking: F(2)=1.5, p=0.24; all other Fs&lt;1 (ANOVA)). 6 Discussion and Conclusions We presented the generation of personal/impersonal style variation in the SAMMIE multimodal dialogue system, and the results of an experiment evaluating the influence of the system output style on the users’ subjective judgments and their formulation of input. Although our results are not conclusive, they point at a range of issues for further research. Regarding users’ attitudes to the system, we found no significant difference among the styles. This is similar to (Brennan and Ohaeri, 1994) who found no difference in intelligence attributed to the system by the users, but it is at odds with the earlier finding that a synthetic voice interface was judged to be more useful when avoiding self-reference by personal pronouns (Nass and Brave, 2005). Whereas (Brennan and Ohaeri, 1994) used a flight reservation dialogue system, (Nass and Brave, 2005) used a phone-based auction system which read out an introduction and five object descriptions. There are two points to note: First, the subjects heard system output that was a read out continuous text rather than turns in an interaction. Th</context>
<context position="27929" citStr="Brennan and Ohaeri, 1994" startWordPosition="4389" endWordPosition="4392"> counted as impersonal are common in both styles. Besides their deliberate use as means to avoid explicit reference to oneself, they also have their normal, neutral usage, and therefore, some of the utterances that we classified as impersonal style may just be neutral formulations, rather than cases of distancing or “de-agentivization”. However, we could not test this hypothesis, because we have not found a way to reliably distinguish between neutral and marked, truly impersonal utterances. This is an issue for future work. The difference between our results concerning alignment and those of (Brennan and Ohaeri, 1994) is not likely to be due to a difference in the degree of interactivity (as with (Nass and Brave, 2005)). We now comment on other differences between our systems, which might have contributed to the differences in results. One aspect where we differ concerns our distinction between personal and impersonal style, both in the implementation of the SAMMIE system and in the experiment: We include the presence/absence of agentivity not only in the system’s reference to itself (akin to (Nass and Brave, 2005) and (Brennan and Ohaeri, 1994)), but also in addressing the user. This concept of the person</context>
<context position="29319" citStr="Brennan and Ohaeri, 1994" startWordPosition="4610" endWordPosition="4613">imension is predominant. The present experiment results make it pertinent that more research into the motives behind expressing or suppressing agentivity in both dimensions is needed. Apart from the linguistic design of the system’s output, other factors influence users’ behavior and perception of the system, and thus might confound experiment results, e.g., functionality, design, ergonomics, speech synthesis and speech recognition. A system with synthesized speech should be more positively rated when it does not refer to itself as an active agent by personal pronouns (Nass and Brave, 2005). (Brennan and Ohaeri, 1994) used a system with written interaction, the SAMMIE system employs the MARY text-to-speech synthesis system (Schr¨oder and Trouvain, 2003) with an MBROLA diphone synthesiser, which produces an acceptable though not outstanding output quality. Our post-hoc analysis showed a tendency towards better judgements of the system by the participants experiencing less speech recognition problems. This is as expected. We did not find any statistically significant effect regarding the style-related features we analyzed. A future experiment should address the possibility of an interaction between system st</context>
</contexts>
<marker>Brennan, Ohaeri, 1994</marker>
<rawString>S. Brennan and J.O. Ohaeri. 1994. Effects of message style on user’s attribution toward agents. In Proceedings of CHI’94 Conference Companion Human Factors in Computing Systems, pages 281–282. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brennan</author>
</authors>
<title>Lexical entrainment in spontaneous dialogue.</title>
<date>1996</date>
<booktitle>In Proceedings of the International Symposium on Spoken Dialogue (ISSD-96),</booktitle>
<pages>41--44</pages>
<contexts>
<context position="5065" citStr="Brennan, 1996" startWordPosition="794" endWordPosition="795">“I” to refer to itself (Nass et al., 2006). Another question is whether system output style influences users’ input formulation, as would be expected due to the phenomenon of alignment, which is generally considered a basic principle in natural language dialogue (Garrod and Pickering, 2004).1 Experiments targeting human-human conversation show that speakers in spontaneous dialogues tend to express themselves in similar ways at lexical and syntactic levels (e.g., (Hadelich et al., 2004; Garrod and Pickering, 2004). Lexical and syntactic alignment is present in human-computer interaction, too. (Brennan, 1996) suggested that users adopt system’s terms to avoid errors, expecting the sys1This dialogue phenomenon goes under a variety of terms in the literature, besides alignment, e.g., accommodation, adaptation, convergence, entrainment or shaping (used, e.g., by (Brennan and Ohaeri, 1994)). tem to be inflexible. However, recent experiments show that alignment in human-computer interaction is also automatic and its strength is comparable to that in human-human communication (Branigan et al., 2003; Pearson et al., 2006). Early results concerning users’ alignment to system output style in the interperso</context>
</contexts>
<marker>Brennan, 1996</marker>
<rawString>S. Brennan. 1996. Lexical entrainment in spontaneous dialogue. In Proceedings of the International Symposium on Spoken Dialogue (ISSD-96), pages 41–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Bunt</author>
<author>M Kipp</author>
<author>M Maybury</author>
<author>W Wahlster</author>
</authors>
<title>Fusion and coordination for multimodal interactive information presentation: Roadmap, architecture, tools, semantics.</title>
<date>2005</date>
<booktitle>Multimodal Intelligent Information Presentation, volume 27 of Text, Speech and Language Technology,</booktitle>
<pages>325--340</pages>
<editor>In O. Stock and M. Zancanaro, editors,</editor>
<publisher>Kluwer Academic.</publisher>
<contexts>
<context position="9055" citStr="Bunt et al., 2005" startWordPosition="1429" endWordPosition="1432">ction as a collaborative problem solving (CPS) process, modeling the tasks and their progression as recipes and a multimodal interpretation that fits any user input into the context of the current task (Blaylock and Allen, 2005). To support dialogue flexibility, we model discourse context, the CPS state and the driver’s attention state by an enriched information state (Kruijff-Korbayov´a et al., 2006a). 3.1 System Architecture The SAMMIE system architecture follows the classical approach of a pipelined architecture with multimodal fusion and fission modules encapsulating the dialogue manager (Bunt et al., 2005). Figure 1 shows the modules and their interaction: Modalityspecific recognizers and analysers provide semantically interpreted input to the multimodal fusion module (interpretation manager in Figure 1), that interprets them in the context of the other modalities and the current dialog context. The dialogue manager decides on the next system move, based on its CPS encoded task model, on the current context and also on the results from calls to the MP3 database. The multimodal fission component then generates the system reaction on a modality-dependent level Figure 1: SAMMIE system architecture</context>
</contexts>
<marker>Bunt, Kipp, Maybury, Wahlster, 2005</marker>
<rawString>H. Bunt, M. Kipp, M. Maybury, and W. Wahlster. 2005. Fusion and coordination for multimodal interactive information presentation: Roadmap, architecture, tools, semantics. In O. Stock and M. Zancanaro, editors, Multimodal Intelligent Information Presentation, volume 27 of Text, Speech and Language Technology, pages 325–340. Kluwer Academic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Garrod</author>
<author>M Pickering</author>
</authors>
<title>Why is conversation so easy?</title>
<date>2004</date>
<journal>TRENDS in Cognitive Sciences,</journal>
<volume>8</volume>
<contexts>
<context position="4742" citStr="Garrod and Pickering, 2004" startWordPosition="747" endWordPosition="750">logy (human vs. machine), etc. On the basis of an investigation of a range of user attitudes to their simulated system with a synthetic vs. a recorded voice, they conclude that a recorded voice system is perceived as more human-like and thus entitled to use “I”, whereas a synthetic-voice system is not perceived as human enough to use “I” to refer to itself (Nass et al., 2006). Another question is whether system output style influences users’ input formulation, as would be expected due to the phenomenon of alignment, which is generally considered a basic principle in natural language dialogue (Garrod and Pickering, 2004).1 Experiments targeting human-human conversation show that speakers in spontaneous dialogues tend to express themselves in similar ways at lexical and syntactic levels (e.g., (Hadelich et al., 2004; Garrod and Pickering, 2004). Lexical and syntactic alignment is present in human-computer interaction, too. (Brennan, 1996) suggested that users adopt system’s terms to avoid errors, expecting the sys1This dialogue phenomenon goes under a variety of terms in the literature, besides alignment, e.g., accommodation, adaptation, convergence, entrainment or shaping (used, e.g., by (Brennan and Ohaeri, </context>
</contexts>
<marker>Garrod, Pickering, 2004</marker>
<rawString>S. Garrod and M. Pickering. 2004. Why is conversation so easy? TRENDS in Cognitive Sciences, 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hadelich</author>
<author>H Branigan</author>
<author>M Pickering</author>
<author>M Crocker</author>
</authors>
<title>Alignment in dialogue: Effects of feedback on lexical overlap within and between participants.</title>
<date>2004</date>
<booktitle>In Proceedings of the AMLaP Conference. Aix en Provence,</booktitle>
<contexts>
<context position="4940" citStr="Hadelich et al., 2004" startWordPosition="776" endWordPosition="779">perceived as more human-like and thus entitled to use “I”, whereas a synthetic-voice system is not perceived as human enough to use “I” to refer to itself (Nass et al., 2006). Another question is whether system output style influences users’ input formulation, as would be expected due to the phenomenon of alignment, which is generally considered a basic principle in natural language dialogue (Garrod and Pickering, 2004).1 Experiments targeting human-human conversation show that speakers in spontaneous dialogues tend to express themselves in similar ways at lexical and syntactic levels (e.g., (Hadelich et al., 2004; Garrod and Pickering, 2004). Lexical and syntactic alignment is present in human-computer interaction, too. (Brennan, 1996) suggested that users adopt system’s terms to avoid errors, expecting the sys1This dialogue phenomenon goes under a variety of terms in the literature, besides alignment, e.g., accommodation, adaptation, convergence, entrainment or shaping (used, e.g., by (Brennan and Ohaeri, 1994)). tem to be inflexible. However, recent experiments show that alignment in human-computer interaction is also automatic and its strength is comparable to that in human-human communication (Bra</context>
</contexts>
<marker>Hadelich, Branigan, Pickering, Crocker, 2004</marker>
<rawString>K. Hadelich, H. Branigan, M. Pickering, and M. Crocker. 2004. Alignment in dialogue: Effects of feedback on lexical overlap within and between participants. In Proceedings of the AMLaP Conference. Aix en Provence, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amy Isard</author>
<author>Carsten Brockmann</author>
<author>Jon Oberlander</author>
</authors>
<title>Individuality and alignment in generated dialogues.</title>
<date>2006</date>
<booktitle>In Proceedings of the 4th International Natural Language Generation Conference (INLG-06),</booktitle>
<pages>22--29</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="3431" citStr="Isard et al., 2006" startWordPosition="532" endWordPosition="535">Section 6 we provide a discussion and conclusions. 2 Previous Work Although recently developed dialogue systems adapt their output to the users in various ways, this 129 usually concerns content selection rather than surface realization. There is to our knowledge no system that varies the style of its output in the interpersonal dimension as we have done in SAMMIE. Work on animated conversational agents has addressed various issues concerning agents displaying their personality, but this usually concerns emotional states and personality traits, rather than the personal/impersonal alteration. (Isard et al., 2006) model personality and alignment in generated dialogues between pairs of agents using OpenCCG and an over-generation and ranking approach, guided by a set of language models. Their approach probably could produce the personal/impersonal style variation as an effect of personality or a side-effect of syntactic alignment. The question whether a system should generate output in personal or impersonal style has been addressed by (Nass and Brave, 2005): They observe that agents that use “I” are generally perceived more like a person than those that do not. However, systems tend to be more positivel</context>
</contexts>
<marker>Isard, Brockmann, Oberlander, 2006</marker>
<rawString>Amy Isard, Carsten Brockmann, and Jon Oberlander. 2006. Individuality and alignment in generated dialogues. In Proceedings of the 4th International Natural Language Generation Conference (INLG-06), pages 22–29, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Kempe</author>
</authors>
<title>PATE a production rule system based on activation and typed feature structure elements. Bachelor Thesis,</title>
<date>2004</date>
<institution>Saarland University,</institution>
<contexts>
<context position="10267" citStr="Kempe, 2004" startWordPosition="1612" endWordPosition="1613">. by selecting the content to present, distributing it appropriately over the available output modalities and finally co-ordinating and synchronizing the output. Modality-specific output modules generate spoken output and an update of the graphical display. All modules interact with the extended information state in which all context information is stored. Many tasks in the SAMMIE system are modeled by a rule-based approach. Discourse modeling, interpretation management, dialogue management, turn planning and linguistic planning are all based on the production rule system PATE (Pfleger, 2004; Kempe, 2004). For speech recognition, we use Nuance. The spoken output is synthesized with the Mary TTS (Schr¨oder and Trouvain, 2003).2 3.2 Generation of Natural Language Output with Variation To generate natural language output in SAMMIE, we developed a template-based generator. It is implemented by a set of sentence planning rules in PATE to build the templates, and a set of XSLT transformations for sentence realization, which yield the output strings. German and English output is produced by accessing different dictionaries in a uniform way. The output is either plain text, if it is to be displayed in</context>
</contexts>
<marker>Kempe, 2004</marker>
<rawString>Benjamin Kempe. 2004. PATE a production rule system based on activation and typed feature structure elements. Bachelor Thesis, Saarland University, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G J M Kruijff</author>
<author>I Kruijff-Korbayov´a</author>
<author>J Bateman</author>
<author>D Dochev</author>
<author>N Gromova</author>
<author>T Hartley</author>
<author>E Teich</author>
<author>S Sharoff</author>
<author>L Sokolova</author>
<author>K Staykova</author>
</authors>
<title>Deliverable TEXS2: Specification of elaborated text structures.</title>
<date>1999</date>
<tech>Technical report, AGILE Project, EU INCO COPERNICUS PL961104.</tech>
<marker>Kruijff, Kruijff-Korbayov´a, Bateman, Dochev, Gromova, Hartley, Teich, Sharoff, Sokolova, Staykova, 1999</marker>
<rawString>G.J.M. Kruijff, I. Kruijff-Korbayov´a, J. Bateman, D. Dochev, N. Gromova, T. Hartley, E. Teich, S. Sharoff, L. Sokolova, and K. Staykova. 1999. Deliverable TEXS2: Specification of elaborated text structures. Technical report, AGILE Project, EU INCO COPERNICUS PL961104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Kruijff-Korbayov´a</author>
<author>O Kukina</author>
</authors>
<title>The effect of dialogue system output style variation on users’ evaluation judgements and input style.</title>
<date>2008</date>
<booktitle>In Proceedings of SigDial’08,</booktitle>
<location>Columbus, Ohio.</location>
<marker>Kruijff-Korbayov´a, Kukina, 2008</marker>
<rawString>I. Kruijff-Korbayov´a and O. Kukina. 2008. The effect of dialogue system output style variation on users’ evaluation judgements and input style. In Proceedings of SigDial’08, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Kruijff-Korbayov´a</author>
<author>G Amores</author>
<author>N Blaylock</author>
<author>S Ericsson</author>
<author>G P´erez</author>
<author>K Georgila</author>
<author>M Kaisser</author>
<author>S Larsson</author>
<author>O Lemon</author>
<author>P Manch´on</author>
<author>J Schehl</author>
</authors>
<title>Deliverable D3.1: Extended information state modeling.</title>
<date>2006</date>
<tech>Technical report, TALK Project, EU FP6,</tech>
<pages>507802</pages>
<marker>Kruijff-Korbayov´a, Amores, Blaylock, Ericsson, P´erez, Georgila, Kaisser, Larsson, Lemon, Manch´on, Schehl, 2006</marker>
<rawString>I. Kruijff-Korbayov´a, G. Amores, N. Blaylock, S. Ericsson, G. P´erez, K. Georgila, M. Kaisser, S. Larsson, O. Lemon, P. Manch´on, and J. Schehl. 2006a. Deliverable D3.1: Extended information state modeling. Technical report, TALK Project, EU FP6, IST-507802.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivana Kruijff-Korbayov´a</author>
<author>Tilman Becker</author>
<author>Nate Blaylock</author>
<author>Ciprian Gerstenberger</author>
<author>Michael Kaisser</author>
<author>Peter Poller</author>
<author>Verena Rieser</author>
<author>Jan Schehl</author>
</authors>
<title>The SAMMIE corpus of multimodal dialogues with an MP3 player.</title>
<date>2006</date>
<booktitle>In Proceedings ofLREC,</booktitle>
<location>Genova, Italy.</location>
<marker>Kruijff-Korbayov´a, Becker, Blaylock, Gerstenberger, Kaisser, Poller, Rieser, Schehl, 2006</marker>
<rawString>Ivana Kruijff-Korbayov´a, Tilman Becker, Nate Blaylock, Ciprian Gerstenberger, Michael Kaisser, Peter Poller, Verena Rieser, and Jan Schehl. 2006b. The SAMMIE corpus of multimodal dialogues with an MP3 player. In Proceedings ofLREC, Genova, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Mutschler</author>
<author>F Steffens</author>
<author>A Korthauer</author>
</authors>
<title>Deliverable D6.4: Final report on multimodal experiments Part I: Evaluation of the SAMMIE system.</title>
<date>2007</date>
<tech>Technical report, TALK Project, EU FP6,</tech>
<pages>507802</pages>
<contexts>
<context position="17167" citStr="Mutschler et al., 2007" startWordPosition="2727" endWordPosition="2730"> the system. The experimental tasks were presented to each participant in randomized order apart from the free use of the system, which was always the last task. The experimenter (E) repeated each task assignment twice to the participant, once in personal and once in impersonal style, as shown in the example below. (10) E: Bitte frage das System nach den Liedern von “Pur”. Du willst also wissen welche Lieder von “Pur” es gibt. E: Please ask the the system about the songs by “Pur”. You would like to know which songs by “Pur” there are. The questionnaire was based on (Nass and Brave, 2005) and (Mutschler et al., 2007). It contained questions with a 6-point scale ranging from 1 (low grade) to 6 (high grade), such as How do you assess the system in general: technical (1) – human-like (6); Communication with the system seemed to you: boring (1) – exciting (6); In terms of usability, the system is: inefficient (1) —efficient(6). The recorded dialogues have been transcribed, the questionnaire responses tabulated. We manually annotated the participants’ utterances (on average 95 per session) with the following features for further analysis: • Construction type: Personal (+/-) Is the utterance a complete sentence</context>
<context position="22645" citStr="Mutschler et al., 2007" startWordPosition="3568" endWordPosition="3571"> participant utterances in a session. On average, around 33% of participant utterances were not understood by the system.5 We classified the participants into three groups according to the performance of speech recognition they experienced: the good group with less than 27% of input not understood (7 participants); the poor group 5This is admittedly rather bad performance, nevertheless it mostly does not prevent the participants from getting their tasks successfully completed within a reasonable time, as was shown in an rigorous usability evaluation of the system in normal driving conditions (Mutschler et al., 2007). 134 Figure 3: Judgments of the system by the “good” and “poor” speech recognition group with more than 37% of input not uderstood (7 participants); the average group (the remaining 14 participants). Speech Recognition and Attitudinal Data We suspected that speech recognition problems might be neutralizing a potential influence of style. Therefore we contrasted the judgments on all six factors between the good and the poor speech recognition group (see Figure 3). The “good” speech recognition group showed higher satisfaction with the communication (t(16)=1.9, p=0.04 (one-tailed)) and evaluate</context>
</contexts>
<marker>Mutschler, Steffens, Korthauer, 2007</marker>
<rawString>H. Mutschler, F. Steffens, and A. Korthauer. 2007. Deliverable D6.4: Final report on multimodal experiments Part I: Evaluation of the SAMMIE system. Technical report, TALK Project, EU FP6, IST-507802.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Nass</author>
<author>S Brave</author>
</authors>
<title>Should voice interfaces say ”I”? Recorded and synthetic voice interfaces’ claims to humanity, chapter 10,</title>
<date>2005</date>
<pages>113--124</pages>
<publisher>The MIT Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="3882" citStr="Nass and Brave, 2005" startWordPosition="602" endWordPosition="605">ents displaying their personality, but this usually concerns emotional states and personality traits, rather than the personal/impersonal alteration. (Isard et al., 2006) model personality and alignment in generated dialogues between pairs of agents using OpenCCG and an over-generation and ranking approach, guided by a set of language models. Their approach probably could produce the personal/impersonal style variation as an effect of personality or a side-effect of syntactic alignment. The question whether a system should generate output in personal or impersonal style has been addressed by (Nass and Brave, 2005): They observe that agents that use “I” are generally perceived more like a person than those that do not. However, systems tend to be more positively rated when consistent with respect to such parameters as personality, gender, ontology (human vs. machine), etc. On the basis of an investigation of a range of user attitudes to their simulated system with a synthetic vs. a recorded voice, they conclude that a recorded voice system is perceived as more human-like and thus entitled to use “I”, whereas a synthetic-voice system is not perceived as human enough to use “I” to refer to itself (Nass et</context>
<context position="6605" citStr="Nass and Brave, 2005" startWordPosition="1023" endWordPosition="1026">m’s intelligence across the different conditions. However, they observed that the anthropomorphic group was more than twice as likely to refer to the computer using the second person pronoun “you” and it used more indirect requests and conventional politeness than the other groups. They conclude that the anthropomorphic style is undesirable for dialogue systems because it encourages more complex user input which is harder to recognize and interpret. The described experiments used either the Wizard-of-Oz paradigm (Brennan and Ohaeri, 1994) or preprogrammed system output (Branigan et al., 2003; Nass and Brave, 2005) and involved written communication. Such methods allow one to test assumptions about idealized human-computer interaction. Experimenting with the SAMMIE system allows us to test whether similar effects arise in an interaction with an actual dialogue system, which is plagued, among other factors, by speech recognition problems. 3 The SAMMIE System SAMMIE is a multimodal dialogue system developed in the TALK project with particular emphasis on multimodal turn-planning and natural language generation to support intuitive mixed-initiative interaction. The SAMMIE system provides a multimodal inter</context>
<context position="17138" citStr="Nass and Brave, 2005" startWordPosition="2722" endWordPosition="2725">ng songs and (4) freeuse of the system. The experimental tasks were presented to each participant in randomized order apart from the free use of the system, which was always the last task. The experimenter (E) repeated each task assignment twice to the participant, once in personal and once in impersonal style, as shown in the example below. (10) E: Bitte frage das System nach den Liedern von “Pur”. Du willst also wissen welche Lieder von “Pur” es gibt. E: Please ask the the system about the songs by “Pur”. You would like to know which songs by “Pur” there are. The questionnaire was based on (Nass and Brave, 2005) and (Mutschler et al., 2007). It contained questions with a 6-point scale ranging from 1 (low grade) to 6 (high grade), such as How do you assess the system in general: technical (1) – human-like (6); Communication with the system seemed to you: boring (1) – exciting (6); In terms of usability, the system is: inefficient (1) —efficient(6). The recorded dialogues have been transcribed, the questionnaire responses tabulated. We manually annotated the participants’ utterances (on average 95 per session) with the following features for further analysis: • Construction type: Personal (+/-) Is the </context>
<context position="18907" citStr="Nass and Brave, 2005" startWordPosition="2994" endWordPosition="2997">junctive mood (eg. “ich h¨atte gerne”) 133 5 Results The results concerning users’ attitudes and alignment are presented in detail in (Kruijff-Korbayov´a and Kukina, 2008). Here we summarize the significant findings and provide an additional analysis of the influence of speech recognition problems. 5.1 Style and Users’ Attitudes The first issue addressed in the experiment was whether the users have different judgments of the personal vs. impersonal version of the system. Since the system used a synthetic voice, the judgments were expected to be more positive in the impersonal style condition (Nass and Brave, 2005). Based on factor analysis performed on attitudinal data from the user questionnaires we created the six indices listed below. All indices were meaningful and reliable. (A detailed description of the indices including the contributing factors from the questionnaires can be found in (Kruijff-Korbayov´a and Kukina, 2008).) 1. General satisfaction with the communication with the system (Cronbach’s α=0.86) 2. Easiness of communication with the system (α=0.83) 3. Usability of the system (α=0.76) 4. Clarity of the system’s speech (α=0.88) 5. Perceived “humanness” of the system (α=0.69) 6. System’s p</context>
<context position="25175" citStr="Nass and Brave, 2005" startWordPosition="3953" endWordPosition="3956">xperiment evaluating the influence of the system output style on the users’ subjective judgments and their formulation of input. Although our results are not conclusive, they point at a range of issues for further research. Regarding users’ attitudes to the system, we found no significant difference among the styles. This is similar to (Brennan and Ohaeri, 1994) who found no difference in intelligence attributed to the system by the users, but it is at odds with the earlier finding that a synthetic voice interface was judged to be more useful when avoiding self-reference by personal pronouns (Nass and Brave, 2005). Whereas (Brennan and Ohaeri, 1994) used a flight reservation dialogue system, (Nass and Brave, 2005) used a phone-based auction system which read out an introduction and five object descriptions. There are two points to note: First, the subjects heard system output that was a read out continuous text rather than turns in an interaction. This may have reinforced the activation of particular style features. Second, the auction task may have sensibilized the subjects to the distinction between subjective (the system’s) vs. objective information presentation, and thus make them more sensitive to</context>
<context position="28032" citStr="Nass and Brave, 2005" startWordPosition="4409" endWordPosition="4412">eference to oneself, they also have their normal, neutral usage, and therefore, some of the utterances that we classified as impersonal style may just be neutral formulations, rather than cases of distancing or “de-agentivization”. However, we could not test this hypothesis, because we have not found a way to reliably distinguish between neutral and marked, truly impersonal utterances. This is an issue for future work. The difference between our results concerning alignment and those of (Brennan and Ohaeri, 1994) is not likely to be due to a difference in the degree of interactivity (as with (Nass and Brave, 2005)). We now comment on other differences between our systems, which might have contributed to the differences in results. One aspect where we differ concerns our distinction between personal and impersonal style, both in the implementation of the SAMMIE system and in the experiment: We include the presence/absence of agentivity not only in the system’s reference to itself (akin to (Nass and Brave, 2005) and (Brennan and Ohaeri, 1994)), but also in addressing the user. This concept of the personal/impersonal distinction was inspired by such differences observed in a study of instructional texts i</context>
<context position="29291" citStr="Nass and Brave, 2005" startWordPosition="4606" endWordPosition="4609">999), where the latter dimension is predominant. The present experiment results make it pertinent that more research into the motives behind expressing or suppressing agentivity in both dimensions is needed. Apart from the linguistic design of the system’s output, other factors influence users’ behavior and perception of the system, and thus might confound experiment results, e.g., functionality, design, ergonomics, speech synthesis and speech recognition. A system with synthesized speech should be more positively rated when it does not refer to itself as an active agent by personal pronouns (Nass and Brave, 2005). (Brennan and Ohaeri, 1994) used a system with written interaction, the SAMMIE system employs the MARY text-to-speech synthesis system (Schr¨oder and Trouvain, 2003) with an MBROLA diphone synthesiser, which produces an acceptable though not outstanding output quality. Our post-hoc analysis showed a tendency towards better judgements of the system by the participants experiencing less speech recognition problems. This is as expected. We did not find any statistically significant effect regarding the style-related features we analyzed. A future experiment should address the possibility of an i</context>
</contexts>
<marker>Nass, Brave, 2005</marker>
<rawString>C. Nass and S. Brave, 2005. Should voice interfaces say ”I”? Recorded and synthetic voice interfaces’ claims to humanity, chapter 10, pages 113–124. The MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Nass</author>
<author>S Brave</author>
<author>L Takayama</author>
</authors>
<title>Socializing consistency: from technical homogeneity to human epitome. In</title>
<date>2006</date>
<pages>373--390</pages>
<location>Armonk, NY:</location>
<contexts>
<context position="4493" citStr="Nass et al., 2006" startWordPosition="710" endWordPosition="713">, 2005): They observe that agents that use “I” are generally perceived more like a person than those that do not. However, systems tend to be more positively rated when consistent with respect to such parameters as personality, gender, ontology (human vs. machine), etc. On the basis of an investigation of a range of user attitudes to their simulated system with a synthetic vs. a recorded voice, they conclude that a recorded voice system is perceived as more human-like and thus entitled to use “I”, whereas a synthetic-voice system is not perceived as human enough to use “I” to refer to itself (Nass et al., 2006). Another question is whether system output style influences users’ input formulation, as would be expected due to the phenomenon of alignment, which is generally considered a basic principle in natural language dialogue (Garrod and Pickering, 2004).1 Experiments targeting human-human conversation show that speakers in spontaneous dialogues tend to express themselves in similar ways at lexical and syntactic levels (e.g., (Hadelich et al., 2004; Garrod and Pickering, 2004). Lexical and syntactic alignment is present in human-computer interaction, too. (Brennan, 1996) suggested that users adopt </context>
</contexts>
<marker>Nass, Brave, Takayama, 2006</marker>
<rawString>C. Nass, S. Brave, and L. Takayama. 2006. Socializing consistency: from technical homogeneity to human epitome. In P. Zhang &amp; D. Galletta (Eds.), Humancomputer interaction in management information systems: Foundations, pages 373–390. Armonk, NY: M. E. Sharpe.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pearson</author>
<author>J Hu</author>
<author>H Branigan</author>
<author>M J Pickering</author>
<author>C I Nass</author>
</authors>
<title>Adaptive language behavior in HCI: how expectations and beliefs about a system affect users’ word choice.</title>
<date>2006</date>
<booktitle>In CHI ’06: Proceedings of the SIGCHI conference on Human Factors in computing systems,</booktitle>
<pages>1177--1180</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5581" citStr="Pearson et al., 2006" startWordPosition="868" endWordPosition="871">ng, 2004). Lexical and syntactic alignment is present in human-computer interaction, too. (Brennan, 1996) suggested that users adopt system’s terms to avoid errors, expecting the sys1This dialogue phenomenon goes under a variety of terms in the literature, besides alignment, e.g., accommodation, adaptation, convergence, entrainment or shaping (used, e.g., by (Brennan and Ohaeri, 1994)). tem to be inflexible. However, recent experiments show that alignment in human-computer interaction is also automatic and its strength is comparable to that in human-human communication (Branigan et al., 2003; Pearson et al., 2006). Early results concerning users’ alignment to system output style in the interpersonal dimension are reported in (Brennan and Ohaeri, 1994): They distinguish three styles: anthropomorphic (the system refers to itself using first person pronouns, like in (1a) above, fluent (complete sentences, but no selfreference) and telegraphic, like (2d). They found no difference in users’ perception of the system’s intelligence across the different conditions. However, they observed that the anthropomorphic group was more than twice as likely to refer to the computer using the second person pronoun “you” </context>
</contexts>
<marker>Pearson, Hu, Branigan, Pickering, Nass, 2006</marker>
<rawString>J. Pearson, J. Hu, H. Branigan, M. J. Pickering, and C. I. Nass. 2006. Adaptive language behavior in HCI: how expectations and beliefs about a system affect users’ word choice. In CHI ’06: Proceedings of the SIGCHI conference on Human Factors in computing systems, pages 1177–1180, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Pfleger</author>
</authors>
<title>Context based multimodal fusion.</title>
<date>2004</date>
<booktitle>In ICMI ’04: Proceedings of the 6th international conference on Multimodal interfaces,</booktitle>
<pages>265--272</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="10253" citStr="Pfleger, 2004" startWordPosition="1610" endWordPosition="1611">em architecture. by selecting the content to present, distributing it appropriately over the available output modalities and finally co-ordinating and synchronizing the output. Modality-specific output modules generate spoken output and an update of the graphical display. All modules interact with the extended information state in which all context information is stored. Many tasks in the SAMMIE system are modeled by a rule-based approach. Discourse modeling, interpretation management, dialogue management, turn planning and linguistic planning are all based on the production rule system PATE (Pfleger, 2004; Kempe, 2004). For speech recognition, we use Nuance. The spoken output is synthesized with the Mary TTS (Schr¨oder and Trouvain, 2003).2 3.2 Generation of Natural Language Output with Variation To generate natural language output in SAMMIE, we developed a template-based generator. It is implemented by a set of sentence planning rules in PATE to build the templates, and a set of XSLT transformations for sentence realization, which yield the output strings. German and English output is produced by accessing different dictionaries in a uniform way. The output is either plain text, if it is to b</context>
</contexts>
<marker>Pfleger, 2004</marker>
<rawString>N. Pfleger. 2004. Context based multimodal fusion. In ICMI ’04: Proceedings of the 6th international conference on Multimodal interfaces, pages 265–272, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Schr¨oder</author>
<author>J Trouvain</author>
</authors>
<title>The German text-tospeech synthesis system MARY: A tool for research, development and teaching.</title>
<date>2003</date>
<journal>International Journal of Speech Technology,</journal>
<pages>6--365</pages>
<marker>Schr¨oder, Trouvain, 2003</marker>
<rawString>M. Schr¨oder and J. Trouvain. 2003. The German text-tospeech synthesis system MARY: A tool for research, development and teaching. International Journal of Speech Technology, 6:365–377.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>