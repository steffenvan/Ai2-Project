<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.019718">
<title confidence="0.999407">
Exploring the Planet of the APEs: a Comparative Study
of State-of-the-art Methods for MT Automatic Post-Editing
</title>
<author confidence="0.994292">
Rajen Chatterjee(1,2), Marion Weller(3), Matteo Negri(2), Marco Turchi(2)
</author>
<affiliation confidence="0.837247">
(1) University of Trento
</affiliation>
<address confidence="0.53248">
(2) FBK - Fondazione Bruno Kessler
(3) IMS, University of Stuttgart
</address>
<email confidence="0.992473">
{chatterjee,negri,turchi}@fbk.eu
{wellermn@ims.uni-stuttgart.de}
</email>
<sectionHeader confidence="0.997349" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999909769230769">
Downstream processing of machine trans-
lation (MT) output promises to be a so-
lution to improve translation quality, es-
pecially when the MT system’s internal
decoding process is not accessible. Both
rule-based and statistical automatic post-
editing (APE) methods have been pro-
posed over the years, but with contrast-
ing results. A missing aspect in previous
evaluations is the assessment of different
methods: i) under comparable conditions,
and ii) on different language pairs featur-
ing variable levels of MT quality. Fo-
cusing on statistical APE methods (more
portable across languages), we propose
the first systematic analysis of two ap-
proaches. To understand their potential,
we compare them in the same conditions
over six language pairs having English
as source. Our results evidence consis-
tent improvements on all language pairs,
a relation between the extent of the gain
and MT output quality, slight but statis-
tically significant performance differences
between the two methods, and their possi-
ble complementarity.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999816730769231">
Automatic post-editing (APE) aims to correct sys-
tematic machine translation (MT) errors. The
problem is appealing for several reasons. On one
side, as pointed out by Parton et al. (2012), APE
systems can improve MT output by exploiting in-
formation unavailable to the decoder, or by per-
forming deeper text analysis that is too expensive
at decoding stage. On the other side, and to our
view more importantly, APE represents the only
way to recover errors produced in “black-box”
conditions in which the MT system is unknown
or its internal decoding process is not accessible.
The task, firstly proposed by Knight and Chan-
der (1994) to cope with article selection in
Japanese to English translation, has been later ad-
dressed in various ways. On one side, rule-based
methods (Rosa et al., 2012) gained limited atten-
tion, probably due to the extensive manual work
they involve and their scarce portability across lan-
guages. On the other side, the statistical approach
proposed by Allen and Hogan (2000) reached ma-
turity in the work by Simard et al. (2007) and in-
spired a number of further investigations (Isabelle
et al., 2007; Dugast et al., 2007; Dugast et al.,
2009; Lagarda et al., 2009; B´echara et al., 2011;
B´echara et al., 2012; Rubino et al., 2012; Rosa et
al., 2013; Lagarda et al., 2014, inter alia).
Such prior works address orthogonal aspects
like: i) performance variations when APE is ap-
plied to correct the output of rule-based vs. statis-
tical MT, ii) the use of APE for error correction
vs. domain adaptation, iii) the difference between
training on general domain vs. domain-specific
data, iv) performance variations when learning
from reference translations vs. human post-edits.
Their common trait is that the reported results are
difficult to generalise. Indeed, most of the works
focus on evaluating a specific method,1 which is
typically applied to one single dataset for a given
language pair. As a result, the global landscape of
the “planet of the APEs” is still blurred and open
to more systematic explorations.
To shed light on the potential of statistical post-
editing, in this paper we examine two alterna-
tive approaches. One is the method proposed in
(Simard et al., 2007), which to date is the most
widely used. The other is the “context-aware” so-
lution proposed in (B´echara et al., 2011) which,
to the best of our knowledge, represents the most
significant variant of (Simard et al., 2007).
The major contribution of our work is the first
systematic analysis of different APE approaches,
</bodyText>
<footnote confidence="0.992434">
1Typically the same of (Simard et al., 2007).
</footnote>
<page confidence="0.770378">
156
</page>
<bodyText confidence="0.8655305">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 156–161,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
which are tested in controlled conditions over sev-
eral language pairs. To ensure the soundness of
the analysis, our experimental setup consists of
a dataset composed of the same English source
sentences with automatic translations into six lan-
guages and respective manual post-edits by pro-
fessional translators. Overall, this represents the
ideal condition to complement prior research with
the missing answers to questions like:
</bodyText>
<listItem confidence="0.8118385">
Q1: Does APE yield consistent MT quality im-
provements across different language pairs?
Q2: What is the relation between the original
MT output quality and the APE results?
Q3: Which of the two analysed APE methods
has the highest potential?
</listItem>
<sectionHeader confidence="0.991218" genericHeader="method">
2 Statistical APE methods
</sectionHeader>
<bodyText confidence="0.999348">
The two methods we analyse follow the same “sta-
tistical phrase-based post-editing” strategy out-
lined by Simard et al. (2007), but differ in the way
data is represented. Let’s give them a closer look.
</bodyText>
<subsectionHeader confidence="0.999119">
2.1 Method 1(Simard et al., 2007)
</subsectionHeader>
<bodyText confidence="0.999963631578947">
The underlying idea is that APE components can
be trained in the same way in which statistical MT
systems are developed – i.e. starting from “paral-
lel data”. Since the goal is to transform rough MT
output into its correct version, parallel data con-
sists of MT output as source texts and correct (hu-
man quality) sentences as target. In (Simard et al.,
2007) these are used to train a phrase-based MT
system, which is then applied to correct the output
of a commercial rule-based MT system.
Positive evaluation results are reported on
English-French, and even better ones on French-
English data. In both cases, statistical APE yields
significant BLEU and TER improvements over the
original MT output. However, since training and
test data for the two language directions are dif-
ferent (in content and size), the measured perfor-
mance variations cannot be directly ascribed to the
effectiveness of the method in the two settings.
</bodyText>
<subsectionHeader confidence="0.999707">
2.2 Method 2 (B´echara et al., 2011)
</subsectionHeader>
<bodyText confidence="0.995632140350877">
One limitation of the “monolingual translation”
approach proposed in (Simard et al., 2007) is that
the basic statistical APE pipeline is only trained on
data in the target language (F), disregarding infor-
mation about the source language (E): Correction
rules learned from (f0, f) pairs2 lose the connec-
tion between the translated words (or phrases) and
the corresponding source terms (e). This implies
that information lost or distorted in the translation
process is out of the reach of the APE component,
and the resulting errors are impossible to recover.
To cope with this issue, B´echara et al. (2011)
propose a “context-aware” variant to represent
the data. For each word f0, the corresponding
source word (or phrase) e is identified through
word alignment and used to obtain a joint rep-
resentation f0#e. The result is an intermediate
language F0#E that represents the new source
side of the parallel data used to train the statis-
tical APE component. Though in principle more
precise, this method can be affected by two prob-
lems. First, preserving the source context comes
at the cost of a larger vocabulary size and, con-
sequently, higher data sparseness. While the ba-
sic statistical APE pipeline combines and exploits
the counts of all the co-occurrences of f0 and f
in the parallel data, its context-aware variant con-
siders each f0#ei as a separate term, thus break-
ing down the co-occurrence counts of f0 and f
into smaller numbers. Second, all these counts can
be influenced by word alignment errors. To cope
with data sparseness and unreliable word align-
ment, B´echara et al. (2011) experiment with differ-
ent thresholds set on word alignment strengths to
filter context information. In particular, they dis-
card the (f0#e, f) pairs in which the f0#e align-
ment score is smaller than the threshold.
The approach, applied to correct the output of a
statistical phrase-based MT system, achieves am-
biguous evaluation results. On French-English,
significant improvements up to 2 BLEU points
are observed both over the baseline (the original
MT output) and the basic method of Simard et
al. (2007). On English-French, however, perfor-
mance slightly drops. Moreover, follow-up exper-
iments with the same method (B´echara, 2014) did
not confirm these results. In light of these ambigu-
ous results and the lack of a systematic compari-
son between the two APE methods, our objective is
to replicate them3 for a fair comparison in a con-
trolled evaluation setting involving different lan-
2Here, f&apos; and f respectively stand for the rough MT out-
put and its correct version in the foreign language F.
3This is done based on the description provided by the
published works. Discrepancies with the actual methods are
possible, due to our misinterpretation or to wrong guesses
about details that are missing in the papers.
</bodyText>
<page confidence="0.987907">
157
</page>
<bodyText confidence="0.828734">
guage pairs.
</bodyText>
<subsectionHeader confidence="0.999458">
2.3 Reimplementing the two methods
</subsectionHeader>
<bodyText confidence="0.99659480952381">
To obtain the statistical APE pipeline that repre-
sents the backbone of both methods we used a
phrase-based Moses system (Koehn et al., 2007).
Our training data (see Section 3) consists of
(source, MT output, post-edition) triplets for six
language pairs having English as source. While
Method 1 uses only the last two elements of the
triplet, all of them play a role in the context-aware
Method 2. Apart from the different data represen-
tation, the training process is identical.
Translation and reordering models were esti-
mated following the Moses protocol with default
setup using MGIZA++ (Gao and Vogel, 2008) for
word alignment.4 For language modeling we used
the KenLM toolkit (Heafield, 2011) for standard
n-gram modeling with an n-gram length of 5. The
APE system for each target language was tuned
on comparable development sets (see below), op-
timizing TER with Minimum Error Rate Training
(Och, 2003) using the post-edited sentences as ref-
erences.
</bodyText>
<sectionHeader confidence="0.999442" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999739142857143">
Some lessons learned from prior works on sta-
tistical APE methods (B´echara, 2014) include:
i) learning from human post-edits is more effec-
tive than learning from (independent) reference
translations, ii) learning from (and applying APE
to) domain-specific data is more promising than
working on general-domain data, iii) correcting
the output of rule-based MT systems is easier than
improving translations from statistical MT. Our
work capitalizes on these findings (we learn from
domain-specific post-edited data and apply APE to
statistical MT), but fills a gap of previous research:
a fair comparative study between different meth-
ods in controlled conditions. The key enabling
factor is the availability, for the first time, of data
consisting of the same source sentences, machine-
translated in several languages and post-edited by
professional translators.
Data. We experiment with the Autodesk Post-
Editing Data corpus,5 which predominantly cov-
ers the domain of software user manuals. English
</bodyText>
<footnote confidence="0.95089525">
4In Method 1, MGIZA++ is used to align f&apos; and f. In
Method 2 it is used to align f&apos; and e, and then f&apos;#e and f.
5https://autodesk.app.box.com/
Autodesk-PostEditing
</footnote>
<table confidence="0.998813555555556">
Lang. No. Vocab. No.
tokens Size Lemmas
En 210,491 10,727 8,260
Cs 202,475 16,716 10,137
De 211,149 17,563 14,368
Es 252,020 11,075 6,683
Fr 263,690 10,928 7,213
It 239,912 10,703 6,549
Pl 206,016 17,027 10,430
</table>
<tableCaption confidence="0.999681">
Table 1: Data statistics for each language.
</tableCaption>
<figureCaption confidence="0.66810975">
sentences are translated into several languages
(30K to 410K translations per language) with Au-
todesk’s in-house MT system (Zhechev, 2012) and
post-edited by professional translators.
</figureCaption>
<bodyText confidence="0.9998215">
Our experiments are run on six language pairs
having English as source and Czech, German,
Spanish, French, Italian and Polish as target. To
set up our controlled environment, we extract all
the (source, MT output, post-edition) triplets shar-
ing the same source (En) sentences across all lan-
guage pairs. Table 1 provides some statistics about
the resulting tri-parallel corpora. After random
shuffling the triplets, we create training (12.2K
triplets), development (2K) and test data (2K)
sharing exactly the same source sentences across
languages. Training and evaluation of our APE
systems are performed on true-case data.
To guarantee similar experimental conditions in
the six language settings, we also train compara-
ble target language models from external data (in-
deed, the 12.2K post-edits would not be enough
to train reliable LMs). We build our LMs from
approximately 2.5M translations of the same En-
glish sentences collected from Europarl (Koehn,
2005), DGT-Translation Memory (Steinberger et
al., 2012), JRC Acquis (Steinberger et al., 2006),
OPUS IT (Tiedemann, ) and other Autodesk data
common to all languages.
Evaluation metric. We evaluate the APE meth-
ods based on their capability to reduce the distance
between the MT output and a correct (fluent and
adequate) translation. As a measure of the amount
of the editing operations needed for the correction,
TER and HTER (Snover et al., 2006) fit for our
purpose. TER and HTER measure the minimum
edit distance between the MT output and its cor-
</bodyText>
<page confidence="0.982275">
158
</page>
<table confidence="0.999543375">
MT Baseline Method 1 Method 2 Oracle
TER TER A % Reduction TER A % Reduction TER
En-De 46.46 43.07 -3.39 7.3 42.79* -3.67 7.9 40.17
En-Cs 44.06 39.38 -4.68 10.62 39.10* -4.96 11.25 36.32
En-Pl 43.02 38.24 -4.78 11.11 37.75* -5.27 12.25 35.05
En-It 34.44 30.43 -4.01 11.64 30.13* -4.31 12.55 28.33
En-Fr 32.76 29.70 -3.06 9.34 29.51 -3.25 9.92 27.12
En-Es 30.90 26.69 -4.21 13.62 26.35* -4.55 14.72 24.34
</table>
<tableCaption confidence="0.976168">
Table 2: Performance of the MT baseline and the APE methods for each language pair. Results for
Method 2 marked with the “*” symbol are statistically significant compared to Method 1.
</tableCaption>
<bodyText confidence="0.9974492">
rect version.6 This can be either a reference trans-
lation created independently from the MT out-
put (TER) or a human post-edition obtained by
manually correcting the MT output (HTER). For
the sake of simplicity, henceforth we will use the
term TER to refer to both situations (though, when
measuring the distance between the MT output
and its human post-edition the actual metric is the
HTER).
Baseline. Similar to all previous works on APE,
our baseline is the MT output as is. Hence, base-
line scores for each language pair correspond to
the TER computed between the original MT out-
put (produced by the “black-box” Autodesk in-
house system) and the human post-edits.
</bodyText>
<sectionHeader confidence="0.999972" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.99991994117647">
Table 2 lists our results, with language pairs or-
dered according to the respective baseline TER.
The positive answer to Q1 (“Does APE yield con-
sistent improvements to MT output?”) is evident:
both APE methods consistently improve MT qual-
ity on all language pairs. TER reductions range
from 3.06 to 5.27 points. Quality improvements
are statistically significant at p &lt; 0.05, measured
by bootstrap test (Koehn, 2004).
In answer to Q2 (“What is the relation between
original MT quality and APE results?”), our con-
trolled experiments evidence for the first time in
APE research that the higher the MT quality, the
higher is the improvement, i.e. percentage of er-
ror reduction, yielded by the APE methods. On
one side, this interesting result may seem counter-
intuitive because a larger room for improvement
</bodyText>
<footnote confidence="0.911329">
6Edit distance is calculated as the number of edits (word
insertions, deletions, substitutions, and shifts) divided by
the number of words in the correct translation. Lower
TER/HTER values indicate better MT quality.
</footnote>
<bodyText confidence="0.999427351351352">
is expected for sentences of poor quality. On the
other side, it reveals that learning from (and cor-
recting) noisy data affected by many errors is par-
ticularly difficult for statistical APE methods. This
finding is violated by En-Fr, for which a reason-
ably good MT quality does not induce a gain in
performance comparable to language pairs featur-
ing similar MT TER (En-It and En-Es). On fur-
ther analysis of the data, we notice that all the tar-
get languages except French keep a coherent be-
haviour with respect to the domain-specific En-
glish terms, which are always either preserved (It)
or translated (other languages). Instead, French
shows an alternation between the two conducts.
One example is the English word “workflow”,
which appears in the French post-editions both as
is (21 sentences) and translated into “flux de tra-
vail” (34 sentences). In contrast, in the other lan-
guage directions all the occurrences of ‘workflow”
are either translated or kept in English. These fre-
quent ambiguities are difficult to manage (espe-
cially if the two forms occur a similar number of
times in the training data), and might motivate the
smaller quality gains observed on En-Fr compared
to the other language pairs.
In answer to Q3 (“Which method has the high-
est potential?”), we observe slight TER reduc-
tions when moving from Method 1 to its “context-
aware” variant.7 Although small (from 0.19 to
0.49 TER points), such gains are statistically sig-
nificant (p &lt; 0.05), except for En-Fr (p &lt; 0.07).
This suggests that linking the MT words to the
source terms can help to recover adequacy errors
that are out of the reach of Method 1.
To better understand to what extent the two
methods behave differently, we calculated the re-
sults of an Oracle system, similar to the one pro-
</bodyText>
<footnote confidence="0.8706305">
7Filtering the context information with thresholds be-
tween 0.6 and 0.8 leads to the best results for all languages.
</footnote>
<page confidence="0.99781">
159
</page>
<bodyText confidence="0.999474633333333">
posed by Rubino et al. (2012), defined by se-
lecting for each test sentence the best post-edit
(lower TER) produced by two approaches. As
shown in the last column of Table 2, such an ora-
cle achieves a significant TER reduction (from 1.8
to 2.78 points) for all the language pairs. We inter-
pret such gains as clues of a possible complemen-
tarity between the two methods, which is worth to
investigate.
As mentioned in Section 2.2, an advantage of
Method 1 is its robust estimation of translation pa-
rameters. In contrast, by exploiting contextual in-
formation from the source, Method 2 is more pre-
cise but potentially affected by data sparsity issues
due to its highly increased vocabulary. In an at-
tempt to use a less sparse model at the level of
word alignment, we trained a SMT system based
on the context-aware representation of Method 2
(f&apos;#e), but with word alignment computed on the
representation of Method 1 (f&apos;). Applying this
method to the three language pairs for which the
two original methods achieved the lowest TER re-
ductions (i.e. En-De, En-Fr and En-Cs) shows that
this simple way to combine Methods 1 and 2 is
able to produce a TER decrement of 0.75 (42.04)
for En-De, 0.60 (38.50) for En-Cs and 0.53 (28.98)
for En-Fr. This seems to validate our intuition
about the possible complementarity of Methods 1
and 2, suggesting a promising direction for future
work.
</bodyText>
<sectionHeader confidence="0.997553" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999981935483871">
We explored the “planet of the APEs” in ideal
conditions (quantity and quality of data) and
with the right equipment (state-of-the-art meth-
ods). The data available (the same English sen-
tences, machine-translated in six languages and
post-edited by professional translators) allowed us
to compare for the first time different approaches
in a fair setting (our first contribution). The two
methods we analysed allowed us to measure con-
sistent improvements on all language pairs (TER
reductions from 7.3% to 14.7% – second contri-
bution), and to observe interesting relations be-
tween the extent of the gain and the original MT
output quality (the higher the quality, the higher
the gain yield by APE – third contribution). This
first study represents a good starting point for fu-
ture quests. A promising direction to explore is the
possible complementarity between the two meth-
ods and the room for mutual improvement. Now
we just have a glimpse of the path (higher ora-
cle results, slight gains with a first combination
method – fourth contribution), but positive prelim-
inary results confirm its existence.
To encourage the replication of our exper-
iments by other researchers and the reuse of
the selected Autodesk data for benchmark-
ing purposes in the same setting, the scripts
developed in this work have been publicly re-
leased. They can be downloaded from: https:
//bitbucket.org/turchmo/apeatfbk/
src/master/papers/ACL2015/.
</bodyText>
<sectionHeader confidence="0.975306" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999268285714286">
This work has been partially supported by the EC-
funded H2020 project QT21 (grant agreement no.
645452). The work of Marion Weller was sup-
ported by the FBK-HLT Summer Internship Pro-
gram 2014. The authors would like to thank Dr.
Ventsislav Zhechev for his support with the Au-
todesk Post-Editing Data corpus.
</bodyText>
<sectionHeader confidence="0.998848" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997646">
Jeffrey Allen and Christopher Hogan. 2000. To-
ward the Development of a Post Editing Module
for Raw Machine Translation Output: A Controlled
Language Perspective. In Third International Con-
trolled Language Applications Workshop (CLAW-
00), pages 62–71.
Hanna B´echara, Yanjun Ma, and Josef van Genabith.
2011. Statistical Post-Editing for a Statistical MT
System. In Proceedings of the 13th Machine Trans-
lation Summit, pages 308–315, Xiamen, China,
September.
Hanna B´echara, Rapha¨el Rubino, Yifan He, Yanjun
Ma, and Josef Genabith. 2012. An Evaluation of
Statistical Post-Editing Systems Applied to RBMT
and SMT Systems. In Proceedings of COLING
2012, pages 215–230, Mumbai, India.
Hanna B´echara. 2014. Statistical Post-editing and
Quality Estimation for Machine Translation Sys-
tems. M.Sc. Thesis, Dublin City University, Dublin.
Loic Dugast, Jean Senellart, and Philipp Koehn. 2007.
Statistical Post-editing on SYSTRAN’s Rule-based
Translation System. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
StatMT ’07, pages 220–223, Stroudsburg, PA, USA.
Loic Dugast, Jean Senellart, and Philipp Koehn.
2009. Statistical Post Editing and Dictionary Ex-
traction: Systran/Edinburgh Submissions for ACL-
WMT2009. In Proceedings of the Fourth Workshop
on Statistical Machine Translation, pages 110–114,
Athens, Greece.
</reference>
<page confidence="0.987561">
160
</page>
<reference confidence="0.997777871559633">
Qin Gao and Stephan Vogel. 2008. Parallel Implemen-
tations of Word Alignment Tool. In Proceedings of
the ACL 2008 Software Engineering, Testing, and
Quality Assurance Workshop, pages 49–57, Colum-
bus, Ohio.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation, pages 187–197, Edinburgh, Scot-
land, United Kingdom.
Pierre Isabelle, Cyril Goutte, and Michel Simard.
2007. Domain Adaptation of MT Systems through
Automatic Post-editing. In Proceedings of the
Eleventh Machine Translation Summit (MT Summit
XI), pages 255–261, Copenhagen, Denmark.
Kevin Knight and Ishwar Chander. 1994. Automated
Postediting of Documents. In Proceedings of the
12th National Conference on Artificial Intelligence,
pages 779–784, Seattle, WA, USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics Compan-
ion Volume Proceedings of the Demo and Poster Ses-
sions, pages 177–180, Prague, Czech Republic.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
EMNLP 2004, pages 388–395, Barcelona, Spain.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
the Tenth Machine Translation Summit (MT Summit
X), pages 79–86, Phuket, Thailand, September.
Antonio L. Lagarda, Vicent Alabau, Francisco Casacu-
berta, Roberto Silva, and Enrique D´ıaz-de Lia˜no.
2009. Statistical Post-editing of a Rule-based Ma-
chine Translation System. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 217–220.
Antonio L. Lagarda, Daniel Ortiz-Mart´ınez, Vicent Al-
abau, and Francisco Casacuberta. 2014. Translat-
ing without in-domain Corpus: Machine Transla-
tion Post-editing with Online Learning Techniques.
Computer Speech &amp; Language.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of the 41st Annual Meeting on Association for Com-
putational Linguistics - Volume 1, ACL ’03, pages
160–167, Sapporo, Japan.
Kristen Parton, Nizar Habash, Kathleen McKeown,
Gonzalo Iglesias, and Adri`a de Gispert. 2012. Can
Automatic Post-Editing Make MT More Meaning-
ful? In Proceedings of the 16th Conference of
the European Association for Machine Translation
(EAMT), pages 111–118, Trento, Italy.
Rudolf Rosa, David Mareˇcek, and Ondˇrej Duˇsek.
2012. DEPFIX: A System for Automatic Correc-
tion of Czech MT Outputs. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion, WMT ’12, pages 362–368, Montreal, Canada.
Rudolf Rosa, David Marecek, and Ales Tamchyna.
2013. Deepfix: Statistical Post-editing of Statistical
Machine Translation Using Deep Syntactic Analy-
sis. In Proceedings of the ACL 2013 Student Re-
search Workshop, pages 172–179, Sofia, Bulgaria.
Rapha¨el Rubino, St´ephane Huet, Fabrice Lef`evre, and
Georges Lenar´es. 2012. Statistical Post-Editing
of Machine Translation for Domain Adaptation. In
Proceedings of the European Association for Ma-
chine Translation (EAMT), pages 221–228, Trento,
Italy.
Michel Simard, Cyril Goutte, and Pierre Isabelle.
2007. Statistical Phrase-Based Post-Editing. In
Proceedings of the Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics (NAACL HLT), pages 508–515,
Rochester, New York.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings ofAssociation forMachine
Translation in the Americas, pages 223–231, Cam-
bridge, Massachusetts, USA.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, Dan Tufis, and Dniel
Varga. 2006. The JRC-Acquis: A Multilingual
Aligned Parallel Corpus with 20+ Languages. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC’2006),
Genoa, Italy.
Ralf Steinberger, Andreas Eisele, Szymon Klocek,
Spyridon Pilos, and Patrick Schl¨uter. 2012. DGT-
TM: A Freely Available Translation Memory in 22
Languages. In Proceedings of the Eight Interna-
tional Conference on Language Resources and Eval-
uation (LREC’12), Istanbul, Turkey.
J¨org Tiedemann. Parallel Data, Tools and Interfaces
in OPUS. In Proceedings of the Eight International
Conference on Language Resources and Evaluation
(LREC’12), pages 2214–2218, Istanbul, Turkey.
Ventsislav Zhechev. 2012. Machine Translation In-
frastructure and Post-Editing Performance at Au-
todesk. In AMTA 2012 Workshop on Post-Editing
Technology and Practice (WPTP 2012), pages 87–
96, San Diego, CA, USA.
</reference>
<page confidence="0.998234">
161
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.760300">
<title confidence="0.997286">Exploring the Planet of the APEs: a Comparative of State-of-the-art Methods for MT Automatic Post-Editing</title>
<author confidence="0.999705">Marion Matteo Marco</author>
<note confidence="0.880904666666667">(1)University of (2)FBK - Fondazione Bruno (3)IMS, University of</note>
<abstract confidence="0.996938407407407">Downstream processing of machine translation (MT) output promises to be a solution to improve translation quality, especially when the MT system’s internal decoding process is not accessible. Both rule-based and statistical automatic postediting (APE) methods have been proposed over the years, but with contrasting results. A missing aspect in previous evaluations is the assessment of different comparable conditions, different language pairs featuring variable levels of MT quality. Focusing on statistical APE methods (more portable across languages), we propose the first systematic analysis of two approaches. To understand their potential, we compare them in the same conditions over six language pairs having English as source. Our results evidence consistent improvements on all language pairs, a relation between the extent of the gain and MT output quality, slight but statistically significant performance differences between the two methods, and their possible complementarity.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jeffrey Allen</author>
<author>Christopher Hogan</author>
</authors>
<title>Toward the Development of a Post Editing Module for Raw Machine Translation Output: A Controlled Language Perspective.</title>
<date>2000</date>
<booktitle>In Third International Controlled Language Applications Workshop (CLAW00),</booktitle>
<pages>62--71</pages>
<contexts>
<context position="2386" citStr="Allen and Hogan (2000)" startWordPosition="362" endWordPosition="365">d to our view more importantly, APE represents the only way to recover errors produced in “black-box” conditions in which the MT system is unknown or its internal decoding process is not accessible. The task, firstly proposed by Knight and Chander (1994) to cope with article selection in Japanese to English translation, has been later addressed in various ways. On one side, rule-based methods (Rosa et al., 2012) gained limited attention, probably due to the extensive manual work they involve and their scarce portability across languages. On the other side, the statistical approach proposed by Allen and Hogan (2000) reached maturity in the work by Simard et al. (2007) and inspired a number of further investigations (Isabelle et al., 2007; Dugast et al., 2007; Dugast et al., 2009; Lagarda et al., 2009; B´echara et al., 2011; B´echara et al., 2012; Rubino et al., 2012; Rosa et al., 2013; Lagarda et al., 2014, inter alia). Such prior works address orthogonal aspects like: i) performance variations when APE is applied to correct the output of rule-based vs. statistical MT, ii) the use of APE for error correction vs. domain adaptation, iii) the difference between training on general domain vs. domain-specific</context>
</contexts>
<marker>Allen, Hogan, 2000</marker>
<rawString>Jeffrey Allen and Christopher Hogan. 2000. Toward the Development of a Post Editing Module for Raw Machine Translation Output: A Controlled Language Perspective. In Third International Controlled Language Applications Workshop (CLAW00), pages 62–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna B´echara</author>
<author>Yanjun Ma</author>
<author>Josef van Genabith</author>
</authors>
<title>Statistical Post-Editing for a Statistical MT System.</title>
<date>2011</date>
<booktitle>In Proceedings of the 13th Machine Translation Summit,</booktitle>
<pages>308--315</pages>
<location>Xiamen, China,</location>
<marker>B´echara, Ma, van Genabith, 2011</marker>
<rawString>Hanna B´echara, Yanjun Ma, and Josef van Genabith. 2011. Statistical Post-Editing for a Statistical MT System. In Proceedings of the 13th Machine Translation Summit, pages 308–315, Xiamen, China, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna B´echara</author>
<author>Yifan He Rapha¨el Rubino</author>
<author>Yanjun Ma</author>
<author>Josef Genabith</author>
</authors>
<title>An Evaluation of Statistical Post-Editing Systems Applied to RBMT and SMT Systems.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>215--230</pages>
<location>Mumbai, India.</location>
<marker>B´echara, Rapha¨el Rubino, Ma, Genabith, 2012</marker>
<rawString>Hanna B´echara, Rapha¨el Rubino, Yifan He, Yanjun Ma, and Josef Genabith. 2012. An Evaluation of Statistical Post-Editing Systems Applied to RBMT and SMT Systems. In Proceedings of COLING 2012, pages 215–230, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna B´echara</author>
</authors>
<title>Statistical Post-editing and Quality Estimation for Machine Translation Systems.</title>
<date>2014</date>
<tech>M.Sc. Thesis,</tech>
<institution>Dublin City University,</institution>
<location>Dublin.</location>
<marker>B´echara, 2014</marker>
<rawString>Hanna B´echara. 2014. Statistical Post-editing and Quality Estimation for Machine Translation Systems. M.Sc. Thesis, Dublin City University, Dublin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Loic Dugast</author>
<author>Jean Senellart</author>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Post-editing on SYSTRAN’s Rule-based Translation System.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07,</booktitle>
<pages>220--223</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2531" citStr="Dugast et al., 2007" startWordPosition="389" endWordPosition="392">r its internal decoding process is not accessible. The task, firstly proposed by Knight and Chander (1994) to cope with article selection in Japanese to English translation, has been later addressed in various ways. On one side, rule-based methods (Rosa et al., 2012) gained limited attention, probably due to the extensive manual work they involve and their scarce portability across languages. On the other side, the statistical approach proposed by Allen and Hogan (2000) reached maturity in the work by Simard et al. (2007) and inspired a number of further investigations (Isabelle et al., 2007; Dugast et al., 2007; Dugast et al., 2009; Lagarda et al., 2009; B´echara et al., 2011; B´echara et al., 2012; Rubino et al., 2012; Rosa et al., 2013; Lagarda et al., 2014, inter alia). Such prior works address orthogonal aspects like: i) performance variations when APE is applied to correct the output of rule-based vs. statistical MT, ii) the use of APE for error correction vs. domain adaptation, iii) the difference between training on general domain vs. domain-specific data, iv) performance variations when learning from reference translations vs. human post-edits. Their common trait is that the reported results</context>
</contexts>
<marker>Dugast, Senellart, Koehn, 2007</marker>
<rawString>Loic Dugast, Jean Senellart, and Philipp Koehn. 2007. Statistical Post-editing on SYSTRAN’s Rule-based Translation System. In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07, pages 220–223, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Loic Dugast</author>
<author>Jean Senellart</author>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Post Editing and Dictionary Extraction: Systran/Edinburgh Submissions for ACLWMT2009.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>110--114</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="2552" citStr="Dugast et al., 2009" startWordPosition="393" endWordPosition="396">ng process is not accessible. The task, firstly proposed by Knight and Chander (1994) to cope with article selection in Japanese to English translation, has been later addressed in various ways. On one side, rule-based methods (Rosa et al., 2012) gained limited attention, probably due to the extensive manual work they involve and their scarce portability across languages. On the other side, the statistical approach proposed by Allen and Hogan (2000) reached maturity in the work by Simard et al. (2007) and inspired a number of further investigations (Isabelle et al., 2007; Dugast et al., 2007; Dugast et al., 2009; Lagarda et al., 2009; B´echara et al., 2011; B´echara et al., 2012; Rubino et al., 2012; Rosa et al., 2013; Lagarda et al., 2014, inter alia). Such prior works address orthogonal aspects like: i) performance variations when APE is applied to correct the output of rule-based vs. statistical MT, ii) the use of APE for error correction vs. domain adaptation, iii) the difference between training on general domain vs. domain-specific data, iv) performance variations when learning from reference translations vs. human post-edits. Their common trait is that the reported results are difficult to gen</context>
</contexts>
<marker>Dugast, Senellart, Koehn, 2009</marker>
<rawString>Loic Dugast, Jean Senellart, and Philipp Koehn. 2009. Statistical Post Editing and Dictionary Extraction: Systran/Edinburgh Submissions for ACLWMT2009. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 110–114, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Gao</author>
<author>Stephan Vogel</author>
</authors>
<title>Parallel Implementations of Word Alignment Tool.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL 2008 Software Engineering, Testing, and Quality Assurance Workshop,</booktitle>
<pages>49--57</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="9595" citStr="Gao and Vogel, 2008" startWordPosition="1541" endWordPosition="1544"> methods To obtain the statistical APE pipeline that represents the backbone of both methods we used a phrase-based Moses system (Koehn et al., 2007). Our training data (see Section 3) consists of (source, MT output, post-edition) triplets for six language pairs having English as source. While Method 1 uses only the last two elements of the triplet, all of them play a role in the context-aware Method 2. Apart from the different data representation, the training process is identical. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment.4 For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. The APE system for each target language was tuned on comparable development sets (see below), optimizing TER with Minimum Error Rate Training (Och, 2003) using the post-edited sentences as references. 3 Experiments Some lessons learned from prior works on statistical APE methods (B´echara, 2014) include: i) learning from human post-edits is more effective than learning from (independent) reference translations, ii) learning from (and applying APE to) d</context>
</contexts>
<marker>Gao, Vogel, 2008</marker>
<rawString>Qin Gao and Stephan Vogel. 2008. Parallel Implementations of Word Alignment Tool. In Proceedings of the ACL 2008 Software Engineering, Testing, and Quality Assurance Workshop, pages 49–57, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: Faster and Smaller Language Model Queries.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>187--197</pages>
<location>Edinburgh, Scotland, United Kingdom.</location>
<contexts>
<context position="9681" citStr="Heafield, 2011" startWordPosition="1556" endWordPosition="1557">s we used a phrase-based Moses system (Koehn et al., 2007). Our training data (see Section 3) consists of (source, MT output, post-edition) triplets for six language pairs having English as source. While Method 1 uses only the last two elements of the triplet, all of them play a role in the context-aware Method 2. Apart from the different data representation, the training process is identical. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment.4 For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. The APE system for each target language was tuned on comparable development sets (see below), optimizing TER with Minimum Error Rate Training (Och, 2003) using the post-edited sentences as references. 3 Experiments Some lessons learned from prior works on statistical APE methods (B´echara, 2014) include: i) learning from human post-edits is more effective than learning from (independent) reference translations, ii) learning from (and applying APE to) domain-specific data is more promising than working on general-domain data, iii) correc</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: Faster and Smaller Language Model Queries. In Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation, pages 187–197, Edinburgh, Scotland, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Isabelle</author>
<author>Cyril Goutte</author>
<author>Michel Simard</author>
</authors>
<title>Domain Adaptation of MT Systems through Automatic Post-editing.</title>
<date>2007</date>
<booktitle>In Proceedings of the Eleventh Machine Translation Summit (MT Summit XI),</booktitle>
<pages>255--261</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="2510" citStr="Isabelle et al., 2007" startWordPosition="385" endWordPosition="388"> MT system is unknown or its internal decoding process is not accessible. The task, firstly proposed by Knight and Chander (1994) to cope with article selection in Japanese to English translation, has been later addressed in various ways. On one side, rule-based methods (Rosa et al., 2012) gained limited attention, probably due to the extensive manual work they involve and their scarce portability across languages. On the other side, the statistical approach proposed by Allen and Hogan (2000) reached maturity in the work by Simard et al. (2007) and inspired a number of further investigations (Isabelle et al., 2007; Dugast et al., 2007; Dugast et al., 2009; Lagarda et al., 2009; B´echara et al., 2011; B´echara et al., 2012; Rubino et al., 2012; Rosa et al., 2013; Lagarda et al., 2014, inter alia). Such prior works address orthogonal aspects like: i) performance variations when APE is applied to correct the output of rule-based vs. statistical MT, ii) the use of APE for error correction vs. domain adaptation, iii) the difference between training on general domain vs. domain-specific data, iv) performance variations when learning from reference translations vs. human post-edits. Their common trait is that</context>
</contexts>
<marker>Isabelle, Goutte, Simard, 2007</marker>
<rawString>Pierre Isabelle, Cyril Goutte, and Michel Simard. 2007. Domain Adaptation of MT Systems through Automatic Post-editing. In Proceedings of the Eleventh Machine Translation Summit (MT Summit XI), pages 255–261, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Ishwar Chander</author>
</authors>
<title>Automated Postediting of Documents.</title>
<date>1994</date>
<booktitle>In Proceedings of the 12th National Conference on Artificial Intelligence,</booktitle>
<pages>779--784</pages>
<location>Seattle, WA, USA.</location>
<contexts>
<context position="2018" citStr="Knight and Chander (1994)" startWordPosition="301" endWordPosition="305">atic post-editing (APE) aims to correct systematic machine translation (MT) errors. The problem is appealing for several reasons. On one side, as pointed out by Parton et al. (2012), APE systems can improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at decoding stage. On the other side, and to our view more importantly, APE represents the only way to recover errors produced in “black-box” conditions in which the MT system is unknown or its internal decoding process is not accessible. The task, firstly proposed by Knight and Chander (1994) to cope with article selection in Japanese to English translation, has been later addressed in various ways. On one side, rule-based methods (Rosa et al., 2012) gained limited attention, probably due to the extensive manual work they involve and their scarce portability across languages. On the other side, the statistical approach proposed by Allen and Hogan (2000) reached maturity in the work by Simard et al. (2007) and inspired a number of further investigations (Isabelle et al., 2007; Dugast et al., 2007; Dugast et al., 2009; Lagarda et al., 2009; B´echara et al., 2011; B´echara et al., 20</context>
</contexts>
<marker>Knight, Chander, 1994</marker>
<rawString>Kevin Knight and Ishwar Chander. 1994. Automated Postediting of Documents. In Proceedings of the 12th National Conference on Artificial Intelligence, pages 779–784, Seattle, WA, USA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="9124" citStr="Koehn et al., 2007" startWordPosition="1465" endWordPosition="1468">o replicate them3 for a fair comparison in a controlled evaluation setting involving different lan2Here, f&apos; and f respectively stand for the rough MT output and its correct version in the foreign language F. 3This is done based on the description provided by the published works. Discrepancies with the actual methods are possible, due to our misinterpretation or to wrong guesses about details that are missing in the papers. 157 guage pairs. 2.3 Reimplementing the two methods To obtain the statistical APE pipeline that represents the backbone of both methods we used a phrase-based Moses system (Koehn et al., 2007). Our training data (see Section 3) consists of (source, MT output, post-edition) triplets for six language pairs having English as source. While Method 1 uses only the last two elements of the triplet, all of them play a role in the context-aware Method 2. Apart from the different data representation, the training process is identical. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment.4 For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gra</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Significance Tests for Machine Translation Evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP 2004,</booktitle>
<pages>388--395</pages>
<location>Barcelona,</location>
<contexts>
<context position="14744" citStr="Koehn, 2004" startWordPosition="2368" endWordPosition="2369">ence, baseline scores for each language pair correspond to the TER computed between the original MT output (produced by the “black-box” Autodesk inhouse system) and the human post-edits. 4 Results Table 2 lists our results, with language pairs ordered according to the respective baseline TER. The positive answer to Q1 (“Does APE yield consistent improvements to MT output?”) is evident: both APE methods consistently improve MT quality on all language pairs. TER reductions range from 3.06 to 5.27 points. Quality improvements are statistically significant at p &lt; 0.05, measured by bootstrap test (Koehn, 2004). In answer to Q2 (“What is the relation between original MT quality and APE results?”), our controlled experiments evidence for the first time in APE research that the higher the MT quality, the higher is the improvement, i.e. percentage of error reduction, yielded by the APE methods. On one side, this interesting result may seem counterintuitive because a larger room for improvement 6Edit distance is calculated as the number of edits (word insertions, deletions, substitutions, and shifts) divided by the number of words in the correct translation. Lower TER/HTER values indicate better MT qual</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In Proceedings of EMNLP 2004, pages 388–395, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Tenth Machine Translation Summit (MT Summit X),</booktitle>
<pages>79--86</pages>
<location>Phuket, Thailand,</location>
<contexts>
<context position="12515" citStr="Koehn, 2005" startWordPosition="1992" endWordPosition="1993">ulting tri-parallel corpora. After random shuffling the triplets, we create training (12.2K triplets), development (2K) and test data (2K) sharing exactly the same source sentences across languages. Training and evaluation of our APE systems are performed on true-case data. To guarantee similar experimental conditions in the six language settings, we also train comparable target language models from external data (indeed, the 12.2K post-edits would not be enough to train reliable LMs). We build our LMs from approximately 2.5M translations of the same English sentences collected from Europarl (Koehn, 2005), DGT-Translation Memory (Steinberger et al., 2012), JRC Acquis (Steinberger et al., 2006), OPUS IT (Tiedemann, ) and other Autodesk data common to all languages. Evaluation metric. We evaluate the APE methods based on their capability to reduce the distance between the MT output and a correct (fluent and adequate) translation. As a measure of the amount of the editing operations needed for the correction, TER and HTER (Snover et al., 2006) fit for our purpose. TER and HTER measure the minimum edit distance between the MT output and its cor158 MT Baseline Method 1 Method 2 Oracle TER TER A % R</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Proceedings of the Tenth Machine Translation Summit (MT Summit X), pages 79–86, Phuket, Thailand, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio L Lagarda</author>
<author>Vicent Alabau</author>
<author>Francisco Casacuberta</author>
<author>Roberto Silva</author>
<author>Enrique D´ıaz-de Lia˜no</author>
</authors>
<title>Statistical Post-editing of a Rule-based Machine Translation System. In</title>
<date>2009</date>
<booktitle>Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>217--220</pages>
<marker>Lagarda, Alabau, Casacuberta, Silva, D´ıaz-de Lia˜no, 2009</marker>
<rawString>Antonio L. Lagarda, Vicent Alabau, Francisco Casacuberta, Roberto Silva, and Enrique D´ıaz-de Lia˜no. 2009. Statistical Post-editing of a Rule-based Machine Translation System. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 217–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio L Lagarda</author>
<author>Daniel Ortiz-Mart´ınez</author>
<author>Vicent Alabau</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Translating without in-domain Corpus: Machine Translation Post-editing with Online Learning Techniques.</title>
<date>2014</date>
<journal>Computer Speech &amp; Language.</journal>
<marker>Lagarda, Ortiz-Mart´ınez, Alabau, Casacuberta, 2014</marker>
<rawString>Antonio L. Lagarda, Daniel Ortiz-Mart´ınez, Vicent Alabau, and Francisco Casacuberta. 2014. Translating without in-domain Corpus: Machine Translation Post-editing with Online Learning Techniques. Computer Speech &amp; Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="9892" citStr="Och, 2003" startWordPosition="1591" endWordPosition="1592">s only the last two elements of the triplet, all of them play a role in the context-aware Method 2. Apart from the different data representation, the training process is identical. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment.4 For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. The APE system for each target language was tuned on comparable development sets (see below), optimizing TER with Minimum Error Rate Training (Och, 2003) using the post-edited sentences as references. 3 Experiments Some lessons learned from prior works on statistical APE methods (B´echara, 2014) include: i) learning from human post-edits is more effective than learning from (independent) reference translations, ii) learning from (and applying APE to) domain-specific data is more promising than working on general-domain data, iii) correcting the output of rule-based MT systems is easier than improving translations from statistical MT. Our work capitalizes on these findings (we learn from domain-specific post-edited data and apply APE to statist</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03, pages 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristen Parton</author>
<author>Nizar Habash</author>
<author>Kathleen McKeown</author>
<author>Gonzalo Iglesias</author>
<author>Adri`a de Gispert</author>
</authors>
<title>Can Automatic Post-Editing Make MT More Meaningful?</title>
<date>2012</date>
<booktitle>In Proceedings of the 16th Conference of the European Association for Machine Translation (EAMT),</booktitle>
<pages>111--118</pages>
<location>Trento, Italy.</location>
<marker>Parton, Habash, McKeown, Iglesias, de Gispert, 2012</marker>
<rawString>Kristen Parton, Nizar Habash, Kathleen McKeown, Gonzalo Iglesias, and Adri`a de Gispert. 2012. Can Automatic Post-Editing Make MT More Meaningful? In Proceedings of the 16th Conference of the European Association for Machine Translation (EAMT), pages 111–118, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rudolf Rosa</author>
<author>David Mareˇcek</author>
<author>Ondˇrej Duˇsek</author>
</authors>
<title>DEPFIX: A System for Automatic Correction of Czech MT Outputs.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation, WMT ’12,</booktitle>
<pages>362--368</pages>
<location>Montreal, Canada.</location>
<marker>Rosa, Mareˇcek, Duˇsek, 2012</marker>
<rawString>Rudolf Rosa, David Mareˇcek, and Ondˇrej Duˇsek. 2012. DEPFIX: A System for Automatic Correction of Czech MT Outputs. In Proceedings of the Seventh Workshop on Statistical Machine Translation, WMT ’12, pages 362–368, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rudolf Rosa</author>
<author>David Marecek</author>
<author>Ales Tamchyna</author>
</authors>
<title>Deepfix: Statistical Post-editing of Statistical Machine Translation Using Deep Syntactic Analysis.</title>
<date>2013</date>
<booktitle>In Proceedings of the ACL 2013 Student Research Workshop,</booktitle>
<pages>172--179</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="2660" citStr="Rosa et al., 2013" startWordPosition="413" endWordPosition="416">ection in Japanese to English translation, has been later addressed in various ways. On one side, rule-based methods (Rosa et al., 2012) gained limited attention, probably due to the extensive manual work they involve and their scarce portability across languages. On the other side, the statistical approach proposed by Allen and Hogan (2000) reached maturity in the work by Simard et al. (2007) and inspired a number of further investigations (Isabelle et al., 2007; Dugast et al., 2007; Dugast et al., 2009; Lagarda et al., 2009; B´echara et al., 2011; B´echara et al., 2012; Rubino et al., 2012; Rosa et al., 2013; Lagarda et al., 2014, inter alia). Such prior works address orthogonal aspects like: i) performance variations when APE is applied to correct the output of rule-based vs. statistical MT, ii) the use of APE for error correction vs. domain adaptation, iii) the difference between training on general domain vs. domain-specific data, iv) performance variations when learning from reference translations vs. human post-edits. Their common trait is that the reported results are difficult to generalise. Indeed, most of the works focus on evaluating a specific method,1 which is typically applied to one</context>
</contexts>
<marker>Rosa, Marecek, Tamchyna, 2013</marker>
<rawString>Rudolf Rosa, David Marecek, and Ales Tamchyna. 2013. Deepfix: Statistical Post-editing of Statistical Machine Translation Using Deep Syntactic Analysis. In Proceedings of the ACL 2013 Student Research Workshop, pages 172–179, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>St´ephane Huet Rapha¨el Rubino</author>
<author>Fabrice Lef`evre</author>
<author>Georges Lenar´es</author>
</authors>
<title>Statistical Post-Editing of Machine Translation for Domain Adaptation.</title>
<date>2012</date>
<booktitle>In Proceedings of the European Association for Machine Translation (EAMT),</booktitle>
<pages>221--228</pages>
<location>Trento, Italy.</location>
<marker>Rapha¨el Rubino, Lef`evre, Lenar´es, 2012</marker>
<rawString>Rapha¨el Rubino, St´ephane Huet, Fabrice Lef`evre, and Georges Lenar´es. 2012. Statistical Post-Editing of Machine Translation for Domain Adaptation. In Proceedings of the European Association for Machine Translation (EAMT), pages 221–228, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Simard</author>
<author>Cyril Goutte</author>
<author>Pierre Isabelle</author>
</authors>
<title>Statistical Phrase-Based Post-Editing.</title>
<date>2007</date>
<booktitle>In Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT),</booktitle>
<pages>508--515</pages>
<location>Rochester, New York.</location>
<contexts>
<context position="2439" citStr="Simard et al. (2007)" startWordPosition="373" endWordPosition="376"> way to recover errors produced in “black-box” conditions in which the MT system is unknown or its internal decoding process is not accessible. The task, firstly proposed by Knight and Chander (1994) to cope with article selection in Japanese to English translation, has been later addressed in various ways. On one side, rule-based methods (Rosa et al., 2012) gained limited attention, probably due to the extensive manual work they involve and their scarce portability across languages. On the other side, the statistical approach proposed by Allen and Hogan (2000) reached maturity in the work by Simard et al. (2007) and inspired a number of further investigations (Isabelle et al., 2007; Dugast et al., 2007; Dugast et al., 2009; Lagarda et al., 2009; B´echara et al., 2011; B´echara et al., 2012; Rubino et al., 2012; Rosa et al., 2013; Lagarda et al., 2014, inter alia). Such prior works address orthogonal aspects like: i) performance variations when APE is applied to correct the output of rule-based vs. statistical MT, ii) the use of APE for error correction vs. domain adaptation, iii) the difference between training on general domain vs. domain-specific data, iv) performance variations when learning from </context>
<context position="3807" citStr="Simard et al., 2007" startWordPosition="601" endWordPosition="604">focus on evaluating a specific method,1 which is typically applied to one single dataset for a given language pair. As a result, the global landscape of the “planet of the APEs” is still blurred and open to more systematic explorations. To shed light on the potential of statistical postediting, in this paper we examine two alternative approaches. One is the method proposed in (Simard et al., 2007), which to date is the most widely used. The other is the “context-aware” solution proposed in (B´echara et al., 2011) which, to the best of our knowledge, represents the most significant variant of (Simard et al., 2007). The major contribution of our work is the first systematic analysis of different APE approaches, 1Typically the same of (Simard et al., 2007). 156 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 156–161, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics which are tested in controlled conditions over several language pairs. To ensure the soundness of the analysis, our experimental setup consists of a dataset composed of the sa</context>
<context position="5053" citStr="Simard et al. (2007)" startWordPosition="789" endWordPosition="792">s with automatic translations into six languages and respective manual post-edits by professional translators. Overall, this represents the ideal condition to complement prior research with the missing answers to questions like: Q1: Does APE yield consistent MT quality improvements across different language pairs? Q2: What is the relation between the original MT output quality and the APE results? Q3: Which of the two analysed APE methods has the highest potential? 2 Statistical APE methods The two methods we analyse follow the same “statistical phrase-based post-editing” strategy outlined by Simard et al. (2007), but differ in the way data is represented. Let’s give them a closer look. 2.1 Method 1(Simard et al., 2007) The underlying idea is that APE components can be trained in the same way in which statistical MT systems are developed – i.e. starting from “parallel data”. Since the goal is to transform rough MT output into its correct version, parallel data consists of MT output as source texts and correct (human quality) sentences as target. In (Simard et al., 2007) these are used to train a phrase-based MT system, which is then applied to correct the output of a commercial rule-based MT system. P</context>
<context position="8223" citStr="Simard et al. (2007)" startWordPosition="1316" endWordPosition="1319">nced by word alignment errors. To cope with data sparseness and unreliable word alignment, B´echara et al. (2011) experiment with different thresholds set on word alignment strengths to filter context information. In particular, they discard the (f0#e, f) pairs in which the f0#e alignment score is smaller than the threshold. The approach, applied to correct the output of a statistical phrase-based MT system, achieves ambiguous evaluation results. On French-English, significant improvements up to 2 BLEU points are observed both over the baseline (the original MT output) and the basic method of Simard et al. (2007). On English-French, however, performance slightly drops. Moreover, follow-up experiments with the same method (B´echara, 2014) did not confirm these results. In light of these ambiguous results and the lack of a systematic comparison between the two APE methods, our objective is to replicate them3 for a fair comparison in a controlled evaluation setting involving different lan2Here, f&apos; and f respectively stand for the rough MT output and its correct version in the foreign language F. 3This is done based on the description provided by the published works. Discrepancies with the actual methods </context>
</contexts>
<marker>Simard, Goutte, Isabelle, 2007</marker>
<rawString>Michel Simard, Cyril Goutte, and Pierre Isabelle. 2007. Statistical Phrase-Based Post-Editing. In Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT), pages 508–515, Rochester, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings ofAssociation forMachine Translation in the Americas,</booktitle>
<pages>223--231</pages>
<location>Cambridge, Massachusetts, USA.</location>
<contexts>
<context position="12959" citStr="Snover et al., 2006" startWordPosition="2062" endWordPosition="2065">post-edits would not be enough to train reliable LMs). We build our LMs from approximately 2.5M translations of the same English sentences collected from Europarl (Koehn, 2005), DGT-Translation Memory (Steinberger et al., 2012), JRC Acquis (Steinberger et al., 2006), OPUS IT (Tiedemann, ) and other Autodesk data common to all languages. Evaluation metric. We evaluate the APE methods based on their capability to reduce the distance between the MT output and a correct (fluent and adequate) translation. As a measure of the amount of the editing operations needed for the correction, TER and HTER (Snover et al., 2006) fit for our purpose. TER and HTER measure the minimum edit distance between the MT output and its cor158 MT Baseline Method 1 Method 2 Oracle TER TER A % Reduction TER A % Reduction TER En-De 46.46 43.07 -3.39 7.3 42.79* -3.67 7.9 40.17 En-Cs 44.06 39.38 -4.68 10.62 39.10* -4.96 11.25 36.32 En-Pl 43.02 38.24 -4.78 11.11 37.75* -5.27 12.25 35.05 En-It 34.44 30.43 -4.01 11.64 30.13* -4.31 12.55 28.33 En-Fr 32.76 29.70 -3.06 9.34 29.51 -3.25 9.92 27.12 En-Es 30.90 26.69 -4.21 13.62 26.35* -4.55 14.72 24.34 Table 2: Performance of the MT baseline and the APE methods for each language pair. Result</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proceedings ofAssociation forMachine Translation in the Americas, pages 223–231, Cambridge, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Steinberger</author>
<author>Bruno Pouliquen</author>
</authors>
<title>Anna Widiger, Camelia Ignat, Tomaz Erjavec, Dan Tufis, and Dniel Varga.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC’2006),</booktitle>
<location>Genoa, Italy.</location>
<marker>Steinberger, Pouliquen, 2006</marker>
<rawString>Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaz Erjavec, Dan Tufis, and Dniel Varga. 2006. The JRC-Acquis: A Multilingual Aligned Parallel Corpus with 20+ Languages. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC’2006), Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Steinberger</author>
<author>Andreas Eisele</author>
<author>Szymon Klocek</author>
<author>Spyridon Pilos</author>
<author>Patrick Schl¨uter</author>
</authors>
<title>DGTTM: A Freely Available Translation Memory in 22 Languages.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<location>Istanbul, Turkey.</location>
<marker>Steinberger, Eisele, Klocek, Pilos, Schl¨uter, 2012</marker>
<rawString>Ralf Steinberger, Andreas Eisele, Szymon Klocek, Spyridon Pilos, and Patrick Schl¨uter. 2012. DGTTM: A Freely Available Translation Memory in 22 Languages. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), Istanbul, Turkey.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>Parallel Data, Tools and Interfaces in OPUS.</title>
<booktitle>In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<pages>2214--2218</pages>
<location>Istanbul, Turkey.</location>
<marker>Tiedemann, </marker>
<rawString>J¨org Tiedemann. Parallel Data, Tools and Interfaces in OPUS. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), pages 2214–2218, Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ventsislav Zhechev</author>
</authors>
<title>Machine Translation Infrastructure and Post-Editing Performance at Autodesk.</title>
<date>2012</date>
<booktitle>In AMTA 2012 Workshop on Post-Editing Technology and Practice (WPTP 2012),</booktitle>
<pages>87--96</pages>
<location>San Diego, CA, USA.</location>
<contexts>
<context position="11508" citStr="Zhechev, 2012" startWordPosition="1839" endWordPosition="1840">ntly covers the domain of software user manuals. English 4In Method 1, MGIZA++ is used to align f&apos; and f. In Method 2 it is used to align f&apos; and e, and then f&apos;#e and f. 5https://autodesk.app.box.com/ Autodesk-PostEditing Lang. No. Vocab. No. tokens Size Lemmas En 210,491 10,727 8,260 Cs 202,475 16,716 10,137 De 211,149 17,563 14,368 Es 252,020 11,075 6,683 Fr 263,690 10,928 7,213 It 239,912 10,703 6,549 Pl 206,016 17,027 10,430 Table 1: Data statistics for each language. sentences are translated into several languages (30K to 410K translations per language) with Autodesk’s in-house MT system (Zhechev, 2012) and post-edited by professional translators. Our experiments are run on six language pairs having English as source and Czech, German, Spanish, French, Italian and Polish as target. To set up our controlled environment, we extract all the (source, MT output, post-edition) triplets sharing the same source (En) sentences across all language pairs. Table 1 provides some statistics about the resulting tri-parallel corpora. After random shuffling the triplets, we create training (12.2K triplets), development (2K) and test data (2K) sharing exactly the same source sentences across languages. Traini</context>
</contexts>
<marker>Zhechev, 2012</marker>
<rawString>Ventsislav Zhechev. 2012. Machine Translation Infrastructure and Post-Editing Performance at Autodesk. In AMTA 2012 Workshop on Post-Editing Technology and Practice (WPTP 2012), pages 87– 96, San Diego, CA, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>