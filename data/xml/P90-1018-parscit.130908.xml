<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.9116848">
A SYNTACTIC FILTER ON PRONOMINAL ANAPHORA FOR SLOT GRAMMAR
Shalom Lappin and Michael McCord
IBM Ti. Watson Research Center
P.O. Box 704
Yorktown Heights, NY 10598
</note>
<email confidence="0.998889">
E-mail: Lappin/McCord@yktvmh.hitnet
</email>
<sectionHeader confidence="0.995671" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999833846153846">
We propose a syntactic filter for identifying
non-coreferential pronoun-NP pairs within a
sentence. The filter applies to the output of a
Slot Grammar parser and is formulated in terms
of the head-argument structures which the parser
generates. It handles control and unbounded de-
pendency constructions without empty categories
or binding chains, by virtue of the unificational
nature of the parser. The filter provides con-
straints for a discourse semantics system, reducing
the search domain to which the inference rules
of the system&apos;s anaphora resolution component
apply.
</bodyText>
<sectionHeader confidence="0.997113" genericHeader="keywords">
I. INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999783576923077">
In this paper we present an implemented al-
gorithm which filters intra-sentential relations of
referential dependence between pronouns and
putative NP antecedents (both full and pronomi-
nal NP&apos;s) for the syntactic representations pro-
vided by an English Slot Grammar parser
(McCord 1989b). or each parse of a sentence,
the algorithm provides a list of pronoun-NI&apos; pairs
where referential dependence of the fir St element
on the second is excluded by syntactic con-
straints. The coverage of the filter has roughly
the same extension as conditions B and C of
Chomsky&apos;s (1981, 1986) binding theory. How-
ever, the formulation of the algorithm is signif-
icantly different from the conditions of the
binding theory, and from proposed implementa-
tions of its conditions. In particular, the fitter
formulates constraints on pronominal anaphora
in terms of the head-argument structures provided
by Slot Grammar syntactic representations rather
than the configurational tree relations, partic-
ularly c-command, on which the binding theory
relies. As .a. result, the statements of the algorithm
apply straightforwardly, and without special pro-
vision, to a wide variety of constructions which
recently proposed implementations of the binding
theory do not handle without additional devices.
Like the Slot Grammar whose input it applies to,
the algorithm runs in Prolog, and it is stated in
essentially declarative terms.
In Section 2 we give a brief description of Slot
Grammar, and the parser we are employing. The
syntactic filter is presented in Section 3, first
through a statement of six constraints, each of
which is sufficient to rule out coreferencc, then
through a detailed description of the algorithm
which implements these constraints. We illus-
trate the algorithm with examples of the lists of
non-coreferential pairs which it provides for par-
ticular parses. In Section 4 we compare our ap-
proach to other proposals for syntactic filtering
of pronominal anaphora which have appeared in
the literature. We discuss Itobbs&apos; algorithm, and
we take up two recent implementations of the
binding theory. Finally, in Section 5 we discuss
filter the integration of our ter into other systems of
anaphora resolution. We indicate how it can be
combined with a VP anaphora algorithm which
we have recently completed. We also outline the
incorporation of our algorithm into LODUS
(Bemth 1989), a system for discourse represen-
tation.
</bodyText>
<sectionHeader confidence="0.980261" genericHeader="introduction">
2. SLOT GRAMMAR
</sectionHeader>
<bodyText confidence="0.999126263157895">
The original work on Slot Grammar was done
around 1976-78 and appeared in (McCord 1980).
Recently, a new version (McCord 1989b) was
developed in a logic programming framework, in
connection with the machine translation system
LMT (McCord 1989a,c,d).
Slot Grammar is lexicalist and is dependen-
cy-oriented. Every phrase has a head word (with
a given word sense and morphosyntactic fea-
tures). The constituents of a phrase besides the
head word (also called the modifiers of the head)
are obtained by &amp;quot;filling&apos; siois associated with the
head. Slots are symbols like sub j, oh _I and i oh j
representing grammatical relations, and are asso-
ciated with a word (sense) in two ways. The
lexical entry for the word specifies a set of corn-
pkmeni slots (corresponding to arguments of the
word sense in logical form); and the grammar
specifies a set of adjunct slots for each part of
</bodyText>
<page confidence="0.997994">
135
</page>
<bodyText confidence="0.936291591304348">
speech. A complement slot can be filled at most
once, and an adjunct slot can by default be filled
any number of times.
The phenomena treated by augmented phrase
structure rules in some grammatical systems are
treated modularly by several different types of
rules in Slot Grammar. The most important type
of rule is the (slot) filler rule, which gives condi-
tions (expressed largely thmugh unification) on
the filler phrase and its relations to the higher
phrase.
Filler rules arc stated (normally) without ref-
erence to conditions on order among. constitu-
ents. But there are separately stated ordering
rules.&apos; Slot/head ordering rules state conditions
on the position (left or right) of the slot (filler)
relative to the head word. Slot/slot ordering rules
place conditions on the relative left-to-right order
of (the fillers of) two slots.
A slot is obligatory (not optional) if it must
be filled, either in the current phrase or in a raised
position through left movement or coordination.
Adjunct slots are always optional. Complement
slots are optional by default, but they may be
specified to be obligatory in a particular lexical
entry, or they may be so specified in the grammar
by obligatory slot rules. Such rules may be un-
conditional or be conditional on the character-
istics of the higher phrase. They also may specify
that a slot is obligatory relative to the filling of
another slot. For example, the direct object slot
in English may be declared obligatory on the
condition that the indirect object slot is filled by
a noun phrase.
One aim of Slot Grammar is to develop a
powerful language-independent module, a
&apos;shell&amp;quot; which can be used together with lan-
guage-dependent modules, reducing the effort of
writing grammars for new languages. The Slot
Grammar shell module includes the parser, which
is a bottom-up chart parser. It also includes most
of the treatment of coordination, unbounded de-
pendencies, controlled subjects, and punctuation.
And the shell contains a system for evaluating
parses, extending IIeidorn&apos;s (1982) parse metric,
which is used not only for ranking final parses but
also for pruning away unlikely partial analyses
during parsing, thus reducing the problem of
parse space explosion. Parse evaluation expresses
preferences for close attachment, for choice of
complements over adjuncts, and for parallelism
in coordination.
Although the shell contains most of the treat-
ment of the above phenomena (coordination,
etc.), a small part of their treatment is necessarily
language-dependent. A (language-specific) gram-
mar can include for instance (1) rules for coordi-
nating feature structures that override the defaults
in the shell; (2) declarations of slots (called ex-
traposer slots) that allow left extraposition of
other slots out of their fillers; (3) language-specific
rules for punctuation that override defaults; and
(4) language-specific controls over parse evalu-
ation that override defaults.
Currently, Slot Grammars are being devel-
oped for English (ESC) by McCord, for Danish
(DSG) by Arendse Bemth, and for German
(GSG) by Ulrike Schwan. ESG uses the UDICT
lexicon (Byrd 1983, Klavans and Wacholder
1989) having over 60,000 lemmas, with an inter-
face that produces slot frames. The filter algo-
rithm has so far been successfully tested with
ESG and GSG. (The adaptation to German was
done by Ulrike Schwall.)
The algorithm applies in a second pass to the
parse output, so the important thing in the re-
mainder of this section is to describe Slot Gram-
mar syntactic analysis structures.
A syntactic structure is a tree; each node of
the tree represents a phrase in the sentence and
has a unique head word. Formally, a phrase is
represented by a term
phrase(X,H,Sense,Features,
SlotFrame,Ext,Mods),
where the components are as follows: (1) )( is a
logical variable called the marker of the phrase.
Unifications of the marker play a crucial role in
the filter algorithm. (2) H is an integer repres-
enting the position of the head word of the
phrase. This integer identifies the phrase
uniquely, and is used in the filter algorithm as the
way of referring to phrases. (3) Sense is the
word sense of the head word. (4) Features is
the feature structure of the head word and of the
phrase, It is a logic term (not an attribute-value
list), which is generally rather sparse in informa-
tion, showing mainly the part of speech and in-
flectional features of the head word. (5)
S lot Fr ame is the list of complement slots, each
slot being in the internal form
s lot (5 lot, Oh , X ), where S lot is the slot name,
Oh shows whether it is an obligatory form of
51ot, and X is the slot marker. The slot marker
is unified (essentially) with the marker of the filler
phrase when the slot is filled, even remotely, as
m left movement or coordination. Such unifica-
tions are important for the filter algorithm. (6)
Ext is the list of slots that have been extraposed
or raised to the level of the current phrase. (7)
The last component Mods represents the modifi-
ers (daughters) of the phrase, and is of the form
mods (LMods , Mods ) where LMods and RMods are
The distinction between slot filler rules and ordering constraints parallels the difference between Immediate Do-
minance Rules and Linear Precedence Rules in G PSG. See Gazdar et al (1985) for a characterization of II) and
I.P rules in GPSG. See (McCord 1989h) for more discussion of the relation of Slot Grammar to other systems.
</bodyText>
<page confidence="0.989998">
136
</page>
<figure confidence="0.791135590909091">
Who did John say wanted to try to find him?
subj(n)
top
subj(n)
auxcmp(inf(bare))
obj(fin)
preinf
comp(enlinfling)
[
i preinf
obj(inf)
obj(fin)
who (12)
dol(X1,X3,X4)
John(M)
say(X4,13,X9,u)
want(X9,12,X2,X12)
preinf(X12)
try(X12,X2,X13)
preinf(X13)
find(113,X2,114,u,
he((14)
</figure>
<bodyText confidence="0.907500875">
noun
verb
noun
verb
verb
preinf
verb
preinf
</bodyText>
<figure confidence="0.991819">
u) verb
noun
1
1
1
</figure>
<figureCaption confidence="0.999806">
Figure 1.
</figureCaption>
<bodyText confidence="0.981850861111111">
the lists of left modifiers and right modifiers, re-
spectively. Each member of a modifier list is of
the form 5 lot :Phrase where Slot is a slot and
Phrase is a phrase which fills Slot. Modifier
lists reflect surface order, and a given slot may
appear more than once (if it is an adjunct). Thus
modifier lists are not attribute-value lists.
In Figure I, a sample parse tree is shown,
displayed by a procedure that uses only one line
per node and exhibits tree structure lines on the
left. In this display, each line (representing a
node) shows (1) the tree connection lines, (2) the
slot filled by the node, (3) the word sense predi-
cation, and (4) the feature structure. The feature
structure is abbreviated here by a display option,
showing only the part of speech. The word sense
predication consists of the sense name of the head
word with the following arguments. The first ar-
gument is the marker variable for the phrase
(node) itself; it is like an event or state variable for
verbs. The remaining arguments are the marker
variables of the slots in the complement slot
frame (u signifies &amp;quot;unbound&amp;quot;). As can be seen in
the display, the complement arguments are um-
fled with the marker variables of the filler com-
plement phrases. Note that in the example the
marker X2 of the &apos;who&apos; phrase is unified with the
subject variables of &apos;want&apos;, &apos;try&apos;, and &apos;find&apos;.
(There are also some unifications created by ad-
junct slot filling, which will not be described
here.)
For the. operation of the filter algorithm, there
is a preliminary step in which pertinent informa-
tion about the parse tree is represented in a man-
ner more convenient for the algorithm. As
indicated above, nodes (phrases) themselves are
represented by the word numbers of their head
words. Properties of phrases and relations be-
tween them are represented by unit clauses
(predications) involving these integers (and other
data), which are asserted into the Prolog work-
space. Because of this &amp;quot;dispersed&amp;quot; representation
with a collection of unit clauses, the original
phrase structure for the whole tree is first
grounded (variables are bound to unique con-
stants) before the unit clauses are created.
As an example for this clausal representation,
the clause has a r g (P , X) says that phrase P has X
one of its arguments; i.e., X is the slot marker
variable for one of the complement slots of P.
For the above sample parse, then, we would get
clauses
hasarg(5,&apos;X2&apos;). hasarg(5,&apos;X12&apos;).
as information about the &apos;want&apos; node (5).
As another example, the clause
phmarker(P.,,X) is added when phrase P has
marker X. hus for the above sample, we would
get the unit clause
phmarker(1,1X2&apos;).
An important predicate for the filter algorithm
is argm, defined by
argm(P,Q) phmarker(P,X) &amp;
hasarg(Q,X).
This says that phrase P is an argument of phrase
Q. This includes remote arguments and con-
trolled subjects, because of the unifications of
marker variables performed by the Slot Grammar
parser. Thus for the above parse, we would get
argm(1,5). argm(1,7). argm(1,9).
showing that &apos;who&apos; is an argument of &apos;want&apos;, &apos;try&apos;,
and&apos;
find&apos;.
</bodyText>
<sectionHeader confidence="0.99279" genericHeader="method">
3. THE FILTER
</sectionHeader>
<page confidence="0.987828">
137
</page>
<table confidence="0.949058095238095">
The Filter Algorithm
A. nonrefdep(P,Q) 4- refpair(P,Q) &amp; ncorefpair(P,Q).
A.1. refpair(P,Q) 4- pron(P) &amp; noun(Q) &amp; P=/Q.
B. ncorefpair(P,Q) 4- nonagr(P,Q) &amp;/.
B.1. nonagr(P,Q) A- numdif(P,Q) 1 typedif(P,Q) 1 persdif(P,Q).
C. ncorefpair(P,Q) 4- proncom(P,Q) &amp;/.
C.1. proncom(P,Q) 4-
a. argrri(P ,H) &amp;
b. (argm(Q,H) &amp;/ 1
c. -pron(Q) &amp;
d. cont(Q,H) &amp;
e. (-subc1cont(Q,T) 1 gt(Q,P)) Sc
f. (-det(Q) 1 gt(Q,P))).
C.2. cont_i(P,Q) 4- argm(P,Q) 1 adjunct(P,Q).
C.2.1. cont(P,Q) A- cont_i(P,Q).
C.2.2. cont(P,Q) 4- cont_i(P,R) &amp; R=/Q &amp; cont(R,Q).
C.3. subc1cont(P,Q) 4- subconj(Q) &amp; cont(P,Q).
D. ncorefpair(P,Q) 4- prepcom(Q,P) Sc!.
D.1. prepcom(Q,P) 4- argm(Q,H) &amp; adjunct(R,H) &amp; prep(R) &amp; argm(P,R).
E. ncorefpair(P,Q) 4- npcom(P,Q) Sc!.
E.1. npcom(Q,P) 4- adjunct(Q,H) &amp; noun(H) &amp;
</table>
<figure confidence="0.856813">
(argm(P,H) 1
adjunct(R,H) &amp; prep(R) &amp; argm(P,R)).
F. ncorefpair(P,Q) A- nppcom(P,Q) &amp;/.
F.1 nppcom(P,Q) 4- adjunct(P,H) &amp; noun(H) &amp;
-pron(Q) &amp; cont(Q,H).
</figure>
<figureCaption confidence="0.99859">
Figure 2.
</figureCaption>
<bodyText confidence="0.9993163125">
In preparation for stating the six constraints,
we adopt the following definitions. The agree-
ment features of an NP are its number, person
and gender features. We will say that a phrase P
is in the argument domain of a phrase N if P an
N arc both arguments of the same head. We will
also say that P is in the adjunct domain of N iff
N is an argument of a head 11, P is the object of
a preposition PREP, and PREP is an adjunct of
II. P is in the NP domain of N ill N is the det-
erminer of a noun Q and (i) P is an argument of
Q, or (ii) P is the object of a preposition PREP
and Prep is an adjunct of Q. The six constraints
are as follows. A pronoun P is not coreferential
with a noun phrase N if any of the following
conditions holds.
</bodyText>
<listItem confidence="0.862452375">
I. P and N have incompatible agreement features.
II. P is in the argument domain of N.
LH. P is in the adjunct domain of N.
IV. P is an argument of a head II, N is not a
pronoun, and N is contained in H.
V. P is in the NP domain of N.
VI. P is the determiner of a noun Q, and N is
contained in Q.
</listItem>
<bodyText confidence="0.637937333333333">
The algorithm which implements 1-VI defines a
predicate nonrefdep(P op which is satisfied by
a pair whose first element is a pronoun and whose
</bodyText>
<figureCaption confidence="0.9017882">
second element is an NP on which the pronoun
cannot be taken as referentially dependent, by
virtue of the syntactic relation between them.
The main clauses of the algorithm are shown in
Figure 2.
</figureCaption>
<bodyText confidence="0.98817015">
Rule A specifies that the main goal
noarefdep(p,Q) is satisfied by &lt;P ,Q&gt; if this pair
is a referential pair (refpair(P,Q)) and a non-
coreferential pair (ncorefpair(P,Q)). A.1 de-
fines a refpair &lt;P ,Q&gt; as one in which P is a
pronoun, Q is a noun (either pronominal or non-
pronominal), and P and Q are distinct. Rules B,
C, D, E, and F provide a disjunctive statement
of the conditions under which the non-corefer-
ence goal ncorefpair (P ,Q) is satisfied, and so
constitute the core of the algorithm. Each of
these rules concludes with a cut to prevent un-
necessary backtracking which could generate
looping.
Rule B, together with B.1, identifies the con-
ditions under which constraint I holds. In the
following example sentences, the pairs consisting
of the second and the first coindexeci expressions
in la-c (and in lc also the pair &lt; &apos;I&apos;,&apos;she&apos; &gt; ) sat-
isfy non ref de p(P ,Q ) by virtue of rule B.
</bodyText>
<note confidence="0.456279">
la. John, said that they; came.
</note>
<page confidence="0.985399">
138
</page>
<bodyText confidence="0.863341588235294">
b. The womani said that he, is funny.
c. believe that she, is competent.
The algorithm identifies &lt; &gt; as a
rionref deo pair in la, which entails that &apos;they&apos;
cannot be taken as coreferential with &apos;John&apos;.
However, (the referent of) &apos;John&apos; could of course
be part of the reference set of &apos;they&apos;, and in suit-
able discourses LODUS could identify this possi-
bility.
Rule C states that &lt;13 ,Q&gt; is a non-coreferential
pair if it satisfies the pr on con( P ,Q) predicate.
This holds under two conditions, corresponding
to disjuncts C.1.a-b and C.1.a,c-f. The first con-
dition specifies that the pronoun P and its puta-
tive antecedent Q are both arguments of the same
phrasal head, and so implements constraint II.
This rules out referential dependence in 2a-b.
</bodyText>
<figureCaption confidence="0.592701">
2a. Mary,. likes her,.
b. She, likes buil.
</figureCaption>
<bodyText confidence="0.6129502">
Given the fact that Slot Grammar unifies the ar-
gument and adjunct variables of a head with the
phrases which fin these variable positions, it will
also exclude coreference in cases of control and
unbounded dependency, as in 3a-c.
</bodyText>
<listItem confidence="0.817779">
3a. John, seems to want to see himi.
b. Which man, did he see?
c. This is the girli John said shei saw.
</listItem>
<bodyText confidence="0.992968416666667">
The second disjunct C.1.a,c-f covers cases in
which the pronoun is an argument which is
higher up in the head-argument structure of the
sentence than a non-pronominal noun. This dis-
junct corresponds to condition IV. C.2-C.2.2
provide a recursive definition of containment
within a phrase. `Ms definition uses the relation
of immediate containment, cont _i (13 ,Q), as the
base of the recursion, where coni ( P ,Q) holds
if Q is either an argument or an adjunct (modifier
or determiner) of a head Q. The second disjunct
blocks coreference in 4a-c.
</bodyText>
<listItem confidence="0.9012">
4a. He. believes that the marl, is amusing.
b. Who, did he, say Joh; kissed?
c. This is the man, he, said Johni
wrote about.
</listItem>
<bodyText confidence="0.932155">
The wh-phrase in 4b and the head noun of the
relative clause in 4c unify with variables in posi-
tions contained within the phrase (more precisely,
the verb which heads the phrase) of which the
pronoun is an argument. Therefore, the algo-
rithm identifies these nouns as impossible ante-
cedents of the pronoun.
The two final conditions of the second dis-
junct, C. 1.e and C. 1.f, describe cases in which the
antecedent of a pronoun is contained in a pre-
ceding adjunct clause, and cases in which the an-
tecedent is the determiner of an NP which
precedes a pronoun, respectively. These clauses
prevent such structures from satisfying the non-
coreference goal, and so permit referential de-
pendence in 5a-b.
</bodyText>
<listItem confidence="0.857599142857143">
5a. After John, sang, he, danced.
b. Johnl&apos;s mother likes him„.
Notice that because a determiner is an adjunct of
an NP and not an argument of the verb of which
the NP is an argument, rule C.1 also permits co-
reference in 6.
6. His, mother likes John,.
</listItem>
<bodyText confidence="0.914459333333333">
However, C.1.a,c-e correctly excludes referential
dependence in 7, where the pronoun is an argu-
ment which is higher than a noun adjunct.
</bodyText>
<listItem confidence="0.724367">
7. He, likes Johni&apos;s mother.
</listItem>
<bodyText confidence="0.98619025">
The algorithm permits backwards anaphora in
cases like 8, where the pronoun is not an argu-
ment of a phrase H to which its antecedent Q bears
the con t (Q,H) relation.
</bodyText>
<listItem confidence="0.988793888888889">
8. After he, sang, John danced.
D-D.1 block coreference between an NP
which is the argument of a head 11, and a pronoun
that is the object of a preposition heading a PP
adjunct of H, as in 9a-c. These rules implement
constraint III.
9a. Sam, spoke about him,.
b. Shei sat near her,.
c. Who, did he, ask for?
</listItem>
<bodyText confidence="0.95297">
Finally, E-E.1 and F realize conditions V and
VI, respectively, in NP internal non-coreference
cases like 10a-c.
</bodyText>
<listItem confidence="0.80935925">
10a. His, portrait of Johrij is interesting.
b. John&apos;s portrait of lumi is interesting.
c. his1 description of the portrait by John,
is interesting.
</listItem>
<bodyText confidence="0.963875727272727">
Let us look at three examples of actual lists
of pairs satisfying the non ref dep predicate which
the algorithm generates for particular parse trees
of Slot Granunar. The items in each pair are
identified by their words and word numbers, cor-
responding to their sequential position in the
string.
When the sentence Who did John say
wanted to try to find him? is given to
the system, the parse is as shown in I•igure 1
above, and the output of the filter is:
</bodyText>
<footnote confidence="0.488175">
Noncoref pairs:
he.10 - who.1
</footnote>
<page confidence="0.998194">
139
</page>
<bodyText confidence="0.709283285714286">
complement clause subject. However, in Figure
4, the infinitival clause is an adjunct of &apos;lectured&apos;
and requires matrix subject control.
Coreference analysis time = 11 msec.
Thus &lt; &apos;him&apos;,&apos;who&apos; &gt; is identified as a non-core-
ferential pair, while coreference between &apos;John&apos;
and &apos;him is allowed.
</bodyText>
<figure confidence="0.983141777777778">
subj(n)
top
obj
preinf
comp(inf)
obj
John(X3)
expect(X1,X3,X4,X5)
Bill (X4)
preinf(X5)
impress(X5,X4,X6)
he (X6)
noun
verb
noun
preinf
verb
noun
</figure>
<bodyText confidence="0.8512106">
In Figure 3, the algorithm correctly lists
&lt; &gt; (6-3) as a non-coreferential pair,
while permitting &apos;him&apos; to take &apos;John&apos; as an ante-
cedent. In Figure 4, it correctly excludes corefer-
ence between &apos;him&apos; and &apos;John&apos; (he.6-John. I), and
allows &apos;him&apos; to be referentially dependent upon
John expected Bill to impress him.
Noncoref pairs:
he.6 Bill.3
Coreference analysis time = msec.
</bodyText>
<figureCaption confidence="0.971478">
Figure 3.
</figureCaption>
<bodyText confidence="0.8162436875">
John lectured Bill to impress him.
subj (n) John (X3 ) noun
•— top lecture(X1 , X3 ,X4) verb
obj Bill(X4) noun
I [ipreinf preinf(X5) preinf
vnfvp impress(X5,X3,X6) verb
obj he(X6) noun
Noncoref pairs:
he.6 - John.1
Coreference analysis time = 5 msec.
Figure 4.
It makes this distinction by virtue of the differ-
ences between the roles of the two infinitival
clauses in these sentences. In Figure 3, the infin-
itival clause is a complement of &apos;expected&apos;, and
this verb is marked for object control of the
</bodyText>
<sectionHeader confidence="0.999071" genericHeader="method">
4. EXISTING PROPOSALS FOR CON-
STRAINING PRONOMINAL ANAPHORA
</sectionHeader>
<bodyText confidence="0.999935882352941">
We will discuss three suggestions which have
been made in the computational literature for
syntactically constraining the relationship be-
tween a pronoun and its set of possible antece-
dents intra-sententially. The first is Hobbs&apos;
(1978) Algorithm, which performs a breadth-first,
left-to-right search of the tree containing the pro-
noun for possible antecedents. The search is re-
stricted to paths above the first NP or S node
containing the pronoun, and so the pronoun
cannot be bound by an antecedent in its minimal
governing category. If no antecedents are found
within the same tree as the pronoun, the trees of
the previous sentences in the text are searched in
order of proximity. There are two main difficul-
ties with this approach. First, it cannot be ap-
plied to cases of control in infinitival clauses, like
those given in Figures 3 and 4, or to unbounded
dependencies, like those in Figure 1 and in ex-
amples 3b-c and 4b-c, without significant modifi-
cation.
Second, the algorithm is inefficient in design
and violates modularity by virtue of the fact that
it computes both intra-sentential constraints on
pronominal anaphora and inter-sentential ante-
cedent possibilities each time it is invoked for a
new pronoun in a tree. Our system computes the
set of pronoun-NP pairs for which coreference is
syntactically excluded in a single pass on a parse
tree. This set provides the input to a semantic-
pragmatic discourse module which determines
anaphora by inference and preference rules.
The other two proposals are presented in
Correa (1988), and in Ingria and Stallard (1989).
Both of these models are implementations of
Chornsky&apos;s Binding theory which make use of
Government Binding type parsers. They employ
essentially the same strategy. This involves com-
puting the set of possible antecedents of an ana-
phor as the NP&apos;s which c-command the anaphor
within a minimal domain (its minimal governing
category).1 The minimal domain of an NP is
characterized as the first 5, or the first NP without
a possessive subject, in which it is contained. The
possible intra-sentential antecedents of a pronoun
are the set of NP&apos;s in the tree which arc not in-
cluded within this minimal domain.
See Reinhart (1976) and (1983) for alternative definitions ole-command, and discussions of the role of this re-
lation in determining the possibilities of anaphora. See Lappin (1985) for additional discussion of the connection
between c-command arid distinct varieties of pronominal artakithora. See Chomsky (1981), (1986a) and (1986h)
for alternative definitions of the notion &apos;government&apos; and &apos;minimal governing category&apos;.
</bodyText>
<page confidence="0.989115">
140
</page>
<bodyText confidence="0.999863088235294">
This approach does sustain modularity by
computing the set of possible antecedents for all
pronouns within a tree in a single pass operation,
prior to the application of inter-sentential search
procedures. The main difficulty with the model
is that because constraints on pronominal ana-
phora are stated entirely in terms of configura-
tional relations of tree geometry, specifically, in
terms of c-command and minimal dominating S
and NP domains, control and unbounded de-
pendency structures can only be handled by ad-
ditional and fairly complex devices. It is
necessary to generate empty categories for PRO
and trace in appropriate positions in parse trees.
Additional algorithms must be invoked to specify
the chains of control (A-binding) for PRO, and
operator (A)-binding for trace in order to link
these categories to the constituents which bind
them. The algorithm which computes possible
antecedents for anaphors and pronouns must be
formulated so that it identifies the head of such a
chain as non-coreferential with a pronoun or
anaphor (in the sense of the Binding theory), if
any element of the chain is excluded as a possible
antecedent.
Neither empty categories nor binding chains
are required in our system. In Slot ar
parse representations, wh-phrases, heads of rela-
tive clauses, and NY&apos;s which control the subjects
of infinitival clauses are unified with the variables
corresponding to the roles they bind in argument
positions. Therefore, the clauses of the algorithm
apply to these constructions directly, and without
additional devices or stipulations.&apos;
</bodyText>
<sectionHeader confidence="0.999984333333333" genericHeader="method">
5. THE INTEGRATION OF THE FILTER
INTO OTHER SYSTEMS OF ANAPHORA
RESOLUTION
</sectionHeader>
<bodyText confidence="0.974540964285714">
We have recently implemented an algorithm
for the interpretation of intrasentential VP ana-
phora structures like those in Ha-c.
I la, John arrived, and Mary did too.
b. 13111 read every hook which Sam said
he did.
c. Max wrote a letter to 13ill before Mary
did to John.
The VP anaphora algorithm generates a second
tree which copies the antecedent verb into the
position of the head of the elliptical VP. It also
lists the new arguments and adjuncts which the
copied verb inherits from its antecedent. We have
integrated our filter on pronominal anaphora into
this algorithm, so that the filter applies to the in-
terpreted trees which the algorithm generates.
consider
12. John likes to him, and Bill does too.
If the filter applies to the parse of 11, it will
identify only &lt; &gt; as a non-corefcr-
ential pair, given that the pair &lt;
doesn&apos;t satisfy any of the conditions of the filter
algorithm. However, when the filter is applied to
the interpreted VP anaphora tree of 12, the filter
algorithm correctly identifies both pronoun-NP
pairs, as shown in the VP output of the algorithm
for 12 given in Figure 5.
John likes him, and Bill does too.
</bodyText>
<table confidence="0.931008266666667">
Antecedent Verb-Elliptical Verb Pairs.
like.2 - do1.7
Elliptical Verb-New Argument Pairs.
like.7 - he.3
Interpreted VP anaphora tree.
r_ subj John(X9) noun
L_ lconj like(X8,X9,X10) verb
1 obj he(X10) noun
top and(X1,X8,X11) verb
[I subj Bill(X12) noun
rconj like(X11,X12,X10) verb
vadV too(X11) adv
Non-Coreferential Pronoun-NP Pairs.
he.3 - John.1, he.3 - Bill.6
Coreference analysis time LI 70 msec.
</table>
<figureCaption confidence="0.969562">
Figure 5.
</figureCaption>
<bodyText confidence="0.999413222222222">
Our filter also provides input to a discourse
understanding system, LODUS, designed and
implemented by A. Berrith, and described in
(Bemth 1988, 1989). LOINS creates a single
discourse structure from the analyses of the SIot
Grammar parser for several sentences. It inter-
prets each sentence analysis in the context con-
sisting of the discourse processed so far, together
with domain knowledge, and it then embeds it
into the discourse structure. The process of in-
terpretation consists in applying rules of inference
which encode semantic and pragmatic (know-
In fact, a more complicated algorithm with approximately the same coverage as our filter can be formulated for
a parser which produces configurational surface trees without empty categories and binding chains, lithe parser
provides deep grammatical roles at some level of representation. The first author has implemented such an al-
gorithm for the PEG parser. For a general description of PEG, see Jensen (1986). The current version of IEG
provides information on deep grammatical roles by means of second pass rules which apply to the initial parse
record structure. The algorithm employs both c-command and reference to deep grammatical roles.
</bodyText>
<page confidence="0.99436">
141
</page>
<bodyText confidence="0.990139952380952">
ledge-based) relations among lexical items, and
discourse structures. The filter reduces the set of
possible antecedents which the anaphora resol-
ution component of LODUS considers for pro-
nouns. For example, this component will not
consider &apos;the cat&apos; or &apos;that&apos; as a possible antece-
dents for either occurrence of &apos;it&apos; in the second
sentence in 13, but only &apos;the mouse&apos; in the first
sentence of this discourse. This is due to the fact
that our filter lists the excluded pairs together
with the parse tree of the second sentence.
13. The mouse ran in.
The cat that saw it ate it.
Thus, the filter significantly reduces the search
space which the anaphora resolution component
of LODUS must process. The interface between
our filter and LODUS embodies the sort of mo-
dular interaction of syntactic and semantic-prag-
matic components which we see as important to
the successful operation and efficiency of any
anaphora resolution system.
</bodyText>
<sectionHeader confidence="0.999221" genericHeader="acknowledgments">
ACKNOWLEDGMENTS
</sectionHeader>
<bodyText confidence="0.999492">
We are grateful to Arendse Bemth, Martin
Chodorow, and Wlodek Zadrozny for helpful
comments and advice on proposals contained in
this paper.
</bodyText>
<sectionHeader confidence="0.999612" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999300485294117">
Bernth, A. (3988) Computational Discourse Se-
mantics, Doctoral Dissertation, U. Copenha-
gen and IBM Research.
Bernth, A. (1989) &amp;quot;Discourse Understanding In
Logic&amp;quot;, Proc. North American Conference on
Logic Programming, pp. 755-771, MIT Press.
Byrd, R. J. (1983) &amp;quot;Word Formation in Natural
Language Processing Systems,&amp;quot; Proceedings
of pp. 704-706.
Chomsky, N. (1981) Lectures on Government and
Binding, Furls, Dordrecht.
Chornsky, N. (1986a) Knowledge of Langwge:
Its Nature, Origin, and Use, Praeger, New
York.
Chornsky, N. (1986b) Barriers, MIT Press,
Cambridge, Mass.
Correa, N. (1988) &amp;quot;A Binding Rule for Govern-
ment-Binding Parsing&amp;quot;, COLING &apos;88, Buda-
pest, pp. 123-129.
Gazdar, G., E. Klein, G. Pullum, and I. Sag,
(1985) Generalized Phrase Structure
Grammar, Blackwell, Oxford.
Heidorn, G. E. (1982) &amp;quot;Experience with an Easily
Computed Metric for Ranking Alternative
Parses,&amp;quot; Proceedings of Annual ACL Meeting,
1982, pp. 82-84.
I lo bb s, J. (1978) &amp;quot;Resolving Pronoun
References&amp;quot;, Lingua 44, pp. 311-338.
Ingria, R. and D. Stallard (1989) &amp;quot;A Computa-
tional Mechanism for Pronominal
Reference&amp;quot;, Proceedings of the 27th Annual
Meeting of the Association for Computational
Linguistics, Vancouver, pp. 262-271.
Jensen, K. (1986) &amp;quot;PEG: A Broad-Coverage
Computational Syntax of English,&amp;quot; Technical
Report, IBM T.J. Watson Research Center,
Yorktown Heights, NY.
Kiavans, J. L. and Wacholder, N. (1989) &amp;quot;Doc-
umentation of Features and Attributes in
UDICT,&amp;quot; Research Report RC14251, IBM
T.J. Watson Research Center, Yorktown
heights, N.Y.
Lappin, S. (1985) &amp;quot;Pronominal Binding and Co-
reference&amp;quot;, Theoretical Linguistics 12, pp.
241-263.
McCord, M. C. (1980) &amp;quot;Slot Grammars,&amp;quot; Com-
putational Linguistics, vol. 6, pp. 31-43.
McCord, M. C. (1989a) &amp;quot;Design of LMT: A
Prolog-based Machine Translation System,&amp;quot;
Computational Linguistics, vol. 15, pp. 33-52.
McCord, M. C. (1989b) &amp;quot;A New Version of Slot
Grammar,&amp;quot; Research Report RC 14506, IBM
Research Division, Yorktown Heights, NY
10598.
McCord, M. C. (1989c) &amp;quot;A New Version of the
Machine Translation System LMT,&amp;quot; to ap-
pear in Proc. International Scientific Syrnpo-
slum on Natural Language and Logic, Springer
Lecture Notes in Computer Science, and in
.I. Literary and Linguistic Computing.
McCord, M. C. (1989d) &amp;quot;LMT,&amp;quot; Proceedings of
MT Summit II, pp. 94-99, Deutsche Gesell-
schaft filr Dokumentation, Frankfurt.
Reinhart, T. (1976) The Syntactic Domain of
Anaphora, Doctoral Dissertation, MIT, Cam-
bridge, Mass.
Reinhart, T. (1983) Anaphora, Croom I &apos;elm,
London.
</reference>
<page confidence="0.997703">
142
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.924717">
<title confidence="0.999468">A SYNTACTIC FILTER ON PRONOMINAL ANAPHORA FOR SLOT GRAMMAR</title>
<author confidence="0.999981">Shalom Lappin</author>
<author confidence="0.999981">Michael McCord</author>
<affiliation confidence="0.999754">IBM Ti. Watson Research Center</affiliation>
<address confidence="0.998735">P.O. Box 704 Yorktown Heights, NY 10598</address>
<email confidence="0.985152">E-mail:Lappin/McCord@yktvmh.hitnet</email>
<abstract confidence="0.995825142857143">We propose a syntactic filter for identifying non-coreferential pronoun-NP pairs within a sentence. The filter applies to the output of a Slot Grammar parser and is formulated in terms of the head-argument structures which the parser generates. It handles control and unbounded dependency constructions without empty categories or binding chains, by virtue of the unificational nature of the parser. The filter provides constraints for a discourse semantics system, reducing the search domain to which the inference rules of the system&apos;s anaphora resolution component apply.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Bernth</author>
</authors>
<date></date>
<booktitle>Computational Discourse Semantics, Doctoral Dissertation, U. Copenhagen and IBM Research.</booktitle>
<marker>Bernth, </marker>
<rawString>Bernth, A. (3988) Computational Discourse Semantics, Doctoral Dissertation, U. Copenhagen and IBM Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bernth</author>
</authors>
<title>Discourse Understanding In Logic&amp;quot;,</title>
<date>1989</date>
<booktitle>Proc. North American Conference on Logic Programming,</booktitle>
<pages>755--771</pages>
<publisher>MIT Press.</publisher>
<marker>Bernth, 1989</marker>
<rawString>Bernth, A. (1989) &amp;quot;Discourse Understanding In Logic&amp;quot;, Proc. North American Conference on Logic Programming, pp. 755-771, MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Byrd</author>
</authors>
<title>Word Formation in Natural Language Processing Systems,&amp;quot;</title>
<date>1983</date>
<booktitle>Proceedings of</booktitle>
<pages>704--706</pages>
<contexts>
<context position="7216" citStr="Byrd 1983" startWordPosition="1138" endWordPosition="1139">ge-dependent. A (language-specific) grammar can include for instance (1) rules for coordinating feature structures that override the defaults in the shell; (2) declarations of slots (called extraposer slots) that allow left extraposition of other slots out of their fillers; (3) language-specific rules for punctuation that override defaults; and (4) language-specific controls over parse evaluation that override defaults. Currently, Slot Grammars are being developed for English (ESC) by McCord, for Danish (DSG) by Arendse Bemth, and for German (GSG) by Ulrike Schwan. ESG uses the UDICT lexicon (Byrd 1983, Klavans and Wacholder 1989) having over 60,000 lemmas, with an interface that produces slot frames. The filter algorithm has so far been successfully tested with ESG and GSG. (The adaptation to German was done by Ulrike Schwall.) The algorithm applies in a second pass to the parse output, so the important thing in the remainder of this section is to describe Slot Grammar syntactic analysis structures. A syntactic structure is a tree; each node of the tree represents a phrase in the sentence and has a unique head word. Formally, a phrase is represented by a term phrase(X,H,Sense,Features, Slo</context>
</contexts>
<marker>Byrd, 1983</marker>
<rawString>Byrd, R. J. (1983) &amp;quot;Word Formation in Natural Language Processing Systems,&amp;quot; Proceedings of pp. 704-706.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<date>1981</date>
<booktitle>Lectures on Government and Binding, Furls,</booktitle>
<location>Dordrecht.</location>
<contexts>
<context position="24214" citStr="Chomsky (1981)" startWordPosition="4081" endWordPosition="4082"> minimal governing category).1 The minimal domain of an NP is characterized as the first 5, or the first NP without a possessive subject, in which it is contained. The possible intra-sentential antecedents of a pronoun are the set of NP&apos;s in the tree which arc not included within this minimal domain. See Reinhart (1976) and (1983) for alternative definitions ole-command, and discussions of the role of this relation in determining the possibilities of anaphora. See Lappin (1985) for additional discussion of the connection between c-command arid distinct varieties of pronominal artakithora. See Chomsky (1981), (1986a) and (1986h) for alternative definitions of the notion &apos;government&apos; and &apos;minimal governing category&apos;. 140 This approach does sustain modularity by computing the set of possible antecedents for all pronouns within a tree in a single pass operation, prior to the application of inter-sentential search procedures. The main difficulty with the model is that because constraints on pronominal anaphora are stated entirely in terms of configurational relations of tree geometry, specifically, in terms of c-command and minimal dominating S and NP domains, control and unbounded dependency structu</context>
</contexts>
<marker>Chomsky, 1981</marker>
<rawString>Chomsky, N. (1981) Lectures on Government and Binding, Furls, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chornsky</author>
</authors>
<title>Knowledge of Langwge: Its Nature, Origin, and Use, Praeger,</title>
<date>1986</date>
<location>New York.</location>
<marker>Chornsky, 1986</marker>
<rawString>Chornsky, N. (1986a) Knowledge of Langwge: Its Nature, Origin, and Use, Praeger, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chornsky</author>
</authors>
<date>1986</date>
<publisher>Barriers, MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<marker>Chornsky, 1986</marker>
<rawString>Chornsky, N. (1986b) Barriers, MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Correa</author>
</authors>
<title>A Binding Rule for Government-Binding Parsing&amp;quot;,</title>
<date>1988</date>
<booktitle>COLING &apos;88,</booktitle>
<pages>123--129</pages>
<location>Budapest,</location>
<contexts>
<context position="23261" citStr="Correa (1988)" startWordPosition="3930" endWordPosition="3931">without significant modification. Second, the algorithm is inefficient in design and violates modularity by virtue of the fact that it computes both intra-sentential constraints on pronominal anaphora and inter-sentential antecedent possibilities each time it is invoked for a new pronoun in a tree. Our system computes the set of pronoun-NP pairs for which coreference is syntactically excluded in a single pass on a parse tree. This set provides the input to a semanticpragmatic discourse module which determines anaphora by inference and preference rules. The other two proposals are presented in Correa (1988), and in Ingria and Stallard (1989). Both of these models are implementations of Chornsky&apos;s Binding theory which make use of Government Binding type parsers. They employ essentially the same strategy. This involves computing the set of possible antecedents of an anaphor as the NP&apos;s which c-command the anaphor within a minimal domain (its minimal governing category).1 The minimal domain of an NP is characterized as the first 5, or the first NP without a possessive subject, in which it is contained. The possible intra-sentential antecedents of a pronoun are the set of NP&apos;s in the tree which arc </context>
</contexts>
<marker>Correa, 1988</marker>
<rawString>Correa, N. (1988) &amp;quot;A Binding Rule for Government-Binding Parsing&amp;quot;, COLING &apos;88, Budapest, pp. 123-129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
<author>E Klein</author>
<author>G Pullum</author>
<author>I Sag</author>
</authors>
<title>Generalized Phrase Structure Grammar,</title>
<date>1985</date>
<location>Blackwell, Oxford.</location>
<contexts>
<context position="9361" citStr="Gazdar et al (1985)" startWordPosition="1521" endWordPosition="1524">(essentially) with the marker of the filler phrase when the slot is filled, even remotely, as m left movement or coordination. Such unifications are important for the filter algorithm. (6) Ext is the list of slots that have been extraposed or raised to the level of the current phrase. (7) The last component Mods represents the modifiers (daughters) of the phrase, and is of the form mods (LMods , Mods ) where LMods and RMods are The distinction between slot filler rules and ordering constraints parallels the difference between Immediate Dominance Rules and Linear Precedence Rules in G PSG. See Gazdar et al (1985) for a characterization of II) and I.P rules in GPSG. See (McCord 1989h) for more discussion of the relation of Slot Grammar to other systems. 136 Who did John say wanted to try to find him? subj(n) top subj(n) auxcmp(inf(bare)) obj(fin) preinf comp(enlinfling) [ i preinf obj(inf) obj(fin) who (12) dol(X1,X3,X4) John(M) say(X4,13,X9,u) want(X9,12,X2,X12) preinf(X12) try(X12,X2,X13) preinf(X13) find(113,X2,114,u, he((14) noun verb noun verb verb preinf verb preinf u) verb noun 1 1 1 Figure 1. the lists of left modifiers and right modifiers, respectively. Each member of a modifier list is of the</context>
</contexts>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gazdar, G., E. Klein, G. Pullum, and I. Sag, (1985) Generalized Phrase Structure Grammar, Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Heidorn</author>
</authors>
<title>Experience with an Easily Computed Metric for Ranking Alternative Parses,&amp;quot;</title>
<date>1982</date>
<booktitle>Proceedings of Annual ACL Meeting,</booktitle>
<pages>82--84</pages>
<marker>Heidorn, 1982</marker>
<rawString>Heidorn, G. E. (1982) &amp;quot;Experience with an Easily Computed Metric for Ranking Alternative Parses,&amp;quot; Proceedings of Annual ACL Meeting, 1982, pp. 82-84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I lo bb s</author>
<author>J</author>
</authors>
<title>Resolving Pronoun References&amp;quot;,</title>
<date>1978</date>
<journal>Lingua</journal>
<volume>44</volume>
<pages>311--338</pages>
<marker>s, J, 1978</marker>
<rawString>I lo bb s, J. (1978) &amp;quot;Resolving Pronoun References&amp;quot;, Lingua 44, pp. 311-338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ingria</author>
<author>D Stallard</author>
</authors>
<title>A Computational Mechanism for Pronominal Reference&amp;quot;,</title>
<date>1989</date>
<booktitle>Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>262--271</pages>
<location>Vancouver,</location>
<contexts>
<context position="23296" citStr="Ingria and Stallard (1989)" startWordPosition="3934" endWordPosition="3937">dification. Second, the algorithm is inefficient in design and violates modularity by virtue of the fact that it computes both intra-sentential constraints on pronominal anaphora and inter-sentential antecedent possibilities each time it is invoked for a new pronoun in a tree. Our system computes the set of pronoun-NP pairs for which coreference is syntactically excluded in a single pass on a parse tree. This set provides the input to a semanticpragmatic discourse module which determines anaphora by inference and preference rules. The other two proposals are presented in Correa (1988), and in Ingria and Stallard (1989). Both of these models are implementations of Chornsky&apos;s Binding theory which make use of Government Binding type parsers. They employ essentially the same strategy. This involves computing the set of possible antecedents of an anaphor as the NP&apos;s which c-command the anaphor within a minimal domain (its minimal governing category).1 The minimal domain of an NP is characterized as the first 5, or the first NP without a possessive subject, in which it is contained. The possible intra-sentential antecedents of a pronoun are the set of NP&apos;s in the tree which arc not included within this minimal do</context>
</contexts>
<marker>Ingria, Stallard, 1989</marker>
<rawString>Ingria, R. and D. Stallard (1989) &amp;quot;A Computational Mechanism for Pronominal Reference&amp;quot;, Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics, Vancouver, pp. 262-271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Jensen</author>
</authors>
<title>PEG: A Broad-Coverage Computational Syntax of English,&amp;quot;</title>
<date>1986</date>
<tech>Technical Report, IBM</tech>
<institution>T.J. Watson Research Center,</institution>
<location>Yorktown Heights, NY.</location>
<contexts>
<context position="28490" citStr="Jensen (1986)" startWordPosition="4766" endWordPosition="4767">so far, together with domain knowledge, and it then embeds it into the discourse structure. The process of interpretation consists in applying rules of inference which encode semantic and pragmatic (knowIn fact, a more complicated algorithm with approximately the same coverage as our filter can be formulated for a parser which produces configurational surface trees without empty categories and binding chains, lithe parser provides deep grammatical roles at some level of representation. The first author has implemented such an algorithm for the PEG parser. For a general description of PEG, see Jensen (1986). The current version of IEG provides information on deep grammatical roles by means of second pass rules which apply to the initial parse record structure. The algorithm employs both c-command and reference to deep grammatical roles. 141 ledge-based) relations among lexical items, and discourse structures. The filter reduces the set of possible antecedents which the anaphora resolution component of LODUS considers for pronouns. For example, this component will not consider &apos;the cat&apos; or &apos;that&apos; as a possible antecedents for either occurrence of &apos;it&apos; in the second sentence in 13, but only &apos;the m</context>
</contexts>
<marker>Jensen, 1986</marker>
<rawString>Jensen, K. (1986) &amp;quot;PEG: A Broad-Coverage Computational Syntax of English,&amp;quot; Technical Report, IBM T.J. Watson Research Center, Yorktown Heights, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Kiavans</author>
<author>N Wacholder</author>
</authors>
<title>Documentation of Features and Attributes in UDICT,&amp;quot;</title>
<date>1989</date>
<journal>Research Report RC14251, IBM T.J. Watson Research</journal>
<location>Center, Yorktown heights, N.Y.</location>
<marker>Kiavans, Wacholder, 1989</marker>
<rawString>Kiavans, J. L. and Wacholder, N. (1989) &amp;quot;Documentation of Features and Attributes in UDICT,&amp;quot; Research Report RC14251, IBM T.J. Watson Research Center, Yorktown heights, N.Y.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lappin</author>
</authors>
<title>Pronominal Binding and Coreference&amp;quot;,</title>
<date>1985</date>
<journal>Theoretical Linguistics</journal>
<volume>12</volume>
<pages>241--263</pages>
<contexts>
<context position="24082" citStr="Lappin (1985)" startWordPosition="4064" endWordPosition="4065">volves computing the set of possible antecedents of an anaphor as the NP&apos;s which c-command the anaphor within a minimal domain (its minimal governing category).1 The minimal domain of an NP is characterized as the first 5, or the first NP without a possessive subject, in which it is contained. The possible intra-sentential antecedents of a pronoun are the set of NP&apos;s in the tree which arc not included within this minimal domain. See Reinhart (1976) and (1983) for alternative definitions ole-command, and discussions of the role of this relation in determining the possibilities of anaphora. See Lappin (1985) for additional discussion of the connection between c-command arid distinct varieties of pronominal artakithora. See Chomsky (1981), (1986a) and (1986h) for alternative definitions of the notion &apos;government&apos; and &apos;minimal governing category&apos;. 140 This approach does sustain modularity by computing the set of possible antecedents for all pronouns within a tree in a single pass operation, prior to the application of inter-sentential search procedures. The main difficulty with the model is that because constraints on pronominal anaphora are stated entirely in terms of configurational relations of </context>
</contexts>
<marker>Lappin, 1985</marker>
<rawString>Lappin, S. (1985) &amp;quot;Pronominal Binding and Coreference&amp;quot;, Theoretical Linguistics 12, pp. 241-263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C McCord</author>
</authors>
<title>Slot Grammars,&amp;quot;</title>
<date>1980</date>
<journal>Computational Linguistics,</journal>
<volume>6</volume>
<pages>31--43</pages>
<contexts>
<context position="3313" citStr="McCord 1980" startWordPosition="510" endWordPosition="511">sals for syntactic filtering of pronominal anaphora which have appeared in the literature. We discuss Itobbs&apos; algorithm, and we take up two recent implementations of the binding theory. Finally, in Section 5 we discuss filter the integration of our ter into other systems of anaphora resolution. We indicate how it can be combined with a VP anaphora algorithm which we have recently completed. We also outline the incorporation of our algorithm into LODUS (Bemth 1989), a system for discourse representation. 2. SLOT GRAMMAR The original work on Slot Grammar was done around 1976-78 and appeared in (McCord 1980). Recently, a new version (McCord 1989b) was developed in a logic programming framework, in connection with the machine translation system LMT (McCord 1989a,c,d). Slot Grammar is lexicalist and is dependency-oriented. Every phrase has a head word (with a given word sense and morphosyntactic features). The constituents of a phrase besides the head word (also called the modifiers of the head) are obtained by &amp;quot;filling&apos; siois associated with the head. Slots are symbols like sub j, oh _I and i oh j representing grammatical relations, and are associated with a word (sense) in two ways. The lexical e</context>
</contexts>
<marker>McCord, 1980</marker>
<rawString>McCord, M. C. (1980) &amp;quot;Slot Grammars,&amp;quot; Computational Linguistics, vol. 6, pp. 31-43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C McCord</author>
</authors>
<title>Design of LMT: A Prolog-based Machine Translation System,&amp;quot;</title>
<date>1989</date>
<journal>Computational Linguistics,</journal>
<volume>15</volume>
<pages>33--52</pages>
<contexts>
<context position="1082" citStr="McCord 1989" startWordPosition="157" endWordPosition="158"> and unbounded dependency constructions without empty categories or binding chains, by virtue of the unificational nature of the parser. The filter provides constraints for a discourse semantics system, reducing the search domain to which the inference rules of the system&apos;s anaphora resolution component apply. I. INTRODUCTION In this paper we present an implemented algorithm which filters intra-sentential relations of referential dependence between pronouns and putative NP antecedents (both full and pronominal NP&apos;s) for the syntactic representations provided by an English Slot Grammar parser (McCord 1989b). or each parse of a sentence, the algorithm provides a list of pronoun-NI&apos; pairs where referential dependence of the fir St element on the second is excluded by syntactic constraints. The coverage of the filter has roughly the same extension as conditions B and C of Chomsky&apos;s (1981, 1986) binding theory. However, the formulation of the algorithm is significantly different from the conditions of the binding theory, and from proposed implementations of its conditions. In particular, the fitter formulates constraints on pronominal anaphora in terms of the head-argument structures provided by S</context>
<context position="3351" citStr="McCord 1989" startWordPosition="516" endWordPosition="517">nal anaphora which have appeared in the literature. We discuss Itobbs&apos; algorithm, and we take up two recent implementations of the binding theory. Finally, in Section 5 we discuss filter the integration of our ter into other systems of anaphora resolution. We indicate how it can be combined with a VP anaphora algorithm which we have recently completed. We also outline the incorporation of our algorithm into LODUS (Bemth 1989), a system for discourse representation. 2. SLOT GRAMMAR The original work on Slot Grammar was done around 1976-78 and appeared in (McCord 1980). Recently, a new version (McCord 1989b) was developed in a logic programming framework, in connection with the machine translation system LMT (McCord 1989a,c,d). Slot Grammar is lexicalist and is dependency-oriented. Every phrase has a head word (with a given word sense and morphosyntactic features). The constituents of a phrase besides the head word (also called the modifiers of the head) are obtained by &amp;quot;filling&apos; siois associated with the head. Slots are symbols like sub j, oh _I and i oh j representing grammatical relations, and are associated with a word (sense) in two ways. The lexical entry for the word specifies a set of c</context>
<context position="9431" citStr="McCord 1989" startWordPosition="1536" endWordPosition="1537">en remotely, as m left movement or coordination. Such unifications are important for the filter algorithm. (6) Ext is the list of slots that have been extraposed or raised to the level of the current phrase. (7) The last component Mods represents the modifiers (daughters) of the phrase, and is of the form mods (LMods , Mods ) where LMods and RMods are The distinction between slot filler rules and ordering constraints parallels the difference between Immediate Dominance Rules and Linear Precedence Rules in G PSG. See Gazdar et al (1985) for a characterization of II) and I.P rules in GPSG. See (McCord 1989h) for more discussion of the relation of Slot Grammar to other systems. 136 Who did John say wanted to try to find him? subj(n) top subj(n) auxcmp(inf(bare)) obj(fin) preinf comp(enlinfling) [ i preinf obj(inf) obj(fin) who (12) dol(X1,X3,X4) John(M) say(X4,13,X9,u) want(X9,12,X2,X12) preinf(X12) try(X12,X2,X13) preinf(X13) find(113,X2,114,u, he((14) noun verb noun verb verb preinf verb preinf u) verb noun 1 1 1 Figure 1. the lists of left modifiers and right modifiers, respectively. Each member of a modifier list is of the form 5 lot :Phrase where Slot is a slot and Phrase is a phrase which </context>
</contexts>
<marker>McCord, 1989</marker>
<rawString>McCord, M. C. (1989a) &amp;quot;Design of LMT: A Prolog-based Machine Translation System,&amp;quot; Computational Linguistics, vol. 15, pp. 33-52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C McCord</author>
</authors>
<title>A New Version of Slot Grammar,&amp;quot;</title>
<date>1989</date>
<booktitle>Research Report RC 14506, IBM Research Division,</booktitle>
<pages>10598</pages>
<location>Yorktown Heights, NY</location>
<contexts>
<context position="1082" citStr="McCord 1989" startWordPosition="157" endWordPosition="158"> and unbounded dependency constructions without empty categories or binding chains, by virtue of the unificational nature of the parser. The filter provides constraints for a discourse semantics system, reducing the search domain to which the inference rules of the system&apos;s anaphora resolution component apply. I. INTRODUCTION In this paper we present an implemented algorithm which filters intra-sentential relations of referential dependence between pronouns and putative NP antecedents (both full and pronominal NP&apos;s) for the syntactic representations provided by an English Slot Grammar parser (McCord 1989b). or each parse of a sentence, the algorithm provides a list of pronoun-NI&apos; pairs where referential dependence of the fir St element on the second is excluded by syntactic constraints. The coverage of the filter has roughly the same extension as conditions B and C of Chomsky&apos;s (1981, 1986) binding theory. However, the formulation of the algorithm is significantly different from the conditions of the binding theory, and from proposed implementations of its conditions. In particular, the fitter formulates constraints on pronominal anaphora in terms of the head-argument structures provided by S</context>
<context position="3351" citStr="McCord 1989" startWordPosition="516" endWordPosition="517">nal anaphora which have appeared in the literature. We discuss Itobbs&apos; algorithm, and we take up two recent implementations of the binding theory. Finally, in Section 5 we discuss filter the integration of our ter into other systems of anaphora resolution. We indicate how it can be combined with a VP anaphora algorithm which we have recently completed. We also outline the incorporation of our algorithm into LODUS (Bemth 1989), a system for discourse representation. 2. SLOT GRAMMAR The original work on Slot Grammar was done around 1976-78 and appeared in (McCord 1980). Recently, a new version (McCord 1989b) was developed in a logic programming framework, in connection with the machine translation system LMT (McCord 1989a,c,d). Slot Grammar is lexicalist and is dependency-oriented. Every phrase has a head word (with a given word sense and morphosyntactic features). The constituents of a phrase besides the head word (also called the modifiers of the head) are obtained by &amp;quot;filling&apos; siois associated with the head. Slots are symbols like sub j, oh _I and i oh j representing grammatical relations, and are associated with a word (sense) in two ways. The lexical entry for the word specifies a set of c</context>
<context position="9431" citStr="McCord 1989" startWordPosition="1536" endWordPosition="1537">en remotely, as m left movement or coordination. Such unifications are important for the filter algorithm. (6) Ext is the list of slots that have been extraposed or raised to the level of the current phrase. (7) The last component Mods represents the modifiers (daughters) of the phrase, and is of the form mods (LMods , Mods ) where LMods and RMods are The distinction between slot filler rules and ordering constraints parallels the difference between Immediate Dominance Rules and Linear Precedence Rules in G PSG. See Gazdar et al (1985) for a characterization of II) and I.P rules in GPSG. See (McCord 1989h) for more discussion of the relation of Slot Grammar to other systems. 136 Who did John say wanted to try to find him? subj(n) top subj(n) auxcmp(inf(bare)) obj(fin) preinf comp(enlinfling) [ i preinf obj(inf) obj(fin) who (12) dol(X1,X3,X4) John(M) say(X4,13,X9,u) want(X9,12,X2,X12) preinf(X12) try(X12,X2,X13) preinf(X13) find(113,X2,114,u, he((14) noun verb noun verb verb preinf verb preinf u) verb noun 1 1 1 Figure 1. the lists of left modifiers and right modifiers, respectively. Each member of a modifier list is of the form 5 lot :Phrase where Slot is a slot and Phrase is a phrase which </context>
</contexts>
<marker>McCord, 1989</marker>
<rawString>McCord, M. C. (1989b) &amp;quot;A New Version of Slot Grammar,&amp;quot; Research Report RC 14506, IBM Research Division, Yorktown Heights, NY 10598.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C McCord</author>
</authors>
<title>A New Version of the Machine Translation System LMT,&amp;quot; to appear in</title>
<date>1989</date>
<booktitle>Proc. International Scientific Syrnposlum on Natural Language and Logic, Springer Lecture Notes in Computer Science, and in .I. Literary and Linguistic Computing.</booktitle>
<contexts>
<context position="1082" citStr="McCord 1989" startWordPosition="157" endWordPosition="158"> and unbounded dependency constructions without empty categories or binding chains, by virtue of the unificational nature of the parser. The filter provides constraints for a discourse semantics system, reducing the search domain to which the inference rules of the system&apos;s anaphora resolution component apply. I. INTRODUCTION In this paper we present an implemented algorithm which filters intra-sentential relations of referential dependence between pronouns and putative NP antecedents (both full and pronominal NP&apos;s) for the syntactic representations provided by an English Slot Grammar parser (McCord 1989b). or each parse of a sentence, the algorithm provides a list of pronoun-NI&apos; pairs where referential dependence of the fir St element on the second is excluded by syntactic constraints. The coverage of the filter has roughly the same extension as conditions B and C of Chomsky&apos;s (1981, 1986) binding theory. However, the formulation of the algorithm is significantly different from the conditions of the binding theory, and from proposed implementations of its conditions. In particular, the fitter formulates constraints on pronominal anaphora in terms of the head-argument structures provided by S</context>
<context position="3351" citStr="McCord 1989" startWordPosition="516" endWordPosition="517">nal anaphora which have appeared in the literature. We discuss Itobbs&apos; algorithm, and we take up two recent implementations of the binding theory. Finally, in Section 5 we discuss filter the integration of our ter into other systems of anaphora resolution. We indicate how it can be combined with a VP anaphora algorithm which we have recently completed. We also outline the incorporation of our algorithm into LODUS (Bemth 1989), a system for discourse representation. 2. SLOT GRAMMAR The original work on Slot Grammar was done around 1976-78 and appeared in (McCord 1980). Recently, a new version (McCord 1989b) was developed in a logic programming framework, in connection with the machine translation system LMT (McCord 1989a,c,d). Slot Grammar is lexicalist and is dependency-oriented. Every phrase has a head word (with a given word sense and morphosyntactic features). The constituents of a phrase besides the head word (also called the modifiers of the head) are obtained by &amp;quot;filling&apos; siois associated with the head. Slots are symbols like sub j, oh _I and i oh j representing grammatical relations, and are associated with a word (sense) in two ways. The lexical entry for the word specifies a set of c</context>
<context position="9431" citStr="McCord 1989" startWordPosition="1536" endWordPosition="1537">en remotely, as m left movement or coordination. Such unifications are important for the filter algorithm. (6) Ext is the list of slots that have been extraposed or raised to the level of the current phrase. (7) The last component Mods represents the modifiers (daughters) of the phrase, and is of the form mods (LMods , Mods ) where LMods and RMods are The distinction between slot filler rules and ordering constraints parallels the difference between Immediate Dominance Rules and Linear Precedence Rules in G PSG. See Gazdar et al (1985) for a characterization of II) and I.P rules in GPSG. See (McCord 1989h) for more discussion of the relation of Slot Grammar to other systems. 136 Who did John say wanted to try to find him? subj(n) top subj(n) auxcmp(inf(bare)) obj(fin) preinf comp(enlinfling) [ i preinf obj(inf) obj(fin) who (12) dol(X1,X3,X4) John(M) say(X4,13,X9,u) want(X9,12,X2,X12) preinf(X12) try(X12,X2,X13) preinf(X13) find(113,X2,114,u, he((14) noun verb noun verb verb preinf verb preinf u) verb noun 1 1 1 Figure 1. the lists of left modifiers and right modifiers, respectively. Each member of a modifier list is of the form 5 lot :Phrase where Slot is a slot and Phrase is a phrase which </context>
</contexts>
<marker>McCord, 1989</marker>
<rawString>McCord, M. C. (1989c) &amp;quot;A New Version of the Machine Translation System LMT,&amp;quot; to appear in Proc. International Scientific Syrnposlum on Natural Language and Logic, Springer Lecture Notes in Computer Science, and in .I. Literary and Linguistic Computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C McCord</author>
</authors>
<title>LMT,&amp;quot;</title>
<date>1989</date>
<booktitle>Proceedings of MT Summit II,</booktitle>
<pages>94--99</pages>
<location>Frankfurt.</location>
<contexts>
<context position="1082" citStr="McCord 1989" startWordPosition="157" endWordPosition="158"> and unbounded dependency constructions without empty categories or binding chains, by virtue of the unificational nature of the parser. The filter provides constraints for a discourse semantics system, reducing the search domain to which the inference rules of the system&apos;s anaphora resolution component apply. I. INTRODUCTION In this paper we present an implemented algorithm which filters intra-sentential relations of referential dependence between pronouns and putative NP antecedents (both full and pronominal NP&apos;s) for the syntactic representations provided by an English Slot Grammar parser (McCord 1989b). or each parse of a sentence, the algorithm provides a list of pronoun-NI&apos; pairs where referential dependence of the fir St element on the second is excluded by syntactic constraints. The coverage of the filter has roughly the same extension as conditions B and C of Chomsky&apos;s (1981, 1986) binding theory. However, the formulation of the algorithm is significantly different from the conditions of the binding theory, and from proposed implementations of its conditions. In particular, the fitter formulates constraints on pronominal anaphora in terms of the head-argument structures provided by S</context>
<context position="3351" citStr="McCord 1989" startWordPosition="516" endWordPosition="517">nal anaphora which have appeared in the literature. We discuss Itobbs&apos; algorithm, and we take up two recent implementations of the binding theory. Finally, in Section 5 we discuss filter the integration of our ter into other systems of anaphora resolution. We indicate how it can be combined with a VP anaphora algorithm which we have recently completed. We also outline the incorporation of our algorithm into LODUS (Bemth 1989), a system for discourse representation. 2. SLOT GRAMMAR The original work on Slot Grammar was done around 1976-78 and appeared in (McCord 1980). Recently, a new version (McCord 1989b) was developed in a logic programming framework, in connection with the machine translation system LMT (McCord 1989a,c,d). Slot Grammar is lexicalist and is dependency-oriented. Every phrase has a head word (with a given word sense and morphosyntactic features). The constituents of a phrase besides the head word (also called the modifiers of the head) are obtained by &amp;quot;filling&apos; siois associated with the head. Slots are symbols like sub j, oh _I and i oh j representing grammatical relations, and are associated with a word (sense) in two ways. The lexical entry for the word specifies a set of c</context>
<context position="9431" citStr="McCord 1989" startWordPosition="1536" endWordPosition="1537">en remotely, as m left movement or coordination. Such unifications are important for the filter algorithm. (6) Ext is the list of slots that have been extraposed or raised to the level of the current phrase. (7) The last component Mods represents the modifiers (daughters) of the phrase, and is of the form mods (LMods , Mods ) where LMods and RMods are The distinction between slot filler rules and ordering constraints parallels the difference between Immediate Dominance Rules and Linear Precedence Rules in G PSG. See Gazdar et al (1985) for a characterization of II) and I.P rules in GPSG. See (McCord 1989h) for more discussion of the relation of Slot Grammar to other systems. 136 Who did John say wanted to try to find him? subj(n) top subj(n) auxcmp(inf(bare)) obj(fin) preinf comp(enlinfling) [ i preinf obj(inf) obj(fin) who (12) dol(X1,X3,X4) John(M) say(X4,13,X9,u) want(X9,12,X2,X12) preinf(X12) try(X12,X2,X13) preinf(X13) find(113,X2,114,u, he((14) noun verb noun verb verb preinf verb preinf u) verb noun 1 1 1 Figure 1. the lists of left modifiers and right modifiers, respectively. Each member of a modifier list is of the form 5 lot :Phrase where Slot is a slot and Phrase is a phrase which </context>
</contexts>
<marker>McCord, 1989</marker>
<rawString>McCord, M. C. (1989d) &amp;quot;LMT,&amp;quot; Proceedings of MT Summit II, pp. 94-99, Deutsche Gesellschaft filr Dokumentation, Frankfurt.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Reinhart</author>
</authors>
<title>The Syntactic Domain of Anaphora, Doctoral Dissertation,</title>
<date>1976</date>
<location>MIT, Cambridge, Mass.</location>
<contexts>
<context position="23921" citStr="Reinhart (1976)" startWordPosition="4040" endWordPosition="4041">these models are implementations of Chornsky&apos;s Binding theory which make use of Government Binding type parsers. They employ essentially the same strategy. This involves computing the set of possible antecedents of an anaphor as the NP&apos;s which c-command the anaphor within a minimal domain (its minimal governing category).1 The minimal domain of an NP is characterized as the first 5, or the first NP without a possessive subject, in which it is contained. The possible intra-sentential antecedents of a pronoun are the set of NP&apos;s in the tree which arc not included within this minimal domain. See Reinhart (1976) and (1983) for alternative definitions ole-command, and discussions of the role of this relation in determining the possibilities of anaphora. See Lappin (1985) for additional discussion of the connection between c-command arid distinct varieties of pronominal artakithora. See Chomsky (1981), (1986a) and (1986h) for alternative definitions of the notion &apos;government&apos; and &apos;minimal governing category&apos;. 140 This approach does sustain modularity by computing the set of possible antecedents for all pronouns within a tree in a single pass operation, prior to the application of inter-sentential searc</context>
</contexts>
<marker>Reinhart, 1976</marker>
<rawString>Reinhart, T. (1976) The Syntactic Domain of Anaphora, Doctoral Dissertation, MIT, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Reinhart</author>
</authors>
<title>Anaphora, Croom I &apos;elm,</title>
<date>1983</date>
<location>London.</location>
<marker>Reinhart, 1983</marker>
<rawString>Reinhart, T. (1983) Anaphora, Croom I &apos;elm, London.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>