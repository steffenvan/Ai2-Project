<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.020818">
<bodyText confidence="0.999542230769231">
translations of the Spanish SpCH utterances, and
to derive Spanish translations of the English Swbd
and MRDA utterances. Of course, translations are
far from perfect; DA classification performance
could likely be improved by using a translation
system trained on spoken dialog. For instance,
Google Translate often failed on certain words like
“i” that are usually capitalized in text. Even so,
when training and testing on translated utterances,
the results with the generic system are surprisingly
good.
The results reported below used the standard
train/test splits provided with the corpora: MRDA
had 51 train meetings/11 test; Swbd had 1115 train
conversations/19 test; SpCH had 80 train conver-
sations/20 test. The SpCH train set is the smallest
at 29k utterances. To avoid issues of differing train
set size when comparing performance of different
models, we reduced the Swbd and MRDA train
sets to the same size as SpCH using randomly se-
lected examples from the full train sets. For each
adaptation experiment, we used the target domain
training set as the unlabeled data, and report per-
formance on the target domain test set. The test
sets contain 4525, 15180, and 3715 utterances for
Swbd, MRDA, and SpCH respectively.
</bodyText>
<sectionHeader confidence="0.999971" genericHeader="abstract">
4 Results
</sectionHeader>
<bodyText confidence="0.999893774193548">
Table 1 shows the class proportions in the training
sets for each domain. MRDA has fewer Backchan-
nels than the the others, which is expected since
the meetings are face-to-face. SpCH has fewer In-
completes and more Questions than the others; the
reasons for this are unclear. Backchannels have
the shortest mean length (less than 2 words) in all
domains. Incompletes are also short, while State-
ments have the longest mean length. The mean
lengths of Statements and Questions are similar
in the English corpora, but are shorter in SpCH.
(This may point to differences in how the utter-
ances were segmented; for instance Swbd utter-
ances can span multiple turns, although 90% are
only one turn long.)
Because of the high class skew, we consider two
different schemes for training the classifiers, and
report different performance measures for each.
To optimize overall accuracy, we use basic un-
weighted training. To optimize average per-class
recall (weighted equally across all classes), we use
weighted training, where each training example is
weighted inversely to its class proportion. We op-
timize the regularization parameter using a source
domain development set corresponding to each
training set. Since the optimum values are close
for all three domains, we choose a single value for
all the accuracy classifiers and a single value for
all the per-class recall classifiers. (Different values
are chosen for different feature types correspond-
ing to the different adaptation methods.)
</bodyText>
<table confidence="0.9901525">
Inc. Stat. Quest. Back.
Swbd 8.1% 67.1% 5.8% 19.1%
MRDA 10.7% 67.9% 7.5% 14.0%
SpCH 5.7% 60.6% 12.1% 21.7%
</table>
<tableCaption confidence="0.8870678">
Table 1: Proportion of utterances in each
DA category (Incomplete, Statement, Question,
Backchannel) in each domain’s training set.
Table 2 gives baseline performance for all train-
test pairs, using translated versions of the test set
</tableCaption>
<bodyText confidence="0.988851714285714">
when the train set differs in language. It also lists
the in-domain results using translated (train and
test) data, and results using the adaptation methods
(which we discuss below). Figure 1 shows details
of the contribution of each class to the average per-
class recall; bar height corresponds to the second
column in Table 2.
</bodyText>
<subsectionHeader confidence="0.998854">
4.1 Baseline performance and analysis
</subsectionHeader>
<bodyText confidence="0.999993913043478">
We observe first that translation does not have a
large effect on in-domain performance; degrada-
tion occurs primarily in Incompletes and Ques-
tions, which depend most on word order and there-
fore might be most sensitive to ordering differ-
ences in the translations. We conclude that it is
possible to perform well on the translated test sets
when the training data is well matched. However,
cross-domain performance degradation is much
worse between pairs that differ in language than
between the two English corpora.
We now describe three kinds of issues contribut-
ing to cross-domain domain degradation, which
we observed anecdotally. First, some highly im-
portant words in one domain are sometimes miss-
ing entirely from another domain. This issue ap-
pears to have a dramatic effect on Backchannel
detection across languages: when optimizing for
average per-class recall, the English-trained clas-
sifiers detect about 20% of the Spanish translated
Backchannels and the Spanish classifier detects
a little over half of the English ones, while they
each detect more than 80% in their own domain.
</bodyText>
<page confidence="0.999343">
47
</page>
<table confidence="0.999729972972973">
train set Acc (%) Avg. Rec. (%)
Test on Swbd
Swbd 89.2 84.9
Swbd translated 86.7 80.4
MRDA baseline 86.4 78.0
MRDA shared only 85.7* 77.7
MRDA SCL 81.8* 69.6
MRDA length only 78.3* 51.4
SpCH baseline 74.5 57.2
SpCH shared only 77.4* 64.2
SpCH SCL 76.8* 64.8
SpCH length only 77.7* 48.2
majority 67.7 25.0
Test on MRDA
MRDA 83.8 80.5
MRDA translated 80.5 74.7
Swbd baseline 81.0 71.6
Swbd shared only 80.1* 72.1
Swbd SCL 75.6* 68.1
Swbd length only 68.6* 44.9
SpCH baseline 66.9 50.5
SpCH shared only 66.8 52.1
SpCH SCL 66.1* 58.4
SpCH length only 68.3* 44.6
majority 65.2 25.0
Test on SpCH
SpCH 83.1 72.8
SpCH translated 82.4 71.3
Swbd baseline 63.8 41.1
Swbd shared only 66.2* 50.9
Swbd SCL 68.2* 47.2
Swbd length only 72.6* 43.6
MRDA baseline 65.1 42.9
MRDA shared only 65.5 51.2
MRDA SCL 67.6* 50.9
MRDA length only 72.6* 44.7
majority 65.3 25.0
</table>
<tableCaption confidence="0.777678333333333">
Table 2: Overall accuracy and average per-class
recall on each test set, using in-domain, in-domain
translated, and cross-domain training. Starred re-
</tableCaption>
<bodyText confidence="0.991791777777778">
sults under the accuracy column are significantly
different from the corresponding cross-domain
baseline under McNemar’s test (p &lt; 0.05). (Sig-
nificance is not calculated for the average per-class
recall column.) “Majority” classifies everything as
Statement.
The reason for the cross-domain drop is that many
backchannel words in the English corpora (uhhuh,
right, yeah) do not overlap with those in the Span-
</bodyText>
<note confidence="0.423168">
Test on Swbd
</note>
<figureCaption confidence="0.934792">
Figure 1: Per-class recall of weighted classifiers
</figureCaption>
<bodyText confidence="0.990510222222222">
in column 2 of Table 2. Bar height represents
average per-class recall; colors indicate contribu-
tion of each class: I=incomplete, S=statement,
Q=question, B=backchannel. (Maximum possible
bar height is 100%, each color 25%).
ish corpora (mmm, si, ya) even after translation—
for example, “ya” becomes “already”, “si” be-
comes “yes”, “right” becomes “derecho”, and “uh-
huh”, “mmm” are unchanged.
A second issue has to do with different kinds
of utterances found in each domain, which some-
times lead to different relationships between fea-
tures and class label. This is sometimes caused
by the translation system; for example, utterances
starting with “es que ...” are usually statements
in SpCH, but without capitalization the translator
often gives “is that ...”. Since “(s)–is–that” is
a cue feature for Question in English, these utter-
ances are usually labeled as Question by the En-
glish domain classifiers. The existence of differ-
ent types of utterances can result in sets of features
that are more highly correlated in one domain than
the other. In both Swbd and translated SpCH, ut-
terances containing the trigram “(s)–but–(/s)” are
most likely to be in the Incomplete class. In Swbd,
the bigram “but–(/s)” rarely occurs outside of that
trigram, but in SpCH it sometimes occurs at the
</bodyText>
<figure confidence="0.997494464285714">
shared
shared
in−domain
in−domain trans.
baseln
SCL
length
baseln
SCL
length
I
S
Q
B
train Swbd train MRDA train SpCH
Test on MRDA
shared
shared
in−domain
in−domain trans.
SCL
length
baseln
SCL
length
baseln
train MRDA train Swbd train SpCH
Test on SpCH
train SpCH train Swbd train MRDA
80
60
40
20
0
80
60
40
20
0
in−domain
in−domain tra
ns .
baseln
shared
SCL
length
baseln
shared
SCL
length
80
60
40
20
0
ave. per−class recall (%)
</figure>
<page confidence="0.996621">
48
</page>
<bodyText confidence="0.999076555555555">
end of long (syntactically-incomplete) Statements,
so it corresponds to much lower likelihood for the
Incomplete class.
The last issue concerns utterances whose true
label probabilities given the word sequence are
not the same across domains. We distinguish two
such kinds utterances. The first are due to class
definition differences across domains and anno-
tators, e.g., long statements or questions that are
also incomplete are more often labeled Incomplete
in SpCH and Swbd than in MRDA. The second
kind are utterances whose class labels are not com-
pletely determined by their word sequence. To
minimize error rate the classifier should label an
utterance with its most frequent class, but that may
differ across domains. For example, “yes” can be
either a Statement of Backchannel; in the English
corpora, it is most likely to be a Statement (“yeah”
is more commonly used for Backchannels). How-
ever, “s´ı” is most likely to be a Backchannel in
SpCH. To measure the effect of differing label
probabilities across domains, we trained “domain-
general” classifiers using concatenated training
sets for each pair of domains. We found that they
performed about the same or only slightly worse
than domain-specific models, so we conclude that
this issue is likely only a minor effect.
</bodyText>
<subsectionHeader confidence="0.997832">
4.2 Adaptation using shared features only
</subsectionHeader>
<bodyText confidence="0.999991102941177">
In the cross-language domain pairs, some dis-
criminative features in one domain are missing
in the other. By removing all features from the
source domain training utterances that are not ob-
served (twice) in the target domain training data,
we force the classifier to learn only on features
that are present in both domains. As seen in
Figure 1, this had the effect of improving re-
call of Backchannels in the four cross-language
cases. Backchannels are the second-most frequent
class after Statements, and are typically short in
all domains. Many typical Backchannel words
are domain-specific; by removing them from the
source data, we force the classifier to attempt to
detect Backchannels based on length alone. The
resulting classifier has a better chance of recog-
nizing target domain Backchannels that lack the
source-only Backchannel words. At the same
time, it mistakes many other short utterances for
Backchannels, and does particularly worse on In-
completes, for which length is also strong cue.
Although average per-class recall improved in all
four cross-language cases, total accuracy only im-
proved significantly in two of those cases, and
for the Swbd/MRDA pair, accuracy got signifi-
cantly worse. The effect on the one-vs.-rest com-
ponent classifiers was mixed: for some (State-
ment and some Backchannel classifiers in the
cross-language cases), accuracy improved, while
in other cases it decreased.
As noted above, the shared feature approach
was investigated by Aue and Gamon (2005), who
argued that its success depends on the assump-
tion that class/feature relationships be the same
across domains. However, we argue here that the
success of this method requires stronger assump-
tions about both the relationship between domains
and the correlations between domain-specific and
shared features. Consider learning a linear model
on either the full source domain feature set or the
reduced shared feature set. In general, the co-
efficients for a given feature will be different in
each model—in the reduced case, the coefficients
incorporate correlation information and label pre-
dictive information for the removed (source-only)
features. This is potentially useful on the tar-
get domain, provided that there exist analogous,
target-only features that have similar correlations
with the shared features, and similar predictive co-
efficients.
For example, consider the discriminative source
and target features “uhhuh” and “mmm,” which
are both are correlated with a shared, noisier, fea-
ture (length). Forcing the model to learn only on
the shared, noisy feature incorporates correlation
information about “uhhuh”, which is similar to
that of “mmm”. Thus, the reduced model is poten-
tially more useful on the target domain, compared
to the full source domain model which might not
put weight on the noisy feature. On the other hand,
the approach is inappropriate in several other sce-
narios. For one, if the target domain utterances
actually represent samples from a subspace of the
source domain, the absence of features is informa-
tive: the fact that an utterance does not contain
“(s)–verdad–(/s)”, for instance, might mean that
it is less likely to be a Question, even if none of
the target domain utterances contain this feature.
</bodyText>
<subsectionHeader confidence="0.998296">
4.3 Adaptation using SCL
</subsectionHeader>
<bodyText confidence="0.999935666666667">
The original formulation of SCL proposed predict-
ing pivot features using the entire feature set, ex-
cept for those features perfectly correlated with
</bodyText>
<page confidence="0.998754">
49
</page>
<bodyText confidence="0.998317509803922">
the pivots (e.g., the pivots themselves). Our ex-
periments with this approach found it unsuitable
for our task, since even after removing the pivots
there are many features which remain highly cor-
related with the pivots due to overlapping n-grams
(i-love vs. love). The number of features that over-
lap with pivots is large, so removing these would
lead to few features being included in the projec-
tions. Therefore, we adopted the multi-view learn-
ing approach suggested by Blitzer et al. (2009).
We split the utterances into two parts; pivot fea-
tures in the first part were predicted with all the
features in the second, and vice versa. We experi-
mented with splitting the utterances in the middle,
but found that since the number of words in the
first part (nearly) predicts the number in the sec-
ond part, all of the features in the first part were
positively predictive of pivots in the second part
so the main dimension learned was length. In the
results presented here, the first part consists of the
first word only, and the second part is the rest of
the utterance. (All utterances in our experiments
have at least one word.) Pivot features are selected
in each part and predicted using a least-squares
linear regression on all features in the other part.
We used the SCL-MI method of Blitzer et al.
(2007) to select pivot features, which requires that
they be common in both domains and have high
mutual information (MI) with the class (according
to the source labels.) We selected features that oc-
curred at least 10 times in each domain and were
in the top 500 ranked MI features for any of the
four classes; this resulted in 78-99 first-part piv-
ots and 787-910 second-part pivots (depending on
the source-target pair). We performed SVD on
the learned prediction weights for each part sep-
arately, and the top (at most) 100 dimensions were
used to project utterances on each side.
In all train-test pairs, the first dimension of the
first part appeared to distinguish short utterance
words from long ones. Such short-utterance words
included backchannels from both domains, in ad-
dition to acknowledgments, exclamations, swear
words and greetings. An analogous dimension ex-
isted in the second part, which captured words cor-
related with short utterances greater than one word
(right, really, interesting). The other dimensions of
both domains were difficult to interpret.
We experimented with using the SCL fea-
tures together with the raw features (n-grams and
length), as suggested by (Blitzer et al., 2006). As
in (Blitzer et al., 2006), we found it necessary to
scale up the SCL features to increase their utiliza-
tion in the presence of the raw features; however,
it was difficult to guess the optimal scaling factor
without having access to labeled target data. The
results here use SCL features only, which also al-
lows us to more clearly investigate the utility of
those features and to compare them with the other
feature sets.
The most notable effect was an improvement
in Backchannel recall, which occurred under both
weighted and unweighted training. In addition,
there was high confusability between Statements
and the other classes, and more false detections
of Backchannels. When optimizing for accuracy,
SCL led to an improvement in accuracy in three
of the four cross-language cases. When optimiz-
ing for average per-class recall, it led to improve-
ment in all cross-language cases; however, re-
call of Statements went down dramatically in all
cases. In addition, while there was no clear ben-
efit of the SCL vs. the shared-feature method on
the cross-language cases, the SCL approach did
much worse than the shared-feature approach on
the Swbd/MRDA pair, causing large degradation
from the baseline.
As we have noted, utterance length appears
to underlie the improvement seen in the cross-
language performance for both the SCL and
shared-feature approaches. Therefore, we include
results for a classifier based only on the length
feature. Optimizing for accuracy, this method
achieves the highest accuracy of all methods in
the cross-language pairs. (It does so by classifying
everything as Statement or Backchannel, although
with weighted training, as shown in Figure 1, it
gets some Incompletes.) However, under weighted
class training, the average per-class recall of this
method is much worse than the shared-feature and
SCL approaches.
Comparison with other SCL tasks Although
we basically take a text classification approach to
the problem of dialog act tagging, our problem dif-
fers in several ways from the sentiment classifi-
cation task in Blitzer et al. (2007). In particular,
utterances are much shorter than documents, and
we use position information via the start/end-of-
sentence tags. Some important DA cue features
(such as the value of the first word) are mutually
exclusive rather than correlated. In this way our
problem resembles the part-of-speech tagging task
</bodyText>
<page confidence="0.990157">
50
</page>
<bodyText confidence="0.997414631578947">
(Blitzer et al., 2006), where the category of each
word is predicted using values of the left, right,
and current word token. In fact, that work used
a kind of multi-view learning for the SCL projec-
tion, with three views corresponding to the three
word categories. However, our problem essen-
tially uses a mix of bag-of-words and position-
based features, which poses a greater challenge
since there is no natural multi-view split. The ap-
proach described here suffers from the fact that it
cannot use all the features available to the base-
line classifier—bigrams and trigrams spanning the
first and second words are left out. It also suffers
from the fact that the first-word pivot feature set is
extremely small—a consequence of the small set
of first words that occur at least 10 times in the
29k-utterance corpora.
act information, in that in-domain performance is
similar on original and translated text.
</bodyText>
<sectionHeader confidence="0.97932" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999679307692308">
We thank Sham Kakade for suggesting the multi-
view SCL method based on utterance splits and
for many other helpful discussions, as well as John
Blitzer for helpful discussions. We thank the three
reviewers for their useful comments.
This research was funded by the Office of
the Director of National Intelligence (ODNI), In-
telligence Advanced Research Projects Activity
(IARPA). All statements of fact, opinion or con-
clusions contained herein are those of the authors
and should not be construed as representing the of-
ficial views or policies of IARPA, the ODNI or the
U.S. Government.
</bodyText>
<sectionHeader confidence="0.999735" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.99999490625">
We have considered two approaches for domain
adaptation for DA tagging, and analyzed their
performance for source/target pairs drawn from
three different domains. For the English domains,
the baseline cross-domain performance was quite
good, and both adaptation methods generally led
to degradation over the baseline. For the cross-
language cases, both methods were effective at im-
proving average per-class recall, and particularly
Backchannel recall. SCL led to significant accu-
racy improvement in three cases, while the shared
feature approach did so in two cases. On the
other hand, SCL showed poor discrimination be-
tween Statements and other classes, and did worse
on the same-language pair that had little cross-
domain degradation. Both methods work by tak-
ing advantage of correlations between shared and
domain-specific class-discriminative features. Un-
fortunately in our task, membership in the rare
classes is often cued by features that are mutually
exclusive, e.g., the starting n-gram for Questions.
Both methods might therefore benefit from addi-
tional shared features that are correlated with these
n-grams, e.g., sentence-final intonation for Ques-
tions. (Indeed, other work on semi-supervised
DA tagging has used a richer feature set: Jeong
et al. (2009) included parse, part-of-speech, and
speaker sequence information, and Venkataraman
et al. (2003) used prosodic information, plus a
sequence-modeling framework.) From the task
perspective, an interesting result is that machine
translation appears to preserve most of the dialog-
</bodyText>
<sectionHeader confidence="0.99882" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995466205882353">
Anthony Aue and Michael Gamon. 2005. Customiz-
ing sentiment classifiers to new domains: a case
study. In Proc. International Conference on Recent
Advances in NLP.
Nuria Bel, Cornelis H. A. Koster, and Marta Ville-
gas. 2003. Cross-lingual text categorization. In
Research and Advanced Technology for Digital Li-
braries, pages 126–139. Springer Berlin / Heidel-
berg.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proc. of the 2006 Conference on
Empirical Methods in Natural Language Process-
ing, pages 120–128.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, Bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 440–447.
John Blitzer, Dean P. Foster, and Sham M. Kakade.
2009. Zero-shot domain adaptation: A multi-view
approach. Technical report, Toyota Technological
Institute TTI-TR-2009-1.
Mark G. Core and James F. Allen. 1997. Coding di-
alogs with the DAMSL annotation scheme. In Proc.
ofthe Working Notes of the AAAI Fall Symposium on
Communicative Action in Humans and Machines.
Mark Dredze, John Blitzer, Partha Pratim Taluk-
dar, Kuzman Ganchev, Jo˜ao Graca, and Fernando
Pereira. 2007. Frustratingly hard domain adapta-
tion for dependency parsing. In Proc. of the CoNLL
Shared Task Session ofEMNLP-CoNLL 2007, pages
1051–1055.
</reference>
<page confidence="0.995489">
51
</page>
<reference confidence="0.979326321839081">
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal ofMa-
chine Learning Research, 9:1871–1874.
Umit Guz, Gokhan Tur, Dilek Hakkani-T¨ur, and
S´ebastien Cuendet. 2010. Cascaded model adapta-
tion for dialog act segmentation and tagging. Com-
puter Speech &amp; Language, 24(2):289–306.
Jiayuan Huang, Alexander J. Smola, Arthur Gretton,
Karsten M. Borgwardt, and Bernhard Sch¨olkopf.
2007. Correcting sample selection bias by unlabeled
data. In Advances in Neural Information Processing
Systems 19, pages 601–608.
Minwoo Jeong, Chin Y. Lin, and Gary G. Lee. 2009.
Semi-supervised speech act recognition in emails
and forums. In Proc. of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1250–1259.
Dan Jurafsky, Liz Shriberg, and Debra Biasca. 1997.
Switchboard SWBD-DAMSL shallow-discourse-
function annotation coders manual, draft 13. Tech-
nical report, University of Colorado at Boulder
Technical Report 97-02.
Lori Levin, Ann Thym´e-Gobbel, Alon Lavie, Klaus
Ries, and Klaus Zechner. 1998. A discourse cod-
ing scheme for conversational Spanish. In Proc. The
5th International Conference on Spoken Language
Processing, pages 2335–2338.
Yang Liu. 2006. Using SVM and error-correcting
codes for multiclass dialog act classification in meet-
ing corpus. In Proc. Interspeech, pages 1938–1941.
Sinno J. Pan, James T. Kwok, and Qiang Yang. 2008.
Transfer learning via dimensionality reduction. In
Proc. of the Twenty-Third AAAI Conference on Arti-
ficial Intelligence.
Brian Roark and Michiel Bacchiani. 2003. Supervised
and unsupervised PCFG adaptation to novel do-
mains. In Proc. of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
pages 126–133.
Sophie Rosset, Delphine Tribout, and Lori Lamel.
2008. Multi-level information and automatic dia-
log act detection in human–human spoken dialogs.
Speech Communication, 50(1):1–13.
Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy
Ang, and Hannah Carvey. 2004. The ICSI meeting
recorder dialog act (MRDA) corpus. In Proc. of the
5th SIGdial Workshop on Discourse and Dialogue,
pages 97–100.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26:339–373.
Dinoj Surendran and Gina-Anne Levow. 2006. Dialog
act tagging with support vector machines and hidden
Markov models. In Proc. Interspeech, pages 1950–
1953.
Gokhan Tur, Dilek Hakkani-T¨ur, and Robert E.
Schapire. 2005. Combining active and semi-
supervised learning for spoken language under-
standing. Speech Communication, 45(2):171–186.
Gokhan Tur, Umit Guz, and Dilek Hakkani-T¨ur.
2007. Model adaptation for dialog act tagging.
In Proc. IEEE Spoken Language Technology Work-
shop, pages 94–97.
Gokhan Tur. 2005. Model adaptation for spoken lan-
guage understanding. In Proc. IEEE International
Conference on Acoustics, Speech, and Signal Pro-
cessing, pages 41–44.
Anand Venkataraman, Luciana Ferrer, Andreas Stol-
cke, and Elizabeth Shriberg. 2003. Training
a prosody-based dialog act tagger from unlabeled
data. In Proc. IEEE International Conference on
Acoustics, Speech, and Signal Processing, volume I,
pages 272–275.
Nick Webb and Ting Liu. 2008. Investigating the
portability of corpus-derived cue phrases for dia-
logue act classification. In Proc. of the 22nd Inter-
national Conference on Computational Linguistics
(Coling 2008), pages 977–984.
Dikan Xing, Wenyuan Dai, Gui-Rong Xue, and Yong
Yu. 2007. Bridged refinement for transfer learn-
ing. In Knowledge Discovery in Databases: PKDD
2007, pages 324–335. Springer Berlin / Heidelberg.
</reference>
<page confidence="0.998856">
52
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<abstract confidence="0.988382356435643">translations of the Spanish SpCH utterances, and to derive Spanish translations of the English Swbd and MRDA utterances. Of course, translations are far from perfect; DA classification performance could likely be improved by using a translation system trained on spoken dialog. For instance, Google Translate often failed on certain words like “i” that are usually capitalized in text. Even so, when training and testing on translated utterances, the results with the generic system are surprisingly good. The results reported below used the standard train/test splits provided with the corpora: MRDA had 51 train meetings/11 test; Swbd had 1115 train conversations/19 test; SpCH had 80 train conversations/20 test. The SpCH train set is the smallest at 29k utterances. To avoid issues of differing train set size when comparing performance of different models, we reduced the Swbd and MRDA train sets to the same size as SpCH using randomly selected examples from the full train sets. For each adaptation experiment, we used the target domain training set as the unlabeled data, and report performance on the target domain test set. The test sets contain 4525, 15180, and 3715 utterances for Swbd, MRDA, and SpCH respectively. 4 Results Table 1 shows the class proportions in the training sets for each domain. MRDA has fewer Backchannels than the the others, which is expected since the meetings are face-to-face. SpCH has fewer Incompletes and more Questions than the others; the reasons for this are unclear. Backchannels have the shortest mean length (less than 2 words) in all domains. Incompletes are also short, while Statements have the longest mean length. The mean lengths of Statements and Questions are similar in the English corpora, but are shorter in SpCH. (This may point to differences in how the utterances were segmented; for instance Swbd utterances can span multiple turns, although 90% are only one turn long.) Because of the high class skew, we consider two different schemes for training the classifiers, and report different performance measures for each. To optimize overall accuracy, we use basic unweighted training. To optimize average per-class recall (weighted equally across all classes), we use weighted training, where each training example is weighted inversely to its class proportion. We optimize the regularization parameter using a source domain development set corresponding to each training set. Since the optimum values are close for all three domains, we choose a single value for all the accuracy classifiers and a single value for all the per-class recall classifiers. (Different values are chosen for different feature types corresponding to the different adaptation methods.) Inc. Stat. Quest. Back. Swbd 8.1% 67.1% 5.8% 19.1% MRDA 10.7% 67.9% 7.5% 14.0% SpCH 5.7% 60.6% 12.1% 21.7% Table 1: Proportion of utterances in each DA category (Incomplete, Statement, Question, Backchannel) in each domain’s training set. Table 2 gives baseline performance for all traintest pairs, using translated versions of the test set when the train set differs in language. It also lists the in-domain results using translated (train and test) data, and results using the adaptation methods (which we discuss below). Figure 1 shows details of the contribution of each class to the average perclass recall; bar height corresponds to the second column in Table 2. 4.1 Baseline performance and analysis We observe first that translation does not have a large effect on in-domain performance; degradation occurs primarily in Incompletes and Questions, which depend most on word order and therefore might be most sensitive to ordering differences in the translations. We conclude that it is possible to perform well on the translated test sets when the training data is well matched. However, cross-domain performance degradation is much worse between pairs that differ in language than between the two English corpora. We now describe three kinds of issues contributing to cross-domain domain degradation, which we observed anecdotally. First, some highly important words in one domain are sometimes missing entirely from another domain. This issue appears to have a dramatic effect on Backchannel detection across languages: when optimizing for average per-class recall, the English-trained classifiers detect about 20% of the Spanish translated Backchannels and the Spanish classifier detects a little over half of the English ones, while they each detect more than 80% in their own domain. 47 train set Acc (%) Avg. Rec. (%) Test on Swbd</abstract>
<note confidence="0.9616379">Swbd 89.2 84.9 Swbd translated 86.7 80.4 MRDA baseline 86.4 78.0 MRDA shared only 85.7* 77.7 MRDA SCL 81.8* 69.6 MRDA length only 78.3* 51.4 SpCH baseline 74.5 57.2 SpCH shared only 77.4* 64.2 SpCH SCL 76.8* 64.8 SpCH length only 48.2 majority 67.7 25.0 Test on MRDA MRDA 83.8 80.5 MRDA translated 80.5 74.7 Swbd baseline 81.0 71.6 Swbd shared only 80.1* 72.1 Swbd SCL 75.6* 68.1 Swbd length only 68.6* 44.9 SpCH baseline 66.9 50.5 SpCH shared only 66.8 52.1 SpCH SCL 66.1* 58.4 SpCH length only 44.6 majority 65.2 25.0 Test on SpCH SpCH 83.1 72.8 SpCH translated 82.4 71.3 Swbd baseline 63.8 41.1 Swbd shared only 66.2* 50.9 Swbd SCL 68.2* 47.2 Swbd length only 43.6</note>
<abstract confidence="0.95682364">MRDA baseline 65.1 42.9 MRDA shared only 65.5 51.2 MRDA SCL 67.6* 50.9 MRDA length only 44.7 majority 65.3 25.0 Table 2: Overall accuracy and average per-class recall on each test set, using in-domain, in-domain translated, and cross-domain training. Starred results under the accuracy column are significantly different from the corresponding cross-domain under McNemar’s test &lt; (Significance is not calculated for the average per-class recall column.) “Majority” classifies everything as Statement. The reason for the cross-domain drop is that many backchannel words in the English corpora (uhhuh, yeah) do not overlap with those in the Span- Test on Swbd Figure 1: Per-class recall of weighted classifiers in column 2 of Table 2. Bar height represents average per-class recall; colors indicate contribution of each class: I=incomplete, S=statement, Q=question, B=backchannel. (Maximum possible bar height is 100%, each color 25%). ish corpora (mmm, si, ya) even after translation— for example, “ya” becomes “already”, “si” becomes “yes”, “right” becomes “derecho”, and “uhhuh”, “mmm” are unchanged. A second issue has to do with different kinds of utterances found in each domain, which sometimes lead to different relationships between features and class label. This is sometimes caused by the translation system; for example, utterances starting with “es que ...” are usually statements in SpCH, but without capitalization the translator gives “is that ...”. Since is a cue feature for Question in English, these utterances are usually labeled as Question by the English domain classifiers. The existence of different types of utterances can result in sets of features that are more highly correlated in one domain than the other. In both Swbd and translated SpCH, utcontaining the trigram are most likely to be in the Incomplete class. In Swbd, bigram rarely occurs outside of that trigram, but in SpCH it sometimes occurs at the shared shared in−domain in−domain trans. baseln SCL length baseln SCL length I S Q B train Swbd train MRDA train SpCH Test on MRDA shared shared in−domain in−domain trans. SCL length baseln SCL length baseln train MRDA train Swbd train SpCH Test on SpCH train SpCH train Swbd train MRDA</abstract>
<note confidence="0.9051845">80 60 40 20 0 80 60 40 20 0</note>
<abstract confidence="0.967441520134228">in−domain in−domain tra ns . baseln shared SCL length baseln shared SCL length 80 60 40 20 0 ave. per−class recall (%) 48 end of long (syntactically-incomplete) Statements, so it corresponds to much lower likelihood for the Incomplete class. The last issue concerns utterances whose true label probabilities given the word sequence are not the same across domains. We distinguish two such kinds utterances. The first are due to class definition differences across domains and annotators, e.g., long statements or questions that are also incomplete are more often labeled Incomplete in SpCH and Swbd than in MRDA. The second kind are utterances whose class labels are not completely determined by their word sequence. To minimize error rate the classifier should label an utterance with its most frequent class, but that may differ across domains. For example, “yes” can be either a Statement of Backchannel; in the English corpora, it is most likely to be a Statement (“yeah” is more commonly used for Backchannels). However, “s´ı” is most likely to be a Backchannel in SpCH. To measure the effect of differing label probabilities across domains, we trained “domaingeneral” classifiers using concatenated training sets for each pair of domains. We found that they performed about the same or only slightly worse than domain-specific models, so we conclude that this issue is likely only a minor effect. 4.2 Adaptation using shared features only In the cross-language domain pairs, some discriminative features in one domain are missing in the other. By removing all features from the source domain training utterances that are not observed (twice) in the target domain training data, we force the classifier to learn only on features that are present in both domains. As seen in Figure 1, this had the effect of improving recall of Backchannels in the four cross-language cases. Backchannels are the second-most frequent class after Statements, and are typically short in all domains. Many typical Backchannel words are domain-specific; by removing them from the source data, we force the classifier to attempt to detect Backchannels based on length alone. The resulting classifier has a better chance of recognizing target domain Backchannels that lack the source-only Backchannel words. At the same time, it mistakes many other short utterances for Backchannels, and does particularly worse on Incompletes, for which length is also strong cue. Although average per-class recall improved in all four cross-language cases, total accuracy only improved significantly in two of those cases, and for the Swbd/MRDA pair, accuracy got significantly worse. The effect on the one-vs.-rest component classifiers was mixed: for some (Statement and some Backchannel classifiers in the cross-language cases), accuracy improved, while in other cases it decreased. As noted above, the shared feature approach was investigated by Aue and Gamon (2005), who argued that its success depends on the assumption that class/feature relationships be the same across domains. However, we argue here that the success of this method requires stronger assumptions about both the relationship between domains and the correlations between domain-specific and shared features. Consider learning a linear model on either the full source domain feature set or the reduced shared feature set. In general, the coefficients for a given feature will be different in each model—in the reduced case, the coefficients incorporate correlation information and label predictive information for the removed (source-only) features. This is potentially useful on the target domain, provided that there exist analogous, target-only features that have similar correlations with the shared features, and similar predictive coefficients. For example, consider the discriminative source and target features “uhhuh” and “mmm,” which are both are correlated with a shared, noisier, feature (length). Forcing the model to learn only on the shared, noisy feature incorporates correlation information about “uhhuh”, which is similar to that of “mmm”. Thus, the reduced model is potentially more useful on the target domain, compared to the full source domain model which might not put weight on the noisy feature. On the other hand, the approach is inappropriate in several other scenarios. For one, if the target domain utterances actually represent samples from a subspace of the source domain, the absence of features is informative: the fact that an utterance does not contain for instance, might mean that it is less likely to be a Question, even if none of the target domain utterances contain this feature. 4.3 Adaptation using SCL The original formulation of SCL proposed predicting pivot features using the entire feature set, except for those features perfectly correlated with 49 the pivots (e.g., the pivots themselves). Our experiments with this approach found it unsuitable for our task, since even after removing the pivots there are many features which remain highly correlated with the pivots due to overlapping n-grams (i-love vs. love). The number of features that overlap with pivots is large, so removing these would lead to few features being included in the projections. Therefore, we adopted the multi-view learning approach suggested by Blitzer et al. (2009). We split the utterances into two parts; pivot features in the first part were predicted with all the features in the second, and vice versa. We experimented with splitting the utterances in the middle, but found that since the number of words in the first part (nearly) predicts the number in the second part, all of the features in the first part were positively predictive of pivots in the second part so the main dimension learned was length. In the results presented here, the first part consists of the first word only, and the second part is the rest of the utterance. (All utterances in our experiments have at least one word.) Pivot features are selected in each part and predicted using a least-squares linear regression on all features in the other part. We used the SCL-MI method of Blitzer et al. (2007) to select pivot features, which requires that they be common in both domains and have high mutual information (MI) with the class (according to the source labels.) We selected features that occurred at least 10 times in each domain and were in the top 500 ranked MI features for any of the four classes; this resulted in 78-99 first-part pivots and 787-910 second-part pivots (depending on the source-target pair). We performed SVD on the learned prediction weights for each part separately, and the top (at most) 100 dimensions were used to project utterances on each side. In all train-test pairs, the first dimension of the first part appeared to distinguish short utterance words from long ones. Such short-utterance words included backchannels from both domains, in addition to acknowledgments, exclamations, swear words and greetings. An analogous dimension existed in the second part, which captured words correlated with short utterances greater than one word (right, really, interesting). The other dimensions of both domains were difficult to interpret. We experimented with using the SCL features together with the raw features (n-grams and length), as suggested by (Blitzer et al., 2006). As in (Blitzer et al., 2006), we found it necessary to scale up the SCL features to increase their utilization in the presence of the raw features; however, it was difficult to guess the optimal scaling factor without having access to labeled target data. The results here use SCL features only, which also allows us to more clearly investigate the utility of those features and to compare them with the other feature sets. The most notable effect was an improvement in Backchannel recall, which occurred under both weighted and unweighted training. In addition, there was high confusability between Statements and the other classes, and more false detections of Backchannels. When optimizing for accuracy, SCL led to an improvement in accuracy in three of the four cross-language cases. When optimizing for average per-class recall, it led to improvement in all cross-language cases; however, recall of Statements went down dramatically in all cases. In addition, while there was no clear benefit of the SCL vs. the shared-feature method on the cross-language cases, the SCL approach did much worse than the shared-feature approach on the Swbd/MRDA pair, causing large degradation from the baseline. As we have noted, utterance length appears to underlie the improvement seen in the crosslanguage performance for both the SCL and shared-feature approaches. Therefore, we include results for a classifier based only on the length feature. Optimizing for accuracy, this method achieves the highest accuracy of all methods in the cross-language pairs. (It does so by classifying everything as Statement or Backchannel, although with weighted training, as shown in Figure 1, it gets some Incompletes.) However, under weighted class training, the average per-class recall of this method is much worse than the shared-feature and SCL approaches. with other SCL tasks we basically take a text classification approach to the problem of dialog act tagging, our problem differs in several ways from the sentiment classification task in Blitzer et al. (2007). In particular, utterances are much shorter than documents, and we use position information via the start/end-ofsentence tags. Some important DA cue features (such as the value of the first word) are mutually exclusive rather than correlated. In this way our problem resembles the part-of-speech tagging task 50 (Blitzer et al., 2006), where the category of each word is predicted using values of the left, right, and current word token. In fact, that work used a kind of multi-view learning for the SCL projection, with three views corresponding to the three word categories. However, our problem essentially uses a mix of bag-of-words and positionbased features, which poses a greater challenge since there is no natural multi-view split. The approach described here suffers from the fact that it cannot use all the features available to the baseline classifier—bigrams and trigrams spanning the first and second words are left out. It also suffers from the fact that the first-word pivot feature set is extremely small—a consequence of the small set of first words that occur at least 10 times in the 29k-utterance corpora. act information, in that in-domain performance is similar on original and translated text. Acknowledgments We thank Sham Kakade for suggesting the multiview SCL method based on utterance splits and for many other helpful discussions, as well as John Blitzer for helpful discussions. We thank the three reviewers for their useful comments. This research was funded by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA). All statements of fact, opinion or conclusions contained herein are those of the authors and should not be construed as representing the official views or policies of IARPA, the ODNI or the U.S. Government. 5 Conclusions We have considered two approaches for domain adaptation for DA tagging, and analyzed their performance for source/target pairs drawn from three different domains. For the English domains, the baseline cross-domain performance was quite good, and both adaptation methods generally led to degradation over the baseline. For the crosslanguage cases, both methods were effective at improving average per-class recall, and particularly Backchannel recall. SCL led to significant accuracy improvement in three cases, while the shared feature approach did so in two cases. On the other hand, SCL showed poor discrimination between Statements and other classes, and did worse on the same-language pair that had little crossdomain degradation. Both methods work by taking advantage of correlations between shared and domain-specific class-discriminative features. Unfortunately in our task, membership in the rare classes is often cued by features that are mutually exclusive, e.g., the starting n-gram for Questions. Both methods might therefore benefit from additional shared features that are correlated with these n-grams, e.g., sentence-final intonation for Questions. (Indeed, other work on semi-supervised DA tagging has used a richer feature set: Jeong et al. (2009) included parse, part-of-speech, and speaker sequence information, and Venkataraman et al. (2003) used prosodic information, plus a sequence-modeling framework.) From the task perspective, an interesting result is that machine appears to preserve most of the dialog- References Anthony Aue and Michael Gamon. 2005. Customizing sentiment classifiers to new domains: a case In International Conference on Recent in Nuria Bel, Cornelis H. A. Koster, and Marta Villegas. 2003. Cross-lingual text categorization. In Research and Advanced Technology for Digital Lipages 126–139. Springer Berlin / Heidelberg.</abstract>
<note confidence="0.784741128571428">John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural corresponlearning. In of the 2006 Conference on Empirical Methods in Natural Language Processpages 120–128. John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classi- In of the 45th Annual Meetof the Association of Computational pages 440–447. John Blitzer, Dean P. Foster, and Sham M. Kakade. 2009. Zero-shot domain adaptation: A multi-view approach. Technical report, Toyota Technological Institute TTI-TR-2009-1. Mark G. Core and James F. Allen. 1997. Coding diwith the DAMSL annotation scheme. In ofthe Working Notes of the AAAI Fall Symposium on Action in Humans and Mark Dredze, John Blitzer, Partha Pratim Taluk- Kuzman Ganchev, Graca, and Fernando Pereira. 2007. Frustratingly hard domain adaptafor dependency parsing. In of the CoNLL Task Session ofEMNLP-CoNLL pages 1051–1055. 51 Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang- Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A for large linear classification. ofMa- Learning 9:1871–1874. Umit Guz, Gokhan Tur, Dilek Hakkani-T¨ur, and S´ebastien Cuendet. 2010. Cascaded model adaptafor dialog act segmentation and tagging. Com- Speech &amp; 24(2):289–306. Jiayuan Huang, Alexander J. Smola, Arthur Gretton, Karsten M. Borgwardt, and Bernhard Sch¨olkopf. 2007. Correcting sample selection bias by unlabeled In in Neural Information Processing pages 601–608. Minwoo Jeong, Chin Y. Lin, and Gary G. Lee. 2009. Semi-supervised speech act recognition in emails forums. In of the 2009 Conference on Empirical Methods in Natural Language Processpages 1250–1259. Dan Jurafsky, Liz Shriberg, and Debra Biasca. 1997. Switchboard SWBD-DAMSL shallow-discoursefunction annotation coders manual, draft 13. Technical report, University of Colorado at Boulder Technical Report 97-02. Lori Levin, Ann Thym´e-Gobbel, Alon Lavie, Klaus Ries, and Klaus Zechner. 1998. A discourse codscheme for conversational Spanish. In The 5th International Conference on Spoken Language pages 2335–2338. Yang Liu. 2006. Using SVM and error-correcting codes for multiclass dialog act classification in meetcorpus. In pages 1938–1941. Sinno J. Pan, James T. Kwok, and Qiang Yang. 2008. Transfer learning via dimensionality reduction. In Proc. of the Twenty-Third AAAI Conference on Arti- Brian Roark and Michiel Bacchiani. 2003. Supervised and unsupervised PCFG adaptation to novel do- In of the 2003 Conference of the North American Chapter of the Association for Computa- Linguistics on Human Language pages 126–133. Sophie Rosset, Delphine Tribout, and Lori Lamel. 2008. Multi-level information and automatic dialog act detection in human–human spoken dialogs. 50(1):1–13.</note>
<author confidence="0.410943">Elizabeth Shriberg</author>
<author confidence="0.410943">Raj Dhillon</author>
<author confidence="0.410943">Sonali Bhagat</author>
<author confidence="0.410943">Jeremy</author>
<abstract confidence="0.683461">Ang, and Hannah Carvey. 2004. The ICSI meeting dialog act (MRDA) corpus. In of the SIGdial Workshop on Discourse and pages 97–100.</abstract>
<author confidence="0.69122825">Dialogue act modeling for</author>
<abstract confidence="0.944019833333333">automatic tagging and recognition of conversational 26:339–373. Dinoj Surendran and Gina-Anne Levow. 2006. Dialog act tagging with support vector machines and hidden models. In pages 1950– 1953.</abstract>
<note confidence="0.715420037037037">Gokhan Tur, Dilek Hakkani-T¨ur, and Robert E. Schapire. 2005. Combining active and semisupervised learning for spoken language under- 45(2):171–186. Gokhan Tur, Umit Guz, and Dilek Hakkani-T¨ur. 2007. Model adaptation for dialog act tagging. IEEE Spoken Language Technology Workpages 94–97. Gokhan Tur. 2005. Model adaptation for spoken lanunderstanding. In IEEE International Conference on Acoustics, Speech, and Signal Propages 41–44. Anand Venkataraman, Luciana Ferrer, Andreas Stolcke, and Elizabeth Shriberg. 2003. Training a prosody-based dialog act tagger from unlabeled In IEEE International Conference on Speech, and Signal volume I, pages 272–275. Nick Webb and Ting Liu. 2008. Investigating the portability of corpus-derived cue phrases for diaact classification. In of the 22nd International Conference on Computational Linguistics pages 977–984. Dikan Xing, Wenyuan Dai, Gui-Rong Xue, and Yong Yu. 2007. Bridged refinement for transfer learn- In Discovery in Databases: PKDD pages 324–335. Springer Berlin / Heidelberg.</note>
<intro confidence="0.602541">52</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anthony Aue</author>
<author>Michael Gamon</author>
</authors>
<title>Customizing sentiment classifiers to new domains: a case study.</title>
<date>2005</date>
<booktitle>In Proc. International Conference on Recent Advances in NLP.</booktitle>
<contexts>
<context position="10530" citStr="Aue and Gamon (2005)" startWordPosition="1700" endWordPosition="1703">es many other short utterances for Backchannels, and does particularly worse on Incompletes, for which length is also strong cue. Although average per-class recall improved in all four cross-language cases, total accuracy only improved significantly in two of those cases, and for the Swbd/MRDA pair, accuracy got significantly worse. The effect on the one-vs.-rest component classifiers was mixed: for some (Statement and some Backchannel classifiers in the cross-language cases), accuracy improved, while in other cases it decreased. As noted above, the shared feature approach was investigated by Aue and Gamon (2005), who argued that its success depends on the assumption that class/feature relationships be the same across domains. However, we argue here that the success of this method requires stronger assumptions about both the relationship between domains and the correlations between domain-specific and shared features. Consider learning a linear model on either the full source domain feature set or the reduced shared feature set. In general, the coefficients for a given feature will be different in each model—in the reduced case, the coefficients incorporate correlation information and label predictive</context>
</contexts>
<marker>Aue, Gamon, 2005</marker>
<rawString>Anthony Aue and Michael Gamon. 2005. Customizing sentiment classifiers to new domains: a case study. In Proc. International Conference on Recent Advances in NLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nuria Bel</author>
<author>Cornelis H A Koster</author>
<author>Marta Villegas</author>
</authors>
<title>Cross-lingual text categorization.</title>
<date>2003</date>
<booktitle>In Research and Advanced Technology for Digital Libraries,</booktitle>
<pages>126--139</pages>
<publisher>Springer</publisher>
<location>Berlin / Heidelberg.</location>
<marker>Bel, Koster, Villegas, 2003</marker>
<rawString>Nuria Bel, Cornelis H. A. Koster, and Marta Villegas. 2003. Cross-lingual text categorization. In Research and Advanced Technology for Digital Libraries, pages 126–139. Springer Berlin / Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Proc. of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>120--128</pages>
<contexts>
<context position="14959" citStr="Blitzer et al., 2006" startWordPosition="2423" endWordPosition="2426">all train-test pairs, the first dimension of the first part appeared to distinguish short utterance words from long ones. Such short-utterance words included backchannels from both domains, in addition to acknowledgments, exclamations, swear words and greetings. An analogous dimension existed in the second part, which captured words correlated with short utterances greater than one word (right, really, interesting). The other dimensions of both domains were difficult to interpret. We experimented with using the SCL features together with the raw features (n-grams and length), as suggested by (Blitzer et al., 2006). As in (Blitzer et al., 2006), we found it necessary to scale up the SCL features to increase their utilization in the presence of the raw features; however, it was difficult to guess the optimal scaling factor without having access to labeled target data. The results here use SCL features only, which also allows us to more clearly investigate the utility of those features and to compare them with the other feature sets. The most notable effect was an improvement in Backchannel recall, which occurred under both weighted and unweighted training. In addition, there was high confusability betwee</context>
<context position="17364" citStr="Blitzer et al., 2006" startWordPosition="2806" endWordPosition="2809"> is much worse than the shared-feature and SCL approaches. Comparison with other SCL tasks Although we basically take a text classification approach to the problem of dialog act tagging, our problem differs in several ways from the sentiment classification task in Blitzer et al. (2007). In particular, utterances are much shorter than documents, and we use position information via the start/end-ofsentence tags. Some important DA cue features (such as the value of the first word) are mutually exclusive rather than correlated. In this way our problem resembles the part-of-speech tagging task 50 (Blitzer et al., 2006), where the category of each word is predicted using values of the left, right, and current word token. In fact, that work used a kind of multi-view learning for the SCL projection, with three views corresponding to the three word categories. However, our problem essentially uses a mix of bag-of-words and positionbased features, which poses a greater challenge since there is no natural multi-view split. The approach described here suffers from the fact that it cannot use all the features available to the baseline classifier—bigrams and trigrams spanning the first and second words are left out.</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proc. of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 120–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>440--447</pages>
<contexts>
<context position="13759" citStr="Blitzer et al. (2007)" startWordPosition="2229" endWordPosition="2232">middle, but found that since the number of words in the first part (nearly) predicts the number in the second part, all of the features in the first part were positively predictive of pivots in the second part so the main dimension learned was length. In the results presented here, the first part consists of the first word only, and the second part is the rest of the utterance. (All utterances in our experiments have at least one word.) Pivot features are selected in each part and predicted using a least-squares linear regression on all features in the other part. We used the SCL-MI method of Blitzer et al. (2007) to select pivot features, which requires that they be common in both domains and have high mutual information (MI) with the class (according to the source labels.) We selected features that occurred at least 10 times in each domain and were in the top 500 ranked MI features for any of the four classes; this resulted in 78-99 first-part pivots and 787-910 second-part pivots (depending on the source-target pair). We performed SVD on the learned prediction weights for each part separately, and the top (at most) 100 dimensions were used to project utterances on each side. In all train-test pairs,</context>
<context position="17029" citStr="Blitzer et al. (2007)" startWordPosition="2754" endWordPosition="2757">ng for accuracy, this method achieves the highest accuracy of all methods in the cross-language pairs. (It does so by classifying everything as Statement or Backchannel, although with weighted training, as shown in Figure 1, it gets some Incompletes.) However, under weighted class training, the average per-class recall of this method is much worse than the shared-feature and SCL approaches. Comparison with other SCL tasks Although we basically take a text classification approach to the problem of dialog act tagging, our problem differs in several ways from the sentiment classification task in Blitzer et al. (2007). In particular, utterances are much shorter than documents, and we use position information via the start/end-ofsentence tags. Some important DA cue features (such as the value of the first word) are mutually exclusive rather than correlated. In this way our problem resembles the part-of-speech tagging task 50 (Blitzer et al., 2006), where the category of each word is predicted using values of the left, right, and current word token. In fact, that work used a kind of multi-view learning for the SCL projection, with three views corresponding to the three word categories. However, our problem e</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 440–447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Dean P Foster</author>
<author>Sham M Kakade</author>
</authors>
<title>Zero-shot domain adaptation: A multi-view approach.</title>
<date>2009</date>
<tech>Technical report,</tech>
<pages>2009--1</pages>
<institution>Toyota Technological Institute</institution>
<contexts>
<context position="12942" citStr="Blitzer et al. (2009)" startWordPosition="2083" endWordPosition="2086">n of SCL proposed predicting pivot features using the entire feature set, except for those features perfectly correlated with 49 the pivots (e.g., the pivots themselves). Our experiments with this approach found it unsuitable for our task, since even after removing the pivots there are many features which remain highly correlated with the pivots due to overlapping n-grams (i-love vs. love). The number of features that overlap with pivots is large, so removing these would lead to few features being included in the projections. Therefore, we adopted the multi-view learning approach suggested by Blitzer et al. (2009). We split the utterances into two parts; pivot features in the first part were predicted with all the features in the second, and vice versa. We experimented with splitting the utterances in the middle, but found that since the number of words in the first part (nearly) predicts the number in the second part, all of the features in the first part were positively predictive of pivots in the second part so the main dimension learned was length. In the results presented here, the first part consists of the first word only, and the second part is the rest of the utterance. (All utterances in our </context>
</contexts>
<marker>Blitzer, Foster, Kakade, 2009</marker>
<rawString>John Blitzer, Dean P. Foster, and Sham M. Kakade. 2009. Zero-shot domain adaptation: A multi-view approach. Technical report, Toyota Technological Institute TTI-TR-2009-1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark G Core</author>
<author>James F Allen</author>
</authors>
<title>Coding dialogs with the DAMSL annotation scheme.</title>
<date>1997</date>
<booktitle>In Proc. ofthe Working Notes of the AAAI Fall Symposium on Communicative Action in Humans and Machines.</booktitle>
<marker>Core, Allen, 1997</marker>
<rawString>Mark G. Core and James F. Allen. 1997. Coding dialogs with the DAMSL annotation scheme. In Proc. ofthe Working Notes of the AAAI Fall Symposium on Communicative Action in Humans and Machines.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>John Blitzer</author>
</authors>
<title>Partha Pratim Talukdar, Kuzman Ganchev, Jo˜ao Graca, and Fernando Pereira.</title>
<date>2007</date>
<booktitle>In Proc. of the CoNLL Shared Task Session ofEMNLP-CoNLL</booktitle>
<pages>1051--1055</pages>
<marker>Dredze, Blitzer, 2007</marker>
<rawString>Mark Dredze, John Blitzer, Partha Pratim Talukdar, Kuzman Ganchev, Jo˜ao Graca, and Fernando Pereira. 2007. Frustratingly hard domain adaptation for dependency parsing. In Proc. of the CoNLL Shared Task Session ofEMNLP-CoNLL 2007, pages 1051–1055.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal ofMachine Learning Research,</journal>
<pages>9--1871</pages>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. Journal ofMachine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Umit Guz</author>
<author>Gokhan Tur</author>
<author>Dilek Hakkani-T¨ur</author>
<author>S´ebastien Cuendet</author>
</authors>
<title>Cascaded model adaptation for dialog act segmentation and tagging.</title>
<date>2010</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>24</volume>
<issue>2</issue>
<marker>Guz, Tur, Hakkani-T¨ur, Cuendet, 2010</marker>
<rawString>Umit Guz, Gokhan Tur, Dilek Hakkani-T¨ur, and S´ebastien Cuendet. 2010. Cascaded model adaptation for dialog act segmentation and tagging. Computer Speech &amp; Language, 24(2):289–306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiayuan Huang</author>
<author>Alexander J Smola</author>
<author>Arthur Gretton</author>
<author>Karsten M Borgwardt</author>
<author>Bernhard Sch¨olkopf</author>
</authors>
<title>Correcting sample selection bias by unlabeled data.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems 19,</booktitle>
<pages>601--608</pages>
<marker>Huang, Smola, Gretton, Borgwardt, Sch¨olkopf, 2007</marker>
<rawString>Jiayuan Huang, Alexander J. Smola, Arthur Gretton, Karsten M. Borgwardt, and Bernhard Sch¨olkopf. 2007. Correcting sample selection bias by unlabeled data. In Advances in Neural Information Processing Systems 19, pages 601–608.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minwoo Jeong</author>
<author>Chin Y Lin</author>
<author>Gary G Lee</author>
</authors>
<title>Semi-supervised speech act recognition in emails and forums.</title>
<date>2009</date>
<booktitle>In Proc. of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1250--1259</pages>
<marker>Jeong, Lin, Lee, 2009</marker>
<rawString>Minwoo Jeong, Chin Y. Lin, and Gary G. Lee. 2009. Semi-supervised speech act recognition in emails and forums. In Proc. of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1250–1259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Jurafsky</author>
<author>Liz Shriberg</author>
<author>Debra Biasca</author>
</authors>
<title>Switchboard SWBD-DAMSL shallow-discoursefunction annotation coders manual, draft 13.</title>
<date>1997</date>
<tech>Technical report,</tech>
<institution>University of Colorado at Boulder</institution>
<marker>Jurafsky, Shriberg, Biasca, 1997</marker>
<rawString>Dan Jurafsky, Liz Shriberg, and Debra Biasca. 1997. Switchboard SWBD-DAMSL shallow-discoursefunction annotation coders manual, draft 13. Technical report, University of Colorado at Boulder Technical Report 97-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lori Levin</author>
<author>Ann Thym´e-Gobbel</author>
<author>Alon Lavie</author>
<author>Klaus Ries</author>
<author>Klaus Zechner</author>
</authors>
<title>A discourse coding scheme for conversational Spanish.</title>
<date>1998</date>
<booktitle>In Proc. The 5th International Conference on Spoken Language Processing,</booktitle>
<pages>2335--2338</pages>
<marker>Levin, Thym´e-Gobbel, Lavie, Ries, Zechner, 1998</marker>
<rawString>Lori Levin, Ann Thym´e-Gobbel, Alon Lavie, Klaus Ries, and Klaus Zechner. 1998. A discourse coding scheme for conversational Spanish. In Proc. The 5th International Conference on Spoken Language Processing, pages 2335–2338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
</authors>
<title>Using SVM and error-correcting codes for multiclass dialog act classification in meeting corpus.</title>
<date>2006</date>
<booktitle>In Proc. Interspeech,</booktitle>
<pages>1938--1941</pages>
<marker>Liu, 2006</marker>
<rawString>Yang Liu. 2006. Using SVM and error-correcting codes for multiclass dialog act classification in meeting corpus. In Proc. Interspeech, pages 1938–1941.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sinno J Pan</author>
<author>James T Kwok</author>
<author>Qiang Yang</author>
</authors>
<title>Transfer learning via dimensionality reduction.</title>
<date>2008</date>
<booktitle>In Proc. of the Twenty-Third AAAI Conference on Artificial Intelligence.</booktitle>
<marker>Pan, Kwok, Yang, 2008</marker>
<rawString>Sinno J. Pan, James T. Kwok, and Qiang Yang. 2008. Transfer learning via dimensionality reduction. In Proc. of the Twenty-Third AAAI Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Michiel Bacchiani</author>
</authors>
<title>Supervised and unsupervised PCFG adaptation to novel domains.</title>
<date>2003</date>
<booktitle>In Proc. of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>126--133</pages>
<marker>Roark, Bacchiani, 2003</marker>
<rawString>Brian Roark and Michiel Bacchiani. 2003. Supervised and unsupervised PCFG adaptation to novel domains. In Proc. of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 126–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sophie Rosset</author>
<author>Delphine Tribout</author>
<author>Lori Lamel</author>
</authors>
<title>Multi-level information and automatic dialog act detection in human–human spoken dialogs.</title>
<date>2008</date>
<journal>Speech Communication,</journal>
<volume>50</volume>
<issue>1</issue>
<marker>Rosset, Tribout, Lamel, 2008</marker>
<rawString>Sophie Rosset, Delphine Tribout, and Lori Lamel. 2008. Multi-level information and automatic dialog act detection in human–human spoken dialogs. Speech Communication, 50(1):1–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Shriberg</author>
<author>Raj Dhillon</author>
<author>Sonali Bhagat</author>
<author>Jeremy Ang</author>
<author>Hannah Carvey</author>
</authors>
<title>The ICSI meeting recorder dialog act (MRDA) corpus.</title>
<date>2004</date>
<booktitle>In Proc. of the 5th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>97--100</pages>
<marker>Shriberg, Dhillon, Bhagat, Ang, Carvey, 2004</marker>
<rawString>Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy Ang, and Hannah Carvey. 2004. The ICSI meeting recorder dialog act (MRDA) corpus. In Proc. of the 5th SIGdial Workshop on Discourse and Dialogue, pages 97–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Klaus Ries</author>
<author>Noah Coccaro</author>
<author>Elizabeth Shriberg</author>
<author>Rebecca Bates</author>
<author>Daniel Jurafsky</author>
<author>Paul Taylor</author>
<author>Rachel Martin</author>
<author>Carol Van Ess-Dykema</author>
<author>Marie Meteer</author>
</authors>
<title>Dialogue act modeling for automatic tagging and recognition of conversational speech.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<pages>26--339</pages>
<marker>Stolcke, Ries, Coccaro, Shriberg, Bates, Jurafsky, Taylor, Martin, Van Ess-Dykema, Meteer, 2000</marker>
<rawString>Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor, Rachel Martin, Carol Van Ess-Dykema, and Marie Meteer. 2000. Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26:339–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dinoj Surendran</author>
<author>Gina-Anne Levow</author>
</authors>
<title>Dialog act tagging with support vector machines and hidden Markov models.</title>
<date>2006</date>
<booktitle>In Proc. Interspeech,</booktitle>
<pages>pages</pages>
<marker>Surendran, Levow, 2006</marker>
<rawString>Dinoj Surendran and Gina-Anne Levow. 2006. Dialog act tagging with support vector machines and hidden Markov models. In Proc. Interspeech, pages 1950– 1953.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gokhan Tur</author>
<author>Dilek Hakkani-T¨ur</author>
<author>Robert E Schapire</author>
</authors>
<title>Combining active and semisupervised learning for spoken language understanding.</title>
<date>2005</date>
<journal>Speech Communication,</journal>
<volume>45</volume>
<issue>2</issue>
<marker>Tur, Hakkani-T¨ur, Schapire, 2005</marker>
<rawString>Gokhan Tur, Dilek Hakkani-T¨ur, and Robert E. Schapire. 2005. Combining active and semisupervised learning for spoken language understanding. Speech Communication, 45(2):171–186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gokhan Tur</author>
<author>Umit Guz</author>
<author>Dilek Hakkani-T¨ur</author>
</authors>
<title>Model adaptation for dialog act tagging.</title>
<date>2007</date>
<booktitle>In Proc. IEEE Spoken Language Technology Workshop,</booktitle>
<pages>94--97</pages>
<marker>Tur, Guz, Hakkani-T¨ur, 2007</marker>
<rawString>Gokhan Tur, Umit Guz, and Dilek Hakkani-T¨ur. 2007. Model adaptation for dialog act tagging. In Proc. IEEE Spoken Language Technology Workshop, pages 94–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gokhan Tur</author>
</authors>
<title>Model adaptation for spoken language understanding.</title>
<date>2005</date>
<booktitle>In Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>41--44</pages>
<marker>Tur, 2005</marker>
<rawString>Gokhan Tur. 2005. Model adaptation for spoken language understanding. In Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing, pages 41–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anand Venkataraman</author>
<author>Luciana Ferrer</author>
<author>Andreas Stolcke</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Training a prosody-based dialog act tagger from unlabeled data. In</title>
<date>2003</date>
<booktitle>Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing, volume I,</booktitle>
<pages>272--275</pages>
<marker>Venkataraman, Ferrer, Stolcke, Shriberg, 2003</marker>
<rawString>Anand Venkataraman, Luciana Ferrer, Andreas Stolcke, and Elizabeth Shriberg. 2003. Training a prosody-based dialog act tagger from unlabeled data. In Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing, volume I, pages 272–275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick Webb</author>
<author>Ting Liu</author>
</authors>
<title>Investigating the portability of corpus-derived cue phrases for dialogue act classification.</title>
<date>2008</date>
<booktitle>In Proc. of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>977--984</pages>
<marker>Webb, Liu, 2008</marker>
<rawString>Nick Webb and Ting Liu. 2008. Investigating the portability of corpus-derived cue phrases for dialogue act classification. In Proc. of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 977–984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dikan Xing</author>
<author>Wenyuan Dai</author>
<author>Gui-Rong Xue</author>
<author>Yong Yu</author>
</authors>
<title>Bridged refinement for transfer learning.</title>
<date>2007</date>
<booktitle>In Knowledge Discovery in Databases: PKDD 2007,</booktitle>
<pages>324--335</pages>
<publisher>Springer</publisher>
<location>Berlin / Heidelberg.</location>
<marker>Xing, Dai, Xue, Yu, 2007</marker>
<rawString>Dikan Xing, Wenyuan Dai, Gui-Rong Xue, and Yong Yu. 2007. Bridged refinement for transfer learning. In Knowledge Discovery in Databases: PKDD 2007, pages 324–335. Springer Berlin / Heidelberg.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>