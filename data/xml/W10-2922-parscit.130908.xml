<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000561">
<title confidence="0.994008">
Online Entropy-based Model of Lexical Category Acquisition
</title>
<author confidence="0.988434">
Grzegorz Chrupała Afra Alishahi
</author>
<affiliation confidence="0.999303">
Saarland University Saarland University
</affiliation>
<email confidence="0.939595">
gchrupala@lsv.uni-saarland.de afra@coli.uni-saarland.de
</email>
<sectionHeader confidence="0.9923" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99991805882353">
Children learn a robust representation of
lexical categories at a young age. We pro-
pose an incremental model of this process
which efficiently groups words into lexi-
cal categories based on their local context
using an information-theoretic criterion.
We train our model on a corpus of child-
directed speech from CHILDES and show
that the model learns a fine-grained set
of intuitive word categories. Furthermore,
we propose a novel evaluation approach
by comparing the efficiency of our induced
categories against other category sets (in-
cluding traditional part of speech tags) in
a variety of language tasks. We show the
categories induced by our model typically
outperform the other category sets.
</bodyText>
<sectionHeader confidence="0.893242" genericHeader="keywords">
1 The Acquisition of Lexical Categories
</sectionHeader>
<bodyText confidence="0.999973962962963">
Psycholinguistic studies suggest that early on chil-
dren acquire robust knowledge of the abstract lex-
ical categories such as nouns, verbs and deter-
miners (e.g., Gelman &amp; Taylor, 1984; Kemp et
al., 2005). Children’s grouping of words into
categories might be based on various cues, in-
cluding phonological and morphological proper-
ties of a word, the distributional information about
its surrounding context, and its semantic features.
Among these, the distributional properties of the
local context of a word have been thoroughly stud-
ied. It has been shown that child-directed speech
provides informative co-occurrence cues, which
can be reliably used to form lexical categories
(Redington et al., 1998; Mintz, 2003).
The process of learning lexical categories by
children is necessarily incremental. Human lan-
guage acquisition is bounded by memory and pro-
cessing limitations, and it is implausible that hu-
mans process large volumes of text at once and
induce an optimum set of categories. Efficient on-
line computational models are needed to investi-
gate whether distributional information is equally
useful in an online process of word categoriza-
tion. However, the few incremental models of
category acquisition which have been proposed
so far are generally inefficient and over-sensitive
to the properties of the input data (Cartwright &amp;
Brent, 1997; Parisien et al., 2008). Moreover, the
unsupervised nature of these models makes their
assessment a challenge, and the evaluation tech-
niques proposed in the literature are limited.
The main contributions of our research are
twofold. First, we propose an incremental en-
tropy model for efficiently clustering words into
categories given their local context. We train our
model on a corpus of child-directed speech from
CHILDES (MacWhinney, 2000) and show that the
model learns a fine-grained set of intuitive word
categories. Second, we propose a novel evalua-
tion approach by comparing the efficiency of our
induced categories against other category sets, in-
cluding the traditional part of speech tags, in a va-
riety of language tasks. We evaluate our model on
word prediction (where a missing word is guessed
based on its sentential context), semantic inference
(where the semantic properties of a novel word are
predicted based on the context), and grammatical-
ity judgment (where the syntactic well-formedness
of a sentence is assessed based on the category la-
bels assigned to its words). The results show that
the categories induced by our model can be suc-
cessfully used in a variety of tasks and typically
perform better than other category sets.
</bodyText>
<sectionHeader confidence="0.7951435" genericHeader="introduction">
1.1 Unsupervised Models of Category
Induction
</sectionHeader>
<bodyText confidence="0.99043325">
Several computational models have used distri-
butional information for categorizing words (e.g.
Brown et al., 1992; Redington et al., 1998; Clark,
2000; Mintz, 2002). The majority of these mod-
</bodyText>
<page confidence="0.963676">
182
</page>
<note confidence="0.955583">
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 182–191,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999897387755102">
els partition the vocabulary into a set of optimum
clusters (e.g., Brown et al., 1992; Clark, 2000).
The generated clusters are intuitive, and can be
used in different tasks such as word prediction
and parsing. Moreover, these models confirm the
learnability of abstract word categories, and show
that distributional cues are a useful source of in-
formation for this purpose. However, (i) they cat-
egorize word types rather than word tokens, and
as such provide no account of words belonging to
more than one category, and (ii) the batch algo-
rithms used by these systems make them implau-
sible for modeling human category induction. Un-
supervised models of PoS tagging such as Gold-
water &amp; Griffiths (2007) do assign labels to word-
tokens, but they still typically use batch process-
ing, and what is even more problematic, they hard-
wire important aspects of the model, such as the
final number of categories.
Only few previously proposed models process
data incrementally, categorize word-tokens and do
not pre-specify a fixed category set. The model
of Cartwright &amp; Brent (1997) uses an algorithm
which incrementally merges word clusters so that
a Minimum Description Length criterion for a
template grammar is optimized. The model treats
whole sentences as contextual units, which sacri-
fices a degree of incrementality, as well as making
it less robust to noise in the input.
Parisien et al. (2008) propose a Bayesian clus-
tering model which copes with ambiguity and ex-
hibits the developmental trends observed in chil-
dren (e.g. the order of acquisition of different cat-
egories). However, their model is overly sen-
sitive to context variability, which results in the
creation of sparse categories. To remedy this is-
sue they introduce a “bootstrapping” component
where the categories assigned to context words are
use to determine the category of the current target
word. They also perform periodical cluster reorga-
nization. These mechanisms improve the overall
performance of the model when trained on large
amounts of training data, but they complicate the
model with ad-hoc extensions and add to the (al-
ready considerable) computational load.
What is lacking is an incremental model of lex-
ical category which can efficiently process natu-
ralistic input data and gradually build robust cate-
gories with little training data.
</bodyText>
<subsectionHeader confidence="0.994466">
1.2 Evaluation of the Induced Categories
</subsectionHeader>
<bodyText confidence="0.99996034">
There is no standard and straightforward method
for evaluating the unsupervised models of cate-
gory learning (see Clark, 2003, for discussion).
Many unsupervised models of lexical category ac-
quisition treat the traditional part of speech (PoS)
tags as the gold standard, and measure the accu-
racy and completeness of their induced categories
based on how closely they resemble the PoS cate-
gories (e.g. Redington et al., 1998; Mintz, 2003;
Parisien et al., 2008). However, it is not at all
clear whether humans form the same types of cate-
gories. In fact, many language tasks might benefit
from finer-grained categories than the traditional
PoS tags used for corpus annotation.
Frank et al. (2009) propose a different, automat-
ically generated set of gold standard categories for
evaluating an unsupervised categorization model.
The gold-standard categories are formed accord-
ing to “substitutability”: if one word can be re-
placed by another and the resulting sentence is still
grammatical, then there is a good chance that the
two words belong to the same category. They ex-
tract 3-word frames from the training data, and
form the gold standard categories based on the
words that appear in the same frame. They em-
phasize that in order to provide some degree of
generalization, different data sets must be used for
forming the gold-standard categories and perform-
ing the evaluation. However, the resulting cate-
gories are bound to be incomplete, and using them
as gold standard inevitably favors categorization
models which use a similar frame-based principle.
All in all, using any set of gold standard cate-
gories for evaluating an unsupervised categoriza-
tion model has the disadvantage of favoring one
set of principles and intuitions over another; that
is, assuming that there is a correct set of cate-
gories which the model should converge to. Al-
ternatively, automatically induced categories can
be evaluated based on how useful they are in per-
forming different tasks. This approach is taken by
Clark (2000), where the perplexity of a finite-state
model is used to compare different category sets.
We build on this idea and propose a more gen-
eral usage-based approach to evaluating the auto-
matically induced categories from a data set, em-
phasizing that the ultimate goal of a category in-
duction model is to form categories that can be ef-
ficiently used in a variety of language tasks. We
argue that for such tasks, a finer-grained set of cat-
</bodyText>
<page confidence="0.998612">
183
</page>
<bodyText confidence="0.9998716">
egories might be more appropriate than the coarse-
grained PoS categories. Therefore, we propose a
number of tasks for which we compare the perfor-
mance based on various category sets, including
those induced by our model.
</bodyText>
<sectionHeader confidence="0.5802185" genericHeader="method">
2 An Incremental Entropy-based Model
of Category Induction
</sectionHeader>
<bodyText confidence="0.9988115">
A model of human category acquisition should
possess two key features:
</bodyText>
<listItem confidence="0.8152634">
• It should process input as it arrives, and incre-
mentally update the current set of clusters.
• The set of clusters should not be fixed in ad-
vance, but rather determined by the charac-
teristics of the input data.
</listItem>
<bodyText confidence="0.999813466666667">
We propose a simple algorithm which fulfills those
two conditions.
Our goal is to categorize word usages based on
the similarity of their form (the content) and their
surrounding words (the context). While grouping
word usages into categories, we attempt to trade
off two conflicting criteria. First, the categories
should be informative about the properties of their
members. Second, the number and distribution of
the categories should be parsimonious. An appro-
priate tool for formalizing both informativeness
and parsimony is information-theoretic entropy.
The parsimony criterion can be formalized as
the entropy of the random variable (Y ) represent-
ing the cluster assignments:
</bodyText>
<equation confidence="0.9887215">
H(Y ) = − EN P(Y = yi) log2(P(Y = yi)) (1)
i=1
</equation>
<bodyText confidence="0.9925802">
where N is the number of clusters and P(Y = yi)
stands for the relative size of the ith cluster.
The informativeness criterion can be formalized
as the conditional entropy of training examples
(X) given the cluster assignments:
</bodyText>
<equation confidence="0.987031285714286">
H(X|Y ) = EN P(Y = yi)H(X|Y = yi) (2)
i=1
and H(X|Y = yi) is calculated as
T
H(X|Y = yi) = − E [P(X = xj|Y = yi)
j=1
× log2(P(X = xj|Y = yi)] (3)
</equation>
<bodyText confidence="0.999891">
where T is the number of word usages in the train-
ing set.
The two criteria presented by Equations 1 and
2 can be combined together as the joint entropy of
the two random variables X and Y :
</bodyText>
<equation confidence="0.986693">
H(X, Y ) = H(X|Y ) + H(Y ) (4)
</equation>
<bodyText confidence="0.9995059">
For a random variable X corresponding to a sin-
gle feature, minimizing the joint entropy H(X, Y )
will trade off our two desired criteria.
The joint entropy will be minimal if each dis-
tinct value of variable X is assigned the same cat-
egory (i.e. same value of Y ). There are many
assignments which satisfy this condition. They
range from putting all values of X in a single cat-
egory, to having a unique category for each unique
value of X. We favor the latter solution algorith-
mically by creating a new category in case of ties.
Finally, since our training examples contain a
bundle of categorical features, we minimize the
joint entropy simultaneously for all the features.
We consider our training examples to be vectors
of random variables (Xj)M1, where each random
variable corresponds to one feature. For an incom-
ing example we will choose the cluster assignment
which leads to the least increase in the joint en-
tropy H(Xj, Y ), summed over all the features j:
</bodyText>
<equation confidence="0.999179">
M M
E H(Xj, Y ) = E [H(Xj|Y ) + H(Y )] (5)
j=1 j=1
M
= E [H(Xj|Y )] + M × H(Y )
j=1
</equation>
<bodyText confidence="0.999969">
In the next section, we present an incremental
algorithm which uses this criterion for inducing
categories from a sequence of input data.
The Incremental Algorithm. For each word us-
age that the model processes at time t, we need to
find the best category among the ones that have
been formed so far, as well as a potential new cat-
egory. The decision is made based on the change
in the function EM1 H(Xj, Y ) (Equation 5) from
point t − 1 to point t, as a result of assigning the
current input xt to a category y:
</bodyText>
<equation confidence="0.988913">
[Hty(Xj, Y ) − Ht 1(Xj,Y)] (6)
</equation>
<bodyText confidence="0.999236666666667">
where Ht�(X, Y ) is the joint entropy of the assign-
ment Y for the input X = {x1, ... , xt1, after the
last input item xt is assigned to the category y.
The winning category y� is the one that leads to the
smallest increase. Ties are broken by preferring a
new category.
</bodyText>
<equation confidence="0.989181">
yˆ= f argminyE{y}N 1 ΔHyt if ∃y. [ΔHytn &lt; ΔHytJ
(7)
=
ΔHty
M
E
j=1
yN+1 otherwise
</equation>
<page confidence="0.985896">
184
</page>
<bodyText confidence="0.992866636363636">
where N is the number of categories created up to
point t, and yN+1 represents a new category.
Efficiency. We maintain the relative size Pt(y)
and the entropy H(Xj|Y = y) for each category
y over time. When performing an assignment of xt
to a category yi, we only need to update the condi-
tional entropies H(Xj|Y = yi) for all features Xj
for this particular category, since other categories
have not changed. For a feature Xj at point t, the
change in the conditional entropy for the selected
category yi is given by:
</bodyText>
<equation confidence="0.9969164">
OHty.(Xj|Y ) = Hty.(Xj|Y ) − Ht−1(Xj|Y )
�= [P(Y = yk)Ht−1(Xj|Y = yi)]
y�oy.
− Pt−1(Y = yi)Ht−1(X|Y = yi)
− Pt(Y = yi)Ht(Xj|Y = yi)
</equation>
<bodyText confidence="0.999976833333333">
where only the last term depends on the current
time index t. Therefore, the entropy H(Xj|Y ) at
each step can be efficiently updated by calculating
this term for the modified category at that step.
A number of previous studies have considered
entropy-based criteria for clustering (e.g. Barbar´a
et al., 2002; Li et al., 2004). The main contri-
bution of our proposed model is the emphasis on
rarely explored combination of the two character-
istics we consider crucial for modeling human cat-
egory acquisition, incrementality and an open set
of clusters.
</bodyText>
<sectionHeader confidence="0.997693" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999930176470588">
We evaluate the categories formed by our model
through three different tasks. The first task is word
prediction, where a target word is predicted based
on the sentential context it appears in. The second
task is to infer the semantic properties of a novel
word based on its context. The third task is to as-
sess the grammaticality of a sentence tagged with
category labels. We run our model on a corpus of
child-directed speech, and use the categories that it
induces from that corpus in the above-mentioned
tasks. For each task, we compare the performance
using our induced categories against the perfor-
mance using other category sets. In the follow-
ing sections, we describe the properties of the data
sets used for training and testing the model, and
the formation of other category sets against which
we compare our model.
</bodyText>
<table confidence="0.9995745">
Data Set Sessions #Sentences #Words
Training 26–28 22,491 125,339
Development 29–30 15,193 85,361
Test 32–33 14,940 84,130
</table>
<tableCaption confidence="0.999527">
Table 1: Experimental data
</tableCaption>
<subsectionHeader confidence="0.993068">
3.1 Input Data
</subsectionHeader>
<bodyText confidence="0.999987">
We use the Manchester corpus (Theakston et al.,
2001) from CHILDES database (MacWhinney,
2000) as experimental data. The Manchester cor-
pus consists of conversations with 12 children be-
tween the ages of eighteen months to three years
old. The corpus is manually tagged using 60 PoS
labels. We use the mother’s speech from tran-
scripts of 6 children, remove punctuation, and con-
catenate the corresponding sessions.
We used data from three sessions as the training
set, two sessions as the development set, and two
sessions as the test set. We discarded all one-word
sentences from the data sets, as they do not pro-
vide any context for our evaluation tasks. Table 1
summarizes the properties of each data set.
</bodyText>
<subsectionHeader confidence="0.998954">
3.2 Category Sets
</subsectionHeader>
<bodyText confidence="0.984959636363636">
We define each word usage in the training or test
data set as a vector of three categorical features:
the content feature (i.e., the focus word in a us-
age), and two context features (i.e. the preceding
and following bigrams). We ran our clustering al-
gorithm on the training set, which resulted in a
set of 944 categories (of which 442 have only one
member). Table 3 shows two sample categories
from the training set, and Figure 1 shows the size
distribution of the categories.
For each evaluation task, we use the following
category sets to label the test set:
OH. The categories induced by our entropy-
based model from the training set, as de-
scribed above.
PoS. The part-of-speech tags the Manchester cor-
pus is annotated with.
Words. The set of all the word types in the data
set (i.e. assuming that all the usages of the
same word form are grouped together).
Parisien. The induced categories by the model of
Parisien et al. (2008) from the training set.
</bodyText>
<page confidence="0.988143">
185
</page>
<table confidence="0.996088">
Gold PoS Words Parisien OH
VI (0.000) 5.294 5.983 4.806
ARI (1.000) 0.139 0.099 0.168
</table>
<tableCaption confidence="0.954733">
Table 2: Comparison against gold PoS tags using
</tableCaption>
<figure confidence="0.917106333333333">
Variation of Information (VI) and Adjusted Rand
Index (ARI).
5 50 500 5000
1 2 5 10 20 50 100
Frequency
Size
</figure>
<figureCaption confidence="0.9944365">
Figure 1: The distribution of the induced cate-
gories based on their size
</figureCaption>
<table confidence="0.990673454545454">
Category size frequencies
Sample Cluster 1 Sample Cluster 2
going (928) than (45)
doing (190) more (20)
back (150) silly (10)
coming (80) bigger (9)
looking (76) frightened (5)
making (64) dark (4)
playing (55) harder (4)
taking (45) funny (3)
� � � � � �
</table>
<tableCaption confidence="0.96483">
Table 3: Sample categories induced from the train-
</tableCaption>
<bodyText confidence="0.95008025">
ing data. The frequency of each word in the cate-
gory is shown in parentheses.
For the first two tasks (word prediction and se-
mantic inference), we do not use the content fea-
ture in labeling the test set, since the assumption
underlying both tasks is that we do not have ac-
cess to the form of the target word. Therefore,
we do not measure the performance of these tasks
on the Words category set. However, we do use
the content feature in labeling the test examples in
grammaticality judgment.
For completeness, in Table 2 we report the
results of evaluation against Gold PoS tags us-
ing two metrics, Variation of Information (Meila,
2003) and Adjusted Rand Index (Hubert &amp; Arabie,
1985).
</bodyText>
<sectionHeader confidence="0.994545" genericHeader="method">
4 Word Prediction
</sectionHeader>
<bodyText confidence="0.99977347826087">
Humans can predict a word based on the context it
is used in with remarkable accuracy (e.g. Lesher et
al., 2002). Different versions of this task such as
Cloze Test (Taylor, 1953) are used for the assess-
ment of native and second language learning.
We simulate this task, where a missing word is
predicted based on its context. We use each of the
category sets introduced in Section 3.2 to label a
word usage in the test set, without using the word
form itself as a feature. That is, we assume that
the target word is unknown, and find the best cat-
egory for it based only on its surrounding context.
We then output a ranked list of the content feature
values of the selected category as the prediction
of the model for the target word. To evaluate this
prediction, we use the reciprocal rank of the target
word in the predicted list.
The third row of Table 4 shows the Mean Re-
ciprocal Rank (MRR) over all the word usages in
the test data across different category sets. The re-
sults show that the category labels predicted by our
model (AH) perform much better than those of
Parisien, but still not as good as the gold-standard
part of speech categories. The fact that PoS tags
are better here does not necessarily mean that the
PoS category set is better for word prediction as
such, since they are manually assigned and thus
noise-free, unlike the automatic category labels
predicted by the two models. In the second set
of experiments described below we try to factor in
the uncertainty about category assignment inher-
ent in automatic labeling.
Using only the best category output by the
model to produce word predictions is simple and
neutral; however, it discards part of the informa-
tion learned by the model. We can predict words
more accurately by combining information from
the whole ranked list of category labels.
We use the AH model to rank the values of the
content feature in the following fashion: for the
current test usage, we rank each cluster assign-
ment y by the change in the AHyz function that
it causes. For each of the assignments, we com-
pute the relative frequencies P(wIyz) of each pos-
sible focus word. The final rank of the word w in
context h is determined by the sum of the cluster-
</bodyText>
<page confidence="0.997633">
186
</page>
<table confidence="0.94228575">
Gold PoS Words Parisien OH
Word Prediction (MRR) 0.354 - 0.212 0.309
Semantic Inference (MAP) 0.351 - 0.213 0.366
Grammaticality Judgment (Accuracy) 0.728 0.685 0.683 0.715
</table>
<tableCaption confidence="0.999738">
Table 4: The performance in each of the three tasks using different category sets.
</tableCaption>
<bodyText confidence="0.952206">
dependent relative frequencies weighted by the
normalized reciprocal ranks of the clusters:
</bodyText>
<equation confidence="0.996893666666667">
R(yz|h)−1
P(w |yz) EN (8)
z�1 R(yz|h)−1
</equation>
<bodyText confidence="0.999689833333333">
where R(yi|h)−1 is the reciprocal rank of cluster
yi for context h according to the model.
We compare the performance of the AH model
with this word-prediction method to that of an
n-gram language model, which is an established
technique for assigning probabilities to words
based on their context. For the language model
we use several n-gram orders (n = 1... 5), and
smooth the n-gram counts using absolute dis-
counting (Zhai &amp; Lafferty, 2004). The probability
of the word w given the context h is given by the
following model of order n:
</bodyText>
<equation confidence="0.9962325">
Pn(w|h) = max (0, c(h, w) − d) + α(h)Pn−1(w|h) (9)
c(h)
</equation>
<bodyText confidence="0.99966275">
where d is the discount parameter, c(·) is the fre-
quency count function, Pn−1 is the lower-order
back-off distribution, and α is the normalization
factor:
</bodyText>
<equation confidence="0.928092">
( ) _ r 1 if r(h) = 0
α h Sldr(h)1
c(h) otherwise (10)
</equation>
<bodyText confidence="0.999606705882353">
and r(h) is the number of distinct words that fol-
low context h in the training corpus.
In addition to the AH model and the n-gram
language models, we also report how well words
can be predicted from their manually assigned PoS
tags from CHILDES: for each token we predict the
most likely word given the token’s true PoS tag
based on frequencies in the training data.
Table 4 summarizes the evaluation results. The
AH model can predict missing words better than
any of the n-gram language models, and even
slightly better than the true POS tags. Given the
simplicity of our clustering model, this is a very
encouraging result. Simple n-gram language mod-
els are known for providing quite a strong base-
line for word prediction; for example, Brown et
al. (1992)’s class-based language model failed to
</bodyText>
<table confidence="0.997467125">
Model MRR
LM n = 1 0.1253
LM n = 2 0.2884
LM n = 3 0.3278
LM n = 4 0.3305
LM n = 5 0.3297
AH 0.3591
Gold POS 0.3540
</table>
<tableCaption confidence="0.991369">
Table 5: Mean reciprocal rank on the word predic-
</tableCaption>
<bodyText confidence="0.851568666666667">
tion task on the test set
improve test-set perplexity over a word-based tri-
gram model.
</bodyText>
<sectionHeader confidence="0.990207" genericHeader="method">
5 Semantic Inference
</sectionHeader>
<bodyText confidence="0.999979">
Several experimental studies have shown that chil-
dren and adults can infer (some aspects of) the se-
mantic properties of a novel word based on the
context it appears in (e.g. Landau &amp; Gleitman,
1985; Gleitman, 1990; Naigles &amp; Hoff-Ginsberg,
1995). For example, in an experimental study by
Fisher et al. (2006), two-year-olds watched as a
hand placed a duck on a box, and pointed to it as a
new word was uttered. Half of the children heard
the word presented as a noun (This is a corp!),
while half heard it as a preposition (This is acorp
my box!). After training, children heard a test sen-
tence (What else is acorp (my box)?) while watch-
ing two test events: one showed another duck be-
side the box, and the other showed a different ob-
ject on the box. Looking-preferences revealed ef-
fects of sentence context: subjects in the preposi-
tion condition interpreted the novel word as a lo-
cation, whereas those in the noun condition inter-
preted it as an object.
To study a similar effect in our model, we as-
sociate each word with a set of semantic features.
For nouns, we extract the semantic features from
WordNet 3.0 (Fellbaum, 1998) as follows: We
take all the hypernyms of the first sense of the
word, and the first word in the synset of each
hypernym to the set of the semantic features of
</bodyText>
<equation confidence="0.971947666666667">
N
P(w|h) =
z�1
</equation>
<page confidence="0.895557">
187
</page>
<table confidence="0.63186425">
ball
� GAME EQUIPMENT#1
� EQUIPMENT#1
� INSTRUMENTALITY#3, INSTRUMENTATION#1
� ARTIFACT#1, ARTEFACT#1
� WHOLE#2, UNIT#6
� OBJECT#1, PHYSICAL OBJECT#1
� PHYSICAL ENTITY#1
</table>
<figure confidence="0.453362">
--+ ENTITY#1
ball: l GAME EQUIPMENT#1,EQUIPMENT#1,
INSTRUMENTALITY#3,ARTIFACT#1, ... }
</figure>
<figureCaption confidence="0.974811">
Figure 2: Semantic features of ball, as extracted
from WordNet.
</figureCaption>
<bodyText confidence="0.99873109375">
the target word (see Figure 2 for an example).
For verbs, we additionally extract features from
a verb-specific resource, VerbNet 2.3 (Schuler,
2005). Due to lack of proper resources for other
lexical categories, we limit our evaluation to nouns
and verbs.
The semantic features of words are not used in
the formation of lexical categories. However, at
each point of time in learning, we can associate
a semantic profile to a category as the aggregated
set of the semantic features of its members: each
feature in the set is assigned a count that indicates
the number of the category members which have
that semantic property. This is done for each of
the category sets described in Section 3.2.
As in the word-prediction task, we use differ-
ent category sets to label each word usage in a test
set based only on the context features of the word.
When the model encounters a novel word, it can
use the semantic profile of the word’s labeled cat-
egory as a prediction of the semantic properties of
that word. We can evaluate the quality of this pre-
diction by comparing the true meaning represen-
tation of the target word (i.e., its set of semantic
features according to the lexicon) against the se-
mantic profile of the selected category. We use the
Mean Average Precision (MAP) (Manning et al.,
2008) for comparing the ranked list of semantic
features predicted by the model with the flat set
of semantic features extracted from WordNet and
VerbNet. Average Precision for a ranked list F
with respect to a set R of correct features is:
</bodyText>
<equation confidence="0.993859">
� �� �
1
APR(F) = |R |_1
r
</equation>
<bodyText confidence="0.999690875">
where P(r) is precision at rank r and 1R is the
indicator function of set R.
The middle row of Table 4 shows the MAP
scores over all the noun or verb usages in the
test set, based on four different category sets. As
can be seen, the categories induced by our model
(OH) outperform all the other category sets. The
word-type categories are particularly unsuitable
for this task, since they provide the least degree
of generalization over the semantic properties of
a group of words. The categories of Parisien
et al. (2008) result in a better performance than
word types, but they are still too sparse for this
task. However, the average score gained by part of
speech tags is also lower than the one by our cat-
egories. This suggests that too broad categories
are also unsuitable for this task, since they can
only provide predictions about the most general
semantic properties, such as ENTITY for nouns,
and ACTION for verbs. These findings again con-
firm our hypothesis that a finer-grained set of cat-
egories that are extracted directly from the input
data provide the highest predictive power in a nat-
uralistic language task such as semantic inference.
</bodyText>
<sectionHeader confidence="0.977989" genericHeader="method">
6 Grammaticality Judgment
</sectionHeader>
<bodyText confidence="0.999938107142857">
Speakers of a natural language have a general
agreement on the grammaticality of different sen-
tences. Grammaticality judgment has been viewed
as one of the main criteria for measuring how
well a language is learned by a human learner.
Experimental studies have shown that children as
young as five years old can judge the grammati-
cality of the sentences that they hear, and that both
children’s and adults’ grammaticality judgments
are influenced by the distributional properties of
words and their context (e.g., Theakston, 2004).
Several methods have been proposed for auto-
matically distinguishing between grammatical and
ungrammatical usages (e.g., Wagner et al., 2007).
The ‘shallow’ methods are mainly based on n-
gram frequencies of words or categories in a cor-
pus, whereas the ‘deep’ methods treat a parsing
failure as an indication of a grammatical error.
Since our focus is on evaluating our category set,
we use trigram probabilities as a measure of gram-
maticality, using Equation 9 with n = 3.
As before, we label each test sentence using dif-
ferent category sets, and calculate the probability
for each trigram in that sentence. We define the
overall grammaticality score of a sentence as the
minimum of the probabilities of all the trigrams in
that sentence. Note that, unlike the previous tasks,
here we do use the content word as a feature in
</bodyText>
<equation confidence="0.998467">
P(r) x 1R(Fr) (11)
</equation>
<page confidence="0.992948">
188
</page>
<bodyText confidence="0.999962205128205">
labeling a test word usage. The actual word form
affects the grammaticality of its usage, and this in-
formation is available to the human subjects who
evaluate the grammaticality of a sentence.
Since we know of no publicly available corpus
of ungrammatical sentences, we artificially con-
struct one: for each sentence in our test data set,
we randomly move one word to another position.1
We define the accuracy of this task as the propor-
tion of the test usages for which the model calcu-
lates a higher grammaticality score for the original
sentence than for its ungrammatical version.
The last row of Table 4 shows the accuracy of
the grammaticality judgment task across different
category sets. As can be seen, the highest accu-
racy in choosing the grammatical sentence over
the ungrammatical one is achieved by using the
PoS categories (0.728), followed by the categories
induced by our model (0.715). These levels of ac-
curacy are rather good considering that some of
the automatically generated errors are also gram-
matical (e.g., there you are vs. you are there, or
can you reach it vs. you can reach it). The results
by the other two category sets are lower and very
close to each other.
These results suggest that, unlike the semantic
inference task, the grammaticality judgment task
might require a coarser-grained set of categories
which provide a higher level of abstraction. How-
ever, taking into account that the PoS categories
are manually assigned to the test usages, the dif-
ference in their performance might be due to lack
of noise in the labeling procedure. We plan to in-
vestigate this matter in future by improving our
categorization model (as discussed in Section 7).
Also, we intend to implement more accurate ways
of estimating grammaticality, using an approach
similar to that described for word prediction task
in Section 4.
</bodyText>
<sectionHeader confidence="0.999528" genericHeader="discussions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999782111111111">
We have proposed an incremental model of lexi-
cal category acquisition based on the distributional
properties of words. Our model uses an informa-
tion theoretic clustering algorithm which attempts
to optimize the category assignments of the in-
coming word usages at each point in time. The
model can efficiently process the training data, and
induce an intuitive set of categories from child-
directed speech. However, due to the incremen-
</bodyText>
<footnote confidence="0.870722">
1We used the software of Foster &amp; Andersen (2009).
</footnote>
<bodyText confidence="0.999977325">
tal nature of the clustering algorithm, it does not
revise its previous decisions according to the data
that it later receives. A potential remedy would be
to consider merging the clusters that have recently
been updated, in order to allow for recovery from
early mistakes the model has made.
We used the categories induced by our model
in word prediction, inferring the semantic prop-
erties of novel words, and grammaticality judg-
ment. Our experimental results show that the per-
formance in these tasks using our categories is
comparable or better than the performance based
on the manually assigned part of speech tags in
our experimental data. Furthermore, in all these
tasks the performance using our categories im-
proves over a previous incremental categorization
model (Parisien et al., 2008). However, the model
of Parisien employs a number of cluster reorgani-
zation techniques which improve the overall qual-
ity of the clusters after processing a substantial
amount of input data. In future we plan to increase
the size of our training data, and perform a more
extensive comparison with the model of Parisien
et al. (2008).
The promising results of our experiments sug-
gest that an information-theoretic approach is a
plausible one for modeling the induction of lexi-
cal categories from distributional data. Our results
imply that in many language tasks, a fine-grained
set of categories which are formed in response to
the properties of the input are more appropriate
than the coarser-grained part of speech categories.
Therefore, the ubiquitous approach of using PoS
categories as the gold standard in evaluating un-
supervised category induction models needs to be
reevaluated. To further investigate this claim, in
future we plan to collect experimental data from
human subjects performing our suggested tasks,
and measure the correlation between their perfor-
mance and that of our model.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999724555555556">
We would like to thank Nicolas Stroppa for
insightful comments on our paper, and Chris
Parisien for sharing the implementation of his
model. Grzegorz Chrupała was funded by the
BMBF project NL-Search under contract number
01IS08020B. Afra Alishahi was funded by IRTG
715 “Language Technology and Cognitive Sys-
tems” provided by the German Research Founda-
tion (DFG).
</bodyText>
<page confidence="0.998505">
189
</page>
<sectionHeader confidence="0.99035" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999884446808511">
Barbar´a, D., Li, Y., &amp; Couto, J. (2002). COOL-
CAT: an entropy-based algorithm for categori-
cal clustering. In Proceedings of the Eleventh
International Conference on Information and
Knowledge Management (pp. 582–589).
Brown, P., Mercer, R., Della Pietra, V., &amp; Lai,
J. (1992). Class-based n-gram models of natu-
ral language. Computational linguistics, 18(4),
467–479.
Cartwright, T., &amp; Brent, M. (1997). Syntac-
tic categorization in early language acquisition:
Formalizing the role of distributional analysis.
Cognition, 63(2), 121–170.
Clark, A.(2000). Inducing syntactic categories by
context distribution clustering. In Proceedings
of the 2nd workshop on Learning Language in
Logic and the 4th conference on Computational
Natural Language Learning (pp. 91–94).
Clark, A. (2003). Combining distributional and
morphological information for part of speech
induction. In Proceedings of the 10th Confer-
ence of the European Chapter of the Association
for Computational Linguistics (pp. 59–66).
Fellbaum, C. (Ed.). (1998). WordNet, an elec-
tronic lexical database. MIT Press.
Fisher, C., Klingler, S., &amp; Song, H. (2006). What
does syntax say about space? 2-year-olds use
sentence structure to learn new prepositions.
Cognition, 101(1), 19–29.
Foster, J., &amp; Andersen, Ø. (2009). GenERRate:
generating errors for use in grammatical error
detection. In Proceedings of the fourth work-
shop on innovative use of nlp for building edu-
cational applications (pp. 82–90).
Frank, S., Goldwater, S., &amp; Keller, F.(2009). Eval-
uating models of syntactic category acquisition
without using a gold standard. In Proceedings
of the 31st Annual Meeting of the Cognitive Sci-
ence Society.
Gelman, S., &amp; Taylor, M. (1984). How two-
year-old children interpret proper and common
names for unfamiliar objects. Child Develop-
ment, 1535–1540.
Gleitman, L.(1990). The structural sources of verb
meanings. Language acquisition, 1(1), 3–55.
Goldwater, S., &amp; Griffiths, T. (2007). A fully
Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the 45th An-
nual Meeting of the Association for Computa-
tional Linguistics (Vol. 45, p. 744).
Hubert, L., &amp; Arabie, P. (1985). Comparing parti-
tions. Journal of classification, 2(1), 193–218.
Kemp, N., Lieven, E., &amp; Tomasello, M. (2005).
Young Children’s Knowledge of the” Deter-
miner” and” Adjective” Categories. Journal
of Speech, Language and Hearing Research,
48(3), 592–609.
Landau, B., &amp; Gleitman, L.(1985). Language and
experience: Evidence from the blind child. Har-
vard University Press Cambridge, Mass.
Lesher, G., Moulton, B., Higginbotham, D., &amp; Al-
sofrom, B. (2002). Limits of human word pre-
diction performance. Proceedings of the CSUN
2002.
Li, T., Ma, S., &amp; Ogihara, M. (2004). Entropy-
based criterion in categorical clustering. In Pro-
ceedings of the 21st International Conference
on Machine Learning (p. 68).
MacWhinney, B. (2000). The CHILDES project:
Tools for analyzing talk. Lawrence Erlbaum As-
sociates Inc, US.
Manning, C., Raghavan, P., &amp; Schtze, H. (2008).
Introduction to Information Retrieval. Cam-
bridge University Press New York, NY, USA.
Meila, M. (2003). Comparing Clusterings by the
Variation of Information. In Learning theory
and kernel machines (pp. 173–187). Springer.
Mintz, T. (2002). Category induction from distri-
butional cues in an artificial language. Memory
and Cognition, 30(5), 678–686.
Mintz, T. (2003). Frequent frames as a cue for
grammatical categories in child directed speech.
Cognition, 90(1), 91–117.
Naigles, L., &amp; Hoff-Ginsberg, E. (1995). Input to
Verb Learning: Evidence for the Plausibility of
Syntactic Bootstrapping. Developmental Psy-
chology, 31(5), 827–37.
Parisien, C., Fazly, A., &amp; Stevenson, S. (2008).
An incremental bayesian model for learning
syntactic categories. In Proceedings of the
Twelfth Conference on Computational Natural
Language Learning.
Redington, M., Crater, N., &amp; Finch, S.(1998). Dis-
tributional information: A powerful cue for ac-
</reference>
<page confidence="0.997601">
190
</page>
<bodyText confidence="0.953690076923077">
quiring syntactic categories. Cognitive Science:
A Multidisciplinary Journal, 22(4), 425–469.
Schuler, K. (2005). VerbNet: A broad-coverage,
comprehensive verb lexicon. Unpublished doc-
toral dissertation, University of Pennsylvania.
Taylor, W. (1953). Cloze procedure: A new tool
for measuring readability. Journalism Quar-
terly, 30(4), 415–433.
Theakston, A.(2004). The role of entrenchment in
childrens and adults performance on grammati-
cality judgment tasks. Cognitive Development,
19(1), 15–34.
Theakston, A., Lieven, E., Pine, J., &amp; Rowland, C.
(2001). The role of performance limitations in
the acquisition of verb-argument structure: An
alternative account. Journal of Child Language,
28(01), 127–152.
Wagner, J., Foster, J., &amp; van Genabith, J.(2007). A
comparative evaluation of deep and shallow ap-
proaches to the automatic detection of common
grammatical errors. Proceedings of EMNLP-
CoNLL-2007.
Zhai, C., &amp; Lafferty, J.(2004). A study of smooth-
ing methods for language models applied to in-
formation retrieval. ACM Transactions on In-
formation Systems (TOIS), 22(2), 214.
</bodyText>
<page confidence="0.997838">
191
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.999862">Online Entropy-based Model of Lexical Category Acquisition</title>
<author confidence="0.985613">Grzegorz Chrupała Afra Alishahi</author>
<affiliation confidence="0.999985">Saarland University Saarland University</affiliation>
<email confidence="0.698009">gchrupala@lsv.uni-saarland.deafra@coli.uni-saarland.de</email>
<abstract confidence="0.988530793478262">Children learn a robust representation of lexical categories at a young age. We propose an incremental model of this process which efficiently groups words into lexical categories based on their local context using an information-theoretic criterion. We train our model on a corpus of childdirected speech from CHILDES and show that the model learns a fine-grained set of intuitive word categories. Furthermore, we propose a novel evaluation approach by comparing the efficiency of our induced categories against other category sets (including traditional part of speech tags) in a variety of language tasks. We show the categories induced by our model typically outperform the other category sets. 1 The Acquisition of Lexical Categories Psycholinguistic studies suggest that early on children acquire robust knowledge of the abstract lexical categories such as nouns, verbs and determiners (e.g., Gelman &amp; Taylor, 1984; Kemp et al., 2005). Children’s grouping of words into categories might be based on various cues, including phonological and morphological properties of a word, the distributional information about its surrounding context, and its semantic features. Among these, the distributional properties of the local context of a word have been thoroughly studied. It has been shown that child-directed speech provides informative co-occurrence cues, which can be reliably used to form lexical categories (Redington et al., 1998; Mintz, 2003). The process of learning lexical categories by children is necessarily incremental. Human language acquisition is bounded by memory and processing limitations, and it is implausible that humans process large volumes of text at once and induce an optimum set of categories. Efficient online computational models are needed to investigate whether distributional information is equally useful in an online process of word categorization. However, the few incremental models of category acquisition which have been proposed so far are generally inefficient and over-sensitive to the properties of the input data (Cartwright &amp; Brent, 1997; Parisien et al., 2008). Moreover, the unsupervised nature of these models makes their assessment a challenge, and the evaluation techniques proposed in the literature are limited. The main contributions of our research are twofold. First, we propose an incremental entropy model for efficiently clustering words into categories given their local context. We train our model on a corpus of child-directed speech from CHILDES (MacWhinney, 2000) and show that the model learns a fine-grained set of intuitive word categories. Second, we propose a novel evaluation approach by comparing the efficiency of our induced categories against other category sets, including the traditional part of speech tags, in a variety of language tasks. We evaluate our model on word prediction (where a missing word is guessed based on its sentential context), semantic inference (where the semantic properties of a novel word are predicted based on the context), and grammaticality judgment (where the syntactic well-formedness of a sentence is assessed based on the category labels assigned to its words). The results show that the categories induced by our model can be successfully used in a variety of tasks and typically perform better than other category sets. 1.1 Unsupervised Models of Category Induction Several computational models have used distributional information for categorizing words (e.g. Brown et al., 1992; Redington et al., 1998; Clark, Mintz, 2002). The majority of these mod- 182 of the Fourteenth Conference on Computational Natural Language pages Sweden, 15-16 July 2010. Association for Computational Linguistics els partition the vocabulary into a set of optimum clusters (e.g., Brown et al., 1992; Clark, 2000). The generated clusters are intuitive, and can be used in different tasks such as word prediction and parsing. Moreover, these models confirm the learnability of abstract word categories, and show that distributional cues are a useful source of information for this purpose. However, (i) they categorize word types rather than word tokens, and as such provide no account of words belonging to more than one category, and (ii) the batch algorithms used by these systems make them implausible for modeling human category induction. Unsupervised models of PoS tagging such as Goldwater &amp; Griffiths (2007) do assign labels to wordtokens, but they still typically use batch processing, and what is even more problematic, they hardwire important aspects of the model, such as the final number of categories. Only few previously proposed models process data incrementally, categorize word-tokens and do not pre-specify a fixed category set. The model of Cartwright &amp; Brent (1997) uses an algorithm which incrementally merges word clusters so that a Minimum Description Length criterion for a template grammar is optimized. The model treats whole sentences as contextual units, which sacrifices a degree of incrementality, as well as making it less robust to noise in the input. Parisien et al. (2008) propose a Bayesian clustering model which copes with ambiguity and exhibits the developmental trends observed in children (e.g. the order of acquisition of different categories). However, their model is overly sensitive to context variability, which results in the creation of sparse categories. To remedy this issue they introduce a “bootstrapping” component where the categories assigned to context words are use to determine the category of the current target word. They also perform periodical cluster reorganization. These mechanisms improve the overall performance of the model when trained on large amounts of training data, but they complicate the model with ad-hoc extensions and add to the (already considerable) computational load. What is lacking is an incremental model of lexical category which can efficiently process naturalistic input data and gradually build robust categories with little training data. 1.2 Evaluation of the Induced Categories There is no standard and straightforward method for evaluating the unsupervised models of category learning (see Clark, 2003, for discussion). Many unsupervised models of lexical category acquisition treat the traditional part of speech (PoS) tags as the gold standard, and measure the accuracy and completeness of their induced categories based on how closely they resemble the PoS categories (e.g. Redington et al., 1998; Mintz, 2003; Parisien et al., 2008). However, it is not at all clear whether humans form the same types of categories. In fact, many language tasks might benefit from finer-grained categories than the traditional PoS tags used for corpus annotation. Frank et al. (2009) propose a different, automatically generated set of gold standard categories for evaluating an unsupervised categorization model. The gold-standard categories are formed according to “substitutability”: if one word can be replaced by another and the resulting sentence is still grammatical, then there is a good chance that the two words belong to the same category. They extract 3-word frames from the training data, and form the gold standard categories based on the words that appear in the same frame. They emphasize that in order to provide some degree of generalization, different data sets must be used for forming the gold-standard categories and performing the evaluation. However, the resulting categories are bound to be incomplete, and using them as gold standard inevitably favors categorization models which use a similar frame-based principle. All in all, using any set of gold standard categories for evaluating an unsupervised categorization model has the disadvantage of favoring one set of principles and intuitions over another; that assuming that there is a of categories which the model should converge to. Alternatively, automatically induced categories can evaluated based on how are in performing different tasks. This approach is taken by Clark (2000), where the perplexity of a finite-state model is used to compare different category sets. We build on this idea and propose a more general usage-based approach to evaluating the automatically induced categories from a data set, emphasizing that the ultimate goal of a category induction model is to form categories that can be efficiently used in a variety of language tasks. We that for such tasks, a finer-grained set of cat- 183 egories might be more appropriate than the coarsegrained PoS categories. Therefore, we propose a number of tasks for which we compare the performance based on various category sets, including those induced by our model. 2 An Incremental Entropy-based Model of Category Induction A model of human category acquisition should possess two key features: • It should process input as it arrives, and incrementally update the current set of clusters. • The set of clusters should not be fixed in advance, but rather determined by the characteristics of the input data. We propose a simple algorithm which fulfills those two conditions. Our goal is to categorize word usages based on the similarity of their form (the content) and their surrounding words (the context). While grouping word usages into categories, we attempt to trade off two conflicting criteria. First, the categories should be informative about the properties of their members. Second, the number and distribution of the categories should be parsimonious. An appropriate tool for formalizing both informativeness and parsimony is information-theoretic entropy. The parsimony criterion can be formalized as entropy of the random variable representing the cluster assignments: = i=1 the number of clusters and = for the relative size of the cluster. The informativeness criterion can be formalized as the conditional entropy of training examples given the cluster assignments: = i=1 = calculated as T = E j=1 the number of word usages in the training set. The two criteria presented by Equations 1 and 2 can be combined together as the joint entropy of two random variables Y = + a random variable to a sinfeature, minimizing the joint entropy Y ) will trade off our two desired criteria. The joint entropy will be minimal if each disvalue of variable assigned the same cat- (i.e. same value of There are many assignments which satisfy this condition. They from putting all values of a single category, to having a unique category for each unique of We favor the latter solution algorithmically by creating a new category in case of ties. Finally, since our training examples contain a bundle of categorical features, we minimize the joint entropy simultaneously for all the features. We consider our training examples to be vectors random variables where each random variable corresponds to one feature. For an incoming example we will choose the cluster assignment which leads to the least increase in the joint en- Y summed over all the features M M EY = E + j=1 j=1 M E j=1 In the next section, we present an incremental algorithm which uses this criterion for inducing categories from a sequence of input data. Incremental Algorithm. each word usthat the model processes at time we need to find the best category among the ones that have been formed so far, as well as a potential new category. The decision is made based on the change the function Y ) 5) from point as a result of assigning the input to a category Y Y ) the joint entropy of the assignthe input = ... , input item is assigned to the category winning category the one that leads to the smallest increase. Ties are broken by preferring a new category. if &lt; (7) = M E j=1 184 the number of categories created up to and represents a new category. maintain the relative size the entropy for each category time. When performing an assignment of a category we only need to update the condientropies for all features for this particular category, since other categories not changed. For a feature point the change in the conditional entropy for the selected given by: = where only the last term depends on the current index Therefore, the entropy at each step can be efficiently updated by calculating this term for the modified category at that step. A number of previous studies have considered entropy-based criteria for clustering (e.g. Barbar´a et al., 2002; Li et al., 2004). The main contribution of our proposed model is the emphasis on rarely explored combination of the two characteristics we consider crucial for modeling human category acquisition, incrementality and an open set of clusters. 3 Experimental Setup We evaluate the categories formed by our model through three different tasks. The first task is word prediction, where a target word is predicted based on the sentential context it appears in. The second task is to infer the semantic properties of a novel word based on its context. The third task is to assess the grammaticality of a sentence tagged with category labels. We run our model on a corpus of child-directed speech, and use the categories that it induces from that corpus in the above-mentioned tasks. For each task, we compare the performance using our induced categories against the performance using other category sets. In the following sections, we describe the properties of the data sets used for training and testing the model, and the formation of other category sets against which we compare our model. Data Set Sessions #Sentences #Words Training 26–28 Development 29–30 Test 32–33 Table 1: Experimental data 3.1 Input Data We use the Manchester corpus (Theakston et al., 2001) from CHILDES database (MacWhinney, 2000) as experimental data. The Manchester corpus consists of conversations with 12 children between the ages of eighteen months to three years old. The corpus is manually tagged using 60 PoS labels. We use the mother’s speech from transcripts of 6 children, remove punctuation, and concatenate the corresponding sessions. We used data from three sessions as the training set, two sessions as the development set, and two sessions as the test set. We discarded all one-word sentences from the data sets, as they do not provide any context for our evaluation tasks. Table 1 summarizes the properties of each data set. 3.2 Category Sets We define each word usage in the training or test data set as a vector of three categorical features: the content feature (i.e., the focus word in a usage), and two context features (i.e. the preceding and following bigrams). We ran our clustering algorithm on the training set, which resulted in a set of 944 categories (of which 442 have only one member). Table 3 shows two sample categories from the training set, and Figure 1 shows the size distribution of the categories. For each evaluation task, we use the following category sets to label the test set: categories induced by our entropybased model from the training set, as described above. part-of-speech tags the Manchester corpus is annotated with. set of all the word types in the data set (i.e. assuming that all the usages of the same word form are grouped together). induced categories by the model of Parisien et al. (2008) from the training set.</abstract>
<note confidence="0.681797571428571">185 Gold PoS Words Parisien OH VI (0.000) 5.294 5.983 4.806 ARI (1.000) 0.139 0.099 0.168 Table 2: Comparison against gold PoS tags using Variation of Information (VI) and Adjusted Rand Index (ARI).</note>
<phone confidence="0.842877">5 50 500 5000 1 2 5 10 20 50 100</phone>
<abstract confidence="0.980302370165746">Frequency Size Figure 1: The distribution of the induced categories based on their size Category size frequencies Sample Cluster 1 Sample Cluster 2 going (928) than (45) doing (190) more (20) back (150) silly (10) coming (80) bigger (9) looking (76) frightened (5) making (64) dark (4) playing (55) harder (4) taking (45) funny (3) � � � � � � Table 3: Sample categories induced from the training data. The frequency of each word in the category is shown in parentheses. For the first two tasks (word prediction and semantic inference), we do not use the content feature in labeling the test set, since the assumption underlying both tasks is that we do not have access to the form of the target word. Therefore, we do not measure the performance of these tasks the set. However, we do use the content feature in labeling the test examples in grammaticality judgment. For completeness, in Table 2 we report the results of evaluation against Gold PoS tags using two metrics, Variation of Information (Meila, 2003) and Adjusted Rand Index (Hubert &amp; Arabie, 1985). 4 Word Prediction Humans can predict a word based on the context it is used in with remarkable accuracy (e.g. Lesher et al., 2002). Different versions of this task such as Cloze Test (Taylor, 1953) are used for the assessment of native and second language learning. We simulate this task, where a missing word is predicted based on its context. We use each of the category sets introduced in Section 3.2 to label a word usage in the test set, without using the word form itself as a feature. That is, we assume that the target word is unknown, and find the best category for it based only on its surrounding context. We then output a ranked list of the content feature values of the selected category as the prediction of the model for the target word. To evaluate this prediction, we use the reciprocal rank of the target word in the predicted list. The third row of Table 4 shows the Mean Reciprocal Rank (MRR) over all the word usages in the test data across different category sets. The results show that the category labels predicted by our perform much better than those of Parisien, but still not as good as the gold-standard part of speech categories. The fact that PoS tags are better here does not necessarily mean that the PoS category set is better for word prediction as such, since they are manually assigned and thus noise-free, unlike the automatic category labels predicted by the two models. In the second set of experiments described below we try to factor in the uncertainty about category assignment inherent in automatic labeling. Using only the best category output by the model to produce word predictions is simple and neutral; however, it discards part of the information learned by the model. We can predict words more accurately by combining information from the whole ranked list of category labels. use the to rank the values of the content feature in the following fashion: for the current test usage, we rank each cluster assignthe change in the that it causes. For each of the assignments, we comthe relative frequencies each posfocus word. The final rank of the word determined by the sum of the cluster- 186 PoS Words Parisien Prediction (MRR) 0.212 0.309 Inference (MAP) 0.351 - 0.213 Judgment (Accuracy) 0.683 0.715 Table 4: The performance in each of the three tasks using different category sets. dependent relative frequencies weighted by the normalized reciprocal ranks of the clusters: is the reciprocal rank of cluster context h according to the model. We compare the performance of the AH model with this word-prediction method to that of an n-gram language model, which is an established technique for assigning probabilities to words based on their context. For the language model we use several n-gram orders (n = 1... 5), and smooth the n-gram counts using absolute discounting (Zhai &amp; Lafferty, 2004). The probability of the word w given the context h is given by the following model of order n: = max − d is the discount parameter, is the frecount function, is the lower-order back-off distribution, and α is the normalization factor: ) _ = 0 h (10) and r(h) is the number of distinct words that follow context h in the training corpus. In addition to the AH model and the n-gram language models, we also report how well words can be predicted from their manually assigned PoS tags from CHILDES: for each token we predict the most likely word given the token’s true PoS tag based on frequencies in the training data. Table 4 summarizes the evaluation results. The AH model can predict missing words better than any of the n-gram language models, and even slightly better than the true POS tags. Given the simplicity of our clustering model, this is a very encouraging result. Simple n-gram language models are known for providing quite a strong baseline for word prediction; for example, Brown et al. (1992)’s class-based language model failed to Model MRR LM n = 1 0.1253 LM n = 2 0.2884 LM n = 3 0.3278 LM n = 4 0.3305 LM n = 5 0.3297 AH 0.3591 Gold POS 0.3540 Table 5: Mean reciprocal rank on the word prediction task on the test set improve test-set perplexity over a word-based trigram model. 5 Semantic Inference Several experimental studies have shown that children and adults can infer (some aspects of) the semantic properties of a novel word based on the context it appears in (e.g. Landau &amp; Gleitman, 1985; Gleitman, 1990; Naigles &amp; Hoff-Ginsberg, 1995). For example, in an experimental study by Fisher et al. (2006), two-year-olds watched as a hand placed a duck on a box, and pointed to it as a new word was uttered. Half of the children heard word presented as a noun is a half heard it as a preposition is acorp After training, children heard a test senelse is acorp (my while watching two test events: one showed another duck beside the box, and the other showed a different object on the box. Looking-preferences revealed effects of sentence context: subjects in the preposition condition interpreted the novel word as a location, whereas those in the noun condition interpreted it as an object. To study a similar effect in our model, we associate each word with a set of semantic features. For nouns, we extract the semantic features from WordNet 3.0 (Fellbaum, 1998) as follows: We take all the hypernyms of the first sense of the word, and the first word in the synset of each hypernym to the set of the semantic features of N = z�1 187 ball lGAME ... 2: Semantic features of as extracted from WordNet. the target word (see Figure 2 for an example). For verbs, we additionally extract features from a verb-specific resource, VerbNet 2.3 (Schuler, 2005). Due to lack of proper resources for other lexical categories, we limit our evaluation to nouns and verbs. The semantic features of words are not used in the formation of lexical categories. However, at each point of time in learning, we can associate profile a category as the aggregated set of the semantic features of its members: each feature in the set is assigned a count that indicates the number of the category members which have that semantic property. This is done for each of the category sets described in Section 3.2. As in the word-prediction task, we use different category sets to label each word usage in a test set based only on the context features of the word. When the model encounters a novel word, it can use the semantic profile of the word’s labeled category as a prediction of the semantic properties of that word. We can evaluate the quality of this preby comparing the representation of the target word (i.e., its set of semantic features according to the lexicon) against the semantic profile of the selected category. We use the Mean Average Precision (MAP) (Manning et al., 2008) for comparing the ranked list of semantic features predicted by the model with the flat set of semantic features extracted from WordNet and Average Precision for a ranked list respect to a set correct features is: 1 = r precision at rank the function of set The middle row of Table 4 shows the MAP scores over all the noun or verb usages in the test set, based on four different category sets. As can be seen, the categories induced by our model outperform all the other category sets. The word-type categories are particularly unsuitable for this task, since they provide the least degree of generalization over the semantic properties of a group of words. The categories of Parisien et al. (2008) result in a better performance than word types, but they are still too sparse for this task. However, the average score gained by part of speech tags is also lower than the one by our categories. This suggests that too broad categories are also unsuitable for this task, since they can only provide predictions about the most general properties, such as nouns, verbs. These findings again confirm our hypothesis that a finer-grained set of categories that are extracted directly from the input data provide the highest predictive power in a naturalistic language task such as semantic inference. 6 Grammaticality Judgment Speakers of a natural language have a general on the different sentences. Grammaticality judgment has been viewed as one of the main criteria for measuring how well a language is learned by a human learner. Experimental studies have shown that children as young as five years old can judge the grammaticality of the sentences that they hear, and that both children’s and adults’ grammaticality judgments are influenced by the distributional properties of words and their context (e.g., Theakston, 2004). Several methods have been proposed for automatically distinguishing between grammatical and ungrammatical usages (e.g., Wagner et al., 2007). The ‘shallow’ methods are mainly based on ngram frequencies of words or categories in a corpus, whereas the ‘deep’ methods treat a parsing failure as an indication of a grammatical error. Since our focus is on evaluating our category set, we use trigram probabilities as a measure of gramusing Equation 9 with As before, we label each test sentence using different category sets, and calculate the probability for each trigram in that sentence. We define the overall grammaticality score of a sentence as the minimum of the probabilities of all the trigrams in that sentence. Note that, unlike the previous tasks, here we do use the content word as a feature in 188 labeling a test word usage. The actual word form affects the grammaticality of its usage, and this information is available to the human subjects who evaluate the grammaticality of a sentence. Since we know of no publicly available corpus of ungrammatical sentences, we artificially construct one: for each sentence in our test data set, randomly move one word to another We define the accuracy of this task as the proportion of the test usages for which the model calculates a higher grammaticality score for the original sentence than for its ungrammatical version. The last row of Table 4 shows the accuracy of the grammaticality judgment task across different category sets. As can be seen, the highest accuracy in choosing the grammatical sentence over the ungrammatical one is achieved by using the PoS categories (0.728), followed by the categories induced by our model (0.715). These levels of accuracy are rather good considering that some of the automatically generated errors are also gram- (e.g., you are are or you reach it can reach The results by the other two category sets are lower and very close to each other. These results suggest that, unlike the semantic inference task, the grammaticality judgment task might require a coarser-grained set of categories which provide a higher level of abstraction. However, taking into account that the PoS categories are manually assigned to the test usages, the difference in their performance might be due to lack of noise in the labeling procedure. We plan to investigate this matter in future by improving our categorization model (as discussed in Section 7). Also, we intend to implement more accurate ways of estimating grammaticality, using an approach similar to that described for word prediction task in Section 4. 7 Discussion We have proposed an incremental model of lexical category acquisition based on the distributional properties of words. Our model uses an information theoretic clustering algorithm which attempts to optimize the category assignments of the incoming word usages at each point in time. The model can efficiently process the training data, and induce an intuitive set of categories from childspeech. However, due to the incremenused the software of Foster &amp; Andersen (2009). tal nature of the clustering algorithm, it does not revise its previous decisions according to the data that it later receives. A potential remedy would be to consider merging the clusters that have recently been updated, in order to allow for recovery from early mistakes the model has made. We used the categories induced by our model in word prediction, inferring the semantic properties of novel words, and grammaticality judgment. Our experimental results show that the performance in these tasks using our categories is comparable or better than the performance based on the manually assigned part of speech tags in our experimental data. Furthermore, in all these tasks the performance using our categories improves over a previous incremental categorization model (Parisien et al., 2008). However, the model of Parisien employs a number of cluster reorganization techniques which improve the overall quality of the clusters after processing a substantial amount of input data. In future we plan to increase the size of our training data, and perform a more extensive comparison with the model of Parisien et al. (2008). The promising results of our experiments suggest that an information-theoretic approach is a plausible one for modeling the induction of lexical categories from distributional data. Our results imply that in many language tasks, a fine-grained set of categories which are formed in response to the properties of the input are more appropriate than the coarser-grained part of speech categories. Therefore, the ubiquitous approach of using PoS categories as the gold standard in evaluating unsupervised category induction models needs to be reevaluated. To further investigate this claim, in future we plan to collect experimental data from human subjects performing our suggested tasks, and measure the correlation between their performance and that of our model. Acknowledgments We would like to thank Nicolas Stroppa for insightful comments on our paper, and Chris Parisien for sharing the implementation of his model. Grzegorz Chrupała was funded by the BMBF project NL-Search under contract number 01IS08020B. Afra Alishahi was funded by IRTG 715 “Language Technology and Cognitive Systems” provided by the German Research Foundation (DFG).</abstract>
<note confidence="0.875223904761905">189 References Barbar´a, D., Li, Y., &amp; Couto, J. (2002). COOL- CAT: an entropy-based algorithm for categoriclustering. In of the Eleventh International Conference on Information and Management 582–589). Brown, P., Mercer, R., Della Pietra, V., &amp; Lai, J. (1992). Class-based n-gram models of natulanguage. 467–479. Cartwright, T., &amp; Brent, M. (1997). Syntactic categorization in early language acquisition: Formalizing the role of distributional analysis. 121–170. Clark, A.(2000). Inducing syntactic categories by distribution clustering. In of the 2nd workshop on Learning Language in Logic and the 4th conference on Computational Language Learning 91–94). Clark, A. (2003). Combining distributional and</note>
<abstract confidence="0.7484378">morphological information for part of speech In of the 10th Conference of the European Chapter of the Association Computational Linguistics 59–66). C. (Ed.). (1998). an eleclexical MIT Press. Fisher, C., Klingler, S., &amp; Song, H. (2006). What does syntax say about space? 2-year-olds use sentence structure to learn new prepositions. 19–29. Foster, J., &amp; Andersen, Ø. (2009). GenERRate: generating errors for use in grammatical error In of the fourth workshop on innovative use of nlp for building eduapplications 82–90). Frank, S., Goldwater, S., &amp; Keller, F.(2009). Evaluating models of syntactic category acquisition using a gold standard. In of the 31st Annual Meeting of the Cognitive Science Society. Gelman, S., &amp; Taylor, M. (1984). How twoyear-old children interpret proper and common for unfamiliar objects. Develop- 1535–1540. Gleitman, L.(1990). The structural sources of verb 3–55. Goldwater, S., &amp; Griffiths, T. (2007). A fully approach to unsupervised part-oftagging. In of the 45th Annual Meeting of the Association for Computa-</abstract>
<note confidence="0.861098341463415">Linguistics 45, p. 744). Hubert, L., &amp; Arabie, P. (1985). Comparing partiof 193–218. Kemp, N., Lieven, E., &amp; Tomasello, M. (2005). Young Children’s Knowledge of the” Deterand” Adjective” Categories. Speech, Language and Hearing 592–609. B., &amp; Gleitman, L.(1985). and Evidence from the blind Harvard University Press Cambridge, Mass. Lesher, G., Moulton, B., Higginbotham, D., &amp; Alsofrom, B. (2002). Limits of human word preperformance. of the CSUN Li, T., Ma, S., &amp; Ogihara, M. (2004). Entropycriterion in categorical clustering. In Proceedings of the 21st International Conference Machine Learning 68). B. (2000). CHILDES project: for analyzing Lawrence Erlbaum Associates Inc, US. Manning, C., Raghavan, P., &amp; Schtze, H. (2008). to Information Cambridge University Press New York, NY, USA. Meila, M. (2003). Comparing Clusterings by the of Information. In theory kernel machines 173–187). Springer. Mintz, T. (2002). Category induction from districues in an artificial language. 678–686. Mintz, T. (2003). Frequent frames as a cue for grammatical categories in child directed speech. 91–117. Naigles, L., &amp; Hoff-Ginsberg, E. (1995). Input to Verb Learning: Evidence for the Plausibility of Bootstrapping. Psy- 827–37. Parisien, C., Fazly, A., &amp; Stevenson, S. (2008). An incremental bayesian model for learning categories. In of the Twelfth Conference on Computational Natural</note>
<affiliation confidence="0.562399">Language Learning.</affiliation>
<address confidence="0.577874">Redington, M., Crater, N., &amp; Finch, S.(1998). Dis-</address>
<abstract confidence="0.943332555555555">information: A powerful cue for ac- 190 syntactic categories. Science: Multidisciplinary 425–469. K. (2005). A broad-coverage, verb Unpublished doctoral dissertation, University of Pennsylvania. Taylor, W. (1953). Cloze procedure: A new tool measuring readability. Quar- 415–433. Theakston, A.(2004). The role of entrenchment in childrens and adults performance on grammatijudgment tasks. 15–34. Theakston, A., Lieven, E., Pine, J., &amp; Rowland, C. (2001). The role of performance limitations in the acquisition of verb-argument structure: An account. of Child 127–152. Wagner, J., Foster, J., &amp; van Genabith, J.(2007). A comparative evaluation of deep and shallow approaches to the automatic detection of common errors. of EMNLP- Zhai, C., &amp; Lafferty, J.(2004). A study of smoothing methods for language models applied to inretrieval. Transactions on In- Systems 214.</abstract>
<intro confidence="0.57126">191</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Barbar´a</author>
<author>Y Li</author>
<author>J Couto</author>
</authors>
<title>COOLCAT: an entropy-based algorithm for categorical clustering.</title>
<date>2002</date>
<booktitle>In Proceedings of the Eleventh International Conference on Information and Knowledge Management</booktitle>
<pages>582--589</pages>
<marker>Barbar´a, Li, Couto, 2002</marker>
<rawString>Barbar´a, D., Li, Y., &amp; Couto, J. (2002). COOLCAT: an entropy-based algorithm for categorical clustering. In Proceedings of the Eleventh International Conference on Information and Knowledge Management (pp. 582–589).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>R Mercer</author>
<author>Della Pietra</author>
<author>V</author>
<author>J Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<pages>467--479</pages>
<contexts>
<context position="3690" citStr="Brown et al., 1992" startWordPosition="559" endWordPosition="562">g word is guessed based on its sentential context), semantic inference (where the semantic properties of a novel word are predicted based on the context), and grammaticality judgment (where the syntactic well-formedness of a sentence is assessed based on the category labels assigned to its words). The results show that the categories induced by our model can be successfully used in a variety of tasks and typically perform better than other category sets. 1.1 Unsupervised Models of Category Induction Several computational models have used distributional information for categorizing words (e.g. Brown et al., 1992; Redington et al., 1998; Clark, 2000; Mintz, 2002). The majority of these mod182 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 182–191, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics els partition the vocabulary into a set of optimum clusters (e.g., Brown et al., 1992; Clark, 2000). The generated clusters are intuitive, and can be used in different tasks such as word prediction and parsing. Moreover, these models confirm the learnability of abstract word categories, and show that distributional cues are a useful </context>
<context position="21971" citStr="Brown et al. (1992)" startWordPosition="3736" endWordPosition="3739"> models, we also report how well words can be predicted from their manually assigned PoS tags from CHILDES: for each token we predict the most likely word given the token’s true PoS tag based on frequencies in the training data. Table 4 summarizes the evaluation results. The AH model can predict missing words better than any of the n-gram language models, and even slightly better than the true POS tags. Given the simplicity of our clustering model, this is a very encouraging result. Simple n-gram language models are known for providing quite a strong baseline for word prediction; for example, Brown et al. (1992)’s class-based language model failed to Model MRR LM n = 1 0.1253 LM n = 2 0.2884 LM n = 3 0.3278 LM n = 4 0.3305 LM n = 5 0.3297 AH 0.3591 Gold POS 0.3540 Table 5: Mean reciprocal rank on the word prediction task on the test set improve test-set perplexity over a word-based trigram model. 5 Semantic Inference Several experimental studies have shown that children and adults can infer (some aspects of) the semantic properties of a novel word based on the context it appears in (e.g. Landau &amp; Gleitman, 1985; Gleitman, 1990; Naigles &amp; Hoff-Ginsberg, 1995). For example, in an experimental study by </context>
</contexts>
<marker>Brown, Mercer, Pietra, V, Lai, 1992</marker>
<rawString>Brown, P., Mercer, R., Della Pietra, V., &amp; Lai, J. (1992). Class-based n-gram models of natural language. Computational linguistics, 18(4), 467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cartwright</author>
<author>M Brent</author>
</authors>
<title>Syntactic categorization in early language acquisition: Formalizing the role of distributional analysis.</title>
<date>1997</date>
<journal>Cognition,</journal>
<volume>63</volume>
<issue>2</issue>
<pages>121--170</pages>
<contexts>
<context position="2282" citStr="Cartwright &amp; Brent, 1997" startWordPosition="337" endWordPosition="340">ess of learning lexical categories by children is necessarily incremental. Human language acquisition is bounded by memory and processing limitations, and it is implausible that humans process large volumes of text at once and induce an optimum set of categories. Efficient online computational models are needed to investigate whether distributional information is equally useful in an online process of word categorization. However, the few incremental models of category acquisition which have been proposed so far are generally inefficient and over-sensitive to the properties of the input data (Cartwright &amp; Brent, 1997; Parisien et al., 2008). Moreover, the unsupervised nature of these models makes their assessment a challenge, and the evaluation techniques proposed in the literature are limited. The main contributions of our research are twofold. First, we propose an incremental entropy model for efficiently clustering words into categories given their local context. We train our model on a corpus of child-directed speech from CHILDES (MacWhinney, 2000) and show that the model learns a fine-grained set of intuitive word categories. Second, we propose a novel evaluation approach by comparing the efficiency </context>
<context position="5027" citStr="Cartwright &amp; Brent (1997)" startWordPosition="772" endWordPosition="775">such provide no account of words belonging to more than one category, and (ii) the batch algorithms used by these systems make them implausible for modeling human category induction. Unsupervised models of PoS tagging such as Goldwater &amp; Griffiths (2007) do assign labels to wordtokens, but they still typically use batch processing, and what is even more problematic, they hardwire important aspects of the model, such as the final number of categories. Only few previously proposed models process data incrementally, categorize word-tokens and do not pre-specify a fixed category set. The model of Cartwright &amp; Brent (1997) uses an algorithm which incrementally merges word clusters so that a Minimum Description Length criterion for a template grammar is optimized. The model treats whole sentences as contextual units, which sacrifices a degree of incrementality, as well as making it less robust to noise in the input. Parisien et al. (2008) propose a Bayesian clustering model which copes with ambiguity and exhibits the developmental trends observed in children (e.g. the order of acquisition of different categories). However, their model is overly sensitive to context variability, which results in the creation of s</context>
</contexts>
<marker>Cartwright, Brent, 1997</marker>
<rawString>Cartwright, T., &amp; Brent, M. (1997). Syntactic categorization in early language acquisition: Formalizing the role of distributional analysis. Cognition, 63(2), 121–170.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Clark</author>
</authors>
<title>Inducing syntactic categories by context distribution clustering.</title>
<booktitle>In Proceedings of the 2nd workshop on Learning Language in Logic and the 4th conference on Computational Natural Language Learning</booktitle>
<pages>91--94</pages>
<marker>Clark, </marker>
<rawString>Clark, A.(2000). Inducing syntactic categories by context distribution clustering. In Proceedings of the 2nd workshop on Learning Language in Logic and the 4th conference on Computational Natural Language Learning (pp. 91–94).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Clark</author>
</authors>
<title>Combining distributional and morphological information for part of speech induction.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics</booktitle>
<pages>59--66</pages>
<contexts>
<context position="6436" citStr="Clark, 2003" startWordPosition="996" endWordPosition="997">so perform periodical cluster reorganization. These mechanisms improve the overall performance of the model when trained on large amounts of training data, but they complicate the model with ad-hoc extensions and add to the (already considerable) computational load. What is lacking is an incremental model of lexical category which can efficiently process naturalistic input data and gradually build robust categories with little training data. 1.2 Evaluation of the Induced Categories There is no standard and straightforward method for evaluating the unsupervised models of category learning (see Clark, 2003, for discussion). Many unsupervised models of lexical category acquisition treat the traditional part of speech (PoS) tags as the gold standard, and measure the accuracy and completeness of their induced categories based on how closely they resemble the PoS categories (e.g. Redington et al., 1998; Mintz, 2003; Parisien et al., 2008). However, it is not at all clear whether humans form the same types of categories. In fact, many language tasks might benefit from finer-grained categories than the traditional PoS tags used for corpus annotation. Frank et al. (2009) propose a different, automatic</context>
</contexts>
<marker>Clark, 2003</marker>
<rawString>Clark, A. (2003). Combining distributional and morphological information for part of speech induction. In Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics (pp. 59–66).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet, an electronic lexical database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="23409" citStr="Fellbaum, 1998" startWordPosition="4005" endWordPosition="4006">preposition (This is acorp my box!). After training, children heard a test sentence (What else is acorp (my box)?) while watching two test events: one showed another duck beside the box, and the other showed a different object on the box. Looking-preferences revealed effects of sentence context: subjects in the preposition condition interpreted the novel word as a location, whereas those in the noun condition interpreted it as an object. To study a similar effect in our model, we associate each word with a set of semantic features. For nouns, we extract the semantic features from WordNet 3.0 (Fellbaum, 1998) as follows: We take all the hypernyms of the first sense of the word, and the first word in the synset of each hypernym to the set of the semantic features of N P(w|h) = z�1 187 ball � GAME EQUIPMENT#1 � EQUIPMENT#1 � INSTRUMENTALITY#3, INSTRUMENTATION#1 � ARTIFACT#1, ARTEFACT#1 � WHOLE#2, UNIT#6 � OBJECT#1, PHYSICAL OBJECT#1 � PHYSICAL ENTITY#1 --+ ENTITY#1 ball: l GAME EQUIPMENT#1,EQUIPMENT#1, INSTRUMENTALITY#3,ARTIFACT#1, ... } Figure 2: Semantic features of ball, as extracted from WordNet. the target word (see Figure 2 for an example). For verbs, we additionally extract features from a ve</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum, C. (Ed.). (1998). WordNet, an electronic lexical database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fisher</author>
<author>S Klingler</author>
<author>H Song</author>
</authors>
<title>What does syntax say about space? 2-year-olds use sentence structure to learn new prepositions.</title>
<date>2006</date>
<journal>Cognition,</journal>
<volume>101</volume>
<issue>1</issue>
<contexts>
<context position="22591" citStr="Fisher et al. (2006)" startWordPosition="3852" endWordPosition="3855">’s class-based language model failed to Model MRR LM n = 1 0.1253 LM n = 2 0.2884 LM n = 3 0.3278 LM n = 4 0.3305 LM n = 5 0.3297 AH 0.3591 Gold POS 0.3540 Table 5: Mean reciprocal rank on the word prediction task on the test set improve test-set perplexity over a word-based trigram model. 5 Semantic Inference Several experimental studies have shown that children and adults can infer (some aspects of) the semantic properties of a novel word based on the context it appears in (e.g. Landau &amp; Gleitman, 1985; Gleitman, 1990; Naigles &amp; Hoff-Ginsberg, 1995). For example, in an experimental study by Fisher et al. (2006), two-year-olds watched as a hand placed a duck on a box, and pointed to it as a new word was uttered. Half of the children heard the word presented as a noun (This is a corp!), while half heard it as a preposition (This is acorp my box!). After training, children heard a test sentence (What else is acorp (my box)?) while watching two test events: one showed another duck beside the box, and the other showed a different object on the box. Looking-preferences revealed effects of sentence context: subjects in the preposition condition interpreted the novel word as a location, whereas those in the</context>
</contexts>
<marker>Fisher, Klingler, Song, 2006</marker>
<rawString>Fisher, C., Klingler, S., &amp; Song, H. (2006). What does syntax say about space? 2-year-olds use sentence structure to learn new prepositions. Cognition, 101(1), 19–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Foster</author>
<author>Ø Andersen</author>
</authors>
<title>GenERRate: generating errors for use in grammatical error detection.</title>
<date>2009</date>
<booktitle>In Proceedings of the</booktitle>
<pages>82--90</pages>
<contexts>
<context position="30362" citStr="Foster &amp; Andersen (2009)" startWordPosition="5176" endWordPosition="5179">e accurate ways of estimating grammaticality, using an approach similar to that described for word prediction task in Section 4. 7 Discussion We have proposed an incremental model of lexical category acquisition based on the distributional properties of words. Our model uses an information theoretic clustering algorithm which attempts to optimize the category assignments of the incoming word usages at each point in time. The model can efficiently process the training data, and induce an intuitive set of categories from childdirected speech. However, due to the incremen1We used the software of Foster &amp; Andersen (2009). tal nature of the clustering algorithm, it does not revise its previous decisions according to the data that it later receives. A potential remedy would be to consider merging the clusters that have recently been updated, in order to allow for recovery from early mistakes the model has made. We used the categories induced by our model in word prediction, inferring the semantic properties of novel words, and grammaticality judgment. Our experimental results show that the performance in these tasks using our categories is comparable or better than the performance based on the manually assigned</context>
</contexts>
<marker>Foster, Andersen, 2009</marker>
<rawString>Foster, J., &amp; Andersen, Ø. (2009). GenERRate: generating errors for use in grammatical error detection. In Proceedings of the fourth workshop on innovative use of nlp for building educational applications (pp. 82–90).</rawString>
</citation>
<citation valid="false">
<authors>
<author>S Frank</author>
<author>S Goldwater</author>
<author>F Keller</author>
</authors>
<title>Evaluating models of syntactic category acquisition without using a gold standard.</title>
<booktitle>In Proceedings of the 31st Annual Meeting of the Cognitive Science Society.</booktitle>
<marker>Frank, Goldwater, Keller, </marker>
<rawString>Frank, S., Goldwater, S., &amp; Keller, F.(2009). Evaluating models of syntactic category acquisition without using a gold standard. In Proceedings of the 31st Annual Meeting of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gelman</author>
<author>M Taylor</author>
</authors>
<title>How twoyear-old children interpret proper and common names for unfamiliar objects. Child Development,</title>
<date>1984</date>
<pages>1535--1540</pages>
<contexts>
<context position="1116" citStr="Gelman &amp; Taylor, 1984" startWordPosition="159" endWordPosition="162">speech from CHILDES and show that the model learns a fine-grained set of intuitive word categories. Furthermore, we propose a novel evaluation approach by comparing the efficiency of our induced categories against other category sets (including traditional part of speech tags) in a variety of language tasks. We show the categories induced by our model typically outperform the other category sets. 1 The Acquisition of Lexical Categories Psycholinguistic studies suggest that early on children acquire robust knowledge of the abstract lexical categories such as nouns, verbs and determiners (e.g., Gelman &amp; Taylor, 1984; Kemp et al., 2005). Children’s grouping of words into categories might be based on various cues, including phonological and morphological properties of a word, the distributional information about its surrounding context, and its semantic features. Among these, the distributional properties of the local context of a word have been thoroughly studied. It has been shown that child-directed speech provides informative co-occurrence cues, which can be reliably used to form lexical categories (Redington et al., 1998; Mintz, 2003). The process of learning lexical categories by children is necessar</context>
</contexts>
<marker>Gelman, Taylor, 1984</marker>
<rawString>Gelman, S., &amp; Taylor, M. (1984). How twoyear-old children interpret proper and common names for unfamiliar objects. Child Development, 1535–1540.</rawString>
</citation>
<citation valid="false">
<authors>
<author>L Gleitman</author>
</authors>
<title>The structural sources of verb meanings.</title>
<journal>Language acquisition,</journal>
<volume>1</volume>
<issue>1</issue>
<pages>3--55</pages>
<marker>Gleitman, </marker>
<rawString>Gleitman, L.(1990). The structural sources of verb meanings. Language acquisition, 1(1), 3–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>T Griffiths</author>
</authors>
<title>A fully Bayesian approach to unsupervised part-ofspeech tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics</booktitle>
<volume>45</volume>
<pages>744</pages>
<contexts>
<context position="4656" citStr="Goldwater &amp; Griffiths (2007)" startWordPosition="711" endWordPosition="715">et al., 1992; Clark, 2000). The generated clusters are intuitive, and can be used in different tasks such as word prediction and parsing. Moreover, these models confirm the learnability of abstract word categories, and show that distributional cues are a useful source of information for this purpose. However, (i) they categorize word types rather than word tokens, and as such provide no account of words belonging to more than one category, and (ii) the batch algorithms used by these systems make them implausible for modeling human category induction. Unsupervised models of PoS tagging such as Goldwater &amp; Griffiths (2007) do assign labels to wordtokens, but they still typically use batch processing, and what is even more problematic, they hardwire important aspects of the model, such as the final number of categories. Only few previously proposed models process data incrementally, categorize word-tokens and do not pre-specify a fixed category set. The model of Cartwright &amp; Brent (1997) uses an algorithm which incrementally merges word clusters so that a Minimum Description Length criterion for a template grammar is optimized. The model treats whole sentences as contextual units, which sacrifices a degree of in</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Goldwater, S., &amp; Griffiths, T. (2007). A fully Bayesian approach to unsupervised part-ofspeech tagging. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (Vol. 45, p. 744).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hubert</author>
<author>P Arabie</author>
</authors>
<title>Comparing partitions.</title>
<date>1985</date>
<journal>Journal of classification,</journal>
<volume>2</volume>
<issue>1</issue>
<pages>193--218</pages>
<contexts>
<context position="17790" citStr="Hubert &amp; Arabie, 1985" startWordPosition="2997" endWordPosition="3000">n parentheses. For the first two tasks (word prediction and semantic inference), we do not use the content feature in labeling the test set, since the assumption underlying both tasks is that we do not have access to the form of the target word. Therefore, we do not measure the performance of these tasks on the Words category set. However, we do use the content feature in labeling the test examples in grammaticality judgment. For completeness, in Table 2 we report the results of evaluation against Gold PoS tags using two metrics, Variation of Information (Meila, 2003) and Adjusted Rand Index (Hubert &amp; Arabie, 1985). 4 Word Prediction Humans can predict a word based on the context it is used in with remarkable accuracy (e.g. Lesher et al., 2002). Different versions of this task such as Cloze Test (Taylor, 1953) are used for the assessment of native and second language learning. We simulate this task, where a missing word is predicted based on its context. We use each of the category sets introduced in Section 3.2 to label a word usage in the test set, without using the word form itself as a feature. That is, we assume that the target word is unknown, and find the best category for it based only on its su</context>
</contexts>
<marker>Hubert, Arabie, 1985</marker>
<rawString>Hubert, L., &amp; Arabie, P. (1985). Comparing partitions. Journal of classification, 2(1), 193–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kemp</author>
<author>E Lieven</author>
<author>M Tomasello</author>
</authors>
<date>2005</date>
<journal>Young Children’s Knowledge of the” Determiner” and” Adjective” Categories. Journal of Speech, Language and Hearing Research,</journal>
<volume>48</volume>
<issue>3</issue>
<pages>592--609</pages>
<contexts>
<context position="1136" citStr="Kemp et al., 2005" startWordPosition="163" endWordPosition="166"> show that the model learns a fine-grained set of intuitive word categories. Furthermore, we propose a novel evaluation approach by comparing the efficiency of our induced categories against other category sets (including traditional part of speech tags) in a variety of language tasks. We show the categories induced by our model typically outperform the other category sets. 1 The Acquisition of Lexical Categories Psycholinguistic studies suggest that early on children acquire robust knowledge of the abstract lexical categories such as nouns, verbs and determiners (e.g., Gelman &amp; Taylor, 1984; Kemp et al., 2005). Children’s grouping of words into categories might be based on various cues, including phonological and morphological properties of a word, the distributional information about its surrounding context, and its semantic features. Among these, the distributional properties of the local context of a word have been thoroughly studied. It has been shown that child-directed speech provides informative co-occurrence cues, which can be reliably used to form lexical categories (Redington et al., 1998; Mintz, 2003). The process of learning lexical categories by children is necessarily incremental. Hum</context>
</contexts>
<marker>Kemp, Lieven, Tomasello, 2005</marker>
<rawString>Kemp, N., Lieven, E., &amp; Tomasello, M. (2005). Young Children’s Knowledge of the” Determiner” and” Adjective” Categories. Journal of Speech, Language and Hearing Research, 48(3), 592–609.</rawString>
</citation>
<citation valid="false">
<authors>
<author>B Landau</author>
<author>L Gleitman</author>
</authors>
<title>Language and experience: Evidence from the blind child.</title>
<publisher>Harvard University Press</publisher>
<location>Cambridge, Mass.</location>
<marker>Landau, Gleitman, </marker>
<rawString>Landau, B., &amp; Gleitman, L.(1985). Language and experience: Evidence from the blind child. Harvard University Press Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Lesher</author>
<author>B Moulton</author>
<author>D Higginbotham</author>
<author>B Alsofrom</author>
</authors>
<title>Limits of human word prediction performance.</title>
<date>2002</date>
<booktitle>Proceedings of the CSUN</booktitle>
<contexts>
<context position="17922" citStr="Lesher et al., 2002" startWordPosition="3021" endWordPosition="3024">t set, since the assumption underlying both tasks is that we do not have access to the form of the target word. Therefore, we do not measure the performance of these tasks on the Words category set. However, we do use the content feature in labeling the test examples in grammaticality judgment. For completeness, in Table 2 we report the results of evaluation against Gold PoS tags using two metrics, Variation of Information (Meila, 2003) and Adjusted Rand Index (Hubert &amp; Arabie, 1985). 4 Word Prediction Humans can predict a word based on the context it is used in with remarkable accuracy (e.g. Lesher et al., 2002). Different versions of this task such as Cloze Test (Taylor, 1953) are used for the assessment of native and second language learning. We simulate this task, where a missing word is predicted based on its context. We use each of the category sets introduced in Section 3.2 to label a word usage in the test set, without using the word form itself as a feature. That is, we assume that the target word is unknown, and find the best category for it based only on its surrounding context. We then output a ranked list of the content feature values of the selected category as the prediction of the mode</context>
</contexts>
<marker>Lesher, Moulton, Higginbotham, Alsofrom, 2002</marker>
<rawString>Lesher, G., Moulton, B., Higginbotham, D., &amp; Alsofrom, B. (2002). Limits of human word prediction performance. Proceedings of the CSUN 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Li</author>
<author>S Ma</author>
<author>M Ogihara</author>
</authors>
<title>Entropybased criterion in categorical clustering.</title>
<date>2004</date>
<booktitle>In Proceedings of the 21st International Conference on Machine Learning (p.</booktitle>
<pages>68</pages>
<contexts>
<context position="13565" citStr="Li et al., 2004" startWordPosition="2265" endWordPosition="2268">egory, since other categories have not changed. For a feature Xj at point t, the change in the conditional entropy for the selected category yi is given by: OHty.(Xj|Y ) = Hty.(Xj|Y ) − Ht−1(Xj|Y ) �= [P(Y = yk)Ht−1(Xj|Y = yi)] y�oy. − Pt−1(Y = yi)Ht−1(X|Y = yi) − Pt(Y = yi)Ht(Xj|Y = yi) where only the last term depends on the current time index t. Therefore, the entropy H(Xj|Y ) at each step can be efficiently updated by calculating this term for the modified category at that step. A number of previous studies have considered entropy-based criteria for clustering (e.g. Barbar´a et al., 2002; Li et al., 2004). The main contribution of our proposed model is the emphasis on rarely explored combination of the two characteristics we consider crucial for modeling human category acquisition, incrementality and an open set of clusters. 3 Experimental Setup We evaluate the categories formed by our model through three different tasks. The first task is word prediction, where a target word is predicted based on the sentential context it appears in. The second task is to infer the semantic properties of a novel word based on its context. The third task is to assess the grammaticality of a sentence tagged wit</context>
</contexts>
<marker>Li, Ma, Ogihara, 2004</marker>
<rawString>Li, T., Ma, S., &amp; Ogihara, M. (2004). Entropybased criterion in categorical clustering. In Proceedings of the 21st International Conference on Machine Learning (p. 68).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacWhinney</author>
</authors>
<title>The CHILDES project: Tools for analyzing talk. Lawrence Erlbaum Associates Inc,</title>
<date>2000</date>
<location>US.</location>
<contexts>
<context position="2726" citStr="MacWhinney, 2000" startWordPosition="406" endWordPosition="407">tal models of category acquisition which have been proposed so far are generally inefficient and over-sensitive to the properties of the input data (Cartwright &amp; Brent, 1997; Parisien et al., 2008). Moreover, the unsupervised nature of these models makes their assessment a challenge, and the evaluation techniques proposed in the literature are limited. The main contributions of our research are twofold. First, we propose an incremental entropy model for efficiently clustering words into categories given their local context. We train our model on a corpus of child-directed speech from CHILDES (MacWhinney, 2000) and show that the model learns a fine-grained set of intuitive word categories. Second, we propose a novel evaluation approach by comparing the efficiency of our induced categories against other category sets, including the traditional part of speech tags, in a variety of language tasks. We evaluate our model on word prediction (where a missing word is guessed based on its sentential context), semantic inference (where the semantic properties of a novel word are predicted based on the context), and grammaticality judgment (where the syntactic well-formedness of a sentence is assessed based on</context>
<context position="14895" citStr="MacWhinney, 2000" startWordPosition="2485" endWordPosition="2486">from that corpus in the above-mentioned tasks. For each task, we compare the performance using our induced categories against the performance using other category sets. In the following sections, we describe the properties of the data sets used for training and testing the model, and the formation of other category sets against which we compare our model. Data Set Sessions #Sentences #Words Training 26–28 22,491 125,339 Development 29–30 15,193 85,361 Test 32–33 14,940 84,130 Table 1: Experimental data 3.1 Input Data We use the Manchester corpus (Theakston et al., 2001) from CHILDES database (MacWhinney, 2000) as experimental data. The Manchester corpus consists of conversations with 12 children between the ages of eighteen months to three years old. The corpus is manually tagged using 60 PoS labels. We use the mother’s speech from transcripts of 6 children, remove punctuation, and concatenate the corresponding sessions. We used data from three sessions as the training set, two sessions as the development set, and two sessions as the test set. We discarded all one-word sentences from the data sets, as they do not provide any context for our evaluation tasks. Table 1 summarizes the properties of eac</context>
</contexts>
<marker>MacWhinney, 2000</marker>
<rawString>MacWhinney, B. (2000). The CHILDES project: Tools for analyzing talk. Lawrence Erlbaum Associates Inc, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
<author>P Raghavan</author>
<author>H Schtze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="25205" citStr="Manning et al., 2008" startWordPosition="4307" endWordPosition="4310">ibed in Section 3.2. As in the word-prediction task, we use different category sets to label each word usage in a test set based only on the context features of the word. When the model encounters a novel word, it can use the semantic profile of the word’s labeled category as a prediction of the semantic properties of that word. We can evaluate the quality of this prediction by comparing the true meaning representation of the target word (i.e., its set of semantic features according to the lexicon) against the semantic profile of the selected category. We use the Mean Average Precision (MAP) (Manning et al., 2008) for comparing the ranked list of semantic features predicted by the model with the flat set of semantic features extracted from WordNet and VerbNet. Average Precision for a ranked list F with respect to a set R of correct features is: � �� � 1 APR(F) = |R |_1 r where P(r) is precision at rank r and 1R is the indicator function of set R. The middle row of Table 4 shows the MAP scores over all the noun or verb usages in the test set, based on four different category sets. As can be seen, the categories induced by our model (OH) outperform all the other category sets. The word-type categories ar</context>
</contexts>
<marker>Manning, Raghavan, Schtze, 2008</marker>
<rawString>Manning, C., Raghavan, P., &amp; Schtze, H. (2008). Introduction to Information Retrieval. Cambridge University Press New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Meila</author>
</authors>
<title>Comparing Clusterings by the Variation of Information.</title>
<date>2003</date>
<booktitle>In Learning theory and kernel machines</booktitle>
<pages>173--187</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="17742" citStr="Meila, 2003" startWordPosition="2991" endWordPosition="2992">f each word in the category is shown in parentheses. For the first two tasks (word prediction and semantic inference), we do not use the content feature in labeling the test set, since the assumption underlying both tasks is that we do not have access to the form of the target word. Therefore, we do not measure the performance of these tasks on the Words category set. However, we do use the content feature in labeling the test examples in grammaticality judgment. For completeness, in Table 2 we report the results of evaluation against Gold PoS tags using two metrics, Variation of Information (Meila, 2003) and Adjusted Rand Index (Hubert &amp; Arabie, 1985). 4 Word Prediction Humans can predict a word based on the context it is used in with remarkable accuracy (e.g. Lesher et al., 2002). Different versions of this task such as Cloze Test (Taylor, 1953) are used for the assessment of native and second language learning. We simulate this task, where a missing word is predicted based on its context. We use each of the category sets introduced in Section 3.2 to label a word usage in the test set, without using the word form itself as a feature. That is, we assume that the target word is unknown, and fi</context>
</contexts>
<marker>Meila, 2003</marker>
<rawString>Meila, M. (2003). Comparing Clusterings by the Variation of Information. In Learning theory and kernel machines (pp. 173–187). Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mintz</author>
</authors>
<title>Category induction from distributional cues in an artificial language.</title>
<date>2002</date>
<journal>Memory and Cognition,</journal>
<volume>30</volume>
<issue>5</issue>
<pages>678--686</pages>
<contexts>
<context position="3741" citStr="Mintz, 2002" startWordPosition="569" endWordPosition="570">tic inference (where the semantic properties of a novel word are predicted based on the context), and grammaticality judgment (where the syntactic well-formedness of a sentence is assessed based on the category labels assigned to its words). The results show that the categories induced by our model can be successfully used in a variety of tasks and typically perform better than other category sets. 1.1 Unsupervised Models of Category Induction Several computational models have used distributional information for categorizing words (e.g. Brown et al., 1992; Redington et al., 1998; Clark, 2000; Mintz, 2002). The majority of these mod182 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 182–191, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics els partition the vocabulary into a set of optimum clusters (e.g., Brown et al., 1992; Clark, 2000). The generated clusters are intuitive, and can be used in different tasks such as word prediction and parsing. Moreover, these models confirm the learnability of abstract word categories, and show that distributional cues are a useful source of information for this purpose. However, (i</context>
</contexts>
<marker>Mintz, 2002</marker>
<rawString>Mintz, T. (2002). Category induction from distributional cues in an artificial language. Memory and Cognition, 30(5), 678–686.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mintz</author>
</authors>
<title>Frequent frames as a cue for grammatical categories in child directed speech.</title>
<date>2003</date>
<journal>Cognition,</journal>
<volume>90</volume>
<issue>1</issue>
<pages>91--117</pages>
<contexts>
<context position="1648" citStr="Mintz, 2003" startWordPosition="241" endWordPosition="242">cal categories such as nouns, verbs and determiners (e.g., Gelman &amp; Taylor, 1984; Kemp et al., 2005). Children’s grouping of words into categories might be based on various cues, including phonological and morphological properties of a word, the distributional information about its surrounding context, and its semantic features. Among these, the distributional properties of the local context of a word have been thoroughly studied. It has been shown that child-directed speech provides informative co-occurrence cues, which can be reliably used to form lexical categories (Redington et al., 1998; Mintz, 2003). The process of learning lexical categories by children is necessarily incremental. Human language acquisition is bounded by memory and processing limitations, and it is implausible that humans process large volumes of text at once and induce an optimum set of categories. Efficient online computational models are needed to investigate whether distributional information is equally useful in an online process of word categorization. However, the few incremental models of category acquisition which have been proposed so far are generally inefficient and over-sensitive to the properties of the in</context>
<context position="6747" citStr="Mintz, 2003" startWordPosition="1046" endWordPosition="1047">exical category which can efficiently process naturalistic input data and gradually build robust categories with little training data. 1.2 Evaluation of the Induced Categories There is no standard and straightforward method for evaluating the unsupervised models of category learning (see Clark, 2003, for discussion). Many unsupervised models of lexical category acquisition treat the traditional part of speech (PoS) tags as the gold standard, and measure the accuracy and completeness of their induced categories based on how closely they resemble the PoS categories (e.g. Redington et al., 1998; Mintz, 2003; Parisien et al., 2008). However, it is not at all clear whether humans form the same types of categories. In fact, many language tasks might benefit from finer-grained categories than the traditional PoS tags used for corpus annotation. Frank et al. (2009) propose a different, automatically generated set of gold standard categories for evaluating an unsupervised categorization model. The gold-standard categories are formed according to “substitutability”: if one word can be replaced by another and the resulting sentence is still grammatical, then there is a good chance that the two words bel</context>
</contexts>
<marker>Mintz, 2003</marker>
<rawString>Mintz, T. (2003). Frequent frames as a cue for grammatical categories in child directed speech. Cognition, 90(1), 91–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Naigles</author>
<author>E Hoff-Ginsberg</author>
</authors>
<title>Input to Verb Learning: Evidence for the Plausibility of Syntactic Bootstrapping.</title>
<date>1995</date>
<journal>Developmental Psychology,</journal>
<volume>31</volume>
<issue>5</issue>
<pages>827--37</pages>
<contexts>
<context position="22528" citStr="Naigles &amp; Hoff-Ginsberg, 1995" startWordPosition="3841" endWordPosition="3844">e a strong baseline for word prediction; for example, Brown et al. (1992)’s class-based language model failed to Model MRR LM n = 1 0.1253 LM n = 2 0.2884 LM n = 3 0.3278 LM n = 4 0.3305 LM n = 5 0.3297 AH 0.3591 Gold POS 0.3540 Table 5: Mean reciprocal rank on the word prediction task on the test set improve test-set perplexity over a word-based trigram model. 5 Semantic Inference Several experimental studies have shown that children and adults can infer (some aspects of) the semantic properties of a novel word based on the context it appears in (e.g. Landau &amp; Gleitman, 1985; Gleitman, 1990; Naigles &amp; Hoff-Ginsberg, 1995). For example, in an experimental study by Fisher et al. (2006), two-year-olds watched as a hand placed a duck on a box, and pointed to it as a new word was uttered. Half of the children heard the word presented as a noun (This is a corp!), while half heard it as a preposition (This is acorp my box!). After training, children heard a test sentence (What else is acorp (my box)?) while watching two test events: one showed another duck beside the box, and the other showed a different object on the box. Looking-preferences revealed effects of sentence context: subjects in the preposition condition</context>
</contexts>
<marker>Naigles, Hoff-Ginsberg, 1995</marker>
<rawString>Naigles, L., &amp; Hoff-Ginsberg, E. (1995). Input to Verb Learning: Evidence for the Plausibility of Syntactic Bootstrapping. Developmental Psychology, 31(5), 827–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Parisien</author>
<author>A Fazly</author>
<author>S Stevenson</author>
</authors>
<title>An incremental bayesian model for learning syntactic categories.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twelfth Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="2306" citStr="Parisien et al., 2008" startWordPosition="341" endWordPosition="344">tegories by children is necessarily incremental. Human language acquisition is bounded by memory and processing limitations, and it is implausible that humans process large volumes of text at once and induce an optimum set of categories. Efficient online computational models are needed to investigate whether distributional information is equally useful in an online process of word categorization. However, the few incremental models of category acquisition which have been proposed so far are generally inefficient and over-sensitive to the properties of the input data (Cartwright &amp; Brent, 1997; Parisien et al., 2008). Moreover, the unsupervised nature of these models makes their assessment a challenge, and the evaluation techniques proposed in the literature are limited. The main contributions of our research are twofold. First, we propose an incremental entropy model for efficiently clustering words into categories given their local context. We train our model on a corpus of child-directed speech from CHILDES (MacWhinney, 2000) and show that the model learns a fine-grained set of intuitive word categories. Second, we propose a novel evaluation approach by comparing the efficiency of our induced categorie</context>
<context position="5348" citStr="Parisien et al. (2008)" startWordPosition="824" endWordPosition="827">ocessing, and what is even more problematic, they hardwire important aspects of the model, such as the final number of categories. Only few previously proposed models process data incrementally, categorize word-tokens and do not pre-specify a fixed category set. The model of Cartwright &amp; Brent (1997) uses an algorithm which incrementally merges word clusters so that a Minimum Description Length criterion for a template grammar is optimized. The model treats whole sentences as contextual units, which sacrifices a degree of incrementality, as well as making it less robust to noise in the input. Parisien et al. (2008) propose a Bayesian clustering model which copes with ambiguity and exhibits the developmental trends observed in children (e.g. the order of acquisition of different categories). However, their model is overly sensitive to context variability, which results in the creation of sparse categories. To remedy this issue they introduce a “bootstrapping” component where the categories assigned to context words are use to determine the category of the current target word. They also perform periodical cluster reorganization. These mechanisms improve the overall performance of the model when trained on</context>
<context position="6771" citStr="Parisien et al., 2008" startWordPosition="1048" endWordPosition="1051">ry which can efficiently process naturalistic input data and gradually build robust categories with little training data. 1.2 Evaluation of the Induced Categories There is no standard and straightforward method for evaluating the unsupervised models of category learning (see Clark, 2003, for discussion). Many unsupervised models of lexical category acquisition treat the traditional part of speech (PoS) tags as the gold standard, and measure the accuracy and completeness of their induced categories based on how closely they resemble the PoS categories (e.g. Redington et al., 1998; Mintz, 2003; Parisien et al., 2008). However, it is not at all clear whether humans form the same types of categories. In fact, many language tasks might benefit from finer-grained categories than the traditional PoS tags used for corpus annotation. Frank et al. (2009) propose a different, automatically generated set of gold standard categories for evaluating an unsupervised categorization model. The gold-standard categories are formed according to “substitutability”: if one word can be replaced by another and the resulting sentence is still grammatical, then there is a good chance that the two words belong to the same category</context>
<context position="16456" citStr="Parisien et al. (2008)" startWordPosition="2757" endWordPosition="2760"> categories (of which 442 have only one member). Table 3 shows two sample categories from the training set, and Figure 1 shows the size distribution of the categories. For each evaluation task, we use the following category sets to label the test set: OH. The categories induced by our entropybased model from the training set, as described above. PoS. The part-of-speech tags the Manchester corpus is annotated with. Words. The set of all the word types in the data set (i.e. assuming that all the usages of the same word form are grouped together). Parisien. The induced categories by the model of Parisien et al. (2008) from the training set. 185 Gold PoS Words Parisien OH VI (0.000) 5.294 5.983 4.806 ARI (1.000) 0.139 0.099 0.168 Table 2: Comparison against gold PoS tags using Variation of Information (VI) and Adjusted Rand Index (ARI). 5 50 500 5000 1 2 5 10 20 50 100 Frequency Size Figure 1: The distribution of the induced categories based on their size Category size frequencies Sample Cluster 1 Sample Cluster 2 going (928) than (45) doing (190) more (20) back (150) silly (10) coming (80) bigger (9) looking (76) frightened (5) making (64) dark (4) playing (55) harder (4) taking (45) funny (3) � � � � � � </context>
<context position="25990" citStr="Parisien et al. (2008)" startWordPosition="4450" endWordPosition="4453">sion for a ranked list F with respect to a set R of correct features is: � �� � 1 APR(F) = |R |_1 r where P(r) is precision at rank r and 1R is the indicator function of set R. The middle row of Table 4 shows the MAP scores over all the noun or verb usages in the test set, based on four different category sets. As can be seen, the categories induced by our model (OH) outperform all the other category sets. The word-type categories are particularly unsuitable for this task, since they provide the least degree of generalization over the semantic properties of a group of words. The categories of Parisien et al. (2008) result in a better performance than word types, but they are still too sparse for this task. However, the average score gained by part of speech tags is also lower than the one by our categories. This suggests that too broad categories are also unsuitable for this task, since they can only provide predictions about the most general semantic properties, such as ENTITY for nouns, and ACTION for verbs. These findings again confirm our hypothesis that a finer-grained set of categories that are extracted directly from the input data provide the highest predictive power in a naturalistic language t</context>
<context position="31159" citStr="Parisien et al., 2008" startWordPosition="5304" endWordPosition="5307">ng the clusters that have recently been updated, in order to allow for recovery from early mistakes the model has made. We used the categories induced by our model in word prediction, inferring the semantic properties of novel words, and grammaticality judgment. Our experimental results show that the performance in these tasks using our categories is comparable or better than the performance based on the manually assigned part of speech tags in our experimental data. Furthermore, in all these tasks the performance using our categories improves over a previous incremental categorization model (Parisien et al., 2008). However, the model of Parisien employs a number of cluster reorganization techniques which improve the overall quality of the clusters after processing a substantial amount of input data. In future we plan to increase the size of our training data, and perform a more extensive comparison with the model of Parisien et al. (2008). The promising results of our experiments suggest that an information-theoretic approach is a plausible one for modeling the induction of lexical categories from distributional data. Our results imply that in many language tasks, a fine-grained set of categories which</context>
</contexts>
<marker>Parisien, Fazly, Stevenson, 2008</marker>
<rawString>Parisien, C., Fazly, A., &amp; Stevenson, S. (2008). An incremental bayesian model for learning syntactic categories. In Proceedings of the Twelfth Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Redington</author>
<author>N Crater</author>
<author>Finch</author>
</authors>
<date>1998</date>
<note>Distributional information: A powerful cue for ac-</note>
<contexts>
<context position="1634" citStr="Redington et al., 1998" startWordPosition="237" endWordPosition="240">dge of the abstract lexical categories such as nouns, verbs and determiners (e.g., Gelman &amp; Taylor, 1984; Kemp et al., 2005). Children’s grouping of words into categories might be based on various cues, including phonological and morphological properties of a word, the distributional information about its surrounding context, and its semantic features. Among these, the distributional properties of the local context of a word have been thoroughly studied. It has been shown that child-directed speech provides informative co-occurrence cues, which can be reliably used to form lexical categories (Redington et al., 1998; Mintz, 2003). The process of learning lexical categories by children is necessarily incremental. Human language acquisition is bounded by memory and processing limitations, and it is implausible that humans process large volumes of text at once and induce an optimum set of categories. Efficient online computational models are needed to investigate whether distributional information is equally useful in an online process of word categorization. However, the few incremental models of category acquisition which have been proposed so far are generally inefficient and over-sensitive to the proper</context>
<context position="3714" citStr="Redington et al., 1998" startWordPosition="563" endWordPosition="566">sed on its sentential context), semantic inference (where the semantic properties of a novel word are predicted based on the context), and grammaticality judgment (where the syntactic well-formedness of a sentence is assessed based on the category labels assigned to its words). The results show that the categories induced by our model can be successfully used in a variety of tasks and typically perform better than other category sets. 1.1 Unsupervised Models of Category Induction Several computational models have used distributional information for categorizing words (e.g. Brown et al., 1992; Redington et al., 1998; Clark, 2000; Mintz, 2002). The majority of these mod182 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 182–191, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics els partition the vocabulary into a set of optimum clusters (e.g., Brown et al., 1992; Clark, 2000). The generated clusters are intuitive, and can be used in different tasks such as word prediction and parsing. Moreover, these models confirm the learnability of abstract word categories, and show that distributional cues are a useful source of information fo</context>
<context position="6734" citStr="Redington et al., 1998" startWordPosition="1042" endWordPosition="1045">n incremental model of lexical category which can efficiently process naturalistic input data and gradually build robust categories with little training data. 1.2 Evaluation of the Induced Categories There is no standard and straightforward method for evaluating the unsupervised models of category learning (see Clark, 2003, for discussion). Many unsupervised models of lexical category acquisition treat the traditional part of speech (PoS) tags as the gold standard, and measure the accuracy and completeness of their induced categories based on how closely they resemble the PoS categories (e.g. Redington et al., 1998; Mintz, 2003; Parisien et al., 2008). However, it is not at all clear whether humans form the same types of categories. In fact, many language tasks might benefit from finer-grained categories than the traditional PoS tags used for corpus annotation. Frank et al. (2009) propose a different, automatically generated set of gold standard categories for evaluating an unsupervised categorization model. The gold-standard categories are formed according to “substitutability”: if one word can be replaced by another and the resulting sentence is still grammatical, then there is a good chance that the </context>
</contexts>
<marker>Redington, Crater, Finch, 1998</marker>
<rawString>Redington, M., Crater, N., &amp; Finch, S.(1998). Distributional information: A powerful cue for ac-</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>