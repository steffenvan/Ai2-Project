<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.032004">
<title confidence="0.997969">
Combining Acoustic Confidences and Pragmatic Plausibility for Classifying
Spoken Chess Move Instructions
</title>
<author confidence="0.986046">
Malte Gabsdil
</author>
<affiliation confidence="0.909141333333333">
Department of Computational Linguistics
Saarland University
Germany
</affiliation>
<email confidence="0.991734">
gabsdil@coli.uni-sb.de
</email>
<sectionHeader confidence="0.995515" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999829333333333">
This paper describes a machine learning ap-
proach to classifying n-best speech recogni-
tion hypotheses as either correctly or incor-
rectly recognised. The learners are trained on
a combination of acoustic confidence features
and move evaluation scores in a chess-playing
scenario. The results show significant improve-
ments over sharp baselines that use confidence
rejection thresholds for classification.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999948714285714">
An important task in designing spoken dialogue systems
is to decide whether a system should accept (consider
correctly recognised) or reject (assume misrecognition)
a user utterance. This decision is often based on acoustic
confidence scores computed by the speech recogniser and
a fixed confidence rejection threshold. However, a draw-
back of this approach is that it does not take into account
that in particular dialogue situations some utterances are
pragmatically more plausible than others.1
This paper describes machine learning experiments
that combine acoustic confidences with move evaluation
scores to classify n-best recognition hypothesis of spoken
chess move instructions (e.g. “pawn e2 e4”) as correctly
or incorrectly recognised. Classifying the n-best recog-
nition hypotheses instead of the single-best (e.g. (Walker
et al., 2000)) has the advantage that a correct hypothe-
sis can be accepted even when it is not the highest scor-
ing recognition result. Previous work on n-best hypothe-
sis reordering (e.g. (Chotimongkol and Rudnicky, 2001))
has focused on selecting hypotheses with the lowest rel-
ative word error rate. In contrast, our approach makes
</bodyText>
<footnote confidence="0.99009175">
1Although it is possible to use dialogue-state dependent
recognition grammars that reflect expectations of what the user
is likely to say next, these expectations do not say anything
about the plausibility of hypotheses.
</footnote>
<bodyText confidence="0.997683">
predictions about whether hypotheses should be accepted
or rejected. The learning experiments show significantly
improved classification results over competitive baselines
and underline the usefulness of incorporating higher-level
information for utterances classification.
</bodyText>
<sectionHeader confidence="0.93996" genericHeader="introduction">
2 Domain and Data Collection
</sectionHeader>
<bodyText confidence="0.999707642857143">
The domain of our research are spoken chess move in-
structions. We chose this scenario as a testbed for our
approach for three main reasons. First, we can use move
evaluation scores computed by a computer chess pro-
gram as a measure of the pragmatic plausibility of hy-
potheses. Second, the domain is simple and allows us
to collect data in a controlled way (e.g. we can control
for player strength), and third, the domain is restricted in
the sense that there is only a finite set of possible legal
moves in every situation. Similar considerations already
let researchers in the 1970s choose chess-playing as an
example scenario for the HEARSAY integrated speech
understanding system (Reddy and Newell, 1974).
We collected spoken chess move instructions in a small
experiment from six pairs of chess players. All sub-
jects were German native speakers and familiar with
the rules of chess. The subject’s task was to re-play
chess games (given to them as graphical representations)
by instructing each other to move pieces on the board.
Altogether, we collected 1978 move instructions under
different experimental conditions (e.g. strong games vs.
weak games) in the following four data sets: 1) language
model, 2) training, 3) development, and 4) test.
The recordings of the language model games were
transcribed and served to construct a context free recog-
nition grammar for the Nuance 8.02 speech recogniser
which was then used to process all other move instruc-
tions with 10-best output.
</bodyText>
<footnote confidence="0.995172">
2http://www.nuance.com. We thank Nuance Inc. for
making their speech recognition software available to us.
</footnote>
<sectionHeader confidence="0.977501" genericHeader="method">
3 Baseline Systems
</sectionHeader>
<bodyText confidence="0.9998684">
The general aim of our experiments is to decide whether
a recognised move instruction is the one intended by the
speaker. A system should accept correct recognition hy-
potheses and reject incorrect ones. We define the follow-
ing two baseline systems for this binary decision task.
</bodyText>
<subsectionHeader confidence="0.99977">
3.1 First Hypothesis Baseline
</subsectionHeader>
<bodyText confidence="0.999983941176471">
The first hypothesis baseline uses a confidence rejection
threshold to decide whether the best recognition hypoth-
esis should be accepted or rejected. To find an optimal
value, we linearly vary the confidence threshold returned
by the Nuance 8.0 recogniser (integral values in the range
) and use it to classify the training and develop-
ment data.
The best performing confidence threshold on the com-
bined training and development data was 17 with an accu-
racy of 63.8%. This low confidence threshold turned out
to be equal to the majority class baseline which is to clas-
sify all hypotheses as correctly recognised. In order to
get a more balanced distribution of classification errors,
we also optimised the confidence threshold according to
the cost measure defined in Section 5. According to this
measure, the optimal confidence rejection threshold is 45
with a classification accuracy of 60.5%.3
</bodyText>
<subsectionHeader confidence="0.99977">
3.2 First Legal Move Baseline
</subsectionHeader>
<bodyText confidence="0.999902">
The first legal move baseline makes use of the constraint
that user utterances only contain moves that are legal in
the current board configuration. We thus first eliminate
all hypotheses that denote illegal moves from the 10-best
output and then apply a confidence rejection threshold to
decide whether the best legal hypothesis should be ac-
cepted or rejected.
The best performing confidence threshold on the com-
bined training and test data for the first legal move base-
line was 23 with an accuracy of 92.4%. This threshold
also optimised the cost measure defined in Section 5. The
performance of both baseline systems on the test data is
reported below in Table 2 together with the results for the
machine learning experiments.
</bodyText>
<sectionHeader confidence="0.996935" genericHeader="method">
4 ML Experiments
</sectionHeader>
<bodyText confidence="0.999962125">
We devise two different machine learning experiments
for selecting hypotheses from the recogniser’s n-best out-
put and from a list of all legal moves given a certain board
configuration.
In Experiment 1, we first filter out all illegal moves
from the n-best recognition results and represent the re-
maining legal moves in terms of 32 dimensional fea-
ture vectors including acoustic confidence scores from
</bodyText>
<footnote confidence="0.812725">
345 is also the default confidence rejection threshold of the
Nuance 8.0 speech recogniser.
</footnote>
<bodyText confidence="0.999917">
the recogniser as well as move evaluation scores from a
computer chess program. We then use machine learners
to decide for each move hypothesis whether it was the
one intended by the speaker. If more than one hypothesis
is classified as correct, we pick the one with the highest
acoustic confidence. If there is no legal move among the
recognition hypotheses or all hypotheses are classified as
incorrect, the input is rejected.
Experiment 2 adds a second classification step to Ex-
periment 1. In case an utterance is rejected in Experiment
1, we try to find the intended move among all legal moves
in the current situation. This is again defined in terms of
a classification problem. All legal moves are represented
by 31 dimensional feature vectors that include “similar-
ity features” with respect to the interpretation of the best
recognition hypothesis and move evaluation scores. Each
move is then classified as either correct or incorrect. We
pick a move if it is the only one that is classified as correct
and all others as incorrect; otherwise the input is rejected.
The average number of legal moves in the development
and training games was 35.3 with a maximum of 61.
</bodyText>
<subsectionHeader confidence="0.987912">
4.1 Feature Sets
</subsectionHeader>
<bodyText confidence="0.999878166666667">
The feature set for the classification of legal move hy-
potheses in the recogniser’s n-best list (Experiment 1)
consists of 32 features that can be coarsely grouped into
six categories (see below). All features were automati-
cally extracted or computed from the output of the speech
recogniser, move evaluation scores, and game logs.
</bodyText>
<listItem confidence="0.9997546">
1. Recognition statistics (3): position in n-best list;
relative position among and total number of legal
moves in n-best list
2. Acoustic confidences (6): overall acoustic confi-
dence; min, max, mean, variance, standard deviation
of individual word confidences
3. Text (1): hypothesis length (in words)
4. Depth1 plausibility (10): raw &amp; normalised move
evaluation score wrt. scores for all legal moves;
score rank; raw score difference to max score;
min, max, mean of raw scores; raw z-score; move
evaluation rank &amp; z-score among n-best legal moves
5. Depth10 plausibility (10): same features as for
depth1 plausibility (at search depth 10)
6. Game (2): ELO (strength) of player; ply number
</listItem>
<bodyText confidence="0.87322175">
The feature set for the classification of all legal moves
in Experiment 2 is summarised below. Each move is rep-
resented in terms of 31 (automatically derived) features
which can again be grouped into 6 different categories.
</bodyText>
<listItem confidence="0.99702875">
1. Similarity (5): difference size; difference bags;
overlap size; overlap bag
2. Acoustic confidences (6): same as in Experiment 1
for best recognition hypothesis
</listItem>
<figure confidence="0.996144818181818">
reject correct/
reject incorrect
Class
accept correct
accept incorrect
move
instruct – move – object –
move – instruct – move
instruct – move
instruct – reject – instruct –
Sequence
</figure>
<tableCaption confidence="0.620462">
Table 1: Cost measure
</tableCaption>
<figure confidence="0.9848095">
Cost
0
2
4
</figure>
<listItem confidence="0.999459857142857">
3. Text (2): length of best recognition hypothesis (in
words) and recognised string (bag of words)
4. Depth1 plausibility (8): same as in Experiment 1
(w/o features relating to n-best legal moves)
5. Depth10 plausibility (8): same as in Experiment 1
(w/o features relating to n-best legal moves)
6. Game (2): same as in Experiment 1
</listItem>
<bodyText confidence="0.999757692307693">
The similarity features are meant to represent how
close a move is to the interpretation of the best recogni-
tion result. The motivation for these features is that the
machine learner might find regularities about what likely
confusions arise in the data. For example, the letters “b”,
“c”, “d”, “e” and “g” are phonemically similar in Ger-
man (as are the letters “a” and “h” and the two digits
“zwei” and “drei”). Although the move representations
are abstractions from the actual verbalisations, the lan-
guage model data showed that most of the subjects re-
ferred to coordinates with single letters and digits and
therefore there is some correspondence between the ab-
stract representations and what was actually said.
</bodyText>
<subsectionHeader confidence="0.978725">
4.2 Learners
</subsectionHeader>
<bodyText confidence="0.999493923076923">
We considered three different machine learners for
the two classification tasks: the memory-based learner
TiMBL (Daelemans et al., 2002), the rule induction
learner RIPPER (Cohen, 1995), and an implementation
of Support Vector Machines, SVM (Joachims, 1999).
We trained all learners with various parameter settings
on the training data and tested them on the development
data. The best results for the first task (selecting legal
moves from n-best lists) were achieved with SVM
whereas RIPPER outperformed the other two learners on
the second task (selecting from all possible legal moves).
SVM and RIPPER where therefore chosen to clas-
sify the test data in the actual experiments.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="evaluation">
5 Results and Evaluation
</sectionHeader>
<subsectionHeader confidence="0.991881">
5.1 Cost Measure
</subsectionHeader>
<bodyText confidence="0.9999957">
We evaluate the task of selecting correct hypotheses with
two different metrics: i) classification accuracy and ii) a
simple cost measure that computes a score for different
classifications on the basis of their confusion matrices.
Table 1 shows how we derived costs from the additional
number of steps (verbal and non-verbal) that have to be
taken in order to carry out a user move instruction. Note
that the cost measure is not validated against user judge-
ments and should therefore only be considered an indica-
tor for the (relative) quality of a classification.
</bodyText>
<subsectionHeader confidence="0.546225">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.993535142857143">
Table 2 reports the raw classification results for the differ-
ent baselines and machine learning experiments together
with their accuracy and associated cost. Here and in sub-
sequent tables, FH and FH refer to the first hypoth-
esis baselines with confidence thresholds 17 and 45 re-
spectively, FLM to the first legal move baseline, and Exp1
and Exp2 to Experiments 1 and 2 respectively.
</bodyText>
<table confidence="0.999109125">
accept reject
FH (Acc: 61.7% Cost: 1230)
correct 489 0
incorrect 306 3
FH (Acc: 64.3% Cost: 1188)
correct 441 48
incorrect 237 72
FLM (Acc: 93.5% Cost: 358)
correct 671 0
incorrect 52 75
Exp1 (Acc: 97.2% Cost: 246)
correct 695 2
incorrect 20 81
Exp2 (Acc: 97.2% Cost: 176)
correct 731 1
incorrect 21 45
</table>
<tableCaption confidence="0.997385">
Table 2: Raw classification results
</tableCaption>
<bodyText confidence="0.999989095238095">
The most striking result in Table 2 is the huge classifi-
cation improvement between the first hypothesis and the
first legal move baselines. For our domain, this shows a
clear advantage of n-best recognition processing filtered
with “hard” domain constraints (i.e. legal moves) over
single-best processing.
Note that the results for Exp1 and Exp2 in Table 2 are
given “by utterance” (i.e. they do not reflect the classi-
fication performance for individual hypotheses from the
n-best lists and the lists of all legal moves). Note also
that both the different baselines and the machine learning
systems have access to different information sources and
therefore what counts as correctly or incorrectly classi-
fied varies. For example, the gold standard for the first
hypothesis baseline only considers the best recognition
result for each move instruction. If this is not the one in-
tended by the speaker, it counts as incorrect in the gold
standard. On the other hand, the first legal move among
the 10-best recognition hypotheses for the same utterance
might well be the correct one and would therefore count
as correct in the gold standard for the FLM baseline.
</bodyText>
<subsectionHeader confidence="0.999248">
5.3 Comparing Classification Systems
</subsectionHeader>
<bodyText confidence="0.982035230769231">
We use the test of independence to compute whether
the classification results are significantly different from
each other. Table 3 reports significance results for com-
paring the different classifications of the test data. The
table entries include the differences in cost and the level
of statistical difference between the confusion matrices
as computed by the statistics ( denotes significance
at , at , and at ). The table
should be read row by row. For example, the top row
in Table 3 compares the classification from Exp2 to all
other classifications. The value means that the
cost compared to FH is reduced by 1054 and that the
confusion matrices are significantly different at .
</bodyText>
<tableCaption confidence="0.86049825">
Table 3: Cost comparisons and levels of significance
for all test games
Tables 4 and 5 compare the performance of the differ-
ent systems for strong and weak games (a variable con-
trolled for during data collection).
Table 4: Cost comparisons and levels of significance
for strong test games
Table 5: Cost comparisons and levels of significance
</tableCaption>
<bodyText confidence="0.932878285714286">
for weak test games
The results show that the machine learning systems
perform better for the strong test data. We conjecture
that the poorer results for the weak data are due to more
bad moves in these games which receive a low evaluation
score and might therefore be considered incorrect by the
learners.
</bodyText>
<sectionHeader confidence="0.999285" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999967217391304">
We presented a machine learning approach that combines
acoustic confidence scores with automatic move evalua-
tions for selecting from the n-best speech recognition hy-
potheses in a chess playing scenario and compared the
results to two different baselines.
The chess scenario is well suited for our experiments
because it allowed us to filter out impossible moves and
to use a computer chess program to assess the plausibil-
ity of legal moves. However, the methodology underly-
ing Experiment 1 can be applied to other spoken dialogue
systems to choose interpretation(s) from a recogniser’s n-
best output. We have successfully used this setup for clas-
sifying hypotheses in a command and control spoken di-
alogue system (Gabsdil and Lemon, subm). Experiment
2 exploits the fact that the number of possible interpreta-
tions is finite in the chess scenario. Although this obvi-
ously does not hold for many dialogue tasks, there are ap-
plications such as call routing (e.g. (Walker et al., 2000))
where the number of possible interpretations is limited in
a similar way. Instead of selecting correct interpretations,
we imagine that one could also use the proposed setup to
decide which of a finite set of dialogue moves was per-
formed by a speaker.
</bodyText>
<sectionHeader confidence="0.998141" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99895276">
Ananlada Chotimongkol and Alexander I. Rudnicky.
2001. N-best Speech Hypotheses Reordering Using
Linear Regression. In Proceedings ofEuroSpeech-01.
William W. Cohen. 1995. Fast Effective Rule Induction.
In Proceedings ofICML-95.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2002. TIMBL: Tilburg Mem-
mory Based Learner, version 4.2, Reference Guide.
Available from http://ilk.kub.nl/downloads/
pub/papers/ilk0201.ps.gz.
Malte Gabsdil and Oliver Lemon. subm. Combining
acoustic and pragmatic features to predict recognition
performance in spoken dialogue systems. Submitted
to ACL-04.
Thorsten Joachims. 1999. Making Large-Scale SVM
Learning Practical. In B. Schlkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods – Sup-
port Vector Learning, pages 41–55. MIT Press.
R. Reddy and A. Newell. 1974. Knowledge and its rep-
resentation in a speech understanding system. In L.W.
Gregg, editor, Knowledge and Cognition.
Marilyn Walker, Jerry Wright, and Irene Langkilde.
2000. Using Natural Language Processing and Dis-
course Features to Identify Understanding Errors in a
Spoken Dialogue System. In Proceedings ofICML-00.
</reference>
<figure confidence="0.99905025">
Exp2
Exp1
FLM
FH
FH
FH
FLM
Exp1
Exp2
Exp1
FLM
FH
FH
FH
FLM
Exp1
Exp2
Exp1
FLM
FH
FH
FH
FLM
Exp1
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.562938">
<title confidence="0.9929405">Combining Acoustic Confidences and Pragmatic Plausibility for Classifying Spoken Chess Move Instructions</title>
<author confidence="0.900573">Malte</author>
<affiliation confidence="0.99916">Department of Computational</affiliation>
<address confidence="0.647544">Saarland</address>
<email confidence="0.99773">gabsdil@coli.uni-sb.de</email>
<abstract confidence="0.9972823">This paper describes a machine learning approach to classifying n-best speech recognition hypotheses as either correctly or incorrectly recognised. The learners are trained on a combination of acoustic confidence features and move evaluation scores in a chess-playing scenario. The results show significant improvements over sharp baselines that use confidence rejection thresholds for classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ananlada Chotimongkol</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>N-best Speech Hypotheses Reordering Using Linear Regression.</title>
<date>2001</date>
<booktitle>In Proceedings ofEuroSpeech-01.</booktitle>
<contexts>
<context position="1688" citStr="Chotimongkol and Rudnicky, 2001" startWordPosition="234" endWordPosition="237">ue situations some utterances are pragmatically more plausible than others.1 This paper describes machine learning experiments that combine acoustic confidences with move evaluation scores to classify n-best recognition hypothesis of spoken chess move instructions (e.g. “pawn e2 e4”) as correctly or incorrectly recognised. Classifying the n-best recognition hypotheses instead of the single-best (e.g. (Walker et al., 2000)) has the advantage that a correct hypothesis can be accepted even when it is not the highest scoring recognition result. Previous work on n-best hypothesis reordering (e.g. (Chotimongkol and Rudnicky, 2001)) has focused on selecting hypotheses with the lowest relative word error rate. In contrast, our approach makes 1Although it is possible to use dialogue-state dependent recognition grammars that reflect expectations of what the user is likely to say next, these expectations do not say anything about the plausibility of hypotheses. predictions about whether hypotheses should be accepted or rejected. The learning experiments show significantly improved classification results over competitive baselines and underline the usefulness of incorporating higher-level information for utterances classific</context>
</contexts>
<marker>Chotimongkol, Rudnicky, 2001</marker>
<rawString>Ananlada Chotimongkol and Alexander I. Rudnicky. 2001. N-best Speech Hypotheses Reordering Using Linear Regression. In Proceedings ofEuroSpeech-01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
</authors>
<title>Fast Effective Rule Induction.</title>
<date>1995</date>
<booktitle>In Proceedings ofICML-95.</booktitle>
<contexts>
<context position="10479" citStr="Cohen, 1995" startWordPosition="1652" endWordPosition="1653"> “g” are phonemically similar in German (as are the letters “a” and “h” and the two digits “zwei” and “drei”). Although the move representations are abstractions from the actual verbalisations, the language model data showed that most of the subjects referred to coordinates with single letters and digits and therefore there is some correspondence between the abstract representations and what was actually said. 4.2 Learners We considered three different machine learners for the two classification tasks: the memory-based learner TiMBL (Daelemans et al., 2002), the rule induction learner RIPPER (Cohen, 1995), and an implementation of Support Vector Machines, SVM (Joachims, 1999). We trained all learners with various parameter settings on the training data and tested them on the development data. The best results for the first task (selecting legal moves from n-best lists) were achieved with SVM whereas RIPPER outperformed the other two learners on the second task (selecting from all possible legal moves). SVM and RIPPER where therefore chosen to classify the test data in the actual experiments. 5 Results and Evaluation 5.1 Cost Measure We evaluate the task of selecting correct hypotheses with two</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>William W. Cohen. 1995. Fast Effective Rule Induction. In Proceedings ofICML-95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Ko van der Sloot</author>
<author>Antal van den Bosch</author>
</authors>
<title>TIMBL: Tilburg Memmory Based Learner, version 4.2, Reference Guide. Available from http://ilk.kub.nl/downloads/</title>
<date>2002</date>
<pages>0201</pages>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 2002</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch. 2002. TIMBL: Tilburg Memmory Based Learner, version 4.2, Reference Guide. Available from http://ilk.kub.nl/downloads/ pub/papers/ilk0201.ps.gz.</rawString>
</citation>
<citation valid="false">
<authors>
<author>subm</author>
</authors>
<title>Combining acoustic and pragmatic features to predict recognition performance in spoken dialogue systems.</title>
<note>Submitted to ACL-04.</note>
<marker>subm, </marker>
<rawString>Malte Gabsdil and Oliver Lemon. subm. Combining acoustic and pragmatic features to predict recognition performance in spoken dialogue systems. Submitted to ACL-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making Large-Scale SVM Learning Practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods – Support Vector Learning,</booktitle>
<pages>41--55</pages>
<editor>In B. Schlkopf, C. Burges, and A. Smola, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="10551" citStr="Joachims, 1999" startWordPosition="1662" endWordPosition="1663">h” and the two digits “zwei” and “drei”). Although the move representations are abstractions from the actual verbalisations, the language model data showed that most of the subjects referred to coordinates with single letters and digits and therefore there is some correspondence between the abstract representations and what was actually said. 4.2 Learners We considered three different machine learners for the two classification tasks: the memory-based learner TiMBL (Daelemans et al., 2002), the rule induction learner RIPPER (Cohen, 1995), and an implementation of Support Vector Machines, SVM (Joachims, 1999). We trained all learners with various parameter settings on the training data and tested them on the development data. The best results for the first task (selecting legal moves from n-best lists) were achieved with SVM whereas RIPPER outperformed the other two learners on the second task (selecting from all possible legal moves). SVM and RIPPER where therefore chosen to classify the test data in the actual experiments. 5 Results and Evaluation 5.1 Cost Measure We evaluate the task of selecting correct hypotheses with two different metrics: i) classification accuracy and ii) a simple cost mea</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making Large-Scale SVM Learning Practical. In B. Schlkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods – Support Vector Learning, pages 41–55. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Reddy</author>
<author>A Newell</author>
</authors>
<title>Knowledge and its representation in a speech understanding system.</title>
<date>1974</date>
<booktitle>Knowledge and Cognition.</booktitle>
<editor>In L.W. Gregg, editor,</editor>
<contexts>
<context position="3030" citStr="Reddy and Newell, 1974" startWordPosition="440" endWordPosition="443">io as a testbed for our approach for three main reasons. First, we can use move evaluation scores computed by a computer chess program as a measure of the pragmatic plausibility of hypotheses. Second, the domain is simple and allows us to collect data in a controlled way (e.g. we can control for player strength), and third, the domain is restricted in the sense that there is only a finite set of possible legal moves in every situation. Similar considerations already let researchers in the 1970s choose chess-playing as an example scenario for the HEARSAY integrated speech understanding system (Reddy and Newell, 1974). We collected spoken chess move instructions in a small experiment from six pairs of chess players. All subjects were German native speakers and familiar with the rules of chess. The subject’s task was to re-play chess games (given to them as graphical representations) by instructing each other to move pieces on the board. Altogether, we collected 1978 move instructions under different experimental conditions (e.g. strong games vs. weak games) in the following four data sets: 1) language model, 2) training, 3) development, and 4) test. The recordings of the language model games were transcrib</context>
</contexts>
<marker>Reddy, Newell, 1974</marker>
<rawString>R. Reddy and A. Newell. 1974. Knowledge and its representation in a speech understanding system. In L.W. Gregg, editor, Knowledge and Cognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
<author>Jerry Wright</author>
<author>Irene Langkilde</author>
</authors>
<title>Using Natural Language Processing and Discourse Features to Identify Understanding Errors in a Spoken Dialogue System. In</title>
<date>2000</date>
<booktitle>Proceedings ofICML-00.</booktitle>
<contexts>
<context position="1481" citStr="Walker et al., 2000" startWordPosition="200" endWordPosition="203">onfidence scores computed by the speech recogniser and a fixed confidence rejection threshold. However, a drawback of this approach is that it does not take into account that in particular dialogue situations some utterances are pragmatically more plausible than others.1 This paper describes machine learning experiments that combine acoustic confidences with move evaluation scores to classify n-best recognition hypothesis of spoken chess move instructions (e.g. “pawn e2 e4”) as correctly or incorrectly recognised. Classifying the n-best recognition hypotheses instead of the single-best (e.g. (Walker et al., 2000)) has the advantage that a correct hypothesis can be accepted even when it is not the highest scoring recognition result. Previous work on n-best hypothesis reordering (e.g. (Chotimongkol and Rudnicky, 2001)) has focused on selecting hypotheses with the lowest relative word error rate. In contrast, our approach makes 1Although it is possible to use dialogue-state dependent recognition grammars that reflect expectations of what the user is likely to say next, these expectations do not say anything about the plausibility of hypotheses. predictions about whether hypotheses should be accepted or r</context>
</contexts>
<marker>Walker, Wright, Langkilde, 2000</marker>
<rawString>Marilyn Walker, Jerry Wright, and Irene Langkilde. 2000. Using Natural Language Processing and Discourse Features to Identify Understanding Errors in a Spoken Dialogue System. In Proceedings ofICML-00.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>