<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000317">
<title confidence="0.997178">
An Architecture for Dialogue Management, Context Tracking,
and Pragmatic Adaptation in Spoken Dialogue Systems
</title>
<author confidence="0.808071">
Susann LuperFoy, Dan Loehr, David Duff, Keith Miller, Florence Reeder, Lisa Harper
</author>
<note confidence="0.6543005">
The MITRE Corporation
1820 Dolley Madison Boulevard, McLean VA 22102 USA
</note>
<email confidence="0.851683">
luperfoy, loehr, duff, keith, freeder, lisah } @mitre.org
</email>
<sectionHeader confidence="0.996944" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999982083333334">
This paper details a software architecture for
discourse processing in spoken dialogue
systems, where the three component tasks of
discourse processing are (1) Dialogue Man-
agement, (2) Context Tracking, and (3)
Pragmatic Adaptation. We define these three
component tasks and describe their roles in a
complex, near-future scenario in which
multiple humans interact with each other
and with computers in multiple, simulta-
neous dialogue exchanges. This paper
reports on the software modules that accom-
plish the three component tasks of discourse
processing, and an architecture for the inter-
action among these modules and with other
modules of the spoken dialogue system. A
motivation of this work is reusable discourse
processing software for integration with
non-discourse modules in spoken dialogue
systems. We document the use of this ar-
chitecture and its components in several
prototypes, and also discuss its potential ap-
plication to spoken dialogue systems defined
in the near-future scenario.
</bodyText>
<sectionHeader confidence="0.972051" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.99995611627907">
We present an architecture for spoken dialogue
systems for both human-computer interaction
and computer mediation or analysis of human
dialogue. The architecture shares many compo-
nents with those of existing spoken dialogue
systems, such as CommandTalk (Moore et al.
1997), Galaxy (Goddeau et al. 1994), TRAINS
(Allen et al. 1995), Verbmobil (Wahlster 1993),
Waxholm (Carlson 1996), and others. Our ar-
chitecture is distinguished from these in its
treatment of discourse-level processing.
Most architectures, including ours, contain mod-
ules for speech recognition and natural language
interpretation (such as morphology, syntax, and
sentential semantics). Many include a module
for interfacing with the back-end application. If
the dialogue is two-way, the architectures also
include modules for natural language generation
and speech synthesis.
Architectures differ in how they handle dis-
course. Some have a single, separate module
labeled &amp;quot;discourse processor&amp;quot;, &amp;quot;dialogue com-
ponent&amp;quot;, or perhaps &amp;quot;contextual interpretation&amp;quot;.
Others, including earlier versions of our system,
bury discourse functions inside other modules,
such as natural language interpretation or the
back-end interface.
An innovation of this work is the compartmen-
talization of discourse processing into three gen-
erically definable components—Dialogue Man-
agement, Context Tracking, and Pragmatic Ad-
aptation (described in Section 1 below)—and the
software control structure for interaction be-
tween these and other components of a spoken
dialogue system (Section 2).
In Section 3, we examine the dialogue process-
ing requirement in a complex scenario involv-
ing multiple users and multiple simultaneous
dialogues of diverse types. We describe how
our architecture supports implementations of
such a scenario. Finally, we describe two im-
plemented spoken dialogue systems that embody
this architecture (Section 4).
</bodyText>
<sectionHeader confidence="0.9407465" genericHeader="method">
1 Component Tasks of Discourse
Processing
</sectionHeader>
<bodyText confidence="0.998372333333333">
We divide discourse-level processing into three
component tasks: Dialogue Management, Con-
text Tracking, and Pragmatic Adaptation.
</bodyText>
<subsectionHeader confidence="0.983283">
1.1 Dialogue Management
</subsectionHeader>
<bodyText confidence="0.999853">
The Dialogue Manager is an oversight module
whose purpose is to facilitate the interaction
between dialogue participants. In a user-initiated
system, the dialogue manager directs the proc-
essing of an input utterance from one component
to another through interpretation and back-end
</bodyText>
<page confidence="0.996698">
794
</page>
<bodyText confidence="0.997483454545455">
system response. In the process, it detects and
handles dialogue trouble, invokes the context
tracker when updates are necessary, generates
system output, and so on.
Our conception of Dialogue Manager as con-
troller becomes increasingly relevant as the
software system moves away from the standard
&amp;quot;NL pipeline&amp;quot; in order to deal with dialogue
disfluencies. Its oversight perspective affords it
(and the architecture) certain capabilities, which
are listed in Table 1.
</bodyText>
<table confidence="0.6944724">
1 Capability
Supports mixed-initiative system by fielding sponta-
neous input from either participant and routing it to
the appropriate components.
2 Supports non-linguistic dialogue &amp;quot;events&amp;quot; by accept-
ing them and routing them to the Context Tracker
(below).
3 Increases overall system performance. For example,
awareness of system output allows the Dialogue
Manager to predict user input, boosting speech
recognition accuracy. Similarly, if the back-end intro-
duces a new word into the discourse, the Dialogue
Manager can request the speech recognizer to add it
to its vocabulary for later recognition.
4 Supports meta-dialogues between the dialogue sys-
</table>
<bodyText confidence="0.35586">
tem itself and either participant. An example might be
a participant&apos;s questions about the status of the dia-
logue system.
5 Acts as a central point for dialogue troubleshooting,
after (Duff et al. 1996). If any component has insuffi-
cient input to perform its task, it can alert the Dia-
logue Manager, which can then reconsult a previously
invoked component for different output.
</bodyText>
<tableCaption confidence="0.965038">
Table 1. Dialogue Manager Capabilities
</tableCaption>
<bodyText confidence="0.999765571428571">
The Dialogue Manager is the primary locus of
the dialogue agent&apos;s outward personality as a
function of interaction style; its simple protocol
specifies conditions for interrupting user speech
for permitting interruption by the user, when to
initiate repair dialogues, and how often to back-
channel.
</bodyText>
<subsectionHeader confidence="0.997747">
1.2 Context Tracking
</subsectionHeader>
<bodyText confidence="0.999857">
The Context Tracker maintains a record of the
discourse context which it and other components
can consult in order to (a) resolve dependent
forms that occur in input utterances and (b) gen-
erate appropriate context-dependent forms for
achieving natural output. Interpretation of defi-
nite pronouns, demonstratives (this, those), in-
dexicals (you, now, here, tomorrow), definite
NPs (a car.. .the car), one-anaphora (the earlier
one) and ellipsis (how about Seattle) all rely on
stored context.
The Context Tracker strives to record only those
entities and events that could become eligible for
reference. Context thus includes linguistic com-
municative acts (verbalizations), non-linguistic
communicative acts (gesture), and non-
communicative events that are deemed salient.
Since determining salience requires a judge-
ment, our implementations rely on heuristic
rules to decide which events and objects get
entered into the context representation. For ex-
ample, the disappearance of a simulated vehicle
off the edge of a map display might be deemed
salient relative to a particular user model, the
discourse history, or the task structure.
</bodyText>
<subsectionHeader confidence="0.993281">
1.3 Pragmatic Adaptation
</subsectionHeader>
<bodyText confidence="0.999979482758621">
The Pragmatic Adaptation module serves as the
boundary between language and action by de-
termining what action to take given an inter-
preted input utterance or a back-end response.
This module&apos;s role is to &amp;quot;make sense&amp;quot; of a
communicative act in the current linguistic and
non-linguistic context.
The Pragmatic Adapter receives an interpreta-
tion of an input utterance with context-
dependent forms resolved. It then proceeds to
translate that utterance into a valid back-end
command. It checks for violations of the Do-
main Model, which contains information about
the back-end system such as allowable parame-
ter values for command arguments. It also
checks for commands that are infelicitous given
the current Back-end State (e.g., the referenced
vehicle does not exist at the moment). The
Pragmatic Adapter combines the result of these
simple tests and a set of if-then heuristics to
determine whether to send through the command
or to intercept the utterance and notify the Dia-
logue Manager to initiate a repair dialogue with
the user.
The Pragmatic Adapter receives output re-
sponses from the back-end and adapts or &amp;quot;trans-
lates&amp;quot; them into natural language communica-
tions which get incorporated by the Context
Tracker into the dialogue history.
</bodyText>
<sectionHeader confidence="0.8018975" genericHeader="method">
2 An Architecture for Spoken
Dialogue Systems
</sectionHeader>
<bodyText confidence="0.9999628">
Having introduced our three discourse compo-
nents, we now present our overall architecture.
It is laid out in Figure 1, and its components are
described in Table 2, starting from the user and
going clockwise. The discourse components are
</bodyText>
<page confidence="0.99092">
795
</page>
<bodyText confidence="0.857796333333333">
left in white, while non-discourse components
have been shaded gray.
= Communication Link — Default Order of Firing (changeable by Dialogue Manager)
</bodyText>
<figureCaption confidence="0.998076">
Figure 1. An Architecture for Spoken Dialogue Systems, with Discourse Components in White
</figureCaption>
<table confidence="0.99186165">
Component (Agent) Brief Description PosAible Input Possible Output
Speech Recognition Convert waveform to string Of words . , Waveform Text string
NL Interpretation Convert words to meaning representation Text string Logical form
Context Tracking Track discourse entities of input utterance, Logical form (with Logical form (with
on Input resolve dependent references dependent references) dependent references
replaced by their referents)
Pragmatic Adaptation Convert logical form to back-end command Logical form Back-end command
on Input
Back End Interface Layer on back end application to allow Back end command Back-end response
&apos;cOmmunica&apos;tion with DialOgue Manager , ..
Pragmatic Adaptation Convert back-end response to logical form Back-end response Logical form
on Output representation of communicative act
Context Tracking Track discourse entities of output utterance, Logical form (w/out Logical form (conditioned
on Output insert dependent references (if desired) dependent references) by discourse context)
:NL Generation &apos; ConVert meaning representation to Logical form Text string
Speech Synthesis Convert words to waveform :,‘ Text string Waveform
Dialogue Manager High-level control, intelligently route Various Various
information between all agents and partici-
pants (see section 1.1) based on its own
protocol for interaction.
</table>
<figure confidence="0.991848965517241">
Pragmatic
Adaptation
(on Input)
Speech
Recognition
Natural
Language
Generation
Context
Tracking
(on Output)
Natural
Language
Interpretation
Context
Tracking
(on Input)
Human
Dialogue
Manager
Back-End
Interface
Back-
End
Speech
Synthesis
Pragmatic
Adaptation
(on Output)
</figure>
<page confidence="0.992209">
796
</page>
<tableCaption confidence="0.997691">
Table 2. Description of the Architecture Components, with Discourse Components in White
</tableCaption>
<bodyText confidence="0.999794307692308">
Several items are of note in Figure 1 and Table
2. First, although a default firing order is
shown, this order is perturbed any time dialogue
trouble arises. For example, a Speech Recogni-
tion (SR) error, may be detected after Natural
Language Interpretation fails to parse the output
of SR. Rather than continuing the flow on to-
wards the back-end, the Dialogue Manager can
re-consult SR for other hypotheses. Alterna-
tively, the Dialogue Manager can fire Natural
Language Generation with an output request for
clarification. That request gets incorporated into
the context representation by Context Tracking,
the dialogue state is &amp;quot;pushed&amp;quot; in a repair dia-
logue, and a string is ultimately sent to Speech
Synthesis for delivery to the user&apos;s ear. The next
utterance is then interpreted in the context of the
repair dialogue.
Note also that Context Tracking and Pragmatics
Adaptation are called twice each: on &amp;quot;input&amp;quot;
(from the user), and on &amp;quot;output&amp;quot; (from the back-
end). The logical Context Tracker may be im-
plemented as one or as two related modules,
together tracking both sides of that dialogue so
that either user or system can make anaphoric
mention of entities introduced earlier.
</bodyText>
<sectionHeader confidence="0.9839395" genericHeader="method">
3 A Near-Future Scenario of Spoken
Dialogue Systems
</sectionHeader>
<subsectionHeader confidence="0.997419">
3.1 The Scenario
</subsectionHeader>
<bodyText confidence="0.9937186">
We build on images from the popular science
fiction series Star Trek as a rich source of dia-
logue types in complex interrelations. These
example dialogues have more primitive cousins
under development today.
</bodyText>
<tableCaption confidence="0.7622125">
Briefly, our example dialogue types are listed in
Table 3.
</tableCaption>
<table confidence="0.999115958333333">
TIpc Metaphor- Ekarnple Par ticiparit Par ncipant Par ticipant
Dialogue Food The &amp;quot;Food Replicator&amp;quot; on Star Trek accepts A B C
with an Replicator structured English command language such as Human Food
Appliance &amp;quot;Tea. Earl Grey. Hot&amp;quot; and produces results in the Replicator
physical world.
Dialogue Ship&apos;s The ship&apos;s computer on Star Trek is an advanced Human Ship&apos;s
with an Computer application which can understand natural Ian- Computer
Application guage queries, and replies either via actions or
via a multimodal interface.
Dialogue Android &amp;quot;Data&amp;quot; on Star Trek converses as a human while Human Android
with an &amp;quot;Data&amp;quot; providing information processing of a computer &amp;quot;Data&amp;quot;
Intelligent and is capable of action in the physical world.
Robot
Computer Universal Star Trek&apos;s &amp;quot;Universal Translator&amp;quot; is capable of Human Human Universal
Mediation Translator automatically interpreting between any two Translator
of Human humans
Dialogue
Computer Conver- The ship&apos;s computer has the ability to retrieve, Human Human Ship&apos;s
Analysis sation play back, and analyze previously-recorded Computer
of Human Playback conversations. In this sense, the dialogue
Dialogue becomes empirical data to be analyzed.
Dialogue Holodeck Star Trek&apos;s &amp;quot;Holodeck&amp;quot; creates simulated hu- Character Character
between mans (or characters) as actors, for the entertain-
2 characters ment or training of human viewers.
</table>
<tableCaption confidence="0.996441">
Table 3. A Scenario of Dialogue Types
</tableCaption>
<page confidence="0.986968">
797
</page>
<subsectionHeader confidence="0.997103">
3.2 Application of the Architecture to
the Scenario
</subsectionHeader>
<bodyText confidence="0.999893">
We now describe the role our architecture, and
specifically our discourse components, play in
these near-future examples.
</bodyText>
<subsubsectionHeader confidence="0.861328">
3.2.1 Dialogue with a Back-End Computer
</subsubsectionHeader>
<bodyText confidence="0.999989571428571">
The first three examples illustrate dialogues in
which a human is talking to a computer. One
dimension distinguishing the three examples is
the agent&apos;s intelligent use of context. In a dia-
logue with an &amp;quot;appliance&amp;quot;, simple, structured,
unambiguous command language utterances are
interpreted one at a time in isolation from prior
dialogue history. The Pragmatic Adaptation
facility can follow a simple scheme for mapping
each utterance to one of a very few back-end
commands. The Context Tracker has no cross-
sentence dependent references to contend with,
and finally, since the appliance provides no lin-
guistic feedback, the Dialogue Manager fires
none of the &amp;quot;output&amp;quot; components (from back-
end to human). In a dialogue with more sophis-
ticated application or with a robot, the Dialogue
Manager, Context Tracker, and Pragmatic
Adapter need greater functionality, to handle
both linguistic and non-linguistic events in both
directions.
</bodyText>
<subsubsectionHeader confidence="0.983798">
3.2.2 Computer-Mediated Dialogue
</subsubsectionHeader>
<bodyText confidence="0.998714333333333">
The fourth example, that of the Universal
Translator, is representative of a general dia-
logue type we label Mediator, in which an agent
plays a mediation role between humans. In ad-
dition to interpretation, other roles of the me-
diator might be (Table 4):
</bodyText>
<table confidence="0.99944575">
# Mediator Role
1 A Genie, which is available for meta-dialogues with
the system itself, instead of with the dialogue partner
(much as a human might ask an interpreter to repeat
the partner&apos;s last utterance).
2 A Moderator, which, in multi-party dialogues, en-
forces an agreed-upon interaction protocol, such as
Robert&apos;s Rules of Order or a talk-show format (under
control of the host).
3 A Bouncer, which decides who may join the dialogue
based on current enrollment (first-come-first-served),
clearance level, invitation list, etc., as well as permit-
ting different types of participation, so that some may
only listen while others may fully participate.
4 A Stenographer, which records the dialogue, and
prepares a &amp;quot;visualization&amp;quot; of the dialogue structure.
</table>
<tableCaption confidence="0.999446">
Table 4. Roles of a Mediator Agent
</tableCaption>
<bodyText confidence="0.999957">
Our architecture is applicable to mediated dia-
logues as well. In fact, it was first developed for
bilingual dialogue in a voice-to-voice machine
translation application. In this application, the
Dialogue Manager is available for meta-
dialogues with either user (as in Could you re-
peat her last utterance?), and the Context
Tracker can use a single discourse representation
structure to track the unfolding context in both
languages.
</bodyText>
<subsubsectionHeader confidence="0.979343">
3.2.3 Computer-Analyzed Dialogue
</subsubsectionHeader>
<bodyText confidence="0.99999275">
Our fifth example, a post-hoc analysis of a dia-
logue, does not require real-time processing. It
is, nonetheless, a dialogue which can be ana-
lyzed using the components of our architecture,
exactly as if it were real-time. The only differ-
ence is that no generation will be required, only
analysis; thus, the Dialogue Manager need only
fire the &amp;quot;input&amp;quot; components on each utterance.
</bodyText>
<subsubsectionHeader confidence="0.991526">
3.2.4 Character-Character Dialogue
</subsubsectionHeader>
<bodyText confidence="0.999974636363636">
Our last example concerns a simulated human
dialogue between two computer characters, for
the benefit of human viewers. Such character-
character dialogues have been produced by sev-
eral researchers, including (Kalra et al. 1998).
Here, the architecture applies at two levels.
First, the architecture can be internal to each
agent, to implement that agent&apos;s conversational
ability. Second, the architecture can be used
externally to analyze the agents&apos; dialogue, as
discussed in the previous section.
</bodyText>
<sectionHeader confidence="0.945354" genericHeader="method">
4 Implementations of the Architecture
</sectionHeader>
<bodyText confidence="0.999915818181818">
We have implemented two spoken dialogue
systems using the architecture presented. The
first is a telephone-based interface to a simulated
employee Time Reporting System (TRS), as
might be used at a large corporation. We then
ported the system to a spoken interface to a bat-
tlefield simulation (Modular Semi-Automated
Forces, or ModSAF).
In our implementation of this architecture, each
component is a unique agent which may reside
on its own platform and communicate over a
network. The middleware our agents use to
communicate is the Open Agent Architecture
(OAA) (Moran et al. 1997) from SRI. The
OAA&apos;s flexibility allowed us to easily hook up
modules and experiment with the division of
labor between the three discourse components
we are studying. We treat the Dialogue Manager
as a special OAA agent that insists on being
called frequently so that it can monitor the pro-
gress of communicative events through the sys-
tem.
</bodyText>
<page confidence="0.993141">
798
</page>
<subsectionHeader confidence="0.998379">
4.1 The Time Reporting System (TRS)
</subsectionHeader>
<bodyText confidence="0.9998954">
The architecture components in our TRS system
are listed in Table 5, along with their specific
implementations used. Each implemented mod-
ule included a thin OAA agent layer, allowing it
to communicate via the OAA.
</bodyText>
<table confidence="0.999356555555555">
Component impientematton 0/
Component
S eech Reco &apos;don,&apos; Marko&apos;, BB
S eech S thesis TrueTalk (Entro,ic)
NL Inte iretation/Generation Simulated
Back-End Interface Simulated
Context Trackinl (Lu erFo 1992)
Pralmatic Ada station Current! Simulated
Dialo!ue Mana•er Current Develo I ment
</table>
<tableCaption confidence="0.969471">
Table 5. Components of TRS System, with
Discourse Components in White
</tableCaption>
<bodyText confidence="0.962174909090909">
Components not in our focus (shaded in gray)
are either commercial or simulated software. For
Context Tracking, we use an algorithm based on
(LuperFoy 1992). For Dialogue Management,
we developed a simple agent able to control a
system-initiated dialogue, as well as handle non-
linguistic events from the back-end. The third
discourse component, Pragmatic Adaptation,
awaits future research, and was simulated for
this system.
Figure 2 presents a sample TRS dialogue.
</bodyText>
<table confidence="0.863486272727273">
System: Welcome. What is your employee number?
User: 12345
System: What is your password?
User: 54321
System: How can I help you?
User: What&apos;s the first charge number?
System: 123GK498J
User: What&apos;s the name of that task?
System: Project X
User: Charge 6 hours to it today for me.
System: 6 hours has been charged to Project X.
</table>
<figureCaption confidence="0.998561">
Figure 2. Sample TRS Dialogue
</figureCaption>
<bodyText confidence="0.997760777777778">
When the user logs in, the back-end system
brings up a non-linguistic event—the list of
tasks, with associated charge numbers, which
belong to the user. The Dialogue Manager re-
ceives this and passes it to the Context Tracker.
The Context Tracker is then able to resolve the
first charge number, as well as subsequent de-
pendent references such as that task, it, and to-
day.
</bodyText>
<subsectionHeader confidence="0.995112">
4.2 The ModSAF Interface
</subsectionHeader>
<bodyText confidence="0.9999519375">
We ported the TRS demo to a simulated battle-
field back-end called ModSAF. We used the
same components with the exception of the
speech recognizer and the back-end interface.
The Dialogue Manager was improved over the
TRS demo in several ways. First, we added the
capability of the Dialogue Manager to dynami-
cally inform the speech recognizer of what input
to expect, i.e., which language model to use. The
Dialogue Manager could also add words to the
speech recognizer&apos;s vocabulary on the fly. We
chose Nuance (from Nuance Communications)
as our speech recognition component specifi-
cally because it supports such run-time updates.
Figure 3 presents a sample ModSAF dialogue.
Note that only the user speaks.
</bodyText>
<listItem confidence="0.9995668">
• Create an MI A2 platoon.
• Name it Bravo.
• Give it location 4 9 degrees 3 0 minutes north,
1 1 degrees 4 5 minutes east.
• Bravo, advance to checkpoint Charlie.
</listItem>
<bodyText confidence="0.830068">
(At this point, a new platoon appears on the screen,
created by another player in the simulation)
</bodyText>
<listItem confidence="0.800183666666667">
• Zoom in on that new platoon.
• Bravo, change location and approach X.
(Where Xis the name of the new platoon.)
</listItem>
<figureCaption confidence="0.994444">
Figure 3. Sample ModSAF Dialogue
</figureCaption>
<bodyText confidence="0.99830224">
When the user asks to create an entity, the Dia-
logue Manager detects the beginning of a sub-
dialogue, and informs the speech recognizer to
restrict its expected grammar to that of entity
creation (name and location). Later, the back-
end (ModSAF) sends the Dialogue Manager a
non-linguistic event, in which a different platoon
(created by another player in the simulation)
appears. This event includes a name for the new
platoon; the Dialogue Manager passes this to the
speech recognizer, so that it may later recognize
it. In addition, the event is passed to the Context
Tracker, so that it may later resolve the reference
that new platoon.
To illustrate some advantages of our architec-
ture, we briefly mention what we needed to
change to port from TRS to ModSAF. First, the
Context Tracker needed no change at
all—operating on linguistic principles, it is do-
main-independent. LuperFoy&apos;s framework does
provide for a layer connected to a knowledge
source, for external context—this would need to
be changed when changing domains. The Dia-
logue Manager also required little change to its
core code, adding only the ability to influence
</bodyText>
<page confidence="0.994515">
799
</page>
<bodyText confidence="0.99975325">
the speech recognizer. The Pragmatic Adapta-
tion Module, being dependent on the domain of
the back-end, is where most changes are needed
when switching domains.
</bodyText>
<sectionHeader confidence="0.929344" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.999981021739131">
We have presented a modular, flexible architec-
ture for spoken dialogue systems which sepa-
rates discourse processing into three component
tasks with three corresponding software mod-
ules: Dialogue Management, Context Tracking,
and Pragmatic Adaptation. We discussed the
roles of these components in a complex, near-
future scenario portraying a variety of dialogue
types. We closed by describing implementations
of these dialogues using the architecture pre-
sented, including development and porting of the
first two discourse components.
The architecture itself is derived from a standard
blackboard control structure. This is appropriate
for our current dialogue processing research in
two ways. First, it does not require a prior full
enumeration of all possible subroutine firing
sequences. Rather, the possibilities emerge from
local decisions made by modules that communi-
cate with the blackboard, depositing data and
consuming data from the blackboard. Second,
as we learn categories of dialogue segment
types, we can move away from the fully decen-
tralized control structure, to one in which the
central Dialogue Manager, as a blackboard
module with special status, assumes increasing
decision power for processing flow, in cases of
dialogue segment type with which it is familiar.
The intended contribution of this work is thus in
the generic definition of standard dialogue func-
tions such as dynamic troubleshooting (repair),
context updating, anaphora resolution, and
translation of natural language interpretations
into functional interface languages of back-end
systems.
Future work includes investigation of issues
raised when a human is engaged in more than
one of our scenario dialogues concurrently. For
example, how does one speech enabled dialogue
system among many determine when it is being
addressed by the user, and how can the system
judge whether the current utterance is human-
computer, i.e., to be fully interpreted and acted
upon by the system as opposed to a human-
human utterance that is to be simply recorded,
transcribed, or translated without interpretation.
</bodyText>
<sectionHeader confidence="0.998478" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999913452380952">
Allen J., Schubert L., Ferguson G., Heeman P.,
Hwang C., Kato T., Light M., Martin N., Miller B.,
Poesio M., Traum D. (1995) The TRAINS Project:
A case study in building a conversational planning
agent. Journal of Experimental and Theoretical Al,
7, pp. 7-48.
Carlson R. (1996) The Dialogue Component in the
Waxholm System. Proc. Twente Workshop on Lan-
guage Technology: Dialogue Management in Natu-
ral Language Systems, University of Twente, the
Netherlands.
Duff D., Gates B., LuperFoy S. (1996) A Centralized
Troubleshooting Mechanism for a Spoken Dia-
logue Interface to a Simulation Application. Proc.
International Conference on Spoken Language
Processing.
Goddeau D., Brill E., Glass J., Pao C., Phillips M.,
Polifroni J., Seneff S., Zue V. (1994) GALAXY: A
Human-Language Interface to On-line Travel In-
formation. Proc. International Conference on Spo-
ken Language Processing.
KaIra P., Thalmann N., Becheiraz, P., Thalmann D.
(1998) Communication Between Synthetic Actors.
In &amp;quot;Automated Spoken Dialogue Systems&amp;quot;, S. Lu-
perFoy, ed. MIT Press (forthcoming).
LuperFoy. S (1992) The Representation of Multi-
modal User-Interface Dialogues Using Discourse
Pegs. Proc. Annual Meeting of the Association for
Computational Linguistics.
Moore R., Dowding J., Bratt H., Gawron J., Gorfu
Y., Cheyer, A. (1997) CommandTalk: A Spoken-
Language Interface for Battlefield Simulations.
Proc. Fifth Conference on Applied Natural Lan-
guage Processing.
Moran D., Cheyer A., Julia L., Martin D. Park S.
(1997) Multimodal User Interfaces in the Open
Agent Architecture. Proc. International Conference
on Intelligent User Interfaces.
Wahlster W. (1993) Verbmobil: Translation of Face-
To-Face Dialogues. In &amp;quot;Grundlagen und An-
wendungen der Kiinstlichen Intelligenz&amp;quot;, 0. Her-
zog, T. Christaller, D. Schiitt, eds., Springer.
</reference>
<page confidence="0.997257">
800
</page>
<bodyText confidence="0.9820838">
Résumé
Cet article Maine une architecture de logiciel
pour le traitement de discours dans les systemes
de dialogue oral, oil figurent les trois taches
suivantes: (1) gestion de dialogue, (2) tracement
de contexte, et (3) adaptation pragmatique.
Nous expliquons ces trois taches composantes et
decrivons leurs roles dans un scenario complexe
du proche avenir dans lequel les humains et les
ordinateurs agissent les uns sur les autres, tout
en faisant partie de multiples dialogues
simultands. Cet article rend compte des modules
qui s&apos;occupent des trois Caches composantes du
traitement de discours, et d&apos;une architecture
facilite l&apos;interaction de ces modules entre eux et
avec d&apos;autres modules du systeme. Ce travail a
pour but de developper un logiciel pour le
traitement de discours qui peut etre et integre
avec des modules non-discours dans les
systemes de dialogue oral. Nous exposons
l&apos;utilisation de cette architecture dans plusieurs
prototypes, et nous discutons egalement la
possibilite de l&apos;application de l&apos;architecture et de
ses composants aux systemes de dialogue
indiques dans le scenario proche-avenir.
</bodyText>
<page confidence="0.997093">
801
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.955756">
<title confidence="0.998823">An Architecture for Dialogue Management, Context Tracking, and Pragmatic Adaptation in Spoken Dialogue Systems</title>
<author confidence="0.99913">Susann LuperFoy</author>
<author confidence="0.99913">Dan Loehr</author>
<author confidence="0.99913">David Duff</author>
<author confidence="0.99913">Keith Miller</author>
<author confidence="0.99913">Florence Reeder</author>
<author confidence="0.99913">Lisa Harper</author>
<affiliation confidence="0.997092">The MITRE Corporation</affiliation>
<address confidence="0.999776">1820 Dolley Madison Boulevard, McLean VA 22102 USA</address>
<email confidence="0.989251">luperfoy,loehr,duff,keith,freeder,lisah}@mitre.org</email>
<abstract confidence="0.99868932">This paper details a software architecture for discourse processing in spoken dialogue systems, where the three component tasks of discourse processing are (1) Dialogue Management, (2) Context Tracking, and (3) Pragmatic Adaptation. We define these three component tasks and describe their roles in a complex, near-future scenario in which multiple humans interact with each other and with computers in multiple, simultaneous dialogue exchanges. This paper reports on the software modules that accomplish the three component tasks of discourse processing, and an architecture for the interaction among these modules and with other modules of the spoken dialogue system. A motivation of this work is reusable discourse processing software for integration with non-discourse modules in spoken dialogue systems. We document the use of this architecture and its components in several prototypes, and also discuss its potential application to spoken dialogue systems defined in the near-future scenario.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allen</author>
<author>L Schubert</author>
<author>G Ferguson</author>
<author>P Heeman</author>
<author>C Hwang</author>
<author>T Kato</author>
<author>M Light</author>
<author>N Martin</author>
<author>B Miller</author>
<author>M Poesio</author>
<author>D Traum</author>
</authors>
<title>The TRAINS Project: A case study in building a conversational planning agent.</title>
<date>1995</date>
<journal>Journal of Experimental and Theoretical Al,</journal>
<volume>7</volume>
<pages>7--48</pages>
<contexts>
<context position="1676" citStr="Allen et al. 1995" startWordPosition="243" endWordPosition="246">sing software for integration with non-discourse modules in spoken dialogue systems. We document the use of this architecture and its components in several prototypes, and also discuss its potential application to spoken dialogue systems defined in the near-future scenario. Introduction We present an architecture for spoken dialogue systems for both human-computer interaction and computer mediation or analysis of human dialogue. The architecture shares many components with those of existing spoken dialogue systems, such as CommandTalk (Moore et al. 1997), Galaxy (Goddeau et al. 1994), TRAINS (Allen et al. 1995), Verbmobil (Wahlster 1993), Waxholm (Carlson 1996), and others. Our architecture is distinguished from these in its treatment of discourse-level processing. Most architectures, including ours, contain modules for speech recognition and natural language interpretation (such as morphology, syntax, and sentential semantics). Many include a module for interfacing with the back-end application. If the dialogue is two-way, the architectures also include modules for natural language generation and speech synthesis. Architectures differ in how they handle discourse. Some have a single, separate modul</context>
</contexts>
<marker>Allen, Schubert, Ferguson, Heeman, Hwang, Kato, Light, Martin, Miller, Poesio, Traum, 1995</marker>
<rawString>Allen J., Schubert L., Ferguson G., Heeman P., Hwang C., Kato T., Light M., Martin N., Miller B., Poesio M., Traum D. (1995) The TRAINS Project: A case study in building a conversational planning agent. Journal of Experimental and Theoretical Al, 7, pp. 7-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Carlson</author>
</authors>
<title>The Dialogue Component in the Waxholm System.</title>
<date>1996</date>
<booktitle>Proc. Twente Workshop on Language Technology: Dialogue Management in Natural Language Systems,</booktitle>
<institution>University of Twente, the Netherlands.</institution>
<contexts>
<context position="1727" citStr="Carlson 1996" startWordPosition="251" endWordPosition="252"> in spoken dialogue systems. We document the use of this architecture and its components in several prototypes, and also discuss its potential application to spoken dialogue systems defined in the near-future scenario. Introduction We present an architecture for spoken dialogue systems for both human-computer interaction and computer mediation or analysis of human dialogue. The architecture shares many components with those of existing spoken dialogue systems, such as CommandTalk (Moore et al. 1997), Galaxy (Goddeau et al. 1994), TRAINS (Allen et al. 1995), Verbmobil (Wahlster 1993), Waxholm (Carlson 1996), and others. Our architecture is distinguished from these in its treatment of discourse-level processing. Most architectures, including ours, contain modules for speech recognition and natural language interpretation (such as morphology, syntax, and sentential semantics). Many include a module for interfacing with the back-end application. If the dialogue is two-way, the architectures also include modules for natural language generation and speech synthesis. Architectures differ in how they handle discourse. Some have a single, separate module labeled &amp;quot;discourse processor&amp;quot;, &amp;quot;dialogue componen</context>
</contexts>
<marker>Carlson, 1996</marker>
<rawString>Carlson R. (1996) The Dialogue Component in the Waxholm System. Proc. Twente Workshop on Language Technology: Dialogue Management in Natural Language Systems, University of Twente, the Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Duff</author>
<author>B Gates</author>
<author>S LuperFoy</author>
</authors>
<title>A Centralized Troubleshooting Mechanism for a Spoken Dialogue Interface to a Simulation Application.</title>
<date>1996</date>
<booktitle>Proc. International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="5026" citStr="Duff et al. 1996" startWordPosition="726" endWordPosition="729">ext Tracker (below). 3 Increases overall system performance. For example, awareness of system output allows the Dialogue Manager to predict user input, boosting speech recognition accuracy. Similarly, if the back-end introduces a new word into the discourse, the Dialogue Manager can request the speech recognizer to add it to its vocabulary for later recognition. 4 Supports meta-dialogues between the dialogue system itself and either participant. An example might be a participant&apos;s questions about the status of the dialogue system. 5 Acts as a central point for dialogue troubleshooting, after (Duff et al. 1996). If any component has insufficient input to perform its task, it can alert the Dialogue Manager, which can then reconsult a previously invoked component for different output. Table 1. Dialogue Manager Capabilities The Dialogue Manager is the primary locus of the dialogue agent&apos;s outward personality as a function of interaction style; its simple protocol specifies conditions for interrupting user speech for permitting interruption by the user, when to initiate repair dialogues, and how often to backchannel. 1.2 Context Tracking The Context Tracker maintains a record of the discourse context wh</context>
</contexts>
<marker>Duff, Gates, LuperFoy, 1996</marker>
<rawString>Duff D., Gates B., LuperFoy S. (1996) A Centralized Troubleshooting Mechanism for a Spoken Dialogue Interface to a Simulation Application. Proc. International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Goddeau</author>
<author>E Brill</author>
<author>J Glass</author>
<author>C Pao</author>
<author>M Phillips</author>
<author>J Polifroni</author>
<author>S Seneff</author>
<author>V Zue</author>
</authors>
<title>GALAXY: A Human-Language Interface to On-line Travel Information.</title>
<date>1994</date>
<booktitle>Proc. International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="1648" citStr="Goddeau et al. 1994" startWordPosition="238" endWordPosition="241">k is reusable discourse processing software for integration with non-discourse modules in spoken dialogue systems. We document the use of this architecture and its components in several prototypes, and also discuss its potential application to spoken dialogue systems defined in the near-future scenario. Introduction We present an architecture for spoken dialogue systems for both human-computer interaction and computer mediation or analysis of human dialogue. The architecture shares many components with those of existing spoken dialogue systems, such as CommandTalk (Moore et al. 1997), Galaxy (Goddeau et al. 1994), TRAINS (Allen et al. 1995), Verbmobil (Wahlster 1993), Waxholm (Carlson 1996), and others. Our architecture is distinguished from these in its treatment of discourse-level processing. Most architectures, including ours, contain modules for speech recognition and natural language interpretation (such as morphology, syntax, and sentential semantics). Many include a module for interfacing with the back-end application. If the dialogue is two-way, the architectures also include modules for natural language generation and speech synthesis. Architectures differ in how they handle discourse. Some h</context>
</contexts>
<marker>Goddeau, Brill, Glass, Pao, Phillips, Polifroni, Seneff, Zue, 1994</marker>
<rawString>Goddeau D., Brill E., Glass J., Pao C., Phillips M., Polifroni J., Seneff S., Zue V. (1994) GALAXY: A Human-Language Interface to On-line Travel Information. Proc. International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P KaIra</author>
<author>N Thalmann</author>
<author>P Becheiraz</author>
<author>D Thalmann</author>
</authors>
<title>Communication Between Synthetic Actors. In &amp;quot;Automated Spoken Dialogue Systems&amp;quot;,</title>
<date>1998</date>
<editor>S. LuperFoy, ed.</editor>
<publisher>MIT Press</publisher>
<marker>KaIra, Thalmann, Becheiraz, Thalmann, 1998</marker>
<rawString>KaIra P., Thalmann N., Becheiraz, P., Thalmann D. (1998) Communication Between Synthetic Actors. In &amp;quot;Automated Spoken Dialogue Systems&amp;quot;, S. LuperFoy, ed. MIT Press (forthcoming).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S</author>
</authors>
<title>The Representation of Multimodal User-Interface Dialogues Using Discourse Pegs.</title>
<date>1992</date>
<booktitle>Proc. Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>S, 1992</marker>
<rawString>LuperFoy. S (1992) The Representation of Multimodal User-Interface Dialogues Using Discourse Pegs. Proc. Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Moore</author>
<author>J Dowding</author>
<author>H Bratt</author>
<author>J Gawron</author>
<author>Y Gorfu</author>
<author>A Cheyer</author>
</authors>
<title>CommandTalk: A SpokenLanguage Interface for Battlefield Simulations.</title>
<date>1997</date>
<booktitle>Proc. Fifth Conference on Applied Natural Language Processing.</booktitle>
<contexts>
<context position="1618" citStr="Moore et al. 1997" startWordPosition="233" endWordPosition="236">em. A motivation of this work is reusable discourse processing software for integration with non-discourse modules in spoken dialogue systems. We document the use of this architecture and its components in several prototypes, and also discuss its potential application to spoken dialogue systems defined in the near-future scenario. Introduction We present an architecture for spoken dialogue systems for both human-computer interaction and computer mediation or analysis of human dialogue. The architecture shares many components with those of existing spoken dialogue systems, such as CommandTalk (Moore et al. 1997), Galaxy (Goddeau et al. 1994), TRAINS (Allen et al. 1995), Verbmobil (Wahlster 1993), Waxholm (Carlson 1996), and others. Our architecture is distinguished from these in its treatment of discourse-level processing. Most architectures, including ours, contain modules for speech recognition and natural language interpretation (such as morphology, syntax, and sentential semantics). Many include a module for interfacing with the back-end application. If the dialogue is two-way, the architectures also include modules for natural language generation and speech synthesis. Architectures differ in how</context>
</contexts>
<marker>Moore, Dowding, Bratt, Gawron, Gorfu, Cheyer, 1997</marker>
<rawString>Moore R., Dowding J., Bratt H., Gawron J., Gorfu Y., Cheyer, A. (1997) CommandTalk: A SpokenLanguage Interface for Battlefield Simulations. Proc. Fifth Conference on Applied Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moran</author>
<author>A Cheyer</author>
<author>L Julia</author>
<author>Martin D Park S</author>
</authors>
<date>1997</date>
<booktitle>Multimodal User Interfaces in the Open Agent Architecture. Proc. International Conference on Intelligent User Interfaces.</booktitle>
<contexts>
<context position="17359" citStr="Moran et al. 1997" startWordPosition="2603" endWordPosition="2606">plementations of the Architecture We have implemented two spoken dialogue systems using the architecture presented. The first is a telephone-based interface to a simulated employee Time Reporting System (TRS), as might be used at a large corporation. We then ported the system to a spoken interface to a battlefield simulation (Modular Semi-Automated Forces, or ModSAF). In our implementation of this architecture, each component is a unique agent which may reside on its own platform and communicate over a network. The middleware our agents use to communicate is the Open Agent Architecture (OAA) (Moran et al. 1997) from SRI. The OAA&apos;s flexibility allowed us to easily hook up modules and experiment with the division of labor between the three discourse components we are studying. We treat the Dialogue Manager as a special OAA agent that insists on being called frequently so that it can monitor the progress of communicative events through the system. 798 4.1 The Time Reporting System (TRS) The architecture components in our TRS system are listed in Table 5, along with their specific implementations used. Each implemented module included a thin OAA agent layer, allowing it to communicate via the OAA. Compo</context>
</contexts>
<marker>Moran, Cheyer, Julia, S, 1997</marker>
<rawString>Moran D., Cheyer A., Julia L., Martin D. Park S. (1997) Multimodal User Interfaces in the Open Agent Architecture. Proc. International Conference on Intelligent User Interfaces.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wahlster</author>
</authors>
<title>Verbmobil: Translation of FaceTo-Face Dialogues.</title>
<date>1993</date>
<booktitle>In &amp;quot;Grundlagen und Anwendungen der Kiinstlichen Intelligenz&amp;quot;,</booktitle>
<volume>0</volume>
<editor>Herzog, T. Christaller, D. Schiitt, eds.,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="1703" citStr="Wahlster 1993" startWordPosition="248" endWordPosition="249">ith non-discourse modules in spoken dialogue systems. We document the use of this architecture and its components in several prototypes, and also discuss its potential application to spoken dialogue systems defined in the near-future scenario. Introduction We present an architecture for spoken dialogue systems for both human-computer interaction and computer mediation or analysis of human dialogue. The architecture shares many components with those of existing spoken dialogue systems, such as CommandTalk (Moore et al. 1997), Galaxy (Goddeau et al. 1994), TRAINS (Allen et al. 1995), Verbmobil (Wahlster 1993), Waxholm (Carlson 1996), and others. Our architecture is distinguished from these in its treatment of discourse-level processing. Most architectures, including ours, contain modules for speech recognition and natural language interpretation (such as morphology, syntax, and sentential semantics). Many include a module for interfacing with the back-end application. If the dialogue is two-way, the architectures also include modules for natural language generation and speech synthesis. Architectures differ in how they handle discourse. Some have a single, separate module labeled &amp;quot;discourse proces</context>
</contexts>
<marker>Wahlster, 1993</marker>
<rawString>Wahlster W. (1993) Verbmobil: Translation of FaceTo-Face Dialogues. In &amp;quot;Grundlagen und Anwendungen der Kiinstlichen Intelligenz&amp;quot;, 0. Herzog, T. Christaller, D. Schiitt, eds., Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>