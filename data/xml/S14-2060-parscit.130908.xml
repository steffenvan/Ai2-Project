<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.031416">
<title confidence="0.960898">
IUCL: Combining Information Sources for SemEval Task 5
</title>
<author confidence="0.999782">
Alex Rudnick, Levi King, Can Liu, Markus Dickinson, Sandra K¨ubler
</author>
<affiliation confidence="0.999559">
Indiana University
</affiliation>
<address confidence="0.661095">
Bloomington, IN, USA
</address>
<email confidence="0.997858">
{alexr,leviking,liucan,md7,skuebler}@indiana.edu
</email>
<sectionHeader confidence="0.997377" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999655">
We describe the Indiana University sys-
tem for SemEval Task 5, the L2 writ-
ing assistant task, as well as some exten-
sions to the system that were completed
after the main evaluation. Our team sub-
mitted translations for all four language
pairs in the evaluation, yielding the top
scores for English-German. The system
is based on combining several information
sources to arrive at a final L2 translation
for a given L1 text fragment, incorporating
phrase tables extracted from bitexts, an L2
language model, a multilingual dictionary,
and dependency-based collocational mod-
els derived from large samples of target-
language text.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999936">
In the L2 writing assistant task, we must translate
an L1 fragment in the midst of an existing, nearly
complete, L2 sentence. With the presence of this
rich target-language context, the task is rather dif-
ferent from a standard machine translation setting,
and our goal with our design was to make effec-
tive use of the L2 context, exploiting collocational
relationships between tokens anywhere in the L2
context and the proposed fragment translations.
Our system proceeds in several stages: (1) look-
ing up or constructing candidate translations for
the L1 fragment, (2) scoring candidate transla-
tions via a language model of the L2, (3) scoring
candidate translations with a dependency-driven
word similarity measure (Lin, 1998) (which we
call SIM), and (4) combining the previous scores
in a log-linear model to arrive at a final n-best
list. Step 1 models transfer knowledge between
</bodyText>
<footnote confidence="0.90772175">
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
</footnote>
<bodyText confidence="0.9997386875">
the L1 and L2; step 2 models facts about the L2
syntax, i.e., which translations fit well into the lo-
cal context; step 3 models collocational and se-
mantic tendencies of the L2; and step 4 gives dif-
ferent weights to each of the three sources of in-
formation. Although we did not finish step 3 in
time for the official results, we discuss it here, as
it represents the most novel aspect of the system –
namely, steps towards the exploitation of the rich
L2 context. In general, our approach is language-
independent, with accuracy varying due to the size
of data sources and quality of input technology
(e.g., syntactic parse accuracy). More features
could easily be added to the log-linear model, and
further explorations of ways to make use of target-
language knowledge could be promising.
</bodyText>
<sectionHeader confidence="0.982236" genericHeader="method">
2 Data Sources
</sectionHeader>
<bodyText confidence="0.9998598">
The data sources serve two major purposes for our
system: For L2 candidate generation, we use Eu-
roparl and BabelNet; and for candidate ranking
based on L2 context, we use Wikipedia and the
Google Books Syntactic N-grams.
Europarl The Europarl Parallel Corpus (Eu-
roparl, v7) (Koehn, 2005) is a corpus of pro-
ceedings of the European Parliament, contain-
ing 21 European languages with sentence align-
ments. From this corpus, we build phrase tables
for English-Spanish, English-German, French-
English, Dutch-English.
BabelNet In the cases where the constructed
phrase tables do not contain a translation for a
source phrase, we need to back off to smaller
phrases and find candidate translations for these
components. To better handle sparsity, we extend
look-up using the multilingual dictionary Babel-
Net, v2.0 (Navigli and Ponzetto, 2012) as a way to
find translation candidates.
</bodyText>
<page confidence="0.98806">
356
</page>
<note confidence="0.730514">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 356–360,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.997615028571428">
Wikipedia For German and Spanish, we use re-
cent Wikipedia dumps, which were converted to
plain text with the Wikipedia Extractor tool.1 To
save time during parsing, sentences longer than 25
words are removed. The remaining sentences are
POS-tagged and dependency parsed using Mate
Parser with its pre-trained models (Bohnet, 2010;
Bohnet and Kuhn, 2012; Seeker and Kuhn, 2013).
To keep our English Wikipedia dataset to a man-
ageable size, we choose an older (2006), smaller
dump. Long sentences are removed, and the re-
maining sentences are POS-tagged and depen-
dency parsed using the pre-trained Stanford Parser
(Klein and Manning, 2003; de Marneffe et al.,
2006). The resulting sizes of the datasets are
(roughly): German: 389M words, 28M sentences;
Spanish: 147M words, 12M sentences; English:
253M words, 15M sentences. Dependencies ex-
tracted from these parsed datasets serve as training
for the SIM system described in section 3.3.
Google Books Syntactic N-grams For English,
we also obtained dependency relationships for our
word similarity statistics using the arcs dataset of
the Google Books Syntactic N-Grams (Goldberg
and Orwant, 2013), which has 919M items, each
of which is a small “syntactic n-gram”, a term
Goldberg and Orwant use to describe short de-
pendency chains, each of which may contain sev-
eral tokens. This data set does not contain the ac-
tual parses of books from the Google Books cor-
pus, but counts of these dependency chains. We
converted the longer chains into their component
(head, dependent, label) triples and then collated
these triples into counts, also for use in the SIM
system.
</bodyText>
<sectionHeader confidence="0.989967" genericHeader="method">
3 System Design
</sectionHeader>
<bodyText confidence="0.9999706">
As previously mentioned, at run-time, our system
decomposes the fragment translation task into two
parts: generating many possible candidate transla-
tions, then scoring and ranking them in the target-
language context.
</bodyText>
<subsectionHeader confidence="0.999273">
3.1 Constructing Candidate Translations
</subsectionHeader>
<bodyText confidence="0.9998354">
As a starting point, we use phrase tables con-
structed in typical SMT fashion, built with the
training scripts packaged with Moses (Koehn et
al., 2007). These scripts preprocess the bitext, es-
timate word alignments with GIZA++ (Och and
</bodyText>
<footnote confidence="0.862918">
1http://medialab.di.unipi.it/wiki/
Wikipedia_Extractor
</footnote>
<bodyText confidence="0.999970739130435">
Ney, 2000) and then extract phrases with the
grow-diag-final-and heuristic.
At translation time, we look for the given
source-language phrase in the phrase table, and if
it is found, we take all translations of that phrase
as our candidates.
When translating a phrase that is not found in
the phrase table, we try to construct a “synthetic
phrase” out of the available components. This
is done by listing, combinatorially, all ways to
decompose the L1 phrase into sub-phrases of at
least one token long. Then for each decomposi-
tion of the input phrase, such that all of its compo-
nents can be found in the phrase table, we gen-
erate a translation by concatenating their target-
language sides. This approach naively assumes
that generating valid L2 text requires no reorder-
ing of the components. Also, since there are 2n−&apos;
possible ways to split an n-token phrase into sub-
sequences (i.e., each token is either the first token
in a new sub-sequence, or it is not), we perform
some heuristic pruning at this step, taking only
the first 100 decompositions, preferring those built
from longer phrase-table entries. Every phrase in
the phrase table, including these synthetic phrases,
has both a “direct” and “inverse” probability score;
for synthetic phrases, we estimate these scores by
taking the product of the corresponding probabili-
ties for the individual components.
In the case that an individual word cannot be
found in the phrase table, the system attempts to
look up the word in BabelNet, estimating the prob-
abilities as uniformly distributed over the available
BabelNet entries. Thus, synthetic phrase table
entries can be constructed by combining phrases
found in the training data and words available in
BabelNet.
For the evaluation, in cases where an L1 phrase
contained words that were neither in our train-
ing data nor BabelNet (and thus were simply out-
of-vocabulary for our system), we took the first
translation for that phrase, without regard to con-
text, from Google Translate, through the semi-
automated Google Docs interface. This approach
is not particularly scalable or reproducible, but
simulates what a user might do in such a situation.
</bodyText>
<subsectionHeader confidence="0.99777">
3.2 Scoring Candidate Translations via a L2
Language Model
</subsectionHeader>
<bodyText confidence="0.999622">
To model how well a phrase fits into the L2 con-
text, we score candidates with an n-gram lan-
</bodyText>
<page confidence="0.991475">
357
</page>
<bodyText confidence="0.999682222222222">
guage model (LM) trained on a large sample of
target-language text. Constructing and querying
a large language model is potentially computa-
tionally expensive, so here we use the KenLM
Language Model Toolkit and its Python interface
(Heafield, 2011). Here our models were trained
on the Wikipedia text mentioned previously (with-
out filtering long sentences), with KenLM set to
5-grams and the default settings.
</bodyText>
<subsectionHeader confidence="0.998397">
3.3 Scoring Candidate Translations via
Dependency-Based Word Similarity
</subsectionHeader>
<bodyText confidence="0.961448083333333">
The candidate ranking based on the n-gram lan-
guage model – while quite useful – is based on
very shallow information. We can also rank the
candidate phrases based on how well each of the
components fits into the L2 context using syntactic
information. In this case, the fitness is measured in
terms of dependency-based word similarity com-
puted from dependency triples consisting of the
the head, the dependent, and the dependency la-
bel. We slightly adapted the word similarity mea-
sure by Lin (1998):
SIM w w 2 * c(h, d, l) 1
</bodyText>
<equation confidence="0.963842">
( i&apos; 2) = c(h, −, l) + c(−, d, l) ( )
</equation>
<bodyText confidence="0.999924434782609">
where h = w1 and d = w2 and c(h, d, l)
is the frequency with which a particular
(head, dependent, label) dependency triple
occurs in the L2 corpus. c(h, −, l) is the fre-
quency with which a word occurs as a head
in a dependency labeled l with any dependent.
c(−, d, l) is the frequency with which a word
occurs as a dependent in a dependency labeled
l with any head. In the measure by Lin (1998),
the numerator is defined as the information of all
dependency features that w1 and w2 share, com-
puted as the negative sum of the log probability of
each dependency feature. Similarly, the denom-
inator is computed as the sum of information of
dependency features for w1 and w2.
To compute the fitness of a word wi for its
context, we consider a set D of all words that are
directly dependency-related to wi. The fitness of
wi is thus computed as:
The fitness of a phrase is the average word sim-
ilarity over all its components. For example, the
fitness of the phrase “eat with chopsticks” would
be computed as:
</bodyText>
<equation confidence="0.84444825">
FIT (eat with chopsticks) =
FIT (eat) + FIT (with) + FIT (chopsticks)
3
(3)
</equation>
<bodyText confidence="0.9999345">
Since we consider the heads and dependents
of a target phrase component, these may be situ-
ated inside or outside the phrase. Both cases are
included in our calculation, thus enabling us to
consider a broader, syntactically determined local
context of the phrase. By basing the calculation on
a single word’s head and dependents, we attempt
to avoid data sparseness issues that we might get
from rare n-gram contexts.
Back-Off Lexical-based dependency triples suf-
fer from data sparsity, so in addition to computing
the lexical fitness of a phrase, we also calculate the
POS fitness. For example, the POS fitness of “eat
with chopsticks” would be computed as follows:
</bodyText>
<equation confidence="0.97819775">
FIT (eat/VBG with/IN chopsticks/NNS) =
FIT (VBG) + FIT (IN) + FIT (NNS)
3
(4)
</equation>
<bodyText confidence="0.99979765">
Storing and Caching The large vocabulary
and huge number of combinations of our
(head, dependent, label) triples poses an effi-
ciency problem when querying the dependency-
based word similarity values. Thus, we stored
the dependency triples in a database with a
Python programming interface (SQLite3) and
built database indices on the frequent query types.
However, for frequently searched dependency
triples, re-querying the database is still inefficient.
Thus, we built a query cache to store the recently-
queried triples. Using the database and cache sig-
nificantly speeds up our system.
This database only stores dependency triples
and their corresponding counts; the dependency-
based similarity value is calculated as needed, for
each particular context. Then, these FIT scores
are combined with the scores from the phrase ta-
ble and language model, using weights tuned by
MERT.
</bodyText>
<equation confidence="0.8354155">
FIT(wi) =
�Dwj SIM(wi, wj)
(2)
|D|
</equation>
<page confidence="0.993808">
358
</page>
<table confidence="0.99946525">
system acc wordacc oofacc oofwordacc
run2 0.665 0.722 0.806 0.857
SIM 0.647 0.706 0.800 0.852
nb 0.657 0.717 0.834 0.868
system acc wordacc oofacc oofwordacc
run2 0.545 0.682 0.691 0.800
SIM 0.549 0.687 0.693 0.800
best 0.733 0.824 0.905 0.938
</table>
<figureCaption confidence="0.9907095">
Figure 1: Scores on the test set for English-
German; here next-best is CNRC-run1.
Figure 3: Scores on the test set for French-English;
here best is UEdin-run1.
</figureCaption>
<table confidence="0.999105375">
system acc wordacc oofacc oofwordacc
run2 0.633 0.72 0.781 0.847
SIM 0.359 0.482 0.462 0.607
best 0.755 0.827 0.920 0.944
system acc wordacc oofacc oofwordacc
run2 0.544 0.679 0.634 0.753
SIM 0.540 0.676 0.635 0.753
best 0.575 0.692 0.733 0.811
</table>
<figureCaption confidence="0.9746215">
Figure 2: Scores on the test set for English-
Spanish; here best is UEdin-run2.
</figureCaption>
<subsectionHeader confidence="0.991039">
3.4 Tuning Weights with MERT
</subsectionHeader>
<bodyText confidence="0.9999808">
In order to rank the various candidate translations,
we must combine the different sources of infor-
mation in some way. Here we use a familiar log-
linear model, taking the log of each score – the di-
rect and inverse translation probabilities, the LM
probability, and the surface and POS SIM scores –
and producing a weighted sum. Since the original
scores are either probabilities or probability-like
(in the range [0,1]), their logs are negative num-
bers, and at translation time we return the trans-
lation (or n-best) with the highest (least negative)
score.
This leaves us with the question of how to
set the weights for the log-linear model; in this
work, we use the ZMERT package (Zaidan, 2009),
which implements the MERT optimization algo-
rithm (Och, 2003), iteratively tuning the feature
weights by repeatedly requesting n-best lists from
the system. We used ZMERT with its default
settings, optimizing our system’s BLEU scores
on the provided development set. We chose, for
convenience, BLEU as a stand-in for the word-
level accuracy score, as BLEU scores are maxi-
mized when the system output matches the refer-
ence translations.
</bodyText>
<sectionHeader confidence="0.999917" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.992935714285714">
In figures 1-4, we show the scores on this year’s
test set for running the two variations of our sys-
tem: runt, the version without the SIM exten-
sions, which we submitted for the evaluation, and
SIM, with the extensions enabled. For compar-
ison, we also include the best (or for English-
German, next-best) submitted system. We see here
</bodyText>
<figureCaption confidence="0.6635125">
Figure 4: Scores on the test set for Dutch-English;
here best is UEdin-run1.
</figureCaption>
<bodyText confidence="0.997855714285714">
that the use of the SIM features did not improve
the performance of the base system, and in the
case of English-Spanish caused significant degra-
dation, which is as of yet unexplained, though we
suspect difficulties parsing the Spanish test set, as
for all of the other language pairs, the effects of
adding SIM features were small.
</bodyText>
<sectionHeader confidence="0.999348" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999978047619048">
We have described our entry for the initial run-
ning of the “L2 Writing Assistant” task and ex-
plained some possible extensions to our base log-
linear model system.
In developing the SIM extensions, we faced
some interesting software engineering challenges,
and we can now produce large databases of depen-
dency relationship counts for various languages.
Unfortunately, these extensions have not yet led
to improvements in performance on this particu-
lar task. The databases themselves seem at least
intuitively promising, capturing interesting infor-
mation about common usage patterns of the tar-
get language. Finding a good way to make use
of this information may involve computing some
measure that we have not yet considered, or per-
haps the insights captured by SIM are covered ef-
fectively by the language model.
We look forward to future developments around
this task and associated applications in helping
language learners communicate effectively.
</bodyText>
<page confidence="0.998439">
359
</page>
<sectionHeader confidence="0.996175" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999866909090909">
Bernd Bohnet and Jonas Kuhn. 2012. The best of
both worlds – A graph-based completion model for
transition-based parsers. In Proceedings of the 13th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics (EACL), pages
77–87, Avignon, France.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (COLING), pages 89–97, Beijing,
China.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In
Proceedings of LREC-06, Genoa, Italy.
Yoav Goldberg and Jon Orwant. 2013. A dataset of
syntactic-ngrams over time from a very large cor-
pus of English books. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
pages 241–247, Atlanta, GA.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation, pages 187–197, Edinburgh, Scot-
land, United Kingdom, July.
Dan Klein and Christopher Manning. 2003. Accurate
unlexicalized parsing. In Proceedings ofACL-2003,
pages 423–430, Sapporo, Japan.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, Prague, Czech Republic.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In International Conference on
Machine Learning (ICML), volume 98, pages 296–
304.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217–
250.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 440–447, Hong
Kong.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160–167, Sap-
poro, Japan, July.
Wolfgang Seeker and Jonas Kuhn. 2013. Morphologi-
cal and syntactic case in statistical dependency pars-
ing. Computational Linguistics, 39(1):23–55.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79–88.
</reference>
<page confidence="0.998251">
360
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.814937">
<title confidence="0.882355">IUCL: Combining Information Sources for SemEval Task 5</title>
<author confidence="0.9922">Alex Rudnick</author>
<author confidence="0.9922">Levi King</author>
<author confidence="0.9922">Can Liu</author>
<author confidence="0.9922">Markus Dickinson</author>
<author confidence="0.9922">Sandra</author>
<affiliation confidence="0.98921">Indiana</affiliation>
<address confidence="0.98302">Bloomington, IN,</address>
<abstract confidence="0.996910352941176">We describe the Indiana University system for SemEval Task 5, the L2 writing assistant task, as well as some extensions to the system that were completed after the main evaluation. Our team submitted translations for all four language pairs in the evaluation, yielding the top scores for English-German. The system is based on combining several information sources to arrive at a final L2 translation for a given L1 text fragment, incorporating phrase tables extracted from bitexts, an L2 language model, a multilingual dictionary, and dependency-based collocational models derived from large samples of targetlanguage text.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Jonas Kuhn</author>
</authors>
<title>The best of both worlds – A graph-based completion model for transition-based parsers.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>77--87</pages>
<location>Avignon, France.</location>
<contexts>
<context position="4126" citStr="Bohnet and Kuhn, 2012" startWordPosition="643" endWordPosition="646"> look-up using the multilingual dictionary BabelNet, v2.0 (Navigli and Ponzetto, 2012) as a way to find translation candidates. 356 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 356–360, Dublin, Ireland, August 23-24, 2014. Wikipedia For German and Spanish, we use recent Wikipedia dumps, which were converted to plain text with the Wikipedia Extractor tool.1 To save time during parsing, sentences longer than 25 words are removed. The remaining sentences are POS-tagged and dependency parsed using Mate Parser with its pre-trained models (Bohnet, 2010; Bohnet and Kuhn, 2012; Seeker and Kuhn, 2013). To keep our English Wikipedia dataset to a manageable size, we choose an older (2006), smaller dump. Long sentences are removed, and the remaining sentences are POS-tagged and dependency parsed using the pre-trained Stanford Parser (Klein and Manning, 2003; de Marneffe et al., 2006). The resulting sizes of the datasets are (roughly): German: 389M words, 28M sentences; Spanish: 147M words, 12M sentences; English: 253M words, 15M sentences. Dependencies extracted from these parsed datasets serve as training for the SIM system described in section 3.3. Google Books Synta</context>
</contexts>
<marker>Bohnet, Kuhn, 2012</marker>
<rawString>Bernd Bohnet and Jonas Kuhn. 2012. The best of both worlds – A graph-based completion model for transition-based parsers. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 77–87, Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
</authors>
<title>Top accuracy and fast dependency parsing is not a contradiction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING),</booktitle>
<pages>89--97</pages>
<location>Beijing, China.</location>
<contexts>
<context position="4103" citStr="Bohnet, 2010" startWordPosition="641" endWordPosition="642">ity, we extend look-up using the multilingual dictionary BabelNet, v2.0 (Navigli and Ponzetto, 2012) as a way to find translation candidates. 356 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 356–360, Dublin, Ireland, August 23-24, 2014. Wikipedia For German and Spanish, we use recent Wikipedia dumps, which were converted to plain text with the Wikipedia Extractor tool.1 To save time during parsing, sentences longer than 25 words are removed. The remaining sentences are POS-tagged and dependency parsed using Mate Parser with its pre-trained models (Bohnet, 2010; Bohnet and Kuhn, 2012; Seeker and Kuhn, 2013). To keep our English Wikipedia dataset to a manageable size, we choose an older (2006), smaller dump. Long sentences are removed, and the remaining sentences are POS-tagged and dependency parsed using the pre-trained Stanford Parser (Klein and Manning, 2003; de Marneffe et al., 2006). The resulting sizes of the datasets are (roughly): German: 389M words, 28M sentences; Spanish: 147M words, 12M sentences; English: 253M words, 15M sentences. Dependencies extracted from these parsed datasets serve as training for the SIM system described in section </context>
</contexts>
<marker>Bohnet, 2010</marker>
<rawString>Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING), pages 89–97, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC-06,</booktitle>
<location>Genoa, Italy.</location>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC-06, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Jon Orwant</author>
</authors>
<title>A dataset of syntactic-ngrams over time from a very large corpus of English books.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM),</booktitle>
<pages>241--247</pages>
<location>Atlanta, GA.</location>
<contexts>
<context position="4917" citStr="Goldberg and Orwant, 2013" startWordPosition="765" endWordPosition="768">maining sentences are POS-tagged and dependency parsed using the pre-trained Stanford Parser (Klein and Manning, 2003; de Marneffe et al., 2006). The resulting sizes of the datasets are (roughly): German: 389M words, 28M sentences; Spanish: 147M words, 12M sentences; English: 253M words, 15M sentences. Dependencies extracted from these parsed datasets serve as training for the SIM system described in section 3.3. Google Books Syntactic N-grams For English, we also obtained dependency relationships for our word similarity statistics using the arcs dataset of the Google Books Syntactic N-Grams (Goldberg and Orwant, 2013), which has 919M items, each of which is a small “syntactic n-gram”, a term Goldberg and Orwant use to describe short dependency chains, each of which may contain several tokens. This data set does not contain the actual parses of books from the Google Books corpus, but counts of these dependency chains. We converted the longer chains into their component (head, dependent, label) triples and then collated these triples into counts, also for use in the SIM system. 3 System Design As previously mentioned, at run-time, our system decomposes the fragment translation task into two parts: generating</context>
</contexts>
<marker>Goldberg, Orwant, 2013</marker>
<rawString>Yoav Goldberg and Jon Orwant. 2013. A dataset of syntactic-ngrams over time from a very large corpus of English books. In Second Joint Conference on Lexical and Computational Semantics (*SEM), pages 241–247, Atlanta, GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: faster and smaller language model queries.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>187--197</pages>
<location>Edinburgh, Scotland, United Kingdom,</location>
<contexts>
<context position="8502" citStr="Heafield, 2011" startWordPosition="1348" endWordPosition="1349">thout regard to context, from Google Translate, through the semiautomated Google Docs interface. This approach is not particularly scalable or reproducible, but simulates what a user might do in such a situation. 3.2 Scoring Candidate Translations via a L2 Language Model To model how well a phrase fits into the L2 context, we score candidates with an n-gram lan357 guage model (LM) trained on a large sample of target-language text. Constructing and querying a large language model is potentially computationally expensive, so here we use the KenLM Language Model Toolkit and its Python interface (Heafield, 2011). Here our models were trained on the Wikipedia text mentioned previously (without filtering long sentences), with KenLM set to 5-grams and the default settings. 3.3 Scoring Candidate Translations via Dependency-Based Word Similarity The candidate ranking based on the n-gram language model – while quite useful – is based on very shallow information. We can also rank the candidate phrases based on how well each of the components fits into the L2 context using syntactic information. In this case, the fitness is measured in terms of dependency-based word similarity computed from dependency triple</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: faster and smaller language model queries. In Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation, pages 187–197, Edinburgh, Scotland, United Kingdom, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL-2003,</booktitle>
<pages>423--430</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="4408" citStr="Klein and Manning, 2003" startWordPosition="689" endWordPosition="692">or German and Spanish, we use recent Wikipedia dumps, which were converted to plain text with the Wikipedia Extractor tool.1 To save time during parsing, sentences longer than 25 words are removed. The remaining sentences are POS-tagged and dependency parsed using Mate Parser with its pre-trained models (Bohnet, 2010; Bohnet and Kuhn, 2012; Seeker and Kuhn, 2013). To keep our English Wikipedia dataset to a manageable size, we choose an older (2006), smaller dump. Long sentences are removed, and the remaining sentences are POS-tagged and dependency parsed using the pre-trained Stanford Parser (Klein and Manning, 2003; de Marneffe et al., 2006). The resulting sizes of the datasets are (roughly): German: 389M words, 28M sentences; Spanish: 147M words, 12M sentences; English: 253M words, 15M sentences. Dependencies extracted from these parsed datasets serve as training for the SIM system described in section 3.3. Google Books Syntactic N-grams For English, we also obtained dependency relationships for our word similarity statistics using the arcs dataset of the Google Books Syntactic N-Grams (Goldberg and Orwant, 2013), which has 919M items, each of which is a small “syntactic n-gram”, a term Goldberg and Or</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher Manning. 2003. Accurate unlexicalized parsing. In Proceedings ofACL-2003, pages 423–430, Sapporo, Japan.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5807" citStr="Koehn et al., 2007" startWordPosition="911" endWordPosition="914">counts of these dependency chains. We converted the longer chains into their component (head, dependent, label) triples and then collated these triples into counts, also for use in the SIM system. 3 System Design As previously mentioned, at run-time, our system decomposes the fragment translation task into two parts: generating many possible candidate translations, then scoring and ranking them in the targetlanguage context. 3.1 Constructing Candidate Translations As a starting point, we use phrase tables constructed in typical SMT fashion, built with the training scripts packaged with Moses (Koehn et al., 2007). These scripts preprocess the bitext, estimate word alignments with GIZA++ (Och and 1http://medialab.di.unipi.it/wiki/ Wikipedia_Extractor Ney, 2000) and then extract phrases with the grow-diag-final-and heuristic. At translation time, we look for the given source-language phrase in the phrase table, and if it is found, we take all translations of that phrase as our candidates. When translating a phrase that is not found in the phrase table, we try to construct a “synthetic phrase” out of the available components. This is done by listing, combinatorially, all ways to decompose the L1 phrase i</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT Summit.</booktitle>
<contexts>
<context position="3045" citStr="Koehn, 2005" startWordPosition="481" endWordPosition="482">ur approach is languageindependent, with accuracy varying due to the size of data sources and quality of input technology (e.g., syntactic parse accuracy). More features could easily be added to the log-linear model, and further explorations of ways to make use of targetlanguage knowledge could be promising. 2 Data Sources The data sources serve two major purposes for our system: For L2 candidate generation, we use Europarl and BabelNet; and for candidate ranking based on L2 context, we use Wikipedia and the Google Books Syntactic N-grams. Europarl The Europarl Parallel Corpus (Europarl, v7) (Koehn, 2005) is a corpus of proceedings of the European Parliament, containing 21 European languages with sentence alignments. From this corpus, we build phrase tables for English-Spanish, English-German, FrenchEnglish, Dutch-English. BabelNet In the cases where the constructed phrase tables do not contain a translation for a source phrase, we need to back off to smaller phrases and find candidate translations for these components. To better handle sparsity, we extend look-up using the multilingual dictionary BabelNet, v2.0 (Navigli and Ponzetto, 2012) as a way to find translation candidates. 356 Proceedi</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In International Conference on Machine Learning (ICML),</booktitle>
<volume>98</volume>
<pages>296--304</pages>
<contexts>
<context position="1587" citStr="Lin, 1998" startWordPosition="240" endWordPosition="241">entence. With the presence of this rich target-language context, the task is rather different from a standard machine translation setting, and our goal with our design was to make effective use of the L2 context, exploiting collocational relationships between tokens anywhere in the L2 context and the proposed fragment translations. Our system proceeds in several stages: (1) looking up or constructing candidate translations for the L1 fragment, (2) scoring candidate translations via a language model of the L2, (3) scoring candidate translations with a dependency-driven word similarity measure (Lin, 1998) (which we call SIM), and (4) combining the previous scores in a log-linear model to arrive at a final n-best list. Step 1 models transfer knowledge between This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ the L1 and L2; step 2 models facts about the L2 syntax, i.e., which translations fit well into the local context; step 3 models collocational and semantic tendencies of the L2; and step 4 gives different weights to each of the thr</context>
<context position="9234" citStr="Lin (1998)" startWordPosition="1466" endWordPosition="1467">et to 5-grams and the default settings. 3.3 Scoring Candidate Translations via Dependency-Based Word Similarity The candidate ranking based on the n-gram language model – while quite useful – is based on very shallow information. We can also rank the candidate phrases based on how well each of the components fits into the L2 context using syntactic information. In this case, the fitness is measured in terms of dependency-based word similarity computed from dependency triples consisting of the the head, the dependent, and the dependency label. We slightly adapted the word similarity measure by Lin (1998): SIM w w 2 * c(h, d, l) 1 ( i&apos; 2) = c(h, −, l) + c(−, d, l) ( ) where h = w1 and d = w2 and c(h, d, l) is the frequency with which a particular (head, dependent, label) dependency triple occurs in the L2 corpus. c(h, −, l) is the frequency with which a word occurs as a head in a dependency labeled l with any dependent. c(−, d, l) is the frequency with which a word occurs as a dependent in a dependency labeled l with any head. In the measure by Lin (1998), the numerator is defined as the information of all dependency features that w1 and w2 share, computed as the negative sum of the log probab</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In International Conference on Machine Learning (ICML), volume 98, pages 296– 304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network.</title>
<date>2012</date>
<journal>Artificial Intelligence,</journal>
<volume>193</volume>
<pages>250</pages>
<contexts>
<context position="3591" citStr="Navigli and Ponzetto, 2012" startWordPosition="562" endWordPosition="565">actic N-grams. Europarl The Europarl Parallel Corpus (Europarl, v7) (Koehn, 2005) is a corpus of proceedings of the European Parliament, containing 21 European languages with sentence alignments. From this corpus, we build phrase tables for English-Spanish, English-German, FrenchEnglish, Dutch-English. BabelNet In the cases where the constructed phrase tables do not contain a translation for a source phrase, we need to back off to smaller phrases and find candidate translations for these components. To better handle sparsity, we extend look-up using the multilingual dictionary BabelNet, v2.0 (Navigli and Ponzetto, 2012) as a way to find translation candidates. 356 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 356–360, Dublin, Ireland, August 23-24, 2014. Wikipedia For German and Spanish, we use recent Wikipedia dumps, which were converted to plain text with the Wikipedia Extractor tool.1 To save time during parsing, sentences longer than 25 words are removed. The remaining sentences are POS-tagged and dependency parsed using Mate Parser with its pre-trained models (Bohnet, 2010; Bohnet and Kuhn, 2012; Seeker and Kuhn, 2013). To keep our English Wikipedia dataset t</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2012. BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network. Artificial Intelligence, 193:217– 250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>440--447</pages>
<location>Hong Kong.</location>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL), pages 440–447, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="13550" citStr="Och, 2003" startWordPosition="2214" endWordPosition="2215">inear model, taking the log of each score – the direct and inverse translation probabilities, the LM probability, and the surface and POS SIM scores – and producing a weighted sum. Since the original scores are either probabilities or probability-like (in the range [0,1]), their logs are negative numbers, and at translation time we return the translation (or n-best) with the highest (least negative) score. This leaves us with the question of how to set the weights for the log-linear model; in this work, we use the ZMERT package (Zaidan, 2009), which implements the MERT optimization algorithm (Och, 2003), iteratively tuning the feature weights by repeatedly requesting n-best lists from the system. We used ZMERT with its default settings, optimizing our system’s BLEU scores on the provided development set. We chose, for convenience, BLEU as a stand-in for the wordlevel accuracy score, as BLEU scores are maximized when the system output matches the reference translations. 4 Experiments In figures 1-4, we show the scores on this year’s test set for running the two variations of our system: runt, the version without the SIM extensions, which we submitted for the evaluation, and SIM, with the exte</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Seeker</author>
<author>Jonas Kuhn</author>
</authors>
<title>Morphological and syntactic case in statistical dependency parsing.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="4150" citStr="Seeker and Kuhn, 2013" startWordPosition="647" endWordPosition="650">ilingual dictionary BabelNet, v2.0 (Navigli and Ponzetto, 2012) as a way to find translation candidates. 356 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 356–360, Dublin, Ireland, August 23-24, 2014. Wikipedia For German and Spanish, we use recent Wikipedia dumps, which were converted to plain text with the Wikipedia Extractor tool.1 To save time during parsing, sentences longer than 25 words are removed. The remaining sentences are POS-tagged and dependency parsed using Mate Parser with its pre-trained models (Bohnet, 2010; Bohnet and Kuhn, 2012; Seeker and Kuhn, 2013). To keep our English Wikipedia dataset to a manageable size, we choose an older (2006), smaller dump. Long sentences are removed, and the remaining sentences are POS-tagged and dependency parsed using the pre-trained Stanford Parser (Klein and Manning, 2003; de Marneffe et al., 2006). The resulting sizes of the datasets are (roughly): German: 389M words, 28M sentences; Spanish: 147M words, 12M sentences; English: 253M words, 15M sentences. Dependencies extracted from these parsed datasets serve as training for the SIM system described in section 3.3. Google Books Syntactic N-grams For English</context>
</contexts>
<marker>Seeker, Kuhn, 2013</marker>
<rawString>Wolfgang Seeker and Jonas Kuhn. 2013. Morphological and syntactic case in statistical dependency parsing. Computational Linguistics, 39(1):23–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
</authors>
<title>Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems.</title>
<date>2009</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>91--79</pages>
<contexts>
<context position="13488" citStr="Zaidan, 2009" startWordPosition="2205" endWordPosition="2206">t sources of information in some way. Here we use a familiar loglinear model, taking the log of each score – the direct and inverse translation probabilities, the LM probability, and the surface and POS SIM scores – and producing a weighted sum. Since the original scores are either probabilities or probability-like (in the range [0,1]), their logs are negative numbers, and at translation time we return the translation (or n-best) with the highest (least negative) score. This leaves us with the question of how to set the weights for the log-linear model; in this work, we use the ZMERT package (Zaidan, 2009), which implements the MERT optimization algorithm (Och, 2003), iteratively tuning the feature weights by repeatedly requesting n-best lists from the system. We used ZMERT with its default settings, optimizing our system’s BLEU scores on the provided development set. We chose, for convenience, BLEU as a stand-in for the wordlevel accuracy score, as BLEU scores are maximized when the system output matches the reference translations. 4 Experiments In figures 1-4, we show the scores on this year’s test set for running the two variations of our system: runt, the version without the SIM extensions,</context>
</contexts>
<marker>Zaidan, 2009</marker>
<rawString>Omar F. Zaidan. 2009. Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems. The Prague Bulletin of Mathematical Linguistics, 91:79–88.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>